paper_240308.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月8日 17:21
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed  6 Mar 24 19:00:00 GMT  to  Thu  7 Mar 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.03996
Date: Wed, 6 Mar 2024 19:12:41 GMT   (268kb)

Title: Rethinking Urban Flood Risk Assessment By Adapting Health Domain
  Perspective
Authors: Zhewei Liu, Kai Yin, Ali Mostafavi
Categories: cs.AI
\\
  Inspired by ideas from health risk assessment, this paper presents a new
perspective for flood risk assessment. The proposed perspective focuses on
three pillars for examining flood risk: (1) inherent susceptibility, (2)
mitigation strategies, and (3) external stressors. These pillars collectively
encompass the physical and environmental characteristics of urban areas, the
effectiveness of human-intervention measures, and the influence of
uncontrollable external factors, offering a fresh point of view for decoding
flood risks. For each pillar, we delineate its individual contributions to
flood risk and illustrate their interactive and overall impact. The
three-pillars model embodies a shift in focus from the quest to precisely model
and quantify flood risk to evaluating pathways to high flood risk. The shift in
perspective is intended to alleviate the quest for quantifying and predicting
flood risk at fine resolutions as a panacea for enhanced flood risk management.
The decomposition of flood risk pathways into the three intertwined pillars
(i.e., inherent factors, mitigation factors, and external factors) enables
evaluation of changes in factors within each pillar enhance and exacerbate
flood risk, creating a platform from which to inform plans, decisions, and
actions. Building on this foundation, we argue that a flood risk pathway
analysis approach, which examines the individual and collective impacts of
inherent factors, mitigation strategies, and external stressors, is essential
for a nuanced evaluation of flood risk. Accordingly, the proposed perspective
could complement the existing frameworks and approaches for flood risk
assessment.
\\ ( https://arxiv.org/abs/2403.03996 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03997
Date: Wed, 6 Mar 2024 19:13:53 GMT   (48kb)

Title: Guiding Enumerative Program Synthesis with Large Language Models
Authors: Yixuan Li, Julian Parsert, Elizabeth Polgreen
Categories: cs.AI
Comments: 27 pages
\\
  Pre-trained Large Language Models (LLMs) are beginning to dominate the
discourse around automatic code generation with natural language
specifications. In contrast, the best-performing synthesizers in the domain of
formal synthesis with precise logical specifications are still based on
enumerative algorithms. In this paper, we evaluate the abilities of LLMs to
solve formal synthesis benchmarks by carefully crafting a library of prompts
for the domain. When one-shot synthesis fails, we propose a novel enumerative
synthesis algorithm, which integrates calls to an LLM into a weighted
probabilistic search. This allows the synthesizer to provide the LLM with
information about the progress of the enumerator, and the LLM to provide the
enumerator with syntactic guidance in an iterative loop. We evaluate our
techniques on benchmarks from the Syntax-Guided Synthesis (SyGuS) competition.
We find that GPT-3.5 as a stand-alone tool for formal synthesis is easily
outperformed by state-of-the-art formal synthesis algorithms, but our approach
integrating the LLM into an enumerative synthesis algorithm shows significant
performance gains over both the LLM and the enumerative synthesizer alone and
the winning SyGuS competition tool.
\\ ( https://arxiv.org/abs/2403.03997 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04017
Date: Wed, 6 Mar 2024 19:59:17 GMT   (51kb)

Title: Learning Guided Automated Reasoning: A Brief Survey
Authors: Lasse Blaauwbroek, David Cerna, Thibault Gauthier, Jan Jakub\r{u}v,
  Cezary Kaliszyk, Martin Suda, Josef Urban
Categories: cs.AI cs.LG cs.LO cs.NE cs.SC
\\
  Automated theorem provers and formal proof assistants are general reasoning
systems that are in theory capable of proving arbitrarily hard theorems, thus
solving arbitrary problems reducible to mathematics and logical reasoning. In
practice, such systems however face large combinatorial explosion, and
therefore include many heuristics and choice points that considerably influence
their performance. This is an opportunity for trained machine learning
predictors, which can guide the work of such reasoning systems. Conversely,
deductive search supported by the notion of logically valid proof allows one to
train machine learning systems on large reasoning corpora. Such bodies of proof
are usually correct by construction and when combined with more and more
precise trained guidance they can be boostrapped into very large corpora, with
increasingly long reasoning chains and possibly novel proof ideas. In this
paper we provide an overview of several automated reasoning and theorem proving
domains and the learning and AI methods that have been so far developed for
them. These include premise selection, proof guidance in several settings, AI
systems and feedback loops iterating between reasoning and learning, and
symbolic classification problems.
\\ ( https://arxiv.org/abs/2403.04017 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04035
Date: Wed, 6 Mar 2024 20:25:04 GMT   (738kb,D)

Title: Personalizing explanations of AI-driven hints to users cognitive
  abilities: an empirical evaluation
Authors: Vedant Bahel, Harshinee Sriram and Cristina Conati
Categories: cs.AI cs.CY cs.HC
\\
  We investigate personalizing the explanations that an Intelligent Tutoring
System generates to justify the hints it provides to students to foster their
learning. The personalization targets students with low levels of two traits,
Need for Cognition and Conscientiousness, and aims to enhance these students'
engagement with the explanations, based on prior findings that these students
do not naturally engage with the explanations but they would benefit from them
if they do. To evaluate the effectiveness of the personalization, we conducted
a user study where we found that our proposed personalization significantly
increases our target users' interaction with the hint explanations, their
understanding of the hints and their learning. Hence, this work provides
valuable insights into effectively personalizing AI-driven explanations for
cognitively demanding tasks such as learning.
\\ ( https://arxiv.org/abs/2403.04035 ,  738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04072
Date: Wed, 6 Mar 2024 22:06:21 GMT   (443kb,D)

Title: Forecasting and Mitigating Disruptions in Public Bus Transit Services
Authors: Chaeeun Han, Jose Paolo Talusan, Dan Freudberg, Ayan Mukhopadhyay,
  Abhishek Dubey, Aron Laszka
Categories: cs.AI cs.CY cs.LG
\\
  Public transportation systems often suffer from unexpected fluctuations in
demand and disruptions, such as mechanical failures and medical emergencies.
These fluctuations and disruptions lead to delays and overcrowding, which are
detrimental to the passengers' experience and to the overall performance of the
transit service. To proactively mitigate such events, many transit agencies
station substitute (reserve) vehicles throughout their service areas, which
they can dispatch to augment or replace vehicles on routes that suffer
overcrowding or disruption. However, determining the optimal locations where
substitute vehicles should be stationed is a challenging problem due to the
inherent randomness of disruptions and due to the combinatorial nature of
selecting locations across a city. In collaboration with the transit agency of
Nashville, TN, we address this problem by introducing data-driven statistical
and machine-learning models for forecasting disruptions and an effective
randomized local-search algorithm for selecting locations where substitute
vehicles are to be stationed. Our research demonstrates promising results in
proactive disruption management, offering a practical and easily implementable
solution for transit agencies to enhance the reliability of their services. Our
results resonate beyond mere operational efficiency: by advancing proactive
strategies, our approach fosters more resilient and accessible public
transportation, contributing to equitable urban mobility and ultimately
benefiting the communities that rely on public transportation the most.
\\ ( https://arxiv.org/abs/2403.04072 ,  443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04087
Date: Wed, 6 Mar 2024 22:32:49 GMT   (2825kb,D)

Title: The Cognitive Type Project -- Mapping Typography to Cognition
Authors: Nik Bear Brown
Categories: cs.AI
\\
  The Cognitive Type Project is focused on developing computational tools to
enable the design of typefaces with varying cognitive properties. This
initiative aims to empower typographers to craft fonts that enhance
click-through rates for online ads, improve reading levels in children's books,
enable dyslexics to create personalized type, or provide insights into customer
reactions to textual content in media. A significant challenge in research
related to mapping typography to cognition is the creation of thousands of
typefaces with minor variations, a process that is both labor-intensive and
requires the expertise of skilled typographers. Cognitive science research
highlights that the design and form of letters, along with the text's overall
layout, are crucial in determining the ease of reading and other cognitive
properties of type such as perceived beauty and memorability. These factors
affect not only the legibility and clarity of information presentation but also
the likability of a typeface.
\\ ( https://arxiv.org/abs/2403.04087 ,  2825kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04105
Date: Wed, 6 Mar 2024 23:17:16 GMT   (1086kb,D)

Title: Artificial Intelligence Exploring the Patent Field
Authors: Lekang Jiang, Stephan Goetz
Categories: cs.AI
Comments: 53 pages, 14 figures, 5 tables
\\
  Advanced language-processing and machine-learning techniques promise massive
efficiency improvements in the previously widely manual field of patent and
technical knowledge management. This field presents large-scale and complex
data with very precise contents and language representation of those contents.
Particularly, patent texts can differ from mundane texts in various aspects,
which entails significant opportunities and challenges. This paper presents a
systematic overview of patent-related tasks and popular methodologies with a
special focus on evolving and promising techniques. Language processing and
particularly large language models as well as the recent boost of general
generative methods promise to become game changers in the patent field. The
patent literature and the fact-based argumentative procedures around patents
appear almost as an ideal use case. However, patents entail a number of
difficulties with which existing models struggle. The paper introduces
fundamental aspects of patents and patent-related data that affect technology
that wants to explore or manage them. It further reviews existing methods and
approaches and points out how important reliable and unbiased evaluation
metrics become. Although research has made substantial progress on certain
tasks, the performance across many others remains suboptimal, sometimes because
of either the special nature of patents and their language or inconsistencies
between legal terms and the everyday meaning of terms. Moreover, yet few
methods have demonstrated the ability to produce satisfactory text for specific
sections of patents. By pointing out key developments, opportunities, and gaps,
we aim to encourage further research and accelerate the advancement of this
field.
\\ ( https://arxiv.org/abs/2403.04105 ,  1086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04106
Date: Wed, 6 Mar 2024 23:20:34 GMT   (1634kb)

Title: Understanding Biology in the Age of Artificial Intelligence
Authors: Elsa Lawrence, Adham El-Shazly, Srijit Seal, Chaitanya K Joshi, Pietro
  Li\`o, Shantanu Singh, Andreas Bender, Pietro Sormanni, Matthew Greenig
Categories: cs.AI
\\
  Modern life sciences research is increasingly relying on artificial
intelligence approaches to model biological systems, primarily centered around
the use of machine learning (ML) models. Although ML is undeniably useful for
identifying patterns in large, complex data sets, its widespread application in
biological sciences represents a significant deviation from traditional methods
of scientific inquiry. As such, the interplay between these models and
scientific understanding in biology is a topic with important implications for
the future of scientific research, yet it is a subject that has received little
attention. Here, we draw from an epistemological toolkit to contextualize
recent applications of ML in biological sciences under modern philosophical
theories of understanding, identifying general principles that can guide the
design and application of ML systems to model biological phenomena and advance
scientific knowledge. We propose that conceptions of scientific understanding
as information compression, qualitative intelligibility, and dependency
relation modelling provide a useful framework for interpreting ML-mediated
understanding of biological systems. Through a detailed analysis of two key
application areas of ML in modern biological research - protein structure
prediction and single cell RNA-sequencing - we explore how these features have
thus far enabled ML systems to advance scientific understanding of their target
phenomena, how they may guide the development of future ML models, and the key
obstacles that remain in preventing ML from achieving its potential as a tool
for biological discovery. Consideration of the epistemological features of ML
applications in biology will improve the prospects of these methods to solve
important problems and advance scientific understanding of living systems.
\\ ( https://arxiv.org/abs/2403.04106 ,  1634kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04121
Date: Thu, 7 Mar 2024 00:36:32 GMT   (5418kb,D)

Title: Can Large Language Models Reason and Plan?
Authors: Subbarao Kambhampati
Categories: cs.AI cs.CL cs.LG
Comments: arXiv admin note: text overlap with arXiv:2402.01817
Journal-ref: Annals of The New York Academy of Sciences; March 2024
DOI: 10.1111/nyas.15125
\\
  While humans sometimes do show the capability of correcting their own
erroneous guesses with self-critiquing, there seems to be no basis for that
assumption in the case of LLMs.
\\ ( https://arxiv.org/abs/2403.04121 ,  5418kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04124
Date: Thu, 7 Mar 2024 00:44:11 GMT   (1720kb,D)

Title: Privacy-preserving Fine-tuning of Large Language Models through Flatness
Authors: Tiejin Chen, Longchao Da, Huixue Zhou, Pingzhi Li, Kaixiong Zhou,
  Tianlong Chen, Hua Wei
Categories: cs.AI
Comments: Accepted to ICLR 2024 SeT LLM Workshop
ACM-class: I.2
\\
  The privacy concerns associated with the use of Large Language Models (LLMs)
have grown recently with the development of LLMs such as ChatGPT. Differential
Privacy (DP) techniques are explored in existing work to mitigate their privacy
risks at the cost of generalization degradation. Our paper reveals that the
flatness of DP-trained models' loss landscape plays an essential role in the
trade-off between their privacy and generalization. We further propose a
holistic framework to enforce appropriate weight flatness, which substantially
improves model generalization with competitive privacy preservation. It
innovates from three coarse-to-grained levels, including perturbation-aware
min-max optimization on model weights within a layer, flatness-guided sparse
prefix-tuning on weights across layers, and weight knowledge distillation
between DP \& non-DP weights copies. Comprehensive experiments of both
black-box and white-box scenarios are conducted to demonstrate the
effectiveness of our proposal in enhancing generalization and maintaining DP
characteristics. For instance, on text classification dataset QNLI, DP-Flat
achieves similar performance with non-private full fine-tuning but with DP
guarantee under privacy budget $\epsilon=3$, and even better performance given
higher privacy budgets. Codes are provided in the supplement.
\\ ( https://arxiv.org/abs/2403.04124 ,  1720kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04132
Date: Thu, 7 Mar 2024 01:22:38 GMT   (1652kb,D)

Title: Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference
Authors: Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas
  Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan,
  Joseph E. Gonzalez, Ion Stoica
Categories: cs.AI cs.CL
\\
  Large Language Models (LLMs) have unlocked new capabilities and applications;
however, evaluating the alignment with human preferences still poses
significant challenges. To address this issue, we introduce Chatbot Arena, an
open platform for evaluating LLMs based on human preferences. Our methodology
employs a pairwise comparison approach and leverages input from a diverse user
base through crowdsourcing. The platform has been operational for several
months, amassing over 240K votes. This paper describes the platform, analyzes
the data we have collected so far, and explains the tried-and-true statistical
methods we are using for efficient and accurate evaluation and ranking of
models. We confirm that the crowdsourced questions are sufficiently diverse and
discriminating and that the crowdsourced human votes are in good agreement with
those of expert raters. These analyses collectively establish a robust
foundation for the credibility of Chatbot Arena. Because of its unique value
and openness, Chatbot Arena has emerged as one of the most referenced LLM
leaderboards, widely cited by leading LLM developers and companies. Our demo is
publicly available at \url{https://chat.lmsys.org}.
\\ ( https://arxiv.org/abs/2403.04132 ,  1652kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04135
Date: Thu, 7 Mar 2024 01:29:48 GMT   (463kb,D)

Title: Unsupervised Learning of Harmonic Analysis Based on Neural HSMM with
  Code Quality Templates
Authors: Yui Uehara
Categories: cs.AI
Comments: 20 pages, 5 figures, the original edition of this paper will be
  published in the ICNMC2024 Proceedings and this arXiv publication is a copy
\\
  This paper presents a method of unsupervised learning of harmonic analysis
based on a hidden semi-Markov model (HSMM). We introduce the chord quality
templates, which specify the probability of pitch class emissions given a root
note and a chord quality. Other probability distributions that comprise the
HSMM are automatically learned via unsupervised learning, which has been a
challenge in existing research. The results of the harmonic analysis of the
proposed model were evaluated using existing labeled data. While our proposed
method has yet to perform as well as existing models that used supervised
learning and complex rule design, it has the advantage of not requiring
expensive labeled data or rule elaboration. Furthermore, we also show how to
recognize the tonic without prior knowledge, based on the transition
probabilities of the Markov model.
\\ ( https://arxiv.org/abs/2403.04135 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04140
Date: Thu, 7 Mar 2024 01:41:12 GMT   (3815kb,D)

Title: Contrastive Augmented Graph2Graph Memory Interaction for Few Shot
  Continual Learning
Authors: Biqing Qi, Junqi Gao, Xingquan Chen, Dong Li, Jianxing Liu, Ligang Wu
  and Bowen Zhou
Categories: cs.AI
Comments: 12 Pages, 5 figures
\\
  Few-Shot Class-Incremental Learning (FSCIL) has gained considerable attention
in recent years for its pivotal role in addressing continuously arriving
classes. However, it encounters additional challenges. The scarcity of samples
in new sessions intensifies overfitting, causing incompatibility between the
output features of new and old classes, thereby escalating catastrophic
forgetting. A prevalent strategy involves mitigating catastrophic forgetting
through the Explicit Memory (EM), which comprise of class prototypes. However,
current EM-based methods retrieves memory globally by performing
Vector-to-Vector (V2V) interaction between features corresponding to the input
and prototypes stored in EM, neglecting the geometric structure of local
features. This hinders the accurate modeling of their positional relationships.
To incorporate information of local geometric structure, we extend the V2V
interaction to Graph-to-Graph (G2G) interaction. For enhancing local structures
for better G2G alignment and the prevention of local feature collapse, we
propose the Local Graph Preservation (LGP) mechanism. Additionally, to address
sample scarcity in classes from new sessions, the Contrast-Augmented G2G
(CAG2G) is introduced to promote the aggregation of same class features thus
helps few-shot learning. Extensive comparisons on CIFAR100, CUB200, and the
challenging ImageNet-R dataset demonstrate the superiority of our method over
existing methods.
\\ ( https://arxiv.org/abs/2403.04140 ,  3815kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04204
Date: Thu, 7 Mar 2024 04:19:13 GMT   (5706kb,D)

Title: On the Essence and Prospect: An Investigation of Alignment Approaches
  for Big Models
Authors: Xinpeng Wang, Shitong Duan, Xiaoyuan Yi, Jing Yao, Shanlin Zhou,
  Zhihua Wei, Peng Zhang, Dongkuan Xu, Maosong Sun, Xing Xie
Categories: cs.AI cs.CL
Comments: 23 pages, 7 figures
\\
  Big models have achieved revolutionary breakthroughs in the field of AI, but
they might also pose potential concerns. Addressing such concerns, alignment
technologies were introduced to make these models conform to human preferences
and values. Despite considerable advancements in the past year, various
challenges lie in establishing the optimal alignment strategy, such as data
cost and scalable oversight, and how to align remains an open question. In this
survey paper, we comprehensively investigate value alignment approaches. We
first unpack the historical context of alignment tracing back to the 1920s
(where it comes from), then delve into the mathematical essence of alignment
(what it is), shedding light on the inherent challenges. Following this
foundation, we provide a detailed examination of existing alignment methods,
which fall into three categories: Reinforcement Learning, Supervised
Fine-Tuning, and In-context Learning, and demonstrate their intrinsic
connections, strengths, and limitations, helping readers better understand this
research area. In addition, two emerging topics, personal alignment, and
multimodal alignment, are also discussed as novel frontiers in this field.
Looking forward, we discuss potential alignment paradigms and how they could
handle remaining challenges, prospecting where future alignment will go.
\\ ( https://arxiv.org/abs/2403.04204 ,  5706kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04261
Date: Thu, 7 Mar 2024 06:52:51 GMT   (1526kb)

Title: Advancing Biomedical Text Mining with Community Challenges
Authors: Hui Zong, Rongrong Wu, Jiaxue Cha, Erman Wu, Jiakun Li, Liang Tao,
  Zuofeng Li, Buzhou Tang, Bairong Shen
Categories: cs.AI cs.CL
\\
  The field of biomedical research has witnessed a significant increase in the
accumulation of vast amounts of textual data from various sources such as
scientific literatures, electronic health records, clinical trial reports, and
social media. However, manually processing and analyzing these extensive and
complex resources is time-consuming and inefficient. To address this challenge,
biomedical text mining, also known as biomedical natural language processing,
has garnered great attention. Community challenge evaluation competitions have
played an important role in promoting technology innovation and
interdisciplinary collaboration in biomedical text mining research. These
challenges provide platforms for researchers to develop state-of-the-art
solutions for data mining and information processing in biomedical research. In
this article, we review the recent advances in community challenges specific to
Chinese biomedical text mining. Firstly, we collect the information of these
evaluation tasks, such as data sources and task types. Secondly, we conduct
systematic summary and comparative analysis, including named entity
recognition, entity normalization, attribute extraction, relation extraction,
event extraction, text classification, text similarity, knowledge graph
construction, question answering, text generation, and large language model
evaluation. Then, we summarize the potential clinical applications of these
community challenge tasks from translational informatics perspective. Finally,
we discuss the contributions and limitations of these community challenges,
while highlighting future directions in the era of large language models.
\\ ( https://arxiv.org/abs/2403.04261 ,  1526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04264
Date: Thu, 7 Mar 2024 06:56:24 GMT   (1502kb,D)

Title: Competitive Facility Location under Random Utilities and Routing
  Constraints
Authors: Hoang Giang Pham, Tien Thanh Dam, Ngan Ha Duong, Tien Mai and Minh
  Hoang Ha
Categories: cs.AI
\\
  In this paper, we study a facility location problem within a competitive
market context, where customer demand is predicted by a random utility choice
model. Unlike prior research, which primarily focuses on simple constraints
such as a cardinality constraint on the number of selected locations, we
introduce routing constraints that necessitate the selection of locations in a
manner that guarantees the existence of a tour visiting all chosen locations
while adhering to a specified tour length upper bound. Such routing constraints
find crucial applications in various real-world scenarios. The problem at hand
features a non-linear objective function, resulting from the utilization of
random utilities, together with complex routing constraints, making it
computationally challenging. To tackle this problem, we explore three types of
valid cuts, namely, outer-approximation and submodular cuts to handle the
nonlinear objective function, as well as sub-tour elimination cuts to address
the complex routing constraints. These lead to the development of two exact
solution methods: a nested cutting plane and nested branch-and-cut algorithms,
where these valid cuts are iteratively added to a master problem through two
nested loops. We also prove that our nested cutting plane method always
converges to optimality after a finite number of iterations. Furthermore, we
develop a local search-based metaheuristic tailored for solving large-scale
instances and show its pros and cons compared to exact methods. Extensive
experiments are conducted on problem instances of varying sizes, demonstrating
that our approach excels in terms of solution quality and computation time when
compared to other baseline approaches.
\\ ( https://arxiv.org/abs/2403.04264 ,  1502kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04280
Date: Thu, 7 Mar 2024 07:24:32 GMT   (221kb,D)

Title: A New Benchmark for Evaluating Automatic Speech Recognition in the
  Arabic Call Domain
Authors: Qusai Abo Obaidah, Muhy Eddin Zater, Adnan Jaljuli, Ali Mahboub, Asma
  Hakouz, Bashar Alfrou, Yazan Estaitia
Categories: cs.AI cs.CL
\\
  This work is an attempt to introduce a comprehensive benchmark for Arabic
speech recognition, specifically tailored to address the challenges of
telephone conversations in Arabic language. Arabic, characterized by its rich
dialectal diversity and phonetic complexity, presents a number of unique
challenges for automatic speech recognition (ASR) systems. These challenges are
further amplified in the domain of telephone calls, where audio quality,
background noise, and conversational speech styles negatively affect
recognition accuracy. Our work aims to establish a robust benchmark that not
only encompasses the broad spectrum of Arabic dialects but also emulates the
real-world conditions of call-based communications. By incorporating diverse
dialectical expressions and accounting for the variable quality of call
recordings, this benchmark seeks to provide a rigorous testing ground for the
development and evaluation of ASR systems capable of navigating the
complexities of Arabic speech in telephonic contexts. This work also attempts
to establish a baseline performance evaluation using state-of-the-art ASR
technologies.
\\ ( https://arxiv.org/abs/2403.04280 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04292
Date: Thu, 7 Mar 2024 07:39:54 GMT   (1148kb)

Title: A challenge in A(G)I, cybernetics revived in the Ouroboros Model as one
  algorithm for all thinking
Authors: Knud Thomsen
Categories: cs.AI
Comments: 26 pages, 11 figures
MSC-class: 93-08
ACM-class: I.2.0; I.2.4
Journal-ref: Artificial Intelligence and Autonomous Systems Volume 1 Issue 1,
  2024
DOI: 10.55092/aias20240001
\\
  A topical challenge for algorithms in general and for automatic image
categorization and generation in particular is presented in the form of a
drawing for AI to understand. In a second vein, AI is challenged to produce
something similar from verbal description. The aim of the paper is to highlight
strengths and deficiencies of current Artificial Intelligence approaches while
coarsely sketching a way forward. A general lack of encompassing
symbol-embedding and (not only) -grounding in some bodily basis is made
responsible for current deficiencies. A concomitant dearth of hierarchical
organization of concepts follows suite. As a remedy for these shortcomings, it
is proposed to take a wide step back and to newly incorporate aspects of
cybernetics and analog control processes. It is claimed that a promising
overarching perspective is provided by the Ouroboros Model with a valid and
versatile algorithmic backbone for general cognition at all accessible levels
of abstraction and capabilities. Reality, rules, truth, and Free Will are all
useful abstractions according to the Ouroboros Model. Logic deduction as well
as intuitive guesses are claimed as produced on the basis of one
compartmentalized memory for schemata and a pattern-matching, i.e., monitoring
process termed consumption analysis. The latter directs attention on short
(attention proper) and also on long times scales (emotional biases). In this
cybernetic approach, discrepancies between expectations and actual activations
(e.g., sensory precepts) drive the general process of cognition and at the same
time steer the storage of new and adapted memory entries. Dedicated structures
in the human brain work in concert according to this scheme.
\\ ( https://arxiv.org/abs/2403.04292 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04293
Date: Thu, 7 Mar 2024 07:40:53 GMT   (5091kb,D)

Title: MKF-ADS: A Multi-Knowledge Fused Anomaly Detection System for Automotive
Authors: Pengzhou Cheng, Zongru Wu, and Gongshen Liu
Categories: cs.AI cs.CR
Comments: 14 figures, 5 tables
\\
  With the requirements of Intelligent Transport Systems (ITSs) for extensive
connectivity of Electronic Control Units (ECUs) to the outside world, safety
and security have become stringent problems. Intrusion detection systems (IDSs)
are a crucial safety component in remediating Controller Area Network (CAN) bus
vulnerabilities. However, supervised-based IDSs fail to identify complexity
attacks and anomaly-based IDSs have higher false alarms owing to capability
bottleneck. In this paper, we propose a novel multi-knowledge fused anomaly
detection model, called MKF-IDS. Specifically, the method designs an
integration framework, including spatial-temporal correlation with an attention
mechanism (STcAM) module and patch sparse-transformer module (PatchST). The
STcAM with fine-pruning uses one-dimensional convolution (Conv1D) to extract
spatial features and subsequently utilizes the Bidirectional Long Short Term
Memory (Bi-LSTM) to extract the temporal features, where the attention
mechanism will focus on the important time steps. Meanwhile, the PatchST
captures the combined long-time historical features from independent univariate
time series. Finally, the proposed method is based on knowledge distillation to
STcAM as a student model for learning intrinsic knowledge and cross the ability
to mimic PatchST. In the detection phase, the MKF-ADS only deploys STcAM to
maintain efficiency in a resource-limited IVN environment. Moreover, the
redundant noisy signal is reduced with bit flip rate and boundary decision
estimation. We conduct extensive experiments on six simulation attack scenarios
across various CAN IDs and time steps, and two real attack scenarios, which
present a competitive prediction and detection performance. Compared with the
baseline in the same paradigm, the error rate and FAR are 2.62% and 2.41% and
achieve a promising F1-score of 97.3%.
\\ ( https://arxiv.org/abs/2403.04293 ,  5091kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04311
Date: Thu, 7 Mar 2024 08:30:26 GMT   (1206kb,D)

Title: ALTO: An Efficient Network Orchestrator for Compound AI Systems
Authors: Keshav Santhanam, Deepti Raghavan, Muhammad Shahir Rahman, Thejas
  Venkatesh, Neha Kunjal, Pratiksha Thaker, Philip Levis, Matei Zaharia
Categories: cs.AI cs.CL cs.DC cs.IR
\\
  We present ALTO, a network orchestrator for efficiently serving compound AI
systems such as pipelines of language models. ALTO achieves high throughput and
low latency by taking advantage of an optimization opportunity specific to
generative language models: streaming intermediate outputs. As language models
produce outputs token by token, ALTO exposes opportunities to stream
intermediate outputs between stages when possible. We highlight two new
challenges of correctness and load balancing which emerge when streaming
intermediate data across distributed pipeline stage instances. We also motivate
the need for an aggregation-aware routing interface and distributed
prompt-aware scheduling to address these challenges. We demonstrate the impact
of ALTO's partial output streaming on a complex chatbot verification pipeline,
increasing throughput by up to 3x for a fixed latency target of 4 seconds /
request while also reducing tail latency by 1.8x compared to a baseline serving
approach.
\\ ( https://arxiv.org/abs/2403.04311 ,  1206kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04343
Date: Thu, 7 Mar 2024 09:11:16 GMT   (8140kb,D)

Title: CoTBal: Comprehensive Task Balancing for Multi-Task Visual Instruction
  Tuning
Authors: Yanqi Dai, Dong Jing, Nanyi Fei, Zhiwu Lu
Categories: cs.AI
\\
  Visual instruction tuning is a key training stage of large multimodal models
(LMMs). Nevertheless, the common practice of indiscriminately mixing
instruction-following data from various tasks may result in suboptimal overall
performance due to different instruction formats and knowledge domains across
tasks. To mitigate this issue, we propose a novel Comprehensive Task Balancing
(CoTBal) algorithm for multi-task visual instruction tuning of LMMs. To our
knowledge, this is the first work that explores multi-task optimization in
visual instruction tuning. Specifically, we consider two key dimensions for
task balancing: (1) Inter-Task Contribution, the phenomenon where learning one
task potentially enhances the performance in other tasks, attributable to the
overlapping knowledge domains, and (2) Intra-Task Difficulty, which refers to
the learning difficulty within a single task. By quantifying these two
dimensions with performance-based metrics, task balancing is thus enabled by
assigning more weights to tasks that offer substantial contributions to others,
receive minimal contributions from others, and also have great intra-task
difficulties. Experiments show that our CoTBal leads to superior overall
performance in multi-task visual instruction tuning.
\\ ( https://arxiv.org/abs/2403.04343 ,  8140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04366
Date: Thu, 7 Mar 2024 09:51:11 GMT   (1643kb,D)

Title: Enhancing Court View Generation with Knowledge Injection and Guidance
Authors: Ang Li, Yiquan Wu, Yifei Liu, Fei Wu, Ming Cai, Kun Kuang
Categories: cs.AI
\\
  Court View Generation (CVG) is a challenging task in the field of Legal
Artificial Intelligence (LegalAI), which aims to generate court views based on
the plaintiff claims and the fact descriptions. While Pretrained Language
Models (PLMs) have showcased their prowess in natural language generation,
their application to the complex, knowledge-intensive domain of CVG often
reveals inherent limitations. In this paper, we present a novel approach, named
Knowledge Injection and Guidance (KIG), designed to bolster CVG using PLMs. To
efficiently incorporate domain knowledge during the training stage, we
introduce a knowledge-injected prompt encoder for prompt tuning, thereby
reducing computational overhead. Moreover, to further enhance the model's
ability to utilize domain knowledge, we employ a generating navigator, which
dynamically guides the text generation process in the inference stage without
altering the model's architecture, making it readily transferable.
Comprehensive experiments on real-world data demonstrate the effectiveness of
our approach compared to several established baselines, especially in the
responsivity of claims, where it outperforms the best baseline by 11.87%.
\\ ( https://arxiv.org/abs/2403.04366 ,  1643kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04369
Date: Thu, 7 Mar 2024 09:57:42 GMT   (1836kb,D)

Title: From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge
  Prediction
Authors: Ang Li, Qiangchao Chen, Yiquan Wu, Ming Cai, Xiang Zhou, Fei Wu, Kun
  Kuang
Categories: cs.AI cs.CL
\\
  Confusing charge prediction is a challenging task in legal AI, which involves
predicting confusing charges based on fact descriptions. While existing charge
prediction methods have shown impressive performance, they face significant
challenges when dealing with confusing charges, such as Snatch and Robbery. In
the legal domain, constituent elements play a pivotal role in distinguishing
confusing charges. Constituent elements are fundamental behaviors underlying
criminal punishment and have subtle distinctions among charges. In this paper,
we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces
domain knowledge regarding constituent elements to guide the model in making
judgments on confusing charges, much like a judge's reasoning process.
Specifically, we first construct a legal knowledge graph containing constituent
elements to help select keywords for each charge, forming a word bag.
Subsequently, to guide the model's attention towards the differentiating
information for each charge within the context, we expand the attention
mechanism and introduce a new loss function with attention supervision through
words in the word bag. We construct the confusing charges dataset from
real-world judicial documents. Experiments demonstrate the effectiveness of our
method, especially in maintaining exceptional performance in imbalanced label
distributions.
\\ ( https://arxiv.org/abs/2403.04369 ,  1836kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04449
Date: Thu, 7 Mar 2024 12:37:52 GMT   (82kb,D)

Title: Feedback-Generation for Programming Exercises With GPT-4
Authors: Imen Azaiz, Natalie Kiesler, Sven Strickroth
Categories: cs.AI
Comments: accepted at ITiCSE 2024, Milan, Italy
\\
  Ever since Large Language Models (LLMs) and related applications have become
broadly available, several studies investigated their potential for assisting
educators and supporting students in higher education. LLMs such as Codex,
GPT-3.5, and GPT 4 have shown promising results in the context of large
programming courses, where students can benefit from feedback and hints if
provided timely and at scale. This paper explores the quality of GPT-4 Turbo's
generated output for prompts containing both the programming task specification
and a student's submission as input. Two assignments from an introductory
programming course were selected, and GPT-4 was asked to generate feedback for
55 randomly chosen, authentic student programming submissions. The output was
qualitatively analyzed regarding correctness, personalization, fault
localization, and other features identified in the material. Compared to prior
work and analyses of GPT-3.5, GPT-4 Turbo shows notable improvements. For
example, the output is more structured and consistent. GPT-4 Turbo can also
accurately identify invalid casing in student programs' output. In some cases,
the feedback also includes the output of the student program. At the same time,
inconsistent feedback was noted such as stating that the submission is correct
but an error needs to be fixed. The present work increases our understanding of
LLMs' potential, limitations, and how to integrate them into e-assessment
systems, pedagogical scenarios, and instructing students who are using
applications based on GPT-4.
\\ ( https://arxiv.org/abs/2403.04449 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04471
Date: Thu, 7 Mar 2024 13:16:07 GMT   (829kb)

Title: The Shutdown Problem: Three Theorems
Authors: Elliott Thornley
Categories: cs.AI
\\
  I explain the shutdown problem: the problem of designing artificial agents
that (1) shut down when a shutdown button is pressed, (2) don't try to prevent
or cause the pressing of the shutdown button, and (3) otherwise pursue goals
competently. I prove three theorems that make the difficulty precise. These
theorems show that agents satisfying some innocuous-seeming conditions will
often try to prevent or cause the pressing of the shutdown button, even in
cases where it's costly to do so. And patience trades off against
shutdownability: the more patient an agent, the greater the costs that agent is
willing to incur to manipulate the shutdown button. I end by noting that these
theorems can guide our search for solutions.
\\ ( https://arxiv.org/abs/2403.04471 ,  829kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04483
Date: Thu, 7 Mar 2024 13:36:08 GMT   (7803kb,D)

Title: GraphInstruct: Empowering Large Language Models with Graph Understanding
  and Reasoning Capability
Authors: Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi
  Jiang, Xing Xie, Hai Jin
Categories: cs.AI cs.CL
Comments: 9 pages
\\
  Evaluating and enhancing the general capabilities of large language models
(LLMs) has been an important research topic. Graph is a common data structure
in the real world, and understanding graph data is a crucial part for advancing
general intelligence. To evaluate and enhance the graph understanding abilities
of LLMs, in this paper, we propose a benchmark named GraphInstruct, which
comprehensively includes 21 classical graph reasoning tasks, providing diverse
graph generation pipelines and detailed reasoning steps. Based on
GraphInstruct, we further construct GraphLM through efficient
instruction-tuning, which shows prominent graph understanding capability. In
order to enhance the LLM with graph reasoning capability as well, we propose a
step mask training strategy, and construct a model named GraphLM+. As one of
the pioneering efforts to enhance the graph understanding and reasoning
abilities of LLMs, extensive experiments have demonstrated the superiority of
GraphLM and GraphLM+ over other LLMs. We look forward to more researchers
exploring the potential of LLMs in the graph data mining domain through
GraphInstruct. Our code for generating GraphInstruct is released publicly at:
https://github.com/CGCL-codes/GraphInstruct.
\\ ( https://arxiv.org/abs/2403.04483 ,  7803kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04504
Date: Thu, 7 Mar 2024 14:04:33 GMT   (11535kb,D)

Title: Improving Matrix Completion by Exploiting Rating Ordinality in Graph
  Neural Networks
Authors: Jaehyun Lee, Seonku Kang, Hwanjo Yu
Categories: cs.AI
Comments: 4 pages, 2 figures, 3 tables
\\
  Matrix completion is an important area of research in recommender systems.
Recent methods view a rating matrix as a user-item bi-partite graph with
labeled edges denoting observed ratings and predict the edges between the user
and item nodes by using the graph neural network (GNN). Despite their
effectiveness, they treat each rating type as an independent relation type and
thus cannot sufficiently consider the ordinal nature of the ratings. In this
paper, we explore a new approach to exploit rating ordinality for GNN, which
has not been studied well in the literature. We introduce a new method, called
ROGMC, to leverage Rating Ordinality in GNN-based Matrix Completion. It uses
cumulative preference propagation to directly incorporate rating ordinality in
GNN's message passing, allowing for users' stronger preferences to be more
emphasized based on inherent orders of rating types. This process is
complemented by interest regularization which facilitates preference learning
using the underlying interest information. Our extensive experiments show that
ROGMC consistently outperforms the existing strategies of using rating types
for GNN. We expect that our attempt to explore the feasibility of utilizing
rating ordinality for GNN may stimulate further research in this direction.
\\ ( https://arxiv.org/abs/2403.04504 ,  11535kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04511
Date: Thu, 7 Mar 2024 14:14:40 GMT   (8287kb,D)

Title: Uncovering the Deep Filter Bubble: Narrow Exposure in Short-Video
  Recommendation
Authors: Nicholas Sukiennik, Chen Gao, Nian Li
Categories: cs.AI
Comments: accepted to WWW 2024
ACM-class: H.3.5
DOI: 10.1145/3589334.3648159
\\
  Filter bubbles have been studied extensively within the context of online
content platforms due to their potential to cause undesirable outcomes such as
user dissatisfaction or polarization. With the rise of short-video platforms,
the filter bubble has been given extra attention because these platforms rely
on an unprecedented use of the recommender system to provide relevant content.
In our work, we investigate the deep filter bubble, which refers to the user
being exposed to narrow content within their broad interests. We accomplish
this using one-year interaction data from a top short-video platform in China,
which includes hierarchical data with three levels of categories for each
video. We formalize our definition of a "deep" filter bubble within this
context, and then explore various correlations within the data: first
understanding the evolution of the deep filter bubble over time, and later
revealing some of the factors that give rise to this phenomenon, such as
specific categories, user demographics, and feedback type. We observe that
while the overall proportion of users in a filter bubble remains largely
constant over time, the depth composition of their filter bubble changes. In
addition, we find that some demographic groups that have a higher likelihood of
seeing narrower content and implicit feedback signals can lead to less bubble
formation. Finally, we propose some ways in which recommender systems can be
designed to reduce the risk of a user getting caught in a bubble.
\\ ( https://arxiv.org/abs/2403.04511 ,  8287kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04541
Date: Thu, 7 Mar 2024 14:36:52 GMT   (128kb,D)

Title: Towards Automatic Composition of ASP Programs from Natural Language
  Specifications
Authors: Manuel Borroto, Irfan Kareem, Francesco Ricca
Categories: cs.AI
\\
  This paper moves the first step towards automating the composition of Answer
Set Programming (ASP) specifications. In particular, the following
contributions are provided: (i) A dataset focused on graph-related problem
specifications, designed to develop and assess tools for ASP automatic coding;
(ii) A two-step architecture, implemented in the NL2ASP tool, for generating
ASP programs from natural language specifications. NL2ASP uses neural machine
translation to transform natural language into Controlled Natural Language
(CNL) statements. Subsequently, CNL statements are converted into ASP code
using the CNL2ASP tool. An experiment confirms the viability of the approach.
\\ ( https://arxiv.org/abs/2403.04541 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04571
Date: Thu, 7 Mar 2024 15:12:06 GMT   (33kb)

Title: Machine learning and information theory concepts towards an AI
  Mathematician
Authors: Yoshua Bengio, Nikolay Malkin
Categories: cs.AI
Comments: To appear in the Bulletin of the AMS, 2024
\\
  The current state-of-the-art in artificial intelligence is impressive,
especially in terms of mastery of language, but not so much in terms of
mathematical reasoning. What could be missing? Can we learn something useful
about that gap from how the brains of mathematicians go about their craft? This
essay builds on the idea that current deep learning mostly succeeds at system 1
abilities -- which correspond to our intuition and habitual behaviors -- but
still lacks something important regarding system 2 abilities -- which include
reasoning and robust uncertainty estimation. It takes an
information-theoretical posture to ask questions about what constitutes an
interesting mathematical statement, which could guide future work in crafting
an AI mathematician. The focus is not on proving a given theorem but on
discovering new and interesting conjectures. The central hypothesis is that a
desirable body of theorems better summarizes the set of all provable
statements, for example by having a small description length while at the same
time being close (in terms of number of derivation steps) to many provable
statements.
\\ ( https://arxiv.org/abs/2403.04571 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04577
Date: Thu, 7 Mar 2024 15:22:07 GMT   (1358kb,D)

Title: Wiki-TabNER:Advancing Table Interpretation Through Named Entity
  Recognition
Authors: Aneta Koleva, Martin Ringsquandl, Ahmed Hatem, Thomas Runkler, Volker
  Tresp
Categories: cs.AI cs.CL
\\
  Web tables contain a large amount of valuable knowledge and have inspired
tabular language models aimed at tackling table interpretation (TI) tasks. In
this paper, we analyse a widely used benchmark dataset for evaluation of TI
tasks, particularly focusing on the entity linking task. Our analysis reveals
that this dataset is overly simplified, potentially reducing its effectiveness
for thorough evaluation and failing to accurately represent tables as they
appear in the real-world. To overcome this drawback, we construct and annotate
a new more challenging dataset. In addition to introducing the new dataset, we
also introduce a novel problem aimed at addressing the entity linking task:
named entity recognition within cells. Finally, we propose a prompting
framework for evaluating the newly developed large language models (LLMs) on
this novel TI task. We conduct experiments on prompting LLMs under various
settings, where we use both random and similarity-based selection to choose the
examples presented to the models. Our ablation study helps us gain insights
into the impact of the few-shot examples. Additionally, we perform qualitative
analysis to gain insights into the challenges encountered by the models and to
understand the limitations of the proposed dataset.
\\ ( https://arxiv.org/abs/2403.04577 ,  1358kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04588
Date: Thu, 7 Mar 2024 15:35:29 GMT   (378kb,D)

Title: Zero-shot cross-modal transfer of Reinforcement Learning policies
  through a Global Workspace
Authors: L\'eopold Mayti\'e, Benjamin Devillers, Alexandre Arnold, Rufin
  VanRullen
Categories: cs.AI
Comments: Under review in a conference
\\
  Humans perceive the world through multiple senses, enabling them to create a
comprehensive representation of their surroundings and to generalize
information across domains. For instance, when a textual description of a scene
is given, humans can mentally visualize it. In fields like robotics and
Reinforcement Learning (RL), agents can also access information about the
environment through multiple sensors; yet redundancy and complementarity
between sensors is difficult to exploit as a source of robustness (e.g. against
sensor failure) or generalization (e.g. transfer across domains). Prior
research demonstrated that a robust and flexible multimodal representation can
be efficiently constructed based on the cognitive science notion of a 'Global
Workspace': a unique representation trained to combine information across
modalities, and to broadcast its signal back to each modality. Here, we explore
whether such a brain-inspired multimodal representation could be advantageous
for RL agents. First, we train a 'Global Workspace' to exploit information
collected about the environment via two input modalities (a visual input, or an
attribute vector representing the state of the agent and/or its environment).
Then, we train a RL agent policy using this frozen Global Workspace. In two
distinct environments and tasks, our results reveal the model's ability to
perform zero-shot cross-modal transfer between input modalities, i.e. to apply
to image inputs a policy previously trained on attribute vectors (and
vice-versa), without additional training or fine-tuning. Variants and ablations
of the full Global Workspace (including a CLIP-like multimodal representation
trained via contrastive learning) did not display the same generalization
abilities.
\\ ( https://arxiv.org/abs/2403.04588 ,  378kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04667
Date: Thu, 7 Mar 2024 17:14:22 GMT   (2385kb,D)

Title: The Social Impact of Generative AI: An Analysis on ChatGPT
Authors: Maria T. Baldassarre, Danilo Caivano, Berenice Fernandez Nieto,
  Domenico Gigante, and Azzurra Ragone
Categories: cs.AI cs.CY cs.ET
Comments: Presented at GoodIT2023 - ACM Conference on Information Technology
  for Social Good
DOI: 10.1145/3582515.3609555
\\
  In recent months, the social impact of Artificial Intelligence (AI) has
gained considerable public interest, driven by the emergence of Generative AI
models, ChatGPT in particular. The rapid development of these models has
sparked heated discussions regarding their benefits, limitations, and
associated risks. Generative models hold immense promise across multiple
domains, such as healthcare, finance, and education, to cite a few, presenting
diverse practical applications. Nevertheless, concerns about potential adverse
effects have elicited divergent perspectives, ranging from privacy risks to
escalating social inequality. This paper adopts a methodology to delve into the
societal implications of Generative AI tools, focusing primarily on the case of
ChatGPT. It evaluates the potential impact on several social sectors and
illustrates the findings of a comprehensive literature review of both positive
and negative effects, emerging trends, and areas of opportunity of Generative
AI models. This analysis aims to facilitate an in-depth discussion by providing
insights that can inspire policy, regulation, and responsible development
practices to foster a human-centered AI.
\\ ( https://arxiv.org/abs/2403.04667 ,  2385kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04732
Date: Thu, 7 Mar 2024 18:35:54 GMT   (372kb,D)

Title: How Far Are We from Intelligent Visual Deductive Reasoning?
Authors: Yizhe Zhang, He Bai, Ruixiang Zhang, Jiatao Gu, Shuangfei Zhai, Josh
  Susskind, Navdeep Jaitly
Categories: cs.AI cs.CL cs.CV
Comments: ICLR 2024 AGI workshop
\\
  Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated
incredible strides on diverse vision language tasks. We dig into vision-based
deductive reasoning, a more sophisticated but less explored realm, and find
previously unexposed blindspots in the current SOTA VLMs. Specifically, we
leverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to
perform multi-hop relational and deductive reasoning relying solely on visual
clues. We perform comprehensive evaluations of several popular VLMs employing
standard strategies such as in-context learning, self-consistency, and
Chain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,
IntelligenceTest, and RAVEN. The results reveal that despite the impressive
capabilities of LLMs in text-based reasoning, we are still far from achieving
comparable proficiency in visual deductive reasoning. We found that certain
standard strategies that are effective when applied to LLMs do not seamlessly
translate to the challenges presented by visual reasoning tasks. Moreover, a
detailed analysis reveals that VLMs struggle to solve these tasks mainly
because they are unable to perceive and comprehend multiple, confounding
abstract patterns in RPM examples.
\\ ( https://arxiv.org/abs/2403.04732 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04031
Date: Wed, 6 Mar 2024 20:22:08 GMT   (750kb,D)

Title: Can Large Language Models do Analytical Reasoning?
Authors: Yebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Hassan Foroosh,
  Dong Yu, Fei Liu
Categories: cs.CL cs.AI
\\
  This paper explores the cutting-edge Large Language Model with analytical
reasoning on sports. Our analytical reasoning embodies the tasks of letting
large language models count how many points each team scores in a quarter in
the NBA and NFL games. Our major discoveries are in two folds. Firstly, we find
among all the models we employed, GPT-4 stands out in effectiveness, followed
by Claude-2.1, with GPT-3.5, Gemini-Pro, and Llama-2-70b lagging behind.
Specifically, we compare three different prompting techniques and a
divide-and-conquer approach, we find that the latter was the most effective.
Our divide-and-conquer approach breaks down play-by-play data into smaller,
more manageable segments, solves each piece individually, and then aggregates
them together. Besides the divide-and-conquer approach, we also explore the
Chain of Thought (CoT) strategy, which markedly improves outcomes for certain
models, notably GPT-4 and Claude-2.1, with their accuracy rates increasing
significantly. However, the CoT strategy has negligible or even detrimental
effects on the performance of other models like GPT-3.5 and Gemini-Pro.
Secondly, to our surprise, we observe that most models, including GPT-4,
struggle to accurately count the total scores for NBA quarters despite showing
strong performance in counting NFL quarter scores. This leads us to further
investigate the factors that impact the complexity of analytical reasoning
tasks with extensive experiments, through which we conclude that task
complexity depends on the length of context, the information density, and the
presence of related information. Our research provides valuable insights into
the complexity of analytical reasoning tasks and potential directions for
developing future large language models.
\\ ( https://arxiv.org/abs/2403.04031 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04073
Date: Wed, 6 Mar 2024 22:06:23 GMT   (1350kb,D)

Title: Semi-Supervised Dialogue Abstractive Summarization via High-Quality
  Pseudolabel Selection
Authors: Jianfeng He, Hang Su, Jason Cai, Igor Shalyminov, Hwanjun Song, Saab
  Mansour
Categories: cs.CL cs.AI
Comments: 21 pages, 10 figures
\\
  Semi-supervised dialogue summarization (SSDS) leverages model-generated
summaries to reduce reliance on human-labeled data and improve the performance
of summarization models. While addressing label noise, previous works on
semi-supervised learning primarily focus on natural language understanding
tasks, assuming each sample has a unique label. However, these methods are not
directly applicable to SSDS, as it is a generative task, and each dialogue can
be summarized in different ways. In this work, we propose a novel scoring
approach, SiCF, which encapsulates three primary dimensions of summarization
model quality: Semantic invariance (indicative of model confidence), Coverage
(factual recall), and Faithfulness (factual precision). Using the SiCF score,
we select unlabeled dialogues with high-quality generated summaries to train
summarization models. Comprehensive experiments on three public datasets
demonstrate the effectiveness of SiCF scores in uncertainty estimation and
semi-supervised learning for dialogue summarization tasks. Our code is
available at \url{https://github.com/amazon-science/summarization-sicf-score}.
\\ ( https://arxiv.org/abs/2403.04073 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04080
Date: Wed, 6 Mar 2024 22:22:02 GMT   (2270kb,D)

Title: Transformers and Language Models in Form Understanding: A Comprehensive
  Review of Scanned Document Analysis
Authors: Abdelrahman Abdallah and Daniel Eberharter and Zoe Pfister and Adam
  Jatowt
Categories: cs.CL cs.CV
\\
  This paper presents a comprehensive survey of research works on the topic of
form understanding in the context of scanned documents. We delve into recent
advancements and breakthroughs in the field, highlighting the significance of
language models and transformers in solving this challenging task. Our research
methodology involves an in-depth analysis of popular documents and forms of
understanding of trends over the last decade, enabling us to offer valuable
insights into the evolution of this domain. Focusing on cutting-edge models, we
showcase how transformers have propelled the field forward, revolutionizing
form-understanding techniques. Our exploration includes an extensive
examination of state-of-the-art language models designed to effectively tackle
the complexities of noisy scanned documents. Furthermore, we present an
overview of the latest and most relevant datasets, which serve as essential
benchmarks for evaluating the performance of selected models. By comparing and
contrasting the capabilities of these models, we aim to provide researchers and
practitioners with useful guidance in choosing the most suitable solutions for
their specific form understanding tasks.
\\ ( https://arxiv.org/abs/2403.04080 ,  2270kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04085
Date: Wed, 6 Mar 2024 22:30:04 GMT   (17150kb,D)

Title: Don't Blame the Data, Blame the Model: Understanding Noise and Bias When
  Learning from Subjective Annotations
Authors: Abhishek Anand, Negar Mokhberian, Prathyusha Naresh Kumar, Anweasha
  Saha, Zihao He, Ashwin Rao, Fred Morstatter, Kristina Lerman
Categories: cs.CL cs.CY
\\
  Researchers have raised awareness about the harms of aggregating labels
especially in subjective tasks that naturally contain disagreements among human
annotators. In this work we show that models that are only provided aggregated
labels show low confidence on high-disagreement data instances. While previous
studies consider such instances as mislabeled, we argue that the reason the
high-disagreement text instances have been hard-to-learn is that the
conventional aggregated models underperform in extracting useful signals from
subjective tasks. Inspired by recent studies demonstrating the effectiveness of
learning from raw annotations, we investigate classifying using Multiple Ground
Truth (Multi-GT) approaches. Our experiments show an improvement of confidence
for the high-disagreement instances.
\\ ( https://arxiv.org/abs/2403.04085 ,  17150kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04158
Date: Thu, 7 Mar 2024 02:30:46 GMT   (4266kb,D)

Title: DA-Net: A Disentangled and Adaptive Network for Multi-Source
  Cross-Lingual Transfer Learning
Authors: Ling Ge, Chunming Hu, Guanghui Ma, Jihong Liu, Hong Zhang
Categories: cs.CL cs.AI
Comments: AAAI 2024
\\
  Multi-Source cross-lingual transfer learning deals with the transfer of task
knowledge from multiple labelled source languages to an unlabeled target
language under the language shift. Existing methods typically focus on
weighting the predictions produced by language-specific classifiers of
different sources that follow a shared encoder. However, all source languages
share the same encoder, which is updated by all these languages. The extracted
representations inevitably contain different source languages' information,
which may disturb the learning of the language-specific classifiers.
Additionally, due to the language gap, language-specific classifiers trained
with source labels are unable to make accurate predictions for the target
language. Both facts impair the model's performance. To address these
challenges, we propose a Disentangled and Adaptive Network (DA-Net). Firstly,
we devise a feedback-guided collaborative disentanglement method that seeks to
purify input representations of classifiers, thereby mitigating mutual
interference from multiple sources. Secondly, we propose a class-aware parallel
adaptation method that aligns class-level distributions for each source-target
language pair, thereby alleviating the language pairs' language gap.
Experimental results on three different tasks involving 38 languages validate
the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2403.04158 ,  4266kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04178
Date: Thu, 7 Mar 2024 03:21:19 GMT   (177kb,D)

Title: Attempt Towards Stress Transfer in Speech-to-Speech Machine Translation
Authors: Sai Akarsh, Vamshi Raghusimha, Anindita Mondal, Anil Vuppala
Categories: cs.CL cs.SD eess.AS
\\
  The language diversity in India's education sector poses a significant
challenge, hindering inclusivity. Despite the democratization of knowledge
through online educational content, the dominance of English, as the internet's
lingua franca, limits accessibility, emphasizing the crucial need for
translation into Indian languages. Despite existing Speech-to-Speech Machine
Translation (SSMT) technologies, the lack of intonation in these systems gives
monotonous translations, leading to a loss of audience interest and
disengagement from the content. To address this, our paper introduces a dataset
with stress annotations in Indian English and also a Text-to-Speech (TTS)
architecture capable of incorporating stress into synthesized speech. This
dataset is used for training a stress detection model, which is then used in
the SSMT system for detecting stress in the source speech and transferring it
into the target language speech. The TTS architecture is based on FastPitch and
can modify the variances based on stressed words given. We present an Indian
English-to-Hindi SSMT system that can transfer stress and aim to enhance the
overall quality and engagement of educational content.
\\ ( https://arxiv.org/abs/2403.04178 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04182
Date: Thu, 7 Mar 2024 03:24:34 GMT   (187kb,D)

Title: Metric-aware LLM inference
Authors: Michal Lukasik, Harikrishna Narasimhan, Aditya Krishna Menon, Felix
  Yu, Sanjiv Kumar
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have demonstrated strong results on a range of
NLP tasks. Typically, outputs are obtained via autoregressive sampling from the
LLM's underlying distribution. We show that this inference strategy can be
suboptimal for a range of tasks and associated evaluation metrics. As a remedy,
we propose metric aware LLM inference: a decision theoretic approach optimizing
for custom metrics at inference time. We report improvements over baselines on
academic benchmarks and publicly available models.
\\ ( https://arxiv.org/abs/2403.04182 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04197
Date: Thu, 7 Mar 2024 03:58:28 GMT   (2469kb,D)

Title: Large Language Models are In-Context Molecule Learners
Authors: Jiatong Li, Wei Liu, Zhihao Ding, Wenqi Fan, Yuqiang Li, Qing Li
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have demonstrated exceptional performance in
biochemical tasks, especially the molecule caption translation task, which aims
to bridge the gap between molecules and natural language texts. However,
previous methods in adapting LLMs to the molecule-caption translation task
required extra domain-specific pre-training stages, suffered weak alignment
between molecular and textual spaces, or imposed stringent demands on the scale
of LLMs. To resolve the challenges, we propose In-Context Molecule Adaptation
(ICMA), as a new paradigm allowing LLMs to learn the molecule-text alignment
from context examples via In-Context Molecule Tuning. Specifically, ICMA
incorporates the following three stages: Cross-modal Retrieval, Post-retrieval
Re-ranking, and In-context Molecule Tuning. Initially, Cross-modal Retrieval
utilizes BM25 Caption Retrieval and Molecule Graph Retrieval to retrieve
informative context examples. Additionally, we also propose Post-retrieval
Re-ranking with Sequence Reversal and Random Walk to further improve the
quality of retrieval results. Finally, In-Context Molecule Tuning unlocks the
in-context molecule learning capability of LLMs with retrieved examples and
adapts the parameters of LLMs for the molecule-caption translation task.
Experimental results demonstrate that ICMT can empower LLMs to achieve
state-of-the-art or comparable performance without extra training corpora and
intricate structures, showing that LLMs are inherently in-context molecule
learners.
\\ ( https://arxiv.org/abs/2403.04197 ,  2469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04212
Date: Thu, 7 Mar 2024 04:33:11 GMT   (526kb,D)

Title: Persona Extraction Through Semantic Similarity for Emotional Support
  Conversation Generation
Authors: Seunghee Han, Se Jin Park, Chae Won Kim, Yong Man Ro
Categories: cs.CL
Comments: Accepted by ICASSP2024
\\
  Providing emotional support through dialogue systems is becoming increasingly
important in today's world, as it can support both mental health and social
interactions in many conversation scenarios. Previous works have shown that
using persona is effective for generating empathetic and supportive responses.
They have often relied on pre-provided persona rather than inferring them
during conversations. However, it is not always possible to obtain a user
persona before the conversation begins. To address this challenge, we propose
PESS (Persona Extraction through Semantic Similarity), a novel framework that
can automatically infer informative and consistent persona from dialogues. We
devise completeness loss and consistency loss based on semantic similarity
scores. The completeness loss encourages the model to generate missing persona
information, and the consistency loss guides the model to distinguish between
consistent and inconsistent persona. Our experimental results demonstrate that
high-quality persona information inferred by PESS is effective in generating
emotionally supportive responses.
\\ ( https://arxiv.org/abs/2403.04212 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04222
Date: Thu, 7 Mar 2024 04:50:38 GMT   (1961kb,D)

Title: Self-Evaluation of Large Language Model based on Glass-box Features
Authors: Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, Tiejun Zhao
Categories: cs.CL
Comments: work in progress
\\
  The proliferation of open-source Large Language Models (LLMs) underscores the
pressing need for evaluation methods. Existing works primarily rely on external
evaluators, focusing on training and prompting strategies. However, a crucial
aspect - model-aware glass-box features - is overlooked. In this study, we
explore the utility of glass-box features under the scenario of
self-evaluation, namely applying an LLM to evaluate its own output. We
investigate various glass-box feature groups and discovered that the softmax
distribution serves as a reliable indicator for quality evaluation.
Furthermore, we propose two strategies to enhance the evaluation by
incorporating features derived from references. Experimental results on public
benchmarks validate the feasibility of self-evaluation of LLMs using glass-box
features.
\\ ( https://arxiv.org/abs/2403.04222 ,  1961kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04224
Date: Thu, 7 Mar 2024 04:54:56 GMT   (1033kb,D)

Title: Aligners: Decoupling LLMs and Alignment
Authors: Lilian Ngweta, Mayank Agarwal, Subha Maity, Alex Gittens, Yuekai Sun,
  Mikhail Yurochkin
Categories: cs.CL cs.AI cs.LG
Comments: Tiny Papers Track at the International Conference on Learning
  Representations (ICLR) 2024
\\
  Large Language Models (LLMs) need to be aligned with human expectations to
ensure their safety and utility in most applications. Alignment is challenging,
costly, and needs to be repeated for every LLM and alignment criterion. We
propose to decouple LLMs and alignment by training aligner models that can be
used to align any LLM for a given criteria on an as-needed basis, thus also
reducing the potential negative impacts of alignment on performance. Our recipe
for training the aligner models solely relies on synthetic data generated with
a (prompted) LLM and can be easily adjusted for a variety of alignment
criteria. We illustrate our method by training an "ethical" aligner and verify
its efficacy empirically.
\\ ( https://arxiv.org/abs/2403.04224 ,  1033kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04233
Date: Thu, 7 Mar 2024 05:26:41 GMT   (1204kb,D)

Title: DEEP-ICL: Definition-Enriched Experts for Language Model In-Context
  Learning
Authors: Xingwei Qu, Yiming Liang, Yucheng Wang, Tianyu Zheng, Tommy Yue, Lei
  Ma, Stephen W. Huang, Jiajun Zhang, Wenhu Chen, Chenghua Lin, Jie Fu, Ge
  Zhang
Categories: cs.CL cs.AI
\\
  It has long been assumed that the sheer number of parameters in large
language models (LLMs) drives in-context learning (ICL) capabilities, enabling
remarkable performance improvements by leveraging task-specific demonstrations.
Challenging this hypothesis, we introduce DEEP-ICL, a novel task Definition
Enriched ExPert Ensembling methodology for ICL. DEEP-ICL explicitly extracts
task definitions from given demonstrations and generates responses through
learning task-specific examples. We argue that improvement from ICL does not
directly rely on model size, but essentially stems from understanding task
definitions and task-guided learning. Inspired by this, DEEP-ICL combines two
3B models with distinct roles (one for concluding task definitions and the
other for learning task demonstrations) and achieves comparable performance to
LLaMA2-13B. Furthermore, our framework outperforms conventional ICL by
overcoming pretraining sequence length limitations, by supporting unlimited
demonstrations. We contend that DEEP-ICL presents a novel alternative for
achieving efficient few-shot learning, extending beyond the conventional ICL.
\\ ( https://arxiv.org/abs/2403.04233 ,  1204kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04247
Date: Thu, 7 Mar 2024 06:10:02 GMT   (3221kb,D)

Title: UltraWiki: Ultra-fine-grained Entity Set Expansion with Negative Seed
  Entities
Authors: Yangning Li, Qingsong Lv, Tianyu Yu, Yinghui Li, Shulin Huang, Tingwei
  Lu, Xuming Hu, Wenhao JIang, Hai-Tao Zheng, Hui Wang
Categories: cs.CL
Comments: submitted to VLDB
\\
  Entity Set Expansion (ESE) aims to identify new entities belonging to the
same semantic class as a given set of seed entities. Traditional methods
primarily relied on positive seed entities to represent a target semantic
class, which poses challenge for the representation of ultra-fine-grained
semantic classes. Ultra-fine-grained semantic classes are defined based on
fine-grained semantic classes with more specific attribute constraints.
Describing it with positive seed entities alone cause two issues: (i) Ambiguity
among ultra-fine-grained semantic classes. (ii) Inability to define "unwanted"
semantic. Due to these inherent shortcomings, previous methods struggle to
address the ultra-fine-grained ESE (Ultra-ESE). To solve this issue, we first
introduce negative seed entities in the inputs, which belong to the same
fine-grained semantic class as the positive seed entities but differ in certain
attributes. Negative seed entities eliminate the semantic ambiguity by contrast
between positive and negative attributes. Meanwhile, it provide a
straightforward way to express "unwanted". To assess model performance in
Ultra-ESE, we constructed UltraWiki, the first large-scale dataset tailored for
Ultra-ESE. UltraWiki encompasses 236 ultra-fine-grained semantic classes, where
each query of them is represented with 3-5 positive and negative seed entities.
A retrieval-based framework RetExpan and a generation-based framework GenExpan
are proposed to comprehensively assess the efficacy of large language models
from two different paradigms in Ultra-ESE. Moreover, we devised three
strategies to enhance models' comprehension of ultra-fine-grained entities
semantics: contrastive learning, retrieval augmentation, and chain-of-thought
reasoning. Extensive experiments confirm the effectiveness of our proposed
strategies and also reveal that there remains a large space for improvement in
Ultra-ESE.
\\ ( https://arxiv.org/abs/2403.04247 ,  3221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04283
Date: Thu, 7 Mar 2024 07:31:00 GMT   (1640kb,D)

Title: Proxy-RLHF: Decoupling Generation and Alignment in Large Language Model
  with Proxy
Authors: Yu Zhu, Chuxiong Sun, Wenfei Yang, Wenqiang Wei, Bo Tang, Tianzhu
  Zhang, Zhiyu Li, Shifeng Zhang, Feiyu Xiong, Jie Hu, Mingchuan yang
Categories: cs.CL cs.AI cs.LG
\\
  Reinforcement Learning from Human Feedback (RLHF) is the prevailing approach
to ensure Large Language Models (LLMs) align with human values. However,
existing RLHF methods require a high computational cost, one main reason being
that RLHF assigns both the generation and alignment tasks to the LLM
simultaneously. In this paper, we introduce Proxy-RLHF, which decouples the
generation and alignment processes of LLMs, achieving alignment with human
values at a much lower computational cost. We start with a novel Markov
Decision Process (MDP) designed for the alignment process and employ
Reinforcement Learning (RL) to train a streamlined proxy model that oversees
the token generation of the LLM, without altering the LLM itself. Experiments
show that our method achieves a comparable level of alignment with only 1\% of
the training parameters of other methods.
\\ ( https://arxiv.org/abs/2403.04283 ,  1640kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04307
Date: Thu, 7 Mar 2024 08:25:46 GMT   (158kb,D)

Title: HaluEval-Wild: Evaluating Hallucinations of Language Models in the Wild
Authors: Zhiying Zhu, Zhiqing Sun, Yiming Yang
Categories: cs.CL
\\
  Hallucinations pose a significant challenge to the reliability of large
language models (LLMs) in critical domains. Recent benchmarks designed to
assess LLM hallucinations within conventional NLP tasks, such as
knowledge-intensive question answering (QA) and summarization, are insufficient
for capturing the complexities of user-LLM interactions in dynamic, real-world
settings. To address this gap, we introduce HaluEval-Wild, the first benchmark
specifically designed to evaluate LLM hallucinations in the wild. We
meticulously collect challenging (adversarially filtered by Alpaca) user
queries from existing real-world user-LLM interaction datasets, including
ShareGPT, to evaluate the hallucination rates of various LLMs. Upon analyzing
the collected queries, we categorize them into five distinct types, which
enables a fine-grained analysis of the types of hallucinations LLMs exhibit,
and synthesize the reference answers with the powerful GPT-4 model and
retrieval-augmented generation (RAG). Our benchmark offers a novel approach
towards enhancing our comprehension and improvement of LLM reliability in
scenarios reflective of real-world interactions.
\\ ( https://arxiv.org/abs/2403.04307 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04314
Date: Thu, 7 Mar 2024 08:32:17 GMT   (8794kb,D)

Title: Can Your Model Tell a Negation from an Implicature? Unravelling
  Challenges With Intent Encoders
Authors: Yuwei Zhang, Siffi Singh, Sailik Sengupta, Igor Shalyminov, Hang Su,
  Hwanjun Song, Saab Mansour
Categories: cs.CL
\\
  Conversational systems often rely on embedding models for intent
classification and intent clustering tasks. The advent of Large Language Models
(LLMs), which enable instructional embeddings allowing one to adjust semantics
over the embedding space using prompts, are being viewed as a panacea for these
downstream conversational tasks. However, traditional evaluation benchmarks
rely solely on task metrics that don't particularly measure gaps related to
semantic understanding. Thus, we propose an intent semantic toolkit that gives
a more holistic view of intent embedding models by considering three tasks--
(1) intent classification, (2) intent clustering, and (3) a novel triplet task.
The triplet task gauges the model's understanding of two semantic concepts
paramount in real-world conversational systems-- negation and implicature. We
observe that current embedding models fare poorly in semantic understanding of
these concepts. To address this, we propose a pre-training approach to improve
the embedding model by leveraging augmentation with data generated by an
auto-regressive model and a contrastive loss term. Our approach improves the
semantic understanding of the intent embedding model on the aforementioned
linguistic dimensions while slightly effecting their performance on downstream
task metrics.
\\ ( https://arxiv.org/abs/2403.04314 ,  8794kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04325
Date: Thu, 7 Mar 2024 08:44:42 GMT   (1360kb,D)

Title: Measuring Meaning Composition in the Human Brain with Composition Scores
  from Large Language Models
Authors: Changjiang Gao, Jixing Li, Jiajun Chen, Shujian Huang
Categories: cs.CL cs.AI
\\
  The process of meaning composition, wherein smaller units like morphemes or
words combine to form the meaning of phrases and sentences, is essential for
human sentence comprehension. Despite extensive neurolinguistic research into
the brain regions involved in meaning composition, a computational metric to
quantify the extent of composition is still lacking. Drawing on the key-value
memory interpretation of transformer feed-forward network blocks, we introduce
the Composition Score, a novel model-based metric designed to quantify the
degree of meaning composition during sentence comprehension. Experimental
findings show that this metric correlates with brain clusters associated with
word frequency, structural processing, and general sensitivity to words,
suggesting the multifaceted nature of meaning composition during human sentence
comprehension.
\\ ( https://arxiv.org/abs/2403.04325 ,  1360kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04376
Date: Thu, 7 Mar 2024 10:06:54 GMT   (6982kb,D)

Title: Computational Modelling of Plurality and Definiteness in Chinese Noun
  Phrases
Authors: Yuqi Liu, Guanyi Chen, Kees van Deemter
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Theoretical linguists have suggested that some languages (e.g., Chinese and
Japanese) are "cooler" than other languages based on the observation that the
intended meaning of phrases in these languages depends more on their contexts.
As a result, many expressions in these languages are shortened, and their
meaning is inferred from the context. In this paper, we focus on the omission
of the plurality and definiteness markers in Chinese noun phrases (NPs) to
investigate the predictability of their intended meaning given the contexts. To
this end, we built a corpus of Chinese NPs, each of which is accompanied by its
corresponding context, and by labels indicating its singularity/plurality and
definiteness/indefiniteness. We carried out corpus assessments and analyses.
The results suggest that Chinese speakers indeed drop plurality and
definiteness markers very frequently. Building on the corpus, we train a bank
of computational models using both classic machine learning models and
state-of-the-art pre-trained language models to predict the plurality and
definiteness of each NP. We report on the performance of these models and
analyse their behaviours.
\\ ( https://arxiv.org/abs/2403.04376 ,  6982kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04382
Date: Thu, 7 Mar 2024 10:20:06 GMT   (656kb,D)

Title: Acceleron: A Tool to Accelerate Research Ideation
Authors: Harshit Nigam, Manasi Patwardhan, Lovekesh Vig, Gautam Shroff
Categories: cs.CL cs.AI
Comments: Accepted at AI2ASE Workshop at AAAI'24 Conference. 13 Pages and 4
  Figures
\\
  Several tools have recently been proposed for assisting researchers during
various stages of the research life-cycle. However, these primarily concentrate
on tasks such as retrieving and recommending relevant literature, reviewing and
critiquing the draft, and writing of research manuscripts. Our investigation
reveals a significant gap in availability of tools specifically designed to
assist researchers during the challenging ideation phase of the research
life-cycle. To aid with research ideation, we propose `Acceleron', a research
accelerator for different phases of the research life cycle, and which is
specially designed to aid the ideation process. Acceleron guides researchers
through the formulation of a comprehensive research proposal, encompassing a
novel research problem. The proposals motivation is validated for novelty by
identifying gaps in the existing literature and suggesting a plausible list of
techniques to solve the proposed problem. We leverage the reasoning and
domain-specific skills of Large Language Models (LLMs) to create an agent-based
architecture incorporating colleague and mentor personas for LLMs. The LLM
agents emulate the ideation process undertaken by researchers, engaging
researchers in an interactive fashion to aid in the development of the research
proposal. Notably, our tool addresses challenges inherent in LLMs, such as
hallucinations, implements a two-stage aspect-based retrieval to manage
precision-recall trade-offs, and tackles issues of unanswerability. As
evaluation, we illustrate the execution of our motivation validation and method
synthesis workflows on proposals from the ML and NLP domain, given by 3
distinct researchers. Our observations and evaluations provided by the
researchers illustrate the efficacy of the tool in terms of assisting
researchers with appropriate inputs at distinct stages and thus leading to
improved time efficiency.
\\ ( https://arxiv.org/abs/2403.04382 ,  656kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04400
Date: Thu, 7 Mar 2024 10:54:27 GMT   (7842kb,D)

Title: Exploring Continual Learning of Compositional Generalization in NLI
Authors: Xiyan Fu, Anette Frank
Categories: cs.CL
\\
  Compositional Natural Language Inference has been explored to assess the true
abilities of neural models to perform NLI. Yet, current evaluations assume
models to have full access to all primitive inferences in advance, in contrast
to humans that continuously acquire inference knowledge. In this paper, we
introduce the Continual Compositional Generalization in Inference (C2Gen NLI)
challenge, where a model continuously acquires knowledge of constituting
primitive inference tasks as a basis for compositional inferences. We explore
how continual learning affects compositional generalization in NLI, by
designing a continual learning setup for compositional NLI inference tasks. Our
experiments demonstrate that models fail to compositionally generalize in a
continual scenario. To address this problem, we first benchmark various
continual learning algorithms and verify their efficacy. We then further
analyze C2Gen, focusing on how to order primitives and compositional inference
types and examining correlations between subtasks. Our analyses show that by
learning subtasks continuously while observing their dependencies and
increasing degrees of difficulty, continual learning can enhance composition
generalization ability.
\\ ( https://arxiv.org/abs/2403.04400 ,  7842kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04417
Date: Thu, 7 Mar 2024 11:30:56 GMT   (21kb)

Title: Promising and worth-to-try future directions for advancing
  state-of-the-art surrogates methods of agent-based models in social and
  health computational sciences
Authors: Atiyah Elsheikh
Categories: cs.CL cs.AI cs.SY eess.SY math.DS
Comments: 4 pages
\\
  The execution and runtime performance of model-based analysis tools for
realistic large-scale ABMs (Agent-Based Models) can be excessively long. This
due to the computational demand exponentially proportional to the model size
(e.g. Population size) and the number of model parameters. Even the runtime of
a single simulation of a realistic ABM may demand huge computational resources
when attempting to employ realistic population size. The main aim of this
ad-hoc brief report is to highlight some of surrogate models that were adequate
and computationally less demanding for nonlinear dynamical models in various
modeling application areas.To the author knowledge, these methods have been
not, at least extensively, employed for ABMs within the field of (SHCS) Social
Health Computational Sciences, yet. Thus, they might be, but not necessarily,
useful in progressing state of the art for establishing surrogate models for
ABMs in the field of SHCS.
\\ ( https://arxiv.org/abs/2403.04417 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04445
Date: Thu, 7 Mar 2024 12:27:08 GMT   (7219kb,D)

Title: Classist Tools: Social Class Correlates with Performance in NLP
Authors: Amanda Cercas Curry, Giuseppe Attanasio, Zeerak Talat and Dirk Hovy
Categories: cs.CL
\\
  Since the foundational work of William Labov on the social stratification of
language (Labov, 1964), linguistics has made concentrated efforts to explore
the links between sociodemographic characteristics and language production and
perception. But while there is strong evidence for socio-demographic
characteristics in language, they are infrequently used in Natural Language
Processing (NLP). Age and gender are somewhat well represented, but Labov's
original target, socioeconomic status, is noticeably absent. And yet it
matters. We show empirically that NLP disadvantages less-privileged
socioeconomic groups. We annotate a corpus of 95K utterances from movies with
social class, ethnicity and geographical language variety and measure the
performance of NLP systems on three tasks: language modelling, automatic speech
recognition, and grammar error correction. We find significant performance
disparities that can be attributed to socioeconomic status as well as ethnicity
and geographical differences. With NLP technologies becoming ever more
ubiquitous and quotidian, they must accommodate all language varieties to avoid
disadvantaging already marginalised groups. We argue for the inclusion of
socioeconomic class in future language technologies.
\\ ( https://arxiv.org/abs/2403.04445 ,  7219kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04454
Date: Thu, 7 Mar 2024 12:47:42 GMT   (519kb,D)

Title: Low-Resource Court Judgment Summarization for Common Law Systems
Authors: Shuaiqi Liu, Jiannong Cao, Yicong Li, Ruosong Yang, Zhiyuan Wen
Categories: cs.CL cs.AI
Comments: First submitted to Information Processing and Management on Oct. 29,
  2023. Major Revision submitted on Mar.6, 2024
ACM-class: I.2.7; I.7
\\
  Common law courts need to refer to similar precedents' judgments to inform
their current decisions. Generating high-quality summaries of court judgment
documents can facilitate legal practitioners to efficiently review previous
cases and assist the general public in accessing how the courts operate and how
the law is applied. Previous court judgment summarization research focuses on
civil law or a particular jurisdiction's judgments. However, judges can refer
to the judgments from all common law jurisdictions. Current summarization
datasets are insufficient to satisfy the demands of summarizing precedents
across multiple jurisdictions, especially when labeled data are scarce for many
jurisdictions. To address the lack of datasets, we present CLSum, the first
dataset for summarizing multi-jurisdictional common law court judgment
documents. Besides, this is the first court judgment summarization work
adopting large language models (LLMs) in data augmentation, summary generation,
and evaluation. Specifically, we design an LLM-based data augmentation method
incorporating legal knowledge. We also propose a legal knowledge enhanced
evaluation metric based on LLM to assess the quality of generated judgment
summaries. Our experimental results verify that the LLM-based summarization
methods can perform well in the few-shot and zero-shot settings. Our LLM-based
data augmentation method can mitigate the impact of low data resources.
Furthermore, we carry out comprehensive comparative experiments to find
essential model components and settings that are capable of enhancing
summarization performance.
\\ ( https://arxiv.org/abs/2403.04454 ,  519kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04460
Date: Thu, 7 Mar 2024 12:57:16 GMT   (3500kb,D)

Title: Pearl: A Review-driven Persona-Knowledge Grounded Conversational
  Recommendation Dataset
Authors: Minjin Kim, Minju Kim, Hana Kim, Beong-woo Kwak, Soyeon Chun, Hyunseo
  Kim, SeongKu Kang, Youngjae Yu, Jinyoung Yeo, Dongha Lee
Categories: cs.CL
\\
  Conversational recommender system is an emerging area that has garnered an
increasing interest in the community, especially with the advancements in large
language models (LLMs) that enable diverse reasoning over conversational input.
Despite the progress, the field has many aspects left to explore. The currently
available public datasets for conversational recommendation lack specific user
preferences and explanations for recommendations, hindering high-quality
recommendations. To address such challenges, we present a novel conversational
recommendation dataset named PEARL, synthesized with persona- and
knowledge-augmented LLM simulators. We obtain detailed persona and knowledge
from real-world reviews and construct a large-scale dataset with over 57k
dialogues. Our experimental results demonstrate that utterances in PEARL
include more specific user preferences, show expertise in the target domain,
and provide recommendations more relevant to the dialogue context than those in
prior datasets.
\\ ( https://arxiv.org/abs/2403.04460 ,  3500kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04481
Date: Thu, 7 Mar 2024 13:30:52 GMT   (7062kb,D)

Title: Do Large Language Model Understand Multi-Intent Spoken Language ?
Authors: Shangjian Yin, Peijie Huang, Yuhong Xu, Haojing Huang, Jiatian Chen
Categories: cs.CL cs.AI
\\
  This study marks a significant advancement by harnessing Large Language
Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a
unique methodology that capitalizes on the generative power of LLMs within an
SLU context. Our innovative technique reconfigures entity slots specifically
for LLM application in multi-intent SLU environments and introduces the concept
of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of
intricate, multi-intent communication within varied domains. The resultant
datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing
benchmarks. Our research illustrates that LLMs can match and potentially excel
beyond the capabilities of current state-of-the-art multi-intent SLU models. It
further explores LLM efficacy across various intent configurations and dataset
proportions. Moreover, we introduce two pioneering metrics, Entity Slot
Accuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth
analysis of LLM proficiency in this complex field.
\\ ( https://arxiv.org/abs/2403.04481 ,  7062kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04507
Date: Thu, 7 Mar 2024 14:07:00 GMT   (2050kb,D)

Title: NLPre: a revised approach towards language-centric benchmarking of
  Natural Language Preprocessing systems
Authors: Martyna Wi\k{a}cek, Piotr Rybak, {\L}ukasz Pszenny, Alina Wr\'oblewska
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  With the advancements of transformer-based architectures, we observe the rise
of natural language preprocessing (NLPre) tools capable of solving preliminary
NLP tasks (e.g. tokenisation, part-of-speech tagging, dependency parsing, or
morphological analysis) without any external linguistic guidance. It is arduous
to compare novel solutions to well-entrenched preprocessing toolkits, relying
on rule-based morphological analysers or dictionaries. Aware of the
shortcomings of existing NLPre evaluation approaches, we investigate a novel
method of reliable and fair evaluation and performance reporting. Inspired by
the GLUE benchmark, the proposed language-centric benchmarking system enables
comprehensive ongoing evaluation of multiple NLPre tools, while credibly
tracking their performance. The prototype application is configured for Polish
and integrated with the thoroughly assembled NLPre-PL benchmark. Based on this
benchmark, we conduct an extensive evaluation of a variety of Polish NLPre
systems. To facilitate the construction of benchmarking environments for other
languages, e.g. NLPre-GA for Irish or NLPre-ZH for Chinese, we ensure full
customization of the publicly released source code of the benchmarking system.
The links to all the resources (deployed platforms, source code, trained
models, datasets etc.) can be found on the project website:
https://sites.google.com/view/nlpre-benchmark.
\\ ( https://arxiv.org/abs/2403.04507 ,  2050kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04510
Date: Thu, 7 Mar 2024 14:12:41 GMT   (3377kb,D)

Title: Where does In-context Translation Happen in Large Language Models
Authors: Suzanna Sia, David Mueller, Kevin Duh
Categories: cs.CL cs.AI
Comments: 19 pages. Under Review
\\
  Self-supervised large language models have demonstrated the ability to
perform Machine Translation (MT) via in-context learning, but little is known
about where the model performs the task with respect to prompt instructions and
demonstration examples. In this work, we attempt to characterize the region
where large language models transition from in-context learners to translation
models. Through a series of layer-wise context-masking experiments on
\textsc{GPTNeo2.7B}, \textsc{Bloom3B}, \textsc{Llama7b} and
\textsc{Llama7b-chat}, we demonstrate evidence of a "task recognition" point
where the translation task is encoded into the input representations and
attention to context is no longer necessary. We further observe correspondence
between the low performance when masking out entire layers, and the task
recognition layers. Taking advantage of this redundancy results in 45\%
computational savings when prompting with 5 examples, and task recognition
achieved at layer 14 / 32. Our layer-wise fine-tuning experiments indicate that
the most effective layers for MT fine-tuning are the layers critical to task
recognition.
\\ ( https://arxiv.org/abs/2403.04510 ,  3377kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04521
Date: Thu, 7 Mar 2024 14:23:25 GMT   (5308kb,D)

Title: Uncertainty-Aware Relational Graph Neural Network for Few-Shot Knowledge
  Graph Completion
Authors: Qian Li, Shu Guo, Yingjia Chen, Cheng Ji, Jiawei Sheng, and Jianxin Li
Categories: cs.CL
\\
  Few-shot knowledge graph completion (FKGC) aims to query the unseen facts of
a relation given its few-shot reference entity pairs. The side effect of noises
due to the uncertainty of entities and triples may limit the few-shot learning,
but existing FKGC works neglect such uncertainty, which leads them more
susceptible to limited reference samples with noises. In this paper, we propose
a novel uncertainty-aware few-shot KG completion framework (UFKGC) to model
uncertainty for a better understanding of the limited data by learning
representations under Gaussian distribution. Uncertainty representation is
first designed for estimating the uncertainty scope of the entity pairs after
transferring feature representations into a Gaussian distribution. Further, to
better integrate the neighbors with uncertainty characteristics for entity
features, we design an uncertainty-aware relational graph neural network
(UR-GNN) to conduct convolution operations between the Gaussian distributions.
Then, multiple random samplings are conducted for reference triples within the
Gaussian distribution to generate smooth reference representations during the
optimization. The final completion score for each query instance is measured by
the designed uncertainty optimization to make our approach more robust to the
noises in few-shot scenarios. Experimental results show that our approach
achieves excellent performance on two benchmark datasets compared to its
competitors.
\\ ( https://arxiv.org/abs/2403.04521 ,  5308kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04639
Date: Thu, 7 Mar 2024 16:29:19 GMT   (211kb)

Title: MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis
Authors: Priya Rani, Gaurav Negi, Theodorus Fransen, John P. McCrae
Categories: cs.CL
Comments: Lrec-Colin 2024
\\
  The present paper introduces new sentiment data, MaCMS, for
Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a
less-resourced minority language. This dataset is the first
Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further,
we also provide a linguistics analysis of the dataset to understand the
structure of code-mixing and a statistical study to understand the language
preferences of speakers with different polarities. With these analyses, we also
train baseline models to evaluate the dataset's quality.
\\ ( https://arxiv.org/abs/2403.04639 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04643
Date: Thu, 7 Mar 2024 16:42:37 GMT   (355kb,D)

Title: QAQ: Quality Adaptive Quantization for LLM KV Cache
Authors: Shichen Dong, Wen Cheng, Jiayu Qin, Wei Wang
Categories: cs.CL
\\
  The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP
applications, particularly in domains such as question-answering systems and
text generation. As the need for longer context grows, a significant bottleneck
in model deployment emerges due to the linear expansion of the Key-Value (KV)
cache with the context length. Existing methods primarily rely on various
hypotheses, such as sorting the KV cache based on attention scores for
replacement or eviction, to compress the KV cache and improve model throughput.
However, heuristics used by these strategies may wrongly evict essential KV
cache, which can significantly degrade model performance. In this paper, we
propose QAQ, a Quality Adaptive Quantization scheme for the KV cache. We
theoretically demonstrate that key cache and value cache exhibit distinct
sensitivities to quantization, leading to the formulation of separate
quantization strategies for their non-uniform quantization. Through the
integration of dedicated outlier handling, as well as an improved
attention-aware approach, QAQ achieves up to 10x the compression ratio of the
KV cache size with a neglectable impact on model performance. QAQ significantly
reduces the practical hurdles of deploying LLMs, opening up new possibilities
for longer-context applications. The code is available at
github.com/ClubieDong/KVCacheQuantization.
\\ ( https://arxiv.org/abs/2403.04643 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04652
Date: Thu, 7 Mar 2024 16:52:49 GMT   (9681kb,D)

Title: Yi: Open Foundation Models by 01.AI
Authors: 01.AI: Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei
  Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng
  Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao
  Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong
  Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, Zonghong Dai
Categories: cs.CL cs.AI
\\
  We introduce the Yi model family, a series of language and multimodal models
that demonstrate strong multi-dimensional capabilities. The Yi model family is
based on 6B and 34B pretrained language models, then we extend them to chat
models, 200K long context models, depth-upscaled models, and vision-language
models. Our base models achieve strong performance on a wide range of
benchmarks like MMLU, and our finetuned chat models deliver strong human
preference rate on major evaluation platforms like AlpacaEval and Chatbot
Arena. Building upon our scalable super-computing infrastructure and the
classical transformer architecture, we attribute the performance of Yi models
primarily to its data quality resulting from our data-engineering efforts. For
pretraining, we construct 3.1 trillion tokens of English and Chinese corpora
using a cascaded data deduplication and quality filtering pipeline. For
finetuning, we polish a small scale (less than 10K) instruction dataset over
multiple iterations such that every single instance has been verified directly
by our machine learning engineers. For vision-language, we combine the chat
language model with a vision transformer encoder and train the model to align
visual representations to the semantic space of the language model. We further
extend the context length to 200K through lightweight continual pretraining and
demonstrate strong needle-in-a-haystack retrieval performance. We show that
extending the depth of the pretrained checkpoint through continual pretraining
further improves performance. We believe that given our current results,
continuing to scale up model parameters using thoroughly optimized data will
lead to even stronger frontier models.
\\ ( https://arxiv.org/abs/2403.04652 ,  9681kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04656
Date: Thu, 7 Mar 2024 16:59:55 GMT   (9684kb,D)

Title: Chain of Thought Explanation for Dialogue State Tracking
Authors: Lin Xu, Ningxin Peng, Daquan Zhou, See-Kiong Ng, Jinlan Fu
Categories: cs.CL
\\
  Dialogue state tracking (DST) aims to record user queries and goals during a
conversational interaction achieved by maintaining a prede- fined set of slots
and their corresponding values. Current approaches decide slot values opaquely,
while humans usually adopt a more deliberate approach by collecting information
from relevant dialogue turns and then reasoning the appropriate values. In this
work, we focus on the steps needed to figure out slot values by proposing a
model named Chain-of-Thought-Explanation (CoTE) for the DST task. CoTE, which
is built on the generative DST framework, is designed to create detailed
explanations step by step after determining the slot values. This process leads
to more accurate and reliable slot values. More-over, to improve the reasoning
ability of the CoTE, we further construct more fluent and high-quality
explanations with automatic paraphrasing, leading the method CoTE-refined.
Experimental results on three widely recognized DST benchmarks-MultiWOZ 2.2,
WoZ 2.0, and M2M-demonstrate the remarkable effectiveness of the CoTE.
Furthermore, through a meticulous fine-grained analysis, we observe significant
benefits of our CoTE on samples characterized by longer dialogue turns, user
responses, and reasoning steps.
\\ ( https://arxiv.org/abs/2403.04656 ,  9684kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04666
Date: Thu, 7 Mar 2024 17:13:12 GMT   (199kb,D)

Title: Telecom Language Models: Must They Be Large?
Authors: Nicola Piovesan, Antonio De Domenico, Fadhel Ayed
Categories: cs.CL cs.LG
\\
  The increasing interest in Large Language Models (LLMs) within the
telecommunications sector underscores their potential to revolutionize
operational efficiency. However, the deployment of these sophisticated models
is often hampered by their substantial size and computational demands, raising
concerns about their viability in resource-constrained environments. Addressing
this challenge, recent advancements have seen the emergence of small language
models that surprisingly exhibit performance comparable to their larger
counterparts in many tasks, such as coding and common-sense reasoning. Phi-2, a
compact yet powerful model, exemplifies this new wave of efficient small
language models. This paper conducts a comprehensive evaluation of Phi-2's
intrinsic understanding of the telecommunications domain. Recognizing the
scale-related limitations, we enhance Phi-2's capabilities through a
Retrieval-Augmented Generation approach, meticulously integrating an extensive
knowledge base specifically curated with telecom standard specifications. The
enhanced Phi-2 model demonstrates a profound improvement in accuracy, answering
questions about telecom standards with a precision that closely rivals the more
resource-intensive GPT-3.5. The paper further explores the refined capabilities
of Phi-2 in addressing problem-solving scenarios within the telecom sector,
highlighting its potential and limitations.
\\ ( https://arxiv.org/abs/2403.04666 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04671
Date: Thu, 7 Mar 2024 17:17:20 GMT   (4363kb,D)

Title: Greater than the sum of its parts: The role of minority and majority
  status in collaborative problem-solving communication
Authors: Jacqueline G. Cavazos, Nia Nixon
Categories: cs.CL
Comments: 14 pages, 3 figures, 3 tables
\\
  Collaborative problem-solving (CPS) is a vital skill used both in the
workplace and in educational environments. CPS is useful in tackling
increasingly complex global, economic, and political issues and is considered a
central 21st century skill. The increasingly connected global community
presents a fruitful opportunity for creative and collaborative problem-solving
interactions and solutions that involve diverse perspectives. Unfortunately,
women and underrepresented minorities (URMs) often face obstacles during
collaborative interactions that hinder their key participation in these
problem-solving conversations. Here, we explored the communication patterns of
minority and non-minority individuals working together in a CPS task. Group
Communication Analysis (GCA), a temporally-sensitive computational linguistic
tool, was used to examine how URM status impacts individuals' sociocognitive
linguistic patterns. Results show differences across racial/ethnic groups in
key sociocognitive features that indicate fruitful collaborative interactions.
We also investigated how the groups' racial/ethnic composition impacts both
individual and group communication patterns. In general, individuals in more
demographically diverse groups displayed more productive communication
behaviors than individuals who were in majority-dominated groups. We discuss
the implications of individual and group diversity on communication patterns
that emerge during CPS and how these patterns can impact collaborative
outcomes.
\\ ( https://arxiv.org/abs/2403.04671 ,  4363kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04696
Date: Thu, 7 Mar 2024 17:44:17 GMT   (3070kb,D)

Title: Fact-Checking the Output of Large Language Models via Token-Level
  Uncertainty Quantification
Authors: Ekaterina Fadeeva, Aleksandr Rubashevskii, Artem Shelmanov, Sergey
  Petrakov, Haonan Li, Hamdy Mubarak, Evgenii Tsymbalov, Gleb Kuzmin, Alexander
  Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) are notorious for hallucinating, i.e., producing
erroneous claims in their output. Such hallucinations can be dangerous, as
occasional factual inaccuracies in the generated text might be obscured by the
rest of the output being generally factual, making it extremely hard for the
users to spot them. Current services that leverage LLMs usually do not provide
any means for detecting unreliable generations. Here, we aim to bridge this
gap. In particular, we propose a novel fact-checking and hallucination
detection pipeline based on token-level uncertainty quantification. Uncertainty
scores leverage information encapsulated in the output of a neural network or
its layers to detect unreliable predictions, and we show that they can be used
to fact-check the atomic claims in the LLM output. Moreover, we present a novel
token-level uncertainty quantification method that removes the impact of
uncertainty about what claim to generate on the current step and what surface
form to use. Our method Claim Conditioned Probability (CCP) measures only the
uncertainty of particular claim value expressed by the model. Experiments on
the task of biography generation demonstrate strong improvements for CCP
compared to the baselines for six different LLMs and three languages. Human
evaluation reveals that the fact-checking pipeline based on uncertainty
quantification is competitive with a fact-checking tool that leverages external
knowledge.
\\ ( https://arxiv.org/abs/2403.04696 ,  3070kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04706
Date: Thu, 7 Mar 2024 18:00:40 GMT   (2554kb,D)

Title: Common 7B Language Models Already Possess Strong Math Capabilities
Authors: Chen Li, Weiqi Wang, Jingcheng Hu, Yixuan Wei, Nanning Zheng, Han Hu,
  Zheng Zhang, Houwen Peng
Categories: cs.CL cs.AI
\\
  Mathematical capabilities were previously believed to emerge in common
language models only at a very large scale or require extensive math-related
pre-training. This paper shows that the LLaMA-2 7B model with common
pre-training already exhibits strong mathematical abilities, as evidenced by
its impressive accuracy of 97.7% and 72.0% on the GSM8K and MATH benchmarks,
respectively, when selecting the best response from 256 random generations. The
primary issue with the current base model is the difficulty in consistently
eliciting its inherent mathematical capabilities. Notably, the accuracy for the
first answer drops to 49.5% and 7.9% on the GSM8K and MATH benchmarks,
respectively. We find that simply scaling up the SFT data can significantly
enhance the reliability of generating correct answers. However, the potential
for extensive scaling is constrained by the scarcity of publicly available math
questions. To overcome this limitation, we employ synthetic data, which proves
to be nearly as effective as real data and shows no clear saturation when
scaled up to approximately one million samples. This straightforward approach
achieves an accuracy of 82.6% on GSM8K and 40.6% on MATH using LLaMA-2 7B
models, surpassing previous models by 14.2% and 20.8%, respectively. We also
provide insights into scaling behaviors across different reasoning complexities
and error types.
\\ ( https://arxiv.org/abs/2403.04706 ,  2554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04746
Date: Thu, 7 Mar 2024 18:50:51 GMT   (7453kb,D)

Title: LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error
Authors: Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su
Categories: cs.CL cs.AI cs.LG
Comments: Code and data available at
  https://github.com/microsoft/simulated-trial-and-error
\\
  Tools are essential for large language models (LLMs) to acquire up-to-date
information and take consequential actions in external environments. Existing
work on tool-augmented LLMs primarily focuses on the broad coverage of tools
and the flexibility of adding new tools. However, a critical aspect that has
surprisingly been understudied is simply how accurately an LLM uses tools for
which it has been trained. We find that existing LLMs, including GPT-4 and
open-source LLMs specifically fine-tuned for tool use, only reach a correctness
rate in the range of 30% to 60%, far from reliable use in practice. We propose
a biologically inspired method for tool-augmented LLMs, simulated trial and
error (STE), that orchestrates three key mechanisms for successful tool use
behaviors in the biological system: trial and error, imagination, and memory.
Specifically, STE leverages an LLM's 'imagination' to simulate plausible
scenarios for using a tool, after which the LLM interacts with the tool to
learn from its execution feedback. Both short-term and long-term memory are
employed to improve the depth and breadth of the exploration, respectively.
Comprehensive experiments on ToolBench show that STE substantially improves
tool learning for LLMs under both in-context learning and fine-tuning settings,
bringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform
GPT-4. We also show effective continual learning of tools via a simple
experience replay strategy.
\\ ( https://arxiv.org/abs/2403.04746 ,  7453kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03967
Date: Wed, 6 Mar 2024 15:41:21 GMT   (893kb,D)

Title: Effect of Ambient-Intrinsic Dimension Gap on Adversarial Vulnerability
Authors: Rajdeep Haldar, Yue Xing, Qifan Song
Categories: cs.LG cs.CR stat.ML
\\
  The existence of adversarial attacks on machine learning models imperceptible
to a human is still quite a mystery from a theoretical perspective. In this
work, we introduce two notions of adversarial attacks: natural or on-manifold
attacks, which are perceptible by a human/oracle, and unnatural or off-manifold
attacks, which are not. We argue that the existence of the off-manifold attacks
is a natural consequence of the dimension gap between the intrinsic and ambient
dimensions of the data. For 2-layer ReLU networks, we prove that even though
the dimension gap does not affect generalization performance on samples drawn
from the observed data space, it makes the clean-trained model more vulnerable
to adversarial perturbations in the off-manifold direction of the data space.
Our main results provide an explicit relationship between the
$\ell_2,\ell_{\infty}$ attack strength of the on/off-manifold attack and the
dimension gap.
\\ ( https://arxiv.org/abs/2403.03967 ,  893kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04007
Date: Wed, 6 Mar 2024 19:39:20 GMT   (3094kb,D)

Title: Sampling-based Safe Reinforcement Learning for Nonlinear Dynamical
  Systems
Authors: Wesley A. Suttle, Vipul K. Sharma, Krishna C. Kosaraju, S.
  Sivaranjani, Ji Liu, Vijay Gupta, Brian M. Sadler
Categories: cs.LG math.OC
Comments: 20 pages, 7 figures
\\
  We develop provably safe and convergent reinforcement learning (RL)
algorithms for control of nonlinear dynamical systems, bridging the gap between
the hard safety guarantees of control theory and the convergence guarantees of
RL theory. Recent advances at the intersection of control and RL follow a
two-stage, safety filter approach to enforcing hard safety constraints:
model-free RL is used to learn a potentially unsafe controller, whose actions
are projected onto safe sets prescribed, for example, by a control barrier
function. Though safe, such approaches lose any convergence guarantees enjoyed
by the underlying RL methods. In this paper, we develop a single-stage,
sampling-based approach to hard constraint satisfaction that learns RL
controllers enjoying classical convergence guarantees while satisfying hard
safety constraints throughout training and deployment. We validate the efficacy
of our approach in simulation, including safe control of a quadcopter in a
challenging obstacle avoidance problem, and demonstrate that it outperforms
existing benchmarks.
\\ ( https://arxiv.org/abs/2403.04007 ,  3094kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04010
Date: Wed, 6 Mar 2024 19:42:34 GMT   (165kb,D)

Title: Three Revisits to Node-Level Graph Anomaly Detection: Outliers, Message
  Passing and Hyperbolic Neural Networks
Authors: Jing Gu, Dongmian Zou
Categories: cs.LG
Comments: Presented at the Second Learning on Graphs Conference (LoG 2023)
\\
  Graph anomaly detection plays a vital role for identifying abnormal instances
in complex networks. Despite advancements of methodology based on deep learning
in recent years, existing benchmarking approaches exhibit limitations that
hinder a comprehensive comparison. In this paper, we revisit datasets and
approaches for unsupervised node-level graph anomaly detection tasks from three
aspects. Firstly, we introduce outlier injection methods that create more
diverse and graph-based anomalies in graph datasets. Secondly, we compare
methods employing message passing against those without, uncovering the
unexpected decline in performance associated with message passing. Thirdly, we
explore the use of hyperbolic neural networks, specifying crucial architecture
and loss design that contribute to enhanced performance. Through rigorous
experiments and evaluations, our study sheds light on general strategies for
improving node-level graph anomaly detection methods.
\\ ( https://arxiv.org/abs/2403.04010 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04012
Date: Wed, 6 Mar 2024 19:46:44 GMT   (247kb,D)

Title: Temporal Cross-Attention for Dynamic Embedding and Tokenization of
  Multimodal Electronic Health Records
Authors: Yingbo Ma, Suraj Kolla, Dhruv Kaliraman, Victoria Nolan, Zhenhong Hu,
  Ziyuan Guan, Yuanfang Ren, Brooke Armfield, Tezcan Ozrazgat-Baslanti, Tyler
  J. Loftus, Parisa Rashidi, Azra Bihorac, Benjamin Shickel
Categories: cs.LG
Comments: ICLR 2024 Workshop on Learning From Time Series for Health. 10 pages,
  3 figures
\\
  The breadth, scale, and temporal granularity of modern electronic health
records (EHR) systems offers great potential for estimating personalized and
contextual patient health trajectories using sequential deep learning. However,
learning useful representations of EHR data is challenging due to its high
dimensionality, sparsity, multimodality, irregular and variable-specific
recording frequency, and timestamp duplication when multiple measurements are
recorded simultaneously. Although recent efforts to fuse structured EHR and
unstructured clinical notes suggest the potential for more accurate prediction
of clinical outcomes, less focus has been placed on EHR embedding approaches
that directly address temporal EHR challenges by learning time-aware
representations from multimodal patient time series. In this paper, we
introduce a dynamic embedding and tokenization framework for precise
representation of multimodal clinical time series that combines novel methods
for encoding time and sequential position with temporal cross-attention. Our
embedding and tokenization framework, when integrated into a multitask
transformer classifier with sliding window attention, outperformed baseline
approaches on the exemplar task of predicting the occurrence of nine
postoperative complications of more than 120,000 major inpatient surgeries
using multimodal data from three hospitals and two academic health centers in
the United States.
\\ ( https://arxiv.org/abs/2403.04012 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04015
Date: Wed, 6 Mar 2024 19:58:19 GMT   (2738kb,D)

Title: Knockoff-Guided Feature Selection via A Single Pre-trained Reinforced
  Agent
Authors: Xinyuan Wang, Dongjie Wang, Wangyang Ying, Rui Xie, Haifeng Chen,
  Yanjie Fu
Categories: cs.LG cs.AI stat.ML
\\
  Feature selection prepares the AI-readiness of data by eliminating redundant
features. Prior research falls into two primary categories: i) Supervised
Feature Selection, which identifies the optimal feature subset based on their
relevance to the target variable; ii) Unsupervised Feature Selection, which
reduces the feature space dimensionality by capturing the essential information
within the feature set instead of using target variable. However, SFS
approaches suffer from time-consuming processes and limited generalizability
due to the dependence on the target variable and downstream ML tasks. UFS
methods are constrained by the deducted feature space is latent and
untraceable. To address these challenges, we introduce an innovative framework
for feature selection, which is guided by knockoff features and optimized
through reinforcement learning, to identify the optimal and effective feature
subset. In detail, our method involves generating "knockoff" features that
replicate the distribution and characteristics of the original features but are
independent of the target variable. Each feature is then assigned a pseudo
label based on its correlation with all the knockoff features, serving as a
novel metric for feature evaluation. Our approach utilizes these pseudo labels
to guide the feature selection process in 3 novel ways, optimized by a single
reinforced agent: 1). A deep Q-network, pre-trained with the original features
and their corresponding pseudo labels, is employed to improve the efficacy of
the exploration process in feature selection. 2). We introduce unsupervised
rewards to evaluate the feature subset quality based on the pseudo labels and
the feature space reconstruction loss to reduce dependencies on the target
variable. 3). A new {\epsilon}-greedy strategy is used, incorporating insights
from the pseudo labels to make the feature selection process more effective.
\\ ( https://arxiv.org/abs/2403.04015 ,  2738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04033
Date: Wed, 6 Mar 2024 20:23:59 GMT   (474kb)

Title: Online Learning with Unknown Constraints
Authors: Karthik Sridharan and Seung Won Wilson Yoo
Categories: cs.LG cs.AI math.ST stat.ML stat.TH
\\
  We consider the problem of online learning where the sequence of actions
played by the learner must adhere to an unknown safety constraint at every
round. The goal is to minimize regret with respect to the best safe action in
hindsight while simultaneously satisfying the safety constraint with high
probability on each round. We provide a general meta-algorithm that leverages
an online regression oracle to estimate the unknown safety constraint, and
converts the predictions of an online learning oracle to predictions that
adhere to the unknown safety constraint. On the theoretical side, our
algorithm's regret can be bounded by the regret of the online regression and
online learning oracles, the eluder dimension of the model class containing the
unknown safety constraint, and a novel complexity measure that captures the
difficulty of safe learning. We complement our result with an asymptotic lower
bound that shows that the aforementioned complexity measure is necessary. When
the constraints are linear, we instantiate our result to provide a concrete
algorithm with $\sqrt{T}$ regret using a scaling transformation that balances
optimistic exploration with pessimistic constraint satisfaction.
\\ ( https://arxiv.org/abs/2403.04033 ,  474kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04036
Date: Wed, 6 Mar 2024 20:33:55 GMT   (832kb,D)

Title: Unsupervised Contrastive Learning for Robust RF Device Fingerprinting
  Under Time-Domain Shift
Authors: Jun Chen, Weng-Keen Wong, Bechir Hamdaoui
Categories: cs.LG cs.AI eess.SP
Comments: 6 pages, 5 figures, accepted by 2024 IEEE International Conference on
  Communications (ICC)
\\
  Radio Frequency (RF) device fingerprinting has been recognized as a potential
technology for enabling automated wireless device identification and
classification. However, it faces a key challenge due to the domain shift that
could arise from variations in the channel conditions and environmental
settings, potentially degrading the accuracy of RF-based device classification
when testing and training data is collected in different domains. This paper
introduces a novel solution that leverages contrastive learning to mitigate
this domain shift problem. Contrastive learning, a state-of-the-art
self-supervised learning approach from deep learning, learns a distance metric
such that positive pairs are closer (i.e. more similar) in the learned metric
space than negative pairs. When applied to RF fingerprinting, our model treats
RF signals from the same transmission as positive pairs and those from
different transmissions as negative pairs. Through experiments on wireless and
wired RF datasets collected over several days, we demonstrate that our
contrastive learning approach captures domain-invariant features, diminishing
the effects of domain-specific variations. Our results show large and
consistent improvements in accuracy (10.8\% to 27.8\%) over baseline models,
thus underscoring the effectiveness of contrastive learning in improving device
classification under domain shift.
\\ ( https://arxiv.org/abs/2403.04036 ,  832kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04037
Date: Wed, 6 Mar 2024 20:34:08 GMT   (202kb,D)

Title: OCD-FL: A Novel Communication-Efficient Peer Selection-based
  Decentralized Federated Learning
Authors: Nizar Masmoudi, Wael Jaafar
Categories: cs.LG cs.DC
Comments: 5 pages, submitted to IEEE Transactions on Vehicular Technology as a
  Correspondance
\\
  The conjunction of edge intelligence and the ever-growing Internet-of-Things
(IoT) network heralds a new era of collaborative machine learning, with
federated learning (FL) emerging as the most prominent paradigm. With the
growing interest in these learning schemes, researchers started addressing some
of their most fundamental limitations. Indeed, conventional FL with a central
aggregator presents a single point of failure and a network bottleneck. To
bypass this issue, decentralized FL where nodes collaborate in a peer-to-peer
network has been proposed. Despite the latter's efficiency, communication costs
and data heterogeneity remain key challenges in decentralized FL. In this
context, we propose a novel scheme, called opportunistic
communication-efficient decentralized federated learning, a.k.a., OCD-FL,
consisting of a systematic FL peer selection for collaboration, aiming to
achieve maximum FL knowledge gain while reducing energy consumption.
Experimental results demonstrate the capability of OCD-FL to achieve similar or
better performances than the fully collaborative FL, while significantly
reducing consumed energy by at least 30% and up to 80%.
\\ ( https://arxiv.org/abs/2403.04037 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04039
Date: Wed, 6 Mar 2024 20:37:29 GMT   (686kb,D)

Title: Sample size planning for conditional counterfactual mean estimation with
  a K-armed randomized experiment
Authors: Gabriel Ruiz
Categories: cs.LG stat.ME stat.ML
Comments: 10 pages, 3 figures
\\
  We cover how to determine a sufficiently large sample size for a $K$-armed
randomized experiment in order to estimate conditional counterfactual
expectations in data-driven subgroups. The sub-groups can be output by any
feature space partitioning algorithm, including as defined by binning users
having similar predictive scores or as defined by a learned policy tree. After
carefully specifying the inference target, a minimum confidence level, and a
maximum margin of error, the key is to turn the original goal into a
simultaneous inference problem where the recommended sample size to offset an
increased possibility of estimation error is directly related to the number of
inferences to be conducted. Given a fixed sample size budget, our result allows
us to invert the question to one about the feasible number of treatment arms or
partition complexity (e.g. number of decision tree leaves). Using policy trees
to learn sub-groups, we evaluate our nominal guarantees on a large
publicly-available randomized experiment test data set.
\\ ( https://arxiv.org/abs/2403.04039 ,  686kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04050
Date: Wed, 6 Mar 2024 20:52:49 GMT   (518kb,D)

Title: Belief-Enriched Pessimistic Q-Learning against Adversarial State
  Perturbations
Authors: Xiaolin Sun, Zizhan Zheng
Categories: cs.LG
Comments: ICLR 2024
\\
  Reinforcement learning (RL) has achieved phenomenal success in various
domains. However, its data-driven nature also introduces new vulnerabilities
that can be exploited by malicious opponents. Recent work shows that a
well-trained RL agent can be easily manipulated by strategically perturbing its
state observations at the test stage. Existing solutions either introduce a
regularization term to improve the smoothness of the trained policy against
perturbations or alternatively train the agent's policy and the attacker's
policy. However, the former does not provide sufficient protection against
strong attacks, while the latter is computationally prohibitive for large
environments. In this work, we propose a new robust RL algorithm for deriving a
pessimistic policy to safeguard against an agent's uncertainty about true
states. This approach is further enhanced with belief state inference and
diffusion-based state purification to reduce uncertainty. Empirical results
show that our approach obtains superb performance under strong attacks and has
a comparable training overhead with regularization-based methods. Our code is
available at https://github.com/SliencerX/Belief-enriched-robust-Q-learning.
\\ ( https://arxiv.org/abs/2403.04050 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04070
Date: Wed, 6 Mar 2024 21:50:52 GMT   (866kb,D)

Title: Improving Adversarial Training using Vulnerability-Aware Perturbation
  Budget
Authors: Olukorede Fakorede, Modeste Atsague, Jin Tian
Categories: cs.LG cs.AI cs.CR cs.CV
Comments: 19 pages, 2 figures
\\
  Adversarial Training (AT) effectively improves the robustness of Deep Neural
Networks (DNNs) to adversarial attacks. Generally, AT involves training DNN
models with adversarial examples obtained within a pre-defined, fixed
perturbation bound. Notably, individual natural examples from which these
adversarial examples are crafted exhibit varying degrees of intrinsic
vulnerabilities, and as such, crafting adversarial examples with fixed
perturbation radius for all instances may not sufficiently unleash the potency
of AT. Motivated by this observation, we propose two simple, computationally
cheap vulnerability-aware reweighting functions for assigning perturbation
bounds to adversarial examples used for AT, named Margin-Weighted Perturbation
Budget (MWPB) and Standard-Deviation-Weighted Perturbation Budget (SDWPB). The
proposed methods assign perturbation radii to individual adversarial samples
based on the vulnerability of their corresponding natural examples.
Experimental results show that the proposed methods yield genuine improvements
in the robustness of AT algorithms against various adversarial attacks.
\\ ( https://arxiv.org/abs/2403.04070 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04081
Date: Wed, 6 Mar 2024 22:24:05 GMT   (2526kb,D)

Title: Directional Smoothness and Gradient Methods: Convergence and Adaptivity
Authors: Aaron Mishkin, Ahmed Khaled, Yuanhao Wang, Aaron Defazio, and Robert
  M. Gower
Categories: cs.LG math.OC
Comments: Twenty-four pages
\\
  We develop new sub-optimality bounds for gradient descent (GD) that depend on
the conditioning of the objective along the path of optimization, rather than
on global, worst-case constants. Key to our proofs is directional smoothness, a
measure of gradient variation that we use to develop upper-bounds on the
objective. Minimizing these upper-bounds requires solving implicit equations to
obtain a sequence of strongly adapted step-sizes; we show that these equations
are straightforward to solve for convex quadratics and lead to new guarantees
for two classical step-sizes. For general functions, we prove that the Polyak
step-size and normalized GD obtain fast, path-dependent rates despite using no
knowledge of the directional smoothness. Experiments on logistic regression
show our convergence guarantees are tighter than the classical theory based on
L-smoothness.
\\ ( https://arxiv.org/abs/2403.04081 ,  2526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04082
Date: Wed, 6 Mar 2024 22:27:30 GMT   (4265kb,D)

Title: Inference via Interpolation: Contrastive Representations Provably Enable
  Planning and Inference
Authors: Benjamin Eysenbach, Vivek Myers, Ruslan Salakhutdinov, Sergey Levine
Categories: cs.LG stat.ML
Comments: Code: https://github.com/vivekmyers/contrastive_planning
\\
  Given time series data, how can we answer questions like "what will happen in
the future?" and "how did we get here?" These sorts of probabilistic inference
questions are challenging when observations are high-dimensional. In this
paper, we show how these questions can have compact, closed form solutions in
terms of learned representations. The key idea is to apply a variant of
contrastive learning to time series data. Prior work already shows that the
representations learned by contrastive learning encode a probability ratio. By
extending prior work to show that the marginal distribution over
representations is Gaussian, we can then prove that joint distribution of
representations is also Gaussian. Taken together, these results show that
representations learned via temporal contrastive learning follow a Gauss-Markov
chain, a graphical model where inference (e.g., prediction, planning) over
representations corresponds to inverting a low-dimensional matrix. In one
special case, inferring intermediate representations will be equivalent to
interpolating between the learned representations. We validate our theory using
numerical simulations on tasks up to 46-dimensions.
\\ ( https://arxiv.org/abs/2403.04082 ,  4265kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04086
Date: Wed, 6 Mar 2024 22:32:48 GMT   (1815kb,D)

Title: Automated Multi-Task Learning for Joint Disease Prediction on Electronic
  Health Records
Authors: Suhan Cui and Prasenjit Mitra
Categories: cs.LG
\\
  In the realm of big data and digital healthcare, Electronic Health Records
(EHR) have become a rich source of information with the potential to improve
patient care and medical research. In recent years, machine learning models
have proliferated for analyzing EHR data to predict patients future health
conditions. Among them, some studies advocate for multi-task learning (MTL) to
jointly predict multiple target diseases for improving the prediction
performance over single task learning. Nevertheless, current MTL frameworks for
EHR data have significant limitations due to their heavy reliance on human
experts to identify task groups for joint training and design model
architectures. To reduce human intervention and improve the framework design,
we propose an automated approach named AutoDP, which can search for the optimal
configuration of task grouping and architectures simultaneously. To tackle the
vast joint search space encompassing task combinations and architectures, we
employ surrogate model-based optimization, enabling us to efficiently discover
the optimal solution. Experimental results on real-world EHR data demonstrate
the efficacy of the proposed AutoDP framework. It achieves significant
performance improvements over both hand-crafted and automated state-of-the-art
methods, also maintains a feasible search cost at the same time.
\\ ( https://arxiv.org/abs/2403.04086 ,  1815kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04099
Date: Wed, 6 Mar 2024 23:03:12 GMT   (2736kb,D)

Title: Many-Objective Multi-Solution Transport
Authors: Ziyue Li, Tian Li, Virginia Smith, Jeff Bilmes, Tianyi Zhou
Categories: cs.LG
\\
  Optimizing the performance of many objectives (instantiated by tasks or
clients) jointly with a few Pareto stationary solutions (models) is critical in
machine learning. However, previous multi-objective optimization methods often
focus on a few number of objectives and cannot scale to many objectives that
outnumber the solutions, leading to either subpar performance or ignored
objectives. We introduce Many-objective multi-solution Transport (MosT), a
framework that finds multiple diverse solutions in the Pareto front of many
objectives. Our insight is to seek multiple solutions, each performing as a
domain expert and focusing on a specific subset of objectives while
collectively covering all of them. MosT formulates the problem as a bi-level
optimization of weighted objectives for each solution, where the weights are
defined by an optimal transport between the objectives and solutions. Our
algorithm ensures convergence to Pareto stationary solutions for complementary
subsets of objectives. On a range of applications in federated learning,
multi-task learning, and mixture-of-prompt learning for LLMs, MosT distinctly
outperforms strong baselines, delivering high-quality, diverse solutions that
profile the entire Pareto frontier, thus ensuring balanced trade-offs across
many objectives.
\\ ( https://arxiv.org/abs/2403.04099 ,  2736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04146
Date: Thu, 7 Mar 2024 01:52:05 GMT   (2187kb)

Title: FL-GUARD: A Holistic Framework for Run-Time Detection and Recovery of
  Negative Federated Learning
Authors: Hong Lin, Lidan Shou, Ke Chen, Gang Chen, Sai Wu
Categories: cs.LG cs.AI cs.DC
Journal-ref: Data Science and Engineering (2024)
DOI: 10.1007/s41019-024-00243-0
\\
  Federated learning (FL) is a promising approach for learning a model from
data distributed on massive clients without exposing data privacy. It works
effectively in the ideal federation where clients share homogeneous data
distribution and learning behavior. However, FL may fail to function
appropriately when the federation is not ideal, amid an unhealthy state called
Negative Federated Learning (NFL), in which most clients gain no benefit from
participating in FL. Many studies have tried to address NFL. However, their
solutions either (1) predetermine to prevent NFL in the entire learning
life-cycle or (2) tackle NFL in the aftermath of numerous learning rounds.
Thus, they either (1) indiscriminately incur extra costs even if FL can perform
well without such costs or (2) waste numerous learning rounds. Additionally,
none of the previous work takes into account the clients who may be
unwilling/unable to follow the proposed NFL solutions when using those
solutions to upgrade an FL system in use. This paper introduces FL-GUARD, a
holistic framework that can be employed on any FL system for tackling NFL in a
run-time paradigm. That is, to dynamically detect NFL at the early stage (tens
of rounds) of learning and then to activate recovery measures when necessary.
Specifically, we devise a cost-effective NFL detection mechanism, which relies
on an estimation of performance gain on clients. Only when NFL is detected, we
activate the NFL recovery process, in which each client learns in parallel an
adapted model when training the global model. Extensive experiment results
confirm the effectiveness of FL-GUARD in detecting NFL and recovering from NFL
to a healthy learning state. We also show that FL-GUARD is compatible with
previous NFL solutions and robust against clients unwilling/unable to take any
recovery measures.
\\ ( https://arxiv.org/abs/2403.04146 ,  2187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04154
Date: Thu, 7 Mar 2024 02:24:45 GMT   (15416kb,D)

Title: Stabilizing Policy Gradients for Stochastic Differential Equations via
  Consistency with Perturbation Process
Authors: Xiangxin Zhou, Liang Wang, Yichi Zhou
Categories: cs.LG
\\
  Considering generating samples with high rewards, we focus on optimizing deep
neural networks parameterized stochastic differential equations (SDEs), the
advanced generative models with high expressiveness, with policy gradient, the
leading algorithm in reinforcement learning. Nevertheless, when applying policy
gradients to SDEs, since the policy gradient is estimated on a finite set of
trajectories, it can be ill-defined, and the policy behavior in data-scarce
regions may be uncontrolled. This challenge compromises the stability of policy
gradients and negatively impacts sample complexity. To address these issues, we
propose constraining the SDE to be consistent with its associated perturbation
process. Since the perturbation process covers the entire space and is easy to
sample, we can mitigate the aforementioned problems. Our framework offers a
general approach allowing for a versatile selection of policy gradient methods
to effectively and efficiently train SDEs. We evaluate our algorithm on the
task of structure-based drug design and optimize the binding affinity of
generated ligand molecules. Our method achieves the best Vina score -9.07 on
the CrossDocked2020 dataset.
\\ ( https://arxiv.org/abs/2403.04154 ,  15416kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04161
Date: Thu, 7 Mar 2024 02:40:42 GMT   (19319kb,D)

Title: SWAP-NAS: Sample-Wise Activation Patterns For Ultra-Fast NAS
Authors: Yameng Peng, Andy Song, Haytham M. Fayek, Vic Ciesielski, Xiaojun
  Chang
Categories: cs.LG cs.CV cs.NE
Comments: ICLR2024 Spotlight
\\
  Training-free metrics (a.k.a. zero-cost proxies) are widely used to avoid
resource-intensive neural network training, especially in Neural Architecture
Search (NAS). Recent studies show that existing training-free metrics have
several limitations, such as limited correlation and poor generalisation across
different search spaces and tasks. Hence, we propose Sample-Wise Activation
Patterns and its derivative, SWAP-Score, a novel high-performance training-free
metric. It measures the expressivity of networks over a batch of input samples.
The SWAP-Score is strongly correlated with ground-truth performance across
various search spaces and tasks, outperforming 15 existing training-free
metrics on NAS-Bench-101/201/301 and TransNAS-Bench-101. The SWAP-Score can be
further enhanced by regularisation, which leads to even higher correlations in
cell-based search space and enables model size control during the search. For
example, Spearman's rank correlation coefficient between regularised SWAP-Score
and CIFAR-100 validation accuracies on NAS-Bench-201 networks is 0.90,
significantly higher than 0.80 from the second-best metric, NWOT. When
integrated with an evolutionary algorithm for NAS, our SWAP-NAS achieves
competitive performance on CIFAR-10 and ImageNet in approximately 6 minutes and
9 minutes of GPU time respectively.
\\ ( https://arxiv.org/abs/2403.04161 ,  19319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04162
Date: Thu, 7 Mar 2024 02:47:08 GMT   (2621kb,D)

Title: Noisy Spiking Actor Network for Exploration
Authors: Ding Chen, Peixi Peng, Tiejun Huang and Yonghong Tian
Categories: cs.LG cs.NE
Comments: 13 pages, 6 figures
\\
  As a general method for exploration in deep reinforcement learning (RL),
NoisyNet can produce problem-specific exploration strategies. Spiking neural
networks (SNNs), due to their binary firing mechanism, have strong robustness
to noise, making it difficult to realize efficient exploration with local
disturbances. To solve this exploration problem, we propose a noisy spiking
actor network (NoisySAN) that introduces time-correlated noise during charging
and transmission. Moreover, a noise reduction method is proposed to find a
stable policy for the agent. Extensive experimental results demonstrate that
our method outperforms the state-of-the-art performance on a wide range of
continuous control tasks from OpenAI gym.
\\ ( https://arxiv.org/abs/2403.04162 ,  2621kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04180
Date: Thu, 7 Mar 2024 03:23:13 GMT   (667kb)

Title: RATSF: Empowering Customer Service Volume Management through
  Retrieval-Augmented Time-Series Forecasting
Authors: Tianfeng Wang, Gaojie Cui
Categories: cs.LG cs.IR
Comments: Submitted for review to KDD24 (ADS Track)
\\
  An efficient customer service management system hinges on precise forecasting
of service volume. In this scenario, where data non-stationarity is pronounced,
successful forecasting heavily relies on identifying and leveraging similar
historical data rather than merely summarizing periodic patterns. Existing
models based on RNN or Transformer architectures often struggle with this
flexible and effective utilization. To address this challenge, we propose an
efficient and adaptable cross-attention module termed RACA, which effectively
leverages historical segments in forecasting task, and we devised a precise
representation scheme for querying historical sequences, coupled with the
design of a knowledge repository. These critical components collectively form
our Retrieval-Augmented Temporal Sequence Forecasting framework (RATSF). RATSF
not only significantly enhances performance in the context of Fliggy hotel
service volume forecasting but, more crucially, can be seamlessly integrated
into other Transformer-based time-series forecasting models across various
application scenarios. Extensive experimentation has validated the
effectiveness and generalizability of this system design across multiple
diverse contexts.
\\ ( https://arxiv.org/abs/2403.04180 ,  667kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04190
Date: Thu, 7 Mar 2024 03:38:44 GMT   (58kb,D)

Title: Generative AI for Synthetic Data Generation: Methods, Challenges and the
  Future
Authors: Xu Guo, Yiqiang Chen
Categories: cs.LG cs.AI cs.CL
ACM-class: I.2.0
\\
  The recent surge in research focused on generating synthetic data from large
language models (LLMs), especially for scenarios with limited data
availability, marks a notable shift in Generative Artificial Intelligence (AI).
Their ability to perform comparably to real-world data positions this approach
as a compelling solution to low-resource challenges. This paper delves into
advanced technologies that leverage these gigantic LLMs for the generation of
task-specific training data. We outline methodologies, evaluation techniques,
and practical applications, discuss the current limitations, and suggest
potential pathways for future research.
\\ ( https://arxiv.org/abs/2403.04190 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04195
Date: Thu, 7 Mar 2024 03:55:56 GMT   (2842kb)

Title: Fill-and-Spill: Deep Reinforcement Learning Policy Gradient Methods for
  Reservoir Operation Decision and Control
Authors: Sadegh Sadeghi Tabas, Vidya Samadi
Categories: cs.LG math.OC
\\
  Changes in demand, various hydrological inputs, and environmental stressors
are among the issues that water managers and policymakers face on a regular
basis. These concerns have sparked interest in applying different techniques to
determine reservoir operation policy decisions. As the resolution of the
analysis increases, it becomes more difficult to effectively represent a
real-world system using traditional methods such as Dynamic Programming (DP)
and Stochastic Dynamic Programming (SDP) for determining the best reservoir
operation policy. One of the challenges is the "curse of dimensionality," which
means the number of samples needed to estimate an arbitrary function with a
given level of accuracy grows exponentially with respect to the number of input
variables (i.e., dimensionality) of the function. Deep Reinforcement Learning
(DRL) is an intelligent approach to overcome the curses of stochastic
optimization problems for reservoir operation policy decisions. To our
knowledge, this study is the first attempt that examine various novel DRL
continuous-action policy gradient methods (PGMs), including Deep Deterministic
Policy Gradients (DDPG), Twin Delayed DDPG (TD3), and two different versions of
Soft Actor-Critic (SAC18 and SAC19) for optimizing reservoir operation policy.
In this study, multiple DRL techniques were implemented in order to find the
optimal operation policy of Folsom Reservoir in California, USA. The reservoir
system supplies agricultural, municipal, hydropower, and environmental flow
demands and flood control operations to the City of Sacramento. Analysis
suggests that the TD3 and SAC are robust to meet the Folsom Reservoir's demands
and optimize reservoir operation policies.
\\ ( https://arxiv.org/abs/2403.04195 ,  2842kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04206
Date: Thu, 7 Mar 2024 04:22:34 GMT   (2925kb,D)

Title: GRAWA: Gradient-based Weighted Averaging for Distributed Training of
  Deep Learning Models
Authors: Tolga Dimlioglu, Anna Choromanska
Categories: cs.LG cs.DC math.OC
Comments: 9 pages main of main text, in total 24
\\
  We study distributed training of deep learning models in time-constrained
environments. We propose a new algorithm that periodically pulls workers
towards the center variable computed as a weighted average of workers, where
the weights are inversely proportional to the gradient norms of the workers
such that recovering the flat regions in the optimization landscape is
prioritized. We develop two asynchronous variants of the proposed algorithm
that we call Model-level and Layer-level Gradient-based Weighted Averaging
(resp. MGRAWA and LGRAWA), which differ in terms of the weighting scheme that
is either done with respect to the entire model or is applied layer-wise. On
the theoretical front, we prove the convergence guarantee for the proposed
approach in both convex and non-convex settings. We then experimentally
demonstrate that our algorithms outperform the competitor methods by achieving
faster convergence and recovering better quality and flatter local optima. We
also carry out an ablation study to analyze the scalability of the proposed
algorithms in more crowded distributed training environments. Finally, we
report that our approach requires less frequent communication and fewer
distributed updates compared to the state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2403.04206 ,  2925kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04207
Date: Thu, 7 Mar 2024 04:23:07 GMT   (1443kb,D)

Title: HeteroSwitch: Characterizing and Taming System-Induced Data
  Heterogeneity in Federated Learning
Authors: Gyudong Kim, Mehdi Ghasemi, Soroush Heidari, Seungryong Kim, Young
  Geun Kim, Sarma Vrudhula, Carole-Jean Wu
Categories: cs.LG cs.DC
\\
  Federated Learning (FL) is a practical approach to train deep learning models
collaboratively across user-end devices, protecting user privacy by retaining
raw data on-device. In FL, participating user-end devices are highly fragmented
in terms of hardware and software configurations. Such fragmentation introduces
a new type of data heterogeneity in FL, namely \textit{system-induced data
heterogeneity}, as each device generates distinct data depending on its
hardware and software configurations. In this paper, we first characterize the
impact of system-induced data heterogeneity on FL model performance. We collect
a dataset using heterogeneous devices with variations across vendors and
performance tiers. By using this dataset, we demonstrate that
\textit{system-induced data heterogeneity} negatively impacts accuracy, and
deteriorates fairness and domain generalization problems in FL. To address
these challenges, we propose HeteroSwitch, which adaptively adopts
generalization techniques (i.e., ISP transformation and SWAD) depending on the
level of bias caused by varying HW and SW configurations. In our evaluation
with a realistic FL dataset (FLAIR), HeteroSwitch reduces the variance of
averaged precision by 6.3\% across device types.
\\ ( https://arxiv.org/abs/2403.04207 ,  1443kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04221
Date: Thu, 7 Mar 2024 04:49:48 GMT   (1430kb,D)

Title: Why Online Reinforcement Learning is Causal
Authors: Oliver Schulte, Pascal Poupart
Categories: cs.LG cs.AI
Comments: 27 pages
ACM-class: I.2.6
\\
  Reinforcement learning (RL) and causal modelling naturally complement each
other. The goal of causal modelling is to predict the effects of interventions
in an environment, while the goal of reinforcement learning is to select
interventions that maximize the rewards the agent receives from the
environment. Reinforcement learning includes the two most powerful sources of
information for estimating causal relationships: temporal ordering and the
ability to act on an environment. This paper examines which reinforcement
learning settings we can expect to benefit from causal modelling, and how. In
online learning, the agent has the ability to interact directly with their
environment, and learn from exploring it. Our main argument is that in online
learning, conditional probabilities are causal, and therefore offline RL is the
setting where causal learning has the most potential to make a difference.
Essentially, the reason is that when an agent learns from their {\em own}
experience, there are no unobserved confounders that influence both the agent's
own exploratory actions and the rewards they receive. Our paper formalizes this
argument. For offline RL, where an agent may and typically does learn from the
experience of {\em others}, we describe previous and new methods for leveraging
a causal model, including support for counterfactual queries.
\\ ( https://arxiv.org/abs/2403.04221 ,  1430kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04236
Date: Thu, 7 Mar 2024 05:38:56 GMT   (187kb)

Title: Regularized DeepIV with Model Selection
Authors: Zihao Li, Hui Lan, Vasilis Syrgkanis, Mengdi Wang, Masatoshi Uehara
Categories: cs.LG econ.EM math.ST stat.ML stat.TH
\\
  In this paper, we study nonparametric estimation of instrumental variable
(IV) regressions. While recent advancements in machine learning have introduced
flexible methods for IV estimation, they often encounter one or more of the
following limitations: (1) restricting the IV regression to be uniquely
identified; (2) requiring minimax computation oracle, which is highly unstable
in practice; (3) absence of model selection procedure. In this paper, we
present the first method and analysis that can avoid all three limitations,
while still enabling general function approximation. Specifically, we propose a
minimax-oracle-free method called Regularized DeepIV (RDIV) regression that can
converge to the least-norm IV solution. Our method consists of two stages:
first, we learn the conditional distribution of covariates, and by utilizing
the learned distribution, we learn the estimator by minimizing a
Tikhonov-regularized loss function. We further show that our method allows
model selection procedures that can achieve the oracle rates in the
misspecified regime. When extended to an iterative estimator, our method
matches the current state-of-the-art convergence rate. Our method is a Tikhonov
regularized variant of the popular DeepIV method with a non-parametric MLE
first-stage estimator, and our results provide the first rigorous guarantees
for this empirically used method, showcasing the importance of regularization
which was absent from the original work.
\\ ( https://arxiv.org/abs/2403.04236 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04253
Date: Thu, 7 Mar 2024 06:35:59 GMT   (5677kb,D)

Title: Mastering Memory Tasks with World Models
Authors: Mohammad Reza Samsami and Artem Zholus and Janarthanan Rajendran and
  Sarath Chandar
Categories: cs.LG
Comments: Published as a conference paper at The International Conference on
  Learning Representations 2024
\\
  Current model-based reinforcement learning (MBRL) agents struggle with
long-term dependencies. This limits their ability to effectively solve tasks
involving extended time gaps between actions and outcomes, or tasks demanding
the recalling of distant observations to inform current actions. To improve
temporal coherence, we integrate a new family of state space models (SSMs) in
world models of MBRL agents to present a new method, Recall to Imagine (R2I).
This integration aims to enhance both long-term memory and long-horizon credit
assignment. Through a diverse set of illustrative tasks, we systematically
demonstrate that R2I not only establishes a new state-of-the-art for
challenging memory and credit assignment RL tasks, such as BSuite and POPGym,
but also showcases superhuman performance in the complex memory domain of
Memory Maze. At the same time, it upholds comparable performance in classic RL
tasks, such as Atari and DMC, suggesting the generality of our method. We also
show that R2I is faster than the state-of-the-art MBRL method, DreamerV3,
resulting in faster wall-time convergence.
\\ ( https://arxiv.org/abs/2403.04253 ,  5677kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04317
Date: Thu, 7 Mar 2024 08:34:57 GMT   (1974kb,D)

Title: Online Adaptation of Language Models with a Memory of Amortized Contexts
Authors: Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh,
  Jonathan Richard Schwarz
Categories: cs.LG cs.CL
Comments: 14 pages
\\
  Due to the rapid generation and dissemination of information, large language
models (LLMs) quickly run out of date despite enormous development costs. Due
to this crucial need to keep models updated, online learning has emerged as a
critical necessity when utilizing LLMs for real-world applications. However,
given the ever-expanding corpus of unseen documents and the large parameter
space of modern LLMs, efficient adaptation is essential. To address these
challenges, we propose Memory of Amortized Contexts (MAC), an efficient and
effective online adaptation framework for LLMs with strong knowledge retention.
We propose an amortized feature extraction and memory-augmentation approach to
compress and extract information from new documents into compact modulations
stored in a memory bank. When answering questions, our model attends to and
extracts relevant knowledge from this memory bank. To learn informative
modulations in an efficient manner, we utilize amortization-based
meta-learning, which substitutes the optimization process with a single forward
pass of the encoder. Subsequently, we learn to choose from and aggregate
selected documents into a single modulation by conditioning on the question,
allowing us to adapt a frozen language model during test time without requiring
further gradient updates. Our experiment demonstrates the superiority of MAC in
multiple aspects, including online adaptation performance, time, and memory
efficiency. Code is available at: https://github.com/jihoontack/MAC.
\\ ( https://arxiv.org/abs/2403.04317 ,  1974kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04337
Date: Thu, 7 Mar 2024 09:02:11 GMT   (1090kb,D)

Title: Explainable AI for Embedded Systems Design: A Case Study of Static
  Redundant NVM Memory Write Prediction
Authors: Abdoulaye Gamati\'e (LIRMM | ADAC), Yuyang Wang (LIRMM | ADAC)
Categories: cs.LG cs.PL cs.SE
\\
  This paper investigates the application of eXplainable Artificial
Intelligence (XAI) in the design of embedded systems using machine learning
(ML). As a case study, it addresses the challenging problem of static silent
store prediction. This involves identifying redundant memory writes based only
on static program features. Eliminating such stores enhances performance and
energy efficiency by reducing memory access and bus traffic, especially in the
presence of emerging non-volatile memory technologies. To achieve this, we
propose a methodology consisting of: 1) the development of relevant ML models
for explaining silent store prediction, and 2) the application of XAI to
explain these models. We employ two state-of-the-art model-agnostic XAI methods
to analyze the causes of silent stores. Through the case study, we evaluate the
effectiveness of the methods. We find that these methods provide explanations
for silent store predictions, which are consistent with known causes of silent
store occurrences from previous studies. Typically, this allows us to confirm
the prevalence of silent stores in operations that write the zero constant into
memory, or the absence of silent stores in operations involving loop induction
variables. This suggests the potential relevance of XAI in analyzing ML models'
decision in embedded system design. From the case study, we share some valuable
insights and pitfalls we encountered. More generally, this study aims to lay
the groundwork for future research in the emerging field of XAI for embedded
system design.
\\ ( https://arxiv.org/abs/2403.04337 ,  1090kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04429
Date: Thu, 7 Mar 2024 11:59:00 GMT   (650kb,D)

Title: Exploring the Influence of Dimensionality Reduction on Anomaly Detection
  Performance in Multivariate Time Series
Authors: Mahsun Altin, Altan Cakir
Categories: cs.LG
Comments: Submitted to Machine Learning
\\
  This paper presents an extensive empirical study on the integration of
dimensionality reduction techniques with advanced unsupervised time series
anomaly detection models, focusing on the MUTANT and Anomaly-Transformer
models. The study involves a comprehensive evaluation across three different
datasets: MSL, SMAP, and SWaT. Each dataset poses unique challenges, allowing
for a robust assessment of the models' capabilities in varied contexts. The
dimensionality reduction techniques examined include PCA, UMAP, Random
Projection, and t-SNE, each offering distinct advantages in simplifying
high-dimensional data. Our findings reveal that dimensionality reduction not
only aids in reducing computational complexity but also significantly enhances
anomaly detection performance in certain scenarios. Moreover, a remarkable
reduction in training times was observed, with reductions by approximately
300\% and 650\% when dimensionality was halved and minimized to the lowest
dimensions, respectively. This efficiency gain underscores the dual benefit of
dimensionality reduction in both performance enhancement and operational
efficiency. The MUTANT model exhibits notable adaptability, especially with
UMAP reduction, while the Anomaly-Transformer demonstrates versatility across
various reduction techniques. These insights provide a deeper understanding of
the synergistic effects of dimensionality reduction and anomaly detection,
contributing valuable perspectives to the field of time series analysis. The
study underscores the importance of selecting appropriate dimensionality
reduction strategies based on specific model requirements and dataset
characteristics, paving the way for more efficient, accurate, and scalable
solutions in anomaly detection.
\\ ( https://arxiv.org/abs/2403.04429 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04430
Date: Thu, 7 Mar 2024 12:00:33 GMT   (3960kb,D)

Title: On-demand Quantization for Green Federated Generative Diffusion in
  Mobile Edge Networks
Authors: Bingkun Lai, Jiayi He, Jiawen Kang, Gaolei Li, Minrui Xu, Tao zhang,
  Shengli Xie
Categories: cs.LG cs.DC cs.NI
\\
  Generative Artificial Intelligence (GAI) shows remarkable productivity and
creativity in Mobile Edge Networks, such as the metaverse and the Industrial
Internet of Things. Federated learning is a promising technique for effectively
training GAI models in mobile edge networks due to its data distribution.
However, there is a notable issue with communication consumption when training
large GAI models like generative diffusion models in mobile edge networks.
Additionally, the substantial energy consumption associated with training
diffusion-based models, along with the limited resources of edge devices and
complexities of network environments, pose challenges for improving the
training efficiency of GAI models. To address this challenge, we propose an
on-demand quantized energy-efficient federated diffusion approach for mobile
edge networks. Specifically, we first design a dynamic quantized federated
diffusion training scheme considering various demands from the edge devices.
Then, we study an energy efficiency problem based on specific quantization
requirements. Numerical results show that our proposed method significantly
reduces system energy consumption and transmitted model size compared to both
baseline federated diffusion and fixed quantized federated diffusion methods
while effectively maintaining reasonable quality and diversity of generated
data.
\\ ( https://arxiv.org/abs/2403.04430 ,  3960kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04431
Date: Thu, 7 Mar 2024 12:03:04 GMT   (520kb)

Title: Boosting Fairness and Robustness in Over-the-Air Federated Learning
Authors: Halil Yigit Oksuz, Fabio Molinari, Henning Sprekeler, Joerg Raisch
Categories: cs.LG cs.CY
Comments: 6 Pages, 2 figures. arXiv admin note: text overlap with
  arXiv:2305.04630
\\
  Over-the-Air Computation is a beyond-5G communication strategy that has
recently been shown to be useful for the decentralized training of machine
learning models due to its efficiency. In this paper, we propose an
Over-the-Air federated learning algorithm that aims to provide fairness and
robustness through minmax optimization. By using the epigraph form of the
problem at hand, we show that the proposed algorithm converges to the optimal
solution of the minmax problem. Moreover, the proposed approach does not
require reconstructing channel coefficients by complex encoding-decoding
schemes as opposed to state-of-the-art approaches. This improves both
efficiency and privacy.
\\ ( https://arxiv.org/abs/2403.04431 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04442
Date: Thu, 7 Mar 2024 12:16:51 GMT   (5502kb,D)

Title: Cooperative Bayesian Optimization for Imperfect Agents
Authors: Ali Khoshvishkaie, Petrus Mikkola, Pierre-Alexandre Murena, Samuel
  Kaski
Categories: cs.LG cs.AI cs.MA
Journal-ref: Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases, ECML PKDD 2023
DOI: 10.1007/978-3-031-43412-9_28
\\
  We introduce a cooperative Bayesian optimization problem for optimizing
black-box functions of two variables where two agents choose together at which
points to query the function but have only control over one variable each. This
setting is inspired by human-AI teamwork, where an AI-assistant helps its human
user solve a problem, in this simplest case, collaborative optimization. We
formulate the solution as sequential decision-making, where the agent we
control models the user as a computationally rational agent with prior
knowledge about the function. We show that strategic planning of the queries
enables better identification of the global maximum of the function as long as
the user avoids excessive exploration. This planning is made possible by using
Bayes Adaptive Monte Carlo planning and by endowing the agent with a user model
that accounts for conservative belief updates and exploratory sampling of the
points to query.
\\ ( https://arxiv.org/abs/2403.04442 ,  5502kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04447
Date: Thu, 7 Mar 2024 12:34:03 GMT   (23kb)

Title: FRRI: a novel algorithm for fuzzy-rough rule induction
Authors: Henri Bollaert, Marko Palangeti\'c, Chris Cornelis, Salvatore Greco,
  Roman S{\l}owi\'nski
Categories: cs.LG cs.AI
\\
  Interpretability is the next frontier in machine learning research. In the
search for white box models - as opposed to black box models, like random
forests or neural networks - rule induction algorithms are a logical and
promising option, since the rules can easily be understood by humans. Fuzzy and
rough set theory have been successfully applied to this archetype, almost
always separately. As both approaches to rule induction involve granular
computing based on the concept of equivalence classes, it is natural to combine
them. The QuickRules\cite{JensenCornelis2009} algorithm was a first attempt at
using fuzzy rough set theory for rule induction. It is based on QuickReduct, a
greedy algorithm for building decision reducts. QuickRules already showed an
improvement over other rule induction methods. However, to evaluate the full
potential of a fuzzy rough rule induction algorithm, one needs to start from
the foundations. In this paper, we introduce a novel rule induction algorithm
called Fuzzy Rough Rule Induction (FRRI). We provide background and explain the
workings of our algorithm. Furthermore, we perform a computational experiment
to evaluate the performance of our algorithm and compare it to other
state-of-the-art rule induction approaches. We find that our algorithm is more
accurate while creating small rulesets consisting of relatively short rules. We
end the paper by outlining some directions for future work.
\\ ( https://arxiv.org/abs/2403.04447 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04453
Date: Thu, 7 Mar 2024 12:45:51 GMT   (377kb,D)

Title: Vlearn: Off-Policy Learning with Efficient State-Value Function
  Estimation
Authors: Fabian Otto, Philipp Becker, Vien Ang Ngo, Gerhard Neumann
Categories: cs.LG
\\
  Existing off-policy reinforcement learning algorithms typically necessitate
an explicit state-action-value function representation, which becomes
problematic in high-dimensional action spaces. These algorithms often encounter
challenges where they struggle with the curse of dimensionality, as maintaining
a state-action-value function in such spaces becomes data-inefficient. In this
work, we propose a novel off-policy trust region optimization approach, called
Vlearn, that eliminates the requirement for an explicit state-action-value
function. Instead, we demonstrate how to efficiently leverage just a
state-value function as the critic, thus overcoming several limitations of
existing methods. By doing so, Vlearn addresses the computational challenges
posed by high-dimensional action spaces. Furthermore, Vlearn introduces an
efficient approach to address the challenges associated with pure state-value
function learning in the off-policy setting. This approach not only simplifies
the implementation of off-policy policy gradient algorithms but also leads to
consistent and robust performance across various benchmark tasks. Specifically,
by removing the need for a state-action-value function Vlearn simplifies the
learning process and allows for more efficient exploration and exploitation in
complex environments
\\ ( https://arxiv.org/abs/2403.04453 ,  377kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04468
Date: Thu, 7 Mar 2024 13:10:37 GMT   (11722kb,D)

Title: A Survey of Graph Neural Networks in Real world: Imbalance, Noise,
  Privacy and OOD Challenges
Authors: Wei Ju, Siyu Yi, Yifan Wang, Zhiping Xiao, Zhengyang Mao, Hourun Li,
  Yiyang Gu, Yifang Qin, Nan Yin, Senzhang Wang, Xinwang Liu, Xiao Luo, Philip
  S. Yu, Ming Zhang
Categories: cs.LG cs.AI cs.IR cs.SI
\\
  Graph-structured data exhibits universality and widespread applicability
across diverse domains, such as social network analysis, biochemistry,
financial fraud detection, and network security. Significant strides have been
made in leveraging Graph Neural Networks (GNNs) to achieve remarkable success
in these areas. However, in real-world scenarios, the training environment for
models is often far from ideal, leading to substantial performance degradation
of GNN models due to various unfavorable factors, including imbalance in data
distribution, the presence of noise in erroneous data, privacy protection of
sensitive information, and generalization capability for out-of-distribution
(OOD) scenarios. To tackle these issues, substantial efforts have been devoted
to improving the performance of GNN models in practical real-world scenarios,
as well as enhancing their reliability and robustness. In this paper, we
present a comprehensive survey that systematically reviews existing GNN models,
focusing on solutions to the four mentioned real-world challenges including
imbalance, noise, privacy, and OOD in practical scenarios that many existing
reviews have not considered. Specifically, we first highlight the four key
challenges faced by existing GNNs, paving the way for our exploration of
real-world GNN models. Subsequently, we provide detailed discussions on these
four aspects, dissecting how these solutions contribute to enhancing the
reliability and robustness of GNN models. Last but not least, we outline
promising directions and offer future perspectives in the field.
\\ ( https://arxiv.org/abs/2403.04468 ,  11722kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04477
Date: Thu, 7 Mar 2024 13:22:25 GMT   (625kb,D)

Title: Hyperparameter Tuning MLPs for Probabilistic Time Series Forecasting
Authors: Kiran Madhusudhanan, Shayan Jawed, Lars Schmidt-Thieme
Categories: cs.LG
Comments: 14 pages, 5 figures, Accepted at PAKDD24
\\
  Time series forecasting attempts to predict future events by analyzing past
trends and patterns. Although well researched, certain critical aspects
pertaining to the use of deep learning in time series forecasting remain
ambiguous. Our research primarily focuses on examining the impact of specific
hyperparameters related to time series, such as context length and validation
strategy, on the performance of the state-of-the-art MLP model in time series
forecasting. We have conducted a comprehensive series of experiments involving
4800 configurations per dataset across 20 time series forecasting datasets, and
our findings demonstrate the importance of tuning these parameters.
Furthermore, in this work, we introduce the largest metadataset for timeseries
forecasting to date, named TSBench, comprising 97200 evaluations, which is a
twentyfold increase compared to previous works in the field. Finally, we
demonstrate the utility of the created metadataset on multi-fidelity
hyperparameter optimization tasks.
\\ ( https://arxiv.org/abs/2403.04477 ,  625kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04482
Date: Thu, 7 Mar 2024 13:33:30 GMT   (3789kb,D)

Title: On the Topology Awareness and Generalization Performance of Graph Neural
  Networks
Authors: Junwei Su, Chuan Wu
Categories: cs.LG
\\
  Many computer vision and machine learning problems are modelled as learning
tasks on graphs, where graph neural networks (GNNs) have emerged as a dominant
tool for learning representations of graph-structured data. A key feature of
GNNs is their use of graph structures as input, enabling them to exploit the
graphs' inherent topological properties-known as the topology awareness of
GNNs. Despite the empirical successes of GNNs, the influence of topology
awareness on generalization performance remains unexplored, particularly for
node-level tasks that diverge from the assumption of data being independent and
identically distributed (I.I.D.). The precise definition and characterization
of the topology awareness of GNNs, especially concerning different topological
features, are still unclear. This paper introduces a comprehensive framework to
characterize the topology awareness of GNNs across any topological feature.
Using this framework, we investigate the effects of topology awareness on GNN
generalization performance. Contrary to the prevailing belief that enhancing
the topology awareness of GNNs is always advantageous, our analysis reveals a
critical insight: improving the topology awareness of GNNs may inadvertently
lead to unfair generalization across structural groups, which might not be
desired in some scenarios. Additionally, we conduct a case study using the
intrinsic graph metric, the shortest path distance, on various benchmark
datasets. The empirical results of this case study confirm our theoretical
insights. Moreover, we demonstrate the practical applicability of our framework
by using it to tackle the cold start problem in graph active learning.
\\ ( https://arxiv.org/abs/2403.04482 ,  3789kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04493
Date: Thu, 7 Mar 2024 13:49:43 GMT   (68kb)

Title: What makes an image realistic?
Authors: Lucas Theis
Categories: cs.LG stat.ML
\\
  The last decade has seen tremendous progress in our ability to generate
realistic-looking data, be it images, text, audio, or video. Here, we discuss
the closely related problem of quantifying realism, that is, designing
functions that can reliably tell realistic data from unrealistic data. This
problem turns out to be significantly harder to solve and remains poorly
understood, despite its prevalence in machine learning and recent breakthroughs
in generative AI. Drawing on insights from algorithmic information theory, we
discuss why this problem is challenging, why a good generative model alone is
insufficient to solve it, and what a good solution would look like. In
particular, we introduce the notion of a universal critic, which unlike
adversarial critics does not require adversarial training. While universal
critics are not immediately practical, they can serve both as a North Star for
guiding practical implementations and as a tool for analyzing existing attempts
to capture realism.
\\ ( https://arxiv.org/abs/2403.04493 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04526
Date: Thu, 7 Mar 2024 14:27:08 GMT   (23111kb,D)

Title: Hyperspectral unmixing for Raman spectroscopy via physics-constrained
  autoencoders
Authors: Dimitar Georgiev, \'Alvaro Fern\'andez-Galiana, Simon Vilms Pedersen,
  Georgios Papadopoulos, Ruoxiao Xie, Molly M. Stevens, Mauricio Barahona
Categories: cs.LG cs.AI cs.CV
\\
  Raman spectroscopy is widely used across scientific domains to characterize
the chemical composition of samples in a non-destructive, label-free manner.
Many applications entail the unmixing of signals from mixtures of molecular
species to identify the individual components present and their proportions,
yet conventional methods for chemometrics often struggle with complex mixture
scenarios encountered in practice. Here, we develop hyperspectral unmixing
algorithms based on autoencoder neural networks, and we systematically validate
them using both synthetic and experimental benchmark datasets created in-house.
Our results demonstrate that unmixing autoencoders provide improved accuracy,
robustness and efficiency compared to standard unmixing methods. We also
showcase the applicability of autoencoders to complex biological settings by
showing improved biochemical characterization of volumetric Raman imaging data
from a monocytic cell.
\\ ( https://arxiv.org/abs/2403.04526 ,  23111kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04529
Date: Thu, 7 Mar 2024 14:28:04 GMT   (1557kb,D)

Title: Enhancing Data Quality in Federated Fine-Tuning of Foundation Models
Authors: Wanru Zhao, Yaxin Du, Nicholas Donald Lane, Siheng Chen, Yanfeng Wang
Categories: cs.LG cs.AI cs.DC
Comments: Accepted at ICLR 2024 Workshop on Navigating and Addressing Data
  Problems for Foundation Models (DPFM)
\\
  In the current landscape of foundation model training, there is a significant
reliance on public domain data, which is nearing exhaustion according to recent
research. To further scale up, it is crucial to incorporate collaboration among
multiple specialized and high-quality private domain data sources. However, the
challenge of training models locally without sharing private data presents
numerous obstacles in data quality control. To tackle this issue, we propose a
data quality control pipeline for federated fine-tuning of foundation models.
This pipeline computes scores reflecting the quality of training data and
determines a global threshold for a unified standard, aiming for improved
global performance. Our experiments show that the proposed quality control
pipeline facilitates the effectiveness and reliability of the model training,
leading to better performance.
\\ ( https://arxiv.org/abs/2403.04529 ,  1557kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04545
Date: Thu, 7 Mar 2024 14:40:53 GMT   (174kb,D)

Title: Improve Generalization Ability of Deep Wide Residual Network with A
  Suitable Scaling Factor
Authors: Songtao Tian, Zixiong Yu
Categories: cs.LG math.ST stat.TH
\\
  Deep Residual Neural Networks (ResNets) have demonstrated remarkable success
across a wide range of real-world applications. In this paper, we identify a
suitable scaling factor (denoted by $\alpha$) on the residual branch of deep
wide ResNets to achieve good generalization ability. We show that if $\alpha$
is a constant, the class of functions induced by Residual Neural Tangent Kernel
(RNTK) is asymptotically not learnable, as the depth goes to infinity. We also
highlight a surprising phenomenon: even if we allow $\alpha$ to decrease with
increasing depth $L$, the degeneration phenomenon may still occur. However,
when $\alpha$ decreases rapidly with $L$, the kernel regression with deep RNTK
with early stopping can achieve the minimax rate provided that the target
regression function falls in the reproducing kernel Hilbert space associated
with the infinite-depth RNTK. Our simulation studies on synthetic data and real
classification tasks such as MNIST, CIFAR10 and CIFAR100 support our
theoretical criteria for choosing $\alpha$.
\\ ( https://arxiv.org/abs/2403.04545 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04546
Date: Thu, 7 Mar 2024 14:42:33 GMT   (397kb,D)

Title: Architectural Blueprint For Heterogeneity-Resilient Federated Learning
Authors: Satwat Bashir, Tasos Dagiuklas, Kasra Kassai, Muddesar Iqbal
Categories: cs.LG cs.DC cs.NI
\\
  This paper proposes a novel three tier architecture for federated learning to
optimize edge computing environments. The proposed architecture addresses the
challenges associated with client data heterogeneity and computational
constraints. It introduces a scalable, privacy preserving framework that
enhances the efficiency of distributed machine learning. Through
experimentation, the paper demonstrates the architecture capability to manage
non IID data sets more effectively than traditional federated learning models.
Additionally, the paper highlights the potential of this innovative approach to
significantly improve model accuracy, reduce communication overhead, and
facilitate broader adoption of federated learning technologies.
\\ ( https://arxiv.org/abs/2403.04546 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04547
Date: Thu, 7 Mar 2024 14:43:17 GMT   (8643kb,D)

Title: CLIP the Bias: How Useful is Balancing Data in Multimodal Learning?
Authors: Ibrahim Alabdulmohsin, Xiao Wang, Andreas Steiner, Priya Goyal,
  Alexander D'Amour, Xiaohua Zhai
Categories: cs.LG cs.AI
Comments: 32 pages, 20 figures, 7 tables
Journal-ref: ICLR 2024
\\
  We study the effectiveness of data-balancing for mitigating biases in
contrastive language-image pretraining (CLIP), identifying areas of strength
and limitation. First, we reaffirm prior conclusions that CLIP models can
inadvertently absorb societal stereotypes. To counter this, we present a novel
algorithm, called Multi-Modal Moment Matching (M4), designed to reduce both
representation and association biases (i.e. in first- and second-order
statistics) in multimodal data. We use M4 to conduct an in-depth analysis
taking into account various factors, such as the model, representation, and
data size. Our study also explores the dynamic nature of how CLIP learns and
unlearns biases. In particular, we find that fine-tuning is effective in
countering representation biases, though its impact diminishes for association
biases. Also, data balancing has a mixed impact on quality: it tends to improve
classification but can hurt retrieval. Interestingly, data and architectural
improvements seem to mitigate the negative impact of data balancing on
performance; e.g. applying M4 to SigLIP-B/16 with data quality filters improves
COCO image-to-text retrieval @5 from 86% (without data balancing) to 87% and
ImageNet 0-shot classification from 77% to 77.5%! Finally, we conclude with
recommendations for improving the efficacy of data balancing in multimodal
systems.
\\ ( https://arxiv.org/abs/2403.04547 ,  8643kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04551
Date: Thu, 7 Mar 2024 14:45:03 GMT   (16883kb,D)

Title: Dissecting Sample Hardness: A Fine-Grained Analysis of Hardness
  Characterization Methods for Data-Centric AI
Authors: Nabeel Seedat, Fergus Imrie, Mihaela van der Schaar
Categories: cs.LG
Comments: Published at International Conference on Learning Representations
  (ICLR) 2024
\\
  Characterizing samples that are difficult to learn from is crucial to
developing highly performant ML models. This has led to numerous Hardness
Characterization Methods (HCMs) that aim to identify "hard" samples. However,
there is a lack of consensus regarding the definition and evaluation of
"hardness". Unfortunately, current HCMs have only been evaluated on specific
types of hardness and often only qualitatively or with respect to downstream
performance, overlooking the fundamental quantitative identification task. We
address this gap by presenting a fine-grained taxonomy of hardness types.
Additionally, we propose the Hardness Characterization Analysis Toolkit
(H-CAT), which supports comprehensive and quantitative benchmarking of HCMs
across the hardness taxonomy and can easily be extended to new HCMs, hardness
types, and datasets. We use H-CAT to evaluate 13 different HCMs across 8
hardness types. This comprehensive evaluation encompassing over 14K setups
uncovers strengths and weaknesses of different HCMs, leading to practical tips
to guide HCM selection and future development. Our findings highlight the need
for more comprehensive HCM evaluation, while we hope our hardness taxonomy and
toolkit will advance the principled evaluation and uptake of data-centric AI
methods.
\\ ( https://arxiv.org/abs/2403.04551 ,  16883kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04558
Date: Thu, 7 Mar 2024 14:56:06 GMT   (1265kb,D)

Title: Reducing self-supervised learning complexity improves weakly-supervised
  classification performance in computational pathology
Authors: Tim Lenz, Omar S. M. El Nahhas, Marta Ligero, Jakob Nikolas Kather
Categories: cs.LG cs.AI cs.CV
Comments: Submitted to MICCAI 2024
\\
  Deep Learning models have been successfully utilized to extract clinically
actionable insights from routinely available histology data. Generally, these
models require annotations performed by clinicians, which are scarce and costly
to generate. The emergence of self-supervised learning (SSL) methods remove
this barrier, allowing for large-scale analyses on non-annotated data. However,
recent SSL approaches apply increasingly expansive model architectures and
larger datasets, causing the rapid escalation of data volumes, hardware
prerequisites, and overall expenses, limiting access to these resources to few
institutions. Therefore, we investigated the complexity of contrastive SSL in
computational pathology in relation to classification performance with the
utilization of consumer-grade hardware. Specifically, we analyzed the effects
of adaptations in data volume, architecture, and algorithms on downstream clas-
sification tasks, emphasizing their impact on computational resources. We
trained breast cancer foundation models on a large public patient cohort and
validated them on various downstream classification tasks in a weakly
supervised manner on two external public patient cohorts. Our experiments
demonstrate that we can improve downstream classification performance whilst
reducing SSL training duration by 90%. In summary, we propose a set of
adaptations which enable the utilization of SSL in computational pathology in
non-resource abundant environments.
\\ ( https://arxiv.org/abs/2403.04558 ,  1265kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04568
Date: Thu, 7 Mar 2024 15:03:50 GMT   (42kb,D)

Title: Improved Algorithm for Adversarial Linear Mixture MDPs with Bandit
  Feedback and Unknown Transition
Authors: Long-Fei Li, Peng Zhao, Zhi-Hua Zhou
Categories: cs.LG stat.ML
Comments: AISTATS 2024
\\
  We study reinforcement learning with linear function approximation, unknown
transition, and adversarial losses in the bandit feedback setting.
Specifically, we focus on linear mixture MDPs whose transition kernel is a
linear mixture model. We propose a new algorithm that attains an
$\widetilde{O}(d\sqrt{HS^3K} + \sqrt{HSAK})$ regret with high probability,
where $d$ is the dimension of feature mappings, $S$ is the size of state space,
$A$ is the size of action space, $H$ is the episode length and $K$ is the
number of episodes. Our result strictly improves the previous best-known
$\widetilde{O}(dS^2 \sqrt{K} + \sqrt{HSAK})$ result in Zhao et al. (2023a)
since $H \leq S$ holds by the layered MDP structure. Our advancements are
primarily attributed to (i) a new least square estimator for the transition
parameter that leverages the visit information of all states, as opposed to
only one state in prior work, and (ii) a new self-normalized concentration
tailored specifically to handle non-independent noises, originally proposed in
the dynamic assortment area and firstly applied in reinforcement learning to
handle correlations between different states.
\\ ( https://arxiv.org/abs/2403.04568 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04580
Date: Thu, 7 Mar 2024 15:26:23 GMT   (4224kb)

Title: Beyond Major Product Prediction: Reproducing Reaction Mechanisms with
  Machine Learning Models Trained on a Large-Scale Mechanistic Dataset
Authors: Joonyoung F. Joung, Mun Hong Fong, Jihye Roh, Zhengkai Tu, John
  Bradshaw, Connor W. Coley
Categories: cs.LG
Comments: 105 pages, 9 figures
\\
  Mechanistic understanding of organic reactions can facilitate reaction
development, impurity prediction, and in principle, reaction discovery. While
several machine learning models have sought to address the task of predicting
reaction products, their extension to predicting reaction mechanisms has been
impeded by the lack of a corresponding mechanistic dataset. In this study, we
construct such a dataset by imputing intermediates between experimentally
reported reactants and products using expert reaction templates and train
several machine learning models on the resulting dataset of 5,184,184
elementary steps. We explore the performance and capabilities of these models,
focusing on their ability to predict reaction pathways and recapitulate the
roles of catalysts and reagents. Additionally, we demonstrate the potential of
mechanistic models in predicting impurities, often overlooked by conventional
models. We conclude by evaluating the generalizability of mechanistic models to
new reaction types, revealing challenges related to dataset diversity,
consecutive predictions, and violations of atom conservation.
\\ ( https://arxiv.org/abs/2403.04580 ,  4224kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04599
Date: Thu, 7 Mar 2024 15:47:52 GMT   (9474kb,D)

Title: Contrastive Continual Learning with Importance Sampling and
  Prototype-Instance Relation Distillation
Authors: Jiyong Li, Dilshod Azizov, Yang Li, Shangsong Liang
Categories: cs.LG
Comments: Accepted by AAAI 2024
\\
  Recently, because of the high-quality representations of contrastive learning
methods, rehearsal-based contrastive continual learning has been proposed to
explore how to continually learn transferable representation embeddings to
avoid the catastrophic forgetting issue in traditional continual settings.
Based on this framework, we propose Contrastive Continual Learning via
Importance Sampling (CCLIS) to preserve knowledge by recovering previous data
distributions with a new strategy for Replay Buffer Selection (RBS), which
minimize estimated variance to save hard negative samples for representation
learning with high quality. Furthermore, we present the Prototype-instance
Relation Distillation (PRD) loss, a technique designed to maintain the
relationship between prototypes and sample representations using a
self-distillation process. Experiments on standard continual learning
benchmarks reveal that our method notably outperforms existing baselines in
terms of knowledge preservation and thereby effectively counteracts
catastrophic forgetting in online contexts. The code is available at
https://github.com/lijy373/CCLIS.
\\ ( https://arxiv.org/abs/2403.04599 ,  9474kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04605
Date: Thu, 7 Mar 2024 15:54:46 GMT   (367kb,D)

Title: In-n-Out: Calibrating Graph Neural Networks for Link Prediction
Authors: Erik Nascimento, Diego Mesquita, Samuel Kaskio, Amauri H Souza
Categories: cs.LG
Comments: 18 pages, 4 figures, 8 tables
\\
  Deep neural networks are notoriously miscalibrated, i.e., their outputs do
not reflect the true probability of the event we aim to predict. While networks
for tabular or image data are usually overconfident, recent works have shown
that graph neural networks (GNNs) show the opposite behavior for node-level
classification. But what happens when we are predicting links? We show that, in
this case, GNNs often exhibit a mixed behavior. More specifically, they may be
overconfident in negative predictions while being underconfident in positive
ones. Based on this observation, we propose IN-N-OUT, the first-ever method to
calibrate GNNs for link prediction. IN-N-OUT is based on two simple intuitions:
i) attributing true/false labels to an edge while respecting a GNNs prediction
should cause but small fluctuations in that edge's embedding; and, conversely,
ii) if we label that same edge contradicting our GNN, embeddings should change
more substantially. An extensive experimental campaign shows that IN-N-OUT
significantly improves the calibration of GNNs in link prediction, consistently
outperforming the baselines available -- which are not designed for this
specific task.
\\ ( https://arxiv.org/abs/2403.04605 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04629
Date: Thu, 7 Mar 2024 16:13:32 GMT   (2583kb,D)

Title: Explaining Bayesian Optimization by Shapley Values Facilitates Human-AI
  Collaboration
Authors: Julian Rodemann, Federico Croppi, Philipp Arens, Yusuf Sale, Julia
  Herbinger, Bernd Bischl, Eyke H\"ullermeier, Thomas Augustin, Conor J. Walsh,
  Giuseppe Casalicchio
Categories: cs.LG cs.AI cs.HC cs.RO stat.ML
Comments: Preprint. Copyright by the authors; 19 pages, 24 figures
ACM-class: I.2.6; I.2.9; F.2.2; J.6
\\
  Bayesian optimization (BO) with Gaussian processes (GP) has become an
indispensable algorithm for black box optimization problems. Not without a dash
of irony, BO is often considered a black box itself, lacking ways to provide
reasons as to why certain parameters are proposed to be evaluated. This is
particularly relevant in human-in-the-loop applications of BO, such as in
robotics. We address this issue by proposing ShapleyBO, a framework for
interpreting BO's proposals by game-theoretic Shapley values.They quantify each
parameter's contribution to BO's acquisition function. Exploiting the linearity
of Shapley values, we are further able to identify how strongly each parameter
drives BO's exploration and exploitation for additive acquisition functions
like the confidence bound. We also show that ShapleyBO can disentangle the
contributions to exploration into those that explore aleatoric and epistemic
uncertainty. Moreover, our method gives rise to a ShapleyBO-assisted human
machine interface (HMI), allowing users to interfere with BO in case proposals
do not align with human reasoning. We demonstrate this HMI's benefits for the
use case of personalizing wearable robotic devices (assistive back exosuits) by
human-in-the-loop BO. Results suggest human-BO teams with access to ShapleyBO
can achieve lower regret than teams without.
\\ ( https://arxiv.org/abs/2403.04629 ,  2583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04636
Date: Thu, 7 Mar 2024 16:21:09 GMT   (255kb,D)

Title: Entropy Aware Message Passing in Graph Neural Networks
Authors: Philipp Nazari, Oliver Lemke, Davide Guidobene, Artiom Gesp
Categories: cs.LG
Comments: 4 pages, 3 figures
ACM-class: I.2.6; I.5.1
\\
  Deep Graph Neural Networks struggle with oversmoothing. This paper introduces
a novel, physics-inspired GNN model designed to mitigate this issue. Our
approach integrates with existing GNN architectures, introducing an
entropy-aware message passing term. This term performs gradient ascent on the
entropy during node aggregation, thereby preserving a certain degree of entropy
in the embeddings. We conduct a comparative analysis of our model against
state-of-the-art GNNs across various common datasets.
\\ ( https://arxiv.org/abs/2403.04636 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04642
Date: Thu, 7 Mar 2024 16:36:29 GMT   (3796kb,D)

Title: Teaching Large Language Models to Reason with Reinforcement Learning
Authors: Alex Havrilla, Yuqing Du, Sharath Chandra Raparthy, Christoforos
  Nalmpantis, Jane Dwivedi-Yu, Maksym Zhuravinskyi, Eric Hambro, Sainbayar
  Sukhbaatar, Roberta Raileanu
Categories: cs.LG
\\
  Reinforcement Learning from Human Feedback (\textbf{RLHF}) has emerged as a
dominant approach for aligning LLM outputs with human preferences. Inspired by
the success of RLHF, we study the performance of multiple algorithms that learn
from feedback (Expert Iteration, Proximal Policy Optimization (\textbf{PPO}),
Return-Conditioned RL) on improving LLM reasoning capabilities. We investigate
both sparse and dense rewards provided to the LLM both heuristically and via a
learned reward model. We additionally start from multiple model sizes and
initializations both with and without supervised fine-tuning (\textbf{SFT})
data. Overall, we find all algorithms perform comparably, with Expert Iteration
performing best in most cases. Surprisingly, we find the sample complexity of
Expert Iteration is similar to that of PPO, requiring at most on the order of
$10^6$ samples to converge from a pretrained checkpoint. We investigate why
this is the case, concluding that during RL training models fail to explore
significantly beyond solutions already produced by SFT models. Additionally, we
discuss a trade off between maj@1 and pass@96 metric performance during SFT
training and how conversely RL training improves both simultaneously. We then
conclude by discussing the implications of our findings for RLHF and the future
role of RL in LLM fine-tuning.
\\ ( https://arxiv.org/abs/2403.04642 ,  3796kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04650
Date: Thu, 7 Mar 2024 16:50:25 GMT   (1567kb,D)

Title: Context-Based Multimodal Fusion
Authors: Bilal Faye, Hanane Azzag, Mustapha Lebbah, Djamel Bouchaffra
Categories: cs.LG cs.AI
\\
  The fusion models, which effectively combine information from different
sources, are widely used in solving multimodal tasks. However, they have
significant limitations related to aligning data distributions across different
modalities. This challenge can lead to inconsistencies and difficulties in
learning robust representations. Alignment models, while specifically
addressing this issue, often require training "from scratch" with large
datasets to achieve optimal results, which can be costly in terms of resources
and time. To overcome these limitations, we propose an innovative model called
Context-Based Multimodal Fusion (CBMF), which combines both modality fusion and
data distribution alignment. In CBMF, each modality is represented by a
specific context vector, fused with the embedding of each modality. This
enables the use of large pre-trained models that can be frozen, reducing the
computational and training data requirements. Additionally, the network learns
to differentiate embeddings of different modalities through fusion with context
and aligns data distributions using a contrastive approach for self-supervised
learning. Thus, CBMF offers an effective and economical solution for solving
complex multimodal tasks.
\\ ( https://arxiv.org/abs/2403.04650 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04670
Date: Thu, 7 Mar 2024 17:16:59 GMT   (345kb,D)

Title: End-to-end Conditional Robust Optimization
Authors: Abhilash Chenreddy and Erick Delage
Categories: cs.LG
\\
  The field of Contextual Optimization (CO) integrates machine learning and
optimization to solve decision making problems under uncertainty. Recently, a
risk sensitive variant of CO, known as Conditional Robust Optimization (CRO),
combines uncertainty quantification with robust optimization in order to
promote safety and reliability in high stake applications. Exploiting modern
differentiable optimization methods, we propose a novel end-to-end approach to
train a CRO model in a way that accounts for both the empirical risk of the
prescribed decisions and the quality of conditional coverage of the contextual
uncertainty set that supports them. While guarantees of success for the latter
objective are impossible to obtain from the point of view of conformal
prediction theory, high quality conditional coverage is achieved empirically by
ingeniously employing a logistic regression differentiable layer within the
calculation of coverage quality in our training loss. We show that the proposed
training algorithms produce decisions that outperform the traditional estimate
then optimize approaches.
\\ ( https://arxiv.org/abs/2403.04670 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04693
Date: Thu, 7 Mar 2024 17:42:40 GMT   (99kb,D)

Title: Analysis of Systems' Performance in Natural Language Processing
  Competitions
Authors: Sergio Nava-Mu\~noz and Mario Graff and Hugo Jair Escalante
Categories: cs.LG
\\
  Collaborative competitions have gained popularity in the scientific and
technological fields. These competitions involve defining tasks, selecting
evaluation scores, and devising result verification methods. In the standard
scenario, participants receive a training set and are expected to provide a
solution for a held-out dataset kept by organizers. An essential challenge for
organizers arises when comparing algorithms' performance, assessing multiple
participants, and ranking them. Statistical tools are often used for this
purpose; however, traditional statistical methods often fail to capture
decisive differences between systems' performance. This manuscript describes an
evaluation methodology for statistically analyzing competition results and
competition. The methodology is designed to be universally applicable; however,
it is illustrated using eight natural language competitions as case studies
involving classification and regression problems. The proposed methodology
offers several advantages, including off-the-shell comparisons with correction
mechanisms and the inclusion of confidence intervals. Furthermore, we introduce
metrics that allow organizers to assess the difficulty of competitions. Our
analysis shows the potential usefulness of our methodology for effectively
evaluating competition results.
\\ ( https://arxiv.org/abs/2403.04693 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04720
Date: Thu, 7 Mar 2024 18:16:29 GMT   (944kb,D)

Title: Rethinking of Encoder-based Warm-start Methods in Hyperparameter
  Optimization
Authors: Dawid P{\l}udowski, Antoni Zajko, Anna Kozak, Katarzyna Wo\'znica
Categories: cs.LG
\\
  Effectively representing heterogeneous tabular datasets for meta-learning
remains an open problem. Previous approaches rely on predefined meta-features,
for example, statistical measures or landmarkers. Encoder-based models, such as
Dataset2Vec, allow us to extract significant meta-features automatically
without human intervention. This research introduces a novel encoder-based
representation of tabular datasets implemented within the liltab package
available on GitHub https://github.com/azoz01/liltab. Our package is based on
an established model for heterogeneous tabular data proposed in [Iwata and
Kumagai, 2020]. The proposed approach employs a different model for encoding
feature relationships, generating alternative representations compared to
existing methods like Dataset2Vec. Both of them leverage the fundamental
assumption of dataset similarity learning. In this work, we evaluate
Dataset2Vec and liltab on two common meta-tasks - representing entire datasets
and hyperparameter optimization warm-start. However, validation on an
independent metaMIMIC dataset highlights the nuanced challenges in
representation learning. We show that general representations may not suffice
for some meta-tasks where requirements are not explicitly considered during
extraction.
  [Iwata and Kumagai, 2020] Tomoharu Iwata and Atsutoshi Kumagai. Meta-learning
from Tasks with Heterogeneous Attribute Spaces. In Advances in Neural
Information Processing Systems, 2020.
\\ ( https://arxiv.org/abs/2403.04720 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04744
Date: Thu, 7 Mar 2024 18:49:32 GMT   (32kb)

Title: SQ Lower Bounds for Non-Gaussian Component Analysis with Weaker
  Assumptions
Authors: Ilias Diakonikolas, Daniel Kane, Lisheng Ren and Yuxin Sun
Categories: cs.LG cs.DS math.ST stat.ML stat.TH
Comments: Conference version published in NeurIPS 2023
\\
  We study the complexity of Non-Gaussian Component Analysis (NGCA) in the
Statistical Query (SQ) model. Prior work developed a general methodology to
prove SQ lower bounds for this task that have been applicable to a wide range
of contexts. In particular, it was known that for any univariate distribution
$A$ satisfying certain conditions, distinguishing between a standard
multivariate Gaussian and a distribution that behaves like $A$ in a random
hidden direction and like a standard Gaussian in the orthogonal complement, is
SQ-hard. The required conditions were that (1) $A$ matches many low-order
moments with the standard univariate Gaussian, and (2) the chi-squared norm of
$A$ with respect to the standard Gaussian is finite. While the moment-matching
condition is necessary for hardness, the chi-squared condition was only
required for technical reasons. In this work, we establish that the latter
condition is indeed not necessary. In particular, we prove near-optimal SQ
lower bounds for NGCA under the moment-matching condition only. Our result
naturally generalizes to the setting of a hidden subspace. Leveraging our
general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of
concrete estimation tasks where existing techniques provide sub-optimal or even
vacuous guarantees.
\\ ( https://arxiv.org/abs/2403.04744 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04747
Date: Thu, 7 Mar 2024 18:52:27 GMT   (182kb,D)

Title: GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural
  Networks
Authors: Lisa Schneckenreiter, Richard Freinschlag, Florian Sestak, Johannes
  Brandstetter, G\"unter Klambauer, Andreas Mayr
Categories: cs.LG cs.AI stat.ML
Comments: Accepted at ICLR 2024 (Tiny Papers Track)
\\
  Graph neural networks (GNNs), and especially message-passing neural networks,
excel in various domains such as physics, drug discovery, and molecular
modeling. The expressivity of GNNs with respect to their ability to
discriminate non-isomorphic graphs critically depends on the functions employed
for message aggregation and graph-level readout. By applying signal propagation
theory, we propose a variance-preserving aggregation function (VPA) that
maintains expressivity, but yields improved forward and backward dynamics.
Experiments demonstrate that VPA leads to increased predictive performance for
popular GNN architectures as well as improved learning dynamics. Our results
could pave the way towards normalizer-free or self-normalizing GNNs.
\\ ( https://arxiv.org/abs/2403.04747 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04759
Date: Thu, 7 Mar 2024 18:56:33 GMT   (5724kb,D)

Title: Lifelong Intelligence Beyond the Edge using Hyperdimensional Computing
Authors: Xiaofan Yu, Anthony Thomas, Ivannia Gomez Moreno, Louis Gutierrez,
  Tajana Rosing
Categories: cs.LG cs.NE
Comments: Accepted by IPSN'24
\\
  On-device learning has emerged as a prevailing trend that avoids the slow
response time and costly communication of cloud-based learning. The ability to
learn continuously and indefinitely in a changing environment, and with
resource constraints, is critical for real sensor deployments. However,
existing designs are inadequate for practical scenarios with (i) streaming data
input, (ii) lack of supervision and (iii) limited on-board resources. In this
paper, we design and deploy the first on-device lifelong learning system called
LifeHD for general IoT applications with limited supervision. LifeHD is
designed based on a novel neurally-inspired and lightweight learning paradigm
called Hyperdimensional Computing (HDC). We utilize a two-tier associative
memory organization to intelligently store and manage high-dimensional,
low-precision vectors, which represent the historical patterns as cluster
centroids. We additionally propose two variants of LifeHD to cope with scarce
labeled inputs and power constraints. We implement LifeHD on off-the-shelf edge
platforms and perform extensive evaluations across three scenarios. Our
measurements show that LifeHD improves the unsupervised clustering accuracy by
up to 74.8% compared to the state-of-the-art NN-based unsupervised lifelong
learning baselines with as much as 34.3x better energy efficiency. Our code is
available at https://github.com/Orienfish/LifeHD.
\\ ( https://arxiv.org/abs/2403.04759 ,  5724kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04763
Date: Thu, 7 Mar 2024 18:57:46 GMT   (485kb,D)

Title: BloomGML: Graph Machine Learning through the Lens of Bilevel
  Optimization
Authors: Amber Yijia Zheng and Tong He and Yixuan Qiu and Minjie Wang and David
  Wipf
Categories: cs.LG
Comments: Publication at AISTATS 2024
\\
  Bilevel optimization refers to scenarios whereby the optimal solution of a
lower-level energy function serves as input features to an upper-level
objective of interest. These optimal features typically depend on tunable
parameters of the lower-level energy in such a way that the entire bilevel
pipeline can be trained end-to-end. Although not generally presented as such,
this paper demonstrates how a variety of graph learning techniques can be
recast as special cases of bilevel optimization or simplifications thereof. In
brief, building on prior work we first derive a more flexible class of energy
functions that, when paired with various descent steps (e.g., gradient descent,
proximal methods, momentum, etc.), form graph neural network (GNN)
message-passing layers; critically, we also carefully unpack where any residual
approximation error lies with respect to the underlying constituent
message-passing functions. We then probe several simplifications of this
framework to derive close connections with non-GNN-based graph learning
approaches, including knowledge graph embeddings, various forms of label
propagation, and efficient graph-regularized MLP models. And finally, we
present supporting empirical results that demonstrate the versatility of the
proposed bilevel lens, which we refer to as BloomGML, referencing that BiLevel
Optimization Offers More Graph Machine Learning. Our code is available at
https://github.com/amberyzheng/BloomGML. Let graph ML bloom.
\\ ( https://arxiv.org/abs/2403.04763 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04764
Date: Thu, 7 Mar 2024 18:58:26 GMT   (677kb,D)

Title: Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a
  provably efficient algorithm for batch Bayesian Optimization
Authors: Zhaolin Ren and Na Li
Categories: cs.LG math.OC stat.ML
\\
  This paper presents a new approach for batch Bayesian Optimization (BO),
where the sampling takes place by minimizing a Thompson Sampling approximation
of a regret to uncertainty ratio. Our objective is able to coordinate the
actions chosen in each batch in a way that minimizes redundancy between points
whilst focusing on points with high predictive means or high uncertainty. We
provide high-probability theoretical guarantees on the regret of our algorithm.
Finally, numerically, we demonstrate that our method attains state-of-the-art
performance on a range of nonconvex test functions, where it outperforms
several competitive benchmark batch BO algorithms by an order of magnitude on
average.
\\ ( https://arxiv.org/abs/2403.04764 ,  677kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.03962 (*cross-listing*)
Date: Fri, 1 Mar 2024 14:23:26 GMT   (223kb,D)

Title: Identify Critical Nodes in Complex Network with Large Language Models
Authors: Jinzhu Mao, Dongyun Zou, Li Sheng, Siyi Liu, Chen Gao, Yue Wang, Yong
  Li
Categories: cs.SI cs.AI cs.NE
\\
  Identifying critical nodes in networks is a classical decision-making task,
and many methods struggle to strike a balance between adaptability and utility.
Therefore, we propose an approach that empowers Evolutionary Algorithm (EA)
with Large Language Models (LLMs), to generate a function called "score\_nodes"
which can further be used to identify crucial nodes based on their assigned
scores. Our model consists of three main components: Manual Initialization,
Population Management, and LLMs-based Evolution. It evolves from initial
populations with a set of designed node scoring functions created manually.
LLMs leverage their strong contextual understanding and rich programming skills
to perform crossover and mutation operations on the individuals, generating
excellent new functions. These functions are then categorized, ranked, and
eliminated to ensure the stable development of the populations while preserving
diversity. Extensive experiments demonstrate the excellent performance of our
method, showcasing its strong generalization ability compared to other
state-of-the-art algorithms. It can consistently and orderly generate diverse
and efficient node scoring functions. All source codes and models that can
reproduce all results in this work are publicly available at this link:
\url{https://anonymous.4open.science/r/LLM4CN-6520}
\\ ( https://arxiv.org/abs/2403.03962 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03993 (*cross-listing*)
Date: Wed, 6 Mar 2024 19:08:28 GMT   (485kb,D)

Title: Personalized Negative Reservoir for Incremental Learning in Recommender
  Systems
Authors: Antonios Valkanas, Yuening Wang, Yingxue Zhang, Mark Coates
Categories: cs.IR cs.AI
\\
  Recommender systems have become an integral part of online platforms. Every
day the volume of training data is expanding and the number of user
interactions is constantly increasing. The exploration of larger and more
expressive models has become a necessary pursuit to improve user experience.
However, this progression carries with it an increased computational burden. In
commercial settings, once a recommendation system model has been trained and
deployed it typically needs to be updated frequently as new client data arrive.
Cumulatively, the mounting volume of data is guaranteed to eventually make full
batch retraining of the model from scratch computationally infeasible. Naively
fine-tuning solely on the new data runs into the well-documented problem of
catastrophic forgetting. Despite the fact that negative sampling is a crucial
part of training with implicit feedback, no specialized technique exists that
is tailored to the incremental learning framework. In this work, we take the
first step to propose, a personalized negative reservoir strategy which is used
to obtain negative samples for the standard triplet loss. This technique
balances alleviation of forgetting with plasticity by encouraging the model to
remember stable user preferences and selectively forget when user interests
change. We derive the mathematical formulation of a negative sampler to
populate and update the reservoir. We integrate our design in three SOTA and
commonly used incremental recommendation models. We show that these concrete
realizations of our negative reservoir framework achieve state-of-the-art
results in standard benchmarks, on multiple standard top-k evaluation metrics.
\\ ( https://arxiv.org/abs/2403.03993 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04001 (*cross-listing*)
Date: Wed, 6 Mar 2024 19:17:49 GMT   (2534kb,D)

Title: Bidirectional Progressive Neural Networks with Episodic Return Progress
  for Emergent Task Sequencing and Robotic Skill Transfer
Authors: Suzan Ece Ada, Hanne Say, Emre Ugur, Erhan Oztop
Categories: cs.RO cs.AI cs.LG
Comments: 9 pages, 5 figures
\\
  Human brain and behavior provide a rich venue that can inspire novel control
and learning methods for robotics. In an attempt to exemplify such a
development by inspiring how humans acquire knowledge and transfer skills among
tasks, we introduce a novel multi-task reinforcement learning framework named
Episodic Return Progress with Bidirectional Progressive Neural Networks
(ERP-BPNN). The proposed ERP-BPNN model (1) learns in a human-like interleaved
manner by (2) autonomous task switching based on a novel intrinsic motivation
signal and, in contrast to existing methods, (3) allows bidirectional skill
transfer among tasks. ERP-BPNN is a general architecture applicable to several
multi-task learning settings; in this paper, we present the details of its
neural architecture and show its ability to enable effective learning and skill
transfer among morphologically different robots in a reaching task. The
developed Bidirectional Progressive Neural Network (BPNN) architecture enables
bidirectional skill transfer without requiring incremental training and
seamlessly integrates with online task arbitration. The task arbitration
mechanism developed is based on soft Episodic Return progress (ERP), a novel
intrinsic motivation (IM) signal. To evaluate our method, we use quantifiable
robotics metrics such as 'expected distance to goal' and 'path straightness' in
addition to the usual reward-based measure of episodic return common in
reinforcement learning. With simulation experiments, we show that ERP-BPNN
achieves faster cumulative convergence and improves performance in all metrics
considered among morphologically different robots compared to the baselines.
\\ ( https://arxiv.org/abs/2403.04001 ,  2534kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04014 (*cross-listing*)
Date: Wed, 6 Mar 2024 19:55:01 GMT   (23510kb,D)

Title: PromptCharm: Text-to-Image Generation through Multi-modal Prompting and
  Refinement
Authors: Zhijie Wang, Yuheng Huang, Da Song, Lei Ma, Tianyi Zhang
Categories: cs.HC cs.AI
Comments: To appear in the 2024 CHI Conference on Human Factors in Computing
  Systems (CHI '24), May 11--16, 2024, Honolulu, HI, USA
DOI: 10.1145/3613904.3642803
\\
  The recent advancements in Generative AI have significantly advanced the
field of text-to-image generation. The state-of-the-art text-to-image model,
Stable Diffusion, is now capable of synthesizing high-quality images with a
strong sense of aesthetics. Crafting text prompts that align with the model's
interpretation and the user's intent thus becomes crucial. However, prompting
remains challenging for novice users due to the complexity of the stable
diffusion model and the non-trivial efforts required for iteratively editing
and refining the text prompts. To address these challenges, we propose
PromptCharm, a mixed-initiative system that facilitates text-to-image creation
through multi-modal prompt engineering and refinement. To assist novice users
in prompting, PromptCharm first automatically refines and optimizes the user's
initial prompt. Furthermore, PromptCharm supports the user in exploring and
selecting different image styles within a large database. To assist users in
effectively refining their prompts and images, PromptCharm renders model
explanations by visualizing the model's attention values. If the user notices
any unsatisfactory areas in the generated images, they can further refine the
images through model attention adjustment or image inpainting within the rich
feedback loop of PromptCharm. To evaluate the effectiveness and usability of
PromptCharm, we conducted a controlled user study with 12 participants and an
exploratory user study with another 12 participants. These two studies show
that participants using PromptCharm were able to create images with higher
quality and better aligned with the user's expectations compared with using two
variants of PromptCharm that lacked interaction or visualization support.
\\ ( https://arxiv.org/abs/2403.04014 ,  23510kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04071 (*cross-listing*)
Date: Wed, 6 Mar 2024 22:04:14 GMT   (1046kb,D)

Title: On-device Self-supervised Learning of Visual Perception Tasks aboard
  Hardware-limited Nano-quadrotors
Authors: Elia Cereda, Manuele Rusci, Alessandro Giusti, Daniele Palossi
Categories: cs.RO cs.AI
Comments: \c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\
  Sub-\SI{50}{\gram} nano-drones are gaining momentum in both academia and
industry. Their most compelling applications rely on onboard deep learning
models for perception despite severe hardware constraints (\ie
sub-\SI{100}{\milli\watt} processor). When deployed in unknown environments not
represented in the training data, these models often underperform due to domain
shift. To cope with this fundamental problem, we propose, for the first time,
on-device learning aboard nano-drones, where the first part of the in-field
mission is dedicated to self-supervised fine-tuning of a pre-trained
convolutional neural network (CNN). Leveraging a real-world vision-based
regression task, we thoroughly explore performance-cost trade-offs of the
fine-tuning phase along three axes: \textit{i}) dataset size (more data
increases the regression performance but requires more memory and longer
computation); \textit{ii}) methodologies (\eg fine-tuning all model parameters
vs. only a subset); and \textit{iii}) self-supervision strategy. Our approach
demonstrates an improvement in mean absolute error up to 30\% compared to the
pre-trained baseline, requiring only \SI{22}{\second} fine-tuning on an
ultra-low-power GWT GAP9 System-on-Chip. Addressing the domain shift problem
via on-device learning aboard nano-drones not only marks a novel result for
hardware-limited robots but lays the ground for more general advancements for
the entire robotics community.
\\ ( https://arxiv.org/abs/2403.04071 ,  1046kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04115 (*cross-listing*)
Date: Thu, 7 Mar 2024 00:09:07 GMT   (24916kb,D)

Title: DNAct: Diffusion Guided Multi-Task 3D Policy Learning
Authors: Ge Yan, Yueh-Hua Wu, Xiaolong Wang
Categories: cs.RO cs.AI cs.CV
\\
  This paper presents DNAct, a language-conditioned multi-task policy framework
that integrates neural rendering pre-training and diffusion training to enforce
multi-modality learning in action sequence spaces. To learn a generalizable
multi-task policy with few demonstrations, the pre-training phase of DNAct
leverages neural rendering to distill 2D semantic features from foundation
models such as Stable Diffusion to a 3D space, which provides a comprehensive
semantic understanding regarding the scene. Consequently, it allows various
applications to challenging robotic tasks requiring rich 3D semantics and
accurate geometry. Furthermore, we introduce a novel approach utilizing
diffusion training to learn a vision and language feature that encapsulates the
inherent multi-modality in the multi-task demonstrations. By reconstructing the
action sequences from different tasks via the diffusion process, the model is
capable of distinguishing different modalities and thus improving the
robustness and the generalizability of the learned representation. DNAct
significantly surpasses SOTA NeRF-based multi-task manipulation approaches with
over 30% improvement in success rate. Project website: dnact.github.io.
\\ ( https://arxiv.org/abs/2403.04115 ,  24916kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04160 (*cross-listing*)
Date: Thu, 7 Mar 2024 02:34:54 GMT   (2523kb,D)

Title: Improving Retrieval in Theme-specific Applications using a Corpus
  Topical Taxonomy
Authors: SeongKu Kang, Shivam Agarwal, Bowen Jin, Dongha Lee, Hwanjo Yu, and
  Jiawei Han
Categories: cs.IR cs.AI
Comments: TheWebConf'24
\\
  Document retrieval has greatly benefited from the advancements of large-scale
pre-trained language models (PLMs). However, their effectiveness is often
limited in theme-specific applications for specialized areas or industries, due
to unique terminologies, incomplete contexts of user queries, and specialized
search intents. To capture the theme-specific information and improve
retrieval, we propose to use a corpus topical taxonomy, which outlines the
latent topic structure of the corpus while reflecting user-interested aspects.
We introduce ToTER (Topical Taxonomy Enhanced Retrieval) framework, which
identifies the central topics of queries and documents with the guidance of the
taxonomy, and exploits their topical relatedness to supplement missing
contexts. As a plug-and-play framework, ToTER can be flexibly employed to
enhance various PLM-based retrievers. Through extensive quantitative, ablative,
and exploratory experiments on two real-world datasets, we ascertain the
benefits of using topical taxonomy for retrieval in theme-specific applications
and demonstrate the effectiveness of ToTER.
\\ ( https://arxiv.org/abs/2403.04160 ,  2523kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04164 (*cross-listing*)
Date: Thu, 7 Mar 2024 02:48:42 GMT   (4370kb)

Title: ProMISe: Promptable Medical Image Segmentation using SAM
Authors: Jinfeng Wang, Sifan Song, Xinkun Wang, Yiyi Wang, Yiyi Miao, Jionglong
  Su, S. Kevin Zhou
Categories: cs.CV cs.AI
\\
  With the proposal of the Segment Anything Model (SAM), fine-tuning SAM for
medical image segmentation (MIS) has become popular. However, due to the large
size of the SAM model and the significant domain gap between natural and
medical images, fine-tuning-based strategies are costly with potential risk of
instability, feature damage and catastrophic forgetting. Furthermore, some
methods of transferring SAM to a domain-specific MIS through fine-tuning
strategies disable the model's prompting capability, severely limiting its
utilization scenarios. In this paper, we propose an Auto-Prompting Module
(APM), which provides SAM-based foundation model with Euclidean adaptive
prompts in the target domain. Our experiments demonstrate that such adaptive
prompts significantly improve SAM's non-fine-tuned performance in MIS. In
addition, we propose a novel non-invasive method called Incremental Pattern
Shifting (IPS) to adapt SAM to specific medical domains. Experimental results
show that the IPS enables SAM to achieve state-of-the-art or competitive
performance in MIS without the need for fine-tuning. By coupling these two
methods, we propose ProMISe, an end-to-end non-fine-tuned framework for
Promptable Medical Image Segmentation. Our experiments demonstrate that both
using our methods individually or in combination achieves satisfactory
performance in low-cost pattern shifting, with all of SAM's parameters frozen.
\\ ( https://arxiv.org/abs/2403.04164 ,  4370kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04175 (*cross-listing*)
Date: Thu, 7 Mar 2024 03:12:31 GMT   (1198kb)

Title: Understanding the PULSAR Effect in Combined Radiotherapy and
  Immunotherapy through Attention Mechanisms with a Transformer Model
Authors: Hao Peng, Casey Moore, Debabrata Saha, Steve Jiang and Robert
  Timmerman
Categories: physics.med-ph cs.AI
\\
  PULSAR (personalized, ultra-fractionated stereotactic adaptive radiotherapy)
is the adaptation of stereotactic ablative radiotherapy towards personalized
cancer management. For the first time, we applied a transformer-based attention
mechanism to investigate the underlying interactions between combined PULSAR
and PD-L1 blockade immunotherapy based on a murine cancer model (Lewis Lung
Carcinoma, LLC). The proposed approach is able to predict the trend of tumor
volume change semi-quantitatively, and excels in identifying the potential
causal relationships through both self-attention and cross-attention scores.
\\ ( https://arxiv.org/abs/2403.04175 ,  1198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04187 (*cross-listing*)
Date: Thu, 7 Mar 2024 03:36:03 GMT   (1984kb,D)

Title: Preference optimization of protein language models as a multi-objective
  binder design paradigm
Authors: Pouria Mistani, Venkatesh Mysore
Categories: physics.bio-ph cs.AI cs.CE q-bio.BM
Comments: Published at the GEM workshop, ICLR 2024. Generative and Experimental
  Perspectives for Biomolecular Design (https://www.gembio.ai/)
\\
  We present a multi-objective binder design paradigm based on instruction
fine-tuning and direct preference optimization (DPO) of autoregressive protein
language models (pLMs). Multiple design objectives are encoded in the language
model through direct optimization on expert curated preference sequence
datasets comprising preferred and dispreferred distributions. We show the
proposed alignment strategy enables ProtGPT2 to effectively design binders
conditioned on specified receptors and a drug developability criterion.
Generated binder samples demonstrate median isoelectric point (pI) improvements
by $17\%-60\%$.
\\ ( https://arxiv.org/abs/2403.04187 ,  1984kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04202 (*cross-listing*)
Date: Thu, 7 Mar 2024 04:12:24 GMT   (28479kb,D)

Title: Dynamics of Moral Behavior in Heterogeneous Populations of Learning
  Agents
Authors: Elizaveta Tennant, Stephen Hailes, Mirco Musolesi
Categories: cs.MA cs.AI cs.CY cs.LG
\\
  Growing concerns about safety and alignment of AI systems highlight the
importance of embedding moral capabilities in artificial agents. A promising
solution is the use of learning from experience, i.e., Reinforcement Learning.
In multi-agent (social) environments, complex population-level phenomena may
emerge from interactions between individual learning agents. Many of the
existing studies rely on simulated social dilemma environments to study the
interactions of independent learning agents. However, they tend to ignore the
moral heterogeneity that is likely to be present in societies of agents in
practice. For example, at different points in time a single learning agent may
face opponents who are consequentialist (i.e., caring about maximizing some
outcome over time) or norm-based (i.e., focusing on conforming to a specific
norm here and now). The extent to which agents' co-development may be impacted
by such moral heterogeneity in populations is not well understood. In this
paper, we present a study of the learning dynamics of morally heterogeneous
populations interacting in a social dilemma setting. Using a Prisoner's Dilemma
environment with a partner selection mechanism, we investigate the extent to
which the prevalence of diverse moral agents in populations affects individual
agents' learning behaviors and emergent population-level outcomes. We observe
several types of non-trivial interactions between pro-social and anti-social
agents, and find that certain classes of moral agents are able to steer selfish
agents towards more cooperative behavior.
\\ ( https://arxiv.org/abs/2403.04202 ,  28479kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04232 (*cross-listing*)
Date: Thu, 7 Mar 2024 05:25:34 GMT   (4989kb,D)

Title: Generalizing Cooperative Eco-driving via Multi-residual Task Learning
Authors: Vindula Jayawardana, Sirui Li, Cathy Wu, Yashar Farid, Kentaro Oguchi
Categories: cs.RO cs.AI cs.LG cs.MA cs.SY eess.SY
Comments: Accepted for publication at ICRA 2024
\\
  Conventional control, such as model-based control, is commonly utilized in
autonomous driving due to its efficiency and reliability. However, real-world
autonomous driving contends with a multitude of diverse traffic scenarios that
are challenging for these planning algorithms. Model-free Deep Reinforcement
Learning (DRL) presents a promising avenue in this direction, but learning DRL
control policies that generalize to multiple traffic scenarios is still a
challenge. To address this, we introduce Multi-residual Task Learning (MRTL), a
generic learning framework based on multi-task learning that, for a set of task
scenarios, decomposes the control into nominal components that are effectively
solved by conventional control methods and residual terms which are solved
using learning. We employ MRTL for fleet-level emission reduction in mixed
traffic using autonomous vehicles as a means of system control. By analyzing
the performance of MRTL across nearly 600 signalized intersections and 1200
traffic scenarios, we demonstrate that it emerges as a promising approach to
synergize the strengths of DRL and conventional methods in generalizable
control.
\\ ( https://arxiv.org/abs/2403.04232 ,  4989kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04246 (*cross-listing*)
Date: Thu, 7 Mar 2024 06:07:31 GMT   (1428kb,D)

Title: Efficient CNN-LSTM based Parameter Estimation of Levy Driven Stochastic
  Differential Equations
Authors: Shuaiyu Li, Yang Ruan, Changzhou Long, Yuzhong Cheng
Categories: stat.ML cs.AI cs.LG
Comments: 2023 International Conference on Machine Learning and Applications
  (ICMLA)
\\
  This study addresses the challenges in parameter estimation of stochastic
differential equations driven by non-Gaussian noises, which are critical in
understanding dynamic phenomena such as price fluctuations and the spread of
infectious diseases. Previous research highlighted the potential of LSTM
networks in estimating parameters of alpha stable Levy driven SDEs but faced
limitations including high time complexity and constraints of the LSTM chaining
property. To mitigate these issues, we introduce the PEnet, a novel
CNN-LSTM-based three-stage model that offers an end to end approach with
superior accuracy and adaptability to varying data structures, enhanced
inference speed for long sequence observations through initial data feature
condensation by CNN, and high generalization capability, allowing its
application to various complex SDE scenarios. Experiments on synthetic datasets
confirm PEnet significant advantage in estimating SDE parameters associated
with noise characteristics, establishing it as a competitive method for SDE
parameter estimation in the presence of Levy noise.
\\ ( https://arxiv.org/abs/2403.04246 ,  1428kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04256 (*cross-listing*)
Date: Thu, 7 Mar 2024 06:38:41 GMT   (2375kb,D)

Title: Federated Recommendation via Hybrid Retrieval Augmented Generation
Authors: Huimin Zeng, Zhenrui Yue, Qian Jiang, Dong Wang
Categories: cs.IR cs.AI
\\
  Federated Recommendation (FR) emerges as a novel paradigm that enables
privacy-preserving recommendations. However, traditional FR systems usually
represent users/items with discrete identities (IDs), suffering from
performance degradation due to the data sparsity and heterogeneity in FR. On
the other hand, Large Language Models (LLMs) as recommenders have proven
effective across various recommendation scenarios. Yet, LLM-based recommenders
encounter challenges such as low inference efficiency and potential
hallucination, compromising their performance in real-world scenarios. To this
end, we propose GPT-FedRec, a federated recommendation framework leveraging
ChatGPT and a novel hybrid Retrieval Augmented Generation (RAG) mechanism.
GPT-FedRec is a two-stage solution. The first stage is a hybrid retrieval
process, mining ID-based user patterns and text-based item features. Next, the
retrieved results are converted into text prompts and fed into GPT for
re-ranking. Our proposed hybrid retrieval mechanism and LLM-based re-rank aims
to extract generalized features from data and exploit pretrained knowledge
within LLM, overcoming data sparsity and heterogeneity in FR. In addition, the
RAG approach also prevents LLM hallucination, improving the recommendation
performance for real-world users. Experimental results on diverse benchmark
datasets demonstrate the superior performance of GPT-FedRec against
state-of-the-art baseline methods.
\\ ( https://arxiv.org/abs/2403.04256 ,  2375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04299 (*cross-listing*)
Date: Thu, 7 Mar 2024 07:58:58 GMT   (2394kb,D)

Title: LitSim: Conflict-aware Policy for Long-term Interactive Traffic
  Simulation
Authors: Haojie Xin, Xiaodong Zhang, Renzhi Tang, Songyang Yan, Qianrui Zhao,
  Chunze Yang, Zijiang Yang
Categories: cs.RO cs.AI
\\
  Simulation is pivotal in evaluating the performance of autonomous driving
systems due to the advantages in efficiency and cost compared to on-road
testing. Realistic multi-agent behavior~(e.g., interactive and long-term) is
needed to narrow the gap between the simulation and the reality. The existing
work has the following shortcomings in achieving this goal:~(1) log replay
offers realistic scenarios but leads to unrealistic collisions due to lacking
dynamic interactions, and~(2) model-based and learning-based solutions
encourage interactions but often deviate from real-world data in long horizons.
In this work, we propose LitSim, a long-term interactive simulation approach
that maximizes realism while avoiding unrealistic collisions. Specifically, we
replay the log for most scenarios and intervene only when LitSim predicts
unrealistic conflicts. We then encourage interactions among the agents and
resolve the conflicts, thereby reducing the likelihood of unrealistic
collisions. We train and validate our model on the real-world dataset NGSIM,
and the experimental results demonstrate that LitSim outperforms the current
popular approaches in realism and reactivity.
\\ ( https://arxiv.org/abs/2403.04299 ,  2394kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04306 (*cross-listing*)
Date: Thu, 7 Mar 2024 08:25:27 GMT   (1677kb,D)

Title: Effectiveness Assessment of Recent Large Vision-Language Models
Authors: Yao Jiang, Xinyu Yan, Ge-Peng Ji, Keren Fu, Meijun Sun, Huan Xiong,
  Deng-Ping Fan, Fahad Shahbaz Khan
Categories: cs.CV cs.AI cs.LG
\\
  The advent of large vision-language models (LVLMs) represents a noteworthy
advancement towards the pursuit of artificial general intelligence. However,
the extent of their efficacy across both specialized and general tasks warrants
further investigation. This article endeavors to evaluate the competency of
popular LVLMs in specialized and general tasks, respectively, aiming to offer a
comprehensive comprehension of these innovative methodologies. To gauge their
efficacy in specialized tasks, we tailor a comprehensive testbed comprising
three distinct scenarios: natural, healthcare, and industrial, encompassing six
challenging tasks. These tasks include salient, camouflaged, and transparent
object detection, as well as polyp and skin lesion detection, alongside
industrial anomaly detection. We examine the performance of three recent
open-source LVLMs -- MiniGPT-v2, LLaVA-1.5, and Shikra -- in the realm of
visual recognition and localization. Moreover, we conduct empirical
investigations utilizing the aforementioned models alongside GPT-4V, assessing
their multi-modal understanding capacities in general tasks such as object
counting, absurd question answering, affordance reasoning, attribute
recognition, and spatial relation reasoning. Our investigations reveal that
these models demonstrate limited proficiency not only in specialized tasks but
also in general tasks. We delve deeper into this inadequacy and suggest several
potential factors, including limited cognition in specialized tasks, object
hallucination, text-to-image interference, and decreased robustness in complex
problems. We hope this study would provide valuable insights for the future
development of LVLMs, augmenting their power in coping with both general and
specialized applications.
\\ ( https://arxiv.org/abs/2403.04306 ,  1677kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04309 (*cross-listing*)
Date: Thu, 7 Mar 2024 08:30:17 GMT   (4717kb,D)

Title: AO-DETR: Anti-Overlapping DETR for X-Ray Prohibited Items Detection
Authors: Mingyuan Li, Tong Jia, Hao Wang, Bowen Ma, Shuyang Lin, Da Cai, and
  Dongyue Chen
Categories: cs.CV cs.AI
\\
  Prohibited item detection in X-ray images is one of the most essential and
highly effective methods widely employed in various security inspection
scenarios. Considering the significant overlapping phenomenon in X-ray
prohibited item images, we propose an Anti-Overlapping DETR (AO-DETR) based on
one of the state-of-the-art general object detectors, DINO. Specifically, to
address the feature coupling issue caused by overlapping phenomena, we
introduce the Category-Specific One-to-One Assignment (CSA) strategy to
constrain category-specific object queries in predicting prohibited items of
fixed categories, which can enhance their ability to extract features specific
to prohibited items of a particular category from the overlapping
foreground-background features. To address the edge blurring problem caused by
overlapping phenomena, we propose the Look Forward Densely (LFD) scheme, which
improves the localization accuracy of reference boxes in mid-to-high-level
decoder layers and enhances the ability to locate blurry edges of the final
layer. Similar to DINO, our AO-DETR provides two different versions with
distinct backbones, tailored to meet diverse application requirements.
Extensive experiments on the PIXray and OPIXray datasets demonstrate that the
proposed method surpasses the state-of-the-art object detectors, indicating its
potential applications in the field of prohibited item detection. The source
code will be released at https://github.com/Limingyuan001/AO-DETR-test.
\\ ( https://arxiv.org/abs/2403.04309 ,  4717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04321 (*cross-listing*)
Date: Thu, 7 Mar 2024 08:37:33 GMT   (15021kb,D)

Title: Discriminative Probing and Tuning for Text-to-Image Generation
Authors: Leigang Qu, Wenjie Wang, Yongqi Li, Hanwang Zhang, Liqiang Nie,
  Tat-Seng Chua
Categories: cs.CV cs.AI cs.CL cs.MM
Comments: CVPR 2024; project page: https://dpt-t2i.github.io/
\\
  Despite advancements in text-to-image generation (T2I), prior methods often
face text-image misalignment problems such as relation confusion in generated
images. Existing solutions involve cross-attention manipulation for better
compositional understanding or integrating large language models for improved
layout planning. However, the inherent alignment capabilities of T2I models are
still inadequate. By reviewing the link between generative and discriminative
modeling, we posit that T2I models' discriminative abilities may reflect their
text-image alignment proficiency during generation. In this light, we advocate
bolstering the discriminative abilities of T2I models to achieve more precise
text-to-image alignment for generation. We present a discriminative adapter
built on T2I models to probe their discriminative abilities on two
representative tasks and leverage discriminative fine-tuning to improve their
text-image alignment. As a bonus of the discriminative adapter, a
self-correction mechanism can leverage discriminative gradients to better align
generated images to text prompts during inference. Comprehensive evaluations
across three benchmark datasets, including both in-distribution and
out-of-distribution scenarios, demonstrate our method's superior generation
performance. Meanwhile, it achieves state-of-the-art discriminative performance
on the two discriminative tasks compared to other generative models.
\\ ( https://arxiv.org/abs/2403.04321 ,  15021kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04326 (*cross-listing*)
Date: Thu, 7 Mar 2024 08:45:31 GMT   (3884kb,D)

Title: Edge-based Parametric Digital Twins for Intelligent Building Indoor
  Climate Modeling
Authors: Zhongjun Ni (1), Chi Zhang (2), Magnus Karlsson (1), Shaofang Gong (1)
  ((1) Department of Science and Technology, Link\"oping University, Campus
  Norrk\"oping, Norrk\"oping, Sweden. (2) Department of Computer Science and
  Engineering, University of Gothenburg, Gothenburg, Sweden.)
Categories: eess.SY cs.AI cs.LG cs.SY
Comments: 8 pages, 8 figures, accepted in the 20th IEEE International
  Conference on Factory Communication Systems
MSC-class: 68T07
ACM-class: I.5.4
\\
  Digital transformation in the built environment generates vast data for
developing data-driven models to optimize building operations. This study
presents an integrated solution utilizing edge computing, digital twins, and
deep learning to enhance the understanding of climate in buildings. Parametric
digital twins, created using an ontology, ensure consistent data representation
across diverse service systems equipped by different buildings. Based on
created digital twins and collected data, deep learning methods are employed to
develop predictive models for identifying patterns in indoor climate and
providing insights. Both the parametric digital twin and deep learning models
are deployed on edge for low latency and privacy compliance. As a
demonstration, a case study was conducted in a historic building in
\"Osterg\"otland, Sweden, to compare the performance of five deep learning
architectures. The results indicate that the time-series dense encoder model
exhibited strong competitiveness in performing multi-horizon forecasts of
indoor temperature and relative humidity with low computational costs.
\\ ( https://arxiv.org/abs/2403.04326 ,  3884kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04359 (*cross-listing*)
Date: Thu, 7 Mar 2024 09:41:11 GMT   (24816kb,D)

Title: Symmetry Considerations for Learning Task Symmetric Robot Policies
Authors: Mayank Mittal, Nikita Rudin, Victor Klemm, Arthur Allshire, Marco
  Hutter
Categories: cs.RO cs.AI
Comments: M. Mittal and N. Rudin contributed equally. Accepted for ICRA 2024
\\
  Symmetry is a fundamental aspect of many real-world robotic tasks. However,
current deep reinforcement learning (DRL) approaches can seldom harness and
exploit symmetry effectively. Often, the learned behaviors fail to achieve the
desired transformation invariances and suffer from motion artifacts. For
instance, a quadruped may exhibit different gaits when commanded to move
forward or backward, even though it is symmetrical about its torso. This issue
becomes further pronounced in high-dimensional or complex environments, where
DRL methods are prone to local optima and fail to explore regions of the state
space equally. Past methods on encouraging symmetry for robotic tasks have
studied this topic mainly in a single-task setting, where symmetry usually
refers to symmetry in the motion, such as the gait patterns. In this paper, we
revisit this topic for goal-conditioned tasks in robotics, where symmetry lies
mainly in task execution and not necessarily in the learned motions themselves.
In particular, we investigate two approaches to incorporate symmetry invariance
into DRL -- data augmentation and mirror loss function. We provide a
theoretical foundation for using augmented samples in an on-policy setting.
Based on this, we show that the corresponding approach achieves faster
convergence and improves the learned behaviors in various challenging robotic
tasks, from climbing boxes with a quadruped to dexterous manipulation.
\\ ( https://arxiv.org/abs/2403.04359 ,  24816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04374 (*cross-listing*)
Date: Thu, 7 Mar 2024 10:06:46 GMT   (1622kb,D)

Title: Model-Free Load Frequency Control of Nonlinear Power Systems Based on
  Deep Reinforcement Learning
Authors: Xiaodi Chen, Meng Zhang, Zhengguang Wu, Ligang Wu and Xiaohong Guan
Categories: eess.SY cs.AI cs.SY
\\
  Load frequency control (LFC) is widely employed in power systems to stabilize
frequency fluctuation and guarantee power quality. However, most existing LFC
methods rely on accurate power system modeling and usually ignore the nonlinear
characteristics of the system, limiting controllers' performance. To solve
these problems, this paper proposes a model-free LFC method for nonlinear power
systems based on deep deterministic policy gradient (DDPG) framework. The
proposed method establishes an emulator network to emulate power system
dynamics. After defining the action-value function, the emulator network is
applied for control actions evaluation instead of the critic network. Then the
actor network controller is effectively optimized by estimating the policy
gradient based on zeroth-order optimization (ZOO) and backpropagation
algorithm. Simulation results and corresponding comparisons demonstrate the
designed controller can generate appropriate control actions and has strong
adaptability for nonlinear power systems.
\\ ( https://arxiv.org/abs/2403.04374 ,  1622kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04427 (*cross-listing*)
Date: Thu, 7 Mar 2024 11:56:36 GMT   (2646kb,D)

Title: Sentiment-driven prediction of financial returns: a Bayesian-enhanced
  FinBERT approach
Authors: Raffaele Giuseppe Cestari and Simone Formentin
Categories: cs.CE cs.AI
Comments: Version exposed at XXV Workshop on Quantitative Finance Bologna
  (Italy), April 11-13 2024 (not peer reviewed but accepted for the workshop)
\\
  Predicting financial returns accurately poses a significant challenge due to
the inherent uncertainty in financial time series data. Enhancing prediction
models' performance hinges on effectively capturing both social and financial
sentiment. In this study, we showcase the efficacy of leveraging sentiment
information extracted from tweets using the FinBERT large language model. By
meticulously curating an optimal feature set through correlation analysis and
employing Bayesian-optimized Recursive Feature Elimination for automatic
feature selection, we surpass existing methodologies, achieving an F1-score
exceeding 70% on the test set. This success translates into demonstrably higher
cumulative profits during backtested trading. Our investigation focuses on
real-world SPY ETF data alongside corresponding tweets sourced from the
StockTwits platform.
\\ ( https://arxiv.org/abs/2403.04427 ,  2646kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04436 (*cross-listing*)
Date: Thu, 7 Mar 2024 12:10:41 GMT   (12319kb,D)

Title: Learning Human-to-Humanoid Real-Time Whole-Body Teleoperation
Authors: Tairan He, Zhengyi Luo, Wenli Xiao, Chong Zhang, Kris Kitani, Changliu
  Liu, Guanya Shi
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
Comments: Project website: https://human2humanoid.com/
\\
  We present Human to Humanoid (H2O), a reinforcement learning (RL) based
framework that enables real-time whole-body teleoperation of a full-sized
humanoid robot with only an RGB camera. To create a large-scale retargeted
motion dataset of human movements for humanoid robots, we propose a scalable
"sim-to-data" process to filter and pick feasible motions using a privileged
motion imitator. Afterwards, we train a robust real-time humanoid motion
imitator in simulation using these refined motions and transfer it to the real
humanoid robot in a zero-shot manner. We successfully achieve teleoperation of
dynamic whole-body motions in real-world scenarios, including walking, back
jumping, kicking, turning, waving, pushing, boxing, etc. To the best of our
knowledge, this is the first demonstration to achieve learning-based real-time
whole-body humanoid teleoperation.
\\ ( https://arxiv.org/abs/2403.04436 ,  12319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04473 (*cross-listing*)
Date: Thu, 7 Mar 2024 13:16:24 GMT   (11933kb,D)

Title: TextMonkey: An OCR-Free Large Multimodal Model for Understanding
  Document
Authors: Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang,
  Xiang Bai
Categories: cs.CV cs.AI
\\
  We present TextMonkey, a large multimodal model (LMM) tailored for
text-centric tasks, including document question answering (DocVQA) and scene
text analysis. Our approach introduces enhancement across several dimensions:
by adopting Shifted Window Attention with zero-initialization, we achieve
cross-window connectivity at higher input resolutions and stabilize early
training; We hypothesize that images may contain redundant tokens, and by using
similarity to filter out significant tokens, we can not only streamline the
token length but also enhance the model's performance. Moreover, by expanding
our model's capabilities to encompass text spotting and grounding, and
incorporating positional information into responses, we enhance
interpretability and minimize hallucinations. Additionally, TextMonkey can be
finetuned to gain the ability to comprehend commands for clicking screenshots.
Overall, our method notably boosts performance across various benchmark
datasets, achieving increases of 5.2%, 6.9%, and 2.8% in Scene Text-Centric
VQA, Document Oriented VQA, and KIE, respectively, especially with a score of
561 on OCRBench, surpassing prior open-sourced large multimodal models for
document understanding. Code will be released at
https://github.com/Yuliang-Liu/Monkey.
\\ ( https://arxiv.org/abs/2403.04473 ,  11933kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04500 (*cross-listing*)
Date: Thu, 7 Mar 2024 13:59:34 GMT   (1469kb,D)

Title: A Learnable Prior Improves Inverse Tumor Growth Modeling
Authors: Jonas Weidner, Ivan Ezhov, Michal Balcerak, Marie-Christin Metz,
  Sergey Litvinov, Sebastian Kaltenbach, Leonhard Feiner, Laurin Lux, Florian
  Kofler, Jana Lipkova, Jonas Latz, Daniel Rueckert, Bjoern Menze, Benedikt
  Wiestler
Categories: physics.med-ph cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Biophysical modeling, particularly involving partial differential equations
(PDEs), offers significant potential for tailoring disease treatment protocols
to individual patients. However, the inverse problem-solving aspect of these
models presents a substantial challenge, either due to the high computational
requirements of model-based approaches or the limited robustness of deep
learning (DL) methods. We propose a novel framework that leverages the unique
strengths of both approaches in a synergistic manner. Our method incorporates a
DL ensemble for initial parameter estimation, facilitating efficient downstream
evolutionary sampling initialized with this DL-based prior. We showcase the
effectiveness of integrating a rapid deep-learning algorithm with a
high-precision evolution strategy in estimating brain tumor cell concentrations
from magnetic resonance images. The DL-Prior plays a pivotal role,
significantly constraining the effective sampling-parameter space. This
reduction results in a fivefold convergence acceleration and a Dice-score of
95%
\\ ( https://arxiv.org/abs/2403.04500 ,  1469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04523 (*cross-listing*)
Date: Thu, 7 Mar 2024 14:25:03 GMT   (20545kb,D)

Title: T-TAME: Trainable Attention Mechanism for Explaining Convolutional
  Networks and Vision Transformers
Authors: Mariano V. Ntrougkas, Nikolaos Gkalelis, Vasileios Mezaris
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: Under review
\\
  The development and adoption of Vision Transformers and other deep-learning
architectures for image classification tasks has been rapid. However, the
"black box" nature of neural networks is a barrier to adoption in applications
where explainability is essential. While some techniques for generating
explanations have been proposed, primarily for Convolutional Neural Networks,
adapting such techniques to the new paradigm of Vision Transformers is
non-trivial. This paper presents T-TAME, Transformer-compatible Trainable
Attention Mechanism for Explanations, a general methodology for explaining deep
neural networks used in image classification tasks. The proposed architecture
and training technique can be easily applied to any convolutional or Vision
Transformer-like neural network, using a streamlined training approach. After
training, explanation maps can be computed in a single forward pass; these
explanation maps are comparable to or outperform the outputs of computationally
expensive perturbation-based explainability techniques, achieving SOTA
performance. We apply T-TAME to three popular deep learning classifier
architectures, VGG-16, ResNet-50, and ViT-B-16, trained on the ImageNet
dataset, and we demonstrate improvements over existing state-of-the-art
explainability methods. A detailed analysis of the results and an ablation
study provide insights into how the T-TAME design choices affect the quality of
the generated explanation maps.
\\ ( https://arxiv.org/abs/2403.04523 ,  20545kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04612 (*cross-listing*)
Date: Thu, 7 Mar 2024 15:58:03 GMT   (5663kb)

Title: A Domain Translation Framework with an Adversarial Denoising Diffusion
  Model to Generate Synthetic Datasets of Echocardiography Images
Authors: Cristiana Tiago, Sten Roar Snare, Jurica Sprem, and Kristin McLeod
Categories: eess.IV cs.AI cs.CV
DOI: 10.1109/ACCESS.2023.3246762
\\
  Currently, medical image domain translation operations show a high demand
from researchers and clinicians. Amongst other capabilities, this task allows
the generation of new medical images with sufficiently high image quality,
making them clinically relevant. Deep Learning (DL) architectures, most
specifically deep generative models, are widely used to generate and translate
images from one domain to another. The proposed framework relies on an
adversarial Denoising Diffusion Model (DDM) to synthesize echocardiography
images and perform domain translation. Contrary to Generative Adversarial
Networks (GANs), DDMs are able to generate high quality image samples with a
large diversity. If a DDM is combined with a GAN, this ability to generate new
data is completed at an even faster sampling time. In this work we trained an
adversarial DDM combined with a GAN to learn the reverse denoising process,
relying on a guide image, making sure relevant anatomical structures of each
echocardiography image were kept and represented on the generated image
samples. For several domain translation operations, the results verified that
such generative model was able to synthesize high quality image samples: MSE:
11.50 +/- 3.69, PSNR (dB): 30.48 +/- 0.09, SSIM: 0.47 +/- 0.03. The proposed
method showed high generalization ability, introducing a framework to create
echocardiography images suitable to be used for clinical research purposes.
\\ ( https://arxiv.org/abs/2403.04612 ,  5663kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04634 (*cross-listing*)
Date: Thu, 7 Mar 2024 16:18:28 GMT   (16130kb,D)

Title: Pix2Gif: Motion-Guided Diffusion for GIF Generation
Authors: Hitesh Kandala, Jianfeng Gao, Jianwei Yang
Categories: cs.CV cs.AI
\\
  We present Pix2Gif, a motion-guided diffusion model for image-to-GIF (video)
generation. We tackle this problem differently by formulating the task as an
image translation problem steered by text and motion magnitude prompts, as
shown in teaser fig. To ensure that the model adheres to motion guidance, we
propose a new motion-guided warping module to spatially transform the features
of the source image conditioned on the two types of prompts. Furthermore, we
introduce a perceptual loss to ensure the transformed feature map remains
within the same space as the target image, ensuring content consistency and
coherence. In preparation for the model training, we meticulously curated data
by extracting coherent image frames from the TGIF video-caption dataset, which
provides rich information about the temporal changes of subjects. After
pretraining, we apply our model in a zero-shot manner to a number of video
datasets. Extensive qualitative and quantitative experiments demonstrate the
effectiveness of our model -- it not only captures the semantic prompt from
text but also the spatial ones from motion guidance. We train all our models
using a single node of 16xV100 GPUs. Code, dataset and models are made public
at: https://hiteshk03.github.io/Pix2Gif/.
\\ ( https://arxiv.org/abs/2403.04634 ,  16130kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04690 (*cross-listing*)
Date: Thu, 7 Mar 2024 17:35:58 GMT   (2215kb,D)

Title: Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self
  Attention at the Threadblock Level
Authors: Ali Hassani, Wen-Mei Hwu, Humphrey Shi
Categories: cs.CV cs.AI cs.LG
Comments: Project page: https://github.com/SHI-Labs/NATTEN
\\
  Neighborhood attention reduces the cost of self attention by restricting each
token's attention span to its nearest neighbors. This restriction,
parameterized by a window size and dilation factor, draws a spectrum of
possible attention patterns between linear projection and self attention.
Neighborhood attention, and more generally sliding window attention patterns,
have long been bounded by infrastructure, particularly in higher-rank spaces
(2-D and 3-D), calling for the development of custom kernels, which have been
limited in either functionality, or performance, if not both. In this work, we
first show that neighborhood attention can be represented as a batched GEMM
problem, similar to standard attention, and implement it for 1-D and 2-D
neighborhood attention. These kernels on average provide 895% and 272%
improvement in full precision latency compared to existing naive kernels for
1-D and 2-D neighborhood attention respectively. We find certain inherent
inefficiencies in all unfused neighborhood attention kernels that bound their
performance and lower-precision scalability. We also developed fused
neighborhood attention; an adaptation of fused dot-product attention kernels
that allow fine-grained control over attention across different spatial axes.
Known for reducing the quadratic time complexity of self attention to a linear
complexity, neighborhood attention can now enjoy a reduced and constant memory
footprint, and record-breaking half precision latency. We observe that our
fused kernels successfully circumvent some of the unavoidable inefficiencies in
unfused implementations. While our unfused GEMM-based kernels only improve half
precision performance compared to naive kernels by an average of 496% and 113%
in 1-D and 2-D problems respectively, our fused kernels improve naive kernels
by an average of 1607% and 581% in 1-D and 2-D problems respectively.
\\ ( https://arxiv.org/abs/2403.04690 ,  2215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04697 (*cross-listing*)
Date: Thu, 7 Mar 2024 17:46:50 GMT   (5448kb,D)

Title: AUFormer: Vision Transformers are Parameter-Efficient Facial Action Unit
  Detectors
Authors: Kaishen Yuan, Zitong Yu, Xin Liu, Weicheng Xie, Huanjing Yue, Jingyu
  Yang
Categories: cs.CV cs.AI
Comments: 19 pages, 6 figures
\\
  Facial Action Units (AU) is a vital concept in the realm of affective
computing, and AU detection has always been a hot research topic. Existing
methods suffer from overfitting issues due to the utilization of a large number
of learnable parameters on scarce AU-annotated datasets or heavy reliance on
substantial additional relevant data. Parameter-Efficient Transfer Learning
(PETL) provides a promising paradigm to address these challenges, whereas its
existing methods lack design for AU characteristics. Therefore, we innovatively
investigate PETL paradigm to AU detection, introducing AUFormer and proposing a
novel Mixture-of-Knowledge Expert (MoKE) collaboration mechanism. An individual
MoKE specific to a certain AU with minimal learnable parameters first
integrates personalized multi-scale and correlation knowledge. Then the MoKE
collaborates with other MoKEs in the expert group to obtain aggregated
information and inject it into the frozen Vision Transformer (ViT) to achieve
parameter-efficient AU detection. Additionally, we design a Margin-truncated
Difficulty-aware Weighted Asymmetric Loss (MDWA-Loss), which can encourage the
model to focus more on activated AUs, differentiate the difficulty of
unactivated AUs, and discard potential mislabeled samples. Extensive
experiments from various perspectives, including within-domain, cross-domain,
data efficiency, and micro-expression domain, demonstrate AUFormer's
state-of-the-art performance and robust generalization abilities without
relying on additional relevant data. The code for AUFormer is available at
https://github.com/yuankaishen2001/AUFormer.
\\ ( https://arxiv.org/abs/2403.04697 ,  5448kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04701 (*cross-listing*)
Date: Thu, 7 Mar 2024 17:48:48 GMT   (45787kb,D)

Title: ObjectCompose: Evaluating Resilience of Vision-Based Models on
  Object-to-Background Compositional Changes
Authors: Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan,
  Fahad Shahbaz Khan
Categories: cs.CV cs.AI
\\
  Given the large-scale multi-modal training of recent vision-based models and
their generalization capabilities, understanding the extent of their robustness
is critical for their real-world deployment. In this work, we evaluate the
resilience of current vision-based models against diverse object-to-background
context variations. The majority of robustness evaluation methods have
introduced synthetic datasets to induce changes to object characteristics
(viewpoints, scale, color) or utilized image transformation techniques
(adversarial changes, common corruptions) on real images to simulate shifts in
distributions. Recent works have explored leveraging large language models and
diffusion models to generate changes in the background. However, these methods
either lack in offering control over the changes to be made or distort the
object semantics, making them unsuitable for the task. Our method, on the other
hand, can induce diverse object-to-background changes while preserving the
original semantics and appearance of the object. To achieve this goal, we
harness the generative capabilities of text-to-image, image-to-text, and
image-to-segment models to automatically generate a broad spectrum of
object-to-background changes. We induce both natural and adversarial background
changes by either modifying the textual prompts or optimizing the latents and
textual embedding of text-to-image models. This allows us to quantify the role
of background context in understanding the robustness and generalization of
deep neural networks. We produce various versions of standard vision datasets
(ImageNet, COCO), incorporating either diverse and realistic backgrounds into
the images or introducing color, texture, and adversarial changes in the
background. We conduct extensive experiment to analyze the robustness of
vision-based models against object-to-background context variations across
diverse tasks.
\\ ( https://arxiv.org/abs/2403.04701 ,  45787kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04758 (*cross-listing*)
Date: Thu, 7 Mar 2024 18:56:31 GMT   (9382kb,D)

Title: KnowledgeVIS: Interpreting Language Models by Comparing
  Fill-in-the-Blank Prompts
Authors: Adam Coscia, Alex Endert
Categories: cs.HC cs.AI cs.CY cs.LG
Comments: Accepted to IEEE TVCG. 20 pages, 10 figures, 1 table. For a demo
  video, see https://youtu.be/hBX4rSUMr_I . For a live demo, visit
  https://adamcoscia.com/papers/knowledgevis/demo/ . The source code is
  available at https://github.com/AdamCoscia/KnowledgeVIS
DOI: 10.1109/TVCG.2023.3346713
\\
  Recent growth in the popularity of large language models has led to their
increased usage for summarizing, predicting, and generating text, making it
vital to help researchers and engineers understand how and why they work. We
present KnowledgeVis, a human-in-the-loop visual analytics system for
interpreting language models using fill-in-the-blank sentences as prompts. By
comparing predictions between sentences, KnowledgeVis reveals learned
associations that intuitively connect what language models learn during
training to natural language tasks downstream, helping users create and test
multiple prompt variations, analyze predicted words using a novel semantic
clustering technique, and discover insights using interactive visualizations.
Collectively, these visualizations help users identify the likelihood and
uniqueness of individual predictions, compare sets of predictions between
prompts, and summarize patterns and relationships between predictions across
all prompts. We demonstrate the capabilities of KnowledgeVis with feedback from
six NLP experts as well as three different use cases: (1) probing biomedical
knowledge in two domain-adapted models; and (2) evaluating harmful identity
stereotypes and (3) discovering facts and relationships between three
general-purpose models.
\\ ( https://arxiv.org/abs/2403.04758 ,  9382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04760 (*cross-listing*)
Date: Thu, 7 Mar 2024 18:56:39 GMT   (8206kb,D)

Title: iScore: Visual Analytics for Interpreting How Language Models
  Automatically Score Summaries
Authors: Adam Coscia, Langdon Holmes, Wesley Morris, Joon Suh Choi, Scott
  Crossley, Alex Endert
Categories: cs.HC cs.AI cs.CY cs.LG
Comments: Accepted to IUI 2024. 16 pages, 5 figures, 1 table. For a demo video,
  see https://youtu.be/EYJX-_fQPf0 . For a live demo, visit
  https://adamcoscia.com/papers/iscore/demo/ . The source code is available at
  https://github.com/AdamCoscia/iScore
DOI: 10.1145/3640543.3645142
\\
  The recent explosion in popularity of large language models (LLMs) has
inspired learning engineers to incorporate them into adaptive educational tools
that automatically score summary writing. Understanding and evaluating LLMs is
vital before deploying them in critical learning environments, yet their
unprecedented size and expanding number of parameters inhibits transparency and
impedes trust when they underperform. Through a collaborative user-centered
design process with several learning engineers building and deploying summary
scoring LLMs, we characterized fundamental design challenges and goals around
interpreting their models, including aggregating large text inputs, tracking
score provenance, and scaling LLM interpretability methods. To address their
concerns, we developed iScore, an interactive visual analytics tool for
learning engineers to upload, score, and compare multiple summaries
simultaneously. Tightly integrated views allow users to iteratively revise the
language in summaries, track changes in the resulting LLM scores, and visualize
model weights at multiple levels of abstraction. To validate our approach, we
deployed iScore with three learning engineers over the course of a month. We
present a case study where interacting with iScore led a learning engineer to
improve their LLM's score accuracy by three percentage points. Finally, we
conducted qualitative interviews with the learning engineers that revealed how
iScore enabled them to understand, evaluate, and build trust in their LLMs
during deployment.
\\ ( https://arxiv.org/abs/2403.04760 ,  8206kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04009 (*cross-listing*)
Date: Wed, 6 Mar 2024 19:41:02 GMT   (717kb,D)

Title: Media Bias Matters: Understanding the Impact of Politically Biased News
  on Vaccine Attitudes in Social Media
Authors: Bohan Jiang, Lu Cheng, Zhen Tan, Ruocheng Guo, Huan Liu
Categories: cs.SI cs.CL cs.CY physics.soc-ph
Comments: 9 pages, 6 figures, 3 tables
\\
  News media has been utilized as a political tool to stray from facts,
presenting biased claims without evidence. Amid the COVID-19 pandemic,
politically biased news (PBN) has significantly undermined public trust in
vaccines, despite strong medical evidence supporting their efficacy. In this
paper, we analyze: (i) how inherent vaccine stances subtly influence
individuals' selection of news sources and participation in social media
discussions; and (ii) the impact of exposure to PBN on users' attitudes toward
vaccines. In doing so, we first curate a comprehensive dataset that connects
PBN with related social media discourse. Utilizing advanced deep learning and
causal inference techniques, we reveal distinct user behaviors between social
media groups with various vaccine stances. Moreover, we observe that
individuals with moderate stances, particularly the vaccine-hesitant majority,
are more vulnerable to the influence of PBN compared to those with extreme
views. Our findings provide critical insights to foster this line of research.
\\ ( https://arxiv.org/abs/2403.04009 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04123 (*cross-listing*)
Date: Thu, 7 Mar 2024 00:44:01 GMT   (578kb,D)

Title: Exploring LLM-based Agents for Root Cause Analysis
Authors: Devjeet Roy, Xuchao Zhang, Rashi Bhave, Chetan Bansal, Pedro
  Las-Casas, Rodrigo Fonseca, Saravan Rajmohan
Categories: cs.SE cs.CL cs.LG
\\
  The growing complexity of cloud based software systems has resulted in
incident management becoming an integral part of the software development
lifecycle. Root cause analysis (RCA), a critical part of the incident
management process, is a demanding task for on-call engineers, requiring deep
domain knowledge and extensive experience with a team's specific services.
Automation of RCA can result in significant savings of time, and ease the
burden of incident management on on-call engineers. Recently, researchers have
utilized Large Language Models (LLMs) to perform RCA, and have demonstrated
promising results. However, these approaches are not able to dynamically
collect additional diagnostic information such as incident related logs,
metrics or databases, severely restricting their ability to diagnose root
causes. In this work, we explore the use of LLM based agents for RCA to address
this limitation. We present a thorough empirical evaluation of a ReAct agent
equipped with retrieval tools, on an out-of-distribution dataset of production
incidents collected at Microsoft. Results show that ReAct performs
competitively with strong retrieval and reasoning baselines, but with highly
increased factual accuracy. We then extend this evaluation by incorporating
discussions associated with incident reports as additional inputs for the
models, which surprisingly does not yield significant performance improvements.
Lastly, we conduct a case study with a team at Microsoft to equip the ReAct
agent with tools that give it access to external diagnostic services that are
used by the team for manual RCA. Our results show how agents can overcome the
limitations of prior work, and practical considerations for implementing such a
system in practice.
\\ ( https://arxiv.org/abs/2403.04123 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04260 (*cross-listing*)
Date: Thu, 7 Mar 2024 06:49:37 GMT   (4068kb,D)

Title: Can Small Language Models be Good Reasoners for Sequential
  Recommendation?
Authors: Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang
  Zhang, Jun Zhou, Liang Pang, Xiao Wang
Categories: cs.IR cs.CL cs.LG
\\
  Large language models (LLMs) open up new horizons for sequential
recommendations, owing to their remarkable language comprehension and
generation capabilities. However, there are still numerous challenges that
should be addressed to successfully implement sequential recommendations
empowered by LLMs. Firstly, user behavior patterns are often complex, and
relying solely on one-step reasoning from LLMs may lead to incorrect or
task-irrelevant responses. Secondly, the prohibitively resource requirements of
LLM (e.g., ChatGPT-175B) are overwhelmingly high and impractical for real
sequential recommender systems. In this paper, we propose a novel Step-by-step
knowLedge dIstillation fraMework for recommendation (SLIM), paving a promising
path for sequential recommenders to enjoy the exceptional reasoning
capabilities of LLMs in a "slim" (i.e., resource-efficient) manner. We
introduce CoT prompting based on user behavior sequences for the larger teacher
model. The rationales generated by the teacher model are then utilized as
labels to distill the downstream smaller student model (e.g., LLaMA2-7B). In
this way, the student model acquires the step-by-step reasoning capabilities in
recommendation tasks. We encode the generated rationales from the student model
into a dense vector, which empowers recommendation in both ID-based and
ID-agnostic scenarios. Extensive experiments demonstrate the effectiveness of
SLIM over state-of-the-art baselines, and further analysis showcasing its
ability to generate meaningful recommendation reasoning at affordable costs.
\\ ( https://arxiv.org/abs/2403.04260 ,  4068kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04327 (*cross-listing*)
Date: Thu, 7 Mar 2024 08:48:04 GMT   (464kb,D)

Title: ProMoAI: Process Modeling with Generative AI
Authors: Humam Kourani, Alessandro Berti, Daniel Schuster, Wil M. P. van der
  Aalst
Categories: cs.DB cs.CL
\\
  ProMoAI is a novel tool that leverages Large Language Models (LLMs) to
automatically generate process models from textual descriptions, incorporating
advanced prompt engineering, error handling, and code generation techniques.
Beyond automating the generation of complex process models, ProMoAI also
supports process model optimization. Users can interact with the tool by
providing feedback on the generated model, which is then used for refining the
process model. ProMoAI utilizes the capabilities LLMs to offer a novel,
AI-driven approach to process modeling, significantly reducing the barrier to
entry for users without deep technical knowledge in process modeling.
\\ ( https://arxiv.org/abs/2403.04327 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04395 (*cross-listing*)
Date: Thu, 7 Mar 2024 10:39:48 GMT   (6369kb,D)

Title: SGNet: Folding Symmetrical Protein Complex with Deep Learning
Authors: Zhaoqun Li, Jingcheng Yu, Qiwei Ye
Categories: q-bio.BM cs.CL
\\
  Deep learning has made significant progress in protein structure prediction,
advancing the development of computational biology. However, despite the high
accuracy achieved in predicting single-chain structures, a significant number
of large homo-oligomeric assemblies exhibit internal symmetry, posing a major
challenge in structure determination. The performances of existing deep
learning methods are limited since the symmetrical protein assembly usually has
a long sequence, making structural computation infeasible. In addition,
multiple identical subunits in symmetrical protein complex cause the issue of
supervision ambiguity in label assignment, requiring a consistent structure
modeling for the training. To tackle these problems, we propose a protein
folding framework called SGNet to model protein-protein interactions in
symmetrical assemblies. SGNet conducts feature extraction on a single subunit
and generates the whole assembly using our proposed symmetry module, which
largely mitigates computational problems caused by sequence length. Thanks to
the elaborate design of modeling symmetry consistently, we can model all global
symmetry types in quaternary protein structure prediction. Extensive
experimental results on a benchmark of symmetrical protein complexes further
demonstrate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2403.04395 ,  6369kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04451 (*cross-listing*)
Date: Thu, 7 Mar 2024 12:43:42 GMT   (536kb,D)

Title: Membership Inference Attacks and Privacy in Topic Modeling
Authors: Nico Manzonelli, Wanrong Zhang, Salil Vadhan
Categories: cs.CR cs.CL cs.LG
Comments: 9 pages + appendices and references. 9 figures. Submitted to USENIX
  '24
\\
  Recent research shows that large language models are susceptible to privacy
attacks that infer aspects of the training data. However, it is unclear if
simpler generative models, like topic models, share similar vulnerabilities. In
this work, we propose an attack against topic models that can confidently
identify members of the training data in Latent Dirichlet Allocation. Our
results suggest that the privacy risks associated with generative modeling are
not restricted to large neural models. Additionally, to mitigate these
vulnerabilities, we explore differentially private (DP) topic modeling. We
propose a framework for private topic modeling that incorporates DP vocabulary
selection as a pre-processing step, and show that it improves privacy while
having limited effects on practical utility.
\\ ( https://arxiv.org/abs/2403.04451 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04618 (*cross-listing*)
Date: Thu, 7 Mar 2024 16:02:31 GMT   (1587kb,D)

Title: Strong Priority and Determinacy in Timed CCS
Authors: Luigi Liquori and Michael Mendler
Categories: cs.PL cs.CL
\\
  Building on the classical theory of process algebra with priorities, we
identify a new scheduling mechanism, called "sequentially constructive
reduction" which is designed to capture the essence of synchronous programming.
The distinctive property of this evaluation strategy is to achieve
determinism-by-construction for multi-cast concurrent communication. In
particular, it permits us to model shared memory multi-threading with reaction
to absence as it lies at the core of the programming language Esterel. In the
technical setting of CCS extended by clocks and priorities, we prove for a
large class of processes, which we call "structurally coherent" the confluence
property for constructive reductions. We further show that under some syntactic
restrictions, called "pivotable" the operators of prefix, summation, parallel
composition, restriction and hiding preserve structural coherence. This covers
a strictly larger class of processes compared to those that are confluent in
Milner's classical theory of CCS without priorities.
\\ ( https://arxiv.org/abs/2403.04618 ,  1587kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04626 (*cross-listing*)
Date: Thu, 7 Mar 2024 16:11:43 GMT   (3373kb,D)

Title: MedFLIP: Medical Vision-and-Language Self-supervised Fast Pre-Training
  with Masked Autoencoder
Authors: Lei Li, Tianfang Zhang, Xinglin Zhang, Jiaqi Liu, Bingqi Ma, Yan Luo,
  Tao Chen
Categories: eess.IV cs.CL cs.CV cs.LG
\\
  Within the domain of medical analysis, extensive research has explored the
potential of mutual learning between Masked Autoencoders(MAEs) and multimodal
data. However, the impact of MAEs on intermodality remains a key challenge. We
introduce MedFLIP, a Fast Language-Image Pre-training method for Medical
analysis. We explore MAEs for zero-shot learning with crossed domains, which
enhances the model ability to learn from limited data, a common scenario in
medical diagnostics. We verify that masking an image does not affect intermodal
learning. Furthermore, we propose the SVD loss to enhance the representation
learning for characteristics of medical images, aiming to improve
classification accuracy by leveraging the structural intricacies of such data.
Lastly, we validate using language will improve the zero-shot performance for
the medical image analysis. MedFLIP scaling of the masking process marks an
advancement in the field, offering a pathway to rapid and precise medical image
analysis without the traditional computational bottlenecks. Through experiments
and validation, MedFLIP demonstrates efficient performance improvements,
setting an explored standard for future research and application in medical
diagnostics.
\\ ( https://arxiv.org/abs/2403.04626 ,  3373kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04593 (*cross-listing*)
Date: Mon, 10 Jul 2023 14:35:12 GMT   (5190kb,D)

Title: DWA: Differential Wavelet Amplifier for Image Super-Resolution
Authors: Brian B. Moser, Stanislav Frolov, Federico Raue, Sebastian Palacio and
  Andreas Dengel
Categories: eess.IV cs.AI cs.CV cs.LG
DOI: 10.1007/978-3-031-44210-0_19
\\
  This work introduces Differential Wavelet Amplifier (DWA), a drop-in module
for wavelet-based image Super-Resolution (SR). DWA invigorates an approach
recently receiving less attention, namely Discrete Wavelet Transformation
(DWT). DWT enables an efficient image representation for SR and reduces the
spatial area of its input by a factor of 4, the overall model size, and
computation cost, framing it as an attractive approach for sustainable ML. Our
proposed DWA model improves wavelet-based SR models by leveraging the
difference between two convolutional filters to refine relevant feature
extraction in the wavelet domain, emphasizing local contrasts and suppressing
common noise in the input signals. We show its effectiveness by integrating it
into existing SR models, e.g., DWSR and MWCNN, and demonstrate a clear
improvement in classical SR tasks. Moreover, DWA enables a direct application
of DWSR and MWCNN to input image space, reducing the DWT representation
channel-wise since it omits traditional DWT.
\\ ( https://arxiv.org/abs/2307.04593 ,  5190kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02741 (*cross-listing*)
Date: Tue, 5 Mar 2024 07:51:38 GMT   (554kb,D)

Title: State-Constrained Zero-Sum Differential Games with One-Sided Information
Authors: Mukesh Ghimire, Lei Zhang, Zhe Xu, Yi Ren
Categories: cs.GT cs.LG
\\
  We study zero-sum differential games with state constraints and one-sided
information, where the informed player (Player 1) has a categorical payoff type
unknown to the uninformed player (Player 2). The goal of Player 1 is to
minimize his payoff without violating the constraints, while that of Player 2
is to either violate the state constraints, or otherwise, to maximize the
payoff. One example of the game is a man-to-man matchup in football. Without
state constraints, Cardaliaguet (2007) showed that the value of such a game
exists and is convex to the common belief of players. Our theoretical
contribution is an extension of this result to differential games with state
constraints and the derivation of the primal and dual subdynamic principles
necessary for computing the behavioral strategies. Compared with existing works
on imperfect-information dynamic games that focus on scalability and
generalization, our focus is instead on revealing the mechanism of belief
manipulation behaviors resulted from information asymmetry and state
constraints. We use a simplified football game to demonstrate the utility of
this work, where we reveal player positions and belief states in which the
attacker should (or should not) play specific random fake moves to take
advantage of information asymmetry, and compute how the defender should
respond.
\\ ( https://arxiv.org/abs/2403.02741 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03960 (*cross-listing*)
Date: Thu, 29 Feb 2024 00:48:17 GMT   (956kb,D)

Title: Assessing the Extrapolation Capability of Template-Free Retrosynthesis
  Models
Authors: Shuan Chen and Yousung Jung
Categories: physics.chem-ph cs.LG
\\
  Despite the acknowledged capability of template-free models in exploring
unseen reaction spaces compared to template-based models for retrosynthesis
prediction, their ability to venture beyond established boundaries remains
relatively uncharted. In this study, we empirically assess the extrapolation
capability of state-of-the-art template-free models by meticulously assembling
an extensive set of out-of-distribution (OOD) reactions. Our findings
demonstrate that while template-free models exhibit potential in predicting
precursors with novel synthesis rules, their top-10 exact-match accuracy in OOD
reactions is strikingly modest (< 1%). Furthermore, despite the capability of
generating novel reactions, our investigation highlights a recurring issue
where more than half of the novel reactions predicted by template-free models
are chemically implausible. Consequently, we advocate for the future
development of template-free models that integrate considerations of chemical
feasibility when navigating unexplored regions of reaction space.
\\ ( https://arxiv.org/abs/2403.03960 ,  956kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03994 (*cross-listing*)
Date: Wed, 6 Mar 2024 19:08:34 GMT   (1129kb,D)

Title: Video Relationship Detection Using Mixture of Experts
Authors: Ala Shaabana and Zahra Gharaee and Paul Fieguth
Categories: cs.CV cs.LG
DOI: 10.1109/ACCESS.2023.3257280
\\
  Machine comprehension of visual information from images and videos by neural
networks faces two primary challenges. Firstly, there exists a computational
and inference gap in connecting vision and language, making it difficult to
accurately determine which object a given agent acts on and represent it
through language. Secondly, classifiers trained by a single, monolithic neural
network often lack stability and generalization. To overcome these challenges,
we introduce MoE-VRD, a novel approach to visual relationship detection
utilizing a mixture of experts. MoE-VRD identifies language triplets in the
form of < subject, predicate, object> tuples to extract relationships from
visual processing. Leveraging recent advancements in visual relationship
detection, MoE-VRD addresses the requirement for action recognition in
establishing relationships between subjects (acting) and objects (being acted
upon). In contrast to single monolithic networks, MoE-VRD employs multiple
small models as experts, whose outputs are aggregated. Each expert in MoE-VRD
specializes in visual relationship learning and object tagging. By utilizing a
sparsely-gated mixture of experts, MoE-VRD enables conditional computation and
significantly enhances neural network capacity without increasing computational
complexity. Our experimental results demonstrate that the conditional
computation capabilities and scalability of the mixture-of-experts approach
lead to superior performance in visual relationship detection compared to
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.03994 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04005 (*cross-listing*)
Date: Wed, 6 Mar 2024 19:29:08 GMT   (20431kb,D)

Title: On the Efficient Marginalization of Probabilistic Sequence Models
Authors: Alex Boyd
Categories: stat.ML cs.LG
\\
  Real-world data often exhibits sequential dependence, across diverse domains
such as human behavior, medicine, finance, and climate modeling. Probabilistic
methods capture the inherent uncertainty associated with prediction in these
contexts, with autoregressive models being especially prominent. This
dissertation focuses on using autoregressive models to answer complex
probabilistic queries that go beyond single-step prediction, such as the timing
of future events or the likelihood of a specific event occurring before
another. In particular, we develop a broad class of novel and efficient
approximation techniques for marginalization in sequential models that are
model-agnostic. These techniques rely solely on access to and sampling from
next-step conditional distributions of a pre-trained autoregressive model,
including both traditional parametric models as well as more recent neural
autoregressive models. Specific approaches are presented for discrete
sequential models, for marked temporal point processes, and for stochastic jump
processes, each tailored to a well-defined class of informative, long-range
probabilistic queries.
\\ ( https://arxiv.org/abs/2403.04005 ,  20431kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04109 (*cross-listing*)
Date: Wed, 6 Mar 2024 23:43:51 GMT   (1198kb,D)

Title: Using Causal Trees to Estimate Personalized Task Difficulty in
  Post-Stroke Individuals
Authors: Nathaniel Dennler, Stefanos Nikolaidis, and Maja Matari\'c
Categories: cs.RO cs.HC cs.LG
Comments: Accepted to the 2023 IROS Workshop on Assistive Robots for Citizens
\\
  Adaptive training programs are crucial for recovery post stroke. However,
developing programs that automatically adapt depends on quantifying how
difficult a task is for a specific individual at a particular stage of their
recovery. In this work, we propose a method that automatically generates
regions of different task difficulty levels based on an individual's
performance. We show that this technique explains the variance in user
performance for a reaching task better than previous approaches to estimating
task difficulty.
\\ ( https://arxiv.org/abs/2403.04109 ,  1198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04114 (*cross-listing*)
Date: Thu, 7 Mar 2024 00:00:02 GMT   (4678kb,D)

Title: Closing the Visual Sim-to-Real Gap with Object-Composable NeRFs
Authors: Nikhil Mishra and Maximilian Sieb and Pieter Abbeel and Xi Chen
Categories: cs.RO cs.CV cs.LG
Comments: ICRA 2024
\\
  Deep learning methods for perception are the cornerstone of many robotic
systems. Despite their potential for impressive performance, obtaining
real-world training data is expensive, and can be impractically difficult for
some tasks. Sim-to-real transfer with domain randomization offers a potential
workaround, but often requires extensive manual tuning and results in models
that are brittle to distribution shift between sim and real. In this work, we
introduce Composable Object Volume NeRF (COV-NeRF), an object-composable NeRF
model that is the centerpiece of a real-to-sim pipeline for synthesizing
training data targeted to scenes and objects from the real world. COV-NeRF
extracts objects from real images and composes them into new scenes, generating
photorealistic renderings and many types of 2D and 3D supervision, including
depth maps, segmentation masks, and meshes. We show that COV-NeRF matches the
rendering quality of modern NeRF methods, and can be used to rapidly close the
sim-to-real gap across a variety of perceptual modalities.
\\ ( https://arxiv.org/abs/2403.04114 ,  4678kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04118 (*cross-listing*)
Date: Thu, 7 Mar 2024 00:20:11 GMT   (8061kb,D)

Title: Globally Stable Neural Imitation Policies
Authors: Amin Abyaneh, Mariana Sosa Guzm\'an, Hsiu-Chin Lin
Categories: cs.RO cs.LG
\\
  Imitation learning presents an effective approach to alleviate the
resource-intensive and time-consuming nature of policy learning from scratch in
the solution space. Even though the resulting policy can mimic expert
demonstrations reliably, it often lacks predictability in unexplored regions of
the state-space, giving rise to significant safety concerns in the face of
perturbations. To address these challenges, we introduce the Stable Neural
Dynamical System (SNDS), an imitation learning regime which produces a policy
with formal stability guarantees. We deploy a neural policy architecture that
facilitates the representation of stability based on Lyapunov theorem, and
jointly train the policy and its corresponding Lyapunov candidate to ensure
global stability. We validate our approach by conducting extensive experiments
in simulation and successfully deploying the trained policies on a real-world
manipulator arm. The experimental results demonstrate that our method overcomes
the instability, accuracy, and computational intensity problems associated with
previous imitation learning methods, making our method a promising solution for
stable policy learning in complex planning scenarios.
\\ ( https://arxiv.org/abs/2403.04118 ,  8061kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04144 (*cross-listing*)
Date: Thu, 7 Mar 2024 01:50:36 GMT   (167kb,D)

Title: FedClust: Optimizing Federated Learning on Non-IID Data through
  Weight-Driven Client Clustering
Authors: Md Sirajul Islam, Simin Javaherian, Fei Xu, Xu Yuan, Li Chen,
  Nian-Feng Tzeng
Categories: cs.DC cs.LG
\\
  Federated learning (FL) is an emerging distributed machine learning paradigm
enabling collaborative model training on decentralized devices without exposing
their local data. A key challenge in FL is the uneven data distribution across
client devices, violating the well-known assumption of
independent-and-identically-distributed (IID) training samples in conventional
machine learning. Clustered federated learning (CFL) addresses this challenge
by grouping clients based on the similarity of their data distributions.
However, existing CFL approaches require a large number of communication rounds
for stable cluster formation and rely on a predefined number of clusters, thus
limiting their flexibility and adaptability. This paper proposes FedClust, a
novel CFL approach leveraging correlations between local model weights and
client data distributions. FedClust groups clients into clusters in a one-shot
manner using strategically selected partial model weights and dynamically
accommodates newcomers in real-time. Experimental results demonstrate FedClust
outperforms baseline approaches in terms of accuracy and communication costs.
\\ ( https://arxiv.org/abs/2403.04144 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04189 (*cross-listing*)
Date: Thu, 7 Mar 2024 03:38:35 GMT   (783kb)

Title: Silicon Photonic 2.5D Interposer Networks for Overcoming Communication
  Bottlenecks in Scale-out Machine Learning Hardware Accelerators
Authors: Febin Sunny, Ebadollah Taheri, Mahdi Nikdast, Sudeep Pasricha
Categories: cs.AR cs.LG eess.SP
\\
  Modern machine learning (ML) applications are becoming increasingly complex
and monolithic (single chip) accelerator architectures cannot keep up with
their energy efficiency and throughput demands. Even though modern digital
electronic accelerators are gradually adopting 2.5D architectures with multiple
smaller chiplets to improve scalability, they face fundamental limitations due
to a reliance on slow metallic interconnects. This paper outlines how optical
communication and computation can be leveraged in 2.5D platforms to realize
energy-efficient and high throughput 2.5D ML accelerator architectures.
\\ ( https://arxiv.org/abs/2403.04189 ,  783kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04234 (*cross-listing*)
Date: Thu, 7 Mar 2024 05:26:52 GMT   (87kb,D)

Title: Fundamental limits of Non-Linear Low-Rank Matrix Estimation
Authors: Pierre Mergny, Justin Ko, Florent Krzakala, Lenka Zdeborov\'a
Categories: stat.ML cs.LG
Comments: 42 pages, 2 figures
\\
  We consider the task of estimating a low-rank matrix from non-linear and
noisy observations. We prove a strong universality result showing that
Bayes-optimal performances are characterized by an equivalent Gaussian model
with an effective prior, whose parameters are entirely determined by an
expansion of the non-linear function. In particular, we show that to
reconstruct the signal accurately, one requires a signal-to-noise ratio growing
as $N^{\frac 12 (1-1/k_F)}$, where $k_F$ is the first non-zero Fisher
information coefficient of the function. We provide asymptotic characterization
for the minimal achievable mean squared error (MMSE) and an approximate
message-passing algorithm that reaches the MMSE under conditions analogous to
the linear version of the problem. We also provide asymptotic errors achieved
by methods such as principal component analysis combined with Bayesian
denoising, and compare them with Bayes-optimal MMSE.
\\ ( https://arxiv.org/abs/2403.04234 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04245 (*cross-listing*)
Date: Thu, 7 Mar 2024 06:06:55 GMT   (4262kb,D)

Title: A Study of Dropout-Induced Modality Bias on Robustness to Missing Video
  Frames for Audio-Visual Speech Recognition
Authors: Yusheng Dai, Hang Chen, Jun Du, Ruoyu Wang, Shihao Chen, Jiefeng Ma,
  Haotian Wang, Chin-Hui Lee
Categories: cs.SD cs.CV cs.LG cs.MM eess.AS
Comments: the paper is accepted by CVPR2024
\\
  Advanced Audio-Visual Speech Recognition (AVSR) systems have been observed to
be sensitive to missing video frames, performing even worse than
single-modality models. While applying the dropout technique to the video
modality enhances robustness to missing frames, it simultaneously results in a
performance loss when dealing with complete data input. In this paper, we
investigate this contrasting phenomenon from the perspective of modality bias
and reveal that an excessive modality bias on the audio caused by dropout is
the underlying reason. Moreover, we present the Modality Bias Hypothesis (MBH)
to systematically describe the relationship between modality bias and
robustness against missing modality in multimodal systems. Building on these
findings, we propose a novel Multimodal Distribution Approximation with
Knowledge Distillation (MDA-KD) framework to reduce over-reliance on the audio
modality and to maintain performance and robustness simultaneously. Finally, to
address an entirely missing modality, we adopt adapters to dynamically switch
decision strategies. The effectiveness of our proposed approach is evaluated
and validated through a series of comprehensive experiments using the MISP2021
and MISP2022 datasets. Our code is available at
https://github.com/dalision/ModalBiasAVSR
\\ ( https://arxiv.org/abs/2403.04245 ,  4262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04259 (*cross-listing*)
Date: Thu, 7 Mar 2024 06:47:45 GMT   (877kb,D)

Title: Decentralized and Equitable Optimal Transport
Authors: Ivan Lau, Shiqian Ma, C\'esar A. Uribe
Categories: math.OC cs.LG
\\
  This paper considers the decentralized (discrete) optimal transport (D-OT)
problem. In this setting, a network of agents seeks to design a transportation
plan jointly, where the cost function is the sum of privately held costs for
each agent. We reformulate the D-OT problem as a constraint-coupled
optimization problem and propose a single-loop decentralized algorithm with an
iteration complexity of O(1/{\epsilon}) that matches existing centralized
first-order approaches. Moreover, we propose the decentralized equitable
optimal transport (DE-OT) problem. In DE-OT, in addition to cooperatively
designing a transportation plan that minimizes transportation costs, agents
seek to ensure equity in their individual costs. The iteration complexity of
the proposed method to solve DE-OT is also O(1/{\epsilon}). This rate improves
existing centralized algorithms, where the best iteration complexity obtained
is O(1/{\epsilon}^2).
\\ ( https://arxiv.org/abs/2403.04259 ,  877kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04268 (*cross-listing*)
Date: Thu, 7 Mar 2024 07:08:57 GMT   (750kb)

Title: Qubit-Wise Architecture Search Method for Variational Quantum Circuits
Authors: Jialin Chen, Zhiqiang Cai, Ke Xu, Di Wu, Wei Cao
Categories: quant-ph cs.LG
\\
  Considering the noise level limit, one crucial aspect for quantum machine
learning is to design a high-performing variational quantum circuit
architecture with small number of quantum gates. As the classical neural
architecture search (NAS), quantum architecture search methods (QAS) employ
methods like reinforcement learning, evolutionary algorithms and supernet
optimiza-tion to improve the search efficiency. In this paper, we propose a
novel qubit-wise architec-ture search (QWAS) method, which progres-sively
search one-qubit configuration per stage, and combine with Monte Carlo Tree
Search al-gorithm to find good quantum architectures by partitioning the search
space into several good and bad subregions. The numerical experimental results
indicate that our proposed method can balance the exploration and exploitation
of cir-cuit performance and size in some real-world tasks, such as MNIST,
Fashion and MOSI. As far as we know, QWAS achieves the state-of-art re-sults of
all tasks in the terms of accuracy and circuit size.
\\ ( https://arxiv.org/abs/2403.04268 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04290 (*cross-listing*)
Date: Thu, 7 Mar 2024 07:39:00 GMT   (5776kb,D)

Title: MedM2G: Unifying Medical Multi-Modal Generation via Cross-Guided
  Diffusion with Visual Invariant
Authors: Chenlu Zhan, Yu Lin, Gaoang Wang, Hongwei Wang, Jian Wu
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by CVPR2024
\\
  Medical generative models, acknowledged for their high-quality sample
generation ability, have accelerated the fast growth of medical applications.
However, recent works concentrate on separate medical generation models for
distinct medical tasks and are restricted to inadequate medical multi-modal
knowledge, constraining medical comprehensive diagnosis. In this paper, we
propose MedM2G, a Medical Multi-Modal Generative framework, with the key
innovation to align, extract, and generate medical multi-modal within a unified
model. Extending beyond single or two medical modalities, we efficiently align
medical multi-modal through the central alignment approach in the unified
space. Significantly, our framework extracts valuable clinical knowledge by
preserving the medical visual invariant of each imaging modal, thereby
enhancing specific medical information for multi-modal generation. By
conditioning the adaptive cross-guided parameters into the multi-flow diffusion
framework, our model promotes flexible interactions among medical multi-modal
for generation. MedM2G is the first medical generative model that unifies
medical generation tasks of text-to-image, image-to-text, and unified
generation of medical modalities (CT, MRI, X-ray). It performs 5 medical
generation tasks across 10 datasets, consistently outperforming various
state-of-the-art works.
\\ ( https://arxiv.org/abs/2403.04290 ,  5776kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04322 (*cross-listing*)
Date: Thu, 7 Mar 2024 08:37:36 GMT   (4182kb,D)

Title: Memetic Differential Evolution Methods for Semi-Supervised Clustering
Authors: Pierluigi Mansueto, Fabio Schoen
Categories: math.OC cs.LG cs.NE
MSC-class: 90C11, 90C30, 90C59
\\
  In this paper, we deal with semi-supervised Minimum Sum-of-Squares Clustering
(MSSC) problems where background knowledge is given in the form of
instance-level constraints. In particular, we take into account "must-link" and
"cannot-link" constraints, each of which indicates if two dataset points should
be associated to the same or to a different cluster. The presence of such
constraints makes the problem at least as hard as its unsupervised version: it
is no more true that each point is associated to its nearest cluster center,
thus requiring some modifications in crucial operations, such as the assignment
step. In this scenario, we propose a novel memetic strategy based on the
Differential Evolution paradigm, directly extending a state-of-the-art
framework recently proposed in the unsupervised clustering literature. As far
as we know, our contribution represents the first attempt to define a memetic
methodology designed to generate a (hopefully) optimal feasible solution for
the semi-supervised MSSC problem. The proposal is compared with some
state-of-the-art algorithms from the literature on a set of well-known
datasets, highlighting its effectiveness and efficiency in finding good quality
clustering solutions.
\\ ( https://arxiv.org/abs/2403.04322 ,  4182kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04329 (*cross-listing*)
Date: Thu, 7 Mar 2024 08:48:42 GMT   (1185kb,D)

Title: A mechanism-informed reinforcement learning framework for shape
  optimization of airfoils
Authors: Jingfeng Wang and Guanghui Hu
Categories: math.NA cs.CE cs.LG cs.NA
Comments: 25 pages
\\
  In this study, we present the mechanism-informed reinforcement learning
framework for airfoil shape optimization. By leveraging the twin delayed deep
deterministic policy gradient algorithm for its notable stability, our approach
addresses the complexities of optimizing shapes governed by fluid dynamics. The
PDEs-based solver is adopted for its accuracy even when the configurations and
geometries are extraordinarily changed during the exploration. Dual-weighted
residual-based mesh refinement strategy is applied to ensure the accurate
calculation of target functionals. To streamline the iterative optimization
process and handle geometric deformations, our approach integrates Laplacian
smoothing, adaptive refinement, and a B\'ezier fitting strategy. This
combination not only remits mesh tangling but also guarantees a precise
manipulation of the airfoil geometry. Our neural network architecture leverages
B\'ezier curves for efficient dimensionality reduction, thereby enhancing the
learning process and ensuring the geometric accuracy of the airfoil shapes. An
attention mechanism is embedded within the network to calculate potential
action on the state as well. Furthermore, we have introduced different reward
and penalty mechanisms tailored to the specific challenges of airfoil
optimization. This algorithm is designed to support the optimization task,
facilitating a more targeted and effective approach for airfoil shape
optimization.
\\ ( https://arxiv.org/abs/2403.04329 ,  1185kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04344 (*cross-listing*)
Date: Thu, 7 Mar 2024 09:12:23 GMT   (440kb,D)

Title: RL-CFR: Improving Action Abstraction for Imperfect Information
  Extensive-Form Games with Reinforcement Learning
Authors: Boning Li, Zhixuan Fang and Longbo Huang
Categories: cs.GT cs.LG
\\
  Effective action abstraction is crucial in tackling challenges associated
with large action spaces in Imperfect Information Extensive-Form Games
(IIEFGs). However, due to the vast state space and computational complexity in
IIEFGs, existing methods often rely on fixed abstractions, resulting in
sub-optimal performance. In response, we introduce RL-CFR, a novel
reinforcement learning (RL) approach for dynamic action abstraction. RL-CFR
builds upon our innovative Markov Decision Process (MDP) formulation, with
states corresponding to public information and actions represented as feature
vectors indicating specific action abstractions. The reward is defined as the
expected payoff difference between the selected and default action
abstractions. RL-CFR constructs a game tree with RL-guided action abstractions
and utilizes counterfactual regret minimization (CFR) for strategy derivation.
Impressively, it can be trained from scratch, achieving higher expected payoff
without increased CFR solving time. In experiments on Heads-up No-limit Texas
Hold'em, RL-CFR outperforms ReBeL's replication and Slumbot, demonstrating
significant win-rate margins of $64\pm 11$ and $84\pm 17$ mbb/hand,
respectively.
\\ ( https://arxiv.org/abs/2403.04344 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04348 (*cross-listing*)
Date: Thu, 7 Mar 2024 09:22:50 GMT   (978kb,D)

Title: LoCoDL: Communication-Efficient Distributed Learning with Local Training
  and Compression
Authors: Laurent Condat, Artavazd Maranjyan, Peter Richt\'arik
Categories: math.OC cs.DC cs.LG
\\
  In Distributed optimization and Learning, and even more in the modern
framework of federated learning, communication, which is slow and costly, is
critical. We introduce LoCoDL, a communication-efficient algorithm that
leverages the two popular and effective techniques of Local training, which
reduces the communication frequency, and Compression, in which short bitstreams
are sent instead of full-dimensional vectors of floats. LoCoDL works with a
large class of unbiased compressors that includes widely-used sparsification
and quantization methods. LoCoDL provably benefits from local training and
compression and enjoys a doubly-accelerated communication complexity, with
respect to the condition number of the functions and the model dimension, in
the general heterogenous regime with strongly convex functions. This is
confirmed in practice, with LoCoDL outperforming existing algorithms.
\\ ( https://arxiv.org/abs/2403.04348 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04385 (*cross-listing*)
Date: Thu, 7 Mar 2024 10:25:23 GMT   (47088kb,D)

Title: Impacts of Color and Texture Distortions on Earth Observation Data in
  Deep Learning
Authors: Martin Willbo, Aleksis Pirinen, John Martinsson, Edvin Listo Zec, Olof
  Mogren, Mikael Nilsson
Categories: cs.CV cs.LG
\\
  Land cover classification and change detection are two important applications
of remote sensing and Earth observation (EO) that have benefited greatly from
the advances of deep learning. Convolutional and transformer-based U-net models
are the state-of-the-art architectures for these tasks, and their performances
have been boosted by an increased availability of large-scale annotated EO
datasets. However, the influence of different visual characteristics of the
input EO data on a model's predictions is not well understood. In this work we
systematically examine model sensitivities with respect to several color- and
texture-based distortions on the input EO data during inference, given models
that have been trained without such distortions. We conduct experiments with
multiple state-of-the-art segmentation networks for land cover classification
and show that they are in general more sensitive to texture than to color
distortions. Beyond revealing intriguing characteristics of widely used land
cover classification models, our results can also be used to guide the
development of more robust models within the EO domain.
\\ ( https://arxiv.org/abs/2403.04385 ,  47088kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04405 (*cross-listing*)
Date: Thu, 7 Mar 2024 11:00:35 GMT   (586kb,D)

Title: Signature Isolation Forest
Authors: Guillaume Staerman, Marta Campi, Gareth W. Peters
Categories: stat.ML cs.LG
\\
  Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly
Detection (AD) algorithm designed for functional data. It relies on a tree
partition procedure where an abnormality score is computed by projecting each
curve observation on a drawn dictionary through a linear inner product. Such
linear inner product and the dictionary are a priori choices that highly
influence the algorithm's performances and might lead to unreliable results,
particularly with complex datasets. This work addresses these challenges by
introducing \textit{Signature Isolation Forest}, a novel AD algorithm class
leveraging the rough path theory's signature transform. Our objective is to
remove the constraints imposed by FIF through the proposition of two algorithms
which specifically target the linearity of the FIF inner product and the choice
of the dictionary. We provide several numerical experiments, including a
real-world applications benchmark showing the relevance of our methods.
\\ ( https://arxiv.org/abs/2403.04405 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04484 (*cross-listing*)
Date: Thu, 7 Mar 2024 13:36:15 GMT   (2447kb,D)

Title: Source Matters: Source Dataset Impact on Model Robustness in Medical
  Imaging
Authors: Dovile Juodelyte, Yucheng Lu, Amelia Jim\'enez-S\'anchez, Sabrina
  Bottazzi, Enzo Ferrante, Veronika Cheplygina
Categories: cs.CV cs.LG
Comments: Submitted to MICCAI 2024
\\
  Transfer learning has become an essential part of medical imaging
classification algorithms, often leveraging ImageNet weights. However, the
domain shift from natural to medical images has prompted alternatives such as
RadImageNet, often demonstrating comparable classification performance.
However, it remains unclear whether the performance gains from transfer
learning stem from improved generalization or shortcut learning. To address
this, we investigate potential confounders -- whether synthetic or sampled from
the data -- across two publicly available chest X-ray and CT datasets. We show
that ImageNet and RadImageNet achieve comparable classification performance,
yet ImageNet is much more prone to overfitting to confounders. We recommend
that researchers using ImageNet-pretrained models reexamine their model
robustness by conducting similar experiments. Our code and experiments are
available at https://github.com/DovileDo/source-matters.
\\ ( https://arxiv.org/abs/2403.04484 ,  2447kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04553 (*cross-listing*)
Date: Thu, 7 Mar 2024 14:48:48 GMT   (221kb,D)

Title: Improvements & Evaluations on the MLCommons CloudMask Benchmark
Authors: Varshitha Chennamsetti, Laiba Mehnaz, Dan Zhao, Banani Ghosh, Sergey
  V. Samsonau
Categories: cs.DC cs.LG
Comments: arXiv admin note: text overlap with arXiv:2401.08636
\\
  In this paper, we report the performance benchmarking results of deep
learning models on MLCommons' Science cloud-masking benchmark using a
high-performance computing cluster at New York University (NYU): NYU Greene.
MLCommons is a consortium that develops and maintains several scientific
benchmarks that can benefit from developments in AI. We provide a description
of the cloud-masking benchmark task, updated code, and the best model for this
benchmark when using our selected hyperparameter settings. Our benchmarking
results include the highest accuracy achieved on the NYU system as well as the
average time taken for both training and inference on the benchmark across
several runs/seeds. Our code can be found on GitHub. MLCommons team has been
kept informed about our progress and may use the developed code for their
future work.
\\ ( https://arxiv.org/abs/2403.04553 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04586 (*cross-listing*)
Date: Thu, 7 Mar 2024 15:30:54 GMT   (18256kb,D)

Title: Learning Agility Adaptation for Flight in Clutter
Authors: Guangyu Zhao, Tianyue Wu, Yeke Chen and Fei Gao
Categories: cs.RO cs.LG
Comments: Submission to Robotics and Automation Letter. 8 pages, 11 figures.
  Project page: https://learning-agility-adaptation.github.io/
\\
  Animals learn to adapt agility of their movements to their capabilities and
the environment they operate in. Mobile robots should also demonstrate this
ability to combine agility and safety. The aim of this work is to endow flight
vehicles with the ability of agility adaptation in prior unknown and partially
observable cluttered environments. We propose a hierarchical learning and
planning framework where we utilize both trial and error to comprehensively
learn an agility policy with the vehicle's observation as the input, and
well-established methods of model-based trajectory generation. Technically, we
use online model-free reinforcement learning and a pre-training-fine-tuning
reward scheme to obtain the deployable policy. The statistical results in
simulation demonstrate the advantages of our method over the constant agility
baselines and an alternative method in terms of flight efficiency and safety.
In particular, the policy leads to intelligent behaviors, such as perception
awareness, which distinguish it from other approaches. By deploying the policy
to hardware, we verify that these advantages can be brought to the real world.
\\ ( https://arxiv.org/abs/2403.04586 ,  18256kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04661 (*cross-listing*)
Date: Thu, 7 Mar 2024 17:07:51 GMT   (214kb,D)

Title: Dynamic Cross Attention for Audio-Visual Person Verification
Authors: R. Gnana Praveen, Jahangir Alam
Categories: cs.CV cs.LG cs.SD eess.AS
Comments: Accepted to FG2024
\\
  Although person or identity verification has been predominantly explored
using individual modalities such as face and voice, audio-visual fusion has
recently shown immense potential to outperform unimodal approaches. Audio and
visual modalities are often expected to pose strong complementary
relationships, which plays a crucial role in effective audio-visual fusion.
However, they may not always strongly complement each other, they may also
exhibit weak complementary relationships, resulting in poor audio-visual
feature representations. In this paper, we propose a Dynamic Cross-Attention
(DCA) model that can dynamically select the cross-attended or unattended
features on the fly based on the strong or weak complementary relationships,
respectively, across audio and visual modalities. In particular, a conditional
gating layer is designed to evaluate the contribution of the cross-attention
mechanism and choose cross-attended features only when they exhibit strong
complementary relationships, otherwise unattended features. Extensive
experiments are conducted on the Voxceleb1 dataset to demonstrate the
robustness of the proposed model. Results indicate that the proposed model
consistently improves the performance on multiple variants of cross-attention
while outperforming the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.04661 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04726 (*cross-listing*)
Date: Thu, 7 Mar 2024 18:23:51 GMT   (46kb,D)

Title: A Sub-Quadratic Time Algorithm for Robust Sparse Mean Estimation
Authors: Ankit Pensia
Categories: cs.DS cs.LG math.ST stat.ML stat.TH
\\
  We study the algorithmic problem of sparse mean estimation in the presence of
adversarial outliers. Specifically, the algorithm observes a \emph{corrupted}
set of samples from $\mathcal{N}(\mu,\mathbf{I}_d)$, where the unknown mean
$\mu \in \mathbb{R}^d$ is constrained to be $k$-sparse. A series of prior works
has developed efficient algorithms for robust sparse mean estimation with
sample complexity $\mathrm{poly}(k,\log d, 1/\epsilon)$ and runtime $d^2
\mathrm{poly}(k,\log d,1/\epsilon)$, where $\epsilon$ is the fraction of
contamination. In particular, the fastest runtime of existing algorithms is
quadratic ($\Omega(d^2)$), which can be prohibitive in high dimensions. This
quadratic barrier in the runtime stems from the reliance of these algorithms on
the sample covariance matrix, which is of size $d^2$. Our main contribution is
an algorithm for robust sparse mean estimation which runs in
\emph{subquadratic} time using $\mathrm{poly}(k,\log d,1/\epsilon)$ samples. We
also provide analogous results for robust sparse PCA. Our results build on
algorithmic advances in detecting weak correlations, a generalized version of
the light-bulb problem by Valiant.
\\ ( https://arxiv.org/abs/2403.04726 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04750 (*cross-listing*)
Date: Thu, 7 Mar 2024 18:53:53 GMT   (6316kb,D)

Title: JAX-SPH: A Differentiable Smoothed Particle Hydrodynamics Framework
Authors: Artur P. Toshev, Harish Ramachandran, Jonas A. Erbesdobler, Gianluca
  Galletti, Johannes Brandstetter, Nikolaus A. Adams
Categories: physics.flu-dyn cs.LG
Comments: Accepted at the ICLR 2024 Workshop on AI4Differential Equations In
  Science
\\
  Particle-based fluid simulations have emerged as a powerful tool for solving
the Navier-Stokes equations, especially in cases that include intricate physics
and free surfaces. The recent addition of machine learning methods to the
toolbox for solving such problems is pushing the boundary of the quality vs.
speed tradeoff of such numerical simulations. In this work, we lead the way to
Lagrangian fluid simulators compatible with deep learning frameworks, and
propose JAX-SPH - a Smoothed Particle Hydrodynamics (SPH) framework implemented
in JAX. JAX-SPH builds on the code for dataset generation from the
LagrangeBench project (Toshev et al., 2023) and extends this code in multiple
ways: (a) integration of further key SPH algorithms, (b) restructuring the code
toward a Python library, (c) verification of the gradients through the solver,
and (d) demonstration of the utility of the gradients for solving inverse
problems as well as a Solver-in-the-Loop application. Our code is available at
https://github.com/tumaer/jax-sph.
\\ ( https://arxiv.org/abs/2403.04750 ,  6316kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2308.06528
replaced with revised version Thu, 7 Mar 2024 18:17:02 GMT   (2066kb,D)

Title: Learning Abstract Visual Reasoning via Task Decomposition: A Case Study
  in Raven Progressive Matrices
Authors: Jakub Kwiatkowski and Krzysztof Krawiec
Categories: cs.AI cs.CV cs.LG
Comments: 22 pages, 10 figures
MSC-class: 68T20 (Primary) 68T05 (Secondary)
ACM-class: I.2.8; I.2.6; I.2.10
\\ ( https://arxiv.org/abs/2308.06528 ,  2066kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04235
replaced with revised version Thu, 7 Mar 2024 10:18:42 GMT   (868kb,D)

Title: Can LLMs Follow Simple Rules?
Authors: Norman Mu, Sarah Chen, Zifan Wang, Sizhe Chen, David Karamardian,
  Lulwa Aljeraisy, Basel Alomair, Dan Hendrycks, David Wagner
Categories: cs.AI cs.CL cs.LG
Comments: Project website: https://eecs.berkeley.edu/~normanmu/llm_rules;
  revised content
\\ ( https://arxiv.org/abs/2311.04235 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14625
replaced with revised version Wed, 6 Mar 2024 22:15:48 GMT   (370kb,D)

Title: Multi-Agent Reinforcement Learning for Assessing False-Data Injection
  Attacks on Transportation Networks
Authors: Taha Eghtesad, Sirui Li, Yevgeniy Vorobeychik, Aron Laszka
Categories: cs.AI cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.14625 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04154
replaced with revised version Thu, 7 Mar 2024 02:34:46 GMT   (1492kb,D)

Title: Read to Play (R2-Play): Decision Transformer with Multimodal Game
  Instruction
Authors: Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu
  Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He and Jie Fu
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.04154 ,  1492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08957
replaced with revised version Thu, 7 Mar 2024 13:02:58 GMT   (6066kb,D)

Title: MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data
Authors: Yinya Huang, Xiaohan Lin, Zhengying Liu, Qingxing Cao, Huajian Xin,
  Haiming Wang, Zhenguo Li, Linqi Song, Xiaodan Liang
Categories: cs.AI cs.CL cs.FL cs.LG cs.PL
Journal-ref: ICLR 2024 spotlight
\\ ( https://arxiv.org/abs/2402.08957 ,  6066kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09565
replaced with revised version Wed, 6 Mar 2024 22:22:33 GMT   (7055kb,D)

Title: Graph-Skeleton: ~1% Nodes are Sufficient to Represent Billion-Scale
  Graph
Authors: Linfeng Cao, Haoran Deng, Yang Yang, Chunping Wang, Lei Chen
Categories: cs.AI
Comments: 21 pages, 11 figures, In Proceedings of the ACM Web Conference 2024
  (WWW'24)
DOI: 10.1145/3589334.3645452
\\ ( https://arxiv.org/abs/2402.09565 ,  7055kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03186
replaced with revised version Thu, 7 Mar 2024 14:41:56 GMT   (18505kb,D)

Title: Towards General Computer Control: A Multimodal Agent for Red Dead
  Redemption II as a Case Study
Authors: Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng
  Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, Yifei Bi,
  Pengjie Gu, Xinrun Wang, B\"orje F. Karlsson, Bo An, Zongqing Lu
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.03186 ,  18505kb)
------------------------------------------------------------------------------
\\
arXiv:2209.01621
replaced with revised version Wed, 6 Mar 2024 21:25:38 GMT   (444kb,D)

Title: Interactive Question Answering Systems: Literature Review
Authors: Giovanni Maria Biancofiore, Yashar Deldjoo, Tommaso Di Noia, Eugenio
  Di Sciascio, Fedelucio Narducci
Categories: cs.CL cs.AI cs.HC cs.IR
Comments: 37 pages, 2 Figures, 6 Tables, submitted to ACM
ACM-class: A.1; H.3.3; I.2.7
\\ ( https://arxiv.org/abs/2209.01621 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11520
replaced with revised version Thu, 7 Mar 2024 00:42:17 GMT   (5689kb,D)

Title: Processing Natural Language on Embedded Devices: How Well Do Transformer
  Models Perform?
Authors: Souvika Sarkar, Mohammad Fakhruddin Babar, Md Mahadi Hassan, Monowar
  Hasan, and Shubhra Kanti Karmaker Santu
Categories: cs.CL cs.SY eess.SY
Journal-ref: ICPE 2024
DOI: 10.1145/3629526.3645054
\\ ( https://arxiv.org/abs/2304.11520 ,  5689kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13733
replaced with revised version Thu, 7 Mar 2024 03:11:47 GMT   (1564kb,D)

Title: Enhancing Large Language Models Against Inductive Instructions with
  Dual-critique Prompting
Authors: Rui Wang, Hongru Wang, Fei Mi, Yi Chen, Boyang Xue, Kam-Fai Wong,
  Ruifeng Xu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.13733 ,  1564kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16344
replaced with revised version Thu, 7 Mar 2024 13:44:27 GMT   (606kb,D)

Title: Enabling and Analyzing How to Efficiently Extract Information from
  Hybrid Long Documents with LLMs
Authors: Chongjian Yue, Xinrun Xu, Xiaojun Ma, Lun Du, Hengyu Liu, Zhiming
  Ding, Yanbing Jiang, Shi Han, Dongmei Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2305.16344 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12679
replaced with revised version Thu, 7 Mar 2024 04:25:50 GMT   (813kb)

Title: Constructing Colloquial Dataset for Persian Sentiment Analysis of Social
  Microblogs
Authors: Mojtaba Mazoochi (ICT Research Institute, Tehran, Iran), Leila Rabiei
  (Iran Telecommunication Research Center (ITRC), Tehran, Iran), Farzaneh
  Rahmani (Computer Department, Mehralborz University, Tehran, Iran), Zeinab
  Rajabi (Computer Department, Hazrat-e Masoumeh University, Qom, Iran)
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.12679 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01715
replaced with revised version Thu, 7 Mar 2024 17:59:25 GMT   (3058kb,D)

Title: Align With Purpose: Optimize Desired Properties in CTC Models with a
  General Plug-and-Play Framework
Authors: Eliya Segev, Maya Alroy, Ronen Katsir, Noam Wies, Ayana Shenhav, Yael
  Ben-Oren, David Zar, Oren Tadmor, Jacob Bitterman, Amnon Shashua and Tal
  Rosenwein
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2307.01715 ,  3058kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05196
replaced with revised version Wed, 6 Mar 2024 20:48:40 GMT   (1260kb,D)

Title: Does Writing with Language Models Reduce Content Diversity?
Authors: Vishakh Padmakumar, He He
Categories: cs.CL cs.CY cs.HC cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2309.05196 ,  1260kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06553
replaced with revised version Thu, 7 Mar 2024 16:12:21 GMT   (13861kb,D)

Title: Query-Dependent Prompt Evaluation and Optimization with Offline Inverse
  RL
Authors: Hao Sun, Alihan H\"uy\"uk, Mihaela van der Schaar
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2309.06553 ,  13861kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06698
replaced with revised version Wed, 6 Mar 2024 20:05:37 GMT   (7744kb,D)

Title: Benchmarking Procedural Language Understanding for Low-Resource
  Languages: A Case Study on Turkish
Authors: Arda Uzunoglu and G\"ozde G\"ul \c{S}ahin
Categories: cs.CL
Comments: 9 pages
\\ ( https://arxiv.org/abs/2309.06698 ,  7744kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07445
replaced with revised version Thu, 7 Mar 2024 13:16:08 GMT   (8742kb,D)

Title: SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic
  Classification in 200+ Languages and Dialects
Authors: David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev,
  Jesujoba O. Alabi, Yanke Mao, Haonan Gao, Annie En-Shiun Lee
Categories: cs.CL
Comments: Accepted to EACL 2024 (main conference)
\\ ( https://arxiv.org/abs/2309.07445 ,  8742kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11143
replaced with revised version Thu, 7 Mar 2024 12:40:21 GMT   (402kb,D)

Title: CoT-BERT: Enhancing Unsupervised Sentence Representation through
  Chain-of-Thought
Authors: Bowen Zhang, Kehua Chang, Chunping Li
Categories: cs.CL cs.AI
Comments: Work in Progress
\\ ( https://arxiv.org/abs/2309.11143 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16540
replaced with revised version Wed, 6 Mar 2024 20:12:01 GMT   (609kb,D)

Title: Unsupervised Pretraining for Fact Verification by Language Model
  Distillation
Authors: Adri\'an Bazaga and Pietro Li\`o and Gos Micklem
Categories: cs.CL cs.LG stat.ML
Comments: ICLR 2024 Camera Ready
\\ ( https://arxiv.org/abs/2309.16540 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07793
replaced with revised version Thu, 7 Mar 2024 17:43:30 GMT   (3859kb,D)

Title: GenTKG: Generative Forecasting on Temporal Knowledge Graph
Authors: Ruotong Liao, Xu Jia, Yunpu Ma, Yangzhe Li, Volker Tresp
Categories: cs.CL cs.AI cs.LG
Comments: 14 pages, Spotlight of Temporal Graph Learning @ NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.07793 ,  3859kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14985
replaced with revised version Thu, 7 Mar 2024 08:41:58 GMT   (4982kb,D)

Title: LLM-Based Agent Society Investigation: Collaboration and Confrontation
  in Avalon Gameplay
Authors: Yihuai Lan, Zhiqiang Hu, Lei Wang, Yang Wang, Deheng Ye, Peilin Zhao,
  Ee-Peng Lim, Hui Xiong, Hao Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.14985 ,  4982kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00176
replaced with revised version Thu, 7 Mar 2024 01:10:43 GMT   (1650kb,D)

Title: ChipNeMo: Domain-Adapted LLMs for Chip Design
Authors: Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel
  Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee,
  Ismet Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri,
  Sharon Clay, Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi,
  Sameer Halepete, Eric Hill, Jiashang Hu, Sumit Jain, Ankit Jindal, Brucek
  Khailany, George Kokai, Kishor Kunal, Xiaowei Li, Charley Lind, Hao Liu,
  Stuart Oberman, Sujeet Omar, Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar,
  Zhengjiang Shao, Hanfei Sun, Pratik P Suthar, Varun Tej, Walker Turner,
  Kaizhe Xu, Haoxing Ren
Categories: cs.CL
Comments: Updated results for ChipNeMo-70B model
\\ ( https://arxiv.org/abs/2311.00176 ,  1650kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09796
replaced with revised version Thu, 7 Mar 2024 16:49:07 GMT   (189kb,D)

Title: Interpreting User Requests in the Context of Natural Language Standing
  Instructions
Authors: Nikita Moghe and Patrick Xia and Jacob Andreas and Jason Eisner and
  Benjamin Van Durme and Harsh Jhamtani
Categories: cs.CL cs.AI
Comments: Updated with results from LLaMA-2
\\ ( https://arxiv.org/abs/2311.09796 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06681
replaced with revised version Thu, 7 Mar 2024 02:06:00 GMT   (10693kb,D)

Title: Steering Llama 2 via Contrastive Activation Addition
Authors: Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger,
  Alexander Matt Turner
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.06681 ,  10693kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01283
replaced with revised version Thu, 7 Mar 2024 14:36:07 GMT   (120kb,D)

Title: Quality and Quantity of Machine Translation References for Automatic
  Metrics
Authors: Vil\'em Zouhar, Ond\v{r}ej Bojar
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.01283 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03223
replaced with revised version Thu, 7 Mar 2024 17:18:18 GMT   (207kb,D)

Title: English Prompts are Better for NLI-based Zero-Shot Emotion
  Classification than Target-Language Prompts
Authors: Patrick Barei{\ss} and Roman Klinger and Jeremy Barnes
Categories: cs.CL
Comments: published at the PromptEng workshop at TheWebConf
DOI: 10.1145/3589335.3651902
\\ ( https://arxiv.org/abs/2402.03223 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11517
replaced with revised version Thu, 7 Mar 2024 13:43:03 GMT   (421kb,D)

Title: Knowledge-to-SQL: Enhancing SQL Generation with Data Expert LLM
Authors: Zijin Hong, Zheng Yuan, Hao Chen, Qinggang Zhang, Feiran Huang, Xiao
  Huang
Categories: cs.CL
Comments: under review
\\ ( https://arxiv.org/abs/2402.11517 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12226
replaced with revised version Thu, 7 Mar 2024 06:31:46 GMT   (2953kb,D)

Title: AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
Authors: Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng
  Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui,
  Tianxiang Sun, Yugang Jiang, Xipeng Qiu
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: 28 pages, 16 figures, under review, work in progress
\\ ( https://arxiv.org/abs/2402.12226 ,  2953kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13587
replaced with revised version Thu, 7 Mar 2024 11:29:50 GMT   (5236kb,D)

Title: A Multimodal In-Context Tuning Approach for E-Commerce Product
  Description Generation
Authors: Yunxin Li, Baotian Hu, Wenhan Luo, Lin Ma, Yuxin Ding, Min Zhang
Categories: cs.CL cs.CV
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.13587 ,  5236kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14860
replaced with revised version Wed, 6 Mar 2024 20:10:11 GMT   (2843kb,D)

Title: Ranking Large Language Models without Ground Truth
Authors: Amit Dhurandhar, Rahul Nair, Moninder Singh, Elizabeth Daly and
  Karthikeyan Natesan Ramamurthy
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.14860 ,  2843kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00758
replaced with revised version Thu, 7 Mar 2024 08:54:19 GMT   (73kb,D)

Title: Mitigating Reversal Curse via Semantic-aware Permutation Training
Authors: Qingyan Guo, Rui Wang, Junliang Guo, Xu Tan, Jiang Bian, Yujiu Yang
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.00758 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00986
replaced with revised version Thu, 7 Mar 2024 18:45:09 GMT   (1341kb,D)

Title: Merging Text Transformer Models from Different Initializations
Authors: Neha Verma, Maha Elbayad
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.00986 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01081
replaced with revised version Wed, 6 Mar 2024 22:25:44 GMT   (1468kb,D)

Title: LAB: Large-Scale Alignment for ChatBots
Authors: Shivchander Sudalairaj, Abhishek Bhandwaldar, Aldo Pareja, Kai Xu,
  David D. Cox, Akash Srivastava
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.01081 ,  1468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01432
replaced with revised version Thu, 7 Mar 2024 11:24:09 GMT   (9659kb,D)

Title: Fine Tuning vs. Retrieval Augmented Generation for Less Popular
  Knowledge
Authors: Heydar Soudani, Evangelos Kanoulas, Faegheh Hasibi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.01432 ,  9659kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02472
replaced with revised version Thu, 7 Mar 2024 02:18:35 GMT   (300kb,D)

Title: OffLanDat: A Community Based Implicit Offensive Language Dataset
  Generated by Large Language Model Through Prompt Engineering
Authors: Amit Das, Mostafa Rahgouy, Dongji Feng, Zheng Zhang, Tathagata
  Bhattacharya, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry
  Dozier and Cheryl Seals
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.02472 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03167
replaced with revised version Wed, 6 Mar 2024 21:01:10 GMT   (12605kb,D)

Title: PARADISE: Evaluating Implicit Planning Skills of Language Models with
  Procedural Warnings and Tips Dataset
Authors: Arda Uzunoglu, Abdalfatah Rashid Safa, G\"ozde G\"ul \c{S}ahin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.03167 ,  12605kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03823
replaced with revised version Thu, 7 Mar 2024 09:10:42 GMT   (2359kb,D)

Title: A Modular Approach for Multimodal Summarization of TV Shows
Authors: Louis Mahon, Mirella Lapata
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.03823 ,  2359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03853
replaced with revised version Thu, 7 Mar 2024 16:21:09 GMT   (2202kb,D)

Title: ShortGPT: Layers in Large Language Models are More Redundant Than You
  Expect
Authors: Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie
  Lu, Xianpei Han, Weipeng Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.03853 ,  2202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03857
replaced with revised version Thu, 7 Mar 2024 14:09:00 GMT   (4262kb,D)

Title: Emojinize: Enriching Any Text with Emoji Translations
Authors: Lars Henning Klein, Roland Aydin, Robert West
Categories: cs.CL cs.HC
\\ ( https://arxiv.org/abs/2403.03857 ,  4262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03883
replaced with revised version Thu, 7 Mar 2024 06:39:32 GMT   (8531kb,D)

Title: SaulLM-7B: A pioneering Large Language Model for Law
Authors: Pierre Colombo, Telmo Pessoa Pires, Malik Boudiaf, Dominic Culver, Rui
  Melo, Caio Corro, Andre F. T. Martins, Fabrizio Esposito, Vera L\'ucia
  Raposo, Sofia Morgado, Michael Desa
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.03883 ,  8531kb)
------------------------------------------------------------------------------
\\
arXiv:2107.07232
replaced with revised version Thu, 7 Mar 2024 17:54:39 GMT   (731kb,D)

Title: On the expressivity of bi-Lipschitz normalizing flows
Authors: Alexandre Verine, Benjamin Negrevergne, Fabrice Rossi, Yann Chevaleyre
Categories: cs.LG stat.ML
Journal-ref: Proceedings of The 14th Asian Conference on Machine Learning, PMLR
  189:1054-1069, 2023
\\ ( https://arxiv.org/abs/2107.07232 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09510
replaced with revised version Thu, 7 Mar 2024 16:15:54 GMT   (3022kb,D)

Title: Self-supervised Trajectory Representation Learning with Temporal
  Regularities and Travel Semantics
Authors: Jiawei Jiang, Dayan Pan, Houxing Ren, Xiaohan Jiang, Chao Li, Jingyuan
  Wang
Categories: cs.LG
Comments: 13 pages, 10 figures, Accepted by ICDE 2023
\\ ( https://arxiv.org/abs/2211.09510 ,  3022kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14960
replaced with revised version Thu, 7 Mar 2024 16:04:24 GMT   (1876kb,D)

Title: Label Alignment Regularization for Distribution Shift
Authors: Ehsan Imani, Guojun Zhang, Runjia Li, Jun Luo, Pascal Poupart, Philip
  H.S. Torr, Yangchen Pan
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2211.14960 ,  1876kb)
------------------------------------------------------------------------------
\\
arXiv:2301.07945
replaced with revised version Thu, 7 Mar 2024 16:00:47 GMT   (7462kb,D)

Title: PDFormer: Propagation Delay-Aware Dynamic Long-Range Transformer for
  Traffic Flow Prediction
Authors: Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, Jingyuan Wang
Categories: cs.LG
Comments: 9 pages, 5 figures, Accepted by AAAI2023
\\ ( https://arxiv.org/abs/2301.07945 ,  7462kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08484
replaced with revised version Thu, 7 Mar 2024 15:37:44 GMT   (458kb,D)

Title: FOSI: Hybrid First and Second Order Optimization
Authors: Hadar Sivan, Moshe Gabel, Assaf Schuster
Categories: cs.LG
Comments: 23 pages, 9 figures. Accepted as a conference paper to ICLR 2024
\\ ( https://arxiv.org/abs/2302.08484 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2303.06965
replaced with revised version Thu, 7 Mar 2024 14:51:12 GMT   (18707kb,D)

Title: Bridging the Gap between Chemical Reaction Pretraining and Conditional
  Molecule Generation with a Unified Model
Authors: Bo Qiang, Yiran Zhou, Yuheng Ding, Ningfeng Liu, Song Song, Liangren
  Zhang, Bo Huang, Zhenming Liu
Categories: cs.LG q-bio.BM
DOI: 10.1038/s42256-023-00764-9
\\ ( https://arxiv.org/abs/2303.06965 ,  18707kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05364
replaced with revised version Thu, 7 Mar 2024 13:48:04 GMT   (19839kb,D)

Title: Diffusion Models for Constrained Domains
Authors: Nic Fishman, Leo Klarner, Valentin De Bortoli, Emile Mathieu, Michael
  Hutchinson
Categories: cs.LG stat.ML
Comments: Published in Transactions on Machine Learning Research (07/2023)
\\ ( https://arxiv.org/abs/2304.05364 ,  19839kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04288
replaced with revised version Thu, 7 Mar 2024 07:27:17 GMT   (85kb)

Title: Towards Achieving Near-optimal Utility for Privacy-Preserving Federated
  Learning via Data Generation and Parameter Distortion
Authors: Xiaojin Zhang, Kai Chen, Qiang Yang
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2305.04288 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05666
replaced with revised version Thu, 7 Mar 2024 17:26:06 GMT   (8823kb,D)

Title: Policy Gradient Methods in the Presence of Symmetries and State
  Abstractions
Authors: Prakash Panangaden, Sahand Rezaei-Shoshtari, Rosie Zhao, David Meger,
  Doina Precup
Categories: cs.LG cs.AI
Comments: Published in the Journal of Machine Learning Research (JMLR). arXiv
  admin note: text overlap with arXiv:2209.07364
\\ ( https://arxiv.org/abs/2305.05666 ,  8823kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05738
replaced with revised version Wed, 6 Mar 2024 20:56:06 GMT   (24445kb,D)

Title: DOCTOR: A Multi-Disease Detection Continual Learning Framework Based on
  Wearable Medical Sensors
Authors: Chia-Hao Li and Niraj K. Jha
Categories: cs.LG cs.HC eess.SP
Comments: 36 pages, 13 figures. This work has been submitted to the ACM for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2305.05738 ,  24445kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18761
replaced with revised version Thu, 7 Mar 2024 00:58:00 GMT   (3252kb,D)

Title: Identifying Spurious Biases Early in Training through the Lens of
  Simplicity Bias
Authors: Yu Yang, Eric Gan, Gintare Karolina Dziugaite, Baharan Mirzasoleiman
Categories: cs.LG cs.CV
Comments: 26 pages, 10 figures
Journal-ref: Proceedings of the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2024, Valencia, Spain. PMLR: Volume 238
\\ ( https://arxiv.org/abs/2305.18761 ,  3252kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19523
replaced with revised version Thu, 7 Mar 2024 02:45:36 GMT   (1280kb,D)

Title: Harnessing Explanations: LLM-to-LM Interpreter for Enhanced
  Text-Attributed Graph Representation Learning
Authors: Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun,
  Bryan Hooi
Categories: cs.LG
Comments: In Proceedings of ICLR 2024
\\ ( https://arxiv.org/abs/2305.19523 ,  1280kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06836
replaced with revised version Thu, 7 Mar 2024 15:29:58 GMT   (73kb,D)

Title: Tackling Heavy-Tailed Rewards in Reinforcement Learning with Function
  Approximation: Minimax Optimal and Instance-Dependent Regret Bounds
Authors: Jiayi Huang, Han Zhong, Liwei Wang, Lin F. Yang
Categories: cs.LG cs.AI stat.ML
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.06836 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12059
replaced with revised version Wed, 6 Mar 2024 21:45:16 GMT   (6251kb,D)

Title: EquiformerV2: Improved Equivariant Transformer for Scaling to
  Higher-Degree Representations
Authors: Yi-Lun Liao, Brandon Wood, Abhishek Das, Tess Smidt
Categories: cs.LG cs.AI physics.comp-ph
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2306.12059 ,  6251kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14131
replaced with revised version Wed, 6 Mar 2024 21:42:22 GMT   (1077kb,D)

Title: Safety-Critical Scenario Generation Via Reinforcement Learning Based
  Editing
Authors: Haolan Liu, Liangjun Zhang, Siva Kumar Sastry Hari, Jishen Zhao
Categories: cs.LG cs.RO
\\ ( https://arxiv.org/abs/2306.14131 ,  1077kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04001
replaced with revised version Thu, 7 Mar 2024 05:56:19 GMT   (9810kb,D)

Title: Polynomial Width is Sufficient for Set Representation with
  High-dimensional Features
Authors: Peihao Wang, Shenghao Yang, Shu Li, Zhangyang Wang, Pan Li
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2307.04001 ,  9810kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11838
replaced with revised version Thu, 7 Mar 2024 04:09:01 GMT   (4989kb,D)

Title: A Benchmark Study on Calibration
Authors: Linwei Tao, Younan Zhu, Haolan Guo, Minjing Dong, Chang Xu
Categories: cs.LG cs.AI stat.ML
Comments: ICLR 2024 poster
\\ ( https://arxiv.org/abs/2308.11838 ,  4989kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12899
replaced with revised version Thu, 7 Mar 2024 16:22:21 GMT   (2684kb,D)

Title: Unified Data Management and Comprehensive Performance Evaluation for
  Urban Spatial-Temporal Prediction [Experiment, Analysis & Benchmark]
Authors: Jiawei Jiang, Chengkai Han, Wayne Xin Zhao, Jingyuan Wang
Categories: cs.LG
Comments: 14 pages, 3 figures, VLDB under review
\\ ( https://arxiv.org/abs/2308.12899 ,  2684kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13662
replaced with revised version Thu, 7 Mar 2024 04:50:04 GMT   (600kb,D)

Title: REFT: Resource-Efficient Federated Training Framework for Heterogeneous
  and Resource-Constrained Environments
Authors: Humaid Ahmed Desai, Amr Hilal, Hoda Eldardiry
Categories: cs.LG cs.DC
Comments: 10 pages, 6 figures
\\ ( https://arxiv.org/abs/2308.13662 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06692
replaced with revised version Thu, 7 Mar 2024 14:57:19 GMT   (446kb,D)

Title: Tackling the Non-IID Issue in Heterogeneous Federated Learning by
  Gradient Harmonization
Authors: Xinyu Zhang, Weiyu Sun, Ying Chen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.06692 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17182
replaced with revised version Thu, 7 Mar 2024 17:32:21 GMT   (4675kb,D)

Title: RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit
  Neural Representations
Authors: Jiajun He, Gergely Flamich, Zongyu Guo, Jos\'e Miguel
  Hern\'andez-Lobato
Categories: cs.LG
Comments: ICLR 2024 camera-ready version; 27 pages, 17 figures
\\ ( https://arxiv.org/abs/2309.17182 ,  4675kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02396
replaced with revised version Thu, 7 Mar 2024 17:47:27 GMT   (774kb,D)

Title: Implicit regularization of multi-task learning and finetuning in
  overparameterized neural networks
Authors: Jack W. Lindsey and Samuel Lippl
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.02396 ,  774kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02611
replaced with revised version Thu, 7 Mar 2024 05:13:11 GMT   (46922kb,D)

Title: Analyzing and Improving Optimal-Transport-based Adversarial Networks
Authors: Jaemoo Choi, Jaewoong Choi, Myungjoo Kang
Categories: cs.LG cs.CV
Comments: 27 pages, 17 figures
\\ ( https://arxiv.org/abs/2310.02611 ,  46922kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03400
replaced with revised version Thu, 7 Mar 2024 12:04:54 GMT   (1734kb,D)

Title: Adapting Large Language Models for Content Moderation: Pitfalls in Data
  Engineering and Supervised Fine-tuning
Authors: Huan Ma, Changqing Zhang, Huazhu Fu, Peilin Zhao, Bingzhe Wu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.03400 ,  1734kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07229
replaced with revised version Thu, 7 Mar 2024 07:14:50 GMT   (36950kb,D)

Title: ProFSA: Self-supervised Pocket Pretraining via Protein
  Fragment-Surroundings Alignment
Authors: Bowen Gao, Yinjun Jia, Yuanle Mo, Yuyan Ni, Weiying Ma, Zhiming Ma,
  Yanyan Lan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.07229 ,  36950kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08775
replaced with revised version Thu, 7 Mar 2024 18:09:00 GMT   (30kb)

Title: When Machine Learning Models Leak: An Exploration of Synthetic Training
  Data
Authors: Manel Slokom and Peter-Paul de Wolf and Martha Larson
Categories: cs.LG
Comments: Original paper published at PSD 2022. The paper was subsequently
  updated
\\ ( https://arxiv.org/abs/2310.08775 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10374
replaced with revised version Thu, 7 Mar 2024 06:44:33 GMT   (2509kb,D)

Title: Multi-Factor Spatio-Temporal Prediction based on Graph Decomposition
  Learning
Authors: Jiahao Ji, Jingyuan Wang, Yu Mou, and Cheng Long
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.10374 ,  2509kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16727
replaced with revised version Thu, 7 Mar 2024 12:55:59 GMT   (168kb,D)

Title: AI Hazard Management: A framework for the systematic management of root
  causes for AI risks
Authors: Ronald Schnitzer, Andreas Hapfelmeier, Sven Gaube, Sonja Zillner
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.16727 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18191
replaced with revised version Thu, 7 Mar 2024 18:10:12 GMT   (899kb,D)

Title: Is Scaling Learned Optimizers Worth It? Evaluating The Value of VeLO's
  4000 TPU Months
Authors: Fady Rezk, Antreas Antoniou, Henry Gouk, Timothy Hospedales
Categories: cs.LG cs.AI math.OC
\\ ( https://arxiv.org/abs/2310.18191 ,  899kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12688
replaced with revised version Thu, 7 Mar 2024 17:00:03 GMT   (1605kb,D)

Title: On the Out-of-Distribution Coverage of Combining Split Conformal
  Prediction and Bayesian Deep Learning
Authors: Paul Scemama, Ariel Kapusta
Categories: cs.LG stat.ML
Comments: 26 pages, 18 figures
\\ ( https://arxiv.org/abs/2311.12688 ,  1605kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15502
replaced with revised version Thu, 7 Mar 2024 08:39:09 GMT   (127kb,D)

Title: The Selected-completely-at-random Complementary Label is a Practical
  Weak Supervision for Multi-class Classification
Authors: Wei Wang, Takashi Ishida, Yu-Jie Zhang, Gang Niu, Masashi Sugiyama
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.15502 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16176
replaced with revised version Wed, 6 Mar 2024 21:57:06 GMT   (13329kb,D)

Title: Mitigating Biases with Diverse Ensembles and Diffusion Models
Authors: Luca Scimeca, Alexander Rubinstein, Damien Teney, Seong Joon Oh,
  Armand Mihai Nicolicioiu, Yoshua Bengio
Categories: cs.LG cs.AI cs.CV
Comments: arXiv admin note: text overlap with arXiv:2310.02230
\\ ( https://arxiv.org/abs/2311.16176 ,  13329kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12339
replaced with revised version Thu, 7 Mar 2024 10:07:02 GMT   (4822kb,D)

Title: Value Explicit Pretraining for Learning Transferable Representations
Authors: Kiran Lekkala, Henghui Bao, Sumedh Sontakke, Laurent Itti
Categories: cs.LG cs.RO
Comments: Accepted at CoRL 2023 Workshop on PRL, Under Review at ICML 2024
\\ ( https://arxiv.org/abs/2312.12339 ,  4822kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00230
replaced with revised version Thu, 7 Mar 2024 10:25:20 GMT   (11758kb,D)

Title: Transformer Multivariate Forecasting: Less is More?
Authors: Jingjing Xu, Caesar Wu, Yuan-Fang Li, Pascal Bouvry
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.00230 ,  11758kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12564
replaced with revised version Thu, 7 Mar 2024 06:00:13 GMT   (6611kb,D)

Title: Graph Contrastive Invariant Learning from the Causal Perspective
Authors: Yanhu Mo, Xiao Wang, Shaohua Fan, Chuan Shi
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2401.12564 ,  6611kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16235
replaced with revised version Thu, 7 Mar 2024 18:27:52 GMT   (758kb)

Title: Player Pressure Map -- A Novel Representation of Pressure in Soccer for
  Evaluating Player Performance in Different Game Contexts
Authors: Chaoyi Gu, Jiaming Na, Yisheng Pei, Varuna De Silva
Categories: cs.LG stat.AP
\\ ( https://arxiv.org/abs/2401.16235 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17435
replaced with revised version Thu, 7 Mar 2024 16:47:00 GMT   (600kb,D)

Title: Can Large Language Models Replace Economic Choice Prediction Labs?
Authors: Eilam Shapira, Omer Madmon, Roi Reichart, Moshe Tennenholtz
Categories: cs.LG cs.AI cs.CL cs.GT cs.HC
\\ ( https://arxiv.org/abs/2401.17435 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02746
replaced with revised version Thu, 7 Mar 2024 15:47:29 GMT   (4583kb,D)

Title: Standard Gaussian Process is All You Need for High-Dimensional Bayesian
  Optimization
Authors: Zhitong Xu, Shandian Zhe
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.02746 ,  4583kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04830
replaced with revised version Thu, 7 Mar 2024 11:56:15 GMT   (288kb,D)

Title: Closing the Gap Between SGP4 and High-Precision Propagation via
  Differentiable Programming
Authors: Giacomo Acciarini, At{\i}l{\i}m G\"une\c{s} Baydin, Dario Izzo
Categories: cs.LG astro-ph.EP
\\ ( https://arxiv.org/abs/2402.04830 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05443
replaced with revised version Thu, 7 Mar 2024 05:27:33 GMT   (36817kb,D)

Title: Scalable Wasserstein Gradient Flow for Generative Modeling through
  Unbalanced Optimal Transport
Authors: Jaemoo Choi, Jaewoong Choi, Myungjoo Kang
Categories: cs.LG cs.CV
Comments: 20 pages, 11 figures
\\ ( https://arxiv.org/abs/2402.05443 ,  36817kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05956
replaced with revised version Thu, 7 Mar 2024 02:11:24 GMT   (2496kb,D)

Title: Pathformer: Multi-scale Transformers with Adaptive Pathways for Time
  Series Forecasting
Authors: Peng Chen, Yingying Zhang, Yunyao Cheng, Yang Shu, Yihang Wang,
  Qingsong Wen, Bin Yang, Chenjuan Guo
Categories: cs.LG
Comments: Accepted by the 12th International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2402.05956 ,  2496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12694
replaced with revised version Thu, 7 Mar 2024 09:47:38 GMT   (10713kb,D)

Title: Revitalizing Multivariate Time Series Forecasting: Learnable
  Decomposition with Inter-Series Dependencies and Intra-Series Variations
  Modeling
Authors: Guoqi Yu, Jing Zou, Xiaowei Hu, Angelica I. Aviles-Rivero, Jing Qin
  and Shujun Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.12694 ,  10713kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16374
replaced with revised version Thu, 7 Mar 2024 05:07:49 GMT   (14381kb,D)

Title: Graph Learning under Distribution Shifts: A Comprehensive Survey on
  Domain Adaptation, Out-of-distribution, and Continual Learning
Authors: Man Wu, Xin Zheng, Qin Zhang, Xiao Shen, Xiong Luo, Xingquan Zhu,
  Shirui Pan
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2402.16374 ,  14381kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16899
replaced with revised version Thu, 7 Mar 2024 05:33:40 GMT   (249kb,D)

Title: A priori Estimates for Deep Residual Network in Continuous-time
  Reinforcement Learning
Authors: Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.16899 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17002
replaced with revised version Thu, 7 Mar 2024 03:03:06 GMT   (4035kb,D)

Title: Discovering Symmetry Group Structures via Implicit Orthogonality Bias
Authors: Dongsung Huh
Categories: cs.LG math.GR math.RT
Comments: 19 pages, 14 figures
\\ ( https://arxiv.org/abs/2402.17002 ,  4035kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18059
replaced with revised version Thu, 7 Mar 2024 05:47:49 GMT   (325kb,D)

Title: Token-Specific Watermarking with Enhanced Detectability and Semantic
  Coherence for Large Language Models
Authors: Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz
  Koushanfar, Pengtao Xie
Categories: cs.LG cs.CL cs.CR
Comments: 16 pages, 9 figures, 2 tables
\\ ( https://arxiv.org/abs/2402.18059 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00858
replaced with revised version Thu, 7 Mar 2024 06:37:44 GMT   (939kb,D)

Title: Direct Alignment of Draft Model for Speculative Decoding with
  Chat-Fine-Tuned LLMs
Authors: Raghavv Goel, Mukul Gagrani, Wonseok Jeon, Junyoung Park, Mingu Lee,
  Christopher Lott
Categories: cs.LG cs.AI cs.CL
Comments: 8 pages, 3 figures, Published at the ICLR 2024 Workshop on
  Understanding of Foundation Models (ME-FoMo)
\\ ( https://arxiv.org/abs/2403.00858 ,  939kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00877
replaced with revised version Thu, 7 Mar 2024 02:48:06 GMT   (487kb,D)

Title: Disaggregated Multi-Tower: Topology-aware Modeling Technique for
  Efficient Large-Scale Recommendation
Authors: Liang Luo, Buyun Zhang, Michael Tsang, Yinbin Ma, Ching-Hsiang Chu,
  Yuxin Chen, Shen Li, Yuchen Hao, Yanli Zhao, Guna Lakshminarayanan, Ellie
  Dingqiao Wen, Jongsoo Park, Dheevatsa Mudigere, Maxim Naumov
Categories: cs.LG cs.DC cs.IR
\\ ( https://arxiv.org/abs/2403.00877 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01112
replaced with revised version Thu, 7 Mar 2024 13:40:04 GMT   (31056kb,D)

Title: Efficient Episodic Memory Utilization of Cooperative Multi-Agent
  Reinforcement Learning
Authors: Hyungho Na, Yunkyeong Seo, Il-chul Moon
Categories: cs.LG cs.MA
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2403.01112 ,  31056kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03218
replaced with revised version Wed, 6 Mar 2024 21:27:11 GMT   (736kb,D)

Title: The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
Authors: Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios,
  Alice Gatti, Justin D. Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan,
  Gabriel Mukobi, Nathan Helm-Burger, Rassin Lababidi, Lennart Justen, Andrew
  B. Liu, Michael Chen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu, Rishub
  Tamirisa, Bhrugu Bharathi, Adam Khoja, Zhenqi Zhao, Ariel Herbert-Voss, Cort
  B. Breuer, Andy Zou, Mantas Mazeika, Zifan Wang, Palash Oswal, Weiran Liu,
  Adam A. Hunt, Justin Tienken-Harder, Kevin Y. Shih, Kemper Talley, John Guan,
  Russell Kaplan, Ian Steneker, David Campbell, Brad Jokubaitis, Alex Levinson,
  Jean Wang, William Qian, Kallol Krishna Karmakar, Steven Basart, Stephen
  Fitz, Mindy Levine, Ponnurangam Kumaraguru, Uday Tupakula, Vijay
  Varadharajan, Yan Shoshitaishvili, Jimmy Ba, Kevin M. Esvelt, Alexandr Wang
  and Dan Hendrycks
Categories: cs.LG cs.AI cs.CL cs.CY
Comments: See the project page at https://wmdp.ai
\\ ( https://arxiv.org/abs/2403.03218 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03223
replaced with revised version Thu, 7 Mar 2024 06:12:56 GMT   (36802kb,D)

Title: Exact Enforcement of Temporal Continuity in Sequential Physics-Informed
  Neural Networks
Authors: Pratanu Roy and Stephen Castonguay
Categories: cs.LG physics.comp-ph
Comments: 30 pages, 13 figures
\\ ( https://arxiv.org/abs/2403.03223 ,  36802kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03542
replaced with revised version Thu, 7 Mar 2024 04:53:51 GMT   (884kb,D)

Title: DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE
  Pre-Training
Authors: Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying,
  Hang Su, Anima Anandkumar, Jian Song, Jun Zhu
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2403.03542 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03643
replaced with revised version Thu, 7 Mar 2024 02:05:28 GMT   (844kb)

Title: A Survey on Applications of Reinforcement Learning in Spatial Resource
  Allocation
Authors: Di Zhang, Moyang Wang, Joseph Mango, Xiang Li, Xianrui Xu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.03643 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2109.14200 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 21:01:13 GMT   (4914kb)

Title: Can phones, syllables, and words emerge as side-products of
  cross-situational audiovisual learning? -- A computational investigation
Authors: Khazar Khorrami, Okko R\"as\"anen
Categories: eess.AS cs.AI cs.CL cs.CV cs.LG cs.SD
Comments: Final manuscript published in Language Development Research under CC
  BY-NC-SA 4.0. Pre-print redistributed through arXiv with permission. Replaces
  corrupted PsyArXiv pre-print repository at https://psyarxiv.com/37zna
ACM-class: I.2.0; I.2.6; I.2.7; I.2.10
Journal-ref: Language Development Research, 2021
DOI: 10.34842/w3vw-s845
\\ ( https://arxiv.org/abs/2109.14200 ,  4914kb)
------------------------------------------------------------------------------
\\
arXiv:2211.02680 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 11:36:54 GMT   (10373kb,D)

Title: Climbing Routes Clustering Using Energy-Efficient Accelerometers
  Attached to the Quickdraws
Authors: Sadaf Moaveninejad, Andrea Janes, Camillo Porcaro, Luca Barletta,
  Lorenzo Mucchi, Massimiliano Pierobon
Categories: eess.SP cs.AI cs.LG
\\ ( https://arxiv.org/abs/2211.02680 ,  10373kb)
------------------------------------------------------------------------------
\\
arXiv:2212.01551
replaced with revised version Thu, 7 Mar 2024 02:10:54 GMT   (2923kb,D)

Title: Quantify the Causes of Causal Emergence: Critical Conditions of
  Uncertainty and Asymmetry in Causal Structure
Authors: Liye Jia, Fengyufan Yang, Ka Lok Man, Erick Purwanto, Sheng-Uei Guan,
  Jeremy Smith, Yutao Yue
Categories: cs.IT cs.AI math.IT physics.comp-ph
Comments: 18 pages, 14 figures
ACM-class: H.1.0; J.2; I.2.m
\\ ( https://arxiv.org/abs/2212.01551 ,  2923kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10774 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 02:07:37 GMT   (6623kb,D)

Title: RDesign: Hierarchical Data-efficient Representation Learning for
  Tertiary Structure-based RNA Design
Authors: Cheng Tan, Yijie Zhang, Zhangyang Gao, Bozhen Hu, Siyuan Li, Zicheng
  Liu, Stan Z. Li
Categories: q-bio.BM cs.AI cs.LG
Comments: 30 pages, 28 figures, 16 tables
\\ ( https://arxiv.org/abs/2301.10774 ,  6623kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16384
replaced with revised version Wed, 6 Mar 2024 22:41:30 GMT   (1211kb,D)

Title: Accelerating Sampling and Aggregation Operations in GNN Frameworks with
  GPU Initiated Direct Storage Accesses
Authors: Jeongmin Brian Park and Vikram Sharma Mailthody and Zaid Qureshi and
  Wen-mei Hwu
Categories: cs.DC cs.AI cs.AR cs.LG
Comments: Under Submission. Source code:
  https://github.com/jeongminpark417/GIDS
\\ ( https://arxiv.org/abs/2306.16384 ,  1211kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12708
replaced with revised version Thu, 7 Mar 2024 02:50:04 GMT   (6512kb,D)

Title: PointSSC: A Cooperative Vehicle-Infrastructure Point Cloud Benchmark for
  Semantic Scene Completion
Authors: Yuxiang Yan, Boda Liu, Jianfei Ai, Qinbu Li, Ru Wan, Jian Pu
Categories: cs.CV cs.AI cs.LG
Comments: ICRA2024, oral & poster
\\ ( https://arxiv.org/abs/2309.12708 ,  6512kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02772
replaced with revised version Thu, 7 Mar 2024 04:18:20 GMT   (514kb,D)

Title: Spike Accumulation Forwarding for Effective Training of Spiking Neural
  Networks
Authors: Ryuji Saiin, Tomoya Shirakawa, Sota Yoshihara, Yoshihide Sawada and
  Hiroyuki Kusumoto
Categories: cs.NE cs.AI
Comments: 12 pages, 5 figures, Appendix:8 pages, 2 figures, v5:We added
  experimental results and considered the situation the SNN have a feedforward
  or feedback connection
\\ ( https://arxiv.org/abs/2310.02772 ,  514kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04218
replaced with revised version Thu, 7 Mar 2024 13:06:30 GMT   (84kb,D)

Title: A Fixed-Parameter Tractable Algorithm for Counting Markov Equivalence
  Classes with the same Skeleton
Authors: Vidya Sagar Sharma
Categories: cs.DS cs.AI cs.LG
Comments: 75 pages, 2 Figures
Journal-ref: 38th Annual AAAI Conference on Artificial Intelligence (AAAI 2024)
\\ ( https://arxiv.org/abs/2310.04218 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07699
replaced with revised version Thu, 7 Mar 2024 18:25:39 GMT   (27562kb,D)

Title: VeCLIP: Improving CLIP Training via Visual-enriched Captions
Authors: Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai,
  Aleksei Timofeev, Xianzhi Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei
  Yang, Meng Cao
Categories: cs.CV cs.AI cs.LG
Comments: CV/ML
\\ ( https://arxiv.org/abs/2310.07699 ,  27562kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01534
replaced with revised version Wed, 6 Mar 2024 21:52:43 GMT   (11941kb,D)

Title: Approximate Multiagent Reinforcement Learning for On-Demand Urban
  Mobility Problem on a Large Map (extended version)
Authors: Daniel Garces, Sushmita Bhattacharya, Dimitri Bertsekas, Stephanie Gil
Categories: cs.MA cs.AI cs.RO
Comments: 11 pages, 5 figures, 1 lemma, and 2 theorems
\\ ( https://arxiv.org/abs/2311.01534 ,  11941kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07619
replaced with revised version Thu, 7 Mar 2024 05:32:37 GMT   (1458kb,D)

Title: Modeling User Viewing Flow Using Large Language Models for Article
  Recommendation
Authors: Zhenghao Liu, Zulong Chen, Moufeng Zhang, Shaoyang Duan, Hong Wen,
  Liangyue Li, Nan Li, Yu Gu and Ge Yu
Categories: cs.IR cs.AI
Comments: Accepted by WebConf 2024
\\ ( https://arxiv.org/abs/2311.07619 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08094
replaced with revised version Thu, 7 Mar 2024 07:20:50 GMT   (581kb)

Title: SkelVIT: Consensus of Vision Transformers for a Lightweight
  Skeleton-Based Action Recognition System
Authors: Ozge Oztimur Karadag
Categories: cs.CV cs.AI cs.LG cs.RO
\\ ( https://arxiv.org/abs/2311.08094 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12943
replaced with revised version Wed, 6 Mar 2024 20:22:06 GMT   (5985kb,D)

Title: InteRACT: Transformer Models for Human Intent Prediction Conditioned on
  Robot Actions
Authors: Kushal Kedia, Atiksh Bhardwaj, Prithwish Dan, Sanjiban Choudhury
Categories: cs.RO cs.AI cs.LG cs.MA
Comments: We release our code and datasets at
  https://portal-cornell.github.io/interact/
\\ ( https://arxiv.org/abs/2311.12943 ,  5985kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00110
replaced with revised version Wed, 6 Mar 2024 20:13:53 GMT   (11525kb,D)

Title: Diffusion Model with Perceptual Loss
Authors: Shanchuan Lin, Xiao Yang
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.00110 ,  11525kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08095
replaced with revised version Thu, 7 Mar 2024 08:40:01 GMT   (13966kb,D)

Title: DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel
  Generation
Authors: Hyung-Seok Oh, Sang-Hoon Lee, Deok-Hyeon Cho, Seong-Whan Lee
Categories: cs.SD cs.AI eess.AS
Comments: 13 pages, 9 figures, 8 tables
\\ ( https://arxiv.org/abs/2401.08095 ,  13966kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10712
replaced with revised version Thu, 7 Mar 2024 06:43:56 GMT   (4873kb,D)

Title: Q&A Prompts: Discovering Rich Visual Clues through Mining
  Question-Answer Prompts for VQA requiring Diverse World Knowledge
Authors: Haibi Wang, Weifeng Ge
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.10712 ,  4873kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14362
replaced with revised version Wed, 6 Mar 2024 20:41:53 GMT   (98kb)

Title: The Typing Cure: Experiences with Large Language Model Chatbots for
  Mental Health Support
Authors: Inhwa Song, Sachin R. Pendse, Neha Kumar, Munmun De Choudhury
Categories: cs.HC cs.AI cs.CY
Comments: The first two authors contributed equally to this work; typos
  corrected
\\ ( https://arxiv.org/abs/2401.14362 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13602
replaced with revised version Thu, 7 Mar 2024 12:24:11 GMT   (4053kb,D)

Title: Hybrid Reasoning Based on Large Language Models for Autonomous Car
  Driving
Authors: Mehdi Azarafza, Mojtaba Nayyeri, Charles Steinmetz, Steffen Staab,
  Achim Rettberg
Categories: cs.CV cs.AI
Comments: 12 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.13602 ,  4053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13754 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 11:51:52 GMT   (13840kb,D)

Title: Reinforcement learning-assisted quantum architecture search for
  variational quantum algorithms
Authors: Akash Kundu
Categories: quant-ph cs.AI cs.LG
Comments: With 154 pages and 46 figures, here lies my PhD thesis. Typos
  corrected!
\\ ( https://arxiv.org/abs/2402.13754 ,  13840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17785
replaced with revised version Thu, 7 Mar 2024 00:32:27 GMT   (3702kb,D)

Title: ByteComposer: a Human-like Melody Composition Method based on Language
  Model Agent
Authors: Xia Liang, Xingjian Du, Jiaju Lin, Pei Zou, Yuan Wan, Bilei Zhu
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2402.17785 ,  3702kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02131
replaced with revised version Thu, 7 Mar 2024 09:42:47 GMT   (10527kb,D)

Title: Deep Reinforcement Learning for Dynamic Algorithm Selection: A
  Proof-of-Principle Study on Differential Evolution
Authors: Hongshu Guo, Yining Ma, Zeyuan Ma, Jiacheng Chen, Xinglin Zhang,
  Zhiguang Cao, Jun Zhang, Yue-Jiao Gong
Categories: cs.NE cs.AI
Comments: Accepted by IEEE Transactions on Systems, Man, and Cybernetics:
  Systems at Thu, Feb 29, 2024
\\ ( https://arxiv.org/abs/2403.02131 ,  10527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03456
replaced with revised version Thu, 7 Mar 2024 05:49:05 GMT   (26697kb,D)

Title: DLP-GAN: learning to draw modern Chinese landscape photos with
  generative adversarial network
Authors: Xiangquan Gui, Binxuan Zhang, Li Li, Yi Yang
Categories: cs.CV cs.AI
Comments: Corrected typos
Journal-ref: Neural Computing and Applications, 2023: 1-18
DOI: 10.1007/s00521-023-09345-8
\\ ( https://arxiv.org/abs/2403.03456 ,  26697kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07805 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 11:52:11 GMT   (2524kb)

Title: EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and
  Dictionary-based Named Entity Recognition from Medical Text
Authors: Rafsan Ahmed, Petter Berntsson, Alexander Skafte, Salma Kazemi Rashed,
  Marcus Klang, Adam Barvesten, Ola Olde, William Lindholm, Antton Lamarca
  Arrizabalaga, Pierre Nugues, Sonja Aits
Categories: q-bio.QM cs.CL
MSC-class: 92-04, 92-08, 68T50
ACM-class: J.3; I.2.7; H.3.3
\\ ( https://arxiv.org/abs/2304.07805 ,  2524kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00014 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 19:06:05 GMT   (5744kb)

Title: A new mapping of technological interdependence
Authors: A. Fronzetti Colladon, B. Guardabascio, F. Venturini
Categories: econ.EM cs.CL cs.SI
ACM-class: I.7; I.2.7; J.4
\\ ( https://arxiv.org/abs/2308.00014 ,  5744kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16347
replaced with revised version Thu, 7 Mar 2024 17:53:35 GMT   (7011kb,D)

Title: Intrinsic Language-Guided Exploration for Complex Long-Horizon Robotic
  Manipulation Tasks
Authors: Eleftherios Triantafyllidis, Filippos Christianos and Zhibin Li
Categories: cs.RO cs.CL cs.LG
Comments: Accepted at the International Conference on Robotics and Automation
  (ICRA), 2024. The manuscript consists of 10 pages and 6 figures
\\ ( https://arxiv.org/abs/2309.16347 ,  7011kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03003
replaced with revised version Thu, 7 Mar 2024 05:03:11 GMT   (430kb,D)

Title: AST-T5: Structure-Aware Pretraining for Code Generation and
  Understanding
Authors: Linyuan Gong, Mostafa Elhoushi, Alvin Cheung
Categories: cs.SE cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.03003 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:1902.06931 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 09:27:39 GMT   (869kb,D)

Title: On the consistency of supervised learning with missing values
Authors: Julie Josse (CMAP, XPOP), Jacob M. Chen, Nicolas Prost (CMAP, XPOP,
  PARIETAL), Ga\"el Varoquaux (PARIETAL), Erwan Scornet (X, CMAP, SU)
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/1902.06931 ,  869kb)
------------------------------------------------------------------------------
\\
arXiv:2208.07581 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 17:23:43 GMT   (19429kb,D)

Title: Regression modelling of spatiotemporal extreme U.S. wildfires via
  partially-interpretable neural networks
Authors: Jordan Richards and Rapha\"el Huser
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2208.07581 ,  19429kb)
------------------------------------------------------------------------------
\\
arXiv:2209.14399
replaced with revised version Thu, 7 Mar 2024 06:22:02 GMT   (2371kb,D)

Title: FIRE: A Failure-Adaptive Reinforcement Learning Framework for Edge
  Computing Migrations
Authors: Marie Siew, Shikhar Sharma, Zekai Li, Kun Guo, Chao Xu, Tania
  Lorido-Botran, Tony Q.S. Quek and Carlee Joe-Wong
Categories: cs.NI cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2209.14399 ,  2371kb)
------------------------------------------------------------------------------
\\
arXiv:2301.01716 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 15:42:33 GMT   (36kb)

Title: First-order penalty methods for bilevel optimization
Authors: Zhaosong Lu and Sanyou Mei
Categories: math.OC cs.LG cs.NA math.NA stat.ML
Comments: Accepted by SIAM Journal on Optimization
MSC-class: 90C26, 90C30, 90C47, 90C99, 65K05
\\ ( https://arxiv.org/abs/2301.01716 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13368 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 11:31:17 GMT   (2277kb,D)

Title: Misspecification-robust Sequential Neural Likelihood for
  Simulation-based Inference
Authors: Ryan P. Kelly and David J. Nott and David T. Frazier and David J.
  Warne and Chris Drovandi
Categories: stat.ME cs.LG stat.CO stat.ML
\\ ( https://arxiv.org/abs/2301.13368 ,  2277kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16198
replaced with revised version Thu, 7 Mar 2024 14:42:09 GMT   (3911kb,D)

Title: Multi-modal learning for geospatial vegetation forecasting
Authors: Vitus Benson, Claire Robin, Christian Requena-Mesa, Lazaro Alonso,
  Nuno Carvalhais, Jos\'e Cort\'es, Zhihan Gao, Nora Linscheid, M\'elanie
  Weynants, Markus Reichstein
Categories: cs.CV cs.LG
Comments: Accepted at CVPR 2024. We provide open source code and pre-trained
  weights to reproduce our experimental results under
  https://github.com/vitusbenson/greenearthnet
\\ ( https://arxiv.org/abs/2303.16198 ,  3911kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12584 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 16:04:12 GMT   (40kb)

Title: Sparse Representer Theorems for Learning in Reproducing Kernel Banach
  Spaces
Authors: Rui Wang, Yuesheng Xu, Mingsong Yan
Categories: math.FA cs.LG
\\ ( https://arxiv.org/abs/2305.12584 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09251 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 03:30:57 GMT   (117kb)

Title: Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative
  Models
Authors: Gen Li, Yuting Wei, Yuxin Chen, Yuejie Chi
Categories: stat.ML cs.IT cs.LG math.IT math.ST stat.TH
Comments: accepted in part to ICLR 2024
\\ ( https://arxiv.org/abs/2306.09251 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05318 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 14:38:27 GMT   (4343kb,D)

Title: Predicting small molecules solubilities on endpoint devices using deep
  ensemble neural networks
Authors: Mayk Caldas Ramos and Andrew D. White
Categories: physics.chem-ph cs.LG
\\ ( https://arxiv.org/abs/2307.05318 ,  4343kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02851 (*cross-listing*)
replaced with revised version Wed, 6 Mar 2024 23:46:19 GMT   (18266kb,D)

Title: Generative Adversarial Networks for Stain Normalisation in
  Histopathology
Authors: Jack Breen, Kieran Zucker, Katie Allen, Nishant Ravikumar, Nicolas M.
  Orsi
Categories: eess.IV cs.CV cs.LG
Comments: Updated to add link to full publication at
  https://doi.org/10.1007/978-3-031-46238-2_11
\\ ( https://arxiv.org/abs/2308.02851 ,  18266kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05564 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 06:28:23 GMT   (19416kb,D)

Title: Large Skew-t Copula Models and Asymmetric Dependence in Intraday Equity
  Returns
Authors: Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn
Categories: econ.EM cs.LG q-fin.ST stat.CO
\\ ( https://arxiv.org/abs/2308.05564 ,  19416kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14785 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 06:20:39 GMT   (1240kb,D)

Title: A correlation-based fuzzy cluster validity index with secondary options
  detector
Authors: Nathakhun Wiroonsri and Onthada Preedasawakul
Categories: stat.ML cs.LG
Comments: 24 pages
MSC-class: 62H30
\\ ( https://arxiv.org/abs/2308.14785 ,  1240kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04475 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 04:32:51 GMT   (5702kb,D)

Title: Crystal Structure Prediction by Joint Equivariant Diffusion
Authors: Rui Jiao, Wenbing Huang, Peijia Lin, Jiaqi Han, Pin Chen, Yutong Lu,
  and Yang Liu
Categories: cond-mat.mtrl-sci cs.LG
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2309.04475 ,  5702kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01678 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 02:08:41 GMT   (5530kb,D)

Title: Score dynamics: scaling molecular dynamics with picoseconds timestep via
  conditional diffusion model
Authors: Tim Hsu, Babak Sadigh, Vasily Bulatov, Fei Zhou
Categories: physics.comp-ph cs.LG
\\ ( https://arxiv.org/abs/2310.01678 ,  5530kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17093
replaced with revised version Thu, 7 Mar 2024 00:30:52 GMT   (3155kb,D)

Title: Efficient Out-of-Distribution Detection with Prototypical
  Semi-Supervised Learning and Foundation Models
Authors: Evelyn Mannix and Howard Bondell
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.17093 ,  3155kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09417 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 02:35:32 GMT   (3961kb,D)

Title: DTP-Net: Learning to Reconstruct EEG signals in Time-Frequency Domain by
  Multi-scale Feature Reuse
Authors: Yan Pei, Jiahui Xu, Qianhao Chen, Chenhao Wang, Feng Yu, Lisan Zhang
  and Wei Luo
Categories: eess.SP cs.LG
Comments: 18 pages, 10 figures
Journal-ref: IEEE Journal of Biomedical and Health Informatics. 2024: 1-12
DOI: 10.1109/JBHI.2024.3358917
\\ ( https://arxiv.org/abs/2312.09417 ,  3961kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11126 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 01:56:56 GMT   (174kb,D)

Title: Harnessing Inherent Noises for Privacy Preservation in Quantum Machine
  Learning
Authors: Keyi Ju, Xiaoqi Qin, Hui Zhong, Xinyue Zhang, Miao Pan, Baoling Liu
Categories: quant-ph cs.CR cs.LG
Comments: 6 pages, 4 figures
\\ ( https://arxiv.org/abs/2312.11126 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10220
replaced with revised version Thu, 7 Mar 2024 08:49:26 GMT   (1863kb,D)

Title: AutoFT: Learning an Objective for Robust Fine-Tuning
Authors: Caroline Choi, Yoonho Lee, Annie Chen, Allan Zhou, Aditi Raghunathan,
  Chelsea Finn
Categories: cs.CV cs.LG
Comments: 18 pages
\\ ( https://arxiv.org/abs/2401.10220 ,  1863kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14512 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 17:45:17 GMT   (1801kb,D)

Title: Who Are We Missing? A Principled Approach to Characterizing the
  Underrepresented Population
Authors: Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph
Categories: stat.ME cs.CY cs.LG stat.AP
Comments: MOUD results TBD
\\ ( https://arxiv.org/abs/2401.14512 ,  1801kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02686 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 04:38:34 GMT   (5402kb,D)

Title: Multi-Region Markovian Gaussian Process: An Efficient Method to Discover
  Directional Communications Across Multiple Brain Regions
Authors: Weihan Li, Chengrui Li, Yule Wang, Anqi Wu
Categories: q-bio.NC cs.LG
\\ ( https://arxiv.org/abs/2402.02686 ,  5402kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16896
replaced with revised version Thu, 7 Mar 2024 15:59:17 GMT   (936kb,D)

Title: On Trojan Signatures in Large Language Models of Code
Authors: Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour
Categories: cs.CR cs.LG cs.SE
Comments: This work has been accepted at the International Conference on
  Learning Representations 2024 Workshop on Secure and Trustworthy Large
  Language Models, SeT LLM @ ICLR 2024 (Vienna, Austria)
\\ ( https://arxiv.org/abs/2402.16896 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18064
replaced with revised version Thu, 7 Mar 2024 11:21:04 GMT   (5570kb,D)

Title: Automated Testing of Spatially-Dependent Environmental Hypotheses
  through Active Transfer Learning
Authors: Nicholas Harrison, Nathan Wallace, Salah Sukkarieh
Categories: cs.RO cs.LG
Comments: Accepted for publication and presentation at ICRA 2024
\\ ( https://arxiv.org/abs/2402.18064 ,  5570kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19212 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 07:09:11 GMT   (38kb)

Title: Deep Reinforcement Learning: A Convex Optimization Approach
Authors: Ather Gattami
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2402.19212 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02639
replaced with revised version Thu, 7 Mar 2024 12:24:53 GMT   (0kb,I)

Title: False Positive Sampling-based Data Augmentation for Enhanced 3D Object
  Detection Accuracy
Authors: Jiyong Oh, Junhaeng Lee, Woongchan Byun, Minsang Kong and Sang Hun Lee
Categories: cs.CV cs.LG
Comments: There was an error in the experiment settings
\\ ( https://arxiv.org/abs/2403.02639 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03391 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 06:03:49 GMT   (407kb,D)

Title: CoRMF: Criticality-Ordered Recurrent Mean Field Ising Solver
Authors: Zhenyu Pan, Ammar Gilani, En-Jui Kuo, Zhuo Liu
Categories: stat.ML cond-mat.stat-mech cs.LG
\\ ( https://arxiv.org/abs/2403.03391 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03669 (*cross-listing*)
replaced with revised version Thu, 7 Mar 2024 09:20:35 GMT   (27kb,D)

Title: Spectral Algorithms on Manifolds through Diffusion
Authors: Weichun Xia and Lei Shi
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2403.03669 ,  27kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03905
replaced with revised version Thu, 7 Mar 2024 02:12:55 GMT   (91kb,D)

Title: Black-Box $k$-to-$1$-PCA Reductions: Theory and Applications
Authors: Arun Jambulapati, Syamantak Kumar, Jerry Li, Shourya Pandey, Ankit
  Pensia, Kevin Tian
Categories: math.NA cs.DS cs.LG cs.NA stat.ML
\\ ( https://arxiv.org/abs/2403.03905 ,  91kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
