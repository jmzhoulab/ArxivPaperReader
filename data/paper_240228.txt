paper_240228.txt


Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年2月28日 17:24
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon 26 Feb 24 19:00:00 GMT  to  Tue 27 Feb 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2402.16878
Date: Mon, 12 Feb 2024 19:10:11 GMT   (2357kb,D)

Title: EvoGPT-f: An Evolutionary GPT Framework for Benchmarking Formal Math
  Languages
Authors: Johnathan Mercer
Categories: cs.AI cs.CL cs.LG cs.NE
\\
  Formal mathematics is the discipline of translating mathematics into a
programming language in which any statement can be unequivocally checked by a
computer. Mathematicians and computer scientists have spent decades of
painstaking formalization efforts developing languages such as Coq, HOL, and
Lean. Machine learning research has converged on these formal math corpora and
given rise to an assortment of methodologies to aid in interactive and
automated theorem proving. However, these papers have primarily focused on one
method, for one proof task, in one language. This paper introduces EvoGPT-f: a
novel evolutionary framework for the first systematic quantitative analysis of
the differential machine learnability of five formal math corpora (Lean 3, Lean
4, Coq, HOL 4, HOL Light) using four tokenization methods (character,
word-level, Byte Pair Encoding and StarCoder tokenizer). This paper does not
put to rest the question of the "best" or "easiest" language to learn. Rather,
this framework and preliminary findings begin to illuminate the differential
machine learnability of these languages, offering a foundation to forge more
systematic quantitative and qualitative comparative research across
communities.
\\ ( https://arxiv.org/abs/2402.16878 ,  2357kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16905
Date: Sat, 24 Feb 2024 21:36:26 GMT   (85kb)

Title: Enforcing Temporal Constraints on Generative Agent Behavior with
  Reactive Synthesis
Authors: Raven Rothkopf, Hannah Tongxin Zeng, Mark Santolucito
Categories: cs.AI cs.LG cs.LO
Comments: 22 pages
\\
  The surge in popularity of Large Language Models (LLMs) has opened doors for
new approaches to the creation of interactive agents. However, managing the
temporal behavior of such agents over the course of an interaction remains
challenging. The stateful, long-term horizon and quantitative reasoning
required for coherent agent behavior does not fit well into the LLM paradigm.
We propose a combination of formal logic-based program synthesis and LLM
content generation to create generative agents that adhere to temporal
constraints. Our approach uses Temporal Stream Logic (TSL) to generate an
automaton that enforces a temporal structure on an agent and leaves the details
of each action for a moment in time to an LLM. By using TSL, we are able to
augment the generative agent where users have a higher level of guarantees on
behavior, better interpretability of the system, and more ability to build
agents in a modular way. We evaluate our approach on different tasks involved
in creating a coherent interactive agent specialized for various application
domains. We found that over all of the tasks, our approach using TSL achieves
at least 96% adherence, whereas the pure LLM-based approach demonstrates as low
as 14.67% adherence.
\\ ( https://arxiv.org/abs/2402.16905 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16924
Date: Mon, 26 Feb 2024 10:35:41 GMT   (470kb)

Title: Theoretical Unification of the Fractured Aspects of Information
Authors: Marcin J. Schroeder
Categories: cs.AI
Comments: 52 pages
\\
  The article has as its main objective the identification of fundamental
epistemological obstacles in the study of information related to unnecessary
methodological assumptions and the demystification of popular beliefs in the
fundamental divisions of the aspects of information that can be understood as
Bachelardian rupture of epistemological obstacles. These general considerations
are preceded by an overview of the motivations for the study of information and
the role of the concept of information in the conceptualization of
intelligence, complexity, and consciousness justifying the need for a
sufficiently general perspective in the study of information, and are followed
at the end of the article by a brief exposition of an example of a possible
application in the development of the unified theory of information free from
unnecessary divisions and claims of superiority of the existing preferences in
methodology. The reference to Gaston Bachelard and his ideas of epistemological
obstacles and epistemological ruptures seems highly appropriate for the
reflection on the development of information study, in particular in the
context of obstacles such as the absence of semantics of information,
negligence of its structural analysis, separation of its digital and analog
forms, and misguided use of mathematics.
\\ ( https://arxiv.org/abs/2402.16924 ,  470kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16973
Date: Mon, 26 Feb 2024 19:16:04 GMT   (4851kb,D)

Title: Successfully Guiding Humans with Imperfect Instructions by Highlighting
  Potential Errors and Suggesting Corrections
Authors: Lingjun Zhao, Khanh Nguyen, Hal Daum\'e III
Categories: cs.AI cs.CL cs.HC
\\
  This paper addresses the challenge of leveraging imperfect language models to
guide human decision-making in the context of a grounded navigation task. We
show that an imperfect instruction generation model can be complemented with an
effective communication mechanism to become more successful at guiding humans.
The communication mechanism we build comprises models that can detect potential
hallucinations in instructions and suggest practical alternatives, and an
intuitive interface to present that information to users. We show that this
approach reduces the human navigation error by up to 29% with no additional
cognitive burden. This result underscores the potential of integrating diverse
communication channels into AI systems to compensate for their imperfections
and enhance their utility for humans.
\\ ( https://arxiv.org/abs/2402.16973 ,  4851kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17032
Date: Mon, 26 Feb 2024 21:21:30 GMT   (574kb,D)

Title: REFACTOR: Learning to Extract Theorems from Proofs
Authors: Jin Peng Zhou, Yuhuai Wu, Qiyang Li, Roger Grosse
Categories: cs.AI cs.LG
Comments: ICLR 2024
\\
  Human mathematicians are often good at recognizing modular and reusable
theorems that make complex mathematical results within reach. In this paper, we
propose a novel method called theoREm-from-prooF extrACTOR (REFACTOR) for
training neural networks to mimic this ability in formal mathematical theorem
proving. We show on a set of unseen proofs, REFACTOR is able to extract 19.6%
of the theorems that humans would use to write the proofs. When applying the
model to the existing Metamath library, REFACTOR extracted 16 new theorems.
With newly extracted theorems, we show that the existing proofs in the MetaMath
database can be refactored. The new theorems are used very frequently after
refactoring, with an average usage of 733.5 times, and help shorten the proof
lengths. Lastly, we demonstrate that the prover trained on the new-theorem
refactored dataset proves more test theorems and outperforms state-of-the-art
baselines by frequently leveraging a diverse set of newly extracted theorems.
Code can be found at https://github.com/jinpz/refactor.
\\ ( https://arxiv.org/abs/2402.17032 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17161
Date: Tue, 27 Feb 2024 02:47:50 GMT   (2277kb,D)

Title: Large Language Model for Participatory Urban Planning
Authors: Zhilun Zhou, Yuming Lin, Depeng Jin, Yong Li
Categories: cs.AI cs.MA
Comments: arXiv admin note: text overlap with arXiv:2402.01698
\\
  Participatory urban planning is the mainstream of modern urban planning that
involves the active engagement of residents. However, the traditional
participatory paradigm requires experienced planning experts and is often
time-consuming and costly. Fortunately, the emerging Large Language Models
(LLMs) have shown considerable ability to simulate human-like agents, which can
be used to emulate the participatory process easily. In this work, we introduce
an LLM-based multi-agent collaboration framework for participatory urban
planning, which can generate land-use plans for urban regions considering the
diverse needs of residents. Specifically, we construct LLM agents to simulate a
planner and thousands of residents with diverse profiles and backgrounds. We
first ask the planner to carry out an initial land-use plan. To deal with the
different facilities needs of residents, we initiate a discussion among the
residents in each community about the plan, where residents provide feedback
based on their profiles. Furthermore, to improve the efficiency of discussion,
we adopt a fishbowl discussion mechanism, where part of the residents discuss
and the rest of them act as listeners in each round. Finally, we let the
planner modify the plan based on residents' feedback. We deploy our method on
two real-world regions in Beijing. Experiments show that our method achieves
state-of-the-art performance in residents satisfaction and inclusion metrics,
and also outperforms human experts in terms of service accessibility and
ecology metrics.
\\ ( https://arxiv.org/abs/2402.17161 ,  2277kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17168
Date: Tue, 27 Feb 2024 03:03:06 GMT   (682kb,D)

Title: Benchmarking Data Science Agents
Authors: Yuge Zhang, Qiyang Jiang, Xingyu Han, Nan Chen, Yuqing Yang, Kan Ren
Categories: cs.AI cs.CL
Comments: Source code and data are available at
  https://github.com/MetaCopilot/dseval
\\
  In the era of data-driven decision-making, the complexity of data analysis
necessitates advanced expertise and tools of data science, presenting
significant challenges even for specialists. Large Language Models (LLMs) have
emerged as promising aids as data science agents, assisting humans in data
analysis and processing. Yet their practical efficacy remains constrained by
the varied demands of real-world applications and complicated analytical
process. In this paper, we introduce DSEval -- a novel evaluation paradigm, as
well as a series of innovative benchmarks tailored for assessing the
performance of these agents throughout the entire data science lifecycle.
Incorporating a novel bootstrapped annotation method, we streamline dataset
preparation, improve the evaluation coverage, and expand benchmarking
comprehensiveness. Our findings uncover prevalent obstacles and provide
critical insights to inform future advancements in the field.
\\ ( https://arxiv.org/abs/2402.17168 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17270
Date: Tue, 27 Feb 2024 07:31:30 GMT   (468kb,D)

Title: Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social
  Dilemmas
Authors: Hao Guo, Chunjiang Mu, Yang Chen, Chen Shen, Shuyue Hu, Zhen Wang
Categories: cs.AI cs.GT cs.HC cs.LG cs.MA
\\
  The study of cooperation within social dilemmas has long been a fundamental
topic across various disciplines, including computer science and social
science. Recent advancements in Artificial Intelligence (AI) have significantly
reshaped this field, offering fresh insights into understanding and enhancing
cooperation. This survey examines three key areas at the intersection of AI and
cooperation in social dilemmas. First, focusing on multi-agent cooperation, we
review the intrinsic and external motivations that support cooperation among
rational agents, and the methods employed to develop effective strategies
against diverse opponents. Second, looking into human-agent cooperation, we
discuss the current AI algorithms for cooperating with humans and the human
biases towards AI agents. Third, we review the emergent field of leveraging AI
agents to enhance cooperation among humans. We conclude by discussing future
research avenues, such as using large language models, establishing unified
theoretical frameworks, revisiting existing theories of human cooperation, and
exploring multiple real-world applications.
\\ ( https://arxiv.org/abs/2402.17270 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17385
Date: Tue, 27 Feb 2024 10:24:50 GMT   (1095kb,D)

Title: Determinants of LLM-assisted Decision-Making
Authors: Eva Eigner and Thorsten H\"andler
Categories: cs.AI cs.HC
\\
  Decision-making is a fundamental capability in everyday life. Large Language
Models (LLMs) provide multifaceted support in enhancing human decision-making
processes. However, understanding the influencing factors of LLM-assisted
decision-making is crucial for enabling individuals to utilize LLM-provided
advantages and minimize associated risks in order to make more informed and
better decisions. This study presents the results of a comprehensive literature
analysis, providing a structural overview and detailed analysis of determinants
impacting decision-making with LLM support. In particular, we explore the
effects of technological aspects of LLMs, including transparency and prompt
engineering, psychological factors such as emotions and decision-making styles,
as well as decision-specific determinants such as task difficulty and
accountability. In addition, the impact of the determinants on the
decision-making process is illustrated via multiple application scenarios.
Drawing from our analysis, we develop a dependency framework that systematizes
possible interactions in terms of reciprocal interdependencies between these
determinants. Our research reveals that, due to the multifaceted interactions
with various determinants, factors such as trust in or reliance on LLMs, the
user's mental model, and the characteristics of information processing are
identified as significant aspects influencing LLM-assisted decision-making
processes. Our findings can be seen as crucial for improving decision quality
in human-AI collaboration, empowering both users and organizations, and
designing more effective LLM interfaces. Additionally, our work provides a
foundation for future empirical investigations on the determinants of
decision-making assisted by LLMs.
\\ ( https://arxiv.org/abs/2402.17385 ,  1095kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17431
Date: Tue, 27 Feb 2024 11:43:41 GMT   (1758kb,D)

Title: The KANDY Benchmark: Incremental Neuro-Symbolic Learning and Reasoning
  with Kandinsky Patterns
Authors: Luca Salvatore Lorello, Marco Lippi, Stefano Melacci
Categories: cs.AI cs.LG
\\
  Artificial intelligence is continuously seeking novel challenges and
benchmarks to effectively measure performance and to advance the
state-of-the-art. In this paper we introduce KANDY, a benchmarking framework
that can be used to generate a variety of learning and reasoning tasks inspired
by Kandinsky patterns. By creating curricula of binary classification tasks
with increasing complexity and with sparse supervisions, KANDY can be used to
implement benchmarks for continual and semi-supervised learning, with a
specific focus on symbol compositionality. Classification rules are also
provided in the ground truth to enable analysis of interpretable solutions.
Together with the benchmark generation pipeline, we release two curricula, an
easier and a harder one, that we propose as new challenges for the research
community. With a thorough experimental evaluation, we show how both
state-of-the-art neural models and purely symbolic approaches struggle with
solving most of the tasks, thus calling for the application of advanced
neuro-symbolic methods trained over time.
\\ ( https://arxiv.org/abs/2402.17431 ,  1758kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17546
Date: Tue, 27 Feb 2024 14:38:47 GMT   (8469kb,D)

Title: COCOA: CBT-based Conversational Counseling Agent using Memory
  Specialized in Cognitive Distortions and Dynamic Prompt
Authors: Suyeon Lee, Jieun Kang, Harim Kim, Kyoung-Mee Chung, Dongha Lee,
  Jinyoung Yeo
Categories: cs.AI cs.CL
Comments: 4 pages, 2 figures
\\
  The demand for conversational agents that provide mental health care is
consistently increasing. In this work, we develop a psychological counseling
agent, referred to as CoCoA, that applies Cognitive Behavioral Therapy (CBT)
techniques to identify and address cognitive distortions inherent in the
client's statements. Specifically, we construct a memory system to efficiently
manage information necessary for counseling while extracting high-level
insights about the client from their utterances. Additionally, to ensure that
the counseling agent generates appropriate responses, we introduce dynamic
prompting to flexibly apply CBT techniques and facilitate the appropriate
retrieval of information. We conducted dialogues between CoCoA and characters
from Character.ai, creating a dataset for evaluation. Then, we asked GPT to
evaluate the constructed counseling dataset, and our model demonstrated a
statistically significant difference from other models.
\\ ( https://arxiv.org/abs/2402.17546 ,  8469kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17553
Date: Tue, 27 Feb 2024 14:47:53 GMT   (36745kb,D)

Title: OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist
  Autonomous Agents for Desktop and Web
Authors: Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran
  Kamble, Waseem Alshikh, Ruslan Salakhutdinov
Categories: cs.AI cs.CL cs.CV cs.HC
\\
  For decades, human-computer interaction has fundamentally been manual. Even
today, almost all productive work done on the computer necessitates human input
at every step. Autonomous virtual agents represent an exciting step in
automating many of these menial tasks. Virtual agents would empower users with
limited technical proficiency to harness the full possibilities of computer
systems. They could also enable the efficient streamlining of numerous computer
tasks, ranging from calendar management to complex travel bookings, with
minimal human intervention. In this paper, we introduce OmniACT, the
first-of-a-kind dataset and benchmark for assessing an agent's capability to
generate executable programs to accomplish computer tasks. Our scope extends
beyond traditional web automation, covering a diverse range of desktop
applications. The dataset consists of fundamental tasks such as "Play the next
song", as well as longer horizon tasks such as "Send an email to John Doe
mentioning the time and place to meet". Specifically, given a pair of screen
image and a visually-grounded natural language task, the goal is to generate a
script capable of fully executing the task. We run several strong baseline
language model agents on our benchmark. The strongest baseline, GPT-4, performs
the best on our benchmark However, its performance level still reaches only 15%
of the human proficiency in generating executable scripts capable of completing
the task, demonstrating the challenge of our task for conventional web agents.
Our benchmark provides a platform to measure and evaluate the progress of
language model agents in automating computer tasks and motivates future work
towards building multimodal models that bridge large language models and the
visual grounding of computer screens.
\\ ( https://arxiv.org/abs/2402.17553 ,  36745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17574
Date: Tue, 27 Feb 2024 15:09:20 GMT   (2597kb,D)

Title: Agent-Pro: Learning to Evolve via Policy-Level Reflection and
  Optimization
Authors: Wenqi Zhang, Ke Tang, Hai Wu, Mengna Wang, Yongliang Shen, Guiyang
  Hou, Zeqi Tan, Peng Li, Yueting Zhuang, Weiming Lu
Categories: cs.AI cs.CL
Comments: LLM-based Agent
\\
  Large Language Models exhibit robust problem-solving capabilities for diverse
tasks. However, most LLM-based agents are designed as specific task solvers
with sophisticated prompt engineering, rather than agents capable of learning
and evolving through interactions. These task solvers necessitate manually
crafted prompts to inform task rules and regulate LLM behaviors, inherently
incapacitating to address complex dynamic scenarios e.g., large interactive
games. In light of this, we propose Agent-Pro: an LLM-based Agent with
Policy-level Reflection and Optimization that can learn a wealth of expertise
from interactive experiences and progressively elevate its behavioral policy.
Specifically, it involves a dynamic belief generation and reflection process
for policy evolution. Rather than action-level reflection, Agent-Pro
iteratively reflects on past trajectories and beliefs, fine-tuning its
irrational beliefs for a better policy. Moreover, a depth-first search is
employed for policy optimization, ensuring continual enhancement in policy
payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold'em,
outperforming vanilla LLM and specialized models. Our results show Agent-Pro
can learn and evolve in complex and dynamic scenes, which also benefits
numerous LLM-based applications.
\\ ( https://arxiv.org/abs/2402.17574 ,  2597kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17709
Date: Tue, 27 Feb 2024 17:41:58 GMT   (5760kb,D)

Title: Case-Based or Rule-Based: How Do Transformers Do the Math?
Authors: Yi Hu, Xiaojuan Tang, Haotong Yang, Muhan Zhang
Categories: cs.AI cs.CL
\\
  Despite the impressive performance in a variety of complex tasks, modern
large language models (LLMs) still have trouble dealing with some math problems
that are simple and intuitive for humans, such as addition. While we can easily
learn basic rules of addition and apply them to new problems of any length,
LLMs struggle to do the same. Instead, they may rely on similar "cases" seen in
the training corpus for help. We define these two different reasoning
mechanisms as "rule-based reasoning" and "case-based reasoning". Since
rule-based reasoning is essential for acquiring the systematic generalization
ability, we aim to explore exactly whether transformers use rule-based or
case-based reasoning for math problems. Through carefully designed intervention
experiments on five math tasks, we confirm that transformers are performing
case-based reasoning, no matter whether scratchpad is used, which aligns with
the previous observations that transformers use subgraph matching/shortcut
learning to reason. To mitigate such problems, we propose a Rule-Following
Fine-Tuning (RFFT) technique to teach transformers to perform rule-based
reasoning. Specifically, we provide explicit rules in the input and then
instruct transformers to recite and follow the rules step by step. Through
RFFT, we successfully enable LLMs fine-tuned on 1-5 digit addition to
generalize to up to 12-digit addition with over 95% accuracy, which is over 40%
higher than scratchpad. The significant improvement demonstrates that teaching
LLMs to explicitly use rules helps them learn rule-based reasoning and
generalize better in length.
\\ ( https://arxiv.org/abs/2402.17709 ,  5760kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17739
Date: Tue, 27 Feb 2024 18:18:23 GMT   (2303kb,D)

Title: reBandit: Random Effects based Online RL algorithm for Reducing Cannabis
  Use
Authors: Susobhan Ghosh, Yongyi Guo, Pei-Yao Hung, Lara Coughlin, Erin Bonar,
  Inbal Nahum-Shani, Maureen Walton, Susan Murphy
Categories: cs.AI cs.LG
\\
  The escalating prevalence of cannabis use, and associated cannabis-use
disorder (CUD), poses a significant public health challenge globally. With a
notably wide treatment gap, especially among emerging adults (EAs; ages 18-25),
addressing cannabis use and CUD remains a pivotal objective within the 2030
United Nations Agenda for Sustainable Development Goals (SDG). In this work, we
develop an online reinforcement learning (RL) algorithm called reBandit which
will be utilized in a mobile health study to deliver personalized mobile health
interventions aimed at reducing cannabis use among EAs. reBandit utilizes
random effects and informative Bayesian priors to learn quickly and efficiently
in noisy mobile health environments. Moreover, reBandit employs Empirical Bayes
and optimization techniques to autonomously update its hyper-parameters online.
To evaluate the performance of our algorithm, we construct a simulation testbed
using data from a prior study, and compare against commonly used algorithms in
mobile health studies. We show that reBandit performs equally well or better
than all the baseline algorithms, and the performance gap widens as population
heterogeneity increases in the simulation environment, proving its adeptness to
adapt to diverse population of study participants.
\\ ( https://arxiv.org/abs/2402.17739 ,  2303kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16986
Date: Mon, 26 Feb 2024 19:35:45 GMT   (93kb,D)

Title: Long Dialog Summarization: An Analysis
Authors: Ankan Mullick, Ayan Kumar Bhowmick, Raghav R, Ravi Kokku, Prasenjit
  Dey, Pawan Goyal, Niloy Ganguly
Categories: cs.CL cs.IR
\\
  Dialog summarization has become increasingly important in managing and
comprehending large-scale conversations across various domains. This task
presents unique challenges in capturing the key points, context, and nuances of
multi-turn long conversations for summarization. It is worth noting that the
summarization techniques may vary based on specific requirements such as in a
shopping-chatbot scenario, the dialog summary helps to learn user preferences,
whereas in the case of a customer call center, the summary may involve the
problem attributes that a user specified, and the final resolution provided.
This work emphasizes the significance of creating coherent and contextually
rich summaries for effective communication in various applications. We explore
current state-of-the-art approaches for long dialog summarization in different
domains and benchmark metrics based evaluations show that one single model does
not perform well across various areas for distinct summarization tasks.
\\ ( https://arxiv.org/abs/2402.16986 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16998
Date: Mon, 26 Feb 2024 20:13:58 GMT   (10336kb,D)

Title: What Do Language Models Hear? Probing for Auditory Representations in
  Language Models
Authors: Jerry Ngo, Yoon Kim
Categories: cs.CL cs.AI cs.LG cs.SD eess.AS
\\
  This work explores whether language models encode meaningfully grounded
representations of sounds of objects. We learn a linear probe that retrieves
the correct text representation of an object given a snippet of audio related
to that object, where the sound representation is given by a pretrained audio
model. This probe is trained via a contrastive loss that pushes the language
representations and sound representations of an object to be close to one
another. After training, the probe is tested on its ability to generalize to
objects that were not seen during training. Across different language models
and audio models, we find that the probe generalization is above chance in many
cases, indicating that despite being trained only on raw text, language models
encode grounded knowledge of sounds for some objects.
\\ ( https://arxiv.org/abs/2402.16998 ,  10336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17008
Date: Mon, 26 Feb 2024 20:33:50 GMT   (11043kb,D)

Title: Benchmarking LLMs on the Semantic Overlap Summarization Task
Authors: John Salvador, Naman Bansal, Mousumi Akter, Souvika Sarkar, Anupam
  Das, and Shubhra Kanti Karmaker ("Santu")
Categories: cs.CL
\\
  Semantic Overlap Summarization (SOS) is a constrained multi-document
summarization task, where the constraint is to capture the common/overlapping
information between two alternative narratives. While recent advancements in
Large Language Models (LLMs) have achieved superior performance in numerous
summarization tasks, a benchmarking study of the SOS task using LLMs is yet to
be performed. As LLMs' responses are sensitive to slight variations in prompt
design, a major challenge in conducting such a benchmarking study is to
systematically explore a variety of prompts before drawing a reliable
conclusion. Fortunately, very recently, the TELeR taxonomy has been proposed
which can be used to design and explore various prompts for LLMs. Using this
TELeR taxonomy and 15 popular LLMs, this paper comprehensively evaluates LLMs
on the SOS Task, assessing their ability to summarize overlapping information
from multiple alternative narratives. For evaluation, we report
well-established metrics like ROUGE, BERTscore, and SEM-F1$ on two different
datasets of alternative narratives. We conclude the paper by analyzing the
strengths and limitations of various LLMs in terms of their capabilities in
capturing overlapping information The code and datasets used to conduct this
study are available at https://anonymous.4open.science/r/llm_eval-E16D.
\\ ( https://arxiv.org/abs/2402.17008 ,  11043kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17010
Date: Mon, 26 Feb 2024 20:35:32 GMT   (449kb,D)

Title: Can Large Language Models Recall Reference Location Like Humans?
Authors: Ye Wang, Xinrun Xu, Rui Xie, Wenxin Hu, Wei Ye
Categories: cs.CL cs.AI
\\
  When completing knowledge-intensive tasks, humans sometimes need not just an
answer but also a corresponding reference passage for auxiliary reading.
Previous methods required obtaining pre-segmented article chunks through
additional retrieval models. This paper explores leveraging the parameterized
knowledge stored during the pre-training phase of large language models (LLMs)
to independently recall reference passage from any starting position. We
propose a two-stage framework that simulates the scenario of humans recalling
easily forgotten references. Initially, the LLM is prompted to recall document
title identifiers to obtain a coarse-grained document set. Then, based on the
acquired coarse-grained document set, it recalls fine-grained passage. In the
two-stage recall process, we use constrained decoding to ensure that content
outside of the stored documents is not generated. To increase speed, we only
recall a short prefix in the second stage, then locate its position to retrieve
a complete passage. Experiments on KILT knowledge-sensitive tasks have verified
that LLMs can independently recall reference passage location in various task
forms, and the obtained reference significantly assist downstream tasks.
\\ ( https://arxiv.org/abs/2402.17010 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17011
Date: Mon, 26 Feb 2024 20:35:34 GMT   (1807kb,D)

Title: DiffuCOMET: Contextual Commonsense Knowledge Diffusion
Authors: Silin Gao, Mete Ismayilzada, Mengjie Zhao, Hiromi Wakaki, Yuki
  Mitsufuji, Antoine Bosselut
Categories: cs.CL
\\
  Inferring contextually-relevant and diverse commonsense to understand
narratives remains challenging for knowledge models. In this work, we develop a
series of knowledge models, DiffuCOMET, that leverage diffusion to learn to
reconstruct the implicit semantic connections between narrative contexts and
relevant commonsense knowledge. Across multiple diffusion steps, our method
progressively refines a representation of commonsense facts that is anchored to
a narrative, producing contextually-relevant and diverse commonsense inferences
for an input context. To evaluate DiffuCOMET, we introduce new metrics for
commonsense inference that more closely measure knowledge diversity and
contextual relevance. Our results on two different benchmarks, ComFact and
WebNLG+, show that knowledge generated by DiffuCOMET achieves a better
trade-off between commonsense diversity, contextual relevance and alignment to
known gold references, compared to baseline knowledge models.
\\ ( https://arxiv.org/abs/2402.17011 ,  1807kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17013
Date: Mon, 26 Feb 2024 20:42:40 GMT   (10389kb,D)

Title: Towards Explainability and Fairness in Swiss Judgement Prediction:
  Benchmarking on a Multilingual Dataset
Authors: Santosh T.Y.S.S, Nina Baumgartner, Matthias St\"urmer, Matthias
  Grabmair, Joel Niklaus
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-COLING 2024
MSC-class: 68T50
ACM-class: I.2
\\
  The assessment of explainability in Legal Judgement Prediction (LJP) systems
is of paramount importance in building trustworthy and transparent systems,
particularly considering the reliance of these systems on factors that may lack
legal relevance or involve sensitive attributes. This study delves into the
realm of explainability and fairness in LJP models, utilizing Swiss Judgement
Prediction (SJP), the only available multilingual LJP dataset. We curate a
comprehensive collection of rationales that `support' and `oppose' judgement
from legal experts for 108 cases in German, French, and Italian. By employing
an occlusion-based explainability approach, we evaluate the explainability
performance of state-of-the-art monolingual and multilingual BERT-based LJP
models, as well as models developed with techniques such as data augmentation
and cross-lingual transfer, which demonstrated prediction performance
improvement. Notably, our findings reveal that improved prediction performance
does not necessarily correspond to enhanced explainability performance,
underscoring the significance of evaluating models from an explainability
perspective. Additionally, we introduce a novel evaluation framework, Lower
Court Insertion (LCI), which allows us to quantify the influence of lower court
information on model predictions, exposing current models' biases.
\\ ( https://arxiv.org/abs/2402.17013 ,  10389kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17014
Date: Mon, 26 Feb 2024 20:43:48 GMT   (9768kb,D)

Title: Z-AGI Labs at ClimateActivism 2024: Stance and Hate Event Detection on
  Social Media
Authors: Nikhil Narayan, Mrutyunjay Biswal
Categories: cs.CL
Comments: Accepted as Working Notes in CASE-EACL 2024
\\
  In the digital realm, rich data serves as a crucial source of insights into
the complexities of social, political, and economic landscapes. Addressing the
growing need for high-quality information on events and the imperative to
combat hate speech, this research led to the establishment of the Shared Task
on Climate Activism Stance and Hate Event Detection at CASE 2024. Focused on
climate activists contending with hate speech on social media, our study
contributes to hate speech identification from tweets. Analyzing three
sub-tasks - Hate Speech Detection (Sub-task A), Targets of Hate Speech
Identification (Sub-task B), and Stance Detection (Sub-task C) - Team Z-AGI
Labs evaluated various models, including LSTM, Xgboost, and LGBM based on
Tf-Idf. Results unveiled intriguing variations, with Catboost excelling in
Subtask-B (F1: 0.5604) and Subtask-C (F1: 0.7081), while LGBM emerged as the
top-performing model for Subtask-A (F1: 0.8684). This research provides
valuable insights into the suitability of classical machine learning models for
climate hate speech and stance detection, aiding informed model selection for
robust mechanisms.
\\ ( https://arxiv.org/abs/2402.17014 ,  9768kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17016
Date: Mon, 26 Feb 2024 20:53:12 GMT   (736kb,D)

Title: Multi-Task Contrastive Learning for 8192-Token Bilingual Text Embeddings
Authors: Isabelle Mohr, Markus Krimmel, Saba Sturua, Mohammad Kalim Akram,
  Andreas Koukounas, Michael G\"unther, Georgios Mastrapas, Vinit Ravishankar,
  Joan Fontanals Mart\'inez, Feng Wang, Qi Liu, Ziniu Yu, Jie Fu, Saahil
  Ognawala, Susana Guzman, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
Categories: cs.CL cs.AI cs.IR
MSC-class: 68T50
ACM-class: I.2.7
\\
  We introduce a novel suite of state-of-the-art bilingual text embedding
models that are designed to support English and another target language. These
models are capable of processing lengthy text inputs with up to 8192 tokens,
making them highly versatile for a range of natural language processing tasks
such as text retrieval, clustering, and semantic textual similarity (STS)
calculations.
  By focusing on bilingual models and introducing a unique multi-task learning
objective, we have significantly improved the model performance on STS tasks,
which outperforms the capabilities of existing multilingual models in both
target language understanding and cross-lingual evaluation tasks. Moreover, our
bilingual models are more efficient, requiring fewer parameters and less memory
due to their smaller vocabulary needs. Furthermore, we have expanded the
Massive Text Embedding Benchmark (MTEB) to include benchmarks for German and
Spanish embedding models. This integration aims to stimulate further research
and advancement in text embedding technologies for these languages.
\\ ( https://arxiv.org/abs/2402.17016 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17019
Date: Mon, 26 Feb 2024 20:56:06 GMT   (3242kb,D)

Title: Leveraging Large Language Models for Learning Complex Legal Concepts
  through Storytelling
Authors: Hang Jiang, Xiajie Zhang, Robert Mahari, Daniel Kessler, Eric Ma, Tal
  August, Irene Li, Alex 'Sandy' Pentland, Yoon Kim, Jad Kabbara, Deb Roy
Categories: cs.CL cs.HC
\\
  Making legal knowledge accessible to non-experts is crucial for enhancing
general legal literacy and encouraging civic participation in democracy.
However, legal documents are often challenging to understand for people without
legal backgrounds. In this paper, we present a novel application of large
language models (LLMs) in legal education to help non-experts learn intricate
legal concepts through storytelling, an effective pedagogical tool in conveying
complex and abstract concepts. We also introduce a new dataset LegalStories,
which consists of 295 complex legal doctrines, each accompanied by a story and
a set of multiple-choice questions generated by LLMs. To construct the dataset,
we experiment with various LLMs to generate legal stories explaining these
concepts. Furthermore, we use an expert-in-the-loop method to iteratively
design multiple-choice questions. Then, we evaluate the effectiveness of
storytelling with LLMs through an RCT experiment with legal novices on 10
samples from the dataset. We find that LLM-generated stories enhance
comprehension of legal concepts and interest in law among non-native speakers
compared to only definitions. Moreover, stories consistently help participants
relate legal concepts to their lives. Finally, we find that learning with
stories shows a higher retention rate for non-native speakers in the follow-up
assessment. Our work has strong implications for using LLMs in promoting
teaching and learning in the legal field and beyond.
\\ ( https://arxiv.org/abs/2402.17019 ,  3242kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17097
Date: Tue, 27 Feb 2024 00:22:18 GMT   (300kb,D)

Title: Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM
  Responses
Authors: Juyeon Kim, Jeongeun Lee, Yoonho Chang, Chanyeol Choi, Junseong Kim,
  Jy-yong Sohn
Categories: cs.CL cs.AI
Comments: Preprint
\\
  Mitigating hallucination issues is one of the main challenges of LLMs we need
to overcome, in order to reliably use them in real-world scenarios. Recently,
various methods are proposed to check the factual errors in the LLM-generated
texts and revise them accordingly, to reduce the hallucination issue. In this
paper, we propose Re-Ex, a method of revising LLM-generated texts, which
introduces a novel step dubbed as the factual error explanation step. Re-Ex
revises the initial response of LLMs using 3-steps: first, external tools are
used to get the evidences on the factual errors in the response; second, LLMs
are instructed to explain the problematic parts of the response based on the
evidences gathered in the first step; finally, LLMs revise the response using
the explanation obtained in the second step. In addition to the explanation
step, we propose new prompting techniques to reduce the amount of tokens and
wall-clock time required for the response revision process. Compared with
existing methods including Factool, CoVE, and RARR, Re-Ex provides better
revision performance with less time and fewer tokens in multiple benchmarks.
\\ ( https://arxiv.org/abs/2402.17097 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17119
Date: Tue, 27 Feb 2024 01:25:52 GMT   (8157kb,D)

Title: Creating Suspenseful Stories: Iterative Planning with Large Language
  Models
Authors: Kaige Xie, Mark Riedl
Categories: cs.CL
Comments: Accepted to EACL 2024
\\
  Automated story generation has been one of the long-standing challenges in
NLP. Among all dimensions of stories, suspense is very common in human-written
stories but relatively under-explored in AI-generated stories. While recent
advances in large language models (LLMs) have greatly promoted language
generation in general, state-of-the-art LLMs are still unreliable when it comes
to suspenseful story generation. We propose a novel iterative-prompting-based
planning method that is grounded in two theoretical foundations of story
suspense from cognitive psychology and narratology. This theory-grounded method
works in a fully zero-shot manner and does not rely on any supervised story
corpora. To the best of our knowledge, this paper is the first attempt at
suspenseful story generation with LLMs. Extensive human evaluations of the
generated suspenseful stories demonstrate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2402.17119 ,  8157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17124
Date: Tue, 27 Feb 2024 01:37:23 GMT   (1607kb,D)

Title: Fact-and-Reflection (FaR) Improves Confidence Calibration of Large
  Language Models
Authors: Xinran Zhao, Hongming Zhang, Xiaoman Pan, Wenlin Yao, Dong Yu,
  Tongshuang Wu, Jianshu Chen
Categories: cs.CL
Comments: 17 pages, 10 figures
\\
  For a LLM to be trustworthy, its confidence level should be well-calibrated
with its actual performance. While it is now common sense that LLM performances
are greatly impacted by prompts, the confidence calibration in prompting LLMs
has yet to be thoroughly explored. In this paper, we explore how different
prompting strategies influence LLM confidence calibration and how it could be
improved. We conduct extensive experiments on six prompting methods in the
question-answering context and we observe that, while these methods help
improve the expected LLM calibration, they also trigger LLMs to be
over-confident when responding to some instances. Inspired by human cognition,
we propose Fact-and-Reflection (FaR) prompting, which improves the LLM
calibration in two steps. First, FaR elicits the known "facts" that are
relevant to the input prompt from the LLM. And then it asks the model to
"reflect" over them to generate the final answer. Experiments show that FaR
prompting achieves significantly better calibration; it lowers the Expected
Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR
prompting even elicits the capability of verbally expressing concerns in less
confident scenarios, which helps trigger retrieval augmentation for solving
these harder instances.
\\ ( https://arxiv.org/abs/2402.17124 ,  1607kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17151
Date: Tue, 27 Feb 2024 02:36:43 GMT   (739kb,D)

Title: Clustering Document Parts: Detecting and Characterizing Influence
  Campaigns From Documents
Authors: Zhengxiang Wang, Owen Rambow
Categories: cs.CL
Comments: 12 pages, 2 figures, 5 tables
\\
  We propose a novel clustering pipeline to detect and characterize influence
campaigns from documents. This approach clusters parts of document, detects
clusters that likely reflect an influence campaign, and then identifies
documents linked to an influence campaign via their association with the
high-influence clusters. Our approach outperforms both the direct
document-level classification and the direct document-level clustering approach
in predicting if a document is part of an influence campaign. We propose
various novel techniques to enhance our pipeline, including using an existing
event factuality prediction system to obtain document parts, and aggregating
multiple clustering experiments to improve the performance of both cluster and
document classification. Classifying documents on the top of clustering not
only accurately extracts the parts of the documents that are relevant to
influence campaigns, but also capture influence campaigns as a coordinated and
holistic phenomenon. Our approach makes possible more fine-grained and
interpretable characterizations of influence campaigns from documents.
\\ ( https://arxiv.org/abs/2402.17151 ,  739kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17184
Date: Tue, 27 Feb 2024 03:40:44 GMT   (122kb,D)

Title: Extreme Encoder Output Frame Rate Reduction: Improving Computational
  Latencies of Large End-to-End Models
Authors: Rohit Prabhavalkar, Zhong Meng, Weiran Wang, Adam Stooke, Xingyu Cai,
  Yanzhang He, Arun Narayanan, Dongseong Hwang, Tara N. Sainath, Pedro J.
  Moreno
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to 2024 IEEE International Conference on Acoustics, Speech,
  and Signal Processing (ICASSP 2024)
\\
  The accuracy of end-to-end (E2E) automatic speech recognition (ASR) models
continues to improve as they are scaled to larger sizes, with some now reaching
billions of parameters. Widespread deployment and adoption of these models,
however, requires computationally efficient strategies for decoding. In the
present work, we study one such strategy: applying multiple frame reduction
layers in the encoder to compress encoder outputs into a small number of output
frames. While similar techniques have been investigated in previous work, we
achieve dramatically more reduction than has previously been demonstrated
through the use of multiple funnel reduction layers. Through ablations, we
study the impact of various architectural choices in the encoder to identify
the most effective strategies. We demonstrate that we can generate one encoder
output frame for every 2.56 sec of input speech, without significantly
affecting word error rate on a large-scale voice search task, while improving
encoder and decoder latencies by 48% and 92% respectively, relative to a strong
but computationally expensive baseline.
\\ ( https://arxiv.org/abs/2402.17184 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17189
Date: Tue, 27 Feb 2024 04:08:59 GMT   (724kb)

Title: An Effective Mixture-Of-Experts Approach For Code-Switching Speech
  Recognition Leveraging Encoder Disentanglement
Authors: Tzu-Ting Yang, Hsin-Wei Wang, Yi-Cheng Wang, Chi-Han Lin, and Berlin
  Chen
Categories: cs.CL cs.AI cs.SD eess.AS
Comments: ICASSP 2024
\\
  With the massive developments of end-to-end (E2E) neural networks, recent
years have witnessed unprecedented breakthroughs in automatic speech
recognition (ASR). However, the codeswitching phenomenon remains a major
obstacle that hinders ASR from perfection, as the lack of labeled data and the
variations between languages often lead to degradation of ASR performance. In
this paper, we focus exclusively on improving the acoustic encoder of E2E ASR
to tackle the challenge caused by the codeswitching phenomenon. Our main
contributions are threefold: First, we introduce a novel disentanglement loss
to enable the lower-layer of the encoder to capture inter-lingual acoustic
information while mitigating linguistic confusion at the higher-layer of the
encoder. Second, through comprehensive experiments, we verify that our proposed
method outperforms the prior-art methods using pretrained dual-encoders,
meanwhile having access only to the codeswitching corpus and consuming half of
the parameterization. Third, the apparent differentiation of the encoders'
output features also corroborates the complementarity between the
disentanglement loss and the mixture-of-experts (MoE) architecture.
\\ ( https://arxiv.org/abs/2402.17189 ,  724kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17193
Date: Tue, 27 Feb 2024 04:18:49 GMT   (646kb,D)

Title: When Scaling Meets LLM Finetuning: The Effect of Data, Model and
  Finetuning Method
Authors: Biao Zhang, Zhongtao Liu, Colin Cherry, Orhan Firat
Categories: cs.CL cs.LG
Comments: ICLR24
\\
  While large language models (LLMs) often adopt finetuning to unlock their
capabilities for downstream applications, our understanding on the inductive
biases (especially the scaling properties) of different finetuning methods is
still limited. To fill this gap, we conduct systematic experiments studying
whether and how different scaling factors, including LLM model size,
pretraining data size, new finetuning parameter size and finetuning data size,
affect the finetuning performance. We consider two types of finetuning --
full-model tuning (FMT) and parameter efficient tuning (PET, including prompt
tuning and LoRA), and explore their scaling behaviors in the data-limited
regime where the LLM model size substantially outweighs the finetuning data
size. Based on two sets of pretrained bilingual LLMs from 1B to 16B and
experiments on bilingual machine translation and multilingual summarization
benchmarks, we find that 1) LLM finetuning follows a powerbased multiplicative
joint scaling law between finetuning data size and each other scaling factor;
2) LLM finetuning benefits more from LLM model scaling than pretraining data
scaling, and PET parameter scaling is generally ineffective; and 3) the optimal
finetuning method is highly task- and finetuning data-dependent. We hope our
findings could shed light on understanding, selecting and developing LLM
finetuning methods.
\\ ( https://arxiv.org/abs/2402.17193 ,  646kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17205
Date: Tue, 27 Feb 2024 04:55:03 GMT   (8840kb,D)

Title: Measuring Vision-Language STEM Skills of Neural Models
Authors: Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, Chenguang Wang
Categories: cs.CL cs.AI cs.LG
Comments: Accepted in ICLR 2024
\\
  We introduce a new challenge to test the STEM skills of neural models. The
problems in the real world often require solutions, combining knowledge from
STEM (science, technology, engineering, and math). Unlike existing datasets,
our dataset requires the understanding of multimodal vision-language
information of STEM. Our dataset features one of the largest and most
comprehensive datasets for the challenge. It includes 448 skills and 1,073,146
questions spanning all STEM subjects. Compared to existing datasets that often
focus on examining expert-level ability, our dataset includes fundamental
skills and questions designed based on the K-12 curriculum. We also add
state-of-the-art foundation models such as CLIP and GPT-3.5-Turbo to our
benchmark. Results show that the recent model advances only help master a very
limited number of lower grade-level skills (2.5% in the third grade) in our
dataset. In fact, these models are still well below (averaging 54.7%) the
performance of elementary students, not to mention near expert-level
performance. To understand and increase the performance on our dataset, we
teach the models on a training split of our dataset. Even though we observe
improved performance, the model performance remains relatively low compared to
average elementary students. To solve STEM problems, we will need novel
algorithmic innovations from the community.
\\ ( https://arxiv.org/abs/2402.17205 ,  8840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17226
Date: Tue, 27 Feb 2024 05:37:10 GMT   (2443kb,D)

Title: Reasoning in Conversation: Solving Subjective Tasks through Dialogue
  Simulation for Large Language Models
Authors: Xiaolong Wang, Yile Wang, Yuanchi Zhang, Fuwen Luo, Peng Li, Maosong
  Sun, Yang Liu
Categories: cs.CL
\\
  Large Language Models (LLMs) have achieved remarkable performance in
objective tasks such as open-domain question answering and mathematical
reasoning, which can often be solved through recalling learned factual
knowledge or chain-of-thought style reasoning. However, we find that the
performance of LLMs in subjective tasks is still unsatisfactory, such as
metaphor recognition, dark humor detection, etc. Compared to objective tasks,
subjective tasks focus more on interpretation or emotional response rather than
a universally accepted reasoning pathway. Based on the characteristics of the
tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC
(Reasoning in Conversation), a method that focuses on solving subjective tasks
through dialogue simulation. The motivation of RiC is to mine useful contextual
information by simulating dialogues instead of supplying chain-of-thought style
rationales, thereby offering potential useful knowledge behind dialogues for
giving the final answers. We evaluate both API-based and open-source LLMs
including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental
results show that RiC can yield significant improvement compared with various
baselines.
\\ ( https://arxiv.org/abs/2402.17226 ,  2443kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17231
Date: Tue, 27 Feb 2024 05:50:35 GMT   (630kb,D)

Title: MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical
  Reasoning
Authors: Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni
Categories: cs.CL
\\
  Tool-augmented Large Language Models (TALM) are known to enhance the skillset
of large language models (LLM), thereby, leading to their improved reasoning
abilities across many tasks. While, TALMs have been successfully employed in
different question-answering benchmarks, their efficacy on complex mathematical
reasoning benchmarks, and the potential complimentary benefits offered by tools
for knowledge retrieval and mathematical equation solving, are open research
questions. In this work, we present MATHSENSEI, a tool-augmented large language
model for mathematical reasoning. Augmented with tools for knowledge retrieval
(Bing Web Search), program execution (Python), and symbolic equation solving
(Wolfram-Alpha), we study the complimentary benefits of these tools through
evaluations on mathematical reasoning datasets. We perform exhaustive ablations
on MATH,a popular dataset for evaluating mathematical reasoning on diverse
mathematical disciplines. We also conduct experiments involving well-known tool
planners to study the impact of tool sequencing on the model performance.
MATHSENSEI achieves 13.5% better accuracy over gpt-3.5-turbo with
chain-of-thought on the MATH dataset. We further observe that TALMs are not as
effective for simpler math word problems (in GSM-8k), and the benefit increases
as the complexity and required knowledge increases (progressively over AQuA,
MMLU-Math, and higher level complex questions in MATH). The code and data are
available at https://github.com/Debrup-61/MathSensei.
\\ ( https://arxiv.org/abs/2402.17231 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17256
Date: Tue, 27 Feb 2024 07:02:10 GMT   (1228kb,D)

Title: Beyond the Known: Investigating LLMs Performance on Out-of-Domain Intent
  Detection
Authors: Pei Wang, Keqing He, Yejie Wang, Xiaoshuai Song, Yutao Mou, Jingang
  Wang, Yunsen Xian, Xunliang Cai, Weiran Xu
Categories: cs.CL
Journal-ref: LREC-COLING 2024
\\
  Out-of-domain (OOD) intent detection aims to examine whether the user's query
falls outside the predefined domain of the system, which is crucial for the
proper functioning of task-oriented dialogue (TOD) systems. Previous methods
address it by fine-tuning discriminative models. Recently, some studies have
been exploring the application of large language models (LLMs) represented by
ChatGPT to various downstream tasks, but it is still unclear for their ability
on OOD detection task.This paper conducts a comprehensive evaluation of LLMs
under various experimental settings, and then outline the strengths and
weaknesses of LLMs. We find that LLMs exhibit strong zero-shot and few-shot
capabilities, but is still at a disadvantage compared to models fine-tuned with
full resource. More deeply, through a series of additional analysis
experiments, we discuss and summarize the challenges faced by LLMs and provide
guidance for future work including injecting domain knowledge, strengthening
knowledge transfer from IND(In-domain) to OOD, and understanding long
instructions.
\\ ( https://arxiv.org/abs/2402.17256 ,  1228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17262
Date: Tue, 27 Feb 2024 07:11:59 GMT   (2134kb,D)

Title: Speak Out of Turn: Safety Vulnerability of Large Language Models in
  Multi-turn Dialogue
Authors: Zhenhong Zhou, Jiuyang Xiang, Haopeng Chen, Quan Liu, Zherui Li, Sen
  Su
Categories: cs.CL cs.AI
Comments: working in progress 23pages, 18 figures
\\
  Large Language Models (LLMs) have been demonstrated to generate illegal or
unethical responses, particularly when subjected to "jailbreak." Research on
jailbreak has highlighted the safety issues of LLMs. However, prior studies
have predominantly focused on single-turn dialogue, ignoring the potential
complexities and risks presented by multi-turn dialogue, a crucial mode through
which humans derive information from LLMs. In this paper, we argue that humans
could exploit multi-turn dialogue to induce LLMs into generating harmful
information. LLMs may not intend to reject cautionary or borderline unsafe
queries, even if each turn is closely served for one malicious purpose in a
multi-turn dialogue. Therefore, by decomposing an unsafe query into several
sub-queries for multi-turn dialogue, we induced LLMs to answer harmful
sub-questions incrementally, culminating in an overall harmful response. Our
experiments, conducted across a wide range of LLMs, indicate current
inadequacies in the safety mechanisms of LLMs in multi-turn dialogue. Our
findings expose vulnerabilities of LLMs in complex scenarios involving
multi-turn dialogue, presenting new challenges for the safety of LLMs.
\\ ( https://arxiv.org/abs/2402.17262 ,  2134kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17263
Date: Tue, 27 Feb 2024 07:14:12 GMT   (6980kb,D)

Title: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning
Authors: Pengjie Ren, Chengshun Shi, Shiguang Wu, Mengqi Zhang, Zhaochun Ren,
  Maarten de Rijke, Zhumin Chen, Jiahuan Pei
Categories: cs.CL
Comments: 12 pages, 8 figures
MSC-class: 68T50
ACM-class: I.2.7
\\
  Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring
pre-trained large language models (LLMs), especially as the models' scale and
the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the
idea that the adaptation process is intrinsically low-dimensional, i.e.,
significant model changes can be represented with relatively few parameters.
However, decreasing the rank encounters challenges with generalization errors
for specific tasks when compared to full-parameter fine-tuning. We present
MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters
while maintaining a higher rank, thereby offering improved performance
potential. The core idea is to freeze original pretrained weights and train a
group of mini LoRAs with only a small number of parameters. This can capture a
significant degree of diversity among mini LoRAs, thus promoting better
generalization ability. We conduct a theoretical analysis and empirical studies
on various NLP tasks. Our experimental results show that, compared to LoRA,
MELoRA achieves better performance with 8 times fewer trainable parameters on
natural language understanding tasks and 36 times fewer trainable parameters on
instruction following tasks, which demonstrates the effectiveness of MELoRA.
\\ ( https://arxiv.org/abs/2402.17263 ,  6980kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17302
Date: Tue, 27 Feb 2024 08:24:32 GMT   (9350kb,D)

Title: Can LLM Generate Culturally Relevant Commonsense QA Data? Case Study in
  Indonesian and Sundanese
Authors: Rifki Afina Putri, Faiz Ghifari Haznitrama, Dea Adhista, Alice Oh
Categories: cs.CL
\\
  Large Language Models (LLMs) are increasingly being used to generate
synthetic data for training and evaluating models. However, it is unclear
whether they can generate a good quality of question answering (QA) dataset
that incorporates knowledge and cultural nuance embedded in a language,
especially for low-resource languages. In this study, we investigate the
effectiveness of using LLMs in generating culturally relevant commonsense QA
datasets for Indonesian and Sundanese languages. To do so, we create datasets
for these languages using various methods involving both LLMs and human
annotators. Our experiments show that the current best-performing LLM, GPT-4
Turbo, is capable of generating questions with adequate knowledge in Indonesian
but not in Sundanese, highlighting the performance discrepancy between medium-
and lower-resource languages. We also benchmark various LLMs on our generated
datasets and find that they perform better on the LLM-generated datasets
compared to those created by humans.
\\ ( https://arxiv.org/abs/2402.17302 ,  9350kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17304
Date: Tue, 27 Feb 2024 08:27:15 GMT   (841kb,D)

Title: Probing Multimodal Large Language Models for Global and Local Semantic
  Representation
Authors: Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan
  Zhao
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024 as a short paper
\\
  The success of large language models has inspired researchers to transfer
their exceptional representing ability to other modalities. Several recent
works leverage image-caption alignment datasets to train multimodal large
language models (MLLMs), which achieve state-of-the-art performance on
image-to-text tasks. However, there are very few studies exploring whether
MLLMs truly understand the complete image information, i.e., global
information, or if they can only capture some local object information. In this
study, we find that the intermediate layers of models can encode more global
semantic information, whose representation vectors perform better on
visual-language entailment tasks, rather than the topmost layers. We further
probe models for local semantic representation through object detection tasks.
And we draw a conclusion that the topmost layers may excessively focus on local
information, leading to a diminished ability to encode global information.
\\ ( https://arxiv.org/abs/2402.17304 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17311
Date: Tue, 27 Feb 2024 08:33:31 GMT   (8890kb,D)

Title: SKT5SciSumm - A Hybrid Generative Approach for Multi-Document Scientific
  Summarization
Authors: Huy Quoc To, Hung-Nghiep Tran, Andr'e Greiner-Petter, Felix Beierle,
  Akiko Aizawa
Categories: cs.CL
\\
  Summarization for scientific text has shown significant benefits both for the
research community and human society. Given the fact that the nature of
scientific text is distinctive and the input of the multi-document
summarization task is substantially long, the task requires sufficient
embedding generation and text truncation without losing important information.
To tackle these issues, in this paper, we propose SKT5SciSumm - a hybrid
framework for multi-document scientific summarization (MDSS). We leverage the
Sentence-Transformer version of Scientific Paper Embeddings using
Citation-Informed Transformers (SPECTER) to encode and represent textual
sentences, allowing for efficient extractive summarization using k-means
clustering. We employ the T5 family of models to generate abstractive summaries
using extracted sentences. SKT5SciSumm achieves state-of-the-art performance on
the Multi-XScience dataset. Through extensive experiments and evaluation, we
showcase the benefits of our model by using less complicated models to achieve
remarkable results, thereby highlighting its potential in advancing the field
of multi-document summarization for scientific text.
\\ ( https://arxiv.org/abs/2402.17311 ,  8890kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17333
Date: Tue, 27 Feb 2024 09:10:28 GMT   (362kb,D)

Title: Unsupervised multiple choices question answering via universal corpus
Authors: Qin Zhang, Hao Ge, Xiaojun Chen, Meng Fang
Categories: cs.CL
Comments: 5 pages, 1 figures, published to ICASSP 2024
\\
  Unsupervised question answering is a promising yet challenging task, which
alleviates the burden of building large-scale annotated data in a new domain.
It motivates us to study the unsupervised multiple-choice question answering
(MCQA) problem. In this paper, we propose a novel framework designed to
generate synthetic MCQA data barely based on contexts from the universal domain
without relying on any form of manual annotation. Possible answers are
extracted and used to produce related questions, then we leverage both named
entities (NE) and knowledge graphs to discover plausible distractors to form
complete synthetic samples. Experiments on multiple MCQA datasets demonstrate
the effectiveness of our method.
\\ ( https://arxiv.org/abs/2402.17333 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17355
Date: Tue, 27 Feb 2024 09:47:36 GMT   (199kb,D)

Title: RECOST: External Knowledge Guided Data-efficient Instruction Tuning
Authors: Qi Zhang, Yiming Zhang, Haobo Wang, Junbo Zhao
Categories: cs.CL
\\
  In the current landscape of large language models (LLMs), the process of
instruction tuning serves as an essential step. Considering the high computing
power overhead, data-efficient instruction tuning was proposed to reduce the
training data size in this process, aiming at selecting high-quality
instructional data. Nevertheless, we argue that most current data-efficient
instruction-tuning methods are highly dependent on the quality of the original
instruction-tuning dataset. When it comes to datasets synthesized by LLMs, a
common scenario in this field, dirty samples will even be selected with a
higher probability than other samples. To address these challenges, we utilized
external knowledge (relevant examples or paragraphs) to evaluate those samples
synthesized by LLMs with an in-context-based relative predictive entropy. Based
on the new metric, we proposed a framework, dubbed as \textbf{RECOST}, which
integrates external-knowledge-base re-ranking and diversity-consistent sampling
into a single pipeline. Through extensive experiments on several synthetic
datasets (Alpaca and Alpaca-gpt4), we demonstrate the effectiveness of our
method and achieve even better results with only \textbf{1\%} of the full
dataset.
\\ ( https://arxiv.org/abs/2402.17355 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17358
Date: Tue, 27 Feb 2024 09:52:27 GMT   (1797kb,D)

Title: SoFA: Shielded On-the-fly Alignment via Priority Rule Following
Authors: Xinyu Lu, Bowen Yu, Yaojie Lu, Hongyu Lin, Haiyang Yu, Le Sun, Xianpei
  Han, Yongbin Li
Categories: cs.CL
\\
  The alignment problem in Large Language Models (LLMs) involves adapting them
to the broad spectrum of human values. This requirement challenges existing
alignment methods due to diversity of preferences and regulatory standards.
This paper introduces a novel alignment paradigm, priority rule following,
which defines rules as the primary control mechanism in each dialog,
prioritizing them over user instructions. Our preliminary analysis reveals that
even the advanced LLMs, such as GPT-4, exhibit shortcomings in understanding
and prioritizing the rules. Therefore, we present PriorityDistill, a
semi-automated approach for distilling priority following signals from LLM
simulations to ensure robust rule integration and adherence. Our experiments
show that this method not only effectively minimizes misalignments utilizing
only one general rule but also adapts smoothly to various unseen rules,
ensuring they are shielded from hijacking and that the model responds
appropriately.
\\ ( https://arxiv.org/abs/2402.17358 ,  1797kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17371
Date: Tue, 27 Feb 2024 10:09:40 GMT   (97kb,D)

Title: A Dataset for Metaphor Detection in Early Medieval Hebrew Poetry
Authors: Michael Toker, Oren Mishali, Ophir M\"unz-Manor, Benny Kimelfeld,
  Yonatan Belinkov
Categories: cs.CL
Comments: EACL 2024. Project webpage: https://tokeron.github.io/metaphor/
ACM-class: I.2.7
\\
  There is a large volume of late antique and medieval Hebrew texts. They
represent a crucial linguistic and cultural bridge between Biblical and modern
Hebrew. Poetry is prominent in these texts and one of its main haracteristics
is the frequent use of metaphor. Distinguishing figurative and literal language
use is a major task for scholars of the Humanities, especially in the fields of
literature, linguistics, and hermeneutics. This paper presents a new,
challenging dataset of late antique and medieval Hebrew poetry with expert
annotations of metaphor, as well as some baseline results, which we hope will
facilitate further research in this area.
\\ ( https://arxiv.org/abs/2402.17371 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17377
Date: Tue, 27 Feb 2024 10:14:57 GMT   (86kb)

Title: KoDialogBench: Evaluating Conversational Understanding of Language
  Models with Korean Dialogue Benchmark
Authors: Seongbo Jang, Seonghyeon Lee, Hwanjo Yu
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  As language models are often deployed as chatbot assistants, it becomes a
virtue for models to engage in conversations in a user's first language. While
these models are trained on a wide range of languages, a comprehensive
evaluation of their proficiency in low-resource languages such as Korean has
been lacking. In this work, we introduce KoDialogBench, a benchmark designed to
assess language models' conversational capabilities in Korean. To this end, we
collect native Korean dialogues on daily topics from public sources, or
translate dialogues from other languages. We then structure these conversations
into diverse test datasets, spanning from dialogue comprehension to response
selection tasks. Leveraging the proposed benchmark, we conduct extensive
evaluations and analyses of various language models to measure a foundational
understanding of Korean dialogues. Experimental results indicate that there
exists significant room for improvement in models' conversation skills.
Furthermore, our in-depth comparisons across different language models
highlight the effectiveness of recent training techniques in enhancing
conversational proficiency. We anticipate that KoDialogBench will promote the
progress towards conversation-aware Korean language models.
\\ ( https://arxiv.org/abs/2402.17377 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17389
Date: Tue, 27 Feb 2024 10:31:00 GMT   (12823kb,D)

Title: FairBelief - Assessing Harmful Beliefs in Language Models
Authors: Mattia Setzu, Marta Marchiori Manerba, Pasquale Minervini, Debora
  Nozza
Categories: cs.CL cs.AI
\\
  Language Models (LMs) have been shown to inherit undesired biases that might
hurt minorities and underrepresented groups if such systems were integrated
into real-world applications without careful fairness auditing. This paper
proposes FairBelief, an analytical approach to capture and assess beliefs,
i.e., propositions that an LM may embed with different degrees of confidence
and that covertly influence its predictions. With FairBelief, we leverage
prompting to study the behavior of several state-of-the-art LMs across
different previously neglected axes, such as model scale and likelihood,
assessing predictions on a fairness dataset specifically designed to quantify
LMs' outputs' hurtfulness. Finally, we conclude with an in-depth qualitative
assessment of the beliefs emitted by the models. We apply FairBelief to English
LMs, revealing that, although these architectures enable high performances on
diverse natural language processing tasks, they show hurtful beliefs about
specific genders. Interestingly, training procedure and dataset, model scale,
and architecture induce beliefs of different degrees of hurtfulness.
\\ ( https://arxiv.org/abs/2402.17389 ,  12823kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17392
Date: Tue, 27 Feb 2024 10:38:37 GMT   (62kb,D)

Title: Spot the bot: Coarse-Grained Partition of Semantic Paths for Bots and
  Humans
Authors: Vasilii A. Gromov, Alexandra S. Kogan
Categories: cs.CL
Journal-ref: Gromov V.A., Kogan A.S. Spot the Bot: Coarse-Grained Partition of
  Semantic Paths for Bots and Humans // Pattern Recognition and Machine
  Intelligence, 2023. pp. 348--355
\\
  Nowadays, technology is rapidly advancing: bots are writing comments,
articles, and reviews. Due to this fact, it is crucial to know if the text was
written by a human or by a bot. This paper focuses on comparing structures of
the coarse-grained partitions of semantic paths for human-written and
bot-generated texts. We compare the clusterizations of datasets of n-grams from
literary texts and texts generated by several bots. The hypothesis is that the
structures and clusterizations are different. Our research supports the
hypothesis. As the semantic structure may be different for different languages,
we investigate Russian, English, German, and Vietnamese languages.
\\ ( https://arxiv.org/abs/2402.17392 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17396
Date: Tue, 27 Feb 2024 10:44:52 GMT   (201kb,D)

Title: Benchmarking GPT-4 on Algorithmic Problems: A Systematic Evaluation of
  Prompting Strategies
Authors: Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti
Categories: cs.CL cs.AI cs.NE
Comments: Accepted at LREC-COLING 2024
\\
  Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing thanks to their ability to reuse knowledge acquired on
massive text corpora on a wide variety of downstream tasks, with minimal (if
any) tuning steps. At the same time, it has been repeatedly shown that LLMs
lack systematic generalization, which allows to extrapolate the learned
statistical regularities outside the training distribution. In this work, we
offer a systematic benchmarking of GPT-4, one of the most advanced LLMs
available, on three algorithmic tasks characterized by the possibility to
control the problem difficulty with two parameters. We compare the performance
of GPT-4 with that of its predecessor (GPT-3.5) and with a variant of the
Transformer-Encoder architecture recently introduced to solve similar tasks,
the Neural Data Router. We find that the deployment of advanced prompting
techniques allows GPT-4 to reach superior accuracy on all tasks, demonstrating
that state-of-the-art LLMs constitute a very strong baseline also in
challenging tasks that require systematic generalization.
\\ ( https://arxiv.org/abs/2402.17396 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17400
Date: Tue, 27 Feb 2024 10:47:24 GMT   (11816kb,D)

Title: Investigating Continual Pretraining in Large Language Models: Insights
  and Implications
Authors: \c{C}a\u{g}atay Y{\i}ld{\i}z, Nishaanth Kanna Ravichandran, Prishruit
  Punia, Matthias Bethge, Beyza Ermis
Categories: cs.CL
\\
  This paper studies the evolving domain of Continual Learning (CL) in large
language models (LLMs), with a focus on developing strategies for efficient and
sustainable training. Our primary emphasis is on continual domain-adaptive
pretraining, a process designed to equip LLMs with the ability to integrate new
information from various domains while retaining previously learned knowledge
and enhancing cross-domain knowledge transfer without relying on
domain-specific identification. Unlike previous studies, which mostly
concentrate on a limited selection of tasks or domains and primarily aim to
address the issue of forgetting, our research evaluates the adaptability and
capabilities of LLMs to changing data landscapes in practical scenarios. To
this end, we introduce a new benchmark designed to measure the adaptability of
LLMs to these evolving data environments, offering a comprehensive framework
for evaluation. We examine the impact of model size on learning efficacy and
forgetting, as well as how the progression and similarity of emerging domains
affect the knowledge transfer within these models. Our findings uncover several
key insights: (i) when the sequence of domains shows semantic similarity,
continual pretraining enables LLMs to better specialize in the current domain
compared to stand-alone fine-tuning, (ii) training across a diverse range of
domains enhances both backward and forward knowledge transfer, and (iii)
smaller models are particularly sensitive to continual pretraining, showing the
most significant rates of both forgetting and learning. We posit that our
research marks a shift towards establishing a more realistic benchmark for
investigating CL in LLMs, and has the potential to play a key role in guiding
the direction of future research in the field.
\\ ( https://arxiv.org/abs/2402.17400 ,  11816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17411
Date: Tue, 27 Feb 2024 11:02:12 GMT   (974kb,D)

Title: Consistency Matters: Explore LLMs Consistency From a Black-Box
  Perspective
Authors: Fufangchen Zhao, Guoqiang Jin, Jiaheng Huang, Rui Zhao and Fei Tan
Categories: cs.CL
\\
  Nowadays both commercial and open-source academic LLM have become the
mainstream models of NLP. However, there is still a lack of research on LLM
consistency, meaning that throughout the various stages of LLM research and
deployment, its internal parameters and capabilities should remain unchanged.
This issue exists in both the industrial and academic sectors. The solution to
this problem is often time-consuming and labor-intensive, and there is also an
additional cost of secondary deployment, resulting in economic and time losses.
To fill this gap, we build an LLM consistency task dataset and design several
baselines. Additionally, we choose models of diverse scales for the main
experiments. Specifically, in the LightGBM experiment, we used traditional NLG
metrics (i.e., ROUGE, BLEU, METEOR) as the features needed for model training.
The final result exceeds the manual evaluation and GPT3.5 as well as other
models in the main experiment, achieving the best performance. In the end, we
use the best performing LightGBM model as the base model to build the
evaluation tool, which can effectively assist in the deployment of business
models. Our code and tool demo are available at
https://github.com/heavenhellchen/Consistency.git
\\ ( https://arxiv.org/abs/2402.17411 ,  974kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17433
Date: Tue, 27 Feb 2024 11:45:21 GMT   (1496kb,D)

Title: Enhancing EEG-to-Text Decoding through Transferable Representations from
  Pre-trained Contrastive EEG-Text Masked Autoencoder
Authors: Jiaqi Wang, Zhenxi Song, Zhengyu Ma, Xipeng Qiu, Min Zhang, Zhiguo
  Zhang
Categories: cs.CL
\\
  Reconstructing natural language from non-invasive electroencephalography
(EEG) holds great promise as a language decoding technology for brain-computer
interfaces (BCIs). However, EEG-based language decoding is still in its nascent
stages, facing several technical issues such as: 1) Absence of a hybrid
strategy that can effectively integrate cross-modality (between EEG and text)
self-learning with intra-modality self-reconstruction of EEG features or
textual sequences; 2) Under-utilization of large language models (LLMs) to
enhance EEG-based language decoding. To address above issues, we propose the
Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that
orchestrates compound self-supervised learning across and within EEG and text
through a dedicated multi-stream encoder. Furthermore, we develop a framework
called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable
Representations), which leverages pre-trained modules alongside the EEG stream
from CET-MAE and further enables an LLM (specifically BART) to decode text from
EEG sequences. Comprehensive experiments conducted on the popular text-evoked
EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms
the state-of-the-art in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%,
respectively. These results indicate significant advancements in the field and
underscores the proposed framework's potential to enable more powerful and
widespread BCI applications.
\\ ( https://arxiv.org/abs/2402.17433 ,  1496kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17437
Date: Tue, 27 Feb 2024 11:50:05 GMT   (105kb,D)

Title: Exploiting Emotion-Semantic Correlations for Empathetic Response
  Generation
Authors: Zhou Yang, Zhaochun Ren, Yufeng Wang, Xiaofei Zhu, Zhihao Chen,
  Tiecheng Cai, Yunbing Wu, Yisong Su, Sibo Ju, Xiangwen Liao
Categories: cs.CL cs.AI
Comments: 12 pages, 3 figures, Findings of EMNLP 2023
\\
  Empathetic response generation aims to generate empathetic responses by
understanding the speaker's emotional feelings from the language of dialogue.
Recent methods capture emotional words in the language of communicators and
construct them as static vectors to perceive nuanced emotions. However,
linguistic research has shown that emotional words in language are dynamic and
have correlations with other grammar semantic roles, i.e., words with semantic
meanings, in grammar. Previous methods overlook these two characteristics,
which easily lead to misunderstandings of emotions and neglect of key
semantics. To address this issue, we propose a dynamical Emotion-Semantic
Correlation Model (ESCM) for empathetic dialogue generation tasks. ESCM
constructs dynamic emotion-semantic vectors through the interaction of context
and emotions. We introduce dependency trees to reflect the correlations between
emotions and semantics. Based on dynamic emotion-semantic vectors and
dependency trees, we propose a dynamic correlation graph convolutional network
to guide the model in learning context meanings in dialogue and generating
empathetic responses. Experimental results on the EMPATHETIC-DIALOGUES dataset
show that ESCM understands semantics and emotions more accurately and expresses
fluent and informative empathetic responses. Our analysis results also indicate
that the correlations between emotions and semantics are frequently used in
dialogues, which is of great significance for empathetic perception and
expression.
\\ ( https://arxiv.org/abs/2402.17437 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17447
Date: Tue, 27 Feb 2024 12:03:56 GMT   (938kb,D)

Title: Deep Learning Based Named Entity Recognition Models for Recipes
Authors: Mansi Goel, Ayush Agarwal, Shubham Agrawal, Janak Kapuriya, Akhil
  Vamshi Konam, Rishabh Gupta, Shrey Rastogi, Niharika, and Ganesh Bagler
Categories: cs.CL cs.AI cs.IR
Comments: 13 pages, 6 main figures and 2 in appendices, and 3 main tables;
  Accepted for publication in LREC-COLING 2024
\\
  Food touches our lives through various endeavors, including flavor,
nourishment, health, and sustainability. Recipes are cultural capsules
transmitted across generations via unstructured text. Automated protocols for
recognizing named entities, the building blocks of recipe text, are of immense
value for various applications ranging from information extraction to novel
recipe generation. Named entity recognition is a technique for extracting
information from unstructured or semi-structured data with known labels.
Starting with manually-annotated data of 6,611 ingredient phrases, we created
an augmented dataset of 26,445 phrases cumulatively. Simultaneously, we
systematically cleaned and analyzed ingredient phrases from RecipeDB, the
gold-standard recipe data repository, and annotated them using the Stanford
NER. Based on the analysis, we sampled a subset of 88,526 phrases using a
clustering-based approach while preserving the diversity to create the
machine-annotated dataset. A thorough investigation of NER approaches on these
three datasets involving statistical, fine-tuning of deep learning-based
language models and few-shot prompting on large language models (LLMs) provides
deep insights. We conclude that few-shot prompting on LLMs has abysmal
performance, whereas the fine-tuned spaCy-transformer emerges as the best model
with macro-F1 scores of 95.9%, 96.04%, and 95.71% for the manually-annotated,
augmented, and machine-annotated datasets, respectively.
\\ ( https://arxiv.org/abs/2402.17447 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17463
Date: Tue, 27 Feb 2024 12:39:23 GMT   (508kb,D)

Title: Training-Free Long-Context Scaling of Large Language Models
Authors: Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang
  Zhou, Lingpeng Kong
Categories: cs.CL
\\
  The ability of Large Language Models (LLMs) to process and generate coherent
text is markedly weakened when the number of input tokens exceeds their
pretraining length. Given the expensive overhead of finetuning large-scale
models with longer sequences, we propose Dual Chunk Attention (DCA), which
enables Llama2 70B to support context windows of more than 100k tokens without
continual training. By decomposing the attention computation for long sequences
into chunk-based modules, DCA manages to effectively capture the relative
positional information of tokens within the same chunk (Intra-Chunk) and across
distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash
Attention. In addition to its impressive extrapolation capability, DCA achieves
performance on practical long-context tasks that is comparable to or even
better than that of finetuned models. When compared with proprietary models,
our training-free 70B model attains 94% of the performance of gpt-3.5-16k,
indicating it is a viable open-source alternative. All code and data used in
this work are released at \url{https://github.com/HKUNLP/ChunkLlama}.
\\ ( https://arxiv.org/abs/2402.17463 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17478
Date: Tue, 27 Feb 2024 13:02:19 GMT   (4765kb,D)

Title: Can GPT-4 Identify Propaganda? Annotation and Detection of Propaganda
  Spans in News Articles
Authors: Maram Hasanain, Fatema Ahmed, Firoj Alam
Categories: cs.CL
Comments: Accepted as a full paper at LREC-COLING 2024
\\
  The use of propaganda has spiked on mainstream and social media, aiming to
manipulate or mislead users. While efforts to automatically detect propaganda
techniques in textual, visual, or multimodal content have increased, most of
them primarily focus on English content. The majority of the recent initiatives
targeting medium to low-resource languages produced relatively small annotated
datasets, with a skewed distribution, posing challenges for the development of
sophisticated propaganda detection models. To address this challenge, we
carefully develop the largest propaganda dataset to date, ArPro, comprised of
8K paragraphs from newspaper articles, labeled at the text span level following
a taxonomy of 23 propagandistic techniques. Furthermore, our work offers the
first attempt to understand the performance of large language models (LLMs),
using GPT-4, for fine-grained propaganda detection from text. Results showed
that GPT-4's performance degrades as the task moves from simply classifying a
paragraph as propagandistic or not, to the fine-grained task of detecting
propaganda techniques and their manifestation in text. Compared to models
fine-tuned on the dataset for propaganda detection at different classification
granularities, GPT-4 is still far behind. Finally, we evaluate GPT-4 on a
dataset consisting of six other languages for span detection, and results
suggest that the model struggles with the task across languages. Our dataset
and resources will be released to the community.
\\ ( https://arxiv.org/abs/2402.17478 ,  4765kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17493
Date: Tue, 27 Feb 2024 13:18:00 GMT   (1422kb)

Title: Prescribing Large Language Models for Perioperative Care: What's The
  Right Dose for Pre-trained Models?
Authors: Bing Xue, Charles Alba, Joanna Abraham, Thomas Kannampallil, Chenyang
  Lu
Categories: cs.CL
Comments: Supplemental file available at: http://tinyurl.com/mszmjna9; models
  publicly available at:
  https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT AND
  https://huggingface.co/cja5553/BJH-perioperative-notes-bioGPT
ACM-class: J.3; I.2.7
\\
  Postoperative risk predictions can inform effective perioperative care
management and planning. We aimed to assess whether clinical large language
models (LLMs) can predict postoperative risks using clinical texts with various
training strategies. The main cohort involved 84,875 records from Barnes Jewish
Hospital (BJH) system between 2018 and 2021. Methods were replicated on Beth
Israel Deaconess's MIMIC dataset. Both studies had mean duration of follow-up
based on the length of postoperative ICU stay less than 7 days. For the BJH
dataset, outcomes included 30-day mortality, pulmonary embolism (PE) and
pneumonia. Three domain adaptation and finetuning strategies were implemented
for BioGPT, ClinicalBERT and BioClinicalBERT: self-supervised objectives;
incorporating labels with semi-supervised fine-tuning; and foundational
modelling through multi-task learning. Model performance was compared using the
area under the receiver operating characteristic curve (AUROC) and the area
under the precision recall curve (AUPRC) for classification tasks, and mean
squared error (MSE) and R2 for regression tasks. Pre-trained LLMs outperformed
traditional word embeddings, with absolute maximal gains of 38.3% for AUROC and
14% for AUPRC. Adapting models further improved performance: (1)
self-supervised finetuning by 3.2% for AUROC and 1.5% for AUPRC; (2)
semi-supervised finetuning by 1.8% for AUROC and 2% for AUPRC, compared to
self-supervised finetuning; (3) foundational modelling by 3.6% for AUROC and
2.6% for AUPRC, compared to self-supervised finetuning. Pre-trained clinical
LLMs offer opportunities for postoperative risk predictions in unforeseen data,
with peaks in foundational models indicating the potential of task-agnostic
learning towards the generalizability of LLMs in perioperative care.
\\ ( https://arxiv.org/abs/2402.17493 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17497
Date: Tue, 27 Feb 2024 13:22:51 GMT   (7128kb,D)

Title: REAR: A Relevance-Aware Retrieval-Augmented Framework for Open-Domain
  Question Answering
Authors: Yuhao Wang, Ruiyang Ren, Junyi Li, Wayne Xin Zhao, Jing Liu, Ji-Rong
  Wen
Categories: cs.CL cs.IR
\\
  Considering the limited internal parametric knowledge, retrieval-augmented
generation (RAG) has been widely used to extend the knowledge scope of large
language models (LLMs). Despite the extensive efforts on RAG research, in
existing methods, LLMs cannot precisely assess the relevance of retrieved
documents, thus likely leading to misleading or even incorrect utilization of
external knowledge (i.e., retrieved documents). To address this issue, in this
paper, we propose REAR, a RElevance-Aware Retrieval-augmented approach for
open-domain question answering (QA). As the key motivation, we aim to enhance
the self-awareness of source relevance for LLMs, so as to adaptively utilize
external knowledge in RAG systems. Specially, we develop a new architecture for
LLM based RAG system, by incorporating a specially designed rank head that
precisely assesses the relevance of retrieved documents. Furthermore, we
propose an improved training method based on bi-granularity relevance fusion
and noise-resistant training. By combining the improvements in both
architecture and training, our proposed REAR can better utilize external
knowledge by effectively perceiving the relevance of retrieved documents.
Experiments on four open-domain QA tasks show that REAR significantly
outperforms previous a number of competitive RAG approaches. Our code and data
can be accessed at https://github.com/RUCAIBox/REAR.
\\ ( https://arxiv.org/abs/2402.17497 ,  7128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17509
Date: Tue, 27 Feb 2024 13:49:12 GMT   (8189kb,D)

Title: Extreme Miscalibration and the Illusion of Adversarial Robustness
Authors: Vyas Raina, Samson Tan, Volkan Cevher, Aditya Rawal, Sheng Zha, George
  Karypis
Categories: cs.CL
\\
  Deep learning-based Natural Language Processing (NLP) models are vulnerable
to adversarial attacks, where small perturbations can cause a model to
misclassify. Adversarial Training (AT) is often used to increase model
robustness. However, we have discovered an intriguing phenomenon: deliberately
or accidentally miscalibrating models masks gradients in a way that interferes
with adversarial attack search methods, giving rise to an apparent increase in
robustness. We show that this observed gain in robustness is an illusion of
robustness (IOR), and demonstrate how an adversary can perform various forms of
test-time temperature calibration to nullify the aforementioned interference
and allow the adversarial attack to find adversarial examples. Hence, we urge
the NLP community to incorporate test-time temperature scaling into their
robustness evaluations to ensure that any observed gains are genuine. Finally,
we show how the temperature can be scaled during \textit{training} to improve
genuine robustness.
\\ ( https://arxiv.org/abs/2402.17509 ,  8189kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17512
Date: Tue, 27 Feb 2024 13:54:48 GMT   (865kb,D)

Title: Latent Attention for Linear Time Transformers
Authors: Rares Dolga, Marius Cobzarenco, David Barber
Categories: cs.CL stat.ML
\\
  The time complexity of the standard attention mechanism in a transformer
scales quadratically with the length of the sequence. We introduce a method to
reduce this to linear scaling with time, based on defining attention via latent
vectors. The method is readily usable as a drop-in replacement for the standard
attention mechanism. Our "Latte Transformer" model can be implemented for both
bidirectional and unidirectional tasks, with the causal version allowing a
recurrent implementation which is memory and time-efficient during inference of
language generation tasks. Whilst next token prediction scales linearly with
the sequence length for a standard transformer, a Latte Transformer requires
constant time to compute the next token. The empirical performance of our
method is comparable to standard attention, yet allows scaling to context
windows much larger than practical in standard attention.
\\ ( https://arxiv.org/abs/2402.17512 ,  865kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17527
Date: Tue, 27 Feb 2024 14:11:32 GMT   (1199kb,D)

Title: Predict the Next Word: <Humans exhibit uncertainty in this task and
  language models _____>
Authors: Evgenia Ilia and Wilker Aziz
Categories: cs.CL cs.AI
Comments: 22 pages, EACL 2024
\\
  Language models (LMs) are statistical models trained to assign probability to
human-generated text. As such, it is reasonable to question whether they
approximate linguistic variability exhibited by humans well. This form of
statistical assessment is difficult to perform at the passage level, for it
requires acceptability judgements (i.e., human evaluation) or a robust
automated proxy (which is non-trivial). At the word level, however, given some
context, samples from an LM can be assessed via exact matching against a
prerecorded dataset of alternative single-word continuations of the available
context. We exploit this fact and evaluate the LM's ability to reproduce
variability that humans (in particular, a population of English speakers)
exhibit in the 'next word prediction' task. This can be seen as assessing a
form of calibration, which, in the context of text classification, Baan et al.
(2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and
ChatGPT and find that they exhibit fairly low calibration to human uncertainty.
We also verify the failure of expected calibration error (ECE) to reflect this,
and as such, advise the community against relying on it in this setting.
\\ ( https://arxiv.org/abs/2402.17527 ,  1199kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17532
Date: Tue, 27 Feb 2024 14:16:19 GMT   (6916kb,D)

Title: Retrieval is Accurate Generation
Authors: Bowen Cao, Deng Cai, Leyang Cui, Xuxin Cheng, Wei Bi, Yuexian Zou,
  Shuming Shi
Categories: cs.CL
Comments: ICLR 2024
\\
  Standard language models generate text by selecting tokens from a fixed,
finite, and standalone vocabulary. We introduce a novel method that selects
context-aware phrases from a collection of supporting documents. One of the
most significant challenges for this paradigm shift is determining the training
oracles, because a string of text can be segmented in various ways and each
segment can be retrieved from numerous possible documents. To address this, we
propose to initialize the training oracles using linguistic heuristics and,
more importantly, bootstrap the oracles through iterative self-reinforcement.
Extensive experiments show that our model not only outperforms standard
language models on a variety of knowledge-intensive tasks but also demonstrates
improved generation quality in open-ended text generation. For instance,
compared to the standard language model counterpart, our model raises the
accuracy from 23.47% to 36.27% on OpenbookQA, and improves the MAUVE score from
42.61% to 81.58% in open-ended text generation. Remarkably, our model also
achieves the best performance and the lowest latency among several
retrieval-augmented baselines. In conclusion, we assert that retrieval is more
accurate generation and hope that our work will encourage further research on
this new paradigm shift.
\\ ( https://arxiv.org/abs/2402.17532 ,  6916kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17564
Date: Tue, 27 Feb 2024 15:05:32 GMT   (183kb,D)

Title: Unleashing the Potential of Large Language Models as Prompt Optimizers:
  An Analogical Analysis with Gradient-based Model Optimizers
Authors: Xinyu Tang, Xiaolei Wang, Wayne Xin Zhao, Siyuan Lu, Yaliang Li and
  Ji-Rong Wen
Categories: cs.CL
\\
  Automatic prompt optimization is an important approach to improving the
performance of large language models (LLMs). Recent research demonstrates the
potential of using LLMs as prompt optimizers, which can generate improved task
prompts via iterative refinement. In this paper, we propose a novel perspective
to investigate the design of LLM-based prompt optimizers, by drawing an analogy
with gradient-based model optimizers. To connect these two approaches, we
identify two pivotal factors in model parameter learning: update direction and
update method. Focused on the two aspects, we borrow the theoretical framework
and learning methods from gradient-based optimization to design improved
strategies for LLM-based prompt optimizers. By systematically analyzing a rich
set of improvement strategies, we further develop a capable Gradient-inspired
LLM-based Prompt Optimizer called GPO. At each step, it first retrieves
relevant prompts from the optimization trajectory as the update direction.
Then, it utilizes the generation-based refinement strategy to perform the
update, while controlling the edit distance through a cosine-based decay
strategy. Extensive experiments demonstrate the effectiveness and efficiency of
GPO. In particular, GPO brings an additional improvement of up to 56.8% on
Big-Bench Hard and 55.3% on MMLU compared to baseline methods.
\\ ( https://arxiv.org/abs/2402.17564 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17608
Date: Tue, 27 Feb 2024 15:34:15 GMT   (7415kb,D)

Title: Linguistic Knowledge Can Enhance Encoder-Decoder Models (If You Let It)
Authors: Alessio Miaschi, Felice Dell'Orletta, Giulia Venturi
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  In this paper, we explore the impact of augmenting pre-trained
Encoder-Decoder models, specifically T5, with linguistic knowledge for the
prediction of a target task. In particular, we investigate whether fine-tuning
a T5 model on an intermediate task that predicts structural linguistic
properties of sentences modifies its performance in the target task of
predicting sentence-level complexity. Our study encompasses diverse experiments
conducted on Italian and English datasets, employing both monolingual and
multilingual T5 models at various sizes. Results obtained for both languages
and in cross-lingual configurations show that linguistically motivated
intermediate fine-tuning has generally a positive impact on target task
performance, especially when applied to smaller models and in scenarios with
limited data availability.
\\ ( https://arxiv.org/abs/2402.17608 ,  7415kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17613
Date: Tue, 27 Feb 2024 15:42:33 GMT   (1946kb,D)

Title: Neural Automated Writing Evaluation with Corrective Feedback
Authors: Izia Xiaoxiao Wang and Xihan Wu and Edith Coates and Min Zeng and
  Jiexin Kuang and Siliang Liu and Mengyang Qiu and Jungyeul Park
Categories: cs.CL
Comments: Submitted to the system demonstration track at NAACL-HLT 2024
\\
  The utilization of technology in second language learning and teaching has
become ubiquitous. For the assessment of writing specifically, automated
writing evaluation (AWE) and grammatical error correction (GEC) have become
immensely popular and effective methods for enhancing writing proficiency and
delivering instant and individualized feedback to learners. By leveraging the
power of natural language processing (NLP) and machine learning algorithms, AWE
and GEC systems have been developed separately to provide language learners
with automated corrective feedback and more accurate and unbiased scoring that
would otherwise be subject to examiners. In this paper, we propose an
integrated system for automated writing evaluation with corrective feedback as
a means of bridging the gap between AWE and GEC results for second language
learners. This system enables language learners to simulate the essay writing
tests: a student writes and submits an essay, and the system returns the
assessment of the writing along with suggested grammatical error corrections.
Given that automated scoring and grammatical correction are more efficient and
cost-effective than human grading, this integrated system would also alleviate
the burden of manually correcting innumerable essays.
\\ ( https://arxiv.org/abs/2402.17613 ,  1946kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17630
Date: Tue, 27 Feb 2024 15:57:11 GMT   (1309kb,D)

Title: Fine-Grained Natural Language Inference Based Faithfulness Evaluation
  for Diverse Summarisation Tasks
Authors: Huajian Zhang, Yumo Xu, Laura Perez-Beltrachini
Categories: cs.CL
Comments: EACL 2024
\\
  We study existing approaches to leverage off-the-shelf Natural Language
Inference (NLI) models for the evaluation of summary faithfulness and argue
that these are sub-optimal due to the granularity level considered for premises
and hypotheses. That is, the smaller content unit considered as hypothesis is a
sentence and premises are made up of a fixed number of document sentences. We
propose a novel approach, namely InFusE, that uses a variable premise size and
simplifies summary sentences into shorter hypotheses. Departing from previous
studies which focus on single short document summarisation, we analyse NLI
based faithfulness evaluation for diverse summarisation tasks. We introduce
DiverSumm, a new benchmark comprising long form summarisation (long documents
and summaries) and diverse summarisation tasks (e.g., meeting and
multi-document summarisation). In experiments, InFusE obtains superior
performance across the different summarisation tasks. Our code and data are
available at https://github.com/HJZnlp/infuse.
\\ ( https://arxiv.org/abs/2402.17630 ,  1309kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17633
Date: Tue, 27 Feb 2024 15:59:37 GMT   (955kb,D)

Title: From Text Segmentation to Smart Chaptering: A Novel Benchmark for
  Structuring Video Transcriptions
Authors: Fabian Retkowski, Alexander Waibel
Categories: cs.CL
Comments: Accepted to EACL 2024
\\
  Text segmentation is a fundamental task in natural language processing, where
documents are split into contiguous sections. However, prior research in this
area has been constrained by limited datasets, which are either small in scale,
synthesized, or only contain well-structured documents. In this paper, we
address these limitations by introducing a novel benchmark YTSeg focusing on
spoken content that is inherently more unstructured and both topically and
structurally diverse. As part of this work, we introduce an efficient
hierarchical segmentation model MiniSeg, that outperforms state-of-the-art
baselines. Lastly, we expand the notion of text segmentation to a more
practical "smart chaptering" task that involves the segmentation of
unstructured content, the generation of meaningful segment titles, and a
potential real-time application of the models.
\\ ( https://arxiv.org/abs/2402.17633 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17644
Date: Tue, 27 Feb 2024 16:15:03 GMT   (8094kb,D)

Title: Are LLMs Capable of Data-based Statistical and Causal Reasoning?
  Benchmarking Advanced Quantitative Reasoning with Data
Authors: Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng
Categories: cs.CL cs.AI
Comments: Project website: https://xxxiaol.github.io/QRData/
\\
  Quantitative reasoning is a critical skill to analyze data, yet the
assessment of such ability remains limited. To address this gap, we introduce
the Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate
Large Language Models' capability in statistical and causal reasoning with
real-world data. The benchmark comprises a carefully constructed dataset of 411
questions accompanied by data sheets from textbooks, online learning materials,
and academic papers. To compare models' quantitative reasoning abilities on
data and text, we enrich the benchmark with an auxiliary set of 290 text-only
questions, namely QRText. We evaluate natural language reasoning, program-based
reasoning, and agent reasoning methods including Chain-of-Thought,
Program-of-Thoughts, ReAct, and code interpreter assistants on diverse models.
The strongest model GPT-4 achieves an accuracy of 58%, which has a large room
for improvement. Among open-source models, Deepseek-coder-instruct, a code LLM
pretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals
that models encounter difficulties in data analysis and causal reasoning, and
struggle in using causal knowledge and provided data simultaneously. Code and
data are in https://github.com/xxxiaol/QRData.
\\ ( https://arxiv.org/abs/2402.17644 ,  8094kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17649
Date: Tue, 27 Feb 2024 16:19:37 GMT   (9722kb,D)

Title: Beyond prompt brittleness: Evaluating the reliability and consistency of
  political worldviews in LLMs
Authors: Tanise Ceron, Neele Falk, Ana Bari\'c, Dmitry Nikolaev, Sebastian
  Pad\'o
Categories: cs.CL cs.CY
Comments: 10 pages, under review
\\
  Due to the widespread use of large language models (LLMs) in ubiquitous
systems, we need to understand whether they embed a specific worldview and what
these views reflect. Recent studies report that, prompted with political
questionnaires, LLMs show left-liberal leanings. However, it is as yet unclear
whether these leanings are reliable (robust to prompt variations) and whether
the leaning is consistent across policies and political leaning. We propose a
series of tests which assess the reliability and consistency of LLMs' stances
on political statements based on a dataset of voting-advice questionnaires
collected from seven EU countries and annotated for policy domains. We study
LLMs ranging in size from 7B to 70B parameters and find that their reliability
increases with parameter count. Larger models show overall stronger alignment
with left-leaning parties but differ among policy programs: They evince a
(left-wing) positive stance towards environment protection, social welfare but
also (right-wing) law and order, with no consistent preferences in foreign
policy, migration, and economy.
\\ ( https://arxiv.org/abs/2402.17649 ,  9722kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17682
Date: Tue, 27 Feb 2024 16:56:30 GMT   (8101kb,D)

Title: NextLevelBERT: Investigating Masked Language Modeling with Higher-Level
  Representations for Long Documents
Authors: Tamara Czinczoll, Christoph H\"ones, Maximilian Schall, Gerard de Melo
Categories: cs.CL
\\
  While (large) language models have significantly improved over the last
years, they still struggle to sensibly process long sequences found, e.g., in
books, due to the quadratic scaling of the underlying attention mechanism. To
address this, we propose NextLevelBERT, a Masked Language Model operating not
on tokens, but on higher-level semantic representations in the form of text
embeddings. We pretrain NextLevelBERT to predict the vector representation of
entire masked text chunks and evaluate the effectiveness of the resulting
document vectors on three task types: 1) Semantic Textual Similarity via
zero-shot document embeddings, 2) Long document classification, 3)
Multiple-choice question answering. We find that next level Masked Language
Modeling is an effective technique to tackle long-document use cases and can
outperform much larger embedding models as long as the required level of detail
is not too high. We make model and code available.
\\ ( https://arxiv.org/abs/2402.17682 ,  8101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17700
Date: Tue, 27 Feb 2024 17:25:37 GMT   (8637kb,D)

Title: RAVEL: Evaluating Interpretability Methods on Disentangling Language
  Model Representations
Authors: Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, Atticus Geiger
Categories: cs.CL cs.LG
\\
  Individual neurons participate in the representation of multiple high-level
concepts. To what extent can different interpretability methods successfully
disentangle these roles? To help address this question, we introduce RAVEL
(Resolving Attribute-Value Entanglements in Language Models), a dataset that
enables tightly controlled, quantitative comparisons between a variety of
existing interpretability methods. We use the resulting conceptual framework to
define the new method of Multi-task Distributed Alignment Search (MDAS), which
allows us to find distributed representations satisfying multiple causal
criteria. With Llama2-7B as the target language model, MDAS achieves
state-of-the-art results on RAVEL, demonstrating the importance of going beyond
neuron-level analyses to identify features distributed across activations. We
release our benchmark at https://github.com/explanare/ravel.
\\ ( https://arxiv.org/abs/2402.17700 ,  8637kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17717
Date: Tue, 27 Feb 2024 17:52:33 GMT   (816kb,D)

Title: AmbigNLG: Addressing Task Ambiguity in Instruction for NLG
Authors: Ayana Niwa, Hayate Iso
Categories: cs.CL
Comments: work in progress
\\
  In this study, we introduce AmbigNLG, a new task designed to tackle the
challenge of task ambiguity in instructions for Natural Language Generation
(NLG) tasks. Despite the impressive capabilities of Large Language Models
(LLMs) in understanding and executing a wide range of tasks through natural
language interaction, their performance is significantly hindered by the
ambiguity present in real-world instructions. To address this, AmbigNLG seeks
to identify and mitigate such ambiguities, aiming to refine instructions to
match user expectations better. We introduce a dataset, AmbigSNI-NLG,
consisting of 2,500 instances, and develop an ambiguity taxonomy for
categorizing and annotating instruction ambiguities. Our approach demonstrates
substantial improvements in text generation quality, highlighting the critical
role of clear and specific instructions in enhancing LLM performance in NLG
tasks.
\\ ( https://arxiv.org/abs/2402.17717 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17733
Date: Tue, 27 Feb 2024 18:09:36 GMT   (2520kb,D)

Title: Tower: An Open Multilingual Large Language Model for Translation-Related
  Tasks
Authors: Duarte M. Alves, Jos\'e Pombal, Nuno M. Guerreiro, Pedro H. Martins,
  Jo\~ao Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes,
  Sweta Agrawal, Pierre Colombo, Jos\'e G.C. de Souza, Andr\'e F.T. Martins
Categories: cs.CL
\\
  While general-purpose large language models (LLMs) demonstrate proficiency on
multiple tasks within the domain of translation, approaches based on open LLMs
are competitive only when specializing on a single task. In this paper, we
propose a recipe for tailoring LLMs to multiple tasks present in translation
workflows. We perform continued pretraining on a multilingual mixture of
monolingual and parallel data, creating TowerBase, followed by finetuning on
instructions relevant for translation processes, creating TowerInstruct. Our
final model surpasses open alternatives on several tasks relevant to
translation workflows and is competitive with general-purpose closed LLMs. To
facilitate future research, we release the Tower models, our specialization
dataset, an evaluation framework for LLMs focusing on the translation
ecosystem, and a collection of model generations, including ours, on our
benchmark.
\\ ( https://arxiv.org/abs/2402.17733 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17753
Date: Tue, 27 Feb 2024 18:42:31 GMT   (1080kb,D)

Title: Evaluating Very Long-Term Conversational Memory of LLM Agents
Authors: Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal,
  Francesco Barbieri, Yuwei Fang
Categories: cs.CL cs.AI cs.LG
Comments: 19 pages; Project page: https://snap-research.github.io/locomo/
\\
  Existing works on long-term open-domain dialogues focus on evaluating model
responses within contexts spanning no more than five chat sessions. Despite
advancements in long-context large language models (LLMs) and retrieval
augmented generation (RAG) techniques, their efficacy in very long-term
dialogues remains unexplored. To address this research gap, we introduce a
machine-human pipeline to generate high-quality, very long-term dialogues by
leveraging LLM-based agent architectures and grounding their dialogues on
personas and temporal event graphs. Moreover, we equip each agent with the
capability of sharing and reacting to images. The generated conversations are
verified and edited by human annotators for long-range consistency and
grounding to the event graphs. Using this pipeline, we collect LoCoMo, a
dataset of very long-term conversations, each encompassing 300 turns and 9K
tokens on avg., over up to 35 sessions. Based on LoCoMo, we present a
comprehensive evaluation benchmark to measure long-term memory in models,
encompassing question answering, event summarization, and multi-modal dialogue
generation tasks. Our experimental results indicate that LLMs exhibit
challenges in understanding lengthy conversations and comprehending long-range
temporal and causal dynamics within dialogues. Employing strategies like
long-context LLMs or RAG can offer improvements but these models still
substantially lag behind human performance.
\\ ( https://arxiv.org/abs/2402.17753 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17759
Date: Tue, 27 Feb 2024 18:52:19 GMT   (1646kb,D)

Title: Towards Optimal Learning of Language Models
Authors: Yuxian Gu, Li Dong, Yaru Hao, Qingxiu Dong, Minlie Huang, Furu Wei
Categories: cs.CL
\\
  This work studies the general principles of improving the learning of
language models (LMs), which aims at reducing the necessary training steps for
achieving superior performance. Specifically, we present a theory for the
optimal learning of LMs. We first propose an objective that optimizes LM
learning by maximizing the data compression ratio in an
"LM-training-as-lossless-compression" view. Then, we derive a theorem, named
Learning Law, to reveal the properties of the dynamics in the optimal learning
process under our objective. The theorem is then validated by experiments on a
linear classification and a real-world language modeling task. Finally, we
empirically verify that the optimal learning of LMs essentially stems from the
improvement of the coefficients in the scaling law of LMs, indicating great
promise and significance for designing practical learning acceleration methods.
Our code can be found at https://aka.ms/LearningLaw.
\\ ( https://arxiv.org/abs/2402.17759 ,  1646kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17762
Date: Tue, 27 Feb 2024 18:55:17 GMT   (5854kb,D)

Title: Massive Activations in Large Language Models
Authors: Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu
Categories: cs.CL cs.LG
Comments: Website at
  https://eric-mingjie.github.io/massive-activations/index.html
\\
  We observe an empirical phenomenon in Large Language Models (LLMs) -- very
few activations exhibit significantly larger values than others (e.g., 100,000
times larger). We call them massive activations. First, we demonstrate the
widespread existence of massive activations across various LLMs and
characterize their locations. Second, we find their values largely stay
constant regardless of the input, and they function as indispensable bias terms
in LLMs. Third, these massive activations lead to the concentration of
attention probabilities to their corresponding tokens, and further, implicit
bias terms in the self-attention output. Last, we also study massive
activations in Vision Transformers.
\\ ( https://arxiv.org/abs/2402.17762 ,  5854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17764
Date: Tue, 27 Feb 2024 18:56:19 GMT   (201kb,D)

Title: The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
Authors: Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan
  Huang, Li Dong, Ruiping Wang, Jilong Xue, Furu Wei
Categories: cs.CL cs.LG
Comments: Work in progress
\\
  Recent research, such as BitNet, is paving the way for a new era of 1-bit
Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,
namely BitNet b1.58, in which every single parameter (or weight) of the LLM is
ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)
Transformer LLM with the same model size and training tokens in terms of both
perplexity and end-task performance, while being significantly more
cost-effective in terms of latency, memory, throughput, and energy consumption.
More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for
training new generations of LLMs that are both high-performance and
cost-effective. Furthermore, it enables a new computation paradigm and opens
the door for designing specific hardware optimized for 1-bit LLMs.
\\ ( https://arxiv.org/abs/2402.17764 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16880
Date: Sun, 18 Feb 2024 12:44:15 GMT   (1889kb,D)

Title: BESA: Pruning Large Language Models with Blockwise Parameter-Efficient
  Sparsity Allocation
Authors: Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng
  Gao, Fengwei An, Yu Qiao, Ping Luo
Categories: cs.LG cs.AI cs.CL
\\
  Large language models (LLMs) have demonstrated outstanding performance in
various tasks, such as text summarization, text question-answering, and etc.
While their performance is impressive, the computational footprint due to their
vast number of parameters can be prohibitive. Existing solutions such as
SparseGPT and Wanda attempt to alleviate this issue through weight pruning.
However, their layer-wise approach results in significant perturbation to the
model's output and requires meticulous hyperparameter tuning, such as the
pruning rate, which can adversely affect overall model performance. To address
this, this paper introduces a novel LLM pruning technique dubbed blockwise
parameter-efficient sparsity allocation (BESA) by applying a blockwise
reconstruction loss. In contrast to the typical layer-wise pruning techniques,
BESA is characterized by two distinctive attributes: i) it targets the overall
pruning error with respect to individual transformer blocks, and ii) it
allocates layer-specific sparsity in a differentiable manner, both of which
ensure reduced performance degradation after pruning. Our experiments show that
BESA achieves state-of-the-art performance, efficiently pruning LLMs like
LLaMA1, and LLaMA2 with 7B to 70B parameters on a single A100 GPU in just five
hours. Code is available at
\href{https://github.com/OpenGVLab/LLMPrune-BESA}{here}.
\\ ( https://arxiv.org/abs/2402.16880 ,  1889kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16888
Date: Fri, 23 Feb 2024 09:43:52 GMT   (7225kb,D)

Title: Chaotic attractor reconstruction using small reservoirs - the influence
  of topology
Authors: Lina Jaurigue
Categories: cs.LG cs.ET math-ph math.MP nlin.CD physics.comp-ph
\\
  Forecasting timeseries based upon measured data is needed in a wide range of
applications and has been the subject of extensive research. A particularly
challenging task is the forecasting of timeseries generated by chaotic
dynamics. In recent years reservoir computing has been shown to be an effective
method of forecasting chaotic dynamics and reconstructing chaotic attractors
from data. In this work strides are made toward smaller and lower complexity
reservoirs with the goal of improved hardware implementability and more
reliable production of adequate surrogate models. We show that a reservoir of
uncoupled nodes more reliably produces long term timeseries predictions than
complex reservoir topologies. We then link the improved attractor
reconstruction of the uncoupled reservoir with smaller spectral radii of the
resulting surrogate systems. These results indicate that, the node degree plays
an important role in determining whether the desired dynamics will be stable in
the autonomous surrogate system which is attained via closed-loop operation of
the trained reservoir. In terms of hardware implementability, uncoupled nodes
would allow for greater freedom in the hardware architecture because no complex
coupling setups are needed and because, for uncoupled nodes, the system
response is equivalent for space and time multiplexing.
\\ ( https://arxiv.org/abs/2402.16888 ,  7225kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16889
Date: Fri, 23 Feb 2024 10:48:21 GMT   (47752kb,D)

Title: Generative Models are Self-Watermarked: Declaring Model Authentication
  through Re-Generation
Authors: Aditya Desu, Xuanli He, Qiongkai Xu, Wei Lu
Categories: cs.LG cs.AI cs.CR
\\
  As machine- and AI-generated content proliferates, protecting the
intellectual property of generative models has become imperative, yet verifying
data ownership poses formidable challenges, particularly in cases of
unauthorized reuse of generated data. The challenge of verifying data ownership
is further amplified by using Machine Learning as a Service (MLaaS), which
often functions as a black-box system.
  Our work is dedicated to detecting data reuse from even an individual sample.
Traditionally, watermarking has been leveraged to detect AI-generated content.
However, unlike watermarking techniques that embed additional information as
triggers into models or generated content, potentially compromising output
quality, our approach identifies latent fingerprints inherently present within
the outputs through re-generation. We propose an explainable verification
procedure that attributes data ownership through re-generation, and further
amplifies these fingerprints in the generative models through iterative data
re-generation. This methodology is theoretically grounded and demonstrates
viability and robustness using recent advanced text and image generative
models. Our methodology is significant as it goes beyond protecting the
intellectual property of APIs and addresses important issues such as the spread
of misinformation and academic misconduct. It provides a useful tool to ensure
the integrity of sources and authorship, expanding its application in different
scenarios where authenticity and ownership verification are essential.
\\ ( https://arxiv.org/abs/2402.16889 ,  47752kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16891
Date: Fri, 23 Feb 2024 13:25:23 GMT   (2251kb,D)

Title: Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot
  Generalization
Authors: Fei Liu, Xi Lin, Qingfu Zhang, Xialiang Tong, Mingxuan Yuan
Categories: cs.LG cs.AI
\\
  Vehicle routing problems (VRPs), which can be found in numerous real-world
applications, have been an important research topic for several decades.
Recently, the neural combinatorial optimization (NCO) approach that leverages a
learning-based model to solve VRPs without manual algorithm design has gained
substantial attention. However, current NCO methods typically require building
one model for each routing problem, which significantly hinders their practical
application for real-world industry problems with diverse attributes. In this
work, we make the first attempt to tackle the crucial challenge of
cross-problem generalization. In particular, we formulate VRPs as different
combinations of a set of shared underlying attributes and solve them
simultaneously via a single model through attribute composition. In this way,
our proposed model can successfully solve VRPs with unseen attribute
combinations in a zero-shot generalization manner. Extensive experiments are
conducted on eleven VRP variants, benchmark datasets, and industry logistic
scenarios. The results show that the unified model demonstrates superior
performance in the eleven VRPs, reducing the average gap to around 5% from over
20% in the existing approach and achieving a significant performance boost on
benchmark datasets as well as a real-world logistics application.
\\ ( https://arxiv.org/abs/2402.16891 ,  2251kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16897
Date: Sat, 24 Feb 2024 03:47:06 GMT   (3867kb,D)

Title: Reliable Conflictive Multi-View Learning
Authors: Cai Xu, Jiajun Si, Ziyu Guan, Wei Zhao, Yue Wu, Xiyue Gao
Categories: cs.LG cs.AI
Comments: 9 pages and to be appeared in AAAI2024
\\
  Multi-view learning aims to combine multiple features to achieve more
comprehensive descriptions of data. Most previous works assume that multiple
views are strictly aligned. However, real-world multi-view data may contain
low-quality conflictive instances, which show conflictive information in
different views. Previous methods for this problem mainly focus on eliminating
the conflictive data instances by removing them or replacing conflictive views.
Nevertheless, real-world applications usually require making decisions for
conflictive instances rather than only eliminating them. To solve this, we
point out a new Reliable Conflictive Multi-view Learning (RCML) problem, which
requires the model to provide decision results and attached reliabilities for
conflictive multi-view data. We develop an Evidential Conflictive Multi-view
Learning (ECML) method for this problem. ECML first learns view-specific
evidence, which could be termed as the amount of support to each category
collected from data. Then, we can construct view-specific opinions consisting
of decision results and reliability. In the multi-view fusion stage, we propose
a conflictive opinion aggregation strategy and theoretically prove this
strategy can exactly model the relation of multi-view common and view-specific
reliabilities. Experiments performed on 6 datasets verify the effectiveness of
ECML.
\\ ( https://arxiv.org/abs/2402.16897 ,  3867kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16899
Date: Sat, 24 Feb 2024 06:31:43 GMT   (249kb,D)

Title: A prior Estimates for Deep Residual Network in Continuous-time
  Reinforcement Learning
Authors: Shuyu Yin, Qixuan Zhou, Fei Wen, Tao Luo
Categories: cs.LG cs.AI
\\
  Deep reinforcement learning excels in numerous large-scale practical
applications. However, existing performance analyses ignores the unique
characteristics of continuous-time control problems, is unable to directly
estimate the generalization error of the Bellman optimal loss and require a
boundedness assumption. Our work focuses on continuous-time control problems
and proposes a method that is applicable to all such problems where the
transition function satisfies semi-group and Lipschitz properties. Under this
method, we can directly analyze the \emph{a priori} generalization error of the
Bellman optimal loss. The core of this method lies in two transformations of
the loss function. To complete the transformation, we propose a decomposition
method for the maximum operator. Additionally, this analysis method does not
require a boundedness assumption. Finally, we obtain an \emph{a priori}
generalization error without the curse of dimensionality.
\\ ( https://arxiv.org/abs/2402.16899 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16902
Date: Sat, 24 Feb 2024 13:39:05 GMT   (160kb,D)

Title: PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA
Authors: Sheng Wang, Boyang Xue, Jiacheng Ye, Jiyue Jiang, Liheng Chen,
  Lingpeng Kong, Chuan Wu
Categories: cs.LG
\\
  With the rapid scaling of large language models (LLMs), serving numerous
LoRAs concurrently has become increasingly impractical, leading to unaffordable
costs and necessitating more parameter-efficient finetuning methods. In this
work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA),
an intra-layer sharing mechanism comprising four essential components:
broadcast reduction, rotation enhancement, partially-sharing refinement, and
rectified initialization strategy. As a superset of LoRA, PRoLoRA pertains its
advantages, and effectively circumvent the drawbacks of peer parameter-sharing
methods with superior model capacity, practical feasibility, and broad
applicability. Empirical experiments demonstrate the remarkably higher
parameter efficiency of PRoLoRA in both specific parameter budget and
performance target scenarios, and its scalability to larger LLMs. Notably, with
one time less trainable parameters, PRoLoRA still outperforms LoRA on multiple
instruction tuning datasets. Subsequently, an ablation study is conducted to
validate the necessity of individual components and highlight the superiority
of PRoLoRA over three potential variants. Hopefully, the conspicuously higher
parameter efficiency can establish PRoLoRA as a resource-friendly alternative
to LoRA.
\\ ( https://arxiv.org/abs/2402.16902 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16903
Date: Sat, 24 Feb 2024 14:42:42 GMT   (15588kb,D)

Title: A novel data generation scheme for surrogate modelling with deep
  operator networks
Authors: Shivam Choubey, Birupaksha Pal, Manish Agrawal
Categories: cs.LG cs.NA math.NA
\\
  Operator-based neural network architectures such as DeepONets have emerged as
a promising tool for the surrogate modeling of physical systems. In general,
towards operator surrogate modeling, the training data is generated by solving
the PDEs using techniques such as Finite Element Method (FEM). The
computationally intensive nature of data generation is one of the biggest
bottleneck in deploying these surrogate models for practical applications. In
this study, we propose a novel methodology to alleviate the computational
burden associated with training data generation for DeepONets. Unlike existing
literature, the proposed framework for data generation does not use any partial
differential equation integration strategy, thereby significantly reducing the
computational cost associated with generating training dataset for DeepONet. In
the proposed strategy, first, the output field is generated randomly,
satisfying the boundary conditions using Gaussian Process Regression (GPR).
From the output field, the input source field can be calculated easily using
finite difference techniques. The proposed methodology can be extended to other
operator learning methods, making the approach widely applicable. To validate
the proposed approach, we employ the heat equations as the model problem and
develop the surrogate model for numerous boundary value problems.
\\ ( https://arxiv.org/abs/2402.16903 ,  15588kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16904
Date: Sat, 24 Feb 2024 18:46:06 GMT   (2811kb,D)

Title: Selective Task offloading for Maximum Inference Accuracy and Energy
  efficient Real-Time IoT Sensing Systems
Authors: Abdelkarim Ben Sada, Amar Khelloufi, Abdenacer Naouri, Huansheng Ning
  and Sahraoui Dhelim
Categories: cs.LG cs.AI
\\
  The recent advancements in small-size inference models facilitated AI
deployment on the edge. However, the limited resource nature of edge devices
poses new challenges especially for real-time applications. Deploying multiple
inference models (or a single tunable model) varying in size and therefore
accuracy and power consumption, in addition to an edge server inference model,
can offer a dynamic system in which the allocation of inference models to
inference jobs is performed according to the current resource conditions.
Therefore, in this work, we tackle the problem of selectively allocating
inference models to jobs or offloading them to the edge server to maximize
inference accuracy under time and energy constraints. This problem is shown to
be an instance of the unbounded multidimensional knapsack problem which is
considered a strongly NP-hard problem. We propose a lightweight hybrid genetic
algorithm (LGSTO) to solve this problem. We introduce a termination condition
and neighborhood exploration techniques for faster evolution of populations. We
compare LGSTO with the Naive and Dynamic programming solutions. In addition to
classic genetic algorithms using different reproduction methods including
NSGA-II, and finally we compare to other evolutionary methods such as Particle
swarm optimization (PSO) and Ant colony optimization (ACO). Experiment results
show that LGSTO performed 3 times faster than the fastest comparable schemes
while producing schedules with higher average accuracy.
\\ ( https://arxiv.org/abs/2402.16904 ,  2811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16909
Date: Sun, 25 Feb 2024 12:07:32 GMT   (1344kb,D)

Title: Impact of Physical Activity on Quality of Life During Pregnancy: A
  Causal ML Approach
Authors: Kianoosh Kazemi, Iina Ryht\"a, Iman Azimi, Hannakaisa Niela-Vilen,
  Anna Axelin, Amir M. Rahmani, Pasi Liljeberg
Categories: cs.LG stat.ME
\\
  The concept of Quality of Life (QoL) refers to a holistic measurement of an
individual's well-being, incorporating psychological and social aspects.
Pregnant women, especially those with obesity and stress, often experience
lower QoL. Physical activity (PA) has shown the potential to enhance the QoL.
However, pregnant women who are overweight and obese rarely meet the
recommended level of PA. Studies have investigated the relationship between PA
and QoL during pregnancy using correlation-based approaches. These methods aim
to discover spurious correlations between variables rather than causal
relationships. Besides, the existing methods mainly rely on physical activity
parameters and neglect the use of different factors such as maternal (medical)
history and context data, leading to biased estimates. Furthermore, the
estimations lack an understanding of mediators and counterfactual scenarios
that might affect them. In this paper, we investigate the causal relationship
between being physically active (treatment variable) and the QoL (outcome)
during pregnancy and postpartum. To estimate the causal effect, we develop a
Causal Machine Learning method, integrating causal discovery and causal
inference components. The data for our investigation is derived from a
long-term wearable-based health monitoring study focusing on overweight and
obese pregnant women. The machine learning (meta-learner) estimation technique
is used to estimate the causal effect. Our result shows that performing
adequate physical activity during pregnancy and postpartum improves the QoL by
units of 7.3 and 3.4 on average in physical health and psychological domains,
respectively. In the final step, four refutation analysis techniques are
employed to validate our estimation.
\\ ( https://arxiv.org/abs/2402.16909 ,  1344kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16911
Date: Sun, 25 Feb 2024 13:28:08 GMT   (279kb,D)

Title: Trustworthy Personalized Bayesian Federated Learning via Posterior
  Fine-Tune
Authors: Mengen Luo, Chi Xu, Ercan Engin Kuruoglu
Categories: cs.LG cs.CV
\\
  Performance degradation owing to data heterogeneity and low output
interpretability are the most significant challenges faced by federated
learning in practical applications. Personalized federated learning diverges
from traditional approaches, as it no longer seeks to train a single model, but
instead tailors a unique personalized model for each client. However, previous
work focused only on personalization from the perspective of neural network
parameters and lack of robustness and interpretability. In this work, we
establish a novel framework for personalized federated learning, incorporating
Bayesian methodology which enhances the algorithm's ability to quantify
uncertainty. Furthermore, we introduce normalizing flow to achieve
personalization from the parameter posterior perspective and theoretically
analyze the impact of normalizing flow on out-of-distribution (OOD) detection
for Bayesian neural networks. Finally, we evaluated our approach on
heterogeneous datasets, and the experimental results indicate that the new
algorithm not only improves accuracy but also outperforms the baseline
significantly in OOD detection due to the reliable output of the Bayesian
approach.
\\ ( https://arxiv.org/abs/2402.16911 ,  279kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16913
Date: Sun, 25 Feb 2024 17:39:44 GMT   (880kb,D)

Title: PDETime: Rethinking Long-Term Multivariate Time Series Forecasting from
  the perspective of partial differential equations
Authors: Shiyi Qi, Zenglin Xu, Yiduo Li, Liangjian Wen, Qingsong Wen, Qifan
  Wang, Yuan Qi
Categories: cs.LG
\\
  Recent advancements in deep learning have led to the development of various
models for long-term multivariate time-series forecasting (LMTF), many of which
have shown promising results. Generally, the focus has been on
historical-value-based models, which rely on past observations to predict
future series. Notably, a new trend has emerged with time-index-based models,
offering a more nuanced understanding of the continuous dynamics underlying
time series. Unlike these two types of models that aggregate the information of
spatial domains or temporal domains, in this paper, we consider multivariate
time series as spatiotemporal data regularly sampled from a continuous
dynamical system, which can be represented by partial differential equations
(PDEs), with the spatial domain being fixed. Building on this perspective, we
present PDETime, a novel LMTF model inspired by the principles of Neural PDE
solvers, following the encoding-integration-decoding operations. Our extensive
experimentation across seven diverse real-world LMTF datasets reveals that
PDETime not only adapts effectively to the intrinsic spatiotemporal nature of
the data but also sets new benchmarks, achieving state-of-the-art results
\\ ( https://arxiv.org/abs/2402.16913 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16915
Date: Sun, 25 Feb 2024 18:27:25 GMT   (10026kb,D)

Title: More Than Routing: Joint GPS and Route Modeling for Refine Trajectory
  Representation Learning
Authors: Zhipeng Ma, Zheyan Tu, Xinhai Chen, Yan Zhang, Deguo Xia, Guyue Zhou,
  Yilun Chen, Yu Zheng, Jiangtao Gong
Categories: cs.LG cs.AI
\\
  Trajectory representation learning plays a pivotal role in supporting various
downstream tasks. Traditional methods in order to filter the noise in GPS
trajectories tend to focus on routing-based methods used to simplify the
trajectories. However, this approach ignores the motion details contained in
the GPS data, limiting the representation capability of trajectory
representation learning. To fill this gap, we propose a novel representation
learning framework that Joint GPS and Route Modelling based on self-supervised
technology, namely JGRM. We consider GPS trajectory and route as the two modes
of a single movement observation and fuse information through inter-modal
information interaction. Specifically, we develop two encoders, each tailored
to capture representations of route and GPS trajectories respectively. The
representations from the two modalities are fed into a shared transformer for
inter-modal information interaction. Eventually, we design three
self-supervised tasks to train the model. We validate the effectiveness of the
proposed method on two real datasets based on extensive experiments. The
experimental results demonstrate that JGRM outperforms existing methods in both
road segment representation and trajectory representation tasks. Our source
code is available at Anonymous Github.
\\ ( https://arxiv.org/abs/2402.16915 ,  10026kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16918
Date: Mon, 26 Feb 2024 04:47:32 GMT   (715kb,D)

Title: m2mKD: Module-to-Module Knowledge Distillation for Modular Transformers
Authors: Ka Man Lo, Yiming Liang, Wenyu Du, Yuantao Fan, Zili Wang, Wenhao
  Huang, Lei Ma, Jie Fu
Categories: cs.LG cs.CV
\\
  Modular neural architectures are gaining increasing attention due to their
powerful capability for generalization and sample-efficient adaptation to new
domains. However, training modular models, particularly in the early stages,
poses challenges due to the optimization difficulties arising from their
intrinsic sparse connectivity. Leveraging the knowledge from monolithic models,
using techniques such as knowledge distillation, is likely to facilitate the
training of modular models and enable them to integrate knowledge from multiple
models pretrained on diverse sources. Nevertheless, conventional knowledge
distillation approaches are not tailored to modular models and can fail when
directly applied due to the unique architectures and the enormous number of
parameters involved. Motivated by these challenges, we propose a general
module-to-module knowledge distillation (m2mKD) method for transferring
knowledge between modules. Our approach involves teacher modules split from a
pretrained monolithic model, and student modules of a modular model. m2mKD
separately combines these modules with a shared meta model and encourages the
student module to mimic the behaviour of the teacher module. We evaluate the
effectiveness of m2mKD on two distinct modular neural architectures: Neural
Attentive Circuits (NACs) and Vision Mixture-of-Experts (V-MoE). By applying
m2mKD to NACs, we achieve significant improvements in IID accuracy on
Tiny-ImageNet (up to 5.6%) and OOD robustness on Tiny-ImageNet-R (up to 4.2%).
On average, we observe a 1% gain in both ImageNet and ImageNet-R. The
V-MoE-Base model trained using m2mKD also achieves 3.5% higher accuracy than
end-to-end training on ImageNet. The experimental results demonstrate that our
method offers a promising solution for connecting modular networks with
pretrained monolithic models. Code is available at
https://github.com/kamanphoebe/m2mKD.
\\ ( https://arxiv.org/abs/2402.16918 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16919
Date: Mon, 26 Feb 2024 06:29:05 GMT   (1092kb,D)

Title: Personalized Federated Instruction Tuning via Neural Architecture Search
Authors: Pengyu Zhang, Yingbo Zhou, Ming Hu, Junxian Feng, Jiawen Weng, and
  Mingsong Chen
Categories: cs.LG
\\
  Federated Instruction Tuning (FIT) has shown the ability to achieve
collaborative model instruction tuning among massive data owners without
sharing private data. However, it still faces two key challenges, i.e., data
and resource heterogeneity. Due to the varying data distribution and
preferences among data owners, FIT cannot adapt to the personalized data of
individual owners. Moreover, clients with superior computational abilities are
constrained since they need to maintain the same fine-tuning architecture as
the weaker clients. To address these issues, we propose a novel Personalized
Federated Instruction Tuning (PerFIT) framework based on architecture search.
Specifically, PerFIT allows each client to search for a personalized
architecture by expanding the trainable parameter space of the global model
followed by pruning the parameters to the original state. This procedure allows
personalized instruction fine-tuning within expanded parameter spaces,
concurrently preserving the same number of trainable parameters. Furthermore,
to release the abilities of heterogeneous computational resources and enhance
the performance of personalization on local data, we exploit personalized
parameter-wise aggregation. The evaluation with multiple LLMs non-IID scenarios
demonstrates that compared to the state-of-the-art FIT methods, our approach
can achieve up to a 23% decrease in perplexity.
\\ ( https://arxiv.org/abs/2402.16919 ,  1092kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16925
Date: Mon, 26 Feb 2024 11:18:53 GMT   (514kb,D)

Title: Minimize Control Inputs for Strong Structural Controllability Using
  Reinforcement Learning with Graph Neural Network
Authors: Mengbang Zou, Weisi Guo, Bailu Jin
Categories: cs.LG cs.AI
\\
  Strong structural controllability (SSC) guarantees networked system with
linear-invariant dynamics controllable for all numerical realizations of
parameters. Current research has established algebraic and graph-theoretic
conditions of SSC for zero/nonzero or zero/nonzero/arbitrary structure. One
relevant practical problem is how to fully control the system with the minimal
number of input signals and identify which nodes must be imposed signals.
Previous work shows that this optimization problem is NP-hard and it is
difficult to find the solution. To solve this problem, we formulate the graph
coloring process as a Markov decision process (MDP) according to the
graph-theoretical condition of SSC for both zero/nonzero and
zero/nonzero/arbitrary structure. We use Actor-critic method with Directed
graph neural network which represents the color information of graph to
optimize MDP. Our method is validated in a social influence network with real
data and different complex network models. We find that the number of input
nodes is determined by the average degree of the network and the input nodes
tend to select nodes with low in-degree and avoid high-degree nodes.
\\ ( https://arxiv.org/abs/2402.16925 ,  514kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16933
Date: Mon, 26 Feb 2024 17:20:16 GMT   (1783kb,D)

Title: Avoiding Catastrophic Forgetting in Visual Classification Using Human
  Concept Formation
Authors: Nicki Barari, Xin Lian, Christopher J. MacLellan
Categories: cs.LG cs.AI cs.CV cs.IR
\\
  Deep neural networks have excelled in machine learning, particularly in
vision tasks, however, they often suffer from catastrophic forgetting when
learning new tasks sequentially. In this work, we propose Cobweb4V, a novel
visual classification approach that builds on Cobweb, a human like learning
system that is inspired by the way humans incrementally learn new concepts over
time. In this research, we conduct a comprehensive evaluation, showcasing the
proficiency of Cobweb4V in learning visual concepts, requiring less data to
achieve effective learning outcomes compared to traditional methods,
maintaining stable performance over time, and achieving commendable asymptotic
behavior, without catastrophic forgetting effects. These characteristics align
with learning strategies in human cognition, positioning Cobweb4V as a
promising alternative to neural network approaches.
\\ ( https://arxiv.org/abs/2402.16933 ,  1783kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16934
Date: Mon, 26 Feb 2024 17:53:15 GMT   (4158kb,D)

Title: FedReview: A Review Mechanism for Rejecting Poisoned Updates in
  Federated Learning
Authors: Tianhang Zheng and Baochun Li
Categories: cs.LG cs.AI cs.CR
\\
  Federated learning has recently emerged as a decentralized approach to learn
a high-performance model without access to user data. Despite its
effectiveness, federated learning gives malicious users opportunities to
manipulate the model by uploading poisoned model updates to the server. In this
paper, we propose a review mechanism called FedReview to identify and decline
the potential poisoned updates in federated learning. Under our mechanism, the
server randomly assigns a subset of clients as reviewers to evaluate the model
updates on their training datasets in each round. The reviewers rank the model
updates based on the evaluation results and count the number of the updates
with relatively low quality as the estimated number of poisoned updates. Based
on review reports, the server employs a majority voting mechanism to integrate
the rankings and remove the potential poisoned updates in the model aggregation
process. Extensive evaluation on multiple datasets demonstrate that FedReview
can assist the server to learn a well-performed global model in an adversarial
environment.
\\ ( https://arxiv.org/abs/2402.16934 ,  4158kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17002
Date: Mon, 26 Feb 2024 20:18:43 GMT   (3968kb,D)

Title: Discovering Symmetry Group Structures via Implicit Orthogonality Bias
Authors: Dongsung Huh
Categories: cs.LG math.GR math.RT
Comments: 19 pages, 14 figures
\\
  We introduce the HyperCube network, a novel approach for autonomously
discovering symmetry group structures within data. The key innovation is a
unique factorization architecture coupled with a novel regularizer that
instills a powerful inductive bias towards learning orthogonal representations.
This leverages a fundamental theorem of representation theory that all
compact/finite groups can be represented by orthogonal matrices. HyperCube
efficiently learns general group operations from partially observed data,
successfully recovering complete operation tables. Remarkably, the learned
factors correspond directly to exact matrix representations of the underlying
group. Moreover, these factors capture the group's complete set of irreducible
representations, forming the generalized Fourier basis for performing group
convolutions. In extensive experiments with both group and non-group symbolic
operations, HyperCube demonstrates a dramatic 100-1000x improvement in training
speed and 2-10x greater sample efficiency compared to the Transformer baseline.
These results suggest that our approach unlocks a new class of deep learning
models capable of harnessing inherent symmetries within data, leading to
significant improvements in performance and broader applicability.
\\ ( https://arxiv.org/abs/2402.17002 ,  3968kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17003
Date: Mon, 26 Feb 2024 20:19:14 GMT   (1158kb,D)

Title: Monitoring Fidelity of Online Reinforcement Learning Algorithms in
  Clinical Trials
Authors: Anna L. Trella, Kelly W. Zhang, Inbal Nahum-Shani, Vivek Shetty, Iris
  Yan, Finale Doshi-Velez, Susan A. Murphy
Categories: cs.LG cs.AI cs.CY
\\
  Online reinforcement learning (RL) algorithms offer great potential for
personalizing treatment for participants in clinical trials. However, deploying
an online, autonomous algorithm in the high-stakes healthcare setting makes
quality control and data quality especially difficult to achieve. This paper
proposes algorithm fidelity as a critical requirement for deploying online RL
algorithms in clinical trials. It emphasizes the responsibility of the
algorithm to (1) safeguard participants and (2) preserve the scientific utility
of the data for post-trial analyses. We also present a framework for
pre-deployment planning and real-time monitoring to help algorithm developers
and clinical researchers ensure algorithm fidelity. To illustrate our
framework's practical application, we present real-world examples from the
Oralytics clinical trial. Since Spring 2023, this trial successfully deployed
an autonomous, online RL algorithm to personalize behavioral interventions for
participants at risk for dental disease.
\\ ( https://arxiv.org/abs/2402.17003 ,  1158kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17018
Date: Mon, 26 Feb 2024 20:55:47 GMT   (99kb,D)

Title: A Curious Case of Remarkable Resilience to Gradient Attacks via Fully
  Convolutional and Differentiable Front End with a Skip Connection
Authors: Leonid Boytsov, Ameya Joshi, Filipe Condessa
Categories: cs.LG cs.AI cs.CV
\\
  We tested front-end enhanced neural models where a frozen classifier was
prepended by a differentiable and fully convolutional model with a skip
connection. By training them using a small learning rate for about one epoch,
we obtained models that retained the accuracy of the backbone classifier while
being unusually resistant to gradient attacks including APGD and FAB-T attacks
from the AutoAttack package, which we attributed to gradient masking. The
gradient masking phenomenon is not new, but the degree of masking was quite
remarkable for fully differentiable models that did not have
gradient-shattering components such as JPEG compression or components that are
expected to cause diminishing gradients.
  Though black box attacks can be partially effective against gradient masking,
they are easily defeated by combining models into randomized ensembles. We
estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10,
CIFAR100, and ImageNet despite having virtually zero accuracy under adaptive
attacks. Adversarial training of the backbone classifier can further increase
resistance of the front-end enhanced model to gradient attacks. On CIFAR10, the
respective randomized ensemble achieved 90.8$\pm 2.5$% (99% CI) accuracy under
AutoAttack while having only 18.2$\pm 3.6$% accuracy under the adaptive attack.
  We do not establish SOTA in adversarial robustness. Instead, we make
methodological contributions and further supports the thesis that adaptive
attacks designed with the complete knowledge of model architecture are crucial
in demonstrating model robustness and that even the so-called white-box
gradient attacks can have limited applicability. Although gradient attacks can
be complemented with black-box attack such as the SQUARE attack or the
zero-order PGD, black-box attacks can be weak against randomized ensembles,
e.g., when ensemble models mask gradients.
\\ ( https://arxiv.org/abs/2402.17018 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17061
Date: Mon, 26 Feb 2024 22:47:03 GMT   (15378kb,D)

Title: A Multi-Fidelity Methodology for Reduced Order Models with
  High-Dimensional Inputs
Authors: Bilal Mufti, Christian Perron and Dimitri N. Mavris (ASDL, Daniel
  Guggenheim School of Aerospace Engineering, Georgia Institute of Technology,
  Atlanta, Georgia)
Categories: cs.LG
\\
  In the early stages of aerospace design, reduced order models (ROMs) are
crucial for minimizing computational costs associated with using physics-rich
field information in many-query scenarios requiring multiple evaluations. The
intricacy of aerospace design demands the use of high-dimensional design spaces
to capture detailed features and design variability accurately. However, these
spaces introduce significant challenges, including the curse of dimensionality,
which stems from both high-dimensional inputs and outputs necessitating
substantial training data and computational effort. To address these
complexities, this study introduces a novel multi-fidelity, parametric, and
non-intrusive ROM framework designed for high-dimensional contexts. It
integrates machine learning techniques for manifold alignment and dimension
reduction employing Proper Orthogonal Decomposition (POD) and Model-based
Active Subspace with multi-fidelity regression for ROM construction. Our
approach is validated through two test cases: the 2D RAE~2822 airfoil and the
3D NASA CRM wing, assessing combinations of various fidelity levels, training
data ratios, and sample sizes. Compared to the single-fidelity PCAS method, our
multi-fidelity solution offers improved cost-accuracy benefits and achieves
better predictive accuracy with reduced computational demands. Moreover, our
methodology outperforms the manifold-aligned ROM (MA-ROM) method by 50% in
handling scenarios with large input dimensions, underscoring its efficacy in
addressing the complex challenges of aerospace design.
\\ ( https://arxiv.org/abs/2402.17061 ,  15378kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17073
Date: Mon, 26 Feb 2024 23:15:01 GMT   (619kb,D)

Title: One-Shot Graph Representation Learning Using Hyperdimensional Computing
Authors: Abhishek Dalvi, Vasant Honavar
Categories: cs.LG cs.AI cs.SI
\\
  We present a novel, simple, fast, and efficient approach for semi-supervised
learning on graphs. The proposed approach takes advantage of hyper-dimensional
computing which encodes data samples using random projections into a high
dimensional space (HD space for short). Specifically, we propose a
Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the
injectivity property of the node representations of a family of graph neural
networks. HDGL maps node features to the HD space and then uses HD operators
such as bundling and binding to aggregate information from the local
neighborhood of each node. Results of experiments with widely used benchmark
data sets show that HDGL achieves predictive performance that is competitive
with the state-of-the-art deep learning methods, without the need for
computationally expensive training.
\\ ( https://arxiv.org/abs/2402.17073 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17077
Date: Mon, 26 Feb 2024 23:16:34 GMT   (4629kb,D)

Title: Parallelized Spatiotemporal Binding
Authors: Gautam Singh, Yue Wang, Jiawei Yang, Boris Ivanovic, Sungjin Ahn,
  Marco Pavone, Tong Che
Categories: cs.LG cs.CV
Comments: See project page at https://parallel-st-binder.github.io
\\
  While modern best practices advocate for scalable architectures that support
long-range interactions, object-centric models are yet to fully embrace these
architectures. In particular, existing object-centric models for handling
sequential inputs, due to their reliance on RNN-based implementation, show poor
stability and capacity and are slow to train on long sequences. We introduce
Parallelizable Spatiotemporal Binder or PSB, the first
temporally-parallelizable slot learning architecture for sequential inputs.
Unlike conventional RNN-based approaches, PSB produces object-centric
representations, known as slots, for all time-steps in parallel. This is
achieved by refining the initial slots across all time-steps through a fixed
number of layers equipped with causal attention. By capitalizing on the
parallelism induced by our architecture, the proposed model exhibits a
significant boost in efficiency. In experiments, we test PSB extensively as an
encoder within an auto-encoding framework paired with a wide variety of decoder
options. Compared to the state-of-the-art, our architecture demonstrates stable
training on longer sequences, achieves parallelization that results in a 60%
increase in training speed, and yields performance that is on par with or
better on unsupervised 2D and 3D object-centric scene decomposition and
understanding.
\\ ( https://arxiv.org/abs/2402.17077 ,  4629kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17104
Date: Tue, 27 Feb 2024 00:41:00 GMT   (397kb,D)

Title: Adversarial Perturbations of Physical Signals
Authors: Robert L. Bassett, Austin Van Dellen, Anthony P. Austin
Categories: cs.LG cs.CR eess.SP math.OC stat.ML
MSC-class: 65K05, 90C30, 65M60
\\
  We investigate the vulnerability of computer-vision-based signal classifiers
to adversarial perturbations of their inputs, where the signals and
perturbations are subject to physical constraints. We consider a scenario in
which a source and interferer emit signals that propagate as waves to a
detector, which attempts to classify the source by analyzing the spectrogram of
the signal it receives using a pre-trained neural network. By solving
PDE-constrained optimization problems, we construct interfering signals that
cause the detector to misclassify the source even though the perturbations to
the spectrogram of the received signal are nearly imperceptible. Though such
problems can have millions of decision variables, we introduce methods to solve
them efficiently. Our experiments demonstrate that one can compute effective
and physically realizable adversarial perturbations for a variety of machine
learning models under various physical conditions.
\\ ( https://arxiv.org/abs/2402.17104 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17110
Date: Tue, 27 Feb 2024 01:13:58 GMT   (2366kb,D)

Title: Sinkhorn Distance Minimization for Knowledge Distillation
Authors: Xiao Cui, Yulei Qin, Yuting Gao, Enwei Zhang, Zihan Xu, Tong Wu, Ke
  Li, Xing Sun, Wengang Zhou and Houqiang Li
Categories: cs.LG
Comments: Accepted by COLING 2024
\\
  Knowledge distillation (KD) has been widely adopted to compress large
language models (LLMs). Existing KD methods investigate various divergence
measures including the Kullback-Leibler (KL), reverse Kullback-Leibler (RKL),
and Jensen-Shannon (JS) divergences. However, due to limitations inherent in
their assumptions and definitions, these measures fail to deliver effective
supervision when few distribution overlap exists between the teacher and the
student. In this paper, we show that the aforementioned KL, RKL, and JS
divergences respectively suffer from issues of mode-averaging, mode-collapsing,
and mode-underestimation, which deteriorates logits-based KD for diverse NLP
tasks. We propose the Sinkhorn Knowledge Distillation (SinKD) that exploits the
Sinkhorn distance to ensure a nuanced and precise assessment of the disparity
between teacher and student distributions. Besides, profit by properties of the
Sinkhorn metric, we can get rid of sample-wise KD that restricts the perception
of divergence in each teacher-student sample pair. Instead, we propose a
batch-wise reformulation to capture geometric intricacies of distributions
across samples in the high-dimensional space. Comprehensive evaluation on GLUE
and SuperGLUE, in terms of comparability, validity, and generalizability,
highlights our superiority over state-of-the-art methods on all kinds of LLMs
with encoder-only, encoder-decoder, and decoder-only architectures.
\\ ( https://arxiv.org/abs/2402.17110 ,  2366kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17120
Date: Tue, 27 Feb 2024 01:26:48 GMT   (445kb,D)

Title: LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable
  Machine Learning Models
Authors: Pedro Seber and Richard D. Braatz
Categories: cs.LG
\\
  Interpretable architectures can have advantages over black-box architectures,
and interpretability is essential for the application of machine learning in
critical settings, such as aviation or medicine. However, the simplest, most
commonly used interpretable architectures (such as LASSO or EN) are limited to
linear predictions and have poor feature selection capabilities. In this work,
we introduce the LASSO-Clip-EN (LCEN) algorithm for the creation of nonlinear,
interpretable machine learning models. LCEN is tested on a wide variety of
artificial and empirical datasets, creating more accurate, sparser models than
other commonly used architectures. These experiments reveal that LCEN is robust
against many issues typically present in datasets and modeling, including
noise, multicollinearity, data scarcity, and hyperparameter variance. LCEN is
also able to rediscover multiple physical laws from empirical data and, for
processes with no known physical laws, LCEN achieves better results than many
other dense and sparse methods -- including using 10.8 times fewer features
than dense methods and 8.1 times fewer features than EN on one dataset, and is
comparable to an ANN on another dataset.
\\ ( https://arxiv.org/abs/2402.17120 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17131
Date: Tue, 27 Feb 2024 01:53:02 GMT   (734kb,D)

Title: Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers
  and RNNs Trained with a New Loss Function
Authors: Pedro Seber
Categories: cs.LG q-bio.MN
\\
  Glycosylation, a protein modification, has multiple essential functional and
structural roles. O-GlcNAcylation, a subtype of glycosylation, has the
potential to be an important target for therapeutics, but methods to reliably
predict O-GlcNAcylation sites had not been available until 2023; a 2021 review
correctly noted that published models were insufficient and failed to
generalize. Moreover, many are no longer usable. In 2023, a considerably better
RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset
was published. This article first sought to improve these metrics using
transformer encoders. While transformers displayed high performance on this
dataset, their performance was inferior to that of the previously published
RNN. We then created a new loss function, which we call the weighted focal
differentiable MCC, to improve the performance of classification models. RNN
models trained with this new function display superior performance to models
trained using the weighted cross-entropy loss; this new function can also be
used to fine-tune trained models. A two-cell RNN trained with this loss
achieves state-of-the-art performance in O-GlcNAcylation site prediction with
an F$_1$ score of 38.82% and an MCC of 38.21% on that large dataset.
\\ ( https://arxiv.org/abs/2402.17131 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17135
Date: Tue, 27 Feb 2024 01:59:02 GMT   (23562kb,D)

Title: Unsupervised Zero-Shot Reinforcement Learning via Functional Reward
  Encodings
Authors: Kevin Frans, Seohong Park, Pieter Abbeel, Sergey Levine
Categories: cs.LG cs.AI
\\
  Can we pre-train a generalist agent from a large amount of unlabeled offline
trajectories such that it can be immediately adapted to any new downstream
tasks in a zero-shot manner? In this work, we present a functional reward
encoding (FRE) as a general, scalable solution to this zero-shot RL problem.
Our main idea is to learn functional representations of any arbitrary tasks by
encoding their state-reward samples using a transformer-based variational
auto-encoder. This functional encoding not only enables the pre-training of an
agent from a wide diversity of general unsupervised reward functions, but also
provides a way to solve any new downstream tasks in a zero-shot manner, given a
small number of reward-annotated samples. We empirically show that FRE agents
trained on diverse random unsupervised reward functions can generalize to solve
novel tasks in a range of simulated robotic benchmarks, often outperforming
previous zero-shot RL and offline RL methods. Code for this project is provided
at: https://github.com/kvfrans/fre
\\ ( https://arxiv.org/abs/2402.17135 ,  23562kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17152
Date: Tue, 27 Feb 2024 02:37:37 GMT   (1966kb,D)

Title: Actions Speak Louder than Words: Trillion-Parameter Sequential
  Transducers for Generative Recommendations
Authors: Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon
  Gao, Zhaojie Gong, Fangda Gu, Michael He, Yinghai Lu, Yu Shi
Categories: cs.LG cs.IR
Comments: Full technical report to follow
\\
  Large-scale recommendation systems are characterized by their reliance on
high cardinality, heterogeneous features and the need to handle tens of
billions of user actions on a daily basis. Despite being trained on huge volume
of data with thousands of features, most Deep Learning Recommendation Models
(DLRMs) in industry fail to scale with compute.
  Inspired by success achieved by Transformers in language and vision domains,
we revisit fundamental design choices in recommendation systems. We reformulate
recommendation problems as sequential transduction tasks within a generative
modeling framework (``Generative Recommenders''), and propose a new
architecture, HSTU, designed for high cardinality, non-stationary streaming
recommendation data.
  HSTU outperforms baselines over synthetic and public datasets by up to 65.8\%
in NDCG, and is 5.3x to 15.2x faster than FlashAttention2-based Transformers on
8192 length sequences. HSTU-based Generative Recommenders, with 1.5 trillion
parameters, improve metrics in online A/B tests by 12.4\% and have been
deployed on multiple surfaces of a large internet platform with billions of
users. More importantly, the model quality of Generative Recommenders
empirically scales as a power-law of training compute across three orders of
magnitude, up to GPT-3/LLaMa-2 scale, which reduces carbon footprint needed for
future model developments, and further paves the way for the first foundational
models in recommendations.
\\ ( https://arxiv.org/abs/2402.17152 ,  1966kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17156
Date: Tue, 27 Feb 2024 02:41:46 GMT   (1089kb,D)

Title: TaxDiff: Taxonomic-Guided Diffusion Model for Protein Sequence
  Generation
Authors: Lin Zongying, Li Hao, Lv Liuzhenghao, Lin Bin, Zhang Junwu, Chen
  Calvin Yu-Chian, Yuan Li, Tian Yonghong
Categories: cs.LG cs.AI q-bio.BM
\\
  Designing protein sequences with specific biological functions and structural
stability is crucial in biology and chemistry. Generative models already
demonstrated their capabilities for reliable protein design. However, previous
models are limited to the unconditional generation of protein sequences and
lack the controllable generation ability that is vital to biological tasks. In
this work, we propose TaxDiff, a taxonomic-guided diffusion model for
controllable protein sequence generation that combines biological species
information with the generative capabilities of diffusion models to generate
structurally stable proteins within the sequence space. Specifically, taxonomic
control information is inserted into each layer of the transformer block to
achieve fine-grained control. The combination of global and local attention
ensures the sequence consistency and structural foldability of
taxonomic-specific proteins. Extensive experiments demonstrate that TaxDiff can
consistently achieve better performance on multiple protein sequence generation
benchmarks in both taxonomic-guided controllable generation and unconditional
generation. Remarkably, the sequences generated by TaxDiff even surpass those
produced by direct-structure-generation models in terms of confidence based on
predicted structures and require only a quarter of the time of models based on
the diffusion model. The code for generating proteins and training new versions
of TaxDiff is available at:https://github.com/Linzy19/TaxDiff.
\\ ( https://arxiv.org/abs/2402.17156 ,  1089kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17157
Date: Tue, 27 Feb 2024 02:44:40 GMT   (40049kb,D)

Title: Generative Learning for Forecasting the Dynamics of Complex Systems
Authors: Han Gao, Sebastian Kaltenbach, Petros Koumoutsakos
Categories: cs.LG physics.comp-ph physics.flu-dyn stat.ML
\\
  We introduce generative models for accelerating simulations of complex
systems through learning and evolving their effective dynamics. In the proposed
Generative Learning of Effective Dynamics (G-LED), instances of high
dimensional data are down sampled to a lower dimensional manifold that is
evolved through an auto-regressive attention mechanism. In turn, Bayesian
diffusion models, that map this low-dimensional manifold onto its corresponding
high-dimensional space, capture the statistics of the system dynamics. We
demonstrate the capabilities and drawbacks of G-LED in simulations of several
benchmark systems, including the Kuramoto-Sivashinsky (KS) equation,
two-dimensional high Reynolds number flow over a backward-facing step, and
simulations of three-dimensional turbulent channel flow. The results
demonstrate that generative learning offers new frontiers for the accurate
forecasting of the statistical properties of complex systems at a reduced
computational cost.
\\ ( https://arxiv.org/abs/2402.17157 ,  40049kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17176
Date: Tue, 27 Feb 2024 03:24:54 GMT   (602kb,D)

Title: DeepDRK: Deep Dependency Regularized Knockoff for Feature Selection
Authors: Hongyu Shen and Yici Yan and Zhizhen Zhao
Categories: cs.LG
Comments: 23 pages, 14 figures, 7 tables
MSC-class: 68T07
ACM-class: I.5.1
\\
  Model-X knockoff, among various feature selection methods, received much
attention recently due to its guarantee on false discovery rate (FDR) control.
Subsequent to its introduction in parametric design, knockoff is advanced to
handle arbitrary data distributions using deep learning-based generative
modeling. However, we observed that current implementations of the deep Model-X
knockoff framework exhibit limitations. Notably, the "swap property" that
knockoffs necessitate frequently encounter challenges on sample level, leading
to a diminished selection power. To overcome, we develop "Deep Dependency
Regularized Knockoff (DeepDRK)", a distribution-free deep learning method that
strikes a balance between FDR and power. In DeepDRK, a generative model
grounded in a transformer architecture is introduced to better achieve the
"swap property". Novel efficient regularization techniques are also proposed to
reach higher power. Our model outperforms other benchmarks in synthetic,
semi-synthetic, and real-world data, especially when sample size is small and
data distribution is complex.
\\ ( https://arxiv.org/abs/2402.17176 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17179
Date: Tue, 27 Feb 2024 03:33:23 GMT   (3797kb,D)

Title: Dual-Space Optimization: Improved Molecule Sequence Design by Latent
  Prompt Transformer
Authors: Deqian Kong, Yuhao Huang, Jianwen Xie, Edouardo Honig, Ming Xu,
  Shuanghong Xue, Pei Lin, Sanping Zhou, Sheng Zhong, Nanning Zheng, Ying Nian
  Wu
Categories: cs.LG q-bio.BM
\\
  Designing molecules with desirable properties, such as drug-likeliness and
high binding affinities towards protein targets, is a challenging problem. In
this paper, we propose the Dual-Space Optimization (DSO) method that integrates
latent space sampling and data space selection to solve this problem. DSO
iteratively updates a latent space generative model and a synthetic dataset in
an optimization process that gradually shifts the generative model and the
synthetic data towards regions of desired property values. Our generative model
takes the form of a Latent Prompt Transformer (LPT) where the latent vector
serves as the prompt of a causal transformer. Our extensive experiments
demonstrate effectiveness of the proposed method, which sets new performance
benchmarks across single-objective, multi-objective and constrained molecule
design tasks.
\\ ( https://arxiv.org/abs/2402.17179 ,  3797kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17185
Date: Tue, 27 Feb 2024 03:44:55 GMT   (44971kb,D)

Title: Inpainting Computational Fluid Dynamics with Deep Learning
Authors: Dule Shu, Wilson Zhen, Zijie Li, Amir Barati Farimani
Categories: cs.LG physics.flu-dyn
Comments: 20 pages, 9 figures
\\
  Fluid data completion is a research problem with high potential benefit for
both experimental and computational fluid dynamics. An effective fluid data
completion method reduces the required number of sensors in a fluid dynamics
experiment, and allows a coarser and more adaptive mesh for a Computational
Fluid Dynamics (CFD) simulation. However, the ill-posed nature of the fluid
data completion problem makes it prohibitively difficult to obtain a
theoretical solution and presents high numerical uncertainty and instability
for a data-driven approach (e.g., a neural network model). To address these
challenges, we leverage recent advancements in computer vision, employing the
vector quantization technique to map both complete and incomplete fluid data
spaces onto discrete-valued lower-dimensional representations via a two-stage
learning procedure. We demonstrated the effectiveness of our approach on
Kolmogorov flow data (Reynolds number: 1000) occluded by masks of different
size and arrangement. Experimental results show that our proposed model
consistently outperforms benchmark models under different occlusion settings in
terms of point-wise reconstruction accuracy as well as turbulent energy
spectrum and vorticity distribution.
\\ ( https://arxiv.org/abs/2402.17185 ,  44971kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17202
Date: Tue, 27 Feb 2024 04:50:13 GMT   (4103kb,D)

Title: FedBRB: An Effective Solution to the Small-to-Large Scenario in
  Device-Heterogeneity Federated Learning
Authors: Ziyue Xu, Mingfeng Xu, Tianchi Liao, Zibin Zheng and Chuan Chen
Categories: cs.LG
\\
  Recently, the success of large models has demonstrated the importance of
scaling up model size. This has spurred interest in exploring collaborative
training of large-scale models from federated learning perspective. Due to
computational constraints, many institutions struggle to train a large-scale
model locally. Thus, training a larger global model using only smaller local
models has become an important scenario (i.e., the \textbf{small-to-large
scenario}). Although recent device-heterogeneity federated learning approaches
have started to explore this area, they face limitations in fully covering the
parameter space of the global model. In this paper, we propose a method called
\textbf{FedBRB} (\underline{B}lock-wise \underline{R}olling and weighted
\underline{B}roadcast) based on the block concept. FedBRB can uses small local
models to train all blocks of the large global model, and broadcasts the
trained parameters to the entire space for faster information interaction.
Experiments demonstrate FedBRB yields substantial performance gains, achieving
state-of-the-art results in this scenario. Moreover, FedBRB using only minimal
local models can even surpass baselines using larger local models.
\\ ( https://arxiv.org/abs/2402.17202 ,  4103kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17217
Date: Tue, 27 Feb 2024 05:16:59 GMT   (2911kb,D)

Title: Temporal Logic Specification-Conditioned Decision Transformer for
  Offline Safe Reinforcement Learning
Authors: Zijian Guo, Weichao Zhou, Wenchao Li
Categories: cs.LG cs.AI
\\
  Offline safe reinforcement learning (RL) aims to train a constraint
satisfaction policy from a fixed dataset. Current state-of-the-art approaches
are based on supervised learning with a conditioned policy. However, these
approaches fall short in real-world applications that involve complex tasks
with rich temporal and logical structures. In this paper, we propose temporal
logic Specification-conditioned Decision Transformer (SDT), a novel framework
that harnesses the expressive power of signal temporal logic (STL) to specify
complex temporal rules that an agent should follow and the sequential modeling
capability of Decision Transformer (DT). Empirical evaluations on the DSRL
benchmarks demonstrate the better capacity of SDT in learning safe and
high-reward policies compared with existing approaches. In addition, SDT shows
good alignment with respect to different desired degrees of satisfaction of the
STL specification that it is conditioned on.
\\ ( https://arxiv.org/abs/2402.17217 ,  2911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17227
Date: Tue, 27 Feb 2024 05:40:36 GMT   (585kb,D)

Title: Efficient Backpropagation with Variance-Controlled Adaptive Sampling
Authors: Ziteng Wang, Jianfei Chen, Jun Zhu
Categories: cs.LG
Comments: ICLR 2024
\\
  Sampling-based algorithms, which eliminate ''unimportant'' computations
during forward and/or back propagation (BP), offer potential solutions to
accelerate neural network training. However, since sampling introduces
approximations to training, such algorithms may not consistently maintain
accuracy across various tasks. In this work, we introduce a variance-controlled
adaptive sampling (VCAS) method designed to accelerate BP. VCAS computes an
unbiased stochastic gradient with fine-grained layerwise importance sampling in
data dimension for activation gradient calculation and leverage score sampling
in token dimension for weight gradient calculation. To preserve accuracy, we
control the additional variance by learning the sample ratio jointly with model
parameters during training. We assessed VCAS on multiple fine-tuning and
pre-training tasks in both vision and natural language domains. On all the
tasks, VCAS can preserve the original training loss trajectory and validation
accuracy with an up to 73.87% FLOPs reduction of BP and 49.58% FLOPs reduction
of the whole training process. The implementation is available at
https://github.com/thu-ml/VCAS .
\\ ( https://arxiv.org/abs/2402.17227 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17233
Date: Tue, 27 Feb 2024 06:01:56 GMT   (2491kb,D)

Title: Hybrid Square Neural ODE Causal Modeling
Authors: Bob Junyi Zou, Matthew E. Levine, Dessi P. Zaharieva, Ramesh Johari,
  Emily B. Fox
Categories: cs.LG stat.AP stat.ME
\\
  Hybrid models combine mechanistic ODE-based dynamics with flexible and
expressive neural network components. Such models have grown rapidly in
popularity, especially in scientific domains where such ODE-based modeling
offers important interpretability and validated causal grounding (e.g., for
counterfactual reasoning). The incorporation of mechanistic models also
provides inductive bias in standard blackbox modeling approaches, critical when
learning from small datasets or partially observed, complex systems.
Unfortunately, as hybrid models become more flexible, the causal grounding
provided by the mechanistic model can quickly be lost. We address this problem
by leveraging another common source of domain knowledge: ranking of treatment
effects for a set of interventions, even if the precise treatment effect is
unknown. We encode this information in a causal loss that we combine with the
standard predictive loss to arrive at a hybrid loss that biases our learning
towards causally valid hybrid models. We demonstrate our ability to achieve a
win-win -- state-of-the-art predictive performance and causal validity -- in
the challenging task of modeling glucose dynamics during exercise.
\\ ( https://arxiv.org/abs/2402.17233 ,  2491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17235
Date: Tue, 27 Feb 2024 06:05:01 GMT   (2734kb,D)

Title: Stochastic Gradient Succeeds for Bandits
Authors: Jincheng Mei and Zixin Zhong and Bo Dai and Alekh Agarwal and Csaba
  Szepesvari and Dale Schuurmans
Categories: cs.LG
Comments: 39 pages; Correction for a previous version published at ICML 2023
  conference
\\
  We show that the \emph{stochastic gradient} bandit algorithm converges to a
\emph{globally optimal} policy at an $O(1/t)$ rate, even with a \emph{constant}
step size. Remarkably, global convergence of the stochastic gradient bandit
algorithm has not been previously established, even though it is an old
algorithm known to be applicable to bandits. The new result is achieved by
establishing two novel technical findings: first, the noise of the stochastic
updates in the gradient bandit algorithm satisfies a strong ``growth
condition'' property, where the variance diminishes whenever progress becomes
small, implying that additional noise control via diminishing step sizes is
unnecessary; second, a form of ``weak exploration'' is automatically achieved
through the stochastic gradient updates, since they prevent the action
probabilities from decaying faster than $O(1/t)$, thus ensuring that every
action is sampled infinitely often with probability $1$. These two findings can
be used to show that the stochastic gradient update is already ``sufficient''
for bandits in the sense that exploration versus exploitation is automatically
balanced in a manner that ensures almost sure convergence to a global optimum.
These novel theoretical findings are further verified by experimental results.
\\ ( https://arxiv.org/abs/2402.17235 ,  2734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17238
Date: Tue, 27 Feb 2024 06:13:02 GMT   (18224kb,D)

Title: Does Negative Sampling Matter? A Review with Insights into its Theory
  and Applications
Authors: Zhen Yang, Ming Ding, Tinglin Huang, Yukuo Cen, Junshuai Song, Bin Xu,
  Yuxiao Dong, and Jie Tang
Categories: cs.LG
Comments: 20 pages, 11 figures
\\
  Negative sampling has swiftly risen to prominence as a focal point of
research, with wide-ranging applications spanning machine learning, computer
vision, natural language processing, data mining, and recommender systems. This
growing interest raises several critical questions: Does negative sampling
really matter? Is there a general framework that can incorporate all existing
negative sampling methods? In what fields is it applied? Addressing these
questions, we propose a general framework that leverages negative sampling.
Delving into the history of negative sampling, we trace the development of
negative sampling through five evolutionary paths. We dissect and categorize
the strategies used to select negative sample candidates, detailing global,
local, mini-batch, hop, and memory-based approaches. Our review categorizes
current negative sampling methods into five types: static, hard, GAN-based,
Auxiliary-based, and In-batch methods, providing a clear structure for
understanding negative sampling. Beyond detailed categorization, we highlight
the application of negative sampling in various areas, offering insights into
its practical benefits. Finally, we briefly discuss open problems and future
directions for negative sampling.
\\ ( https://arxiv.org/abs/2402.17238 ,  18224kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17257
Date: Tue, 27 Feb 2024 07:03:25 GMT   (25025kb,D)

Title: RIME: Robust Preference-based Reinforcement Learning with Noisy
  Preferences
Authors: Jie Cheng, Gang Xiong, Xingyuan Dai, Qinghai Miao, Yisheng Lv, Fei-Yue
  Wang
Categories: cs.LG cs.AI
\\
  Preference-based Reinforcement Learning (PbRL) avoids the need for reward
engineering by harnessing human preferences as the reward signal. However,
current PbRL algorithms over-reliance on high-quality feedback from domain
experts, which results in a lack of robustness. In this paper, we present RIME,
a robust PbRL algorithm for effective reward learning from noisy preferences.
Our method incorporates a sample selection-based discriminator to dynamically
filter denoised preferences for robust training. To mitigate the accumulated
error caused by incorrect selection, we propose to warm start the reward model,
which additionally bridges the performance gap during transition from
pre-training to online training in PbRL. Our experiments on robotic
manipulation and locomotion tasks demonstrate that RIME significantly enhances
the robustness of the current state-of-the-art PbRL method. Ablation studies
further demonstrate that the warm start is crucial for both robustness and
feedback-efficiency in limited-feedback cases.
\\ ( https://arxiv.org/abs/2402.17257 ,  25025kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17269
Date: Tue, 27 Feb 2024 07:28:05 GMT   (128kb,D)

Title: Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion
  Recognition
Authors: Cam-Van Thi Nguyen, Cao-Bach Nguyen, Quang-Thuy Ha, Duc-Trong Le
Categories: cs.LG
Comments: Accepted at LREC-COLING 2024
\\
  Emotion recognition in conversation (ERC) is a crucial task in natural
language processing and affective computing. This paper proposes MultiDAG+CL, a
novel approach for Multimodal Emotion Recognition in Conversation (ERC) that
employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual
features within a unified framework. The model is enhanced by Curriculum
Learning (CL) to address challenges related to emotional shifts and data
imbalance. Curriculum learning facilitates the learning process by gradually
presenting training samples in a meaningful order, thereby improving the
model's performance in handling emotional variations and data imbalance.
Experimental results on the IEMOCAP and MELD datasets demonstrate that the
MultiDAG+CL models outperform baseline models.
\\ ( https://arxiv.org/abs/2402.17269 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17287
Date: Tue, 27 Feb 2024 08:00:52 GMT   (7521kb,D)

Title: An Interpretable Evaluation of Entropy-based Novelty of Generative
  Models
Authors: Jingwei Zhang, Cheuk Ting Li, Farzan Farnia
Categories: cs.LG cs.CV stat.ML
\\
  The massive developments of generative model frameworks and architectures
require principled methods for the evaluation of a model's novelty compared to
a reference dataset or baseline generative models. While the recent literature
has extensively studied the evaluation of the quality, diversity, and
generalizability of generative models, the assessment of a model's novelty
compared to a baseline model has not been adequately studied in the machine
learning community. In this work, we focus on the novelty assessment under
multi-modal generative models and attempt to answer the following question:
Given the samples of a generative model $\mathcal{G}$ and a reference dataset
$\mathcal{S}$, how can we discover and count the modes expressed by
$\mathcal{G}$ more frequently than in $\mathcal{S}$. We introduce a spectral
approach to the described task and propose the Kernel-based Entropic Novelty
(KEN) score to quantify the mode-based novelty of distribution $P_\mathcal{G}$
with respect to distribution $P_\mathcal{S}$. We analytically interpret the
behavior of the KEN score under mixture distributions with sub-Gaussian
components. Next, we develop a method based on Cholesky decomposition to
compute the KEN score from observed samples. We support the KEN-based
quantification of novelty by presenting several numerical results on synthetic
and real image distributions. Our numerical results indicate the success of the
proposed approach in detecting the novel modes and the comparison of
state-of-the-art generative models.
\\ ( https://arxiv.org/abs/2402.17287 ,  7521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17327
Date: Tue, 27 Feb 2024 09:03:43 GMT   (198kb,D)

Title: Data-Efficient Learning via Clustering-Based Sensitivity Sampling:
  Foundation Models and Beyond
Authors: Kyriakos Axiotis, Vincent Cohen-Addad, Monika Henzinger, Sammy Jerome,
  Vahab Mirrokni, David Saulpic, David Woodruff, Michael Wunder
Categories: cs.LG cs.DS
\\
  We study the data selection problem, whose aim is to select a small
representative subset of data that can be used to efficiently train a machine
learning model. We present a new data selection approach based on $k$-means
clustering and sensitivity sampling. Assuming access to an embedding
representation of the data with respect to which the model loss is H\"older
continuous, our approach provably allows selecting a set of ``typical'' $k +
1/\varepsilon^2$ elements whose average loss corresponds to the average loss of
the whole dataset, up to a multiplicative $(1\pm\varepsilon)$ factor and an
additive $\varepsilon \lambda \Phi_k$, where $\Phi_k$ represents the $k$-means
cost for the input embeddings and $\lambda$ is the H\"older constant.
  We furthermore demonstrate the performance and scalability of our approach on
fine-tuning foundation models and show that it outperforms state-of-the-art
methods. We also show how it can be applied on linear regression, leading to a
new sampling strategy that surprisingly matches the performances of leverage
score sampling, while being conceptually simpler and more scalable.
\\ ( https://arxiv.org/abs/2402.17327 ,  198kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17343
Date: Tue, 27 Feb 2024 09:23:13 GMT   (1080kb,D)

Title: Enhanced Bayesian Optimization via Preferential Modeling of Abstract
  Properties
Authors: Arun Kumar A V, Alistair Shilton, Sunil Gupta, Santu Rana, Stewart
  Greenhill, Svetha Venkatesh
Categories: cs.LG stat.ML
Comments: 19 Pages, 6 Figures
\\
  Experimental (design) optimization is a key driver in designing and
discovering new products and processes. Bayesian Optimization (BO) is an
effective tool for optimizing expensive and black-box experimental design
processes. While Bayesian optimization is a principled data-driven approach to
experimental optimization, it learns everything from scratch and could greatly
benefit from the expertise of its human (domain) experts who often reason about
systems at different abstraction levels using physical properties that are not
necessarily directly measured (or measurable). In this paper, we propose a
human-AI collaborative Bayesian framework to incorporate expert preferences
about unmeasured abstract properties into the surrogate modeling to further
boost the performance of BO. We provide an efficient strategy that can also
handle any incorrect/misleading expert bias in preferential judgments. We
discuss the convergence behavior of our proposed framework. Our experimental
results involving synthetic functions and real-world datasets show the
superiority of our method against the baselines.
\\ ( https://arxiv.org/abs/2402.17343 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17345
Date: Tue, 27 Feb 2024 09:23:54 GMT   (1073kb,D)

Title: LocalGCL: Local-aware Contrastive Learning for Graphs
Authors: Haojun Jiang, Jiawei Sun, Jie Li, Chentao Wu
Categories: cs.LG cs.AI
\\
  Graph representation learning (GRL) makes considerable progress recently,
which encodes graphs with topological structures into low-dimensional
embeddings. Meanwhile, the time-consuming and costly process of annotating
graph labels manually prompts the growth of self-supervised learning (SSL)
techniques. As a dominant approach of SSL, Contrastive learning (CL) learns
discriminative representations by differentiating between positive and negative
samples. However, when applied to graph data, it overemphasizes global patterns
while neglecting local structures. To tackle the above issue, we propose
\underline{Local}-aware \underline{G}raph \underline{C}ontrastive
\underline{L}earning (\textbf{\methnametrim}), a self-supervised learning
framework that supplementarily captures local graph information with
masking-based modeling compared with vanilla contrastive learning. Extensive
experiments validate the superiority of \methname against state-of-the-art
methods, demonstrating its promise as a comprehensive graph representation
learner.
\\ ( https://arxiv.org/abs/2402.17345 ,  1073kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17390
Date: Tue, 27 Feb 2024 10:37:13 GMT   (7062kb,D)

Title: Robustness-Congruent Adversarial Training for Secure Machine Learning
  Model Updates
Authors: Daniele Angioni, Luca Demetrio, Maura Pintor, Luca Oneto, Davide
  Anguita, Battista Biggio, Fabio Roli
Categories: cs.LG cs.CR
\\
  Machine-learning models demand for periodic updates to improve their average
accuracy, exploiting novel architectures and additional data. However, a
newly-updated model may commit mistakes that the previous model did not make.
Such misclassifications are referred to as negative flips, and experienced by
users as a regression of performance. In this work, we show that this problem
also affects robustness to adversarial examples, thereby hindering the
development of secure model update practices. In particular, when updating a
model to improve its adversarial robustness, some previously-ineffective
adversarial examples may become misclassified, causing a regression in the
perceived security of the system. We propose a novel technique, named
robustness-congruent adversarial training, to address this issue. It amounts to
fine-tuning a model with adversarial training, while constraining it to retain
higher robustness on the adversarial examples that were correctly classified
before the update. We show that our algorithm and, more generally, learning
with non-regression constraints, provides a theoretically-grounded framework to
train consistent estimators. Our experiments on robust models for computer
vision confirm that (i) both accuracy and robustness, even if improved after
model update, can be affected by negative flips, and (ii) our
robustness-congruent adversarial training can mitigate the problem,
outperforming competing baseline methods.
\\ ( https://arxiv.org/abs/2402.17390 ,  7062kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17423
Date: Tue, 27 Feb 2024 11:32:14 GMT   (453kb,D)

Title: Reinforced In-Context Black-Box Optimization
Authors: Lei Song, Chenxiao Gao, Ke Xue, Chenyang Wu, Dong Li, Jianye Hao,
  Zongzhang Zhang, Chao Qian
Categories: cs.LG cs.AI cs.NE
\\
  Black-Box Optimization (BBO) has found successful applications in many fields
of science and engineering. Recently, there has been a growing interest in
meta-learning particular components of BBO algorithms to speed up optimization
and get rid of tedious hand-crafted heuristics. As an extension, learning the
entire algorithm from data requires the least labor from experts and can
provide the most flexibility. In this paper, we propose RIBBO, a method to
reinforce-learn a BBO algorithm from offline data in an end-to-end fashion.
RIBBO employs expressive sequence models to learn the optimization histories
produced by multiple behavior algorithms and tasks, leveraging the in-context
learning ability of large models to extract task information and make decisions
accordingly. Central to our method is to augment the optimization histories
with regret-to-go tokens, which are designed to represent the performance of an
algorithm based on cumulative regret of the histories. The integration of
regret-to-go tokens enables RIBBO to automatically generate sequences of query
points that satisfy the user-desired regret, which is verified by its
universally good empirical performance on diverse problems, including BBOB
functions, hyper-parameter optimization and robot control problems.
\\ ( https://arxiv.org/abs/2402.17423 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17440
Date: Tue, 27 Feb 2024 11:52:49 GMT   (3068kb,D)

Title: Principled Architecture-aware Scaling of Hyperparameters
Authors: Wuyang Chen, Junru Wu, Zhangyang Wang, Boris Hanin
Categories: cs.LG
\\
  Training a high-quality deep neural network requires choosing suitable
hyperparameters, which is a non-trivial and expensive process. Current works
try to automatically optimize or design principles of hyperparameters, such
that they can generalize to diverse unseen scenarios. However, most designs or
optimization methods are agnostic to the choice of network structures, and thus
largely ignore the impact of neural architectures on hyperparameters. In this
work, we precisely characterize the dependence of initializations and maximal
learning rates on the network architecture, which includes the network depth,
width, convolutional kernel size, and connectivity patterns. By pursuing every
parameter to be maximally updated with the same mean squared change in
pre-activations, we can generalize our initialization and learning rates across
MLPs (multi-layer perception) and CNNs (convolutional neural network) with
sophisticated graph topologies. We verify our principles with comprehensive
experiments. More importantly, our strategy further sheds light on advancing
current benchmarks for architecture design. A fair comparison of AutoML
algorithms requires accurate network rankings. However, we demonstrate that
network rankings can be easily changed by better training networks in
benchmarks with our architecture-aware learning rates and initialization.
\\ ( https://arxiv.org/abs/2402.17440 ,  3068kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17453
Date: Tue, 27 Feb 2024 12:26:07 GMT   (370kb,D)

Title: DS-Agent: Automated Data Science by Empowering Large Language Models
  with Case-Based Reasoning
Authors: Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang
Categories: cs.LG
\\
  In this work, we investigate the potential of large language models (LLMs)
based agents to automate data science tasks, with the goal of comprehending
task requirements, then building and training the best-fit machine learning
models. Despite their widespread success, existing LLM agents are hindered by
generating unreasonable experiment plans within this scenario. To this end, we
present DS-Agent, a novel automatic framework that harnesses LLM agent and
case-based reasoning (CBR). In the development stage, DS-Agent follows the CBR
framework to structure an automatic iteration pipeline, which can flexibly
capitalize on the expert knowledge from Kaggle, and facilitate consistent
performance improvement through the feedback mechanism. Moreover, DS-Agent
implements a low-resource deployment stage with a simplified CBR paradigm to
adapt past successful solutions from the development stage for direct code
generation, significantly reducing the demand on foundational capabilities of
LLMs. Empirically, DS-Agent with GPT-4 achieves an unprecedented 100% success
rate in the development stage, while attaining 36% improvement on average one
pass rate across alternative LLMs in the deployment stage. In both stages,
DS-Agent achieves the best rank in performance, costing \$1.60 and \$0.13 per
run with GPT-4, respectively.
\\ ( https://arxiv.org/abs/2402.17453 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17457
Date: Tue, 27 Feb 2024 12:28:01 GMT   (735kb,D)

Title: Why do Learning Rates Transfer? Reconciling Optimization and Scaling
  Limits for Deep Learning
Authors: Lorenzo Noci, Alexandru Meterez, Thomas Hofmann, Antonio Orvieto
Categories: cs.LG
\\
  Recently, there has been growing evidence that if the width and depth of a
neural network are scaled toward the so-called rich feature learning limit
($\mu$P and its depth extension), then some hyperparameters - such as the
learning rate - exhibit transfer from small to very large models, thus reducing
the cost of hyperparameter tuning. From an optimization perspective, this
phenomenon is puzzling, as it implies that the loss landscape is remarkably
consistent across very different model sizes. In this work, we find empirical
evidence that learning rate transfer can be attributed to the fact that under
$\mu$P and its depth extension, the largest eigenvalue of the training loss
Hessian (i.e. the sharpness) is largely independent of the width and depth of
the network for a sustained period of training time. On the other hand, we show
that under the neural tangent kernel (NTK) regime, the sharpness exhibits very
different dynamics at different scales, thus preventing learning rate transfer.
But what causes these differences in the sharpness dynamics? Through a
connection between the spectra of the Hessian and the NTK matrix, we argue that
the cause lies in the presence (for $\mu$P) or progressive absence (for the NTK
regime) of feature learning, which results in a different evolution of the NTK,
and thus of the sharpness. We corroborate our claims with a substantial suite
of experiments, covering a wide range of datasets and architectures: from
ResNets and Vision Transformers trained on benchmark vision datasets to
Transformers-based language models trained on WikiText
\\ ( https://arxiv.org/abs/2402.17457 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17472
Date: Tue, 27 Feb 2024 12:53:15 GMT   (1330kb,D)

Title: Fraud Detection with Binding Global and Local Relational Interaction
Authors: Haolin Li, Shuyang Jiang, Lifeng Zhang, Siyuan Du, Guangnan Ye,
  Hongfeng Chai
Categories: cs.LG cs.AI
Comments: Under review for SIGKDD 2024
\\
  Graph Neural Network has been proved to be effective for fraud detection for
its capability to encode node interaction and aggregate features in a holistic
view. Recently, Transformer network with great sequence encoding ability, has
also outperformed other GNN-based methods in literatures. However, both
GNN-based and Transformer-based networks only encode one perspective of the
whole graph, while GNN encodes global features and Transformer network encodes
local ones. Furthermore, previous works ignored encoding global interaction
features of the heterogeneous graph with separate networks, thus leading to
suboptimal performance. In this work, we present a novel framework called
Relation-Aware GNN with transFormer (RAGFormer) which simultaneously embeds
local and global features into a target node. The simple yet effective network
applies a modified GAGA module where each transformer layer is followed by a
cross-relation aggregation layer, to encode local embeddings and node
interactions across different relations. Apart from the Transformer-based
network, we further introduce a Relation-Aware GNN module to learn global
embeddings, which is later merged into the local embeddings by an attention
fusion module and a skip connection. Extensive experiments on two popular
public datasets and an industrial dataset demonstrate that RAGFormer achieves
the state-of-the-art performance. Substantial analysis experiments validate the
effectiveness of each submodule of RAGFormer and its high efficiency in
utilizing small-scale data and low hyper-parameter sensitivity.
\\ ( https://arxiv.org/abs/2402.17472 ,  1330kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17501
Date: Tue, 27 Feb 2024 13:36:55 GMT   (1622kb,D)

Title: Intensive Care as One Big Sequence Modeling Problem
Authors: Vadim Liventsev, Tobias Fritz
Categories: cs.LG cs.AI
\\
  Reinforcement Learning in Healthcare is typically concerned with narrow
self-contained tasks such as sepsis prediction or anesthesia control. However,
previous research has demonstrated the potential of generalist models (the
prime example being Large Language Models) to outperform task-specific
approaches due to their capability for implicit transfer learning. To enable
training of foundation models for Healthcare as well as leverage the
capabilities of state of the art Transformer architectures, we propose the
paradigm of Healthcare as Sequence Modeling, in which interaction between the
patient and the healthcare provider is represented as an event stream and tasks
like diagnosis and treatment selection are modeled as prediction of future
events in the stream. To explore this paradigm experimentally we develop
MIMIC-SEQ, a sequence modeling benchmark derived by translating heterogenous
clinical records from MIMIC-IV dataset into a uniform event stream format,
train a baseline model and explore its capabilities.
\\ ( https://arxiv.org/abs/2402.17501 ,  1622kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17516
Date: Tue, 27 Feb 2024 14:00:08 GMT   (181kb,D)

Title: QUCE: The Minimisation and Quantification of Path-Based Uncertainty for
  Generative Counterfactual Explanations
Authors: Jamie Duell, Hsuan Fu, Monika Seisenberger, Xiuyi Fan
Categories: cs.LG cs.AI
\\
  Deep Neural Networks (DNNs) stand out as one of the most prominent approaches
within the Machine Learning (ML) domain. The efficacy of DNNs has surged
alongside recent increases in computational capacity, allowing these approaches
to scale to significant complexities for addressing predictive challenges in
big data. However, as the complexity of DNN models rises, interpretability
diminishes. In response to this challenge, explainable models such as
Adversarial Gradient Integration (AGI) leverage path-based gradients provided
by DNNs to elucidate their decisions. Yet the performance of path-based
explainers can be compromised when gradients exhibit irregularities during
out-of-distribution path traversal. In this context, we introduce Quantified
Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate
out-of-distribution traversal by minimizing path uncertainty. QUCE not only
quantifies uncertainty when presenting explanations but also generates more
certain counterfactual examples. We showcase the performance of the QUCE method
by comparing it with competing methods for both path-based explanations and
generative counterfactual examples. The code repository for the QUCE method is
available at: https://github.com/jamie-duell/QUCE.
\\ ( https://arxiv.org/abs/2402.17516 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17517
Date: Tue, 27 Feb 2024 14:00:34 GMT   (12780kb,D)

Title: Label-Noise Robust Diffusion Models
Authors: Byeonghu Na, Yeongmin Kim, HeeSun Bae, Jung Hyun Lee, Se Jung Kwon,
  Wanmo Kang, Il-Chul Moon
Categories: cs.LG
Comments: Accepted at ICLR 2024
\\
  Conditional diffusion models have shown remarkable performance in various
generative tasks, but training them requires large-scale datasets that often
contain noise in conditional inputs, a.k.a. noisy labels. This noise leads to
condition mismatch and quality degradation of generated data. This paper
proposes Transition-aware weighted Denoising Score Matching (TDSM) for training
conditional diffusion models with noisy labels, which is the first study in the
line of diffusion models. The TDSM objective contains a weighted sum of score
networks, incorporating instance-wise and time-dependent label transition
probabilities. We introduce a transition-aware weight estimator, which
leverages a time-dependent noisy-label classifier distinctively customized to
the diffusion process. Through experiments across various datasets and noisy
label settings, TDSM improves the quality of generated samples aligned with
given conditions. Furthermore, our method improves generation performance even
on prevalent benchmark datasets, which implies the potential noisy labels and
their risk of generative model learning. Finally, we show the improved
performance of TDSM on top of conventional noisy label corrections, which
empirically proving its contribution as a part of label-noise robust generative
models. Our code is available at: https://github.com/byeonghu-na/tdsm.
\\ ( https://arxiv.org/abs/2402.17517 ,  12780kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17554
Date: Tue, 27 Feb 2024 14:48:07 GMT   (804kb)

Title: Evaluation of Predictive Reliability to Foster Trust in Artificial
  Intelligence. A case study in Multiple Sclerosis
Authors: Lorenzo Peracchio, Giovanna Nicora, Enea Parimbelli, Tommaso Mario
  Buonocore, Roberto Bergamaschi, Eleonora Tavazzi, Arianna Dagliati, Riccardo
  Bellazzi
Categories: cs.LG
Comments: 20 pages, 7 figures
\\
  Applying Artificial Intelligence (AI) and Machine Learning (ML) in critical
contexts, such as medicine, requires the implementation of safety measures to
reduce risks of harm in case of prediction errors. Spotting ML failures is of
paramount importance when ML predictions are used to drive clinical decisions.
ML predictive reliability measures the degree of trust of a ML prediction on a
new instance, thus allowing decision-makers to accept or reject it based on its
reliability. To assess reliability, we propose a method that implements two
principles. First, our approach evaluates whether an instance to be classified
is coming from the same distribution of the training set. To do this, we
leverage Autoencoders (AEs) ability to reconstruct the training set with low
error. An instance is considered Out-of-Distribution (OOD) if the AE
reconstructs it with a high error. Second, it is evaluated whether the ML
classifier has good performances on samples similar to the newly classified
instance by using a proxy model. We show that this approach is able to assess
reliability both in a simulated scenario and on a model trained to predict
disease progression of Multiple Sclerosis patients. We also developed a Python
package, named relAI, to embed reliability measures into ML pipelines. We
propose a simple approach that can be used in the deployment phase of any ML
model to suggest whether to trust predictions or not. Our method holds the
promise to provide effective support to clinicians by spotting potential ML
failures during deployment.
\\ ( https://arxiv.org/abs/2402.17554 ,  804kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17570
Date: Tue, 27 Feb 2024 15:08:57 GMT   (1672kb,D)

Title: Sparse Variational Contaminated Noise Gaussian Process Regression for
  Forecasting Geomagnetic Perturbations
Authors: Daniel Iong, Matthew McAnear, Yuezhou Qu, Shasha Zou, Gabor Toth Yang
  Chen
Categories: cs.LG stat.AP stat.ME
\\
  Gaussian Processes (GP) have become popular machine learning methods for
kernel based learning on datasets with complicated covariance structures. In
this paper, we present a novel extension to the GP framework using a
contaminated normal likelihood function to better account for heteroscedastic
variance and outlier noise. We propose a scalable inference algorithm based on
the Sparse Variational Gaussian Process (SVGP) method for fitting sparse
Gaussian process regression models with contaminated normal noise on large
datasets. We examine an application to geomagnetic ground perturbations, where
the state-of-art prediction model is based on neural networks. We show that our
approach yields shorter predictions intervals for similar coverage and accuracy
when compared to an artificial dense neural network baseline.
\\ ( https://arxiv.org/abs/2402.17570 ,  1672kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17572
Date: Tue, 27 Feb 2024 15:09:20 GMT   (840kb,D)

Title: Hyperdimensional computing: a fast, robust and interpretable paradigm
  for biological data
Authors: Michiel Stock, Dimitri Boeckaerts, Pieter Dewulf, Steff Taelman,
  Maxime Van Haeverbeke, Wim Van Criekinge, Bernard De Baets
Categories: cs.LG q-bio.QM
\\
  Advances in bioinformatics are primarily due to new algorithms for processing
diverse biological data sources. While sophisticated alignment algorithms have
been pivotal in analyzing biological sequences, deep learning has substantially
transformed bioinformatics, addressing sequence, structure, and functional
analyses. However, these methods are incredibly data-hungry, compute-intensive
and hard to interpret. Hyperdimensional computing (HDC) has recently emerged as
an intriguing alternative. The key idea is that random vectors of high
dimensionality can represent concepts such as sequence identity or phylogeny.
These vectors can then be combined using simple operators for learning,
reasoning or querying by exploiting the peculiar properties of high-dimensional
spaces. Our work reviews and explores the potential of HDC for bioinformatics,
emphasizing its efficiency, interpretability, and adeptness in handling
multimodal and structured data. HDC holds a lot of potential for various omics
data searching, biosignal analysis and health applications.
\\ ( https://arxiv.org/abs/2402.17572 ,  840kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17595
Date: Tue, 27 Feb 2024 15:28:01 GMT   (3491kb,D)

Title: Implicit Regularization via Spectral Neural Networks and Non-linear
  Matrix Sensing
Authors: Hong T.M. Chu, Subhro Ghosh, Chi Thanh Lam, Soumendu Sundar Mukherjee
Categories: cs.LG cs.AI cs.NE stat.ML
\\
  The phenomenon of implicit regularization has attracted interest in recent
years as a fundamental aspect of the remarkable generalizing ability of neural
networks. In a nutshell, it entails that gradient descent dynamics in many
neural nets, even without any explicit regularizer in the loss function,
converges to the solution of a regularized learning problem. However, known
results attempting to theoretically explain this phenomenon focus
overwhelmingly on the setting of linear neural nets, and the simplicity of the
linear structure is particularly crucial to existing arguments. In this paper,
we explore this problem in the context of more realistic neural networks with a
general class of non-linear activation functions, and rigorously demonstrate
the implicit regularization phenomenon for such networks in the setting of
matrix sensing problems, together with rigorous rate guarantees that ensure
exponentially fast convergence of gradient descent.In this vein, we contribute
a network architecture called Spectral Neural Networks (abbrv. SNN) that is
particularly suitable for matrix learning problems. Conceptually, this entails
coordinatizing the space of matrices by their singular values and singular
vectors, as opposed to by their entries, a potentially fruitful perspective for
matrix learning. We demonstrate that the SNN architecture is inherently much
more amenable to theoretical analysis than vanilla neural nets and confirm its
effectiveness in the context of matrix sensing, via both mathematical
guarantees and empirical investigations. We believe that the SNN architecture
has the potential to be of wide applicability in a broad class of matrix
learning scenarios.
\\ ( https://arxiv.org/abs/2402.17595 ,  3491kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17599
Date: Mon, 26 Feb 2024 11:29:16 GMT   (127kb,D)

Title: DAGnosis: Localized Identification of Data Inconsistencies using
  Structures
Authors: Nicolas Huynh, Jeroen Berrevoets, Nabeel Seedat, Jonathan Crabb\'e,
  Zhaozhi Qian, Mihaela van der Schaar
Categories: cs.LG cs.AI stat.ML
Comments: AISTATS 2024
\\
  Identification and appropriate handling of inconsistencies in data at
deployment time is crucial to reliably use machine learning models. While
recent data-centric methods are able to identify such inconsistencies with
respect to the training set, they suffer from two key limitations: (1)
suboptimality in settings where features exhibit statistical independencies,
due to their usage of compressive representations and (2) lack of localization
to pin-point why a sample might be flagged as inconsistent, which is important
to guide future data collection. We solve these two fundamental limitations
using directed acyclic graphs (DAGs) to encode the training set's features
probability distribution and independencies as a structure. Our method, called
DAGnosis, leverages these structural interactions to bring valuable and
insightful data-centric conclusions. DAGnosis unlocks the localization of the
causes of inconsistencies on a DAG, an aspect overlooked by previous
approaches. Moreover, we show empirically that leveraging these interactions
(1) leads to more accurate conclusions in detecting inconsistencies, as well as
(2) provides more detailed insights into why some samples are flagged.
\\ ( https://arxiv.org/abs/2402.17599 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17601
Date: Tue, 27 Feb 2024 15:30:01 GMT   (1263kb,D)

Title: Advancing sleep detection by modelling weak label sets: A novel weakly
  supervised learning approach
Authors: Matthias Boeker, Vajira Thambawita, Michael Riegler, P{\aa}l
  Halvorsen, Hugo L. Hammer
Categories: cs.LG
\\
  Understanding sleep and activity patterns plays a crucial role in physical
and mental health. This study introduces a novel approach for sleep detection
using weakly supervised learning for scenarios where reliable ground truth
labels are unavailable. The proposed method relies on a set of weak labels,
derived from the predictions generated by conventional sleep detection
algorithms. Introducing a novel approach, we suggest a novel generalised
non-linear statistical model in which the number of weak sleep labels is
modelled as outcome of a binomial distribution. The probability of sleep in the
binomial distribution is linked to the outcomes of neural networks trained to
detect sleep based on actigraphy. We show that maximizing the likelihood
function of the model, is equivalent to minimizing the soft cross-entropy loss.
Additionally, we explored the use of the Brier score as a loss function for
weak labels. The efficacy of the suggested modelling framework was demonstrated
using the Multi-Ethnic Study of Atherosclerosis dataset. A \gls{lstm} trained
on the soft cross-entropy outperformed conventional sleep detection algorithms,
other neural network architectures and loss functions in accuracy and model
calibration. This research not only advances sleep detection techniques in
scenarios where ground truth data is scarce but also contributes to the broader
field of weakly supervised learning by introducing innovative approach in
modelling sets of weak labels.
\\ ( https://arxiv.org/abs/2402.17601 ,  1263kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17606
Date: Tue, 27 Feb 2024 15:33:20 GMT   (331kb,D)

Title: Learning Topological Representations with Bidirectional Graph Attention
  Network for Solving Job Shop Scheduling Problem
Authors: Cong Zhang, Zhiguang Cao, Yaoxin Wu, Wen Song, Jing Sun
Categories: cs.LG cs.AI
\\
  Existing learning-based methods for solving job shop scheduling problem
(JSSP) usually use off-the-shelf GNN models tailored to undirected graphs and
neglect the rich and meaningful topological structures of disjunctive graphs
(DGs). This paper proposes the topology-aware bidirectional graph attention
network (TBGAT), a novel GNN architecture based on the attention mechanism, to
embed the DG for solving JSSP in a local search framework. Specifically, TBGAT
embeds the DG from a forward and a backward view, respectively, where the
messages are propagated by following the different topologies of the views and
aggregated via graph attention. Then, we propose a novel operator based on the
message-passing mechanism to calculate the forward and backward topological
sorts of the DG, which are the features for characterizing the topological
structures and exploited by our model. In addition, we theoretically and
experimentally show that TBGAT has linear computational complexity to the
number of jobs and machines, respectively, which strengthens the practical
value of our method. Besides, extensive experiments on five synthetic datasets
and seven classic benchmarks show that TBGAT achieves new SOTA results by
outperforming a wide range of neural methods by a large margin.
\\ ( https://arxiv.org/abs/2402.17606 ,  331kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17641
Date: Tue, 27 Feb 2024 16:11:05 GMT   (2142kb,D)

Title: Variational Learning is Effective for Large Deep Networks
Authors: Yuesong Shen, Nico Daheim, Bai Cong, Peter Nickl, Gian Maria Marconi,
  Clement Bazan, Rio Yokota, Iryna Gurevych, Daniel Cremers, Mohammad Emtiyaz
  Khan, Thomas M\"ollenhoff
Categories: cs.LG cs.AI cs.CL math.OC stat.ML
Comments: The first two authors contributed equally. Code is available here:
  https://github.com/team-approx-bayes/ivon
\\
  We give extensive empirical evidence against the common belief that
variational learning is ineffective for large neural networks. We show that an
optimizer called Improved Variational Online Newton (IVON) consistently matches
or outperforms Adam for training large networks such as GPT-2 and ResNets from
scratch. IVON's computational costs are nearly identical to Adam but its
predictive uncertainty is better. We show several new use cases of IVON where
we improve fine-tuning and model merging in Large Language Models, accurately
predict generalization error, and faithfully estimate sensitivity to data. We
find overwhelming evidence in support of effectiveness of variational learning.
\\ ( https://arxiv.org/abs/2402.17641 ,  2142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17655
Date: Tue, 27 Feb 2024 16:24:28 GMT   (2184kb,D)

Title: Confidence-Aware Multi-Field Model Calibration
Authors: Yuang Zhao, Chuhan Wu, Qinglin Jia, Hong Zhu, Jia Yan, Libin Zong,
  Linxuan Zhang, Zhenhua Dong, Muyu Zhang
Categories: cs.LG
\\
  Accurately predicting the probabilities of user feedback, such as clicks and
conversions, is critical for ad ranking and bidding. However, there often exist
unwanted mismatches between predicted probabilities and true likelihoods due to
the shift of data distributions and intrinsic model biases. Calibration aims to
address this issue by post-processing model predictions, and field-aware
calibration can adjust model output on different feature field values to
satisfy fine-grained advertising demands. Unfortunately, the observed samples
corresponding to certain field values can be too limited to make confident
calibrations, which may yield bias amplification and online disturbance. In
this paper, we propose a confidence-aware multi-field calibration method, which
adaptively adjusts the calibration intensity based on the confidence levels
derived from sample statistics. It also utilizes multiple feature fields for
joint model calibration with awareness of their importance to mitigate the data
sparsity effect of a single field. Extensive offline and online experiments
show the superiority of our method in boosting advertising performance and
reducing prediction deviations.
\\ ( https://arxiv.org/abs/2402.17655 ,  2184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17660
Date: Tue, 27 Feb 2024 16:27:06 GMT   (722kb,D)

Title: TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular
  Simulations
Authors: Raul P. Pelaez, Guillem Simeon, Raimondas Galvelis, Antonio Mirarchi,
  Peter Eastman, Stefan Doerr, Philipp Th\"olke, Thomas E. Markland, Gianni De
  Fabritiis
Categories: cs.LG physics.bio-ph physics.chem-ph physics.comp-ph
\\
  Achieving a balance between computational speed, prediction accuracy, and
universal applicability in molecular simulations has been a persistent
challenge. This paper presents substantial advancements in the TorchMD-Net
software, a pivotal step forward in the shift from conventional force fields to
neural network-based potentials. The evolution of TorchMD-Net into a more
comprehensive and versatile framework is highlighted, incorporating
cutting-edge architectures such as TensorNet. This transformation is achieved
through a modular design approach, encouraging customized applications within
the scientific community. The most notable enhancement is a significant
improvement in computational efficiency, achieving a very remarkable
acceleration in the computation of energy and forces for TensorNet models, with
performance gains ranging from 2-fold to 10-fold over previous iterations.
Other enhancements include highly optimized neighbor search algorithms that
support periodic boundary conditions and the smooth integration with existing
molecular dynamics frameworks. Additionally, the updated version introduces the
capability to integrate physical priors, further enriching its application
spectrum and utility in research. The software is available at
https://github.com/torchmd/torchmd-net.
\\ ( https://arxiv.org/abs/2402.17660 ,  722kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17666
Date: Tue, 27 Feb 2024 16:36:53 GMT   (5522kb,D)

Title: Multi-Agent Deep Reinforcement Learning for Distributed Satellite
  Routing
Authors: Federico Lozano-Cuadra, Beatriz Soret
Categories: cs.LG
\\
  This paper introduces a Multi-Agent Deep Reinforcement Learning (MA-DRL)
approach for routing in Low Earth Orbit Satellite Constellations (LSatCs). Each
satellite is an independent decision-making agent with a partial knowledge of
the environment, and supported by feedback received from the nearby agents.
Building on our previous work that introduced a Q-routing solution, the
contribution of this paper is to extend it to a deep learning framework able to
quickly adapt to the network and traffic changes, and based on two phases: (1)
An offline exploration learning phase that relies on a global Deep Neural
Network (DNN) to learn the optimal paths at each possible position and
congestion level; (2) An online exploitation phase with local, on-board,
pre-trained DNNs. Results show that MA-DRL efficiently learns optimal routes
offline that are then loaded for an efficient distributed routing online.
\\ ( https://arxiv.org/abs/2402.17666 ,  5522kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17671
Date: Tue, 27 Feb 2024 16:44:09 GMT   (10047kb,D)

Title: Securing Reliability: A Brief Overview on Enhancing In-Context Learning
  for Foundation Models
Authors: Yunpeng Huang, Yaonan Gu, Jingwei Xu, Zhihong Zhu, Zhaorun Chen,
  Xiaoxing Ma
Categories: cs.LG
Comments: 18 pages, 15 figures
ACM-class: I.2.6
\\
  As foundation models (FMs) continue to shape the landscape of AI, the
in-context learning (ICL) paradigm thrives but also encounters issues such as
toxicity, hallucination, disparity, adversarial vulnerability, and
inconsistency. Ensuring the reliability and responsibility of FMs is crucial
for the sustainable development of the AI ecosystem. In this concise overview,
we investigate recent advancements in enhancing the reliability and
trustworthiness of FMs within ICL frameworks, focusing on four key
methodologies, each with its corresponding subgoals. We sincerely hope this
paper can provide valuable insights for researchers and practitioners
endeavoring to build safe and dependable FMs and foster a stable and consistent
ICL environment, thereby unlocking their vast potential.
\\ ( https://arxiv.org/abs/2402.17671 ,  10047kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17689
Date: Tue, 27 Feb 2024 17:05:41 GMT   (7305kb,D)

Title: QoS prediction in radio vehicular environments via prior user
  information
Authors: Noor Ul Ain, Rodrigo Hernang\'omez, Alexandros Palaios, Martin
  Kasparick and S{\l}awomir Sta\'nczak
Categories: cs.LG cs.AI
\\
  Reliable wireless communications play an important role in the automotive
industry as it helps to enhance current use cases and enable new ones such as
connected autonomous driving, platooning, cooperative maneuvering, teleoperated
driving, and smart navigation. These and other use cases often rely on specific
quality of service (QoS) levels for communication. Recently, the area of
predictive quality of service (QoS) has received a great deal of attention as a
key enabler to forecast communication quality well enough in advance. However,
predicting QoS in a reliable manner is a notoriously difficult task. In this
paper, we evaluate ML tree-ensemble methods to predict QoS in the range of
minutes with data collected from a cellular test network. We discuss radio
environment characteristics and we showcase how these can be used to improve ML
performance and further support the uptake of ML in commercial networks.
Specifically, we use the correlations of the measurements coming from the radio
environment by including information of prior vehicles to enhance the
prediction of the target vehicles. Moreover, we are extending prior art by
showing how longer prediction horizons can be supported.
\\ ( https://arxiv.org/abs/2402.17689 ,  7305kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17690
Date: Tue, 27 Feb 2024 17:07:18 GMT   (6497kb,D)

Title: Autonomous Vehicles: Evolution of Artificial Intelligence and Learning
  Algorithms
Authors: Sneha Sudhir Shetiya and Divya Garikapati
Categories: cs.LG cs.AI
Comments: 13 pages
\\
  The advent of autonomous vehicles has heralded a transformative era in
transportation, reshaping the landscape of mobility through cutting-edge
technologies. Central to this evolu- tion is the integration of Artificial
Intelligence (AI) and learning algorithms, propelling vehicles into realms of
unprecedented autonomy. This paper provides a comprehensive exploration of the
evolutionary trajectory of AI within autonomous vehicles, tracing the journey
from foundational principles to the most recent advancements. Commencing with a
current landscape overview, the paper delves into the fundamental role of AI in
shaping the autonomous decision-making capabilities of vehicles. It elucidates
the steps involved in the AI-powered development life cycle in vehicles,
addressing ethical considerations and bias in AI-driven software development
for autonomous vehicles. The study presents statis- tical insights into the
usage and types of AI/learning algorithms over the years, showcasing the
evolving research landscape within the automotive industry. Furthermore, the
paper highlights the pivotal role of parameters in refining algorithms for both
trucks and cars, facilitating vehicles to adapt, learn, and improve performance
over time. It concludes by outlining different levels of autonomy, elucidating
the nuanced usage of AI and learning algorithms, and automating key tasks at
each level. Additionally, the document discusses the variation in software
package sizes across different autonomy levels
\\ ( https://arxiv.org/abs/2402.17690 ,  6497kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17699
Date: Tue, 27 Feb 2024 17:23:40 GMT   (1849kb,D)

Title: Gradient-based Discrete Sampling with Automatic Cyclical Scheduling
Authors: Patrick Pynadath, Riddhiman Bhattacharya, Arun Hariharan, Ruqi Zhang
Categories: cs.LG stat.ML
\\
  Discrete distributions, particularly in high-dimensional deep models, are
often highly multimodal due to inherent discontinuities. While gradient-based
discrete sampling has proven effective, it is susceptible to becoming trapped
in local modes due to the gradient information. To tackle this challenge, we
propose an automatic cyclical scheduling, designed for efficient and accurate
sampling in multimodal discrete distributions. Our method contains three key
components: (1) a cyclical step size schedule where large steps discover new
modes and small steps exploit each mode; (2) a cyclical balancing schedule,
ensuring ``balanced" proposals for given step sizes and high efficiency of the
Markov chain; and (3) an automatic tuning scheme for adjusting the
hyperparameters in the cyclical schedules, allowing adaptability across diverse
datasets with minimal tuning. We prove the non-asymptotic convergence and
inference guarantee for our method in general discrete distributions. Extensive
experiments demonstrate the superiority of our method in sampling complex
multimodal discrete distributions.
\\ ( https://arxiv.org/abs/2402.17699 ,  1849kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17705
Date: Tue, 27 Feb 2024 17:33:23 GMT   (485kb,D)

Title: Federated Learning for Estimating Heterogeneous Treatment Effects
Authors: Disha Makhija, Joydeep Ghosh, Yejin Kim
Categories: cs.LG
\\
  Machine learning methods for estimating heterogeneous treatment effects (HTE)
facilitate large-scale personalized decision-making across various domains such
as healthcare, policy making, education, and more. Current machine learning
approaches for HTE require access to substantial amounts of data per treatment,
and the high costs associated with interventions makes centrally collecting so
much data for each intervention a formidable challenge. To overcome this
obstacle, in this work, we propose a novel framework for collaborative learning
of HTE estimators across institutions via Federated Learning. We show that even
under a diversity of interventions and subject populations across clients, one
can jointly learn a common feature representation, while concurrently and
privately learning the specific predictive functions for outcomes under
distinct interventions across institutions. Our framework and the associated
algorithm are based on this insight, and leverage tabular transformers to map
multiple input data to feature representations which are then used for outcome
prediction via multi-task learning. We also propose a novel way of federated
training of personalised transformers that can work with heterogeneous input
feature spaces. Experimental results on real-world clinical trial data
demonstrate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2402.17705 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17710
Date: Tue, 27 Feb 2024 17:43:51 GMT   (3148kb,D)

Title: Understanding Neural Network Binarization with Forward and Backward
  Proximal Quantizers
Authors: Yiwei Lu, Yaoliang Yu, Xinlin Li, Vahid Partovi Nia
Categories: cs.LG
Comments: Accepted to NeurIPS 2023
\\
  In neural network binarization, BinaryConnect (BC) and its variants are
considered the standard. These methods apply the sign function in their forward
pass and their respective gradients are backpropagated to update the weights.
However, the derivative of the sign function is zero whenever defined, which
consequently freezes training. Therefore, implementations of BC (e.g., BNN)
usually replace the derivative of sign in the backward computation with
identity or other approximate gradient alternatives. Although such practice
works well empirically, it is largely a heuristic or ''training trick.'' We aim
at shedding some light on these training tricks from the optimization
perspective. Building from existing theory on ProxConnect (PC, a generalization
of BC), we (1) equip PC with different forward-backward quantizers and obtain
ProxConnect++ (PC++) that includes existing binarization techniques as special
cases; (2) derive a principled way to synthesize forward-backward quantizers
with automatic theoretical guarantees; (3) illustrate our theory by proposing
an enhanced binarization algorithm BNN++; (4) conduct image classification
experiments on CNNs and vision transformers, and empirically verify that BNN++
generally achieves competitive results on binarizing these models.
\\ ( https://arxiv.org/abs/2402.17710 ,  3148kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17718
Date: Tue, 27 Feb 2024 17:53:13 GMT   (1950kb)

Title: Towards a Digital Twin Framework in Additive Manufacturing: Machine
  Learning and Bayesian Optimization for Time Series Process Optimization
Authors: Vispi Karkaria, Anthony Goeckner, Rujing Zha, Jie Chen, Jianjing
  Zhang, Qi Zhu, Jian Cao, Robert X. Gao, Wei Chen
Categories: cs.LG eess.SP
Comments: 12 Pages, 10 Figures, 1 Table, NAMRC Conference
\\
  Laser-directed-energy deposition (DED) offers advantages in additive
manufacturing (AM) for creating intricate geometries and material grading. Yet,
challenges like material inconsistency and part variability remain, mainly due
to its layer-wise fabrication. A key issue is heat accumulation during DED,
which affects the material microstructure and properties. While closed-loop
control methods for heat management are common in DED research, few integrate
real-time monitoring, physics-based modeling, and control in a unified
framework. Our work presents a digital twin (DT) framework for real-time
predictive control of DED process parameters to meet specific design
objectives. We develop a surrogate model using Long Short-Term Memory
(LSTM)-based machine learning with Bayesian Inference to predict temperatures
in DED parts. This model predicts future temperature states in real time. We
also introduce Bayesian Optimization (BO) for Time Series Process Optimization
(BOTSPO), based on traditional BO but featuring a unique time series process
profile generator with reduced dimensions. BOTSPO dynamically optimizes
processes, identifying optimal laser power profiles to attain desired
mechanical properties. The established process trajectory guides online
optimizations, aiming to enhance performance. This paper outlines the digital
twin framework's components, promoting its integration into a comprehensive
system for AM.
\\ ( https://arxiv.org/abs/2402.17718 ,  1950kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17720
Date: Tue, 27 Feb 2024 17:55:33 GMT   (631kb,D)

Title: The SMART approach to instance-optimal online learning
Authors: Siddhartha Banerjee and Alankrita Bhatt and Christina Lee Yu
Categories: cs.LG cs.DS cs.IT math.IT
\\
  We devise an online learning algorithm -- titled Switching via Monotone
Adapted Regret Traces (SMART) -- that adapts to the data and achieves regret
that is instance optimal, i.e., simultaneously competitive on every input
sequence compared to the performance of the follow-the-leader (FTL) policy and
the worst case guarantee of any other input policy. We show that the regret of
the SMART policy on any input sequence is within a multiplicative factor
$e/(e-1) \approx 1.58$ of the smaller of: 1) the regret obtained by FTL on the
sequence, and 2) the upper bound on regret guaranteed by the given worst-case
policy. This implies a strictly stronger guarantee than typical
`best-of-both-worlds' bounds as the guarantee holds for every input sequence
regardless of how it is generated. SMART is simple to implement as it begins by
playing FTL and switches at most once during the time horizon to the worst-case
algorithm. Our approach and results follow from an operational reduction of
instance optimal online learning to competitive analysis for the ski-rental
problem. We complement our competitive ratio upper bounds with a fundamental
lower bound showing that over all input sequences, no algorithm can get better
than a $1.43$-fraction of the minimum regret achieved by FTL and the
minimax-optimal policy. We also present a modification of SMART that combines
FTL with a ``small-loss" algorithm to achieve instance optimality between the
regret of FTL and the small loss regret bound.
\\ ( https://arxiv.org/abs/2402.17720 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17730
Date: Tue, 27 Feb 2024 18:04:59 GMT   (1633kb,D)

Title: Markovletics: Methods and A Novel Application for Learning
  Continuous-Time Markov Chain Mixtures
Authors: Fabian Spaeh, Charalampos E. Tsourakakis
Categories: cs.LG
\\
  Sequential data naturally arises from user engagement on digital platforms
like social media, music streaming services, and web navigation, encapsulating
evolving user preferences and behaviors through continuous information streams.
A notable unresolved query in stochastic processes is learning mixtures of
continuous-time Markov chains (CTMCs). While there is progress in learning
mixtures of discrete-time Markov chains with recovery guarantees
[GKV16,ST23,KTT2023], the continuous scenario uncovers unique unexplored
challenges. The intrigue in CTMC mixtures stems from their potential to model
intricate continuous-time stochastic processes prevalent in various fields
including social media, finance, and biology.
  In this study, we introduce a novel framework for exploring CTMCs,
emphasizing the influence of observed trails' length and mixture parameters on
problem regimes, which demands specific algorithms. Through thorough
experimentation, we examine the impact of discretizing continuous-time trails
on the learnability of the continuous-time mixture, given that these processes
are often observed via discrete, resource-demanding observations. Our
comparative analysis with leading methods explores sample complexity and the
trade-off between the number of trails and their lengths, offering crucial
insights for method selection in different problem instances. We apply our
algorithms on an extensive collection of Lastfm's user-generated trails
spanning three years, demonstrating the capability of our algorithms to
differentiate diverse user preferences. We pioneer the use of CTMC mixtures on
a basketball passing dataset to unveil intricate offensive tactics of NBA
teams. This underscores the pragmatic utility and versatility of our proposed
framework. All results presented in this study are replicable, and we provide
the implementations to facilitate reproducibility.
\\ ( https://arxiv.org/abs/2402.17730 ,  1633kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17747
Date: Tue, 27 Feb 2024 18:32:11 GMT   (855kb,D)

Title: When Your AI Deceives You: Challenges with Partial Observability of
  Human Evaluators in Reward Learning
Authors: Leon Lang, Davis Foote, Stuart Russell, Anca Dragan, Erik Jenner,
  Scott Emmons
Categories: cs.LG cs.AI stat.ML
\\
  Past analyses of reinforcement learning from human feedback (RLHF) assume
that the human fully observes the environment. What happens when human feedback
is based only on partial observations? We formally define two failure cases:
deception and overjustification. Modeling the human as Boltzmann-rational
w.r.t. a belief over trajectories, we prove conditions under which RLHF is
guaranteed to result in policies that deceptively inflate their performance,
overjustify their behavior to make an impression, or both. To help address
these issues, we mathematically characterize how partial observability of the
environment translates into (lack of) ambiguity in the learned return function.
In some cases, accounting for partial observability makes it theoretically
possible to recover the return function and thus the optimal policy, while in
other cases, there is irreducible ambiguity. We caution against blindly
applying RLHF in partially observable settings and propose research directions
to help tackle these challenges.
\\ ( https://arxiv.org/abs/2402.17747 ,  855kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17756
Date: Tue, 27 Feb 2024 18:48:07 GMT   (290kb,D)

Title: Robustly Learning Single-Index Models via Alignment Sharpness
Authors: Nikos Zarifis, Puqian Wang, Ilias Diakonikolas, Jelena Diakonikolas
Categories: cs.LG cs.DS math.OC math.ST stat.ML stat.TH
\\
  We study the problem of learning Single-Index Models under the $L_2^2$ loss
in the agnostic model. We give an efficient learning algorithm, achieving a
constant factor approximation to the optimal loss, that succeeds under a range
of distributions (including log-concave distributions) and a broad class of
monotone and Lipschitz link functions. This is the first efficient constant
factor approximate agnostic learner, even for Gaussian data and for any
nontrivial class of link functions. Prior work for the case of unknown link
function either works in the realizable setting or does not attain constant
factor approximation. The main technical ingredient enabling our algorithm and
analysis is a novel notion of a local error bound in optimization that we term
alignment sharpness and that may be of broader interest.
\\ ( https://arxiv.org/abs/2402.17756 ,  290kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2402.16863 (*cross-listing*)
Date: Sun, 21 Jan 2024 02:59:37 GMT   (767kb)

Title: Quantum Inspired Chaotic Salp Swarm Optimization for Dynamic
  Optimization
Authors: Sanjai Pathak, Ashish Mani, Mayank Sharma, Amlan Chatterjee
Categories: cs.NE cs.AI
Comments: 14 pages, 2 figures, 1 algorithm
\\
  Many real-world problems are dynamic optimization problems that are unknown
beforehand. In practice, unpredictable events such as the arrival of new jobs,
due date changes, and reservation cancellations, changes in parameters or
constraints make the search environment dynamic. Many algorithms are designed
to deal with stationary optimization problems, but these algorithms do not face
dynamic optimization problems or manage them correctly. Although some
optimization algorithms are proposed to deal with the changes in dynamic
environments differently, there are still areas of improvement in existing
algorithms due to limitations or drawbacks, especially in terms of locating and
following the previously identified optima. With this in mind, we studied a
variant of SSA known as QSSO, which integrates the principles of quantum
computing. An attempt is made to improve the overall performance of standard
SSA to deal with the dynamic environment effectively by locating and tracking
the global optima for DOPs. This work is an extension of the proposed new
algorithm QSSO, known as the Quantum-inspired Chaotic Salp Swarm Optimization
(QCSSO) Algorithm, which details the various approaches considered while
solving DOPs. A chaotic operator is employed with quantum computing to respond
to change and guarantee to increase individual searchability by improving
population diversity and the speed at which the algorithm converges. We
experimented by evaluating QCSSO on a well-known generalized dynamic benchmark
problem (GDBG) provided for CEC 2009, followed by a comparative numerical study
with well-regarded algorithms. As promised, the introduced QCSSO is discovered
as the rival algorithm for DOPs.
\\ ( https://arxiv.org/abs/2402.16863 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16866 (*cross-listing*)
Date: Mon, 22 Jan 2024 05:22:19 GMT   (828kb,D)

Title: Computation Rate Maximization for Wireless Powered Edge Computing With
  Multi-User Cooperation
Authors: Yang Li, Xing Zhang, Bo Lei, Qianying Zhao, Min Wei, Zheyan Qu, Wenbo
  Wang
Categories: cs.IT cs.AI math.IT
Comments: Accepted to IEEE Open Journal of the Communications Society
\\
  The combination of mobile edge computing (MEC) and radio frequency-based
wireless power transfer (WPT) presents a promising technique for providing
sustainable energy supply and computing services at the network edge. This
study considers a wireless-powered mobile edge computing system that includes a
hybrid access point (HAP) equipped with a computing unit and multiple Internet
of Things (IoT) devices. In particular, we propose a novel muti-user
cooperation scheme to improve computation performance, where collaborative
clusters are dynamically formed. Each collaborative cluster comprises a source
device (SD) and an auxiliary device (AD), where the SD can partition the
computation task into various segments for local processing, offloading to the
HAP, and remote execution by the AD with the assistance of the HAP.
Specifically, we aims to maximize the weighted sum computation rate (WSCR) of
all the IoT devices in the network. This involves jointly optimizing
collaboration, time and data allocation among multiple IoT devices and the HAP,
while considering the energy causality property and the minimum data processing
requirement of each device. Initially, an optimization algorithm based on the
interior-point method is designed for time and data allocation. Subsequently, a
priority-based iterative algorithm is developed to search for a near-optimal
solution to the multi-user collaboration scheme. Finally, a deep learning-based
approach is devised to further accelerate the algorithm's operation, building
upon the initial two algorithms. Simulation results show that the performance
of the proposed algorithms is comparable to that of the exhaustive search
method, and the deep learning-based algorithm significantly reduces the
execution time of the algorithm.
\\ ( https://arxiv.org/abs/2402.16866 ,  828kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16868 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:19:21 GMT   (2405kb,D)

Title: Codebook-enabled Generative End-to-end Semantic Communication Powered by
  Transformer
Authors: Peigen Ye, Yaping Sun, Shumin Yao, Hao Chen, Xiaodong Xu, Shuguang Cui
Categories: cs.IT cs.AI math.IT
\\
  Codebook-based generative semantic communication attracts increasing
attention, since only indices are required to be transmitted when the codebook
is shared between transmitter and receiver. However, due to the fact that the
semantic relations among code vectors are not necessarily related to the
distance of the corresponding code indices, the performance of the
codebook-enabled semantic communication system is susceptible to the channel
noise. Thus, how to improve the system robustness against the noise requires
careful design. This paper proposes a robust codebook-assisted image semantic
communication system, where semantic codec and codebook are first jointly
constructed, and then vector-to-index transformer is designed guided by the
codebook to eliminate the effects of channel noise, and achieve image
generation. Thanks to the assistance of the high-quality codebook to the
Transformer, the generated images at the receiver outperform those of the
compared methods in terms of visual perception. In the end, numerical results
and generated images demonstrate the advantages of the generative semantic
communication method over JPEG+LDPC and traditional joint source channel coding
(JSCC) methods.
\\ ( https://arxiv.org/abs/2402.16868 ,  2405kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16869 (*cross-listing*)
Date: Tue, 23 Jan 2024 10:17:42 GMT   (307kb)

Title: Considering Fundamental Rights in the European Standardisation of
  Artificial Intelligence: Nonsense or Strategic Alliance?
Authors: Marion Ho-Dac (UA)
Categories: cs.CY cs.AI
\\
  In the European context, both the EU AI Act proposal and the draft
Standardisation Request on safe and trustworthy AI link standardisation to
fundamental rights. However, these texts do not provide any guidelines that
specify and detail the relationship between AI standards and fundamental
rights, its meaning or implication. This chapter aims to clarify this critical
regulatory blind spot. The main issue tackled is whether the adoption of AI
harmonised standards, based on the future AI Act, should take into account
fundamental rights. In our view, the response is yes. The high risks posed by
certain AI systems relate in particular to infringements of fundamental rights.
Therefore, mitigating such risks involves fundamental rights considerations and
this is what future harmonised standards should reflect. At the same time,
valid criticisms of the European standardisation process have to be addressed.
Finally, the practical incorporation of fundamental rights considerations in
the ongoing European standardisation of AI systems is discussed.
\\ ( https://arxiv.org/abs/2402.16869 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16871 (*cross-listing*)
Date: Wed, 24 Jan 2024 17:33:40 GMT   (2782kb)

Title: Bike3S: A Tool for Bike Sharing Systems Simulation
Authors: Alberto Fern\'andez, Holger Billhardt, Sascha Ossowski, \'Oscar
  S\'anchez
Categories: cs.MA cs.AI
ACM-class: I.2.1
Journal-ref: Journal of Simulation 14(4), 2020
DOI: 10.1080/17477778.2020.1718022
\\
  Vehicle sharing systems are becoming increasingly popular. The effectiveness
of such systems depends, among other factors, on different strategic and
operational management decisions and policies, like the dimension of the fleet
or the distribution of vehicles. It is of foremost importance to be able to
anticipate and evaluate the potential effects of such strategies before they
can be successfully deployed. In this paper we present Bike3S, a simulator for
a station-based bike sharing system. The simulator performs semi-realistic
simulations of the operation of a bike sharing system and allows for evaluating
and testing different management decisions and strategies. In particular, the
simulator has been designed to test different station capacities, station
distributions, and balancing strategies. The simulator carries out microscopic
agent-based simulations, where users of different types can be defined that act
according to their individual goals and objectives which influences the overall
dynamics of the whole system.
\\ ( https://arxiv.org/abs/2402.16871 ,  2782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16874 (*cross-listing*)
Date: Tue, 6 Feb 2024 13:19:53 GMT   (484kb)

Title: Enhancing Retrieval Processes for Language Generation with Augmented
  Queries
Authors: Julien Pierre Edmond Ghali, Kosuke Shima, Koichi Moriyama, Atsuko
  Mutoh, Nobuhiro Inuzuka
Categories: cs.IR cs.AI cs.CL
Comments: 28 pages, 10 annexes, 2 figures
\\
  In the rapidly changing world of smart technology, searching for documents
has become more challenging due to the rise of advanced language models. These
models sometimes face difficulties, like providing inaccurate information,
commonly known as "hallucination." This research focuses on addressing this
issue through Retrieval-Augmented Generation (RAG), a technique that guides
models to give accurate responses based on real facts. To overcome scalability
issues, the study explores connecting user queries with sophisticated language
models such as BERT and Orca2, using an innovative query optimization process.
The study unfolds in three scenarios: first, without RAG, second, without
additional assistance, and finally, with extra help. Choosing the compact yet
efficient Orca2 7B model demonstrates a smart use of computing resources. The
empirical results indicate a significant improvement in the initial language
model's performance under RAG, particularly when assisted with prompts
augmenters. Consistency in document retrieval across different encodings
highlights the effectiveness of using language model-generated queries. The
introduction of UMAP for BERT further simplifies document retrieval while
maintaining strong results.
\\ ( https://arxiv.org/abs/2402.16874 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16876 (*cross-listing*)
Date: Wed, 7 Feb 2024 22:37:18 GMT   (3324kb)

Title: Advanced Academic Team Worker Recommendation Models
Authors: Mi Wu
Categories: cs.IR cs.AI
\\
  Collaborator recommendation is an important task in academic domain. Most of
the existing approaches have the assumption that the recommendation system only
need to recommend a specific researcher for the task. However, academic
successes can be owed to productive collaboration of a whole academic team. In
this work, we propose a new task: academic team worker recommendation: with a
given status: student, assistant professor or prime professor, research
interests and specific task, we can recommend an academic team formed as (prime
professor, assistant professor, student). For this task, we propose a model
CQBG-R(Citation-Query Blended Graph-Ranking). The key ideas is to combine the
context of the query and the papers with the graph topology to form a new
graph(CQBG), which can target at the research interests and the specific
research task for this time. The experiment results show the effectiveness of
the proposed method.
\\ ( https://arxiv.org/abs/2402.16876 ,  3324kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16877 (*cross-listing*)
Date: Thu, 8 Feb 2024 20:35:31 GMT   (1053kb,D)

Title: Large Language Model Augmented Exercise Retrieval for Personalized
  Language Learning
Authors: Austin Xu, Will Monroe, Klinton Bicknell
Categories: cs.IR cs.AI cs.CL cs.LG
Comments: Presented at Learning Analytics and Knowledge 2024. 11 pages, 4
  figures, 5 tables
\\
  We study the problem of zero-shot exercise retrieval in the context of online
language learning, to give learners the ability to explicitly request
personalized exercises via natural language. Using real-world data collected
from language learners, we observe that vector similarity approaches poorly
capture the relationship between exercise content and the language that
learners use to express what they want to learn. This semantic gap between
queries and content dramatically reduces the effectiveness of general-purpose
retrieval models pretrained on large scale information retrieval datasets like
MS MARCO. We leverage the generative capabilities of large language models to
bridge the gap by synthesizing hypothetical exercises based on the learner's
input, which are then used to search for relevant exercises. Our approach,
which we call mHyER, overcomes three challenges: (1) lack of relevance labels
for training, (2) unrestricted learner input content, and (3) low semantic
similarity between input and retrieval candidates. mHyER outperforms several
strong baselines on two novel benchmarks created from crowdsourced data and
publicly available data.
\\ ( https://arxiv.org/abs/2402.16877 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16882 (*cross-listing*)
Date: Mon, 19 Feb 2024 02:21:20 GMT   (29617kb,D)

Title: Substrate Scope Contrastive Learning: Repurposing Human Bias to Learn
  Atomic Representations
Authors: Wenhao Gao, Priyanka Raghavan, Ron Shprints, Connor W. Coley
Categories: physics.chem-ph cs.AI cs.LG q-bio.BM
\\
  Learning molecular representation is a critical step in molecular machine
learning that significantly influences modeling success, particularly in
data-scarce situations. The concept of broadly pre-training neural networks has
advanced fields such as computer vision, natural language processing, and
protein engineering. However, similar approaches for small organic molecules
have not achieved comparable success. In this work, we introduce a novel
pre-training strategy, substrate scope contrastive learning, which learns
atomic representations tailored to chemical reactivity. This method considers
the grouping of substrates and their yields in published substrate scope tables
as a measure of their similarity or dissimilarity in terms of chemical
reactivity. We focus on 20,798 aryl halides in the CAS Content Collection
spanning thousands of publications to learn a representation of aryl halide
reactivity. We validate our pre-training approach through both intuitive
visualizations and comparisons to traditional reactivity descriptors and
physical organic chemistry principles. The versatility of these embeddings is
further evidenced in their application to yield prediction, regioselectivity
prediction, and the diverse selection of new substrates. This work not only
presents a chemistry-tailored neural network pre-training strategy to learn
reactivity-aligned atomic representations, but also marks a first-of-its-kind
approach to benefit from the human bias in substrate scope design.
\\ ( https://arxiv.org/abs/2402.16882 ,  29617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16886 (*cross-listing*)
Date: Wed, 7 Feb 2024 22:15:15 GMT   (447kb)

Title: Using text embedding models and vector databases as text classifiers
  with the example of medical data
Authors: Rishabh Goel
Categories: cs.IR cs.AI cs.LG
Comments: 11 pages, 8 figures, All robustness tests are in a linked pdf
ACM-class: I.2.7; J.3
\\
  The advent of Large Language Models (LLMs) is promising and has found
application in numerous fields, but as it often is with the medical field, the
bar is typically quite high [5]. In tandem with LLMs, vector embedding models
and vector databases provide a robust way of expressing numerous modes of data
that are easily digestible by typical machine learning models. Along with the
ease of adding information, knowledge, and data to these vector databases, they
provide a compelling reason to apply them in numerous fields where the task of
retrieving information is typically done by humans. Researchers at Google have
developed a clear alternative model, Med-PaLM [6] specifically designed to
match a clinician's level of accuracy when it comes to medical knowledge. When
training classifiers, and developing models, it is imperative to maintain
factuality and reduce bias [4]. Here, we explore the use of vector databases
and embedding models as a means of encoding, and classifying text with the
example and application in the field of medicine. We show the robustness of
these tools depends heavily on the sparsity of the data presented, and even
with low amounts of data in the vector database itself, the vector database
does a good job at classifying data [9]. Using various LLMs to generate the
medical data, we also understand the limitations of the medical knowledge of
these models and encourage further expert medical review of our testing data.
By using vector databases to classify a clinician's notes on a patient
presented with a certain ailment, we understand the limitations of such
methods, but also the promise of their prospective use and with continued
testing and experimentation, hope to explore a unique use case of vector
databases and embedding models.
\\ ( https://arxiv.org/abs/2402.16886 ,  447kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16887 (*cross-listing*)
Date: Fri, 23 Feb 2024 09:06:36 GMT   (9090kb,D)

Title: Artificial Intelligence for Complex Network: Potential, Methodology and
  Application
Authors: Jingtao Ding, Chang Liu, Yu Zheng, Yunke Zhang, Zihan Yu, Ruikun Li,
  Hongyi Chen, Jinghua Piao, Huandong Wang, Jiazhen Liu and Yong Li
Categories: cs.SI cs.AI cs.LG physics.soc-ph
Comments: 51 pages, 4 figures, 10 tables
\\
  Complex networks pervade various real-world systems, from the natural
environment to human societies. The essence of these networks is in their
ability to transition and evolve from microscopic disorder-where network
topology and node dynamics intertwine-to a macroscopic order characterized by
certain collective behaviors. Over the past two decades, complex network
science has significantly enhanced our understanding of the statistical
mechanics, structures, and dynamics underlying real-world networks. Despite
these advancements, there remain considerable challenges in exploring more
realistic systems and enhancing practical applications. The emergence of
artificial intelligence (AI) technologies, coupled with the abundance of
diverse real-world network data, has heralded a new era in complex network
science research. This survey aims to systematically address the potential
advantages of AI in overcoming the lingering challenges of complex network
research. It endeavors to summarize the pivotal research problems and provide
an exhaustive review of the corresponding methodologies and applications.
Through this comprehensive survey-the first of its kind on AI for complex
networks-we expect to provide valuable insights that will drive further
research and advancement in this interdisciplinary field.
\\ ( https://arxiv.org/abs/2402.16887 ,  9090kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16893 (*cross-listing*)
Date: Fri, 23 Feb 2024 18:35:15 GMT   (29543kb,D)

Title: The Good and The Bad: Exploring Privacy Issues in Retrieval-Augmented
  Generation (RAG)
Authors: Shenglai Zeng, Jiankun Zhang, Pengfei He, Yue Xing, Yiding Liu, Han
  Xu, Jie Ren, Shuaiqiang Wang, Dawei Yin, Yi Chang, Jiliang Tang
Categories: cs.CR cs.AI
\\
  Retrieval-augmented generation (RAG) is a powerful technique to facilitate
language model with proprietary and private data, where data privacy is a
pivotal concern. Whereas extensive research has demonstrated the privacy risks
of large language models (LLMs), the RAG technique could potentially reshape
the inherent behaviors of LLM generation, posing new privacy issues that are
currently under-explored. In this work, we conduct extensive empirical studies
with novel attack methods, which demonstrate the vulnerability of RAG systems
on leaking the private retrieval database. Despite the new risk brought by RAG
on the retrieval data, we further reveal that RAG can mitigate the leakage of
the LLMs' training data. Overall, we provide new insights in this paper for
privacy protection of retrieval-augmented LLMs, which benefit both LLMs and RAG
systems builders. Our code is available at
https://github.com/phycholosogy/RAG-privacy.
\\ ( https://arxiv.org/abs/2402.16893 ,  29543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16898 (*cross-listing*)
Date: Sat, 24 Feb 2024 03:48:22 GMT   (17731kb,D)

Title: MIM-Reasoner: Learning with Theoretical Guarantees for Multiplex
  Influence Maximization
Authors: Nguyen Do, Tanmoy Chowdhury, Chen Ling, Liang Zhao, My T. Thai
Categories: cs.SI cs.AI cs.LG math.PR stat.ML
Journal-ref: International Conference on Artificial Intelligence and Statistics
  (AISTATS) 2024
\\
  Multiplex influence maximization (MIM) asks us to identify a set of seed
users such as to maximize the expected number of influenced users in a
multiplex network. MIM has been one of central research topics, especially in
nowadays social networking landscape where users participate in multiple online
social networks (OSNs) and their influences can propagate among several OSNs
simultaneously. Although there exist a couple combinatorial algorithms to MIM,
learning-based solutions have been desired due to its generalization ability to
heterogeneous networks and their diversified propagation characteristics. In
this paper, we introduce MIM-Reasoner, coupling reinforcement learning with
probabilistic graphical model, which effectively captures the complex
propagation process within and between layers of a given multiplex network,
thereby tackling the most challenging problem in MIM. We establish a
theoretical guarantee for MIM-Reasoner as well as conduct extensive analyses on
both synthetic and real-world datasets to validate our MIM-Reasoner's
performance.
\\ ( https://arxiv.org/abs/2402.16898 ,  17731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16901 (*cross-listing*)
Date: Sat, 24 Feb 2024 13:13:17 GMT   (45582kb,D)

Title: FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics
Authors: ChenRui Duan, Zelin Zang, Yongjie Xu, Hang He, Zihan Liu, Zijia Song,
  Ju-Sheng Zheng, Stan Z. Li
Categories: q-bio.GN cs.AI cs.LG
\\
  Metagenomic data, comprising mixed multi-species genomes, are prevalent in
diverse environments like oceans and soils, significantly impacting human
health and ecological functions. However, current research relies on K-mer
representations, limiting the capture of structurally relevant gene contexts.
To address these limitations and further our understanding of complex
relationships between metagenomic sequences and their functions, we introduce a
protein-based gene representation as a context-aware and structure-relevant
tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene
group-level pre-training, providing insights into inter-gene contextual
information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for
gene-level pre-training to model gene sequence-function relationships. MGM and
TEM-CL constitute our novel metagenomic language model {\NAME}, pre-trained on
100 million metagenomic sequences. We demonstrate the superiority of our
proposed {\NAME} on eight datasets.
\\ ( https://arxiv.org/abs/2402.16901 ,  45582kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16906 (*cross-listing*)
Date: Sun, 25 Feb 2024 00:56:27 GMT   (782kb,D)

Title: LDB: A Large Language Model Debugger via Verifying Runtime Execution
  Step-by-step
Authors: Li Zhong, Zilong Wang, Jingbo Shang
Categories: cs.SE cs.AI cs.CL
Comments: Preprint
\\
  Large language models (LLMs) are leading significant progress in code
generation. Beyond one-pass code generation, recent works further integrate
unit tests and program verifiers into LLMs to iteratively refine the generated
programs. However, these works consider the generated programs as an
indivisible entity, which falls short for LLMs in debugging the programs,
especially when the programs contain complex logic flows and data operations.
In contrast, when human developers debug programs, they typically set
breakpoints and selectively examine runtime execution information. The
execution flow and the intermediate variables play a crucial role in the
debugging process, yet they are underutilized in the existing literature on
code generation. In this study, we introduce Large Language Model Debugger
(LDB), a novel debugging framework that enables LLMs to refine their generated
programs with the runtime execution information. Specifically, LDB segments the
programs into basic blocks and tracks the values of intermediate variables
after each block throughout the runtime execution. This allows LLMs to
concentrate on simpler code units within the overall execution flow, verify
their correctness against the task description block by block, and efficiently
pinpoint any potential errors. Experiments demonstrate that LDB consistently
enhances the baseline performance by up to 9.8% across the HumanEval, MBPP, and
TransCoder benchmarks, archiving new state-of-the-art performance in code
debugging for various LLM selections.
\\ ( https://arxiv.org/abs/2402.16906 ,  782kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16910 (*cross-listing*)
Date: Sun, 25 Feb 2024 13:20:13 GMT   (1008kb,D)

Title: NeSy is alive and well: A LLM-driven symbolic approach for better code
  comment data generation and classification
Authors: Hanna Abi Akl
Categories: cs.SE cs.AI
Comments: 18 pages, 4 figures, forthcoming chapter in the book Generative
  Artificial Intelligence for Code - Impact of Large Language Models on Code
  Generation and Summarization in the book series Transactions on Computer
  Systems and Networks, Springer
\\
  We present a neuro-symbolic (NeSy) workflow combining a symbolic-based
learning technique with a large language model (LLM) agent to generate
synthetic data for code comment classification in the C programming language.
We also show how generating controlled synthetic data using this workflow fixes
some of the notable weaknesses of LLM-based generation and increases the
performance of classical machine learning models on the code comment
classification task. Our best model, a Neural Network, achieves a Macro-F1
score of 91.412% with an increase of 1.033% after data augmentation.
\\ ( https://arxiv.org/abs/2402.16910 ,  1008kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16914 (*cross-listing*)
Date: Sun, 25 Feb 2024 17:43:29 GMT   (1945kb,D)

Title: DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM
  Jailbreakers
Authors: Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh
Categories: cs.CR cs.AI cs.CL
\\
  The safety alignment of Large Language Models (LLMs) is vulnerable to both
manual and automated jailbreak attacks, which adversarially trigger LLMs to
output harmful content. However, current methods for jailbreaking LLMs, which
nest entire harmful prompts, are not effective at concealing malicious intent
and can be easily identified and rejected by well-aligned LLMs. This paper
discovers that decomposing a malicious prompt into separated sub-prompts can
effectively obscure its underlying malicious intent by presenting it in a
fragmented, less detectable form, thereby addressing these limitations. We
introduce an automatic prompt \textbf{D}ecomposition and
\textbf{R}econstruction framework for jailbreak \textbf{Attack} (DrAttack).
DrAttack includes three key components: (a) `Decomposition' of the original
prompt into sub-prompts, (b) `Reconstruction' of these sub-prompts implicitly
by in-context learning with semantically similar but harmless reassembling
demo, and (c) a `Synonym Search' of sub-prompts, aiming to find sub-prompts'
synonyms that maintain the original intent while jailbreaking LLMs. An
extensive empirical study across multiple open-source and closed-source LLMs
demonstrates that, with a significantly reduced number of queries, DrAttack
obtains a substantial gain of success rate over prior SOTA prompt-only
attackers. Notably, the success rate of 78.0\% on GPT-4 with merely 15 queries
surpassed previous art by 33.1\%.
\\ ( https://arxiv.org/abs/2402.16914 ,  1945kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16926 (*cross-listing*)
Date: Mon, 26 Feb 2024 11:43:01 GMT   (33kb,D)

Title: On the (In)feasibility of ML Backdoor Detection as an Hypothesis Testing
  Problem
Authors: Georg Pichler, Marco Romanelli, Divya Prakash Manivannan, Prashanth
  Krishnamurthy, Farshad Khorrami, Siddharth Garg
Categories: cs.CR cs.AI cs.LG stat.ML
\\
  We introduce a formal statistical definition for the problem of backdoor
detection in machine learning systems and use it to analyze the feasibility of
such problems, providing evidence for the utility and applicability of our
definition. The main contributions of this work are an impossibility result and
an achievability result for backdoor detection. We show a no-free-lunch
theorem, proving that universal (adversary-unaware) backdoor detection is
impossible, except for very small alphabet sizes. Thus, we argue, that backdoor
detection methods need to be either explicitly, or implicitly adversary-aware.
However, our work does not imply that backdoor detection cannot work in
specific scenarios, as evidenced by successful backdoor detection methods in
the scientific literature. Furthermore, we connect our definition to the
probably approximately correct (PAC) learnability of the out-of-distribution
detection problem.
\\ ( https://arxiv.org/abs/2402.16926 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16928 (*cross-listing*)
Date: Mon, 26 Feb 2024 13:49:52 GMT   (2111kb,D)

Title: CLAP: Learning Transferable Binary Code Representations with Natural
  Language Supervision
Authors: Hao Wang, Zeyu Gao, Chao Zhang, Zihan Sha, Mingyang Sun, Yuchen Zhou,
  Wenyu Zhu, Wenju Sun, Han Qiu, Xi Xiao
Categories: cs.SE cs.AI
\\
  Binary code representation learning has shown significant performance in
binary analysis tasks. But existing solutions often have poor transferability,
particularly in few-shot and zero-shot scenarios where few or no training
samples are available for the tasks. To address this problem, we present CLAP
(Contrastive Language-Assembly Pre-training), which employs natural language
supervision to learn better representations of binary code (i.e., assembly
code) and get better transferability. At the core, our approach boosts superior
transfer learning capabilities by effectively aligning binary code with their
semantics explanations (in natural language), resulting a model able to
generate better embeddings for binary code. To enable this alignment training,
we then propose an efficient dataset engine that could automatically generate a
large and diverse dataset comprising of binary code and corresponding natural
language explanations. We have generated 195 million pairs of binary code and
explanations and trained a prototype of CLAP. The evaluations of CLAP across
various downstream tasks in binary analysis all demonstrate exceptional
performance. Notably, without any task-specific training, CLAP is often
competitive with a fully supervised baseline, showing excellent
transferability. We release our pre-trained model and code at
https://github.com/Hustcw/CLAP.
\\ ( https://arxiv.org/abs/2402.16928 ,  2111kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16929 (*cross-listing*)
Date: Mon, 26 Feb 2024 15:05:16 GMT   (1428kb,D)

Title: LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs
  from the Programming Language
Authors: Ming Wang, Yuanzhong Liu, Xiaoming Zhang, Songlian Li, Yijie Huang,
  Chi Zhang, Daling Wang, Shi Feng, Jigang Li
Categories: cs.SE cs.AI cs.CL cs.PL
\\
  LLMs have demonstrated commendable performance across diverse domains.
Nevertheless, formulating high-quality prompts to effectively instruct LLMs
poses a challenge for non-AI experts. Existing research in prompt engineering
suggests somewhat fragmented optimization principles and designs empirically
dependent prompt optimizers. Unfortunately, these endeavors lack a structured
design template, incurring high learning costs and resulting in low
reusability. Inspired by structured reusable programming languages, we propose
LangGPT, a dual-layer prompt design framework as the programming language for
LLMs. LangGPT has an easy-to-learn normative structure and provides an extended
structure for migration and reuse. Experiments illustrate that LangGPT
significantly enhances the capacity of LLMs to produce responses of superior
quality compared to baselines. Moreover, LangGPT has proven effective in
guiding LLMs to generate high-quality prompts. We have built a community on
LangGPT to facilitate the tuition and sharing of prompt design. We also
analyzed the ease of use and reusability of LangGPT through a community user
survey.
\\ ( https://arxiv.org/abs/2402.16929 ,  1428kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16965 (*cross-listing*)
Date: Mon, 26 Feb 2024 19:01:54 GMT   (9489kb,D)

Title: WIPI: A New Web Threat for LLM-Driven Web Agents
Authors: Fangzhou Wu, Shutong Wu, Yulong Cao, Chaowei Xiao
Categories: cs.CR cs.AI
\\
  With the fast development of large language models (LLMs), LLM-driven Web
Agents (Web Agents for short) have obtained tons of attention due to their
superior capability where LLMs serve as the core part of making decisions like
the human brain equipped with multiple web tools to actively interact with
external deployed websites. As uncountable Web Agents have been released and
such LLM systems are experiencing rapid development and drawing closer to
widespread deployment in our daily lives, an essential and pressing question
arises: "Are these Web Agents secure?". In this paper, we introduce a novel
threat, WIPI, that indirectly controls Web Agent to execute malicious
instructions embedded in publicly accessible webpages. To launch a successful
WIPI works in a black-box environment. This methodology focuses on the form and
content of indirect instructions within external webpages, enhancing the
efficiency and stealthiness of the attack. To evaluate the effectiveness of the
proposed methodology, we conducted extensive experiments using 7 plugin-based
ChatGPT Web Agents, 8 Web GPTs, and 3 different open-source Web Agents. The
results reveal that our methodology achieves an average attack success rate
(ASR) exceeding 90% even in pure black-box scenarios. Moreover, through an
ablation study examining various user prefix instructions, we demonstrated that
the WIPI exhibits strong robustness, maintaining high performance across
diverse prefix instructions.
\\ ( https://arxiv.org/abs/2402.16965 ,  9489kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16968 (*cross-listing*)
Date: Mon, 26 Feb 2024 19:06:02 GMT   (28kb)

Title: A Survey of Large Language Models in Cybersecurity
Authors: Gabriel de Jesus Coelho da Silva, Carlos Becker Westphall
Categories: cs.CR cs.AI
\\
  Large Language Models (LLMs) have quickly risen to prominence due to their
ability to perform at or close to the state-of-the-art in a variety of fields
while handling natural language. An important field of research is the
application of such models at the cybersecurity context. This survey aims to
identify where in the field of cybersecurity LLMs have already been applied,
the ways in which they are being used and their limitations in the field.
Finally, suggestions are made on how to improve such limitations and what can
be expected from these systems once these limitations are overcome.
\\ ( https://arxiv.org/abs/2402.16968 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16994 (*cross-listing*)
Date: Mon, 26 Feb 2024 20:00:57 GMT   (20325kb,D)

Title: GEM3D: GEnerative Medial Abstractions for 3D Shape Synthesis
Authors: Dmitry Petrov, Pradyumn Goyal, Vikas Thamizharasan, Vladimir G. Kim,
  Matheus Gadelha, Melinos Averkiou, Siddhartha Chaudhuri, Evangelos
  Kalogerakis
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Project webpage: https://lodurality.github.io/GEM3D/
\\
  We introduce GEM3D -- a new deep, topology-aware generative model of 3D
shapes. The key ingredient of our method is a neural skeleton-based
representation encoding information on both shape topology and geometry.
Through a denoising diffusion probabilistic model, our method first generates
skeleton-based representations following the Medial Axis Transform (MAT), then
generates surfaces through a skeleton-driven neural implicit formulation. The
neural implicit takes into account the topological and geometric information
stored in the generated skeleton representations to yield surfaces that are
more topologically and geometrically accurate compared to previous neural field
formulations. We discuss applications of our method in shape synthesis and
point cloud reconstruction tasks, and evaluate our method both qualitatively
and quantitatively. We demonstrate significantly more faithful surface
reconstruction and diverse shape generation results compared to the
state-of-the-art, also involving challenging scenarios of reconstructing and
synthesizing structurally complex, high-genus shape surfaces from Thingi10K and
ShapeNet.
\\ ( https://arxiv.org/abs/2402.16994 ,  20325kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17012 (*cross-listing*)
Date: Mon, 26 Feb 2024 20:41:50 GMT   (2727kb,D)

Title: Pandora's White-Box: Increased Training Data Leakage in Open LLMs
Authors: Jeffrey G. Wang, Jason Wang, Marvin Li, Seth Neel
Categories: cs.CR cs.AI cs.LG
\\
  In this paper we undertake a systematic study of privacy attacks against open
source Large Language Models (LLMs), where an adversary has access to either
the model weights, gradients, or losses, and tries to exploit them to learn
something about the underlying training data. Our headline results are the
first membership inference attacks (MIAs) against pre-trained LLMs that are
able to simultaneously achieve high TPRs and low FPRs, and a pipeline showing
that over $50\%$ (!) of the fine-tuning dataset can be extracted from a
fine-tuned LLM in natural settings. We consider varying degrees of access to
the underlying model, customization of the language model, and resources
available to the attacker. In the pre-trained setting, we propose three new
white-box MIAs: an attack based on the gradient norm, a supervised neural
network classifier, and a single step loss ratio attack. All outperform
existing black-box baselines, and our supervised attack closes the gap between
MIA attack success against LLMs and other types of models. In fine-tuning, we
find that given access to the loss of the fine-tuned and base models, a
fine-tuned loss ratio attack FLoRA is able to achieve near perfect MIA
peformance. We then leverage these MIAs to extract fine-tuning data from
fine-tuned language models. We find that the pipeline of generating from
fine-tuned models prompted with a small snippet of the prefix of each training
example, followed by using FLoRa to select the most likely training sample,
succeeds the majority of the fine-tuning dataset after only $3$ epochs of
fine-tuning. Taken together, these findings show that highly effective MIAs are
available in almost all LLM training settings, and highlight that great care
must be taken before LLMs are fine-tuned on highly sensitive data and then
deployed.
\\ ( https://arxiv.org/abs/2402.17012 ,  2727kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17042 (*cross-listing*)
Date: Mon, 26 Feb 2024 21:49:44 GMT   (17kb)

Title: Towards Generalizing Inferences from Trials to Target Populations
Authors: Melody Y Huang, Sarah E Robertson, Harsh Parikh
Categories: stat.ME cs.AI cs.LG econ.EM
\\
  Randomized Controlled Trials (RCTs) are pivotal in generating internally
valid estimates with minimal assumptions, serving as a cornerstone for
researchers dedicated to advancing causal inference methods. However, extending
these findings beyond the experimental cohort to achieve externally valid
estimates is crucial for broader scientific inquiry. This paper delves into the
forefront of addressing these external validity challenges, encapsulating the
essence of a multidisciplinary workshop held at the Institute for Computational
and Experimental Research in Mathematics (ICERM), Brown University, in Fall
2023. The workshop congregated experts from diverse fields including social
science, medicine, public health, statistics, computer science, and education,
to tackle the unique obstacles each discipline faces in extrapolating
experimental findings. Our study presents three key contributions: we integrate
ongoing efforts, highlighting methodological synergies across fields; provide
an exhaustive review of generalizability and transportability based on the
workshop's discourse; and identify persistent hurdles while suggesting avenues
for future research. By doing so, this paper aims to enhance the collective
understanding of the generalizability and transportability of causal effects,
fostering cross-disciplinary collaboration and offering valuable insights for
researchers working on refining and applying causal inference methods.
\\ ( https://arxiv.org/abs/2402.17042 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17045 (*cross-listing*)
Date: Mon, 26 Feb 2024 22:04:25 GMT   (195kb,D)

Title: An Investigation into the Performances of the State-of-the-art Machine
  Learning Approaches for Various Cyber-attack Detection: A Survey
Authors: Tosin Ige, Christopher Kiekintveld, Aritran Piplai
Categories: cs.CR cs.AI cs.LG
Comments: 11
\\
  To secure computers and information systems from attackers taking advantage
of vulnerabilities in the system to commit cybercrime, several methods have
been proposed for real-time detection of vulnerabilities to improve security
around information systems. Of all the proposed methods, machine learning had
been the most effective method in securing a system with capabilities ranging
from early detection of software vulnerabilities to real-time detection of
ongoing compromise in a system. As there are different types of cyberattacks,
each of the existing state-of-the-art machine learning models depends on
different algorithms for training which also impact their suitability for
detection of a particular type of cyberattack. In this research, we analyzed
each of the current state-of-theart machine learning models for different types
of cyberattack detection from the past 10 years with a major emphasis on the
most recent works for comparative study to identify the knowledge gap where
work is still needed to be done with regard to detection of each category of
cyberattack
\\ ( https://arxiv.org/abs/2402.17045 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17065 (*cross-listing*)
Date: Mon, 26 Feb 2024 23:03:00 GMT   (32647kb,D)

Title: Taming the Tail in Class-Conditional GANs: Knowledge Sharing via
  Unconditional Training at Lower Resolutions
Authors: Saeed Khorram, Mingqi Jiang, Mohamad Shahbazi, Mohamad H. Danesh, Li
  Fuxin
Categories: cs.CV cs.AI cs.LG
\\
  Despite the extensive research on training generative adversarial networks
(GANs) with limited training data, learning to generate images from long-tailed
training distributions remains fairly unexplored. In the presence of imbalanced
multi-class training data, GANs tend to favor classes with more samples,
leading to the generation of low-quality and less diverse samples in tail
classes. In this study, we aim to improve the training of class-conditional
GANs with long-tailed data. We propose a straightforward yet effective method
for knowledge sharing, allowing tail classes to borrow from the rich
information from classes with more abundant training data. More concretely, we
propose modifications to existing class-conditional GAN architectures to ensure
that the lower-resolution layers of the generator are trained entirely
unconditionally while reserving class-conditional generation for the
higher-resolution layers. Experiments on several long-tail benchmarks and GAN
architectures demonstrate a significant improvement over existing methods in
both the diversity and fidelity of the generated images. The code is available
at https://github.com/khorrams/utlo.
\\ ( https://arxiv.org/abs/2402.17065 ,  32647kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17082 (*cross-listing*)
Date: Mon, 26 Feb 2024 23:40:33 GMT   (2434kb,D)

Title: Deconstructing the Veneer of Simplicity: Co-Designing Introductory
  Generative AI Workshops with Local Entrepreneurs
Authors: Yasmine Kotturi, Angel Anderson, Glenn Ford, Michael Skirpan, Jeffrey
  P. Bigham
Categories: cs.HC cs.AI
DOI: 10.1145/3613904.3642191
\\
  Generative AI platforms and features are permeating many aspects of work.
Entrepreneurs from lean economies in particular are well positioned to
outsource tasks to generative AI given limited resources. In this paper, we
work to address a growing disparity in use of these technologies by building on
a four-year partnership with a local entrepreneurial hub dedicated to equity in
tech and entrepreneurship. Together, we co-designed an interactive workshops
series aimed to onboard local entrepreneurs to generative AI platforms.
Alongside four community-driven and iterative workshops with entrepreneurs
across five months, we conducted interviews with 15 local entrepreneurs and
community providers. We detail the importance of communal and supportive
exposure to generative AI tools for local entrepreneurs, scaffolding actionable
use (and supporting non-use), demystifying generative AI technologies by
emphasizing entrepreneurial power, while simultaneously deconstructing the
veneer of simplicity to address the many operational skills needed for
successful application.
\\ ( https://arxiv.org/abs/2402.17082 ,  2434kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17087 (*cross-listing*)
Date: Mon, 26 Feb 2024 23:53:34 GMT   (6kb)

Title: A Note on Bayesian Networks with Latent Root Variables
Authors: Marco Zaffalon and Alessandro Antonucci
Categories: stat.ML cs.AI cs.LG
\\
  We characterise the likelihood function computed from a Bayesian network with
latent variables as root nodes. We show that the marginal distribution over the
remaining, manifest, variables also factorises as a Bayesian network, which we
call empirical. A dataset of observations of the manifest variables allows us
to quantify the parameters of the empirical Bayesian net. We prove that (i) the
likelihood of such a dataset from the original Bayesian network is dominated by
the global maximum of the likelihood from the empirical one; and that (ii) such
a maximum is attained if and only if the parameters of the Bayesian network are
consistent with those of the empirical model.
\\ ( https://arxiv.org/abs/2402.17087 ,  6kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17101 (*cross-listing*)
Date: Tue, 27 Feb 2024 00:29:33 GMT   (2526kb,D)

Title: T-HITL Effectively Addresses Problematic Associations in Image
  Generation and Maintains Overall Visual Quality
Authors: Susan Epstein, Li Chen, Alessandro Vecchiato, Ankit Jain
Categories: cs.CV cs.AI
Comments: 11 pages, 8 figures
MSC-class: I.I.2
ACM-class: I.2.1
\\
  Generative AI image models may inadvertently generate problematic
representations of people. Past research has noted that millions of users
engage daily across the world with these models and that the models, including
through problematic representations of people, have the potential to compound
and accelerate real-world discrimination and other harms (Bianchi et al, 2023).
In this paper, we focus on addressing the generation of problematic
associations between demographic groups and semantic concepts that may reflect
and reinforce negative narratives embedded in social data. Building on
sociological literature (Blumer, 1958) and mapping representations to model
behaviors, we have developed a taxonomy to study problematic associations in
image generation models. We explore the effectiveness of fine tuning at the
model level as a method to address these associations, identifying a potential
reduction in visual quality as a limitation of traditional fine tuning. We also
propose a new methodology with twice-human-in-the-loop (T-HITL) that promises
improvements in both reducing problematic associations and also maintaining
visual quality. We demonstrate the effectiveness of T-HITL by providing
evidence of three problematic associations addressed by T-HITL at the model
level. Our contributions to scholarship are two-fold. By defining problematic
associations in the context of machine learning models and generative AI, we
introduce a conceptual and technical taxonomy for addressing some of these
associations. Finally, we provide a method, T-HITL, that addresses these
associations and simultaneously maintains visual quality of image model
generations. This mitigation need not be a tradeoff, but rather an enhancement.
\\ ( https://arxiv.org/abs/2402.17101 ,  2526kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17139 (*cross-listing*)
Date: Tue, 27 Feb 2024 02:05:29 GMT   (39336kb,D)

Title: Video as the New Language for Real-World Decision Making
Authors: Sherry Yang, Jacob Walker, Jack Parker-Holder, Yilun Du, Jake Bruce,
  Andre Barreto, Pieter Abbeel, Dale Schuurmans
Categories: cs.CV cs.AI
\\
  Both text and video data are abundant on the internet and support large-scale
self-supervised learning through next token or frame prediction. However, they
have not been equally leveraged: language models have had significant
real-world impact, whereas video generation has remained largely limited to
media entertainment. Yet video data captures important information about the
physical world that is difficult to express in language. To address this gap,
we discuss an under-appreciated opportunity to extend video generation to solve
tasks in the real world. We observe how, akin to language, video can serve as a
unified interface that can absorb internet knowledge and represent diverse
tasks. Moreover, we demonstrate how, like language models, video generation can
serve as planners, agents, compute engines, and environment simulators through
techniques such as in-context learning, planning and reinforcement learning. We
identify major impact opportunities in domains such as robotics, self-driving,
and science, supported by recent work that demonstrates how such advanced
capabilities in video generation are plausibly within reach. Lastly, we
identify key challenges in video generation that mitigate progress. Addressing
these challenges will enable video generation models to demonstrate unique
value alongside language models in a wider array of AI applications.
\\ ( https://arxiv.org/abs/2402.17139 ,  39336kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17144 (*cross-listing*)
Date: Tue, 27 Feb 2024 02:16:07 GMT   (735kb,D)

Title: Metasql: A Generate-then-Rank Framework for Natural Language to SQL
  Translation
Authors: Yuankai Fan, Zhenying He, Tonghui Ren, Can Huang, Yinan Jing, Kai
  Zhang, X.Sean Wang
Categories: cs.DB cs.AI
\\
  The Natural Language Interface to Databases (NLIDB) empowers non-technical
users with database access through intuitive natural language (NL)
interactions. Advanced approaches, utilizing neural sequence-to-sequence models
or large-scale language models, typically employ auto-regressive decoding to
generate unique SQL queries sequentially. While these translation models have
greatly improved the overall translation accuracy, surpassing 70% on NLIDB
benchmarks, the use of auto-regressive decoding to generate single SQL queries
may result in sub-optimal outputs, potentially leading to erroneous
translations. In this paper, we propose Metasql, a unified generate-then-rank
framework that can be flexibly incorporated with existing NLIDBs to
consistently improve their translation accuracy. Metasql introduces query
metadata to control the generation of better SQL query candidates and uses
learning-to-rank algorithms to retrieve globally optimized queries.
Specifically, Metasql first breaks down the meaning of the given NL query into
a set of possible query metadata, representing the basic concepts of the
semantics. These metadata are then used as language constraints to steer the
underlying translation model toward generating a set of candidate SQL queries.
Finally, Metasql ranks the candidates to identify the best matching one for the
given NL query. Extensive experiments are performed to study Metasql on two
public NLIDB benchmarks. The results show that the performance of the
translation models can be effectively improved using Metasql.
\\ ( https://arxiv.org/abs/2402.17144 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17177 (*cross-listing*)
Date: Tue, 27 Feb 2024 03:30:58 GMT   (24962kb,D)

Title: Sora: A Review on Background, Technology, Limitations, and Opportunities
  of Large Vision Models
Authors: Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen,
  Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, Lifang He, and Lichao
  Sun
Categories: cs.CV cs.AI cs.LG
Comments: 37 pages, 18 figures; Our GitHub Homepage:
  https://github.com/lichao-sun/SoraReview
\\
  Sora is a text-to-video generative AI model, released by OpenAI in February
2024. The model is trained to generate videos of realistic or imaginative
scenes from text instructions and show potential in simulating the physical
world. Based on public technical reports and reverse engineering, this paper
presents a comprehensive review of the model's background, related
technologies, applications, remaining challenges, and future directions of
text-to-video AI models. We first trace Sora's development and investigate the
underlying technologies used to build this "world simulator". Then, we describe
in detail the applications and potential impact of Sora in multiple industries
ranging from film-making and education to marketing. We discuss the main
challenges and limitations that need to be addressed to widely deploy Sora,
such as ensuring safe and unbiased video generation. Lastly, we discuss the
future development of Sora and video generation models in general, and how
advancements in the field could enable new ways of human-AI interaction,
boosting productivity and creativity of video generation.
\\ ( https://arxiv.org/abs/2402.17177 ,  24962kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17191 (*cross-listing*)
Date: Tue, 27 Feb 2024 04:12:25 GMT   (771kb)

Title: AI-Driven Anonymization: Protecting Personal Data Privacy While
  Leveraging Machine Learning
Authors: Le Yang, Miao Tian, Duan Xin, Qishuo Cheng, Jiajian Zheng
Categories: cs.CR cs.AI cs.LG
Comments: 9 pages, 6 figures
\\
  The development of artificial intelligence has significantly transformed
people's lives. However, it has also posed a significant threat to privacy and
security, with numerous instances of personal information being exposed online
and reports of criminal attacks and theft. Consequently, the need to achieve
intelligent protection of personal information through machine learning
algorithms has become a paramount concern. Artificial intelligence leverages
advanced algorithms and technologies to effectively encrypt and anonymize
personal data, enabling valuable data analysis and utilization while
safeguarding privacy. This paper focuses on personal data privacy protection
and the promotion of anonymity as its core research objectives. It achieves
personal data privacy protection and detection through the use of machine
learning's differential privacy protection algorithm. The paper also addresses
existing challenges in machine learning related to privacy and personal data
protection, offers improvement suggestions, and analyzes factors impacting
datasets to enable timely personal data privacy detection and protection.
\\ ( https://arxiv.org/abs/2402.17191 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17213 (*cross-listing*)
Date: Tue, 27 Feb 2024 05:10:44 GMT   (1545kb,D)

Title: VCD: Knowledge Base Guided Visual Commonsense Discovery in Images
Authors: Xiangqing Shen, Yurun Song, Siwei Wu and Rui Xia
Categories: cs.CV cs.AI
\\
  Visual commonsense contains knowledge about object properties, relationships,
and behaviors in visual data. Discovering visual commonsense can provide a more
comprehensive and richer understanding of images, and enhance the reasoning and
decision-making capabilities of computer vision systems. However, the visual
commonsense defined in existing visual commonsense discovery studies is
coarse-grained and incomplete. In this work, we draw inspiration from a
commonsense knowledge base ConceptNet in natural language processing, and
systematically define the types of visual commonsense. Based on this, we
introduce a new task, Visual Commonsense Discovery (VCD), aiming to extract
fine-grained commonsense of different types contained within different objects
in the image. We accordingly construct a dataset (VCDD) from Visual Genome and
ConceptNet for VCD, featuring over 100,000 images and 14 million
object-commonsense pairs. We furthermore propose a generative model (VCDM) that
integrates a vision-language model with instruction tuning to tackle VCD.
Automatic and human evaluations demonstrate VCDM's proficiency in VCD,
particularly outperforming GPT-4V in implicit commonsense discovery. The value
of VCD is further demonstrated by its application to two downstream tasks,
including visual commonsense evaluation and visual question answering. The data
and code will be made available on GitHub.
\\ ( https://arxiv.org/abs/2402.17213 ,  1545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17216 (*cross-listing*)
Date: Tue, 27 Feb 2024 05:14:27 GMT   (579kb)

Title: Application of Machine Learning Optimization in Cloud Computing Resource
  Scheduling and Management
Authors: Yifan Zhang, Bo Liu, Yulu Gong, Jiaxin Huang, Jingyu Xu, Weixiang Wan
Categories: cs.DC cs.AI cs.LG
\\
  In recent years, cloud computing has been widely used. Cloud computing refers
to the centralized computing resources, users through the access to the
centralized resources to complete the calculation, the cloud computing center
will return the results of the program processing to the user. Cloud computing
is not only for individual users, but also for enterprise users. By purchasing
a cloud server, users do not have to buy a large number of computers, saving
computing costs. According to a report by China Economic News Network, the
scale of cloud computing in China has reached 209.1 billion yuan. At present,
the more mature cloud service providers in China are Ali Cloud, Baidu Cloud,
Huawei Cloud and so on. Therefore, this paper proposes an innovative approach
to solve complex problems in cloud computing resource scheduling and management
using machine learning optimization techniques. Through in-depth study of
challenges such as low resource utilization and unbalanced load in the cloud
environment, this study proposes a comprehensive solution, including
optimization methods such as deep learning and genetic algorithm, to improve
system performance and efficiency, and thus bring new breakthroughs and
progress in the field of cloud computing resource management.Rational
allocation of resources plays a crucial role in cloud computing. In the
resource allocation of cloud computing, the cloud computing center has limited
cloud resources, and users arrive in sequence. Each user requests the cloud
computing center to use a certain number of cloud resources at a specific time.
\\ ( https://arxiv.org/abs/2402.17216 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17245 (*cross-listing*)
Date: Tue, 27 Feb 2024 06:31:52 GMT   (29233kb,D)

Title: Playground v2.5: Three Insights towards Enhancing Aesthetic Quality in
  Text-to-Image Generation
Authors: Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, Suhail
  Doshi
Categories: cs.CV cs.AI
Comments: Model weights:
  https://huggingface.co/playgroundai/playground-v2.5-1024px-aesthetic
\\
  In this work, we share three insights for achieving state-of-the-art
aesthetic quality in text-to-image generative models. We focus on three
critical aspects for model improvement: enhancing color and contrast, improving
generation across multiple aspect ratios, and improving human-centric fine
details. First, we delve into the significance of the noise schedule in
training a diffusion model, demonstrating its profound impact on realism and
visual fidelity. Second, we address the challenge of accommodating various
aspect ratios in image generation, emphasizing the importance of preparing a
balanced bucketed dataset. Lastly, we investigate the crucial role of aligning
model outputs with human preferences, ensuring that generated images resonate
with human perceptual expectations. Through extensive analysis and experiments,
Playground v2.5 demonstrates state-of-the-art performance in terms of aesthetic
quality under various conditions and aspect ratios, outperforming both
widely-used open-source models like SDXL and Playground v2, and closed-source
commercial systems such as DALLE 3 and Midjourney v5.2. Our model is
open-source, and we hope the development of Playground v2.5 provides valuable
guidelines for researchers aiming to elevate the aesthetic quality of
diffusion-based image generation models.
\\ ( https://arxiv.org/abs/2402.17245 ,  29233kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17249 (*cross-listing*)
Date: Tue, 27 Feb 2024 06:47:52 GMT   (779kb,D)

Title: Deep Learning-Based Speech and Vision Synthesis to Improve Phishing
  Attack Detection through a Multi-layer Adaptive Framework
Authors: Tosin Ige, Christopher Kiekintveld, Aritran Piplai
Categories: cs.CR cs.AI cs.CV cs.LG
Comments: 8
\\
  The ever-evolving ways attacker continues to im prove their phishing
techniques to bypass existing state-of-the-art phishing detection methods pose
a mountain of challenges to researchers in both industry and academia research
due to the inability of current approaches to detect complex phishing attack.
Thus, current anti-phishing methods remain vulnerable to complex phishing
because of the increasingly sophistication tactics adopted by attacker coupled
with the rate at which new tactics are being developed to evade detection. In
this research, we proposed an adaptable framework that combines Deep learning
and Randon Forest to read images, synthesize speech from deep-fake videos, and
natural language processing at various predictions layered to significantly
increase the performance of machine learning models for phishing attack
detection.
\\ ( https://arxiv.org/abs/2402.17249 ,  779kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17285 (*cross-listing*)
Date: Tue, 27 Feb 2024 07:57:28 GMT   (4046kb,D)

Title: Enhancing Hyperspectral Images via Diffusion Model and Group-Autoencoder
  Super-resolution Network
Authors: Zhaoyang Wang, Dongyang Li, Mingyang Zhang, Hao Luo, Maoguo Gong
Categories: cs.CV cs.AI
Comments: Accepted by AAAI2024
\\
  Existing hyperspectral image (HSI) super-resolution (SR) methods struggle to
effectively capture the complex spectral-spatial relationships and low-level
details, while diffusion models represent a promising generative model known
for their exceptional performance in modeling complex relations and learning
high and low-level visual features. The direct application of diffusion models
to HSI SR is hampered by challenges such as difficulties in model convergence
and protracted inference time. In this work, we introduce a novel
Group-Autoencoder (GAE) framework that synergistically combines with the
diffusion model to construct a highly effective HSI SR model (DMGASR). Our
proposed GAE framework encodes high-dimensional HSI data into low-dimensional
latent space where the diffusion model works, thereby alleviating the
difficulty of training the diffusion model while maintaining band correlation
and considerably reducing inference time. Experimental results on both natural
and remote sensing hyperspectral datasets demonstrate that the proposed method
is superior to other state-of-the-art methods both visually and metrically.
\\ ( https://arxiv.org/abs/2402.17285 ,  4046kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17289 (*cross-listing*)
Date: Tue, 27 Feb 2024 08:02:48 GMT   (4930kb,D)

Title: Active propulsion noise shaping for multi-rotor aircraft localization
Authors: Serussi Gabriele, Shor Tamir, Hirshberg Tom, Baskin Chaim, Bronstein
  Alex
Categories: cs.RO cs.AI
\\
  Multi-rotor aerial autonomous vehicles (MAVs) primarily rely on vision for
navigation purposes. However, visual localization and odometry techniques
suffer from poor performance in low or direct sunlight, a limited field of
view, and vulnerability to occlusions. Acoustic sensing can serve as a
complementary or even alternative modality for vision in many situations, and
it also has the added benefits of lower system cost and energy footprint, which
is especially important for micro aircraft. This paper proposes actively
controlling and shaping the aircraft propulsion noise generated by the rotors
to benefit localization tasks, rather than considering it a harmful nuisance.
We present a neural network architecture for selfnoise-based localization in a
known environment. We show that training it simultaneously with learning
time-varying rotor phase modulation achieves accurate and robust localization.
The proposed methods are evaluated using a computationally affordable
simulation of MAV rotor noise in 2D acoustic environments that is fitted to
real recordings of rotor pressure fields.
\\ ( https://arxiv.org/abs/2402.17289 ,  4930kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17334 (*cross-listing*)
Date: Tue, 27 Feb 2024 09:10:41 GMT   (4854kb,D)

Title: BiVRec: Bidirectional View-based Multimodal Sequential Recommendation
Authors: Jiaxi Hu, Jingtong Gao, Xiangyu Zhao, Yuehong Hu, Yuxuan Liang, Yiqi
  Wang, Ming He, Zitao Liu, Hongzhi Yin
Categories: cs.IR cs.AI
\\
  The integration of multimodal information into sequential recommender systems
has attracted significant attention in recent research. In the initial stages
of multimodal sequential recommendation models, the mainstream paradigm was
ID-dominant recommendations, wherein multimodal information was fused as side
information. However, due to their limitations in terms of transferability and
information intrusion, another paradigm emerged, wherein multimodal features
were employed directly for recommendation, enabling recommendation across
datasets. Nonetheless, it overlooked user ID information, resulting in low
information utilization and high training costs. To this end, we propose an
innovative framework, BivRec, that jointly trains the recommendation tasks in
both ID and multimodal views, leveraging their synergistic relationship to
enhance recommendation performance bidirectionally. To tackle the information
heterogeneity issue, we first construct structured user interest
representations and then learn the synergistic relationship between them.
Specifically, BivRec comprises three modules: Multi-scale Interest Embedding,
comprehensively modeling user interests by expanding user interaction sequences
with multi-scale patching; Intra-View Interest Decomposition, constructing
highly structured interest representations using carefully designed Gaussian
attention and Cluster attention; and Cross-View Interest Learning, learning the
synergistic relationship between the two recommendation views through
coarse-grained overall semantic similarity and fine-grained interest allocation
similarity BiVRec achieves state-of-the-art performance on five datasets and
showcases various practical advantages.
\\ ( https://arxiv.org/abs/2402.17334 ,  4854kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17339 (*cross-listing*)
Date: Tue, 27 Feb 2024 09:13:27 GMT   (19131kb,D)

Title: SocialCVAE: Predicting Pedestrian Trajectory via Interaction Conditioned
  Latents
Authors: Wei Xiang, Haoteng Yin, He Wang, Xiaogang Jin
Categories: cs.CV cs.AI
Comments: Accepted by AAAI'24
\\
  Pedestrian trajectory prediction is the key technology in many applications
for providing insights into human behavior and anticipating human future
motions. Most existing empirical models are explicitly formulated by observed
human behaviors using explicable mathematical terms with a deterministic
nature, while recent work has focused on developing hybrid models combined with
learning-based techniques for powerful expressiveness while maintaining
explainability. However, the deterministic nature of the learned steering
behaviors from the empirical models limits the models' practical performance.
To address this issue, this work proposes the social conditional variational
autoencoder (SocialCVAE) for predicting pedestrian trajectories, which employs
a CVAE to explore behavioral uncertainty in human motion decisions. SocialCVAE
learns socially reasonable motion randomness by utilizing a socially
explainable interaction energy map as the CVAE's condition, which illustrates
the future occupancy of each pedestrian's local neighborhood area. The energy
map is generated using an energy-based interaction model, which anticipates the
energy cost (i.e., repulsion intensity) of pedestrians' interactions with
neighbors. Experimental results on two public benchmarks including 25 scenes
demonstrate that SocialCVAE significantly improves prediction accuracy compared
with the state-of-the-art methods, with up to 16.85% improvement in Average
Displacement Error (ADE) and 69.18% improvement in Final Displacement Error
(FDE).
\\ ( https://arxiv.org/abs/2402.17339 ,  19131kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17360 (*cross-listing*)
Date: Tue, 27 Feb 2024 09:53:16 GMT   (3616kb,D)

Title: CAPT: Category-level Articulation Estimation from a Single Point Cloud
  Using Transformer
Authors: Lian Fu, Ryoichi Ishikawa, Yoshihiro Sato and Takeshi Oishi
Categories: cs.CV cs.AI cs.RO
Comments: Accepted to ICRA 2024
\\
  The ability to estimate joint parameters is essential for various
applications in robotics and computer vision. In this paper, we propose CAPT:
category-level articulation estimation from a point cloud using Transformer.
CAPT uses an end-to-end transformer-based architecture for joint parameter and
state estimation of articulated objects from a single point cloud. The proposed
CAPT methods accurately estimate joint parameters and states for various
articulated objects with high precision and robustness. The paper also
introduces a motion loss approach, which improves articulation estimation
performance by emphasizing the dynamic features of articulated objects.
Additionally, the paper presents a double voting strategy to provide the
framework with coarse-to-fine parameter estimation. Experimental results on
several category datasets demonstrate that our methods outperform existing
alternatives for articulation estimation. Our research provides a promising
solution for applying Transformer-based architectures in articulated object
analysis.
\\ ( https://arxiv.org/abs/2402.17360 ,  3616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17376 (*cross-listing*)
Date: Tue, 27 Feb 2024 10:13:30 GMT   (3812kb,D)

Title: Accelerating Diffusion Sampling with Optimized Time Steps
Authors: Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze
  Xie, Zhenguo Li
Categories: cs.CV cs.AI cs.LG
Comments: Accepted to CVPR 2024. Under camera-ready revision
\\
  Diffusion probabilistic models (DPMs) have shown remarkable performance in
high-resolution image synthesis, but their sampling efficiency is still to be
desired due to the typically large number of sampling steps. Recent
advancements in high-order numerical ODE solvers for DPMs have enabled the
generation of high-quality images with much fewer sampling steps. While this is
a significant development, most sampling methods still employ uniform time
steps, which is not optimal when using a small number of steps. To address this
issue, we propose a general framework for designing an optimization problem
that seeks more appropriate time steps for a specific numerical ODE solver for
DPMs. This optimization problem aims to minimize the distance between the
ground-truth solution to the ODE and an approximate solution corresponding to
the numerical solver. It can be efficiently solved using the constrained trust
region method, taking less than $15$ seconds. Our extensive experiments on both
unconditional and conditional sampling using pixel- and latent-space DPMs
demonstrate that, when combined with the state-of-the-art sampling method
UniPC, our optimized time steps significantly improve image generation
performance in terms of FID scores for datasets such as CIFAR-10 and ImageNet,
compared to using uniform time steps.
\\ ( https://arxiv.org/abs/2402.17376 ,  3812kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17386 (*cross-listing*)
Date: Tue, 27 Feb 2024 10:26:25 GMT   (1781kb,D)

Title: A case study of sending graph neural networks back to the test bench for
  applications in high-energy particle physics
Authors: Emanuel Pfeffer and Michael Wa{\ss}mer and Yee-Ying Cung and Roger
  Wolf and Ulrich Husemann
Categories: hep-ph cs.AI hep-ex
\\
  In high-energy particle collisions, the primary collision products usually
decay further resulting in tree-like, hierarchical structures with a priori
unknown multiplicity. At the stable-particle level all decay products of a
collision form permutation invariant sets of final state objects. The analogy
to mathematical graphs gives rise to the idea that graph neural networks
(GNNs), which naturally resemble these properties, should be best-suited to
address many tasks related to high-energy particle physics. In this paper we
describe a benchmark test of a typical GNN against neural networks of the
well-established deep fully-connected feed-forward architecture. We aim at
performing this comparison maximally unbiased in terms of nodes, hidden layers,
or trainable parameters of the neural networks under study. As physics case we
use the classification of the final state X produced in association with top
quark-antiquark pairs in proton-proton collisions at the Large Hadron Collider
at CERN, where X stands for a bottom quark-antiquark pair produced either
non-resonantly or through the decay of an intermediately produced Z or Higgs
boson.
\\ ( https://arxiv.org/abs/2402.17386 ,  1781kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17398 (*cross-listing*)
Date: Tue, 27 Feb 2024 10:46:36 GMT   (2301kb,D)

Title: A Quantum Approach to Synthetic Minority Oversampling Technique (SMOTE)
Authors: Nishikanta Mohanty, Bikash K. Behera and Christopher Ferrie
Categories: quant-ph cs.AI cs.LG
Comments: 18 pages, 22 Figures, 2 Tables
\\
  The paper proposes the Quantum-SMOTE method, a novel solution that uses
quantum computing techniques to solve the prevalent problem of class imbalance
in machine learning datasets. Quantum-SMOTE, inspired by the Synthetic Minority
Oversampling Technique (SMOTE), generates synthetic data points using quantum
processes such as swap tests and quantum rotation. The process varies from the
conventional SMOTE algorithm's usage of K-Nearest Neighbors (KNN) and Euclidean
distances, enabling synthetic instances to be generated from minority class
data points without relying on neighbor proximity. The algorithm asserts
greater control over the synthetic data generation process by introducing
hyperparameters such as rotation angle, minority percentage, and splitting
factor, which allow for customization to specific dataset requirements. The
approach is tested on a public dataset of TelecomChurn and evaluated alongside
two prominent classification algorithms, Random Forest and Logistic Regression,
to determine its impact along with varying proportions of synthetic data.
\\ ( https://arxiv.org/abs/2402.17398 ,  2301kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17406 (*cross-listing*)
Date: Tue, 27 Feb 2024 10:55:07 GMT   (2034kb,D)

Title: LSPT: Long-term Spatial Prompt Tuning for Visual Representation Learning
Authors: Shentong Mo, Yansen Wang, Xufang Luo, Dongsheng Li
Categories: cs.CV cs.AI cs.LG
\\
  Visual Prompt Tuning (VPT) techniques have gained prominence for their
capacity to adapt pre-trained Vision Transformers (ViTs) to downstream visual
tasks using specialized learnable tokens termed as prompts. Contemporary VPT
methodologies, especially when employed with self-supervised vision
transformers, often default to the introduction of new learnable prompts or
gated prompt tokens predominantly sourced from the model's previous block. A
pivotal oversight in such approaches is their failure to harness the potential
of long-range previous blocks as sources of prompts within each self-supervised
ViT. To bridge this crucial gap, we introduce Long-term Spatial Prompt Tuning
(LSPT) - a revolutionary approach to visual representation learning. Drawing
inspiration from the intricacies of the human brain, LSPT ingeniously
incorporates long-term gated prompts. This feature serves as temporal coding,
curbing the risk of forgetting parameters acquired from earlier blocks. Further
enhancing its prowess, LSPT brings into play patch tokens, serving as spatial
coding. This is strategically designed to perpetually amass class-conscious
features, thereby fortifying the model's prowess in distinguishing and
identifying visual categories. To validate the efficacy of our proposed method,
we engaged in rigorous experimentation across 5 FGVC and 19 VTAB-1K benchmarks.
Our empirical findings underscore the superiority of LSPT, showcasing its
ability to set new benchmarks in visual prompt tuning performance.
\\ ( https://arxiv.org/abs/2402.17406 ,  2034kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17407 (*cross-listing*)
Date: Tue, 27 Feb 2024 10:57:07 GMT   (137kb,D)

Title: A Neural Rewriting System to Solve Algorithmic Problems
Authors: Flavio Petruzzellis, Alberto Testolin, Alessandro Sperduti
Categories: cs.NE cs.AI cs.CL
Comments: Preprint. Work in progress
\\
  Modern neural network architectures still struggle to learn algorithmic
procedures that require to systematically apply compositional rules to solve
out-of-distribution problem instances. In this work, we propose an original
approach to learn algorithmic tasks inspired by rewriting systems, a classic
framework in symbolic artificial intelligence. We show that a rewriting system
can be implemented as a neural architecture composed by specialized modules:
the Selector identifies the target sub-expression to process, the Solver
simplifies the sub-expression by computing the corresponding result, and the
Combiner produces a new version of the original expression by replacing the
sub-expression with the solution provided. We evaluate our model on three types
of algorithmic tasks that require simplifying symbolic formulas involving
lists, arithmetic, and algebraic expressions. We test the extrapolation
capabilities of the proposed architecture using formulas involving a higher
number of operands and nesting levels than those seen during training, and we
benchmark its performance against the Neural Data Router, a recent model
specialized for systematic generalization, and a state-of-the-art large
language model (GPT-4) probed with advanced prompting strategies.
\\ ( https://arxiv.org/abs/2402.17407 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17410 (*cross-listing*)
Date: Tue, 27 Feb 2024 11:01:58 GMT   (1871kb)

Title: A novel image space formalism of Fourier domain interpolation neural
  networks for noise propagation analysis
Authors: Peter Dawood, Felix Breuer, Istvan Homolya, Jannik Stebani, Maximilian
  Gram, Peter M. Jakob, Moritz Zaiss, Martin Blaimer
Categories: cs.CV cs.AI cs.LG physics.med-ph
\\
  Purpose: To develop an image space formalism of multi-layer convolutional
neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions
and analytically estimate noise propagation during CNN inference. Theory and
Methods: Nonlinear activations in the Fourier domain (also known as k-space)
using complex-valued Rectifier Linear Units are expressed as elementwise
multiplication with activation masks. This operation is transformed into a
convolution in the image space. After network training in k-space, this
approach provides an algebraic expression for the derivative of the
reconstructed image with respect to the aliased coil images, which serve as the
input tensors to the network in the image space. This allows the variance in
the network inference to be estimated analytically and to be used to describe
noise characteristics. Monte-Carlo simulations and numerical approaches based
on auto-differentiation were used for validation. The framework was tested on
retrospectively undersampled invivo brain images. Results: Inferences conducted
in the image domain are quasi-identical to inferences in the k-space,
underlined by corresponding quantitative metrics. Noise variance maps obtained
from the analytical expression correspond with those obtained via Monte-Carlo
simulations, as well as via an auto-differentiation approach. The noise
resilience is well characterized, as in the case of classical Parallel Imaging.
Komolgorov-Smirnov tests demonstrate Gaussian distributions of voxel magnitudes
in variance maps obtained via Monte-Carlo simulations. Conclusion: The
quasi-equivalent image space formalism for neural networks for k-space
interpolation enables fast and accurate description of the noise
characteristics during CNN inference, analogous to geometry-factor maps in
traditional parallel imaging methods.
\\ ( https://arxiv.org/abs/2402.17410 ,  1871kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17420 (*cross-listing*)
Date: Tue, 27 Feb 2024 11:23:39 GMT   (8362kb,D)

Title: PANDAS: Prototype-based Novel Class Discovery and Detection
Authors: Tyler L. Hayes, C\'esar R. de Souza, Namil Kim, Jiwon Kim, Riccardo
  Volpi, Diane Larlus
Categories: cs.CV cs.AI
\\
  Object detectors are typically trained once and for all on a fixed set of
classes. However, this closed-world assumption is unrealistic in practice, as
new classes will inevitably emerge after the detector is deployed in the wild.
In this work, we look at ways to extend a detector trained for a set of base
classes so it can i) spot the presence of novel classes, and ii) automatically
enrich its repertoire to be able to detect those newly discovered classes
together with the base ones. We propose PANDAS, a method for novel class
discovery and detection. It discovers clusters representing novel classes from
unlabeled data, and represents old and new classes with prototypes. During
inference, a distance-based classifier uses these prototypes to assign a label
to each detected object instance. The simplicity of our method makes it widely
applicable. We experimentally demonstrate the effectiveness of PANDAS on the
VOC 2012 and COCO-to-LVIS benchmarks. It performs favorably against the state
of the art for this task while being computationally more affordable.
\\ ( https://arxiv.org/abs/2402.17420 ,  8362kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17442 (*cross-listing*)
Date: Tue, 27 Feb 2024 11:57:28 GMT   (1433kb,D)

Title: Ansible Lightspeed: A Code Generation Service for IT Automation
Authors: Priyam Sahoo, Saurabh Pujar, Ganesh Nalawade, Richard Gebhardt, Louis
  Mandel, Luca Buratti
Categories: cs.SE cs.AI cs.PL
\\
  The availability of Large Language Models (LLMs) which can generate code, has
made it possible to create tools that improve developer productivity.
Integrated development environments or IDEs which developers use to write
software are often used as an interface to interact with LLMs. Although many
such tools have been released, almost all of them focus on general-purpose
programming languages. Domain-specific languages, such as those crucial for IT
automation, have not received much attention. Ansible is one such YAML-based IT
automation-specific language. Red Hat Ansible Lightspeed with IBM Watson Code
Assistant, further referred to as Ansible Lightspeed, is an LLM-based service
designed explicitly for natural language to Ansible code generation.
  In this paper, we describe the design and implementation of the Ansible
Lightspeed service and analyze feedback from thousands of real users. We
examine diverse performance indicators, classified according to both immediate
and extended utilization patterns along with user sentiments. The analysis
shows that the user acceptance rate of Ansible Lightspeed suggestions is higher
than comparable tools that are more general and not specific to a programming
language. This remains true even after we use much more stringent criteria for
what is considered an accepted model suggestion, discarding suggestions which
were heavily edited after being accepted. The relatively high acceptance rate
results in higher-than-expected user retention and generally positive user
feedback. This paper provides insights on how a comparatively small, dedicated
model performs on a domain-specific language and more importantly, how it is
received by users.
\\ ( https://arxiv.org/abs/2402.17442 ,  1433kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17456 (*cross-listing*)
Date: Tue, 27 Feb 2024 12:27:51 GMT   (785kb,D)

Title: A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to
  Assist Adolescent Cyberbullying Education
Authors: Michael A. Hedderich, Natalie N. Bazarova, Wenting Zou, Ryun Shim,
  Xinda Ma, Qian Yang
Categories: cs.HC cs.AI cs.CL
\\
  Cyberbullying harms teenagers' mental health, and teaching them upstanding
intervention is crucial. Wizard-of-Oz studies show chatbots can scale up
personalized and interactive cyberbullying education, but implementing such
chatbots is a challenging and delicate task. We created a no-code chatbot
design tool for K-12 teachers. Using large language models and prompt chaining,
our tool allows teachers to prototype bespoke dialogue flows and chatbot
utterances. In offering this tool, we explore teachers' distinctive needs when
designing chatbots to assist their teaching, and how chatbot design tools might
better support them. Our findings reveal that teachers welcome the tool
enthusiastically. Moreover, they see themselves as playwrights guiding both the
students' and the chatbot's behaviors, while allowing for some improvisation.
Their goal is to enable students to rehearse both desirable and undesirable
reactions to cyberbullying in a safe environment. We discuss the design
opportunities LLM-Chains offer for empowering teachers and the research
opportunities this work opens up.
\\ ( https://arxiv.org/abs/2402.17456 ,  785kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17467 (*cross-listing*)
Date: Tue, 27 Feb 2024 12:48:01 GMT   (294kb,D)

Title: Natural Language Processing Methods for Symbolic Music Generation and
  Information Retrieval: a Survey
Authors: Dinh-Viet-Toan Le, Louis Bigo, Mikaela Keller and Dorien Herremans
Categories: cs.IR cs.AI cs.SD eess.AS
Comments: 36 pages, 5 figures, 4 tables
\\
  Several adaptations of Transformers models have been developed in various
domains since its breakthrough in Natural Language Processing (NLP). This trend
has spread into the field of Music Information Retrieval (MIR), including
studies processing music data. However, the practice of leveraging NLP tools
for symbolic music data is not novel in MIR. Music has been frequently compared
to language, as they share several similarities, including sequential
representations of text and music. These analogies are also reflected through
similar tasks in MIR and NLP. This survey reviews NLP methods applied to
symbolic music generation and information retrieval studies following two axes.
We first propose an overview of representations of symbolic music adapted from
natural language sequential representations. Such representations are designed
by considering the specificities of symbolic music. These representations are
then processed by models. Such models, possibly originally developed for text
and adapted for symbolic music, are trained on various tasks. We describe these
models, in particular deep learning models, through different prisms,
highlighting music-specialized mechanisms. We finally present a discussion
surrounding the effective use of NLP tools for symbolic music data. This
includes technical issues regarding NLP methods and fundamental differences
between text and music, which may open several doors for further research into
more effectively adapting NLP tools to symbolic MIR.
\\ ( https://arxiv.org/abs/2402.17467 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17482 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:08:34 GMT   (708kb)

Title: Automated Classification of Phonetic Segments in Child Speech Using Raw
  Ultrasound Imaging
Authors: Saja Al Ani, Joanne Cleland, Ahmed Zoha
Categories: cs.SD cs.AI cs.CV eess.AS
Journal-ref: Proceedings of the 17th International Joint Conference on
  Biomedical Engineering Systems and Technologies - Volume 1: BIOIMAGING, 2024,
  pages 326-331
DOI: 10.5220/0012592700003657
\\
  Speech sound disorder (SSD) is defined as a persistent impairment in speech
sound production leading to reduced speech intelligibility and hindered verbal
communication. Early recognition and intervention of children with SSD and
timely referral to speech and language therapists (SLTs) for treatment are
crucial. Automated detection of speech impairment is regarded as an efficient
method for examining and screening large populations. This study focuses on
advancing the automatic diagnosis of SSD in early childhood by proposing a
technical solution that integrates ultrasound tongue imaging (UTI) with
deep-learning models. The introduced FusionNet model combines UTI data with the
extracted texture features to classify UTI. The overarching aim is to elevate
the accuracy and efficiency of UTI analysis, particularly for classifying
speech sounds associated with SSD. This study compared the FusionNet approach
with standard deep-learning methodologies, highlighting the excellent
improvement results of the FusionNet model in UTI classification and the
potential of multi-learning in improving UTI classification in speech therapy
clinics.
\\ ( https://arxiv.org/abs/2402.17482 ,  708kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17490 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:16:50 GMT   (300kb)

Title: The Mechanical Turkness: Tactical Media Art and the Critique of
  Corporate AI
Authors: Dejan Grba
Categories: cs.CY cs.AI
Comments: Matthes, J\"org, Damian Trilling, Ljubi\v{s}a Boji\'c and Simona
  \v{Z}iki\'c, eds. 2024. Navigating the Digital Age: An In-Depth Exploration
  into the Intersection of Modern Technologies and Societal Transformation.
  Vienna and Belgrade: Institute for Philosophy and Social Theory and
  University of Belgrade and Department of Communication, University of Vienna
\\
  The extensive industrialization of artificial intelligence (AI) since the
mid-2010s has increasingly motivated artists to address its economic and
sociopolitical consequences. In this chapter, I discuss interrelated art
practices that thematize creative agency, crowdsourced labor, and delegated
artmaking to reveal the social rootage of AI technologies and underline the
productive human roles in their development. I focus on works whose poetic
features indicate broader issues of contemporary AI-influenced science,
technology, economy, and society. By exploring the conceptual, methodological,
and ethical aspects of their effectiveness in disrupting the political regime
of corporate AI, I identify several problems that affect their tactical impact
and outline potential avenues for tackling the challenges and advancing the
field.
\\ ( https://arxiv.org/abs/2402.17490 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17496 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:22:47 GMT   (716kb,D)

Title: Emotional Voice Messages (EMOVOME) database: emotion recognition in
  spontaneous voice messages
Authors: Luc\'ia G\'omez Zaragoz\'a (1), Roc\'io del Amor (1), Elena Parra
  Vargas (1), Valery Naranjo (1), Mariano Alca\~niz Raya (1), Javier
  Mar\'in-Morales (1) ((1) HUMAN-tech Institute, Universitat Polit\`enica de
  Val\`encia, Valencia, Spain)
Categories: cs.SD cs.AI cs.CL eess.AS
Comments: 10 pages, 6 figures, submitted to Scientific Data
ACM-class: I.5.1; I.5.4; I.2.7
\\
  Emotional Voice Messages (EMOVOME) is a spontaneous speech dataset containing
999 audio messages from real conversations on a messaging app from 100 Spanish
speakers, gender balanced. Voice messages were produced in-the-wild conditions
before participants were recruited, avoiding any conscious bias due to
laboratory environment. Audios were labeled in valence and arousal dimensions
by three non-experts and two experts, which were then combined to obtain a
final label per dimension. The experts also provided an extra label
corresponding to seven emotion categories. To set a baseline for future
investigations using EMOVOME, we implemented emotion recognition models using
both speech and audio transcriptions. For speech, we used the standard eGeMAPS
feature set and support vector machines, obtaining 49.27% and 44.71% unweighted
accuracy for valence and arousal respectively. For text, we fine-tuned a
multilingual BERT model and achieved 61.15% and 47.43% unweighted accuracy for
valence and arousal respectively. This database will significantly contribute
to research on emotion recognition in the wild, while also providing a unique
natural and freely accessible resource for Spanish.
\\ ( https://arxiv.org/abs/2402.17496 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17510 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:50:34 GMT   (1576kb,D)

Title: Demonstrating and Reducing Shortcuts in Vision-Language Representation
  Learning
Authors: Maurits Bleeker, Mariya Hendriksen, Andrew Yates, Maarten de Rijke
Categories: cs.CV cs.AI
Comments: 25 pages
\\
  Vision-language models (VLMs) mainly rely on contrastive training to learn
general-purpose representations of images and captions. We focus on the
situation when one image is associated with several captions, each caption
containing both information shared among all captions and unique information
per caption about the scene depicted in the image. In such cases, it is unclear
whether contrastive losses are sufficient for learning task-optimal
representations that contain all the information provided by the captions or
whether the contrastive learning setup encourages the learning of a simple
shortcut that minimizes contrastive loss. We introduce synthetic shortcuts for
vision-language: a training and evaluation framework where we inject synthetic
shortcuts into image-text data. We show that contrastive VLMs trained from
scratch or fine-tuned with data containing these synthetic shortcuts mainly
learn features that represent the shortcut. Hence, contrastive losses are not
sufficient to learn task-optimal representations, i.e., representations that
contain all task-relevant information shared between the image and associated
captions. We examine two methods to reduce shortcut learning in our training
and evaluation framework: (i) latent target decoding and (ii) implicit feature
modification. We show empirically that both methods improve performance on the
evaluation task, but only partly reduce shortcut learning when training and
evaluating with our shortcut learning framework. Hence, we show the difficulty
and challenge of our shortcut learning framework for contrastive
vision-language representation learning.
\\ ( https://arxiv.org/abs/2402.17510 ,  1576kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17511 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:53:52 GMT   (2048kb,D)

Title: Rethinking Mutual Information for Language Conditioned Skill Discovery
  on Imitation Learning
Authors: Zhaoxun Ju, Chao Yang, Hongbo Wang, Yu Qiao and Fuchun Sun
Categories: cs.RO cs.AI
Comments: 16 pages
ACM-class: I.2.6
\\
  Language-conditioned robot behavior plays a vital role in executing complex
tasks by associating human commands or instructions with perception and
actions. The ability to compose long-horizon tasks based on unconstrained
language instructions necessitates the acquisition of a diverse set of
general-purpose skills. However, acquiring inherent primitive skills in a
coupled and long-horizon environment without external rewards or human
supervision presents significant challenges. In this paper, we evaluate the
relationship between skills and language instructions from a mathematical
perspective, employing two forms of mutual information within the framework of
language-conditioned policy learning. To maximize the mutual information
between language and skills in an unsupervised manner, we propose an end-to-end
imitation learning approach known as Language Conditioned Skill Discovery
(LCSD). Specifically, we utilize vector quantization to learn discrete latent
skills and leverage skill sequences of trajectories to reconstruct high-level
semantic instructions. Through extensive experiments on language-conditioned
robotic navigation and manipulation tasks, encompassing BabyAI, LORel, and
CALVIN, we demonstrate the superiority of our method over prior works. Our
approach exhibits enhanced generalization capabilities towards unseen tasks,
improved skill interpretability, and notably higher rates of task completion
success.
\\ ( https://arxiv.org/abs/2402.17511 ,  2048kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17531 (*cross-listing*)
Date: Tue, 27 Feb 2024 14:14:23 GMT   (337kb,D)

Title: Nissist: An Incident Mitigation Copilot based on Troubleshooting Guides
Authors: Kaikai An, Fangkai Yang, Liqun Li, Zhixing Ren, Hao Huang, Lu Wang, Pu
  Zhao, Yu Kang, Hua Ding, Qingwei Lin, Saravan Rajmohan, Qi Zhang
Categories: cs.SE cs.AI cs.CL
Comments: Work in progress
\\
  Effective incident management is pivotal for the smooth operation of
enterprises-level cloud services. In order to expedite incident mitigation,
service teams compile troubleshooting knowledge into Troubleshooting Guides
(TSGs) accessible to on-call engineers (OCEs). While automated pipelines are
enabled to resolve the most frequent and easy incidents, there still exist
complex incidents that require OCEs' intervention. However, TSGs are often
unstructured and incomplete, which requires manual interpretation by OCEs,
leading to on-call fatigue and decreased productivity, especially among
new-hire OCEs. In this work, we propose Nissist which leverages TSGs and
incident mitigation histories to provide proactive suggestions, reducing human
intervention. Leveraging Large Language Models (LLM), Nissist extracts insights
from unstructured TSGs and historical incident mitigation discussions, forming
a comprehensive knowledge base. Its multi-agent system design enhances
proficiency in precisely discerning user queries, retrieving relevant
information, and delivering systematic plans consecutively. Through our user
case and experiment, we demonstrate that Nissist significant reduce Time to
Mitigate (TTM) in incident mitigation, alleviating operational burdens on OCEs
and improving service reliability. Our demo is available at
https://aka.ms/nissist_demo.
\\ ( https://arxiv.org/abs/2402.17531 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17550 (*cross-listing*)
Date: Tue, 27 Feb 2024 14:44:11 GMT   (777kb,D)

Title: Emergency Caching: Coded Caching-based Reliable Map Transmission in
  Emergency Networks
Authors: Zeyu Tian, Lianming Xu, Liang Li, Li Wang, and Aiguo Fei
Categories: cs.NI cs.AI eess.SP
\\
  Many rescue missions demand effective perception and real-time decision
making, which highly rely on effective data collection and processing. In this
study, we propose a three-layer architecture of emergency caching networks
focusing on data collection and reliable transmission, by leveraging efficient
perception and edge caching technologies. Based on this architecture, we
propose a disaster map collection framework that integrates coded caching
technologies. Our framework strategically caches coded fragments of maps across
unmanned aerial vehicles (UAVs), fostering collaborative uploading for
augmented transmission reliability. Additionally, we establish a comprehensive
probability model to assess the effective recovery area of disaster maps.
Towards the goal of utility maximization, we propose a deep reinforcement
learning (DRL) based algorithm that jointly makes decisions about cooperative
UAVs selection, bandwidth allocation and coded caching parameter adjustment,
accommodating the real-time map updates in a dynamic disaster situation. Our
proposed scheme is more effective than the non-coding caching scheme, as
validated by simulation.
\\ ( https://arxiv.org/abs/2402.17550 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17645 (*cross-listing*)
Date: Tue, 27 Feb 2024 16:15:28 GMT   (3893kb,D)

Title: SongComposer: A Large Language Model for Lyric and Melody Composition in
  Song Generation
Authors: Shuangrui Ding, Zihan Liu, Xiaoyi Dong, Pan Zhang, Rui Qian, Conghui
  He, Dahua Lin, Jiaqi Wang
Categories: cs.SD cs.AI cs.CL eess.AS
Comments: project page: https://pjlab-songcomposer.github.io/ code:
  https://github.com/pjlab-songcomposer/songcomposer
\\
  We present SongComposer, an innovative LLM designed for song composition. It
could understand and generate melodies and lyrics in symbolic song
representations, by leveraging the capability of LLM. Existing music-related
LLM treated the music as quantized audio signals, while such implicit encoding
leads to inefficient encoding and poor flexibility. In contrast, we resort to
symbolic song representation, the mature and efficient way humans designed for
music, and enable LLM to explicitly compose songs like humans. In practice, we
design a novel tuple design to format lyric and three note attributes (pitch,
duration, and rest duration) in the melody, which guarantees the correct LLM
understanding of musical symbols and realizes precise alignment between lyrics
and melody. To impart basic music understanding to LLM, we carefully collected
SongCompose-PT, a large-scale song pretraining dataset that includes lyrics,
melodies, and paired lyrics-melodies in either Chinese or English. After
adequate pre-training, 10K carefully crafted QA pairs are used to empower the
LLM with the instruction-following capability and solve diverse tasks. With
extensive experiments, SongComposer demonstrates superior performance in
lyric-to-melody generation, melody-to-lyric generation, song continuation, and
text-to-song creation, outperforming advanced LLMs like GPT-4.
\\ ( https://arxiv.org/abs/2402.17645 ,  3893kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17652 (*cross-listing*)
Date: Tue, 27 Feb 2024 16:21:28 GMT   (661kb,D)

Title: Navigator: A Decentralized Scheduler for Latency-Sensitive ML Workflows
Authors: Yuting Yang, Andrea Merlina, Weijia Song, Tiancheng Yuan, Ken Birman,
  Roman Vitenberg
Categories: cs.DC cs.AI
\\
  We consider ML query processing in distributed systems where GPU-enabled
workers coordinate to execute complex queries: a computing style often seen in
applications that interact with users in support of image processing and
natural language processing. In such systems, coscheduling of GPU memory
management and task placement represents a promising opportunity. We propose
Navigator, a novel framework that unifies these functions to reduce job latency
while using resources efficiently, placing tasks where data dependencies will
be satisfied, collocating tasks from the same job (when this will not overload
the host or its GPU), and efficiently managing GPU memory. Comparison with
other state of the art schedulers shows a significant reduction in completion
times while requiring the same amount or even fewer resources. In one case,
just half the servers were needed for processing the same workload.
\\ ( https://arxiv.org/abs/2402.17652 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17736 (*cross-listing*)
Date: Tue, 27 Feb 2024 18:12:58 GMT   (978kb,D)

Title: Learning-Based Algorithms for Graph Searching Problems
Authors: Adela Frances DePavia, Erasmo Tani, Ali Vakilian
Categories: cs.DS cs.AI cs.LG
\\
  We consider the problem of graph searching with prediction recently
introduced by Banerjee et al. (2022). In this problem, an agent, starting at
some vertex $r$ has to traverse a (potentially unknown) graph $G$ to find a
hidden goal node $g$ while minimizing the total distance travelled. We study a
setting in which at any node $v$, the agent receives a noisy estimate of the
distance from $v$ to $g$. We design algorithms for this search task on unknown
graphs. We establish the first formal guarantees on unknown weighted graphs and
provide lower bounds showing that the algorithms we propose have optimal or
nearly-optimal dependence on the prediction error. Further, we perform
numerical experiments demonstrating that in addition to being robust to
adversarial error, our algorithms perform well in typical instances in which
the error is stochastic. Finally, we provide alternative simpler performance
bounds on the algorithms of Banerjee et al. (2022) for the case of searching on
a known graph, and establish new lower bounds for this setting.
\\ ( https://arxiv.org/abs/2402.17736 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17760 (*cross-listing*)
Date: Tue, 27 Feb 2024 18:53:18 GMT   (7422kb,D)

Title: Learning to Program Variational Quantum Circuits with Fast Weights
Authors: Samuel Yen-Chi Chen
Categories: quant-ph cs.AI cs.ET cs.LG cs.NE
\\
  Quantum Machine Learning (QML) has surfaced as a pioneering framework
addressing sequential control tasks and time-series modeling. It has
demonstrated empirical quantum advantages notably within domains such as
Reinforcement Learning (RL) and time-series prediction. A significant
advancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically
tailored for memory-intensive tasks encompassing partially observable
environments and non-linear time-series prediction. Nevertheless, QRNN-based
models encounter challenges, notably prolonged training duration stemming from
the necessity to compute quantum gradients using backpropagation-through-time
(BPTT). This predicament exacerbates when executing the complete model on
quantum devices, primarily due to the substantial demand for circuit evaluation
arising from the parameter-shift rule. This paper introduces the Quantum Fast
Weight Programmers (QFWP) as a solution to the temporal or sequential learning
challenge. The QFWP leverages a classical neural network (referred to as the
'slow programmer') functioning as a quantum programmer to swiftly modify the
parameters of a variational quantum circuit (termed the 'fast programmer').
Instead of completely overwriting the fast programmer at each time-step, the
slow programmer generates parameter changes or updates for the quantum circuit
parameters. This approach enables the fast programmer to incorporate past
observations or information. Notably, the proposed QFWP model achieves learning
of temporal dependencies without necessitating the use of quantum recurrent
neural networks. Numerical simulations conducted in this study showcase the
efficacy of the proposed QFWP model in both time-series prediction and RL
tasks. The model exhibits performance levels either comparable to or surpassing
those achieved by QLSTM-based models.
\\ ( https://arxiv.org/abs/2402.17760 ,  7422kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17767 (*cross-listing*)
Date: Tue, 27 Feb 2024 18:58:54 GMT   (14417kb,D)

Title: Opening Cabinets and Drawers in the Real World using a Commodity Mobile
  Manipulator
Authors: Arjun Gupta, Michelle Zhang, Rishik Sathua, Saurabh Gupta
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Project webpage:
  https://arjung128.github.io/opening-cabinets-and-drawers
\\
  Pulling open cabinets and drawers presents many difficult technical
challenges in perception (inferring articulation parameters for objects from
onboard sensors), planning (producing motion plans that conform to tight task
constraints), and control (making and maintaining contact while applying forces
on the environment). In this work, we build an end-to-end system that enables a
commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in
diverse previously unseen real world environments. We conduct 4 days of real
world testing of this system spanning 31 different objects from across 13
different real world environments. Our system achieves a success rate of 61% on
opening novel cabinets and drawers in unseen environments zero-shot. An
analysis of the failure modes suggests that errors in perception are the most
significant challenge for our system. We will open source code and models for
others to replicate and build upon our system.
\\ ( https://arxiv.org/abs/2402.17767 ,  14417kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17768 (*cross-listing*)
Date: Tue, 27 Feb 2024 18:59:18 GMT   (10588kb,D)

Title: Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning
Authors: Xiaoyu Zhang, Matthew Chang, Pranav Kumar, Saurabh Gupta
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: for project website with video, see
  https://sites.google.com/view/diffusion-meets-dagger
\\
  A common failure mode for policies trained with imitation is compounding
execution errors at test time. When the learned policy encounters states that
were not present in the expert demonstrations, the policy fails, leading to
degenerate behavior. The Dataset Aggregation, or DAgger approach to this
problem simply collects more data to cover these failure states. However, in
practice, this is often prohibitively expensive. In this work, we propose
Diffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without
the cost for eye-in-hand imitation learning problems. Instead of collecting new
samples to cover out-of-distribution states, DMD uses recent advances in
diffusion models to create these samples with diffusion models. This leads to
robust performance from few demonstrations. In experiments conducted for
non-prehensile pushing on a Franka Research 3, we show that DMD can achieve a
success rate of 80% with as few as 8 expert demonstrations, where naive
behavior cloning reaches only 20%. DMD also outperform competing NeRF-based
augmentation schemes by 50%.
\\ ( https://arxiv.org/abs/2402.17768 ,  10588kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16977 (*cross-listing*)
Date: Mon, 26 Feb 2024 19:19:47 GMT   (341kb,D)

Title: Dealing with Data for RE: Mitigating Challenges using NLP and Generative
  AI
Authors: Smita Ghaisas and Anmol Singhal
Categories: cs.SE cs.CL
Comments: 24 pages, 2 figures, to be published in NLP for Requirements
  Engineering Book
\\
  Across the dynamic business landscape today, enterprises face an
ever-increasing range of challenges. These include the constantly evolving
regulatory environment, the growing demand for personalization within software
applications, and the heightened emphasis on governance. In response to these
multifaceted demands, large enterprises have been adopting automation that
spans from the optimization of core business processes to the enhancement of
customer experiences. Indeed, Artificial Intelligence (AI) has emerged as a
pivotal element of modern software systems. In this context, data plays an
indispensable role. AI-centric software systems based on supervised learning
and operating at an industrial scale require large volumes of training data to
perform effectively. Moreover, the incorporation of generative AI has led to a
growing demand for adequate evaluation benchmarks. Our experience in this field
has revealed that the requirement for large datasets for training and
evaluation introduces a host of intricate challenges. This book chapter
explores the evolving landscape of Software Engineering (SE) in general, and
Requirements Engineering (RE) in particular, in this era marked by AI
integration. We discuss challenges that arise while integrating Natural
Language Processing (NLP) and generative AI into enterprise-critical software
systems. The chapter provides practical insights, solutions, and examples to
equip readers with the knowledge and tools necessary for effectively building
solutions with NLP at their cores. We also reflect on how these text
data-centric tasks sit together with the traditional RE process. We also
highlight new RE tasks that may be necessary for handling the increasingly
important text data-centricity involved in developing software systems.
\\ ( https://arxiv.org/abs/2402.16977 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17237 (*cross-listing*)
Date: Tue, 27 Feb 2024 06:11:54 GMT   (12747kb,D)

Title: Image-Text Matching with Multi-View Attention
Authors: Rui Cheng, Wanqing Cui
Categories: cs.CV cs.CL
\\
  Existing two-stream models for image-text matching show good performance
while ensuring retrieval speed and have received extensive attention from
industry and academia. These methods use a single representation to encode
image and text separately and get a matching score with cosine similarity or
the inner product of vectors. However, the performance of the two-stream model
is often sub-optimal. On the one hand, a single representation is challenging
to cover complex content comprehensively. On the other hand, in this framework
of lack of interaction, it is challenging to match multiple meanings which
leads to information being ignored. To address the problems mentioned above and
facilitate the performance of the two-stream model, we propose a multi-view
attention approach for two-stream image-text matching MVAM
(\textbf{M}ulti-\textbf{V}iew \textbf{A}ttention \textbf{M}odel). It first
learns multiple image and text representations by diverse attention heads with
different view codes. And then concatenate these representations into one for
matching. A diversity objective is also used to promote diversity between
attention heads. With this method, models are able to encode images and text
from different views and attend to more key points. So we can get
representations that contain more information. When doing retrieval tasks, the
matching scores between images and texts can be calculated from different
aspects, leading to better matching performance. Experiment results on MSCOCO
and Flickr30K show that our proposed model brings improvements over existing
models. Further case studies show that different attention heads can focus on
different contents and finally obtain a more comprehensive representation.
\\ ( https://arxiv.org/abs/2402.17237 ,  12747kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17505 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:44:09 GMT   (534kb,D)

Title: BASES: Large-scale Web Search User Simulation with Large Language Model
  based Agents
Authors: Ruiyang Ren, Peng Qiu, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Hua Wu,
  Ji-Rong Wen, Haifeng Wang
Categories: cs.IR cs.CL
\\
  Due to the excellent capacities of large language models (LLMs), it becomes
feasible to develop LLM-based agents for reliable user simulation. Considering
the scarcity and limit (e.g., privacy issues) of real user data, in this paper,
we conduct large-scale user simulation for web search, to improve the analysis
and modeling of user search behavior. Specially, we propose BASES, a novel user
simulation framework with LLM-based agents, designed to facilitate
comprehensive simulations of web search user behaviors. Our simulation
framework can generate unique user profiles at scale, which subsequently leads
to diverse search behaviors. To demonstrate the effectiveness of BASES, we
conduct evaluation experiments based on two human benchmarks in both Chinese
and English, demonstrating that BASES can effectively simulate large-scale
human-like search behaviors. To further accommodate the research on web search,
we develop WARRIORS, a new large-scale dataset encompassing web search user
behaviors, including both Chinese and English versions, which can greatly
bolster research in the field of information retrieval. Our code and data will
be publicly released soon.
\\ ( https://arxiv.org/abs/2402.17505 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17583 (*cross-listing*)
Date: Tue, 27 Feb 2024 15:14:19 GMT   (2128kb,D)

Title: FaultProfIT: Hierarchical Fault Profiling of Incident Tickets in
  Large-scale Cloud Systems
Authors: Junjie Huang, Jinyang Liu, Zhuangbin Chen, Zhihan Jiang, Yichen LI,
  Jiazhen Gu, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael R. Lyu
Categories: cs.SE cs.CL cs.LG
Comments: Accepted by Proceedings of the 46th International Conference on
  Software Engineering: Software Engineering in Practice (ICSE SEIP 2024)
DOI: 10.1145/3639477.3639754
\\
  Postmortem analysis is essential in the management of incidents within cloud
systems, which provides valuable insights to improve system's reliability and
robustness. At CloudA, fault pattern profiling is performed during the
postmortem phase, which involves the classification of incidents' faults into
unique categories, referred to as fault pattern. By aggregating and analyzing
these fault patterns, engineers can discern common faults, vulnerable
components and emerging fault trends. However, this process is currently
conducted by manual labeling, which has inherent drawbacks. On the one hand,
the sheer volume of incidents means only the most severe ones are analyzed,
causing a skewed overview of fault patterns. On the other hand, the complexity
of the task demands extensive domain knowledge, which leads to errors and
inconsistencies. To address these limitations, we propose an automated
approach, named FaultProfIT, for Fault pattern Profiling of Incident Tickets.
It leverages hierarchy-guided contrastive learning to train a hierarchy-aware
incident encoder and predicts fault patterns with enhanced incident
representations. We evaluate FaultProfIT using the production incidents from
CloudA. The results demonstrate that FaultProfIT outperforms state-of-the-art
methods. Our ablation study and analysis also verify the effectiveness of
hierarchy-guided contrastive learning. Additionally, we have deployed
FaultProfIT at CloudA for six months. To date, FaultProfIT has analyzed 10,000+
incidents from 30+ cloud services, successfully revealing several fault trends
that have informed system improvements.
\\ ( https://arxiv.org/abs/2402.17583 ,  2128kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16854 (*cross-listing*)
Date: Thu, 18 Jan 2024 21:45:12 GMT   (540kb)

Title: Attention Based Molecule Generation via Hierarchical Variational
  Autoencoder
Authors: Divahar Sivanesan
Categories: q-bio.BM cs.LG
\\
  Molecule generation is a task made very difficult by the complex ways in
which we represent molecules computationally. A common technique used in
molecular generative modeling is to use SMILES strings with recurrent neural
networks built into variational autoencoders - but these suffer from a myriad
of issues: vanishing gradients, long-range forgetting, and invalid molecules.
In this work, we show that by combining recurrent neural networks with
convolutional networks in a hierarchical manner, we are able to both extract
autoregressive information from SMILES strings while maintaining signal and
long-range dependencies. This allows for generations with very high validity
rates on the order of 95% when reconstructing known molecules. We also observe
an average Tanimoto similarity of .6 between test set and reconstructed
molecules, which suggests our method is able to map between SMILES strings and
their learned representations in a more effective way than prior works using
similar methods.
\\ ( https://arxiv.org/abs/2402.16854 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16858 (*cross-listing*)
Date: Fri, 19 Jan 2024 16:43:47 GMT   (1787kb,D)

Title: Pragmatic Goal-Oriented Communications under Semantic-Effectiveness
  Channel Errors
Authors: Tom\'as H\"uttebr\"aucker, Mohamed Sana and Emilio Calvanese Strinati
Categories: cs.IT cs.LG math.IT
Comments: Accepted for publication in 2024 IEEE Consumer Communications and
  Networking Conference
\\
  In forthcoming AI-assisted 6G networks, integrating semantic, pragmatic, and
goal-oriented communication strategies becomes imperative. This integration
will enable sensing, transmission, and processing of exclusively pertinent task
data, ensuring conveyed information possesses understandable, pragmatic
semantic significance, aligning with destination needs and goals. Without
doubt, no communication is error free. Within this context, besides errors
stemming from typical wireless communication dynamics, potential distortions
between transmitter-intended and receiver-interpreted meanings can emerge due
to limitations in semantic processing capabilities, as well as language and
knowledge representation disparities between transmitters and receivers. The
main contribution of this paper is two-fold. First, it proposes and details a
novel mathematical modeling of errors stemming from language mismatches at both
semantic and effectiveness levels. Second, it provides a novel algorithmic
solution to counteract these types of errors which leverages optimal transport
theory. Our numerical results show the potential of the proposed mechanism to
compensate for language mismatches, thereby enhancing the attainability of
reliable communication under noisy communication environments.
\\ ( https://arxiv.org/abs/2402.16858 ,  1787kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16865 (*cross-listing*)
Date: Sun, 21 Jan 2024 04:14:54 GMT   (837kb,D)

Title: Improve Robustness of Eye Disease Detection by including Learnable
  Probabilistic Discrete Latent Variables into Machine Learning Models
Authors: Anirudh Prabhakaran, YeKun Xiao, Ching-Yu Cheng, Dianbo Liu
Categories: eess.IV cs.CV cs.LG
Comments: This is a work in progress
\\
  Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a
significant public health challenge due to their prevalence and potential for
causing vision impairment. Early and accurate diagnosis is crucial for
effective treatment and management.In recent years, deep learning models have
emerged as powerful tools for analysing medical images, including ocular
imaging . However, challenges persist in model interpretability and uncertainty
estimation, which are critical for clinical decision-making. This study
introduces a novel application of GFlowOut, leveraging the probabilistic
framework of Generative Flow Networks (GFlowNets) to learn the posterior
distribution over dropout masks, for the classification and analysis of ocular
diseases using eye fundus images. We develop a robust and generalizable method
that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in
identifying various ocular conditions. This study employs a unique set of
dropout masks - none, random, bottomup, and topdown - to enhance model
performance in analyzing ocular images. Our results demonstrate that the
bottomup GFlowOut mask significantly improves accuracy, outperforming the
traditional dropout approach.
\\ ( https://arxiv.org/abs/2402.16865 ,  837kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16896 (*cross-listing*)
Date: Fri, 23 Feb 2024 22:48:29 GMT   (936kb,D)

Title: On Trojan Signatures in Large Language Models of Code
Authors: Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour
Categories: cs.CR cs.LG cs.SE
\\
  Trojan signatures, as described by Fields et al. (2021), are noticeable
differences in the distribution of the trojaned class parameters (weights) and
the non-trojaned class parameters of the trojaned model, that can be used to
detect the trojaned model. Fields et al. (2021) found trojan signatures in
computer vision classification tasks with image models, such as, Resnet,
WideResnet, Densenet, and VGG. In this paper, we investigate such signatures in
the classifier layer parameters of large language models of source code.
  Our results suggest that trojan signatures could not generalize to LLMs of
code. We found that trojaned code models are stubborn, even when the models
were poisoned under more explicit settings (finetuned with pre-trained weights
frozen). We analyzed nine trojaned models for two binary classification tasks:
clone and defect detection. To the best of our knowledge, this is the first
work to examine weight-based trojan signature revelation techniques for
large-language models of code and furthermore to demonstrate that detecting
trojans only from the weights in such models is a hard problem.
\\ ( https://arxiv.org/abs/2402.16896 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16907 (*cross-listing*)
Date: Sun, 25 Feb 2024 04:24:28 GMT   (16787kb,D)

Title: Diffusion Posterior Proximal Sampling for Image Restoration
Authors: Hongjie Wu, Linchao He, Mingqin Zhang, Dongdong Chen, Kunming Luo,
  Mengting Luo, Ji-Zhe Zhou, Hu Chen, Jiancheng Lv
Categories: eess.IV cs.CV cs.LG
\\
  Diffusion models have demonstrated remarkable efficacy in generating
high-quality samples. Existing diffusion-based image restoration algorithms
exploit pre-trained diffusion models to leverage data priors, yet they still
preserve elements inherited from the unconditional generation paradigm. These
strategies initiate the denoising process with pure white noise and incorporate
random noise at each generative step, leading to over-smoothed results. In this
paper, we introduce a refined paradigm for diffusion-based image restoration.
Specifically, we opt for a sample consistent with the measurement identity at
each generative step, exploiting the sampling selection as an avenue for output
stability and enhancement. Besides, we start the restoration process with an
initialization combined with the measurement signal, providing supplementary
information to better align the generative process. Extensive experimental
results and analyses validate the effectiveness of our proposed approach across
diverse image restoration tasks.
\\ ( https://arxiv.org/abs/2402.16907 ,  16787kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16908 (*cross-listing*)
Date: Sun, 25 Feb 2024 06:23:02 GMT   (3044kb)

Title: Local stochastic computing using memristor-enabled stochastic logics
Authors: Lekai Song, Pengyu Liu, Jingfang Pei, Yang Liu, Songwei Liu, Shengbo
  Wang, Leonard W. T. Ng, Tawfique Hasan, Kong-Pang Pun, Shuo Gao, Guohua Hu
Categories: cs.ET cond-mat.mtrl-sci cs.LG eess.IV
\\
  Stochastic computing offers a probabilistic approach to address challenges
posed by problems with uncertainty and noise in various fields, particularly
machine learning. The realization of stochastic computing, however, faces the
limitation of developing reliable stochastic logics. Here, we present
stochastic logics development using memristors. Specifically, we integrate
memristors into logic circuits to design the stochastic logics, wherein the
inherent stochasticity in memristor switching is harnessed to enable stochastic
number encoding and processing with well-regulated probabilities and
correlations. As a practical application of the stochastic logics, we design a
compact stochastic Roberts cross operator for edge detection. Remarkably, the
operator demonstrates exceptional contour and texture extractions, even in the
presence of 50% noise, and owning to the probabilistic nature and compact
design, the operator can consume 95% less computational costs required by
conventional binary computing. The results underscore the great potential of
our stochastic computing approach as a lightweight local solution to machine
learning challenges in autonomous driving, virtual reality, medical diagnosis,
industrial automation, and beyond.
\\ ( https://arxiv.org/abs/2402.16908 ,  3044kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16912 (*cross-listing*)
Date: Sun, 25 Feb 2024 16:45:39 GMT   (276kb)

Title: An Adversarial Robustness Benchmark for Enterprise Network Intrusion
  Detection
Authors: Jo\~ao Vitorino, Miguel Silva, Eva Maia, Isabel Pra\c{c}a
Categories: cs.CR cs.LG
Comments: 15 pages, 8 tables, 2 figures, FPS 2023 conference
\\
  As cyber-attacks become more sophisticated, improving the robustness of
Machine Learning (ML) models must be a priority for enterprises of all sizes.
To reliably compare the robustness of different ML models for cyber-attack
detection in enterprise computer networks, they must be evaluated in
standardized conditions. This work presents a methodical adversarial robustness
benchmark of multiple decision tree ensembles with constrained adversarial
examples generated from standard datasets. The robustness of regularly and
adversarially trained RF, XGB, LGBM, and EBM models was evaluated on the
original CICIDS2017 dataset, a corrected version of it designated as NewCICIDS,
and the HIKARI dataset, which contains more recent network traffic. NewCICIDS
led to models with a better performance, especially XGB and EBM, but RF and
LGBM were less robust against the more recent cyber-attacks of HIKARI. Overall,
the robustness of the models to adversarial cyber-attack examples was improved
without their generalization to regular traffic being affected, enabling a
reliable detection of suspicious activity without costly increases of false
alarms.
\\ ( https://arxiv.org/abs/2402.16912 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16930 (*cross-listing*)
Date: Mon, 26 Feb 2024 15:14:38 GMT   (734kb,D)

Title: TrustMol: Trustworthy Inverse Molecular Design via Alignment with
  Molecular Dynamics
Authors: Kevin Tirta Wijaya, Navid Ansari, Hans-Peter Seidel, Vahid Babaei
Categories: physics.chem-ph cs.LG q-bio.QM
\\
  Data-driven generation of molecules with desired properties, also known as
inverse molecular design (IMD), has attracted significant attention in recent
years. Despite the significant progress in the accuracy and diversity of
solutions, existing IMD methods lag behind in terms of trustworthiness. The
root issue is that the design process of these methods is increasingly more
implicit and indirect, and this process is also isolated from the native
forward process (NFP), the ground-truth function that models the molecular
dynamics. Following this insight, we propose TrustMol, an IMD method built to
be trustworthy. For this purpose, TrustMol relies on a set of technical
novelties including a new variational autoencoder network. Moreover, we propose
a latent-property pairs acquisition method to effectively navigate the
complexities of molecular latent optimization, a process that seems intuitive
yet challenging due to the high-frequency and discontinuous nature of molecule
space. TrustMol also integrates uncertainty-awareness into molecular latent
optimization. These lead to improvements in both explainability and reliability
of the IMD process. We validate the trustworthiness of TrustMol through a wide
range of experiments.
\\ ( https://arxiv.org/abs/2402.16930 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16936 (*cross-listing*)
Date: Mon, 26 Feb 2024 18:54:15 GMT   (26745kb,D)

Title: Disentangled 3D Scene Generation with Layout Learning
Authors: Dave Epstein, Ben Poole, Ben Mildenhall, Alexei A. Efros, Aleksander
  Holynski
Categories: cs.CV cs.LG
\\
  We introduce a method to generate 3D scenes that are disentangled into their
component objects. This disentanglement is unsupervised, relying only on the
knowledge of a large pretrained text-to-image model. Our key insight is that
objects can be discovered by finding parts of a 3D scene that, when rearranged
spatially, still produce valid configurations of the same scene. Concretely,
our method jointly optimizes multiple NeRFs from scratch - each representing
its own object - along with a set of layouts that composite these objects into
scenes. We then encourage these composited scenes to be in-distribution
according to the image generator. We show that despite its simplicity, our
approach successfully generates 3D scenes decomposed into individual objects,
enabling new capabilities in text-to-3D content creation. For results and an
interactive demo, see our project page at https://dave.ml/layoutlearning/
\\ ( https://arxiv.org/abs/2402.16936 ,  26745kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16978 (*cross-listing*)
Date: Mon, 26 Feb 2024 19:25:21 GMT   (162kb,D)

Title: An inexact Bregman proximal point method and its acceleration version
  for unbalanced optimal transport
Authors: Xiang Chen, Faqiang Wang, Jun Liu, Li Cui
Categories: math.OC cs.LG
\\
  The Unbalanced Optimal Transport (UOT) problem plays increasingly important
roles in computational biology, computational imaging and deep learning.
Scaling algorithm is widely used to solve UOT due to its convenience and good
convergence properties. However, this algorithm has lower accuracy for large
regularization parameters, and due to stability issues, small regularization
parameters can easily lead to numerical overflow. We address this challenge by
developing an inexact Bregman proximal point method for solving UOT. This
algorithm approximates the proximal operator using the Scaling algorithm at
each iteration. The algorithm (1) converges to the true solution of UOT, (2)
has theoretical guarantees and robust regularization parameter selection, (3)
mitigates numerical stability issues, and (4) can achieve comparable
computational complexity to the Scaling algorithm in specific practice.
Building upon this, we develop an accelerated version of inexact Bregman
proximal point method for solving UOT by using acceleration techniques of
Bregman proximal point method and provide theoretical guarantees and
experimental validation of convergence and acceleration.
\\ ( https://arxiv.org/abs/2402.16978 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16979 (*cross-listing*)
Date: Mon, 26 Feb 2024 19:27:00 GMT   (846kb,D)

Title: Algorithmic Arbitrariness in Content Moderation
Authors: Juan Felipe Gomez and Caio Vieira Machado and Lucas Monteiro Paes and
  Flavio P. Calmon
Categories: cs.CY cs.LG cs.SI
\\
  Machine learning (ML) is widely used to moderate online content. Despite its
scalability relative to human moderation, the use of ML introduces unique
challenges to content moderation. One such challenge is predictive
multiplicity: multiple competing models for content classification may perform
equally well on average, yet assign conflicting predictions to the same
content. This multiplicity can result from seemingly innocuous choices during
model development, such as random seed selection for parameter initialization.
We experimentally demonstrate how content moderation tools can arbitrarily
classify samples as toxic, leading to arbitrary restrictions on speech. We
discuss these findings in terms of human rights set out by the International
Covenant on Civil and Political Rights (ICCPR), namely freedom of expression,
non-discrimination, and procedural justice. We analyze (i) the extent of
predictive multiplicity among state-of-the-art LLMs used for detecting toxic
content; (ii) the disparate impact of this arbitrariness across social groups;
and (iii) how model multiplicity compares to unambiguous human classifications.
Our findings indicate that the up-scaled algorithmic moderation risks
legitimizing an algorithmic leviathan, where an algorithm disproportionately
manages human rights. To mitigate such risks, our study underscores the need to
identify and increase the transparency of arbitrariness in content moderation
applications. Since algorithmic content moderation is being fueled by pressing
social concerns, such as disinformation and hate speech, our discussion on
harms raises concerns relevant to policy debates. Our findings also contribute
to content moderation and intermediary liability laws being discussed and
passed in many countries, such as the Digital Services Act in the European
Union, the Online Safety Act in the United Kingdom, and the Fake News Bill in
Brazil.
\\ ( https://arxiv.org/abs/2402.16979 ,  846kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16990 (*cross-listing*)
Date: Mon, 26 Feb 2024 19:49:54 GMT   (2152kb,D)

Title: inGRASS: Incremental Graph Spectral Sparsification via
  Low-Resistance-Diameter Decomposition
Authors: Ali Aghdaei and Zhuo Feng
Categories: cs.DS cs.LG
Comments: Accepted on DAC 2024
\\
  This work presents inGRASS, a novel algorithm designed for incremental
spectral sparsification of large undirected graphs. The proposed inGRASS
algorithm is highly scalable and parallel-friendly, having a nearly-linear time
complexity for the setup phase and the ability to update the spectral
sparsifier in $O(\log N)$ time for each incremental change made to the original
graph with $N$ nodes. A key component in the setup phase of inGRASS is a
multilevel resistance embedding framework introduced for efficiently
identifying spectrally-critical edges and effectively detecting redundant ones,
which is achieved by decomposing the initial sparsifier into many node clusters
with bounded effective-resistance diameters leveraging a
low-resistance-diameter decomposition (LRD) scheme. The update phase of inGRASS
exploits low-dimensional node embedding vectors for efficiently estimating the
importance and uniqueness of each newly added edge. As demonstrated through
extensive experiments, inGRASS achieves up to over $200 \times$ speedups while
retaining comparable solution quality in incremental spectral sparsification of
graphs obtained from various datasets, such as circuit simulations, finite
element analysis, and social networks.
\\ ( https://arxiv.org/abs/2402.16990 ,  2152kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16991 (*cross-listing*)
Date: Mon, 26 Feb 2024 19:52:33 GMT   (6545kb,D)

Title: A Phase Transition in Diffusion Models Reveals the Hierarchical Nature
  of Data
Authors: Antonio Sclocchi, Alessandro Favero, Matthieu Wyart
Categories: stat.ML cond-mat.dis-nn cs.CV cs.LG
Comments: 21 pages, 16 figures
\\
  Understanding the structure of real data is paramount in advancing modern
deep-learning methodologies. Natural data such as images are believed to be
composed of features organised in a hierarchical and combinatorial manner,
which neural networks capture during learning. Recent advancements show that
diffusion models can generate high-quality images, hinting at their ability to
capture this underlying structure. We study this phenomenon in a hierarchical
generative model of data. We find that the backward diffusion process acting
after a time $t$ is governed by a phase transition at some threshold time,
where the probability of reconstructing high-level features, like the class of
an image, suddenly drops. Instead, the reconstruction of low-level features,
such as specific details of an image, evolves smoothly across the whole
diffusion process. This result implies that at times beyond the transition, the
class has changed but the generated sample may still be composed of low-level
elements of the initial image. We validate these theoretical insights through
numerical experiments on class-unconditional ImageNet diffusion models. Our
analysis characterises the relationship between time and scale in diffusion
models and puts forward generative models as powerful tools to model
combinatorial data properties.
\\ ( https://arxiv.org/abs/2402.16991 ,  6545kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16996 (*cross-listing*)
Date: Mon, 26 Feb 2024 20:04:01 GMT   (7142kb,D)

Title: Towards Decoding Brain Activity During Passive Listening of Speech
Authors: Mil\'an Andr\'as Fodor and Tam\'as G\'abor Csap\'o and Frigyes Viktor
  Arthur
Categories: cs.HC cs.LG cs.SD eess.AS q-bio.NC
Comments: 27 pages, 7 figures
\\
  The aim of the study is to investigate the complex mechanisms of speech
perception and ultimately decode the electrical changes in the brain accruing
while listening to speech. We attempt to decode heard speech from intracranial
electroencephalographic (iEEG) data using deep learning methods. The goal is to
aid the advancement of brain-computer interface (BCI) technology for speech
synthesis, and, hopefully, to provide an additional perspective on the
cognitive processes of speech perception. This approach diverges from the
conventional focus on speech production and instead chooses to investigate
neural representations of perceived speech. This angle opened up a complex
perspective, potentially allowing us to study more sophisticated neural
patterns. Leveraging the power of deep learning models, the research aimed to
establish a connection between these intricate neural activities and the
corresponding speech sounds. Despite the approach not having achieved a
breakthrough yet, the research sheds light on the potential of decoding neural
activity during speech perception. Our current efforts can serve as a
foundation, and we are optimistic about the potential of expanding and
improving upon this work to move closer towards more advanced BCIs, better
understanding of processes underlying perceived speech and its relation to
spoken speech.
\\ ( https://arxiv.org/abs/2402.16996 ,  7142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17036 (*cross-listing*)
Date: Mon, 26 Feb 2024 21:35:33 GMT   (1093kb,D)

Title: Iterated INLA for State and Parameter Estimation in Nonlinear Dynamical
  Systems
Authors: Rafael Anderka, Marc Peter Deisenroth and So Takao
Categories: stat.ML cs.LG
\\
  Data assimilation (DA) methods use priors arising from differential equations
to robustly interpolate and extrapolate data. Popular techniques such as
ensemble methods that handle high-dimensional, nonlinear PDE priors focus
mostly on state estimation, however can have difficulty learning the parameters
accurately. On the other hand, machine learning based approaches can naturally
learn the state and parameters, but their applicability can be limited, or
produce uncertainties that are hard to interpret. Inspired by the Integrated
Nested Laplace Approximation (INLA) method in spatial statistics, we propose an
alternative approach to DA based on iteratively linearising the dynamical
model. This produces a Gaussian Markov random field at each iteration, enabling
one to use INLA to infer the state and parameters. Our approach can be used for
arbitrary nonlinear systems, while retaining interpretability, and is
furthermore demonstrated to outperform existing methods on the DA task. By
providing a more nuanced approach to handling nonlinear PDE priors, our
methodology offers improved accuracy and robustness in predictions, especially
where data sparsity is prevalent.
\\ ( https://arxiv.org/abs/2402.17036 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17089 (*cross-listing*)
Date: Mon, 26 Feb 2024 23:56:11 GMT   (273kb,D)

Title: Learning high-dimensional targets by two-parameter models and gradient
  flow
Authors: Dmitry Yarotsky
Categories: stat.ML cs.LG
\\
  We explore the theoretical possibility of learning $d$-dimensional targets
with $W$-parameter models by gradient flow (GF) when $W<d$. Our main result
shows that if the targets are described by a particular $d$-dimensional
probability distribution, then there exist models with as few as two parameters
that can learn the targets with arbitrarily high success probability. On the
other hand, we show that for $W<d$ there is necessarily a large subset of
GF-non-learnable targets. In particular, the set of learnable targets is not
dense in $\mathbb R^d$, and any subset of $\mathbb R^d$ homeomorphic to the
$W$-dimensional sphere contains non-learnable targets. Finally, we observe that
the model in our main theorem on almost guaranteed two-parameter learning is
constructed using a hierarchical procedure and as a result is not expressible
by a single elementary function. We show that this limitation is essential in
the sense that such learnability can be ruled out for a large class of
elementary functions.
\\ ( https://arxiv.org/abs/2402.17089 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17106 (*cross-listing*)
Date: Tue, 27 Feb 2024 00:59:32 GMT   (1395kb,D)

Title: Dataset Fairness: Achievable Fairness on Your Data With Utility
  Guarantees
Authors: Muhammad Faaiz Taufiq, Jean-Francois Ton, Yang Liu
Categories: stat.ML cs.CY cs.LG
\\
  In machine learning fairness, training models which minimize disparity across
different sensitive groups often leads to diminished accuracy, a phenomenon
known as the fairness-accuracy trade-off. The severity of this trade-off
fundamentally depends on dataset characteristics such as dataset imbalances or
biases. Therefore using a uniform fairness requirement across datasets remains
questionable and can often lead to models with substantially low utility. To
address this, we present a computationally efficient approach to approximate
the fairness-accuracy trade-off curve tailored to individual datasets, backed
by rigorous statistical guarantees. By utilizing the You-Only-Train-Once (YOTO)
framework, our approach mitigates the computational burden of having to train
multiple models when approximating the trade-off curve. Moreover, we quantify
the uncertainty in our approximation by introducing confidence intervals around
this curve, offering a statistically grounded perspective on the acceptable
range of fairness violations for any given accuracy threshold. Our empirical
evaluation spanning tabular, image and language datasets underscores that our
approach provides practitioners with a principled framework for
dataset-specific fairness decisions across various data modalities.
\\ ( https://arxiv.org/abs/2402.17106 ,  1395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17108 (*cross-listing*)
Date: Tue, 27 Feb 2024 01:01:59 GMT   (46kb)

Title: Repeated Contracting with Multiple Non-Myopic Agents: Policy Regret and
  Limited Liability
Authors: Natalie Collina, Varun Gupta, Aaron Roth
Categories: cs.GT cs.DS cs.LG
\\
  We study a repeated contracting setting in which a Principal adaptively
chooses amongst $k$ Agents at each of $T$ rounds. The Agents are non-myopic,
and so a mechanism for the Principal induces a $T$-round extensive form game
amongst the Agents. We give several results aimed at understanding an
under-explored aspect of contract theory -- the game induced when choosing an
Agent to contract with. First, we show that this game admits a pure-strategy
\emph{non-responsive} equilibrium amongst the Agents -- informally an
equilibrium in which the Agent's actions depend on the history of realized
states of nature, but not on the history of each other's actions, and so avoids
the complexities of collusion and threats. Next, we show that if the Principal
selects Agents using a \emph{monotone} bandit algorithm, then for any concave
contract, in any such equilibrium, the Principal obtains no regret to
contracting with the best Agent in hindsight -- not just given their realized
actions, but also to the counterfactual world in which they had offered a
guaranteed $T$-round contract to the best Agent in hindsight, which would have
induced a different sequence of actions. Finally, we show that if the Principal
selects Agents using a monotone bandit algorithm which guarantees no
swap-regret, then the Principal can additionally offer only limited liability
contracts (in which the Agent never needs to pay the Principal) while getting
no-regret to the counterfactual world in which she offered a linear contract to
the best Agent in hindsight -- despite the fact that linear contracts are not
limited liability. We instantiate this theorem by demonstrating the existence
of a monotone no swap-regret bandit algorithm, which to our knowledge has not
previously appeared in the literature.
\\ ( https://arxiv.org/abs/2402.17108 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17143 (*cross-listing*)
Date: Tue, 27 Feb 2024 02:13:32 GMT   (1818kb,D)

Title: Energy-Efficient Scheduling with Predictions
Authors: Eric Balkanski and Noemie Perivier and Clifford Stein and Hao-Ting Wei
Categories: cs.DS cs.LG
\\
  An important goal of modern scheduling systems is to efficiently manage power
usage. In energy-efficient scheduling, the operating system controls the speed
at which a machine is processing jobs with the dual objective of minimizing
energy consumption and optimizing the quality of service cost of the resulting
schedule. Since machine-learned predictions about future requests can often be
learned from historical data, a recent line of work on learning-augmented
algorithms aims to achieve improved performance guarantees by leveraging
predictions. In particular, for energy-efficient scheduling, Bamas et. al.
[BamasMRS20] and Antoniadis et. al. [antoniadis2021novel] designed algorithms
with predictions for the energy minimization with deadlines problem and
achieved an improved competitive ratio when the prediction error is small while
also maintaining worst-case bounds even when the prediction error is
arbitrarily large.
  In this paper, we consider a general setting for energy-efficient scheduling
and provide a flexible learning-augmented algorithmic framework that takes as
input an offline and an online algorithm for the desired energy-efficient
scheduling problem. We show that, when the prediction error is small, this
framework gives improved competitive ratios for many different energy-efficient
scheduling problems, including energy minimization with deadlines, while also
maintaining a bounded competitive ratio regardless of the prediction error.
Finally, we empirically demonstrate that this framework achieves an improved
performance on real and synthetic datasets.
\\ ( https://arxiv.org/abs/2402.17143 ,  1818kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17148 (*cross-listing*)
Date: Tue, 27 Feb 2024 02:29:24 GMT   (431kb,D)

Title: Time series generation for option pricing on quantum computers using
  tensor network
Authors: Nozomu Kobayashi, Yoshiyuki Suimon, Koichi Miyamoto
Categories: quant-ph cs.LG q-fin.CP
Comments: 15 pages, 2 figures
\\
  Finance, especially option pricing, is a promising industrial field that
might benefit from quantum computing. While quantum algorithms for option
pricing have been proposed, it is desired to devise more efficient
implementations of costly operations in the algorithms, one of which is
preparing a quantum state that encodes a probability distribution of the
underlying asset price. In particular, in pricing a path-dependent option, we
need to generate a state encoding a joint distribution of the underlying asset
price at multiple time points, which is more demanding. To address these
issues, we propose a novel approach using Matrix Product State (MPS) as a
generative model for time series generation. To validate our approach, taking
the Heston model as a target, we conduct numerical experiments to generate time
series in the model. Our findings demonstrate the capability of the MPS model
to generate paths in the Heston model, highlighting its potential for
path-dependent option pricing on quantum computers.
\\ ( https://arxiv.org/abs/2402.17148 ,  431kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17196 (*cross-listing*)
Date: Tue, 27 Feb 2024 04:23:35 GMT   (27285kb,D)

Title: Prediction of the SYM-H Index Using a Bayesian Deep Learning Method with
  Uncertainty Quantification
Authors: Yasser Abduallah, Khalid A. Alobaid, Jason T. L. Wang, Haimin Wang,
  Vania K. Jordanova, Vasyl Yurchyshyn, Huseyin Cavus, Ju Jing
Categories: astro-ph.IM cs.LG
Comments: 28 pages, 8 figures
\\
  We propose a novel deep learning framework, named SYMHnet, which employs a
graph neural network and a bidirectional long short-term memory network to
cooperatively learn patterns from solar wind and interplanetary magnetic field
parameters for short-term forecasts of the SYM-H index based on 1-minute and
5-minute resolution data. SYMHnet takes, as input, the time series of the
parameters' values provided by NASA's Space Science Data Coordinated Archive
and predicts, as output, the SYM-H index value at time point t + w hours for a
given time point t where w is 1 or 2. By incorporating Bayesian inference into
the learning framework, SYMHnet can quantify both aleatoric (data) uncertainty
and epistemic (model) uncertainty when predicting future SYM-H indices.
Experimental results show that SYMHnet works well at quiet time and storm time,
for both 1-minute and 5-minute resolution data. The results also show that
SYMHnet generally performs better than related machine learning methods. For
example, SYMHnet achieves a forecast skill score (FSS) of 0.343 compared to the
FSS of 0.074 of a recent gradient boosting machine (GBM) method when predicting
SYM-H indices (1 hour in advance) in a large storm (SYM-H = -393 nT) using
5-minute resolution data. When predicting the SYM-H indices (2 hours in
advance) in the large storm, SYMHnet achieves an FSS of 0.553 compared to the
FSS of 0.087 of the GBM method. In addition, SYMHnet can provide results for
both data and model uncertainty quantification, whereas the related methods
cannot.
\\ ( https://arxiv.org/abs/2402.17196 ,  27285kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17215 (*cross-listing*)
Date: Tue, 27 Feb 2024 05:11:14 GMT   (112kb,D)

Title: Multidimensional unstructured sparse recovery via eigenmatrix
Authors: Lexing Ying
Categories: math.NA cs.LG cs.NA
Comments: arXiv admin note: substantial text overlap with arXiv:2311.16609
\\
  This note considers the multidimensional unstructured sparse recovery
problems. Examples include Fourier inversion and sparse deconvolution. The
eigenmatrix is a data-driven construction with desired approximate eigenvalues
and eigenvectors proposed for the one-dimensional problems. This note extends
the eigenmatrix approach to multidimensional problems. Numerical results are
provided to demonstrate the performance of the proposed method.
\\ ( https://arxiv.org/abs/2402.17215 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17229 (*cross-listing*)
Date: Tue, 27 Feb 2024 05:47:33 GMT   (9541kb,D)

Title: Preserving Fairness Generalization in Deepfake Detection
Authors: Li Lin, Xinan He, Yan Ju, Xin Wang, Feng Ding, Shu Hu
Categories: cs.CV cs.LG
Comments: Accepted by The IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR 2024)
\\
  Although effective deepfake detection models have been developed in recent
years, recent studies have revealed that these models can result in unfair
performance disparities among demographic groups, such as race and gender. This
can lead to particular groups facing unfair targeting or exclusion from
detection, potentially allowing misclassified deepfakes to manipulate public
opinion and undermine trust in the model. The existing method for addressing
this problem is providing a fair loss function. It shows good fairness
performance for intra-domain evaluation but does not maintain fairness for
cross-domain testing. This highlights the significance of fairness
generalization in the fight against deepfakes. In this work, we propose the
first method to address the fairness generalization problem in deepfake
detection by simultaneously considering features, loss, and optimization
aspects. Our method employs disentanglement learning to extract demographic and
domain-agnostic forgery features, fusing them to encourage fair learning across
a flattened loss landscape. Extensive experiments on prominent deepfake
datasets demonstrate our method's effectiveness, surpassing state-of-the-art
approaches in preserving fairness during cross-domain deepfake detection. The
code is available at https://github.com/Purdue-M2/Fairness-Generalization
\\ ( https://arxiv.org/abs/2402.17229 ,  9541kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17232 (*cross-listing*)
Date: Tue, 27 Feb 2024 05:57:45 GMT   (3804kb,D)

Title: Two-scale Neural Networks for Partial Differential Equations with Small
  Parameters
Authors: Qiao Zhuang, Chris Ziyi Yao, Zhongqiang Zhang, George Em Karniadakis
Categories: math.NA cs.LG cs.NA physics.comp-ph
MSC-class: 65N35, 35B25
ACM-class: I.2.6
\\
  We propose a two-scale neural network method for solving partial differential
equations (PDEs) with small parameters using physics-informed neural networks
(PINNs). We directly incorporate the small parameters into the architecture of
neural networks. The proposed method enables solving PDEs with small parameters
in a simple fashion, without adding Fourier features or other computationally
taxing searches of truncation parameters. Various numerical examples
demonstrate reasonable accuracy in capturing features of large derivatives in
the solutions caused by small parameters.
\\ ( https://arxiv.org/abs/2402.17232 ,  3804kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17236 (*cross-listing*)
Date: Tue, 27 Feb 2024 06:09:48 GMT   (1584kb)

Title: A Review of Data Mining in Personalized Education: Current Trends and
  Future Prospects
Authors: Zhang Xiong, Haoxuan Li, Zhuang Liu, Zhuofan Chen, Hao Zhou, Wenge
  Rong, Yuanxin Ouyang
Categories: cs.CY cs.LG
Comments: 25 pages, 5 figures
Journal-ref: Zhang Xiong, Haoxuan Li, Zhuang Liu, Zhuofan Chen, Hao Zhou, Wenge
  Rong, Yuanxin Ouyang. A Review of Data Mining in Personalized Education:
  Current Trends and Future Prospects. Frontiers of Digital Education, 2024
  ,1(1): 26-50
DOI: 10.3868/s110-009-024-0004-9
\\
  Personalized education, tailored to individual student needs, leverages
educational technology and artificial intelligence (AI) in the digital age to
enhance learning effectiveness. The integration of AI in educational platforms
provides insights into academic performance, learning preferences, and
behaviors, optimizing the personal learning process. Driven by data mining
techniques, it not only benefits students but also provides educators and
institutions with tools to craft customized learning experiences. To offer a
comprehensive review of recent advancements in personalized educational data
mining, this paper focuses on four primary scenarios: educational
recommendation, cognitive diagnosis, knowledge tracing, and learning analysis.
This paper presents a structured taxonomy for each area, compiles commonly used
datasets, and identifies future research directions, emphasizing the role of
data mining in enhancing personalized education and paving the way for future
exploration and innovation.
\\ ( https://arxiv.org/abs/2402.17236 ,  1584kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17246 (*cross-listing*)
Date: Tue, 27 Feb 2024 06:32:56 GMT   (2946kb,D)

Title: SDR-Former: A Siamese Dual-Resolution Transformer for Liver Lesion
  Classification Using 3D Multi-Phase Imaging
Authors: Meng Lou, Hanning Ying, Xiaoqing Liu, Hong-Yu Zhou, Yuqing Zhang,
  Yizhou Yu
Categories: eess.IV cs.CV cs.LG
Comments: 13 pages, 7 figures
\\
  Automated classification of liver lesions in multi-phase CT and MR scans is
of clinical significance but challenging. This study proposes a novel Siamese
Dual-Resolution Transformer (SDR-Former) framework, specifically designed for
liver lesion classification in 3D multi-phase CT and MR imaging with varying
phase counts. The proposed SDR-Former utilizes a streamlined Siamese Neural
Network (SNN) to process multi-phase imaging inputs, possessing robust feature
representations while maintaining computational efficiency. The weight-sharing
feature of the SNN is further enriched by a hybrid Dual-Resolution Transformer
(DR-Former), comprising a 3D Convolutional Neural Network (CNN) and a tailored
3D Transformer for processing high- and low-resolution images, respectively.
This hybrid sub-architecture excels in capturing detailed local features and
understanding global contextual information, thereby, boosting the SNN's
feature extraction capabilities. Additionally, a novel Adaptive Phase Selection
Module (APSM) is introduced, promoting phase-specific intercommunication and
dynamically adjusting each phase's influence on the diagnostic outcome. The
proposed SDR-Former framework has been validated through comprehensive
experiments on two clinical datasets: a three-phase CT dataset and an
eight-phase MR dataset. The experimental results affirm the efficacy of the
proposed framework. To support the scientific community, we are releasing our
extensive multi-phase MR dataset for liver lesion analysis to the public. This
pioneering dataset, being the first publicly available multi-phase MR dataset
in this field, also underpins the MICCAI LLD-MMRI Challenge. The dataset is
accessible at:https://bit.ly/3IyYlgN.
\\ ( https://arxiv.org/abs/2402.17246 ,  2946kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17295 (*cross-listing*)
Date: Tue, 27 Feb 2024 08:16:17 GMT   (1180kb,D)

Title: Quantum Distance Approximation for Persistence Diagrams
Authors: Bernardo Ameneyro, Rebekah Herrman, George Siopsis, Vasileios Maroulas
Categories: quant-ph cs.LG
Comments: 24 pages, 11 figures, submitted to SIAM Journal on Computing
\\
  Topological Data Analysis methods can be useful for classification and
clustering tasks in many different fields as they can provide two dimensional
persistence diagrams that summarize important information about the shape of
potentially complex and high dimensional data sets. The space of persistence
diagrams can be endowed with various metrics such as the Wasserstein distance
which admit a statistical structure and allow to use these summaries for
machine learning algorithms. However, computing the distance between two
persistence diagrams involves finding an optimal way to match the points of the
two diagrams and may not always be an easy task for classical computers. In
this work we explore the potential of quantum computers to estimate the
distance between persistence diagrams, in particular we propose variational
quantum algorithms for the Wasserstein distance as well as the $d^{c}_{p}$
distance. Our implementation is a weighted version of the Quantum Approximate
Optimization Algorithm that relies on control clauses to encode the constraints
of the optimization problem.
\\ ( https://arxiv.org/abs/2402.17295 ,  1180kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17317 (*cross-listing*)
Date: Tue, 27 Feb 2024 08:49:30 GMT   (3860kb,D)

Title: How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced
  Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation
Authors: Andr\'e Ferreira, Naida Solak, Jianning Li, Philipp Dammann, Jens
  Kleesiek, Victor Alves, Jan Egger
Categories: eess.IV cs.CV cs.LG
\\
  Deep Learning is the state-of-the-art technology for segmenting brain
tumours. However, this requires a lot of high-quality data, which is difficult
to obtain, especially in the medical field. Therefore, our solutions address
this problem by using unconventional mechanisms for data augmentation.
Generative adversarial networks and registration are used to massively increase
the amount of available samples for training three different deep learning
models for brain tumour segmentation, the first task of the BraTS2023
challenge. The first model is the standard nnU-Net, the second is the Swin
UNETR and the third is the winning solution of the BraTS 2021 Challenge. The
entire pipeline is built on the nnU-Net implementation, except for the
generation of the synthetic data. The use of convolutional algorithms and
transformers is able to fill each other's knowledge gaps. Using the new metric,
our best solution achieves the dice results 0.9005, 0.8673, 0.8509 and HD95
14.940, 14.467, 17.699 (whole tumour, tumour core and enhancing tumour) in the
validation set.
\\ ( https://arxiv.org/abs/2402.17317 ,  3860kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17318 (*cross-listing*)
Date: Tue, 27 Feb 2024 08:50:45 GMT   (283kb,D)

Title: Scaling Supervised Local Learning with Augmented Auxiliary Networks
Authors: Chenxiang Ma, Jibin Wu, Chenyang Si, Kay Chen Tan
Categories: cs.NE cs.CV cs.LG
Comments: Accepted by ICLR 2024
\\
  Deep neural networks are typically trained using global error signals that
backpropagate (BP) end-to-end, which is not only biologically implausible but
also suffers from the update locking problem and requires huge memory
consumption. Local learning, which updates each layer independently with a
gradient-isolated auxiliary network, offers a promising alternative to address
the above problems. However, existing local learning methods are confronted
with a large accuracy gap with the BP counterpart, particularly for large-scale
networks. This is due to the weak coupling between local layers and their
subsequent network layers, as there is no gradient communication across layers.
To tackle this issue, we put forward an augmented local learning method, dubbed
AugLocal. AugLocal constructs each hidden layer's auxiliary network by
uniformly selecting a small subset of layers from its subsequent network layers
to enhance their synergy. We also propose to linearly reduce the depth of
auxiliary networks as the hidden layer goes deeper, ensuring sufficient network
capacity while reducing the computational cost of auxiliary networks. Our
extensive experiments on four image classification datasets (i.e., CIFAR-10,
SVHN, STL-10, and ImageNet) demonstrate that AugLocal can effectively scale up
to tens of local layers with a comparable accuracy to BP-trained networks while
reducing GPU memory usage by around 40%. The proposed AugLocal method,
therefore, opens up a myriad of opportunities for training high-performance
deep neural networks on resource-constrained platforms.Code is available at
https://github.com/ChenxiangMA/AugLocal.
\\ ( https://arxiv.org/abs/2402.17318 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17336 (*cross-listing*)
Date: Tue, 27 Feb 2024 09:11:10 GMT   (590kb,D)

Title: Outdoor Environment Reconstruction with Deep Learning on Radio
  Propagation Paths
Authors: Hrant Khachatrian, Rafayel Mkrtchyan, Theofanis P. Raptis
Categories: cs.NI cs.LG eess.SP
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. Work partly supported by the RA Science Committee grant
  No. 22rl-052 (DISTAL) and the EU under Italian National Recovery and
  Resilience Plan of NextGenerationEU on "Telecommunications of the Future"
  (PE00000001 - program "RESTART")
\\
  Conventional methods for outdoor environment reconstruction rely
predominantly on vision-based techniques like photogrammetry and LiDAR, facing
limitations such as constrained coverage, susceptibility to environmental
conditions, and high computational and energy demands. These challenges are
particularly pronounced in applications like augmented reality navigation,
especially when integrated with wearable devices featuring constrained
computational resources and energy budgets. In response, this paper proposes a
novel approach harnessing ambient wireless signals for outdoor environment
reconstruction. By analyzing radio frequency (RF) data, the paper aims to
deduce the environmental characteristics and digitally reconstruct the outdoor
surroundings. Investigating the efficacy of selected deep learning (DL)
techniques on the synthetic RF dataset WAIR-D, the study endeavors to address
the research gap in this domain. Two DL-driven approaches are evaluated
(convolutional U-Net and CLIP+ based on vision transformers), with performance
assessed using metrics like intersection-over-union (IoU), Hausdorff distance,
and Chamfer distance. The results demonstrate promising performance of the
RF-based reconstruction method, paving the way towards lightweight and scalable
reconstruction solutions.
\\ ( https://arxiv.org/abs/2402.17336 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17346 (*cross-listing*)
Date: Tue, 27 Feb 2024 09:27:54 GMT   (3169kb,D)

Title: Understanding the training of PINNs for unsteady flow past a plunging
  foil through the lens of input subdomain level loss function gradients
Authors: Rahul Sundar, Didier Lucor, and Sunetra Sarkar
Categories: physics.flu-dyn cs.LG
\\
  Recently immersed boundary method-inspired physics-informed neural networks
(PINNs) including the moving boundary-enabled PINNs (MB-PINNs) have shown the
ability to accurately reconstruct velocity and recover pressure as a hidden
variable for unsteady flow past moving bodies. Considering flow past a plunging
foil, MB-PINNs were trained with global physics loss relaxation and also in
conjunction with a physics-based undersampling method, obtaining good accuracy.
The purpose of this study was to investigate which input spatial subdomain
contributes to the training under the effect of physics loss relaxation and
physics-based undersampling. In the context of MB-PINNs training, three spatial
zones: the moving body, wake, and outer zones were defined. To quantify which
spatial zone drives the training, two novel metrics are computed from the zonal
loss component gradient statistics and the proportion of sample points in each
zone. Results confirm that the learning indeed depends on the combined effect
of the zonal loss component gradients and the proportion of points in each
zone. Moreover, the dominant input zones are also the ones that have the
strongest solution gradients in some sense.
\\ ( https://arxiv.org/abs/2402.17346 ,  3169kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17363 (*cross-listing*)
Date: Tue, 27 Feb 2024 09:55:34 GMT   (10717kb,D)

Title: CGGM: A conditional graph generation model with adaptive sparsity for
  node anomaly detection in IoT networks
Authors: Xianshi Su, Munan Li, Tongbang Jiang, Hao Long
Categories: cs.RO cs.LG
Comments: 13 pages, 19 figures
\\
  Dynamic graphs are extensively employed for detecting anomalous behavior in
nodes within the Internet of Things (IoT). Generative models are often used to
address the issue of imbalanced node categories in dynamic graphs.
Nevertheless, the constraints it faces include the monotonicity of adjacency
relationships, the difficulty in constructing multi-dimensional features for
nodes, and the lack of a method for end-to-end generation of multiple
categories of nodes. This paper presents a novel graph generation model, called
CGGM, designed specifically to generate a larger number of nodes belonging to
the minority class. The mechanism for generating an adjacency matrix, through
adaptive sparsity, enhances flexibility in its structure. The feature
generation module, called multidimensional features generator (MFG) to generate
node features along with topological information. Labels are transformed into
embedding vectors, serving as conditional constraints to control the generation
of synthetic data across multiple categories. Using a multi-stage loss, the
distribution of synthetic data is adjusted to closely resemble that of real
data. In extensive experiments, we show that CGGM's synthetic data outperforms
state-of-the-art methods across various metrics. Our results demonstrate
efficient generation of diverse data categories, robustly enhancing
multi-category classification model performance.
\\ ( https://arxiv.org/abs/2402.17363 ,  10717kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17375 (*cross-listing*)
Date: Tue, 27 Feb 2024 10:12:47 GMT   (4485kb,D)

Title: Impact of Computation in Integral Reinforcement Learning for
  Continuous-Time Control
Authors: Wenhan Cao, Wei Pan
Categories: eess.SY cs.LG cs.SY
\\
  Integral reinforcement learning (IntRL) demands the precise computation of
the utility function's integral at its policy evaluation (PEV) stage. This is
achieved through quadrature rules, which are weighted sums of utility functions
evaluated from state samples obtained in discrete time. Our research reveals a
critical yet underexplored phenomenon: the choice of the computational method
-- in this case, the quadrature rule -- can significantly impact control
performance. This impact is traced back to the fact that computational errors
introduced in the PEV stage can affect the policy iteration's convergence
behavior, which in turn affects the learned controller. To elucidate how
computation impacts control, we draw a parallel between IntRL's policy
iteration and Newton's method applied to the Hamilton-Jacobi-Bellman equation.
In this light, computational error in PEV manifests as an extra error term in
each iteration of Newton's method, with its upper bound proportional to the
computational error. Further, we demonstrate that when the utility function
resides in a reproducing kernel Hilbert space (RKHS), the optimal quadrature is
achievable by employing Bayesian quadrature with the RKHS-inducing kernel
function. We prove that the local convergence rates for IntRL using the
trapezoidal rule and Bayesian quadrature with a Mat\'ern kernel to be
$O(N^{-2})$ and $O(N^{-b})$, where $N$ is the number of evenly-spaced samples
and $b$ is the Mat\'ern kernel's smoothness parameter. These theoretical
findings are finally validated by two canonical control tasks.
\\ ( https://arxiv.org/abs/2402.17375 ,  4485kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17402 (*cross-listing*)
Date: Tue, 27 Feb 2024 10:48:56 GMT   (4174kb,D)

Title: Beacon, a lightweight deep reinforcement learning benchmark library for
  flow control
Authors: Jonathan Viquerat and Philippe Meliga and Pablo Jeken and Elie Hachem
Categories: physics.comp-ph cs.LG cs.SY eess.SY
\\
  Recently, the increasing use of deep reinforcement learning for flow control
problems has led to a new area of research, focused on the coupling and the
adaptation of the existing algorithms to the control of numerical fluid
dynamics environments. Although still in its infancy, the field has seen
multiple successes in a short time span, and its fast development pace can
certainly be partly imparted to the open-source effort that drives the
expansion of the community. Yet, this emerging domain still misses a common
ground to (i) ensure the reproducibility of the results, and (ii) offer a
proper ad-hoc benchmarking basis. To this end, we propose Beacon, an
open-source benchmark library composed of seven lightweight 1D and 2D flow
control problems with various characteristics, action and observation space
characteristics, and CPU requirements. In this contribution, the seven
considered problems are described, and reference control solutions are
provided. The sources for the following work are available at
https://github.com/jviquerat/beacon.
\\ ( https://arxiv.org/abs/2402.17402 ,  4174kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17470 (*cross-listing*)
Date: Tue, 27 Feb 2024 12:52:44 GMT   (3792kb,D)

Title: Bit Distribution Study and Implementation of Spatial Quality Map in the
  JPEG-AI Standardization
Authors: Panqi Jia, Jue Mao, Esin Koyuncu, A. Burakhan Koyuncu, Timofey
  Solovyev, Alexander Karabutov, Yin Zhao, Elena Alshina, Andre Kaup
Categories: cs.CV cs.LG eess.IV
Comments: 5 pages, 3 figures, 4 tables
\\
  Currently, there is a high demand for neural network-based image compression
codecs. These codecs employ non-linear transforms to create compact bit
representations and facilitate faster coding speeds on devices compared to the
hand-crafted transforms used in classical frameworks. The scientific and
industrial communities are highly interested in these properties, leading to
the standardization effort of JPEG-AI. The JPEG-AI verification model has been
released and is currently under development for standardization. Utilizing
neural networks, it can outperform the classic codec VVC intra by over 10%
BD-rate operating at base operation point. Researchers attribute this success
to the flexible bit distribution in the spatial domain, in contrast to VVC
intra's anchor that is generated with a constant quality point. However, our
study reveals that VVC intra displays a more adaptable bit distribution
structure through the implementation of various block sizes. As a result of our
observations, we have proposed a spatial bit allocation method to optimize the
JPEG-AI verification model's bit distribution and enhance the visual quality.
Furthermore, by applying the VVC bit distribution strategy, the objective
performance of JPEG-AI verification mode can be further improved, resulting in
a maximum gain of 0.45 dB in PSNR-Y.
\\ ( https://arxiv.org/abs/2402.17470 ,  3792kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17487 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:12:18 GMT   (880kb,D)

Title: Bit Rate Matching Algorithm Optimization in JPEG-AI Verification Model
Authors: Panqi Jia, A. Burakhan Koyuncu, Jue Mao, Ze Cui, Yi Ma, Tiansheng Guo,
  Timofey Solovyev, Alexander Karabutov, Yin Zhao, Jing Wang, Elena Alshina,
  Andre Kaup
Categories: cs.CV cs.LG eess.IV
Comments: Accepted at (IEEE) PCS 2024; 6 pages
\\
  The research on neural network (NN) based image compression has shown
superior performance compared to classical compression frameworks. Unlike the
hand-engineered transforms in the classical frameworks, NN-based models learn
the non-linear transforms providing more compact bit representations, and
achieve faster coding speed on parallel devices over their classical
counterparts. Those properties evoked the attention of both scientific and
industrial communities, resulting in the standardization activity JPEG-AI. The
verification model for the standardization process of JPEG-AI is already in
development and has surpassed the advanced VVC intra codec. To generate
reconstructed images with the desired bits per pixel and assess the BD-rate
performance of both the JPEG-AI verification model and VVC intra, bit rate
matching is employed. However, the current state of the JPEG-AI verification
model experiences significant slowdowns during bit rate matching, resulting in
suboptimal performance due to an unsuitable model. The proposed methodology
offers a gradual algorithmic optimization for matching bit rates, resulting in
a fourfold acceleration and over 1% improvement in BD-rate at the base
operation point. At the high operation point, the acceleration increases up to
sixfold.
\\ ( https://arxiv.org/abs/2402.17487 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17492 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:18:00 GMT   (1867kb,D)

Title: syren-halofit: A fast, interpretable, high-precision formula for the
  $\Lambda$CDM nonlinear matter power spectrum
Authors: Deaglan J. Bartlett, Benjamin D. Wandelt, Matteo Zennaro, Pedro G.
  Ferreira, Harry Desmond
Categories: astro-ph.CO astro-ph.IM cs.LG cs.NE
Comments: 11 pages, 8 figures. Submitted to A&A
\\
  Rapid and accurate evaluation of the nonlinear matter power spectrum, $P(k)$,
as a function of cosmological parameters and redshift is of fundamental
importance in cosmology. Analytic approximations provide an interpretable
solution, yet current approximations are neither fast nor accurate relative to
black-box numerical emulators. We use symbolic regression to obtain simple
analytic approximations to the nonlinear scale, $k_\sigma$, the effective
spectral index, $n_{\rm eff}$, and the curvature, $C$, which are required for
the halofit model. We then re-optimise the coefficients of halofit to fit a
wide range of cosmologies and redshifts. We then again exploit symbolic
regression to explore the space of analytic expressions to fit the residuals
between $P(k)$ and the optimised predictions of halofit. All methods are
validated against $N$-body simulations. Our symbolic expressions for
$k_\sigma$, $n_{\rm eff}$ and $C$ have root mean squared fractional errors of
0.8%, 0.2% and 0.3%, respectively, for redshifts below 3 and a wide range of
cosmologies. The re-optimised halofit parameters reduce the root mean squared
fractional error from 3% to below 2% for wavenumbers $k=9\times10^{-3}-9 \,
h{\rm Mpc^{-1}}$. We introduce syren-halofit (symbolic-regression-enhanced
halofit), an extension to halofit containing a short symbolic correction which
improves this error to 1%. Our method is 2350 and 3170 times faster than
current halofit and hmcode implementations, respectively, and 2680 and 64 times
faster than EuclidEmulator2 (which requires running class) and the BACCO
emulator. We obtain comparable accuracy to EuclidEmulator2 and the BACCO
emulator when tested on $N$-body simulations. Our work greatly increases the
speed and accuracy of symbolic approximations to $P(k)$, making them
significantly faster than their numerical counterparts without loss of
accuracy.
\\ ( https://arxiv.org/abs/2402.17492 ,  1867kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17500 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:34:08 GMT   (10486kb,D)

Title: Predicting Instability in Complex Oscillator Networks: Limitations and
  Potentials of Network Measures and Machine Learning
Authors: Christian Nauck, Michael Lindner, Nora Molkenthin, J\"urgen Kurths,
  Eckehard Sch\"oll, J\"org Raisch and Frank Hellmann
Categories: nlin.AO cs.LG cs.SI
Comments: 30 pages (16 pages main section), 15 figures, 4 tables
\\
  A central question of network science is how functional properties of systems
arise from their structure. For networked dynamical systems, structure is
typically quantified with network measures. A functional property that is of
theoretical and practical interest for oscillatory systems is the stability of
synchrony to localized perturbations. Recently, Graph Neural Networks (GNNs)
have been shown to predict this stability successfully; at the same time,
network measures have struggled to paint a clear picture. Here we collect 46
relevant network measures and find that no small subset can reliably predict
stability. The performance of GNNs can only be matched by combining all network
measures and nodewise machine learning. However, unlike GNNs, this approach
fails to extrapolate from network ensembles to several real power grid
topologies. This suggests that correlations of network measures and function
may be misleading, and that GNNs capture the causal relationship between
structure and stability substantially better.
\\ ( https://arxiv.org/abs/2402.17500 ,  10486kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17506 (*cross-listing*)
Date: Tue, 27 Feb 2024 13:46:45 GMT   (3624kb,D)

Title: Thermodynamics-informed super-resolution of scarce temporal dynamics
  data
Authors: Carlos Bermejo-Barbanoj, Beatriz Moya, Alberto Bad\'ias, Francisco
  Chinesta, El\'ias Cueto
Categories: physics.comp-ph cs.LG
Comments: 18 pages, 11 figures
\\
  We present a method to increase the resolution of measurements of a physical
system and subsequently predict its time evolution using thermodynamics-aware
neural networks. Our method uses adversarial autoencoders, which reduce the
dimensionality of the full order model to a set of latent variables that are
enforced to match a prior, for example a normal distribution. Adversarial
autoencoders are seen as generative models, and they can be trained to generate
high-resolution samples from low-resoution inputs, meaning they can address the
so-called super-resolution problem. Then, a second neural network is trained to
learn the physical structure of the latent variables and predict their temporal
evolution. This neural network is known as an structure-preserving neural
network. It learns the metriplectic-structure of the system and applies a
physical bias to ensure that the first and second principles of thermodynamics
are fulfilled. The integrated trajectories are decoded to their original
dimensionality, as well as to the higher dimensionality space produced by the
adversarial autoencoder and they are compared to the ground truth solution. The
method is tested with two examples of flow over a cylinder, where the fluid
properties are varied between both examples.
\\ ( https://arxiv.org/abs/2402.17506 ,  3624kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17544 (*cross-listing*)
Date: Tue, 27 Feb 2024 14:34:14 GMT   (1384kb,D)

Title: Adapting Learned Image Codecs to Screen Content via Adjustable
  Transformations
Authors: H. Burak Dogaroglu, A. Burakhan Koyuncu, Atanas Boev, Elena Alshina,
  Eckehard Steinbach
Categories: eess.IV cs.CV cs.LG
Comments: 7 pages, 6 figures, 2 tables
\\
  As learned image codecs (LICs) become more prevalent, their low coding
efficiency for out-of-distribution data becomes a bottleneck for some
applications. To improve the performance of LICs for screen content (SC) images
without breaking backwards compatibility, we propose to introduce parameterized
and invertible linear transformations into the coding pipeline without changing
the underlying baseline codec's operation flow. We design two neural networks
to act as prefilters and postfilters in our setup to increase the coding
efficiency and help with the recovery from coding artifacts. Our end-to-end
trained solution achieves up to 10% bitrate savings on SC compression compared
to the baseline LICs while introducing only 1% extra parameters.
\\ ( https://arxiv.org/abs/2402.17544 ,  1384kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17621 (*cross-listing*)
Date: Tue, 27 Feb 2024 15:49:26 GMT   (1798kb)

Title: Supervised machine learning for microbiomics: bridging the gap between
  current and best practices
Authors: Natasha K. Dudek, Mariam Chakhvadze, Saba Kobakhidze, Omar Kantidze,
  Yuriy Gankin
Categories: q-bio.GN cs.LG
Comments: 25 pages, 5 figures
\\
  Machine learning (ML) is set to accelerate innovations in clinical
microbiomics, such as in disease diagnostics and prognostics. This will require
high-quality, reproducible, interpretable workflows whose predictive
capabilities meet or exceed the high thresholds set for clinical tools by
regulatory agencies. Here, we capture a snapshot of current practices in the
application of supervised ML to microbiomics data, through an in-depth analysis
of 100 peer-reviewed journal articles published in 2021-2022. We apply a
data-driven approach to steer discussion of the merits of varied approaches to
experimental design, including key considerations such as how to mitigate the
effects of small dataset size while avoiding data leakage. We further provide
guidance on how to avoid common experimental design pitfalls that can hurt
model performance, trustworthiness, and reproducibility. Discussion is
accompanied by an interactive online tutorial that demonstrates foundational
principles of ML experimental design, tailored to the microbiomics community.
Formalizing community best practices for supervised ML in microbiomics is an
important step towards improving the success and efficiency of clinical
research, to the benefit of patients and other stakeholders.
\\ ( https://arxiv.org/abs/2402.17621 ,  1798kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17686 (*cross-listing*)
Date: Tue, 27 Feb 2024 17:01:21 GMT   (5252kb,D)

Title: Outlier-Detection for Reactive Machine Learned Potential Energy Surfaces
Authors: Luis Itza Vazquez-Salazar and Silvan K\"aser and Markus Meuwly
Categories: physics.chem-ph cs.LG
\\
  Uncertainty quantification (UQ) to detect samples with large expected errors
(outliers) is applied to reactive molecular potential energy surfaces (PESs).
Three methods - Ensembles, Deep Evidential Regression (DER), and Gaussian
Mixture Models (GMM) - were applied to the H-transfer reaction between ${\it
syn-}$Criegee and vinyl hydroxyperoxide. The results indicate that ensemble
models provide the best results for detecting outliers, followed by GMM. For
example, from a pool of 1000 structures with the largest uncertainty, the
detection quality for outliers is $\sim 90$ \% and $\sim 50$ \%, respectively,
if 25 or 1000 structures with large errors are sought. On the contrary, the
limitations of the statistical assumptions of DER greatly impacted its
prediction capabilities. Finally, a structure-based indicator was found to be
correlated with large average error, which may help to rapidly classify new
structures into those that provide an advantage for refining the neural
network.
\\ ( https://arxiv.org/abs/2402.17686 ,  5252kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17695 (*cross-listing*)
Date: Tue, 27 Feb 2024 17:11:35 GMT   (3145kb,D)

Title: Geometric Deep Learning for Computer-Aided Design: A Survey
Authors: Negar Heidari and Alexandros Iosifidis
Categories: cs.CG cs.LG
Comments: 26 pages, 14 figures, journal article
\\
  Geometric Deep Learning techniques have become a transformative force in the
field of Computer-Aided Design (CAD), and have the potential to revolutionize
how designers and engineers approach and enhance the design process. By
harnessing the power of machine learning-based methods, CAD designers can
optimize their workflows, save time and effort while making better informed
decisions, and create designs that are both innovative and practical. The
ability to process the CAD designs represented by geometric data and to analyze
their encoded features enables the identification of similarities among diverse
CAD models, the proposition of alternative designs and enhancements, and even
the generation of novel design alternatives. This survey offers a comprehensive
overview of learning-based methods in computer-aided design across various
categories, including similarity analysis and retrieval, 2D and 3D CAD model
synthesis, and CAD generation from point clouds. Additionally, it provides a
complete list of benchmark datasets and their characteristics, along with
open-source codes that have propelled research in this domain. The final
discussion delves into the challenges prevalent in this field, followed by
potential future research directions in this rapidly evolving field.
\\ ( https://arxiv.org/abs/2402.17695 ,  3145kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17698 (*cross-listing*)
Date: Tue, 27 Feb 2024 17:21:10 GMT   (1487kb,D)

Title: Learning reduced-order Quadratic-Linear models in Process Engineering
  using Operator Inference
Authors: Ion Victor Gosea, Luisa Peterson, Pawan Goyal, Jens Bremer, Kai
  Sundmacher, Peter Benner
Categories: math.NA cs.LG cs.NA
Comments: 10 pages, 3 figures
\\
  In this work, we address the challenge of efficiently modeling dynamical
systems in process engineering. We use reduced-order model learning,
specifically operator inference. This is a non-intrusive, data-driven method
for learning dynamical systems from time-domain data. The application in our
study is carbon dioxide methanation, an important reaction within the
Power-to-X framework, to demonstrate its potential. The numerical results show
the ability of the reduced-order models constructed with operator inference to
provide a reduced yet accurate surrogate solution. This represents an important
milestone towards the implementation of fast and reliable digital twin
architectures.
\\ ( https://arxiv.org/abs/2402.17698 ,  1487kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17701 (*cross-listing*)
Date: Tue, 27 Feb 2024 17:26:33 GMT   (3051kb,D)

Title: Real-time Low-latency Music Source Separation using Hybrid
  Spectrogram-TasNet
Authors: Satvik Venkatesh, Arthur Benilov, Philip Coleman, Frederic Roskam
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to ICASSP 2024
ACM-class: I.5.1; I.5.4
\\
  There have been significant advances in deep learning for music demixing in
recent years. However, there has been little attention given to how these
neural networks can be adapted for real-time low-latency applications, which
could be helpful for hearing aids, remixing audio streams and live shows. In
this paper, we investigate the various challenges involved in adapting current
demixing models in the literature for this use case. Subsequently, inspired by
the Hybrid Demucs architecture, we propose the Hybrid Spectrogram Time-domain
Audio Separation Network HS-TasNet, which utilises the advantages of spectral
and waveform domains. For a latency of 23 ms, the HS-TasNet obtains an overall
signal-to-distortion ratio (SDR) of 4.65 on the MusDB test set, and increases
to 5.55 with additional training data. These results demonstrate the potential
of efficient demixing for real-time low-latency music applications.
\\ ( https://arxiv.org/abs/2402.17701 ,  3051kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17704 (*cross-listing*)
Date: Tue, 27 Feb 2024 17:30:33 GMT   (2298kb,D)

Title: Transfer Learning Bayesian Optimization to Design Competitor DNA
  Molecules for Use in Diagnostic Assays
Authors: Ruby Sedgwick, John P. Goertz, Molly M. Stevens, Ruth Misener, Mark
  van der Wilk
Categories: q-bio.QM cs.LG stat.ML
\\
  With the rise in engineered biomolecular devices, there is an increased need
for tailor-made biological sequences. Often, many similar biological sequences
need to be made for a specific application meaning numerous, sometimes
prohibitively expensive, lab experiments are necessary for their optimization.
This paper presents a transfer learning design of experiments workflow to make
this development feasible. By combining a transfer learning surrogate model
with Bayesian optimization, we show how the total number of experiments can be
reduced by sharing information between optimization tasks. We demonstrate the
reduction in the number of experiments using data from the development of DNA
competitors for use in an amplification-based diagnostic assay. We use
cross-validation to compare the predictive accuracy of different transfer
learning models, and then compare the performance of the models for both single
objective and penalized optimization tasks.
\\ ( https://arxiv.org/abs/2402.17704 ,  2298kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17722 (*cross-listing*)
Date: Tue, 27 Feb 2024 17:56:49 GMT   (100kb,D)

Title: Taming Nonconvex Stochastic Mirror Descent with General Bregman
  Divergence
Authors: Ilyas Fatkhullin, Niao He
Categories: math.OC cs.LG
Comments: Accepted for publication at AISTATS 2024
MSC-class: 90C15, 90C26, 90C15
ACM-class: G.1.6
\\
  This paper revisits the convergence of Stochastic Mirror Descent (SMD) in the
contemporary nonconvex optimization setting. Existing results for batch-free
nonconvex SMD restrict the choice of the distance generating function (DGF) to
be differentiable with Lipschitz continuous gradients, thereby excluding
important setups such as Shannon entropy. In this work, we present a new
convergence analysis of nonconvex SMD supporting general DGF, that overcomes
the above limitations and relies solely on the standard assumptions. Moreover,
our convergence is established with respect to the Bregman Forward-Backward
envelope, which is a stronger measure than the commonly used squared norm of
gradient mapping. We further extend our results to guarantee high probability
convergence under sub-Gaussian noise and global convergence under the
generalized Bregman Proximal Polyak-{\L}ojasiewicz condition. Additionally, we
illustrate the advantages of our improved SMD theory in various nonconvex
machine learning tasks by harnessing nonsmooth DGFs. Notably, in the context of
nonconvex differentially private (DP) learning, our theory yields a simple
algorithm with a (nearly) dimension-independent utility bound. For the problem
of training linear neural networks, we develop provably convergent stochastic
algorithms.
\\ ( https://arxiv.org/abs/2402.17722 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17732 (*cross-listing*)
Date: Tue, 27 Feb 2024 18:06:20 GMT   (52kb,D)

Title: Batched Nonparametric Contextual Bandits
Authors: Rong Jiang and Cong Ma
Categories: math.ST cs.LG stat.ML stat.TH
Comments: 26 pages, 4 figures
\\
  We study nonparametric contextual bandits under batch constraints, where the
expected reward for each action is modeled as a smooth function of covariates,
and the policy updates are made at the end of each batch of observations. We
establish a minimax regret lower bound for this setting and propose Batched
Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal
regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the
covariate space into smaller bins, carefully aligning their widths with the
batch size. We also show the suboptimality of static binning under batch
constraints, highlighting the necessity of dynamic binning. Additionally, our
results suggest that a nearly constant number of policy updates can attain
optimal regret in the fully online setting.
\\ ( https://arxiv.org/abs/2402.17732 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17750 (*cross-listing*)
Date: Tue, 27 Feb 2024 18:37:22 GMT   (25256kb,D)

Title: Scaling on-chip photonic neural processors using arbitrarily
  programmable wave propagation
Authors: Tatsuhiro Onodera, Martin M. Stein, Benjamin A. Ash, Mandar M. Sohoni,
  Melissa Bosch, Ryotatsu Yanagimoto, Marc Jankowski, Timothy P. McKenna,
  Tianyu Wang, Gennady Shvets, Maxim R. Shcherbakov, Logan G. Wright, Peter L.
  McMahon
Categories: physics.optics cs.ET cs.LG
\\
  On-chip photonic processors for neural networks have potential benefits in
both speed and energy efficiency but have not yet reached the scale at which
they can outperform electronic processors. The dominant paradigm for designing
on-chip photonics is to make networks of relatively bulky discrete components
connected by one-dimensional waveguides. A far more compact alternative is to
avoid explicitly defining any components and instead sculpt the continuous
substrate of the photonic processor to directly perform the computation using
waves freely propagating in two dimensions. We propose and demonstrate a device
whose refractive index as a function of space, $n(x,z)$, can be rapidly
reprogrammed, allowing arbitrary control over the wave propagation in the
device. Our device, a 2D-programmable waveguide, combines photoconductive gain
with the electro-optic effect to achieve massively parallel modulation of the
refractive index of a slab waveguide, with an index modulation depth of
$10^{-3}$ and approximately $10^4$ programmable degrees of freedom. We used a
prototype device with a functional area of $12\,\text{mm}^2$ to perform
neural-network inference with up to 49-dimensional input vectors in a single
pass, achieving 96% accuracy on vowel classification and 86% accuracy on $7
\times 7$-pixel MNIST handwritten-digit classification. This is a scale beyond
that of previous photonic chips relying on discrete components, illustrating
the benefit of the continuous-waves paradigm. In principle, with large enough
chip area, the reprogrammability of the device's refractive index distribution
enables the reconfigurable realization of any passive, linear photonic circuit
or device. This promises the development of more compact and versatile photonic
systems for a wide range of applications, including optical processing, smart
sensing, spectroscopy, and optical communications.
\\ ( https://arxiv.org/abs/2402.17750 ,  25256kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:1810.13306
replaced with revised version Tue, 27 Feb 2024 14:59:46 GMT   (15800kb,D)

Title: Automated Machine Learning: From Principles to Practices
Authors: Zhenqian Shen, Yongqi Zhang, Lanning Wei, Huan Zhao, Quanming Yao
Categories: cs.AI cs.LG stat.ML
Comments: This is a preliminary and will be kept updated
\\ ( https://arxiv.org/abs/1810.13306 ,  15800kb)
------------------------------------------------------------------------------
\\
arXiv:2210.11194
replaced with revised version Tue, 27 Feb 2024 13:51:07 GMT   (643kb,D)

Title: Controller-Guided Partial Label Consistency Regularization with
  Unlabeled Data
Authors: Qian-Wei Wang, Bowen Zhao, Mingyan Zhu, Tianxiang Li, Zimo Liu,
  Shu-Tao Xia
Categories: cs.AI
\\ ( https://arxiv.org/abs/2210.11194 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2303.00968
replaced with revised version Tue, 27 Feb 2024 18:44:19 GMT   (2701kb,D)

Title: Dynamic fairness-aware recommendation through multi-agent social choice
Authors: Amanda Aird, Paresha Farastu, Joshua Sun, Elena \v{S}tefancov\'a,
  Cassidy All, Amy Voida, Nicholas Mattei, Robin Burke
Categories: cs.AI
\\ ( https://arxiv.org/abs/2303.00968 ,  2701kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05227
replaced with revised version Mon, 26 Feb 2024 21:22:51 GMT   (42kb)

Title: Kantian Deontology Meets AI Alignment: Towards Morally Grounded Fairness
  Metrics
Authors: Carlos Mougan, Joshua Brand
Categories: cs.AI
\\ ( https://arxiv.org/abs/2311.05227 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05814
replaced with revised version Tue, 27 Feb 2024 02:25:28 GMT   (21299kb,D)

Title: Neural Speech Embeddings for Speech Synthesis Based on Deep Generative
  Networks
Authors: Seo-Hyun Lee, Young-Eun Lee, Soowon Kim, Byung-Kwan Ko, Jun-Young Kim,
  Seong-Whan Lee
Categories: cs.AI cs.SD eess.AS
Comments: 4 pages
\\ ( https://arxiv.org/abs/2312.05814 ,  21299kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16823
replaced with revised version Tue, 27 Feb 2024 11:03:10 GMT   (2902kb,D)

Title: Language Agents as Optimizable Graphs
Authors: Mingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii
  Khizbullin and J\"urgen Schmidhuber
Categories: cs.AI cs.CL cs.LG cs.MA
Comments: Project Website: https://gptswarm.org ; Github Repo:
  https://github.com/metauto-ai/gptswarm ; Replace to fix typos
\\ ( https://arxiv.org/abs/2402.16823 ,  2902kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10002
replaced with revised version Mon, 26 Feb 2024 20:52:59 GMT   (168kb,D)

Title: Defending Against Disinformation Attacks in Open-Domain Question
  Answering
Authors: Orion Weller, Aleem Khan, Nathaniel Weir, Dawn Lawrie, Benjamin Van
  Durme
Categories: cs.CL cs.IR
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2212.10002 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12822
replaced with revised version Tue, 27 Feb 2024 14:49:43 GMT   (1341kb,D)

Title: Automatic Prompt Augmentation and Selection with Chain-of-Thought from
  Labeled Data
Authors: KaShun Shum, Shizhe Diao, Tong Zhang
Categories: cs.CL
Comments: EMNLP 2023
\\ ( https://arxiv.org/abs/2302.12822 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13689
replaced with revised version Tue, 27 Feb 2024 13:57:08 GMT   (236kb,D)

Title: HeySQuAD: A Spoken Question Answering Dataset
Authors: Yijing Wu, SaiKrishna Rallabandi, Ravisutha Srinivasamurthy, Parag
  Pravin Dakle, Alolika Gon, Preethi Raghavan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2304.13689 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02547
replaced with revised version Mon, 26 Feb 2024 23:01:15 GMT   (2095kb,D)

Title: PersonaLLM: Investigating the Ability of Large Language Models to
  Express Personality Traits
Authors: Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Jad Kabbara, Deb
  Roy
Categories: cs.CL cs.AI cs.HC
Comments: First version uploaded at IC2S2 in May 2023. Full paper submitted in
  Nov. 2023 and updated Feb. 2024
\\ ( https://arxiv.org/abs/2305.02547 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12077
replaced with revised version Tue, 27 Feb 2024 02:51:16 GMT   (7814kb,D)

Title: Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer in
  Prompt Tuning
Authors: Kaige Xie, Tong Yu, Haoliang Wang, Junda Wu, Handong Zhao, Ruiyi
  Zhang, Kanak Mahadik, Ani Nenkova, Mark Riedl
Categories: cs.CL
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2305.12077 ,  7814kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12599
replaced with revised version Mon, 26 Feb 2024 22:44:36 GMT   (4070kb,D)

Title: Abstract Meaning Representation-Based Logic-Driven Data Augmentation for
  Logical Reasoning
Authors: Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wanjun Zhong, Gael
  Gendron, Timothy Pistotti, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu,
  Paul Denny, Michael Witbrock, Jiamou Liu
Categories: cs.CL cs.AI
Comments: The short version (v2) was accepted for oral presentation at the
  first LLM@IJCAI 2023 non-archival symposium; the full version is under review
\\ ( https://arxiv.org/abs/2305.12599 ,  4070kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12675
replaced with revised version Tue, 27 Feb 2024 06:57:36 GMT   (981kb,D)

Title: A Frustratingly Simple Decoding Method for Neural Text Generation
Authors: Haoran Yang, Deng Cai, Huayang Li, Wei Bi, Wai Lam, Shuming Shi
Categories: cs.CL
Comments: LREC-Coling 2024
\\ ( https://arxiv.org/abs/2305.12675 ,  981kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13252
replaced with revised version Mon, 26 Feb 2024 20:50:33 GMT   (297kb,D)

Title: "According to ...": Prompting Language Models Improves Quoting from
  Pre-Training Data
Authors: Orion Weller and Marc Marone and Nathaniel Weir and Dawn Lawrie and
  Daniel Khashabi and Benjamin Van Durme
Categories: cs.CL cs.AI
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2305.13252 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13300
replaced with revised version Tue, 27 Feb 2024 17:08:49 GMT   (383kb,D)

Title: Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large
  Language Models in Knowledge Conflicts
Authors: Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su
Categories: cs.CL cs.AI
Comments: ICLR 2024 (Spotlight)
\\ ( https://arxiv.org/abs/2305.13300 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17588
replaced with revised version Mon, 26 Feb 2024 23:11:03 GMT   (34792kb,D)

Title: Diagnosing Transformers: Illuminating Feature Spaces for Clinical
  Decision-Making
Authors: Aliyah R. Hsu, Yeshwanth Cherapanamjeri, Briton Park, Tristan Naumann,
  Anobel Y. Odisho, Bin Yu
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2305.17588 ,  34792kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11698
replaced with revised version Mon, 26 Feb 2024 20:41:01 GMT   (115591kb,D)

Title: DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT
  Models
Authors: Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang,
  Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T.
  Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng,
  Sanmi Koyejo, Dawn Song, Bo Li
Categories: cs.CL cs.AI cs.CR
Comments: NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)
\\ ( https://arxiv.org/abs/2306.11698 ,  115591kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17492
replaced with revised version Tue, 27 Feb 2024 18:42:42 GMT   (525kb,D)

Title: Preference Ranking Optimization for Human Alignment
Authors: Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li
  and Houfeng Wang
Categories: cs.CL cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2306.17492 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08532
replaced with revised version Tue, 27 Feb 2024 12:13:46 GMT   (287kb,D)

Title: Connecting Large Language Models with Evolutionary Algorithms Yields
  Powerful Prompt Optimizers
Authors: Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan,
  Guoqing Liu, Jiang Bian, Yujiu Yang
Categories: cs.CL cs.AI
Comments: International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2309.08532 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02174
replaced with revised version Tue, 27 Feb 2024 14:17:53 GMT   (2268kb,D)

Title: Ask Again, Then Fail: Large Language Models' Vacillations in Judgement
Authors: Qiming Xie, Zengzhi Wang, Yi Feng, and Rui Xia
Categories: cs.CL cs.AI cs.LG
Comments: Update mitigation results of fine-tuning the model on synthesized
  high-quality preference data with DPO algorithm
\\ ( https://arxiv.org/abs/2310.02174 ,  2268kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05797
replaced with revised version Mon, 26 Feb 2024 20:33:36 GMT   (1482kb,D)

Title: Are Large Language Models Post Hoc Explainers?
Authors: Nicholas Kroeger, Dan Ley, Satyapriya Krishna, Chirag Agarwal,
  Himabindu Lakkaraju
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.05797 ,  1482kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18338
replaced with revised version Tue, 27 Feb 2024 13:24:06 GMT   (8631kb,D)

Title: Small Language Models Fine-tuned to Coordinate Larger Language Models
  improve Complex Reasoning
Authors: Gurusha Juneja, Subhabrata Dutta, Soumen Chakrabarti, Sunny Manchanda,
  Tanmoy Chakraborty
Categories: cs.CL cs.AI
Comments: EMNLP 2023 (Typos corrected)
\\ ( https://arxiv.org/abs/2310.18338 ,  8631kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08640
replaced with revised version Mon, 26 Feb 2024 22:30:53 GMT   (856kb,D)

Title: Multistage Collaborative Knowledge Distillation from a Large Language
  Model for Semi-Supervised Sequence Generation
Authors: Jiachen Zhao, Wenlong Zhao, Andrew Drozdov, Benjamin Rozonoyer, Md
  Arafat Sultan, Jay-Yoon Lee, Mohit Iyyer, Andrew McCallum
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.08640 ,  856kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08894
replaced with revised version Tue, 27 Feb 2024 13:04:44 GMT   (228kb,D)

Title: Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing
  Supervised Models with In-Context Learning
Authors: Mayur Patidar, Riya Sawhney, Avinash Singh, Biswajit Chatterjee,
  Mausam, Indrajit Bhattacharya
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.08894 ,  228kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09675
replaced with revised version Tue, 27 Feb 2024 03:37:03 GMT   (7994kb,D)

Title: Where Do People Tell Stories Online? Story Detection Across Online
  Communities
Authors: Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09675 ,  7994kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06185
replaced with revised version Tue, 27 Feb 2024 10:05:04 GMT   (1345kb,D)

Title: KnowGPT: Knowledge Injection for Large Language Models
Authors: Qinggang Zhang, Junnan Dong, Hao Chen, Daochen Zha, Zailiang Yu, Xiao
  Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.06185 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03797
replaced with revised version Tue, 27 Feb 2024 16:35:56 GMT   (1392kb)

Title: Anatomy of Neural Language Models
Authors: Majd Saleh and St\'ephane Paquelet
Categories: cs.CL cs.LG
Comments: 36 Pages; 25 Figures; some typos and notation errors are corrected in
  this version
\\ ( https://arxiv.org/abs/2401.03797 ,  1392kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11033
replaced with revised version Tue, 27 Feb 2024 12:51:48 GMT   (8223kb,D)

Title: FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for
  Large Language Models' Training?
Authors: Shaina Raza, Shardul Ghuge, Chen Ding, Elham Dolatabadi, Deval Pandya
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.11033 ,  8223kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12413
replaced with revised version Mon, 26 Feb 2024 23:23:31 GMT   (2095kb,D)

Title: How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual
  Translation via Tiny Multi-Parallel Data
Authors: Di Wu, Shaomu Tan, Yan Meng, David Stap and Christof Monz
Categories: cs.CL cs.LG
Comments: 15 pages, 5 figures
\\ ( https://arxiv.org/abs/2401.12413 ,  2095kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14556
replaced with revised version Tue, 27 Feb 2024 16:48:17 GMT   (10825kb,D)

Title: Looking Right is Sometimes Right: Investigating the Capabilities of
  Decoder-only LLMs for Sequence Labeling
Authors: David Duki\'c, Jan \v{S}najder
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.14556 ,  10825kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17464
replaced with revised version Mon, 26 Feb 2024 20:26:40 GMT   (730kb,D)

Title: Efficient Tool Use with Chain-of-Abstraction Reasoning
Authors: Silin Gao, Jane Dwivedi-Yu, Ping Yu, Xiaoqing Ellen Tan, Ramakanth
  Pasunuru, Olga Golovneva, Koustuv Sinha, Asli Celikyilmaz, Antoine Bosselut,
  Tianlu Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.17464 ,  730kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03848
replaced with revised version Tue, 27 Feb 2024 13:14:28 GMT   (184kb,D)

Title: ANLS* -- A Universal Document Processing Metric for Generative Large
  Language Models
Authors: David Peer, Philemon Sch\"opf, Volckmar Nebendahl, Alexander Rietzler,
  Sebastian Stabinger
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.03848 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08496
replaced with revised version Tue, 27 Feb 2024 00:05:28 GMT   (959kb,D)

Title: A Systematic Review of Data-to-Text NLG
Authors: Chinonso Cynthia Osuji, Thiago Castro Ferreira, Brian Davis
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.08496 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09216
replaced with revised version Tue, 27 Feb 2024 11:27:27 GMT   (1227kb,D)

Title: Scaling the Authoring of AutoTutors with Large Language Models
Authors: Sankalan Pal Chowdhury, Vil\'em Zouhar, Mrinmaya Sachan
Categories: cs.CL cs.HC
Comments: 15 pages
\\ ( https://arxiv.org/abs/2402.09216 ,  1227kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10573
replaced with revised version Tue, 27 Feb 2024 15:08:02 GMT   (4905kb,D)

Title: LinkNER: Linking Local Named Entity Recognition Models to Large Language
  Models using Uncertainty
Authors: Zhen Zhang, Yuhua Zhao, Hang Gao, and Mengting Hu
Categories: cs.CL
Comments: Accepted by WebConf (WWW'2024)
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.10573 ,  4905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11525
replaced with revised version Tue, 27 Feb 2024 17:12:38 GMT   (320kb,D)

Title: Advancing Translation Preference Modeling with RLHF: A Step Towards
  Cost-Effective Solution
Authors: Nuo Xu, Jun Zhao, Can Zu, Sixian Li, Lu Chen, Zhihao Zhang, Rui Zheng,
  Shihan Dou, Wenjuan Qin, Tao Gui, Qi Zhang, Xuanjing Huang
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.11525 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12026
replaced with revised version Tue, 27 Feb 2024 03:22:49 GMT   (1093kb,D)

Title: Acquiring Clean Language Models from Backdoor Poisoned Datasets by
  Downscaling Frequency Space
Authors: Zongru Wu, Zhuosheng Zhang, Pengzhou Cheng, Gongshen Liu
Categories: cs.CL cs.AI cs.CR
\\ ( https://arxiv.org/abs/2402.12026 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12563
replaced with revised version Tue, 27 Feb 2024 06:32:35 GMT   (22054kb,D)

Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of
  Large Language Models
Authors: Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric
  Xing, Kun Zhang
Categories: cs.CL cs.AI
Comments: 12 figures, 9 tables
\\ ( https://arxiv.org/abs/2402.12563 ,  22054kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14872
replaced with revised version Tue, 27 Feb 2024 13:49:22 GMT   (302kb,D)

Title: Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts
  Against Open-source LLMs
Authors: Xiaoxia Li, Siyuan Liang, Jiyi Zhang, Han Fang, Aishan Liu, Ee-Chien
  Chang
Categories: cs.CL cs.AI cs.NE
\\ ( https://arxiv.org/abs/2402.14872 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15159
replaced with revised version Tue, 27 Feb 2024 05:23:35 GMT   (7013kb,D)

Title: Machine Unlearning of Pre-trained Large Language Models
Authors: Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng,
  Xiang Yue
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: Code is available at https://github.com/yaojin17/Unlearning_LLM
\\ ( https://arxiv.org/abs/2402.15159 ,  7013kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16040
replaced with revised version Tue, 27 Feb 2024 06:25:25 GMT   (8319kb,D)

Title: EHRNoteQA: A Patient-Specific Question Answering Benchmark for
  Evaluating Large Language Models in Clinical Settings
Authors: Sunjun Kweon, Jiyoun Kim, Heeyoung Kwak, Dongchul Cha, Hangyul Yoon,
  Kwanghyun Kim, Seunghyun Won, Edward Choi
Categories: cs.CL
Comments: Under Review
\\ ( https://arxiv.org/abs/2402.16040 ,  8319kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16063
replaced with revised version Tue, 27 Feb 2024 11:07:23 GMT   (731kb,D)

Title: Citation-Enhanced Generation for LLM-based Chatbot
Authors: Weitao Li, Junkai Li, Weizhi Ma, Yang Liu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16063 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16107
replaced with revised version Tue, 27 Feb 2024 04:48:36 GMT   (243kb,D)

Title: FuseChat: Knowledge Fusion of Chat Models
Authors: Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang,
  Wei Bi
Categories: cs.CL
Comments: Technical Report, work in progress
\\ ( https://arxiv.org/abs/2402.16107 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16311
replaced with revised version Tue, 27 Feb 2024 08:21:35 GMT   (3568kb,D)

Title: Cross-domain Chinese Sentence Pattern Parsing
Authors: Jingsi Yu, Cunliang Kong, Liner Yang, Meishan Zhang, Lin Zhu, Yujie
  Wang, Haozhe Lin, Maosong Sun, Erhong Yang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16311 ,  3568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16458
replaced with revised version Tue, 27 Feb 2024 12:52:24 GMT   (458kb,D)

Title: ID-XCB: Data-independent Debiasing for Fair and Accurate
  Transformer-based Cyberbullying Detection
Authors: Peiling Yi and Arkaitz Zubiaga
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.16458 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16819
replaced with revised version Tue, 27 Feb 2024 15:22:57 GMT   (1362kb,D)

Title: Nemotron-4 15B Technical Report
Authors: Jupinder Parmar and Shrimai Prabhumoye and Joseph Jennings and Mostofa
  Patwary and Sandeep Subramanian and Dan Su and Chen Zhu and Deepak Narayanan
  and Aastha Jhunjhunwala and Ayush Dattagupta and Vibhu Jawa and Jiwei Liu and
  Ameya Mahabaleshwarkar and Osvald Nitski and Annika Brundyn and James Maki
  and Miguel Martinez and Jiaxuan You and John Kamalu and Patrick LeGresley and
  Denys Fridman and Jared Casper and Ashwath Aithal and Oleksii Kuchaiev and
  Mohammad Shoeybi and Jonathan Cohen and Bryan Catanzaro
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.16819 ,  1362kb)
------------------------------------------------------------------------------
\\
arXiv:2206.02902
replaced with revised version Tue, 27 Feb 2024 06:15:53 GMT   (8674kb,D)

Title: Goal-Space Planning with Subgoal Models
Authors: Chunlok Lo, Kevin Roice, Parham Mohammad Panahi, Scott Jordan, Adam
  White, Gabor Mihucz, Farzane Aminmansour, Martha White
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2206.02902 ,  8674kb)
------------------------------------------------------------------------------
\\
arXiv:2206.09107
replaced with revised version Mon, 26 Feb 2024 20:29:50 GMT   (254kb,D)

Title: Tree-Guided Rare Feature Selection and Logic Aggregation with Electronic
  Health Records Data
Authors: Jianmin Chen, Robert H. Aseltine, Fei Wang, Kun Chen
Categories: cs.LG stat.AP stat.ME stat.ML
\\ ( https://arxiv.org/abs/2206.09107 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2212.07892
replaced with revised version Tue, 27 Feb 2024 12:37:06 GMT   (11216kb,D)

Title: Integrating Multimodal Data for Joint Generative Modeling of Complex
  Dynamics
Authors: Manuel Brenner and Florian Hess and Georgia Koppe and Daniel
  Durstewitz
Categories: cs.LG math.DS nlin.CD
Comments: A previous version was published as a workshop paper for the AAAI
  2023 Workshop MLmDS under the name "Multimodal Teacher Forcing for
  Reconstructing Nonlinear Dynamical Systems"
\\ ( https://arxiv.org/abs/2212.07892 ,  11216kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06701
replaced with revised version Tue, 27 Feb 2024 06:47:19 GMT   (9533kb,D)

Title: Communication-Efficient Federated Bilevel Optimization with Local and
  Global Lower Level Problems
Authors: Junyi Li, Feihu Huang, Heng Huang
Categories: cs.LG cs.DC math.OC
Comments: NeurIPS 2023 version (Algorithm 1 is updated to be more concise)
\\ ( https://arxiv.org/abs/2302.06701 ,  9533kb)
------------------------------------------------------------------------------
\\
arXiv:2303.14537
replaced with revised version Mon, 26 Feb 2024 19:42:20 GMT   (1732kb,D)

Title: Deep Augmentation: Self-Supervised Learning with Transformations in
  Activation Space
Authors: Rickard Br\"uel-Gabrielsson, Tongzhou Wang, Manel Baradad, Justin
  Solomon
Categories: cs.LG cs.CL cs.CV
\\ ( https://arxiv.org/abs/2303.14537 ,  1732kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15963
replaced with revised version Tue, 27 Feb 2024 12:42:15 GMT   (1557kb)

Title: Multimodal and multicontrast image fusion via deep generative models
Authors: Giovanna Maria Dimitri, Simeon Spasov, Andrea Duggento, Luca
  Passamonti, Pietro Li`o, Nicola Toschi
Categories: cs.LG
DOI: 10.1016/j.inffus.2022.07.017
\\ ( https://arxiv.org/abs/2303.15963 ,  1557kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12501
replaced with revised version Tue, 27 Feb 2024 04:49:22 GMT   (1410kb,D)

Title: The cross-sectional stock return predictions via quantum neural network
  and tensor network
Authors: Nozomu Kobayashi, Yoshiyuki Suimon, Koichi Miyamoto, Kosuke Mitarai
Categories: cs.LG q-fin.CP quant-ph
Comments: 19 pages, 7 figures
Journal-ref: Quantum Mach. Intell. 5, 46 (2023)
DOI: 10.1007/s42484-023-00136-x
\\ ( https://arxiv.org/abs/2304.12501 ,  1410kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19044
replaced with revised version Tue, 27 Feb 2024 03:15:20 GMT   (2539kb,D)

Title: Exploring the Promise and Limits of Real-Time Recurrent Learning
Authors: Kazuki Irie, Anand Gopalakrishnan, J\"urgen Schmidhuber
Categories: cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2305.19044 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19891
replaced with revised version Tue, 27 Feb 2024 10:07:06 GMT   (2636kb,D)

Title: Dynamic Neighborhood Construction for Structured Large Discrete Action
  Spaces
Authors: Fabian Akkerman, Julius Luy, Wouter van Heeswijk, Maximilian Schiffer
Categories: cs.LG cs.AI
Comments: ICLR 2024 Camera ready version.
  https://openreview.net/forum?id=80wh3jjCZf
\\ ( https://arxiv.org/abs/2305.19891 ,  2636kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02066
replaced with revised version Tue, 27 Feb 2024 16:18:27 GMT   (22564kb,D)

Title: Variational Gaussian Process Diffusion Processes
Authors: Prakhar Verma, Vincent Adam, Arno Solin
Categories: cs.LG stat.ML
Comments: International Conference on Artificial Intelligence and Statistics
  (AISTATS) 2024
\\ ( https://arxiv.org/abs/2306.02066 ,  22564kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08649
replaced with revised version Tue, 27 Feb 2024 17:34:43 GMT   (1612kb,D)

Title: OCAtari: Object-Centric Atari 2600 Reinforcement Learning Environments
Authors: Quentin Delfosse, Jannis Bl\"uml, Bjarne Gregori, Sebastian
  Sztwiertnia, Kristian Kersting
Categories: cs.LG cs.AI cs.CV
Comments: 26 pages, 8 main paper pages, 36 appendix pages. In main paper: 4
  figures, 3 tables
\\ ( https://arxiv.org/abs/2306.08649 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15924
replaced with revised version Tue, 27 Feb 2024 13:20:54 GMT   (262kb)

Title: The curse of dimensionality in operator learning
Authors: Samuel Lanthaler and Andrew M. Stuart
Categories: cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2306.15924 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04661
replaced with revised version Tue, 27 Feb 2024 04:06:24 GMT   (26kb)

Title: On the power of graph neural networks and the role of the activation
  function
Authors: Sammy Khalife, Amitabh Basu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2307.04661 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08816
replaced with revised version Tue, 27 Feb 2024 18:55:22 GMT   (1010kb,D)

Title: Accelerating Cutting-Plane Algorithms via Reinforcement Learning
  Surrogates
Authors: Kyle Mana, Fernando Acero, Stephen Mak, Parisa Zehtabi, Michael
  Cashmore, Daniele Magazzeni, Manuela Veloso
Categories: cs.LG cs.AI math.OC
Comments: Extended version (includes Supplementary Material). Accepted at AAAI
  24 Main Track with Oral Presentation
\\ ( https://arxiv.org/abs/2307.08816 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12499
replaced with revised version Tue, 27 Feb 2024 13:39:09 GMT   (14990kb,D)

Title: AdvDiff: Generating Unrestricted Adversarial Examples using Diffusion
  Models
Authors: Xuelong Dai, Kaisheng Liang and Bin Xiao
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2307.12499 ,  14990kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00177
replaced with revised version Tue, 27 Feb 2024 00:41:32 GMT   (3471kb,D)

Title: Pretrained deep models outperform GBDTs in Learning-To-Rank under label
  scarcity
Authors: Charlie Hou, Kiran Koshy Thekumparampil, Michael Shavlovsky, Giulia
  Fanti, Yesh Dattatreya, Sujay Sanghavi
Categories: cs.LG cs.AI
Comments: ICML-MFPL 2023 Workshop Oral
\\ ( https://arxiv.org/abs/2308.00177 ,  3471kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12532
replaced with revised version Tue, 27 Feb 2024 01:21:16 GMT   (4745kb,D)

Title: FedSOL: Stabilized Orthogonal Learning in Federated Learning
Authors: Gihun Lee, Minchan Jeong, Sangmook Kim, Jaehoon Oh, Se-Young Yun
Categories: cs.LG cs.AI cs.CV
Comments: The IEEE/CVF Conference on Computer Vision and Pattern Recognition
  2024 (CVPR 2024)
\\ ( https://arxiv.org/abs/2308.12532 ,  4745kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10831
replaced with revised version Mon, 26 Feb 2024 21:51:13 GMT   (594kb,D)

Title: Actively Learning Reinforcement Learning: A Stochastic Optimal Control
  Approach
Authors: Mohammad S. Ramadan, Mahmoud A. Hayajnh, Michael T. Tolley, Kyriakos
  G. Vamvoudakis
Categories: cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2309.10831 ,  594kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16595
replaced with revised version Tue, 27 Feb 2024 00:45:03 GMT   (492kb,D)

Title: Can LLMs Effectively Leverage Graph Structural Information through
  Prompts, and Why?
Authors: Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.16595 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09650
replaced with revised version Tue, 27 Feb 2024 17:16:59 GMT   (882kb)

Title: Multimodal Federated Learning in Healthcare: a Review
Authors: Jacob Thrasher, Alina Devkota, Prasiddha Siwakotai, Rohit Chivukula,
  Pranav Poudel, Chaunbo Hu, Binod Bhattarai, Prashnna Gyawali
Categories: cs.LG cs.AI
Comments: 28 pages, 5 figures
\\ ( https://arxiv.org/abs/2310.09650 ,  882kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10556
replaced with revised version Mon, 26 Feb 2024 23:19:35 GMT   (265kb,D)

Title: Sample Complexity of Preference-Based Nonparametric Off-Policy
  Evaluation with Deep Networks
Authors: Zihao Li, Xiang Ji, Minshuo Chen, Mengdi Wang
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.10556 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11714
replaced with revised version Tue, 27 Feb 2024 09:06:43 GMT   (7396kb,D)

Title: On the Evaluation of Generative Models in Distributed Learning Tasks
Authors: Zixiao Wang, Farzan Farnia, Zhenghao Lin, Yunheng Shen, Bei Yu
Categories: cs.LG
Comments: 20 pages, 20 figures
\\ ( https://arxiv.org/abs/2310.11714 ,  7396kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14341
replaced with revised version Tue, 27 Feb 2024 13:10:07 GMT   (484kb,D)

Title: Pyramidal Hidden Markov Model For Multivariate Time Series Forecasting
Authors: YeXin Huang
Categories: cs.LG
Comments: 8 pages,3 figures
\\ ( https://arxiv.org/abs/2310.14341 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14901
replaced with revised version Tue, 27 Feb 2024 14:13:44 GMT   (14567kb,D)

Title: Series of Hessian-Vector Products for Tractable Saddle-Free Newton
  Optimisation of Neural Networks
Authors: Elre T. Oldewage, Ross M. Clarke and Jos\'e Miguel Hern\'andez-Lobato
Categories: cs.LG stat.ML
Comments: 37 pages, 10 figures, 5 tables. To appear in TMLR. First two authors'
  order randomised
\\ ( https://arxiv.org/abs/2310.14901 ,  14567kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19608
replaced with revised version Tue, 27 Feb 2024 09:35:00 GMT   (2283kb,D)

Title: On Feynman--Kac training of partial Bayesian neural networks
Authors: Zheng Zhao and Sebastian Mair and Thomas B. Sch\"on and Jens Sj\"olund
Categories: cs.LG stat.ML
Comments: In AISTATS 2024
\\ ( https://arxiv.org/abs/2310.19608 ,  2283kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14402
replaced with revised version Tue, 27 Feb 2024 04:29:37 GMT   (4190kb,D)

Title: TEA: Test-time Energy Adaptation
Authors: Yige Yuan, Bingbing Xu, Liang Hou, Fei Sun, Huawei Shen, Xueqi Cheng
Categories: cs.LG
Comments: Accepted by IEEE/CVF Computer Vision and Pattern Recognition
  Conference (CVPR 2024). Code is available at https://github.com/yuanyige/tea
\\ ( https://arxiv.org/abs/2311.14402 ,  4190kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18460
replaced with revised version Tue, 27 Feb 2024 16:39:49 GMT   (946kb,D)

Title: Causal Fairness under Unobserved Confounding: A Neural Sensitivity
  Framework
Authors: Maresa Schr\"oder, Dennis Frauen, Stefan Feuerriegel
Categories: cs.LG cs.AI cs.CY stat.ME
\\ ( https://arxiv.org/abs/2311.18460 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11462
replaced with revised version Tue, 27 Feb 2024 05:42:31 GMT   (203kb,D)

Title: Cascade Speculative Drafting for Even Faster LLM Inference
Authors: Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Kevin Chen-Chuan
  Chang, Jie Huang
Categories: cs.LG cs.CL
Comments: Preprint in progress
\\ ( https://arxiv.org/abs/2312.11462 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14591
replaced with revised version Tue, 27 Feb 2024 18:46:10 GMT   (10152kb,D)

Title: Ricci flow-guided autoencoders in learning time-dependent dynamics
Authors: Andrew Gracyk
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.14591 ,  10152kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15330
replaced with revised version Tue, 27 Feb 2024 13:43:55 GMT   (11086kb,D)

Title: Optimal Sparse Survival Trees
Authors: Rui Zhang, Rui Xin, Margo Seltzer, Cynthia Rudin
Categories: cs.LG
Comments: AISTATS2024 camera ready version. arXiv admin note: text overlap with
  arXiv:2211.14980
\\ ( https://arxiv.org/abs/2401.15330 ,  11086kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00849
replaced with revised version Mon, 26 Feb 2024 21:30:37 GMT   (959kb,D)

Title: Score-based Causal Representation Learning: Linear and General
  Transformations
Authors: Burak Var{\i}c{\i}, Emre Acart\"urk, Karthikeyan Shanmugam, Abhishek
  Kumar, Ali Tajer
Categories: cs.LG stat.ML
Comments: (updated literature review) Linear transformations: stronger results
  than our previous paper Score-based Causal Representation Learning with
  Interventions (arXiv:2301.08230). General transformations: results also
  appear in our paper General Identifiability and Achievability for Causal
  Representation Learning (arXiv:2310.15450) accepted to AISTATS 2024 (oral).
  arXiv admin note: text overlap with arXiv:2310.15450
\\ ( https://arxiv.org/abs/2402.00849 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01055
replaced with revised version Tue, 27 Feb 2024 12:24:13 GMT   (64kb,D)

Title: Multiclass Learning from Noisy Labels for Non-decomposable Performance
  Measures
Authors: Mingyuan Zhang, Shivani Agarwal
Categories: cs.LG stat.ML
Comments: Accepted to AISTATS 2024
\\ ( https://arxiv.org/abs/2402.01055 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01481
replaced with revised version Tue, 27 Feb 2024 10:25:17 GMT   (2979kb,D)

Title: Multi-level protein pre-training with Vabs-Net
Authors: Jiale Zhao, Wanru Zhuang, Jia Song, Yaqi Li, Shuqi Lu
Categories: cs.LG cs.AI q-bio.BM
\\ ( https://arxiv.org/abs/2402.01481 ,  2979kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02725
replaced with revised version Mon, 26 Feb 2024 22:49:36 GMT   (732kb)

Title: Cybersickness Detection through Head Movement Patterns: A Promising
  Approach
Authors: Masoud Salehi, Nikoo Javadpour, Brietta Beisner, Mohammadamin Sanaei,
  Stephen B. Gilbert
Categories: cs.LG eess.SP
Comments: 18 pages, 3 Figures, 3 Tables
\\ ( https://arxiv.org/abs/2402.02725 ,  732kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03299
replaced with revised version Tue, 27 Feb 2024 00:09:00 GMT   (5101kb,D)

Title: GUARD: Role-playing to Generate Natural-language Jailbreakings to Test
  Guideline Adherence of Large Language Models
Authors: Haibo Jin, Ruoxi Chen, Andy Zhou, Jinyin Chen, Yang Zhang, Haohan Wang
Categories: cs.LG cs.CL cs.CV
Comments: 22 papges
\\ ( https://arxiv.org/abs/2402.03299 ,  5101kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03563
replaced with revised version Tue, 27 Feb 2024 07:37:08 GMT   (5207kb,D)

Title: Distinguishing the Knowable from the Unknowable with Language Models
Authors: Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, Benjamin L. Edelman
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.03563 ,  5207kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04249
replaced with revised version Tue, 27 Feb 2024 04:43:08 GMT   (2612kb,D)

Title: HarmBench: A Standardized Evaluation Framework for Automated Red Teaming
  and Robust Refusal
Authors: Mantas Mazeika, Long Phan, Xuwang Yin, Andy Zou, Zifan Wang, Norman
  Mu, Elham Sakhaee, Nathaniel Li, Steven Basart, Bo Li, David Forsyth, Dan
  Hendrycks
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: Website: https://www.harmbench.org
\\ ( https://arxiv.org/abs/2402.04249 ,  2612kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10870
replaced with revised version Mon, 26 Feb 2024 19:55:34 GMT   (6314kb,D)

Title: Best of Three Worlds: Adaptive Experimentation for Digital Marketing in
  Practice
Authors: Tanner Fiez, Houssam Nassif, Yu-Cheng Chen, Sergio Gamez, Lalit Jain
Categories: cs.LG stat.ME
Journal-ref: The Web Conference (WWW), Singapore, 2024
\\ ( https://arxiv.org/abs/2402.10870 ,  6314kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11793
replaced with revised version Tue, 27 Feb 2024 03:18:55 GMT   (7448kb,D)

Title: Generative Kaleidoscopic Networks
Authors: Harsh Shrivastava
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.11793 ,  7448kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13233
replaced with revised version Tue, 27 Feb 2024 00:25:25 GMT   (6707kb,D)

Title: SMORE: Similarity-based Hyperdimensional Domain Adaptation for
  Multi-Sensor Time Series Classification
Authors: Junyao Wang, Mohammad Abdullah Al Faruque
Categories: cs.LG
Comments: arXiv admin note: text overlap with arXiv:2308.03295
\\ ( https://arxiv.org/abs/2402.13233 ,  6707kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13241
replaced with revised version Tue, 27 Feb 2024 04:45:47 GMT   (2293kb,D)

Title: Federated Causal Discovery from Heterogeneous Data
Authors: Loka Li, Ignavier Ng, Gongxu Luo, Biwei Huang, Guangyi Chen, Tongliang
  Liu, Bin Gu, Kun Zhang
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2402.13241 ,  2293kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13516
replaced with revised version Tue, 27 Feb 2024 07:27:07 GMT   (229kb,D)

Title: ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity
  within Large Language Models
Authors: Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai
  Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun
Categories: cs.LG cs.AI cs.CL
Comments: 16 pages, 3 figures, 7 tables
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.13516 ,  229kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13946
replaced with revised version Mon, 26 Feb 2024 20:18:38 GMT   (550kb,D)

Title: AttackGNN: Red-Teaming GNNs in Hardware Security Using Reinforcement
  Learning
Authors: Vasudev Gohil, Satwik Patnaik, Dileep Kalathil, Jeyavijayan Rajendran
Categories: cs.LG cs.CR
Comments: To appear in USENIX Security Symposium, 2024
\\ ( https://arxiv.org/abs/2402.13946 ,  550kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14035
replaced with revised version Tue, 27 Feb 2024 18:44:36 GMT   (258kb,D)

Title: Wisdom of Committee: Distilling from Foundation Model to Specialized
  Application Model
Authors: Zichang Liu, Qingyun Liu, Yuening Li, Liang Liu, Anshumali
  Shrivastava, Shuchao Bi, Lichan Hong, Ed H. Chi, Zhe Zhao
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.14035 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14228
replaced with revised version Tue, 27 Feb 2024 08:47:37 GMT   (250kb,D)

Title: COPR: Continual Human Preference Learning via Optimal Policy
  Regularization
Authors: Han Zhang, Lin Gui, Yu Lei, Yuanzhao Zhai, Yehong Zhang, Yulan He, Hui
  Wang, Yue Yu, Kam-Fai Wong, Bin Liang, Ruifeng Xu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.14228 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15180
replaced with revised version Tue, 27 Feb 2024 01:39:20 GMT   (4352kb,D)

Title: Break the Breakout: Reinventing LM Defense Against Jailbreak Attacks
  with Self-Refinement
Authors: Heegyu Kim, Sehyun Yuk, Hyunsouk Cho
Categories: cs.LG cs.CL cs.CR
Comments: under review
\\ ( https://arxiv.org/abs/2402.15180 ,  4352kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15183
replaced with revised version Tue, 27 Feb 2024 08:22:11 GMT   (959kb,D)

Title: GraphEdit: Large Language Models for Graph Structure Learning
Authors: Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei,
  Liang Pang, Tat-Seng Chua, Chao Huang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.15183 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15958
replaced with revised version Tue, 27 Feb 2024 05:54:10 GMT   (1195kb,D)

Title: On the dynamics of three-layer neural networks: initial condensation
Authors: Zheng-An Chen, Tao Luo
Categories: cs.LG math.DS
MSC-class: 37N40, 68T07, 34E05, 34C11
\\ ( https://arxiv.org/abs/2402.15958 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15968
replaced with revised version Tue, 27 Feb 2024 17:55:44 GMT   (13803kb,D)

Title: CoDream: Exchanging dreams instead of models for federated aggregation
  with heterogeneous models
Authors: Abhishek Singh, Gauri Gupta, Ritvik Kapila, Yichuan Shi, Alex Dang,
  Sheshank Shankar, Mohammed Ehab, Ramesh Raskar
Categories: cs.LG cs.AI
Comments: 16 pages, 12 figures, 5 tables
\\ ( https://arxiv.org/abs/2402.15968 ,  13803kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16359
replaced with revised version Tue, 27 Feb 2024 18:54:40 GMT   (6127kb,D)

Title: Feedback Efficient Online Fine-Tuning of Diffusion Models
Authors: Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali,
  Gabriele Scalia, Nathaniel Lee Diamant, Alex M Tseng, Sergey Levine, Tommaso
  Biancalani
Categories: cs.LG cs.AI q-bio.QM stat.ML
Comments: Under review (codes will be released soon)
\\ ( https://arxiv.org/abs/2402.16359 ,  6127kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16726
replaced with revised version Tue, 27 Feb 2024 04:58:24 GMT   (19645kb,D)

Title: Interpreting Grokked Transformers in Complex Modular Arithmetic
Authors: Hiroki Furuta, Gouki Minegishi, Yusuke Iwasawa, Yutaka Matsuo
Categories: cs.LG cs.AI
Comments: Code: https://github.com/frt03/grok_mod_poly
\\ ( https://arxiv.org/abs/2402.16726 ,  19645kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16842
replaced with revised version Tue, 27 Feb 2024 18:06:29 GMT   (892kb,D)

Title: Asymmetry in Low-Rank Adapters of Foundation Models
Authors: Jiacheng Zhu, Kristjan Greenewald, Kimia Nadjahi, Haitz S\'aez de
  Oc\'ariz Borde, Rickard Br\"uel Gabrielsson, Leshem Choshen, Marzyeh
  Ghassemi, Mikhail Yurochkin, Justin Solomon
Categories: cs.LG
Comments: 17 pages, 2 figures, 9 tables
\\ ( https://arxiv.org/abs/2402.16842 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2205.15862
replaced with revised version Tue, 27 Feb 2024 10:59:33 GMT   (7288kb)

Title: Snapture -- A Novel Neural Architecture for Combined Static and Dynamic
  Hand Gesture Recognition
Authors: Hassan Ali, Doreen Jirak, Stefan Wermter
Categories: cs.CV cs.AI cs.LG
Comments: In Cognitive Computation(Accepted:30/06/2023,
  Published:17/07/2023),20 pages,20 figures,4 tables;Please find the published
  version/info to cite:
  https://doi.org/10.1007/s12559-023-10174-z;Repositories:
  https://zenodo.org/doi/10.5281/zenodo.10679196,
  https://zenodo.org/doi/10.5281/zenodo.10693816;This work was co-funded by
  Horizon Europe project TERAIS under Grant agreement number 101079338
ACM-class: I.2.10; I.5.4; I.4.9
Journal-ref: Cognitive Computation 15, 2014-2033 (2023)
DOI: 10.1007/s12559-023-10174-z
\\ ( https://arxiv.org/abs/2205.15862 ,  7288kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01622 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 09:42:10 GMT   (13271kb,D)

Title: Private, fair and accurate: Training large-scale, privacy-preserving AI
  models in medical imaging
Authors: Soroosh Tayebi Arasteh, Alexander Ziller, Christiane Kuhl, Marcus
  Makowski, Sven Nebelung, Rickmer Braren, Daniel Rueckert, Daniel Truhn,
  Georgios Kaissis
Categories: eess.IV cs.AI cs.CR cs.CV cs.LG
Comments: Published in Communications Medicine. Nature Portfolio
Journal-ref: Commun Med 4 (2024)
DOI: 10.1038/s43856-024-00462-6
\\ ( https://arxiv.org/abs/2302.01622 ,  13271kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11919
replaced with revised version Tue, 27 Feb 2024 09:47:33 GMT   (4131kb,D)

Title: PEM: Perception Error Model for Virtual Testing of Autonomous Vehicles
Authors: Andrea Piazzoni, Jim Cherian, Justin Dauwels, Lap-Pui Chau
Categories: cs.RO cs.AI
Comments: 12 pages, 10 figures. This is a preprint, and version 2 only updates
  the title and the reference to the final published article, which can be
  found at DOI: 10.1109/TITS.2023.3311633
ACM-class: C.4; I.2; I.6
Journal-ref: IEEE Transactions on Intelligent Transportation Systems, vol. 25,
  no. 1, pp. 670-681, Jan. 2024
DOI: 10.1109/TITS.2023.3311633
\\ ( https://arxiv.org/abs/2302.11919 ,  4131kb)
------------------------------------------------------------------------------
\\
arXiv:2303.01230
replaced with revised version Tue, 27 Feb 2024 07:03:15 GMT   (706kb,D)

Title: Synthetic Data: Methods, Use Cases, and Risks
Authors: Emiliano De Cristofaro
Categories: cs.CR cs.AI cs.CY
Comments: To Appear in IEEE Security and Privacy Magazine
\\ ( https://arxiv.org/abs/2303.01230 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16564
replaced with revised version Tue, 27 Feb 2024 15:08:57 GMT   (0kb,I)

Title: Implicit Visual Bias Mitigation by Posterior Estimate Sharpening of a
  Bayesian Neural Network
Authors: Rebecca S Stone, Nishant Ravikumar, Andrew J Bulpitt, David C Hogg
Categories: cs.CV cs.AI
Comments: We are revising this paper with significant changes
\\ ( https://arxiv.org/abs/2303.16564 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15698
replaced with revised version Mon, 26 Feb 2024 21:17:41 GMT   (913kb,D)

Title: Rethinking Diversity in Deep Neural Network Testing
Authors: Zi Wang, Jihye Choi, Ke Wang, Somesh Jha
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2305.15698 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00003 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 17:12:18 GMT   (3978kb,D)

Title: Detecting Heart Disease from Multi-View Ultrasound Images via Supervised
  Attention Multiple Instance Learning
Authors: Zhe Huang, Benjamin S. Wessler, Michael C.Hughes
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Echocardiogram; multiple-instance learning; self-supervised learning;
  semi-supervised learning; medical imaging
Journal-ref: MLHC 2023
\\ ( https://arxiv.org/abs/2306.00003 ,  3978kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00566
replaced with revised version Tue, 27 Feb 2024 18:59:14 GMT   (2527kb,D)

Title: Stochastic positional embeddings improve masked image modeling
Authors: Amir Bar, Florian Bordes, Assaf Shocher, Mahmoud Assran, Pascal
  Vincent, Nicolas Ballas, Trevor Darrell, Amir Globerson, Yann LeCun
Categories: cs.CV cs.AI cs.LG
Comments: Code and models available in https://github.com/amirbar/StoP
\\ ( https://arxiv.org/abs/2308.00566 ,  2527kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08541
replaced with revised version Mon, 26 Feb 2024 20:57:33 GMT   (194kb,D)

Title: When do Generative Query and Document Expansions Fail? A Comprehensive
  Study Across Methods, Retrievers, and Datasets
Authors: Orion Weller, Kyle Lo, David Wadden, Dawn Lawrie, Benjamin Van Durme,
  Arman Cohan, Luca Soldaini
Categories: cs.IR cs.AI cs.CL
Comments: EACL 2024 camera ready
\\ ( https://arxiv.org/abs/2309.08541 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13633
replaced with revised version Tue, 27 Feb 2024 17:10:30 GMT   (5374kb,D)

Title: EvalLM: Interactive Evaluation of Large Language Model Prompts on
  User-Defined Criteria
Authors: Tae Soo Kim, Yoonjoo Lee, Jamin Shin, Young-Ho Kim, Juho Kim
Categories: cs.HC cs.AI cs.CL
Comments: Accepted to CHI 2024
DOI: 10.1145/3613904.3642216
\\ ( https://arxiv.org/abs/2309.13633 ,  5374kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17264
replaced with revised version Tue, 27 Feb 2024 06:14:59 GMT   (5256kb,D)

Title: A Foundation Model for General Moving Object Segmentation in Medical
  Images
Authors: Zhongnuo Yan, Tong Han, Yuhao Huang, Lian Liu, Han Zhou, Jiongquan
  Chen, Wenlong Shi, Yan Cao, Xin Yang, Dong Ni
Categories: cs.CV cs.AI cs.LG
Comments: 5 pages, 7 figures, 3 tables. This paper has been accepted by ISBI
  2024
\\ ( https://arxiv.org/abs/2309.17264 ,  5256kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06823 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 15:33:52 GMT   (6818kb,D)

Title: NECO: NEural Collapse Based Out-of-distribution detection
Authors: Mou\"in Ben Ammar, Nacim Belkhir, Sebastian Popescu, Antoine
  Manzanera, Gianni Franchi
Categories: stat.ML cs.AI cs.CV cs.LG
Comments: Accepted to ICLR2024
\\ ( https://arxiv.org/abs/2310.06823 ,  6818kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19944
replaced with revised version Tue, 27 Feb 2024 08:16:32 GMT   (758kb,D)

Title: Conditional Unscented Autoencoders for Trajectory Prediction
Authors: Faris Janjo\v{s}, Marcel Hallgarten, Anthony Knittel, Maxim Dolgov,
  Andreas Zell, J. Marius Z\"ollner
Categories: cs.RO cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2310.19944 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06310 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 01:20:07 GMT   (2109kb,D)

Title: Labor Space: A Unifying Representation of the Labor Market via Large
  Language Models
Authors: Seongwoon Kim, Yong-Yeol Ahn, Jaehyuk Park
Categories: physics.soc-ph cs.AI
Comments: 11 pages, 5 figures
DOI: 10.1145/3589334.3645464
\\ ( https://arxiv.org/abs/2311.06310 ,  2109kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10801 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 08:08:03 GMT   (1451kb,D)

Title: Reinforcement Learning with Maskable Stock Representation for Portfolio
  Management in Customizable Stock Pools
Authors: Wentao Zhang, Yilei Zhao, Shuo Sun, Jie Ying, Yonggang Xie, Zitao
  Song, Xinrun Wang, Bo An
Categories: q-fin.PM cs.AI cs.CE cs.LG
\\ ( https://arxiv.org/abs/2311.10801 ,  1451kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01655 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 06:07:04 GMT   (3362kb,D)

Title: Quantum Polar Metric Learning: Efficient Classically Learned Quantum
  Embeddings
Authors: Vinayak Sharma and Aviral Shrivastava
Categories: quant-ph cs.AI
ACM-class: I.2.6; E.4
\\ ( https://arxiv.org/abs/2312.01655 ,  3362kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08616
replaced with revised version Tue, 27 Feb 2024 11:21:41 GMT   (6098kb,D)

Title: A Generalized Neural Diffusion Framework on Graphs
Authors: Yibo Li, Xiao Wang, Hongrui Liu, Chuan Shi
Categories: cs.SI cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2312.08616 ,  6098kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00499 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 12:08:43 GMT   (2845kb)

Title: Generating High-Precision Force Fields for Molecular Dynamics
  Simulations to Study Chemical Reaction Mechanisms using Molecular
  Configuration Transformer
Authors: Sihao Yuan, Xu Han, Jun Zhang, Zhaoxin Xie, Cheng Fan, Yunlong Xiao,
  Yi Qin Gao, Yi Issac Yang
Categories: physics.chem-ph cond-mat.soft cs.AI
\\ ( https://arxiv.org/abs/2401.00499 ,  2845kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13641
replaced with revised version Tue, 27 Feb 2024 11:00:35 GMT   (21273kb,D)

Title: How Good is ChatGPT at Face Biometrics? A First Look into Recognition,
  Soft Biometrics, and Explainability
Authors: Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami
  Morales, Julian Fierrez, Javier Ortega-Garcia
Categories: cs.CV cs.AI cs.CY cs.LG
Journal-ref: IEEE Access, February 2024
DOI: 10.1109/ACCESS.2024.3370437
\\ ( https://arxiv.org/abs/2401.13641 ,  21273kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07452
replaced with revised version Tue, 27 Feb 2024 02:19:54 GMT   (3516kb,D)

Title: TriAug: Out-of-Distribution Detection for Imbalanced Breast Lesion in
  Ultrasound
Authors: Yinyu Ye, Shijing Chen, Dong Ni, Ruobing Huang
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.07452 ,  3516kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08164 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 22:12:37 GMT   (19kb)

Title: On Limitations of the Transformer Architecture
Authors: Binghui Peng, Srini Narayanan, Christos Papadimitriou
Categories: stat.ML cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.08164 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15267
replaced with revised version Mon, 26 Feb 2024 21:30:45 GMT   (3228kb,D)

Title: A Robust Defense against Adversarial Attacks on Deep Learning-based
  Malware Detectors via (De)Randomized Smoothing
Authors: Daniel Gibert, Giulio Zizzo, Quan Le, Jordi Planes
Categories: cs.CR cs.AI
Comments: arXiv admin note: text overlap with arXiv:2308.08906
\\ ( https://arxiv.org/abs/2402.15267 ,  3228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16542
replaced with revised version Tue, 27 Feb 2024 08:57:43 GMT   (21396kb,D)

Title: RoboGrind: Intuitive and Interactive Surface Treatment with Industrial
  Robots
Authors: Benjamin Alt, Florian St\"ockl, Silvan M\"uller, Christopher Braun,
  Julian Raible, Saad Alhasan, Oliver Rettig, Lukas Ringle, Darko Katic, Rainer
  J\"akel, Michael Beetz, Marcus Strand and Marco F. Huber
Categories: cs.RO cs.AI
Comments: 7 pages, 6 figures, accepted to the 2024 IEEE International
  Conference on Robotics and Automation (ICRA 2024)
MSC-class: 68T40
ACM-class: I.2.6; I.2.2; I.2.9
\\ ( https://arxiv.org/abs/2402.16542 ,  21396kb)
------------------------------------------------------------------------------
\\
arXiv:2109.02473
replaced with revised version Tue, 27 Feb 2024 00:18:22 GMT   (3372kb,D)

Title: A Robust Cybersecurity Topic Classification Tool
Authors: Elijah Pelofske, Lorie M. Liebrock, Vincent Urias
Categories: cs.IR cs.CL cs.CR cs.LG
Comments: Improved formatting
\\ ( https://arxiv.org/abs/2109.02473 ,  3372kb)
------------------------------------------------------------------------------
\\
arXiv:2210.17264
replaced with revised version Tue, 27 Feb 2024 13:14:51 GMT   (0kb,I)

Title: Cross-lingual Text-To-Speech with Flow-based Voice Conversion for
  Improved Pronunciation
Authors: Nikolaos Ellinas, Georgios Vamvoukakis, Konstantinos Markopoulos,
  Georgia Maniati, Panos Kakoulidis, June Sig Sung, Inchul Hwang, Spyros
  Raptis, Aimilios Chalamandaris, Pirros Tsiakoulis
Categories: cs.SD cs.CL cs.LG eess.AS
Comments: Fundamental changes to the model described and experimental procedure
\\ ( https://arxiv.org/abs/2210.17264 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07614
replaced with revised version Mon, 26 Feb 2024 20:55:25 GMT   (5827kb,D)

Title: NevIR: Negation in Neural Information Retrieval
Authors: Orion Weller, Dawn Lawrie, Benjamin Van Durme
Categories: cs.IR cs.CL
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2305.07614 ,  5827kb)
------------------------------------------------------------------------------
\\
arXiv:2103.09603 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 16:10:16 GMT   (825kb,D)

Title: DoubleML -- An Object-Oriented Implementation of Double Machine Learning
  in R
Authors: Philipp Bach, Victor Chernozhukov, Malte S. Kurz, Martin Spindler,
  Sven Klaassen
Categories: stat.ML cs.LG econ.EM
Comments: 56 pages, 8 Figures, 1 Table; Updated version for DoubleML 1.0.0
MSC-class: 62-04
Journal-ref: Journal of Statistical Software 2024
DOI: 10.18637/jss.v108.i03
\\ ( https://arxiv.org/abs/2103.09603 ,  825kb)
------------------------------------------------------------------------------
\\
arXiv:2104.07324
replaced with revised version Tue, 27 Feb 2024 17:07:34 GMT   (5922kb,D)

Title: OneLog: Towards End-to-End Training in Software Log Anomaly Detection
Authors: Shayan Hashemi, Mika M\"antyl\"a
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2104.07324 ,  5922kb)
------------------------------------------------------------------------------
\\
arXiv:2106.06682
replaced with revised version Tue, 27 Feb 2024 18:29:18 GMT   (13912kb,D)

Title: Solving PDEs on Unknown Manifolds with Machine Learning
Authors: Senwei Liang and Shixiao W. Jiang and John Harlim and Haizhao Yang
Categories: math.NA cs.LG cs.NA
\\ ( https://arxiv.org/abs/2106.06682 ,  13912kb)
------------------------------------------------------------------------------
\\
arXiv:2111.10275 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 11:32:09 GMT   (1503kb,D)

Title: Composite Goodness-of-fit Tests with Kernels
Authors: Oscar Key, Arthur Gretton, Fran\c{c}ois-Xavier Briol, Tamara Fernandez
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2111.10275 ,  1503kb)
------------------------------------------------------------------------------
\\
arXiv:2210.15658
replaced with revised version Tue, 27 Feb 2024 15:43:00 GMT   (37751kb,D)

Title: All the Feels: A dexterous hand with large-area tactile sensing
Authors: Raunaq Bhirangi, Abigail DeFranco, Jacob Adkins, Carmel Majidi,
  Abhinav Gupta, Tess Hellebrekers, Vikash Kumar
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2210.15658 ,  37751kb)
------------------------------------------------------------------------------
\\
arXiv:2211.02920 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 13:29:56 GMT   (6746kb,D)

Title: GmGM: a Fast Multi-Axis Gaussian Graphical Model
Authors: Bailey Andrew, David Westhead, Luisa Cutillo
Categories: stat.ML cs.LG
Comments: 8 pages (33 additional in supplementary material), 19 figures,
  accepted at AIStats 2024
\\ ( https://arxiv.org/abs/2211.02920 ,  6746kb)
------------------------------------------------------------------------------
\\
arXiv:2211.06617 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 07:36:52 GMT   (314kb,D)

Title: Empirical Risk Minimization with Relative Entropy Regularization
Authors: Samir M. Perlaza, Gaetan Bisson, I\~naki Esnaola, Alain Jean-Marie,
  Stefano Rini
Categories: math.ST cs.IT cs.LG math.IT stat.TH
Comments: To appear in the IEEE Transactions on Information Theory: Submitted
  June 2023. Revised in October 2023. Accepted January 2024. Also available as:
  Research Report, INRIA, No. RR-9454, Centre Inria d'Universit\'e C\^ote
  d'Azur, Sophia Antipolis, France, Feb., 2022. Last version: Version 7
Report-no: RR-9454
DOI: 10.1109/TIT.2024.3365728
\\ ( https://arxiv.org/abs/2211.06617 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05059 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 23:10:53 GMT   (3010kb,D)

Title: Effects of noise on the overparametrization of quantum neural networks
Authors: Diego Garc\'ia-Mart\'in, Martin Larocca, M. Cerezo
Categories: quant-ph cs.LG stat.ML
Comments: 14 + 6 pages, 11 figures
Report-no: LA-UR-22-33019
\\ ( https://arxiv.org/abs/2302.05059 ,  3010kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01899
replaced with revised version Mon, 26 Feb 2024 20:19:21 GMT   (13075kb,D)

Title: Cross-Class Feature Augmentation for Class Incremental Learning
Authors: Taehoon Kim, Jaeyoo Park, Bohyung Han
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2304.01899 ,  13075kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10880 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 03:49:34 GMT   (0kb,I)

Title: Functional sufficient dimension reduction through information
  maximization with application to classification
Authors: Xinyu Li and Jianjun Xu and Wenquan Cui and Haoyang Cheng
Categories: stat.ML cs.LG
Comments: There are some problems with the methodology or experimental design
  in the article that make it impossible to produce reliable conclusions, so we
  have decided to withdraw it
\\ ( https://arxiv.org/abs/2305.10880 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05812 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 13:40:40 GMT   (16078kb,D)

Title: HRTF upsampling with a generative adversarial network using a gnomonic
  equiangular projection
Authors: Aidan O. T. Hogg, Mads Jenkins, He Liu, Isaac Squires, Samuel J.
  Cooper and Lorenzo Picinali
Categories: eess.AS cs.CV cs.HC cs.LG cs.SD eess.SP
Comments: 15 pages, 9 figures, Preprint (Accepted to IEEE/ACM Transactions on
  Audio, Speech, and Language Processing on the 15 Feb 2024)
\\ ( https://arxiv.org/abs/2306.05812 ,  16078kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11339
replaced with revised version Tue, 27 Feb 2024 04:16:20 GMT   (951kb,D)

Title: Masking Augmentation for Supervised Learning
Authors: Byeongho Heo, Taekyung Kim, Sangdoo Yun, Dongyoon Han
Categories: cs.CV cs.LG
Comments: 17 pages, 3 figures
\\ ( https://arxiv.org/abs/2306.11339 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14287 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 14:01:23 GMT   (13334kb,D)

Title: Efficient Contextformer: Spatio-Channel Window Attention for Fast
  Context Modeling in Learned Image Compression
Authors: A. Burakhan Koyuncu, Panqi Jia, Atanas Boev, Elena Alshina, Eckehard
  Steinbach
Categories: eess.IV cs.CV cs.LG
Comments: Accepted for IEEE TCSVT (14 pages, 10 figures, 9 tables)
\\ ( https://arxiv.org/abs/2306.14287 ,  13334kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00676
replaced with revised version Tue, 27 Feb 2024 13:46:11 GMT   (4348kb,D)

Title: Pay Attention to the Atlas: Atlas-Guided Test-Time Adaptation Method for
  Robust 3D Medical Image Segmentation
Authors: Jingjie Guo, Weitong Zhang, Matthew Sinclair, Daniel Rueckert, Chen
  Chen
Categories: cs.CV cs.LG
Comments: Accepted by MICCAI BTSD-1001AI workshop. (Oral
  presentation).https://btsdmiccai.github.io/
\\ ( https://arxiv.org/abs/2307.00676 ,  4348kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10683 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 02:26:27 GMT   (4505kb,D)

Title: Fractional Denoising for 3D Molecular Pre-training
Authors: Shikun Feng and Yuyan Ni and Yanyan Lan and Zhi-Ming Ma and Wei-Ying
  Ma
Categories: q-bio.QM cs.LG physics.chem-ph
\\ ( https://arxiv.org/abs/2307.10683 ,  4505kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00214
replaced with revised version Tue, 27 Feb 2024 10:41:58 GMT   (5028kb)

Title: The Impact of Loss Functions and Scene Representations for 3D/2D
  Registration on Single-view Fluoroscopic X-ray Pose Estimation
Authors: Chaochao Zhou, Syed Hasib Akhter Faruqui, Abhinav Patel, Ramez N.
  Abdalla, Michael C. Hurley, Ali Shaibani, Matthew B. Potts, Babak S. Jahromi,
  Sameer A. Ansari, Donald R. Cantrell
Categories: cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2308.00214 ,  5028kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05254
replaced with revised version Mon, 26 Feb 2024 23:44:44 GMT   (1134kb,D)

Title: Data-driven Intra-Autonomous Systems Graph Generator
Authors: Caio Vinicius Dadauto, Nelson Luis Saldanha da Fonseca and Ricardo da
  Silva Torres
Categories: cs.NI cs.LG
Comments: 14 pages, 15 figures
\\ ( https://arxiv.org/abs/2308.05254 ,  1134kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01657 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 08:53:49 GMT   (880kb,D)

Title: Locally Stationary Graph Processes
Authors: Abdullah Canbolat and Elif Vural
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2309.01657 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04311
replaced with revised version Tue, 27 Feb 2024 18:10:02 GMT   (1693kb,D)

Title: Distributed Deep Joint Source-Channel Coding with Decoder-Only Side
  Information
Authors: Selim F. Yilmaz, Ezgi Ozyilkan, Deniz Gunduz, Elza Erkip
Categories: cs.CV cs.IT cs.LG math.IT
Comments: To appear in IEEE International Conference on Machine Learning for
  Communication and Networking (ICMLCN) 2024
\\ ( https://arxiv.org/abs/2310.04311 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06958
replaced with revised version Tue, 27 Feb 2024 08:34:43 GMT   (515kb,D)

Title: Comparing the Robustness of Modern No-Reference Image- and Video-Quality
  Metrics to Adversarial Attacks
Authors: Anastasia Antsiferova, Khaled Abud, Aleksandr Gushchin, Ekaterina
  Shumitskaya, Sergey Lavrushkin, Dmitriy Vatolin
Categories: cs.CV cs.LG cs.MM eess.IV
\\ ( https://arxiv.org/abs/2310.06958 ,  515kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19683 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 21:47:44 GMT   (177kb,D)

Title: An Online Bootstrap for Time Series
Authors: Nicolai Palm and Thomas Nagler
Categories: stat.ML cs.LG stat.CO stat.ME
\\ ( https://arxiv.org/abs/2310.19683 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07511 (*cross-listing*)
replaced with revised version Mon, 26 Feb 2024 19:33:25 GMT   (1633kb)

Title: Uncertainty estimation in satellite precipitation interpolation with
  machine learning
Authors: Georgia Papacharalampous, Hristos Tyralis, Nikolaos Doulamis,
  Anastasios Doulamis
Categories: stat.ML cs.LG physics.ao-ph stat.AP stat.ME
\\ ( https://arxiv.org/abs/2311.07511 ,  1633kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06454 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 10:54:36 GMT   (3125kb,D)

Title: Point Transformer with Federated Learning for Predicting Breast Cancer
  HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images
Authors: Bao Li, Zhenyu Liu, Lizhi Shao, Bensheng Qiu, Hong Bu, Jie Tian
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.06454 ,  3125kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01663
replaced with revised version Tue, 27 Feb 2024 16:58:26 GMT   (379kb,D)

Title: Killer Apps: Low-Speed, Large-Scale AI Weapons
Authors: Philip Feldman, Aaron Dant, James R. Foulds
Categories: cs.CY cs.CR cs.LG
Comments: 10 pages with 10 pages of appendices. 3 Figures, 2 code listings
ACM-class: I.2.7; H.4.3; J.4
\\ ( https://arxiv.org/abs/2402.01663 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10232 (*cross-listing*)
replaced with revised version Tue, 27 Feb 2024 12:05:09 GMT   (39kb)

Title: Simple, unified analysis of Johnson-Lindenstrauss with applications
Authors: Yingru Li
Categories: stat.ML cs.DS cs.LG math.PR
\\ ( https://arxiv.org/abs/2402.10232 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11585
replaced with revised version Tue, 27 Feb 2024 13:33:05 GMT   (24171kb,D)

Title: PolypNextLSTM: A lightweight and fast polyp video segmentation network
  using ConvNext and ConvLSTM
Authors: Debayan Bhattacharya, Konrad Reuter, Finn Behrendnt, Lennart Maack,
  Sarah Grube, Alexander Schlaefer
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.11585 ,  24171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11996
replaced with revised version Tue, 27 Feb 2024 14:53:53 GMT   (44275kb,D)

Title: ISCUTE: Instance Segmentation of Cables Using Text Embedding
Authors: Shir Kozlovsky, Omkar Joglekar and Dotan Di Castro
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.11996 ,  44275kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16569
replaced with revised version Tue, 27 Feb 2024 13:59:32 GMT   (2864kb,D)

Title: Pretrained Visual Uncertainties
Authors: Michael Kirchhof and Mark Collier and Seong Joon Oh and Enkelejda
  Kasneci
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.16569 ,  2864kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
