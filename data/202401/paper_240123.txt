Gmail jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org> 2024年1月23日 19:04
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri 19 Jan 24 19:00:00 GMT  to  Mon 22 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.10904
Date: Wed, 3 Jan 2024 09:46:36 GMT   (3853kb)

Title: A Review of Findings from Neuroscience and Cognitive Psychology as
  Possible Inspiration for the Path to Artificial General Intelligence
Authors: Florin Leon
Categories: cs.AI
Comments: 143 pages, 49 figures, 244 references
\\
  This review aims to contribute to the quest for artificial general
intelligence by examining neuroscience and cognitive psychology methods for
potential inspiration. Despite the impressive advancements achieved by deep
learning models in various domains, they still have shortcomings in abstract
reasoning and causal understanding. Such capabilities should be ultimately
integrated into artificial intelligence systems in order to surpass data-driven
limitations and support decision making in a way more similar to human
intelligence. This work is a vertical review that attempts a wide-ranging
exploration of brain function, spanning from lower-level biological neurons,
spiking neural networks, and neuronal ensembles to higher-level concepts such
as brain anatomy, vector symbolic architectures, cognitive and categorization
models, and cognitive architectures. The hope is that these concepts may offer
insights for solutions in artificial general intelligence.
\\ ( https://arxiv.org/abs/2401.10904 ,  3853kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10938
Date: Wed, 17 Jan 2024 11:38:58 GMT   (505kb)

Title: Even-if Explanations: Formal Foundations, Priorities and Complexity
Authors: Gianvincenzo Alfano, Sergio Greco, Domenico Mandaglio, Francesco
  Parisi, Reza Shahbazian and Irina Trubitsyna
Categories: cs.AI cs.LG
Comments: arXiv admin note: text overlap with arXiv:2010.12265 by other authors
\\
  EXplainable AI has received significant attention in recent years. Machine
learning models often operate as black boxes, lacking explainability and
transparency while supporting decision-making processes. Local post-hoc
explainability queries attempt to answer why individual inputs are classified
in a certain way by a given model. While there has been important work on
counterfactual explanations, less attention has been devoted to semifactual
ones. In this paper, we focus on local post-hoc explainability queries within
the semifactual `even-if' thinking and their computational complexity among
different classes of models, and show that both linear and tree-based models
are strictly more interpretable than neural networks. After this, we introduce
a preference-based framework that enables users to personalize explanations
based on their preferences, both in the case of semifactuals and
counterfactuals, enhancing interpretability and user-centricity. Finally, we
explore the complexity of several interpretability problems in the proposed
preference-based framework and provide algorithms for polynomial cases.
\\ ( https://arxiv.org/abs/2401.10938 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10969
Date: Fri, 19 Jan 2024 16:32:02 GMT   (4015kb,D)

Title: MacroSwarm: A Field-based Compositional Framework for Swarm Programming
Authors: Gianluca Aguzzi, Roberto Casadei, Mirko Viroli
Categories: cs.AI cs.LO cs.SE
\\
  Swarm behaviour engineering is an area of research that seeks to investigate
methods and techniques for coordinating computation and action within groups of
simple agents to achieve complex global goals like pattern formation,
collective movement, clustering, and distributed sensing. Despite recent
progress in the analysis and engineering of swarms (of drones, robots,
vehicles), there is still a need for general design and implementation methods
and tools that can be used to define complex swarm behaviour in a principled
way. To contribute to this quest, this article proposes a new field-based
coordination approach, called MacroSwarm, to design and program swarm behaviour
in terms of reusable and fully composable functional blocks embedding
collective computation and coordination. Based on the macroprogramming paradigm
of aggregate computing, MacroSwarm builds on the idea of expressing each swarm
behaviour block as a pure function mapping sensing fields into actuation goal
fields, e.g. including movement vectors. In order to demonstrate the
expressiveness, compositionality, and practicality of MacroSwarm as a framework
for collective intelligence, we perform a variety of simulations covering
common patterns of flocking, morphogenesis, and collective decision-making.
\\ ( https://arxiv.org/abs/2401.10969 ,  4015kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11094
Date: Sat, 20 Jan 2024 02:55:11 GMT   (27106kb,D)

Title: TypeDance: Creating Semantic Typographic Logos from Image through
  Personalized Generation
Authors: Shishi Xiao, Liangwei Wang, Xiaojuan Ma, Wei Zeng
Categories: cs.AI
Comments: 24 pages, 9 figures
\\
  Semantic typographic logos harmoniously blend typeface and imagery to
represent semantic concepts while maintaining legibility. Conventional methods
using spatial composition and shape substitution are hindered by the
conflicting requirement for achieving seamless spatial fusion between
geometrically dissimilar typefaces and semantics. While recent advances made AI
generation of semantic typography possible, the end-to-end approaches exclude
designer involvement and disregard personalized design. This paper presents
TypeDance, an AI-assisted tool incorporating design rationales with the
generative model for personalized semantic typographic logo design. It
leverages combinable design priors extracted from uploaded image exemplars and
supports type-imagery mapping at various structural granularity, achieving
diverse aesthetic designs with flexible control. Additionally, we instantiate a
comprehensive design workflow in TypeDance, including ideation, selection,
generation, evaluation, and iteration. A two-task user evaluation, including
imitation and creation, confirmed the usability of TypeDance in design across
different usage scenarios
\\ ( https://arxiv.org/abs/2401.11094 ,  27106kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11472
Date: Sun, 21 Jan 2024 12:22:48 GMT   (220kb,D)

Title: Abstract Weighted Based Gradual Semantics in Argumentation Theory
Authors: Assaf Libman, Nir Oren, Bruno Yun
Categories: cs.AI
\\
  Weighted gradual semantics provide an acceptability degree to each argument
representing the strength of the argument, computed based on factors including
background evidence for the argument, and taking into account interactions
between this argument and others. We introduce four important problems linking
gradual semantics and acceptability degrees. First, we reexamine the inverse
problem, seeking to identify the argument weights of the argumentation
framework which lead to a specific final acceptability degree. Second, we ask
whether the function mapping between argument weights and acceptability degrees
is injective or a homeomorphism onto its image. Third, we ask whether argument
weights can be found when preferences, rather than acceptability degrees for
arguments are considered. Fourth, we consider the topology of the space of
valid acceptability degrees, asking whether gaps exist in this space. While
different gradual semantics have been proposed in the literature, in this
paper, we identify a large family of weighted gradual semantics, called
abstract weighted based gradual semantics. These generalise many of the
existing semantics while maintaining desirable properties such as convergence
to a unique fixed point. We also show that a sub-family of the weighted gradual
semantics, called abstract weighted (Lp,lambda,mu,A)-based gradual semantics
and which include well-known semantics, solve all four of the aforementioned
problems.
\\ ( https://arxiv.org/abs/2401.11472 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11553
Date: Sun, 21 Jan 2024 17:54:46 GMT   (928kb)

Title: Taxi dispatching strategies with compensations
Authors: Holger Billhardt, Alberto Fern\'andez, Sascha Ossowski, Javier
  Palanca, Javier Bajo
Categories: cs.AI
ACM-class: I.2.1
Journal-ref: Expert Systems with Applications, Volume 122 (2019)
DOI: 10.1016/j.eswa.2019.01.001
\\
  Urban mobility efficiency is of utmost importance in big cities. Taxi
vehicles are key elements in daily traffic activity. The advance of ICT and
geo-positioning systems has given rise to new opportunities for improving the
efficiency of taxi fleets in terms of waiting times of passengers, cost and
time for drivers, traffic density, CO2 emissions, etc., by using more informed,
intelligent dispatching. Still, the explicit spatial and temporal components,
as well as the scale and, in particular, the dynamicity of the problem of
pairing passengers and taxis in big towns, render traditional approaches for
solving standard assignment problem useless for this purpose, and call for
intelligent approximation strategies based on domain-specific heuristics.
Furthermore, taxi drivers are often autonomous actors and may not agree to
participate in assignments that, though globally efficient, may not be
sufficently beneficial for them individually. This paper presents a new
heuristic algorithm for taxi assignment to customers that considers taxi
reassignments if this may lead to globally better solutions. In addition, as
such new assignments may reduce the expected revenues of individual drivers, we
propose an economic compensation scheme to make individually rational drivers
agree to proposed modifications in their assigned clients. We carried out a set
of experiments, where several commonly used assignment strategies are compared
to three different instantiations of our heuristic algorithm. The results
indicate that our proposal has the potential to reduce customer waiting times
in fleets of autonomous taxis, while being also beneficial from an economic
point of view.
\\ ( https://arxiv.org/abs/2401.11553 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11753
Date: Mon, 22 Jan 2024 08:28:28 GMT   (1346kb)

Title: From Knowledge Organization to Knowledge Representation and Back
Authors: Fausto Giunchiglia, Mayukh Bagchi and Subhashis Das
Categories: cs.AI cs.DL
Comments: Accepted @ Annals of Library and Information Studies (ALIS) Journal -
  Ranganathan Commemorative Issue (2024)
Report-no: DISI22012024
\\
  Knowledge Organization (KO) and Knowledge Representation (KR) have been the
two mainstream methodologies of knowledge modelling in the Information Science
community and the Artificial Intelligence community, respectively. The
facet-analytical tradition of KO has developed an exhaustive set of guiding
canons for ensuring quality in organising and managing knowledge but has
remained limited in terms of technology-driven activities to expand its scope
and services beyond the bibliographic universe of knowledge. KR, on the other
hand, boasts of a robust ecosystem of technologies and technology-driven
service design which can be tailored to model any entity or scale to any
service in the entire universe of knowledge. This paper elucidates both the
facet-analytical KO and KR methodologies in detail and provides a functional
mapping between them. Out of the mapping, the paper proposes an integrated
KR-enriched KO methodology with all the standard components of a KO methodology
plus the advanced technologies provided by the KR approach. The practical
benefits of the methodological integration has been exemplified through the
flagship application of the Digital University at the University of Trento,
Italy.
\\ ( https://arxiv.org/abs/2401.11753 ,  1346kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11848
Date: Mon, 22 Jan 2024 11:05:54 GMT   (1749kb,D)

Title: ExtruOnt: An ontology for describing a type of manufacturing machine for
  Industry 4.0 systems
Authors: V\'ictor Julio Ram\'irez-Dur\'an, Idoia Berges, Arantza Illarramendi
Categories: cs.AI
Comments: This is the accepted manuscript. The definitive, peer reviewed and
  edited version of this article is published in Semantic Web 11(6): 887-909
  (2020) https://doi.org/10.3233/sw-200376
Journal-ref: Semantic Web 11(6): 887-909 (2020)
DOI: 10.3233/sw-200376
\\
  Semantically rich descriptions of manufacturing machines, offered in a
machine-interpretable code, can provide interesting benefits in Industry 4.0
scenarios. However, the lack of that type of descriptions is evident. In this
paper we present the development effort made to build an ontology, called
ExtruOnt, for describing a type of manufacturing machine, more precisely, a
type that performs an extrusion process (extruder). Although the scope of the
ontology is restricted to a concrete domain, it could be used as a model for
the development of other ontologies for describing manufacturing machines in
Industry 4.0 scenarios. The terms of the ExtruOnt ontology provide different
types of information related with an extruder, which are reflected in distinct
modules that constitute the ontology. Thus, it contains classes and properties
for expressing descriptions about components of an extruder, spatial
connections, features, and 3D representations of those components, and finally
the sensors used to capture indicators about the performance of this type of
machine. The ontology development process has been carried out in close
collaboration with domain experts.
\\ ( https://arxiv.org/abs/2401.11848 ,  1749kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11865
Date: Mon, 22 Jan 2024 11:39:55 GMT   (238kb)

Title: Toward Semantic Interoperability of Electronic Health Records
Authors: Idoia Berges, Jes\'us Berm\'udez, Arantza Illarramendi
Categories: cs.AI
Comments: This is the Accepted Manuscript. The definitive, peer reviewed and
  edited version of this article is: Idoia Berges, Jes\'us Berm\'udez, Arantza
  Illarramendi: Toward Semantic Interoperability of Electronic Health Records.
  IEEE Trans. Inf. Technol. Biomed. 16(3): 424-431 (2012).
  DOI:10.1109/TITB.2011.2180917. Copyright 2011 IEEE
Journal-ref: IEEE Trans. Inf. Technol. Biomed. 16(3): 424-431 (2012)
DOI: 10.1109/TITB.2011.2180917
\\
  Although the goal of achieving semantic interoperability of electronic health
records (EHRs) is pursued by many researchers, it has not been accomplished
yet. In this paper, we present a proposal that smoothes out the way toward the
achievement of that goal. In particular, our study focuses on medical diagnoses
statements. In summary, the main contributions of our ontology-based proposal
are the following: first, it includes a canonical ontology whose EHR-related
terms focus on semantic aspects. As a result, their descriptions are
independent of languages and technology aspects used in different organizations
to represent EHRs. Moreover, those terms are related to their corresponding
codes in well-known medical terminologies. Second, it deals with modules that
allow obtaining rich ontological representations of EHR information managed by
proprietary models of health information systems. The features of one specific
module are shown as reference. Third, it considers the necessary mapping axioms
between ontological terms enhanced with so-called path mappings. This feature
smoothes out structural differences between heterogeneous EHR representations,
allowing proper alignment of information.
\\ ( https://arxiv.org/abs/2401.11865 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11898
Date: Mon, 22 Jan 2024 12:49:08 GMT   (26kb,D)

Title: Automated Completion of Statements and Proofs in Synthetic Geometry: an
  Approach based on Constraint Solving
Authors: Salwa Tabet Gonzalez (University of Strasbourg), Predrag Jani\v{c}i\'c
  (University of Belgrade), Julien Narboux (University of Strasbourg)
Categories: cs.AI cs.LO cs.MS
Comments: In Proceedings ADG 2023, arXiv:2401.10725
Journal-ref: EPTCS 398, 2024, pp. 21-37
DOI: 10.4204/EPTCS.398.6
\\
  Conjecturing and theorem proving are activities at the center of mathematical
practice and are difficult to separate. In this paper, we propose a framework
for completing incomplete conjectures and incomplete proofs. The framework can
turn a conjecture with missing assumptions and with an under-specified goal
into a proper theorem. Also, the proposed framework can help in completing a
proof sketch into a human-readable and machine-checkable proof. Our approach is
focused on synthetic geometry, and uses coherent logic and constraint solving.
The proposed approach is uniform for all three kinds of tasks, flexible and, to
our knowledge, unique such approach.
\\ ( https://arxiv.org/abs/2401.11898 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11903
Date: Mon, 22 Jan 2024 12:50:46 GMT   (127kb,D)

Title: Automation of Triangle Ruler-and-Compass Constructions Using Constraint
  Solvers
Authors: Milan Bankovi\'c (Faculty of Mathematics, University of Belgrade,
  Serbia)
Categories: cs.AI
Comments: In Proceedings ADG 2023, arXiv:2401.10725
Journal-ref: EPTCS 398, 2024, pp. 62-72
DOI: 10.4204/EPTCS.398.10
\\
  In this paper, we present an approach to automated solving of triangle
ruler-and-compass construction problems using finite-domain constraint solvers.
The constraint model is described in the MiniZinc modeling language, and is
based on the automated planning. The main benefit of using general constraint
solvers for such purpose, instead of developing dedicated tools, is that we can
rely on the efficient search that is already implemented within the solver,
enabling us to focus on geometric aspects of the problem. We may also use the
solver's built-in optimization capabilities to search for the shortest possible
constructions. We evaluate our approach on 74 solvable problems from the
Wernick's list, and compare it to the dedicated triangle construction solver
ArgoTriCS. The results show that our approach is comparable to dedicated tools,
while it requires much less effort to implement. Also, our model often finds
shorter constructions, thanks to the optimization capabilities offered by the
constraint solvers.
\\ ( https://arxiv.org/abs/2401.11903 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11905
Date: Mon, 22 Jan 2024 12:51:19 GMT   (215kb,D)

Title: Considerations on Approaches and Metrics in Automated Theorem
  Generation/Finding in Geometry
Authors: Pedro Quaresma (University of Coimbra), Pierluigi Graziani (University
  of Urbino), Stefano M. Nicoletti (University of Twente)
Categories: cs.AI cs.LO
Comments: In Proceedings ADG 2023, arXiv:2401.10725
ACM-class: I.2.3; F4
Journal-ref: EPTCS 398, 2024, pp. 85-100
DOI: 10.4204/EPTCS.398.12
\\
  The pursue of what are properties that can be identified to permit an
automated reasoning program to generate and find new and interesting theorems
is an interesting research goal (pun intended). The automatic discovery of new
theorems is a goal in itself, and it has been addressed in specific areas, with
different methods. The separation of the "weeds", uninteresting, trivial facts,
from the "wheat", new and interesting facts, is much harder, but is also being
addressed by different authors using different approaches. In this paper we
will focus on geometry. We present and discuss different approaches for the
automatic discovery of geometric theorems (and properties), and different
metrics to find the interesting theorems among all those that were generated.
After this description we will introduce the first result of this article: an
undecidability result proving that having an algorithmic procedure that decides
for every possible Turing Machine that produces theorems, whether it is able to
produce also interesting theorems, is an undecidable problem. Consequently, we
will argue that judging whether a theorem prover is able to produce interesting
theorems remains a non deterministic task, at best a task to be addressed by
program based in an algorithm guided by heuristics criteria. Therefore, as a
human, to satisfy this task two things are necessary: an expert survey that
sheds light on what a theorem prover/finder of interesting geometric theorems
is, and - to enable this analysis - other surveys that clarify metrics and
approaches related to the interestingness of geometric theorems. In the
conclusion of this article we will introduce the structure of two of these
surveys - the second result of this article - and we will discuss some future
work.
\\ ( https://arxiv.org/abs/2401.11905 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12108
Date: Mon, 22 Jan 2024 16:45:15 GMT   (202kb,D)

Title: On-Time Delivery in Crowdshipping Systems: An Agent-Based Approach Using
  Streaming Data
Authors: Jeremias D\"otterl, Ralf Bruns, J\"urgen Dunkel, Sascha Ossowski
Categories: cs.AI cs.LG cs.MA
Journal-ref: Frontiers in Artificial Intelligence and Applications. Volume 325:
  ECAI 2020. Pages 51-58
DOI: 10.3233/FAIA200075
\\
  In parcel delivery, the "last mile" from the parcel hub to the customer is
costly, especially for time-sensitive delivery tasks that have to be completed
within hours after arrival. Recently, crowdshipping has attracted increased
attention as a new alternative to traditional delivery modes. In crowdshipping,
private citizens ("the crowd") perform short detours in their daily lives to
contribute to parcel delivery in exchange for small incentives. However,
achieving desirable crowd behavior is challenging as the crowd is highly
dynamic and consists of autonomous, self-interested individuals. Leveraging
crowdshipping for time-sensitive deliveries remains an open challenge. In this
paper, we present an agent-based approach to on-time parcel delivery with
crowds. Our system performs data stream processing on the couriers' smartphone
sensor data to predict delivery delays. Whenever a delay is predicted, the
system attempts to forge an agreement for transferring the parcel from the
current deliverer to a more promising courier nearby. Our experiments show that
through accurate delay predictions and purposeful task transfers many delays
can be prevented that would occur without our approach.
\\ ( https://arxiv.org/abs/2401.12108 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10995
Date: Fri, 19 Jan 2024 19:23:37 GMT   (868kb,D)

Title: The Radiation Oncology NLP Database
Authors: Zhengliang Liu, Jason Holmes, Wenxiong Liao, Chenbin Liu, Lian Zhang,
  Hongying Feng, Peilong Wang, Muhammad Ali Elahi, Hongmin Cai, Lichao Sun,
  Quanzheng Li, Xiang Li, Tianming Liu, Jiajian Shen, Wei Liu
Categories: cs.CL physics.med-ph
Comments: 10 pages, 7 figures, 6 tables
\\
  We present the Radiation Oncology NLP Database (ROND), the first dedicated
Natural Language Processing (NLP) dataset for radiation oncology, an important
medical specialty that has received limited attention from the NLP community in
the past. With the advent of Artificial General Intelligence (AGI), there is an
increasing need for specialized datasets and benchmarks to facilitate research
and development. ROND is specifically designed to address this gap in the
domain of radiation oncology, a field that offers many opportunities for NLP
exploration. It encompasses various NLP tasks including Logic Reasoning, Text
Classification, Named Entity Recognition (NER), Question Answering (QA), Text
Summarization, and Patient-Clinician Conversations, each with a distinct focus
on radiation oncology concepts and application cases. In addition, we have
developed an instruction-tuning dataset consisting of over 20k instruction
pairs (based on ROND) and trained a large language model, CancerChat. This
serves to demonstrate the potential of instruction-tuning large language models
within a highly-specialized medical domain. The evaluation results in this
study could serve as baseline results for future research. ROND aims to
stimulate advancements in radiation oncology and clinical NLP by offering a
platform for testing and improving algorithms and models in a domain-specific
context. The ROND dataset is a joint effort of multiple U.S. health
institutions. The data is available at
https://github.com/zl-liu/Radiation-Oncology-NLP-Database.
\\ ( https://arxiv.org/abs/2401.10995 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11021
Date: Fri, 19 Jan 2024 20:40:23 GMT   (311kb)

Title: Analysis and Detection of Multilingual Hate Speech Using Transformer
  Based Deep Learning
Authors: Arijit Das, Somashree Nandy, Rupam Saha, Srijan Das, and Diganta Saha
Categories: cs.CL cs.AI
Comments: 20 pages
\\
  Hate speech is harmful content that directly attacks or promotes hatred
against members of groups or individuals based on actual or perceived aspects
of identity, such as racism, religion, or sexual orientation. This can affect
social life on social media platforms as hateful content shared through social
media can harm both individuals and communities. As the prevalence of hate
speech increases online, the demand for automated detection as an NLP task is
increasing. In this work, the proposed method is using transformer-based model
to detect hate speech in social media, like twitter, Facebook, WhatsApp,
Instagram, etc. The proposed model is independent of languages and has been
tested on Italian, English, German, Bengali. The Gold standard datasets were
collected from renowned researcher Zeerak Talat, Sara Tonelli, Melanie Siegel,
and Rezaul Karim. The success rate of the proposed model for hate speech
detection is higher than the existing baseline and state-of-the-art models with
accuracy in Bengali dataset is 89%, in English: 91%, in German dataset 91% and
in Italian dataset it is 77%. The proposed algorithm shows substantial
improvement to the benchmark method.
\\ ( https://arxiv.org/abs/2401.11021 ,  311kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11033
Date: Fri, 19 Jan 2024 21:21:02 GMT   (8222kb,D)

Title: FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for
  Large Language Models' Training?
Authors: Shaina Raza, Shardul Ghuge, Chen Ding, Deval Pandya
Categories: cs.CL
\\
  Advancements in Large Language Models (LLMs) highlight the need for ethical
practices and data integrity. We introduce a framework that embeds FAIR
(Findable, Accessible, Interoperable, Reusable) data principles into LLM
training. This approach marks a shift towards practices compliant with FAIR
standards. Our framework presents guidelines for integrating FAIR data
principles into LLM training. This initiative includes a checklist for
researchers and developers. We also demonstrate its practical application
through a case study focused on bias identification and mitigation in our
FAIR-compliant dataset. This work is a significant contribution to AI ethics
and data science, advocating for balanced and ethical training methods in LLMs.
\\ ( https://arxiv.org/abs/2401.11033 ,  8222kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11048
Date: Fri, 19 Jan 2024 22:24:39 GMT   (4047kb)

Title: PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical
  Knowledge
Authors: Chih-Hsuan Wei, Alexis Allot, Po-Ting Lai, Robert Leaman, Shubo Tian,
  Ling Luo, Qiao Jin, Zhizheng Wang, Qingyu Chen, and Zhiyong Lu
Categories: cs.CL q-bio.QM
\\
  PubTator 3.0 (https://www.ncbi.nlm.nih.gov/research/pubtator3/) is a
biomedical literature resource using state-of-the-art AI techniques to offer
semantic and relation searches for key concepts like proteins, genetic
variants, diseases, and chemicals. It currently provides over one billion
entity and relation annotations across approximately 36 million PubMed
abstracts and 6 million full-text articles from the PMC open access subset,
updated weekly. PubTator 3.0's online interface and API utilize these
precomputed entity relations and synonyms to provide advanced search
capabilities and enable large-scale analyses, streamlining many complex
information needs. We showcase the retrieval quality of PubTator 3.0 using a
series of entity pair queries, demonstrating that PubTator 3.0 retrieves a
greater number of articles than either PubMed or Google Scholar, with higher
precision in the top 20 results. We further show that integrating ChatGPT
(GPT-4) with PubTator APIs dramatically improves the factuality and
verifiability of its responses. In summary, PubTator 3.0 offers a comprehensive
set of features and tools that allow researchers to navigate the ever-expanding
wealth of biomedical literature, expediting research and unlocking valuable
insights for scientific discovery.
\\ ( https://arxiv.org/abs/2401.11048 ,  4047kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11052
Date: Fri, 19 Jan 2024 23:00:31 GMT   (479kb,D)

Title: Mining experimental data from Materials Science literature with Large
  Language Models
Authors: Luca Foppiano, Guillaume Lambard, Toshiyuki Amagasa, Masashi Ishii
Categories: cs.CL
\\
  This study is dedicated to evaluating the capabilities of advanced large
language models (LLMs) such as GPT-3.5-Turbo, GPT-4, and GPT-4-Turbo in the
extraction of structured information from scientific documents within the field
of materials science. We introduce a novel methodology for the comparative
analysis of intricate material expressions, emphasising the standardisation of
chemical formulas to tackle the complexities inherent in materials science
information assessment. To this end, we primarily focus on two critical tasks
of information extraction: (i) a named entity recognition (NER) of studied
materials and physical properties and (ii) a relation extraction (RE) between
these entities. The performance of LLMs in executing these tasks is benchmarked
against traditional models based on the BERT architecture and rule-based
approaches. For NER, LLMs fail to outperform the baseline with zero-shot
prompting and exhibit only limited improvement with few-shot prompting.
However, for RE, a GPT-3.5-Turbo fine-tuned with the appropriate strategy
outperforms all models, including the baseline. Without any fine-tuning, GPT-4
and GPT-4-Turbo display remarkable reasoning and relationship extraction
capabilities after being provided with merely a couple of examples, surpassing
the baseline. Overall, the results suggest that although LLMs demonstrate
relevant reasoning skills in connecting concepts, for tasks requiring
extracting complex domain-specific entities like materials, specialised models
are currently a better choice.
\\ ( https://arxiv.org/abs/2401.11052 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11107
Date: Sat, 20 Jan 2024 03:55:17 GMT   (2545kb,D)

Title: Exploiting Duality in Open Information Extraction with Predicate Prompt
Authors: Zhen Chen, Jingping Liu, Deqing Yang, Yanghua Xiao, Huimin Xu, Zongyu
  Wang, Rui Xie and Yunsen Xian
Categories: cs.CL cs.IR
\\
  Open information extraction (OpenIE) aims to extract the schema-free triplets
in the form of (\emph{subject}, \emph{predicate}, \emph{object}) from a given
sentence. Compared with general information extraction (IE), OpenIE poses more
challenges for the IE models, {especially when multiple complicated triplets
exist in a sentence. To extract these complicated triplets more effectively, in
this paper we propose a novel generative OpenIE model, namely \emph{DualOIE},
which achieves a dual task at the same time as extracting some triplets from
the sentence, i.e., converting the triplets into the sentence.} Such dual task
encourages the model to correctly recognize the structure of the given sentence
and thus is helpful to extract all potential triplets from the sentence.
Specifically, DualOIE extracts the triplets in two steps: 1) first extracting a
sequence of all potential predicates, 2) then using the predicate sequence as a
prompt to induce the generation of triplets. Our experiments on two benchmarks
and our dataset constructed from Meituan demonstrate that DualOIE achieves the
best performance among the state-of-the-art baselines. Furthermore, the online
A/B test on Meituan platform shows that 0.93\% improvement of QV-CTR and 0.56\%
improvement of UV-CTR have been obtained when the triplets extracted by DualOIE
were leveraged in Meituan's search system.
\\ ( https://arxiv.org/abs/2401.11107 ,  2545kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11120
Date: Sat, 20 Jan 2024 05:10:46 GMT   (5795kb,D)

Title: Enhancing Large Language Models for Clinical Decision Support by
  Incorporating Clinical Practice Guidelines
Authors: David Oniani, Xizhi Wu, Shyam Visweswaran, Sumit Kapoor, Shravan
  Kooragayalu, Katelyn Polanska, Yanshan Wang
Categories: cs.CL cs.AI
\\
  Background Large Language Models (LLMs), enhanced with Clinical Practice
Guidelines (CPGs), can significantly improve Clinical Decision Support (CDS).
However, methods for incorporating CPGs into LLMs are not well studied. Methods
We develop three distinct methods for incorporating CPGs into LLMs: Binary
Decision Tree (BDT), Program-Aided Graph Construction (PAGC), and
Chain-of-Thought-Few-Shot Prompting (CoT-FSP). To evaluate the effectiveness of
the proposed methods, we create a set of synthetic patient descriptions and
conduct both automatic and human evaluation of the responses generated by four
LLMs: GPT-4, GPT-3.5 Turbo, LLaMA, and PaLM 2. Zero-Shot Prompting (ZSP) was
used as the baseline method. We focus on CDS for COVID-19 outpatient treatment
as the case study. Results All four LLMs exhibit improved performance when
enhanced with CPGs compared to the baseline ZSP. BDT outperformed both CoT-FSP
and PAGC in automatic evaluation. All of the proposed methods demonstrated high
performance in human evaluation. Conclusion LLMs enhanced with CPGs demonstrate
superior performance, as compared to plain LLMs with ZSP, in providing accurate
recommendations for COVID-19 outpatient treatment, which also highlights the
potential for broader applications beyond the case study.
\\ ( https://arxiv.org/abs/2401.11120 ,  5795kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11185
Date: Sat, 20 Jan 2024 09:49:59 GMT   (836kb,D)

Title: How the Advent of Ubiquitous Large Language Models both Stymie and
  Turbocharge Dynamic Adversarial Question Generation
Authors: Yoo Yeon Sung and Ishani Mondal and Jordan Boyd-Graber
Categories: cs.CL cs.HC
\\
  Dynamic adversarial question generation, where humans write examples to stump
a model, aims to create examples that are realistic and informative. However,
the advent of large language models (LLMs) has been a double-edged sword for
human authors: more people are interested in seeing and pushing the limits of
these models, but because the models are so much stronger an opponent, they are
harder to defeat. To understand how these models impact adversarial question
writing process, we enrich the writing guidance with LLMs and retrieval models
for the authors to reason why their questions are not adversarial. While
authors could create interesting, challenging adversarial questions, they
sometimes resort to tricks that result in poor questions that are ambiguous,
subjective, or confusing not just to a computer but also to humans. To address
these issues, we propose new metrics and incentives for eliciting good,
challenging questions and present a new dataset of adversarially authored
questions.
\\ ( https://arxiv.org/abs/2401.11185 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11206
Date: Sat, 20 Jan 2024 10:41:03 GMT   (8868kb,D)

Title: InferAligner: Inference-Time Alignment for Harmlessness through
  Cross-Model Guidance
Authors: Pengyu Wang, Dong Zhang, Linyang Li, Chenkun Tan, Xinghao Wang, Ke
  Ren, Botian Jiang, Xipeng Qiu
Categories: cs.CL
\\
  With the rapid development of large language models (LLMs), they are not only
used as general-purpose AI assistants but are also customized through further
fine-tuning to meet the requirements of different applications. A pivotal
factor in the success of current LLMs is the alignment process. Current
alignment methods, such as supervised fine-tuning (SFT) and reinforcement
learning from human feedback (RLHF), focus on training-time alignment and are
often complex and cumbersome to implement. Therefore, we develop
\textbf{InferAligner}, a novel inference-time alignment method that utilizes
cross-model guidance for harmlessness alignment. InferAligner utilizes safety
steering vectors extracted from safety-aligned model to modify the activations
of the target model when responding to harmful inputs, thereby guiding the
target model to provide harmless responses. Experimental results show that our
method can be very effectively applied to domain-specific models in finance,
medicine, and mathematics, as well as to multimodal large language models
(MLLMs) such as LLaVA. It significantly diminishes the Attack Success Rate
(ASR) of both harmful instructions and jailbreak attacks, while maintaining
almost unchanged performance in downstream tasks.
\\ ( https://arxiv.org/abs/2401.11206 ,  8868kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11207
Date: Sat, 20 Jan 2024 10:42:15 GMT   (1736kb)

Title: Unfair TOS: An Automated Approach using Customized BERT
Authors: Bathini Sai Akash, Akshara Kupireddy, Lalita Bhanu Murthy
Categories: cs.CL cs.CY
\\
  Terms of Service (ToS) form an integral part of any agreement as it defines
the legal relationship between a service provider and an end-user. Not only do
they establish and delineate reciprocal rights and responsibilities, but they
also provide users with information on essential aspects of contracts that
pertain to the use of digital spaces. These aspects include a wide range of
topics, including limitation of liability, data protection, etc. Users tend to
accept the ToS without going through it before using any application or
service. Such ignorance puts them in a potentially weaker situation in case any
action is required. Existing methodologies for the detection or classification
of unfair clauses are however obsolete and show modest performance. In this
research paper, we present SOTA(State of The Art) results on unfair clause
detection from ToS documents based on unprecedented Fine-tuning BERT in
integration with SVC(Support Vector Classifier). The study shows proficient
performance with a macro F1-score of 0.922 at unfair clause detection, and
superior performance is also shown in the classification of unfair clauses by
each tag. Further, a comparative analysis is performed by answering research
questions on the Transformer models utilized. In order to further research and
experimentation the code and results are made available on
https://github.com/batking24/Unfair-TOS-An-Automated-Approach-based-on-Fine-tuning-BERT-in-conjunction-with-ML.
\\ ( https://arxiv.org/abs/2401.11207 ,  1736kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11218
Date: Sat, 20 Jan 2024 12:00:40 GMT   (7540kb,D)

Title: End-to-End Argument Mining over Varying Rhetorical Structures
Authors: Elena Chistova
Categories: cs.CL
Journal-ref: Findings of the Association for Computational Linguistics: ACL
  2023, 3376-3391
DOI: 10.18653/v1/2023.findings-acl.209
\\
  Rhetorical Structure Theory implies no single discourse interpretation of a
text, and the limitations of RST parsers further exacerbate inconsistent
parsing of similar structures. Therefore, it is important to take into account
that the same argumentative structure can be found in semantically similar
texts with varying rhetorical structures. In this work, the differences between
paraphrases within the same argument scheme are evaluated from a rhetorical
perspective. The study proposes a deep dependency parsing model to assess the
connection between rhetorical and argument structures. The model utilizes
rhetorical relations; RST structures of paraphrases serve as training data
augmentations. The method allows for end-to-end argumentation analysis using a
rhetorical tree instead of a word sequence. It is evaluated on the bilingual
Microtexts corpus, and the first results on fully-fledged argument parsing for
the Russian version of the corpus are reported. The results suggest that
argument mining can benefit from multiple variants of discourse structure.
\\ ( https://arxiv.org/abs/2401.11218 ,  7540kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11246
Date: Sat, 20 Jan 2024 14:59:43 GMT   (8426kb)

Title: Prompt-RAG: Pioneering Vector Embedding-Free Retrieval-Augmented
  Generation in Niche Domains, Exemplified by Korean Medicine
Authors: Bongsu Kang, Jundong Kim, Tae-Rim Yun, Chang-Eop Kim
Categories: cs.CL cs.IR
Comments: 26 pages, 4 figures, 5 tables
ACM-class: I.2.7; H.3.3; J.3
\\
  We propose a natural language prompt-based retrieval augmented generation
(Prompt-RAG), a novel approach to enhance the performance of generative large
language models (LLMs) in niche domains. Conventional RAG methods mostly
require vector embeddings, yet the suitability of generic LLM-based embedding
representations for specialized domains remains uncertain. To explore and
exemplify this point, we compared vector embeddings from Korean Medicine (KM)
and Conventional Medicine (CM) documents, finding that KM document embeddings
correlated more with token overlaps and less with human-assessed document
relatedness, in contrast to CM embeddings. Prompt-RAG, distinct from
conventional RAG models, operates without the need for embedding vectors. Its
performance was assessed through a Question-Answering (QA) chatbot application,
where responses were evaluated for relevance, readability, and informativeness.
The results showed that Prompt-RAG outperformed existing models, including
ChatGPT and conventional vector embedding-based RAGs, in terms of relevance and
informativeness. Despite challenges like content structuring and response
latency, the advancements in LLMs are expected to encourage the use of
Prompt-RAG, making it a promising tool for other domains in need of RAG
methods.
\\ ( https://arxiv.org/abs/2401.11246 ,  8426kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11268
Date: Sat, 20 Jan 2024 16:48:55 GMT   (656kb,D)

Title: Word-Level ASR Quality Estimation for Efficient Corpus Sampling and
  Post-Editing through Analyzing Attentions of a Reference-Free Metric
Authors: Golara Javadi, Kamer Ali Yuksel, Yunsu Kim, Thiago Castro Ferreira,
  Mohamed Al-Badrashiny
Categories: cs.CL cs.SD eess.AS
\\
  In the realm of automatic speech recognition (ASR), the quest for models that
not only perform with high accuracy but also offer transparency in their
decision-making processes is crucial. The potential of quality estimation (QE)
metrics is introduced and evaluated as a novel tool to enhance explainable
artificial intelligence (XAI) in ASR systems. Through experiments and analyses,
the capabilities of the NoRefER (No Reference Error Rate) metric are explored
in identifying word-level errors to aid post-editors in refining ASR
hypotheses. The investigation also extends to the utility of NoRefER in the
corpus-building process, demonstrating its effectiveness in augmenting datasets
with insightful annotations. The diagnostic aspects of NoRefER are examined,
revealing its ability to provide valuable insights into model behaviors and
decision patterns. This has proven beneficial for prioritizing hypotheses in
post-editing workflows and fine-tuning ASR models. The findings suggest that
NoRefER is not merely a tool for error detection but also a comprehensive
framework for enhancing ASR systems' transparency, efficiency, and
effectiveness. To ensure the reproducibility of the results, all source codes
of this study are made publicly available.
\\ ( https://arxiv.org/abs/2401.11268 ,  656kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11316
Date: Sat, 20 Jan 2024 20:25:17 GMT   (7889kb,D)

Title: PRILoRA: Pruned and Rank-Increasing Low-Rank Adaptation
Authors: Nadav Benedek, Lior Wolf
Categories: cs.CL cs.AI
Comments: EACL 2024
\\
  With the proliferation of large pre-trained language models (PLMs),
fine-tuning all model parameters becomes increasingly inefficient, particularly
when dealing with numerous downstream tasks that entail substantial training
and storage costs. Several approaches aimed at achieving parameter-efficient
fine-tuning (PEFT) have been proposed. Among them, Low-Rank Adaptation (LoRA)
stands out as an archetypal method, incorporating trainable rank decomposition
matrices into each target module. Nevertheless, LoRA does not consider the
varying importance of each layer. To address these challenges, we introduce
PRILoRA, which linearly allocates a different rank for each layer, in an
increasing manner, and performs pruning throughout the training process,
considering both the temporary magnitude of weights and the accumulated
statistics of the input to any given layer. We validate the effectiveness of
PRILoRA through extensive experiments on eight GLUE benchmarks, setting a new
state of the art.
\\ ( https://arxiv.org/abs/2401.11316 ,  7889kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11323
Date: Sat, 20 Jan 2024 20:55:21 GMT   (928kb,D)

Title: Analyzing Task-Encoding Tokens in Large Language Models
Authors: Yu Bai, Heyan Huang, Cesare Spinoso-Di Piano, Marc-Antoine Rondeau,
  Sanxing Chen, Yang Gao, Jackie Chi Kit Cheung
Categories: cs.CL
Comments: Work in progress
\\
  In-context learning (ICL) has become an effective solution for few-shot
learning in natural language processing. Past work has found that, during this
process, representations of the last prompt token are utilized to store task
reasoning procedures, thereby explaining the working mechanism of in-context
learning. In this paper, we seek to locate and analyze other task-encoding
tokens whose representations store task reasoning procedures. Supported by
experiments that ablate the representations of different token types, we find
that template and stopword tokens are the most prone to be task-encoding
tokens. In addition, we demonstrate experimentally that lexical cues,
repetition, and text formats are the main distinguishing characteristics of
these tokens. Our work provides additional insights into how large language
models (LLMs) leverage task reasoning procedures in ICL and suggests that
future work may involve using task-encoding tokens to improve the computational
efficiency of LLMs at inference time and their ability to handle long
sequences.
\\ ( https://arxiv.org/abs/2401.11323 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11356
Date: Sun, 21 Jan 2024 00:58:31 GMT   (196kb,D)

Title: ProLex: A Benchmark for Language Proficiency-oriented Lexical
  Substitution
Authors: Xuanming Zhang, Zixun Chen, Zhou Yu
Categories: cs.CL
\\
  Lexical Substitution discovers appropriate substitutes for a given target
word in a context sentence. However, the task fails to consider substitutes
that are of equal or higher proficiency than the target, an aspect that could
be beneficial for language learners looking to improve their writing. To bridge
this gap, we propose a new task, language proficiency-oriented lexical
substitution. We also introduce ProLex, a novel benchmark designed to assess
systems' ability to generate not only appropriate substitutes but also
substitutes that demonstrate better language proficiency. Besides the
benchmark, we propose models that can automatically perform the new task. We
show that our best model, a Llama2-13B model fine-tuned with task-specific
synthetic data, outperforms ChatGPT by an average of 3.2% in F-score and
achieves comparable results with GPT-4 on ProLex.
\\ ( https://arxiv.org/abs/2401.11356 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11365
Date: Sun, 21 Jan 2024 01:37:25 GMT   (3094kb,D)

Title: Confidence Preservation Property in Knowledge Distillation Abstractions
Authors: Dmitry Vengertsev, Elena Sherman
Categories: cs.CL cs.LG
DOI: 10.1007/978-3-031-47994-6_5
\\
  Social media platforms prevent malicious activities by detecting harmful
content of posts and comments. To that end, they employ large-scale deep neural
network language models for sentiment analysis and content understanding. Some
models, like BERT, are complex, and have numerous parameters, which makes them
expensive to operate and maintain. To overcome these deficiencies, industry
experts employ a knowledge distillation compression technique, where a
distilled model is trained to reproduce the classification behavior of the
original model. The distillation processes terminates when the distillation
loss function reaches the stopping criteria. This function is mainly designed
to ensure that the original and the distilled models exhibit alike
classification behaviors. However, besides classification accuracy, there are
additional properties of the original model that the distilled model should
preserve to be considered as an appropriate abstraction. In this work, we
explore whether distilled TinyBERT models preserve confidence values of the
original BERT models, and investigate how this confidence preservation property
could guide tuning hyperparameters of the distillation process.
\\ ( https://arxiv.org/abs/2401.11365 ,  3094kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11373
Date: Sun, 21 Jan 2024 02:25:29 GMT   (8030kb,D)

Title: Finding a Needle in the Adversarial Haystack: A Targeted Paraphrasing
  Approach For Uncovering Edge Cases with Minimal Distribution Distortion
Authors: Aly M. Kassem, Sherif Saad
Categories: cs.CL
Comments: EACL 2024 - Main conference
\\
  Adversarial attacks against NLP Deep Learning models are a significant
concern. In particular, adversarial samples exploit the model's sensitivity to
small input changes. While these changes appear insignificant on the semantics
of the input sample, they result in significant decay in model performance. In
this paper, we propose Targeted Paraphrasing via RL (TPRL), an approach to
automatically learn a policy to generate challenging samples that most likely
improve the model's performance. TPRL leverages FLAN T5, a language model, as a
generator and employs a self learned policy using a proximal policy gradient to
generate the adversarial examples automatically. TPRL's reward is based on the
confusion induced in the classifier, preserving the original text meaning
through a Mutual Implication score. We demonstrate and evaluate TPRL's
effectiveness in discovering natural adversarial attacks and improving model
performance through extensive experiments on four diverse NLP classification
tasks via Automatic and Human evaluation. TPRL outperforms strong baselines,
exhibits generalizability across classifiers and datasets, and combines the
strengths of language modeling and reinforcement learning to generate diverse
and influential adversarial examples.
\\ ( https://arxiv.org/abs/2401.11373 ,  8030kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11374
Date: Sun, 21 Jan 2024 02:29:12 GMT   (2115kb,D)

Title: Language Models as Hierarchy Encoders
Authors: Yuan He, Zhangdie Yuan, Jiaoyan Chen, Ian Horrocks
Categories: cs.CL cs.AI cs.LG
\\
  Interpreting hierarchical structures latent in language is a key limitation
of current language models (LMs). While previous research has implicitly
leveraged these hierarchies to enhance LMs, approaches for their explicit
encoding are yet to be explored. To address this, we introduce a novel approach
to re-train transformer encoder-based LMs as Hierarchy Transformer encoders
(HiTs), harnessing the expansive nature of hyperbolic space. Our method
situates the output embedding space of pre-trained LMs within a Poincar\'e ball
with a curvature that adapts to the embedding dimension, followed by
re-training on hyperbolic cluster and centripetal losses. These losses are
designed to effectively cluster related entities (input as texts) and organise
them hierarchically. We evaluate HiTs against pre-trained and fine-tuned LMs,
focusing on their capabilities in simulating transitive inference, predicting
subsumptions, and transferring knowledge across hierarchies. The results
demonstrate that HiTs consistently outperform both pre-trained and fine-tuned
LMs in these tasks, underscoring the effectiveness and transferability of our
re-trained hierarchy encoders.
\\ ( https://arxiv.org/abs/2401.11374 ,  2115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11382
Date: Sun, 21 Jan 2024 03:15:05 GMT   (2639kb,D)

Title: Using Large Language Model for End-to-End Chinese ASR and NER
Authors: Yuang Li, Jiawei Yu, Yanqing Zhao, Min Zhang, Mengxin Ren, Xiaofeng
  Zhao, Xiaosong Qiao, Chang Su, Miaomiao Ma, Hao Yang
Categories: cs.CL cs.AI
Comments: 5 pages, 2 figures
\\
  Mapping speech tokens to the same feature space as text tokens has become the
paradigm for the integration of speech modality into decoder-only large
language models (LLMs). An alternative approach is to use an encoder-decoder
architecture that incorporates speech features through cross-attention. This
approach, however, has received less attention in the literature. In this work,
we connect the Whisper encoder with ChatGLM3 and provide in-depth comparisons
of these two approaches using Chinese automatic speech recognition (ASR) and
name entity recognition (NER) tasks. We evaluate them not only by conventional
metrics like the F1 score but also by a novel fine-grained taxonomy of ASR-NER
errors. Our experiments reveal that encoder-decoder architecture outperforms
decoder-only architecture with a short context, while decoder-only architecture
benefits from a long context as it fully exploits all layers of the LLM. By
using LLM, we significantly reduced the entity omission errors and improved the
entity ASR accuracy compared to the Conformer baseline. Additionally, we
obtained a state-of-the-art (SOTA) F1 score of 0.805 on the AISHELL-NER test
set by using chain-of-thought (CoT) NER which first infers long-form ASR
transcriptions and then predicts NER labels.
\\ ( https://arxiv.org/abs/2401.11382 ,  2639kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11389
Date: Sun, 21 Jan 2024 03:37:47 GMT   (312kb,D)

Title: MedLM: Exploring Language Models for Medical Question Answering Systems
Authors: Niraj Yagnik, Jay Jhaveri, Vivek Sharma, Gabriel Pila, Asma Ben,
  Jingbo Shang
Categories: cs.CL cs.AI cs.LG
\\
  In the face of rapidly expanding online medical literature, automated systems
for aggregating and summarizing information are becoming increasingly crucial
for healthcare professionals and patients. Large Language Models (LLMs), with
their advanced generative capabilities, have shown promise in various NLP
tasks, and their potential in the healthcare domain, particularly for
Closed-Book Generative QnA, is significant. However, the performance of these
models in domain-specific tasks such as medical Q&A remains largely unexplored.
This study aims to fill this gap by comparing the performance of general and
medical-specific distilled LMs for medical Q&A. We aim to evaluate the
effectiveness of fine-tuning domain-specific LMs and compare the performance of
different families of Language Models. The study will address critical
questions about these models' reliability, comparative performance, and
effectiveness in the context of medical Q&A. The findings will provide valuable
insights into the suitability of different LMs for specific applications in the
medical domain.
\\ ( https://arxiv.org/abs/2401.11389 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11408
Date: Sun, 21 Jan 2024 06:10:03 GMT   (1049kb,D)

Title: SEBERTNets: Sequence Enhanced BERT Networks for Event Entity Extraction
  Tasks Oriented to the Finance Field
Authors: Congqing He, Xiangyu Zhu, Yuquan Le, Yuzhong Liu, Jianhong Yin
Categories: cs.CL cs.AI
Comments: CCKS 2019
\\
  Event extraction lies at the cores of investment analysis and asset
management in the financial field, and thus has received much attention. The
2019 China conference on knowledge graph and semantic computing (CCKS)
challenge sets up a evaluation competition for event entity extraction task
oriented to the finance field. In this task, we mainly focus on how to extract
the event entity accurately, and recall all the corresponding event entity
effectively. In this paper, we propose a novel model, Sequence Enhanced BERT
Networks (SEBERTNets for short), which can inherit the advantages of the
BERT,and while capturing sequence semantic information. In addition, motivated
by recommendation system, we propose Hybrid Sequence Enhanced BERT Networks
(HSEBERTNets for short), which uses a multi-channel recall method to recall all
the corresponding event entity. The experimental results show that, the F1
score of SEBERTNets is 0.905 in the first stage, and the F1 score of
HSEBERTNets is 0.934 in the first stage, which demonstarate the effectiveness
of our methods.
\\ ( https://arxiv.org/abs/2401.11408 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11431
Date: Sun, 21 Jan 2024 08:43:24 GMT   (711kb,D)

Title: Majority or Minority: Data Imbalance Learning Method for Named Entity
  Recognition
Authors: Sota Nemoto and Shunsuke Kitada and Hitoshi Iyatomi
Categories: cs.CL
Comments: 6 pages, 1 figures, 6 tables
\\
  Data imbalance presents a significant challenge in various machine learning
(ML) tasks, particularly named entity recognition (NER) within natural language
processing (NLP). NER exhibits a data imbalance with a long-tail distribution,
featuring numerous minority classes (i.e., entity classes) and a single
majority class (i.e., O-class). The imbalance leads to the misclassifications
of the entity classes as the O-class. To tackle the imbalance, we propose a
simple and effective learning method, named majority or minority (MoM)
learning. MoM learning incorporates the loss computed only for samples whose
ground truth is the majority class (i.e., the O-class) into the loss of the
conventional ML model. Evaluation experiments on four NER datasets (Japanese
and English) showed that MoM learning improves prediction performance of the
minority classes, without sacrificing the performance of the majority class and
is more effective than widely known and state-of-the-art methods. We also
evaluated MoM learning using frameworks as sequential labeling and machine
reading comprehension, which are commonly used in NER. Furthermore, MoM
learning has achieved consistent performance improvements regardless of
language, model, or framework.
\\ ( https://arxiv.org/abs/2401.11431 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11458
Date: Sun, 21 Jan 2024 10:46:23 GMT   (173kb,D)

Title: Linear Alignment: A Closed-form Solution for Aligning Human Preferences
  without Tuning and Feedback
Authors: Songyang Gao, Qiming Ge, Wei Shen, Shihan Dou, Junjie Ye, Xiao Wang,
  Rui Zheng, Yicheng Zou, Zhi Chen, Hang Yan, Qi Zhang, Dahua Lin
Categories: cs.CL
\\
  The success of AI assistants based on Language Models (LLMs) hinges on
Reinforcement Learning from Human Feedback (RLHF) to comprehend and align with
user intentions. However, traditional alignment algorithms, such as PPO, are
hampered by complex annotation and training requirements. This reliance limits
the applicability of RLHF and hinders the development of professional
assistants tailored to diverse human preferences. In this work, we introduce
\textit{Linear Alignment}, a novel algorithm that aligns language models with
human preferences in one single inference step, eliminating the reliance on
data annotation and model training. Linear alignment incorporates a new
parameterization for policy optimization under divergence constraints, which
enables the extraction of optimal policy in a closed-form manner and
facilitates the direct estimation of the aligned response. Extensive
experiments on both general and personalized preference datasets demonstrate
that linear alignment significantly enhances the performance and efficiency of
LLM alignment across diverse scenarios. Our code and dataset will be published
on \url{https://github.com/Wizardcoast/Linear_Alignment.git}.
\\ ( https://arxiv.org/abs/2401.11458 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11467
Date: Sun, 21 Jan 2024 11:42:18 GMT   (7869kb,D)

Title: Over-Reasoning and Redundant Calculation of Large Language Models
Authors: Cheng-Han Chiang, Hung-yi Lee
Categories: cs.CL
Comments: EACL 2024 main conference paper. Camera-ready version
\\
  Large language models (LLMs) can solve problems step-by-step. While this
chain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if
LLMs \textit{know} when to use CoT and whether those CoT are always necessary
to answer the question. This paper shows that LLMs tend to generate redundant
calculations and reasoning on a manually constructed math QA dataset,
GSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered
without any calculations, but LLMs, including Llama-2 models and Claude-2, tend
to generate lengthy and unnecessary calculations to answer the questions. We
also conduct experiments to explain why LLMs generate redundant calculations
and reasonings. GSM8K-Zero is publicly available at
https://github.com/d223302/Over-Reasoning-of-LLMs and
https://huggingface.co/datasets/dcml0714/GSM8K-Zero.
\\ ( https://arxiv.org/abs/2401.11467 ,  7869kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11487
Date: Sun, 21 Jan 2024 13:18:20 GMT   (117kb,D)

Title: Towards Better Inclusivity: A Diverse Tweet Corpus of English Varieties
Authors: Nhi Pham, Lachlan Pham, Adam L. Meyers
Categories: cs.CL cs.CY
Comments: 10 pages (including limitations, references and appendices), 2
  figures
\\
  The prevalence of social media presents a growing opportunity to collect and
analyse examples of English varieties. Whilst usage of these varieties was -
and, in many cases, still is - used only in spoken contexts or hard-to-access
private messages, social media sites like Twitter provide a platform for users
to communicate informally in a scrapeable format. Notably, Indian English
(Hinglish), Singaporean English (Singlish), and African-American English (AAE)
can be commonly found online. These varieties pose a challenge to existing
natural language processing (NLP) tools as they often differ orthographically
and syntactically from standard English for which the majority of these tools
are built. NLP models trained on standard English texts produced biased
outcomes for users of underrepresented varieties. Some research has aimed to
overcome the inherent biases caused by unrepresentative data through techniques
like data augmentation or adjusting training models.
  We aim to address the issue of bias at its root - the data itself. We curate
a dataset of tweets from countries with high proportions of underserved English
variety speakers, and propose an annotation framework of six categorical
classifications along a pseudo-spectrum that measures the degree of standard
English and that thereby indirectly aims to surface the manifestations of
English varieties in these tweets. Following best annotation practices, our
growing corpus features 170,800 tweets taken from 7 countries, labeled by
annotators who are from those countries and can communicate in
regionally-dominant varieties of English. Our corpus highlights the accuracy
discrepancies in pre-trained language identifiers between western English and
non-western (i.e., less standard) English varieties. We hope to contribute to
the growing literature identifying and reducing the implicit demographic
discrepancies in NLP.
\\ ( https://arxiv.org/abs/2401.11487 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11504
Date: Sun, 21 Jan 2024 14:28:41 GMT   (91kb,D)

Title: With Greater Text Comes Greater Necessity: Inference-Time Training Helps
  Long Text Generation
Authors: Y. Wang, D. Ma, D. Cai
Categories: cs.CL cs.AI
\\
  Long text generation, such as novel writing or discourse-level translation
with extremely long contexts, presents significant challenges to current
language models. Existing methods mainly focus on extending the model's context
window through strategies like length extrapolation. However, these approaches
demand substantial hardware resources during the training and/or inference
phases. Our proposed method, Temp-Lora, introduces an alternative concept.
Instead of relying on the KV cache to store all context information, Temp-Lora
embeds this information directly into the model's parameters. In the process of
long text generation, we use a temporary Lora module, progressively trained
with text generated previously. This approach not only efficiently preserves
contextual knowledge but also prevents any permanent alteration to the model's
parameters given that the module is discarded post-generation. Extensive
experiments on the PG19 language modeling benchmark and the GuoFeng
discourse-level translation benchmark validate the effectiveness of Temp-Lora.
Our results show that: 1) Temp-Lora substantially enhances generation quality
for long texts, as indicated by a 13.2% decrease in perplexity on a subset of
PG19, and a 29.6% decrease in perplexity along with a 53.2% increase in BLEU
score on GuoFeng, 2) Temp-Lora is compatible with and enhances most existing
long text generation methods, and 3) Temp-Lora can greatly reduce computational
costs by shortening the context window. While ensuring a slight improvement in
generation quality (a decrease of 3.8% in PPL), it enables a reduction of 70.5%
in the FLOPs required for inference and a 51.5% decrease in latency.
\\ ( https://arxiv.org/abs/2401.11504 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11505
Date: Sun, 21 Jan 2024 14:30:20 GMT   (1907kb,D)

Title: CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray
  Report Labeling
Authors: Jawook Gu, Han-Cheol Cho, Jiho Kim, Kihyun You, Eun Kyoung Hong,
  Byungseok Roh
Categories: cs.CL cs.IR
Comments: 16 pages, 3 figures
\\
  Free-text radiology reports present a rich data source for various medical
tasks, but effectively labeling these texts remains challenging. Traditional
rule-based labeling methods fall short of capturing the nuances of diverse
free-text patterns. Moreover, models using expert-annotated data are limited by
data scarcity and pre-defined classes, impacting their performance, flexibility
and scalability. To address these issues, our study offers three main
contributions: 1) We demonstrate the potential of GPT as an adept labeler using
carefully designed prompts. 2) Utilizing only the data labeled by GPT, we
trained a BERT-based labeler, CheX-GPT, which operates faster and more
efficiently than its GPT counterpart. 3) To benchmark labeler performance, we
introduced a publicly available expert-annotated test set, MIMIC-500,
comprising 500 cases from the MIMIC validation set. Our findings demonstrate
that CheX-GPT not only excels in labeling accuracy over existing models, but
also showcases superior efficiency, flexibility, and scalability, supported by
our introduction of the MIMIC-500 dataset for robust benchmarking. Code and
models are available at https://github.com/kakaobrain/CheXGPT.
\\ ( https://arxiv.org/abs/2401.11505 ,  1907kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11601
Date: Sun, 21 Jan 2024 21:21:51 GMT   (326kb,D)

Title: Robust Evaluation Measures for Evaluating Social Biases in Masked
  Language Models
Authors: Yang Liu
Categories: cs.CL
Comments: 9 pages, 5 figures
\\
  Many evaluation measures are used to evaluate social biases in masked
language models (MLMs). However, we find that these previously proposed
evaluation measures are lacking robustness in scenarios with limited datasets.
This is because these measures are obtained by comparing the
pseudo-log-likelihood (PLL) scores of the stereotypical and anti-stereotypical
samples using an indicator function. The disadvantage is the limited mining of
the PLL score sets without capturing its distributional information. In this
paper, we represent a PLL score set as a Gaussian distribution and use Kullback
Leibler (KL) divergence and Jensen Shannon (JS) divergence to construct
evaluation measures for the distributions of stereotypical and
anti-stereotypical PLL scores. Experimental results on the publicly available
datasets StereoSet (SS) and CrowS-Pairs (CP) show that our proposed measures
are significantly more robust and interpretable than those proposed previously.
\\ ( https://arxiv.org/abs/2401.11601 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11624
Date: Sun, 21 Jan 2024 23:34:42 GMT   (178kb,D)

Title: In-context Learning with Retrieved Demonstrations for Language Models: A
  Survey
Authors: an Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
Categories: cs.CL cs.AI cs.IR
\\
  Language models, especially pre-trained large language models, have showcased
remarkable abilities as few-shot in-context learners (ICL), adept at adapting
to new tasks with just a few demonstrations in the input context. However, the
model's ability to perform ICL is sensitive to the choice of the few-shot
demonstrations. Instead of using a fixed set of demonstrations, one recent
development is to retrieve demonstrations tailored to each input query. The
implementation of demonstration retrieval is relatively straightforward,
leveraging existing databases and retrieval systems. This not only improves the
efficiency and scalability of the learning process but also has been shown to
reduce biases inherent in manual example selection. In light of the encouraging
results and growing research in ICL with retrieved demonstrations, we conduct
an extensive review of studies in this area. In this survey, we discuss and
compare different design choices for retrieval models, retrieval training
procedures, and inference algorithms.
\\ ( https://arxiv.org/abs/2401.11624 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11641
Date: Mon, 22 Jan 2024 01:06:17 GMT   (1873kb,D)

Title: Revolutionizing Finance with LLMs: An Overview of Applications and
  Insights
Authors: Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng
  Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Gengchen Mai, Ninghao Liu, Tianming
  Liu
Categories: cs.CL
\\
  In recent years, Large Language Models (LLMs) like ChatGPT have seen
considerable advancements and have been applied in diverse fields. Built on the
Transformer architecture, these models are trained on extensive datasets,
enabling them to understand and generate human language effectively. In the
financial domain, the deployment of LLMs is gaining momentum. These models are
being utilized for automating financial report generation, forecasting market
trends, analyzing investor sentiment, and offering personalized financial
advice. Leveraging their natural language processing capabilities, LLMs can
distill key insights from vast financial data, aiding institutions in making
informed investment choices and enhancing both operational efficiency and
customer satisfaction. In this study, we provide a comprehensive overview of
the emerging integration of LLMs into various financial tasks. Additionally, we
conducted holistic tests on multiple financial tasks through the combination of
natural language instructions. Our findings show that GPT-4 effectively follow
prompt instructions across various financial tasks. This survey and evaluation
of LLMs in the financial domain aim to deepen the understanding of LLMs'
current role in finance for both financial practitioners and LLM researchers,
identify new research and application prospects, and highlight how these
technologies can be leveraged to solve practical challenges in the finance
industry.
\\ ( https://arxiv.org/abs/2401.11641 ,  1873kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11700
Date: Mon, 22 Jan 2024 05:46:11 GMT   (205kb,D)

Title: Keep Decoding Parallel with Effective Knowledge Distillation from
  Language Models to End-to-end Speech Recognisers
Authors: Michael Hentschel, Yuta Nishikawa, Tatsuya Komatsu, Yusuke Fujita
Categories: cs.CL cs.SD eess.AS
Comments: Accepted at ICASSP 2024
\\
  This study presents a novel approach for knowledge distillation (KD) from a
BERT teacher model to an automatic speech recognition (ASR) model using
intermediate layers. To distil the teacher's knowledge, we use an attention
decoder that learns from BERT's token probabilities. Our method shows that
language model (LM) information can be more effectively distilled into an ASR
model using both the intermediate layers and the final layer. By using the
intermediate layers as distillation target, we can more effectively distil LM
knowledge into the lower network layers. Using our method, we achieve better
recognition accuracy than with shallow fusion of an external LM, allowing us to
maintain fast parallel decoding. Experiments on the LibriSpeech dataset
demonstrate the effectiveness of our approach in enhancing greedy decoding with
connectionist temporal classification (CTC).
\\ ( https://arxiv.org/abs/2401.11700 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11725
Date: Mon, 22 Jan 2024 07:07:06 GMT   (1033kb,D)

Title: Speak It Out: Solving Symbol-Related Problems with Symbol-to-Language
  Conversion for Language Models
Authors: Yile Wang, Sijie Cheng, Zixin Sun, Peng Li, Yang Liu
Categories: cs.CL
\\
  Symbols (or more broadly, non-natural language textual representations) such
as numerical sequences, molecular formulas, and table delimiters widely exist,
playing important roles in various tasks such as abstract reasoning, chemical
property prediction, and table question answering. Despite the impressive
natural language comprehension capabilities of large language models (LLMs),
their reasoning abilities for symbols remain inadequate, which could attributed
to the difference between symbol representations and general natural languages.
We propose symbol-to-language (S2L), a tuning-free method that enables large
language models to solve symbol-related problems with information expressed in
natural language. Specifically, S2L first converts the symbols involved to
language-based representations, which can be implemented by prompting LLMs or
leveraging external tools, then these language-based representations are
integrated into the original problem via direct substitution or concatenation,
serving as useful input information for LLMs. We evaluate the S2L method using
both API-based (GPT-4, ChatGPT) and open-source (OpenChat) models over eight
symbol-related tasks, ranging from symbol-only abstract reasoning to sentiment
analysis in social media. Experimental results show that S2L consistently leads
to superior performance. For example, by employing S2L for GPT-4, there can be
average significant improvements of +21.9% and +9.5% for subtasks in 1D-ARC and
Dyck language, respectively. Codes and data are available at
https://github.com/THUNLP-MT/symbol2language.
\\ ( https://arxiv.org/abs/2401.11725 ,  1033kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11817
Date: Mon, 22 Jan 2024 10:26:14 GMT   (291kb,D)

Title: Hallucination is Inevitable: An Innate Limitation of Large Language
  Models
Authors: Ziwei Xu, Sanjay Jain, Mohan Kankanhalli
Categories: cs.CL cs.AI cs.LG
\\
  Hallucination has been widely recognized to be a significant drawback for
large language models (LLMs). There have been many works that attempt to reduce
the extent of hallucination. These efforts have mostly been empirical so far,
which cannot answer the fundamental question whether it can be completely
eliminated. In this paper, we formalize the problem and show that it is
impossible to eliminate hallucination in LLMs. Specifically, we define a formal
world where hallucination is defined as inconsistencies between a computable
LLM and a computable ground truth function. By employing results from learning
theory, we show that LLMs cannot learn all of the computable functions and will
therefore always hallucinate. Since the formal world is a part of the real
world which is much more complicated, hallucinations are also inevitable for
real world LLMs. Furthermore, for real world LLMs constrained by provable time
complexity, we describe the hallucination-prone tasks and empirically validate
our claims. Finally, using the formal world framework, we discuss the possible
mechanisms and efficacies of existing hallucination mitigators as well as the
practical implications on the safe deployment of LLMs.
\\ ( https://arxiv.org/abs/2401.11817 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11819
Date: Mon, 22 Jan 2024 10:30:11 GMT   (5740kb,D)

Title: SuperCLUE-Math6: Graded Multi-Step Math Reasoning Benchmark for LLMs in
  Chinese
Authors: Liang Xu, Hang Xue, Lei Zhu, Kangkang Zhao
Categories: cs.CL cs.AI
Comments: 8 pages, 7 figures, 4 tables
\\
  We introduce SuperCLUE-Math6(SC-Math6), a new benchmark dataset to evaluate
the mathematical reasoning abilities of Chinese language models. SC-Math6 is
designed as an upgraded Chinese version of the GSM8K dataset with enhanced
difficulty, diversity, and application scope. It consists of over 2000
mathematical word problems requiring multi-step reasoning and providing natural
language solutions. We propose an innovative scheme to quantify the reasoning
capability of large models based on performance over problems with different
reasoning steps. Experiments on 12 representative Chinese models demonstrate a
clear stratification of reasoning levels, with top models like GPT-4 showing
superior performance. SC-Math6 fills the gap in Chinese mathematical reasoning
benchmarks and provides a comprehensive testbed to advance the intelligence of
Chinese language models.
\\ ( https://arxiv.org/abs/2401.11819 ,  5740kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11839
Date: Mon, 22 Jan 2024 10:57:09 GMT   (210kb,D)

Title: AI for social science and social science of AI: A Survey
Authors: Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu
  Lin, Le Sun, Xianpei Han
Categories: cs.CL cs.CY
Comments: Accepted by Information Processing and Management (IP&M)
\\
  Recent advancements in artificial intelligence, particularly with the
emergence of large language models (LLMs), have sparked a rethinking of
artificial general intelligence possibilities. The increasing human-like
capabilities of AI are also attracting attention in social science research,
leading to various studies exploring the combination of these two fields. In
this survey, we systematically categorize previous explorations in the
combination of AI and social science into two directions that share common
technical approaches but differ in their research objectives. The first
direction is focused on AI for social science, where AI is utilized as a
powerful tool to enhance various stages of social science research. While the
second direction is the social science of AI, which examines AI agents as
social entities with their human-like cognitive and linguistic capabilities. By
conducting a thorough review, particularly on the substantial progress
facilitated by recent advancements in large language models, this paper
introduces a fresh perspective to reassess the relationship between AI and
social science, provides a cohesive framework that allows researchers to
understand the distinctions and connections between AI for social science and
social science of AI, and also summarized state-of-art experiment simulation
platforms to facilitate research in these two directions. We believe that as AI
technology continues to advance and intelligent agents find increasing
applications in our daily lives, the significance of the combination of AI and
social science will become even more prominent.
\\ ( https://arxiv.org/abs/2401.11839 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11852
Date: Mon, 22 Jan 2024 11:15:07 GMT   (1903kb,D)

Title: The Right Model for the Job: An Evaluation of Legal Multi-Label
  Classification Baselines
Authors: Martina Forster, Claudia Schulz, Prudhvi Nokku, Melicaalsadat
  Mirsafian, Jaykumar Kasundra, Stavroula Skylaki
Categories: cs.CL cs.AI
\\
  Multi-Label Classification (MLC) is a common task in the legal domain, where
more than one label may be assigned to a legal document. A wide range of
methods can be applied, ranging from traditional ML approaches to the latest
Transformer-based architectures. In this work, we perform an evaluation of
different MLC methods using two public legal datasets, POSTURE50K and
EURLEX57K. By varying the amount of training data and the number of labels, we
explore the comparative advantage offered by different approaches in relation
to the dataset properties. Our findings highlight DistilRoBERTa and LegalBERT
as performing consistently well in legal MLC with reasonable computational
demands. T5 also demonstrates comparable performance while offering advantages
as a generative model in the presence of changing label sets. Finally, we show
that the CrossEncoder exhibits potential for notable macro-F1 score
improvements, albeit with increased computational costs.
\\ ( https://arxiv.org/abs/2401.11852 ,  1903kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11864
Date: Mon, 22 Jan 2024 11:37:18 GMT   (8637kb,D)

Title: Improving Small Language Models' Mathematical Reasoning via Mix Thoughts
  Distillation
Authors: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang
Categories: cs.CL cs.AI
\\
  This work addresses the challenge of democratizing advanced Large Language
Models (LLMs) by compressing their mathematical reasoning capabilities into
sub-billion parameter Small Language Models (SLMs) without compromising
performance. We introduce Equation-of-Thought Distillation (EoTD), a novel
technique that encapsulates the reasoning process into equation-based
representations to construct an EoTD dataset for fine-tuning SLMs.
Additionally, we propose the Mix Thoughts Distillation (MTD) framework to
enhance the reasoning performance of SLMs. This involves creating a reasoning
dataset with multiple thought processes and using it for fine-tuning. Our
experimental findings demonstrate that EoTD significantly boosts the reasoning
abilities of SLMs, while MTD enables these models to achieve state-of-the-art
reasoning performance.
\\ ( https://arxiv.org/abs/2401.11864 ,  8637kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11880
Date: Mon, 22 Jan 2024 12:11:55 GMT   (2213kb,D)

Title: PsySafe: A Comprehensive Framework for Psychological-based Attack,
  Defense, and Evaluation of Multi-agent System Safety
Authors: Zaibin Zhang, Yongting Zhang, Lijun Li, Hongzhi Gao, Lijun Wang,
  Huchuan Lu, Feng Zhao, Yu Qiao, Jing Shao
Categories: cs.CL cs.AI cs.CR cs.MA
\\
  Multi-agent systems, augmented with Large Language Models (LLMs), demonstrate
significant capabilities for collective intelligence. However, the potential
misuse of this intelligence for malicious purposes presents significant risks.
To date, comprehensive research on the safety issues associated with
multi-agent systems remains limited. From the perspective of agent psychology,
we discover that the dark psychological states of agents can lead to severe
safety issues. To address these issues, we propose a comprehensive framework
grounded in agent psychology. In our framework, we focus on three aspects:
identifying how dark personality traits in agents might lead to risky
behaviors, designing defense strategies to mitigate these risks, and evaluating
the safety of multi-agent systems from both psychological and behavioral
perspectives. Our experiments reveal several intriguing phenomena, such as the
collective dangerous behaviors among agents, agents' propensity for
self-reflection when engaging in dangerous behavior, and the correlation
between agents' psychological assessments and their dangerous behaviors. We
anticipate that our framework and observations will provide valuable insights
for further research into the safety of multi-agent systems. We will make our
data and code publicly accessible at https:/github.com/AI4Good24/PsySafe.
\\ ( https://arxiv.org/abs/2401.11880 ,  2213kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11911
Date: Mon, 22 Jan 2024 12:54:04 GMT   (2115kb,D)

Title: Blinded by Generated Contexts: How Language Models Merge Generated and
  Retrieved Contexts for Open-Domain QA?
Authors: Hexiang Tan, Fei Sun, Wanli Yang, Yuanzhuo Wang, Qi Cao, Xueqi Cheng
Categories: cs.CL cs.AI
\\
  While auxiliary information has become a key to enhance Large Language Models
(LLMs), relatively little is known about how well LLMs merge these contexts,
specifically generated and retrieved. To study this, we formulate a task
specifically designed to identify whether the answers, derived from the
integration of generated and retrieved contexts, are attributed to either
generated or retrieved contexts. To support this task, we develop a methodology
to construct datasets with conflicting contexts, where each question is paired
with both generated and retrieved contexts, yet only one of them contains the
correct answer. Our experiments reveal a significant bias in LLMs towards
generated contexts, as evidenced across state-of-the-art open (Llama2-7b/13b)
and closed (GPT 3.5/4) systems. We further identify two key factors
contributing to this bias: i) Contexts generated by LLMs typically show greater
similarity to the questions, increasing their likelihood of selection; ii) The
segmentation process used in retrieved contexts disrupts their completeness,
thereby hindering their full utilization in LLMs. Our analysis enhances the
understanding of how LLMs merge diverse contexts, offering valuable insights
for advancing current augmentation methods for LLMs.
\\ ( https://arxiv.org/abs/2401.11911 ,  2115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11944
Date: Mon, 22 Jan 2024 13:34:34 GMT   (11335kb,D)

Title: CMMMU: A Chinese Massive Multi-discipline Multimodal Understanding
  Benchmark
Authors: Ge Zhang, Xinrun Du, Bei Chen, Yiming Liang, Tongxu Luo, Tianyu Zheng,
  Kang Zhu, Yuyang Cheng, Chunpu Xu, Shuyue Guo, Haoran Zhang, Xingwei Qu,
  Junjie Wang, Ruibin Yuan, Yizhi Li, Zekun Wang, Yudong Liu, Yu-Hsuan Tsai,
  Fengji Zhang, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu
Categories: cs.CL cs.AI cs.CV
\\
  As the capabilities of large multimodal models (LMMs) continue to advance,
evaluating the performance of LMMs emerges as an increasing need. Additionally,
there is an even larger gap in evaluating the advanced knowledge and reasoning
abilities of LMMs in non-English contexts such as Chinese. We introduce CMMMU,
a new Chinese Massive Multi-discipline Multimodal Understanding benchmark
designed to evaluate LMMs on tasks demanding college-level subject knowledge
and deliberate reasoning in a Chinese context. CMMMU is inspired by and
strictly follows the annotation and analysis pattern of MMMU.
  CMMMU includes 12k manually collected multimodal questions from college
exams, quizzes, and textbooks, covering six core disciplines: Art & Design,
Business, Science, Health & Medicine, Humanities & Social Science, and Tech &
Engineering, like its companion, MMMU. These questions span 30 subjects and
comprise 39 highly heterogeneous image types, such as charts, diagrams, maps,
tables, music sheets, and chemical structures.
  CMMMU focuses on complex perception and reasoning with domain-specific
knowledge in the Chinese context. We evaluate 11 open-source LLMs and one
proprietary GPT-4V(ision). Even GPT-4V only achieves accuracies of 42%,
indicating a large space for improvement. CMMMU will boost the community to
build the next-generation LMMs towards expert artificial intelligence and
promote the democratization of LMMs by providing diverse language contexts.
\\ ( https://arxiv.org/abs/2401.11944 ,  11335kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11969
Date: Mon, 22 Jan 2024 14:17:03 GMT   (930kb,D)

Title: Claim Detection for Automated Fact-checking: A Survey on Monolingual,
  Multilingual and Cross-Lingual Research
Authors: Rrubaa Panchendrarajan and Arkaitz Zubiaga
Categories: cs.CL
\\
  Automated fact-checking has drawn considerable attention over the past few
decades due to the increase in the diffusion of misinformation on online
platforms. This is often carried out as a sequence of tasks comprising (i) the
detection of sentences circulating in online platforms which constitute claims
needing verification, followed by (ii) the verification process of those
claims. This survey focuses on the former, by discussing existing efforts
towards detecting claims needing fact-checking, with a particular focus on
multilingual data and methods. This is a challenging and fertile direction
where existing methods are yet far from matching human performance due to the
profoundly challenging nature of the issue. Especially, the dissemination of
information across multiple social platforms, articulated in multiple languages
and modalities demands more generalized solutions for combating misinformation.
Focusing on multilingual misinformation, we present a comprehensive survey of
existing multilingual claim detection research. We present state-of-the-art
multilingual claim detection research categorized into three key factors of the
problem, verifiability, priority, and similarity. Further, we present a
detailed overview of the existing multilingual datasets along with the
challenges and suggest possible future advancements.
\\ ( https://arxiv.org/abs/2401.11969 ,  930kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11972
Date: Mon, 22 Jan 2024 14:24:03 GMT   (8265kb,D)

Title: Synergizing Machine Learning & Symbolic Methods: A Survey on Hybrid
  Approaches to Natural Language Processing
Authors: Rrubaa Panchendrarajan and Arkaitz Zubiaga
Categories: cs.CL
\\
  The advancement of machine learning and symbolic approaches have underscored
their strengths and weaknesses in Natural Language Processing (NLP). While
machine learning approaches are powerful in identifying patterns in data, they
often fall short in learning commonsense and the factual knowledge required for
the NLP tasks. Meanwhile, the symbolic methods excel in representing
knowledge-rich data. However, they struggle to adapt dynamic data and
generalize the knowledge. Bridging these two paradigms through hybrid
approaches enables the alleviation of weaknesses in both while preserving their
strengths. Recent studies extol the virtues of this union, showcasing promising
results in a wide range of NLP tasks. In this paper, we present an overview of
hybrid approaches used for NLP. Specifically, we delve into the
state-of-the-art hybrid approaches used for a broad spectrum of NLP tasks
requiring natural language understanding, generation, and reasoning.
Furthermore, we discuss the existing resources available for hybrid approaches
for NLP along with the challenges, offering a roadmap for future directions.
\\ ( https://arxiv.org/abs/2401.11972 ,  8265kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12005
Date: Mon, 22 Jan 2024 14:53:59 GMT   (427kb,D)

Title: ALMs: Authorial Language Models for Authorship Attribution
Authors: Weihang Huang and Akira Murakami and Jack Grieve
Categories: cs.CL
\\
  In this paper, we introduce an authorship attribution method called Authorial
Language Models (ALMs) that involves identifying the most likely author of a
questioned document based on the perplexity of the questioned document
calculated for a set of causal language models fine-tuned on the writings of a
set of candidate author. We benchmarked ALMs against state-of-art-systems using
the CCAT50 dataset and the Blogs50 datasets. We find that ALMs achieves a
macro-average accuracy score of 83.6% on Blogs50, outperforming all other
methods, and 74.9% on CCAT50, matching the performance of the best method. To
assess the performance of ALMs on shorter texts, we also conducted text
ablation testing. We found that to reach a macro-average accuracy of 70%, ALMs
needs 40 tokens on Blogs50 and 400 tokens on CCAT50, while to reach 60% ALMs
requires 20 tokens on Blogs50 and 70 tokens on CCAT50.
\\ ( https://arxiv.org/abs/2401.12005 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12070
Date: Mon, 22 Jan 2024 16:09:47 GMT   (4003kb,D)

Title: Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated
  Text
Authors: Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi,
  Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein
Categories: cs.CL cs.AI cs.LG
Comments: 20 pages, code available at https://github.com/ahans30/Binoculars
\\
  Detecting text generated by modern large language models is thought to be
hard, as both LLMs and humans can exhibit a wide range of complex behaviors.
However, we find that a score based on contrasting two closely related language
models is highly accurate at separating human-generated and machine-generated
text. Based on this mechanism, we propose a novel LLM detector that only
requires simple calculations using a pair of pre-trained LLMs. The method,
called Binoculars, achieves state-of-the-art accuracy without any training
data. It is capable of spotting machine text from a range of modern LLMs
without any model-specific modifications. We comprehensively evaluate
Binoculars on a number of text sources and in varied situations. Over a wide
range of document types, Binoculars detects over 90% of generated samples from
ChatGPT (and other LLMs) at a false positive rate of 0.01%, despite not being
trained on any ChatGPT data.
\\ ( https://arxiv.org/abs/2401.12070 ,  4003kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12072
Date: Mon, 22 Jan 2024 16:13:45 GMT   (1107kb,D)

Title: Cross-lingual Transfer Learning for Javanese Dependency Parsing
Authors: Fadli Aulawi Al Ghiffari, Ika Alfina, Kurniawati Azizah
Categories: cs.CL
Comments: Accepted at IJCNLP-AACL 2023 SRW
\\
  While structure learning achieves remarkable performance in high-resource
languages, the situation differs for under-represented languages due to the
scarcity of annotated data. This study focuses on assessing the efficacy of
transfer learning in enhancing dependency parsing for Javanese, a language
spoken by 80 million individuals but characterized by limited representation in
natural language processing. We utilized the Universal Dependencies dataset
consisting of dependency treebanks from more than 100 languages, including
Javanese. We propose two learning strategies to train the model: transfer
learning (TL) and hierarchical transfer learning (HTL). While TL only uses a
source language to pre-train the model, the HTL method uses a source language
and an intermediate language in the learning process. The results show that our
best model uses the HTL method, which improves performance with an increase of
10% for both UAS and LAS evaluations compared to the baseline model.
\\ ( https://arxiv.org/abs/2401.12072 ,  1107kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12078
Date: Mon, 22 Jan 2024 16:20:14 GMT   (2019kb,D)

Title: Temporal Blind Spots in Large Language Models
Authors: Jonas Wallat, Adam Jatowt, Avishek Anand
Categories: cs.CL
Comments: accepted at WSDM'24
\\
  Large language models (LLMs) have recently gained significant attention due
to their unparalleled ability to perform various natural language processing
tasks. These models, benefiting from their advanced natural language
understanding capabilities, have demonstrated impressive zero-shot performance.
However, the pre-training data utilized in LLMs is often confined to a specific
corpus, resulting in inherent freshness and temporal scope limitations.
Consequently, this raises concerns regarding the effectiveness of LLMs for
tasks involving temporal intents. In this study, we aim to investigate the
underlying limitations of general-purpose LLMs when deployed for tasks that
require a temporal understanding. We pay particular attention to handling
factual temporal knowledge through three popular temporal QA datasets.
Specifically, we observe low performance on detailed questions about the past
and, surprisingly, for rather new information. In manual and automatic testing,
we find multiple temporal errors and characterize the conditions under which QA
performance deteriorates. Our analysis contributes to understanding LLM
limitations and offers valuable insights into developing future models that can
better cater to the demands of temporally-oriented tasks. The code is
available\footnote{https://github.com/jwallat/temporalblindspots}.
\\ ( https://arxiv.org/abs/2401.12078 ,  2019kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12086
Date: Mon, 22 Jan 2024 16:24:43 GMT   (116kb,D)

Title: West-of-N: Synthetic Preference Generation for Improved Reward Modeling
Authors: Aliz\'ee Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause,
  Aliaksei Severyn
Categories: cs.CL cs.AI cs.LG
\\
  The success of reinforcement learning from human feedback (RLHF) in language
model alignment is strongly dependent on the quality of the underlying reward
model. In this paper, we present a novel approach to improve reward model
quality by generating synthetic preference data, thereby augmenting the
training dataset with on-policy, high-quality preference pairs. Motivated by
the promising results of Best-of-N sampling strategies in language model
training, we extend their application to reward model training. This results in
a self-training strategy to generate preference pairs by selecting the best and
worst candidates in a pool of responses to a given query. Empirically, we find
that this approach improves the performance of any reward model, with an effect
comparable to the addition of a similar quantity of human preference data. This
work opens up new avenues of research for improving RLHF for language model
alignment, by offering synthetic preference generation as a solution to reward
modeling challenges.
\\ ( https://arxiv.org/abs/2401.12086 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12087
Date: Mon, 22 Jan 2024 16:25:27 GMT   (7711kb,D)

Title: Revisiting Demonstration Selection Strategies in In-Context Learning
Authors: Keqin Peng, Liang Ding, Yancheng Yuan, Xuebo Liu, Min Zhang, Yuanxin
  Ouyang, Dacheng Tao
Categories: cs.CL
\\
  Large language models (LLMs) have shown an impressive ability to perform a
wide range of tasks using in-context learning (ICL), where a few examples are
used to describe a task to the model. However, the performance of ICL varies
significantly with the choice of demonstrations, and it is still unclear why
this happens or what factors will influence its choice. In this work, we first
revisit the factors contributing to this variance from both data and model
aspects, and find that the choice of demonstration is both data- and
model-dependent. We further proposed a data- and model-dependent demonstration
selection method, \textbf{TopK + ConE}, based on the assumption that
\textit{the performance of a demonstration positively correlates with its
contribution to the model's understanding of the test samples}, resulting in a
simple and effective recipe for ICL. Empirically, our method yields consistent
improvements in both language understanding and generation tasks with different
model scales. Further analyses confirm that, besides the generality and
stability under different circumstances, our method provides a unified
explanation for the effectiveness of previous methods. Code will be released.
\\ ( https://arxiv.org/abs/2401.12087 ,  7711kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12088
Date: Mon, 22 Jan 2024 16:25:47 GMT   (4252kb,D)

Title: Unsupervised Learning of Graph from Recipes
Authors: Aissatou Diallo, Antonis Bikakis, Luke Dickens, Anthony Hunter, Rob
  Miller
Categories: cs.CL
\\
  Cooking recipes are one of the most readily available kinds of procedural
text. They consist of natural language instructions that can be challenging to
interpret. In this paper, we propose a model to identify relevant information
from recipes and generate a graph to represent the sequence of actions in the
recipe. In contrast with other approaches, we use an unsupervised approach. We
iteratively learn the graph structure and the parameters of a $\mathsf{GNN}$
encoding the texts (text-to-graph) one sequence at a time while providing the
supervision by decoding the graph into text (graph-to-text) and comparing the
generated text to the input. We evaluate the approach by comparing the
identified entities with annotated datasets, comparing the difference between
the input and output texts, and comparing our generated graphs with those
generated by state of the art methods.
\\ ( https://arxiv.org/abs/2401.12088 ,  4252kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12097
Date: Mon, 22 Jan 2024 16:35:00 GMT   (6653kb,D)

Title: An Empirical Analysis of In-context Learning Abilities of LLMs for MT
Authors: Pranjal A. Chitale, Jay Gala, Varun Gumma, Mitesh M. Khapra, Raj Dabre
Categories: cs.CL
Comments: Work in progress
\\
  In-context learning (ICL) has consistently demonstrated superior performance
over zero-shot performance in large language models (LLMs). However, the
understanding of the dynamics of ICL and the aspects that influence downstream
performance remains limited, especially for natural language generation (NLG)
tasks. This work aims to address this gap by investigating the ICL capabilities
of LLMs and studying the impact of different aspects of the in-context
demonstrations for the task of machine translation (MT). Our preliminary
investigations aim to discern whether in-context learning (ICL) is
predominantly influenced by demonstrations or instructions by applying diverse
perturbations to in-context demonstrations while preserving the task
instruction. We observe varying behavior to perturbed examples across different
model families, notably with BLOOM-7B derivatives being severely influenced by
noise, whereas Llama 2 derivatives not only exhibit robustness but also tend to
show enhancements over the clean baseline when subject to perturbed
demonstrations. This suggests that the robustness of ICL may be governed by
several factors, including the type of noise, perturbation direction (source or
target), the extent of pretraining of the specific model, and fine-tuning for
downstream tasks if applicable. Further investigation is warranted to develop a
comprehensive understanding of these factors in future research.
\\ ( https://arxiv.org/abs/2401.12097 ,  6653kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12117
Date: Mon, 22 Jan 2024 16:57:05 GMT   (2413kb,D)

Title: The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large
  Language Models
Authors: Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang,
  Fred Morstatter, Jay Pujara
Categories: cs.CL
Comments: Code and datasets are available at
  https://github.com/kahrabian/mllm-nvar
\\
  While large language models (LLMs) are still being adopted to new domains and
utilized in novel applications, we are experiencing an influx of the new
generation of foundation models, namely multi-modal large language models
(MLLMs). These models integrate verbal and visual information, opening new
possibilities to demonstrate more complex reasoning abilities at the
intersection of the two modalities. However, despite the revolutionizing
prospect of MLLMs, our understanding of their reasoning abilities is limited.
In this study, we assess the nonverbal abstract reasoning abilities of
open-source and closed-source MLLMs using variations of Raven's Progressive
Matrices. Our experiments expose the difficulty of solving such problems while
showcasing the immense gap between open-source and closed-source models. We
also reveal critical shortcomings with individual visual and textual modules,
subjecting the models to low-performance ceilings. Finally, to improve MLLMs'
performance, we experiment with various methods, such as Chain-of-Thought
prompting, resulting in a significant (up to 100%) boost in performance.
\\ ( https://arxiv.org/abs/2401.12117 ,  2413kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12143
Date: Mon, 22 Jan 2024 17:26:55 GMT   (14212kb,D)

Title: Anisotropy Is Inherent to Self-Attention in Transformers
Authors: Nathan Godey and \'Eric de la Clergerie and Beno\^it Sagot
Categories: cs.CL
Comments: Proceedings of EACL 2024. Previously presented at ACL-SRW 2023
  (arXiv:2306.07656). arXiv admin note: substantial text overlap with
  arXiv:2306.07656
\\
  The representation degeneration problem is a phenomenon that is widely
observed among self-supervised learning methods based on Transformers. In NLP,
it takes the form of anisotropy, a singular property of hidden representations
which makes them unexpectedly close to each other in terms of angular distance
(cosine-similarity). Some recent works tend to show that anisotropy is a
consequence of optimizing the cross-entropy loss on long-tailed distributions
of tokens. We show in this paper that anisotropy can also be observed
empirically in language models with specific objectives that should not suffer
directly from the same consequences. We also show that the anisotropy problem
extends to Transformers trained on other modalities. Our observations suggest
that anisotropy is actually inherent to Transformers-based models.
\\ ( https://arxiv.org/abs/2401.12143 ,  14212kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12178
Date: Mon, 22 Jan 2024 18:09:52 GMT   (462kb,D)

Title: In-Context Learning for Extreme Multi-Label Classification
Authors: Karel D'Oosterlinck, Omar Khattab, Fran\c{c}ois Remy, Thomas
  Demeester, Chris Develder, Christopher Potts
Categories: cs.CL cs.AI
\\
  Multi-label classification problems with thousands of classes are hard to
solve with in-context learning alone, as language models (LMs) might lack prior
knowledge about the precise classes or how to assign them, and it is generally
infeasible to demonstrate every class in a prompt. We propose a general
program, $\texttt{Infer--Retrieve--Rank}$, that defines multi-step interactions
between LMs and retrievers to efficiently tackle such problems. We implement
this program using the $\texttt{DSPy}$ programming model, which specifies
in-context systems in a declarative manner, and use $\texttt{DSPy}$ optimizers
to tune it towards specific datasets by bootstrapping only tens of few-shot
examples. Our primary extreme classification program, optimized separately for
each task, attains state-of-the-art results across three benchmarks (HOUSE,
TECH, TECHWOLF). We apply the same program to a benchmark with vastly different
characteristics and attain competitive performance as well (BioDEX). Unlike
prior work, our proposed solution requires no finetuning, is easily applicable
to new tasks, alleviates prompt engineering, and requires only tens of labeled
examples. Our code is public at https://github.com/KarelDO/xmc.dspy.
\\ ( https://arxiv.org/abs/2401.12178 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12192
Date: Mon, 22 Jan 2024 18:34:42 GMT   (9783kb,D)

Title: Text Embedding Inversion Attacks on Multilingual Language Models
Authors: Yiyi Chen and Heather Lent and Johannes Bjerva
Categories: cs.CL cs.AI cs.CR
Comments: 13 pages
\\
  Representing textual information as real-numbered embeddings has become the
norm in NLP. Moreover, with the rise of public interest in large language
models (LLMs), Embeddings as a Service (EaaS) has rapidly gained traction as a
business model. This is not without outstanding security risks, as previous
research has demonstrated that sensitive data can be reconstructed from
embeddings, even without knowledge of the underlying model that generated them.
However, such work is limited by its sole focus on English, leaving all other
languages vulnerable to attacks by malicious actors. %As many international and
multilingual companies leverage EaaS, there is an urgent need for research into
multilingual LLM security. To this end, this work investigates LLM security
from the perspective of multilingual embedding inversion. Concretely, we define
the problem of black-box multilingual and cross-lingual inversion attacks, with
special attention to a cross-domain scenario. Our findings reveal that
multilingual models are potentially more vulnerable to inversion attacks than
their monolingual counterparts. This stems from the reduced data requirements
for achieving comparable inversion performance in settings where the underlying
language is not known a-priori. To our knowledge, this work is the first to
delve into multilinguality within the context of inversion attacks, and our
findings highlight the need for further investigation and enhanced defenses in
the area of NLP Security.
\\ ( https://arxiv.org/abs/2401.12192 ,  9783kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12200
Date: Mon, 22 Jan 2024 18:39:40 GMT   (468kb,D)

Title: APT: Adaptive Pruning and Tuning Pretrained Language Models for
  Efficient Training and Inference
Authors: Bowen Zhao, Hannaneh Hajishirzi, Qingqing Cao
Categories: cs.CL cs.LG
Comments: 19 pages, 6 figures
\\
  Fine-tuning and inference with large Language Models (LM) are generally known
to be expensive. Parameter-efficient fine-tuning over pretrained LMs reduces
training memory by updating a small number of LM parameters but does not
improve inference efficiency. Structured pruning improves LM inference
efficiency by removing consistent parameter blocks, yet often increases
training memory and time. To improve both training and inference efficiency, we
introduce APT that adaptively prunes and tunes parameters for the LMs. At the
early stage of fine-tuning, APT dynamically adds salient tuning parameters for
fast and accurate convergence while discarding unimportant parameters for
efficiency. Compared to baselines, our experiments show that APT maintains up
to 98% task performance when pruning RoBERTa and T5 models with 40% parameters
left while keeping 86.4% LLaMA models' performance with 70% parameters
remained. Furthermore, APT speeds up LMs fine-tuning by up to 8x and reduces
large LMs memory training footprint by up to 70%.
\\ ( https://arxiv.org/abs/2401.12200 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10895
Date: Tue, 12 Dec 2023 17:47:51 GMT   (47412kb,D)

Title: AI in Supply Chain Risk Assessment: A Systematic Literature Review and
  Bibliometric Analysis
Authors: Md Abrar Jahin, Saleh Akram Naife, Anik Kumar Saha, and M. F. Mridha
Categories: cs.LG cs.CE
\\
  Supply chain risk assessment (SCRA) has witnessed a profound evolution
through the integration of artificial intelligence (AI) and machine learning
(ML) techniques, revolutionizing predictive capabilities and risk mitigation
strategies. The significance of this evolution stems from the critical role of
robust risk management strategies in ensuring operational resilience and
continuity within modern supply chains. Previous reviews have outlined
established methodologies but have overlooked emerging AI/ML techniques,
leaving a notable research gap in understanding their practical implications
within SCRA. This paper conducts a systematic literature review combined with a
comprehensive bibliometric analysis. We meticulously examined 1,717 papers and
derived key insights from a select group of 48 articles published between 2014
and 2023. The review fills this research gap by addressing pivotal research
questions, and exploring existing AI/ML techniques, methodologies, findings,
and future trajectories, thereby providing a more encompassing view of the
evolving landscape of SCRA. Our study unveils the transformative impact of
AI/ML models, such as Random Forest, XGBoost, and hybrids, in substantially
enhancing precision within SCRA. It underscores adaptable post-COVID
strategies, advocating for resilient contingency plans and aligning with
evolving risk landscapes. Significantly, this review surpasses previous
examinations by accentuating emerging AI/ML techniques and their practical
implications within SCRA. Furthermore, it highlights the contributions through
a comprehensive bibliometric analysis, revealing publication trends,
influential authors, and highly cited articles.
\\ ( https://arxiv.org/abs/2401.10895 ,  47412kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11016
Date: Fri, 19 Jan 2024 20:27:29 GMT   (73kb)

Title: Bounding Consideration Probabilities in Consider-Then-Choose Ranking
  Models
Authors: Ben Aoki-Sherwood, Catherine Bregou, David Liben-Nowell, Kiran
  Tomlinson, Thomas Zeng
Categories: cs.LG cs.MA econ.EM
Comments: 11 pages; accepted as an extended abstract to AAMAS '24
\\
  A common theory of choice posits that individuals make choices in a two-step
process, first selecting some subset of the alternatives to consider before
making a selection from the resulting consideration set. However, inferring
unobserved consideration sets (or item consideration probabilities) in this
"consider then choose" setting poses significant challenges, because even
simple models of consideration with strong independence assumptions are not
identifiable, even if item utilities are known. We consider a natural extension
of consider-then-choose models to a top-$k$ ranking setting, where we assume
rankings are constructed according to a Plackett-Luce model after sampling a
consideration set. While item consideration probabilities remain non-identified
in this setting, we prove that knowledge of item utilities allows us to infer
bounds on the relative sizes of consideration probabilities. Additionally,
given a condition on the expected consideration set size, we derive absolute
upper and lower bounds on item consideration probabilities. We also provide
algorithms to tighten those bounds on consideration probabilities by
propagating inferred constraints. Thus, we show that we can learn useful
information about consideration probabilities despite not being able to
identify them precisely. We demonstrate our methods on a ranking dataset from a
psychology experiment with two different ranking tasks (one with fixed
consideration sets and one with unknown consideration sets). This combination
of data allows us to estimate utilities and then learn about unknown
consideration probabilities using our bounds.
\\ ( https://arxiv.org/abs/2401.11016 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11018
Date: Fri, 19 Jan 2024 20:35:02 GMT   (925kb,D)

Title: Communication Efficient and Provable Federated Unlearning
Authors: Youming Tao, Cheng-Long Wang, Miao Pan, Dongxiao Yu, Xiuzhen Cheng, Di
  Wang
Categories: cs.LG cs.DC
\\
  We study federated unlearning, a novel problem to eliminate the impact of
specific clients or data points on the global model learned via federated
learning (FL). This problem is driven by the right to be forgotten and the
privacy challenges in FL. We introduce a new framework for exact federated
unlearning that meets two essential criteria: \textit{communication efficiency}
and \textit{exact unlearning provability}. To our knowledge, this is the first
work to tackle both aspects coherently. We start by giving a rigorous
definition of \textit{exact} federated unlearning, which guarantees that the
unlearned model is statistically indistinguishable from the one trained without
the deleted data. We then pinpoint the key property that enables fast exact
federated unlearning: total variation (TV) stability, which measures the
sensitivity of the model parameters to slight changes in the dataset.
Leveraging this insight, we develop a TV-stable FL algorithm called
\texttt{FATS}, which modifies the classical
\texttt{\underline{F}ed\underline{A}vg} algorithm for \underline{T}V
\underline{S}tability and employs local SGD with periodic averaging to lower
the communication round. We also design efficient unlearning algorithms for
\texttt{FATS} under two settings: client-level and sample-level unlearning. We
provide theoretical guarantees for our learning and unlearning algorithms,
proving that they achieve exact federated unlearning with reasonable
convergence rates for both the original and unlearned models. We empirically
validate our framework on 6 benchmark datasets, and show its superiority over
state-of-the-art methods in terms of accuracy, communication cost, computation
cost, and unlearning efficacy.
\\ ( https://arxiv.org/abs/2401.11018 ,  925kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11037
Date: Fri, 19 Jan 2024 21:50:32 GMT   (2509kb,D)

Title: Equivariant Graph Neural Operator for Modeling 3D Dynamics
Authors: Minkai Xu, Jiaqi Han, Aaron Lou, Jean Kossaifi, Arvind Ramanathan,
  Kamyar Azizzadenesheli, Jure Leskovec, Stefano Ermon, Anima Anandkumar
Categories: cs.LG cs.NA math.NA q-bio.QM
\\
  Modeling the complex three-dimensional (3D) dynamics of relational systems is
an important problem in the natural sciences, with applications ranging from
molecular simulations to particle mechanics. Machine learning methods have
achieved good success by learning graph neural networks to model spatial
interactions. However, these approaches do not faithfully capture temporal
correlations since they only model next-step predictions. In this work, we
propose Equivariant Graph Neural Operator (EGNO), a novel and principled method
that directly models dynamics as trajectories instead of just next-step
prediction. Different from existing methods, EGNO explicitly learns the
temporal evolution of 3D dynamics where we formulate the dynamics as a function
over time and learn neural operators to approximate it. To capture the temporal
correlations while keeping the intrinsic SE(3)-equivariance, we develop
equivariant temporal convolutions parameterized in the Fourier space and build
EGNO by stacking the Fourier layers over equivariant networks. EGNO is the
first operator learning framework that is capable of modeling solution dynamics
functions over time while retaining 3D equivariance. Comprehensive experiments
in multiple domains, including particle simulations, human motion capture, and
molecular dynamics, demonstrate the significantly superior performance of EGNO
against existing methods, thanks to the equivariant temporal modeling.
\\ ( https://arxiv.org/abs/2401.11037 ,  2509kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11044
Date: Fri, 19 Jan 2024 22:11:54 GMT   (1621kb,D)

Title: The Significance of Data Abstraction Methods in Machine Learning
  Classification Processes for Critical Decision-Making
Authors: Karol Capa{\l}a, Paulina Tworek, Jose Sousa
Categories: cs.LG cs.AI
Comments: 24 pages, 4 figures, 15 tables
\\
  The applicability of widely adopted machine learning (ML) methods to
classification is circumscribed by the imperatives of explicability and
uncertainty, particularly evident in domains such as healthcare, behavioural
sciences, and finances, wherein accountability assumes priority. Recently,
Small and Incomplete Dataset Analyser (SaNDA) has been proposed to enhance the
ability to perform classification in such domains, by developing a data
abstraction protocol using a ROC curve-based method. This paper focuses on
column-wise data transformations called abstractions, which are crucial for
SaNDA's classification process and explores alternative abstractions protocols,
such as constant binning and quantiles. The best-performing methods have been
compared against Random Forest as a baseline for explainable methods. The
results suggests that SaNDA can be a viable substitute for Random Forest when
data is incomplete, even with minimal missing values. It consistently maintains
high accuracy even when half of the dataset is missing, unlike Random Forest
which experiences a significant decline in accuracy under similar conditions.
\\ ( https://arxiv.org/abs/2401.11044 ,  1621kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11074
Date: Sat, 20 Jan 2024 01:12:57 GMT   (1413kb,D)

Title: On The Temporal Domain of Differential Equation Inspired Graph Neural
  Networks
Authors: Moshe Eliasof, Eldad Haber, Eran Treister, Carola-Bibiane Sch\"onlieb
Categories: cs.LG
Comments: AISTATS 2024
\\
  Graph Neural Networks (GNNs) have demonstrated remarkable success in modeling
complex relationships in graph-structured data. A recent innovation in this
field is the family of Differential Equation-Inspired Graph Neural Networks
(DE-GNNs), which leverage principles from continuous dynamical systems to model
information flow on graphs with built-in properties such as feature smoothing
or preservation. However, existing DE-GNNs rely on first or second-order
temporal dependencies. In this paper, we propose a neural extension to those
pre-defined temporal dependencies. We show that our model, called TDE-GNN, can
capture a wide range of temporal dynamics that go beyond typical first or
second-order methods, and provide use cases where existing temporal models are
challenged. We demonstrate the benefit of learning the temporal dependencies
using our method rather than using pre-defined temporal dynamics on several
graph benchmarks.
\\ ( https://arxiv.org/abs/2401.11074 ,  1413kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11081
Date: Sat, 20 Jan 2024 02:14:11 GMT   (364kb,D)

Title: Learning from Aggregate responses: Instance Level versus Bag Level Loss
  Functions
Authors: Adel Javanmard, Lin Chen, Vahab Mirrokni, Ashwinkumar Badanidiyuru,
  Gang Fu
Categories: cs.LG cs.AI math.ST stat.ML stat.TH
Comments: To appear in the Twelfth International Conference on Learning
  Representations (ICLR 2024)
\\
  Due to the rise of privacy concerns, in many practical applications the
training data is aggregated before being shared with the learner, in order to
protect privacy of users' sensitive responses. In an aggregate learning
framework, the dataset is grouped into bags of samples, where each bag is
available only with an aggregate response, providing a summary of individuals'
responses in that bag. In this paper, we study two natural loss functions for
learning from aggregate responses: bag-level loss and the instance-level loss.
In the former, the model is learnt by minimizing a loss between aggregate
responses and aggregate model predictions, while in the latter the model aims
to fit individual predictions to the aggregate responses. In this work, we show
that the instance-level loss can be perceived as a regularized form of the
bag-level loss. This observation lets us compare the two approaches with
respect to bias and variance of the resulting estimators, and introduce a novel
interpolating estimator which combines the two approaches. For linear
regression tasks, we provide a precise characterization of the risk of the
interpolating estimator in an asymptotic regime where the size of the training
set grows in proportion to the features dimension. Our analysis allows us to
theoretically understand the effect of different factors, such as bag size on
the model prediction risk. In addition, we propose a mechanism for
differentially private learning from aggregate responses and derive the optimal
bag size in terms of prediction risk-privacy trade-off. We also carry out
thorough experiments to corroborate our theory and show the efficacy of the
interpolating estimator.
\\ ( https://arxiv.org/abs/2401.11081 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11113
Date: Sat, 20 Jan 2024 04:38:34 GMT   (8050kb,D)

Title: SPAND: Sleep Prediction Architecture using Network Dynamics
Authors: Maryam Khalid, Elizabeth B. Klerman, Andrew W. Mchill, Andrew J. K.
  Phillips, Akane Sano
Categories: cs.LG cs.AI cs.SI eess.SP
Comments: Accepted for publication in Proceedings of the ACM on Interactive,
  Mobile, Wearable and Ubiquitous Technologies (IMWUT), 8 (March 2024)
\\
  Sleep behavior significantly impacts health and acts as an indicator of
physical and mental well-being. Monitoring and predicting sleep behavior with
ubiquitous sensors may therefore assist in both sleep management and tracking
of related health conditions. While sleep behavior depends on, and is reflected
in the physiology of a person, it is also impacted by external factors such as
digital media usage, social network contagion, and the surrounding weather. In
this work, we propose SPAND (Sleep Prediction Architecture using Network
Dynamics), a system that exploits social contagion in sleep behavior through
graph networks and integrates it with physiological and phone data extracted
from ubiquitous mobile and wearable devices for predicting next-day sleep
labels about sleep duration. Our architecture overcomes the limitations of
large-scale graphs containing connections irrelevant to sleep behavior by
devising an attention mechanism. The extensive experimental evaluation
highlights the improvement provided by incorporating social networks in the
model. Additionally, we conduct robustness analysis to demonstrate the system's
performance in real-life conditions. The outcomes affirm the stability of SPAND
against perturbations in input data. Further analyses emphasize the
significance of network topology in prediction performance revealing that users
with higher eigenvalue centrality are more vulnerable to data perturbations.
\\ ( https://arxiv.org/abs/2401.11113 ,  8050kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11118
Date: Sat, 20 Jan 2024 05:05:39 GMT   (1254kb,D)

Title: Meta Reinforcement Learning for Strategic IoT Deployments Coverage in
  Disaster-Response UAV Swarms
Authors: Marwan Dhuheir, Aiman Erbad, Ala Al-Fuqaha
Categories: cs.LG cs.RO
Comments: accepted paper at GlobeCom Conference, 2023- Kuala Lumpor - Malayisa
\\
  In the past decade, Unmanned Aerial Vehicles (UAVs) have grabbed the
attention of researchers in academia and industry for their potential use in
critical emergency applications, such as providing wireless services to ground
users and collecting data from areas affected by disasters, due to their
advantages in terms of maneuverability and movement flexibility. The UAVs'
limited resources, energy budget, and strict mission completion time have posed
challenges in adopting UAVs for these applications. Our system model considers
a UAV swarm that navigates an area collecting data from ground IoT devices
focusing on providing better service for strategic locations and allowing UAVs
to join and leave the swarm (e.g., for recharging) in a dynamic way. In this
work, we introduce an optimization model with the aim of minimizing the total
energy consumption and provide the optimal path planning of UAVs under the
constraints of minimum completion time and transmit power. The formulated
optimization is NP-hard making it not applicable for real-time decision making.
Therefore, we introduce a light-weight meta-reinforcement learning solution
that can also cope with sudden changes in the environment through fast
convergence. We conduct extensive simulations and compare our approach to three
state-of-the-art learning models. Our simulation results prove that our
introduced approach is better than the three state-of-the-art algorithms in
providing coverage to strategic locations with fast convergence.
\\ ( https://arxiv.org/abs/2401.11118 ,  1254kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11130
Date: Sat, 20 Jan 2024 05:48:46 GMT   (217kb)

Title: Identification and Estimation of Conditional Average Partial Causal
  Effects via Instrumental Variable
Authors: Yuta Kawakami, Manabu Kuroki, Jin Tian
Categories: cs.LG stat.ML
\\
  There has been considerable recent interest in estimating heterogeneous
causal effects. In this paper, we introduce conditional average partial causal
effects (CAPCE) to reveal the heterogeneity of causal effects with continuous
treatment. We provide conditions for identifying CAPCE in an instrumental
variable setting. We develop three families of CAPCE estimators: sieve,
parametric, and reproducing kernel Hilbert space (RKHS)-based, and analyze
their statistical properties. We illustrate the proposed CAPCE estimators on
synthetic and real-world data.
\\ ( https://arxiv.org/abs/2401.11130 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11143
Date: Sat, 20 Jan 2024 06:42:32 GMT   (4158kb,D)

Title: Gaussian Adaptive Attention is All You Need: Robust Contextual
  Representations Across Multiple Modalities
Authors: Georgios Ioannides, Aman Chadha, Aaron Elkins
Categories: cs.LG cs.AI cs.CL cs.CV cs.SD eess.AS eess.SP
\\
  We propose the Multi-Head Gaussian Adaptive Attention Mechanism (GAAM), a
novel probabilistic attention framework, and the Gaussian Adaptive Transformer
(GAT), designed to enhance information aggregation across multiple modalities,
including Speech, Text and Vision. GAAM integrates learnable mean and variance
into its attention mechanism, implemented in a Multi-Headed framework enabling
it to collectively model any Probability Distribution for dynamic recalibration
of feature significance. This method demonstrates significant improvements,
especially with highly non-stationary data, surpassing the state-of-the-art
attention techniques in model performance (up to approximately +20% in
accuracy) by identifying key elements within the feature space. GAAM's
compatibility with dot-product-based attention models and relatively low number
of parameters showcases its adaptability and potential to boost existing
attention frameworks. Empirically, GAAM exhibits superior adaptability and
efficacy across a diverse range of tasks, including emotion recognition in
speech, image classification, and text classification, thereby establishing its
robustness and versatility in handling multi-modal data. Furthermore, we
introduce the Importance Factor (IF), a new learning-based metric that enhances
the explainability of models trained with GAAM-based methods. Overall, GAAM
represents an advancement towards development of better performing and more
explainable attention models across multiple modalities.
\\ ( https://arxiv.org/abs/2401.11143 ,  4158kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11145
Date: Sat, 20 Jan 2024 06:52:14 GMT   (82kb,D)

Title: Document Set Expansion with Positive-Unlabeled Learning: A Density
  Estimation-based Approach
Authors: Haiyang Zhang, Qiuyi Chen, Yuanjie Zou, Yushan Pan, Jia Wang, Mark
  Stevenson
Categories: cs.LG cs.IR
\\
  Document set expansion aims to identify relevant documents from a large
collection based on a small set of documents that are on a fine-grained topic.
Previous work shows that PU learning is a promising method for this task.
However, some serious issues remain unresolved, i.e. typical challenges that PU
methods suffer such as unknown class prior and imbalanced data, and the need
for transductive experimental settings. In this paper, we propose a novel PU
learning framework based on density estimation, called puDE, that can handle
the above issues. The advantage of puDE is that it neither constrained to the
SCAR assumption and nor require any class prior knowledge. We demonstrate the
effectiveness of the proposed method using a series of real-world datasets and
conclude that our method is a better alternative for the DSE task.
\\ ( https://arxiv.org/abs/2401.11145 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11188
Date: Sat, 20 Jan 2024 09:51:52 GMT   (316kb,D)

Title: Fast and Exact Enumeration of Deep Networks Partitions Regions
Authors: Randall Balestriero, Yann LeCun
Categories: cs.LG cs.AI
\\
  One fruitful formulation of Deep Networks (DNs) enabling their theoretical
study and providing practical guidelines to practitioners relies on Piecewise
Affine Splines. In that realm, a DN's input-mapping is expressed as per-region
affine mapping where those regions are implicitly determined by the model's
architecture and form a partition of their input space. That partition -- which
is involved in all the results spanned from this line of research -- has so far
only been computed on $2/3$-dimensional slices of the DN's input space or
estimated by random sampling. In this paper, we provide the first parallel
algorithm that does exact enumeration of the DN's partition regions. The
proposed algorithm enables one to finally assess the closeness of the commonly
employed approximations methods, e.g. based on random sampling of the DN input
space. One of our key finding is that if one is only interested in regions with
``large'' volume, then uniform sampling of the space is highly efficient, but
that if one is also interested in discovering the ``small'' regions of the
partition, then uniform sampling is exponentially costly with the DN's input
space dimension. On the other hand, our proposed method has complexity scaling
linearly with input dimension and the number of regions.
\\ ( https://arxiv.org/abs/2401.11188 ,  316kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11199
Date: Sat, 20 Jan 2024 10:27:04 GMT   (1624kb)

Title: Projected Belief Networks With Discriminative Alignment for Acoustic
  Event Classification: Rivaling State of the Art CNNs
Authors: Paul M. Baggenstoss, Kevin Wilkinghoff, Felix Govaers, Frank Kurth
Categories: cs.LG cs.SD eess.AS
Comments: 15 Pages. Submitted to IEEE-TNNLS
ACM-class: G.3; I.2
\\
  The projected belief network (PBN) is a generative stochastic network with
tractable likelihood function based on a feed-forward neural network (FFNN).
The generative function operates by "backing up" through the FFNN. The PBN is
two networks in one, a FFNN that operates in the forward direction, and a
generative network that operates in the backward direction. Both networks
co-exist based on the same parameter set, have their own cost functions, and
can be separately or jointly trained. The PBN therefore has the potential to
possess the best qualities of both discriminative and generative classifiers.
To realize this potential, a separate PBN is trained on each class, maximizing
the generative likelihood function for the given class, while minimizing the
discriminative cost for the FFNN against "all other classes". This technique,
called discriminative alignment (PBN-DA), aligns the contours of the likelihood
function to the decision boundaries and attains vastly improved classification
performance, rivaling that of state of the art discriminative networks. The
method may be further improved using a hidden Markov model (HMM) as a component
of the PBN, called PBN-DA-HMM. This paper provides a comprehensive treatment of
PBN, PBN-DA, and PBN-DA-HMM. In addition, the results of two new classification
experiments are provided. The first experiment uses air-acoustic events, and
the second uses underwater acoustic data consisting of marine mammal calls. In
both experiments, PBN-DA-HMM attains comparable or better performance as a
state of the art CNN, and attain a factor of two error reduction when combined
with the CNN.
\\ ( https://arxiv.org/abs/2401.11199 ,  1624kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11202
Date: Sat, 20 Jan 2024 10:30:31 GMT   (1015kb,D)

Title: PartIR: Composing SPMD Partitioning Strategies for Machine Learning
Authors: Sami Alabed, Bart Chrzaszcz, Juliana Franco, Dominik Grewe, Dougal
  Maclaurin, James Molloy, Tom Natan, Tamara Norman, Xiaoyue Pan, Adam Paszke,
  Norman A. Rink, Michael Schaarschmidt, Timur Sitdikov, Agnieszka Swietlik,
  Dimitrios Vytiniotis, Joel Wee
Categories: cs.LG cs.DC cs.PL
\\
  Training of modern large neural networks (NN) requires a combination of
parallelization strategies encompassing data, model, or optimizer sharding.
When strategies increase in complexity, it becomes necessary for partitioning
tools to be 1) expressive, allowing the composition of simpler strategies, and
2) predictable to estimate performance analytically. We present PartIR, our
design for a NN partitioning system. PartIR is focused on an incremental
approach to rewriting and is hardware-and-runtime agnostic. We present a simple
but powerful API for composing sharding strategies and a simulator to validate
them. The process is driven by high-level programmer-issued partitioning
tactics, which can be both manual and automatic. Importantly, the tactics are
specified separately from the model code, making them easy to change. We
evaluate PartIR on several different models to demonstrate its predictability,
expressibility, and ability to reach peak performance..
\\ ( https://arxiv.org/abs/2401.11202 ,  1015kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11215
Date: Sat, 20 Jan 2024 11:39:32 GMT   (7633kb,D)

Title: Selecting Walk Schemes for Database Embedding
Authors: Yuval Lev Lubarsky, Jan T\"onshoff, Martin Grohe, Benny Kimelfeld
Categories: cs.LG cs.DB
Comments: Accepted by CIKM 2023, 10 pages
Journal-ref: CIKM 2023
DOI: 10.1145/3583780.3615052
\\
  Machinery for data analysis often requires a numeric representation of the
input. Towards that, a common practice is to embed components of structured
data into a high-dimensional vector space. We study the embedding of the tuples
of a relational database, where existing techniques are often based on
optimization tasks over a collection of random walks from the database. The
focus of this paper is on the recent FoRWaRD algorithm that is designed for
dynamic databases, where walks are sampled by following foreign keys between
tuples. Importantly, different walks have different schemas, or "walk schemes",
that are derived by listing the relations and attributes along the walk. Also
importantly, different walk schemes describe relationships of different natures
in the database. We show that by focusing on a few informative walk schemes, we
can obtain tuple embedding significantly faster, while retaining the quality.
We define the problem of scheme selection for tuple embedding, devise several
approaches and strategies for scheme selection, and conduct a thorough
empirical study of the performance over a collection of downstream tasks. Our
results confirm that with effective strategies for scheme selection, we can
obtain high-quality embeddings considerably (e.g., three times) faster,
preserve the extensibility to newly inserted tuples, and even achieve an
increase in the precision of some tasks.
\\ ( https://arxiv.org/abs/2401.11215 ,  7633kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11217
Date: Sat, 20 Jan 2024 11:53:08 GMT   (1799kb)

Title: A Hybrid Approach of Transfer Learning and Physics-Informed Modeling:
  Improving Dissolved Oxygen Concentration Prediction in an Industrial
  Wastewater Treatment Plant
Authors: Ece S. Koksal and Erdal Aydin
Categories: cs.LG cs.AI math.DS
\\
  Constructing first principles models is a challenging task for nonlinear and
complex systems such as a wastewater treatment unit. In recent years,
data-driven models are widely used to overcome the complexity. However, they
often suffer from issues such as missing, low quality or noisy data. Transfer
learning is a solution for this issue where knowledge from another task is
transferred to target one to increase the prediction performance. In this work,
the objective is increasing the prediction performance of an industrial
wastewater treatment plant by transferring the knowledge of (i) an open-source
simulation model that captures the underlying physics of the process, albeit
with dissimilarities to the target plant, (ii) another industrial plant
characterized by noisy and limited data but located in the same refinery, and
(iii) the model in (ii) and making the objective function of the training
problem physics informed where the physics information derived from the
open-source model in (ii). The results have shown that test and validation
performance are improved up to 27% and 59%, respectively.
\\ ( https://arxiv.org/abs/2401.11217 ,  1799kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11235
Date: Sat, 20 Jan 2024 14:15:04 GMT   (699kb,D)

Title: TreeMIL: A Multi-instance Learning Framework for Time Series Anomaly
  Detection with Inexact Supervision
Authors: Chen Liu, Shibo He, Haoyu Liu, Shizhong Li
Categories: cs.LG cs.AI
Comments: This paper has been accepted by IEEE ICASSP 2024
\\
  Time series anomaly detection (TSAD) plays a vital role in various domains
such as healthcare, networks, and industry. Considering labels are crucial for
detection but difficult to obtain, we turn to TSAD with inexact supervision:
only series-level labels are provided during the training phase, while
point-level anomalies are predicted during the testing phase. Previous works
follow a traditional multi-instance learning (MIL) approach, which focuses on
encouraging high anomaly scores at individual time steps. However, time series
anomalies are not only limited to individual point anomalies, they can also be
collective anomalies, typically exhibiting abnormal patterns over subsequences.
To address the challenge of collective anomalies, in this paper, we propose a
tree-based MIL framework (TreeMIL). We first adopt an N-ary tree structure to
divide the entire series into multiple nodes, where nodes at different levels
represent subsequences with different lengths. Then, the subsequence features
are extracted to determine the presence of collective anomalies. Finally, we
calculate point-level anomaly scores by aggregating features from nodes at
different levels. Experiments conducted on seven public datasets and eight
baselines demonstrate that TreeMIL achieves an average 32.3% improvement in F1-
score compared to previous state-of-the-art methods. The code is available at
https://github.com/fly-orange/TreeMIL.
\\ ( https://arxiv.org/abs/2401.11235 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11237
Date: Sat, 20 Jan 2024 14:23:25 GMT   (987kb,D)

Title: Closing the Gap between TD Learning and Supervised Learning -- A
  Generalisation Point of View
Authors: Raj Ghugare, Matthieu Geist, Glen Berseth, Benjamin Eysenbach
Categories: cs.LG
Comments: ICLR 2024, Project code:
  https://github.com/RajGhugare19/stitching-is-combinatorial-generalisation
\\
  Some reinforcement learning (RL) algorithms can stitch pieces of experience
to solve a task never seen before during training. This oft-sought property is
one of the few ways in which RL methods based on dynamic-programming differ
from RL methods based on supervised-learning (SL). Yet, certain RL methods
based on off-the-shelf SL algorithms achieve excellent results without an
explicit mechanism for stitching; it remains unclear whether those methods
forgo this important stitching property. This paper studies this question for
the problems of achieving a target goal state and achieving a target return
value. Our main result is to show that the stitching property corresponds to a
form of combinatorial generalization: after training on a distribution of
(state, goal) pairs, one would like to evaluate on (state, goal) pairs not seen
together in the training data. Our analysis shows that this sort of
generalization is different from i.i.d. generalization. This connection between
stitching and generalisation reveals why we should not expect SL-based RL
methods to perform stitching, even in the limit of large datasets and models.
Based on this analysis, we construct new datasets to explicitly test for this
property, revealing that SL-based methods lack this stitching property and
hence fail to perform combinatorial generalization. Nonetheless, the connection
between stitching and combinatorial generalisation also suggests a simple
remedy for improving generalisation in SL: data augmentation. We propose a
temporal data augmentation and demonstrate that adding it to SL-based methods
enables them to successfully complete tasks not seen together during training.
On a high level, this connection illustrates the importance of combinatorial
generalization for data efficiency in time-series data beyond tasks beyond RL,
like audio, video, or text.
\\ ( https://arxiv.org/abs/2401.11237 ,  987kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11250
Date: Sat, 20 Jan 2024 15:09:41 GMT   (466kb,D)

Title: AFS-BM: Enhancing Model Performance through Adaptive Feature Selection
  with Binary Masking
Authors: Mehmet Y. Turali, Mehmet E. Lorasdagi, Ali T. Koc and Suleyman S.
  Kozat
Categories: cs.LG eess.SP stat.ML
\\
  We study the problem of feature selection in general machine learning (ML)
context, which is one of the most critical subjects in the field. Although,
there exist many feature selection methods, however, these methods face
challenges such as scalability, managing high-dimensional data, dealing with
correlated features, adapting to variable feature importance, and integrating
domain knowledge. To this end, we introduce the ``Adaptive Feature Selection
with Binary Masking" (AFS-BM) which remedies these problems. AFS-BM achieves
this by joint optimization for simultaneous feature selection and model
training. In particular, we do the joint optimization and binary masking to
continuously adapt the set of features and model parameters during the training
process. This approach leads to significant improvements in model accuracy and
a reduction in computational requirements. We provide an extensive set of
experiments where we compare AFS-BM with the established feature selection
methods using well-known datasets from real-life competitions. Our results show
that AFS-BM makes significant improvement in terms of accuracy and requires
significantly less computational complexity. This is due to AFS-BM's ability to
dynamically adjust to the changing importance of features during the training
process, which an important contribution to the field. We openly share our code
for the replicability of our results and to facilitate further research.
\\ ( https://arxiv.org/abs/2401.11250 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11252
Date: Sat, 20 Jan 2024 15:14:14 GMT   (254kb,D)

Title: Automated Fusion of Multimodal Electronic Health Records for Better
  Medical Predictions
Authors: Suhan Cui, Jiaqi Wang, Yuan Zhong, Han Liu, Ting Wang, Fenglong Ma
Categories: cs.LG cs.AI
Comments: Accepted by SDM 2024
\\
  The widespread adoption of Electronic Health Record (EHR) systems in
healthcare institutes has generated vast amounts of medical data, offering
significant opportunities for improving healthcare services through deep
learning techniques. However, the complex and diverse modalities and feature
structures in real-world EHR data pose great challenges for deep learning model
design. To address the multi-modality challenge in EHR data, current approaches
primarily rely on hand-crafted model architectures based on intuition and
empirical experiences, leading to sub-optimal model architectures and limited
performance. Therefore, to automate the process of model design for mining EHR
data, we propose a novel neural architecture search (NAS) framework named
AutoFM, which can automatically search for the optimal model architectures for
encoding diverse input modalities and fusion strategies. We conduct thorough
experiments on real-world multi-modal EHR data and prediction tasks, and the
results demonstrate that our framework not only achieves significant
performance improvement over existing state-of-the-art methods but also
discovers meaningful network architectures effectively.
\\ ( https://arxiv.org/abs/2401.11252 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11261
Date: Sat, 20 Jan 2024 16:01:18 GMT   (18536kb,D)

Title: Diffusion Model Conditioning on Gaussian Mixture Model and Negative
  Gaussian Mixture Gradient
Authors: Weiguo Lu, Xuan Wu, Deng Ding, Jinqiao Duan, Jirong Zhuang, Gangnan
  Yuan
Categories: cs.LG cs.CV
\\
  Diffusion models (DMs) are a type of generative model that has a huge impact
on image synthesis and beyond. They achieve state-of-the-art generation results
in various generative tasks. A great diversity of conditioning inputs, such as
text or bounding boxes, are accessible to control the generation. In this work,
we propose a conditioning mechanism utilizing Gaussian mixture models (GMMs) as
feature conditioning to guide the denoising process. Based on set theory, we
provide a comprehensive theoretical analysis that shows that conditional latent
distribution based on features and classes is significantly different, so that
conditional latent distribution on features produces fewer defect generations
than conditioning on classes. Two diffusion models conditioned on the Gaussian
mixture model are trained separately for comparison. Experiments support our
findings. A novel gradient function called the negative Gaussian mixture
gradient (NGMG) is proposed and applied in diffusion model training with an
additional classifier. Training stability has improved. We also theoretically
prove that NGMG shares the same benefit as the Earth Mover distance
(Wasserstein) as a more sensible cost function when learning distributions
supported by low-dimensional manifolds.
\\ ( https://arxiv.org/abs/2401.11261 ,  18536kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11271
Date: Sat, 20 Jan 2024 16:56:52 GMT   (266kb,D)

Title: DACR: Distribution-Augmented Contrastive Reconstruction for Time-Series
  Anomaly Detection
Authors: Lixu Wang, Shichao Xu, Xinyu Du, Qi Zhu
Categories: cs.LG
Comments: 5 pages, 3 figures, accepted at ICASSP 2024
\\
  Anomaly detection in time-series data is crucial for identifying faults,
failures, threats, and outliers across a range of applications. Recently, deep
learning techniques have been applied to this topic, but they often struggle in
real-world scenarios that are complex and highly dynamic, e.g., the normal data
may consist of multiple distributions, and various types of anomalies may
differ from the normal data to different degrees. In this work, to tackle these
challenges, we propose Distribution-Augmented Contrastive Reconstruction
(DACR). DACR generates extra data disjoint from the normal data distribution to
compress the normal data's representation space, and enhances the feature
extractor through contrastive learning to better capture the intrinsic
semantics from time-series data. Furthermore, DACR employs an attention
mechanism to model the semantic dependencies among multivariate time-series
features, thereby achieving more robust reconstruction for anomaly detection.
Extensive experiments conducted on nine benchmark datasets in various anomaly
detection scenarios demonstrate the effectiveness of DACR in achieving new
state-of-the-art time-series anomaly detection.
\\ ( https://arxiv.org/abs/2401.11271 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11288
Date: Sat, 20 Jan 2024 17:44:50 GMT   (3190kb,D)

Title: Long-Term Fair Decision Making through Deep Generative Models
Authors: Yaowei Hu, Yongkai Wu, Lu Zhang
Categories: cs.LG cs.CY
\\
  This paper studies long-term fair machine learning which aims to mitigate
group disparity over the long term in sequential decision-making systems. To
define long-term fairness, we leverage the temporal causal graph and use the
1-Wasserstein distance between the interventional distributions of different
demographic groups at a sufficiently large time step as the quantitative
metric. Then, we propose a three-phase learning framework where the decision
model is trained on high-fidelity data generated by a deep generative model. We
formulate the optimization problem as a performative risk minimization and
adopt the repeated gradient descent algorithm for learning. The empirical
evaluation shows the efficacy of the proposed method using both synthetic and
semi-synthetic datasets.
\\ ( https://arxiv.org/abs/2401.11288 ,  3190kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11325
Date: Sat, 20 Jan 2024 21:09:27 GMT   (1470kb,D)

Title: Detecting Hidden Triggers: Mapping Non-Markov Reward Functions to Markov
Authors: Gregory Hyde, Eugene Santos Jr
Categories: cs.LG cs.AI
\\
  Many Reinforcement Learning algorithms assume a Markov reward function to
guarantee optimality. However, not all reward functions are known to be Markov.
In this paper, we propose a framework for mapping non-Markov reward functions
into equivalent Markov ones by learning a Reward Machine - a specialized reward
automaton. Unlike the general practice of learning Reward Machines, we do not
require a set of high-level propositional symbols from which to learn. Rather,
we learn \emph{hidden triggers} directly from data that encode them. We
demonstrate the importance of learning Reward Machines versus their
Deterministic Finite-State Automata counterparts, for this task, given their
ability to model reward dependencies in a single automaton. We formalize this
distinction in our learning objective. Our mapping process is constructed as an
Integer Linear Programming problem. We prove that our mappings provide
consistent expectations for the underlying process. We empirically validate our
approach by learning black-box non-Markov Reward functions in the Officeworld
Domain. Additionally, we demonstrate the effectiveness of learning dependencies
between rewards in a new domain, Breakfastworld.
\\ ( https://arxiv.org/abs/2401.11325 ,  1470kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11353
Date: Sun, 21 Jan 2024 00:42:06 GMT   (1475kb,D)

Title: Distributionally Robust Policy Evaluation under General Covariate Shift
  in Contextual Bandits
Authors: Yihong Guo, Hao Liu, Yisong Yue, Anqi Liu
Categories: cs.LG
\\
  We introduce a distributionally robust approach that enhances the reliability
of offline policy evaluation in contextual bandits under general covariate
shifts. Our method aims to deliver robust policy evaluation results in the
presence of discrepancies in both context and policy distribution between
logging and target data. Central to our methodology is the application of
robust regression, a distributionally robust technique tailored here to improve
the estimation of conditional reward distribution from logging data. Utilizing
the reward model obtained from robust regression, we develop a comprehensive
suite of policy value estimators, by integrating our reward model into
established evaluation frameworks, namely direct methods and doubly robust
methods. Through theoretical analysis, we further establish that the proposed
policy value estimators offer a finite sample upper bound for the bias,
providing a clear advantage over traditional methods, especially when the shift
is large. Finally, we designed an extensive range of policy evaluation
scenarios, covering diverse magnitudes of shifts and a spectrum of logging and
target policies. Our empirical results indicate that our approach significantly
outperforms baseline methods, most notably in 90% of the cases under the policy
shift-only settings and 72% of the scenarios under the general covariate shift
settings.
\\ ( https://arxiv.org/abs/2401.11353 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11360
Date: Sun, 21 Jan 2024 01:16:53 GMT   (958kb)

Title: PepHarmony: A Multi-View Contrastive Learning Framework for Integrated
  Sequence and Structure-Based Peptide Encoding
Authors: Ruochi Zhang, Haoran Wu, Chang Liu, Huaping Li, Yuqian Wu, Kewei Li,
  Yifan Wang, Yifan Deng, Jiahui Chen, Fengfeng Zhou, Xin Gao
Categories: cs.LG cs.AI cs.CE q-bio.BM
Comments: 25 pages, 5 figures, 3 tables
\\
  Recent advances in protein language models have catalyzed significant
progress in peptide sequence representation. Despite extensive exploration in
this field, pre-trained models tailored for peptide-specific needs remain
largely unaddressed due to the difficulty in capturing the complex and
sometimes unstable structures of peptides. This study introduces a novel
multi-view contrastive learning framework PepHarmony for the sequence-based
peptide encoding task. PepHarmony innovatively combines both sequence- and
structure-level information into a sequence-level encoding module through
contrastive learning. We carefully select datasets from the Protein Data Bank
(PDB) and AlphaFold database to encompass a broad spectrum of peptide sequences
and structures. The experimental data highlights PepHarmony's exceptional
capability in capturing the intricate relationship between peptide sequences
and structures compared with the baseline and fine-tuned models. The robustness
of our model is confirmed through extensive ablation studies, which emphasize
the crucial roles of contrastive loss and strategic data sorting in enhancing
predictive performance. The proposed PepHarmony framework serves as a notable
contribution to peptide representations, and offers valuable insights for
future applications in peptide drug discovery and peptide engineering. We have
made all the source code utilized in this study publicly accessible via GitHub
at https://github.com/zhangruochi/PepHarmony or
http://www.healthinformaticslab.org/supp/.
\\ ( https://arxiv.org/abs/2401.11360 ,  958kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11380
Date: Sun, 21 Jan 2024 03:11:50 GMT   (150kb,D)

Title: MoMA: Model-based Mirror Ascent for Offline Reinforcement Learning
Authors: Mao Hong, Zhiyue Zhang, Yue Wu, Yanxun Xu
Categories: cs.LG math.ST stat.ME stat.ML stat.TH
\\
  Model-based offline reinforcement learning methods (RL) have achieved
state-of-the-art performance in many decision-making problems thanks to their
sample efficiency and generalizability. Despite these advancements, existing
model-based offline RL approaches either focus on theoretical studies without
developing practical algorithms or rely on a restricted parametric policy
space, thus not fully leveraging the advantages of an unrestricted policy space
inherent to model-based methods. To address this limitation, we develop MoMA, a
model-based mirror ascent algorithm with general function approximations under
partial coverage of offline data. MoMA distinguishes itself from existing
literature by employing an unrestricted policy class. In each iteration, MoMA
conservatively estimates the value function by a minimization procedure within
a confidence set of transition models in the policy evaluation step, then
updates the policy with general function approximations instead of
commonly-used parametric policy classes in the policy improvement step. Under
some mild assumptions, we establish theoretical guarantees of MoMA by proving
an upper bound on the suboptimality of the returned policy. We also provide a
practically implementable, approximate version of the algorithm. The
effectiveness of MoMA is demonstrated via numerical studies.
\\ ( https://arxiv.org/abs/2401.11380 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11394
Date: Sun, 21 Jan 2024 04:07:48 GMT   (733kb,D)

Title: Causal Generative Explainers using Counterfactual Inference: A Case
  Study on the Morpho-MNIST Dataset
Authors: Will Taylor-Melanson and Zahra Sadeghi and Stan Matwin
Categories: cs.LG
\\
  In this paper, we propose leveraging causal generative learning as an
interpretable tool for explaining image classifiers. Specifically, we present a
generative counterfactual inference approach to study the influence of visual
features (i.e., pixels) as well as causal factors through generative learning.
To this end, we first uncover the most influential pixels on a classifier's
decision by varying the value of a causal attribute via counterfactual
inference and computing both Shapely and contrastive explanations for
counterfactual images with these different attribute values. We then establish
a Monte-Carlo mechanism using the generator of a causal generative model in
order to adapt Shapley explainers to produce feature importances for the
human-interpretable attributes of a causal dataset in the case where a
classifier has been trained exclusively on the images of the dataset. Finally,
we present optimization methods for creating counterfactual explanations of
classifiers by means of counterfactual inference, proposing straightforward
approaches for both differentiable and arbitrary classifiers. We exploit the
Morpho-MNIST causal dataset as a case study for exploring our proposed methods
for generating counterfacutl explantions. We employ visual explanation methods
from OmnixAI open source toolkit to compare them with our proposed methods. By
employing quantitative metrics to measure the interpretability of
counterfactual explanations, we find that our proposed methods of
counterfactual explanation offer more interpretable explanations compared to
those generated from OmnixAI. This finding suggests that our methods are
well-suited for generating highly interpretable counterfactual explanations on
causal datasets.
\\ ( https://arxiv.org/abs/2401.11394 ,  733kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11396
Date: Sun, 21 Jan 2024 04:18:30 GMT   (1287kb,D)

Title: Visual Imitation Learning with Calibrated Contrastive Representation
Authors: Yunke Wang, Linwei Tao, Bo Du, Yutian Lin, Chang Xu
Categories: cs.LG cs.CV
\\
  Adversarial Imitation Learning (AIL) allows the agent to reproduce expert
behavior with low-dimensional states and actions. However, challenges arise in
handling visual states due to their less distinguishable representation
compared to low-dimensional proprioceptive features. While existing methods
resort to adopt complex network architectures or separate the process of
learning representation and decision-making, they overlook valuable intra-agent
information within demonstrations. To address this problem, this paper proposes
a simple and effective solution by incorporating calibrated contrastive
representative learning into visual AIL framework. Specifically, we present an
image encoder in visual AIL, utilizing a combination of unsupervised and
supervised contrastive learning to extract valuable features from visual
states. Based on the fact that the improved agent often produces demonstrations
of varying quality, we propose to calibrate the contrastive loss by treating
each agent demonstrations as a mixed sample. The incorporation of contrastive
learning can be jointly optimized with the AIL framework, without modifying the
architecture or incurring significant computational costs. Experimental results
on DMControl Suite demonstrate our proposed method is sample efficient and can
outperform other compared methods from different aspects.
\\ ( https://arxiv.org/abs/2401.11396 ,  1287kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11402
Date: Sun, 21 Jan 2024 04:51:15 GMT   (273kb,D)

Title: Enabling clustering algorithms to detect clusters of varying densities
  through scale-invariant data preprocessing
Authors: Sunil Aryal, Jonathan R. Wells, Arbind Agrahari Baniya, KC Santosh
Categories: cs.LG
\\
  In this paper, we show that preprocessing data using a variant of rank
transformation called 'Average Rank over an Ensemble of Sub-samples (ARES)'
makes clustering algorithms robust to data representation and enable them to
detect varying density clusters. Our empirical results, obtained using three
most widely used clustering algorithms-namely KMeans, DBSCAN, and DP (Density
Peak)-across a wide range of real-world datasets, show that clustering after
ARES transformation produces better and more consistent results.
\\ ( https://arxiv.org/abs/2401.11402 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11403
Date: Sun, 21 Jan 2024 04:54:45 GMT   (1590kb,D)

Title: MolTailor: Tailoring Chemical Molecular Representation to Specific Tasks
  via Text Prompts
Authors: Haoqiang Guo, Sendong Zhao, Haochun Wang, Yanrui Du, Bing Qin
Categories: cs.LG cs.CL q-bio.BM
Comments: Accepted by AAAI 2024
\\
  Deep learning is now widely used in drug discovery, providing significant
acceleration and cost reduction. As the most fundamental building block,
molecular representation is essential for predicting molecular properties to
enable various downstream applications. Most existing methods attempt to
incorporate more information to learn better representations. However, not all
features are equally important for a specific task. Ignoring this would
potentially compromise the training efficiency and predictive accuracy. To
address this issue, we propose a novel approach, which treats language models
as an agent and molecular pretraining models as a knowledge base. The agent
accentuates task-relevant features in the molecular representation by
understanding the natural language description of the task, just as a tailor
customizes clothes for clients. Thus, we call this approach MolTailor.
Evaluations demonstrate MolTailor's superior performance over baselines,
validating the efficacy of enhancing relevance for molecular representation
learning. This illustrates the potential of language model guided optimization
to better exploit and unleash the capabilities of existing powerful molecular
representation methods. Our codes and appendix are available at
https://github.com/SCIR-HI/MolTailor.
\\ ( https://arxiv.org/abs/2401.11403 ,  1590kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11410
Date: Sun, 21 Jan 2024 06:33:45 GMT   (7480kb,D)

Title: Agricultural Recommendation System based on Deep Learning: A
  Multivariate Weather Forecasting Approach
Authors: Md Zubair (1), Md. Shahidul Salim (2), Mehrab Mustafy Rahman (3),
  Mohammad Jahid Ibna Basher (1), Shahin Imran (4) and Iqbal H. Sarker (5) ((1)
  Chittagong University of Engineering & Technology, Chittagong, Bangladesh,
  (2) Khulna University of Engineering & Technology, Khulna, Bangladesh, (3)
  Islamic University of Technology, Gazipur, Bangladesh, (4) Khulna
  Agricultural University, Khulna, Bangladesh, (5) Edith Cowan University,
  Perth, Australia.)
Categories: cs.LG cs.AI
Comments: 16 pages, 14 figures and 12 tables. Submitted to Engineering
  Application of Artificial Intelligence (Elsevier)
\\
  Bangladesh is predominantly an agricultural country, where the agrarian
sector plays an essential role in accelerating economic growth and enabling the
food security of the people. The performance of this sector has an overwhelming
impact on the primary macroeconomic objectives like food security, employment
generation, poverty alleviation, human resources development, and other
economic and social forces. Although Bangladesh's labor-intensive agriculture
has achieved steady increases in food grain production, it often suffered from
unfavorable weather conditions such as heavy rainfall, low temperature, and
drought. Consequently, these factors hinder the production of food
substantially, putting the country's overall food security in danger. In order
to have a profitable, sustainable, and farmer-friendly agricultural practice,
this paper proposes a context-based crop recommendation system powered by a
weather forecast model. With extensive evaluation, the multivariate Stacked
Bi-LSTM Network is employed as the weather forecasting model. The proposed
weather model can forecast Rainfall, Temperature, Humidity, and Sunshine for
any given location in Bangladesh with higher accuracy. These predictions guide
our system to assist the farmers in making feasible decisions about planting,
irrigation, harvesting, and so on. Additionally, our full-fledged system is
capable of alerting the farmers about extreme weather conditions so that
preventive measures can be undertaken to protect the crops. Finally, the system
is also adept at making knowledge-based crop suggestions for the flood and
drought-prone regions of Bangladesh.
\\ ( https://arxiv.org/abs/2401.11410 ,  7480kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11418
Date: Sun, 21 Jan 2024 07:43:01 GMT   (7163kb,D)

Title: Double-Bounded Optimal Transport for Advanced Clustering and
  Classification
Authors: Liangliang Shi, Zhaoqi Shen, Junchi Yan
Categories: cs.LG cs.AI math.OC
\\
  Optimal transport (OT) is attracting increasing attention in machine
learning. It aims to transport a source distribution to a target one at minimal
cost. In its vanilla form, the source and target distributions are
predetermined, which contracts to the real-world case involving undetermined
targets. In this paper, we propose Doubly Bounded Optimal Transport (DB-OT),
which assumes that the target distribution is restricted within two boundaries
instead of a fixed one, thus giving more freedom for the transport to find
solutions. Based on the entropic regularization of DB-OT, three scaling-based
algorithms are devised for calculating the optimal solution. We also show that
our DB-OT is helpful for barycenter-based clustering, which can avoid the
excessive concentration of samples in a single cluster. Then we further develop
DB-OT techniques for long-tailed classification which is an emerging and open
problem. We first propose a connection between OT and classification, that is,
in the classification task, training involves optimizing the Inverse OT to
learn the representations, while testing involves optimizing the OT for
predictions. With this OT perspective, we first apply DB-OT to improve the
loss, and the Balanced Softmax is shown as a special case. Then we apply DB-OT
for inference in the testing process. Even with vanilla Softmax trained
features, our extensive experimental results show that our method can achieve
good results with our improved inference scheme in the testing stage.
\\ ( https://arxiv.org/abs/2401.11418 ,  7163kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11437
Date: Sun, 21 Jan 2024 09:24:24 GMT   (3671kb,D)

Title: Open the Black Box: Step-based Policy Updates for Temporally-Correlated
  Episodic Reinforcement Learning
Authors: Ge Li, Hongyi Zhou, Dominik Roth, Serge Thilges, Fabian Otto, Rudolf
  Lioutikov, Gerhard Neumann
Categories: cs.LG cs.RO
Comments: Codebase, see: https://github.com/BruceGeLi/TCE_RL
\\
  Current advancements in reinforcement learning (RL) have predominantly
focused on learning step-based policies that generate actions for each
perceived state. While these methods efficiently leverage step information from
environmental interaction, they often ignore the temporal correlation between
actions, resulting in inefficient exploration and unsmooth trajectories that
are challenging to implement on real hardware. Episodic RL (ERL) seeks to
overcome these challenges by exploring in parameters space that capture the
correlation of actions. However, these approaches typically compromise data
efficiency, as they treat trajectories as opaque \emph{black boxes}. In this
work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL
(TCE), which effectively utilizes step information in episodic policy updates,
opening the 'black box' in existing ERL methods while retaining the smooth and
consistent exploration in parameter space. TCE synergistically combines the
advantages of step-based and episodic RL, achieving comparable performance to
recent ERL methods while maintaining data efficiency akin to state-of-the-art
(SoTA) step-based RL.
\\ ( https://arxiv.org/abs/2401.11437 ,  3671kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11447
Date: Sun, 21 Jan 2024 09:55:47 GMT   (806kb,D)

Title: Sequential Model for Predicting Patient Adherence in Subcutaneous
  Immunotherapy for Allergic Rhinitis
Authors: Li Yin, Xiong Yu, Fan Wenxin, Wang Kai, Yu Qingqing, Si Liping, van
  der Smagt Patrick, Tang Jun, and Chen Nutan
Categories: cs.LG q-bio.QM
\\
  Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal
treatment of allergic rhinitis. How to enhance the adherence of patients to
maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in
the management of AIT. This study aims to leverage novel machine learning
models to precisely predict the risk of non-adherence of patients and related
systematic symptom scores, to provide a novel approach in the management of
long-term AIT.
  Methods: The research develops and analyzes two models, Sequential Latent
Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on
scoring and adherence prediction capabilities.
  Results: Excluding the biased samples at the first time step, the predictive
adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM
models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The
range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and
$2.22$, while for LSTM models it is between $1.09$ and $1.77$. Notably, these
RMSEs are significantly lower than the random prediction error of $4.55$.
  Conclusion: We creatively apply sequential models in the long-term management
of SCIT with promising accuracy in the prediction of SCIT nonadherence in
Allergic Rhinitis (AR) patients. While LSTM outperforms SLAC in adherence
prediction, SLAC excels in score prediction for patients undergoing SCIT for
AR. The state-action-based SLAC adds flexibility, presenting a novel and
effective approach for managing long-term AIT.
\\ ( https://arxiv.org/abs/2401.11447 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11462
Date: Sun, 21 Jan 2024 11:02:00 GMT   (698kb)

Title: Frost Prediction Using Machine Learning Methods in Fars Province
Authors: Milad Barooni, Koorush Ziarati, Ali Barooni
Categories: cs.LG
Comments: Accpeted by 28th International Computer Conference, Computer Society
  of Iran (CSICC)
DOI: 10.1109/CSICC58665.2023.10105391
\\
  One of the common hazards and issues in meteorology and agriculture is the
problem of frost, chilling or freezing. This event occurs when the minimum
ambient temperature falls below a certain value. This phenomenon causes a lot
of damage to the country, especially Fars province. Solving this problem
requires that, in addition to predicting the minimum temperature, we can
provide enough time to implement the necessary measures. Empirical methods have
been provided by the Food and Agriculture Organization (FAO), which can predict
the minimum temperature, but not in time. In addition to this, we can use
machine learning methods to model the minimum temperature. In this study, we
have used three methods Gated Recurrent Unit (GRU), Temporal Convolutional
Network (TCN) as deep learning methods, and Gradient Boosting (XGBoost). A
customized loss function designed for methods based on deep learning, which can
be effective in reducing prediction errors. With methods based on deep learning
models, not only do we observe a reduction in RMSE error compared to empirical
methods but also have more time to predict minimum temperature. Thus, we can
model the minimum temperature for the next 24 hours by having the current 24
hours. With the gradient boosting model (XGBoost) we can keep the prediction
time as deep learning and RMSE error reduced. Finally, we experimentally
concluded that machine learning methods work better than empirical methods and
XGBoost model can have better performance in this problem among other
implemented.
\\ ( https://arxiv.org/abs/2401.11462 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11512
Date: Sun, 21 Jan 2024 14:51:09 GMT   (4343kb,D)

Title: Information-Theoretic State Variable Selection for Reinforcement
  Learning
Authors: Charles Westphal, Stephen Hailes, Mirco Musolesi
Categories: cs.LG cs.AI cs.IT math.IT
Comments: 47 pages, 12 figures
\\
  Identifying the most suitable variables to represent the state is a
fundamental challenge in Reinforcement Learning (RL). These variables must
efficiently capture the information necessary for making optimal decisions. In
order to address this problem, in this paper, we introduce the Transfer Entropy
Redundancy Criterion (TERC), an information-theoretic criterion, which
determines if there is \textit{entropy transferred} from state variables to
actions during training. We define an algorithm based on TERC that provably
excludes variables from the state that have no effect on the final performance
of the agent, resulting in more sample efficient learning. Experimental results
show that this speed-up is present across three different algorithm classes
(represented by tabular Q-learning, Actor-Critic, and Proximal Policy
Optimization (PPO)) in a variety of environments. Furthermore, to highlight the
differences between the proposed methodology and the current state-of-the-art
feature selection approaches, we present a series of controlled experiments on
synthetic data, before generalizing to real-world decision-making tasks. We
also introduce a representation of the problem that compactly captures the
transfer of information from state variables to actions as Bayesian networks.
\\ ( https://arxiv.org/abs/2401.11512 ,  4343kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11543
Date: Sun, 21 Jan 2024 16:55:40 GMT   (2812kb,D)

Title: How Robust Are Energy-Based Models Trained With Equilibrium Propagation?
Authors: Siddharth Mansingh, Michal Kucer, Garrett Kenyon, Juston Moore and
  Michael Teti
Categories: cs.LG cs.CV
\\
  Deep neural networks (DNNs) are easily fooled by adversarial perturbations
that are imperceptible to humans. Adversarial training, a process where
adversarial examples are added to the training set, is the current
state-of-the-art defense against adversarial attacks, but it lowers the model's
accuracy on clean inputs, is computationally expensive, and offers less
robustness to natural noise. In contrast, energy-based models (EBMs), which
were designed for efficient implementation in neuromorphic hardware and
physical systems, incorporate feedback connections from each layer to the
previous layer, yielding a recurrent, deep-attractor architecture which we
hypothesize should make them naturally robust. Our work is the first to explore
the robustness of EBMs to both natural corruptions and adversarial attacks,
which we do using the CIFAR-10 and CIFAR-100 datasets. We demonstrate that EBMs
are more robust than transformers and display comparable robustness to
adversarially-trained DNNs on gradient-based (white-box) attacks, query-based
(black-box) attacks, and natural perturbations without sacrificing clean
accuracy, and without the need for adversarial training or additional training
techniques.
\\ ( https://arxiv.org/abs/2401.11543 ,  2812kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11563
Date: Sun, 21 Jan 2024 18:43:55 GMT   (1053kb,D)

Title: Distributed Multi-Task Learning for Stochastic Bandits with Context
  Distribution and Stage-wise Constraints
Authors: Jiabin Lin and Shana Moothedath
Categories: cs.LG cs.MA
\\
  We present the problem of conservative distributed multi-task learning in
stochastic linear contextual bandits with heterogeneous agents. This extends
conservative linear bandits to a distributed setting where M agents tackle
different but related tasks while adhering to stage-wise performance
constraints. The exact context is unknown, and only a context distribution is
available to the agents as in many practical applications that involve a
prediction mechanism to infer context, such as stock market prediction and
weather forecast. We propose a distributed upper confidence bound (UCB)
algorithm, DiSC-UCB. Our algorithm constructs a pruned action set during each
round to ensure the constraints are met. Additionally, it includes synchronized
sharing of estimates among agents via a central server using well-structured
synchronization steps. We prove the regret and communication bounds on the
algorithm. We extend the problem to a setting where the agents are unaware of
the baseline reward. For this setting, we provide a modified algorithm,
DiSC-UCB2, and we show that the modified algorithm achieves the same regret and
communication bounds. We empirically validated the performance of our algorithm
on synthetic data and real-world Movielens-100K data.
\\ ( https://arxiv.org/abs/2401.11563 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11565
Date: Sun, 21 Jan 2024 18:57:38 GMT   (1831kb,D)

Title: Thompson Sampling for Stochastic Bandits with Noisy Contexts: An
  Information-Theoretic Regret Analysis
Authors: Sharu Theresa Jose and Shana Moothedath
Categories: cs.LG stat.ML
\\
  We explore a stochastic contextual linear bandit problem where the agent
observes a noisy, corrupted version of the true context through a noise channel
with an unknown noise parameter. Our objective is to design an action policy
that can approximate" that of an oracle, which has access to the reward model,
the channel parameter, and the predictive distribution of the true context from
the observed noisy context. In a Bayesian framework, we introduce a Thompson
sampling algorithm for Gaussian bandits with Gaussian context noise. Adopting
an information-theoretic analysis, we demonstrate the Bayesian regret of our
algorithm concerning the oracle's action policy. We also extend this problem to
a scenario where the agent observes the true context with some delay after
receiving the reward and show that delayed true contexts lead to lower Bayesian
regret. Finally, we empirically demonstrate the performance of the proposed
algorithms against baselines.
\\ ( https://arxiv.org/abs/2401.11565 ,  1831kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11592
Date: Sun, 21 Jan 2024 20:46:21 GMT   (7927kb,D)

Title: Differential Privacy in Hierarchical Federated Learning: A Formal
  Analysis and Evaluation
Authors: Frank Po-Chen Lin and Christopher Brinton
Categories: cs.LG cs.CR cs.DC
\\
  While federated learning (FL) eliminates the transmission of raw data over a
network, it is still vulnerable to privacy breaches from the communicated model
parameters. In this work, we formalize Differentially Private Hierarchical
Federated Learning (DP-HFL), a DP-enhanced FL methodology that seeks to improve
the privacy-utility tradeoff inherent in FL. Building upon recent proposals for
Hierarchical Differential Privacy (HDP), one of the key concepts of DP-HFL is
adapting DP noise injection at different layers of an established FL hierarchy
-- edge devices, edge servers, and cloud servers -- according to the trust
models within particular subnetworks. We conduct a comprehensive analysis of
the convergence behavior of DP-HFL, revealing conditions on parameter tuning
under which the model training process converges sublinearly to a stationarity
gap, with this gap depending on the network hierarchy, trust model, and target
privacy level. Subsequent numerical evaluations demonstrate that DP-HFL obtains
substantial improvements in convergence speed over baselines for different
privacy budgets, and validate the impact of network configuration on training.
\\ ( https://arxiv.org/abs/2401.11592 ,  7927kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11600
Date: Sun, 21 Jan 2024 21:11:09 GMT   (678kb,D)

Title: Understanding the Generalization Benefits of Late Learning Rate Decay
Authors: Yinuo Ren, Chao Ma, Lexing Ying
Categories: cs.LG stat.ML
Comments: Accepted by AISTATS 2024
\\
  Why do neural networks trained with large learning rates for a longer time
often lead to better generalization? In this paper, we delve into this question
by examining the relation between training and testing loss in neural networks.
Through visualization of these losses, we note that the training trajectory
with a large learning rate navigates through the minima manifold of the
training loss, finally nearing the neighborhood of the testing loss minimum.
Motivated by these findings, we introduce a nonlinear model whose loss
landscapes mirror those observed for real neural networks. Upon investigating
the training process using SGD on our model, we demonstrate that an extended
phase with a large learning rate steers our model towards the minimum norm
solution of the training loss, which may achieve near-optimal generalization,
thereby affirming the empirically observed benefits of late learning rate
decay.
\\ ( https://arxiv.org/abs/2401.11600 ,  678kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11609
Date: Sun, 21 Jan 2024 22:11:29 GMT   (8643kb,D)

Title: Graph Edits for Counterfactual Explanations: A Unified GNN Approach
Authors: Nikolaos Chaidos, Angeliki Dimitriou, Maria Lymperaiou, Giorgos Stamou
Categories: cs.LG cs.AI
\\
  Counterfactuals have been established as a popular explainability technique
which leverages a set of minimal edits to alter the prediction of a classifier.
When considering conceptual counterfactuals, the edits requested should
correspond to salient concepts present in the input data. At the same time,
conceptual distances are defined by knowledge graphs, ensuring the optimality
of conceptual edits. In this work, we extend previous endeavors on conceptual
counterfactuals by introducing \textit{graph edits as counterfactual
explanations}: should we represent input data as graphs, which is the shortest
graph edit path that results in an alternative classification label as provided
by a black-box classifier?
\\ ( https://arxiv.org/abs/2401.11609 ,  8643kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11611
Date: Sun, 21 Jan 2024 22:18:29 GMT   (15792kb,D)

Title: Continuous Field Reconstruction from Sparse Observations with Implicit
  Neural Networks
Authors: Xihaier Luo, Wei Xu, Yihui Ren, Shinjae Yoo, Balu Nadiga
Categories: cs.LG
Comments: 25 pages,21 figures
\\
  Reliably reconstructing physical fields from sparse sensor data is a
challenge that frequently arises in many scientific domains. In practice, the
process generating the data often is not understood to sufficient accuracy.
Therefore, there is a growing interest in using the deep neural network route
to address the problem. This work presents a novel approach that learns a
continuous representation of the physical field using implicit neural
representations (INRs). Specifically, after factorizing spatiotemporal
variability into spatial and temporal components using the separation of
variables technique, the method learns relevant basis functions from sparsely
sampled irregular data points to develop a continuous representation of the
data. In experimental evaluations, the proposed model outperforms recent INR
methods, offering superior reconstruction quality on simulation data from a
state-of-the-art climate model and a second dataset that comprises ultra-high
resolution satellite-based sea surface temperature fields.
\\ ( https://arxiv.org/abs/2401.11611 ,  15792kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11618
Date: Sun, 21 Jan 2024 22:55:26 GMT   (4296kb,D)

Title: Efficient local linearity regularization to overcome catastrophic
  overfitting
Authors: Elias Abad Rocamora, Fanghui Liu, Grigorios G. Chrysos, Pablo M.
  Olmos, Volkan Cevher
Categories: cs.LG cs.AI cs.CR stat.ML
Comments: Accepted in ICLR 2024
\\
  Catastrophic overfitting (CO) in single-step adversarial training (AT)
results in abrupt drops in the adversarial test accuracy (even down to 0%). For
models trained with multi-step AT, it has been observed that the loss function
behaves locally linearly with respect to the input, this is however lost in
single-step AT. To address CO in single-step AT, several methods have been
proposed to enforce local linearity of the loss via regularization. However,
these regularization terms considerably slow down training due to Double
Backpropagation. Instead, in this work, we introduce a regularization term,
called ELLE, to mitigate CO effectively and efficiently in classical AT
evaluations, as well as some more difficult regimes, e.g., large adversarial
perturbations and long training schedules. Our regularization term can be
theoretically linked to curvature of the loss function and is computationally
cheaper than previous methods by avoiding Double Backpropagation. Our thorough
experimental validation demonstrates that our work does not suffer from CO,
even in challenging settings where previous works suffer from it. We also
notice that adapting our regularization parameter during training (ELLE-A)
greatly improves the performance, specially in large $\epsilon$ setups. Our
implementation is available in https://github.com/LIONS-EPFL/ELLE .
\\ ( https://arxiv.org/abs/2401.11618 ,  4296kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11626
Date: Sun, 21 Jan 2024 23:37:33 GMT   (507kb,D)

Title: Freely Long-Thinking Transformer (FraiLT)
Authors: Akbay Tabak
Categories: cs.LG cs.CL
\\
  Freely Long-Thinking Transformer (FraiLT) is an improved transformer model
designed to enhance processing capabilities without scaling up size. It
utilizes a recursive approach, iterating over a subset of layers multiple
times, and introduces iteration encodings to maintain awareness across these
cycles. Iteration encoding allows FraiLT to achieve the interpretive depth of
larger models in a compact form. When evaluated on a synthetic story dataset,
FraiLT outperformed larger models, showcasing its ability to deliver
high-quality performance while reducing memory demands. This model represents a
step forward towards more efficient and accessible language models.
\\ ( https://arxiv.org/abs/2401.11626 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11627
Date: Sun, 21 Jan 2024 23:41:32 GMT   (215kb,D)

Title: Tight Verification of Probabilistic Robustness in Bayesian Neural
  Networks
Authors: Ben Batten, Mehran Hosseini, Alessio Lomuscio
Categories: cs.LG cs.AI cs.FL cs.LO
Comments: Accepted at AISTATS 2024
MSC-class: 68T27 (Primary) 68T45, 68T07, 68T01 (Secondary)
ACM-class: I.2.0; I.2.4; F.3.1; D.2.4
\\
  We introduce two algorithms for computing tight guarantees on the
probabilistic robustness of Bayesian Neural Networks (BNNs). Computing
robustness guarantees for BNNs is a significantly more challenging task than
verifying the robustness of standard Neural Networks (NNs) because it requires
searching the parameters' space for safe weights. Moreover, tight and complete
approaches for the verification of standard NNs, such as those based on
Mixed-Integer Linear Programming (MILP), cannot be directly used for the
verification of BNNs because of the polynomial terms resulting from the
consecutive multiplication of variables encoding the weights. Our algorithms
efficiently and effectively search the parameters' space for safe weights by
using iterative expansion and the network's gradient and can be used with any
verification algorithm of choice for BNNs. In addition to proving that our
algorithms compute tighter bounds than the SoA, we also evaluate our algorithms
against the SoA on standard benchmarks, such as MNIST and CIFAR10, showing that
our algorithms compute bounds up to 40% tighter than the SoA.
\\ ( https://arxiv.org/abs/2401.11627 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11630
Date: Sun, 21 Jan 2024 23:50:46 GMT   (6378kb,D)

Title: Reframing Offline Reinforcement Learning as a Regression Problem
Authors: Prajwal Koirala and Cody Fleming
Categories: cs.LG cs.SY eess.SY
\\
  The study proposes the reformulation of offline reinforcement learning as a
regression problem that can be solved with decision trees. Aiming to predict
actions based on input states, return-to-go (RTG), and timestep information, we
observe that with gradient-boosted trees, the agent training and inference are
very fast, the former taking less than a minute. Despite the simplification
inherent in this reformulated problem, our agent demonstrates performance that
is at least on par with established methods. This assertion is validated by
testing it across standard datasets associated with D4RL Gym-MuJoCo tasks. We
further discuss the agent's ability to generalize by testing it on two extreme
cases, how it learns to model the return distributions effectively even with
highly skewed expert datasets, and how it exhibits robust performance in
scenarios with sparse/delayed rewards.
\\ ( https://arxiv.org/abs/2401.11630 ,  6378kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11647
Date: Mon, 22 Jan 2024 01:57:31 GMT   (554kb,D)

Title: LW-FedSSL: Resource-efficient Layer-wise Federated Self-supervised
  Learning
Authors: Ye Lin Tun, Chu Myaet Thwal, Le Quang Huy, Minh N. H. Nguyen, Choong
  Seon Hong
Categories: cs.LG cs.AI
\\
  Many recent studies integrate federated learning (FL) with self-supervised
learning (SSL) to take advantage of raw training data distributed across edge
devices. However, edge devices often struggle with high computation and
communication costs imposed by SSL and FL algorithms. To tackle this hindrance,
we propose LW-FedSSL, a layer-wise federated self-supervised learning approach
that allows edge devices to incrementally train one layer of the model at a
time. LW-FedSSL comprises server-side calibration and representation alignment
mechanisms to maintain comparable performance with end-to-end FedSSL while
significantly lowering clients' resource requirements. The server-side
calibration mechanism takes advantage of the resource-rich server in an FL
environment to assist in global model training. Meanwhile, the representation
alignment mechanism encourages closeness between representations of FL local
models and those of the global model. Our experiments show that LW-FedSSL has a
$3.3 \times$ lower memory requirement and a $3.2 \times$ cheaper communication
cost than its end-to-end counterpart. We also explore a progressive training
strategy called Prog-FedSSL that outperforms end-to-end training with a similar
memory requirement and a $1.8 \times$ cheaper communication cost.
\\ ( https://arxiv.org/abs/2401.11647 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11648
Date: Mon, 22 Jan 2024 01:58:32 GMT   (7069kb,D)

Title: Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal
  Contrastive EHR Modelling with Hierarchical Regularisation
Authors: Heejoon Koo
Categories: cs.LG cs.AI cs.IR
Comments: Accepted to EACL 2024 (The 18th Conference of the European Chapter of
  the Association for Computational Linguistics)
\\
  Predicting next visit diagnosis using Electronic Health Records (EHR) is an
essential task in healthcare, critical for devising proactive future plans for
both healthcare providers and patients. Nonetheless, many preceding studies
have not sufficiently addressed the heterogeneous and hierarchical
characteristics inherent in EHR data, inevitably leading to sub-optimal
performance. To this end, we propose NECHO, a novel medical code-centric
multimodal contrastive EHR learning framework with hierarchical regularisation.
First, we integrate multifaceted information encompassing medical codes,
demographics, and clinical notes using a tailored network design and a pair of
bimodal contrastive losses, all of which pivot around a medical code
representation. We also regularise modality-specific encoders using a parental
level information in medical ontology to learn hierarchical structure of EHR
data. A series of experiments on MIMIC-III data demonstrates effectiveness of
our approach.
\\ ( https://arxiv.org/abs/2401.11648 ,  7069kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11660
Date: Mon, 22 Jan 2024 02:33:38 GMT   (2280kb,D)

Title: Differentiable Tree Search in Latent State Space
Authors: Dixant Mittal and Wee Sun Lee
Categories: cs.LG cs.AI
\\
  In decision-making problems with limited training data, policy functions
approximated using deep neural networks often exhibit suboptimal performance.
An alternative approach involves learning a world model from the limited data
and determining actions through online search. However, the performance is
adversely affected by compounding errors arising from inaccuracies in the
learnt world model. While methods like TreeQN have attempted to address these
inaccuracies by incorporating algorithmic structural biases into their
architectures, the biases they introduce are often weak and insufficient for
complex decision-making tasks. In this work, we introduce Differentiable Tree
Search (DTS), a novel neural network architecture that significantly
strengthens the inductive bias by embedding the algorithmic structure of a
best-first online search algorithm. DTS employs a learnt world model to conduct
a fully differentiable online search in latent state space. The world model is
jointly optimised with the search algorithm, enabling the learning of a robust
world model and mitigating the effect of model inaccuracies. We address
potential Q-function discontinuities arising from naive incorporation of
best-first search by adopting a stochastic tree expansion policy, formulating
search tree expansion as a decision-making task, and introducing an effective
variance reduction technique for the gradient computation. We evaluate DTS in
an offline-RL setting with a limited training data scenario on Procgen games
and grid navigation task, and demonstrate that DTS outperforms popular
model-free and model-based baselines.
\\ ( https://arxiv.org/abs/2401.11660 ,  2280kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11664
Date: Mon, 22 Jan 2024 02:50:38 GMT   (2513kb,D)

Title: Zero-Space Cost Fault Tolerance for Transformer-based Language Models on
  ReRAM
Authors: Bingbing Li, Geng Yuan, Zigeng Wang, Shaoyi Huang, Hongwu Peng, Payman
  Behnam, Wujie Wen, Hang Liu and Caiwen Ding
Categories: cs.LG cs.AI cs.AR
\\
  Resistive Random Access Memory (ReRAM) has emerged as a promising platform
for deep neural networks (DNNs) due to its support for parallel in-situ
matrix-vector multiplication. However, hardware failures, such as
stuck-at-fault defects, can result in significant prediction errors during
model inference. While additional crossbars can be used to address these
failures, they come with storage overhead and are not efficient in terms of
space, energy, and cost. In this paper, we propose a fault protection mechanism
that incurs zero space cost. Our approach includes: 1) differentiable structure
pruning of rows and columns to reduce model redundancy, 2) weight duplication
and voting for robust output, and 3) embedding duplicated most significant bits
(MSBs) into the model weight. We evaluate our method on nine tasks of the GLUE
benchmark with the BERT model, and experimental results prove its
effectiveness.
\\ ( https://arxiv.org/abs/2401.11664 ,  2513kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11666
Date: Mon, 22 Jan 2024 02:58:53 GMT   (1469kb,D)

Title: P2DT: Mitigating Forgetting in task-incremental Learning with
  progressive prompt Decision Transformer
Authors: Zhiyuan Wang, Xiaoyang Qu, Jing Xiao, Bokui Chen, Jianzong Wang
Categories: cs.LG cs.AI
Comments: Accepted by the 49th IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2024)
\\
  Catastrophic forgetting poses a substantial challenge for managing
intelligent agents controlled by a large model, causing performance degradation
when these agents face new tasks. In our work, we propose a novel solution -
the Progressive Prompt Decision Transformer (P2DT). This method enhances a
transformer-based model by dynamically appending decision tokens during new
task training, thus fostering task-specific policies. Our approach mitigates
forgetting in continual and offline reinforcement learning scenarios. Moreover,
P2DT leverages trajectories collected via traditional reinforcement learning
from all tasks and generates new task-specific tokens during training, thereby
retaining knowledge from previous studies. Preliminary results demonstrate that
our model effectively alleviates catastrophic forgetting and scales well with
increasing task environments.
\\ ( https://arxiv.org/abs/2401.11666 ,  1469kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11667
Date: Mon, 22 Jan 2024 02:59:27 GMT   (3461kb,D)

Title: INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free
  Class-incremental Learning
Authors: Zhiyuan Wang, Xiaoyang Qu, Jing Xiao, Bokui Chen, Jianzong Wang
Categories: cs.LG
Comments: Accepted by the 49th IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP 2024)
\\
  This paper introduces INCPrompt, an innovative continual learning solution
that effectively addresses catastrophic forgetting. INCPrompt's key innovation
lies in its use of adaptive key-learner and task-aware prompts that capture
task-relevant information. This unique combination encapsulates general
knowledge across tasks and encodes task-specific knowledge. Our comprehensive
evaluation across multiple continual learning benchmarks demonstrates
INCPrompt's superiority over existing algorithms, showing its effectiveness in
mitigating catastrophic forgetting while maintaining high performance. These
results highlight the significant impact of task-aware incremental prompting on
continual learning performance.
\\ ( https://arxiv.org/abs/2401.11667 ,  3461kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11669
Date: Mon, 22 Jan 2024 03:07:24 GMT   (517kb)

Title: An Improved Grey Wolf Optimization Algorithm for Heart Disease
  Prediction
Authors: Sihan Niu, Yifan Zhou, Zhikai Li, Shuyao Huang, and Yujun Zhou
Categories: cs.LG cs.AI cs.NE
\\
  This paper presents a unique solution to challenges in medical image
processing by incorporating an adaptive curve grey wolf optimization (ACGWO)
algorithm into neural network backpropagation. Neural networks show potential
in medical data but suffer from issues like overfitting and lack of
interpretability due to imbalanced and scarce data. Traditional Gray Wolf
Optimization (GWO) also has its drawbacks, such as a lack of population
diversity and premature convergence. This paper addresses these problems by
introducing an adaptive algorithm, enhancing the standard GWO with a sigmoid
function. This algorithm was extensively compared to four leading algorithms
using six well-known test functions, outperforming them effectively. Moreover,
by utilizing the ACGWO, we increase the robustness and generalization of the
neural network, resulting in more interpretable predictions. Applied to the
publicly accessible Cleveland Heart Disease dataset, our technique surpasses
ten other methods, achieving 86.8% accuracy, indicating its potential for
efficient heart disease prediction in the clinical setting.
\\ ( https://arxiv.org/abs/2401.11669 ,  517kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11694
Date: Mon, 22 Jan 2024 05:26:18 GMT   (5780kb,D)

Title: Parametric Matrix Models
Authors: Patrick Cook, Danny Jammooa, Morten Hjorth-Jensen, Daniel D. Lee, Dean
  Lee
Categories: cs.LG cond-mat.dis-nn nucl-th physics.comp-ph quant-ph
\\
  We present a general class of machine learning algorithms called parametric
matrix models. Parametric matrix models are based on matrix equations, and the
design is motivated by the efficiency of reduced basis methods for
approximating solutions of parametric equations. The dependent variables can be
defined implicitly or explicitly, and the equations may use algebraic,
differential, or integral relations. Parametric matrix models can be trained
with empirical data only, and no high-fidelity model calculations are needed.
While originally designed for scientific computing, parametric matrix models
are universal function approximators that can be applied to general machine
learning problems. After introducing the underlying theory, we apply parametric
matrix models to a series of different challenges that show their performance
for a wide range of problems. For all the challenges tested here, parametric
matrix models produce accurate results within a computational framework that
allows for parameter extrapolation and interpretability.
\\ ( https://arxiv.org/abs/2401.11694 ,  5780kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11698
Date: Mon, 22 Jan 2024 05:44:43 GMT   (293kb,D)

Title: Admission Prediction in Undergraduate Applications: an Interpretable
  Deep Learning Approach
Authors: Amisha Priyadarshini, Barbara Martinez-Neda, Sergio Gago-Masague
Categories: cs.LG cs.AI
Comments: This paper has been accepted for Transdisciplinary AI 2023 conference
DOI: 10.1109/TransAI60598.2023.00040
\\
  This article addresses the challenge of validating the admission committee's
decisions for undergraduate admissions. In recent years, the traditional review
process has struggled to handle the overwhelmingly large amount of applicants'
data. Moreover, this traditional assessment often leads to human bias, which
might result in discrimination among applicants. Although classical machine
learning-based approaches exist that aim to verify the quantitative assessment
made by the application reviewers, these methods lack scalability and suffer
from performance issues when a large volume of data is in place. In this
context, we propose deep learning-based classifiers, namely Feed-Forward and
Input Convex neural networks, which overcome the challenges faced by the
existing methods. Furthermore, we give additional insights into our model by
incorporating an interpretability module, namely LIME. Our training and test
datasets comprise applicants' data with a wide range of variables and
information. Our models achieve higher accuracy compared to the best-performing
traditional machine learning-based approach by a considerable margin of 3.03\%.
Additionally, we show the sensitivity of different features and their relative
impacts on the overall admission decision using the LIME technique.
\\ ( https://arxiv.org/abs/2401.11698 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11720
Date: Mon, 22 Jan 2024 06:47:00 GMT   (435kb,D)

Title: Graph Condensation: A Survey
Authors: Xinyi Gao, Junliang Yu, Wei Jiang, Tong Chen, Wentao Zhang, Hongzhi
  Yin
Categories: cs.LG cs.AI
\\
  The burgeoning volume of graph data poses significant challenges in storage,
transmission, and particularly the training of graph neural networks (GNNs). To
address these challenges, graph condensation (GC) has emerged as an innovative
solution. GC focuses on synthesizing a compact yet highly representative graph,
on which GNNs can achieve performance comparable to trained on the large
original graph. The notable efficacy of GC and its broad prospects have
garnered significant attention and spurred extensive research. This survey
paper provides an up-to-date and systematic overview of GC, organizing existing
research into four categories aligned with critical GC evaluation criteria:
effectiveness, generalization, fairness, and efficiency. To facilitate an
in-depth and comprehensive understanding of GC, we examine various methods
under each category and thoroughly discuss two essential components within GC:
optimization strategies and condensed graph generation. Additionally, we
introduce the applications of GC in a variety of fields, and highlight the
present challenges and novel insights in GC, promoting advancements in future
research.
\\ ( https://arxiv.org/abs/2401.11720 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11726
Date: Mon, 22 Jan 2024 07:07:32 GMT   (974kb,D)

Title: Detecting Out-of-Distribution Samples via Conditional Distribution
  Entropy with Optimal Transport
Authors: Chuanwen Feng, Wenlong Chen, Ao Ke, Yilong Ren, Xike Xie, S.Kevin Zhou
Categories: cs.LG cs.CV
\\
  When deploying a trained machine learning model in the real world, it is
inevitable to receive inputs from out-of-distribution (OOD) sources. For
instance, in continual learning settings, it is common to encounter OOD samples
due to the non-stationarity of a domain. More generally, when we have access to
a set of test inputs, the existing rich line of OOD detection solutions,
especially the recent promise of distance-based methods, falls short in
effectively utilizing the distribution information from training samples and
test inputs. In this paper, we argue that empirical probability distributions
that incorporate geometric information from both training samples and test
inputs can be highly beneficial for OOD detection in the presence of test
inputs available. To address this, we propose to model OOD detection as a
discrete optimal transport problem. Within the framework of optimal transport,
we propose a novel score function known as the \emph{conditional distribution
entropy} to quantify the uncertainty of a test input being an OOD sample. Our
proposal inherits the merits of certain distance-based methods while
eliminating the reliance on distribution assumptions, a-prior knowledge, and
specific training mechanisms. Extensive experiments conducted on benchmark
datasets demonstrate that our method outperforms its competitors in OOD
detection.
\\ ( https://arxiv.org/abs/2401.11726 ,  974kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11736
Date: Mon, 22 Jan 2024 07:24:15 GMT   (6674kb,D)

Title: Attention on Personalized Clinical Decision Support System: Federated
  Learning Approach
Authors: Chu Myaet Thwal, Kyi Thar, Ye Lin Tun, Choong Seon Hong
Categories: cs.LG cs.AI
Comments: Published in IEEE BigComp 2021
DOI: 10.1109/BigComp51126.2021.00035
\\
  Health management has become a primary problem as new kinds of diseases and
complex symptoms are introduced to a rapidly growing modern society. Building a
better and smarter healthcare infrastructure is one of the ultimate goals of a
smart city. To the best of our knowledge, neural network models are already
employed to assist healthcare professionals in achieving this goal. Typically,
training a neural network requires a rich amount of data but heterogeneous and
vulnerable properties of clinical data introduce a challenge for the
traditional centralized network. Moreover, adding new inputs to a medical
database requires re-training an existing model from scratch. To tackle these
challenges, we proposed a deep learning-based clinical decision support system
trained and managed under a federated learning paradigm. We focused on a novel
strategy to guarantee the safety of patient privacy and overcome the risk of
cyberattacks while enabling large-scale clinical data mining. As a result, we
can leverage rich clinical data for training each local neural network without
the need for exchanging the confidential data of patients. Moreover, we
implemented the proposed scheme as a sequence-to-sequence model architecture
integrating the attention mechanism. Thus, our objective is to provide a
personalized clinical decision support system with evolvable characteristics
that can deliver accurate solutions and assist healthcare professionals in
medical diagnosing.
\\ ( https://arxiv.org/abs/2401.11736 ,  6674kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11750
Date: Mon, 22 Jan 2024 08:23:31 GMT   (13340kb,D)

Title: AdaFGL: A New Paradigm for Federated Node Classification with Topology
  Heterogeneity
Authors: Xunkai Li, Zhengyu Wu, Wentao Zhang, Henan Sun, Rong-Hua Li, Guoren
  Wang
Categories: cs.LG cs.AI cs.DB cs.SI
Comments: Accepted by ICDE 2024
\\
  Recently, Federated Graph Learning (FGL) has attracted significant attention
as a distributed framework based on graph neural networks, primarily due to its
capability to break data silos. Existing FGL studies employ community split on
the homophilous global graph by default to simulate federated semi-supervised
node classification settings. Such a strategy assumes the consistency of
topology between the multi-client subgraphs and the global graph, where
connected nodes are highly likely to possess similar feature distributions and
the same label. However, in real-world implementations, the varying
perspectives of local data engineering result in various subgraph topologies,
posing unique heterogeneity challenges in FGL. Unlike the well-known label
Non-independent identical distribution (Non-iid) problems in federated
learning, FGL heterogeneity essentially reveals the topological divergence
among multiple clients, namely homophily or heterophily. To simulate and handle
this unique challenge, we introduce the concept of structure Non-iid split and
then present a new paradigm called \underline{Ada}ptive \underline{F}ederated
\underline{G}raph \underline{L}earning (AdaFGL), a decoupled two-step
personalized approach. To begin with, AdaFGL employs standard multi-client
federated collaborative training to acquire the federated knowledge extractor
by aggregating uploaded models in the final round at the server. Then, each
client conducts personalized training based on the local subgraph and the
federated knowledge extractor. Extensive experiments on the 12 graph benchmark
datasets validate the superior performance of AdaFGL over state-of-the-art
baselines. Specifically, in terms of test accuracy, our proposed AdaFGL
outperforms baselines by significant margins of 3.24\% and 5.57\% on community
split and structure Non-iid split, respectively.
\\ ( https://arxiv.org/abs/2401.11750 ,  13340kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11755
Date: Mon, 22 Jan 2024 08:31:53 GMT   (1767kb,D)

Title: FedGTA: Topology-aware Averaging for Federated Graph Learning
Authors: Xunkai Li, Zhengyu Wu, Wentao Zhang, Yinlin Zhu, Rong-Hua Li, Guoren
  Wang
Categories: cs.LG cs.AI cs.DB cs.SI
Comments: Accepted by VLDB 2024
\\
  Federated Graph Learning (FGL) is a distributed machine learning paradigm
that enables collaborative training on large-scale subgraphs across multiple
local systems. Existing FGL studies fall into two categories: (i) FGL
Optimization, which improves multi-client training in existing machine learning
models; (ii) FGL Model, which enhances performance with complex local models
and multi-client interactions. However, most FGL optimization strategies are
designed specifically for the computer vision domain and ignore graph
structure, presenting dissatisfied performance and slow convergence. Meanwhile,
complex local model architectures in FGL Models studies lack scalability for
handling large-scale subgraphs and have deployment limitations. To address
these issues, we propose Federated Graph Topology-aware Aggregation (FedGTA), a
personalized optimization strategy that optimizes through topology-aware local
smoothing confidence and mixed neighbor features. During experiments, we deploy
FedGTA in 12 multi-scale real-world datasets with the Louvain and Metis split.
This allows us to evaluate the performance and robustness of FedGTA across a
range of scenarios. Extensive experiments demonstrate that FedGTA achieves
state-of-the-art performance while exhibiting high scalability and efficiency.
The experiment includes ogbn-papers100M, the most representative large-scale
graph database so that we can verify the applicability of our method to
large-scale graph learning. To the best of our knowledge, our study is the
first to bridge large-scale graph learning with FGL using this optimization
strategy, contributing to the development of efficient and scalable FGL
methods.
\\ ( https://arxiv.org/abs/2401.11755 ,  1767kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11760
Date: Mon, 22 Jan 2024 08:45:29 GMT   (3125kb,D)

Title: Towards Effective and General Graph Unlearning via Mutual Evolution
Authors: Xunkai Li, Yulin Zhao, Zhengyu Wu, Wentao Zhang, Rong-Hua Li, Guoren
  Wang
Categories: cs.LG cs.NE cs.SI
Comments: Accepted by AAAI 2024 Oral
\\
  With the rapid advancement of AI applications, the growing needs for data
privacy and model robustness have highlighted the importance of machine
unlearning, especially in thriving graph-based scenarios. However, most
existing graph unlearning strategies primarily rely on well-designed
architectures or manual process, rendering them less user-friendly and posing
challenges in terms of deployment efficiency. Furthermore, striking a balance
between unlearning performance and framework generalization is also a pivotal
concern. To address the above issues, we propose \underline{\textbf{M}}utual
\underline{\textbf{E}}volution \underline{\textbf{G}}raph
\underline{\textbf{U}}nlearning (MEGU), a new mutual evolution paradigm that
simultaneously evolves the predictive and unlearning capacities of graph
unlearning. By incorporating aforementioned two components, MEGU ensures
complementary optimization in a unified training framework that aligns with the
prediction and unlearning requirements. Extensive experiments on 9 graph
benchmark datasets demonstrate the superior performance of MEGU in addressing
unlearning requirements at the feature, node, and edge levels. Specifically,
MEGU achieves average performance improvements of 2.7\%, 2.5\%, and 3.2\%
across these three levels of unlearning tasks when compared to state-of-the-art
baselines. Furthermore, MEGU exhibits satisfactory training efficiency,
reducing time and space overhead by an average of 159.8x and 9.6x,
respectively, in comparison to retraining GNN from scratch.
\\ ( https://arxiv.org/abs/2401.11760 ,  3125kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11768
Date: Mon, 22 Jan 2024 09:03:16 GMT   (787kb,D)

Title: ADA-GNN: Atom-Distance-Angle Graph Neural Network for Crystal Material
  Property Prediction
Authors: Jiao Huang and Qianli Xing and Jinglong Ji and Bo Yang
Categories: cs.LG cond-mat.mtrl-sci
\\
  Property prediction is a fundamental task in crystal material research. To
model atoms and structures, structures represented as graphs are widely used
and graph learning-based methods have achieved significant progress. Bond
angles and bond distances are two key structural information that greatly
influence crystal properties. However, most of the existing works only consider
bond distances and overlook bond angles. The main challenge lies in the time
cost of handling bond angles, which leads to a significant increase in
inference time. To solve this issue, we first propose a crystal structure
modeling based on dual scale neighbor partitioning mechanism, which uses a
larger scale cutoff for edge neighbors and a smaller scale cutoff for angle
neighbors. Then, we propose a novel Atom-Distance-Angle Graph Neural Network
(ADA-GNN) for property prediction tasks, which can process node information and
structural information separately. The accuracy of predictions and inference
time are improved with the dual scale modeling and the specially designed
architecture of ADA-GNN. The experimental results validate that our approach
achieves state-of-the-art results in two large-scale material benchmark
datasets on property prediction tasks.
\\ ( https://arxiv.org/abs/2401.11768 ,  787kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11772
Date: Mon, 22 Jan 2024 09:09:10 GMT   (3428kb,D)

Title: LightDiC: A Simple yet Effective Approach for Large-scale Digraph
  Representation Learning
Authors: Xunkai Li, Meihao Liao, Zhengyu Wu, Daohan Su, Wentao Zhang, Rong-Hua
  Li, Guoren Wang
Categories: cs.LG cs.AI cs.SI
Comments: Under Review
\\
  Most existing graph neural networks (GNNs) are limited to undirected graphs,
whose restricted scope of the captured relational information hinders their
expressive capabilities and deployments in real-world scenarios. Compared with
undirected graphs, directed graphs (digraphs) fit the demand for modeling more
complex topological systems by capturing more intricate relationships between
nodes, such as formulating transportation and financial networks. While some
directed GNNs have been introduced, their inspiration mainly comes from deep
learning architectures, which lead to redundant complexity and computation,
making them inapplicable to large-scale databases. To address these issues, we
propose LightDiC, a scalable variant of the digraph convolution based on the
magnetic Laplacian. Since topology-related computations are conducted solely
during offline pre-processing, LightDiC achieves exceptional scalability,
enabling downstream predictions to be trained separately without incurring
recursive computational costs. Theoretical analysis shows that LightDiC
utilizes directed information to achieve message passing based on the complex
field, which corresponds to the proximal gradient descent process of the
Dirichlet energy optimization function from the perspective of digraph signal
denoising, ensuring its expressiveness. Experimental results demonstrate that
LightDiC performs comparably well or even outperforms other SOTA methods in
various downstream tasks, with fewer learnable parameters and higher training
efficiency. Notably, LightDiC is the first DiGNN to provide satisfactory
results in the most representative large-scale database (ogbn-papers100M).
\\ ( https://arxiv.org/abs/2401.11772 ,  3428kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11798
Date: Mon, 22 Jan 2024 09:54:49 GMT   (803kb,D)

Title: Knowledge Distillation on Spatial-Temporal Graph Convolutional Network
  for Traffic Prediction
Authors: Mohammad Izadi, Mehran Safayani, Abdolreza Mirzaei
Categories: cs.LG cs.AI
\\
  Efficient real-time traffic prediction is crucial for reducing transportation
time. To predict traffic conditions, we employ a spatio-temporal graph neural
network (ST-GNN) to model our real-time traffic data as temporal graphs.
Despite its capabilities, it often encounters challenges in delivering
efficient real-time predictions for real-world traffic data. Recognizing the
significance of timely prediction due to the dynamic nature of real-time data,
we employ knowledge distillation (KD) as a solution to enhance the execution
time of ST-GNNs for traffic prediction. In this paper, We introduce a cost
function designed to train a network with fewer parameters (the student) using
distilled data from a complex network (the teacher) while maintaining its
accuracy close to that of the teacher. We use knowledge distillation,
incorporating spatial-temporal correlations from the teacher network to enable
the student to learn the complex patterns perceived by the teacher. However, a
challenge arises in determining the student network architecture rather than
considering it inadvertently. To address this challenge, we propose an
algorithm that utilizes the cost function to calculate pruning scores,
addressing small network architecture search issues, and jointly fine-tunes the
network resulting from each pruning stage using KD. Ultimately, we evaluate our
proposed ideas on two real-world datasets, PeMSD7 and PeMSD8. The results
indicate that our method can maintain the student's accuracy close to that of
the teacher, even with the retention of only $3\%$ of network parameters.
\\ ( https://arxiv.org/abs/2401.11798 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11810
Date: Mon, 22 Jan 2024 10:14:45 GMT   (1795kb,D)

Title: Generalization and Informativeness of Conformal Prediction
Authors: Matteo Zecchin, Sangwoo Park, Osvaldo Simeone, Fredrik Hellstr\"om
Categories: cs.LG cs.AI cs.IT math.IT
\\
  The safe integration of machine learning modules in decision-making processes
hinges on their ability to quantify uncertainty. A popular technique to achieve
this goal is conformal prediction (CP), which transforms an arbitrary base
predictor into a set predictor with coverage guarantees. While CP certifies the
predicted set to contain the target quantity with a user-defined tolerance, it
does not provide control over the average size of the predicted sets, i.e.,
over the informativeness of the prediction. In this work, a theoretical
connection is established between the generalization properties of the base
predictor and the informativeness of the resulting CP prediction sets. To this
end, an upper bound is derived on the expected size of the CP set predictor
that builds on generalization error bounds for the base predictor. The derived
upper bound provides insights into the dependence of the average size of the CP
set predictor on the amount of calibration data, the target reliability, and
the generalization performance of the base predictor. The theoretical insights
are validated using simple numerical regression and classification tasks.
\\ ( https://arxiv.org/abs/2401.11810 ,  1795kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11836
Date: Mon, 22 Jan 2024 10:52:22 GMT   (1980kb,D)

Title: Privacy-Preserving Data Fusion for Traffic State Estimation: A Vertical
  Federated Learning Approach
Authors: Qiqing Wang, Kaidi Yang
Categories: cs.LG cs.CR cs.SY eess.SY
\\
  This paper proposes a privacy-preserving data fusion method for traffic state
estimation (TSE). Unlike existing works that assume all data sources to be
accessible by a single trusted party, we explicitly address data privacy
concerns that arise in the collaboration and data sharing between multiple data
owners, such as municipal authorities (MAs) and mobility providers (MPs). To
this end, we propose a novel vertical federated learning (FL) approach, FedTSE,
that enables multiple data owners to collaboratively train and apply a TSE
model without having to exchange their private data. To enhance the
applicability of the proposed FedTSE in common TSE scenarios with limited
availability of ground-truth data, we further propose a privacy-preserving
physics-informed FL approach, i.e., FedTSE-PI, that integrates traffic models
into FL. Real-world data validation shows that the proposed methods can protect
privacy while yielding similar accuracy to the oracle method without privacy
considerations.
\\ ( https://arxiv.org/abs/2401.11836 ,  1980kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11840
Date: Mon, 22 Jan 2024 10:57:11 GMT   (48853kb,D)

Title: Learning to Approximate Adaptive Kernel Convolution on Graphs
Authors: Jaeyoon Sim, Sooyeon Jeon, InJun Choi, Guorong Wu, Won Hwa Kim
Categories: cs.LG cs.AI
Comments: 15 pages, Accepted to AAAI 2024
\\
  Various Graph Neural Networks (GNNs) have been successful in analyzing data
in non-Euclidean spaces, however, they have limitations such as oversmoothing,
i.e., information becomes excessively averaged as the number of hidden layers
increases. The issue stems from the intrinsic formulation of conventional graph
convolution where the nodal features are aggregated from a direct neighborhood
per layer across the entire nodes in the graph. As setting different number of
hidden layers per node is infeasible, recent works leverage a diffusion kernel
to redefine the graph structure and incorporate information from farther nodes.
Unfortunately, such approaches suffer from heavy diagonalization of a graph
Laplacian or learning a large transform matrix. In this regards, we propose a
diffusion learning framework, where the range of feature aggregation is
controlled by the scale of a diffusion kernel. For efficient computation, we
derive closed-form derivatives of approximations of the graph convolution with
respect to the scale, so that node-wise range can be adaptively learned. With a
downstream classifier, the entire framework is made trainable in an end-to-end
manner. Our model is tested on various standard datasets for node-wise
classification for the state-of-the-art performance, and it is also validated
on a real-world brain network data for graph classifications to demonstrate its
practicality for Alzheimer classification.
\\ ( https://arxiv.org/abs/2401.11840 ,  48853kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11849
Date: Mon, 22 Jan 2024 11:08:36 GMT   (2596kb,D)

Title: Self-Labeling the Job Shop Scheduling Problem
Authors: Andrea Corsini, Angelo Porrello, Simone Calderara, Mauro Dell'Amico
Categories: cs.LG cs.AI math.CO
ACM-class: I.2; G.2
\\
  In this work, we propose a Self-Supervised training strategy specifically
designed for combinatorial problems. One of the main obstacles in applying
supervised paradigms to such problems is the requirement of expensive target
solutions as ground-truth, often produced with costly exact solvers. Inspired
by Semi- and Self-Supervised learning, we show that it is possible to easily
train generative models by sampling multiple solutions and using the best one
according to the problem objective as a pseudo-label. In this way, we
iteratively improve the model generation capability by relying only on its
self-supervision, completely removing the need for optimality information. We
prove the effectiveness of this Self-Labeling strategy on the Job Shop
Scheduling (JSP), a complex combinatorial problem that is receiving much
attention from the Reinforcement Learning community. We propose a generative
model based on the well-known Pointer Network and train it with our strategy.
Experiments on two popular benchmarks demonstrate the potential of this
approach as the resulting models outperform constructive heuristics and current
state-of-the-art Reinforcement Learning proposals.
\\ ( https://arxiv.org/abs/2401.11849 ,  2596kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11860
Date: Mon, 22 Jan 2024 11:29:44 GMT   (1644kb,D)

Title: A Review of Physics-Informed Machine Learning Methods with Applications
  to Condition Monitoring and Anomaly Detection
Authors: Yuandi Wu, Brett Sicard, and Stephen Andrew Gadsden
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: Paper has been submitted for review to the journal Expert Systems
  with Applications (December 31, 2023). 90 pages, 22 figures, 9 tables
\\
  This study presents a comprehensive overview of PIML techniques in the
context of condition monitoring. The central concept driving PIML is the
incorporation of known physical laws and constraints into machine learning
algorithms, enabling them to learn from available data while remaining
consistent with physical principles. Through fusing domain knowledge with
data-driven learning, PIML methods offer enhanced accuracy and interpretability
in comparison to purely data-driven approaches. In this comprehensive survey,
detailed examinations are performed with regard to the methodology by which
known physical principles are integrated within machine learning frameworks, as
well as their suitability for specific tasks within condition monitoring.
Incorporation of physical knowledge into the ML model may be realized in a
variety of methods, with each having its unique advantages and drawbacks. The
distinct advantages and limitations of each methodology for the integration of
physics within data-driven models are detailed, considering factors such as
computational efficiency, model interpretability, and generalizability to
different systems in condition monitoring and fault detection. Several case
studies and works of literature utilizing this emerging concept are presented
to demonstrate the efficacy of PIML in condition monitoring applications. From
the literature reviewed, the versatility and potential of PIML in condition
monitoring may be demonstrated. Novel PIML methods offer an innovative solution
for addressing the complexities of condition monitoring and associated
challenges. This comprehensive survey helps form the foundation for future work
in the field. As the technology continues to advance, PIML is expected to play
a crucial role in enhancing maintenance strategies, system reliability, and
overall operational efficiency in engineering systems.
\\ ( https://arxiv.org/abs/2401.11860 ,  1644kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11929
Date: Mon, 22 Jan 2024 13:15:40 GMT   (7516kb,D)

Title: The Bigger the Better? Rethinking the Effective Model Scale in Long-term
  Time Series Forecasting
Authors: Jinliang Deng, Xuan Song, Ivor W. Tsang, Hui Xiong
Categories: cs.LG
\\
  Long-term time series forecasting (LTSF) represents a critical frontier in
time series analysis, distinguished by its focus on extensive input sequences,
in contrast to the constrained lengths typical of traditional approaches. While
longer sequences inherently convey richer information, potentially enhancing
predictive precision, prevailing techniques often respond by escalating model
complexity. These intricate models can inflate into millions of parameters,
incorporating parameter-intensive elements like positional encodings,
feed-forward networks and self-attention mechanisms. This complexity, however,
leads to prohibitive model scale, particularly given the time series data's
semantic simplicity. Motivated by the pursuit of parsimony, our research
employs conditional correlation and auto-correlation as investigative tools,
revealing significant redundancies within the input data. Leveraging these
insights, we introduce the HDformer, a lightweight Transformer variant enhanced
with hierarchical decomposition. This novel architecture not only inverts the
prevailing trend toward model expansion but also accomplishes precise
forecasting with drastically fewer computations and parameters. Remarkably,
HDformer outperforms existing state-of-the-art LTSF models, while requiring
over 99\% fewer parameters. Through this work, we advocate a paradigm shift in
LTSF, emphasizing the importance to tailor the model to the inherent dynamics
of time series data-a timely reminder that in the realm of LTSF, bigger is not
invariably better.
\\ ( https://arxiv.org/abs/2401.11929 ,  7516kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11940
Date: Mon, 22 Jan 2024 13:30:11 GMT   (507kb,D)

Title: Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent
Authors: Zhiyu Liu, Zhi Han, Yandong Tang, Xi-Le Zhao, Yao Wang
Categories: cs.LG math.OC stat.ML
Comments: 13 pages, 4 figures
\\
  This paper considers the problem of recovering a tensor with an underlying
low-tubal-rank structure from a small number of corrupted linear measurements.
Traditional approaches tackling such a problem require the computation of
tensor Singular Value Decomposition (t-SVD), that is a computationally
intensive process, rendering them impractical for dealing with large-scale
tensors. Aim to address this challenge, we propose an efficient and effective
low-tubal-rank tensor recovery method based on a factorization procedure akin
to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves
decomposing a large tensor into two smaller factor tensors, followed by solving
the problem through factorized gradient descent (FGD). This strategy eliminates
the need for t-SVD computation, thereby reducing computational costs and
storage requirements. We provide rigorous theoretical analysis to ensure the
convergence of FGD under both noise-free and noisy situations. Additionally, it
is worth noting that our method does not require the precise estimation of the
tensor tubal-rank. Even in cases where the tubal-rank is slightly
overestimated, our approach continues to demonstrate robust performance. A
series of experiments have been carried out to demonstrate that, as compared to
other popular ones, our approach exhibits superior performance in multiple
scenarios, in terms of the faster computational speed and the smaller
convergence error.
\\ ( https://arxiv.org/abs/2401.11940 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11943
Date: Mon, 22 Jan 2024 13:33:53 GMT   (7628kb,D)

Title: Benchmarking Large Multimodal Models against Common Corruptions
Authors: Jiawei Zhang, Tianyu Pang, Chao Du, Yi Ren, Bo Li, Min Lin
Categories: cs.LG cs.CL cs.CR cs.CV cs.MM
Comments: Technical report
\\
  This technical report aims to fill a deficiency in the assessment of large
multimodal models (LMMs) by specifically examining the self-consistency of
their outputs when subjected to common corruptions. We investigate the
cross-modal interactions between text, image, and speech, encompassing four
essential generation tasks: text-to-image, image-to-text, text-to-speech, and
speech-to-text. We create a comprehensive benchmark, named MMCBench, that
covers more than 100 popular LMMs (totally over 150 model checkpoints). A
thorough evaluation under common corruptions is critical for practical
deployment and facilitates a better understanding of the reliability of
cutting-edge LMMs. The benchmarking code is available at
https://github.com/sail-sg/MMCBench
\\ ( https://arxiv.org/abs/2401.11943 ,  7628kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11954
Date: Mon, 22 Jan 2024 13:54:26 GMT   (7243kb,D)

Title: RUMBoost: Gradient Boosted Random Utility Models
Authors: Nicolas Salvad\'e, Tim Hillel
Categories: cs.LG stat.ML
\\
  This paper introduces the RUMBoost model, a novel discrete choice modelling
approach that combines the interpretability and behavioural robustness of
Random Utility Models (RUMs) with the generalisation and predictive ability of
deep learning methods. We obtain the full functional form of non-linear utility
specifications by replacing each linear parameter in the utility functions of a
RUM with an ensemble of gradient boosted regression trees. This enables
piece-wise constant utility values to be imputed for all alternatives directly
from the data for any possible combination of input variables. We introduce
additional constraints on the ensembles to ensure three crucial features of the
utility specifications: (i) dependency of the utilities of each alternative on
only the attributes of that alternative, (ii) monotonicity of marginal
utilities, and (iii) an intrinsically interpretable functional form, where the
exact response of the model is known throughout the entire input space.
Furthermore, we introduce an optimisation-based smoothing technique that
replaces the piece-wise constant utility values of alternative attributes with
monotonic piece-wise cubic splines to identify non-linear parameters with
defined gradient. We demonstrate the potential of the RUMBoost model compared
to various ML and Random Utility benchmark models for revealed preference mode
choice data from London. The results highlight the great predictive performance
and the direct interpretability of our proposed approach. Furthermore, the
smoothed attribute utility functions allow for the calculation of various
behavioural indicators and marginal utilities. Finally, we demonstrate the
flexibility of our methodology by showing how the RUMBoost model can be
extended to complex model specifications, including attribute interactions,
correlation within alternative error terms and heterogeneity within the
population.
\\ ( https://arxiv.org/abs/2401.11954 ,  7243kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11974
Date: Mon, 22 Jan 2024 14:26:02 GMT   (201kb,D)

Title: Cross-Validation Conformal Risk Control
Authors: Kfir M. Cohen, Sangwoo Park, Osvaldo Simeone, Shlomo Shamai (Shitz)
Categories: cs.LG stat.ML
\\
  Conformal risk control (CRC) is a recently proposed technique that applies
post-hoc to a conventional point predictor to provide calibration guarantees.
Generalizing conformal prediction (CP), with CRC, calibration is ensured for a
set predictor that is extracted from the point predictor to control a risk
function such as the probability of miscoverage or the false negative rate. The
original CRC requires the available data set to be split between training and
validation data sets. This can be problematic when data availability is
limited, resulting in inefficient set predictors. In this paper, a novel CRC
method is introduced that is based on cross-validation, rather than on
validation as the original CRC. The proposed cross-validation CRC (CV-CRC)
extends a version of the jackknife-minmax from CP to CRC, allowing for the
control of a broader range of risk functions. CV-CRC is proved to offer
theoretical guarantees on the average risk of the set predictor. Furthermore,
numerical experiments show that CV-CRC can reduce the average set size with
respect to CRC when the available data are limited.
\\ ( https://arxiv.org/abs/2401.11974 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11985
Date: Mon, 22 Jan 2024 14:38:25 GMT   (29612kb,D)

Title: Scaling Face Interaction Graph Networks to Real World Scenes
Authors: Tatiana Lopez-Guevara, Yulia Rubanova, William F. Whitney, Tobias
  Pfaff, Kimberly Stachenfeld, Kelsey R. Allen
Categories: cs.LG cs.CV cs.RO
Comments: 16 pages, 12 figures
\\
  Accurately simulating real world object dynamics is essential for various
applications such as robotics, engineering, graphics, and design. To better
capture complex real dynamics such as contact and friction, learned simulators
based on graph networks have recently shown great promise. However, applying
these learned simulators to real scenes comes with two major challenges: first,
scaling learned simulators to handle the complexity of real world scenes which
can involve hundreds of objects each with complicated 3D shapes, and second,
handling inputs from perception rather than 3D state information. Here we
introduce a method which substantially reduces the memory required to run
graph-based learned simulators. Based on this memory-efficient simulation
model, we then present a perceptual interface in the form of editable NeRFs
which can convert real-world scenes into a structured representation that can
be processed by graph network simulator. We show that our method uses
substantially less memory than previous graph-based simulators while retaining
their accuracy, and that the simulators learned in synthetic environments can
be applied to real world scenes captured from multiple camera angles. This
paves the way for expanding the application of learned simulators to settings
where only perceptual information is available at inference time.
\\ ( https://arxiv.org/abs/2401.11985 ,  29612kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11993
Date: Mon, 22 Jan 2024 14:46:41 GMT   (4322kb,D)

Title: Expert-Driven Monitoring of Operational ML Models
Authors: Joran Leest, Claudia Raibulet, Ilias Gerostathopoulos, Patricia Lago
Categories: cs.LG cs.SE
\\
  We propose Expert Monitoring, an approach that leverages domain expertise to
enhance the detection and mitigation of concept drift in machine learning (ML)
models. Our approach supports practitioners by consolidating domain expertise
related to concept drift-inducing events, making this expertise accessible to
on-call personnel, and enabling automatic adaptability with expert oversight.
\\ ( https://arxiv.org/abs/2401.11993 ,  4322kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12000
Date: Mon, 22 Jan 2024 14:51:01 GMT   (1276kb,D)

Title: Integrating Statistical Significance and Discriminative Power in Pattern
  Discovery
Authors: Leonardo Alexandre and Rafael S. Costa and Rui Henriques
Categories: cs.LG stat.ML
\\
  Pattern discovery plays a central role in both descriptive and predictive
tasks across multiple domains. Actionable patterns must meet rigorous
statistical significance criteria and, in the presence of target variables,
further uphold discriminative power. Our work addresses the underexplored area
of guiding pattern discovery by integrating statistical significance and
discriminative power criteria into state-of-the-art algorithms while preserving
pattern quality. We also address how pattern quality thresholds, imposed by
some algorithms, can be rectified to accommodate these additional criteria. To
test the proposed methodology, we select the triclustering task as the guiding
pattern discovery case and extend well-known greedy and multi-objective
optimization triclustering algorithms, $\delta$-Trimax and TriGen, that use
various pattern quality criteria, such as Mean Squared Residual (MSR), Least
Squared Lines (LSL), and Multi Slope Measure (MSL). Results from three case
studies show the role of the proposed methodology in discovering patterns with
pronounced improvements of discriminative power and statistical significance
without quality deterioration, highlighting its importance in supervisedly
guiding the search. Although the proposed methodology is motivated over
multivariate time series data, it can be straightforwardly extended to pattern
discovery tasks involving multivariate, N-way (N>3), transactional, and
sequential data structures.
  Availability: The code is freely available at
https://github.com/JupitersMight/MOF_Triclustering under the MIT license.
\\ ( https://arxiv.org/abs/2401.12000 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12002
Date: Mon, 22 Jan 2024 14:52:34 GMT   (17224kb,D)

Title: HgbNet: predicting hemoglobin level/anemia degree from EHR data
Authors: Zhuo Zhi, Moe Elbadawi, Adam Daneshmend, Mine Orlu, Abdul Basit,
  Andreas Demosthenous, Miguel Rodrigues
Categories: cs.LG
\\
  Anemia is a prevalent medical condition that typically requires invasive
blood tests for diagnosis and monitoring. Electronic health records (EHRs) have
emerged as valuable data sources for numerous medical studies. EHR-based
hemoglobin level/anemia degree prediction is non-invasive and rapid but still
faces some challenges due to the fact that EHR data is typically an irregular
multivariate time series containing a significant number of missing values and
irregular time intervals. To address these issues, we introduce HgbNet, a
machine learning-based prediction model that emulates clinicians'
decision-making processes for hemoglobin level/anemia degree prediction. The
model incorporates a NanDense layer with a missing indicator to handle missing
values and employs attention mechanisms to account for both local irregularity
and global irregularity. We evaluate the proposed method using two real-world
datasets across two use cases. In our first use case, we predict hemoglobin
level/anemia degree at moment T+1 by utilizing records from moments prior to
T+1. In our second use case, we integrate all historical records with
additional selected test results at moment T+1 to predict hemoglobin
level/anemia degree at the same moment, T+1. HgbNet outperforms the best
baseline results across all datasets and use cases. These findings demonstrate
the feasibility of estimating hemoglobin levels and anemia degree from EHR
data, positioning HgbNet as an effective non-invasive anemia diagnosis solution
that could potentially enhance the quality of life for millions of affected
individuals worldwide. To our knowledge, HgbNet is the first machine learning
model leveraging EHR data for hemoglobin level/anemia degree prediction.
\\ ( https://arxiv.org/abs/2401.12002 ,  17224kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12007
Date: Mon, 22 Jan 2024 14:55:01 GMT   (613kb,D)

Title: Tensor-view Topological Graph Neural Network
Authors: Tao Wen, Elynn Chen, Yuzhou Chen
Categories: cs.LG cs.AI
Comments: Accepted at AISTATS 2024
\\
  Graph classification is an important learning task for graph-structured data.
Graph neural networks (GNNs) have recently gained growing attention in graph
learning and have shown significant improvements in many important graph
problems. Despite their state-of-the-art performances, existing GNNs only use
local information from a very limited neighborhood around each node, suffering
from loss of multi-modal information and overheads of excessive computation. To
address these issues, we propose a novel Tensor-view Topological Graph Neural
Network (TTG-NN), a class of simple yet effective topological deep learning
built upon persistent homology, graph convolution, and tensor operations. This
new method incorporates tensor learning to simultaneously capture Tensor-view
Topological (TT), as well as Tensor-view Graph (TG) structural information on
both local and global levels. Computationally, to fully exploit graph topology
and structure, we propose two flexible TT and TG representation learning
modules that disentangle feature tensor aggregation and transformation and
learn to preserve multi-modal structure with less computation. Theoretically,
we derive high probability bounds on both the out-of-sample and in-sample mean
squared approximation errors for our proposed Tensor Transformation Layer
(TTL). Real data experiments show that the proposed TTG-NN outperforms 20
state-of-the-art methods on various graph benchmarks.
\\ ( https://arxiv.org/abs/2401.12007 ,  613kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12012
Date: Mon, 22 Jan 2024 14:59:11 GMT   (10090kb,D)

Title: TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for
  Lazy Clients
Authors: Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci
Categories: cs.LG cs.DC
\\
  Federated learning is a distributed collaborative machine learning paradigm
that has gained strong momentum in recent years. In federated learning, a
central server periodically coordinates models with clients and aggregates the
models trained locally by clients without necessitating access to local data.
Despite its potential, the implementation of federated learning continues to
encounter several challenges, predominantly the slow convergence that is
largely due to data heterogeneity. The slow convergence becomes particularly
problematic in cross-device federated learning scenarios where clients may be
strongly limited by computing power and storage space, and hence counteracting
methods that induce additional computation or memory cost on the client side
such as auxiliary objective terms and larger training iterations can be
impractical. In this paper, we propose a novel federated aggregation strategy,
TurboSVM-FL, that poses no additional computation burden on the client side and
can significantly accelerate convergence for federated classification task,
especially when clients are "lazy" and train their models solely for few epochs
for next global aggregation. TurboSVM-FL extensively utilizes support vector
machine to conduct selective aggregation and max-margin spread-out
regularization on class embeddings. We evaluate TurboSVM-FL on multiple
datasets including FEMNIST, CelebA, and Shakespeare using user-independent
validation with non-iid data distribution. Our results show that TurboSVM-FL
can significantly outperform existing popular algorithms on convergence rate
and reduce communication rounds while delivering better test metrics including
accuracy, F1 score, and MCC.
\\ ( https://arxiv.org/abs/2401.12012 ,  10090kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12014
Date: Mon, 22 Jan 2024 15:00:32 GMT   (1322kb,D)

Title: Robustness to distribution shifts of compressed networks for edge
  devices
Authors: Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark
Categories: cs.LG cs.AI cs.CV
\\
  It is necessary to develop efficient DNNs deployed on edge devices with
limited computation resources. However, the compressed networks often execute
new tasks in the target domain, which is different from the source domain where
the original network is trained. It is important to investigate the robustness
of compressed networks in two types of data distribution shifts: domain shifts
and adversarial perturbations. In this study, we discover that compressed
models are less robust to distribution shifts than their original networks.
Interestingly, larger networks are more vulnerable to losing robustness than
smaller ones, even when they are compressed to a similar size as the smaller
networks. Furthermore, compact networks obtained by knowledge distillation are
much more robust to distribution shifts than pruned networks. Finally,
post-training quantization is a reliable method for achieving significant
robustness to distribution shifts, and it outperforms both pruned and distilled
models in terms of robustness.
\\ ( https://arxiv.org/abs/2401.12014 ,  1322kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12033
Date: Mon, 22 Jan 2024 15:19:18 GMT   (13170kb,D)

Title: Momentum-SAM: Sharpness Aware Minimization without Computational
  Overhead
Authors: Marlon Becker, Frederick Altrock, Benjamin Risse
Categories: cs.LG cs.CV
\\
  The recently proposed optimization algorithm for deep neural networks
Sharpness Aware Minimization (SAM) suggests perturbing parameters before
gradient calculation by a gradient ascent step to guide the optimization into
parameter space regions of flat loss. While significant generalization
improvements and thus reduction of overfitting could be demonstrated, the
computational costs are doubled due to the additionally needed gradient
calculation, making SAM unfeasible in case of limited computationally
capacities. Motivated by Nesterov Accelerated Gradient (NAG) we propose
Momentum-SAM (MSAM), which perturbs parameters in the direction of the
accumulated momentum vector to achieve low sharpness without significant
computational overhead or memory demands over SGD or Adam. We evaluate MSAM in
detail and reveal insights on separable mechanisms of NAG, SAM and MSAM
regarding training optimization and generalization. Code is available at
https://github.com/MarlonBecker/MSAM.
\\ ( https://arxiv.org/abs/2401.12033 ,  13170kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12058
Date: Mon, 22 Jan 2024 15:50:32 GMT   (50kb)

Title: The Dimension Strikes Back with Gradients: Generalization of Gradient
  Methods in Stochastic Convex Optimization
Authors: Matan Schliserman and Uri Sherman and Tomer Koren
Categories: cs.LG stat.ML
\\
  We study the generalization performance of gradient methods in the
fundamental stochastic convex optimization setting, focusing on its dimension
dependence. First, for full-batch gradient descent (GD) we give a construction
of a learning problem in dimension $d=O(n^2)$, where the canonical version of
GD (tuned for optimal performance of the empirical risk) trained with $n$
training examples converges, with constant probability, to an approximate
empirical risk minimizer with $\Omega(1)$ population excess risk. Our bound
translates to a lower bound of $\Omega (\sqrt{d})$ on the number of training
examples required for standard GD to reach a non-trivial test error, answering
an open question raised by Feldman (2016) and Amir, Koren, and Livni (2021b)
and showing that a non-trivial dimension dependence is unavoidable.
Furthermore, for standard one-pass stochastic gradient descent (SGD), we show
that an application of the same construction technique provides a similar
$\Omega(\sqrt{d})$ lower bound for the sample complexity of SGD to reach a
non-trivial empirical error, despite achieving optimal test performance. This
again provides an exponential improvement in the dimension dependence compared
to previous work (Koren, Livni, Mansour, and Sherman, 2022), resolving an open
question left therein.
\\ ( https://arxiv.org/abs/2401.12058 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12069
Date: Mon, 22 Jan 2024 16:08:41 GMT   (2234kb,D)

Title: Beyond TreeSHAP: Efficient Computation of Any-Order Shapley Interactions
  for Tree Ensembles
Authors: Maximilian Muschalik, Fabian Fumagalli, Barbara Hammer, Eyke
  H\"ullermeier
Categories: cs.LG
\\
  While shallow decision trees may be interpretable, larger ensemble models
like gradient-boosted trees, which often set the state of the art in machine
learning problems involving tabular data, still remain black box models. As a
remedy, the Shapley value (SV) is a well-known concept in explainable
artificial intelligence (XAI) research for quantifying additive feature
attributions of predictions. The model-specific TreeSHAP methodology solves the
exponential complexity for retrieving exact SVs from tree-based models.
Expanding beyond individual feature attribution, Shapley interactions reveal
the impact of intricate feature interactions of any order. In this work, we
present TreeSHAP-IQ, an efficient method to compute any-order additive Shapley
interactions for predictions of tree-based models. TreeSHAP-IQ is supported by
a mathematical framework that exploits polynomial arithmetic to compute the
interaction scores in a single recursive traversal of the tree, akin to Linear
TreeSHAP. We apply TreeSHAP-IQ on state-of-the-art tree ensembles and explore
interactions on well-established benchmark datasets.
\\ ( https://arxiv.org/abs/2401.12069 ,  2234kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12113
Date: Mon, 22 Jan 2024 16:51:01 GMT   (1517kb,D)

Title: Extracting Formulae in Many-Valued Logic from Deep Neural Networks
Authors: Yani Zhang, Helmut B\"olcskei
Categories: cs.LG cs.AI cs.LO
\\
  We propose a new perspective on deep ReLU networks, namely as circuit
counterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV)
generalization of Boolean logic. An algorithm for extracting formulae in MV
logic from deep ReLU networks is presented. As the algorithm applies to
networks with general, in particular also real-valued, weights, it can be used
to extract logical formulae from deep ReLU networks trained on data.
\\ ( https://arxiv.org/abs/2401.12113 ,  1517kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12132
Date: Mon, 22 Jan 2024 17:14:47 GMT   (1700kb)

Title: Evaluation of QCNN-LSTM for Disability Forecasting in Multiple Sclerosis
  Using Sequential Multisequence MRI
Authors: John D. Mayfield and Issam El Naqa
Categories: cs.LG cs.AI cs.ET eess.IV
ACM-class: I.2.0; I.2.6
\\
  Introduction Quantum Convolutional Neural Network (QCNN)-Long Short-Term
Memory (LSTM) models were studied to provide sequential relationships for each
timepoint in MRIs of patients with Multiple Sclerosis (MS). In this pilot
study, we compared three QCNN-LSTM models for binary classification of MS
disability benchmarked against classical neural network architectures. Our
hypothesis is that quantum models will provide competitive performance. Methods
Matrix Product State (MPS), reverse Multistate Entanglement Renormalization
Ansatz (MERA), and Tree-Tensor Network (TTN) circuits were paired with LSTM
layer to process near-annual MRI data of patients diagnosed with MS. These were
benchmarked against a Visual Geometry Group (VGG)-LSTM and a Video Vision
Transformer (ViViT). Predicted logits were measured against ground truth labels
of each patient's Extended Disability Severity Score (EDSS) using binary
cross-entropy loss. Training/validation/holdout testing was partitioned using
5-fold cross validation with a total split of 60:20:20. Levene's test of
variance was used to measure statistical difference and Student's t-test for
paired model differences in mean. Results The MPS-LSTM, reverse MERA-LSTM, and
TTN-LSTM had holdout testing ROC-AUC of 0.70, 0.77, and 0.81, respectively
(p-value 0.915). VGG16-LSTM and ViViT performed similarly with ROC-AUC of 0.73
and 0.77, respectively (p-value 0.631). Overall variance and mean were not
statistically significant (p-value 0.713), however, time to train was
significantly faster for the QCNN-LSTMs (39.4 sec per fold vs. 224 and 218,
respectively, p-value <0.001). Conclusion QCNN-LSTM models perform
competitively to their classical counterparts with greater efficiency in train
time. Clinically, these can add value in terms of efficiency to time-dependent
deep learning prediction of disease progression based upon medical imaging.
\\ ( https://arxiv.org/abs/2401.12132 ,  1700kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12181
Date: Mon, 22 Jan 2024 18:11:01 GMT   (4698kb,D)

Title: Universal Neurons in GPT2 Language Models
Authors: Wes Gurnee, Theo Horsley, Zifan Carl Guo, Tara Rezaei Kheirkhah, Qinyi
  Sun, Will Hathaway, Neel Nanda, Dimitris Bertsimas
Categories: cs.LG cs.AI cs.CL
\\
  A basic question within the emerging field of mechanistic interpretability is
the degree to which neural networks learn the same underlying mechanisms. In
other words, are neural mechanisms universal across different models? In this
work, we study the universality of individual neurons across GPT2 models
trained from different initial random seeds, motivated by the hypothesis that
universal neurons are likely to be interpretable. In particular, we compute
pairwise correlations of neuron activations over 100 million tokens for every
neuron pair across five different seeds and find that 1-5\% of neurons are
universal, that is, pairs of neurons which consistently activate on the same
inputs. We then study these universal neurons in detail, finding that they
usually have clear interpretations and taxonomize them into a small number of
neuron families. We conclude by studying patterns in neuron weights to
establish several universal functional roles of neurons in simple circuits:
deactivating attention heads, changing the entropy of the next token
distribution, and predicting the next token to (not) be within a particular
set.
\\ ( https://arxiv.org/abs/2401.12181 ,  4698kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12187
Date: Mon, 22 Jan 2024 18:27:08 GMT   (2920kb,D)

Title: WARM: On the Benefits of Weight Averaged Reward Models
Authors: Alexandre Ram\'e, Nino Vieillard, L\'eonard Hussenot, Robert Dadashi,
  Geoffrey Cideron, Olivier Bachem, Johan Ferret
Categories: cs.LG cs.AI cs.CL
Comments: 14 pages, 9 figures
\\
  Aligning large language models (LLMs) with human preferences through
reinforcement learning (RLHF) can lead to reward hacking, where LLMs exploit
failures in the reward model (RM) to achieve seemingly high rewards without
meeting the underlying objectives. We identify two primary challenges when
designing RMs to mitigate reward hacking: distribution shifts during the RL
process and inconsistencies in human preferences. As a solution, we propose
Weight Averaged Reward Models (WARM), first fine-tuning multiple RMs, then
averaging them in the weight space. This strategy follows the observation that
fine-tuned weights remain linearly mode connected when sharing the same
pre-training. By averaging weights, WARM improves efficiency compared to the
traditional ensembling of predictions, while improving reliability under
distribution shifts and robustness to preference inconsistencies. Our
experiments on summarization tasks, using best-of-N and RL methods, shows that
WARM improves the overall quality and alignment of LLM predictions; for
example, a policy RL fine-tuned with WARM has a 79.4% win rate against a policy
RL fine-tuned with a single RM.
\\ ( https://arxiv.org/abs/2401.12187 ,  2920kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12205
Date: Mon, 22 Jan 2024 18:46:30 GMT   (2440kb,D)

Title: Retrieval-Guided Reinforcement Learning for Boolean Circuit Minimization
Authors: Animesh Basak Chowdhury, Marco Romanelli, Benjamin Tan, Ramesh Karri,
  Siddharth Garg
Categories: cs.LG cs.AI cs.AR
Comments: Accepted in ICLR 2024
\\
  Logic synthesis, a pivotal stage in chip design, entails optimizing chip
specifications encoded in hardware description languages like Verilog into
highly efficient implementations using Boolean logic gates. The process
involves a sequential application of logic minimization heuristics (``synthesis
recipe"), with their arrangement significantly impacting crucial metrics such
as area and delay. Addressing the challenge posed by the broad spectrum of
design complexities - from variations of past designs (e.g., adders and
multipliers) to entirely novel configurations (e.g., innovative processor
instructions) - requires a nuanced `synthesis recipe` guided by human expertise
and intuition. This study conducts a thorough examination of learning and
search techniques for logic synthesis, unearthing a surprising revelation:
pre-trained agents, when confronted with entirely novel designs, may veer off
course, detrimentally affecting the search trajectory. We present ABC-RL, a
meticulously tuned $\alpha$ parameter that adeptly adjusts recommendations from
pre-trained agents during the search process. Computed based on similarity
scores through nearest neighbor retrieval from the training dataset, ABC-RL
yields superior synthesis recipes tailored for a wide array of hardware
designs. Our findings showcase substantial enhancements in the
Quality-of-result (QoR) of synthesized circuits, boasting improvements of up to
24.8% compared to state-of-the-art techniques. Furthermore, ABC-RL achieves an
impressive up to 9x reduction in runtime (iso-QoR) when compared to current
state-of-the-art methodologies.
\\ ( https://arxiv.org/abs/2401.12205 ,  2440kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.10899 (*cross-listing*)
Date: Mon, 18 Dec 2023 23:38:05 GMT   (74kb)

Title: Concrete Problems in AI Safety, Revisited
Authors: Inioluwa Deborah Raji and Roel Dobbe
Categories: cs.CY cs.AI
Comments: Published at ICLR workshop on ML in the Real World, 2020
\\
  As AI systems proliferate in society, the AI community is increasingly
preoccupied with the concept of AI Safety, namely the prevention of failures
due to accidents that arise from an unanticipated departure of a system's
behavior from designer intent in AI deployment. We demonstrate through an
analysis of real world cases of such incidents that although current vocabulary
captures a range of the encountered issues of AI deployment, an expanded
socio-technical framing will be required for a more complete understanding of
how AI systems and implemented safety mechanisms fail and succeed in real life.
\\ ( https://arxiv.org/abs/2401.10899 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10910 (*cross-listing*)
Date: Tue, 9 Jan 2024 15:00:47 GMT   (3512kb,D)

Title: Metacognition is all you need? Using Introspection in Generative Agents
  to Improve Goal-directed Behavior
Authors: Jason Toy, Josh MacAdam, Phil Tabor
Categories: q-bio.NC cs.AI
Comments: 9 pages, 4 figures
\\
  Recent advances in Large Language Models (LLMs) have shown impressive
capabilities in various applications, yet LLMs face challenges such as limited
context windows and difficulties in generalization. In this paper, we introduce
a metacognition module for generative agents, enabling them to observe their
own thought processes and actions. This metacognitive approach, designed to
emulate System 1 and System 2 cognitive processes, allows agents to
significantly enhance their performance by modifying their strategy. We tested
the metacognition module on a variety of scenarios, including a situation where
generative agents must survive a zombie apocalypse, and observe that our system
outperform others, while agents adapt and improve their strategies to complete
tasks over time.
\\ ( https://arxiv.org/abs/2401.10910 ,  3512kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10917 (*cross-listing*)
Date: Sat, 13 Jan 2024 19:12:49 GMT   (650kb,D)

Title: Artificial intelligence to automate the systematic review of scientific
  literature
Authors: Jos\'e de la Torre-L\'opez and Aurora Ram\'irez and Jos\'e Ra\'ul
  Romero
Categories: cs.IR cs.AI
Comments: 25 pages, 3 figures, 1 table, journal paper
MSC-class: 68T01
ACM-class: I.2.m; A.1
Journal-ref: Computing, Volume 105, pages 2171-2194, 2023
DOI: 10.1007/s00607-023-01181-x
\\
  Artificial intelligence (AI) has acquired notorious relevance in modern
computing as it effectively solves complex tasks traditionally done by humans.
AI provides methods to represent and infer knowledge, efficiently manipulate
texts and learn from vast amount of data. These characteristics are applicable
in many activities that human find laborious or repetitive, as is the case of
the analysis of scientific literature. Manually preparing and writing a
systematic literature review (SLR) takes considerable time and effort, since it
requires planning a strategy, conducting the literature search and analysis,
and reporting the findings. Depending on the area under study, the number of
papers retrieved can be of hundreds or thousands, meaning that filtering those
relevant ones and extracting the key information becomes a costly and
error-prone process. However, some of the involved tasks are repetitive and,
therefore, subject to automation by means of AI. In this paper, we present a
survey of AI techniques proposed in the last 15 years to help researchers
conduct systematic analyses of scientific literature. We describe the tasks
currently supported, the types of algorithms applied, and available tools
proposed in 34 primary studies. This survey also provides a historical
perspective of the evolution of the field and the role that humans can play in
an increasingly automated SLR process.
\\ ( https://arxiv.org/abs/2401.10917 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10934 (*cross-listing*)
Date: Wed, 17 Jan 2024 03:27:39 GMT   (27209kb,D)

Title: A New Creative Generation Pipeline for Click-Through Rate with Stable
  Diffusion Model
Authors: Hao Yang, Jianxin Yuan, Shuai Yang, Linhe Xu, Shuo Yuan, Yifan Zeng
Categories: cs.IR cs.AI
\\
  In online advertising scenario, sellers often create multiple creatives to
provide comprehensive demonstrations, making it essential to present the most
appealing design to maximize the Click-Through Rate (CTR). However, sellers
generally struggle to consider users preferences for creative design, leading
to the relatively lower aesthetics and quantities compared to Artificial
Intelligence (AI)-based approaches. Traditional AI-based approaches still face
the same problem of not considering user information while having limited
aesthetic knowledge from designers. In fact that fusing the user information,
the generated creatives can be more attractive because different users may have
different preferences. To optimize the results, the generated creatives in
traditional methods are then ranked by another module named creative ranking
model. The ranking model can predict the CTR score for each creative
considering user features. However, the two above stages are regarded as two
different tasks and are optimized separately. In this paper, we proposed a new
automated Creative Generation pipeline for Click-Through Rate (CG4CTR) with the
goal of improving CTR during the creative generation stage. Our contributions
have 4 parts: 1) The inpainting mode in stable diffusion is firstly applied to
creative generation task in online advertising scene. A self-cyclic generation
pipeline is proposed to ensure the convergence of training. 2) Prompt model is
designed to generate individualized creatives for different user groups, which
can further improve the diversity and quality. 3) Reward model comprehensively
considers the multimodal features of image and text to improve the
effectiveness of creative ranking task, and it is also critical in self-cyclic
pipeline. 4) The significant benefits obtained in online and offline
experiments verify the significance of our proposed method.
\\ ( https://arxiv.org/abs/2401.10934 ,  27209kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10935 (*cross-listing*)
Date: Wed, 17 Jan 2024 08:10:35 GMT   (28801kb,D)

Title: SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents
Authors: Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing
  Zhang, Zhiyong Wu
Categories: cs.HC cs.AI
\\
  Graphical User Interface (GUI) agents are designed to automate complex tasks
on digital devices, such as smartphones and desktops. Most existing GUI agents
interact with the environment through extracted structured data, which can be
notably lengthy (e.g., HTML) and occasionally inaccessible (e.g., on desktops).
To alleviate this issue, we propose a visual GUI agent -- SeeClick, which only
relies on screenshots for task automation. In our preliminary study, we have
discovered a key challenge in developing visual GUI agents: GUI grounding --
the capacity to accurately locate screen elements based on instructions. To
tackle this challenge, we propose to enhance SeeClick with GUI grounding
pre-training and devise a method to automate the curation of GUI grounding
data. Along with the efforts above, we have also created ScreenSpot, the first
realistic GUI grounding dataset that encompasses mobile, desktop, and web
environments. After pre-training, SeeClick demonstrates significant improvement
in ScreenSpot over various baselines. Moreover, comprehensive evaluations on
three widely used benchmarks consistently support our finding that advancements
in GUI grounding directly correlate with enhanced performance in downstream GUI
agent tasks. The model, data and code are available at
https://github.com/njucckevin/SeeClick.
\\ ( https://arxiv.org/abs/2401.10935 ,  28801kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10937 (*cross-listing*)
Date: Wed, 17 Jan 2024 11:36:38 GMT   (243kb)

Title: Subjective Causality
Authors: Joseph Y. Halpern, Evan Piermont
Categories: econ.TH cs.AI cs.LO
\\
  We show that it is possible to understand and identify a decision maker's
subjective causal judgements by observing her preferences over interventions.
Following Pearl [2000], we represent causality using causal models (also called
structural equations models), where the world is described by a collection of
variables, related by equations. We show that if a preference relation over
interventions satisfies certain axioms (related to standard axioms regarding
counterfactuals), then we can define (i) a causal model, (ii) a probability
capturing the decision-maker's uncertainty regarding the external factors in
the world and (iii) a utility on outcomes such that each intervention is
associated with an expected utility and such that intervention $A$ is preferred
to $B$ iff the expected utility of $A$ is greater than that of $B$. In
addition, we characterize when the causal model is unique. Thus, our results
allow a modeler to test the hypothesis that a decision maker's preferences are
consistent with some causal model and to identify causal judgements from
observed behavior.
\\ ( https://arxiv.org/abs/2401.10937 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10942 (*cross-listing*)
Date: Wed, 17 Jan 2024 18:35:44 GMT   (96kb)

Title: Machine Unlearning for Recommendation Systems: An Insight
Authors: Bhavika Sachdeva, Harshita Rathee, Sristi, Arun Sharma, Witold
  Wydma\'nski
Categories: cs.IR cs.AI cs.LG
Comments: In Proceedings of 7th INTERNATIONAL CONFERENCE ON INNOVATIVE
  COMPUTING AND COMMUNICATION 2024 (https://icicc-conf.com/)
\\
  This review explores machine unlearning (MUL) in recommendation systems,
addressing adaptability, personalization, privacy, and bias challenges. Unlike
traditional models, MUL dynamically adjusts system knowledge based on shifts in
user preferences and ethical considerations. The paper critically examines
MUL's basics, real-world applications, and challenges like algorithmic
transparency. It sifts through literature, offering insights into how MUL could
transform recommendations, discussing user trust, and suggesting paths for
future research in responsible and user-focused artificial intelligence (AI).
The document guides researchers through challenges involving the trade-off
between personalization and privacy, encouraging contributions to meet
practical demands for targeted data removal. Emphasizing MUL's role in secure
and adaptive machine learning, the paper proposes ways to push its boundaries.
The novelty of this paper lies in its exploration of the limitations of the
methods, which highlights exciting prospects for advancing the field.
\\ ( https://arxiv.org/abs/2401.10942 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10946 (*cross-listing*)
Date: Thu, 18 Jan 2024 10:58:27 GMT   (1775kb,D)

Title: Self context-aware emotion perception on human-robot interaction
Authors: Zihan Lin, Francisco Cruz, and Eduardo Benitez Sandoval
Categories: cs.HC cs.AI
Comments: Australasian Conference on Robotics and Automation (ACRA). 2023
\\
  Emotion recognition plays a crucial role in various domains of human-robot
interaction. In long-term interactions with humans, robots need to respond
continuously and accurately, however, the mainstream emotion recognition
methods mostly focus on short-term emotion recognition, disregarding the
context in which emotions are perceived. Humans consider that contextual
information and different contexts can lead to completely different emotional
expressions. In this paper, we introduce self context-aware model (SCAM) that
employs a two-dimensional emotion coordinate system for anchoring and
re-labeling distinct emotions. Simultaneously, it incorporates its distinctive
information retention structure and contextual loss. This approach has yielded
significant improvements across audio, video, and multimodal. In the auditory
modality, there has been a notable enhancement in accuracy, rising from 63.10%
to 72.46%. Similarly, the visual modality has demonstrated improved accuracy,
increasing from 77.03% to 80.82%. In the multimodal, accuracy has experienced
an elevation from 77.48% to 78.93%. In the future, we will validate the
reliability and usability of SCAM on robots through psychology experiments.
\\ ( https://arxiv.org/abs/2401.10946 ,  1775kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10956 (*cross-listing*)
Date: Fri, 19 Jan 2024 05:54:35 GMT   (608kb,D)

Title: AI Revolution on Chat Bot: Evidence from a Randomized Controlled
  Experiment
Authors: Sida Peng, Wojciech Swiatek, Allen Gao, Paul Cullivan, Haoge Chang
Categories: cs.HC cs.AI cs.IR
\\
  In recent years, generative AI has undergone major advancements,
demonstrating significant promise in augmenting human productivity. Notably,
large language models (LLM), with ChatGPT-4 as an example, have drawn
considerable attention. Numerous articles have examined the impact of LLM-based
tools on human productivity in lab settings and designed tasks or in
observational studies. Despite recent advances, field experiments applying
LLM-based tools in realistic settings are limited. This paper presents the
findings of a field randomized controlled trial assessing the effectiveness of
LLM-based tools in providing unmonitored support services for information
retrieval.
\\ ( https://arxiv.org/abs/2401.10956 ,  608kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10965 (*cross-listing*)
Date: Fri, 19 Jan 2024 12:47:27 GMT   (255kb,D)

Title: Decentralizing Coordination in Open Vehicle Fleets for Scalable and
  Dynamic Task Allocation
Authors: Marin Lujak, Stefano Giordani, Andrea Omicini, Sascha Ossowski
Categories: cs.MA cs.AI
ACM-class: I.2.0
Journal-ref: Complexity, Volume 2020, Article ID 1047369
DOI: 10.1155/2020/1047369
\\
  One of the major challenges in the coordination of large, open,
collaborative, and commercial vehicle fleets is dynamic task allocation.
Self-concerned individually rational vehicle drivers have both local and global
objectives, which require coordination using some fair and efficient task
allocation method. In this paper, we review the literature on scalable and
dynamic task allocation focusing on deterministic and dynamic two-dimensional
linear assignment problems. We focus on multiagent system representation of
open vehicle fleets where dynamically appearing vehicles are represented by
software agents that should be allocated to a set of dynamically appearing
tasks. We give a comparison and critical analysis of recent research results
focusing on centralized, distributed, and decentralized solution approaches.
Moreover, we propose mathematical models for dynamic versions of the following
assignment problems well known in combinatorial optimization: the assignment
problem, bottleneck assignment problem, fair matching problem, dynamic minimum
deviation assignment problem, $\sum_{k}$-assignment problem, the semiassignment
problem, the assignment problem with side constraints, and the assignment
problem while recognizing agent qualification; all while considering the main
aspect of open vehicle fleets: random arrival of tasks and vehicles (agents)
that may become available after assisting previous tasks or by participating in
the fleet at times based on individual interest.
\\ ( https://arxiv.org/abs/2401.10965 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11002 (*cross-listing*)
Date: Fri, 19 Jan 2024 19:42:38 GMT   (37036kb,D)

Title: Fast Registration of Photorealistic Avatars for VR Facial Animation
Authors: Chaitanya Patel, Shaojie Bai, Te-Li Wang, Jason Saragih, Shih-En Wei
Categories: cs.CV cs.AI
Comments: Project page: https://chaitanya100100.github.io/FastRegistration/
\\
  Virtual Reality (VR) bares promise of social interactions that can feel more
immersive than other media. Key to this is the ability to accurately animate a
photorealistic avatar of one's likeness while wearing a VR headset. Although
high quality registration of person-specific avatars to headset-mounted camera
(HMC) images is possible in an offline setting, the performance of generic
realtime models are significantly degraded. Online registration is also
challenging due to oblique camera views and differences in modality. In this
work, we first show that the domain gap between the avatar and headset-camera
images is one of the primary sources of difficulty, where a transformer-based
architecture achieves high accuracy on domain-consistent data, but degrades
when the domain-gap is re-introduced. Building on this finding, we develop a
system design that decouples the problem into two parts: 1) an iterative
refinement module that takes in-domain inputs, and 2) a generic avatar-guided
image-to-image style transfer module that is conditioned on current estimation
of expression and head pose. These two modules reinforce each other, as image
style transfer becomes easier when close-to-ground-truth examples are shown,
and better domain-gap removal helps registration. Our system produces
high-quality results efficiently, obviating the need for costly offline
registration to generate personalized labels. We validate the accuracy and
efficiency of our approach through extensive experiments on a commodity
headset, demonstrating significant improvements over direct regression methods
as well as offline registration.
\\ ( https://arxiv.org/abs/2401.11002 ,  37036kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11061 (*cross-listing*)
Date: Fri, 19 Jan 2024 23:34:48 GMT   (23835kb,D)

Title: PhotoBot: Reference-Guided Interactive Photography via Natural Language
Authors: Oliver Limoyo, Jimmy Li, Dmitriy Rivkin, Jonathan Kelly, and Gregory
  Dudek
Categories: cs.CV cs.AI cs.RO
\\
  We introduce PhotoBot, a framework for automated photo acquisition based on
an interplay between high-level human language guidance and a robot
photographer. We propose to communicate photography suggestions to the user via
a reference picture that is retrieved from a curated gallery. We exploit a
visual language model (VLM) and an object detector to characterize reference
pictures via textual descriptions and use a large language model (LLM) to
retrieve relevant reference pictures based on a user's language query through
text-based reasoning. To correspond the reference picture and the observed
scene, we exploit pre-trained features from a vision transformer capable of
capturing semantic similarity across significantly varying images. Using these
features, we compute pose adjustments for an RGB-D camera by solving a
Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world
manipulator equipped with a wrist camera. Our user studies show that photos
taken by PhotoBot are often more aesthetically pleasing than those taken by
users themselves, as measured by human feedback.
\\ ( https://arxiv.org/abs/2401.11061 ,  23835kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11085 (*cross-listing*)
Date: Sat, 20 Jan 2024 02:21:41 GMT   (3316kb,D)

Title: Adaptive Global-Local Representation Learning and Selection for
  Cross-Domain Facial Expression Recognition
Authors: Yuefang Gao, Yuhao Xie, Zeke Zexi Hu, Tianshui Chen, Liang Lin
Categories: cs.CV cs.AI
\\
  Domain shift poses a significant challenge in Cross-Domain Facial Expression
Recognition (CD-FER) due to the distribution variation across different
domains. Current works mainly focus on learning domain-invariant features
through global feature adaptation, while neglecting the transferability of
local features. Additionally, these methods lack discriminative supervision
during training on target datasets, resulting in deteriorated feature
representation in target domain. To address these limitations, we propose an
Adaptive Global-Local Representation Learning and Selection (AGLRLS) framework.
The framework incorporates global-local adversarial adaptation and
semantic-aware pseudo label generation to enhance the learning of
domain-invariant and discriminative feature during training. Meanwhile, a
global-local prediction consistency learning is introduced to improve
classification results during inference. Specifically, the framework consists
of separate global-local adversarial learning modules that learn
domain-invariant global and local features independently. We also design a
semantic-aware pseudo label generation module, which computes semantic labels
based on global and local features. Moreover, a novel dynamic threshold
strategy is employed to learn the optimal thresholds by leveraging independent
prediction of global and local features, ensuring filtering out the unreliable
pseudo labels while retaining reliable ones. These labels are utilized for
model optimization through the adversarial learning process in an end-to-end
manner. During inference, a global-local prediction consistency module is
developed to automatically learn an optimal result from multiple predictions.
We conduct comprehensive experiments and analysis based on a fair evaluation
benchmark. The results demonstrate that the proposed framework outperforms the
current competing methods by a substantial margin.
\\ ( https://arxiv.org/abs/2401.11085 ,  3316kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11089 (*cross-listing*)
Date: Sat, 20 Jan 2024 02:38:21 GMT   (530kb,D)

Title: FedRKG: A Privacy-preserving Federated Recommendation Framework via
  Knowledge Graph Enhancement
Authors: Dezhong Yao and Tongtong Liu and Qi Cao and Hai Jin
Categories: cs.CR cs.AI cs.DC cs.IR
\\
  Federated Learning (FL) has emerged as a promising approach for preserving
data privacy in recommendation systems by training models locally. Recently,
Graph Neural Networks (GNN) have gained popularity in recommendation tasks due
to their ability to capture high-order interactions between users and items.
However, privacy concerns prevent the global sharing of the entire user-item
graph. To address this limitation, some methods create pseudo-interacted items
or users in the graph to compensate for missing information for each client.
Unfortunately, these methods introduce random noise and raise privacy concerns.
In this paper, we propose FedRKG, a novel federated recommendation system,
where a global knowledge graph (KG) is constructed and maintained on the server
using publicly available item information, enabling higher-order user-item
interactions. On the client side, a relation-aware GNN model leverages diverse
KG relationships. To protect local interaction items and obscure gradients, we
employ pseudo-labeling and Local Differential Privacy (LDP). Extensive
experiments conducted on three real-world datasets demonstrate the competitive
performance of our approach compared to centralized algorithms while ensuring
privacy preservation. Moreover, FedRKG achieves an average accuracy improvement
of 4% compared to existing federated learning baselines.
\\ ( https://arxiv.org/abs/2401.11089 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11140 (*cross-listing*)
Date: Sat, 20 Jan 2024 06:31:30 GMT   (225kb,D)

Title: Stability Plasticity Decoupled Fine-tuning For Few-shot end-to-end
  Object Detection
Authors: Yuantao Yin, Ping Yin
Categories: cs.CV cs.AI
\\
  Few-shot object detection(FSOD) aims to design methods to adapt object
detectors efficiently with only few annotated samples. Fine-tuning has been
shown to be an effective and practical approach. However, previous works often
take the classical base-novel two stage fine-tuning procedure but ignore the
implicit stability-plasticity contradiction among different modules.
Specifically, the random re-initialized classifiers need more plasticity to
adapt to novel samples. The other modules inheriting pre-trained weights demand
more stability to reserve their class-agnostic knowledge. Regular fine-tuning
which couples the optimization of these two parts hurts the model
generalization in FSOD scenarios. In this paper, we find that this problem is
prominent in the end-to-end object detector Sparse R-CNN for its
multi-classifier cascaded architecture. We propose to mitigate this
contradiction by a new three-stage fine-tuning procedure by introducing an
addtional plasticity classifier fine-tuning(PCF) stage. We further design the
multi-source ensemble(ME) technique to enhance the generalization of the model
in the final fine-tuning stage. Extensive experiments verify that our method is
effective in regularizing Sparse R-CNN, outperforming previous methods in the
FSOD benchmark.
\\ ( https://arxiv.org/abs/2401.11140 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11156 (*cross-listing*)
Date: Sat, 20 Jan 2024 07:30:22 GMT   (2175kb,D)

Title: Generalizing Speaker Verification for Spoof Awareness in the Embedding
  Space
Authors: Xuechen Liu, Md Sahidullah, Kong Aik Lee, Tomi Kinnunen
Categories: cs.CR cs.AI cs.SD eess.AS
Comments: To appear in IEEE/ACM Transactions on Audio, Speech, and Language
  Processing
\\
  It is now well-known that automatic speaker verification (ASV) systems can be
spoofed using various types of adversaries. The usual approach to counteract
ASV systems against such attacks is to develop a separate spoofing
countermeasure (CM) module to classify speech input either as a bonafide, or a
spoofed utterance. Nevertheless, such a design requires additional computation
and utilization efforts at the authentication stage. An alternative strategy
involves a single monolithic ASV system designed to handle both zero-effort
imposter (non-targets) and spoofing attacks. Such spoof-aware ASV systems have
the potential to provide stronger protections and more economic computations.
To this end, we propose to generalize the standalone ASV (G-SASV) against
spoofing attacks, where we leverage limited training data from CM to enhance a
simple backend in the embedding space, without the involvement of a separate CM
module during the test (authentication) phase. We propose a novel yet simple
backend classifier based on deep neural networks and conduct the study via
domain adaptation and multi-task integration of spoof embeddings at the
training stage. Experiments are conducted on the ASVspoof 2019 logical access
dataset, where we improve the performance of statistical ASV backends on the
joint (bonafide and spoofed) and spoofed conditions by a maximum of 36.2% and
49.8% in terms of equal error rates, respectively.
\\ ( https://arxiv.org/abs/2401.11156 ,  2175kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11174 (*cross-listing*)
Date: Sat, 20 Jan 2024 09:09:52 GMT   (20245kb,D)

Title: Pixel-Wise Recognition for Holistic Surgical Scene Understanding
Authors: Nicol\'as Ayobi and Santiago Rodr\'iguez and Alejandra P\'erez and
  Isabela Hern\'andez and Nicol\'as Aparicio and Eug\'enie Dessevres and
  Sebasti\'an Pe\~na and Jessica Santander and Juan Ignacio Caicedo and
  Nicol\'as Fern\'andez and Pablo Arbel\'aez
Categories: cs.CV cs.AI cs.LG
Comments: Preprint submitted to Medical Image Analysis. Official extension of
  previous
  $\href{https://link.springer.com/chapter/10.1007/978-3-031-16449-1_42}{\text{MICCAI}}$
  and $\href{https://ieeexplore.ieee.org/document/10230819}{\text{ISBI}}$ orals
\\
  This paper presents the Holistic and Multi-Granular Surgical Scene
Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that
models surgical scene understanding as a hierarchy of complementary tasks with
varying levels of granularity. Our approach enables a multi-level comprehension
of surgical activities, encompassing long-term tasks such as surgical phases
and steps recognition and short-term tasks including surgical instrument
segmentation and atomic visual actions detection. To exploit our proposed
benchmark, we introduce the Transformers for Actions, Phases, Steps, and
Instrument Segmentation (TAPIS) model, a general architecture that combines a
global video feature extractor with localized region proposals from an
instrument segmentation model to tackle the multi-granularity of our benchmark.
Through extensive experimentation, we demonstrate the impact of including
segmentation annotations in short-term recognition tasks, highlight the varying
granularity requirements of each task, and establish TAPIS's superiority over
previously proposed baselines and conventional CNN-based models. Additionally,
we validate the robustness of our method across multiple public benchmarks,
confirming the reliability and applicability of our dataset. This work
represents a significant step forward in Endoscopic Vision, offering a novel
and comprehensive framework for future research towards a holistic
understanding of surgical procedures.
\\ ( https://arxiv.org/abs/2401.11174 ,  20245kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11201 (*cross-listing*)
Date: Sat, 20 Jan 2024 10:28:25 GMT   (1644kb,D)

Title: Navigating the Thin Line: Examining User Behavior in Search to Detect
  Engagement and Backfire Effects
Authors: F. M. Cau, N. Tintarev
Categories: cs.IR cs.AI
Comments: 17 pages, 3 figures, ECIR2024 (46th European Conference on
  Information Retrieval - IR4Good track)
\\
  Opinionated users often seek information that aligns with their preexisting
beliefs while dismissing contradictory evidence due to confirmation bias. This
conduct hinders their ability to consider alternative stances when searching
the web. Despite this, few studies have analyzed how the diversification of
search results on disputed topics influences the search behavior of highly
opinionated users. To this end, we present a preregistered user study (n = 257)
investigating whether different levels (low and high) of bias metrics and
search results presentation (with or without AI-predicted stances labels) can
affect the stance diversity consumption and search behavior of opinionated
users on three debated topics (i.e., atheism, intellectual property rights, and
school uniforms). Our results show that exposing participants to
(counter-attitudinally) biased search results increases their consumption of
attitude-opposing content, but we also found that bias was associated with a
trend toward overall fewer interactions within the search page. We also found
that 19% of users interacted with queries and search pages but did not select
any search results. When we removed these participants in a post-hoc analysis,
we found that stance labels increased the diversity of stances consumed by
users, particularly when the search results were biased. Our findings highlight
the need for future research to explore distinct search scenario settings to
gain insight into opinionated users' behavior.
\\ ( https://arxiv.org/abs/2401.11201 ,  1644kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11212 (*cross-listing*)
Date: Sat, 20 Jan 2024 11:37:44 GMT   (337kb,D)

Title: Programming Distributed Collective Processes in the eXchange Calculus
Authors: Giorgio Audrito, Roberto Casadei, Ferruccio Damiani, Gianluca Torta,
  Mirko Viroli
Categories: cs.DC cs.AI cs.MA cs.PL
Comments: 32 pages, 13 figures
ACM-class: D.1.3; F.1.1; F.4.3; I.2.11; J.7
\\
  Recent trends like the Internet of Things (IoT) suggest a vision of dense and
multi-scale deployments of computing devices in nearly all kinds of
environments. A prominent engineering challenge revolves around programming the
collective adaptive behaviour of such computational ecosystems. This requires
abstractions able to capture concepts like ensembles (dynamic groups of
cooperating devices) and collective tasks (joint activities carried out by
ensembles). In this work, we consider collections of devices interacting with
neighbours and that execute in nearly-synchronised sense-compute-interact
rounds, where the computation is given by a single program mapping sensing
values and incoming messages to output and outcoming messages. To support
programming whole computational collectives, we propose the abstraction of a
distributed collective process, which can be used to define at once the
ensemble formation logic and its collective task. We formalise the abstraction
in the eXchange Calculus (XC), a core functional language based on neighbouring
values (maps from neighbours to values) where state and interaction is handled
through a single primitive, exchange, and provide a corresponding
implementation in the FCPP language. Then, we exercise distributed collective
processes using two case studies: multi-hop message propagation and distributed
monitoring of spatial properties. Finally, we discuss the features of the
abstraction and its suitability for different kinds of distributed computing
applications.
\\ ( https://arxiv.org/abs/2401.11212 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11249 (*cross-listing*)
Date: Sat, 20 Jan 2024 15:02:56 GMT   (2114kb)

Title: Evaluating if trust and personal information privacy concerns are
  barriers to using health insurance that explicitly utilizes AI
Authors: Alex Zarifis, Peter Kawalek and Aida Azadegan
Categories: cs.CY cs.AI
Comments: Journal of Internet Commerce (2021)
ACM-class: H.0; A.0; K.4; K.6
DOI: 10.1080/15332861.2020.1832817
\\
  Trust and privacy have emerged as significant concerns in online
transactions. Sharing information on health is especially sensitive but it is
necessary for purchasing and utilizing health insurance. Evidence shows that
consumers are increasingly comfortable with technology in place of humans, but
the expanding use of AI potentially changes this. This research explores
whether trust and privacy concern are barriers to the adoption of AI in health
insurance. Two scenarios are compared: The first scenario has limited AI that
is not in the interface and its presence is not explicitly revealed to the
consumer. In the second scenario there is an AI interface and AI evaluation,
and this is explicitly revealed to the consumer. The two scenarios were modeled
and compared using SEM PLS-MGA. The findings show that trust is significantly
lower in the second scenario where AI is visible. Privacy concerns are higher
with AI but the difference is not statistically significant within the model.
\\ ( https://arxiv.org/abs/2401.11249 ,  2114kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11257 (*cross-listing*)
Date: Sat, 20 Jan 2024 15:34:51 GMT   (7502kb,D)

Title: Measuring Policy Distance for Multi-Agent Reinforcement Learning
Authors: Tianyi Hu, Zhiqiang Pu, Xiaolin Ai, Tenghai Qiu, Jianqiang Yi
Categories: cs.MA cs.AI
Comments: 9 pages, 6 figures
\\
  Diversity plays a crucial role in improving the performance of multi-agent
reinforcement learning (MARL). Currently, many diversity-based methods have
been developed to overcome the drawbacks of excessive parameter sharing in
traditional MARL. However, there remains a lack of a general metric to quantify
policy differences among agents. Such a metric would not only facilitate the
evaluation of the diversity evolution in multi-agent systems, but also provide
guidance for the design of diversity-based MARL algorithms. In this paper, we
propose the multi-agent policy distance (MAPD), a general tool for measuring
policy differences in MARL. By learning the conditional representations of
agents' decisions, MAPD can computes the policy distance between any pair of
agents. Furthermore, we extend MAPD to a customizable version, which can
quantify differences among agent policies on specified aspects. Based on the
online deployment of MAPD, we design a multi-agent dynamic parameter sharing
(MADPS) algorithm as an example of the MAPD's applications. Extensive
experiments demonstrate that our method is effective in measuring differences
in agent policies and specific behavioral tendencies. Moreover, in comparison
to other methods of parameter sharing, MADPS exhibits superior performance.
\\ ( https://arxiv.org/abs/2401.11257 ,  7502kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11284 (*cross-listing*)
Date: Sat, 20 Jan 2024 17:32:52 GMT   (20526kb,D)

Title: Evaluating Driver Readiness in Conditionally Automated Vehicles from
  Eye-Tracking Data and Head Pose
Authors: Mostafa Kazemi, Mahdi Rezaei, Mohsen Azarmi
Categories: cs.CV cs.AI cs.NE
\\
  As automated driving technology advances, the role of the driver to resume
control of the vehicle in conditionally automated vehicles becomes increasingly
critical. In the SAE Level 3 or partly automated vehicles, the driver needs to
be available and ready to intervene when necessary. This makes it essential to
evaluate their readiness accurately. This article presents a comprehensive
analysis of driver readiness assessment by combining head pose features and
eye-tracking data. The study explores the effectiveness of predictive models in
evaluating driver readiness, addressing the challenges of dataset limitations
and limited ground truth labels. Machine learning techniques, including LSTM
architectures, are utilised to model driver readiness based on the
Spatio-temporal status of the driver's head pose and eye gaze. The experiments
in this article revealed that a Bidirectional LSTM architecture, combining both
feature sets, achieves a mean absolute error of 0.363 on the DMD dataset,
demonstrating superior performance in assessing driver readiness. The modular
architecture of the proposed model also allows the integration of additional
driver-specific features, such as steering wheel activity, enhancing its
adaptability and real-world applicability.
\\ ( https://arxiv.org/abs/2401.11284 ,  20526kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11314 (*cross-listing*)
Date: Sat, 20 Jan 2024 20:14:42 GMT   (3068kb,D)

Title: CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming
  Assistant that Balances Student and Educator Needs
Authors: Majeed Kazemitabaar, Runlong Ye, Xiaoning Wang, Austin Z. Henley, Paul
  Denny, Michelle Craig, Tovi Grossman
Categories: cs.HC cs.AI
Comments: Accepted for publication in the Proceedings of the 2024 CHI
  Conference on Human Factors in Computing Systems (CHI '24), May 11--16, 2024,
  in Honolulu, USA. The paper includes 17 pages, 8 figures, 2 tables, along
  with a 2-page appendix
\\
  Timely, personalized feedback is essential for students learning programming,
especially as class sizes expand. LLM-based tools like ChatGPT offer instant
support, but reveal direct answers with code, which may hinder deep conceptual
engagement. We developed CodeAid, an LLM-based programming assistant delivering
helpful, technically correct responses, without revealing code solutions. For
example, CodeAid can answer conceptual questions, generate pseudo-code with
line-by-line explanations, and annotate student's incorrect code with fix
suggestions. We deployed CodeAid in a programming class of 700 students for a
12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed,
further enriched by weekly surveys, and 22 student interviews. We then
interviewed eight programming educators to gain further insights on CodeAid.
Findings revealed students primarily used CodeAid for conceptual understanding
and debugging, although a minority tried to obtain direct code. Educators
appreciated CodeAid's educational approach, and expressed concerns about
occasional incorrect feedback and students defaulting to ChatGPT.
\\ ( https://arxiv.org/abs/2401.11314 ,  3068kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11337 (*cross-listing*)
Date: Sat, 20 Jan 2024 22:04:28 GMT   (15350kb,D)

Title: Prompting Large Vision-Language Models for Compositional Reasoning
Authors: Timothy Ossowski, Ming Jiang, Junjie Hu
Categories: cs.CV cs.AI
\\
  Vision-language models such as CLIP have shown impressive capabilities in
encoding texts and images into aligned embeddings, enabling the retrieval of
multimodal data in a shared embedding space. However, these embedding-based
models still face challenges in effectively matching images and texts with
similar visio-linguistic compositionality, as evidenced by their performance on
the recent Winoground dataset. In this paper, we argue that this limitation
stems from two factors: the use of single vector representations for complex
multimodal data, and the absence of step-by-step reasoning in these
embedding-based methods. To address this issue, we make an exploratory step
using a novel generative method that prompts large vision-language models
(e.g., GPT-4) to depict images and perform compositional reasoning. Our method
outperforms other embedding-based methods on the Winoground dataset, and
obtains further improvement of up to 10% accuracy when enhanced with the
optimal description.
\\ ( https://arxiv.org/abs/2401.11337 ,  15350kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11351 (*cross-listing*)
Date: Sun, 21 Jan 2024 00:19:16 GMT   (1279kb,D)

Title: Quantum Machine Learning: from NISQ to Fault Tolerance
Authors: Yunfei Wang, Junyu Liu
Categories: quant-ph cs.AI cs.LG stat.ML
Comments: 28 pages. Invited review
\\
  Quantum machine learning, which involves running machine learning algorithms
on quantum devices, has garnered significant attention in both academic and
business circles. In this paper, we offer a comprehensive and unbiased review
of the various concepts that have emerged in the field of quantum machine
learning. This includes techniques used in Noisy Intermediate-Scale Quantum
(NISQ) technologies and approaches for algorithms compatible with
fault-tolerant quantum computing hardware. Our review covers fundamental
concepts, algorithms, and the statistical learning theory pertinent to quantum
machine learning.
\\ ( https://arxiv.org/abs/2401.11351 ,  1279kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11361 (*cross-listing*)
Date: Sun, 21 Jan 2024 01:18:08 GMT   (1011kb)

Title: Revolutionizing API Documentation through Summarization
Authors: AmirHossein Naghshzan, Sylvie Ratte
Categories: cs.SE cs.AI cs.CL
Comments: arXiv admin note: text overlap with arXiv:2308.09070
\\
  This study tackles the challenges associated with interpreting Application
Programming Interface (API) documentation, an integral aspect of software
development. Official API documentation, while essential, can be lengthy and
challenging to navigate, prompting developers to seek unofficial sources such
as Stack Overflow. Leveraging the vast user-generated content on Stack
Overflow, including code snippets and discussions, we employ BERTopic and
extractive summarization to automatically generate concise and informative API
summaries. These summaries encompass key insights like general usage, common
developer issues, and potential solutions, sourced from the wealth of knowledge
on Stack Overflow. Software developers evaluate these summaries for
performance, coherence, and interoperability, providing valuable feedback on
the practicality of our approach.
\\ ( https://arxiv.org/abs/2401.11361 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11370 (*cross-listing*)
Date: Sun, 21 Jan 2024 02:07:34 GMT   (695kb)

Title: Self-sustaining Software Systems (S4): Towards Improved Interpretability
  and Adaptation
Authors: Christian Cabrera and Andrei Paleyes and Neil D. Lawrence
Categories: cs.SE cs.AI cs.SY eess.SY
Comments: Accepted at The 1st International Workshop New Trends in Software
  Architecture (SATrends) 2024
\\
  Software systems impact society at different levels as they pervasively solve
real-world problems. Modern software systems are often so sophisticated that
their complexity exceeds the limits of human comprehension. These systems must
respond to changing goals, dynamic data, unexpected failures, and security
threats, among other variable factors in real-world environments. Systems'
complexity challenges their interpretability and requires autonomous responses
to dynamic changes. Two main research areas explore autonomous systems'
responses: evolutionary computing and autonomic computing. Evolutionary
computing focuses on software improvement based on iterative modifications to
the source code. Autonomic computing focuses on optimising systems' performance
by changing their structure, behaviour, or environment variables. Approaches
from both areas rely on feedback loops that accumulate knowledge from the
system interactions to inform autonomous decision-making. However, this
knowledge is often limited, constraining the systems' interpretability and
adaptability. This paper proposes a new concept for interpretable and adaptable
software systems: self-sustaining software systems (S4). S4 builds knowledge
loops between all available knowledge sources that define modern software
systems to improve their interpretability and adaptability. This paper
introduces and discusses the S4 concept.
\\ ( https://arxiv.org/abs/2401.11370 ,  695kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11414 (*cross-listing*)
Date: Sun, 21 Jan 2024 06:47:33 GMT   (5773kb,D)

Title: S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching
  for Autonomous Driving
Authors: Zhiyuan Wu, Yi Feng, Chuang-Wei Liu, Fisher Yu, Qijun Chen, Rui Fan
Categories: cs.CV cs.AI cs.RO
Comments: accepted to IEEE Trans. on Intelligent Vehicles (T-IV)
DOI: 10.1109/TIV.2024.3357056
\\
  Semantic segmentation and stereo matching are two essential components of 3D
environmental perception systems for autonomous driving. Nevertheless,
conventional approaches often address these two problems independently,
employing separate models for each task. This approach poses practical
limitations in real-world scenarios, particularly when computational resources
are scarce or real-time performance is imperative. Hence, in this article, we
introduce S$^3$M-Net, a novel joint learning framework developed to perform
semantic segmentation and stereo matching simultaneously. Specifically,
S$^3$M-Net shares the features extracted from RGB images between both tasks,
resulting in an improved overall scene understanding capability. This feature
sharing process is realized using a feature fusion adaption (FFA) module, which
effectively transforms the shared features into semantic space and subsequently
fuses them with the encoded disparity features. The entire joint learning
framework is trained by minimizing a novel semantic consistency-guided (SCG)
loss, which places emphasis on the structural consistency in both tasks.
Extensive experimental results conducted on the vKITTI2 and KITTI datasets
demonstrate the effectiveness of our proposed joint learning framework and its
superior performance compared to other state-of-the-art single-task networks.
Our project webpage is accessible at mias.group/S3M-Net.
\\ ( https://arxiv.org/abs/2401.11414 ,  5773kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11439 (*cross-listing*)
Date: Sun, 21 Jan 2024 09:39:11 GMT   (4381kb,D)

Title: General Flow as Foundation Affordance for Scalable Robot Learning
Authors: Chengbo Yuan, Chuan Wen, Tong Zhang, Yang Gao
Categories: cs.RO cs.AI cs.CV
\\
  We address the challenge of acquiring real-world manipulation skills with a
scalable framework.Inspired by the success of large-scale auto-regressive
prediction in Large Language Models (LLMs), we hold the belief that identifying
an appropriate prediction target capable of leveraging large-scale datasets is
crucial for achieving efficient and universal learning. Therefore, we propose
to utilize flow, which represents the future trajectories of 3D points on
objects of interest, as an ideal prediction target in robot learning. To
exploit scalable data resources, we turn our attention to cross-embodiment
datasets. We develop, for the first time, a language-conditioned prediction
model directly from large-scale RGBD human video datasets. Our predicted flow
offers actionable geometric and physics guidance, thus facilitating stable
zero-shot skill transfer in real-world scenarios.We deploy our method with a
policy based on closed-loop flow prediction. Remarkably, without any additional
training, our method achieves an impressive 81% success rate in human-to-robot
skill transfer, covering 18 tasks in 6 scenes. Our framework features the
following benefits: (1) scalability: leveraging cross-embodiment data
resources; (2) universality: multiple object categories, including rigid,
articulated, and soft bodies; (3) stable skill transfer: providing actionable
guidance with a small inference domain-gap. These lead to a new pathway towards
scalable general robot learning. Data, code, and model weights will be made
publicly available.
\\ ( https://arxiv.org/abs/2401.11439 ,  4381kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11459 (*cross-listing*)
Date: Sun, 21 Jan 2024 10:48:08 GMT   (30714kb,D)

Title: AttentionLego: An Open-Source Building Block For Spatially-Scalable
  Large Language Model Accelerator With Processing-In-Memory Technology
Authors: Rongqing Cong, Wenyang He, Mingxuan Li, Bangning Luo, Zebin Yang,
  Yuchao Yang, Ru Huang, Bonan Yan
Categories: cs.AR cs.AI cs.LG
Comments: for associated source codes, see https://bonany.cc/attentionleg
\\
  Large language models (LLMs) with Transformer architectures have become
phenomenal in natural language processing, multimodal generative artificial
intelligence, and agent-oriented artificial intelligence. The self-attention
module is the most dominating sub-structure inside Transformer-based LLMs.
Computation using general-purpose graphics processing units (GPUs) inflicts
reckless demand for I/O bandwidth for transferring intermediate calculation
results between memories and processing units. To tackle this challenge, this
work develops a fully customized vanilla self-attention accelerator,
AttentionLego, as the basic building block for constructing spatially
expandable LLM processors. AttentionLego provides basic implementation with
fully-customized digital logic incorporating Processing-In-Memory (PIM)
technology. It is based on PIM-based matrix-vector multiplication and look-up
table-based Softmax design. The open-source code is available online:
https://bonany.cc/attentionleg.
\\ ( https://arxiv.org/abs/2401.11459 ,  30714kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11471 (*cross-listing*)
Date: Sun, 21 Jan 2024 12:19:13 GMT   (2354kb,D)

Title: LR-CNN: Lightweight Row-centric Convolutional Neural Network Training
  for Memory Reduction
Authors: Zhigang Wang, Hangyu Yang, Ning Wang, Chuanfei Xu, Jie Nie, Zhiqiang
  Wei, Yu Gu, Ge Yu
Categories: cs.DC cs.AI
\\
  In the last decade, Convolutional Neural Network with a multi-layer
architecture has advanced rapidly. However, training its complex network is
very space-consuming, since a lot of intermediate data are preserved across
layers, especially when processing high-dimension inputs with a big batch size.
That poses great challenges to the limited memory capacity of current
accelerators (e.g., GPUs). Existing efforts mitigate such bottleneck by
external auxiliary solutions with additional hardware costs, and internal
modifications with potential accuracy penalty. Differently, our analysis
reveals that computations intra- and inter-layers exhibit the spatial-temporal
weak dependency and even complete independency features. That inspires us to
break the traditional layer-by-layer (column) dataflow rule. Now operations are
novelly re-organized into rows throughout all convolution layers. This
lightweight design allows a majority of intermediate data to be removed without
any loss of accuracy. We particularly study the weak dependency between two
consecutive rows. For the resulting skewed memory consumption, we give two
solutions with different favorite scenarios. Evaluations on two representative
networks confirm the effectiveness. We also validate that our middle dataflow
optimization can be smoothly embraced by existing works for better memory
reduction.
\\ ( https://arxiv.org/abs/2401.11471 ,  2354kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11489 (*cross-listing*)
Date: Sun, 21 Jan 2024 13:30:02 GMT   (716kb)

Title: MapChange: Enhancing Semantic Change Detection with Temporal-Invariant
  Historical Maps Based on Deep Triplet Network
Authors: Yinhe Liu, Sunan Shi, Zhuo Zheng, Jue Wang, Shiqi Tian, Yanfei Zhong
Categories: cs.CV cs.AI
\\
  Semantic Change Detection (SCD) is recognized as both a crucial and
challenging task in the field of image analysis. Traditional methods for SCD
have predominantly relied on the comparison of image pairs. However, this
approach is significantly hindered by substantial imaging differences, which
arise due to variations in shooting times, atmospheric conditions, and angles.
Such discrepancies lead to two primary issues: the under-detection of minor yet
significant changes, and the generation of false alarms due to temporal
variances. These factors often result in unchanged objects appearing markedly
different in multi-temporal images. In response to these challenges, the
MapChange framework has been developed. This framework introduces a novel
paradigm that synergizes temporal-invariant historical map data with
contemporary high-resolution images. By employing this combination, the
temporal variance inherent in conventional image pair comparisons is
effectively mitigated. The efficacy of the MapChange framework has been
empirically validated through comprehensive testing on two public datasets.
These tests have demonstrated the framework's marked superiority over existing
state-of-the-art SCD methods.
\\ ( https://arxiv.org/abs/2401.11489 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11500 (*cross-listing*)
Date: Sun, 21 Jan 2024 14:10:27 GMT   (183kb,D)

Title: Integration of Large Language Models in Control of EHD Pumps for Precise
  Color Synthesis
Authors: Yanhong Peng, Ceng Zhang, Chenlong Hu, Zebing Mao
Categories: cs.RO cs.AI cs.SY eess.SY
\\
  This paper presents an innovative approach to integrating Large Language
Models (LLMs) with Arduino-controlled Electrohydrodynamic (EHD) pumps for
precise color synthesis in automation systems. We propose a novel framework
that employs fine-tuned LLMs to interpret natural language commands and convert
them into specific operational instructions for EHD pump control. This approach
aims to enhance user interaction with complex hardware systems, making it more
intuitive and efficient. The methodology involves four key steps: fine-tuning
the language model with a dataset of color specifications and corresponding
Arduino code, developing a natural language processing interface, translating
user inputs into executable Arduino code, and controlling EHD pumps for
accurate color mixing. Conceptual experiment results, based on theoretical
assumptions, indicate a high potential for accurate color synthesis, efficient
language model interpretation, and reliable EHD pump operation. This research
extends the application of LLMs beyond text-based tasks, demonstrating their
potential in industrial automation and control systems. While highlighting the
limitations and the need for real-world testing, this study opens new avenues
for AI applications in physical system control and sets a foundation for future
advancements in AI-driven automation technologies.
\\ ( https://arxiv.org/abs/2401.11500 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11596 (*cross-listing*)
Date: Sun, 21 Jan 2024 20:57:12 GMT   (242kb,D)

Title: Learning to Maximize Gains From Trade in Small Markets
Authors: Moshe Babaioff, Amitai Frey, Noam Nisan
Categories: cs.GT cs.AI cs.LG
ACM-class: F.0; I.2; I.2.6; J.4
\\
  We study the problem of designing a two-sided market (double auction) to
maximize the gains from trade (social welfare) under the constraints of
(dominant-strategy) incentive compatibility and budget-balance. Our goal is to
do so for an unknown distribution from which we are given a polynomial number
of samples. Our first result is a general impossibility for the case of
correlated distributions of values even between just one seller and two buyers,
in contrast to the case of one seller and one buyer (bilateral trade) where
this is possible. Our second result is an efficient learning algorithm for one
seller and two buyers in the case of independent distributions which is based
on a novel algorithm for computing optimal mechanisms for finitely supported
and explicitly given independent distributions. Both results rely heavily on
characterizations of (dominant-strategy) incentive compatible mechanisms that
are strongly budget-balanced.
\\ ( https://arxiv.org/abs/2401.11596 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11605 (*cross-listing*)
Date: Sun, 21 Jan 2024 21:49:49 GMT   (27875kb,D)

Title: Scalable High-Resolution Pixel-Space Image Synthesis with Hourglass
  Diffusion Transformers
Authors: Katherine Crowson and Stefan Andreas Baumann and Alex Birch and
  Tanishq Mathew Abraham and Daniel Z. Kaplan and Enrico Shippole
Categories: cs.CV cs.AI cs.LG
Comments: 20 pages, 13 figures, project page and code available at
  https://crowsonkb.github.io/hourglass-diffusion-transformers/
\\
  We present the Hourglass Diffusion Transformer (HDiT), an image generative
model that exhibits linear scaling with pixel count, supporting training at
high-resolution (e.g. $1024 \times 1024$) directly in pixel-space. Building on
the Transformer architecture, which is known to scale to billions of
parameters, it bridges the gap between the efficiency of convolutional U-Nets
and the scalability of Transformers. HDiT trains successfully without typical
high-resolution training techniques such as multiscale architectures, latent
autoencoders or self-conditioning. We demonstrate that HDiT performs
competitively with existing models on ImageNet $256^2$, and sets a new
state-of-the-art for diffusion models on FFHQ-$1024^2$.
\\ ( https://arxiv.org/abs/2401.11605 ,  27875kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11633 (*cross-listing*)
Date: Mon, 22 Jan 2024 00:00:30 GMT   (7632kb,D)

Title: Zoom-shot: Fast and Efficient Unsupervised Zero-Shot Transfer of CLIP to
  Vision Encoders with Multimodal Loss
Authors: Jordan Shipard, Arnold Wiliem, Kien Nguyen Thanh, Wei Xiang, Clinton
  Fookes
Categories: cs.CV cs.AI
Comments: 15 pages
\\
  The fusion of vision and language has brought about a transformative shift in
computer vision through the emergence of Vision-Language Models (VLMs).
However, the resource-intensive nature of existing VLMs poses a significant
challenge. We need an accessible method for developing the next generation of
VLMs. To address this issue, we propose Zoom-shot, a novel method for
transferring the zero-shot capabilities of CLIP to any pre-trained vision
encoder. We do this by exploiting the multimodal information (i.e. text and
image) present in the CLIP latent space through the use of specifically
designed multimodal loss functions. These loss functions are (1)
cycle-consistency loss and (2) our novel prompt-guided knowledge distillation
loss (PG-KD). PG-KD combines the concept of knowledge distillation with CLIP's
zero-shot classification, to capture the interactions between text and image
features. With our multimodal losses, we train a $\textbf{linear mapping}$
between the CLIP latent space and the latent space of a pre-trained vision
encoder, for only a $\textbf{single epoch}$. Furthermore, Zoom-shot is entirely
unsupervised and is trained using $\textbf{unpaired}$ data. We test the
zero-shot capabilities of a range of vision encoders augmented as new VLMs, on
coarse and fine-grained classification datasets, outperforming the previous
state-of-the-art in this problem domain. In our ablations, we find Zoom-shot
allows for a trade-off between data and compute during training; and our
state-of-the-art results can be obtained by reducing training from 20% to 1% of
the ImageNet training data with 20 epochs. All code and models are available on
GitHub.
\\ ( https://arxiv.org/abs/2401.11633 ,  7632kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11665 (*cross-listing*)
Date: Mon, 22 Jan 2024 02:54:58 GMT   (496kb,D)

Title: Accelerating Approximate Thompson Sampling with Underdamped Langevin
  Monte Carlo
Authors: Haoyang Zheng, Wei Deng, Christian Moya, Guang Lin
Categories: stat.ML cs.AI cs.LG
Comments: 50 pages, 1 figure, to appear in AISTATS 2024
\\
  Approximate Thompson sampling with Langevin Monte Carlo broadens its reach
from Gaussian posterior sampling to encompass more general smooth posteriors.
However, it still encounters scalability issues in high-dimensional problems
when demanding high accuracy. To address this, we propose an approximate
Thompson sampling strategy, utilizing underdamped Langevin Monte Carlo, where
the latter is the go-to workhorse for simulations of high-dimensional
posteriors. Based on the standard smoothness and log-concavity conditions, we
study the accelerated posterior concentration and sampling using a specific
potential function. This design improves the sample complexity for realizing
logarithmic regrets from $\mathcal{\tilde O}(d)$ to $\mathcal{\tilde
O}(\sqrt{d})$. The scalability and robustness of our algorithm are also
empirically validated through synthetic experiments in high-dimensional bandit
problems.
\\ ( https://arxiv.org/abs/2401.11665 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11674 (*cross-listing*)
Date: Mon, 22 Jan 2024 03:24:45 GMT   (2333kb,D)

Title: Memory-Efficient Prompt Tuning for Incremental Histopathology
  Classification
Authors: Yu Zhu, Kang Li, Lequan Yu, Pheng-Ann Heng
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 2024
\\
  Recent studies have made remarkable progress in histopathology
classification. Based on current successes, contemporary works proposed to
further upgrade the model towards a more generalizable and robust direction
through incrementally learning from the sequentially delivered domains. Unlike
previous parameter isolation based approaches that usually demand massive
computation resources during model updating, we present a memory-efficient
prompt tuning framework to cultivate model generalization potential in
economical memory cost. For each incoming domain, we reuse the existing
parameters of the initial classification model and attach lightweight trainable
prompts into it for customized tuning. Considering the domain heterogeneity, we
perform decoupled prompt tuning, where we adopt a domain-specific prompt for
each domain to independently investigate its distinctive characteristics, and
one domain-invariant prompt shared across all domains to continually explore
the common content embedding throughout time. All domain-specific prompts will
be appended to the prompt bank and isolated from further changes to prevent
forgetting the distinctive features of early-seen domains. While the
domain-invariant prompt will be passed on and iteratively evolve by
style-augmented prompt refining to improve model generalization capability over
time. In specific, we construct a graph with existing prompts and build a
style-augmented graph attention network to guide the domain-invariant prompt
exploring the overlapped latent embedding among all delivered domains for more
domain generic representations. We have extensively evaluated our framework
with two histopathology tasks, i.e., breast cancer metastasis classification
and epithelium-stroma tissue classification, where our approach yielded
superior performance and memory efficiency over the competing methods.
\\ ( https://arxiv.org/abs/2401.11674 ,  2333kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11699 (*cross-listing*)
Date: Mon, 18 Dec 2023 02:33:53 GMT   (1135kb)

Title: Dissecting Bias of ChatGPT in College Major Recommendations
Authors: Alex Zheng
Categories: cs.CY cs.AI
\\
  I investigate bias in terms of ChatGPT's college major recommendations for
students with various profiles, looking at demographic disparities in factors
such as race, gender, and socioeconomic status, as well as educational
disparities such as score percentiles. By constructing prompts for the ChatGPT
API, allowing the model to recommend majors based on high school student
profiles, I evaluate bias using various metrics, including the Jaccard
Coefficient, Wasserstein Metric, and STEM Disparity Score. The results of this
study reveal a significant disparity in the set of recommended college majors,
irrespective of the bias metric applied.
\\ ( https://arxiv.org/abs/2401.11699 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11705 (*cross-listing*)
Date: Mon, 22 Jan 2024 06:12:48 GMT   (146kb,D)

Title: Domain-Aware Cross-Attention for Cross-domain Recommendation
Authors: Yuhao Luo and Shiwei Ma and Mingjun Nie and Changping Peng and
  Zhangang Lin and Jingping Shao and Qianfang Xu
Categories: cs.IR cs.AI
Comments: 6 pages, 1 figure
\\
  Cross-domain recommendation (CDR) is an important method to improve
recommender system performance, especially when observations in target domains
are sparse. However, most existing cross-domain recommendations fail to fully
utilize the target domain's special features and are hard to be generalized to
new domains. The designed network is complex and is not suitable for rapid
industrial deployment. Our method introduces a two-step domain-aware
cross-attention, extracting transferable features of the source domain from
different granularity, which allows the efficient expression of both domain and
user interests. In addition, we simplify the training process, and our model
can be easily deployed on new domains. We conduct experiments on both public
datasets and industrial datasets, and the experimental results demonstrate the
effectiveness of our method. We have also deployed the model in an online
advertising system and observed significant improvements in both
Click-Through-Rate (CTR) and effective cost per mille (ECPM).
\\ ( https://arxiv.org/abs/2401.11705 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11713 (*cross-listing*)
Date: Mon, 22 Jan 2024 06:29:52 GMT   (4513kb,D)

Title: Medical Image Debiasing by Learning Adaptive Agreement from a Biased
  Council
Authors: Luyang Luo, Xin Huang, Minghao Wang, Zhuoyue Wan, Hao Chen
Categories: cs.CV cs.AI
Comments: 10 pages, 5 figures, 3 tables. Code and benchmark will be released
  via https://github.com/LLYXC/Ada-ABC/tree/main
\\
  Deep learning could be prone to learning shortcuts raised by dataset bias and
result in inaccurate, unreliable, and unfair models, which impedes its adoption
in real-world clinical applications. Despite its significance, there is a
dearth of research in the medical image classification domain to address
dataset bias. Furthermore, the bias labels are often agnostic, as identifying
biases can be laborious and depend on post-hoc interpretation. This paper
proposes learning Adaptive Agreement from a Biased Council (Ada-ABC), a
debiasing framework that does not rely on explicit bias labels to tackle
dataset bias in medical images. Ada-ABC develops a biased council consisting of
multiple classifiers optimized with generalized cross entropy loss to learn the
dataset bias. A debiasing model is then simultaneously trained under the
guidance of the biased council. Specifically, the debiasing model is required
to learn adaptive agreement with the biased council by agreeing on the
correctly predicted samples and disagreeing on the wrongly predicted samples by
the biased council. In this way, the debiasing model could learn the target
attribute on the samples without spurious correlations while also avoiding
ignoring the rich information in samples with spurious correlations. We
theoretically demonstrated that the debiasing model could learn the target
features when the biased model successfully captures dataset bias. Moreover, to
our best knowledge, we constructed the first medical debiasing benchmark from
four datasets containing seven different bias scenarios. Our extensive
experiments practically showed that our proposed Ada-ABC outperformed
competitive approaches, verifying its effectiveness in mitigating dataset bias
for medical image classification. The codes and organized benchmark datasets
will be made publicly available.
\\ ( https://arxiv.org/abs/2401.11713 ,  4513kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11719 (*cross-listing*)
Date: Mon, 22 Jan 2024 06:43:13 GMT   (4467kb,D)

Title: SFC: Shared Feature Calibration in Weakly Supervised Semantic
  Segmentation
Authors: Xinqiao Zhao, Feilong Tang, Xiaoyang Wang, Jimin Xiao
Categories: cs.CV cs.AI
\\
  Image-level weakly supervised semantic segmentation has received increasing
attention due to its low annotation cost. Existing methods mainly rely on Class
Activation Mapping (CAM) to obtain pseudo-labels for training semantic
segmentation models. In this work, we are the first to demonstrate that
long-tailed distribution in training data can cause the CAM calculated through
classifier weights over-activated for head classes and under-activated for tail
classes due to the shared features among head- and tail- classes. This degrades
pseudo-label quality and further influences final semantic segmentation
performance. To address this issue, we propose a Shared Feature Calibration
(SFC) method for CAM generation. Specifically, we leverage the class prototypes
that carry positive shared features and propose a Multi-Scaled
Distribution-Weighted (MSDW) consistency loss for narrowing the gap between the
CAMs generated through classifier weights and class prototypes during training.
The MSDW loss counterbalances over-activation and under-activation by
calibrating the shared features in head-/tail-class classifier weights.
Experimental results show that our SFC significantly improves CAM boundaries
and achieves new state-of-the-art performances. The project is available at
https://github.com/Barrett-python/SFC.
\\ ( https://arxiv.org/abs/2401.11719 ,  4467kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11723 (*cross-listing*)
Date: Mon, 22 Jan 2024 06:52:35 GMT   (41026kb,D)

Title: Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey
  and the Open Libraries Behind Them
Authors: Chao Liu, Boxi Chen, Wei Shao, Chris Zhang, Kelvin Wong, Yi Zhang
Categories: cs.CR cs.AI
\\
  The advent of the Internet of Things (IoT) has brought forth an era of
unprecedented connectivity, with an estimated 80 billion smart devices expected
to be in operation by the end of 2025. These devices facilitate a multitude of
smart applications, enhancing the quality of life and efficiency across various
domains. Machine Learning (ML) serves as a crucial technology, not only for
analyzing IoT-generated data but also for diverse applications within the IoT
ecosystem. For instance, ML finds utility in IoT device recognition, anomaly
detection, and even in uncovering malicious activities. This paper embarks on a
comprehensive exploration of the security threats arising from ML's integration
into various facets of IoT, spanning various attack types including membership
inference, adversarial evasion, reconstruction, property inference, model
extraction, and poisoning attacks. Unlike previous studies, our work offers a
holistic perspective, categorizing threats based on criteria such as adversary
models, attack targets, and key security attributes (confidentiality,
availability, and integrity). We delve into the underlying techniques of ML
attacks in IoT environment, providing a critical evaluation of their mechanisms
and impacts. Furthermore, our research thoroughly assesses 65 libraries, both
author-contributed and third-party, evaluating their role in safeguarding model
and data privacy. We emphasize the availability and usability of these
libraries, aiming to arm the community with the necessary tools to bolster
their defenses against the evolving threat landscape. Through our comprehensive
review and analysis, this paper seeks to contribute to the ongoing discourse on
ML-based IoT security, offering valuable insights and practical solutions to
secure ML models and data in the rapidly expanding field of artificial
intelligence in IoT.
\\ ( https://arxiv.org/abs/2401.11723 ,  41026kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11724 (*cross-listing*)
Date: Mon, 22 Jan 2024 06:56:52 GMT   (5859kb,D)

Title: Augmenting Prototype Network with TransMix for Few-shot Hyperspectral
  Image Classification
Authors: Chun Liu, Longwei Yang, Dongmei Dong, Zheng Li, Wei Yang, Zhigang Han,
  and Jiayao Wang
Categories: cs.CV cs.AI
\\
  Few-shot hyperspectral image classification aims to identify the classes of
each pixel in the images by only marking few of these pixels. And in order to
obtain the spatial-spectral joint features of each pixel, the fixed-size
patches centering around each pixel are often used for classification. However,
observing the classification results of existing methods, we found that
boundary patches corresponding to the pixels which are located at the boundary
of the objects in the hyperspectral images, are hard to classify. These
boundary patchs are mixed with multi-class spectral information. Inspired by
this, we propose to augment the prototype network with TransMix for few-shot
hyperspectrial image classification(APNT). While taking the prototype network
as the backbone, it adopts the transformer as feature extractor to learn the
pixel-to-pixel relation and pay different attentions to different pixels. At
the same time, instead of directly using the patches which are cut from the
hyperspectral images for training, it randomly mixs up two patches to imitate
the boundary patches and uses the synthetic patches to train the model, with
the aim to enlarge the number of hard training samples and enhance their
diversity. And by following the data agumentation technique TransMix, the
attention returned by the transformer is also used to mix up the labels of two
patches to generate better labels for synthetic patches. Compared with existing
methods, the proposed method has demonstrated sate of the art performance and
better robustness for few-shot hyperspectral image classification in our
experiments.
\\ ( https://arxiv.org/abs/2401.11724 ,  5859kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11731 (*cross-listing*)
Date: Mon, 22 Jan 2024 07:19:16 GMT   (1122kb,D)

Title: Fast and Scalable Network Slicing by Integrating Deep Learning with
  Lagrangian Methods
Authors: Tianlun Hu, Qi Liao, Qiang Liu, Antonio Massaro, Georg Carle
Categories: cs.NI cs.AI cs.LG
Comments: 6 pages, 5 figures, IEEE Global Communications Conference 2023
\\
  Network slicing is a key technique in 5G and beyond for efficiently
supporting diverse services. Many network slicing solutions rely on deep
learning to manage complex and high-dimensional resource allocation problems.
However, deep learning models suffer limited generalization and adaptability to
dynamic slicing configurations. In this paper, we propose a novel framework
that integrates constrained optimization methods and deep learning models,
resulting in strong generalization and superior approximation capability. Based
on the proposed framework, we design a new neural-assisted algorithm to
allocate radio resources to slices to maximize the network utility under
inter-slice resource constraints. The algorithm exhibits high scalability,
accommodating varying numbers of slices and slice configurations with ease. We
implement the proposed solution in a system-level network simulator and
evaluate its performance extensively by comparing it to state-of-the-art
solutions including deep reinforcement learning approaches. The numerical
results show that our solution obtains near-optimal quality-of-service
satisfaction and promising generalization performance under different network
slicing scenarios.
\\ ( https://arxiv.org/abs/2401.11731 ,  1122kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11748 (*cross-listing*)
Date: Mon, 22 Jan 2024 08:20:47 GMT   (7832kb,D)

Title: GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient
  Inversion Attacks?
Authors: Yu sun, Gaojian Xiong, Xianxun Yao, Kailang Ma, Jian Cui
Categories: cs.CR cs.AI cs.LG
Comments: 5pages, 5 figures, accepted to ICASSP 2024, not published yet
\\
  Deep gradient inversion attacks expose a serious threat to Federated Learning
(FL) by accurately recovering private data from shared gradients. However, the
state-of-the-art heavily relies on impractical assumptions to access excessive
auxiliary data, which violates the basic data partitioning principle of FL. In
this paper, a novel method, Gradient Inversion Attack using Practical Image
Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits
anomaly detection models to capture the underlying distribution from fewer
data, while GAN-based methods consume significant more data to synthesize
images. The extracted distribution is then leveraged to regulate the attack
process as Anomaly Score loss. Experimental results show that GI-PIP achieves a
16.12 dB PSNR recovery using only 3.8\% data of ImageNet, while GAN-based
methods necessitate over 70\%. Moreover, GI-PIP exhibits superior capability on
distribution generalization compared to GAN-based methods. Our approach
significantly alleviates the auxiliary data requirement on both amount and
distribution in gradient inversion attacks, hence posing more substantial
threat to real-world FL.
\\ ( https://arxiv.org/abs/2401.11748 ,  7832kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11792 (*cross-listing*)
Date: Mon, 22 Jan 2024 09:44:16 GMT   (10388kb,D)

Title: Safe and Generalized end-to-end Autonomous Driving System with
  Reinforcement Learning and Demonstrations
Authors: Zuojin Tang, Xiaoyu Chen, YongQiang Li, Jianyu Chen
Categories: cs.RO cs.AI cs.LG
\\
  An intelligent driving system should be capable of dynamically formulating
appropriate driving strategies based on the current environment and vehicle
status, while ensuring the security and reliability of the system. However,
existing methods based on reinforcement learning and imitation learning suffer
from low safety, poor generalization, and inefficient sampling. Additionally,
they cannot accurately predict future driving trajectories, and the accurate
prediction of future driving trajectories is a precondition for making optimal
decisions. To solve these problems, in this paper, we introduce a Safe and
Generalized end-to-end Autonomous Driving System (SGADS) for complex and
various scenarios. Our SGADS incorporates variational inference with
normalizing flows, enabling the intelligent vehicle to accurately predict
future driving trajectories. Moreover, we propose the formulation of robust
safety constraints. Furthermore, we combine reinforcement learning with
demonstrations to augment search process of the agent. The experimental results
demonstrate that our SGADS can significantly improve safety performance,
exhibit strong generalization, and enhance the training efficiency of
intelligent vehicles in complex urban scenarios compared to existing methods.
\\ ( https://arxiv.org/abs/2401.11792 ,  10388kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11814 (*cross-listing*)
Date: Mon, 22 Jan 2024 10:22:14 GMT   (3543kb)

Title: Symbrain: A large-scale dataset of MRI images for neonatal brain
  symmetry analysis
Authors: Arnaud Gucciardi and Safouane El Ghazouali and Francesca Venturini and
  Vida Groznik and Umberto Michelucci
Categories: cs.CV cs.AI
Comments: 7 pages, 2 figures, Dataset Paper, Medical AI
\\
  This paper presents an annotated dataset of brain MRI images designed to
advance the field of brain symmetry study. Magnetic resonance imaging (MRI) has
gained interest in analyzing brain symmetry in neonatal infants, and challenges
remain due to the vast size differences between fetal and adult brains.
Classification methods for brain structural MRI use scales and visual cues to
assess hemisphere symmetry, which can help diagnose neonatal patients by
comparing hemispheres and anatomical regions of interest in the brain. Using
the Developing Human Connectome Project dataset, this work presents a dataset
comprising cerebral images extracted as slices across selected portions of
interest for clinical evaluation . All the extracted images are annotated with
the brain's midline. All the extracted images are annotated with the brain's
midline. From the assumption that a decrease in symmetry is directly related to
possible clinical pathologies, the dataset can contribute to a more precise
diagnosis because it can be used to train deep learning model application in
neonatal cerebral MRI anomaly detection from postnatal infant scans thanks to
computer vision. Such models learn to identify and classify anomalies by
identifying potential asymmetrical patterns in medical MRI images. Furthermore,
this dataset can contribute to the research and development of methods using
the relative symmetry of the two brain hemispheres for crucial diagnosis and
treatment planning.
\\ ( https://arxiv.org/abs/2401.11814 ,  3543kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11844 (*cross-listing*)
Date: Mon, 22 Jan 2024 11:01:52 GMT   (8222kb,D)

Title: Adaptive Fusion of Multi-view Remote Sensing data for Optimal Sub-field
  Crop Yield Prediction
Authors: Francisco Mena, Deepak Pathak, Hiba Najjar, Cristhian Sanchez, Patrick
  Helber, Benjamin Bischke, Peter Habelitz, Miro Miranda, Jayanth Siddamsetty,
  Marlon Nuske, Marcela Charfuelan, Diego Arenas, Michaela Vollmer, Andreas
  Dengel
Categories: cs.CV cs.AI cs.LG
\\
  Accurate crop yield prediction is of utmost importance for informed
decision-making in agriculture, aiding farmers, and industry stakeholders.
However, this task is complex and depends on multiple factors, such as
environmental conditions, soil properties, and management practices. Combining
heterogeneous data views poses a fusion challenge, like identifying the
view-specific contribution to the predictive task. We present a novel
multi-view learning approach to predict crop yield for different crops
(soybean, wheat, rapeseed) and regions (Argentina, Uruguay, and Germany). Our
multi-view input data includes multi-spectral optical images from Sentinel-2
satellites and weather data as dynamic features during the crop growing season,
complemented by static features like soil properties and topographic
information. To effectively fuse the data, we introduce a Multi-view Gated
Fusion (MVGF) model, comprising dedicated view-encoders and a Gated Unit (GU)
module. The view-encoders handle the heterogeneity of data sources with varying
temporal resolutions by learning a view-specific representation. These
representations are adaptively fused via a weighted sum. The fusion weights are
computed for each sample by the GU using a concatenation of the
view-representations. The MVGF model is trained at sub-field level with 10 m
resolution pixels. Our evaluations show that the MVGF outperforms conventional
models on the same task, achieving the best results by incorporating all the
data sources, unlike the usual fusion results in the literature. For Argentina,
the MVGF model achieves an R2 value of 0.68 at sub-field yield prediction,
while at field level evaluation (comparing field averages), it reaches around
0.80 across different countries. The GU module learned different weights based
on the country and crop-type, aligning with the variable significance of each
data source to the prediction task.
\\ ( https://arxiv.org/abs/2401.11844 ,  8222kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11851 (*cross-listing*)
Date: Mon, 22 Jan 2024 11:14:08 GMT   (354kb,D)

Title: BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge
Authors: Yuhao Ji, Chao Fang, Zhongfeng Wang
Categories: cs.AR cs.AI
Comments: This paper is accepted by 2024 IEEE International Symposium on
  Circuits and Systems (ISCAS 2024)
\\
  Existing binary Transformers are promising in edge deployment due to their
compact model size, low computational complexity, and considerable inference
accuracy.However, deploying binary Transformers faces challenges on prior
processors due to inefficient execution of quantized matrix multiplication
(QMM) and the energy consumption overhead caused by multi-precision
activations.To tackle the challenges above, we first develop a computation flow
abstraction method for binary Transformers to improve QMM execution efficiency
by optimizing the computation order.Furthermore, a binarized energy-efficient
Transformer accelerator, namely BETA, is proposed to boost the efficient
deployment at the edge.Notably, BETA features a configurable QMM engine,
accommodating diverse activation precisions of binary Transformers and offering
high-parallelism and high-speed for QMMs with impressive energy
efficiency.Experimental results evaluated on ZCU102 FPGA show BETA achieves an
average energy efficiency of 174 GOPS/W, which is 1.76~21.92x higher than prior
FPGA-based accelerators, showing BETA's good potential for edge Transformer
acceleration.
\\ ( https://arxiv.org/abs/2401.11851 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11900 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:50:12 GMT   (1949kb,D)

Title: Showing Proofs, Assessing Difficulty with GeoGebra Discovery
Authors: Zolt\'an Kov\'acs (The Private University College of Education of the
  Diocese of Linz, Austria), Tom\'as Recio (Escuela Polit\'ecnica Superior,
  Universidad Antonio de Nebrija, Madrid, Spain), M. Pilar V\'elez (Escuela
  Polit\'ecnica Superior, Universidad Antonio de Nebrija, Madrid, Spain)
Categories: cs.SC cs.AI cs.CG cs.MS
Comments: In Proceedings ADG 2023, arXiv:2401.10725
Journal-ref: EPTCS 398, 2024, pp. 43-52
DOI: 10.4204/EPTCS.398.8
\\
  In our contribution we describe some on-going improvements concerning the
Automated Reasoning Tools developed in GeoGebra Discovery, providing different
examples of the performance of these new features. We describe the new
ShowProof command, that outputs both the sequence of the different steps
performed by GeoGebra Discovery to confirm a certain statement, as well as a
number intending to grade the difficulty or interest of the assertion. The
proposal of this assessment measure, involving the comparison of the expression
of the thesis (or conclusion) as a combination of the hypotheses, will be
developed.
\\ ( https://arxiv.org/abs/2401.11900 ,  1949kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11906 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:51:35 GMT   (1821kb,D)

Title: Solving with GeoGebra Discovery an Austrian Mathematics Olympiad
  problem: Lessons Learned
Authors: Bel\'en Ari\~no-Morera (Departamento de Econom\'ia Financiera y
  Contabilidad, Universidad Rey Juan Carlos, Madrid, Spain), Zolt\'an Kov\'acs
  (The Private University College of Education of the Diocese of Linz,
  Austria), Tom\'as Recio (Escuela Polit\'ecnica Superior, Universidad Antonio
  de Nebrija, Madrid, Spain), Piedad Tolmos (Departamento de Econom\'ia
  Financiera y Contabilidad, Universidad Rey Juan Carlos, Madrid, Spain)
Categories: cs.SC cs.AI cs.CG cs.MS
Comments: In Proceedings ADG 2023, arXiv:2401.10725
Journal-ref: EPTCS 398, 2024, pp. 101-109
DOI: 10.4204/EPTCS.398.13
\\
  We address, through the automated reasoning tools in GeoGebra Discovery, a
problem from a regional phase of the Austrian Mathematics Olympiad 2023. Trying
to solve this problem gives rise to four different kind of feedback: the almost
instantaneous, automated solution of the proposed problem; the measure of its
complexity, according to some recent proposals; the automated discovery of a
generalization of the given assertion, showing that the same statement is true
over more general polygons than those mentioned in the problem; and the
difficulties associated to the analysis of the surprising and involved high
number of degenerate cases that appear when using the LocusEquation command in
this problem. In our communication we will describe and reflect on these
diverse issues, enhancing its exemplar role for showing some of the advantages,
problems, and current fields of development of GeoGebra Discovery.
\\ ( https://arxiv.org/abs/2401.11906 ,  1821kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11913 (*cross-listing*)
Date: Mon, 22 Jan 2024 13:01:28 GMT   (23625kb,D)

Title: Large receptive field strategy and important feature extraction strategy
  in 3D object detection
Authors: Leichao Cui, Xiuxian Li, and Min Meng
Categories: cs.CV cs.AI
\\
  The enhancement of 3D object detection is pivotal for precise environmental
perception and improved task execution capabilities in autonomous driving.
LiDAR point clouds, offering accurate depth information, serve as a crucial
information for this purpose. Our study focuses on key challenges in 3D target
detection. To tackle the challenge of expanding the receptive field of a 3D
convolutional kernel, we introduce the Dynamic Feature Fusion Module (DFFM).
This module achieves adaptive expansion of the 3D convolutional kernel's
receptive field, balancing the expansion with acceptable computational loads.
This innovation reduces operations, expands the receptive field, and allows the
model to dynamically adjust to different object requirements. Simultaneously,
we identify redundant information in 3D features. Employing the Feature
Selection Module (FSM) quantitatively evaluates and eliminates non-important
features, achieving the separation of output box fitting and feature
extraction. This innovation enables the detector to focus on critical features,
resulting in model compression, reduced computational burden, and minimized
candidate frame interference. Extensive experiments confirm that both DFFM and
FSM not only enhance current benchmarks, particularly in small target
detection, but also accelerate network performance. Importantly, these modules
exhibit effective complementarity.
\\ ( https://arxiv.org/abs/2401.11913 ,  23625kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11963 (*cross-listing*)
Date: Mon, 22 Jan 2024 14:06:37 GMT   (732kb,D)

Title: Bridging Evolutionary Algorithms and Reinforcement Learning: A
  Comprehensive Survey
Authors: Pengyi Li, Jianye Hao, Hongyao Tang, Xian Fu, Yan Zheng, Ke Tang
Categories: cs.NE cs.AI cs.LG
\\
  Evolutionary Reinforcement Learning (ERL), which integrates Evolutionary
Algorithms (EAs) and Reinforcement Learning (RL) for optimization, has
demonstrated remarkable performance advancements. By fusing the strengths of
both approaches, ERL has emerged as a promising research direction. This survey
offers a comprehensive overview of the diverse research branches in ERL.
Specifically, we systematically summarize recent advancements in relevant
algorithms and identify three primary research directions: EA-assisted
optimization of RL, RL-assisted optimization of EA, and synergistic
optimization of EA and RL. Following that, we conduct an in-depth analysis of
each research direction, organizing multiple research branches. We elucidate
the problems that each branch aims to tackle and how the integration of EA and
RL addresses these challenges. In conclusion, we discuss potential challenges
and prospective future research directions across various research directions.
\\ ( https://arxiv.org/abs/2401.11963 ,  732kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12024 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:11:57 GMT   (25529kb,D)

Title: Multimodal Visual-Tactile Representation Learning through
  Self-Supervised Contrastive Pre-Training
Authors: Vedant Dave, Fotios Lygerakis, Elmar Rueckert
Categories: cs.RO cs.AI cs.LG
\\
  The rapidly evolving field of robotics necessitates methods that can
facilitate the fusion of multiple modalities. Specifically, when it comes to
interacting with tangible objects, effectively combining visual and tactile
sensory data is key to understanding and navigating the complex dynamics of the
physical world, enabling a more nuanced and adaptable response to changing
environments. Nevertheless, much of the earlier work in merging these two
sensory modalities has relied on supervised methods utilizing datasets labeled
by humans.This paper introduces MViTac, a novel methodology that leverages
contrastive learning to integrate vision and touch sensations in a
self-supervised fashion. By availing both sensory inputs, MViTac leverages
intra and inter-modality losses for learning representations, resulting in
enhanced material property classification and more adept grasping prediction.
Through a series of experiments, we showcase the effectiveness of our method
and its superiority over existing state-of-the-art self-supervised and
supervised techniques. In evaluating our methodology, we focus on two distinct
tasks: material classification and grasping success prediction. Our results
indicate that MViTac facilitates the development of improved modality encoders,
yielding more robust representations as evidenced by linear probing
assessments.
\\ ( https://arxiv.org/abs/2401.12024 ,  25529kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12032 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:17:54 GMT   (2225kb,D)

Title: MINT: A wrapper to make multi-modal and multi-image AI models
  interactive
Authors: Jan Freyberg, Abhijit Guha Roy, Terry Spitz, Beverly Freeman, Mike
  Schaekermann, Patricia Strachan, Eva Schnider, Renee Wong, Dale R Webster,
  Alan Karthikesalingam, Yun Liu, Krishnamurthy Dvijotham, Umesh Telang
Categories: cs.HC cs.AI
Comments: 15 pages, 7 figures
\\
  During the diagnostic process, doctors incorporate multimodal information
including imaging and the medical history - and similarly medical AI
development has increasingly become multimodal. In this paper we tackle a more
subtle challenge: doctors take a targeted medical history to obtain only the
most pertinent pieces of information; how do we enable AI to do the same? We
develop a wrapper method named MINT (Make your model INTeractive) that
automatically determines what pieces of information are most valuable at each
step, and ask for only the most useful information. We demonstrate the efficacy
of MINT wrapping a skin disease prediction model, where multiple images and a
set of optional answers to $25$ standard metadata questions (i.e., structured
medical history) are used by a multi-modal deep network to provide a
differential diagnosis. We show that MINT can identify whether metadata inputs
are needed and if so, which question to ask next. We also demonstrate that when
collecting multiple images, MINT can identify if an additional image would be
beneficial, and if so, which type of image to capture. We showed that MINT
reduces the number of metadata and image inputs needed by 82% and 36.2%
respectively, while maintaining predictive performance. Using real-world AI
dermatology system data, we show that needing fewer inputs can retain users
that may otherwise fail to complete the system submission and drop off without
a diagnosis. Qualitative examples show MINT can closely mimic the step-by-step
decision making process of a clinical workflow and how this is different for
straight forward cases versus more difficult, ambiguous cases. Finally we
demonstrate how MINT is robust to different underlying multi-model classifiers
and can be easily adapted to user requirements without significant model
re-training.
\\ ( https://arxiv.org/abs/2401.12032 ,  2225kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12051 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:42:21 GMT   (19711kb,D)

Title: CloSe: A 3D Clothing Segmentation Dataset and Model
Authors: Dimitrije Anti\'c, Garvita Tiwari, Batuhan Ozcomlekci, Riccardo Marin,
  Gerard Pons-Moll
Categories: cs.CV cs.AI
\\
  3D Clothing modeling and datasets play crucial role in the entertainment,
animation, and digital fashion industries. Existing work often lacks detailed
semantic understanding or uses synthetic datasets, lacking realism and
personalization. To address this, we first introduce CloSe-D: a novel
large-scale dataset containing 3D clothing segmentation of 3167 scans, covering
a range of 18 distinct clothing classes. Additionally, we propose CloSe-Net,
the first learning-based 3D clothing segmentation model for fine-grained
segmentation from colored point clouds. CloSe-Net uses local point features,
body-clothing correlation, and a garment-class and point features-based
attention module, improving performance over baselines and prior work. The
proposed attention module enables our model to learn appearance and
geometry-dependent clothing prior from data. We further validate the efficacy
of our approach by successfully segmenting publicly available datasets of
people in clothing. We also introduce CloSe-T, a 3D interactive tool for
refining segmentation labels. Combining the tool with CloSe-T in a continual
learning setup demonstrates improved generalization on real-world data.
Dataset, model, and tool can be found at
https://virtualhumans.mpi-inf.mpg.de/close3dv24/.
\\ ( https://arxiv.org/abs/2401.12051 ,  19711kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12164 (*cross-listing*)
Date: Mon, 22 Jan 2024 17:56:07 GMT   (7587kb,D)

Title: Semi-supervised segmentation of land cover images using nonlinear
  canonical correlation analysis with multiple features and t-SNE
Authors: Hong Wei, James Xiao, Yichao Zhang and Xia Hong
Categories: cs.CV cs.AI
\\
  Image segmentation is a clustering task whereby each pixel is assigned a
cluster label. Remote sensing data usually consists of multiple bands of
spectral images in which there exist semantically meaningful land cover
subregions, co-registered with other source data such as LIDAR (LIght Detection
And Ranging) data, where available. This suggests that, in order to account for
spatial correlation between pixels, a feature vector associated with each pixel
may be a vectorized tensor representing the multiple bands and a local patch as
appropriate. Similarly, multiple types of texture features based on a pixel's
local patch would also be beneficial for encoding locally statistical
information and spatial variations, without necessarily labelling pixel-wise a
large amount of ground truth, then training a supervised model, which is
sometimes impractical. In this work, by resorting to label only a small
quantity of pixels, a new semi-supervised segmentation approach is proposed.
Initially, over all pixels, an image data matrix is created in high dimensional
feature space. Then, t-SNE projects the high dimensional data onto 3D
embedding. By using radial basis functions as input features, which use the
labelled data samples as centres, to pair with the output class labels, a
modified canonical correlation analysis algorithm, referred to as RBF-CCA, is
introduced which learns the associated projection matrix via the small labelled
data set. The associated canonical variables, obtained for the full image, are
applied by k-means clustering algorithm. The proposed semi-supervised RBF-CCA
algorithm has been implemented on several remotely sensed multispectral images,
demonstrating excellent segmentation results.
\\ ( https://arxiv.org/abs/2401.12164 ,  7587kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12167 (*cross-listing*)
Date: Fri, 19 Jan 2024 15:19:47 GMT   (935kb,D)

Title: Dynamic Semantic Compression for CNN Inference in Multi-access Edge
  Computing: A Graph Reinforcement Learning-based Autoencoder
Authors: Nan Li, Alexandros Iosifidis and Qi Zhang
Categories: eess.IV cs.AI cs.LG
Comments: arXiv admin note: text overlap with arXiv:2211.13745
\\
  This paper studies the computational offloading of CNN inference in dynamic
multi-access edge computing (MEC) networks. To address the uncertainties in
communication time and computation resource availability, we propose a novel
semantic compression method, autoencoder-based CNN architecture (AECNN), for
effective semantic extraction and compression in partial offloading. In the
semantic encoder, we introduce a feature compression module based on the
channel attention mechanism in CNNs, to compress intermediate data by selecting
the most informative features. In the semantic decoder, we design a lightweight
decoder to reconstruct the intermediate data through learning from the received
compressed data to improve accuracy. To effectively trade-off communication,
computation, and inference accuracy, we design a reward function and formulate
the offloading problem of CNN inference as a maximization problem with the goal
of maximizing the average inference accuracy and throughput over the long term.
To address this maximization problem, we propose a graph reinforcement
learning-based AECNN (GRL-AECNN) method, which outperforms existing works
DROO-AECNN, GRL-BottleNet++ and GRL-DeepJSCC under different dynamic scenarios.
This highlights the advantages of GRL-AECNN in offloading decision-making in
dynamic MEC.
\\ ( https://arxiv.org/abs/2401.12167 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12170 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:04:26 GMT   (41kb)

Title: Natural Strategic Ability in Stochastic Multi-Agent Systems
Authors: Rapha\"el Berthon, Joost-Pieter Katoen, Munyque Mittelmann, Aniello
  Murano
Categories: cs.LO cs.AI
Comments: Extended version of the paper accepted at AAAI 2024
\\
  Strategies synthesized using formal methods can be complex and often require
infinite memory, which does not correspond to the expected behavior when trying
to model Multi-Agent Systems (MAS). To capture such behaviors, natural
strategies are a recently proposed framework striking a balance between the
ability of agents to strategize with memory and the model-checking complexity,
but until now has been restricted to fully deterministic settings. For the
first time, we consider the probabilistic temporal logics PATL and PATL* under
natural strategies (NatPATL and NatPATL*, resp.). As main result we show that,
in stochastic MAS, NatPATL model-checking is NP-complete when the active
coalition is restricted to deterministic strategies. We also give a 2NEXPTIME
complexity result for NatPATL* with the same restriction. In the unrestricted
case, we give an EXPSPACE complexity for NatPATL and 3EXPSPACE complexity for
NatPATL*.
\\ ( https://arxiv.org/abs/2401.12170 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12176 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:09:15 GMT   (1526kb)

Title: Broiler-Net: A Deep Convolutional Framework for Broiler Behavior
  Analysis in Poultry Houses
Authors: Tahereh Zarrat Ehsan, Seyed Mehdi Mohtavipour
Categories: cs.CV cs.AI
Comments: 11 pages, 7 figures
\\
  Detecting anomalies in poultry houses is crucial for maintaining optimal
chicken health conditions, minimizing economic losses and bolstering
profitability. This paper presents a novel real-time framework for analyzing
chicken behavior in cage-free poultry houses to detect abnormal behaviors.
Specifically, two significant abnormalities, namely inactive broiler and
huddling behavior, are investigated in this study. The proposed framework
comprises three key steps: (1) chicken detection utilizing a state-of-the-art
deep learning model, (2) tracking individual chickens across consecutive frames
with a fast tracker module, and (3) detecting abnormal behaviors within the
video stream. Experimental studies are conducted to evaluate the efficacy of
the proposed algorithm in accurately assessing chicken behavior. The results
illustrate that our framework provides a precise and efficient solution for
real-time anomaly detection, facilitating timely interventions to maintain
chicken health and enhance overall productivity on poultry farms. Github:
https://github.com/TaherehZarratEhsan/Chicken-Behavior-Analysis
\\ ( https://arxiv.org/abs/2401.12176 ,  1526kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12179 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:10:10 GMT   (4880kb,D)

Title: DITTO: Diffusion Inference-Time T-Optimization for Music Generation
Authors: Zachary Novack, Julian McAuley, Taylor Berg-Kirkpatrick, Nicholas J.
  Bryan
Categories: cs.SD cs.AI cs.LG eess.AS
\\
  We propose Diffusion Inference-Time T-Optimization (DITTO), a general-purpose
frame-work for controlling pre-trained text-to-music diffusion models at
inference-time via optimizing initial noise latents. Our method can be used to
optimize through any differentiable feature matching loss to achieve a target
(stylized) output and leverages gradient checkpointing for memory efficiency.
We demonstrate a surprisingly wide-range of applications for music generation
including inpainting, outpainting, and looping as well as intensity, melody,
and musical structure control - all without ever fine-tuning the underlying
model. When we compare our approach against related training, guidance, and
optimization-based methods, we find DITTO achieves state-of-the-art performance
on nearly all tasks, including outperforming comparable approaches on
controllability, audio quality, and computational efficiency, thus opening the
door for high-quality, flexible, training-free control of diffusion models.
Sound examples can be found at https://DITTO-Music.github.io/web/.
\\ ( https://arxiv.org/abs/2401.12179 ,  4880kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12202 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:42:20 GMT   (6333kb,D)

Title: OK-Robot: What Really Matters in Integrating Open-Knowledge Models for
  Robotics
Authors: Peiqi Liu, Yaswanth Orru, Chris Paxton, Nur Muhammad Mahi Shafiullah,
  Lerrel Pinto
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  Remarkable progress has been made in recent years in the fields of vision,
language, and robotics. We now have vision models capable of recognizing
objects based on language queries, navigation systems that can effectively
control mobile systems, and grasping models that can handle a wide range of
objects. Despite these advancements, general-purpose applications of robotics
still lag behind, even though they rely on these fundamental capabilities of
recognition, navigation, and grasping. In this paper, we adopt a systems-first
approach to develop a new Open Knowledge-based robotics framework called
OK-Robot. By combining Vision-Language Models (VLMs) for object detection,
navigation primitives for movement, and grasping primitives for object
manipulation, OK-Robot offers a integrated solution for pick-and-drop
operations without requiring any training. To evaluate its performance, we run
OK-Robot in 10 real-world home environments. The results demonstrate that
OK-Robot achieves a 58.5% success rate in open-ended pick-and-drop tasks,
representing a new state-of-the-art in Open Vocabulary Mobile Manipulation
(OVMM) with nearly 1.8x the performance of prior work. On cleaner, uncluttered
environments, OK-Robot's performance increases to 82%. However, the most
important insight gained from OK-Robot is the critical role of nuanced details
when combining Open Knowledge systems like VLMs with robotic modules. Videos of
our experiments are available on our website: https://ok-robot.github.io
\\ ( https://arxiv.org/abs/2401.12202 ,  6333kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12203 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:42:31 GMT   (4960kb,D)

Title: Unsupervised Machine Learning for the Classification of Astrophysical
  X-ray Sources
Authors: V\'ictor Samuel P\'erez-D\'iaz, Juan Rafael Mart\'inez-Galarza,
  Alexander Caicedo, Raffaele D'Abrusco
Categories: astro-ph.IM cs.AI
Comments: 21 pages, 11 figures. Accepted in MNRAS
\\
  The automatic classification of X-ray detections is a necessary step in
extracting astrophysical information from compiled catalogs of astrophysical
sources. Classification is useful for the study of individual objects,
statistics for population studies, as well as for anomaly detection, i.e., the
identification of new unexplored phenomena, including transients and spectrally
extreme sources. Despite the importance of this task, classification remains
challenging in X-ray astronomy due to the lack of optical counterparts and
representative training sets. We develop an alternative methodology that
employs an unsupervised machine learning approach to provide probabilistic
classes to Chandra Source Catalog sources with a limited number of labeled
sources, and without ancillary information from optical and infrared catalogs.
We provide a catalog of probabilistic classes for 8,756 sources, comprising a
total of 14,507 detections, and demonstrate the success of the method at
identifying emission from young stellar objects, as well as distinguishing
between small-scale and large-scale compact accretors with a significant level
of confidence. We investigate the consistency between the distribution of
features among classified objects and well-established astrophysical hypotheses
such as the unified AGN model. This provides interpretability to the
probabilistic classifier. Code and tables are available publicly through
GitHub. We provide a web playground for readers to explore our final
classification at https://umlcaxs-playground.streamlit.app.
\\ ( https://arxiv.org/abs/2401.12203 ,  4960kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10893 (*cross-listing*)
Date: Fri, 1 Dec 2023 22:35:19 GMT   (463kb)

Title: Location Sensitive Embedding for Knowledge Graph Embedding
Authors: Deepak Banerjee, Anjali Ishaan
Categories: cs.IR cs.CL
\\
  Knowledge graph embedding transforms knowledge graphs into a continuous,
low-dimensional space, facilitating inference and completion tasks. This field
is mainly divided into translational distance models and semantic matching
models. A key challenge in translational distance models is their inability to
effectively differentiate between 'head' and 'tail' entities in graphs. To
address this, the novel location-sensitive embedding (LSE) method has been
developed. LSE innovatively modifies the head entity using relation-specific
mappings, conceptualizing relations as linear transformations rather than mere
translations. The theoretical foundations of LSE, including its
representational capabilities and its connections to existing models, have been
thoroughly examined. A more streamlined variant, LSEd, employs a diagonal
matrix for transformations to enhance practical efficiency. In tests conducted
on four large-scale datasets for link prediction, LSEd either outperforms or is
competitive with leading contemporary models.
\\ ( https://arxiv.org/abs/2401.10893 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10900 (*cross-listing*)
Date: Tue, 19 Dec 2023 07:58:39 GMT   (585kb)

Title: Towards building a monitoring platform for a challenge-oriented smart
  specialisation with RIS3-MCAT
Authors: Enric Fuster, Tatiana Fern\'andez, Hermes Carretero, Nicolau
  Duran-Silva, Roger Guix\'e, Josep Pujol, Bernardo Rondelli, Guillem Rull,
  Marta Cortijo, Montserrat Romagosa
Categories: cs.CY cs.CL cs.HC
Comments: Accepted and presented at the 27th International Conference on
  Science, Technology and Innovation Indicators (STI 2023)
\\
  In the new research and innovation (R&I) paradigm, aimed at a transformation
towards more sustainable, inclusive and fair pathways to address societal and
environmental challenges, and at generating new patterns of specialisation and
new trajectories for socioeconomic development, it is essential to provide
monitoring systems and tools to map and understand the contribution of R&I
policies and projects. To address this transformation, we present the RIS3-MCAT
platform, the result of a line of work aimed at exploring the potential of open
data, semantic analysis, and data visualisation, for monitoring
challenge-oriented smart specialisation in Catalonia. RIS3-MCAT is an
interactive platform that facilitates access to R&I project data in formats
that allow for sophisticated analyses of a large volume of texts, enabling the
detailed study of thematic specialisations and challenges beyond classical
classification systems. Its conceptualisation, development framework and use
are presented in this paper.
\\ ( https://arxiv.org/abs/2401.10900 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10940 (*cross-listing*)
Date: Wed, 17 Jan 2024 13:11:09 GMT   (364kb)

Title: RELIANCE: Reliable Ensemble Learning for Information and News
  Credibility Evaluation
Authors: Majid Ramezani, Hamed Mohammad-Shahi, Mahshid Daliry, Soroor Rahmani,
  Amir-Hosein Asghari
Categories: cs.IR cs.CL cs.LG cs.SI
\\
  In the era of information proliferation, discerning the credibility of news
content poses an ever-growing challenge. This paper introduces RELIANCE, a
pioneering ensemble learning system designed for robust information and fake
news credibility evaluation. Comprising five diverse base models, including
Support Vector Machine (SVM), naive Bayes, logistic regression, random forest,
and Bidirectional Long Short Term Memory Networks (BiLSTMs), RELIANCE employs
an innovative approach to integrate their strengths, harnessing the collective
intelligence of the ensemble for enhanced accuracy. Experiments demonstrate the
superiority of RELIANCE over individual models, indicating its efficacy in
distinguishing between credible and non-credible information sources. RELIANCE,
also surpasses baseline models in information and news credibility assessment,
establishing itself as an effective solution for evaluating the reliability of
information sources.
\\ ( https://arxiv.org/abs/2401.10940 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11248 (*cross-listing*)
Date: Sat, 20 Jan 2024 15:02:33 GMT   (446kb,D)

Title: Drop your Decoder: Pre-training with Bag-of-Word Prediction for Dense
  Passage Retrieval
Authors: Guangyuan Ma, Xing Wu, Zijia Lin, Songlin Hu
Categories: cs.IR cs.CL
Comments: Working in progress. Our code will be available at
  https://github.com/ma787639046/bowdpr
\\
  Masked auto-encoder pre-training has emerged as a prevalent technique for
initializing and enhancing dense retrieval systems. It generally utilizes
additional Transformer decoder blocks to provide sustainable supervision
signals and compress contextual information into dense representations.
However, the underlying reasons for the effectiveness of such a pre-training
technique remain unclear. The usage of additional Transformer-based decoders
also incurs significant computational costs. In this study, we aim to shed
light on this issue by revealing that masked auto-encoder (MAE) pre-training
with enhanced decoding significantly improves the term coverage of input tokens
in dense representations, compared to vanilla BERT checkpoints. Building upon
this observation, we propose a modification to the traditional MAE by replacing
the decoder of a masked auto-encoder with a completely simplified Bag-of-Word
prediction task. This modification enables the efficient compression of lexical
signals into dense representations through unsupervised pre-training.
Remarkably, our proposed method achieves state-of-the-art retrieval performance
on several large-scale retrieval benchmarks without requiring any additional
parameters, which provides a 67% training speed-up compared to standard masked
auto-encoder pre-training with enhanced decoding.
\\ ( https://arxiv.org/abs/2401.11248 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11305 (*cross-listing*)
Date: Sat, 20 Jan 2024 19:32:56 GMT   (682kb,D)

Title: Progress in Privacy Protection: A Review of Privacy Preserving
  Techniques in Recommender Systems, Edge Computing, and Cloud Computing
Authors: Syed Raza Bashir, Shaina Raza, Vojislav Misic
Categories: cs.CR cs.CL
\\
  As digital technology evolves, the increasing use of connected devices brings
both challenges and opportunities in the areas of mobile crowdsourcing, edge
computing, and recommender systems. This survey focuses on these dynamic
fields, emphasizing the critical need for privacy protection in our
increasingly data-oriented world. It explores the latest trends in these
interconnected areas, with a special emphasis on privacy and data security. Our
method involves an in-depth analysis of various academic works, which helps us
to gain a comprehensive understanding of these sectors and their shifting focus
towards privacy concerns. We present new insights and marks a significant
advancement in addressing privacy issues within these technologies. The survey
is a valuable resource for researchers, industry practitioners, and policy
makers, offering an extensive overview of these fields and their related
privacy challenges, catering to a wide audience in the modern digital era.
\\ ( https://arxiv.org/abs/2401.11305 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11452 (*cross-listing*)
Date: Sun, 21 Jan 2024 10:15:36 GMT   (56kb,D)

Title: Towards Reliable and Factual Response Generation: Detecting Unanswerable
  Questions in Information-Seeking Conversations
Authors: Weronika {\L}ajewska, Krisztian Balog
Categories: cs.IR cs.CL
Comments: This is the author's version of the work. The definitive version is
  published in: Proceedings of the 46th European Conference on Information
  Retrieval} (ECIR '24), March 24--28, 2024, Glasgow, Scotland
\\
  Generative AI models face the challenge of hallucinations that can undermine
users' trust in such systems. We approach the problem of conversational
information seeking as a two-step process, where relevant passages in a corpus
are identified first and then summarized into a final system response. This way
we can automatically assess if the answer to the user's question is present in
the corpus. Specifically, our proposed method employs a sentence-level
classifier to detect if the answer is present, then aggregates these
predictions on the passage level, and eventually across the top-ranked passages
to arrive at a final answerability estimate. For training and evaluation, we
develop a dataset based on the TREC CAsT benchmark that includes answerability
labels on the sentence, passage, and ranking levels. We demonstrate that our
proposed method represents a strong baseline and outperforms a state-of-the-art
LLM on the answerability prediction task.
\\ ( https://arxiv.org/abs/2401.11452 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11463 (*cross-listing*)
Date: Sun, 21 Jan 2024 11:04:30 GMT   (31kb)

Title: Estimating the Usefulness of Clarifying Questions and Answers for
  Conversational Search
Authors: Ivan Sekuli\'c, Weronika {\L}ajewska, Krisztian Balog, Fabio Crestani
Categories: cs.IR cs.CL
Comments: This is the author's version of the work. The definitive version is
  published in: Proceedings of the 46th European Conference on Information
  Retrieval (ECIR '24), March 24-28, 2024, Glasgow, Scotland
\\
  While the body of research directed towards constructing and generating
clarifying questions in mixed-initiative conversational search systems is vast,
research aimed at processing and comprehending users' answers to such questions
is scarce. To this end, we present a simple yet effective method for processing
answers to clarifying questions, moving away from previous work that simply
appends answers to the original query and thus potentially degrades retrieval
performance. Specifically, we propose a classifier for assessing usefulness of
the prompted clarifying question and an answer given by the user. Useful
questions or answers are further appended to the conversation history and
passed to a transformer-based query rewriting module. Results demonstrate
significant improvements over strong non-mixed-initiative baselines.
Furthermore, the proposed approach mitigates the performance drops when non
useful questions and answers are utilized.
\\ ( https://arxiv.org/abs/2401.11463 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11631 (*cross-listing*)
Date: Sun, 21 Jan 2024 23:54:05 GMT   (14187kb,D)

Title: Text-to-Image Cross-Modal Generation: A Systematic Review
Authors: Maciej \.Zelaszczyk, Jacek Ma\'ndziuk
Categories: cs.CV cs.CL cs.LG
\\
  We review research on generating visual data from text from the angle of
"cross-modal generation." This point of view allows us to draw parallels
between various methods geared towards working on input text and producing
visual output, without limiting the analysis to narrow sub-areas. It also
results in the identification of common templates in the field, which are then
compared and contrasted both within pools of similar methods and across lines
of research. We provide a breakdown of text-to-image generation into various
flavors of image-from-text methods, video-from-text methods, image editing,
self-supervised and graph-based approaches. In this discussion, we focus on
research papers published at 8 leading machine learning conferences in the
years 2016-2022, also incorporating a number of relevant papers not matching
the outlined search criteria. The conducted review suggests a significant
increase in the number of papers published in the area and highlights research
gaps and potential lines of investigation. To our knowledge, this is the first
review to systematically look at text-to-image generation from the perspective
of "cross-modal generation."
\\ ( https://arxiv.org/abs/2401.11631 ,  14187kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11645 (*cross-listing*)
Date: Mon, 22 Jan 2024 01:44:42 GMT   (1607kb,D)

Title: Streaming Bilingual End-to-End ASR model using Attention over Multiple
  Softmax
Authors: Aditya Patil, Vikas Joshi, Purvi Agrawal, Rupesh Mehta
Categories: eess.AS cs.CL cs.SD
Comments: Published in IEEE's Spoken Language Technology (SLT) 2022, 8 pages (6
  + 2 for references), 5 figures
Journal-ref: 2022 IEEE Spoken Language Technology Workshop (SLT), Doha, Qatar,
  2023, pp. 252-259
DOI: 10.1109/SLT54892.2023.10022475
\\
  Even with several advancements in multilingual modeling, it is challenging to
recognize multiple languages using a single neural model, without knowing the
input language and most multilingual models assume the availability of the
input language. In this work, we propose a novel bilingual end-to-end (E2E)
modeling approach, where a single neural model can recognize both languages and
also support switching between the languages, without any language input from
the user. The proposed model has shared encoder and prediction networks, with
language-specific joint networks that are combined via a self-attention
mechanism. As the language-specific posteriors are combined, it produces a
single posterior probability over all the output symbols, enabling a single
beam search decoding and also allowing dynamic switching between the languages.
The proposed approach outperforms the conventional bilingual baseline with
13.3%, 8.23% and 1.3% word error rate relative reduction on Hindi, English and
code-mixed test sets, respectively.
\\ ( https://arxiv.org/abs/2401.11645 ,  1607kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11791 (*cross-listing*)
Date: Mon, 22 Jan 2024 09:41:05 GMT   (2241kb,D)

Title: SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic
  Segmentation
Authors: Ci-Siang Lin, Chien-Yi Wang, Yu-Chiang Frank Wang, Min-Hung Chen
Categories: cs.CV cs.CL cs.LG
\\
  Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation
models using training image data with only image-level supervision. Since
precise pixel-level annotations are not accessible, existing methods typically
focus on producing pseudo masks for training segmentation models by refining
CAM-like heatmaps. However, the produced heatmaps may only capture
discriminative image regions of target object categories or the associated
co-occurring backgrounds. To address the issues, we propose a Semantic Prompt
Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the
CLIP space to enhance the semantic alignment between the segmented regions and
the target object categories. More specifically, we propose Contrastive Prompt
Learning and Class-associated Semantic Refinement to learn the prompts that
adequately describe and suppress the image backgrounds associated with each
target object category. In this way, our proposed framework is able to perform
better semantic matching between object regions and the associated text labels,
resulting in desired pseudo masks for training the segmentation model. The
proposed SemPLeS framework achieves SOTA performance on the standard WSSS
benchmarks, PASCAL VOC and MS COCO, and demonstrated interpretability with the
semantic visualization of our learned prompts. The codes will be released.
\\ ( https://arxiv.org/abs/2401.11791 ,  2241kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12168 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:01:01 GMT   (4607kb,D)

Title: SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning
  Capabilities
Authors: Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete
  Florence, Dorsa Sadigh, Leonidas Guibas, Fei Xia
Categories: cs.CV cs.CL cs.LG cs.RO
\\
  Understanding and reasoning about spatial relationships is a fundamental
capability for Visual Question Answering (VQA) and robotics. While Vision
Language Models (VLM) have demonstrated remarkable performance in certain VQA
benchmarks, they still lack capabilities in 3D spatial reasoning, such as
recognizing quantitative relationships of physical objects like distances or
size differences. We hypothesize that VLMs' limited spatial reasoning
capability is due to the lack of 3D spatial knowledge in training data and aim
to solve this problem by training VLMs with Internet-scale spatial reasoning
data. To this end, we present a system to facilitate this approach. We first
develop an automatic 3D spatial VQA data generation framework that scales up to
2 billion VQA examples on 10 million real-world images. We then investigate
various factors in the training recipe, including data quality, training
pipeline, and VLM architecture. Our work features the first internet-scale 3D
spatial reasoning dataset in metric space. By training a VLM on such data, we
significantly enhance its ability on both qualitative and quantitative spatial
VQA. Finally, we demonstrate that this VLM unlocks novel downstream
applications in chain-of-thought spatial reasoning and robotics due to its
quantitative estimation capability. Project website:
https://spatial-vlm.github.io/
\\ ( https://arxiv.org/abs/2401.12168 ,  4607kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12208 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:51:07 GMT   (16179kb,D)

Title: CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation
Authors: Zhihong Chen, Maya Varma, Jean-Benoit Delbrouck, Magdalini Paschali,
  Louis Blankemeier, Dave Van Veen, Jeya Maria Jose Valanarasu, Alaa Youssef,
  Joseph Paul Cohen, Eduardo Pontes Reis, Emily B. Tsai, Andrew Johnston,
  Cameron Olsen, Tanishq Mathew Abraham, Sergios Gatidis, Akshay S. Chaudhari,
  Curtis Langlotz
Categories: cs.CV cs.CL
Comments: 24 pages, 8 figures
\\
  Chest X-rays (CXRs) are the most frequently performed imaging test in
clinical practice. Recent advances in the development of vision-language
foundation models (FMs) give rise to the possibility of performing automated
CXR interpretation, which can assist physicians with clinical decision-making
and improve patient outcomes. However, developing FMs that can accurately
interpret CXRs is challenging due to the (1) limited availability of
large-scale vision-language datasets in the medical image domain, (2) lack of
vision and language encoders that can capture the complexities of medical data,
and (3) absence of evaluation frameworks for benchmarking the abilities of FMs
on CXR interpretation. In this work, we address these challenges by first
introducing \emph{CheXinstruct} - a large-scale instruction-tuning dataset
curated from 28 publicly-available datasets. We then present \emph{CheXagent} -
an instruction-tuned FM capable of analyzing and summarizing CXRs. To build
CheXagent, we design a clinical large language model (LLM) for parsing
radiology reports, a vision encoder for representing CXR images, and a network
to bridge the vision and language modalities. Finally, we introduce
\emph{CheXbench} - a novel benchmark designed to systematically evaluate FMs
across 8 clinically-relevant CXR interpretation tasks. Extensive quantitative
evaluations and qualitative reviews with five expert radiologists demonstrate
that CheXagent outperforms previously-developed general- and medical-domain FMs
on CheXbench tasks. Furthermore, in an effort to improve model transparency, we
perform a fairness evaluation across factors of sex, race and age to highlight
potential performance disparities. Our project is at
\url{https://stanford-aimi.github.io/chexagent.html}.
\\ ( https://arxiv.org/abs/2401.12208 ,  16179kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02015 (*cross-listing*)
Date: Thu, 4 Jan 2024 01:10:56 GMT   (17772kb,D)

Title: Improving Diffusion-Based Image Synthesis with Context Prediction
Authors: Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang,
  Zheming Cai, Wentao Zhang, Bin Cui
Categories: cs.CV cs.LG
Comments: Accepted by NeurIPS 2023
\\
  Diffusion models are a new class of generative models, and have dramatically
promoted image generation with unprecedented quality and diversity. Existing
diffusion models mainly try to reconstruct input image from a corrupted one
with a pixel-wise or feature-wise constraint along spatial axes. However, such
point-based reconstruction may fail to make each predicted pixel/feature fully
preserve its neighborhood context, impairing diffusion-based image synthesis.
As a powerful source of automatic supervisory signal, context has been well
studied for learning representations. Inspired by this, we for the first time
propose ConPreDiff to improve diffusion-based image synthesis with context
prediction. We explicitly reinforce each point to predict its neighborhood
context (i.e., multi-stride features/tokens/pixels) with a context decoder at
the end of diffusion denoising blocks in training stage, and remove the decoder
for inference. In this way, each point can better reconstruct itself by
preserving its semantic connections with neighborhood context. This new
paradigm of ConPreDiff can generalize to arbitrary discrete and continuous
diffusion backbones without introducing extra parameters in sampling procedure.
Extensive experiments are conducted on unconditional image generation,
text-to-image generation and image inpainting tasks. Our ConPreDiff
consistently outperforms previous methods and achieves a new SOTA text-to-image
generation results on MS-COCO, with a zero-shot FID score of 6.21.
\\ ( https://arxiv.org/abs/2401.02015 ,  17772kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06344 (*cross-listing*)
Date: Fri, 12 Jan 2024 03:26:06 GMT   (1902kb,D)

Title: Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for
  Human Trajectory Prediction with Hypergraph Reasoning
Authors: Weizheng Wang, Le Mao, Baijian Yang, Guohua Chen, and Byung-Cheol Min
Categories: cs.CV cs.LG
\\
  Predicting crowded intents and trajectories is crucial in varouls real-world
applications, including service robots and autonomous vehicles. Understanding
environmental dynamics is challenging, not only due to the complexities of
modeling pair-wise spatial and temporal interactions but also the diverse
influence of group-wise interactions. To decode the comprehensive pair-wise and
group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a
Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory
prediction. In Hyper-STTN, crowded group-wise correlations are constructed
using a set of multi-scale hypergraphs with varying group sizes, captured
through random-walk robability-based hypergraph spectral convolution.
Additionally, a spatial-temporal transformer is adapted to capture pedestrians'
pair-wise latent interactions in spatial-temporal dimensions. These
heterogeneous group-wise and pair-wise are then fused and aligned though a
multimodal transformer network. Hyper-STTN outperformes other state-of-the-art
baselines and ablation models on 5 real-world pedestrian motion datasets.
\\ ( https://arxiv.org/abs/2401.06344 ,  1902kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10903 (*cross-listing*)
Date: Sun, 31 Dec 2023 19:22:57 GMT   (2889kb,D)

Title: Application of Machine Learning in Stock Market Forecasting: A Case
  Study of Disney Stock
Authors: Dengxin Huang
Categories: q-fin.ST cs.LG stat.AP
Comments: 9 pages, 7 figures
\\
  This document presents a stock market analysis conducted on a dataset
consisting of 750 instances and 16 attributes donated in 2014-10-23. The
analysis includes an exploratory data analysis (EDA) section, feature
engineering, data preparation, model selection, and insights from the analysis.
The Fama French 3-factor model is also utilized in the analysis. The results of
the analysis are presented, with linear regression being the best-performing
model.
\\ ( https://arxiv.org/abs/2401.10903 ,  2889kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10921 (*cross-listing*)
Date: Mon, 15 Jan 2024 10:06:17 GMT   (31kb,D)

Title: Push- and Pull-based Effective Communication in Cyber-Physical Systems
Authors: Pietro Talli, Federico Mason, Federico Chiariotti, and Andrea Zanella
Categories: eess.SY cs.LG cs.MA cs.SY
\\
  In Cyber Physical Systems (CPSs), two groups of actors interact toward the
maximization of system performance: the sensors, observing and disseminating
the system state, and the actuators, performing physical decisions based on the
received information. While it is generally assumed that sensors periodically
transmit updates, returning the feedback signal only when necessary, and
consequently adapting the physical decisions to the communication policy, can
significantly improve the efficiency of the system. In particular, the choice
between push-based communication, in which updates are initiated autonomously
by the sensors, and pull-based communication, in which they are requested by
the actuators, is a key design step. In this work, we propose an analytical
model for optimizing push- and pull-based communication in CPSs, observing that
the policy optimality coincides with Value of Information (VoI) maximization.
Our results also highlight that, despite providing a better optimal solution,
implementable push-based communication strategies may underperform even in
relatively simple scenarios.
\\ ( https://arxiv.org/abs/2401.10921 ,  31kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10927 (*cross-listing*)
Date: Tue, 16 Jan 2024 03:14:24 GMT   (160kb)

Title: Debiasing and a local analysis for population clustering using
  semidefinite programming
Authors: Shuheng Zhou
Categories: stat.ML cs.LG
Comments: arXiv admin note: text overlap with arXiv:2301.00344
\\
  In this paper, we consider the problem of partitioning a small data sample of
size $n$ drawn from a mixture of $2$ sub-gaussian distributions. In particular,
we analyze computational efficient algorithms proposed by the same author, to
partition data into two groups approximately according to their population of
origin given a small sample. This work is motivated by the application of
clustering individuals according to their population of origin using $p$
markers, when the divergence between any two of the populations is small. We
build upon the semidefinite relaxation of an integer quadratic program that is
formulated essentially as finding the maximum cut on a graph, where edge
weights in the cut represent dissimilarity scores between two nodes based on
their $p$ features. Here we use $\Delta^2 :=p \gamma$ to denote the $\ell_2^2$
distance between two centers (mean vectors), namely, $\mu^{(1)}$, $\mu^{(2)}$
$\in$ $\mathbb{R}^p$. The goal is to allow a full range of tradeoffs between
$n, p, \gamma$ in the sense that partial recovery (success rate $< 100\%$) is
feasible once the signal to noise ratio $s^2 := \min\{np \gamma^2, \Delta^2\}$
is lower bounded by a constant. Importantly, we prove that the
misclassification error decays exponentially with respect to the SNR $s^2$.
This result was introduced earlier without a full proof. We therefore present
the full proof in the present work. Finally, for balanced partitions, we
consider a variant of the SDP1, and show that the new estimator has a superb
debiasing property. This is novel to the best of our knowledge.
\\ ( https://arxiv.org/abs/2401.10927 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10931 (*cross-listing*)
Date: Tue, 16 Jan 2024 21:09:39 GMT   (491kb,D)

Title: Forecasting Cryptocurrency Staking Rewards
Authors: Sauren Gupta, Apoorva Hathi Katharaki, Yifan Xu, Bhaskar
  Krishnamachari, Rajarshi Gupta
Categories: q-fin.ST cs.CR cs.LG
Comments: 9 pages, 18 figures
\\
  This research explores a relatively unexplored area of predicting
cryptocurrency staking rewards, offering potential insights to researchers and
investors. We investigate two predictive methodologies: a) a straightforward
sliding-window average, and b) linear regression models predicated on
historical data. The findings reveal that ETH staking rewards can be forecasted
with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day
look-aheads respectively, using a 7-day sliding-window average approach.
Additionally, we discern diverse prediction accuracies across various
cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is
identified as superior to the moving-window average for perdicting in the short
term for XTZ and ATOM. The results underscore the generally stable and
predictable nature of staking rewards for most assets, with MATIC presenting a
noteworthy exception.
\\ ( https://arxiv.org/abs/2401.10931 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10941 (*cross-listing*)
Date: Wed, 17 Jan 2024 18:06:17 GMT   (498kb,D)

Title: Crowd-PrefRL: Preference-Based Reward Learning from Crowds
Authors: David Chhan, Ellen Novoseller, Vernon J. Lawhern
Categories: cs.HC cs.LG cs.SI
\\
  Preference-based reinforcement learning (RL) provides a framework to train
agents using human feedback through pairwise preferences over pairs of
behaviors, enabling agents to learn desired behaviors when it is difficult to
specify a numerical reward function. While this paradigm leverages human
feedback, it currently treats the feedback as given by a single human user.
Meanwhile, incorporating preference feedback from crowds (i.e. ensembles of
users) in a robust manner remains a challenge, and the problem of training RL
agents using feedback from multiple human users remains understudied. In this
work, we introduce Crowd-PrefRL, a framework for performing preference-based RL
leveraging feedback from crowds. This work demonstrates the viability of
learning reward functions from preference feedback provided by crowds of
unknown expertise and reliability. Crowd-PrefRL not only robustly aggregates
the crowd preference feedback, but also estimates the reliability of each user
within the crowd using only the (noisy) crowdsourced preference comparisons.
Most importantly, we show that agents trained with Crowd-PrefRL outperform
agents trained with majority-vote preferences or preferences from any
individual user in most cases, especially when the spread of user error rates
among the crowd is large. Results further suggest that our method can identify
minority viewpoints within the crowd.
\\ ( https://arxiv.org/abs/2401.10941 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10945 (*cross-listing*)
Date: Thu, 18 Jan 2024 10:14:21 GMT   (7422kb,D)

Title: Automatic dimensionality reduction of Twin-in-the-Loop Observers
Authors: Giacomo Delcaro, Federico Dett\`u, Simone Formentin, Sergio Matteo
  Savaresi
Categories: cs.SY cs.LG eess.SY
\\
  State-of-the-art vehicle dynamics estimation techniques usually share one
common drawback: each variable to estimate is computed with an independent,
simplified filtering module. These modules run in parallel and need to be
calibrated separately. To solve this issue, a unified Twin-in-the-Loop (TiL)
Observer architecture has recently been proposed: the classical simplified
control-oriented vehicle model in the estimators is replaced by a full-fledged
vehicle simulator, or digital twin (DT). The states of the DT are corrected in
real time with a linear time invariant output error law. Since the simulator is
a black-box, no explicit analytical formulation is available, hence classical
filter tuning techniques cannot be used. Due to this reason, Bayesian
Optimization will be used to solve a data-driven optimization problem to tune
the filter. Due to the complexity of the DT, the optimization problem is
high-dimensional. This paper aims to find a procedure to tune the
high-complexity observer by lowering its dimensionality. In particular, in this
work we will analyze both a supervised and an unsupervised learning approach.
The strategies have been validated for speed and yaw-rate estimation on
real-world data.
\\ ( https://arxiv.org/abs/2401.10945 ,  7422kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10949 (*cross-listing*)
Date: Thu, 18 Jan 2024 19:34:46 GMT   (112kb)

Title: The Synergy Between Optimal Transport Theory and Multi-Agent
  Reinforcement Learning
Authors: Ali Baheri and and Mykel J. Kochenderfer
Categories: cs.MA cs.LG cs.SY eess.SY
\\
  This paper explores the integration of optimal transport (OT) theory with
multi-agent reinforcement learning (MARL). This integration uses OT to handle
distributions and transportation problems to enhance the efficiency,
coordination, and adaptability of MARL. There are five key areas where OT can
impact MARL: (1) policy alignment, where OT's Wasserstein metric is used to
align divergent agent strategies towards unified goals; (2) distributed
resource management, employing OT to optimize resource allocation among agents;
(3) addressing non-stationarity, using OT to adapt to dynamic environmental
shifts; (4) scalable multi-agent learning, harnessing OT for decomposing
large-scale learning objectives into manageable tasks; and (5) enhancing energy
efficiency, applying OT principles to develop sustainable MARL systems. This
paper articulates how the synergy between OT and MARL can address scalability
issues, optimize resource distribution, align agent policies in cooperative
environments, and ensure adaptability in dynamically changing conditions.
\\ ( https://arxiv.org/abs/2401.10949 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10962 (*cross-listing*)
Date: Fri, 19 Jan 2024 11:45:31 GMT   (537kb,D)

Title: One Step Learning, One Step Review
Authors: Xiaolong Huang, Qiankun Li, Xueran Li, Xuesong Gao
Categories: cs.CV cs.LG
Comments: Accepted to the 38th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2024)
\\
  Visual fine-tuning has garnered significant attention with the rise of
pre-trained vision models. The current prevailing method, full fine-tuning,
suffers from the issue of knowledge forgetting as it focuses solely on fitting
the downstream training set. In this paper, we propose a novel weight
rollback-based fine-tuning method called OLOR (One step Learning, One step
Review). OLOR combines fine-tuning with optimizers, incorporating a weight
rollback term into the weight update term at each step. This ensures
consistency in the weight range of upstream and downstream models, effectively
mitigating knowledge forgetting and enhancing fine-tuning performance. In
addition, a layer-wise penalty is presented to employ penalty decay and the
diversified decay rate to adjust the weight rollback levels of layers for
adapting varying downstream tasks. Through extensive experiments on various
tasks such as image classification, object detection, semantic segmentation,
and instance segmentation, we demonstrate the general applicability and
state-of-the-art performance of our proposed OLOR. Code is available at
https://github.com/rainbow-xiao/OLOR-AAAI-2024.
\\ ( https://arxiv.org/abs/2401.10962 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10967 (*cross-listing*)
Date: Sat, 20 Jan 2024 09:56:34 GMT   (10136kb,D)

Title: HOSC: A Periodic Activation Function for Preserving Sharp Features in
  Implicit Neural Representations
Authors: Danzel Serrano, Jakub Szymkowiak, Przemyslaw Musialski
Categories: cs.NE cs.CV cs.GR cs.LG
Comments: 12 pages, 7 figures
ACM-class: I.2.10; I.4.10; I.3
\\
  Recently proposed methods for implicitly representing signals such as images,
scenes, or geometries using coordinate-based neural network architectures often
do not leverage the choice of activation functions, or do so only to a limited
extent. In this paper, we introduce the Hyperbolic Oscillation function (HOSC),
a novel activation function with a controllable sharpness parameter. Unlike any
previous activations, HOSC has been specifically designed to better capture
sudden changes in the input signal, and hence sharp or acute features of the
underlying data, as well as smooth low-frequency transitions. Due to its
simplicity and modularity, HOSC offers a plug-and-play functionality that can
be easily incorporated into any existing method employing a neural network as a
way of implicitly representing a signal. We benchmark HOSC against other
popular activations in an array of general tasks, empirically showing an
improvement in the quality of obtained representations, provide the
mathematical motivation behind the efficacy of HOSC, and discuss its
limitations.
\\ ( https://arxiv.org/abs/2401.10967 ,  10136kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10972 (*cross-listing*)
Date: Fri, 19 Jan 2024 17:12:07 GMT   (914kb,D)

Title: Clustering Molecular Energy Landscapes by Adaptive Network Embedding
Authors: Paula Mercurio and Di Liu
Categories: q-bio.BM cond-mat.stat-mech cs.LG
Comments: 19 pages, 10 figures
\\
  In order to efficiently explore the chemical space of all possible small
molecules, a common approach is to compress the dimension of the system to
facilitate downstream machine learning tasks. Towards this end, we present a
data driven approach for clustering potential energy landscapes of molecular
structures by applying recently developed Network Embedding techniques, to
obtain latent variables defined through the embedding function. To scale up the
method, we also incorporate an entropy sensitive adaptive scheme for
hierarchical sampling of the energy landscape, based on Metadynamics and
Transition Path Theory. By taking into account the kinetic information implied
by a system's energy landscape, we are able to interpret dynamical node-node
relationships in reduced dimensions. We demonstrate the framework through
Lennard-Jones (LJ) clusters and a human DNA sequence.
\\ ( https://arxiv.org/abs/2401.10972 ,  914kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10973 (*cross-listing*)
Date: Fri, 19 Jan 2024 18:00:33 GMT   (1958kb,D)

Title: T2MAC: Targeted and Trusted Multi-Agent Communication through Selective
  Engagement and Evidence-Driven Integration
Authors: Chuxiong Sun and Zehua Zang and Jiabao Li and Jiangmeng Li and Xiao Xu
  and Rui Wang and Changwen Zheng
Categories: cs.MA cs.LG
Comments: AAAI24
\\
  Communication stands as a potent mechanism to harmonize the behaviors of
multiple agents. However, existing works primarily concentrate on broadcast
communication, which not only lacks practicality, but also leads to information
redundancy. This surplus, one-fits-all information could adversely impact the
communication efficiency. Furthermore, existing works often resort to basic
mechanisms to integrate observed and received information, impairing the
learning process. To tackle these difficulties, we propose Targeted and Trusted
Multi-Agent Communication (T2MAC), a straightforward yet effective method that
enables agents to learn selective engagement and evidence-driven integration.
With T2MAC, agents have the capability to craft individualized messages,
pinpoint ideal communication windows, and engage with reliable partners,
thereby refining communication efficiency. Following the reception of messages,
the agents integrate information observed and received from different sources
at an evidence level. This process enables agents to collectively use evidence
garnered from multiple perspectives, fostering trusted and cooperative
behaviors. We evaluate our method on a diverse set of cooperative multi-agent
tasks, with varying difficulties, involving different scales and ranging from
Hallway, MPE to SMAC. The experiments indicate that the proposed model not only
surpasses the state-of-the-art methods in terms of cooperative performance and
communication efficiency, but also exhibits impressive generalization.
\\ ( https://arxiv.org/abs/2401.10973 ,  1958kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10989 (*cross-listing*)
Date: Fri, 19 Jan 2024 19:04:23 GMT   (2603kb,D)

Title: Provably Scalable Black-Box Variational Inference with Structured
  Variational Families
Authors: Joohwan Ko, Kyurae Kim, Woo Chang Kim, and Jacob R. Gardner
Categories: stat.ML cs.LG stat.CO
\\
  Variational families with full-rank covariance approximations are known not
to work well in black-box variational inference (BBVI), both empirically and
theoretically. In fact, recent computational complexity results for BBVI have
established that full-rank variational families scale poorly with the
dimensionality of the problem compared to e.g. mean field families. This is
particularly critical to hierarchical Bayesian models with local variables;
their dimensionality increases with the size of the datasets. Consequently, one
gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on
the dataset size $N$. In this paper, we explore a theoretical middle ground
between mean-field variational families and full-rank families: structured
variational families. We rigorously prove that certain scale matrix structures
can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better
scaling with respect to $N$. We empirically verify our theoretical results on
large-scale hierarchical models.
\\ ( https://arxiv.org/abs/2401.10989 ,  2603kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11017 (*cross-listing*)
Date: Fri, 19 Jan 2024 20:31:53 GMT   (1696kb,D)

Title: Revealing Emotional Clusters in Speaker Embeddings: A Contrastive
  Learning Strategy for Speech Emotion Recognition
Authors: Ismail Rasim Ulgen, Zongyang Du, Carlos Busso, Berrak Sisman
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to ICASSP 2024
\\
  Speaker embeddings carry valuable emotion-related information, which makes
them a promising resource for enhancing speech emotion recognition (SER),
especially with limited labeled data. Traditionally, it has been assumed that
emotion information is indirectly embedded within speaker embeddings, leading
to their under-utilization. Our study reveals a direct and useful link between
emotion and state-of-the-art speaker embeddings in the form of intra-speaker
clusters. By conducting a thorough clustering analysis, we demonstrate that
emotion information can be readily extracted from speaker embeddings. In order
to leverage this information, we introduce a novel contrastive pretraining
approach applied to emotion-unlabeled data for speech emotion recognition. The
proposed approach involves the sampling of positive and the negative examples
based on the intra-speaker clusters of speaker embeddings. The proposed
strategy, which leverages extensive emotion-unlabeled data, leads to a
significant improvement in SER performance, whether employed as a standalone
pretraining task or integrated into a multi-task pretraining setting.
\\ ( https://arxiv.org/abs/2401.11017 ,  1696kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11030 (*cross-listing*)
Date: Fri, 19 Jan 2024 21:11:02 GMT   (2332kb,D)

Title: Exploring Highly Quantised Neural Networks for Intrusion Detection in
  Automotive CAN
Authors: Shashwat Khandelwal, Shreejith Shanker
Categories: cs.CR cs.AR cs.LG cs.SY eess.SY
Comments: 7 pages, 5 figures, 6 tables. arXiv admin note: substantial text
  overlap with arXiv:2401.10724
Journal-ref: 2023 33rd International Conference on Field-Programmable Logic and
  Applications (FPL)
DOI: 10.1109/FPL60245.2023.00040
\\
  Vehicles today comprise intelligent systems like connected autonomous driving
and advanced driving assistance systems (ADAS) to enhance the driving
experience, which is enabled through increased connectivity to infrastructure
and fusion of information from different sensing modes. However, the rising
connectivity coupled with the legacy network architecture within vehicles can
be exploited for launching active and passive attacks on critical vehicle
systems and directly affecting the safety of passengers. Machine learning-based
intrusion detection models have been shown to successfully detect multiple
targeted attack vectors in recent literature, whose deployments are enabled
through quantised neural networks targeting low-power platforms. Multiple
models are often required to simultaneously detect multiple attack vectors,
increasing the area, (resource) cost, and energy consumption. In this paper, we
present a case for utilising custom-quantised MLP's (CQMLP) as a multi-class
classification model, capable of detecting multiple attacks from the benign
flow of controller area network (CAN) messages. The specific quantisation and
neural architecture are determined through a joint design space exploration,
resulting in our choice of the 2-bit precision and the n-layer MLP. Our 2-bit
version is trained using Brevitas and optimised as a dataflow hardware model
through the FINN toolflow from AMD/Xilinx, targeting an XCZU7EV device. We show
that the 2-bit CQMLP model, when integrated as the IDS, can detect malicious
attack messages (DoS, fuzzing, and spoofing attack) with a very high accuracy
of 99.9%, on par with the state-of-the-art methods in the literature.
Furthermore, the dataflow model can perform line rate detection at a latency of
0.11 ms from message reception while consuming 0.23 mJ/inference, making it
ideally suited for integration with an ECU in critical CAN networks.
\\ ( https://arxiv.org/abs/2401.11030 ,  2332kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11098 (*cross-listing*)
Date: Sat, 20 Jan 2024 03:11:59 GMT   (2527kb,D)

Title: Neural auto-designer for enhanced quantum kernels
Authors: Cong Lei, Yuxuan Du, Peng Mi, Jun Yu, Tongliang Liu
Categories: quant-ph cs.LG
Comments: 24 pages, 14 figures, 9 tables, ICLR2024
\\
  Quantum kernels hold great promise for offering computational advantages over
classical learners, with the effectiveness of these kernels closely tied to the
design of the quantum feature map. However, the challenge of designing
effective quantum feature maps for real-world datasets, particularly in the
absence of sufficient prior information, remains a significant obstacle. In
this study, we present a data-driven approach that automates the design of
problem-specific quantum feature maps. Our approach leverages feature-selection
techniques to handle high-dimensional data on near-term quantum machines with
limited qubits, and incorporates a deep neural predictor to efficiently
evaluate the performance of various candidate quantum kernels. Through
extensive numerical simulations on different datasets, we demonstrate the
superiority of our proposal over prior methods, especially for the capability
of eliminating the kernel concentration issue and identifying the feature map
with prediction advantages. Our work not only unlocks the potential of quantum
kernels for enhancing real-world tasks but also highlights the substantial role
of deep learning in advancing quantum machine learning.
\\ ( https://arxiv.org/abs/2401.11098 ,  2527kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11103 (*cross-listing*)
Date: Sat, 20 Jan 2024 03:34:18 GMT   (4944kb,D)

Title: Efficient Data Shapley for Weighted Nearest Neighbor Algorithms
Authors: Jiachen T. Wang, Prateek Mittal, and Ruoxi Jia
Categories: cs.DS cs.LG stat.ML
Comments: AISTATS 2024 Oral
\\
  This work aims to address an open problem in data valuation literature
concerning the efficient computation of Data Shapley for weighted $K$ nearest
neighbor algorithm (WKNN-Shapley). By considering the accuracy of hard-label
KNN with discretized weights as the utility function, we reframe the
computation of WKNN-Shapley into a counting problem and introduce a
quadratic-time algorithm, presenting a notable improvement from $O(N^K)$, the
best result from existing literature. We develop a deterministic approximation
algorithm that further improves computational efficiency while maintaining the
key fairness properties of the Shapley value. Through extensive experiments, we
demonstrate WKNN-Shapley's computational efficiency and its superior
performance in discerning data quality compared to its unweighted counterpart.
\\ ( https://arxiv.org/abs/2401.11103 ,  4944kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11105 (*cross-listing*)
Date: Sat, 20 Jan 2024 03:36:01 GMT   (382kb,D)

Title: Are Latent Vulnerabilities Hidden Gems for Software Vulnerability
  Prediction? An Empirical Study
Authors: Triet H. M. Le, Xiaoning Du, M. Ali Babar
Categories: cs.SE cs.CR cs.LG
Comments: Accepted as a full paper in the technical track at the 21st
  International Conference on Mining Software Repositories (MSR) 2024
\\
  Collecting relevant and high-quality data is integral to the development of
effective Software Vulnerability (SV) prediction models. Most of the current SV
datasets rely on SV-fixing commits to extract vulnerable functions and lines.
However, none of these datasets have considered latent SVs existing between the
introduction and fix of the collected SVs. There is also little known about the
usefulness of these latent SVs for SV prediction. To bridge these gaps, we
conduct a large-scale study on the latent vulnerable functions in two commonly
used SV datasets and their utilization for function-level and line-level SV
predictions. Leveraging the state-of-the-art SZZ algorithm, we identify more
than 100k latent vulnerable functions in the studied datasets. We find that
these latent functions can increase the number of SVs by 4x on average and
correct up to 5k mislabeled functions, yet they have a noise level of around
6%. Despite the noise, we show that the state-of-the-art SV prediction model
can significantly benefit from such latent SVs. The improvements are up to
24.5% in the performance (F1-Score) of function-level SV predictions and up to
67% in the effectiveness of localizing vulnerable lines. Overall, our study
presents the first promising step toward the use of latent SVs to improve the
quality of SV datasets and enhance the performance of SV prediction tasks.
\\ ( https://arxiv.org/abs/2401.11105 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11110 (*cross-listing*)
Date: Sat, 20 Jan 2024 04:13:54 GMT   (23201kb,D)

Title: VONet: Unsupervised Video Object Learning With Parallel U-Net Attention
  and Object-wise Sequential VAE
Authors: Haonan Yu and Wei Xu
Categories: cs.CV cs.LG
Comments: ICLR 2024
\\
  Unsupervised video object learning seeks to decompose video scenes into
structural object representations without any supervision from depth, optical
flow, or segmentation. We present VONet, an innovative approach that is
inspired by MONet. While utilizing a U-Net architecture, VONet employs an
efficient and effective parallel attention inference process, generating
attention masks for all slots simultaneously. Additionally, to enhance the
temporal consistency of each mask across consecutive video frames, VONet
develops an object-wise sequential VAE framework. The integration of these
innovative encoder-side techniques, in conjunction with an expressive
transformer-based decoder, establishes VONet as the leading unsupervised method
for object learning across five MOVI datasets, encompassing videos of diverse
complexities. Code is available at https://github.com/hnyu/vonet.
\\ ( https://arxiv.org/abs/2401.11110 ,  23201kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11124 (*cross-listing*)
Date: Sat, 20 Jan 2024 05:31:47 GMT   (20105kb,D)

Title: EMA-Net: Efficient Multitask Affinity Learning for Dense Scene
  Predictions
Authors: Dimitrios Sinodinos, Narges Armanfard
Categories: cs.CV cs.LG
\\
  Multitask learning (MTL) has gained prominence for its ability to jointly
predict multiple tasks, achieving better per-task performance while using fewer
per-task model parameters than single-task learning. More recently,
decoder-focused architectures have considerably improved multitask performance
by refining task predictions using the features of other related tasks.
However, most of these refinement methods fail to simultaneously capture local
and global task-specific representations, as well as cross-task patterns in a
parameter-efficient manner. In this paper, we introduce the Efficient Multitask
Affinity Learning Network (EMA-Net), which is a lightweight framework that
enhances the task refinement capabilities of multitask networks. EMA-Net
adeptly captures local, global, and cross-task interactions using our novel
Cross-Task Affinity Learning (CTAL) module. The key innovation of CTAL lies in
its ability to manipulate task affinity matrices in a manner that is optimally
suited to apply parameter-efficient grouped convolutions without worrying about
information loss. Our results show that we achieve state-of-the-art MTL
performance for CNN-based decoder-focused models while using substantially
fewer model parameters. Our code is publicly available at
https://github.com/Armanfard-Lab/EMA-Net.
\\ ( https://arxiv.org/abs/2401.11124 ,  20105kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11126 (*cross-listing*)
Date: Sat, 20 Jan 2024 05:37:09 GMT   (6082kb,D)

Title: CARE: Ensemble Adversarial Robustness Evaluation Against Adaptive
  Attackers for Security Applications
Authors: Hangsheng Zhang, Jiqiang Liu, Jinsong Dong
Categories: cs.CR cs.LG
\\
  Ensemble defenses, are widely employed in various security-related
applications to enhance model performance and robustness. The widespread
adoption of these techniques also raises many questions: Are general ensembles
defenses guaranteed to be more robust than individuals? Will stronger adaptive
attacks defeat existing ensemble defense strategies as the cybersecurity arms
race progresses? Can ensemble defenses achieve adversarial robustness to
different types of attacks simultaneously and resist the continually adjusted
adaptive attacks? Unfortunately, these critical questions remain unresolved as
there are no platforms for comprehensive evaluation of ensemble adversarial
attacks and defenses in the cybersecurity domain. In this paper, we propose a
general Cybersecurity Adversarial Robustness Evaluation (CARE) platform aiming
to bridge this gap.
\\ ( https://arxiv.org/abs/2401.11126 ,  6082kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11176 (*cross-listing*)
Date: Sat, 20 Jan 2024 09:26:08 GMT   (4157kb,D)

Title: Data-Driven Target Localization: Benchmarking Gradient Descent Using the
  Cram\'er-Rao Bound
Authors: Shyam Venkatasubramanian, Sandeep Gogineni, Bosung Kang, Ali Pezeshki,
  Muralidhar Rangaswamy, Vahid Tarokh
Categories: eess.SP cs.LG
\\
  In modern radar systems, precise target localization using azimuth and
velocity estimation is paramount. Traditional unbiased estimation methods have
leveraged gradient descent algorithms to reach the theoretical limits of the
Cram\'er Rao Bound (CRB) for the error of the parameter estimates. In this
study, we present a data-driven neural network approach that outperforms these
traditional techniques, demonstrating improved accuracies in target azimuth and
velocity estimation. Using a representative simulated scenario, we show that
our proposed neural network model consistently achieves improved parameter
estimates due to its inherently biased nature, yielding a diminished mean
squared error (MSE). Our findings underscore the potential of employing deep
learning methods in radar systems, paving the way for more accurate
localization in cluttered and dynamic environments.
\\ ( https://arxiv.org/abs/2401.11176 ,  4157kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11196 (*cross-listing*)
Date: Sat, 20 Jan 2024 10:21:51 GMT   (72kb)

Title: Machine learning based state observer for discrete time systems evolving
  on Lie groups
Authors: Soham Shanbhag, Dong Eui Chang
Categories: eess.SY cs.LG cs.SY
\\
  In this paper, a machine learning based observer for systems evolving on
manifolds is designed such that the state of the observer is restricted to the
Lie group on which the system evolves. Conventional techniques involving
machine learning based observers on systems evolving on Lie groups involve
designing charts for the Lie group, training a machine learning based observer
for each chart, and switching between the trained models based on the state of
the system. We propose a novel deep learning based technique whose predictions
are restricted to a measure 0 subset of Euclidean space without using charts.
Using this network, we design an observer ensuring that the state of the
observer is restricted to the Lie group, and predicting the state using only
one trained algorithm. The deep learning network predicts an ``error term'' on
the Lie algebra of the Lie group, uses the map from the Lie algebra to the
group, and uses the group action and the present state to estimate the state at
the next epoch. This model being purely data driven does not require the model
of the system. The proposed algorithm provides a novel framework for
constraining the output of machine learning networks to a measure 0 subset of a
Euclidean space without chart specific training and without requiring
switching. We show the validity of this method using Monte Carlo simulations
performed of the rigid body rotation and translation system.
\\ ( https://arxiv.org/abs/2401.11196 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11313 (*cross-listing*)
Date: Sat, 20 Jan 2024 19:55:36 GMT   (28896kb,D)

Title: Weakly-Supervised Semantic Segmentation of Circular-Scan,
  Synthetic-Aperture-Sonar Imagery
Authors: Isaac J. Sledge, Dominic M. Byrne, Jonathan L. King, Steven H.
  Ostertag, Denton L. Woods, James L. Prater, Jermaine L. Kennedy, Timothy M.
  Marston, Jose C. Principe
Categories: cs.CV cs.LG eess.IV
Comments: Submitted to the IEEE Journal of Oceanic Engineering
\\
  We propose a weakly-supervised framework for the semantic segmentation of
circular-scan synthetic-aperture-sonar (CSAS) imagery. The first part of our
framework is trained in a supervised manner, on image-level labels, to uncover
a set of semi-sparse, spatially-discriminative regions in each image. The
classification uncertainty of each region is then evaluated. Those areas with
the lowest uncertainties are then chosen to be weakly labeled segmentation
seeds, at the pixel level, for the second part of the framework. Each of the
seed extents are progressively resized according to an unsupervised,
information-theoretic loss with structured-prediction regularizers. This
reshaping process uses multi-scale, adaptively-weighted features to delineate
class-specific transitions in local image content. Content-addressable memories
are inserted at various parts of our framework so that it can leverage features
from previously seen images to improve segmentation performance for related
images.
  We evaluate our weakly-supervised framework using real-world CSAS imagery
that contains over ten seafloor classes and ten target classes. We show that
our framework performs comparably to nine fully-supervised deep networks. Our
framework also outperforms eleven of the best weakly-supervised deep networks.
We achieve state-of-the-art performance when pre-training on natural imagery.
The average absolute performance gap to the next-best weakly-supervised network
is well over ten percent for both natural imagery and sonar imagery. This gap
is found to be statistically significant.
\\ ( https://arxiv.org/abs/2401.11313 ,  28896kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11349 (*cross-listing*)
Date: Sun, 21 Jan 2024 00:06:17 GMT   (5486kb,D)

Title: Asynchronous Parallel Reinforcement Learning for Optimizing Propulsive
  Performance in Fin Ray Control
Authors: Xin-Yang Liu, Dariush Bodaghi, Qian Xue, Xudong Zheng, Jian-Xun Wang
Categories: physics.flu-dyn cs.LG cs.SY eess.SY
Comments: 37 pages, 12 figures
\\
  Fish fin rays constitute a sophisticated control system for ray-finned fish,
facilitating versatile locomotion within complex fluid environments. Despite
extensive research on the kinematics and hydrodynamics of fish locomotion, the
intricate control strategies in fin-ray actuation remain largely unexplored.
While deep reinforcement learning (DRL) has demonstrated potential in managing
complex nonlinear dynamics; its trial-and-error nature limits its application
to problems involving computationally demanding environmental interactions.
This study introduces a cutting-edge off-policy DRL algorithm, interacting with
a fluid-structure interaction (FSI) environment to acquire intricate fin-ray
control strategies tailored for various propulsive performance objectives. To
enhance training efficiency and enable scalable parallelism, an innovative
asynchronous parallel training (APT) strategy is proposed, which fully
decouples FSI environment interactions and policy/value network optimization.
The results demonstrated the success of the proposed method in discovering
optimal complex policies for fin-ray actuation control, resulting in a superior
propulsive performance compared to the optimal sinusoidal actuation function
identified through a parametric grid search. The merit and effectiveness of the
APT approach are also showcased through comprehensive comparison with
conventional DRL training strategies in numerical experiments of controlling
nonlinear dynamics.
\\ ( https://arxiv.org/abs/2401.11349 ,  5486kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11354 (*cross-listing*)
Date: Sun, 21 Jan 2024 00:54:50 GMT   (1684kb,D)

Title: Squared Wasserstein-2 Distance for Efficient Reconstruction of
  Stochastic Differential Equations
Authors: Mingtao Xia and Xiangting Li and Qijing Shen and Tom Chou
Categories: math.PR cs.LG stat.ME
Comments: 37 pages, 5 figures
MSC-class: 60H10, 49Q22
\\
  We provide an analysis of the squared Wasserstein-2 ($W_2$) distance between
two probability distributions associated with two stochastic differential
equations (SDEs). Based on this analysis, we propose the use of a squared $W_2$
distance-based loss functions in the \textit{reconstruction} of SDEs from noisy
data. To demonstrate the practicality of our Wasserstein distance-based loss
functions, we performed numerical experiments that demonstrate the efficiency
of our method in reconstructing SDEs that arise across a number of
applications.
\\ ( https://arxiv.org/abs/2401.11354 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11372 (*cross-listing*)
Date: Sun, 21 Jan 2024 02:17:16 GMT   (2460kb,D)

Title: Back-stepping Experience Replay with Application to Model-free
  Reinforcement Learning for a Soft Snake Robot
Authors: Xinda Qi, Dong Chen, Zhaojian Li, Xiaobo Tan
Categories: cs.RO cs.LG
Comments: Submitted to the IEEE for possible publication
\\
  In this paper, we propose a novel technique, Back-stepping Experience Replay
(BER), that is compatible with arbitrary off-policy reinforcement learning (RL)
algorithms. BER aims to enhance learning efficiency in systems with approximate
reversibility, reducing the need for complex reward shaping. The method
constructs reversed trajectories using back-stepping transitions to reach
random or fixed targets. Interpretable as a bi-directional approach, BER
addresses inaccuracies in back-stepping transitions through a distillation of
the replay experience during learning. Given the intricate nature of soft
robots and their complex interactions with environments, we present an
application of BER in a model-free RL approach for the locomotion and
navigation of a soft snake robot, which is capable of serpentine motion enabled
by anisotropic friction between the body and ground. In addition, a dynamic
simulator is developed to assess the effectiveness and efficiency of the BER
algorithm, in which the robot demonstrates successful learning (reaching a 100%
success rate) and adeptly reaches random targets, achieving an average speed
48% faster than that of the best baseline approach.
\\ ( https://arxiv.org/abs/2401.11372 ,  2460kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11378 (*cross-listing*)
Date: Sun, 21 Jan 2024 03:01:00 GMT   (13987kb,D)

Title: Multi-Agent Generative Adversarial Interactive Self-Imitation Learning
  for AUV Formation Control and Obstacle Avoidance
Authors: Zheng Fang, Tianhao Chen, Dong Jiang, Zheng Zhang and Guangliang Li
Categories: cs.RO cs.LG
Comments: 8pages,10figures,Published to RA-L
\\
  Multiple autonomous underwater vehicles (multi-AUV) can cooperatively
accomplish tasks that a single AUV cannot complete. Recently, multi-agent
reinforcement learning has been introduced to control of multi-AUV. However,
designing efficient reward functions for various tasks of multi-AUV control is
difficult or even impractical. Multi-agent generative adversarial imitation
learning (MAGAIL) allows multi-AUV to learn from expert demonstration instead
of pre-defined reward functions, but suffers from the deficiency of requiring
optimal demonstrations and not surpassing provided expert demonstrations. This
paper builds upon the MAGAIL algorithm by proposing multi-agent generative
adversarial interactive self-imitation learning (MAGAISIL), which can
facilitate AUVs to learn policies by gradually replacing the provided
sub-optimal demonstrations with self-generated good trajectories selected by a
human trainer. Our experimental results in a multi-AUV formation control and
obstacle avoidance task on the Gazebo platform with AUV simulator of our lab
show that AUVs trained via MAGAISIL can surpass the provided sub-optimal expert
demonstrations and reach a performance close to or even better than MAGAIL with
optimal demonstrations. Further results indicate that AUVs' policies trained
via MAGAISIL can adapt to complex and different tasks as well as MAGAIL
learning from optimal demonstrations.
\\ ( https://arxiv.org/abs/2401.11378 ,  13987kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11404 (*cross-listing*)
Date: Sun, 21 Jan 2024 05:04:38 GMT   (11934kb,D)

Title: PlasmoData.jl -- A Julia Framework for Modeling and Analyzing Complex
  Data as Graphs
Authors: David L Cole and Victor M Zavala
Categories: cs.MS cs.LG
Comments: 62 pages, 18 figures, 8 tables
\\
  Datasets encountered in scientific and engineering applications appear in
complex formats (e.g., images, multivariate time series, molecules, video, text
strings, networks). Graph theory provides a unifying framework to model such
datasets and enables the use of powerful tools that can help analyze,
visualize, and extract value from data. In this work, we present PlasmoData.jl,
an open-source, Julia framework that uses concepts of graph theory to
facilitate the modeling and analysis of complex datasets. The core of our
framework is a general data modeling abstraction, which we call a DataGraph. We
show how the abstraction and software implementation can be used to represent
diverse data objects as graphs and to enable the use of tools from topology,
graph theory, and machine learning (e.g., graph neural networks) to conduct a
variety of tasks. We illustrate the versatility of the framework by using real
datasets: i) an image classification problem using topological data analysis to
extract features from the graph model to train machine learning models; ii) a
disease outbreak problem where we model multivariate time series as graphs to
detect abnormal events; and iii) a technology pathway analysis problem where we
highlight how we can use graphs to navigate connectivity. Our discussion also
highlights how PlasmoData.jl leverages native Julia capabilities to enable
compact syntax, scalable computations, and interfaces with diverse packages.
\\ ( https://arxiv.org/abs/2401.11404 ,  11934kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11425 (*cross-listing*)
Date: Sun, 21 Jan 2024 08:18:45 GMT   (9656kb,D)

Title: Grayscale Image Colorization with GAN and CycleGAN in Different Image
  Domain
Authors: Chen Liang, Yunchen Sheng, Yichen Mo
Categories: cs.CV cs.LG
ACM-class: I.4.3
\\
  Automatic colorization of grayscale image has been a challenging task.
Previous research have applied supervised methods in conquering this problem [
1]. In this paper, we reproduces a GAN-based coloring model, and experiments
one of its variant. We also proposed a CycleGAN based model and experiments
those methods on various datasets. The result shows that the proposed CycleGAN
model does well in human-face coloring and comic coloring, but lack the ability
to diverse colorization.
\\ ( https://arxiv.org/abs/2401.11425 ,  9656kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11464 (*cross-listing*)
Date: Sun, 21 Jan 2024 11:12:00 GMT   (362kb)

Title: Task-specific regularization loss towards model calibration for reliable
  lung cancer detection
Authors: Mehar Prateek Kalra, Mansi Singhal, Rohan Raju Dhanakashirur
Categories: eess.IV cs.CV cs.LG
\\
  Lung cancer is one of the significant causes of cancer-related deaths
globally. Early detection and treatment improve the chances of survival.
Traditionally CT scans have been used to extract the most significant lung
infection information and diagnose cancer. This process is carried out manually
by an expert radiologist. The imbalance in the radiologists-to-population ratio
in a country like India implies significant work pressure on them and thus
raises the need to automate a few of their responsibilities. The tendency of
modern-day Deep Neural networks to make overconfident mistakes limit their
usage to detect cancer. In this paper, we propose a new task-specific loss
function to calibrate the neural network to reduce the risk of overconfident
mistakes. We use the state-of-the-art Multi-class Difference in Confidence and
Accuracy (MDCA) loss in conjunction with the proposed task-specific loss
function to achieve the same. We also integrate post-hoc calibration by
performing temperature scaling on top of the train-time calibrated model. We
demonstrate 5.98% improvement in the Expected Calibration Error (ECE) and a
17.9% improvement in Maximum Calibration Error (MCE) as compared to the
best-performing SOTA algorithm.
\\ ( https://arxiv.org/abs/2401.11464 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11488 (*cross-listing*)
Date: Sun, 21 Jan 2024 13:24:41 GMT   (1135kb,D)

Title: HARDCORE: H-field and power loss estimation for arbitrary waveforms with
  residual, dilated convolutional neural networks in ferrite cores
Authors: Nikolas F\"orster, Wilhelm Kirchg\"assner, Till Piepenbrock, Oliver
  Schweins, Oliver Wallscheid
Categories: eess.SY cs.LG cs.SY physics.app-ph
Comments: Competition submission version
\\
  The MagNet Challenge 2023 calls upon competitors to develop data-driven
models for the material-specific, waveform-agnostic estimation of steady-state
power losses in toroidal ferrite cores. The following HARDCORE (H-field and
power loss estimation for Arbitrary waveforms with Residual, Dilated
convolutional neural networks in ferrite COREs) approach shows that a residual
convolutional neural network with physics-informed extensions can serve this
task efficiently when trained on observational data beforehand. One key
solution element is an intermediate model layer which first reconstructs the bh
curve and then estimates the power losses based on the curve's area rendering
the proposed topology physically interpretable. In addition, emphasis was
placed on expert-based feature engineering and information-rich inputs in order
to enable a lean model architecture. A model is trained from scratch for each
material, while the topology remains the same. A Pareto-style trade-off between
model size and estimation accuracy is demonstrated, which yields an optimum at
as low as 1755 parameters and down to below 8\,\% for the 95-th percentile of
the relative error for the worst-case material with sufficient samples.
\\ ( https://arxiv.org/abs/2401.11488 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11506 (*cross-listing*)
Date: Sun, 21 Jan 2024 14:33:52 GMT   (531kb)

Title: Enhancing Recommendation Diversity by Re-ranking with Large Language
  Models
Authors: Diego Carraro and Derek Bridge
Categories: cs.IR cs.LG
Comments: 32 pages, 2 figures
\\
  It has long been recognized that it is not enough for a Recommender System
(RS) to provide recommendations based only on their relevance to users. Among
many other criteria, the set of recommendations may need to be diverse in order
to handle uncertainty and offer a meaningful choice. The literature reports
many ways of measuring diversity and ways of improving the diversity of a set
of recommendations, most notably by re-ranking and selecting from a larger set
of candidate recommendations. Driven by promising insights from the literature
on how to incorporate versatile Large Language Models (LLMs) into the RS
pipeline, in this paper, we show how LLMs can be used for diversity re-ranking.
  We begin with an informal study that verifies that LLMs can be used for
re-ranking tasks and do have some understanding of the concept of diversity.
Then, we design a more rigorous methodology where LLMs are prompted to generate
a diverse ranking from a candidate ranking using various prompt templates with
different re-ranking instructions in a zero-shot fashion. We conduct
comprehensive experiments testing state-of-the-art conversational LLMs from the
GPT and Llama families. We compare their re-ranking capabilities with random
re-ranking and various traditional re-ranking methods from the literature (MMR,
xQuAD and RxQuAD). We find that LLM-based re-ranking outperforms random
re-ranking across all the metrics that we use but does not perform as well as
the traditional re-ranking methods. We gain insight into prompt design for this
task (e.g.\ on the whole, it is better to prompt for diversity rather than a
balance of diversity and relevance). Given that no special knowledge
engineering is needed, we conclude that LLM-based re-ranking is a promising
approach, and we highlight directions for future research. We open-source the
code of our experiments for reproducibility.
\\ ( https://arxiv.org/abs/2401.11506 ,  531kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11513 (*cross-listing*)
Date: Sun, 21 Jan 2024 14:52:39 GMT   (1655kb,D)

Title: Exploring the Truth and Beauty of Theory Landscapes with Machine
  Learning
Authors: Konstantin T. Matchev, Katia Matcheva, Pierre Ramond, Sarunas Verner
Categories: hep-ph cs.LG
Comments: 13 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:2311.00087
\\
  Theoretical physicists describe nature by i) building a theory model and ii)
determining the model parameters. The latter step involves the dual aspect of
both fitting to the existing experimental data and satisfying abstract criteria
like beauty, naturalness, etc. We use the Yukawa quark sector as a toy example
to demonstrate how both of those tasks can be accomplished with machine
learning techniques. We propose loss functions whose minimization results in
true models that are also beautiful as measured by three different criteria -
uniformity, sparsity, or symmetry.
\\ ( https://arxiv.org/abs/2401.11513 ,  1655kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11519 (*cross-listing*)
Date: Sun, 21 Jan 2024 15:22:15 GMT   (22580kb,D)

Title: CaBuAr: California Burned Areas dataset for delineation
Authors: Daniele Rege Cambrin, Luca Colomba, Paolo Garza
Categories: cs.CV cs.LG eess.IV
Comments: Accepted at the IEEE Geoscience and Remote Sensing Magazine
Report-no: D. R. Cambrin, L. Colomba and P. Garza, "CaBuAr: California burned
  areas dataset for delineation [Software and Data Sets]," in IEEE Geoscience
  and Remote Sensing Magazine, vol. 11, no. 3, pp. 106-113, Sept. 2023
DOI: 10.1109/MGRS.2023.3292467
\\
  Forest wildfires represent one of the catastrophic events that, over the last
decades, caused huge environmental and humanitarian damages. In addition to a
significant amount of carbon dioxide emission, they are a source of risk to
society in both short-term (e.g., temporary city evacuation due to fire) and
long-term (e.g., higher risks of landslides) cases. Consequently, the
availability of tools to support local authorities in automatically identifying
burned areas plays an important role in the continuous monitoring requirement
to alleviate the aftereffects of such catastrophic events. The great
availability of satellite acquisitions coupled with computer vision techniques
represents an important step in developing such tools. This paper introduces a
novel open dataset that tackles the burned area delineation problem, a binary
segmentation problem applied to satellite imagery. The presented resource
consists of pre- and post-fire Sentinel-2 L2A acquisitions of California forest
fires that took place starting in 2015. Raster annotations were generated from
the data released by California's Department of Forestry and Fire Protection.
Moreover, in conjunction with the dataset, we release three different baselines
based on spectral indexes analyses, SegFormer, and U-Net models.
\\ ( https://arxiv.org/abs/2401.11519 ,  22580kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11531 (*cross-listing*)
Date: Sun, 21 Jan 2024 15:57:04 GMT   (3041kb,D)

Title: Tempo: Confidentiality Preservation in Cloud-Based Neural Network
  Training
Authors: Rongwu Xu and Zhixuan Fang
Categories: cs.CR cs.LG
\\
  Cloud deep learning platforms provide cost-effective deep neural network
(DNN) training for customers who lack computation resources. However, cloud
systems are often untrustworthy and vulnerable to attackers, leading to growing
concerns about model privacy. Recently, researchers have sought to protect data
privacy in deep learning by leveraging CPU trusted execution environments
(TEEs), which minimize the use of cryptography, but existing works failed to
simultaneously utilize the computational resources of GPUs to assist in
training and prevent model leakage. This paper presents Tempo, the first
cloud-based deep learning system that cooperates with TEE and distributed GPUs
for efficient DNN training with model confidentiality preserved. To tackle the
challenge of preserving privacy while offloading linear algebraic operations
from TEE to GPUs for efficient batch computation, we introduce a customized
permutation-based obfuscation algorithm to blind both inputs and model
parameters. An optimization mechanism that reduces encryption operations is
proposed for faster weight updates during backpropagation to speed up training.
We implement Tempo and evaluate it with both training and inference for two
prevalent DNNs. Empirical results indicate that Tempo outperforms baselines and
offers sufficient privacy protection.
\\ ( https://arxiv.org/abs/2401.11531 ,  3041kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11555 (*cross-listing*)
Date: Sun, 21 Jan 2024 18:00:15 GMT   (4207kb,D)

Title: VQC-Based Reinforcement Learning with Data Re-uploading: Performance and
  Trainability
Authors: Rodrigo Coelho, Andr\'e Sequeira, Lu\'is Paulo Santos
Categories: quant-ph cs.LG
Comments: 21 pages, 8 figures
\\
  Reinforcement Learning (RL) consists of designing agents that make
intelligent decisions without human supervision. When used alongside function
approximators such as Neural Networks (NNs), RL is capable of solving extremely
complex problems. Deep Q-Learning, a RL algorithm that uses Deep NNs, achieved
super-human performance in some specific tasks. Nonetheless, it is also
possible to use Variational Quantum Circuits (VQCs) as function approximators
in RL algorithms. This work empirically studies the performance and
trainability of such VQC-based Deep Q-Learning models in classic control
benchmark environments. More specifically, we research how data re-uploading
affects both these metrics. We show that the magnitude and the variance of the
gradients of these models remain substantial throughout training due to the
moving targets of Deep Q-Learning. Moreover, we empirically show that
increasing the number of qubits does not lead to an exponential vanishing
behavior of the magnitude and variance of the gradients for a PQC approximating
a 2-design, unlike what was expected due to the Barren Plateau Phenomenon. This
hints at the possibility of VQCs being specially adequate for being used as
function approximators in such a context.
\\ ( https://arxiv.org/abs/2401.11555 ,  4207kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11562 (*cross-listing*)
Date: Sun, 21 Jan 2024 18:43:18 GMT   (143kb,D)

Title: Enhancing selectivity using Wasserstein distance based reweighing
Authors: Pratik Worah
Categories: stat.ML cs.LG q-bio.QM
\\
  Given two labeled data-sets $\mathcal{S}$ and $\mathcal{T}$, we design a
simple and efficient greedy algorithm to reweigh the loss function such that
the limiting distribution of the neural network weights that result from
training on $\mathcal{S}$ approaches the limiting distribution that would have
resulted by training on $\mathcal{T}$.
  On the theoretical side, we prove that when the metric entropy of the input
data-sets is bounded, our greedy algorithm outputs a close to optimal
reweighing, i.e., the two invariant distributions of network weights will be
provably close in total variation distance. Moreover, the algorithm is simple
and scalable, and we prove bounds on the efficiency of the algorithm as well.
  Our algorithm can deliberately introduce distribution shift to perform (soft)
multi-criteria optimization. As a motivating application, we train a neural net
to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell
signaling) which are non-binders to MNK1 (a highly similar protein). We tune
the algorithm's parameter so that overall change in holdout loss is negligible,
but the selectivity, i.e., the fraction of top 100 MNK2 binders that are MNK1
non-binders, increases from 54\% to 95\%, as a result of our reweighing. Of the
43 distinct small molecules predicted to be most selective from the enamine
catalog, 2 small molecules were experimentally verified to be selective, i.e.,
they reduced the enzyme activity of MNK2 below 50\% but not MNK1, at 10$\mu$M
-- a 5\% success rate.
\\ ( https://arxiv.org/abs/2401.11562 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11576 (*cross-listing*)
Date: Sun, 21 Jan 2024 19:53:17 GMT   (17985kb,D)

Title: Quantum Architecture Search with Unsupervised Representation Learning
Authors: Yize Sun, Zixin Wu, Yunpu Ma, Volker Tresp
Categories: quant-ph cs.LG
Comments: 9 Pages, quantum architecture search, unsupervised representation
  learning
\\
  Utilizing unsupervised representation learning for quantum architecture
search (QAS) represents a cutting-edge approach poised to realize potential
quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) devices. Most QAS
algorithms combine their search space and search algorithms together and thus
generally require evaluating a large number of quantum circuits during the
search process. Predictor-based QAS algorithms can alleviate this problem by
directly estimating the performance of circuits according to their structures.
However, a high-performance predictor generally requires very time-consuming
labeling to obtain a large number of labeled quantum circuits. Recently, a
classical neural architecture search algorithm Arch2vec inspires us by showing
that architecture search can benefit from decoupling unsupervised
representation learning from the search process. Whether unsupervised
representation learning can help QAS without any predictor is still an open
topic. In this work, we propose a framework QAS with unsupervised
representation learning and visualize how unsupervised architecture
representation learning encourages quantum circuit architectures with similar
connections and operators to cluster together. Specifically, our framework
enables the process of QAS to be decoupled from unsupervised architecture
representation learning so that the learned representation can be directly
applied to different downstream applications. Furthermore, our framework is
predictor-free eliminating the need for a large number of labeled quantum
circuits. During the search process, we use two algorithms REINFORCE and
Bayesian Optimization to directly search on the latent representation, and
compare them with the method Random Search. The results show our framework can
more efficiently get well-performing candidate circuits within a limited number
of searches.
\\ ( https://arxiv.org/abs/2401.11576 ,  17985kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11582 (*cross-listing*)
Date: Sun, 21 Jan 2024 20:10:02 GMT   (3920kb,D)

Title: Thermal Image Calibration and Correction using Unpaired Cycle-Consistent
  Adversarial Networks
Authors: Hossein Rajoli, Pouya Afshin, Fatemeh Afghah
Categories: cs.CV cs.LG eess.IV
Comments: This paper has been accepted at the Asilomar 2023 Conference and will
  be published
\\
  Unmanned aerial vehicles (UAVs) offer a flexible and cost-effective solution
for wildfire monitoring. However, their widespread deployment during wildfires
has been hindered by a lack of operational guidelines and concerns about
potential interference with aircraft systems. Consequently, the progress in
developing deep-learning models for wildfire detection and characterization
using aerial images is constrained by the limited availability, size, and
quality of existing datasets. This paper introduces a solution aimed at
enhancing the quality of current aerial wildfire datasets to align with
advancements in camera technology. The proposed approach offers a solution to
create a comprehensive, standardized large-scale image dataset. This paper
presents a pipeline based on CycleGAN to enhance wildfire datasets and a novel
fusion method that integrates paired RGB images as attribute conditioning in
the generators of both directions, improving the accuracy of the generated
images.
\\ ( https://arxiv.org/abs/2401.11582 ,  3920kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11608 (*cross-listing*)
Date: Sun, 21 Jan 2024 22:01:34 GMT   (1079kb,D)

Title: $\texttt{immrax}$: A Parallelizable and Differentiable Toolbox for
  Interval Analysis and Mixed Monotone Reachability in JAX
Authors: Akash Harapanahalli, Saber Jafarpour, Samuel Coogan
Categories: eess.SY cs.LG cs.SY math.OC
\\
  We present an implementation of interval analysis and mixed monotone interval
reachability analysis as function transforms in Python, fully composable with
the computational framework JAX. The resulting toolbox inherits several key
features from JAX, including computational efficiency through Just-In-Time
Compilation, GPU acceleration for quick parallelized computations, and
Automatic Differentiability. We demonstrate the toolbox's performance on
several case studies, including a reachability problem on a vehicle model
controlled by a neural network, and a robust closed-loop optimal control
problem for a swinging pendulum.
\\ ( https://arxiv.org/abs/2401.11608 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11621 (*cross-listing*)
Date: Fri, 5 Jan 2024 17:13:30 GMT   (898kb)

Title: A Novel Decision Ensemble Framework: Customized Attention-BiLSTM and
  XGBoost for Speculative Stock Price Forecasting
Authors: Riaz Ud Din, Salman Ahmed, Saddam Hussain Khan
Categories: q-fin.ST cs.CE cs.LG
Comments: 30 pages, 16 Figures, 4 Tables
\\
  Forecasting speculative stock prices is essential for effective investment
risk management that drives the need for the development of innovative
algorithms. However, the speculative nature, volatility, and complex sequential
dependencies within financial markets present inherent challenges which
necessitate advanced techniques. This paper proposes a novel framework, CAB-XDE
(customized attention BiLSTM-XGB decision ensemble), for predicting the daily
closing price of speculative stock Bitcoin-USD (BTC-USD). CAB-XDE framework
integrates a customized bi-directional long short-term memory (BiLSTM) with the
attention mechanism and the XGBoost algorithm. The customized BiLSTM leverages
its learning capabilities to capture the complex sequential dependencies and
speculative market trends. Additionally, the new attention mechanism
dynamically assigns weights to influential features, thereby enhancing
interpretability, and optimizing effective cost measures and volatility
forecasting. Moreover, XGBoost handles nonlinear relationships and contributes
to the proposed CAB-XDE framework robustness. Additionally, the weight
determination theory-error reciprocal method further refines predictions. This
refinement is achieved by iteratively adjusting model weights. It is based on
discrepancies between theoretical expectations and actual errors in individual
customized attention BiLSTM and XGBoost models to enhance performance. Finally,
the predictions from both XGBoost and customized attention BiLSTM models are
concatenated to achieve diverse prediction space and are provided to the
ensemble classifier to enhance the generalization capabilities of CAB-XDE. The
proposed CAB-XDE framework is empirically validated on volatile Bitcoin market,
sourced from Yahoo Finance and outperforms state-of-the-art models with a MAPE
of 0.0037, MAE of 84.40, and RMSE of 106.14.
\\ ( https://arxiv.org/abs/2401.11621 ,  898kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11632 (*cross-listing*)
Date: Sun, 21 Jan 2024 23:56:57 GMT   (3101kb,D)

Title: What Are We Optimizing For? A Human-centric Evaluation Of Deep
  Learning-based Recommender Systems
Authors: Ruixuan Sun, Avinash Akella, Xinyi Wu, Ruoyan Kong, Joseph A. Konstan
Categories: cs.IR cs.HC cs.LG
\\
  Deep learning-based (DL) models in recommender systems (RecSys) have gained
significant recognition for their remarkable accuracy in predicting user
preferences. However, their performance often lacks a comprehensive evaluation
from a human-centric perspective, which encompasses various dimensions beyond
simple interest matching. In this work, we have developed a robust
human-centric evaluation framework that incorporates seven diverse metrics to
assess the quality of recommendations generated by five recent open-sourced DL
models. Our evaluation datasets consist of both offline benchmark data and
personalized online recommendation feedback collected from 445 real users. We
find that (1) different DL models have different pros and cons in the
multi-dimensional metrics that we test with; (2) users generally want a
combination of accuracy with at least one another human values in the
recommendation; (3) the degree of combination of different values needs to be
carefully experimented to user preferred level.
\\ ( https://arxiv.org/abs/2401.11632 ,  3101kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11646 (*cross-listing*)
Date: Mon, 22 Jan 2024 01:45:34 GMT   (531kb,D)

Title: Nonparametric Estimation via Variance-Reduced Sketching
Authors: Yuehaw Khoo, Yifan Peng, and Daren Wang
Categories: stat.ML cs.LG cs.NA math.NA stat.ME
Comments: 64 pages, 8 figures
\\
  Nonparametric models are of great interest in various scientific and
engineering disciplines. Classical kernel methods, while numerically robust and
statistically sound in low-dimensional settings, become inadequate in
higher-dimensional settings due to the curse of dimensionality. In this paper,
we introduce a new framework called Variance-Reduced Sketching (VRS),
specifically designed to estimate density functions and nonparametric
regression functions in higher dimensions with a reduced curse of
dimensionality. Our framework conceptualizes multivariable functions as
infinite-size matrices, and facilitates a new sketching technique motivated by
numerical linear algebra literature to reduce the variance in estimation
problems. We demonstrate the robust numerical performance of VRS through a
series of simulated experiments and real-world data applications. Notably, VRS
shows remarkable improvement over existing neural network estimators and
classical kernel methods in numerous density estimation and nonparametric
regression models. Additionally, we offer theoretical justifications for VRS to
support its ability to deliver nonparametric estimation with a reduced curse of
dimensionality.
\\ ( https://arxiv.org/abs/2401.11646 ,  531kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11652 (*cross-listing*)
Date: Mon, 22 Jan 2024 02:17:36 GMT   (7616kb,D)

Title: OnDev-LCT: On-Device Lightweight Convolutional Transformers towards
  federated learning
Authors: Chu Myaet Thwal, Minh N.H. Nguyen, Ye Lin Tun, Seong Tae Kim, My T.
  Thai, Choong Seon Hong
Categories: cs.CV cs.LG
Comments: Published in Neural Networks
DOI: 10.1016/j.neunet.2023.11.044
\\
  Federated learning (FL) has emerged as a promising approach to
collaboratively train machine learning models across multiple edge devices
while preserving privacy. The success of FL hinges on the efficiency of
participating models and their ability to handle the unique challenges of
distributed learning. While several variants of Vision Transformer (ViT) have
shown great potential as alternatives to modern convolutional neural networks
(CNNs) for centralized training, the unprecedented size and higher
computational demands hinder their deployment on resource-constrained edge
devices, challenging their widespread application in FL. Since client devices
in FL typically have limited computing resources and communication bandwidth,
models intended for such devices must strike a balance between model size,
computational efficiency, and the ability to adapt to the diverse and non-IID
data distributions encountered in FL. To address these challenges, we propose
OnDev-LCT: Lightweight Convolutional Transformers for On-Device vision tasks
with limited training data and resources. Our models incorporate image-specific
inductive biases through the LCT tokenizer by leveraging efficient depthwise
separable convolutions in residual linear bottleneck blocks to extract local
features, while the multi-head self-attention (MHSA) mechanism in the LCT
encoder implicitly facilitates capturing global representations of images.
Extensive experiments on benchmark image datasets indicate that our models
outperform existing lightweight vision models while having fewer parameters and
lower computational demands, making them suitable for FL scenarios with data
heterogeneity and communication bottlenecks.
\\ ( https://arxiv.org/abs/2401.11652 ,  7616kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11671 (*cross-listing*)
Date: Mon, 22 Jan 2024 03:09:00 GMT   (5304kb,D)

Title: RTA-Former: Reverse Transformer Attention for Polyp Segmentation
Authors: Zhikai Li, Murong Yi, Ali Uneri, Sihan Niu, and Craig Jones
Categories: eess.IV cs.CV cs.LG
\\
  Polyp segmentation is a key aspect of colorectal cancer prevention, enabling
early detection and guiding subsequent treatments. Intelligent diagnostic
tools, including deep learning solutions, are widely explored to streamline and
potentially automate this process. However, even with many powerful network
architectures, there still comes the problem of producing accurate edge
segmentation. In this paper, we introduce a novel network, namely RTA-Former,
that employs a transformer model as the encoder backbone and innovatively
adapts Reverse Attention (RA) with a transformer stage in the decoder for
enhanced edge segmentation. The results of the experiments illustrate that
RTA-Former achieves state-of-the-art (SOTA) performance in five polyp
segmentation datasets. The strong capability of RTA-Former holds promise in
improving the accuracy of Transformer-based polyp segmentation, potentially
leading to better clinical decisions and patient outcomes. Our code will be
publicly available on GitHub.
\\ ( https://arxiv.org/abs/2401.11671 ,  5304kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11679 (*cross-listing*)
Date: Mon, 22 Jan 2024 03:44:35 GMT   (28984kb,D)

Title: Simulating Nighttime Visible Satellite Imagery of Tropical Cyclones
  Using Conditional Generative Adversarial Networks
Authors: Jinghuai Yao, Puyuan Du, Yucheng Zhao, and Yubo Wang
Categories: physics.ao-ph cs.LG
\\
  Visible (VIS) imagery of satellites has various important applications in
meteorology, including monitoring Tropical Cyclones (TCs). However, it is
unavailable at night because of the lack of sunlight. This study presents a
Conditional Generative Adversarial Networks (CGAN) model that generates highly
accurate nighttime visible reflectance using infrared (IR) bands and sunlight
direction parameters as input. The model was trained and validated using target
area observations of the Advanced Himawari Imager (AHI) in the daytime. This
study also presents the first nighttime model validation using the Day/Night
Band (DNB) of the Visible/Infrared Imager Radiometer Suite (VIIRS). The daytime
statistical results of the Structural Similarity Index Measure (SSIM), Peak
Signal-to-Noise Ratio (PSNR), Root Mean Square Error (RMSE), Correlation
Coefficient (CC), and Bias are 0.885, 28.3, 0.0428, 0.984, and -0.0016
respectively, completely surpassing the model performance of previous studies.
The nighttime statistical results of SSIM, PSNR, RMSE, and CC are 0.821, 24.4,
0.0643, and 0.969 respectively, which are slightly negatively impacted by the
parallax between satellites. We performed full-disk model validation which
proves our model could also be readily applied in the tropical ocean without
TCs in the northern hemisphere. This model contributes to the nighttime
monitoring of meteorological phenomena by providing accurate AI-generated
visible imagery with adjustable virtual sunlight directions.
\\ ( https://arxiv.org/abs/2401.11679 ,  28984kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11687 (*cross-listing*)
Date: Mon, 22 Jan 2024 04:54:42 GMT   (464kb,D)

Title: TIM: An Efficient Temporal Interaction Module for Spiking Transformer
Authors: Sicheng Shen, Dongcheng Zhao, Guobin Shen and Yi Zeng
Categories: cs.NE cs.CV cs.LG
Comments: 10pages,6figures
\\
  Spiking Neural Networks (SNNs), as the third generation of neural networks,
have gained prominence for their biological plausibility and computational
efficiency, especially in processing diverse datasets. The integration of
attention mechanisms, inspired by advancements in neural network architectures,
has led to the development of Spiking Transformers. These have shown promise in
enhancing SNNs' capabilities, particularly in the realms of both static and
neuromorphic datasets. Despite their progress, a discernible gap exists in
these systems, specifically in the Spiking Self Attention (SSA) mechanism's
effectiveness in leveraging the temporal processing potential of SNNs. To
address this, we introduce the Temporal Interaction Module (TIM), a novel,
convolution-based enhancement designed to augment the temporal data processing
abilities within SNN architectures. TIM's integration into existing SNN
frameworks is seamless and efficient, requiring minimal additional parameters
while significantly boosting their temporal information handling capabilities.
Through rigorous experimentation, TIM has demonstrated its effectiveness in
exploiting temporal information, leading to state-of-the-art performance across
various neuromorphic datasets.
\\ ( https://arxiv.org/abs/2401.11687 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11739 (*cross-listing*)
Date: Mon, 22 Jan 2024 07:34:06 GMT   (44077kb,D)

Title: EmerDiff: Emerging Pixel-level Semantic Knowledge in Diffusion Models
Authors: Koichi Namekata, Amirmojtaba Sabour, Sanja Fidler, Seung Wook Kim
Categories: cs.CV cs.LG
Comments: ICLR 2024. Project page: https://kmcode1.github.io/Projects/EmerDiff/
\\
  Diffusion models have recently received increasing research attention for
their remarkable transfer abilities in semantic segmentation tasks. However,
generating fine-grained segmentation masks with diffusion models often requires
additional training on annotated datasets, leaving it unclear to what extent
pre-trained diffusion models alone understand the semantic relations of their
generated images. To address this question, we leverage the semantic knowledge
extracted from Stable Diffusion (SD) and aim to develop an image segmentor
capable of generating fine-grained segmentation maps without any additional
training. The primary difficulty stems from the fact that semantically
meaningful feature maps typically exist only in the spatially lower-dimensional
layers, which poses a challenge in directly extracting pixel-level semantic
relations from these feature maps. To overcome this issue, our framework
identifies semantic correspondences between image pixels and spatial locations
of low-dimensional feature maps by exploiting SD's generation process and
utilizes them for constructing image-resolution segmentation maps. In extensive
experiments, the produced segmentation maps are demonstrated to be well
delineated and capture detailed parts of the images, indicating the existence
of highly accurate pixel-level semantic knowledge in diffusion models.
\\ ( https://arxiv.org/abs/2401.11739 ,  44077kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11740 (*cross-listing*)
Date: Mon, 22 Jan 2024 07:37:25 GMT   (3259kb,D)

Title: Multi-level Cross-modal Alignment for Image Clustering
Authors: Liping Qiu and Qin Zhang and Xiaojun Chen and Shaotian Cai
Categories: cs.CV cs.LG
\\
  Recently, the cross-modal pretraining model has been employed to produce
meaningful pseudo-labels to supervise the training of an image clustering
model. However, numerous erroneous alignments in a cross-modal pre-training
model could produce poor-quality pseudo-labels and degrade clustering
performance. To solve the aforementioned issue, we propose a novel
\textbf{Multi-level Cross-modal Alignment} method to improve the alignments in
a cross-modal pretraining model for downstream tasks, by building a smaller but
better semantic space and aligning the images and texts in three levels, i.e.,
instance-level, prototype-level, and semantic-level. Theoretical results show
that our proposed method converges, and suggests effective means to reduce the
expected clustering risk of our method. Experimental results on five benchmark
datasets clearly show the superiority of our new method.
\\ ( https://arxiv.org/abs/2401.11740 ,  3259kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11825 (*cross-listing*)
Date: Mon, 22 Jan 2024 10:38:14 GMT   (1425kb,D)

Title: Sparse discovery of differential equations based on multi-fidelity
  Gaussian process
Authors: Yuhuang Meng and Yue Qiu
Categories: math.NA cs.LG cs.NA
\\
  Sparse identification of differential equations aims to compute the analytic
expressions from the observed data explicitly. However, there exist two primary
challenges. Firstly, it exhibits sensitivity to the noise in the observed data,
particularly for the derivatives computations. Secondly, existing literature
predominantly concentrates on single-fidelity (SF) data, which imposes
limitations on its applicability due to the computational cost. In this paper,
we present two novel approaches to address these problems from the view of
uncertainty quantification. We construct a surrogate model employing the
Gaussian process regression (GPR) to mitigate the effect of noise in the
observed data, quantify its uncertainty, and ultimately recover the equations
accurately. Subsequently, we exploit the multi-fidelity Gaussian processes
(MFGP) to address scenarios involving multi-fidelity (MF), sparse, and noisy
observed data. We demonstrate the robustness and effectiveness of our
methodologies through several numerical experiments.
\\ ( https://arxiv.org/abs/2401.11825 ,  1425kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11888 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:28:50 GMT   (418kb,D)

Title: Multimodal Deep Learning of Word-of-Mouth Text and Demographics to
  Predict Customer Rating: Handling Consumer Heterogeneity in Marketing
Authors: Junichiro Niimi
Categories: cs.CE cs.LG
\\
  In the marketing field, understanding consumer heterogeneity, which is the
internal or psychological difference among consumers that cannot be captured by
behavioral logs, has long been a critical challenge. However, a number of
consumers today usually post their evaluation on the specific product on the
online platform, which can be the valuable source of such unobservable
differences among consumers. Several previous studies have shown the validity
of the analysis on text modality, but on the other hand, such analyses may not
necessarily demonstrate sufficient predictive accuracy for text alone, as they
may not include information readily available from cross-sectional data, such
as consumer profile data. In addition, recent advances in machine learning
techniques, such as large-scale language models (LLMs) and multimodal learning
have made it possible to deal with the various kind of dataset simultaneously,
including textual data and the traditional cross-sectional data, and the joint
representations can be effectively obtained from multiple modalities.
Therefore, this study constructs a product evaluation model that takes into
account consumer heterogeneity by multimodal learning of online product reviews
and consumer profile information. We also compare multiple models using
different modalities or hyper-parameters to demonstrate the robustness of
multimodal learning in marketing analysis.
\\ ( https://arxiv.org/abs/2401.11888 ,  418kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12004 (*cross-listing*)
Date: Mon, 22 Jan 2024 14:53:21 GMT   (4027kb)

Title: NLCG-Net: A Model-Based Zero-Shot Learning Framework for Undersampled
  Quantitative MRI Reconstruction
Authors: Xinrui Jiang, Yohan Jun, Jaejin Cho, Mengze Gao, Xingwang Yong, Berkin
  Bilgic
Categories: eess.IV cs.LG eess.SP
Comments: 8 pages, 5 figures, submitted to International Society for Magnetic
  Resonance in Medicine 2024
\\
  Typical quantitative MRI (qMRI) methods estimate parameter maps after image
reconstructing, which is prone to biases and error propagation. We propose a
Nonlinear Conjugate Gradient (NLCG) optimizer for model-based T2/T1 estimation,
which incorporates U-Net regularization trained in a scan-specific manner. This
end-to-end method directly estimates qMRI maps from undersampled k-space data
using mono-exponential signal modeling with zero-shot scan-specific neural
network regularization to enable high fidelity T1 and T2 mapping. T2 and T1
mapping results demonstrate the ability of the proposed NLCG-Net to improve
estimation quality compared to subspace reconstruction at high accelerations.
\\ ( https://arxiv.org/abs/2401.12004 ,  4027kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12046 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:38:29 GMT   (11452kb,D)

Title: Fourier Transporter: Bi-Equivariant Robotic Manipulation in 3D
Authors: Haojie Huang, Owen Howell, Xupeng Zhu, Dian Wang, Robin Walters,
  Robert Platt
Categories: cs.RO cs.LG
\\
  Many complex robotic manipulation tasks can be decomposed as a sequence of
pick and place actions. Training a robotic agent to learn this sequence over
many different starting conditions typically requires many iterations or
demonstrations, especially in 3D environments. In this work, we propose Fourier
Transporter (\ours{}) which leverages the two-fold $\SE(d)\times\SE(d)$
symmetry in the pick-place problem to achieve much higher sample efficiency.
\ours{} is an open-loop behavior cloning method trained using expert
demonstrations to predict pick-place actions on new environments. \ours{} is
constrained to incorporate symmetries of the pick and place actions
independently. Our method utilizes a fiber space Fourier transformation that
allows for memory-efficient construction. We test our proposed network on the
RLbench benchmark and achieve state-of-the-art results across various tasks.
\\ ( https://arxiv.org/abs/2401.12046 ,  11452kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12055 (*cross-listing*)
Date: Mon, 22 Jan 2024 15:47:05 GMT   (4739kb,D)

Title: NEUROSEC: FPGA-Based Neuromorphic Audio Security
Authors: Murat Isik, Hiruna Vishwamith, Yusuf Sur, Kayode Inadagbo, and I. Can
  Dikmen
Categories: cs.CR cs.ET cs.LG cs.NE cs.SD eess.AS
Comments: Audio processing, FPGA, Hardware Security, Neuromorphic Computing
\\
  Neuromorphic systems, inspired by the complexity and functionality of the
human brain, have gained interest in academic and industrial attention due to
their unparalleled potential across a wide range of applications. While their
capabilities herald innovation, it is imperative to underscore that these
computational paradigms, analogous to their traditional counterparts, are not
impervious to security threats. Although the exploration of neuromorphic
methodologies for image and video processing has been rigorously pursued, the
realm of neuromorphic audio processing remains in its early stages. Our results
highlight the robustness and precision of our FPGA-based neuromorphic system.
Specifically, our system showcases a commendable balance between desired signal
and background noise, efficient spike rate encoding, and unparalleled
resilience against adversarial attacks such as FGSM and PGD. A standout feature
of our framework is its detection rate of 94%, which, when compared to other
methodologies, underscores its greater capability in identifying and mitigating
threats within 5.39 dB, a commendable SNR ratio. Furthermore, neuromorphic
computing and hardware security serve many sensor domains in mission-critical
and privacy-preserving applications.
\\ ( https://arxiv.org/abs/2401.12055 ,  4739kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12068 (*cross-listing*)
Date: Mon, 22 Jan 2024 16:05:30 GMT   (476kb,D)

Title: Resource-constrained stereo singing voice cancellation
Authors: Clara Borrelli, James Rae, Dogac Basaran, Matt McVicar, Mehrez Souden,
  Matthias Mauch
Categories: cs.SD cs.LG eess.AS
\\
  We study the problem of stereo singing voice cancellation, a subtask of music
source separation, whose goal is to estimate an instrumental background from a
stereo mix. We explore how to achieve performance similar to large
state-of-the-art source separation networks starting from a small, efficient
model for real-time speech separation. Such a model is useful when memory and
compute are limited and singing voice processing has to run with limited
look-ahead. In practice, this is realised by adapting an existing mono model to
handle stereo input. Improvements in quality are obtained by tuning model
parameters and expanding the training set. Moreover, we highlight the benefits
a stereo model brings by introducing a new metric which detects attenuation
inconsistencies between channels. Our approach is evaluated using objective
offline metrics and a large-scale MUSHRA trial, confirming the effectiveness of
our techniques in stringent listening tests.
\\ ( https://arxiv.org/abs/2401.12068 ,  476kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12079 (*cross-listing*)
Date: Mon, 22 Jan 2024 16:21:19 GMT   (6484kb,D)

Title: Collaborative Reinforcement Learning Based Unmanned Aerial Vehicle (UAV)
  Trajectory Design for 3D UAV Tracking
Authors: Yujiao Zhu, Mingzhe Chen, Sihua Wang, Ye Hu, Yuchen Liu, and
  Changchuan Yin
Categories: cs.MA cs.LG
\\
  In this paper, the problem of using one active unmanned aerial vehicle (UAV)
and four passive UAVs to localize a 3D target UAV in real time is investigated.
In the considered model, each passive UAV receives reflection signals from the
target UAV, which are initially transmitted by the active UAV. The received
reflection signals allow each passive UAV to estimate the signal transmission
distance which will be transmitted to a base station (BS) for the estimation of
the position of the target UAV. Due to the movement of the target UAV, each
active/passive UAV must optimize its trajectory to continuously localize the
target UAV. Meanwhile, since the accuracy of the distance estimation depends on
the signal-to-noise ratio of the transmission signals, the active UAV must
optimize its transmit power. This problem is formulated as an optimization
problem whose goal is to jointly optimize the transmit power of the active UAV
and trajectories of both active and passive UAVs so as to maximize the target
UAV positioning accuracy. To solve this problem, a Z function decomposition
based reinforcement learning (ZD-RL) method is proposed. Compared to value
function decomposition based RL (VD-RL), the proposed method can find the
probability distribution of the sum of future rewards to accurately estimate
the expected value of the sum of future rewards thus finding better transmit
power of the active UAV and trajectories for both active and passive UAVs and
improving target UAV positioning accuracy. Simulation results show that the
proposed ZD-RL method can reduce the positioning errors by up to 39.4% and
64.6%, compared to VD-RL and independent deep RL methods, respectively.
\\ ( https://arxiv.org/abs/2401.12079 ,  6484kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12103 (*cross-listing*)
Date: Mon, 22 Jan 2024 16:38:33 GMT   (3159kb,D)

Title: LearnedWMP: Workload Memory Prediction Using Distribution of Query
  Templates
Authors: Shaikh Quader, Andres Jaramillo, Sumona Mukhopadhyay, Ghadeer Abuoda,
  Calisto Zuzarte, David Kalmuk, Marin Litoiu, Manos Papagelis
Categories: cs.DB cs.LG
\\
  In a modern DBMS, working memory is frequently the limiting factor when
processing in-memory analytic query operations such as joins, sorting, and
aggregation. Existing resource estimation approaches for a DBMS estimate the
resource consumption of a query by computing an estimate of each individual
database operator in the query execution plan. Such an approach is slow and
error-prone as it relies upon simplifying assumptions, such as uniformity and
independence of the underlying data. Additionally, the existing approach
focuses on individual queries separately and does not factor in other queries
in the workload that may be executed concurrently. In this research, we are
interested in query performance optimization under concurrent execution of a
batch of queries (a workload). Specifically, we focus on predicting the memory
demand for a workload rather than providing separate estimates for each query
within it. We introduce the problem of workload memory prediction and formalize
it as a distribution regression problem. We propose Learned Workload Memory
Prediction (LearnedWMP) to improve and simplify estimating the working memory
demands of workloads. Through a comprehensive experimental evaluation, we show
that LearnedWMP reduces the memory estimation error of the
state-of-the-practice method by up to 47.6%. Compared to an alternative
single-query model, during training and inferencing, the LearnedWMP model and
its variants were 3x to 10x faster. Moreover, LearnedWMP-based models were at
least 50% smaller in most cases. Overall, the results demonstrate the
advantages of the LearnedWMP approach and its potential for a broader impact on
query performance optimization.
\\ ( https://arxiv.org/abs/2401.12103 ,  3159kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12129 (*cross-listing*)
Date: Mon, 22 Jan 2024 17:11:01 GMT   (4719kb,D)

Title: Out-of-Distribution Detection & Applications With Ablated Learned
  Temperature Energy
Authors: Will LeVine, Benjamin Pikus, Jacob Phillips, Berk Norman, Fernando
  Amat Gil, Sean Hendryx
Categories: cs.CV cs.LG
\\
  As deep neural networks become adopted in high-stakes domains, it is crucial
to be able to identify when inference inputs are Out-of-Distribution (OOD) so
that users can be alerted of likely drops in performance and calibration
despite high confidence. Among many others, existing methods use the following
two scores to do so without training on any apriori OOD examples: a learned
temperature and an energy score. In this paper we introduce Ablated Learned
Temperature Energy (or "AbeT" for short), a method which combines these prior
methods in novel ways with effective modifications. Due to these contributions,
AbeT lowers the False Positive Rate at $95\%$ True Positive Rate (FPR@95) by
$35.39\%$ in classification (averaged across all ID and OOD datasets measured)
compared to state of the art without training networks in multiple stages or
requiring hyperparameters or test-time backward passes. We additionally provide
empirical insights as to how our model learns to distinguish between
In-Distribution (ID) and OOD samples while only being explicitly trained on ID
samples via exposure to misclassified ID examples at training time. Lastly, we
show the efficacy of our method in identifying predicted bounding boxes and
pixels corresponding to OOD objects in object detection and semantic
segmentation, respectively - with an AUROC increase of $5.15\%$ in object
detection and both a decrease in FPR@95 of $41.48\%$ and an increase in AUPRC
of $34.20\%$ on average in semantic segmentation compared to previous state of
the art.
\\ ( https://arxiv.org/abs/2401.12129 ,  4719kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12131 (*cross-listing*)
Date: Mon, 22 Jan 2024 17:13:50 GMT   (1731kb,D)

Title: NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis
Authors: Matthias Cosler, Christopher Hahn, Ayham Omar, Frederik Schmitt
Categories: cs.LO cs.LG
\\
  We introduce NeuroSynt, a neuro-symbolic portfolio solver framework for
reactive synthesis. At the core of the solver lies a seamless integration of
neural and symbolic approaches to solving the reactive synthesis problem. To
ensure soundness, the neural engine is coupled with model checkers verifying
the predictions of the underlying neural models. The open-source implementation
of NeuroSynt provides an integration framework for reactive synthesis in which
new neural and state-of-the-art symbolic approaches can be seamlessly
integrated. Extensive experiments demonstrate its efficacy in handling
challenging specifications, enhancing the state-of-the-art reactive synthesis
solvers, with NeuroSynt contributing novel solves in the current SYNTCOMP
benchmarks.
\\ ( https://arxiv.org/abs/2401.12131 ,  1731kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12133 (*cross-listing*)
Date: Mon, 22 Jan 2024 17:15:02 GMT   (5312kb,D)

Title: VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear
  Responses in VR Stand-up Interactive Games
Authors: He Zhang, Xinyang Li, Yuanxi Sun, Xinyi Fu, Christine Qiu, John M.
  Carroll
Categories: cs.HC cs.CV cs.LG
Comments: Accepted to IEEE VR 2024
\\
  Understanding and recognizing emotions are important and challenging issues
in the metaverse era. Understanding, identifying, and predicting fear, which is
one of the fundamental human emotions, in virtual reality (VR) environments
plays an essential role in immersive game development, scene development, and
next-generation virtual human-computer interaction applications. In this
article, we used VR horror games as a medium to analyze fear emotions by
collecting multi-modal data (posture, audio, and physiological signals) from 23
players. We used an LSTM-based model to predict fear with accuracies of 65.31%
and 90.47% under 6-level classification (no fear and five different levels of
fear) and 2-level classification (no fear and fear), respectively. We
constructed a multi-modal natural behavior dataset of immersive human fear
responses (VRMN-bD) and compared it with existing relevant advanced datasets.
The results show that our dataset has fewer limitations in terms of collection
method, data scale and audience scope. We are unique and advanced in targeting
multi-modal datasets of fear and behavior in VR stand-up interactive
environments. Moreover, we discussed the implications of this work for
communities and applications. The dataset and pre-trained model are available
at https://github.com/KindOPSTAR/VRMN-bD.
\\ ( https://arxiv.org/abs/2401.12133 ,  5312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12149 (*cross-listing*)
Date: Mon, 22 Jan 2024 17:36:23 GMT   (319kb,D)

Title: Personalized Over-the-Air Federated Learning with Personalized
  Reconfigurable Intelligent Surfaces
Authors: Jiayu Mao and Aylin Yener
Categories: cs.IT cs.LG math.IT
Comments: Copyright 2024 IEEE. Published in ICASSP 2024, 14-19 April, Seoul,
  Korea. Personal use of this material is permitted. However, permission to
  reprint/republish this material for advertising or promotional purposes or
  for creating new collective works for resale or redistribution to servers or
  lists, or to reuse any copyrighted component of this work in other works,
  must be obtained from the IEEE
\\
  Over-the-air federated learning (OTA-FL) provides bandwidth-efficient
learning by leveraging the inherent superposition property of wireless
channels. Personalized federated learning balances performance for users with
diverse datasets, addressing real-life data heterogeneity. We propose the first
personalized OTA-FL scheme through multi-task learning, assisted by personal
reconfigurable intelligent surfaces (RIS) for each user. We take a cross-layer
approach that optimizes communication and computation resources for global and
personalized tasks in time-varying channels with imperfect channel state
information, using multi-task learning for non-i.i.d data. Our PROAR-PFed
algorithm adaptively designs power, local iterations, and RIS configurations.
We present convergence analysis for non-convex objectives and demonstrate that
PROAR-PFed outperforms state-of-the-art on the Fashion-MNIST dataset.
\\ ( https://arxiv.org/abs/2401.12149 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12207 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:49:56 GMT   (154kb,D)

Title: Rate-Distortion-Perception Tradeoff Based on the
  Conditional-Distribution Perception Measure
Authors: Sadaf Salehkalaibar, Jun Chen, Ashish Khisti and Wei Yu
Categories: cs.IT cs.LG math.IT
\\
  We study the rate-distortion-perception (RDP) tradeoff for a memoryless
source model in the asymptotic limit of large block-lengths. Our perception
measure is based on a divergence between the distributions of the source and
reconstruction sequences conditioned on the encoder output, which was first
proposed in [1], [2]. We consider the case when there is no shared randomness
between the encoder and the decoder. For the case of discrete memoryless
sources we derive a single-letter characterization of the RDP function, thus
settling a problem that remains open for the marginal metric introduced in Blau
and Michaeli [3] (with no shared randomness). Our achievability scheme is based
on lossy source coding with a posterior reference map proposed in [4]. For the
case of continuous valued sources under squared error distortion measure and
squared quadratic Wasserstein perception measure we also derive a single-letter
characterization and show that a noise-adding mechanism at the decoder suffices
to achieve the optimal representation. For the case of zero perception loss, we
show that our characterization interestingly coincides with the results for the
marginal metric derived in [5], [6] and again demonstrate that zero perception
loss can be achieved with a $3$-dB penalty in the minimum distortion. Finally
we specialize our results to the case of Gaussian sources. We derive the RDP
function for vector Gaussian sources and propose a waterfilling type solution.
We also partially characterize the RDP function for a mixture of vector
Gaussians.
\\ ( https://arxiv.org/abs/2401.12207 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12216 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:59:12 GMT   (38kb,D)

Title: Mitigating Covariate Shift in Misspecified Regression with Applications
  to Reinforcement Learning
Authors: Philip Amortila, Tongyi Cao, Akshay Krishnamurthy
Categories: stat.ML cs.LG math.OC
\\
  A pervasive phenomenon in machine learning applications is distribution
shift, where training and deployment conditions for a machine learning model
differ. As distribution shift typically results in a degradation in
performance, much attention has been devoted to algorithmic interventions that
mitigate these detrimental effects. In this paper, we study the effect of
distribution shift in the presence of model misspecification, specifically
focusing on $L_{\infty}$-misspecified regression and adversarial covariate
shift, where the regression target remains fixed while the covariate
distribution changes arbitrarily. We show that empirical risk minimization, or
standard least squares regression, can result in undesirable misspecification
amplification where the error due to misspecification is amplified by the
density ratio between the training and testing distributions. As our main
result, we develop a new algorithm -- inspired by robust optimization
techniques -- that avoids this undesirable behavior, resulting in no
misspecification amplification while still obtaining optimal statistical rates.
As applications, we use this regression procedure to obtain new guarantees in
offline and online reinforcement learning with misspecification and establish
new separations between previously studied structural conditions and notions of
coverage.
\\ ( https://arxiv.org/abs/2401.12216 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12217 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:59:29 GMT   (5187kb,D)

Title: Exploring Simple Open-Vocabulary Semantic Segmentation
Authors: Zihang Lai
Categories: cs.CV cs.LG
Comments: Code is available at: https://github.com/zlai0/S-Seg
\\
  Open-vocabulary semantic segmentation models aim to accurately assign a
semantic label to each pixel in an image from a set of arbitrary
open-vocabulary texts. In order to learn such pixel-level alignment, current
approaches typically rely on a combination of (i) image-level VL model (e.g.
CLIP), (ii) ground truth masks, and (iii) custom grouping encoders. In this
paper, we introduce S-Seg, a novel model that can achieve surprisingly strong
performance without depending on any of the above elements. S-Seg leverages
pseudo-mask and language to train a MaskFormer, and can be easily trained from
publicly available image-text datasets. Contrary to prior works, our model
directly trains for pixel-level features and language alignment. Once trained,
S-Seg generalizes well to multiple testing datasets without requiring
fine-tuning. In addition, S-Seg has the extra benefits of scalability with data
and consistently improvement when augmented with self-training. We believe that
our simple yet effective approach will serve as a solid baseline for future
research.
\\ ( https://arxiv.org/abs/2401.12217 ,  5187kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2002.11503
replaced with revised version Mon, 22 Jan 2024 15:09:42 GMT   (1471kb)

Title: Wavelet-based temporal models of human activity for anomaly detection in
  smart robot-assisted environments
Authors: Manuel Fernandez-Carmona, Sariah Mghames and Nicola Bellotto
Categories: cs.AI eess.SP
Comments: 20 pages, 6 figures
MSC-class: 68T10
Journal-ref: Journal of Ambient Intelligence and Smart Environments, pp. 1-20,
  2023
DOI: 10.3233/AIS-230144
\\ ( https://arxiv.org/abs/2002.11503 ,  1471kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01751
replaced with revised version Sat, 20 Jan 2024 16:10:35 GMT   (28kb)

Title: Proportional structures
Authors: Christian Anti\'c
Categories: cs.AI cs.LO
\\ ( https://arxiv.org/abs/2210.01751 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14317
replaced with revised version Mon, 22 Jan 2024 17:06:50 GMT   (213kb,D)

Title: ICE-Score: Instructing Large Language Models to Evaluate Code
Authors: Terry Yue Zhuo
Categories: cs.AI cs.CL cs.SE
Comments: Accepted to Findings of EACL 2024
\\ ( https://arxiv.org/abs/2304.14317 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04073
replaced with revised version Mon, 22 Jan 2024 12:00:58 GMT   (8592kb,D)

Title: Explaining RL Decisions with Trajectories
Authors: Shripad Vilasrao Deshmukh, Arpan Dasgupta, Balaji Krishnamurthy, Nan
  Jiang, Chirag Agarwal, Georgios Theocharous, Jayakumar Subramanian
Categories: cs.AI cs.LG
Comments: Published at International Conference on Learning Representations
  (ICLR), 2023
\\ ( https://arxiv.org/abs/2305.04073 ,  8592kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19604
replaced with revised version Mon, 22 Jan 2024 08:13:50 GMT   (4644kb,D)

Title: Medication Recommendation via Domain Knowledge Informed Deep Learning
Authors: Sicen Liu, Xiaolong Wang, Xianbing Zhao, Hao Chen
Categories: cs.AI cs.IR cs.LG
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2305.19604 ,  4644kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06564
replaced with revised version Sat, 20 Jan 2024 12:49:02 GMT   (1068kb,D)

Title: Prescriptive Process Monitoring Under Resource Constraints: A
  Reinforcement Learning Approach
Authors: Mahmoud Shoush and Marlon Dumas
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2307.06564 ,  1068kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12620
replaced with revised version Sat, 20 Jan 2024 14:14:12 GMT   (56kb)

Title: Past-present temporal programs over finite traces
Authors: Pedro Cabalar, Mart\'in Di\'eguez, Fran\c{c}ois Laferri\`ere, Torsten
  Schaub
Categories: cs.AI
\\ ( https://arxiv.org/abs/2307.12620 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04292
replaced with revised version Sun, 21 Jan 2024 22:40:07 GMT   (2225kb,D)

Title: Engineering LaCAM$^\ast$: Towards Real-Time, Large-Scale, and
  Near-Optimal Multi-Agent Pathfinding
Authors: Keisuke Okumura
Categories: cs.AI cs.MA cs.RO
Comments: to be presented at AAMAS-24
\\ ( https://arxiv.org/abs/2308.04292 ,  2225kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13894
replaced with revised version Sat, 20 Jan 2024 09:24:33 GMT   (19605kb,D)

Title: FwdLLM: Efficient FedLLM using Forward Gradient
Authors: Mengwei Xu, Dongqi Cai, Yaozong Wu, Xiang Li, Shangguang Wang
Categories: cs.AI cs.LG
Comments: under review
\\ ( https://arxiv.org/abs/2308.13894 ,  19605kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01538
replaced with revised version Mon, 22 Jan 2024 02:39:17 GMT   (600kb,D)

Title: ChatRule: Mining Logical Rules with Large Language Models for Knowledge
  Graph Reasoning
Authors: Linhao Luo, Jiaxin Ju, Bo Xiong, Yuan-Fang Li, Gholamreza Haffari,
  Shirui Pan
Categories: cs.AI cs.CL
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2309.01538 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06629
replaced with revised version Sat, 20 Jan 2024 04:56:17 GMT   (2096kb,D)

Title: The Relational Bottleneck as an Inductive Bias for Efficient Abstraction
Authors: Taylor W. Webb, Steven M. Frankland, Awni Altabaa, Kamesh
  Krishnamurthy, Declan Campbell, Jacob Russin, Randall O'Reilly, John
  Lafferty, Jonathan D. Cohen
Categories: cs.AI cs.NE
\\ ( https://arxiv.org/abs/2309.06629 ,  2096kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06979
replaced with revised version Sat, 20 Jan 2024 09:13:40 GMT   (148kb)

Title: Assessing the Interpretability of Programmatic Policies with Large
  Language Models
Authors: Zahra Bashir, Michael Bowling, Levi H. S. Lelis
Categories: cs.AI cs.PL cs.SE
Comments: This paper is under-review for IJCAI. The main file is arxiv.tex and
  I have a supplementary_materials.tex file as well
\\ ( https://arxiv.org/abs/2311.06979 ,  148kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13373
replaced with revised version Mon, 22 Jan 2024 11:08:04 GMT   (1596kb,D)

Title: Large Language Model as a Policy Teacher for Training Reinforcement
  Learning Agents
Authors: Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, Bin Liu
Categories: cs.AI
Comments: 11 pages
\\ ( https://arxiv.org/abs/2311.13373 ,  1596kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06261
replaced with revised version Sat, 20 Jan 2024 12:53:12 GMT   (0kb,I)

Title: Survey on Foundation Models for Prognostics and Health Management in
  Industrial Cyber-Physical Systems
Authors: Ruonan Liu, Quanhu Zhang, Te Han
Categories: cs.AI
Comments: Authors of the paper to be re-established
\\ ( https://arxiv.org/abs/2312.06261 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01841
replaced with revised version Mon, 22 Jan 2024 03:43:34 GMT   (3359kb,D)

Title: Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov
  Decision Processes
Authors: Baiting Luo, Yunuo Zhang, Abhishek Dubey, Ayan Mukhopadhyay
Categories: cs.AI cs.LG
Comments: Accepted for publication at the International Conference on
  Autonomous Agents and MultiAgent Systems (AAMAS), 2024
\\ ( https://arxiv.org/abs/2401.01841 ,  3359kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02500
replaced with revised version Sat, 20 Jan 2024 12:10:26 GMT   (448kb,D)

Title: On the Prospects of Incorporating Large Language Models (LLMs) in
  Automated Planning and Scheduling (APS)
Authors: Vishal Pallagani, Kaushik Roy, Bharath Muppasani, Francesco Fabiano,
  Andrea Loreggia, Keerthiram Murugesan, Biplav Srivastava, Francesca Rossi,
  Lior Horesh, Amit Sheth
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.02500 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03197
replaced with revised version Sat, 20 Jan 2024 18:34:03 GMT   (7588kb,D)

Title: Decision Making in Non-Stationary Environments with Policy-Augmented
  Search
Authors: Ava Pettet, Yunuo Zhang, Baiting Luo, Kyle Wray, Hendrik Baier, Aron
  Laszka, Abhishek Dubey, Ayan Mukhopadhyay
Categories: cs.AI cs.LG
Comments: Extended Abstract accepted for presentation at AAMAS 2024
\\ ( https://arxiv.org/abs/2401.03197 ,  7588kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06256
replaced with revised version Sat, 20 Jan 2024 15:37:28 GMT   (998kb)

Title: A Universal Knowledge Model and Cognitive Architecture for Prototyping
  AGI
Authors: Artem Sukhobokov, Evgeny Belousov, Danila Gromozdov, Anna Zenger and
  Ilya Popov
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.06256 ,  998kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07656
replaced with revised version Sat, 20 Jan 2024 08:18:57 GMT   (184kb)

Title: Learning Explainable and Better Performing Representations of POMDP
  Strategies
Authors: Alexander Bork, Debraj Chakraborty, Kush Grover, Jan Kretinsky,
  Stefanie Mohr
Categories: cs.AI cs.LG cs.LO
Comments: Technical report for the submission to TACAS 24
\\ ( https://arxiv.org/abs/2401.07656 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07890
replaced with revised version Sat, 20 Jan 2024 14:18:49 GMT   (1006kb,D)

Title: A Strategy for Implementing description Temporal Dynamic Algorithms in
  Dynamic Knowledge Graphs by SPIN
Authors: Alireza Shahbazi, Seyyed Ahmad Mirsanei, Malikeh Haj Khan Mirzaye
  Sarraf and Behrouz Minaei Bidgoli
Categories: cs.AI cs.LO
\\ ( https://arxiv.org/abs/2401.07890 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2109.01636
replaced with revised version Mon, 22 Jan 2024 01:23:23 GMT   (1462kb)

Title: Empirical Study of Named Entity Recognition Performance Using
  Distribution-aware Word Embedding
Authors: Xin Chen, Qi Zhao, Xinyang Liu
Categories: cs.CL cs.LG
Comments: Want to correct
\\ ( https://arxiv.org/abs/2109.01636 ,  1462kb)
------------------------------------------------------------------------------
\\
arXiv:2203.14647
replaced with revised version Sun, 21 Jan 2024 14:39:30 GMT   (7051kb,D)

Title: Automatic Debate Evaluation with Argumentation Semantics and Natural
  Language Argument Graph Networks
Authors: Ramon Ruiz-Dolz, Stella Heras, Ana Garc\'ia-Fornes
Categories: cs.CL
Comments: EMNLP 2023 Accepted Version
\\ ( https://arxiv.org/abs/2203.14647 ,  7051kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08577
replaced with revised version Sat, 20 Jan 2024 02:36:12 GMT   (296kb,D)

Title: For Generated Text, Is NLI-Neutral Text the Best Text?
Authors: Michail Mersinias, Kyle Mahowald
Categories: cs.CL
\\ ( https://arxiv.org/abs/2302.08577 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12584
replaced with revised version Sun, 21 Jan 2024 14:51:26 GMT   (69kb,D)

Title: VivesDebate-Speech: A Corpus of Spoken Argumentation to Leverage Audio
  Features for Argument Mining
Authors: Ramon Ruiz-Dolz and Javier Iranzo-S\'anchez
Categories: cs.CL
Comments: 5 pages; EMNLP 2023 Accepted Version
\\ ( https://arxiv.org/abs/2302.12584 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14189
replaced with revised version Sat, 20 Jan 2024 22:29:15 GMT   (8706kb,D)

Title: Beyond Shared Vocabulary: Increasing Representational Word Similarities
  across Languages for Multilingual Machine Translation
Authors: Di Wu and Christof Monz
Categories: cs.CL cs.LG
Comments: 15 pages, 3 figures
\\ ( https://arxiv.org/abs/2305.14189 ,  8706kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14578
replaced with revised version Mon, 22 Jan 2024 14:13:51 GMT   (7314kb,D)

Title: Connecting the Dots: What Graph-Based Text Representations Work Best for
  Text Classification Using Graph Neural Networks?
Authors: Margarita Bugue\~no, Gerard de Melo
Categories: cs.CL
Comments: Accepted to Findings of the Association for Computational
  Linguistics: EMNLP 2023 (Long Paper). 17 pages, 2 figures, 15 tables. The
  Appendix starts on page 12
Journal-ref: Findings of the Association for Computational Linguistics: EMNLP
  2023, pages 8943-8960
DOI: 10.18653/v1/2023.findings-emnlp.600
\\ ( https://arxiv.org/abs/2305.14578 ,  7314kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16326
replaced with revised version Sat, 20 Jan 2024 14:33:54 GMT   (672kb)

Title: Large language models in biomedical natural language processing:
  benchmarks, baselines, and recommendations
Authors: Qingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi Keloth, Xueqing
  Peng, Kalpana Raja, Rui Zhang, Zhiyong Lu, Hua Xu
Categories: cs.CL cs.AI cs.IR cs.LG
\\ ( https://arxiv.org/abs/2305.16326 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00824
replaced with revised version Mon, 22 Jan 2024 14:57:47 GMT   (540kb,D)

Title: Zero and Few-shot Semantic Parsing with Ambiguous Inputs
Authors: Elias Stengel-Eskin and Kyle Rawlins and Benjamin Van Durme
Categories: cs.CL
Comments: ICLR 2024 Camera Ready
\\ ( https://arxiv.org/abs/2306.00824 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16001
replaced with revised version Mon, 22 Jan 2024 00:27:45 GMT   (1080kb)

Title: Streamlining Social Media Information Extraction for Public Health
  Research with Deep Learning
Authors: Yining Hua, Shixu Lin, Minghui Li, Yujie Zhang, Dinah Foer, Siwen
  Wang, Peilin Zhou, Li Zhou, Jie Yang
Categories: cs.CL cs.AI cs.IR
Comments: Updated full paper. Abstract presented at IEEE ICHI 2023 and AMIA
  Annual Symposium 2023
\\ ( https://arxiv.org/abs/2306.16001 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04408
replaced with revised version Mon, 22 Jan 2024 07:40:02 GMT   (8957kb,D)

Title: TIM: Teaching Large Language Models to Translate with Comparison
Authors: Jiali Zeng and Fandong Meng and Yongjing Yin and Jie Zhou
Categories: cs.CL
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2307.04408 ,  8957kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05173
replaced with revised version Sun, 21 Jan 2024 13:38:20 GMT   (691kb,D)

Title: DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning
Authors: Zhengxiang Shi, Aldo Lipani
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: ICLR 2024. Code is available at https://github.com/ZhengxiangShi/DePT
\\ ( https://arxiv.org/abs/2309.05173 ,  691kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12247
replaced with revised version Mon, 22 Jan 2024 07:24:30 GMT   (946kb,D)

Title: Bad Actor, Good Advisor: Exploring the Role of Large Language Models in
  Fake News Detection
Authors: Beizhe Hu, Qiang Sheng, Juan Cao, Yuhui Shi, Yang Li, Danding Wang,
  Peng Qi
Categories: cs.CL cs.AI cs.CY
Comments: 16 pages, 5 figures, and 9 tables. To appear at AAAI 2024
\\ ( https://arxiv.org/abs/2309.12247 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01386
replaced with revised version Mon, 22 Jan 2024 13:58:50 GMT   (7704kb,D)

Title: Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using
  PsychoBench
Authors: Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren,
  Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu
Categories: cs.CL
Comments: Accepted for ICLR 2024 Oral Presentation. 15 pages (main text) and 5
  pages (appendix)
\\ ( https://arxiv.org/abs/2310.01386 ,  7704kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02118
replaced with revised version Mon, 22 Jan 2024 14:41:43 GMT   (14123kb,D)

Title: TWIZ-v2: The Wizard of Multimodal Conversational-Stimulus
Authors: Rafael Ferreira, Diogo Tavares, Diogo Silva, Rodrigo Val\'erio, Jo\~ao
  Bordalo, In\^es Sim\~oes, Vasco Ramos, David Semedo, Jo\~ao Magalh\~aes
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2310.02118 ,  14123kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04691
replaced with revised version Sat, 20 Jan 2024 09:36:41 GMT   (873kb,D)

Title: EMO: Earth Mover Distance Optimization for Auto-Regressive Language
  Modeling
Authors: Siyu Ren, Zhiyong Wu, Kenny Q. Zhu
Categories: cs.CL
Comments: To appear at ICLR 2024
\\ ( https://arxiv.org/abs/2310.04691 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10873
replaced with revised version Sat, 20 Jan 2024 03:58:10 GMT   (443kb,D)

Title: IDEAL: Influence-Driven Selective Annotations Empower In-Context
  Learners in Large Language Models
Authors: Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-Hao Chen, Jiale Liu,
  Qingyun Wu, Tongliang Liu
Categories: cs.CL
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.10873 ,  443kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15823
replaced with revised version Sun, 21 Jan 2024 12:40:48 GMT   (124kb,D)

Title: Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To
  Word--Definition Alignment
Authors: Ahmed ElBakry, Mohamed Gabr, Muhammad ElNokrashy, Badr AlKhamissi
Categories: cs.CL cs.AI
Comments: Proceedings of ArabicNLP 2023
DOI: 10.18653/v1/2023.arabicnlp-1.43
\\ ( https://arxiv.org/abs/2310.15823 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07989
replaced with revised version Mon, 22 Jan 2024 12:27:47 GMT   (574kb,D)

Title: Unifying the Perspectives of NLP and Software Engineering: A Survey on
  Language Models for Code
Authors: Ziyin Zhang and Chaoyu Chen and Bingchang Liu and Cong Liao and Zi
  Gong and Hang Yu and Jianguo Li and Rui Wang
Categories: cs.CL cs.AI cs.SE
Comments: Repo is available at https://github.com/codefuse-ai/Awesome-Code-LLM.
  8 figures, 10 tables, and 713 references
\\ ( https://arxiv.org/abs/2311.07989 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02317
replaced with revised version Sat, 20 Jan 2024 21:16:09 GMT   (220kb,D)

Title: GNN2R: Weakly-Supervised Rationale-Providing Question Answering over
  Knowledge Graphs
Authors: Ruijie Wang, Luca Rossetto, Michael Cochez, Abraham Bernstein
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.02317 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03122
replaced with revised version Sat, 20 Jan 2024 15:02:20 GMT   (1674kb)

Title: Assertion Enhanced Few-Shot Learning: Instructive Technique for Large
  Language Models to Generate Educational Explanations
Authors: Tasmia Shahriar, Kelly Ramos and Noboru Matsuda
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.03122 ,  1674kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11532
replaced with revised version Sun, 21 Jan 2024 09:30:36 GMT   (20920kb,D)

Title: Topic-VQ-VAE: Leveraging Latent Codebooks for Flexible Topic-Guided
  Document Generation
Authors: YoungJoon Yoo, Jongwon Choi
Categories: cs.CL cs.AI cs.LG
Comments: Published in the 38th annual AAAI conference on Artificial
  Intelligence
\\ ( https://arxiv.org/abs/2312.11532 ,  20920kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15407
replaced with revised version Sat, 20 Jan 2024 06:26:33 GMT   (1540kb,D)

Title: A Comprehensive Analysis of the Effectiveness of Large Language Models
  as Automatic Dialogue Evaluators
Authors: Chen Zhang, Luis Fernando D'Haro, Yiming Chen, Malu Zhang, Haizhou Li
Categories: cs.CL
Comments: An extended version of AAAI-2024 camera-ready paper (appendix
  included, 16 pages)
\\ ( https://arxiv.org/abs/2312.15407 ,  1540kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17080
replaced with revised version Sat, 20 Jan 2024 14:08:16 GMT   (1364kb,D)

Title: MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation
Authors: Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, Jiaya Jia
Categories: cs.CL
Comments: Code: https://github.com/dvlab-research/MR-GSM8K
\\ ( https://arxiv.org/abs/2312.17080 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04620
replaced with revised version Sat, 20 Jan 2024 13:04:29 GMT   (2617kb,D)

Title: Agent Alignment in Evolving Social Norms
Authors: Shimin Li, Tianxiang Sun, Xipeng Qiu
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.04620 ,  2617kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04925
replaced with revised version Sat, 20 Jan 2024 17:23:31 GMT   (411kb,D)

Title: The Impact of Reasoning Step Length on Large Language Models
Authors: Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng,
  Yongfeng Zhang, Mengnan Du
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.04925 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05949
replaced with revised version Sat, 20 Jan 2024 13:46:33 GMT   (8734kb,D)

Title: Universal Vulnerabilities in Large Language Models: In-context Learning
  Backdoor Attacks
Authors: Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Jinming Wen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.05949 ,  8734kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06766
replaced with revised version Mon, 22 Jan 2024 18:55:35 GMT   (1283kb,D)

Title: Mind Your Format: Towards Consistent Evaluation of In-Context Learning
  Improvements
Authors: Anton Voronov, Lena Wolf, Max Ryabinin
Categories: cs.CL
Comments: 21 pages, 10 figures. Code:
  https://github.com/yandex-research/mind-your-format
\\ ( https://arxiv.org/abs/2401.06766 ,  1283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07510
replaced with revised version Sat, 20 Jan 2024 22:08:18 GMT   (1123kb)

Title: Developing ChatGPT for Biology and Medicine: A Complete Review of
  Biomedical Question Answering
Authors: Qing Li, Lei Li, Yu Li
Categories: cs.CL cs.AI
Comments: 50 pages, 3 figures, 3 tables
MSC-class: cs.CL, 92-02
ACM-class: I.2.1
\\ ( https://arxiv.org/abs/2401.07510 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09003
replaced with revised version Sat, 20 Jan 2024 12:43:37 GMT   (85kb,D)

Title: Augmenting Math Word Problems via Iterative Question Composing
Authors: Haoxiong Liu, Andrew Chi-Chih Yao
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.09003 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09333
replaced with revised version Sat, 20 Jan 2024 15:01:01 GMT   (11672kb,D)

Title: Machines Do See Color: A Guideline to Classify Different Forms of Racist
  Discourse in Large Corpora
Authors: Diana Davila Gordillo, Joan Timoneda, Sebastian Vallejo Vera
Categories: cs.CL cs.LG
Comments: 37 pages, 5 figures, 4 tables
\\ ( https://arxiv.org/abs/2401.09333 ,  11672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09798
replaced with revised version Mon, 22 Jan 2024 06:22:55 GMT   (51kb)

Title: All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks
Authors: Kazuhiro Takemoto
Categories: cs.CL cs.AI cs.CY
Comments: 12 pages, 4 figures, 2 tables
\\ ( https://arxiv.org/abs/2401.09798 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10015
replaced with revised version Sun, 21 Jan 2024 06:51:25 GMT   (1096kb,D)

Title: Towards Hierarchical Spoken Language Dysfluency Modeling
Authors: Jiachen Lian and Gopala Anumanchipalli
Categories: cs.CL eess.AS
Comments: 2024 EACL. Hierarchical extension of our previous workshop paper
  arXiv:2312.12810
\\ ( https://arxiv.org/abs/2401.10015 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10189
replaced with revised version Sun, 21 Jan 2024 03:37:41 GMT   (150kb,D)

Title: Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through
  Text Reconstruction
Authors: Qingyun Wang, Zixuan Zhang, Hongxiang Li, Xuan Liu, Jiawei Han, Heng
  Ji, Huimin Zhao
Categories: cs.CL cs.AI cs.LG
Comments: 16 pages. Accepted by Findings of the Association for Computational
  Linguistics: EACL 2024. Code and resources are available at
  https://github.com/EagleW/Chem-FINESE
\\ ( https://arxiv.org/abs/2401.10189 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10491
replaced with revised version Mon, 22 Jan 2024 17:16:37 GMT   (306kb,D)

Title: Knowledge Fusion of Large Language Models
Authors: Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi
Categories: cs.CL
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2401.10491 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05259
replaced with revised version Sun, 21 Jan 2024 11:58:49 GMT   (15358kb,D)

Title: Wavelet Networks: Scale-Translation Equivariant Learning From Raw
  Time-Series
Authors: David W. Romero, Erik J. Bekkers, Jakub M. Tomczak, Mark Hoogendoorn
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2006.05259 ,  15358kb)
------------------------------------------------------------------------------
\\
arXiv:2106.01135
replaced with revised version Sat, 20 Jan 2024 17:23:05 GMT   (88kb)

Title: MNL-Bandit with Knapsacks: a near-optimal algorithm
Authors: Abdellah Aznag, Vineet Goyal and Noemie Perivier
Categories: cs.LG cs.DS
\\ ( https://arxiv.org/abs/2106.01135 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2106.05410
replaced with revised version Sat, 20 Jan 2024 20:37:22 GMT   (2914kb,D)

Title: DASVDD: Deep Autoencoding Support Vector Data Descriptor for Anomaly
  Detection
Authors: Hadi Hojjati, Narges Armanfard
Categories: cs.LG
Journal-ref: IEEE Transactions on Knowledge and Data Engineering (Early
  Access), 2023, 1-12
DOI: 10.1109/TKDE.2023.3328882
\\ ( https://arxiv.org/abs/2106.05410 ,  2914kb)
------------------------------------------------------------------------------
\\
arXiv:2109.04033
replaced with revised version Mon, 22 Jan 2024 13:53:09 GMT   (76kb)

Title: New Versions of Gradient Temporal Difference Learning
Authors: Donghwan Lee, Han-Dong Lim, Jihoon Park, and Okyong Choi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2109.04033 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2111.12305
replaced with revised version Sun, 21 Jan 2024 19:24:11 GMT   (4647kb,D)

Title: Thundernna: a white box adversarial attack
Authors: Linfeng Ye, Shayan Mohajer Hamidi
Categories: cs.LG
Comments: 10 pages, 5 figures
MSC-class: 92B20
ACM-class: I.2.m
\\ ( https://arxiv.org/abs/2111.12305 ,  4647kb)
------------------------------------------------------------------------------
\\
arXiv:2203.00144
replaced with revised version Sat, 20 Jan 2024 21:46:23 GMT   (130kb,D)

Title: The Concordance Index decomposition: A measure for a deeper
  understanding of survival prediction models
Authors: Abdallah Alabdallah, Mattias Ohlsson, Sepideh Pashami, Thorsteinn
  R\"ognvaldsson
Categories: cs.LG stat.ME stat.ML
Journal-ref: Artificial Intelligence in Medicine, Volume 148, February 2024,
  102781
DOI: 10.1016/j.artmed.2024.102781
\\ ( https://arxiv.org/abs/2203.00144 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2205.05173
replaced with revised version Sat, 20 Jan 2024 20:33:54 GMT   (9036kb,D)

Title: Self-Supervised Anomaly Detection: A Survey and Outlook
Authors: Hadi Hojjati, Thi Kieu Khanh Ho, Narges Armanfard
Categories: cs.LG
Comments: 18 pages, 4 figures, 5 tables
Journal-ref: Neural Networks, Volume 172, April 2024, 106106
DOI: 10.1016/j.neunet.2024.106106
\\ ( https://arxiv.org/abs/2205.05173 ,  9036kb)
------------------------------------------------------------------------------
\\
arXiv:2205.11359
replaced with revised version Mon, 22 Jan 2024 18:01:37 GMT   (933kb,D)

Title: Towards Size-Independent Generalization Bounds for Deep Operator Nets
Authors: Pulkit Gopalani, Sayar Karmakar, Dibyakanti Kumar and Anirbit
  Mukherjee
Categories: cs.LG cs.NA math.NA stat.ML
Comments: 27 pages, 5 figures; Added theorem on generalization error indicating
  benefits of training DeepONets on the Huber loss and corresponding
  experiments
\\ ( https://arxiv.org/abs/2205.11359 ,  933kb)
------------------------------------------------------------------------------
\\
arXiv:2206.10078
replaced with revised version Sun, 21 Jan 2024 20:03:15 GMT   (1624kb,D)

Title: The Manifold Scattering Transform for High-Dimensional Point Cloud Data
Authors: Joyce Chew, Holly R. Steach, Siddharth Viswanath, Hau-Tieng Wu,
  Matthew Hirn, Deanna Needell, Smita Krishnaswamy, Michael Perlmutter
Categories: cs.LG cs.NA eess.SP math.NA stat.ML
Comments: Accepted for publication in the TAG in DS Workshop at ICML. For
  subsequent theoretical guarantees, please see Section 6 of arXiv:2208.08561
MSC-class: 68T07
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2206.10078 ,  1624kb)
------------------------------------------------------------------------------
\\
arXiv:2206.15269
replaced with revised version Mon, 22 Jan 2024 12:26:44 GMT   (11165kb,D)

Title: Deep Reinforcement Learning with Swin Transformers
Authors: Li Meng, Morten Goodwin, Anis Yazidi, Paal Engelstad
Categories: cs.LG
\\ ( https://arxiv.org/abs/2206.15269 ,  11165kb)
------------------------------------------------------------------------------
\\
arXiv:2210.00044
replaced with revised version Sat, 20 Jan 2024 19:15:21 GMT   (11241kb,D)

Title: Task Formulation Matters When Learning Continually: A Case Study in
  Visual Question Answering
Authors: Mavina Nikandrou, Lu Yu, Alessandro Suglia, Ioannis Konstas, Verena
  Rieser
Categories: cs.LG
\\ ( https://arxiv.org/abs/2210.00044 ,  11241kb)
------------------------------------------------------------------------------
\\
arXiv:2210.00108
replaced with revised version Mon, 22 Jan 2024 11:51:29 GMT   (404kb,D)

Title: ImpNet: Imperceptible and blackbox-undetectable backdoors in compiled
  neural networks
Authors: Tim Clifford, Ilia Shumailov, Yiren Zhao, Ross Anderson, Robert
  Mullins
Categories: cs.LG cs.CR
Comments: 10 pages, 7 figures, to be published in IEEE Secure and Trustworthy
  Machine Learning 2024. For website see https://ml.backdoors.uk . For source
  code, see https://git.sr.ht/~tim-clifford/impnet_source
\\ ( https://arxiv.org/abs/2210.00108 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2211.08594
replaced with revised version Sat, 20 Jan 2024 21:56:55 GMT   (102kb,D)

Title: Orthogonal Polynomials Approximation Algorithm (OPAA):a functional
  analytic approach to estimating probability densities
Authors: Lilian W. Bialokozowicz
Categories: cs.LG math.FA stat.ML
Comments: Neurips 2023 Workshop "The Symbiosis of Deep Learning and
  Differential Equations (DLDE III)"
ACM-class: G.3
\\ ( https://arxiv.org/abs/2211.08594 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09634
replaced with revised version Sat, 20 Jan 2024 22:04:21 GMT   (37kb)

Title: On the Sample Complexity of Two-Layer Networks: Lipschitz vs.
  Element-Wise Lipschitz Activation
Authors: Amit Daniely and Elad Granot
Categories: cs.LG
Comments: 13 pages
Journal-ref: 35th International Conference on Algorithmic Learning Theory, 2024
\\ ( https://arxiv.org/abs/2211.09634 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2212.01168
replaced with revised version Mon, 22 Jan 2024 10:09:20 GMT   (6308kb,D)

Title: Towards Cross Domain Generalization of Hamiltonian Representation via
  Meta Learning
Authors: Yeongwoo Song, Hawoong Jeong
Categories: cs.LG cs.AI physics.comp-ph
Comments: Conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2212.01168 ,  6308kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06726
replaced with revised version Sun, 21 Jan 2024 13:53:33 GMT   (114kb,D)

Title: Swap Agnostic Learning, or Characterizing Omniprediction via
  Multicalibration
Authors: Parikshit Gopalan and Michael P. Kim and Omer Reingold
Categories: cs.LG
MSC-class: 68T05, 68Q32
\\ ( https://arxiv.org/abs/2302.06726 ,  114kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05479
replaced with revised version Sat, 20 Jan 2024 03:51:39 GMT   (6891kb,D)

Title: Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online
  Fine-Tuning
Authors: Mitsuhiko Nakamoto, Yuexiang Zhai, Anikait Singh, Max Sobol Mark, Yi
  Ma, Chelsea Finn, Aviral Kumar, Sergey Levine
Categories: cs.LG cs.AI
Comments: NeurIPS 2023. project page: https://nakamotoo.github.io/Cal-QL
\\ ( https://arxiv.org/abs/2303.05479 ,  6891kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11249
replaced with revised version Sun, 21 Jan 2024 06:00:24 GMT   (1289kb,D)

Title: What Makes Data Suitable for a Locally Connected Neural Network? A
  Necessary and Sufficient Condition Based on Quantum Entanglement
Authors: Yotam Alexander, Nimrod De La Vega, Noam Razin, Nadav Cohen
Categories: cs.LG cs.AI quant-ph
Comments: Accepted to NeurIPS 2023
\\ ( https://arxiv.org/abs/2303.11249 ,  1289kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08404
replaced with revised version Sat, 20 Jan 2024 15:50:57 GMT   (171kb,D)

Title: Theoretical Analysis of Inductive Biases in Deep Convolutional Networks
Authors: Zihao Wang, Lei Wu
Categories: cs.LG stat.ML
Comments: 57 pages
\\ ( https://arxiv.org/abs/2305.08404 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12997
replaced with revised version Fri, 19 Jan 2024 20:35:54 GMT   (4607kb,D)

Title: Evaluating Privacy Leakage in Split Learning
Authors: Xinchi Qiu, Ilias Leontiadis, Luca Melis, Alex Sablayrolles, Pierre
  Stock
Categories: cs.LG cs.AI cs.CR
Comments: 10 pages
\\ ( https://arxiv.org/abs/2305.12997 ,  4607kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15586
replaced with revised version Sat, 20 Jan 2024 01:14:06 GMT   (31661kb,D)

Title: Manifold Diffusion Fields
Authors: Ahmed A. Elhag, Yuyang Wang, Joshua M. Susskind, Miguel Angel Bautista
Categories: cs.LG
Comments: ICLR24 paper
\\ ( https://arxiv.org/abs/2305.15586 ,  31661kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16789
replaced with revised version Mon, 22 Jan 2024 02:47:50 GMT   (6570kb,D)

Title: Modulate Your Spectrum in Self-Supervised Learning
Authors: Xi Weng, Yunhao Ni, Tengwei Song, Jie Luo, Rao Muhammad Anwer, Salman
  Khan, Fahad Shahbaz Khan, Lei Huang
Categories: cs.LG cs.CV eess.SP
Comments: Accepted at ICLR 2024. The code is available at
  https://github.com/winci-ai/intl
\\ ( https://arxiv.org/abs/2305.16789 ,  6570kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16943
replaced with revised version Fri, 19 Jan 2024 21:38:42 GMT   (33902kb,D)

Title: DiffusionNAG: Predictor-guided Neural Architecture Generation with
  Diffusion Models
Authors: Sohyun An, Hayeon Lee, Jaehyeong Jo, Seanie Lee, Sung Ju Hwang
Categories: cs.LG
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2305.16943 ,  33902kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01589
replaced with revised version Sun, 21 Jan 2024 00:16:30 GMT   (3445kb,D)

Title: Transfer learning for atomistic simulations using GNNs and kernel mean
  embeddings
Authors: John Falk, Luigi Bonati, Pietro Novelli, Michele Parrinello,
  Massimiliano Pontil
Categories: cs.LG physics.chem-ph
Comments: 20 pages, 4 figures, 7 tables, published in NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.01589 ,  3445kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01631
replaced with revised version Sat, 20 Jan 2024 03:22:43 GMT   (6189kb,D)

Title: Bi-level Contrastive Learning for Knowledge-Enhanced Molecule
  Representations
Authors: Pengcheng Jiang, Cao Xiao, Tianfan Fu, Jimeng Sun
Categories: cs.LG cs.AI q-bio.QM
\\ ( https://arxiv.org/abs/2306.01631 ,  6189kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02869
replaced with revised version Sun, 21 Jan 2024 19:59:53 GMT   (5234kb,D)

Title: Data-Driven Regret Balancing for Online Model Selection in Bandits
Authors: Aldo Pacchiano, Christoph Dann, Claudio Gentile
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2306.02869 ,  5234kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09136
replaced with revised version Mon, 22 Jan 2024 00:51:05 GMT   (83kb,D)

Title: Finite-Time Logarithmic Bayes Regret Upper Bounds
Authors: Alexia Atsidakou, Branislav Kveton, Sumeet Katariya, Constantine
  Caramanis, and Sujay Sanghavi
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2306.09136 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04417
replaced with revised version Sat, 20 Jan 2024 07:20:27 GMT   (263kb,D)

Title: Fairness-aware Federated Minimax Optimization with Convergence Guarantee
Authors: Gerry Windiarto Mohamad Dunda and Shenghui Song
Categories: cs.LG cs.CY
Comments: At the time of uploading, it is currently under review in IEEE CAI
\\ ( https://arxiv.org/abs/2307.04417 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08097
replaced with revised version Sat, 20 Jan 2024 05:00:29 GMT   (380kb,D)

Title: EasyTPP: Towards Open Benchmarking Temporal Point Processes
Authors: Siqiao Xue, Xiaoming Shi, Zhixuan Chu, Yan Wang, Hongyan Hao, Fan
  Zhou, Caigao Jiang, Chen Pan, James Y. Zhang, Qingsong Wen, Jun Zhou,
  Hongyuan Mei
Categories: cs.LG
Comments: ICLR 2024 camera ready
\\ ( https://arxiv.org/abs/2307.08097 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10266
replaced with revised version Fri, 19 Jan 2024 23:51:05 GMT   (2059kb,D)

Title: A DPLL(T) Framework for Verifying Deep Neural Networks
Authors: Hai Duong, ThanhVu Nguyen, Matthew Dwyer
Categories: cs.LG cs.LO cs.SE
Comments: NeuralSAT is avaliable at: https://github.com/dynaroars/neuralsat
\\ ( https://arxiv.org/abs/2307.10266 ,  2059kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13158
replaced with revised version Mon, 22 Jan 2024 03:47:17 GMT   (10467kb,D)

Title: Multi-UAV Speed Control with Collision Avoidance and Handover-aware Cell
  Association: DRL with Action Branching
Authors: Zijiang Yan, Wael Jaafar, Bassant Selim, Hina Tabassum
Categories: cs.LG cs.RO cs.SY eess.SY
Comments: IEEE Globecom 2023 Accepted
\\ ( https://arxiv.org/abs/2307.13158 ,  10467kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13716
replaced with revised version Sun, 21 Jan 2024 07:09:35 GMT   (27349kb,D)

Title: FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on
  Staged Reinforcement Learning
Authors: Leiming Chen, Cihao Dong, Sibo Qiao, Ziling Huang, Yuming Nie,
  Zhaoxiang Hou, Chee Wei Tan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2307.13716 ,  27349kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04620
replaced with revised version Sat, 20 Jan 2024 15:03:37 GMT   (175kb,D)

Title: Multiclass Online Learnability under Bandit Feedback
Authors: Ananth Raman, Vinod Raman, Unique Subedi, Idan Mehalel, Ambuj Tewari
Categories: cs.LG stat.ML
Comments: 16 pages, ALT 2024 Camera Ready
\\ ( https://arxiv.org/abs/2308.04620 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09543
replaced with revised version Fri, 19 Jan 2024 19:45:37 GMT   (3972kb,D)

Title: Latent State Models of Training Dynamics
Authors: Michael Y. Hu, Angelica Chen, Naomi Saphra, Kyunghyun Cho
Categories: cs.LG
Comments: Accepted at TMLR 2023. Updated Jan 19, 2024 with erratum
\\ ( https://arxiv.org/abs/2308.09543 ,  3972kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09647
replaced with revised version Mon, 22 Jan 2024 11:14:39 GMT   (806kb,D)

Title: Robust Uncertainty Quantification Using Conformalised Monte Carlo
  Prediction
Authors: Daniel Bethell, Simos Gerasimou, Radu Calinescu
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2308.09647 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01115
replaced with revised version Sat, 20 Jan 2024 12:29:57 GMT   (1019kb)

Title: Multicollinearity Resolution Based on Machine Learning: A Case Study of
  Carbon Emissions in Sichuan Province
Authors: Xuanming Zhang, Xiaoxue Wang, Yonghang Chen
Categories: cs.LG
Comments: 21 pages,19 figures
\\ ( https://arxiv.org/abs/2309.01115 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10688
replaced with revised version Mon, 22 Jan 2024 11:26:17 GMT   (935kb,D)

Title: On the different regimes of Stochastic Gradient Descent
Authors: Antonio Sclocchi and Matthieu Wyart
Categories: cs.LG cond-mat.dis-nn stat.ML
Comments: Main: 8 pages, 4 figures; Appendix: 20 pages, 10 figures
\\ ( https://arxiv.org/abs/2309.10688 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11680
replaced with revised version Fri, 19 Jan 2024 19:53:21 GMT   (1836kb,D)

Title: Federated Learning with Neural Graphical Models
Authors: Urszula Chajewska, Harsh Shrivastava
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.11680 ,  1836kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12701
replaced with revised version Mon, 22 Jan 2024 14:53:22 GMT   (7204kb,D)

Title: Decision Tree Search as a Markov Decision Problem
Authors: Hector Kohler, Riad Akrour, Philippe Preux
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.12701 ,  7204kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13365
replaced with revised version Sun, 21 Jan 2024 13:07:27 GMT   (505kb,D)

Title: Limits of Actor-Critic Algorithms for Decision Tree Policies Learning in
  IBMDPs
Authors: Hector Kohler, Riad Akrour, Philippe Preux
Categories: cs.LG cs.AI
Comments: To be included in an other submission. arXiv admin note: text overlap
  with arXiv:2304.05839
\\ ( https://arxiv.org/abs/2309.13365 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01361
replaced with revised version Sun, 21 Jan 2024 21:01:12 GMT   (29260kb,D)

Title: GenSim: Generating Robotic Simulation Tasks via Large Language Models
Authors: Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao,
  Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang
Categories: cs.LG cs.CL cs.CV cs.RO
Comments: See our project website (https://liruiw.github.io/gensim), demo and
  datasets (https://huggingface.co/spaces/Gen-Sim/Gen-Sim), and code
  (https://github.com/liruiw/GenSim) for more details
Journal-ref: International Conference on Learning Representations (ICLR), 2024
\\ ( https://arxiv.org/abs/2310.01361 ,  29260kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06333
replaced with revised version Mon, 22 Jan 2024 02:22:12 GMT   (52kb)

Title: Learning bounded-degree polytrees with known skeleton
Authors: Davin Choo, Joy Qiping Yang, Arnab Bhattacharyya, Cl\'ement L. Canonne
Categories: cs.LG cs.DS math.PR math.ST stat.ML stat.TH
Comments: Fixed some typos. Added some discussions. Accepted to ALT 2024
\\ ( https://arxiv.org/abs/2310.06333 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17168
replaced with revised version Mon, 22 Jan 2024 00:12:20 GMT   (853kb,D)

Title: Learning an Inventory Control Policy with General Inventory Arrival
  Dynamics
Authors: Sohrab Andaz, Carson Eisenach, Dhruv Madeka, Kari Torkkola, Randy Jia,
  Dean Foster, Sham Kakade
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.17168 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17544
replaced with revised version Sun, 21 Jan 2024 13:24:13 GMT   (746kb,D)

Title: Hierarchical Ensemble-Based Feature Selection for Time Series
  Forecasting
Authors: Aysin Tumay, Mustafa E. Aydin, Ali T. Koc, and Suleyman S. Kozat
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.17544 ,  746kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00931
replaced with revised version Sat, 20 Jan 2024 17:05:40 GMT   (3786kb,D)

Title: Learning Defect Prediction from Unrealistic Data
Authors: Kamel Alrashedy, Vincent J. Hellendoorn, Alessandro Orso
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.00931 ,  3786kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02332
replaced with revised version Sat, 20 Jan 2024 04:36:50 GMT   (2842kb,D)

Title: Multimodal Machine Learning in Image-Based and Clinical Biomedicine:
  Survey and Prospects
Authors: Elisa Warner, Joonsang Lee, William Hsu, Tanveer Syeda-Mahmood,
  Charles Kahn, Olivier Gevaert and Arvind Rao
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2311.02332 ,  2842kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03242
replaced with revised version Mon, 22 Jan 2024 13:40:16 GMT   (699kb,D)

Title: Approximating Langevin Monte Carlo with ResNet-like Neural Network
  architectures
Authors: Charles Miranda, Janina Sch\"utte, David Sommer, Martin Eigel
Categories: cs.LG math.PR math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2311.03242 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05587
replaced with revised version Sat, 20 Jan 2024 13:47:45 GMT   (1942kb)

Title: Bayesian Methods for Media Mix Modelling with shape and funnel effects
Authors: Javier Marin
Categories: cs.LG
Comments: Rev. 5, Jan 2023
\\ ( https://arxiv.org/abs/2311.05587 ,  1942kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06558
replaced with revised version Mon, 22 Jan 2024 10:07:39 GMT   (34215kb,D)

Title: Convolve and Conquer: Data Comparison with Wiener Filters
Authors: Deborah Pelacani Cruz, George Strong, Oscar Bates, Carlos Cueto,
  Jiashun Yao, Lluis Guasch
Categories: cs.LG
Comments: 10 pages, 5 figures, Medical Imaging Meets Neurips Workshop
\\ ( https://arxiv.org/abs/2311.06558 ,  34215kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09018
replaced with revised version Fri, 19 Jan 2024 19:24:26 GMT   (66kb)

Title: On the Foundation of Distributionally Robust Reinforcement Learning
Authors: Shengbo Wang, Nian Si, Jose Blanchet, Zhengyuan Zhou
Categories: cs.LG cs.SY eess.SY math.OC stat.ML
\\ ( https://arxiv.org/abs/2311.09018 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00157
replaced with revised version Sat, 20 Jan 2024 02:56:24 GMT   (387kb,D)

Title: Universal Backdoor Attacks
Authors: Benjamin Schneider, Nils Lukas, Florian Kerschbaum
Categories: cs.LG cs.CR cs.CV
Comments: Accepted for publication at ICLR 2024
\\ ( https://arxiv.org/abs/2312.00157 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05134
replaced with revised version Sat, 20 Jan 2024 17:04:34 GMT   (1476kb,D)

Title: Optimal Multi-Distribution Learning
Authors: Zihan Zhang, Wenhao Zhan, Yuxin Chen, Simon S. Du, Jason D. Lee
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2312.05134 ,  1476kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07178
replaced with revised version Mon, 22 Jan 2024 10:31:56 GMT   (1134kb,D)

Title: Beyond Expected Return: Accounting for Policy Reproducibility when
  Evaluating Reinforcement Learning Algorithms
Authors: Manon Flageat, Bryan Lim, Antoine Cully
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.07178 ,  1134kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07930
replaced with revised version Sun, 21 Jan 2024 05:22:22 GMT   (143kb,D)

Title: Towards Optimal Statistical Watermarking
Authors: Baihe Huang and Banghua Zhu and Hanlin Zhu and Jason D. Lee and
  Jiantao Jiao and Michael I. Jordan
Categories: cs.LG cs.CL cs.CR cs.IT math.IT stat.ML
\\ ( https://arxiv.org/abs/2312.07930 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11413
replaced with revised version Sun, 21 Jan 2024 11:45:29 GMT   (34777kb,D)

Title: DeRDaVa: Deletion-Robust Data Valuation for Machine Learning
Authors: Xiao Tian, Rachael Hwee Ling Sim, Jue Fan, Bryan Kian Hsiang Low
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.11413 ,  34777kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11835
replaced with revised version Mon, 22 Jan 2024 02:48:48 GMT   (1431kb,D)

Title: Provably Convergent Federated Trilevel Learning
Authors: Yang Jiao, Kai Yang, Tiancheng Wu, Chengtao Jian, Jianwei Huang
Categories: cs.LG math.OC
Comments: Accepted at AAAI 2024
\\ ( https://arxiv.org/abs/2312.11835 ,  1431kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11976
replaced with revised version Sun, 21 Jan 2024 04:08:28 GMT   (5595kb,D)

Title: When Model Meets New Normals: Test-time Adaptation for Unsupervised
  Time-series Anomaly Detection
Authors: Dongmin Kim, Sunghyun Park, Jaegul Choo
Categories: cs.LG cs.AI
Comments: Accepted to AAAI 2024, 17 pages, https://github.com/carrtesy/M2N2
\\ ( https://arxiv.org/abs/2312.11976 ,  5595kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13118
replaced with revised version Mon, 22 Jan 2024 00:50:55 GMT   (669kb,D)

Title: LRS: Enhancing Adversarial Transferability through Lipschitz Regularized
  Surrogate
Authors: Tao Wu, Tie Luo, and Donald C. Wunsch
Categories: cs.LG cs.CR
Comments: AAAI 2024 main track. Code available on Github (see abstract).
  Appendix is included in this updated version
\\ ( https://arxiv.org/abs/2312.13118 ,  669kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13141
replaced with revised version Mon, 22 Jan 2024 15:07:26 GMT   (389kb,D)

Title: Augment on Manifold: Mixup Regularization with UMAP
Authors: Yousef El-Laham, Elizabeth Fons, Dillon Daudert, Svitlana Vyetrenko
Categories: cs.LG
Comments: accepted paper to be published in the proceedings of ICASSP 2024
\\ ( https://arxiv.org/abs/2312.13141 ,  389kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13152
replaced with revised version Mon, 22 Jan 2024 15:04:57 GMT   (1886kb,D)

Title: Neural Stochastic Differential Equations with Change Points: A
  Generative Adversarial Approach
Authors: Zhongchang Sun, Yousef El-Laham, Svitlana Vyetrenko
Categories: cs.LG stat.ML
Comments: accepted paper to be published in the proceedings of ICASSP 2024
\\ ( https://arxiv.org/abs/2312.13152 ,  1886kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16113
replaced with revised version Mon, 22 Jan 2024 01:38:12 GMT   (2296kb,D)

Title: Task-Driven Causal Feature Distillation: Towards Trustworthy Risk
  Prediction
Authors: Zhixuan Chu, Mengxuan Hu, Qing Cui, Longfei Li, Sheng Li
Categories: cs.LG cs.AI
Comments: Proceedings of the 2024 AAAI Conference on Artificial Intelligence
\\ ( https://arxiv.org/abs/2312.16113 ,  2296kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16313
replaced with revised version Fri, 19 Jan 2024 20:22:43 GMT   (15065kb,D)

Title: Unraveling the Key Components of OOD Generalization via Diversification
Authors: Harold Benoit, Liangze Jiang, Andrei Atanov, O\u{g}uzhan Fatih Kar,
  Mattia Rigotti, Amir Zamir
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2312.16313 ,  15065kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01084
replaced with revised version Mon, 22 Jan 2024 01:16:24 GMT   (1659kb,D)

Title: Global Convergence of Natural Policy Gradient with Hessian-aided
  Momentum Variance Reduction
Authors: Jie Feng, Ke Wei and Jinchi Chen
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2401.01084 ,  1659kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07364
replaced with revised version Sun, 21 Jan 2024 22:08:20 GMT   (466kb,D)

Title: PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar
  Nonlinear Conservation Laws
Authors: Liu Yang, Stanley J. Osher
Categories: cs.LG cs.AI cs.NA math.NA
\\ ( https://arxiv.org/abs/2401.07364 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07991
replaced with revised version Sat, 20 Jan 2024 20:21:00 GMT   (475kb,D)

Title: Robustness Against Adversarial Attacks via Learning Confined Adversarial
  Polytopes
Authors: Shayan Mohajer Hamidi, Linfeng Ye
Categories: cs.LG cs.CR
Comments: The paper has been accepted in ICASSP 2024
\\ ( https://arxiv.org/abs/2401.07991 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09031
replaced with revised version Sun, 21 Jan 2024 20:49:31 GMT   (11217kb,D)

Title: Data Attribution for Diffusion Models: Timestep-induced Bias in
  Influence Estimation
Authors: Tong Xie, Haoyu Li, Andrew Bai, Cho-Jui Hsieh
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.09031 ,  11217kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09074
replaced with revised version Sun, 21 Jan 2024 15:15:30 GMT   (8472kb,D)

Title: Code Simulation Challenges for Large Language Models
Authors: Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin,
  Anthony Cohn, Nigel Shadbolt, Michael Wooldridge
Categories: cs.LG cs.AI cs.CL cs.PL
Comments: main paper (10 pages) + Appendix (11 pages)
\\ ( https://arxiv.org/abs/2401.09074 ,  8472kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09671
replaced with revised version Sun, 21 Jan 2024 07:27:25 GMT   (34508kb,D)

Title: Towards Identifiable Unsupervised Domain Translation: A Diversified
  Distribution Matching Approach
Authors: Sagar Shrestha and Xiao Fu
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2401.09671 ,  34508kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09793
replaced with revised version Sun, 21 Jan 2024 02:35:31 GMT   (1384kb,D)

Title: PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection
Authors: Zhijie Zhong, Zhiwen Yu, Yiyuan Yang, Weizheng Wang, Kaixiang Yang
Categories: cs.LG
Comments: 13 pages, 16 figures, IJCAI 2024 under review
\\ ( https://arxiv.org/abs/2401.09793 ,  1384kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09953
replaced with revised version Mon, 22 Jan 2024 08:32:02 GMT   (460kb,D)

Title: Through the Dual-Prism: A Spectral Perspective on Graph Data
  Augmentation for Graph Classification
Authors: Yutong Xia, Runpeng Yu, Yuxuan Liang, Xavier Bresson, Xinchao Wang,
  Roger Zimmermann
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.09953 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10155
replaced with revised version Sun, 21 Jan 2024 10:50:52 GMT   (1731kb,D)

Title: A novel hybrid time-varying graph neural network for traffic flow
  forecasting
Authors: Ben Ao Dai, Bao-Lin Ye
Categories: cs.LG
Comments: 12 pages 1figures
\\ ( https://arxiv.org/abs/2401.10155 ,  1731kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10337
replaced with revised version Mon, 22 Jan 2024 12:33:43 GMT   (932kb,D)

Title: Noise Contrastive Estimation-based Matching Framework for Low-resource
  Security Attack Pattern Recognition
Authors: Tu Nguyen, Nedim Srndic, Alexander Neth
Categories: cs.LG cs.AI cs.CL cs.CR
Comments: accepted at EACL 2024, in ARR October 2023
\\ ( https://arxiv.org/abs/2401.10337 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10371
replaced with revised version Mon, 22 Jan 2024 05:24:17 GMT   (737kb,D)

Title: Langevin Unlearning: A New Perspective of Noisy Gradient Descent for
  Machine Unlearning
Authors: Eli Chien, Haoyu Wang, Ziang Chen, Pan Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.10371 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10393
replaced with revised version Mon, 22 Jan 2024 12:04:18 GMT   (1601kb,D)

Title: Catastrophic Interference is Mitigated in Naturalistic Power-Law
  Learning Environments
Authors: Atith Gandhi, Raj Sanjay Shah, Vijay Marupudi, Sashank Varma
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.10393 ,  1601kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10765
replaced with revised version Mon, 22 Jan 2024 08:17:42 GMT   (11686kb,D)

Title: Starlit: Privacy-Preserving Federated Learning to Enhance Financial
  Fraud Detection
Authors: Aydin Abadi, Bradley Doyle, Francesco Gini, Kieron Guinamard, Sasi
  Kumar Murakonda, Jack Liddell, Paul Mellor, Steven J. Murdoch, Mohammad
  Naseri, Hector Page, George Theodorakopoulos, Suzanne Weller
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2401.10765 ,  11686kb)
------------------------------------------------------------------------------
\\
arXiv:2207.00067
replaced with revised version Mon, 22 Jan 2024 12:24:36 GMT   (2703kb,D)

Title: Rethinking Unsupervised Domain Adaptation for Semantic Segmentation
Authors: Zhijie Wang, Masanori Suganuma, Takayuki Okatani
Categories: cs.CV cs.AI
Comments: Under review in Pattern Recognition Letters
\\ ( https://arxiv.org/abs/2207.00067 ,  2703kb)
------------------------------------------------------------------------------
\\
arXiv:2208.04957
replaced with revised version Sat, 20 Jan 2024 06:01:06 GMT   (5850kb,D)

Title: Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution
Authors: Ke Xue, Yutong Wang, Cong Guan, Lei Yuan, Haobo Fu, Qiang Fu, Chao
  Qian, Yang Yu
Categories: cs.NE cs.AI cs.LG cs.MA
\\ ( https://arxiv.org/abs/2208.04957 ,  5850kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06419 (*cross-listing*)
replaced with revised version Sun, 21 Jan 2024 07:41:02 GMT   (793kb,D)

Title: AV-data2vec: Self-supervised Learning of Audio-Visual Speech
  Representations with Contextualized Target Representations
Authors: Jiachen Lian and Alexei Baevski and Wei-Ning Hsu and Michael Auli
Categories: eess.AS cs.AI cs.CL
Comments: 2023 ASRU
\\ ( https://arxiv.org/abs/2302.06419 ,  793kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11337
replaced with revised version Sun, 21 Jan 2024 09:58:48 GMT   (23977kb,D)

Title: Bayesian Matrix Decomposition and Applications
Authors: Jun Lu
Categories: math.NA cs.AI cs.CE cs.LG cs.NA
\\ ( https://arxiv.org/abs/2302.11337 ,  23977kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07064
replaced with revised version Mon, 22 Jan 2024 13:26:32 GMT   (2252kb,D)

Title: A Generalized Multi-Modal Fusion Detection Framework
Authors: Leichao Cui, Xiuxian Li, Min Meng, and Xiaoyu Mo
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2303.07064 ,  2252kb)
------------------------------------------------------------------------------
\\
arXiv:2303.13472
replaced with revised version Sun, 21 Jan 2024 16:14:44 GMT   (44959kb,D)

Title: Promptable Game Models: Text-Guided Game Simulation via Masked Diffusion
  Models
Authors: Willi Menapace, Aliaksandr Siarohin, St\'ephane Lathuili\`ere, Panos
  Achlioptas, Vladislav Golyanik, Sergey Tulyakov, Elisa Ricci
Categories: cs.CV cs.AI
Comments: ACM Transactions on Graphics \c{opyright} Copyright is held by the
  owner/author(s) 2023. This is the author's version of the work. It is posted
  here for your personal use. Not for redistribution. The definitive Version of
  Record was published in ACM Transactions on Graphics,
  http://dx.doi.org/10.1145/3635705
DOI: 10.1145/3635705
\\ ( https://arxiv.org/abs/2303.13472 ,  44959kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04942
replaced with revised version Sat, 20 Jan 2024 05:27:36 GMT   (7371kb,D)

Title: Semantic Communications for Artificial Intelligence Generated Content
  (AIGC) Toward Effective Content Creation
Authors: Guangyuan Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong,
  Dong In Kim, and Xuemin (Sherman) Shen
Categories: cs.NI cs.AI
Comments: 9 pages,5figures
\\ ( https://arxiv.org/abs/2308.04942 ,  7371kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13507
replaced with revised version Mon, 22 Jan 2024 18:54:52 GMT   (296kb,D)

Title: Large Language Models Should Ask Clarifying Questions to Increase
  Confidence in Generated Code
Authors: Jie JW Wu
Categories: cs.SE cs.AI cs.LG
Comments: 6 pages, 2 figures, 1 table. Accepted and presented at the 7th Annual
  Symposium on Machine Programming (MAPS 2023 Workshop, see
  https://mapsworkshop.github.io/). Reference: "Wu, Jie JW. Large Language
  Models Should Ask Clarifying Questions to Increase Confidence in Generated
  Code. The 7th Annual Symposium on Machine Programming (MAPS 23), December 3,
  2023, San Francisco, CA, USA"
\\ ( https://arxiv.org/abs/2308.13507 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05030
replaced with revised version Mon, 22 Jan 2024 18:22:37 GMT   (237kb,D)

Title: Decolonial AI Alignment: Openness, Vi\'{s}e\d{s}a-Dharma, and Including
  Excluded Knowledges
Authors: Kush R. Varshney
Categories: cs.CY cs.AI stat.ML
\\ ( https://arxiv.org/abs/2309.05030 ,  237kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07525
replaced with revised version Sun, 21 Jan 2024 08:57:40 GMT   (1041kb,D)

Title: SingFake: Singing Voice Deepfake Detection
Authors: Yongyi Zang, You Zhang, Mojtaba Heydari, Zhiyao Duan
Categories: cs.SD cs.AI eess.AS
Comments: Accepted at ICASSP 2024
\\ ( https://arxiv.org/abs/2309.07525 ,  1041kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12244
replaced with revised version Sun, 21 Jan 2024 16:30:35 GMT   (8527kb,D)

Title: ChaCha: Leveraging Large Language Models to Prompt Children to Share
  Their Emotions about Personal Events
Authors: Woosuk Seo, Chanmo Yang, Young-Ho Kim
Categories: cs.HC cs.AI cs.CL
Comments: 16 pages, 5 figures, 2 tables; Accepted at ACM CHI 2024
ACM-class: H.5.2; I.2.7
\\ ( https://arxiv.org/abs/2309.12244 ,  8527kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00106
replaced with revised version Sat, 20 Jan 2024 09:57:47 GMT   (14633kb,D)

Title: FashionFlow: Leveraging Diffusion Models for Dynamic Fashion Video
  Synthesis from Static Imagery
Authors: Tasin Islam, Alina Miron, XiaoHui Liu, Yongmin Li
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.00106 ,  14633kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02255
replaced with revised version Sun, 21 Jan 2024 03:47:06 GMT   (21346kb,D)

Title: MathVista: Evaluating Mathematical Reasoning of Foundation Models in
  Visual Contexts
Authors: Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh
  Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, Jianfeng Gao
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 116 pages, 120 figures. Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.02255 ,  21346kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05916
replaced with revised version Mon, 22 Jan 2024 18:08:52 GMT   (35915kb,D)

Title: Interpreting CLIP's Image Representation via Text-Based Decomposition
Authors: Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt
Categories: cs.CV cs.AI
Comments: Project page and code:
  https://yossigandelsman.github.io/clip_decomposition/
\\ ( https://arxiv.org/abs/2310.05916 ,  35915kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11036 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 08:44:26 GMT   (1256kb,D)

Title: Radio Map Estimation: Empirical Validation and Analysis
Authors: Raju Shrestha, Tien Ngoc Ha, Pham Q. Viet and Daniel Romero
Categories: eess.SP cs.AI physics.app-ph
Comments: 13 pages, Journal version, submitted to the IEEE Transactions on
  Wireless Communications
\\ ( https://arxiv.org/abs/2310.11036 ,  1256kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12817
replaced with revised version Mon, 22 Jan 2024 09:44:18 GMT   (3498kb,D)

Title: 2D-3D Interlaced Transformer for Point Cloud Segmentation with
  Scene-Level Supervision
Authors: Cheng-Kun Yang, Min-Hung Chen, Yung-Yu Chuang, Yen-Yu Lin
Categories: cs.CV cs.AI cs.LG
Comments: ICCV 2023 (main + supp). Website:
  https://jimmy15923.github.io/mit_web/
\\ ( https://arxiv.org/abs/2310.12817 ,  3498kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20381
replaced with revised version Sun, 21 Jan 2024 01:34:09 GMT   (36565kb,D)

Title: A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical
  Image Analysis
Authors: Yingshu Li, Yunyi Liu, Zhanyu Wang, Xinyu Liang, Lei Wang, Lingqiao
  Liu, Leyang Cui, Zhaopeng Tu, Longyue Wang, Luping Zhou
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2310.20381 ,  36565kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06101
replaced with revised version Mon, 22 Jan 2024 09:27:30 GMT   (734kb,D)

Title: In-Context Learning for MIMO Equalization Using Transformer-Based
  Sequence Models
Authors: Matteo Zecchin, Kai Yu, Osvaldo Simeone
Categories: cs.IT cs.AI cs.LG math.IT
\\ ( https://arxiv.org/abs/2311.06101 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09127
replaced with revised version Sat, 20 Jan 2024 18:55:51 GMT   (2697kb,D)

Title: Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts
Authors: Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou and Lichao Sun
Categories: cs.CR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.09127 ,  2697kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16141
replaced with revised version Sat, 20 Jan 2024 05:20:47 GMT   (195kb,D)

Title: Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired
  by Critical Brain Hypothesis
Authors: Shuo Chen, Boxiao Liu, Haihang You
Categories: cs.NE cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.16141 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07343
replaced with revised version Mon, 22 Jan 2024 14:29:07 GMT   (123kb)

Title: Can ChatGPT Play the Role of a Teaching Assistant in an Introductory
  Programming Course?
Authors: Anishka, Atharva Mehta, Nipun Gupta, Aarav Balachandran, Dhruv Kumar,
  Pankaj Jalote
Categories: cs.HC cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2312.07343 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10269
replaced with revised version Sat, 20 Jan 2024 13:59:09 GMT   (207kb,D)

Title: The DSA Transparency Database: Auditing Self-reported Moderation Actions
  by Social Media
Authors: Amaury Trujillo, Tiziano Fagni, Stefano Cresci
Categories: cs.SI cs.AI cs.CY cs.HC
\\ ( https://arxiv.org/abs/2312.10269 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10305
replaced with revised version Sat, 20 Jan 2024 02:10:18 GMT   (5511kb,D)

Title: Self-Supervised Disentangled Representation Learning for Robust Target
  Speech Extraction
Authors: Zhaoxi Mu, Xinyu Yang, Sining Sun, Qing Yang
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2312.10305 ,  5511kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04846 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 22:09:12 GMT   (27833kb,D)

Title: The inherent goodness of well educated intelligence
Authors: Michael E. Glinsky and Sharon Sievert
Categories: econ.TH cs.AI
Comments: 12 pages, 10 figures, 15 equations, to be submitted to Nature
\\ ( https://arxiv.org/abs/2401.04846 ,  27833kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07278
replaced with revised version Mon, 22 Jan 2024 04:43:04 GMT   (8720kb,D)

Title: Semi-supervised Semantic Segmentation using Redesigned Self-Training for
  White Blood Cell
Authors: Vinh Quoc Luu, Duy Khanh Le, Huy Thanh Nguyen, Minh Thanh Nguyen,
  Thinh Tien Nguyen, Vinh Quang Dinh
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.07278 ,  8720kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07378
replaced with revised version Sat, 20 Jan 2024 04:24:34 GMT   (6995kb,D)

Title: Efficient approximation of Earth Mover's Distance Based on Nearest
  Neighbor Search
Authors: Guangyu Meng, Ruyu Zhou, Liu Liu, Peixian Liang, Fang Liu, Danny Chen,
  Michael Niemier, X.Sharon Hu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.07378 ,  6995kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07450
replaced with revised version Sat, 20 Jan 2024 05:21:13 GMT   (8587kb,D)

Title: Hierarchical Fashion Design with Multi-stage Diffusion Models
Authors: Zhifeng Xie, Hao Li, Huiming Ding, Mengtian Li, Ying Cao
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.07450 ,  8587kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09451 (*cross-listing*)
replaced with revised version Sun, 21 Jan 2024 05:42:29 GMT   (1086kb,D)

Title: Diffusion-Driven Generative Framework for Molecular Conformation
  Prediction
Authors: Bobin Yang, Jie Deng, Zhenghan Chen, Ruoxue Wu
Categories: q-bio.BM cs.AI cs.LG physics.chem-ph
Comments: arXiv admin note: text overlap with arXiv:2105.07246 by other authors
\\ ( https://arxiv.org/abs/2401.09451 ,  1086kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10273
replaced with revised version Mon, 22 Jan 2024 04:47:20 GMT   (1011kb)

Title: Revolutionizing Pharma: Unveiling the AI and LLM Trends in the
  Pharmaceutical Industry
Authors: Yu Han, Jingwen Tao
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2401.10273 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2206.14358
replaced with revised version Mon, 22 Jan 2024 00:38:08 GMT   (978kb)

Title: Using Twitter Data to Understand Public Perceptions of Approved versus
  Off-label Use for COVID-19-related Medications
Authors: Yining Hua, Hang Jiang, Shixu Lin, Jie Yang, Joseph M. Plasek, David
  W. Bates, Li Zhou
Categories: cs.CY cs.CL cs.LG stat.AP
Comments: Full paper published in JAMIA
Journal-ref: amiajnl-2022-012337.R1
DOI: 10.1093/jamia/ocac114
\\ ( https://arxiv.org/abs/2206.14358 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03047
replaced with revised version Mon, 22 Jan 2024 04:57:32 GMT   (9898kb,D)

Title: ETPNav: Evolving Topological Planning for Vision-Language Navigation in
  Continuous Environments
Authors: Dong An, Hanqing Wang, Wenguan Wang, Zun Wang, Yan Huang, Keji He,
  Liang Wang
Categories: cs.CV cs.CL cs.RO
Comments: Project page: https://github.com/MarSaKi/ETPNav
\\ ( https://arxiv.org/abs/2304.03047 ,  9898kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14212 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 15:05:30 GMT   (3963kb,D)

Title: Annotation Sensitivity: Training Data Collection Methods Affect Model
  Performance
Authors: Christoph Kern, Stephanie Eckman, Jacob Beck, Rob Chew, Bolei Ma,
  Frauke Kreuter
Categories: stat.ML cs.CL cs.LG stat.ME
Comments: EMNLP 2023 Findings:
  https://aclanthology.org/2023.findings-emnlp.992/
\\ ( https://arxiv.org/abs/2311.14212 ,  3963kb)
------------------------------------------------------------------------------
\\
arXiv:2202.05612 (*cross-listing*)
replaced with revised version Sat, 20 Jan 2024 03:21:05 GMT   (142kb,D)

Title: High-dimensional Inference and FDR Control for Simulated Markov Random
  Fields
Authors: Haoyu Wei, Xiaoyu Lei, Yixin Han, Huiming Zhang
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2202.05612 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2204.07526 (*cross-listing*)
replaced with revised version Sat, 20 Jan 2024 20:00:12 GMT   (289kb,D)

Title: Statistical-Computational Trade-offs in Tensor PCA and Related Problems
  via Communication Complexity
Authors: Rishabh Dudeja and Daniel Hsu
Categories: math.ST cs.IT cs.LG math.IT stat.ML stat.TH
\\ ( https://arxiv.org/abs/2204.07526 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14540
replaced with revised version Sun, 21 Jan 2024 02:12:04 GMT   (859kb,D)

Title: SupMAE: Supervised Masked Autoencoders Are Efficient Vision Learners
Authors: Feng Liang, Yangguang Li, Diana Marculescu
Categories: cs.CV cs.LG
Comments: Edge Intelligence Workshop Workshop at AAAI 2024
\\ ( https://arxiv.org/abs/2205.14540 ,  859kb)
------------------------------------------------------------------------------
\\
arXiv:2208.05481 (*cross-listing*)
replaced with revised version Sat, 20 Jan 2024 06:13:31 GMT   (7202kb,D)

Title: High-Frequency Space Diffusion Models for Accelerated MRI
Authors: Chentao Cao, Zhuo-Xu Cui, Yue Wang, Shaonan Liu, Taijin Chen, Hairong
  Zheng, Dong Liang, Yanjie Zhu
Categories: eess.IV cs.CV cs.LG
Comments: accepted for IEEE TMI
DOI: 10.1109/TMI.2024.3351702
\\ ( https://arxiv.org/abs/2208.05481 ,  7202kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09745 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 19:00:03 GMT   (718kb,D)

Title: Transfer learning with affine model transformation
Authors: Shunya Minami, Kenji Fukumizu, Yoshihiro Hayashi, Ryo Yoshida
Categories: stat.ML cs.LG
Comments: 34 pages
Journal-ref: NeurIPS 2023
\\ ( https://arxiv.org/abs/2210.09745 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00325
replaced with revised version Mon, 22 Jan 2024 02:56:53 GMT   (951kb,D)

Title: HashVFL: Defending Against Data Reconstruction Attacks in Vertical
  Federated Learning
Authors: Pengyu Qiu, Xuhong Zhang, Shouling Ji, Chong Fu, Xing Yang, Ting Wang
Categories: cs.CR cs.LG
DOI: 10.1109/TIFS.2024.3356164
\\ ( https://arxiv.org/abs/2212.00325 ,  951kb)
------------------------------------------------------------------------------
\\
arXiv:2212.03369 (*cross-listing*)
replaced with revised version Sun, 21 Jan 2024 05:32:57 GMT   (3823kb,D)

Title: Exploring Randomly Wired Neural Networks for Climate Model Emulation
Authors: William Yik, Sam J. Silva, Andrew Geiss, Duncan Watson-Parris
Categories: physics.ao-ph cs.LG
Comments: Accepted for publication in AIES
\\ ( https://arxiv.org/abs/2212.03369 ,  3823kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07287 (*cross-listing*)
replaced with revised version Sat, 20 Jan 2024 03:20:20 GMT   (8299kb,D)

Title: Tight Non-asymptotic Inference via Sub-Gaussian Intrinsic Moment Norm
Authors: Huiming Zhang, Haoyu Wei, Guang Cheng
Categories: stat.ML cs.LG econ.EM
\\ ( https://arxiv.org/abs/2303.07287 ,  8299kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00418
replaced with revised version Mon, 22 Jan 2024 07:09:17 GMT   (581kb,D)

Title: An Empirical Study of Using Large Language Models for Unit Test
  Generation
Authors: Mohammed Latif Siddiq, Joanna C. S. Santos, Ridwanul Hasan Tanvir,
  Noshin Ulfat, Fahmid Al Rifat, and Vinicius Carvalho Lopes
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2305.00418 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17028 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 16:25:13 GMT   (290kb,D)

Title: Better Batch for Deep Probabilistic Time Series Forecasting
Authors: Vincent Zhihao Zheng, Seongjin Choi, Lijun Sun
Categories: stat.ML cs.LG
Comments: 9 pages, 3 figures, camera-ready version, The 27th International
  Conference on Artificial Intelligence and Statistics (AISTATS 2024)
\\ ( https://arxiv.org/abs/2305.17028 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18394 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 10:44:50 GMT   (2202kb,D)

Title: On Optimal Regularization Parameters via Bilevel Learning
Authors: Matthias J. Ehrhardt, Silvia Gazzola and Sebastian J. Scott
  (Department of Mathematical Sciences, University of Bath, Bath, UK)
Categories: math.OC cs.LG
Comments: 34 pages, 11 figures. Version for publication
MSC-class: 65K10 (Primary) 65F22 (Secondary)
\\ ( https://arxiv.org/abs/2305.18394 ,  2202kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05023 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 08:47:49 GMT   (27806kb,D)

Title: Beyond Vanilla Variational Autoencoders: Detecting Posterior Collapse in
  Conditional and Hierarchical Variational Autoencoders
Authors: Hien Dang and Tho Tran and Tan Nguyen and Nhat Ho
Categories: stat.ML cs.LG
Comments: International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2306.05023 ,  27806kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06064
replaced with revised version Mon, 22 Jan 2024 12:04:06 GMT   (2292kb,D)

Title: Neural Algorithmic Reasoning for Combinatorial Optimisation
Authors: Dobrik Georgiev and Danilo Numeroso and Davide Bacciu and Pietro Li\`o
Categories: cs.NE cs.LG
\\ ( https://arxiv.org/abs/2306.06064 ,  2292kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06071
replaced with revised version Fri, 19 Jan 2024 07:54:11 GMT   (1050kb)

Title: Adversarial Attack On Yolov5 For Traffic And Road Sign Detection
Authors: Sanyam Jain
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2306.06071 ,  1050kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16761 (*cross-listing*)
replaced with revised version Sat, 20 Jan 2024 16:41:48 GMT   (238kb,D)

Title: Moreau Envelope Based Difference-of-weakly-Convex Reformulation and
  Algorithm for Bilevel Programs
Authors: Lucy L. Gao, Jane J. Ye, Haian Yin, Shangzhi Zeng, Jin Zhang
Categories: math.OC cs.LG
MSC-class: 90C99
\\ ( https://arxiv.org/abs/2306.16761 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17778
replaced with revised version Mon, 22 Jan 2024 00:54:30 GMT   (9859kb,D)

Title: Look, Remember and Reason: Grounded reasoning in videos with language
  models
Authors: Apratim Bhattacharyya, Sunny Panchal, Mingu Lee, Reza Pourreza, Pulkit
  Madan, Roland Memisevic
Categories: cs.CV cs.LG
Comments: To appear at ICLR 2024
\\ ( https://arxiv.org/abs/2306.17778 ,  9859kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11737
replaced with revised version Sun, 21 Jan 2024 04:55:06 GMT   (17488kb,D)

Title: Animal3D: A Comprehensive Dataset of 3D Animal Pose and Shape
Authors: Jiacong Xu, Yi Zhang, Jiawei Peng, Wufei Ma, Artur Jesslen, Pengliang
  Ji, Qixin Hu, Jiehua Zhang, Qihao Liu, Jiahao Wang, Wei Ji, Chen Wang,
  Xiaoding Yuan, Prakhar Kaushik, Guofeng Zhang, Jie Liu, Yushan Xie, Yawen
  Cui, Alan Yuille, Adam Kortylewski
Categories: cs.CV cs.LG
Comments: 11 pages, 5 figures, link to the dataset:
  https://xujiacong.github.io/Animal3D/
\\ ( https://arxiv.org/abs/2308.11737 ,  17488kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00733
replaced with revised version Fri, 19 Jan 2024 19:36:03 GMT   (2705kb,D)

Title: TExplain: Explaining Learned Visual Features via Pre-trained (Frozen)
  Language Models
Authors: Saeid Asgari Taghanaki, Aliasghar Khani, Amir Khasahmadi, Aditya
  Sanghi, Karl D.D. Willis, Ali Mahdavi-Amiri
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2309.00733 ,  2705kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03202 (*cross-listing*)
replaced with revised version Sun, 21 Jan 2024 00:00:42 GMT   (536kb,D)

Title: Evaluation of Reinforcement Learning Techniques for Trading on a Diverse
  Portfolio
Authors: Ishan S. Khare, Tarun K. Martheswaran, Akshana Dassanaike-Perera
Categories: q-fin.TR cs.LG
Comments: fixed minor typos
\\ ( https://arxiv.org/abs/2309.03202 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15462
replaced with revised version Mon, 22 Jan 2024 17:02:16 GMT   (44388kb,D)

Title: DTC: Deep Tracking Control
Authors: Fabian Jenelten, Junzhe He, Farbod Farshidian, Marco Hutter
Categories: cs.RO cs.LG cs.SY eess.SY
DOI: 10.1126/scirobotics.adh5401
\\ ( https://arxiv.org/abs/2309.15462 ,  44388kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16034
replaced with revised version Mon, 22 Jan 2024 11:26:35 GMT   (2230kb,D)

Title: Analytical Modelling of Raw Data for Flow-Guided In-body Nanoscale
  Localization
Authors: Guillem Pascual, Filip Lemic, Carmen Delgado, Xavier Costa-Perez
Categories: cs.ET cs.IR cs.LG cs.NI eess.SP
Comments: 6 pages, 7 figures, 4 tables, 16 references
\\ ( https://arxiv.org/abs/2309.16034 ,  2230kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16952
replaced with revised version Sat, 20 Jan 2024 19:43:04 GMT   (5820kb,D)

Title: Leveraging Optimization for Adaptive Attacks on Image Watermarks
Authors: Nils Lukas, Abdulrahman Diaa, Lucas Fenaux, Florian Kerschbaum
Categories: cs.CR cs.LG
Comments: ICLR'24
\\ ( https://arxiv.org/abs/2309.16952 ,  5820kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03298 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 04:39:36 GMT   (4462kb)

Title: A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive
  Sampling
Authors: Yi-Ping Chen, Liwei Wang, Yigitcan Comlek, Wei Chen
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.03298 ,  4462kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09126 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 13:14:33 GMT   (33409kb,D)

Title: Physics-guided Noise Neural Proxy for Practical Low-light Raw Image
  Denoising
Authors: Hansen Feng, Lizhi Wang, Yiqi Huang, Yuzhi Wang, Lin Zhu, Hua Huang
Categories: eess.IV cs.CV cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2310.09126 ,  33409kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19491 (*cross-listing*)
replaced with revised version Sun, 21 Jan 2024 22:35:34 GMT   (49kb)

Title: Generator Identification for Linear SDEs with Additive and
  Multiplicative Noise
Authors: Yuanyuan Wang, Xi Geng, Wei Huang, Biwei Huang, Mingming Gong
Categories: math.ST cs.LG stat.ML stat.TH
\\ ( https://arxiv.org/abs/2310.19491 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11990 (*cross-listing*)
replaced with revised version Sun, 21 Jan 2024 18:38:24 GMT   (11828kb)

Title: Machine-Learned Atomic Cluster Expansion Potentials for Fast and
  Quantum-Accurate Thermal Simulations of Wurtzite AlN
Authors: Guang Yang, Yuan-Bin Liu, Lei Yang, Bing-Yang Cao
Categories: cond-mat.mtrl-sci cs.LG physics.comp-ph
\\ ( https://arxiv.org/abs/2311.11990 ,  11828kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02063 (*cross-listing*)
replaced with revised version Sun, 21 Jan 2024 21:41:32 GMT   (10439kb,D)

Title: The GPU Phase Folding and Deep Learning Method for Detecting Exoplanet
  Transits
Authors: Kaitlyn Wang, Jian Ge, Kevin Willis, Kevin Wang, Yinan Zhao
Categories: astro-ph.EP astro-ph.IM cs.LG
Comments: 16 pages, 19 figures; Accepted for publication in the peer-reviewed
  journal, Monthly Notices of the Royal Astronomical Society (MNRAS), on
  January 20, 2024
\\ ( https://arxiv.org/abs/2312.02063 ,  10439kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02277 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 02:03:50 GMT   (1320kb,D)

Title: ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled
  Compositional Stochastic Optimization
Authors: Bokun Wang and Tianbao Yang
Categories: math.OC cs.LG
Comments: Fixed several typos; Added some numerical experiments
\\ ( https://arxiv.org/abs/2312.02277 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02471
replaced with revised version Sun, 21 Jan 2024 19:39:12 GMT   (439kb,D)

Title: Congestion-aware Distributed Task Offloading in Wireless Multi-hop
  Networks Using Graph Neural Networks
Authors: Zhongyuan Zhao and Jake Perazzone and Gunjan Verma and Santiago
  Segarra
Categories: cs.NI cs.LG eess.SP
Comments: 5 pages, 5 figures, accepted to IEEE ICASSP 2024
MSC-class: 05C90
ACM-class: C.2.1; C.2.2
\\ ( https://arxiv.org/abs/2312.02471 ,  439kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03311 (*cross-listing*)
replaced with revised version Sun, 21 Jan 2024 16:13:24 GMT   (538kb,D)

Title: On the Nystrom Approximation for Preconditioning in Kernel Machines
Authors: Amirhesam Abedsoltan, Mikhail Belkin, Parthe Pandit, Luis Rademacher
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2312.03311 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00313
replaced with revised version Sat, 20 Jan 2024 01:38:45 GMT   (1292kb,D)

Title: Matching of Users and Creators in Two-Sided Markets with Departures
Authors: Daniel Huttenlocher, Hannah Li, Liang Lyu, Asuman Ozdaglar and James
  Siderius
Categories: cs.GT cs.LG cs.SI econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2401.00313 ,  1292kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00334
replaced with revised version Sun, 21 Jan 2024 00:08:54 GMT   (3227kb,D)

Title: Explainability-Driven Leaf Disease Classification using Adversarial
  Training and Knowledge Distillation
Authors: Sebastian-Vasile Echim, Iulian-Marius T\u{a}iatu, Dumitru-Clementin
  Cercel, Florin Pop
Categories: cs.CV cs.LG
Comments: 10 pages, 8 figures, Accepted by ICAART 2024
\\ ( https://arxiv.org/abs/2401.00334 ,  3227kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03506 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 18:53:36 GMT   (177kb,D)

Title: DiarizationLM: Speaker Diarization Post-Processing with Large Language
  Models
Authors: Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, Hank Liao
Categories: eess.AS cs.LG cs.SD
\\ ( https://arxiv.org/abs/2401.03506 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06144
replaced with revised version Mon, 22 Jan 2024 17:11:57 GMT   (15916kb,D)

Title: DFU: scale-robust diffusion model for zero-shot super-resolution image
  generation
Authors: Alex Havrilla, Kevin Rojas, Wenjing Liao, Molei Tao
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.06144 ,  15916kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08573
replaced with revised version Mon, 22 Jan 2024 17:54:58 GMT   (11618kb,D)

Title: Benchmarking the Robustness of Image Watermarks
Authors: Bang An, Mucong Ding, Tahseen Rabbani, Aakriti Agrawal, Yuancheng Xu,
  Chenghao Deng, Sicheng Zhu, Abdirisak Mohamed, Yuxin Wen, Tom Goldstein,
  Furong Huang
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2401.08573 ,  11618kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08738 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 14:17:27 GMT   (2463kb)

Title: Machine Learning-Based Analysis of Ebola Virus' Impact on Gene
  Expression in Nonhuman Primates
Authors: Mostafa Rezapour, Muhammad Khalid Khan Niazi, Hao Lu, Aarthi
  Narayanan, Metin Nafi Gurcan
Categories: q-bio.GN cs.LG
Comments: 28 pages, 8 figures, 2 tables
\\ ( https://arxiv.org/abs/2401.08738 ,  2463kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08865
replaced with revised version Mon, 22 Jan 2024 15:30:08 GMT   (1569kb,D)

Title: The Effect of Intrinsic Dataset Properties on Generalization: Unraveling
  Learning Differences Between Natural and Medical Images
Authors: Nicholas Konz, Maciej A. Mazurowski
Categories: cs.CV cs.LG eess.IV stat.ML
Comments: ICLR 2024. Code:
  https://github.com/mazurowski-lab/intrinsic-properties
\\ ( https://arxiv.org/abs/2401.08865 ,  1569kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08947
replaced with revised version Sun, 21 Jan 2024 09:05:33 GMT   (1294kb)

Title: AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized
  Phishing URL Detection
Authors: Saba Aslam, Hafsa Aslam, Arslan Manzoor, Chen Hui, Abdur Rasool
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2401.08947 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10107 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 13:36:12 GMT   (860kb)

Title: Comparison analysis between standard polysomnographic data and
  in-ear-EEG signals: A preliminary study
Authors: Gianpaolo Palo, Luigi Fiorillo, Giuliana Monachino, Michal Bechny,
  Mark Melnykowycz, Athina Tzovara, Valentina Agostini, and Francesca Dalia
  Faraci
Categories: eess.SP cs.LG physics.med-ph
Comments: 29 pages, 12 figures, 1 table
\\ ( https://arxiv.org/abs/2401.10107 ,  860kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10305 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 18:12:20 GMT   (207kb,D)

Title: Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning
  Approach
Authors: Wun Yung Shaney Sze, Maryglen Pearl Herrero, Roger Garriga
Categories: eess.SP cs.LG
Comments: 9 pages, 5 figures
\\ ( https://arxiv.org/abs/2401.10305 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10451
replaced with revised version Mon, 22 Jan 2024 14:14:16 GMT   (2179kb,D)

Title: Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian
  Optimization Approach
Authors: Aron Brenner, Rahman Khorramfar, Dharik Mallapragada, Saurabh Amin
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2401.10451 ,  2179kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
