Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月25日 17:14
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 23 Jan 24 19:00:00 GMT  to  Wed 24 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.13006
Date: Tue, 23 Jan 2024 06:30:47 GMT   (10854kb,D)

Title: CIMGEN: Controlled Image Manipulation by Finetuning Pretrained
  Generative Models on Limited Data
Authors: Chandrakanth Gudavalli, Erik Rosten, Lakshmanan Nataraj, Shivkumar
  Chandrasekaran, B. S. Manjunath
Categories: cs.AI cs.LG eess.IV
\\
  Content creation and image editing can benefit from flexible user controls. A
common intermediate representation for conditional image generation is a
semantic map, that has information of objects present in the image. When
compared to raw RGB pixels, the modification of semantic map is much easier.
One can take a semantic map and easily modify the map to selectively insert,
remove, or replace objects in the map. The method proposed in this paper takes
in the modified semantic map and alter the original image in accordance to the
modified map. The method leverages traditional pre-trained image-to-image
translation GANs, such as CycleGAN or Pix2Pix GAN, that are fine-tuned on a
limited dataset of reference images associated with the semantic maps. We
discuss the qualitative and quantitative performance of our technique to
illustrate its capacity and possible applications in the fields of image
forgery and image editing. We also demonstrate the effectiveness of the
proposed image forgery technique in thwarting the numerous deep learning-based
image forensic techniques, highlighting the urgent need to develop robust and
generalizable image forensic tools in the fight against the spread of fake
media.
\\ ( https://arxiv.org/abs/2401.13006 ,  10854kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13110
Date: Tue, 23 Jan 2024 21:47:12 GMT   (1415kb,D)

Title: XAI for All: Can Large Language Models Simplify Explainable AI?
Authors: Philip Mavrepis, Georgios Makridis, Georgios Fatouros, Vasileios
  Koukos, Maria Margarita Separdani, Dimosthenis Kyriazis
Categories: cs.AI cs.HC
\\
  The field of Explainable Artificial Intelligence (XAI) often focuses on users
with a strong technical background, making it challenging for non-experts to
understand XAI methods. This paper presents "x-[plAIn]", a new approach to make
XAI more accessible to a wider audience through a custom Large Language Model
(LLM), developed using ChatGPT Builder. Our goal was to design a model that can
generate clear, concise summaries of various XAI methods, tailored for
different audiences, including business professionals and academics. The key
feature of our model is its ability to adapt explanations to match each
audience group's knowledge level and interests. Our approach still offers
timely insights, facilitating the decision-making process by the end users.
Results from our use-case studies show that our model is effective in providing
easy-to-understand, audience-specific explanations, regardless of the XAI
method used. This adaptability improves the accessibility of XAI, bridging the
gap between complex AI technologies and their practical applications. Our
findings indicate a promising direction for LLMs in making advanced AI concepts
more accessible to a diverse range of users.
\\ ( https://arxiv.org/abs/2401.13110 ,  1415kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13112
Date: Tue, 23 Jan 2024 21:48:52 GMT   (819kb,D)

Title: DISCOUNT: Distributional Counterfactual Explanation With Optimal
  Transport
Authors: Lei You, Lele Cao, Mattias Nilsson
Categories: cs.AI stat.ML
Comments: Under review in ICML 2024
\\
  Counterfactual Explanations (CE) is the de facto method for providing insight
and interpretability in black-box decision-making models by identifying
alternative input instances that lead to different outcomes. This paper extends
the concept of CEs to a distributional context, broadening the scope from
individual data points to entire input and output distributions, named
Distributional Counterfactual Explanation (DCE). In DCE, our focus shifts to
analyzing the distributional properties of the factual and counterfactual,
drawing parallels to the classical approach of assessing individual instances
and their resulting decisions. We leverage Optimal Transport (OT) to frame a
chance-constrained optimization problem, aiming to derive a counterfactual
distribution that closely aligns with its factual counterpart, substantiated by
statistical confidence. Our proposed optimization method, DISCOUNT,
strategically balances this confidence across both input and output
distributions. This algorithm is accompanied by an analysis of its convergence
rate. The efficacy of our proposed method is substantiated through a series of
illustrative case studies, highlighting its potential in providing deep
insights into decision-making models.
\\ ( https://arxiv.org/abs/2401.13112 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13192
Date: Wed, 24 Jan 2024 02:36:52 GMT   (1584kb)

Title: Generative Design of Crystal Structures by Point Cloud Representations
  and Diffusion Model
Authors: Zhelin Li, Rami Mrad, Runxian Jiao, Guan Huang, Jun Shan, Shibing Chu
  and Yuanping Chen
Categories: cs.AI cond-mat.mtrl-sci cs.LG physics.comp-ph
Comments: I am ready to submit to a journal, but I have not
\\
  Efficiently generating energetically stable crystal structures has long been
a challenge in material design, primarily due to the immense arrangement of
atoms in a crystal lattice. To facilitate the discovery of stable material, we
present a framework for the generation of synthesizable materials, leveraging a
point cloud representation to encode intricate structural information. At the
heart of this framework lies the introduction of a diffusion model as its
foundational pillar. To gauge the efficacy of our approach, we employ it to
reconstruct input structures from our training datasets, rigorously validating
its high reconstruction performance. Furthermore, we demonstrate the profound
potential of Point Cloud-Based Crystal Diffusion (PCCD) by generating entirely
new materials, emphasizing their synthesizability. Our research stands as a
noteworthy contribution to the advancement of materials design and synthesis
through the cutting-edge avenue of generative design instead of the
conventional substitution or experience-based discovery.
\\ ( https://arxiv.org/abs/2401.13192 ,  1584kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13408
Date: Wed, 24 Jan 2024 12:08:58 GMT   (653kb)

Title: Causal Perception
Authors: Jose M. Alvarez and Salvatore Ruggieri
Categories: cs.AI cs.CY cs.HC
Comments: arXiv admin note: text overlap with arXiv:2305.09535 by other authors
\\
  Perception occurs when two individuals interpret the same information
differently. Despite being a known phenomenon with implications for bias in
decision-making, as individuals' experience determines interpretation,
perception remains largely overlooked in automated decision-making (ADM)
systems. In particular, it can have considerable effects on the fairness or
fair usage of an ADM system, as fairness itself is context-specific and its
interpretation dependent on who is judging. In this work, we formalize
perception under causal reasoning to capture the act of interpretation by an
individual. We also formalize individual experience as additional causal
knowledge that comes with and is used by an individual. Further, we define and
discuss loaded attributes, which are attributes prone to evoke perception.
Sensitive attributes, such as gender and race, are clear examples of loaded
attributes. We define two kinds of causal perception, unfaithful and
inconsistent, based on the causal properties of faithfulness and consistency.
We illustrate our framework through a series of decision-making examples and
discuss relevant fairness applications. The goal of this work is to position
perception as a parameter of interest, useful for extending the standard,
single interpretation ADM problem formulation.
\\ ( https://arxiv.org/abs/2401.13408 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13604
Date: Wed, 24 Jan 2024 17:14:50 GMT   (1509kb,D)

Title: Stream-based perception for cognitive agents in mobile ecosystems
Authors: Jeremias D\"otterl, Ralf Bruns, J\"urgen Dunkel, Sascha Ossowski
Categories: cs.AI cs.MA
Journal-ref: AI Communications, vol. 32, no. 4, pp. 271-286, 2019
DOI: 10.3233/AIC-190614
\\
  Cognitive agent abstractions can help to engineer intelligent systems across
mobile devices. On smartphones, the data obtained from onboard sensors can give
valuable insights into the user's current situation. Unfortunately, today's
cognitive agent frameworks cannot cope well with the challenging
characteristics of sensor data. Sensor data is located on a low abstraction
level and the individual data elements are not meaningful when observed in
isolation. In contrast, cognitive agents operate on high-level percepts and
lack the means to effectively detect complex spatio-temporal patterns in
sequences of multiple percepts. In this paper, we present a stream-based
perception approach that enables the agents to perceive meaningful situations
in low-level sensor data streams. We present a crowdshipping case study where
autonomous, self-interested agents collaborate to deliver parcels to their
destinations. We show how situations derived from smartphone sensor data can
trigger and guide auctions, which the agents use to reach agreements.
Experiments with real smartphone data demonstrate the benefits of stream-based
agent perception.
\\ ( https://arxiv.org/abs/2401.13604 ,  1509kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12980
Date: Thu, 4 Jan 2024 23:05:39 GMT   (1327kb,D)

Title: Identifying Risk Patterns in Brazilian Police Reports Preceding
  Femicides: A Long Short Term Memory (LSTM) Based Analysis
Authors: Vinicius Lima, Jaque Almeida de Oliveira
Categories: cs.CL
Comments: IEEE Global Humanitarian Technology Conference (GHTC) 2023
\\
  Femicide refers to the killing of a female victim, often perpetrated by an
intimate partner or family member, and is also associated with gender-based
violence. Studies have shown that there is a pattern of escalating violence
leading up to these killings, highlighting the potential for prevention if the
level of danger to the victim can be assessed. Machine learning offers a
promising approach to address this challenge by predicting risk levels based on
textual descriptions of the violence. In this study, we employed the Long Short
Term Memory (LSTM) technique to identify patterns of behavior in Brazilian
police reports preceding femicides. Our first objective was to classify the
content of these reports as indicating either a lower or higher risk of the
victim being murdered, achieving an accuracy of 66%. In the second approach, we
developed a model to predict the next action a victim might experience within a
sequence of patterned events. Both approaches contribute to the understanding
and assessment of the risks associated with domestic violence, providing
authorities with valuable insights to protect women and prevent situations from
escalating.
\\ ( https://arxiv.org/abs/2401.12980 ,  1327kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12981
Date: Wed, 10 Jan 2024 03:44:15 GMT   (423kb)

Title: A General-purpose AI Avatar in Healthcare
Authors: Nicholas Yan, Gil Alterovitz
Categories: cs.CL
\\
  Recent advancements in machine learning and natural language processing have
led to the rapid development of artificial intelligence (AI) as a valuable tool
in the healthcare industry. Using large language models (LLMs) as
conversational agents or chatbots has the potential to assist doctors in
diagnosing patients, detecting early symptoms of diseases, and providing health
advice to patients. This paper focuses on the role of chatbots in healthcare
and explores the use of avatars to make AI interactions more appealing to
patients. A framework of a general-purpose AI avatar application is
demonstrated by using a three-category prompt dictionary and prompt improvement
mechanism. A two-phase approach is suggested to fine-tune a general-purpose AI
language model and create different AI avatars to discuss medical issues with
users. Prompt engineering enhances the chatbot's conversational abilities and
personality traits, fostering a more human-like interaction with patients.
Ultimately, the injection of personality into the chatbot could potentially
increase patient engagement. Future directions for research include
investigating ways to improve chatbots' understanding of context and ensuring
the accuracy of their outputs through fine-tuning with specialized medical data
sets.
\\ ( https://arxiv.org/abs/2401.12981 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12982
Date: Thu, 11 Jan 2024 08:17:42 GMT   (14312kb)

Title: Text Classification: A Review, Empirical, and Experimental Evaluation
Authors: Kamal Taha, Paul D. Yoo, Chan Yeun, Aya Taha
Categories: cs.CL
\\
  The explosive and widespread growth of data necessitates the use of text
classification to extract crucial information from vast amounts of data.
Consequently, there has been a surge of research in both classical and deep
learning text classification methods. Despite the numerous methods proposed in
the literature, there is still a pressing need for a comprehensive and
up-to-date survey. Existing survey papers categorize algorithms for text
classification into broad classes, which can lead to the misclassification of
unrelated algorithms and incorrect assessments of their qualities and behaviors
using the same metrics. To address these limitations, our paper introduces a
novel methodological taxonomy that classifies algorithms hierarchically into
fine-grained classes and specific techniques. The taxonomy includes methodology
categories, methodology techniques, and methodology sub-techniques. Our study
is the first survey to utilize this methodological taxonomy for classifying
algorithms for text classification. Furthermore, our study also conducts
empirical evaluation and experimental comparisons and rankings of different
algorithms that employ the same specific sub-technique, different
sub-techniques within the same technique, different techniques within the same
category, and categories
\\ ( https://arxiv.org/abs/2401.12982 ,  14312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12983
Date: Sat, 13 Jan 2024 19:19:04 GMT   (8766kb)

Title: Assessing Large Language Models in Mechanical Engineering Education: A
  Study on Mechanics-Focused Conceptual Understanding
Authors: Jie Tian, Jixin Hou, Zihao Wu, Peng Shu, Zhengliang Liu, Yujie Xiang,
  Beikang Gu, Nicholas Filla, Yiwei Li, Ning Liu, Xianyan Chen, Keke Tang,
  Tianming Liu, and Xianqiao Wang
Categories: cs.CL cs.AI physics.ed-ph
Comments: 30 pages, 7 figures, and 1 table
\\
  This study is a pioneering endeavor to investigate the capabilities of Large
Language Models (LLMs) in addressing conceptual questions within the domain of
mechanical engineering with a focus on mechanics. Our examination involves a
manually crafted exam encompassing 126 multiple-choice questions, spanning
various aspects of mechanics courses, including Fluid Mechanics, Mechanical
Vibration, Engineering Statics and Dynamics, Mechanics of Materials, Theory of
Elasticity, and Continuum Mechanics. Three LLMs, including ChatGPT (GPT-3.5),
ChatGPT (GPT-4), and Claude (Claude-2.1), were subjected to evaluation against
engineering faculties and students with or without mechanical engineering
background. The findings reveal GPT-4's superior performance over the other two
LLMs and human cohorts in answering questions across various mechanics topics,
except for Continuum Mechanics. This signals the potential future improvements
for GPT models in handling symbolic calculations and tensor analyses. The
performances of LLMs were all significantly improved with explanations prompted
prior to direct responses, underscoring the crucial role of prompt engineering.
Interestingly, GPT-3.5 demonstrates improved performance with prompts covering
a broader domain, while GPT-4 excels with prompts focusing on specific
subjects. Finally, GPT-4 exhibits notable advancements in mitigating input
bias, as evidenced by guessing preferences for humans. This study unveils the
substantial potential of LLMs as highly knowledgeable assistants in both
mechanical pedagogy and scientific research.
\\ ( https://arxiv.org/abs/2401.12983 ,  8766kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12985
Date: Mon, 15 Jan 2024 15:27:18 GMT   (1305kb,D)

Title: The Effect of Human v/s Synthetic Test Data and Round-tripping on
  Assessment of Sentiment Analysis Systems for Bias
Authors: Kausik Lakkaraju, Aniket Gupta, Biplav Srivastava, Marco Valtorta,
  Dezhi Wu
Categories: cs.CL
Comments: arXiv admin note: text overlap with arXiv:2302.02038
\\
  Sentiment Analysis Systems (SASs) are data-driven Artificial Intelligence
(AI) systems that output polarity and emotional intensity when given a piece of
text as input. Like other AIs, SASs are also known to have unstable behavior
when subjected to changes in data which can make it problematic to trust out of
concerns like bias when AI works with humans and data has protected attributes
like gender, race, and age. Recently, an approach was introduced to assess SASs
in a blackbox setting without training data or code, and rating them for bias
using synthetic English data. We augment it by introducing two human-generated
chatbot datasets and also consider a round-trip setting of translating the data
from one language to the same through an intermediate language. We find that
these settings show SASs performance in a more realistic light. Specifically,
we find that rating SASs on the chatbot data showed more bias compared to the
synthetic data, and round-tripping using Spanish and Danish as intermediate
languages reduces the bias (up to 68% reduction) in human-generated data while,
in synthetic data, it takes a surprising turn by increasing the bias! Our
findings will help researchers and practitioners refine their SAS testing
strategies and foster trust as SASs are considered part of more
mission-critical applications for global use.
\\ ( https://arxiv.org/abs/2401.12985 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12986
Date: Tue, 16 Jan 2024 04:05:25 GMT   (713kb,D)

Title: Crowdsourced Adaptive Surveys
Authors: Yamil Velez
Categories: cs.CL cs.AI cs.HC stat.AP
Comments: 25 pages, 5 figures
\\
  Public opinion surveys are vital for informing democratic decision-making,
but responding to rapidly changing information environments and measuring
beliefs within niche communities can be challenging for traditional survey
methods. This paper introduces a crowdsourced adaptive survey methodology
(CSAS) that unites advances in natural language processing and adaptive
algorithms to generate question banks that evolve with user input. The CSAS
method converts open-ended text provided by participants into Likert-style
items and applies a multi-armed bandit algorithm to determine user-provided
questions that should be prioritized in the survey. The method's adaptive
nature allows for the exploration of new survey questions, while imposing
minimal costs in survey length. Applications in the domains of Latino
information environments and issue importance showcase CSAS's ability to
identify claims or issues that might otherwise be difficult to track using
standard approaches. I conclude by discussing the method's potential for
studying topics where participant-generated content might improve our
understanding of public opinion.
\\ ( https://arxiv.org/abs/2401.12986 ,  713kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12987
Date: Tue, 16 Jan 2024 07:18:41 GMT   (2707kb,D)

Title: TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition
  in Conversation
Authors: Taeyang Yun, Hyunkuk Lim, Jeonghwan Lee, Min Song
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: 13 pages, 7 figures
\\
  Emotion Recognition in Conversation (ERC) plays a crucial role in enabling
dialogue systems to effectively respond to user requests. The emotions in a
conversation can be identified by the representations from various modalities,
such as audio, visual, and text. However, due to the weak contribution of
non-verbal modalities to recognize emotions, multimodal ERC has always been
considered a challenging task. In this paper, we propose Teacher-leading
Multimodal fusion network for ERC (TelME). TelME incorporates cross-modal
knowledge distillation to transfer information from a language model acting as
the teacher to the non-verbal students, thereby optimizing the efficacy of the
weak modalities. We then combine multimodal features using a shifting fusion
approach in which student networks support the teacher. TelME achieves
state-of-the-art performance in MELD, a multi-speaker conversation dataset for
ERC. Finally, we demonstrate the effectiveness of our components through
additional experiments.
\\ ( https://arxiv.org/abs/2401.12987 ,  2707kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12988
Date: Tue, 16 Jan 2024 13:54:43 GMT   (830kb)

Title: Few-Shot Learning for Chronic Disease Management: Leveraging Large
  Language Models and Multi-Prompt Engineering with Medical Knowledge Injection
Authors: Haoxin Liu, Wenli Zhang, Jiaheng Xie, Buomsoo Kim, Zhu Zhang, Yidong
  Chai
Categories: cs.CL cs.AI
MSC-class: K.5
ACM-class: I.2.7; H.4.m
\\
  This study harnesses state-of-the-art AI technology for chronic disease
management, specifically in detecting various mental disorders through
user-generated textual content. Existing studies typically rely on fully
supervised machine learning, which presents challenges such as the
labor-intensive manual process of annotating extensive training data for each
disease and the need to design specialized deep learning architectures for each
problem. To address such challenges, we propose a novel framework that
leverages advanced AI techniques, including large language models and
multi-prompt engineering. Specifically, we address two key technical challenges
in data-driven chronic disease management: (1) developing personalized prompts
to represent each user's uniqueness and (2) incorporating medical knowledge
into prompts to provide context for chronic disease detection, instruct
learning objectives, and operationalize prediction goals. We evaluate our
method using four mental disorders, which are prevalent chronic diseases
worldwide, as research cases. On the depression detection task, our method (F1
= 0.975~0.978) significantly outperforms traditional supervised learning
paradigms, including feature engineering (F1 = 0.760) and architecture
engineering (F1 = 0.756). Meanwhile, our approach demonstrates success in
few-shot learning, i.e., requiring only a minimal number of training examples
to detect chronic diseases based on user-generated textual content (i.e., only
2, 10, or 100 subjects). Moreover, our method can be generalized to other
mental disorder detection tasks, including anorexia, pathological gambling, and
self-harm (F1 = 0.919~0.978).
\\ ( https://arxiv.org/abs/2401.12988 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12989
Date: Tue, 16 Jan 2024 14:40:54 GMT   (2182kb,D)

Title: Into the crossfire: evaluating the use of a language model to
  crowdsource gun violence reports
Authors: Adriano Belisario, Scott Hale, Luc Rocher
Categories: cs.CL cs.IR
\\
  Gun violence is a pressing and growing human rights issue that affects nearly
every dimension of the social fabric, from healthcare and education to
psychology and the economy. Reliable data on firearm events is paramount to
developing more effective public policy and emergency responses. However, the
lack of comprehensive databases and the risks of in-person surveys prevent
human rights organizations from collecting needed data in most countries. Here,
we partner with a Brazilian human rights organization to conduct a systematic
evaluation of language models to assist with monitoring real-world firearm
events from social media data. We propose a fine-tuned BERT-based model trained
on Twitter (now X) texts to distinguish gun violence reports from ordinary
Portuguese texts. Our model achieves a high AUC score of 0.97. We then
incorporate our model into a web application and test it in a live
intervention. We study and interview Brazilian analysts who continuously
fact-check social media texts to identify new gun violence events. Qualitative
assessments show that our solution helped all analysts use their time more
efficiently and expanded their search capacities. Quantitative assessments show
that the use of our model was associated with more analysts' interactions with
online users reporting gun violence. Taken together, our findings suggest that
modern Natural Language Processing techniques can help support the work of
human rights organizations.
\\ ( https://arxiv.org/abs/2401.12989 ,  2182kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12990
Date: Tue, 16 Jan 2024 16:05:54 GMT   (7465kb,D)

Title: Topic Modelling: Going Beyond Token Outputs
Authors: Lowri Williams, Eirini Anthi, Laura Arman, Pete Burnap
Categories: cs.CL cs.LG
\\
  Topic modelling is a text mining technique for identifying salient themes
from a number of documents. The output is commonly a set of topics consisting
of isolated tokens that often co-occur in such documents. Manual effort is
often associated with interpreting a topic's description from such tokens.
However, from a human's perspective, such outputs may not adequately provide
enough information to infer the meaning of the topics; thus, their
interpretability is often inaccurately understood. Although several studies
have attempted to automatically extend topic descriptions as a means of
enhancing the interpretation of topic models, they rely on external language
sources that may become unavailable, must be kept up-to-date to generate
relevant results, and present privacy issues when training on or processing
data. This paper presents a novel approach towards extending the output of
traditional topic modelling methods beyond a list of isolated tokens. This
approach removes the dependence on external sources by using the textual data
itself by extracting high-scoring keywords and mapping them to the topic
model's token outputs. To measure the interpretability of the proposed outputs
against those of the traditional topic modelling approach, independent
annotators manually scored each output based on their quality and usefulness,
as well as the efficiency of the annotation task. The proposed approach
demonstrated higher quality and usefulness, as well as higher efficiency in the
annotation task, in comparison to the outputs of a traditional topic modelling
method, demonstrating an increase in their interpretability.
\\ ( https://arxiv.org/abs/2401.12990 ,  7465kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12992
Date: Wed, 17 Jan 2024 11:52:40 GMT   (297kb,D)

Title: TranSentence: Speech-to-speech Translation via Language-agnostic
  Sentence-level Speech Encoding without Language-parallel Data
Authors: Seung-Bin Kim, Sang-Hoon Lee, Seong-Whan Lee
Categories: cs.CL cs.SD eess.AS
Comments: Accepted by ICASSP 2024
\\
  Although there has been significant advancement in the field of
speech-to-speech translation, conventional models still require
language-parallel speech data between the source and target languages for
training. In this paper, we introduce TranSentence, a novel speech-to-speech
translation without language-parallel speech data. To achieve this, we first
adopt a language-agnostic sentence-level speech encoding that captures the
semantic information of speech, irrespective of language. We then train our
model to generate speech based on the encoded embedding obtained from a
language-agnostic sentence-level speech encoder that is pre-trained with
various languages. With this method, despite training exclusively on the target
language's monolingual data, we can generate target language speech in the
inference stage using language-agnostic speech embedding from the source
language speech. Furthermore, we extend TranSentence to multilingual
speech-to-speech translation. The experimental results demonstrate that
TranSentence is superior to other models.
\\ ( https://arxiv.org/abs/2401.12992 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12993
Date: Wed, 17 Jan 2024 14:33:13 GMT   (385kb)

Title: Estimating the severity of dental and oral problems via sentiment
  classification over clinical reports
Authors: Sare Mahdavifar, Seyed Mostafa Fakhrahmad, Elham Ansarifard
Categories: cs.CL
\\
  Analyzing authors' sentiments in texts as a technique for identifying text
polarity can be practical and useful in various fields, including medicine and
dentistry. Currently, due to factors such as patients' limited knowledge about
their condition, difficulties in accessing specialist doctors, or fear of
illness, particularly in pandemic conditions, there might be a delay between
receiving a radiology report and consulting a doctor. In some cases, this delay
can pose significant risks to the patient, making timely decision-making
crucial. Having an automatic system that can inform patients about the
deterioration of their condition by analyzing the text of radiology reports
could greatly impact timely decision-making. In this study, a dataset
comprising 1,134 cone-beam computed tomography (CBCT) photo reports was
collected from the Shiraz University of Medical Sciences. Each case was
examined, and an expert labeled a severity level for the patient's condition on
each document. After preprocessing all the text data, a deep learning model
based on Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)
network architecture, known as CNN-LSTM, was developed to detect the severity
level of the patient's problem based on sentiment analysis in the radiologist's
report. The model's performance was evaluated on two datasets, each with two
and four classes, in both imbalanced and balanced scenarios. Finally, to
demonstrate the effectiveness of our model, we compared its performance with
that of other classification models. The results, along with one-way ANOVA and
Tukey's test, indicated that our proposed model (CNN-LSTM) performed the best
according to precision, recall, and f-measure criteria. This suggests that it
can be a reliable model for estimating the severity of oral and dental
diseases, thereby assisting patients.
\\ ( https://arxiv.org/abs/2401.12993 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12994
Date: Thu, 18 Jan 2024 05:17:18 GMT   (783kb,D)

Title: Automated Scoring of Clinical Patient Notes using Advanced NLP and
  Pseudo Labeling
Authors: Jingyu Xu, Yifeng Jiang, Bin Yuan, Shulin Li, Tianbo Song
Categories: cs.CL
\\
  Clinical patient notes are critical for documenting patient interactions,
diagnoses, and treatment plans in medical practice. Ensuring accurate
evaluation of these notes is essential for medical education and certification.
However, manual evaluation is complex and time-consuming, often resulting in
variability and resource-intensive assessments. To tackle these challenges,
this research introduces an approach leveraging state-of-the-art Natural
Language Processing (NLP) techniques, specifically Masked Language Modeling
(MLM) pretraining, and pseudo labeling. Our methodology enhances efficiency and
effectiveness, significantly reducing training time without compromising
performance. Experimental results showcase improved model performance,
indicating a potential transformation in clinical note assessment.
\\ ( https://arxiv.org/abs/2401.12994 ,  783kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12995
Date: Thu, 18 Jan 2024 15:21:16 GMT   (252kb,D)

Title: Harmonizing Code-mixed Conversations: Personality-assisted Code-mixed
  Response Generation in Dialogues
Authors: Shivani Kumar, Tanmoy Chakraborty
Categories: cs.CL
Comments: 14 pages, 8 figures, 7 tables. Accepted at EACL (findings) 2024
\\
  Code-mixing, the blending of multiple languages within a single conversation,
introduces a distinctive challenge, particularly in the context of response
generation. Capturing the intricacies of code-mixing proves to be a formidable
task, given the wide-ranging variations influenced by individual speaking
styles and cultural backgrounds. In this study, we explore response generation
within code-mixed conversations. We introduce a novel approach centered on
harnessing the Big Five personality traits acquired in an unsupervised manner
from the conversations to bolster the performance of response generation. These
inferred personality attributes are seamlessly woven into the fabric of the
dialogue context, using a novel fusion mechanism, PA3. It uses an effective
two-step attention formulation to fuse the dialogue and personality
information. This fusion not only enhances the contextual relevance of
generated responses but also elevates the overall performance of the model. Our
experimental results, grounded in a dataset comprising of multi-party
Hindi-English code-mix conversations, highlight the substantial advantages
offered by personality-infused models over their conventional counterparts.
This is evident in the increase observed in ROUGE and BLUE scores for the
response generation task when the identified personality is seamlessly
integrated into the dialogue context. Qualitative assessment for personality
identification and response generation aligns well with our quantitative
results.
\\ ( https://arxiv.org/abs/2401.12995 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12996
Date: Thu, 18 Jan 2024 18:08:16 GMT   (1115kb)

Title: A Comparison of Veterans with Problematic Opioid Use Identified through
  Natural Language Processing of Clinical Notes versus Using Diagnostic Codes
Authors: Terri Elizabeth Workman, Joel Kupersmith, Phillip Ma, Christopher
  Spevak, Friedhelm Sandbrink, Yan Cheng Qing Zeng-Treitler
Categories: cs.CL cs.AI cs.LG
Comments: 17 pages, 4 figures, 8 tables
ACM-class: J.3
\\
  Background: Electronic health records (EHRs) are a data source for opioid
research. Opioid use disorder is known to be under-coded as a diagnosis, yet
problematic opioid use can be documented in clinical notes.
  Objectives: Our goals were 1) to identify problematic opioid use from a full
range of clinical notes; and 2) to compare the characteristics of patients
identified as having problematic opioid use, exclusively documented in clinical
notes, to those having documented ICD opioid use disorder diagnostic codes.
  Materials and Methods: We developed and applied a natural language processing
(NLP) tool to the clinical notes of a patient cohort (n=222,371) from two
Veteran Affairs service regions to identify patients with problematic opioid
use. We also used a set of ICD diagnostic codes to identify patients with
opioid use disorder from the same cohort. We compared the demographic and
clinical characteristics of patients identified only through NLP, to those of
patients identified through ICD codes.
  Results: NLP exclusively identified 57,331 patients; 6,997 patients had
positive ICD code identifications. Patients exclusively identified through NLP
were more likely to be women. Those identified through ICD codes were more
likely to be male, younger, have concurrent benzodiazepine prescriptions, more
comorbidities, more care encounters, and less likely to be married. Patients in
the NLP and ICD groups had substantially elevated comorbidity levels compared
to patients not documented as experiencing problematic opioid use.
  Conclusions: NLP is a feasible approach for identifying problematic opioid
use not otherwise recorded by ICD codes. Clinicians may be reluctant to code
for opioid use disorder. It is therefore incumbent on the healthcare team to
search for documentation of opioid concerns within clinical notes.
\\ ( https://arxiv.org/abs/2401.12996 ,  1115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12997
Date: Fri, 19 Jan 2024 07:34:36 GMT   (61kb,D)

Title: Progressive Distillation Based on Masked Generation Feature Method for
  Knowledge Graph Completion
Authors: Cunhang Fan, Yujie Chen, Jun Xue, Yonghui Kong, Jianhua Tao, Zhao Lv
Categories: cs.CL
Comments: 7 pages, 3 figures
\\
  In recent years, knowledge graph completion (KGC) models based on pre-trained
language model (PLM) have shown promising results. However, the large number of
parameters and high computational cost of PLM models pose challenges for their
application in downstream tasks. This paper proposes a progressive distillation
method based on masked generation features for KGC task, aiming to
significantly reduce the complexity of pre-trained models. Specifically, we
perform pre-distillation on PLM to obtain high-quality teacher models, and
compress the PLM network to obtain multi-grade student models. However,
traditional feature distillation suffers from the limitation of having a single
representation of information in teacher models. To solve this problem, we
propose masked generation of teacher-student features, which contain richer
representation information. Furthermore, there is a significant gap in
representation ability between teacher and student. Therefore, we design a
progressive distillation method to distill student models at each grade level,
enabling efficient knowledge transfer from teachers to students. The
experimental results demonstrate that the model in the pre-distillation stage
surpasses the existing state-of-the-art methods. Furthermore, in the
progressive distillation stage, the model significantly reduces the model
parameters while maintaining a certain level of performance. Specifically, the
model parameters of the lower-grade student model are reduced by 56.7\%
compared to the baseline.
\\ ( https://arxiv.org/abs/2401.12997 ,  61kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12998
Date: Sat, 20 Jan 2024 03:41:23 GMT   (1184kb)

Title: Evaluating and Enhancing Large Language Models Performance in
  Domain-specific Medicine: Osteoarthritis Management with DocOA
Authors: Xi Chen, MingKe You, Li Wang, WeiZhi Liu, Yu Fu, Jie Xu, Shaoting
  Zhang, Gang Chen, Jian Li
Categories: cs.CL cs.AI
Comments: 16 Pages, 7 Figures
\\
  The efficacy of large language models (LLMs) in domain-specific medicine,
particularly for managing complex diseases such as osteoarthritis (OA), remains
largely unexplored. This study focused on evaluating and enhancing the clinical
capabilities of LLMs in specific domains, using osteoarthritis (OA) management
as a case study. A domain specific benchmark framework was developed, which
evaluate LLMs across a spectrum from domain-specific knowledge to clinical
applications in real-world clinical scenarios. DocOA, a specialized LLM
tailored for OA management that integrates retrieval-augmented generation (RAG)
and instruction prompts, was developed. The study compared the performance of
GPT-3.5, GPT-4, and a specialized assistant, DocOA, using objective and human
evaluations. Results showed that general LLMs like GPT-3.5 and GPT-4 were less
effective in the specialized domain of OA management, particularly in providing
personalized treatment recommendations. However, DocOA showed significant
improvements. This study introduces a novel benchmark framework which assesses
the domain-specific abilities of LLMs in multiple aspects, highlights the
limitations of generalized LLMs in clinical contexts, and demonstrates the
potential of tailored approaches for developing domain-specific medical LLMs.
\\ ( https://arxiv.org/abs/2401.12998 ,  1184kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13060
Date: Tue, 23 Jan 2024 19:32:54 GMT   (1684kb,D)

Title: TCE at Qur'an QA 2023 Shared Task: Low Resource Enhanced
  Transformer-based Ensemble Approach for Qur'anic QA
Authors: Mohammed Alaa Elkomy, Amany Sarhan
Categories: cs.CL cs.AI
\\
  In this paper, we present our approach to tackle Qur'an QA 2023 shared tasks
A and B. To address the challenge of low-resourced training data, we rely on
transfer learning together with a voting ensemble to improve prediction
stability across multiple runs. Additionally, we employ different architectures
and learning mechanisms for a range of Arabic pre-trained transformer-based
models for both tasks. To identify unanswerable questions, we propose using a
thresholding mechanism. Our top-performing systems greatly surpass the baseline
performance on the hidden split, achieving a MAP score of 25.05% for task A and
a partial Average Precision (pAP) of 57.11% for task B.
\\ ( https://arxiv.org/abs/2401.13060 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13085
Date: Tue, 23 Jan 2024 20:54:40 GMT   (6715kb,D)

Title: IndiText Boost: Text Augmentation for Low Resource India Languages
Authors: Onkar Litake, Niraj Yagnik and Shreyas Labhsetwar
Categories: cs.CL cs.AI cs.LG
\\
  Text Augmentation is an important task for low-resource languages. It helps
deal with the problem of data scarcity. A data augmentation strategy is used to
deal with the problem of data scarcity. Through the years, much work has been
done on data augmentation for the English language. In contrast, very less work
has been done on Indian languages. This is contrary to the fact that data
augmentation is used to deal with data scarcity. In this work, we focus on
implementing techniques like Easy Data Augmentation, Back Translation,
Paraphrasing, Text Generation using LLMs, and Text Expansion using LLMs for
text classification on different languages. We focus on 6 Indian languages
namely: Sindhi, Marathi, Hindi, Gujarati, Telugu, and Sanskrit. According to
our knowledge, no such work exists for text augmentation on Indian languages.
We carry out binary as well as multi-class text classification to make our
results more comparable. We get surprising results as basic data augmentation
techniques surpass LLMs.
\\ ( https://arxiv.org/abs/2401.13085 ,  6715kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13086
Date: Tue, 23 Jan 2024 20:55:49 GMT   (1040kb)

Title: Towards Trustable Language Models: Investigating Information Quality of
  Large Language Models
Authors: Rick Rejeleene, Xiaowei Xu, John Talburt
Categories: cs.CL cs.AI cs.LG
Comments: 31 pages
\\
  Large language models (LLM) are generating information at a rapid pace,
requiring users to increasingly rely and trust the data. Despite remarkable
advances of LLM, Information generated by LLM is not completely trustworthy,
due to challenges in information quality. Specifically, integrity of
Information quality decreases due to unreliable, biased, tokenization during
pre-training of LLM. Moreover, due to decreased information quality issues, has
led towards hallucination, fabricated information. Unreliable information can
lead towards flawed decisions in businesses, which impacts economic activity.
In this work, we introduce novel mathematical information quality evaluation of
LLM, we furthermore analyze and highlight information quality challenges,
scaling laws to systematically scale language models.
\\ ( https://arxiv.org/abs/2401.13086 ,  1040kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13129
Date: Tue, 23 Jan 2024 22:36:03 GMT   (441kb,D)

Title: Seed-Guided Fine-Grained Entity Typing in Science and Engineering
  Domains
Authors: Yu Zhang, Yunyi Zhang, Yanzhen Shen, Yu Deng, Lucian Popa, Larisa
  Shwartz, ChengXiang Zhai, Jiawei Han
Categories: cs.CL cs.SE
Comments: 9 pages; Accepted to AAAI 2024 (Code:
  https://github.com/yuzhimanhua/SEType)
\\
  Accurately typing entity mentions from text segments is a fundamental task
for various natural language processing applications. Many previous approaches
rely on massive human-annotated data to perform entity typing. Nevertheless,
collecting such data in highly specialized science and engineering domains
(e.g., software engineering and security) can be time-consuming and costly,
without mentioning the domain gaps between training and inference data if the
model needs to be applied to confidential datasets. In this paper, we study the
task of seed-guided fine-grained entity typing in science and engineering
domains, which takes the name and a few seed entities for each entity type as
the only supervision and aims to classify new entity mentions into both seen
and unseen types (i.e., those without seed entities). To solve this problem, we
propose SEType which first enriches the weak supervision by finding more
entities for each seen type from an unlabeled corpus using the contextualized
representations of pre-trained language models. It then matches the enriched
entities to unlabeled text to get pseudo-labeled samples and trains a textual
entailment model that can make inferences for both seen and unseen types.
Extensive experiments on two datasets covering four domains demonstrate the
effectiveness of SEType in comparison with various baselines.
\\ ( https://arxiv.org/abs/2401.13129 ,  441kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13133
Date: Tue, 23 Jan 2024 22:49:19 GMT   (1973kb,D)

Title: Analyzing COVID-19 Vaccination Sentiments in Nigerian Cyberspace:
  Insights from a Manually Annotated Twitter Dataset
Authors: Ibrahim Said Ahmad, Lukman Jibril Aliyu, Abubakar Auwal Khalid, Saminu
  Muhammad Aliyu, Shamsuddeen Hassan Muhammad, Idris Abdulmumin, Bala Mairiga
  Abduljalil, Bello Shehu Bello, Amina Imam Abubakar
Categories: cs.CL cs.SI
\\
  Numerous successes have been achieved in combating the COVID-19 pandemic,
initially using various precautionary measures like lockdowns, social
distancing, and the use of face masks. More recently, various vaccinations have
been developed to aid in the prevention or reduction of the severity of the
COVID-19 infection. Despite the effectiveness of the precautionary measures and
the vaccines, there are several controversies that are massively shared on
social media platforms like Twitter. In this paper, we explore the use of
state-of-the-art transformer-based language models to study people's acceptance
of vaccines in Nigeria. We developed a novel dataset by crawling multi-lingual
tweets using relevant hashtags and keywords. Our analysis and visualizations
revealed that most tweets expressed neutral sentiments about COVID-19 vaccines,
with some individuals expressing positive views, and there was no strong
preference for specific vaccine types, although Moderna received slightly more
positive sentiment. We also found out that fine-tuning a pre-trained LLM with
an appropriate dataset can yield competitive results, even if the LLM was not
initially pre-trained on the specific language of that dataset.
\\ ( https://arxiv.org/abs/2401.13133 ,  1973kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13136
Date: Tue, 23 Jan 2024 23:12:09 GMT   (658kb,D)

Title: The Language Barrier: Dissecting Safety Challenges of LLMs in
  Multilingual Contexts
Authors: Lingfeng Shen, Weiting Tan, Sihao Chen, Yunmo Chen, Jingyu Zhang,
  Haoran Xu, Boyuan Zheng, Philipp Koehn, Daniel Khashabi
Categories: cs.CL cs.AI
\\
  As the influence of large language models (LLMs) spans across global
communities, their safety challenges in multilingual settings become paramount
for alignment research. This paper examines the variations in safety challenges
faced by LLMs across different languages and discusses approaches to
alleviating such concerns. By comparing how state-of-the-art LLMs respond to
the same set of malicious prompts written in higher- vs. lower-resource
languages, we observe that (1) LLMs tend to generate unsafe responses much more
often when a malicious prompt is written in a lower-resource language, and (2)
LLMs tend to generate more irrelevant responses to malicious prompts in
lower-resource languages. To understand where the discrepancy can be
attributed, we study the effect of instruction tuning with reinforcement
learning from human feedback (RLHF) or supervised finetuning (SFT) on the
HH-RLHF dataset. Surprisingly, while training with high-resource languages
improves model alignment, training in lower-resource languages yields minimal
improvement. This suggests that the bottleneck of cross-lingual alignment is
rooted in the pretraining stage. Our findings highlight the challenges in
cross-lingual LLM safety, and we hope they inform future research in this
direction.
\\ ( https://arxiv.org/abs/2401.13136 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13165
Date: Wed, 24 Jan 2024 00:58:30 GMT   (271kb)

Title: Misgendering and Assuming Gender in Machine Translation when Working
  with Low-Resource Languages
Authors: Sourojit Ghosh, Srishti Chatterjee
Categories: cs.CL
Comments: Upcoming Publication, Gendered Technology in Translation and
  Interpreting Centering Rights in the Development of Language Technology
\\
  This chapter focuses on gender-related errors in machine translation (MT) in
the context of low-resource languages. We begin by explaining what low-resource
languages are, examining the inseparable social and computational factors that
create such linguistic hierarchies. We demonstrate through a case study of our
mother tongue Bengali, a global language spoken by almost 300 million people
but still classified as low-resource, how gender is assumed and inferred in
translations to and from the high(est)-resource English when no such
information is provided in source texts. We discuss the postcolonial and
societal impacts of such errors leading to linguistic erasure and
representational harms, and conclude by discussing potential solutions towards
uplifting languages by providing them more agency in MT conversations.
\\ ( https://arxiv.org/abs/2401.13165 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13170
Date: Wed, 24 Jan 2024 01:30:25 GMT   (100kb,D)

Title: CFMatch: Aligning Automated Answer Equivalence Evaluation with Expert
  Judgments For Open-Domain Question Answering
Authors: Zongxia Li, Ishani Mondal, Yijun Liang, Huy Nghiem, and Jordan
  Boyd-Graber
Categories: cs.CL
Comments: 18 pages, two figures, 6 tables. QA evaluation python package
  available in https://github.com/zli12321/qa_metrics.git
\\
  Question answering (QA) can only make progress if we know if an answer is
correct, but for many of the most challenging and interesting QA examples,
current evaluation metrics to determine answer equivalence (AE) often do not
align with human judgments, particularly more verbose, free-form answers from
large language models (LLM). There are two challenges: a lack of data and that
models are too big: LLM-based scorers can correlate better with human judges,
but this task has only been tested on limited QA datasets, and even when
available, update of the model is limited because LLMs are large and often
expensive. We rectify both of these issues by providing clear and consistent
guidelines for evaluating AE in machine QA adopted from professional human QA
contests. We also introduce a combination of standard evaluation and a more
efficient, robust, and lightweight discriminate AE classifier-based matching
method (CFMatch, smaller than 1 MB), trained and validated to more accurately
evaluate answer correctness in accordance with adopted expert AE rules that are
more aligned with human judgments.
\\ ( https://arxiv.org/abs/2401.13170 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13178
Date: Wed, 24 Jan 2024 01:51:00 GMT   (2581kb,D)

Title: AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents
Authors: Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui
  Jin, Zhenzhong Lan, Lingpeng Kong, Junxian He
Categories: cs.CL cs.AI cs.LG
Comments: Preprint
\\
  Evaluating large language models (LLMs) as general-purpose agents is
essential for understanding their capabilities and facilitating their
integration into practical applications. However, the evaluation process
presents substantial challenges. A primary obstacle is the benchmarking of
agent performance across diverse scenarios within a unified framework,
especially in maintaining partially-observable environments and ensuring
multi-round interactions. Moreover, current evaluation frameworks mostly focus
on the final success rate, revealing few insights during the process and
failing to provide a deep understanding of the model abilities. To address
these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark
and accompanied open-source evaluation framework tailored to analytical
evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric
that captures incremental advancements as well as a comprehensive evaluation
toolkit that features easy assessment of agents for multi-faceted analysis
through interactive visualization. This not only sheds light on the
capabilities and limitations of LLM agents but also propels the
interpretability of their performance to the forefront. Ultimately, AgentBoard
serves as a significant step towards demystifying agent behaviors and
accelerating the development of stronger LLM agents.
\\ ( https://arxiv.org/abs/2401.13178 ,  2581kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13218
Date: Wed, 24 Jan 2024 04:13:28 GMT   (7738kb,D)

Title: ULTRA: Unleash LLMs' Potential for Event Argument Extraction through
  Hierarchical Modeling and Pair-wise Refinement
Authors: Xinliang Frederick Zhang, Carter Blum, Temma Choji, Shalin Shah,
  Alakananda Vempala
Categories: cs.CL
\\
  Structural extraction of events within discourse is critical since it avails
a deeper understanding of communication patterns and behavior trends. Event
argument extraction (EAE), at the core of event-centric understanding, is the
task of identifying role-specific text spans (i.e., arguments) for a given
event. Document-level EAE (DocEAE) focuses on arguments that are scattered
across an entire document. In this work, we explore the capabilities of open
source Large Language Models (LLMs), i.e., Flan-UL2, for the DocEAE task. To
this end, we propose ULTRA, a hierarchical framework that extracts event
arguments more cost-effectively -- the method needs as few as 50 annotations
and doesn't require hitting costly API endpoints. Further, it alleviates the
positional bias issue intrinsic to LLMs. ULTRA first sequentially reads text
chunks of a document to generate a candidate argument set, upon which ULTRA
learns to drop non-pertinent candidates through self-refinement. We further
introduce LEAFER to address the challenge LLMs face in locating the exact
boundary of an argument span. ULTRA outperforms strong baselines, which include
strong supervised models and ChatGPT, by 9.8% when evaluated by the exact match
(EM) metric.
\\ ( https://arxiv.org/abs/2401.13218 ,  7738kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13223
Date: Wed, 24 Jan 2024 04:28:50 GMT   (886kb,D)

Title: TAT-LLM: A Specialized Language Model for Discrete Reasoning over
  Tabular and Textual Data
Authors: Fengbin Zhu, Ziyang Liu, Fuli Feng, Chao Wang, Moxin Li, Tat-Seng Chua
Categories: cs.CL cs.AI
\\
  In this work, we address question answering (QA) over a hybrid of tabular and
textual data that are very common content on the Web (e.g. SEC filings), where
discrete reasoning capabilities are often required. Recently, large language
models (LLMs) like GPT-4 have demonstrated strong multi-step reasoning
capabilities. We then consider harnessing the amazing power of LLMs to solve
our task. We abstract a Step-wise Pipeline for tabular and textual QA, which
consists of three key steps, including Extractor, Reasoner and Executor, and
initially design an instruction to instantiate the pipeline and validate that
GPT-4 outperforms all existing methods. However, utilizing an online LLM like
GPT-4 holds various challenges in terms of cost, latency, and data security
risk, which motivates us to specialize smaller LLMs in this task. We develop a
TAT-LLM language model by fine-tuning LLaMA 2 with the training data generated
automatically from existing expert-annotated datasets following the Step-wise
Pipeline. The experimental results have verified that our TAT-LLM model can
outperform all baseline models, including the previous best fine-tuned models
and very large-scale LLMs like GPT-4 on FinQA, TAT-QA and TAT-DQA benchmarks.
We hope our work can serve as a pioneering example of specializing smaller
language models for specific tasks.
\\ ( https://arxiv.org/abs/2401.13223 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13227
Date: Wed, 24 Jan 2024 04:50:16 GMT   (7901kb,D)

Title: Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large
  Language Models
Authors: Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei and Xueqi Chen
Categories: cs.CL cs.AI cs.LG cs.SI
\\
  Exploring the application of large-scale language models to graph learning is
a novel endeavor. However, the vast amount of information inherent in large
graphs poses significant challenges to this process. This paper focuses on the
link prediction task and introduces LPNL (Link Prediction via Natural
Language), a framework based on a large language model designed for scalable
link prediction on large-scale heterogeneous graphs.We design novel prompts for
link prediction that articulate graph details in natural language. We propose a
two-stage sampling pipeline to extract crucial information from large-scale
heterogeneous graphs, and a divide-and-conquer strategy to control the input
token count within predefined limits, addressing the challenge of overwhelming
information. We fine-tune a T5 model based on our self-supervised learning
designed for for link prediction. Extensive experiments on a large public
heterogeneous graphs demonstrate that LPNL outperforms various advanced
baselines, highlighting its remarkable performance in link prediction tasks on
large-scale graphs.
\\ ( https://arxiv.org/abs/2401.13227 ,  7901kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13229
Date: Wed, 24 Jan 2024 04:57:32 GMT   (7956kb,D)

Title: From Random to Informed Data Selection: A Diversity-Based Approach to
  Optimize Human Annotation and Few-Shot Learning
Authors: Alexandre Alcoforado, Thomas Palmeira Ferraz, Lucas Hideki Okamura,
  Israel Campos Fama, Arnold Moya Lavado, B\'arbara Dias Bueno, Bruno Veloso,
  Anna Helena Reali Costa
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at PROPOR 2024 - The 16th International Conference on
  Computational Processing of Portuguese
\\
  A major challenge in Natural Language Processing is obtaining annotated data
for supervised learning. An option is the use of crowdsourcing platforms for
data annotation. However, crowdsourcing introduces issues related to the
annotator's experience, consistency, and biases. An alternative is to use
zero-shot methods, which in turn have limitations compared to their few-shot or
fully supervised counterparts. Recent advancements driven by large language
models show potential, but struggle to adapt to specialized domains with
severely limited data. The most common approaches therefore involve the human
itself randomly annotating a set of datapoints to build initial datasets. But
randomly sampling data to be annotated is often inefficient as it ignores the
characteristics of the data and the specific needs of the model. The situation
worsens when working with imbalanced datasets, as random sampling tends to
heavily bias towards the majority classes, leading to excessive annotated data.
To address these issues, this paper contributes an automatic and informed data
selection architecture to build a small dataset for few-shot learning. Our
proposal minimizes the quantity and maximizes diversity of data selected for
human annotation, while improving model performance.
\\ ( https://arxiv.org/abs/2401.13229 ,  7956kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13246
Date: Wed, 24 Jan 2024 06:10:51 GMT   (7880kb,D)

Title: SEER: Facilitating Structured Reasoning and Explanation via
  Reinforcement Learning
Authors: Guoxin Chen and Kexin Tang and Chao Yang and Fuying Ye and Yu Qiao and
  Yiming Qian
Categories: cs.CL
Comments: Ongoing Work
\\
  Elucidating the reasoning process with structured explanations from question
to answer is fundamentally crucial, as it significantly enhances the
interpretability and trustworthiness of question-answering (QA) systems.
However, structured explanations demand models to perform intricate structured
reasoning, which poses great challenges. Most existing methods focus on
single-step reasoning through supervised learning, ignoring logical
dependencies between steps. Meanwhile, existing reinforcement learning
(RL)-based methods overlook the structured relationships, impeding RL's
potential in structured reasoning. In this paper, we propose SEER, a novel
method that maximizes a structure-based return to facilitate structured
reasoning and explanation. Our proposed structure-based return precisely
describes the hierarchical and branching structure inherent in structured
reasoning, effectively capturing the intricate relationships between states. We
also introduce a fine-grained reward function to meticulously delineate diverse
reasoning steps. Extensive experiments show that SEER significantly outperforms
state-of-the-art methods, achieving an absolute improvement of 6.9% over
RL-based methods on EntailmentBank, a 4.4% average improvement on STREET
benchmark, and exhibiting outstanding efficiency and cross-dataset
generalization performance.
\\ ( https://arxiv.org/abs/2401.13246 ,  7880kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13256
Date: Wed, 24 Jan 2024 06:50:20 GMT   (1159kb,D)

Title: UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for
  Personalized Dialogue Systems
Authors: Hongru Wang, Wenyu Huang, Yang Deng, Rui Wang, Zezhong Wang, Yufei
  Wang, Fei Mi, Jeff Z. Pan, Kam-Fai Wong
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) has shown exceptional capabilities in many
natual language understanding and generation tasks. However, the
personalization issue still remains a much-coveted property, especially when it
comes to the multiple sources involved in the dialogue system. To better plan
and incorporate the use of multiple sources in generating personalized
response, we firstly decompose it into three sub-tasks: Knowledge Source
Selection, Knowledge Retrieval, and Response Generation. We then propose a
novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG)
Specifically, we unify these three sub-tasks with different formulations into
the same sequence-to-sequence paradigm during the training, to adaptively
retrieve evidences and evaluate the relevance on-demand using special tokens,
called acting tokens and evaluation tokens. Enabling language models to
generate acting tokens facilitates interaction with various knowledge sources,
allowing them to adapt their behavior to diverse task requirements. Meanwhile,
evaluation tokens gauge the relevance score between the dialogue context and
the retrieved evidence. In addition, we carefully design a self-refinement
mechanism to iteratively refine the generated response considering 1) the
consistency scores between the generated response and retrieved evidence; and
2) the relevance scores. Experiments on two personalized datasets (DuLeMon and
KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge
source selection and response generation task with itself as a retriever in a
unified manner. Extensive analyses and discussions are provided for shedding
some new perspectives for personalized dialogue systems.
\\ ( https://arxiv.org/abs/2401.13256 ,  1159kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13260
Date: Wed, 24 Jan 2024 06:55:55 GMT   (791kb,D)

Title: MF-AED-AEC: Speech Emotion Recognition by Leveraging Multimodal Fusion,
  ASR Error Detection, and ASR Error Correction
Authors: Jiajun He, Xiaohan Shi, Xingfeng Li, Tomoki Toda
Categories: cs.CL cs.MM cs.SD eess.AS
Comments: Accepted by ICASSP 2024
\\
  The prevalent approach in speech emotion recognition (SER) involves
integrating both audio and textual information to comprehensively identify the
speaker's emotion, with the text generally obtained through automatic speech
recognition (ASR). An essential issue of this approach is that ASR errors from
the text modality can worsen the performance of SER. Previous studies have
proposed using an auxiliary ASR error detection task to adaptively assign
weights of each word in ASR hypotheses. However, this approach has limited
improvement potential because it does not address the coherence of semantic
information in the text. Additionally, the inherent heterogeneity of different
modalities leads to distribution gaps between their representations, making
their fusion challenging. Therefore, in this paper, we incorporate two
auxiliary tasks, ASR error detection (AED) and ASR error correction (AEC), to
enhance the semantic coherence of ASR text, and further introduce a novel
multi-modal fusion (MF) method to learn shared representations across
modalities. We refer to our method as MF-AED-AEC. Experimental results indicate
that MF-AED-AEC significantly outperforms the baseline model by a margin of
4.1\%.
\\ ( https://arxiv.org/abs/2401.13260 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13275
Date: Wed, 24 Jan 2024 07:34:55 GMT   (363kb,D)

Title: Can AI Assistants Know What They Don't Know?
Authors: Qinyuan Cheng and Tianxiang Sun and Xiangyang Liu and Wenwei Zhang and
  Zhangyue Yin and Shimin Li and Linyang Li and Kai Chen and Xipeng Qiu
Categories: cs.CL cs.AI
Comments: Work in progress
\\
  Recently, AI assistants based on large language models (LLMs) show surprising
performance in many tasks, such as dialogue, solving math problems, writing
code, and using tools. Although LLMs possess intensive world knowledge, they
still make factual errors when facing some knowledge intensive tasks, like
open-domain question answering. These untruthful responses from the AI
assistant may cause significant risks in practical applications. We believe
that an AI assistant's refusal to answer questions it does not know is a
crucial method for reducing hallucinations and making the assistant truthful.
Therefore, in this paper, we ask the question "Can AI assistants know what they
don't know and express them through natural language?" To answer this question,
we construct a model-specific "I don't know" (Idk) dataset for an assistant,
which contains its known and unknown questions, based on existing open-domain
question answering datasets. Then we align the assistant with its corresponding
Idk dataset and observe whether it can refuse to answer its unknown questions
after alignment. Experimental results show that after alignment with Idk
datasets, the assistant can refuse to answer most its unknown questions. For
questions they attempt to answer, the accuracy is significantly higher than
before the alignment.
\\ ( https://arxiv.org/abs/2401.13275 ,  363kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13298
Date: Wed, 24 Jan 2024 08:37:16 GMT   (10226kb,D)

Title: Towards Explainable Harmful Meme Detection through Multimodal Debate
  between Large Language Models
Authors: Hongzhan Lin, Ziyang Luo, Wei Gao, Jing Ma, Bo Wang, Ruichao Yang
Categories: cs.CL cs.AI
Comments: The first work towards explainable harmful meme detection by
  harnessing advanced LLMs
Journal-ref: The ACM Web Conference 2024
\\
  The age of social media is flooded with Internet memes, necessitating a clear
grasp and effective identification of harmful ones. This task presents a
significant challenge due to the implicit meaning embedded in memes, which is
not explicitly conveyed through the surface text and image. However, existing
harmful meme detection methods do not present readable explanations that unveil
such implicit meaning to support their detection decisions. In this paper, we
propose an explainable approach to detect harmful memes, achieved through
reasoning over conflicting rationales from both harmless and harmful positions.
Specifically, inspired by the powerful capacity of Large Language Models (LLMs)
on text generation and reasoning, we first elicit multimodal debate between
LLMs to generate the explanations derived from the contradictory arguments.
Then we propose to fine-tune a small language model as the debate judge for
harmfulness inference, to facilitate multimodal fusion between the harmfulness
rationales and the intrinsic multimodal information within memes. In this way,
our model is empowered to perform dialectical reasoning over intricate and
implicit harm-indicative patterns, utilizing multimodal explanations
originating from both harmless and harmful arguments. Extensive experiments on
three public meme datasets demonstrate that our harmful meme detection approach
achieves much better performance than state-of-the-art methods and exhibits a
superior capacity for explaining the meme harmfulness of the model predictions.
\\ ( https://arxiv.org/abs/2401.13298 ,  10226kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13303
Date: Wed, 24 Jan 2024 08:57:39 GMT   (176kb,D)

Title: MaLA-500: Massive Language Adaptation of Large Language Models
Authors: Peiqin Lin, Shaoxiong Ji, J\"org Tiedemann, Andr\'e F. T. Martins,
  Hinrich Sch\"utze
Categories: cs.CL
\\
  Large language models have advanced the state of the art in natural language
processing. However, their predominant design for English or a limited set of
languages creates a substantial gap in their effectiveness for low-resource
languages. To bridge this gap, we introduce MaLA-500, a novel large language
model designed to cover an extensive range of 534 languages. To train MaLA-500,
we employ vocabulary extension and continued pretraining on LLaMA 2 with
Glot500-c. Our experiments on SIB-200 show that MaLA-500 achieves
state-of-the-art in-context learning results. We release MaLA-500 at
https://huggingface.co/MaLA-LM
\\ ( https://arxiv.org/abs/2401.13303 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13398
Date: Wed, 24 Jan 2024 11:52:05 GMT   (187kb)

Title: Text Categorization Can Enhance Domain-Agnostic Stopword Extraction
Authors: Houcemeddine Turki, Naome A. Etori, Mohamed Ali Hadj Taieb,
  Abdul-Hakeem Omotayo, Chris Chinenye Emezue, Mohamed Ben Aouicha, Ayodele
  Awokoya, Falalu Ibrahim Lawan, Doreen Nixdorf
Categories: cs.CL cs.LG
Comments: A Project Report for the Masakhane Research Community
\\
  This paper investigates the role of text categorization in streamlining
stopword extraction in natural language processing (NLP), specifically focusing
on nine African languages alongside French. By leveraging the MasakhaNEWS,
African Stopwords Project, and MasakhaPOS datasets, our findings emphasize that
text categorization effectively identifies domain-agnostic stopwords with over
80% detection success rate for most examined languages. Nevertheless,
linguistic variances result in lower detection rates for certain languages.
Interestingly, we find that while over 40% of stopwords are common across news
categories, less than 15% are unique to a single category. Uncommon stopwords
add depth to text but their classification as stopwords depends on context.
Therefore combining statistical and linguistic approaches creates comprehensive
stopword lists, highlighting the value of our hybrid method. This research
enhances NLP for African languages and underscores the importance of text
categorization in stopword extraction.
\\ ( https://arxiv.org/abs/2401.13398 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13444
Date: Wed, 24 Jan 2024 13:36:50 GMT   (167kb,D)

Title: Clue-Guided Path Exploration: An Efficient Knowledge Base
  Question-Answering Framework with Low Computational Resource Consumption
Authors: Dehao Tao, Feng Huang, Yongfeng Huang and Minghu Jiang
Categories: cs.CL cs.AI
\\
  In recent times, large language models (LLMs) have showcased remarkable
capabilities. However, updating their knowledge poses challenges, potentially
leading to inaccuracies when confronted with unfamiliar queries. While
integrating knowledge graphs with LLMs has been explored, existing approaches
treat LLMs as primary decision-makers, imposing high demands on their
capabilities. This is particularly unsuitable for LLMs with lower computational
costs and relatively poorer performance. In this paper, we introduce a
Clue-Guided Path Exploration framework (CGPE) that efficiently merges a
knowledge base with an LLM, placing less stringent requirements on the model's
capabilities. Inspired by the method humans use to manually retrieve knowledge,
CGPE employs information from the question as clues to systematically explore
the required knowledge path within the knowledge base. Experiments on
open-source datasets reveal that CGPE outperforms previous methods and is
highly applicable to LLMs with fewer parameters. In some instances, even
ChatGLM3, with its 6 billion parameters, can rival the performance of GPT-4.
Furthermore, the results indicate a minimal invocation frequency of CGPE on
LLMs, suggesting reduced computational overhead. For organizations and
individuals facing constraints in computational resources, our research offers
significant practical value.
\\ ( https://arxiv.org/abs/2401.13444 ,  167kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13463
Date: Wed, 24 Jan 2024 14:08:38 GMT   (763kb,D)

Title: SpeechDPR: End-to-End Spoken Passage Retrieval for Open-Domain Spoken
  Question Answering
Authors: Chyi-Jiunn Lin, Guan-Ting Lin, Yung-Sung Chuang, Wei-Lun Wu, Shang-Wen
  Li, Abdelrahman Mohamed, Hung-yi Lee, Lin-shan Lee
Categories: cs.CL cs.IR cs.SD eess.AS
Comments: Accepted at ICASSP 2024
\\
  Spoken Question Answering (SQA) is essential for machines to reply to user's
question by finding the answer span within a given spoken passage. SQA has been
previously achieved without ASR to avoid recognition errors and
Out-of-Vocabulary (OOV) problems. However, the real-world problem of
Open-domain SQA (openSQA), in which the machine needs to first retrieve
passages that possibly contain the answer from a spoken archive in addition,
was never considered. This paper proposes the first known end-to-end framework,
Speech Dense Passage Retriever (SpeechDPR), for the retrieval component of the
openSQA problem. SpeechDPR learns a sentence-level semantic representation by
distilling knowledge from the cascading model of unsupervised ASR (UASR) and
text dense retriever (TDR). No manually transcribed speech data is needed.
Initial experiments showed performance comparable to the cascading model of
UASR and TDR, and significantly better when UASR was poor, verifying this
approach is more robust to speech recognition errors.
\\ ( https://arxiv.org/abs/2401.13463 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13512
Date: Wed, 24 Jan 2024 15:10:13 GMT   (938kb,D)

Title: Can GPT-3.5 Generate and Code Discharge Summaries?
Authors: Mat\'u\v{s} Falis, Aryo Pradipta Gema, Hang Dong, Luke Daines,
  Siddharth Basetti, Michael Holder, Rose S Penfold, Alexandra Birch, Beatrice
  Alex
Categories: cs.CL
Comments: 15 pages; 250 words in abstract; 3,929 words in main body; 2 figures
  (0 black and white, 2 colour); 4 tables; 34 references
\\
  Objective: To investigate GPT-3.5 in generating and coding medical documents
with ICD-10 codes for data augmentation on low-resources labels.
  Materials and Methods: Employing GPT-3.5 we generated and coded 9,606
discharge summaries based on lists of ICD-10 code descriptions of patients with
infrequent (generation) codes within the MIMIC-IV dataset. Combined with the
baseline training set, this formed an augmented training set. Neural coding
models were trained on baseline and augmented data and evaluated on a MIMIC-IV
test set. We report micro- and macro-F1 scores on the full codeset, generation
codes, and their families. Weak Hierarchical Confusion Matrices were employed
to determine within-family and outside-of-family coding errors in the latter
codesets. The coding performance of GPT-3.5 was evaluated both on prompt-guided
self-generated data and real MIMIC-IV data. Clinical professionals evaluated
the clinical acceptability of the generated documents.
  Results: Augmentation slightly hinders the overall performance of the models
but improves performance for the generation candidate codes and their families,
including one unseen in the baseline training data. Augmented models display
lower out-of-family error rates. GPT-3.5 can identify ICD-10 codes by the
prompted descriptions, but performs poorly on real data. Evaluators note the
correctness of generated concepts while suffering in variety, supporting
information, and narrative.
  Discussion and Conclusion: GPT-3.5 alone is unsuitable for ICD-10 coding.
Augmentation positively affects generation code families but mainly benefits
codes with existing examples. Augmentation reduces out-of-family errors.
Discharge summaries generated by GPT-3.5 state prompted concepts correctly but
lack variety, and authenticity in narratives. They are unsuitable for clinical
practice.
\\ ( https://arxiv.org/abs/2401.13512 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13527
Date: Wed, 24 Jan 2024 15:25:01 GMT   (611kb,D)

Title: SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation
Authors: Dong Zhang, Xin Zhang, Jun Zhan, Shimin Li, Yaqian Zhou, Xipeng Qiu
Categories: cs.CL cs.SD eess.AS
Comments: work in progress
\\
  Benefiting from effective speech modeling, current Speech Large Language
Models (SLLMs) have demonstrated exceptional capabilities in in-context speech
generation and efficient generalization to unseen speakers. However, the
prevailing information modeling process is encumbered by certain redundancies,
leading to inefficiencies in speech generation. We propose Chain-of-Information
Generation (CoIG), a method for decoupling semantic and perceptual information
in large-scale speech generation. Building on this, we develop SpeechGPT-Gen,
an 8-billion-parameter SLLM efficient in semantic and perceptual information
modeling. It comprises an autoregressive model based on LLM for semantic
information modeling and a non-autoregressive model employing flow matching for
perceptual information modeling. Additionally, we introduce the novel approach
of infusing semantic information into the prior distribution to enhance the
efficiency of flow matching. Extensive experimental results demonstrate that
SpeechGPT-Gen markedly excels in zero-shot text-to-speech, zero-shot voice
conversion, and speech-to-speech dialogue, underscoring CoIG's remarkable
proficiency in capturing and modeling speech's semantic and perceptual
dimensions. Code and models are available at
https://github.com/0nutation/SpeechGPT.
\\ ( https://arxiv.org/abs/2401.13527 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13565
Date: Wed, 24 Jan 2024 16:21:28 GMT   (113kb,D)

Title: Large Malaysian Language Model Based on Mistral for Enhanced Local
  Language Understanding
Authors: Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan
Categories: cs.CL
\\
  In this paper, we present significant advancements in the pretraining of
Mistral 7B, a large-scale language model, using a dataset of 32.6 GB,
equivalent to 1.1 billion tokens. We explore the impact of extending the
context length, releasing models with context lengths of 4096 and 32768 tokens,
and further refining performance with a specialized 16384 context length
instruction-tuned model, we called it Malaysian Mistral.
  Our experiments demonstrate the efficacy of continue pretraining and the
influence of extended context lengths on Mistral 7B's language understanding
capabilities. Additionally, we release a model specifically tuned with a 16384
context length instruction, showcasing its potential for capturing nuanced
language intricacies.
  Furthermore, our research contributes to the benchmarking of Malaysian
Mistral against prominent language models, including ChatGPT3.5 and Claude 2.
We present compelling results indicating Malaysian Mistral's superior
performance on Tatabahasa (Malay grammar) test set, particularly when
fine-tuned with instructions.
  All models released at
https://huggingface.co/collections/mesolitica/malaysian-mistral-7b-6528f2ec825f4bba46c1700c
\\ ( https://arxiv.org/abs/2401.13565 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13588
Date: Wed, 24 Jan 2024 16:52:37 GMT   (820kb)

Title: Evaluation of General Large Language Models in Contextually Assessing
  Semantic Concepts Extracted from Adult Critical Care Electronic Health Record
  Notes
Authors: Darren Liu, Cheng Ding, Delgersuren Bold, Monique Bouvier, Jiaying Lu,
  Benjamin Shickel, Craig S. Jabaley, Wenhui Zhang, Soojin Park, Michael J.
  Young, Mark S. Wainwright, Gilles Clermont, Parisa Rashidi, Eric S.
  Rosenthal, Laurie Dimisko, Ran Xiao, Joo Heung Yoon, Carl Yang, Xiao Hu
Categories: cs.CL cs.AI cs.SE
\\
  The field of healthcare has increasingly turned its focus towards Large
Language Models (LLMs) due to their remarkable performance. However, their
performance in actual clinical applications has been underexplored. Traditional
evaluations based on question-answering tasks don't fully capture the nuanced
contexts. This gap highlights the need for more in-depth and practical
assessments of LLMs in real-world healthcare settings. Objective: We sought to
evaluate the performance of LLMs in the complex clinical context of adult
critical care medicine using systematic and comprehensible analytic methods,
including clinician annotation and adjudication. Methods: We investigated the
performance of three general LLMs in understanding and processing real-world
clinical notes. Concepts from 150 clinical notes were identified by MetaMap and
then labeled by 9 clinicians. Each LLM's proficiency was evaluated by
identifying the temporality and negation of these concepts using different
prompts for an in-depth analysis. Results: GPT-4 showed overall superior
performance compared to other LLMs. In contrast, both GPT-3.5 and
text-davinci-003 exhibit enhanced performance when the appropriate prompting
strategies are employed. The GPT family models have demonstrated considerable
efficiency, evidenced by their cost-effectiveness and time-saving capabilities.
Conclusion: A comprehensive qualitative performance evaluation framework for
LLMs is developed and operationalized. This framework goes beyond singular
performance aspects. With expert annotations, this methodology not only
validates LLMs' capabilities in processing complex medical data but also
establishes a benchmark for future LLM evaluations across specialized domains.
\\ ( https://arxiv.org/abs/2401.13588 ,  820kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13594
Date: Wed, 24 Jan 2024 17:01:42 GMT   (3627kb,D)

Title: Graph Guided Question Answer Generation for Procedural
  Question-Answering
Authors: Hai X. Pham, Isma Hadji, Xinnuo Xu, Ziedune Degutyte, Jay Rainey,
  Evangelos Kazakos, Afsaneh Fazly, Georgios Tzimiropoulos, Brais Martinez
Categories: cs.CL cs.AI
Comments: Accepted to EACL 2024 as long paper. 25 pages including appendix
MSC-class: I.2.7
\\
  In this paper, we focus on task-specific question answering (QA). To this
end, we introduce a method for generating exhaustive and high-quality training
data, which allows us to train compact (e.g., run on a mobile device),
task-specific QA models that are competitive against GPT variants. The key
technological enabler is a novel mechanism for automatic question-answer
generation from procedural text which can ingest large amounts of textual
instructions and produce exhaustive in-domain QA training data. While current
QA data generation methods can produce well-formed and varied data, their
non-exhaustive nature is sub-optimal for training a QA model. In contrast, we
leverage the highly structured aspect of procedural text and represent each
step and the overall flow of the procedure as graphs. We then condition on
graph nodes to automatically generate QA pairs in an exhaustive and
controllable manner. Comprehensive evaluations of our method show that: 1)
small models trained with our data achieve excellent performance on the target
QA task, even exceeding that of GPT3 and ChatGPT despite being several orders
of magnitude smaller. 2) semantic coverage is the key indicator for downstream
QA performance. Crucially, while large language models excel at syntactic
diversity, this does not necessarily result in improvements on the end QA
model. In contrast, the higher semantic coverage provided by our method is
critical for QA performance.
\\ ( https://arxiv.org/abs/2401.13594 ,  3627kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13598
Date: Wed, 24 Jan 2024 17:04:28 GMT   (712kb,D)

Title: Consistency Guided Knowledge Retrieval and Denoising in LLMs for
  Zero-shot Document-level Relation Triplet Extraction
Authors: Qi Sun and Kun Huang and Xiaocui Yang and Rong Tong and Kun Zhang and
  Soujanya Poria
Categories: cs.CL
Comments: Accepted by WWW 2024
\\
  Document-level Relation Triplet Extraction (DocRTE) is a fundamental task in
information systems that aims to simultaneously extract entities with semantic
relations from a document. Existing methods heavily rely on a substantial
amount of fully labeled data. However, collecting and annotating data for newly
emerging relations is time-consuming and labor-intensive. Recent advanced Large
Language Models (LLMs), such as ChatGPT and LLaMA, exhibit impressive long-text
generation capabilities, inspiring us to explore an alternative approach for
obtaining auto-labeled documents with new relations. In this paper, we propose
a Zero-shot Document-level Relation Triplet Extraction (ZeroDocRTE) framework,
which generates labeled data by retrieval and denoising knowledge from LLMs,
called GenRDK. Specifically, we propose a chain-of-retrieval prompt to guide
ChatGPT to generate labeled long-text data step by step. To improve the quality
of synthetic data, we propose a denoising strategy based on the consistency of
cross-document knowledge. Leveraging our denoised synthetic data, we proceed to
fine-tune the LLaMA2-13B-Chat for extracting document-level relation triplets.
We perform experiments for both zero-shot document-level relation and triplet
extraction on two public datasets. The experimental results illustrate that our
GenRDK framework outperforms strong baselines.
\\ ( https://arxiv.org/abs/2401.13598 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13601
Date: Wed, 24 Jan 2024 17:10:45 GMT   (5256kb,D)

Title: MM-LLMs: Recent Advances in MultiModal Large Language Models
Authors: Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu,
  Dong Yu
Categories: cs.CL
Comments: Work in progress
\\
  In the past year, MultiModal Large Language Models (MM-LLMs) have undergone
substantial advancements, augmenting off-the-shelf LLMs to support MM inputs or
outputs via cost-effective training strategies. The resulting models not only
preserve the inherent reasoning and decision-making capabilities of LLMs but
also empower a diverse range of MM tasks. In this paper, we provide a
comprehensive survey aimed at facilitating further research of MM-LLMs.
Specifically, we first outline general design formulations for model
architecture and training pipeline. Subsequently, we provide brief
introductions of $26$ existing MM-LLMs, each characterized by its specific
formulations. Additionally, we review the performance of MM-LLMs on mainstream
benchmarks and summarize key training recipes to enhance the potency of
MM-LLMs. Lastly, we explore promising directions for MM-LLMs while concurrently
maintaining a real-time tracking website for the latest developments in the
field. We hope that this survey contributes to the ongoing advancement of the
MM-LLMs domain.
\\ ( https://arxiv.org/abs/2401.13601 ,  5256kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13621
Date: Wed, 24 Jan 2024 17:48:45 GMT   (119kb,D)

Title: DenoSent: A Denoising Objective for Self-Supervised Sentence
  Representation Learning
Authors: Xinghao Wang, Junliang He, Pengyu Wang, Yunhua Zhou, Tianxiang Sun,
  Xipeng Qiu
Categories: cs.CL
Comments: AAAI 2024
\\
  Contrastive-learning-based methods have dominated sentence representation
learning. These methods regularize the representation space by pulling similar
sentence representations closer and pushing away the dissimilar ones and have
been proven effective in various NLP tasks, e.g., semantic textual similarity
(STS) tasks. However, it is challenging for these methods to learn fine-grained
semantics as they only learn from the inter-sentence perspective, i.e., their
supervision signal comes from the relationship between data samples. In this
work, we propose a novel denoising objective that inherits from another
perspective, i.e., the intra-sentence perspective. By introducing both discrete
and continuous noise, we generate noisy sentences and then train our model to
restore them to their original form. Our empirical evaluations demonstrate that
this approach delivers competitive results on both semantic textual similarity
(STS) and a wide range of transfer tasks, standing up well in comparison to
contrastive-learning-based methods. Notably, the proposed intra-sentence
denoising objective complements existing inter-sentence contrastive
methodologies and can be integrated with them to further enhance performance.
Our code is available at https://github.com/xinghaow99/DenoSent.
\\ ( https://arxiv.org/abs/2401.13621 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13660
Date: Wed, 24 Jan 2024 18:53:53 GMT   (767kb,D)

Title: MambaByte: Token-free Selective State Space Model
Authors: Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M Rush
Categories: cs.CL cs.LG
\\
  Token-free language models learn directly from raw bytes and remove the bias
of subword tokenization. Operating on bytes, however, results in significantly
longer sequences, and standard autoregressive Transformers scale poorly in such
settings. We experiment with MambaByte, a token-free adaptation of the Mamba
state space model, trained autoregressively on byte sequences. Our experiments
indicate the computational efficiency of MambaByte compared to other byte-level
models. We also find MambaByte to be competitive with and even outperform
state-of-the-art subword Transformers. Furthermore, owing to linear scaling in
length, MambaByte benefits from fast inference compared to Transformers. Our
findings establish the viability of MambaByte in enabling token-free language
modeling.
\\ ( https://arxiv.org/abs/2401.13660 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13009
Date: Tue, 23 Jan 2024 08:51:39 GMT   (670kb,D)

Title: Comparative Study of Causal Discovery Methods for Cyclic Models with
  Hidden Confounders
Authors: Boris Lorbeer, Mustafa Mohsen
Categories: cs.LG stat.ME stat.ML
\\
  Nowadays, the need for causal discovery is ubiquitous. A better understanding
of not just the stochastic dependencies between parts of a system, but also the
actual cause-effect relations, is essential for all parts of science. Thus, the
need for reliable methods to detect causal directions is growing constantly. In
the last 50 years, many causal discovery algorithms have emerged, but most of
them are applicable only under the assumption that the systems have no feedback
loops and that they are causally sufficient, i.e. that there are no unmeasured
subsystems that can affect multiple measured variables. This is unfortunate
since those restrictions can often not be presumed in practice. Feedback is an
integral feature of many processes, and real-world systems are rarely
completely isolated and fully measured. Fortunately, in recent years, several
techniques, that can cope with cyclic, causally insufficient systems, have been
developed. And with multiple methods available, a practical application of
those algorithms now requires knowledge of the respective strengths and
weaknesses. Here, we focus on the problem of causal discovery for sparse linear
models which are allowed to have cycles and hidden confounders. We have
prepared a comprehensive and thorough comparative study of four causal
discovery techniques: two versions of the LLC method [10] and two variants of
the ASP-based algorithm [11]. The evaluation investigates the performance of
those techniques for various experiments with multiple interventional setups
and different dataset sizes.
\\ ( https://arxiv.org/abs/2401.13009 ,  670kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13034
Date: Tue, 23 Jan 2024 19:00:02 GMT   (2552kb,D)

Title: Locality Sensitive Sparse Encoding for Learning World Models Online
Authors: Zichen Liu, Chao Du, Wee Sun Lee, Min Lin
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\
  Acquiring an accurate world model online for model-based reinforcement
learning (MBRL) is challenging due to data nonstationarity, which typically
causes catastrophic forgetting for neural networks (NNs). From the online
learning perspective, a Follow-The-Leader (FTL) world model is desirable, which
optimally fits all previous experiences at each round. Unfortunately, NN-based
models need re-training on all accumulated data at every interaction step to
achieve FTL, which is computationally expensive for lifelong agents. In this
paper, we revisit models that can achieve FTL with incremental updates.
Specifically, our world model is a linear regression model supported by
nonlinear random features. The linear part ensures efficient FTL update while
the nonlinear random feature empowers the fitting of complex environments. To
best trade off model capacity and computation efficiency, we introduce a
locality sensitive sparse encoding, which allows us to conduct efficient sparse
updates even with very high dimensional nonlinear features. We validate the
representation power of our encoding and verify that it allows efficient online
learning under data covariate shift. We also show, in the Dyna MBRL setting,
that our world models learned online using a single pass of trajectory data
either surpass or match the performance of deep world models trained with
replay and other continual learning methods.
\\ ( https://arxiv.org/abs/2401.13034 ,  2552kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13096
Date: Tue, 23 Jan 2024 21:20:48 GMT   (731kb,D)

Title: Probabilistic Demand Forecasting with Graph Neural Networks
Authors: Nikita Kozodoi, Elizaveta Zinovyeva, Simon Valentin, Jo\~ao Pereira,
  Rodrigo Agundez
Categories: cs.LG stat.ML
Comments: Preprint of the paper accepted to ECML PKDD 2023 ML4ITS Workshop
\\
  Demand forecasting is a prominent business use case that allows retailers to
optimize inventory planning, logistics, and core business decisions. One of the
key challenges in demand forecasting is accounting for relationships and
interactions between articles. Most modern forecasting approaches provide
independent article-level predictions that do not consider the impact of
related articles. Recent research has attempted addressing this challenge using
Graph Neural Networks (GNNs) and showed promising results. This paper builds on
previous research on GNNs and makes two contributions. First, we integrate a
GNN encoder into a state-of-the-art DeepAR model. The combined model produces
probabilistic forecasts, which are crucial for decision-making under
uncertainty. Second, we propose to build graphs using article attribute
similarity, which avoids reliance on a pre-defined graph structure. Experiments
on three real-world datasets show that the proposed approach consistently
outperforms non-graph benchmarks. We also show that our approach produces
article embeddings that encode article similarity and demand dynamics and are
useful for other downstream business tasks beyond forecasting.
\\ ( https://arxiv.org/abs/2401.13096 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13098
Date: Tue, 23 Jan 2024 21:22:51 GMT   (5696kb,D)

Title: Gravity-Informed Deep Learning Framework for Predicting Ship Traffic
  Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge
Authors: Ruixin Song, Gabriel Spadon, Sarah Bailey, Ronald Pelot, Stan Matwin,
  Amilcar Soares
Categories: cs.LG cs.AI cs.SI stat.AP
Comments: 26 pages, 7 figures, under review
\\
  Invasive species in water bodies pose a major threat to the environment and
biodiversity globally. Due to increased transportation and trade, non-native
species have been introduced to new environments, causing damage to ecosystems
and leading to economic losses in agriculture, forestry, and fisheries.
Therefore, there is a pressing need for risk assessment and management
techniques to mitigate the impact of these invasions. This study aims to
develop a new physics-inspired model to forecast maritime shipping traffic and
thus inform risk assessment of invasive species spread through global
transportation networks. Inspired by the gravity model for international
trades, our model considers various factors that influence the likelihood and
impact of vessel activities, such as shipping flux density, distance between
ports, trade flow, and centrality measures of transportation hubs.
Additionally, by analyzing the risk network of invasive species, we provide a
comprehensive framework for assessing the invasion threat level given a pair of
origin and destination. Accordingly, this paper introduces transformers to
gravity models to rebuild the short- and long-term dependencies that make the
risk analysis feasible. Thus, we introduce a physics-inspired framework that
achieves an 89% segmentation accuracy for existing and non-existing
trajectories and an 84.8% accuracy for the number of vessels flowing between
key port areas, representing more than 10% improvement over the traditional
deep-gravity model. Along these lines, this research contributes to a better
understanding of invasive species risk assessment. It allows policymakers,
conservationists, and stakeholders to prioritize management actions by
identifying high-risk invasion pathways. Besides, our model is versatile and
can include new data sources, making it suitable for assessing species invasion
risks in a changing global landscape.
\\ ( https://arxiv.org/abs/2401.13098 ,  5696kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13099
Date: Tue, 23 Jan 2024 21:23:51 GMT   (152kb)

Title: Sparse identification of nonlinear dynamics in the presence of library
  and system uncertainty
Authors: Andrew O'Brien
Categories: cs.LG cs.AI
\\
  The SINDy algorithm has been successfully used to identify the governing
equations of dynamical systems from time series data. However, SINDy assumes
the user has prior knowledge of the variables in the system and of a function
library that can act as a basis for the system. In this paper, we demonstrate
on real world data how the Augmented SINDy algorithm outperforms SINDy in the
presence of system variable uncertainty. We then show SINDy can be further
augmented to perform robustly when both kinds of uncertainty are present.
\\ ( https://arxiv.org/abs/2401.13099 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13115
Date: Tue, 23 Jan 2024 21:51:51 GMT   (2257kb,D)

Title: Contractive Diffusion Probabilistic Models
Authors: Wenpin Tang and Hanyang Zhao
Categories: cs.LG
\\
  Diffusion probabilistic models (DPMs) have emerged as a promising technology
in generative modeling. The success of DPMs relies on two ingredients: time
reversal of Markov diffusion processes and score matching. Most existing work
implicitly assumes that score matching is close to perfect, while this
assumption is questionable. In view of possibly unguaranteed score matching, we
propose a new criterion -- the contraction of backward sampling in the design
of DPMs. This leads to a novel class of contractive DPMs (CDPMs), including
contractive Ornstein-Uhlenbeck (OU) processes and contractive sub-variance
preserving (sub-VP) stochastic differential equations (SDEs). The key insight
is that the contraction in the backward process narrows score matching errors,
as well as discretization error. Thus, the proposed CDPMs are robust to both
sources of error. Our proposal is supported by theoretical results, and is
corroborated by experiments. Notably, contractive sub-VP shows the best
performance among all known SDE-based DPMs on the CIFAR-10 dataset.
\\ ( https://arxiv.org/abs/2401.13115 ,  2257kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13148
Date: Tue, 23 Jan 2024 23:50:19 GMT   (14015kb,D)

Title: NLBAC: A Neural Ordinary Differential Equations-based Framework for
  Stable and Safe Reinforcement Learning
Authors: Liqun Zhao, Keyan Miao, Konstantinos Gatsis, Antonis Papachristodoulou
Categories: cs.LG cs.RO cs.SY eess.SY
Comments: The comprehensive version of one paper submitted to 6th Annual
  Learning for Dynamics & Control Conference (L4DC 2024)
\\
  Reinforcement learning (RL) excels in applications such as video games and
robotics, but ensuring safety and stability remains challenging when using RL
to control real-world systems where using model-free algorithms suffering from
low sample efficiency might be prohibitive. This paper first provides safety
and stability definitions for the RL system, and then introduces a Neural
ordinary differential equations-based Lyapunov-Barrier Actor-Critic (NLBAC)
framework that leverages Neural Ordinary Differential Equations (NODEs) to
approximate system dynamics and integrates the Control Barrier Function (CBF)
and Control Lyapunov Function (CLF) frameworks with the actor-critic method to
assist in maintaining the safety and stability for the system. Within this
framework, we employ the augmented Lagrangian method to update the RL-based
controller parameters. Additionally, we introduce an extra backup controller in
situations where CBF constraints for safety and the CLF constraint for
stability cannot be satisfied simultaneously. Simulation results demonstrate
that the framework leads the system to approach the desired state and allows
fewer violations of safety constraints with better sample efficiency compared
to other methods.
\\ ( https://arxiv.org/abs/2401.13148 ,  14015kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13157
Date: Wed, 24 Jan 2024 00:33:53 GMT   (3384kb,D)

Title: Time-Aware Knowledge Representations of Dynamic Objects with
  Multidimensional Persistence
Authors: Baris Coskunuzer, Ignacio Segovia-Dominguez, Yuzhou Chen and Yulia R.
  Gel
Categories: cs.LG cs.AI
Journal-ref: AAAI 2024
\\
  Learning time-evolving objects such as multivariate time series and dynamic
networks requires the development of novel knowledge representation mechanisms
and neural network architectures, which allow for capturing implicit
time-dependent information contained in the data. Such information is typically
not directly observed but plays a key role in the learning task performance. In
turn, lack of time dimension in knowledge encoding mechanisms for
time-dependent data leads to frequent model updates, poor learning performance,
and, as a result, subpar decision-making. Here we propose a new approach to a
time-aware knowledge representation mechanism that notably focuses on implicit
time-dependent topological information along multiple geometric dimensions. In
particular, we propose a new approach, named \textit{Temporal MultiPersistence}
(TMP), which produces multidimensional topological fingerprints of the data by
using the existing single parameter topological summaries. The main idea behind
TMP is to merge the two newest directions in topological representation
learning, that is, multi-persistence which simultaneously describes data shape
evolution along multiple key parameters, and zigzag persistence to enable us to
extract the most salient data shape information over time. We derive
theoretical guarantees of TMP vectorizations and show its utility, in
application to forecasting on benchmark traffic flow, Ethereum blockchain, and
electrocardiogram datasets, demonstrating the competitive performance,
especially, in scenarios of limited data records. In addition, our TMP method
improves the computational efficiency of the state-of-the-art multipersistence
summaries up to 59.5 times.
\\ ( https://arxiv.org/abs/2401.13157 ,  3384kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13160
Date: Wed, 24 Jan 2024 00:36:13 GMT   (184kb,D)

Title: SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced
  Token Detection
Authors: Ke Ye, Heinrich Jiang, Afshin Rostamizadeh, Ayan Chakrabarti, Giulia
  DeSalvo, Jean-Fran\c{c}ois Kagy, Lazaros Karydas, Gui Citovsky, Sanjiv Kumar
Categories: cs.LG cs.CL
Comments: 9+13 pages, 5 figures
\\
  Pre-training large language models is known to be extremely resource
intensive and often times inefficient, under-utilizing the information
encapsulated in the training text sequences. In this paper, we present SpacTor,
a new training procedure consisting of (1) a hybrid objective combining span
corruption (SC) and token replacement detection (RTD), and (2) a two-stage
curriculum that optimizes the hybrid objective over the initial $\tau$
iterations, then transitions to standard SC loss. We show empirically that the
effectiveness of the hybrid objective is tied to the two-stage pre-training
schedule, and provide extensive analysis on why this is the case. In our
experiments with encoder-decoder architectures (T5) on a variety of NLP tasks,
SpacTor-T5 yields the same downstream performance as standard SC pre-training,
while enabling a 50% reduction in pre-training iterations and 40% reduction in
total FLOPs. Alternatively, given the same amount of computing budget, we find
that SpacTor results in significantly improved downstream benchmark
performance.
\\ ( https://arxiv.org/abs/2401.13160 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13171
Date: Wed, 24 Jan 2024 01:33:39 GMT   (7250kb,D)

Title: Compositional Generative Inverse Design
Authors: Tailin Wu, Takashi Maruyama, Long Wei, Tao Zhang, Yilun Du, Gianluca
  Iaccarino, Jure Leskovec
Categories: cs.LG cs.AI cs.CE
Comments: ICLR 2024 spotlight. 30 pages, 17 figures
\\
  Inverse design, where we seek to design input variables in order to optimize
an underlying objective function, is an important problem that arises across
fields such as mechanical engineering to aerospace engineering. Inverse design
is typically formulated as an optimization problem, with recent works
leveraging optimization across learned dynamics models. However, as models are
optimized they tend to fall into adversarial modes, preventing effective
sampling. We illustrate that by instead optimizing over the learned energy
function captured by the diffusion model, we can avoid such adversarial
examples and significantly improve design performance. We further illustrate
how such a design system is compositional, enabling us to combine multiple
different diffusion models representing subcomponents of our desired system to
design systems with every specified component. In an N-body interaction task
and a challenging 2D multi-airfoil design task, we demonstrate that by
composing the learned diffusion model at test time, our method allows us to
design initial states and boundary shapes that are more complex than those in
the training data. Our method outperforms state-of-the-art neural inverse
design method by an average of 41.5% in prediction MAE and 14.3% in design
objective for the N-body dataset and discovers formation flying to minimize
drag in the multi-airfoil design task. Project website and code can be found at
https://github.com/AI4Science-WestlakeU/cindm.
\\ ( https://arxiv.org/abs/2401.13171 ,  7250kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13185
Date: Wed, 24 Jan 2024 02:16:03 GMT   (25kb)

Title: Shortcutting Cross-Validation: Efficiently Deriving Column-Wise Centered
  and Scaled Training Set $\mathbf{X}^\mathbf{T}\mathbf{X}$ and
  $\mathbf{X}^\mathbf{T}\mathbf{Y}$ Without Full Recomputation of Matrix
  Products or Statistical Moments
Authors: Ole-Christian Galbo Engstr{\o}m
Categories: cs.LG cs.DS cs.MS
Comments: 24 pages, 1 table, 6 algorithms
\\
  Cross-validation is a widely used technique for assessing the performance of
predictive models on unseen data. Many predictive models, such as Kernel-Based
Partial Least-Squares (PLS) models, require the computation of
$\mathbf{X}^{\mathbf{T}}\mathbf{X}$ and $\mathbf{X}^{\mathbf{T}}\mathbf{Y}$
using only training set samples from the input and output matrices,
$\mathbf{X}$ and $\mathbf{Y}$, respectively. In this work, we present three
algorithms that efficiently compute these matrices. The first one allows no
column-wise preprocessing. The second one allows column-wise centering around
the training set means. The third one allows column-wise centering and
column-wise scaling around the training set means and standard deviations.
Demonstrating correctness and superior computational complexity, they offer
significant cross-validation speedup compared with straight-forward
cross-validation and previous work on fast cross-validation - all without data
leakage. Their suitability for parallelization is highlighted with an
open-source Python implementation combining our algorithms with Improved Kernel
PLS.
\\ ( https://arxiv.org/abs/2401.13185 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13200
Date: Wed, 24 Jan 2024 03:03:17 GMT   (13215kb,D)

Title: Topology-aware Embedding Memory for Learning on Expanding Graphs
Authors: Xikun Zhang, Dongjin Song, Yixin Chen, Dacheng Tao
Categories: cs.LG
\\
  Memory replay based techniques have shown great success for continual
learning with incrementally accumulated Euclidean data. Directly applying them
to continually expanding graphs, however, leads to the potential memory
explosion problem due to the need to buffer representative nodes and their
associated topological neighborhood structures. To this end, we systematically
analyze the key challenges in the memory explosion problem, and present a
general framework, i.e., Parameter Decoupled Graph Neural Networks (PDGNNs)
with Topology-aware Embedding Memory (TEM), to tackle this issue. The proposed
framework not only reduces the memory space complexity from $\mathcal{O}(nd^L)$
to $\mathcal{O}(n)$~\footnote{$n$: memory budget, $d$: average node degree,
$L$: the radius of the GNN receptive field}, but also fully utilizes the
topological information for memory replay. Specifically, PDGNNs decouple
trainable parameters from the computation ego-subgraph via
\textit{Topology-aware Embeddings} (TEs), which compress ego-subgraphs into
compact vectors (i.e., TEs) to reduce the memory consumption. Based on this
framework, we discover a unique \textit{pseudo-training effect} in continual
learning on expanding graphs and this effect motivates us to develop a novel
\textit{coverage maximization sampling} strategy that can enhance the
performance with a tight memory budget. Thorough empirical studies demonstrate
that, by tackling the memory explosion problem and incorporating topological
information into memory replay, PDGNNs with TEM significantly outperform
state-of-the-art techniques, especially in the challenging class-incremental
setting.
\\ ( https://arxiv.org/abs/2401.13200 ,  13215kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13206
Date: Wed, 24 Jan 2024 03:28:48 GMT   (1576kb)

Title: Self-Improving Interference Management Based on Deep Learning With
  Uncertainty Quantification
Authors: Hyun-Suk Lee, Do-Yup Kim, Kyungsik Min
Categories: cs.LG
\\
  This paper presents a groundbreaking self-improving interference management
framework tailored for wireless communications, integrating deep learning with
uncertainty quantification to enhance overall system performance. Our approach
addresses the computational challenges inherent in traditional
optimization-based algorithms by harnessing deep learning models to predict
optimal interference management solutions. A significant breakthrough of our
framework is its acknowledgment of the limitations inherent in data-driven
models, particularly in scenarios not adequately represented by the training
dataset. To overcome these challenges, we propose a method for uncertainty
quantification, accompanied by a qualifying criterion, to assess the
trustworthiness of model predictions. This framework strategically alternates
between model-generated solutions and traditional algorithms, guided by a
criterion that assesses the prediction credibility based on quantified
uncertainties. Experimental results validate the framework's efficacy,
demonstrating its superiority over traditional deep learning models, notably in
scenarios underrepresented in the training dataset. This work marks a
pioneering endeavor in harnessing self-improving deep learning for interference
management, through the lens of uncertainty quantification.
\\ ( https://arxiv.org/abs/2401.13206 ,  1576kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13210
Date: Wed, 24 Jan 2024 03:43:45 GMT   (5546kb,D)

Title: Multitask Active Learning for Graph Anomaly Detection
Authors: Wenjing Chang, Kay Liu, Kaize Ding, Philip S. Yu, Jianjun Yu
Categories: cs.LG cs.SI
Comments: Preprint. Under review. Code available at
  https://github.com/AhaChang/MITIGATE
\\
  In the web era, graph machine learning has been widely used on ubiquitous
graph-structured data. As a pivotal component for bolstering web security and
enhancing the robustness of graph-based applications, the significance of graph
anomaly detection is continually increasing. While Graph Neural Networks (GNNs)
have demonstrated efficacy in supervised and semi-supervised graph anomaly
detection, their performance is contingent upon the availability of sufficient
ground truth labels. The labor-intensive nature of identifying anomalies from
complex graph structures poses a significant challenge in real-world
applications. Despite that, the indirect supervision signals from other tasks
(e.g., node classification) are relatively abundant. In this paper, we propose
a novel MultItask acTIve Graph Anomaly deTEction framework, namely MITIGATE.
Firstly, by coupling node classification tasks, MITIGATE obtains the capability
to detect out-of-distribution nodes without known anomalies. Secondly, MITIGATE
quantifies the informativeness of nodes by the confidence difference across
tasks, allowing samples with conflicting predictions to provide informative yet
not excessively challenging information for subsequent training. Finally, to
enhance the likelihood of selecting representative nodes that are distant from
known patterns, MITIGATE adopts a masked aggregation mechanism for distance
measurement, considering both inherent features of nodes and current labeled
status. Empirical studies on four datasets demonstrate that MITIGATE
significantly outperforms the state-of-the-art methods for anomaly detection.
Our code is publicly available at: https://github.com/AhaChang/MITIGATE.
\\ ( https://arxiv.org/abs/2401.13210 ,  5546kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13216
Date: Wed, 24 Jan 2024 03:57:45 GMT   (996kb,D)

Title: On Principled Local Optimization Methods for Federated Learning
Authors: Honglin Yuan
Categories: cs.LG cs.DC math.OC stat.ML
Comments: Stanford University Doctoral Dissertation
\\
  Federated Learning (FL), a distributed learning paradigm that scales
on-device learning collaboratively, has emerged as a promising approach for
decentralized AI applications. Local optimization methods such as Federated
Averaging (FedAvg) are the most prominent methods for FL applications. Despite
their simplicity and popularity, the theoretical understanding of local
optimization methods is far from clear. This dissertation aims to advance the
theoretical foundation of local methods in the following three directions.
  First, we establish sharp bounds for FedAvg, the most popular algorithm in
Federated Learning. We demonstrate how FedAvg may suffer from a notion we call
iterate bias, and how an additional third-order smoothness assumption may
mitigate this effect and lead to better convergence rates. We explain this
phenomenon from a Stochastic Differential Equation (SDE) perspective.
  Second, we propose Federated Accelerated Stochastic Gradient Descent (FedAc),
the first principled acceleration of FedAvg, which provably improves the
convergence rate and communication efficiency. Our technique uses on a
potential-based perturbed iterate analysis, a novel stability analysis of
generalized accelerated SGD, and a strategic tradeoff between acceleration and
stability.
  Third, we study the Federated Composite Optimization problem, which extends
the classic smooth setting by incorporating a shared non-smooth regularizer. We
show that direct extensions of FedAvg may suffer from the "curse of primal
averaging," resulting in slow convergence. As a solution, we propose a new
primal-dual algorithm, Federated Dual Averaging, which overcomes the curse of
primal averaging by employing a novel inter-client dual averaging procedure.
\\ ( https://arxiv.org/abs/2401.13216 ,  996kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13236
Date: Wed, 24 Jan 2024 05:41:34 GMT   (824kb,D)

Title: How to Collaborate: Towards Maximizing the Generalization Performance in
  Cross-Silo Federated Learning
Authors: Yuchang Sun and Marios Kountouris and Jun Zhang
Categories: cs.LG cs.DC
\\
  Federated learning (FL) has attracted vivid attention as a privacy-preserving
distributed learning framework. In this work, we focus on cross-silo FL, where
clients become the model owners after training and are only concerned about the
model's generalization performance on their local data. Due to the data
heterogeneity issue, asking all the clients to join a single FL training
process may result in model performance degradation. To investigate the
effectiveness of collaboration, we first derive a generalization bound for each
client when collaborating with others or when training independently. We show
that the generalization performance of a client can be improved only by
collaborating with other clients that have more training data and similar data
distribution. Our analysis allows us to formulate a client utility maximization
problem by partitioning clients into multiple collaborating groups. A
hierarchical clustering-based collaborative training (HCCT) scheme is then
proposed, which does not need to fix in advance the number of groups. We
further analyze the convergence of HCCT for general non-convex loss functions
which unveils the effect of data similarity among clients. Extensive
simulations show that HCCT achieves better generalization performance than
baseline schemes, whereas it degenerates to independent training and
conventional FL in specific scenarios.
\\ ( https://arxiv.org/abs/2401.13236 ,  824kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13239
Date: Wed, 24 Jan 2024 05:57:36 GMT   (431kb,D)

Title: Adaptive Crowdsourcing Via Self-Supervised Learning
Authors: Anmol Kagrecha, Henrik Marklund, Benjamin Van Roy, Hong Jun Jeon,
  Richard Zeckhauser
Categories: cs.LG cs.HC
Comments: 29 pages, 3 figures
\\
  Common crowdsourcing systems average estimates of a latent quantity of
interest provided by many crowdworkers to produce a group estimate. We develop
a new approach -- just-predict-others -- that leverages self-supervised
learning and a novel aggregation scheme. This approach adapts weights assigned
to crowdworkers based on estimates they provided for previous quantities. When
skills vary across crowdworkers or their estimates correlate, the weighted sum
offers a more accurate group estimate than the average. Existing algorithms
such as expectation maximization can, at least in principle, produce similarly
accurate group estimates. However, their computational requirements become
onerous when complex models, such as neural networks, are required to express
relationships among crowdworkers. Just-predict-others accommodates such
complexity as well as many other practical challenges. We analyze the efficacy
of just-predict-others through theoretical and computational studies. Among
other things, we establish asymptotic optimality as the number of engagements
per crowdworker grows.
\\ ( https://arxiv.org/abs/2401.13239 ,  431kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13282
Date: Wed, 24 Jan 2024 07:47:01 GMT   (14877kb,D)

Title: RefreshNet: Learning Multiscale Dynamics through Hierarchical Refreshing
Authors: Junaid Farooq, Danish Rafiq, Pantelis R. Vlachas, Mohammad Abid Bazaz
Categories: cs.LG cs.AI
\\
  Forecasting complex system dynamics, particularly for long-term predictions,
is persistently hindered by error accumulation and computational burdens. This
study presents RefreshNet, a multiscale framework developed to overcome these
challenges, delivering an unprecedented balance between computational
efficiency and predictive accuracy. RefreshNet incorporates convolutional
autoencoders to identify a reduced order latent space capturing essential
features of the dynamics, and strategically employs multiple recurrent neural
network (RNN) blocks operating at varying temporal resolutions within the
latent space, thus allowing the capture of latent dynamics at multiple temporal
scales. The unique "refreshing" mechanism in RefreshNet allows coarser blocks
to reset inputs of finer blocks, effectively controlling and alleviating error
accumulation. This design demonstrates superiority over existing techniques
regarding computational efficiency and predictive accuracy, especially in
long-term forecasting. The framework is validated using three benchmark
applications: the FitzHugh-Nagumo system, the Reaction-Diffusion equation, and
Kuramoto-Sivashinsky dynamics. RefreshNet significantly outperforms
state-of-the-art methods in long-term forecasting accuracy and speed, marking a
significant advancement in modeling complex systems and opening new avenues in
understanding and predicting their behavior.
\\ ( https://arxiv.org/abs/2401.13282 ,  14877kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13301
Date: Wed, 24 Jan 2024 08:49:50 GMT   (196kb)

Title: Classification of Radiologically Isolated Syndrome and Clinically
  Isolated Syndrome with Machine-Learning Techniques
Authors: V Mato-Abad, A Labiano-Fontcuberta, S Rodriguez-Yanez, R
  Garcia-Vazquez, CR Munteanu, J Andrade-Garda, A Domingo-Santos, V Galan
  Sanchez-Seco, Y Aladro, ML Martinez-Gines, L Ayuso, J Benito-Leon
Categories: cs.LG
Comments: 24 pages, 2 tables
DOI: 10.1111/ene.13923
\\
  Background and purpose: The unanticipated detection by magnetic resonance
imaging (MRI) in the brain of asymptomatic subjects of white matter lesions
suggestive of multiple sclerosis (MS) has been named radiologically isolated
syndrome (RIS). As the difference between early MS [i.e. clinically isolated
syndrome (CIS)] and RIS is the occurrence of a clinical event, it is logical to
improve detection of the subclinical form without interfering with MRI as there
are radiological diagnostic criteria for that. Our objective was to use
machine-learning classification methods to identify morphometric measures that
help to discriminate patients with RIS from those with CIS.
  Methods: We used a multimodal 3-T MRI approach by combining MRI biomarkers
(cortical thickness, cortical and subcortical grey matter volume, and white
matter integrity) of a cohort of 17 patients with RIS and 17 patients with CIS
for single-subject level classification.
  Results: The best proposed models to predict the diagnosis of CIS and RIS
were based on the Naive Bayes, Bagging and Multilayer Perceptron classifiers
using only three features: the left rostral middle frontal gyrus volume and the
fractional anisotropy values in the right amygdala and right lingual gyrus. The
Naive Bayes obtained the highest accuracy [overall classification, 0.765; area
under the receiver operating characteristic (AUROC), 0.782].
  Conclusions: A machine-learning approach applied to multimodal MRI data may
differentiate between the earliest clinical expressions of MS (CIS and RIS)
with an accuracy of 78%.
  Keywords: Bagging; Multilayer Perceptron; Naive Bayes classifier; clinically
isolated syndrome; diffusion tensor imaging; machine-learning; magnetic
resonance imaging; multiple sclerosis; radiologically isolated syndrome.
\\ ( https://arxiv.org/abs/2401.13301 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13327
Date: Wed, 24 Jan 2024 09:44:57 GMT   (7898kb,D)

Title: Generating Synthetic Health Sensor Data for Privacy-Preserving Wearable
  Stress Detection
Authors: Lucas Lange and Nils Wenzlitschke and Erhard Rahm
Categories: cs.LG cs.CR
\\
  Smartwatch health sensor data is increasingly utilized in smart health
applications and patient monitoring, including stress detection. However, such
medical data often comprises sensitive personal information and is
resource-intensive to acquire for research purposes. In response to this
challenge, we introduce the privacy-aware synthetization of multi-sensor
smartwatch health readings related to moments of stress. Our method involves
the generation of synthetic sequence data through Generative Adversarial
Networks (GANs), coupled with the implementation of Differential Privacy (DP)
safeguards for protecting patient information during model training. To ensure
the integrity of our synthetic data, we employ a range of quality assessments
and monitor the plausibility between synthetic and original data. To test the
usefulness, we create private machine learning models on a commonly used,
albeit small, stress detection dataset, exploring strategies for enhancing the
existing data foundation with our synthetic data. Through our GAN-based
augmentation methods, we observe improvements in model performance, both in
non-private (0.45% F1) and private (11.90-15.48% F1) training scenarios. We
underline the potential of differentially private synthetic data in optimizing
utility-privacy trade-offs, especially with limited availability of real
training samples.
\\ ( https://arxiv.org/abs/2401.13327 ,  7898kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13330
Date: Wed, 24 Jan 2024 09:48:12 GMT   (1447kb,D)

Title: NACHOS: Neural Architecture Search for Hardware Constrained Early Exit
  Neural Networks
Authors: Matteo Gambella, Jary Pomponi, Simone Scardapane, and Manuel Roveri
Categories: cs.LG cs.CV
\\
  Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN)
with Early Exit Classifiers (EECs), to provide predictions at intermediate
points of the processing when enough confidence in classification is achieved.
This leads to many benefits in terms of effectiveness and efficiency.
Currently, the design of EENNs is carried out manually by experts, a complex
and time-consuming task that requires accounting for many aspects, including
the correct placement, the thresholding, and the computational overhead of the
EECs. For this reason, the research is exploring the use of Neural Architecture
Search (NAS) to automatize the design of EENNs. Currently, few comprehensive
NAS solutions for EENNs have been proposed in the literature, and a fully
automated, joint design strategy taking into consideration both the backbone
and the EECs remains an open problem. To this end, this work presents Neural
Architecture Search for Hardware Constrained Early Exit Neural Networks
(NACHOS), the first NAS framework for the design of optimal EENNs satisfying
constraints on the accuracy and the number of Multiply and Accumulate (MAC)
operations performed by the EENNs at inference time. In particular, this
provides the joint design of backbone and EECs to select a set of admissible
(i.e., respecting the constraints) Pareto Optimal Solutions in terms of best
tradeoff between the accuracy and number of MACs. The results show that the
models designed by NACHOS are competitive with the state-of-the-art EENNs.
Additionally, this work investigates the effectiveness of two novel
regularization terms designed for the optimization of the auxiliary classifiers
of the EENN
\\ ( https://arxiv.org/abs/2401.13330 ,  1447kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13334
Date: Wed, 24 Jan 2024 09:59:22 GMT   (17584kb,D)

Title: Explainable Bayesian Optimization
Authors: Tanmay Chakraborty, Christin Seifert, Christian Wirth
Categories: cs.LG cs.AI
\\
  In industry, Bayesian optimization (BO) is widely applied in the human-AI
collaborative parameter tuning of cyber-physical systems. However, BO's
solutions may deviate from human experts' actual goal due to approximation
errors and simplified objectives, requiring subsequent tuning. The black-box
nature of BO limits the collaborative tuning process because the expert does
not trust the BO recommendations. Current explainable AI (XAI) methods are not
tailored for optimization and thus fall short of addressing this gap. To bridge
this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based
explainability method that produces high quality explanations through
multiobjective optimization. Our evaluation of benchmark optimization problems
and real-world hyperparameter optimization tasks demonstrates TNTRules'
superiority over state-of-the-art XAI methods in generating high quality
explanations. This work contributes to the intersection of BO and XAI,
providing interpretable optimization techniques for real-world applications.
\\ ( https://arxiv.org/abs/2401.13334 ,  17584kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13360
Date: Wed, 24 Jan 2024 10:37:28 GMT   (2949kb,D)

Title: Debiased Sample Selection for Combating Noisy Labels
Authors: Qi Wei, Lei Feng, Haobo Wang, Bo An
Categories: cs.LG
\\
  Learning with noisy labels aims to ensure model generalization given a
label-corrupted training set. The sample selection strategy achieves promising
performance by selecting a label-reliable subset for model training. In this
paper, we empirically reveal that existing sample selection methods suffer from
both data and training bias that are represented as imbalanced selected sets
and accumulation errors in practice, respectively. However, only the training
bias was handled in previous studies. To address this limitation, we propose a
noIse-Tolerant Expert Model (ITEM) for debiased learning in sample selection.
Specifically, to mitigate the training bias, we design a robust network
architecture that integrates with multiple experts. Compared with the
prevailing double-branch network, our network exhibits better performance of
selection and prediction by ensembling these experts while training with fewer
parameters. Meanwhile, to mitigate the data bias, we propose a mixed sampling
strategy based on two weight-based data samplers. By training on the mixture of
two class-discriminative mini-batches, the model mitigates the effect of the
imbalanced training set while avoiding sparse representations that are easily
caused by sampling strategies. Extensive experiments and analyses demonstrate
the effectiveness of ITEM. Our code is available at this url
\href{https://github.com/1998v7/ITEM}{ITEM}.
\\ ( https://arxiv.org/abs/2401.13360 ,  2949kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13366
Date: Wed, 24 Jan 2024 10:51:15 GMT   (2775kb,D)

Title: Mitigating System Bias in Resource Constrained Asynchronous Federated
  Learning Systems
Authors: Jikun Gao, Ioannis Mavromatis, Peizheng Li, Pietro Carnelli, Aftab
  Khan
Categories: cs.LG
Comments: 6 pages, 5 figures. This work has been accepted by PerCom PerconAI
  workshop 2024
\\
  Federated learning (FL) systems face performance challenges in dealing with
heterogeneous devices and non-identically distributed data across clients. We
propose a dynamic global model aggregation method within Asynchronous Federated
Learning (AFL) deployments to address these issues. Our aggregation method
scores and adjusts the weighting of client model updates based on their upload
frequency to accommodate differences in device capabilities. Additionally, we
also immediately provide an updated global model to clients after they upload
their local models to reduce idle time and improve training efficiency. We
evaluate our approach within an AFL deployment consisting of 10 simulated
clients with heterogeneous compute constraints and non-IID data. The simulation
results, using the FashionMNIST dataset, demonstrate over 10% and 19%
improvement in global model accuracy compared to state-of-the-art methods
PAPAYA and FedAsync, respectively. Our dynamic aggregation method allows
reliable global model training despite limiting client resources and
statistical data heterogeneity. This improves robustness and scalability for
real-world FL deployments.
\\ ( https://arxiv.org/abs/2401.13366 ,  2775kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13391
Date: Wed, 24 Jan 2024 11:41:30 GMT   (1642kb,D)

Title: Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely
  on between-group metrics
Authors: Sofie Goethals, Toon Calders, David Martens
Categories: cs.LG
\\
  Artificial Intelligence (AI) finds widespread applications across various
domains, sparking concerns about fairness in its deployment. While fairness in
AI remains a central concern, the prevailing discourse often emphasizes
outcome-based metrics without a nuanced consideration of the differential
impacts within subgroups. Bias mitigation techniques do not only affect the
ranking of pairs of instances across sensitive groups, but often also
significantly affect the ranking of instances within these groups. Such changes
are hard to explain and raise concerns regarding the validity of the
intervention. Unfortunately, these effects largely remain under the radar in
the accuracy-fairness evaluation framework that is usually applied. This paper
challenges the prevailing metrics for assessing bias mitigation techniques,
arguing that they do not take into account the changes within-groups and that
the resulting prediction labels fall short of reflecting real-world scenarios.
We propose a paradigm shift: initially, we should focus on generating the most
precise ranking for each subgroup. Following this, individuals should be chosen
from these rankings to meet both fairness standards and practical
considerations.
\\ ( https://arxiv.org/abs/2401.13391 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13447
Date: Wed, 24 Jan 2024 13:42:24 GMT   (1023kb,D)

Title: Symbolic Equation Solving via Reinforcement Learning
Authors: Lennart Dabelow and Masahito Ueda
Categories: cs.LG cs.MS cs.SC
Comments: 12 pages, 4 figures + appendices 17 pages, 1 figure, 16 tables
\\
  Machine-learning methods are gradually being adopted in a great variety of
social, economic, and scientific contexts, yet they are notorious for
struggling with exact mathematics. A typical example is computer algebra, which
includes tasks like simplifying mathematical terms, calculating formal
derivatives, or finding exact solutions of algebraic equations. Traditional
software packages for these purposes are commonly based on a huge database of
rules for how a specific operation (e.g., differentiation) transforms a certain
term (e.g., sine function) into another one (e.g., cosine function). Thus far,
these rules have usually needed to be discovered and subsequently programmed by
humans. Focusing on the paradigmatic example of solving linear equations in
symbolic form, we demonstrate how the process of finding elementary
transformation rules and step-by-step solutions can be automated using
reinforcement learning with deep neural networks.
\\ ( https://arxiv.org/abs/2401.13447 ,  1023kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13460
Date: Wed, 24 Jan 2024 14:02:09 GMT   (3085kb,D)

Title: Multi-Agent Diagnostics for Robustness via Illuminated Diversity
Authors: Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder,
  Tim Rockt\"aschel
Categories: cs.LG cs.AI cs.MA
\\
  In the rapidly advancing field of multi-agent systems, ensuring robustness in
unfamiliar and adversarial settings is crucial. Notwithstanding their
outstanding performance in familiar environments, these systems often falter in
new situations due to overfitting during the training phase. This is especially
pronounced in settings where both cooperative and competitive behaviours are
present, encapsulating a dual nature of overfitting and generalisation
challenges. To address this issue, we present Multi-Agent Diagnostics for
Robustness via Illuminated Diversity (MADRID), a novel approach for generating
diverse adversarial scenarios that expose strategic vulnerabilities in
pre-trained multi-agent policies. Leveraging the concepts from open-ended
learning, MADRID navigates the vast space of adversarial settings, employing a
target policy's regret to gauge the vulnerabilities of these settings. We
evaluate the effectiveness of MADRID on the 11vs11 version of Google Research
Football, one of the most complex environments for multi-agent reinforcement
learning. Specifically, we employ MADRID for generating a diverse array of
adversarial settings for TiZero, the state-of-the-art approach which "masters"
the game through 45 days of training on a large-scale distributed
infrastructure. We expose key shortcomings in TiZero's tactical
decision-making, underlining the crucial importance of rigorous evaluation in
multi-agent systems.
\\ ( https://arxiv.org/abs/2401.13460 ,  3085kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13530
Date: Wed, 24 Jan 2024 15:35:44 GMT   (58kb)

Title: Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein
  Probabilistic Space
Authors: Mingyang Yi, Bohan Wang
Categories: cs.LG
\\
  Recently, optimization on the Riemannian manifold has provided new insights
to the optimization community. In this regard, the manifold taken as the
probability measure metric space equipped with the second-order Wasserstein
distance is of particular interest, since optimization on it can be linked to
practical sampling processes. In general, the oracle (continuous) optimization
method on Wasserstein space is Riemannian gradient flow (i.e., Langevin
dynamics when minimizing KL divergence). In this paper, we aim to enrich the
continuous optimization methods in the Wasserstein space by extending the
gradient flow into the stochastic gradient descent (SGD) flow and stochastic
variance reduction gradient (SVRG) flow. The two flows on Euclidean space are
standard stochastic optimization methods, while their Riemannian counterparts
are not explored yet. By leveraging the structures in Wasserstein space, we
construct a stochastic differential equation (SDE) to approximate the discrete
dynamics of desired stochastic methods in the corresponded random vector space.
Then, the flows of probability measures are naturally obtained by applying
Fokker-Planck equation to such SDE. Furthermore, the convergence rates of the
proposed Riemannian stochastic flows are proven, and they match the results in
Euclidean space.
\\ ( https://arxiv.org/abs/2401.13530 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13544
Date: Wed, 24 Jan 2024 16:02:14 GMT   (27080kb,D)

Title: Beyond Concept Bottleneck Models: How to Make Black Boxes Intervenable?
Authors: Ri\v{c}ards Marcinkevi\v{c}s, Sonia Laguna, Moritz Vandenhirtz, Julia
  E. Vogt
Categories: cs.LG stat.ML
\\
  Recently, interpretable machine learning has re-explored concept bottleneck
models (CBM), comprising step-by-step prediction of the high-level concepts
from the raw features and the target variable from the predicted concepts. A
compelling advantage of this model class is the user's ability to intervene on
the predicted concept values, affecting the model's downstream output. In this
work, we introduce a method to perform such concept-based interventions on
already-trained neural networks, which are not interpretable by design, given
an annotated validation set. Furthermore, we formalise the model's
intervenability as a measure of the effectiveness of concept-based
interventions and leverage this definition to fine-tune black-box models.
Empirically, we explore the intervenability of black-box classifiers on
synthetic tabular and natural image benchmarks. We demonstrate that fine-tuning
improves intervention effectiveness and often yields better-calibrated
predictions. To showcase the practical utility of the proposed techniques, we
apply them to deep chest X-ray classifiers and show that fine-tuned black boxes
can be as intervenable and more performant than CBMs.
\\ ( https://arxiv.org/abs/2401.13544 ,  27080kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13558
Date: Wed, 24 Jan 2024 16:14:38 GMT   (9335kb,D)

Title: Task structure and nonlinearity jointly determine learned
  representational geometry
Authors: Matteo Alleman, Jack W Lindsey, Stefano Fusi
Categories: cs.LG
\\
  The utility of a learned neural representation depends on how well its
geometry supports performance in downstream tasks. This geometry depends on the
structure of the inputs, the structure of the target outputs, and the
architecture of the network. By studying the learning dynamics of networks with
one hidden layer, we discovered that the network's activation function has an
unexpectedly strong impact on the representational geometry: Tanh networks tend
to learn representations that reflect the structure of the target outputs,
while ReLU networks retain more information about the structure of the raw
inputs. This difference is consistently observed across a broad class of
parameterized tasks in which we modulated the degree of alignment between the
geometry of the task inputs and that of the task labels. We analyzed the
learning dynamics in weight space and show how the differences between the
networks with Tanh and ReLU nonlinearities arise from the asymmetric asymptotic
behavior of ReLU, which leads feature neurons to specialize for different
regions of input space. By contrast, feature neurons in Tanh networks tend to
inherit the task label structure. Consequently, when the target outputs are low
dimensional, Tanh networks generate neural representations that are more
disentangled than those obtained with a ReLU nonlinearity. Our findings shed
light on the interplay between input-output geometry, nonlinearity, and learned
representations in neural networks.
\\ ( https://arxiv.org/abs/2401.13558 ,  9335kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13586
Date: Wed, 24 Jan 2024 16:51:23 GMT   (9867kb,D)

Title: Prompt Weight Experiments for LLM Instruction Fine-Tuning
Authors: Mathew Huerta-Enochian
Categories: cs.LG cs.AI cs.CL
Comments: 5 pages of content. 5 pages for limitations, acknowledgments,
  references, and appendix. 3 figures
\\
  We present a small study analyzing how prompt token classification loss
weighting (PLW) affects the performance of 7B-size LLaMA models fine-tuned on
instruction tasks. We recreated Stanford's Alpaca experiment with both LLaMA 1
and LLaMA 2 using multiple instruction datasets. We found that models
fine-tuned on our short-completion dataset have a negative quadratic
relationship with PLW while models fine-tuned on long-completion datasets were
unaffected by PLW.
\\ ( https://arxiv.org/abs/2401.13586 ,  9867kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13649
Date: Wed, 24 Jan 2024 18:35:21 GMT   (8665kb,D)

Title: VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web
  Tasks
Authors: Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim,
  Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried
Categories: cs.LG cs.CL cs.CV
Comments: 24 pages. Project page: https://jykoh.com/vwa
\\
  Autonomous agents capable of planning, reasoning, and executing actions on
the web offer a promising avenue for automating computer tasks. However, the
majority of existing benchmarks primarily focus on text-based agents,
neglecting many natural tasks that require visual information to effectively
solve. Given that most computer interfaces cater to human perception, visual
information often augments textual data in ways that text-only models struggle
to harness effectively. To bridge this gap, we introduce VisualWebArena, a
benchmark designed to assess the performance of multimodal web agents on
realistic \textit{visually grounded tasks}. VisualWebArena comprises of a set
of diverse and complex web-based tasks that evaluate various capabilities of
autonomous multimodal agents. To perform on this benchmark, agents need to
accurately process image-text inputs, interpret natural language instructions,
and execute actions on websites to accomplish user-defined objectives. We
conduct an extensive evaluation of state-of-the-art LLM-based autonomous
agents, including several multimodal models. Through extensive quantitative and
qualitative analysis, we identify several limitations of text-only LLM agents,
and reveal gaps in the capabilities of state-of-the-art multimodal language
agents. VisualWebArena provides a framework for evaluating multimodal
autonomous language agents, and offers insights towards building stronger
autonomous agents for the web. Our code, baseline models, and data is publicly
available at https://jykoh.com/vwa.
\\ ( https://arxiv.org/abs/2401.13649 ,  8665kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13652
Date: Wed, 24 Jan 2024 18:44:14 GMT   (2952kb,D)

Title: Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity
  Detectors
Authors: Francesco Della Santa and Sandra Pieraccini
Categories: cs.LG cs.AI cs.NA math.NA
MSC-class: 68T07, 03D32, 65D40
\\
  In this paper, we present a novel approach for detecting the discontinuity
interfaces of a discontinuous function. This approach leverages Graph-Informed
Neural Networks (GINNs) and sparse grids to address discontinuity detection
also in domains of dimension larger than 3. GINNs, trained to identify troubled
points on sparse grids, exploit graph structures built on the grids to achieve
efficient and accurate discontinuity detection performances. We also introduce
a recursive algorithm for general sparse grid-based detectors, characterized by
convergence properties and easy applicability. Numerical experiments on
functions with dimensions n = 2 and n = 4 demonstrate the efficiency and robust
generalization of GINNs in detecting discontinuity interfaces. Notably, the
trained GINNs offer portability and versatility, allowing integration into
various algorithms and sharing among users.
\\ ( https://arxiv.org/abs/2401.13652 ,  2952kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13657
Date: Wed, 24 Jan 2024 18:49:30 GMT   (368kb,D)

Title: Inadequacy of common stochastic neural networks for reliable clinical
  decision support
Authors: Adrian Lindenmeyer, Malte Blattmann, Stefan Franke, Thomas Neumuth,
  Daniel Schneider
Categories: cs.LG cs.AI
Comments: Keywords: probabilistic inference, uncertainty estimation,
  uncertainty quantification, epistemic uncertainty, clinical prognosis,
  electronic health records
\\
  Widespread adoption of AI for medical decision making is still hindered due
to ethical and safety-related concerns. For AI-based decision support systems
in healthcare settings it is paramount to be reliable and trustworthy. Common
deep learning approaches, however, have the tendency towards overconfidence
under data shift. Such inappropriate extrapolation beyond evidence-based
scenarios may have dire consequences. This highlights the importance of
reliable estimation of local uncertainty and its communication to the end user.
While stochastic neural networks have been heralded as a potential solution to
these issues, this study investigates their actual reliability in clinical
applications. We centered our analysis on the exemplary use case of mortality
prediction for ICU hospitalizations using EHR from MIMIC3 study. For
predictions on the EHR time series, Encoder-Only Transformer models were
employed. Stochasticity of model functions was achieved by incorporating common
methods such as Bayesian neural network layers and model ensembles. Our models
achieve state of the art performance in terms of discrimination performance
(AUC ROC: 0.868+-0.011, AUC PR: 0.554+-0.034) and calibration on the mortality
prediction benchmark. However, epistemic uncertainty is critically
underestimated by the selected stochastic deep learning methods. A heuristic
proof for the responsible collapse of the posterior distribution is provided.
Our findings reveal the inadequacy of commonly used stochastic deep learning
approaches to reliably recognize OoD samples. In both methods, unsubstantiated
model confidence is not prevented due to strongly biased functional posteriors,
rendering them inappropriate for reliable clinical decision support. This
highlights the need for approaches with more strictly enforced or inherent
distance-awareness to known data points, e.g., using kernel-based techniques.
\\ ( https://arxiv.org/abs/2401.13657 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13662
Date: Wed, 24 Jan 2024 18:56:53 GMT   (327kb,D)

Title: The Definitive Guide to Policy Gradients in Deep Reinforcement Learning:
  Theory, Algorithms and Implementations
Authors: Matthias Lehmann
Categories: cs.LG cs.AI
\\
  In recent years, various powerful policy gradient algorithms have been
proposed in deep reinforcement learning. While all these algorithms build on
the Policy Gradient Theorem, the specific design choices differ significantly
across algorithms. We provide a holistic overview of on-policy policy gradient
algorithms to facilitate the understanding of both their theoretical
foundations and their practical implementations. In this overview, we include a
detailed proof of the continuous version of the Policy Gradient Theorem,
convergence results and a comprehensive discussion of practical algorithms. We
compare the most prominent algorithms on continuous control environments and
provide insights on the benefits of regularization. All code is available at
https://github.com/Matt00n/PolicyGradientsJax.
\\ ( https://arxiv.org/abs/2401.13662 ,  327kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.12999 (*cross-listing*)
Date: Mon, 22 Jan 2024 09:16:41 GMT   (264kb,D)

Title: Quantum-Inspired Machine Learning for Molecular Docking
Authors: Runqiu Shu, Bowen Liu, Zhaoping Xiong, Xiaopeng Cui, Yunting Li, Wei
  Cui, Man-Hong Yung and Nan Qiao
Categories: physics.chem-ph cs.AI cs.LG
\\
  Molecular docking is an important tool for structure-based drug design,
accelerating the efficiency of drug development. Complex and dynamic binding
processes between proteins and small molecules require searching and sampling
over a wide spatial range. Traditional docking by searching for possible
binding sites and conformations is computationally complex and results poorly
under blind docking. Quantum-inspired algorithms combining quantum properties
and annealing show great advantages in solving combinatorial optimization
problems. Inspired by this, we achieve an improved in blind docking by using
quantum-inspired combined with gradients learned by deep learning in the
encoded molecular space. Numerical simulation shows that our method outperforms
traditional docking algorithms and deep learning-based algorithms over 10\%.
Compared to the current state-of-the-art deep learning-based docking algorithm
DiffDock, the success rate of Top-1 (RMSD<2) achieves an improvement from 33\%
to 35\% in our same setup. In particular, a 6\% improvement is realized in the
high-precision region(RMSD<1) on molecules data unseen in DiffDock, which
demonstrates the well-generalized of our method.
\\ ( https://arxiv.org/abs/2401.12999 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13001 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:33:11 GMT   (27954kb,D)

Title: PatternPortrait: Draw Me Like One of Your Scribbles
Authors: Sabine Wieluch, Friedhelm Schwenker
Categories: cs.GR cs.AI cs.LG
\\
  This paper introduces a process for generating abstract portrait drawings
from pictures. Their unique style is created by utilizing single freehand
pattern sketches as references to generate unique patterns for shading. The
method involves extracting facial and body features from images and
transforming them into vector lines. A key aspect of the research is the
development of a graph neural network architecture designed to learn sketch
stroke representations in vector form, enabling the generation of diverse
stroke variations. The combination of these two approaches creates joyful
abstract drawings that are realized via a pen plotter. The presented process
garnered positive feedback from an audience of approximately 280 participants.
\\ ( https://arxiv.org/abs/2401.13001 ,  27954kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13002 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:52:55 GMT   (906kb,D)

Title: Theorem Discovery Amongst Cyclic Polygons
Authors: Philip Todd (Saltire Software)
Categories: cs.CG cs.AI cs.MS
Comments: In Proceedings ADG 2023, arXiv:2401.10725
Journal-ref: EPTCS 398, 2024, pp. 153-164
DOI: 10.4204/EPTCS.398.18
\\
  We examine a class of geometric theorems on cyclic 2n-gons. We prove that if
we take n disjoint pairs of sides, each pair separated by an even number of
polygon sides, then there is a linear combination of the angles between those
sides which is constant. We present a formula for the linear combination, which
provides a theorem statement in terms of those angles. We describe a program
which uses this result to generate new geometry proof problems and their
solutions.
\\ ( https://arxiv.org/abs/2401.13002 ,  906kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13049 (*cross-listing*)
Date: Tue, 23 Jan 2024 19:17:20 GMT   (28985kb,D)

Title: CIS-UNet: Multi-Class Segmentation of the Aorta in Computed Tomography
  Angiography via Context-Aware Shifted Window Self-Attention
Authors: Muhammad Imran, Jonathan R Krebs, Veera Rajasekhar Reddy Gopu, Brian
  Fazzone, Vishal Balaji Sivaraman, Amarjeet Kumar, Chelsea Viscardi, Robert
  Evans Heithaus, Benjamin Shickel, Yuyin Zhou, Michol A Cooper, Wei Shao
Categories: eess.IV cs.AI cs.CV cs.GT cs.LG
\\
  Advancements in medical imaging and endovascular grafting have facilitated
minimally invasive treatments for aortic diseases. Accurate 3D segmentation of
the aorta and its branches is crucial for interventions, as inaccurate
segmentation can lead to erroneous surgical planning and endograft
construction. Previous methods simplified aortic segmentation as a binary image
segmentation problem, overlooking the necessity of distinguishing between
individual aortic branches. In this paper, we introduce Context Infused
Swin-UNet (CIS-UNet), a deep learning model designed for multi-class
segmentation of the aorta and thirteen aortic branches. Combining the strengths
of Convolutional Neural Networks (CNNs) and Swin transformers, CIS-UNet adopts
a hierarchical encoder-decoder structure comprising a CNN encoder, symmetric
decoder, skip connections, and a novel Context-aware Shifted Window
Self-Attention (CSW-SA) as the bottleneck block. Notably, CSW-SA introduces a
unique utilization of the patch merging layer, distinct from conventional Swin
transformers. It efficiently condenses the feature map, providing a global
spatial context and enhancing performance when applied at the bottleneck layer,
offering superior computational efficiency and segmentation accuracy compared
to the Swin transformers. We trained our model on computed tomography (CT)
scans from 44 patients and tested it on 15 patients. CIS-UNet outperformed the
state-of-the-art SwinUNetR segmentation model, which is solely based on Swin
transformers, by achieving a superior mean Dice coefficient of 0.713 compared
to 0.697, and a mean surface distance of 2.78 mm compared to 3.39 mm.
CIS-UNet's superior 3D aortic segmentation offers improved precision and
optimization for planning endovascular treatments. Our dataset and code will be
publicly available.
\\ ( https://arxiv.org/abs/2401.13049 ,  28985kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13068 (*cross-listing*)
Date: Tue, 23 Jan 2024 19:48:34 GMT   (336kb,D)

Title: Local Background Estimation for Improved Gas Plume Identification in
  Hyperspectral Images
Authors: Scout Jarman, Zigfried Hampel-Arias, Adra Carr, Kevin R. Moon
Categories: cs.CV cs.AI
Comments: Submitted to International Geoscience and Remote Sensing Symposium
  (IGARSS), 2024. 5 pages, 2 figures
\\
  Deep learning identification models have shown promise for identifying gas
plumes in Longwave IR hyperspectral images of urban scenes, particularly when a
large library of gases are being considered. Because many gases have similar
spectral signatures, it is important to properly estimate the signal from a
detected plume. Typically, a scene's global mean spectrum and covariance matrix
are estimated to whiten the plume's signal, which removes the background's
signature from the gas signature. However, urban scenes can have many different
background materials that are spatially and spectrally heterogeneous. This can
lead to poor identification performance when the global background estimate is
not representative of a given local background material. We use image
segmentation, along with an iterative background estimation algorithm, to
create local estimates for the various background materials that reside
underneath a gas plume. Our method outperforms global background estimation on
a set of simulated and real gas plumes. This method shows promise in increasing
deep learning identification confidence, while being simple and easy to tune
when considering diverse plumes.
\\ ( https://arxiv.org/abs/2401.13068 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13081 (*cross-listing*)
Date: Tue, 23 Jan 2024 20:26:52 GMT   (858kb,D)

Title: Free Form Medical Visual Question Answering in Radiology
Authors: Abhishek Narayanan, Rushabh Musthyala, Rahul Sankar, Anirudh Prasad
  Nistala, Pranav Singh and Jacopo Cirrone
Categories: cs.CV cs.AI
Comments: 6 pages and 4 figures
\\
  Visual Question Answering (VQA) in the medical domain presents a unique,
interdisciplinary challenge, combining fields such as Computer Vision, Natural
Language Processing, and Knowledge Representation. Despite its importance,
research in medical VQA has been scant, only gaining momentum since 2018.
Addressing this gap, our research delves into the effective representation of
radiology images and the joint learning of multimodal representations,
surpassing existing methods. We innovatively augment the SLAKE dataset,
enabling our model to respond to a more diverse array of questions, not limited
to the immediate content of radiology or pathology images. Our model achieves a
top-1 accuracy of 79.55\% with a less complex architecture, demonstrating
comparable performance to current state-of-the-art models. This research not
only advances medical VQA but also opens avenues for practical applications in
diagnostic settings.
\\ ( https://arxiv.org/abs/2401.13081 ,  858kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13097 (*cross-listing*)
Date: Tue, 23 Jan 2024 21:22:06 GMT   (3112kb)

Title: Digital Divides in Scene Recognition: Uncovering Socioeconomic Biases in
  Deep Learning Systems
Authors: Michelle R. Greene, Mariam Josyula, Wentao Si and Jennifer A. Hart
Categories: cs.CV cs.AI
Comments: 20 pages, 3 figures, 3 tables
MSC-class: 68-02
ACM-class: I.2.m
\\
  Computer-based scene understanding has influenced fields ranging from urban
planning to autonomous vehicle performance, yet little is known about how well
these technologies work across social differences. We investigate the biases of
deep convolutional neural networks (dCNNs) in scene classification, using
nearly one million images from global and US sources, including user-submitted
home photographs and Airbnb listings. We applied statistical models to quantify
the impact of socioeconomic indicators such as family income, Human Development
Index (HDI), and demographic factors from public data sources (CIA and US
Census) on dCNN performance. Our analyses revealed significant socioeconomic
bias, where pretrained dCNNs demonstrated lower classification accuracy, lower
classification confidence, and a higher tendency to assign labels that could be
offensive when applied to homes (e.g., "ruin", "slum"), especially in images
from homes with lower socioeconomic status (SES). This trend is consistent
across two datasets of international images and within the diverse economic and
racial landscapes of the United States. This research contributes to
understanding biases in computer vision, emphasizing the need for more
inclusive and representative training datasets. By mitigating the bias in the
computer vision pipelines, we can ensure fairer and more equitable outcomes for
applied computer vision, including home valuation and smart home security
systems. There is urgency in addressing these biases, which can significantly
impact critical decisions in urban development and resource allocation. Our
findings also motivate the development of AI systems that better understand and
serve diverse communities, moving towards technology that equitably benefits
all sectors of society.
\\ ( https://arxiv.org/abs/2401.13097 ,  3112kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13138 (*cross-listing*)
Date: Tue, 23 Jan 2024 23:18:33 GMT   (718kb,D)

Title: Visibility into AI Agents
Authors: Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond,
  Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt,
  Lennart Heim, Markus Anderljung
Categories: cs.CY cs.AI
Comments: Under review
\\
  Increased delegation of commercial, scientific, governmental, and personal
activities to AI agents -- systems capable of pursuing complex goals with
limited supervision -- may exacerbate existing societal risks and introduce new
risks. Understanding and mitigating these risks involves critically evaluating
existing governance structures, revising and adapting these structures where
needed, and ensuring accountability of key stakeholders. Information about
where, why, how, and by whom certain AI agents are used, which we refer to as
\textbf{visibility}, is critical to these objectives. In this paper, we assess
three categories of measures to increase visibility into AI agents:
\textbf{agent identifiers}, \textbf{real-time monitoring}, and \textbf{activity
logging}. For each, we outline potential implementations that vary in
intrusiveness and informativeness. We analyze how the measures apply across a
spectrum of centralized through decentralized deployment contexts, accounting
for various actors in the supply chain including hardware and software service
providers. Finally, we discuss the implications of our measures for privacy and
concentration of power. Further work into understanding the measures and
mitigating their negative impacts can help to build a foundation for the
governance of AI agents.
\\ ( https://arxiv.org/abs/2401.13138 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13193 (*cross-listing*)
Date: Wed, 24 Jan 2024 02:42:50 GMT   (9332kb,D)

Title: Catch-Up Mix: Catch-Up Class for Struggling Filters in CNN
Authors: Minsoo Kang, Minkoo Kang, Suhyun Kim
Categories: cs.CV cs.AI
Comments: Published at AAAI2024, Equal contribution of first two authors
\\
  Deep learning has made significant advances in computer vision, particularly
in image classification tasks. Despite their high accuracy on training data,
deep learning models often face challenges related to complexity and
overfitting. One notable concern is that the model often relies heavily on a
limited subset of filters for making predictions. This dependency can result in
compromised generalization and an increased vulnerability to minor variations.
While regularization techniques like weight decay, dropout, and data
augmentation are commonly used to address this issue, they may not directly
tackle the reliance on specific filters. Our observations reveal that the heavy
reliance problem gets severe when slow-learning filters are deprived of
learning opportunities due to fast-learning filters. Drawing inspiration from
image augmentation research that combats over-reliance on specific image
regions by removing and replacing parts of images, our idea is to mitigate the
problem of over-reliance on strong filters by substituting highly activated
features. To this end, we present a novel method called Catch-up Mix, which
provides learning opportunities to a wide range of filters during training,
focusing on filters that may lag behind. By mixing activation maps with
relatively lower norms, Catch-up Mix promotes the development of more diverse
representations and reduces reliance on a small subset of filters. Experimental
results demonstrate the superiority of our method in various vision
classification datasets, providing enhanced robustness.
\\ ( https://arxiv.org/abs/2401.13193 ,  9332kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13201 (*cross-listing*)
Date: Wed, 24 Jan 2024 03:07:26 GMT   (5786kb)

Title: MLLMReID: Multimodal Large Language Model-based Person Re-identification
Authors: Shan Yang, Yongfei Zhang
Categories: cs.CV cs.AI cs.CL
\\
  Multimodal large language models (MLLM) have achieved satisfactory results in
many tasks. However, their performance in the task of person re-identification
(ReID) has not been explored to date. This paper will investigate how to adapt
them for the task of ReID. An intuitive idea is to fine-tune MLLM with ReID
image-text datasets, and then use their visual encoder as a backbone for ReID.
However, there still exist two apparent issues: (1) Designing instructions for
ReID, MLLMs may overfit specific instructions, and designing a variety of
instructions will lead to higher costs. (2) Latent image feature vectors from
LLMs are not involved in loss computation. Instructional learning, aligning
image-text features, results in indirect optimization and a learning objective
that inadequately utilizes features, limiting effectiveness in person feature
learning. To address these problems, this paper proposes MLLMReID: Multimodal
Large Language Model-based ReID. Firstly, we proposed Common Instruction, a
simple approach that leverages the essence ability of LLMs to continue writing,
avoiding complex and diverse instruction design. Secondly, we proposed
DirectReID, which effectively employs the latent image feature vectors of
images outputted by LLMs in ReID tasks. The experimental results demonstrate
the superiority of our method. We will open-source the code on GitHub.
\\ ( https://arxiv.org/abs/2401.13201 ,  5786kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13205 (*cross-listing*)
Date: Wed, 24 Jan 2024 03:26:34 GMT   (4354kb,D)

Title: Boosting the Transferability of Adversarial Examples via Local Mixup and
  Adaptive Step Size
Authors: Junlin Liu and Xinchen Lyu
Categories: cs.CV cs.AI
\\
  Adversarial examples are one critical security threat to various visual
applications, where injected human-imperceptible perturbations can confuse the
output.Generating transferable adversarial examples in the black-box setting is
crucial but challenging in practice. Existing input-diversity-based methods
adopt different image transformations, but may be inefficient due to
insufficient input diversity and an identical perturbation step size. Motivated
by the fact that different image regions have distinctive weights in
classification, this paper proposes a black-box adversarial generative
framework by jointly designing enhanced input diversity and adaptive step
sizes. We design local mixup to randomly mix a group of transformed adversarial
images, strengthening the input diversity. For precise adversarial generation,
we project the perturbation into the $tanh$ space to relax the boundary
constraint. Moreover, the step sizes of different regions can be dynamically
adjusted by integrating a second-order momentum.Extensive experiments on
ImageNet validate that our framework can achieve superior transferability
compared to state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2401.13205 ,  4354kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13212 (*cross-listing*)
Date: Wed, 24 Jan 2024 03:49:51 GMT   (395kb,D)

Title: AdCorDA: Classifier Refinement via Adversarial Correction and Domain
  Adaptation
Authors: Lulan Shen, Ali Edalati, Brett Meyer, Warren Gross, James J. Clark
Categories: cs.CV cs.AI cs.LG
\\
  This paper describes a simple yet effective technique for refining a
pretrained classifier network. The proposed AdCorDA method is based on
modification of the training set and making use of the duality between network
weights and layer inputs. We call this input space training. The method
consists of two stages - adversarial correction followed by domain adaptation.
Adversarial correction uses adversarial attacks to correct incorrect
training-set classifications. The incorrectly classified samples of the
training set are removed and replaced with the adversarially corrected samples
to form a new training set, and then, in the second stage, domain adaptation is
performed back to the original training set. Extensive experimental validations
show significant accuracy boosts of over 5% on the CIFAR-100 dataset. The
technique can be straightforwardly applied to refinement of weight-quantized
neural networks, where experiments show substantial enhancement in performance
over the baseline. The adversarial correction technique also results in
enhanced robustness to adversarial attacks.
\\ ( https://arxiv.org/abs/2401.13212 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13214 (*cross-listing*)
Date: Wed, 24 Jan 2024 03:56:33 GMT   (7146kb,D)

Title: AMANet: Advancing SAR Ship Detection with Adaptive Multi-Hierarchical
  Attention Network
Authors: Xiaolin Ma, Junkai Cheng, Aihua Li, Yuhua Zhang, Zhilong Lin
Categories: cs.CV cs.AI cs.LG
Comments: 11 pages, 7 figures
MSC-class: 68T45
ACM-class: I.2.10
\\
  Recently, methods based on deep learning have been successfully applied to
ship detection for synthetic aperture radar (SAR) images. Despite the
development of numerous ship detection methodologies, detecting small and
coastal ships remains a significant challenge due to the limited features and
clutter in coastal environments. For that, a novel adaptive multi-hierarchical
attention module (AMAM) is proposed to learn multi-scale features and
adaptively aggregate salient features from various feature layers, even in
complex environments. Specifically, we first fuse information from adjacent
feature layers to enhance the detection of smaller targets, thereby achieving
multi-scale feature enhancement. Then, to filter out the adverse effects of
complex backgrounds, we dissect the previously fused multi-level features on
the channel, individually excavate the salient regions, and adaptively
amalgamate features originating from different channels. Thirdly, we present a
novel adaptive multi-hierarchical attention network (AMANet) by embedding the
AMAM between the backbone network and the feature pyramid network (FPN).
Besides, the AMAM can be readily inserted between different frameworks to
improve object detection. Lastly, extensive experiments on two large-scale SAR
ship detection datasets demonstrate that our AMANet method is superior to
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.13214 ,  7146kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13219 (*cross-listing*)
Date: Wed, 24 Jan 2024 04:16:28 GMT   (2733kb,D)

Title: TEPI: Taxonomy-aware Embedding and Pseudo-Imaging for Scarcely-labeled
  Zero-shot Genome Classification
Authors: Sathyanarayanan Aakur, Vishalini R. Laguduva, Priyadharsini
  Ramamurthy, Akhilesh Ramachandran
Categories: q-bio.GN cs.AI cs.LG
Comments: Accepted to IEEE JBHI
\\
  A species' genetic code or genome encodes valuable evolutionary, biological,
and phylogenetic information that aids in species recognition, taxonomic
classification, and understanding genetic predispositions like drug resistance
and virulence. However, the vast number of potential species poses significant
challenges in developing a general-purpose whole genome classification tool.
Traditional bioinformatics tools have made notable progress but lack
scalability and are computationally expensive. Machine learning-based
frameworks show promise but must address the issue of large classification
vocabularies with long-tail distributions. In this study, we propose addressing
this problem through zero-shot learning using TEPI, Taxonomy-aware Embedding
and Pseudo-Imaging. We represent each genome as pseudo-images and map them to a
taxonomy-aware embedding space for reasoning and classification. This embedding
space captures compositional and phylogenetic relationships of species,
enabling predictions in extensive search spaces. We evaluate TEPI using two
rigorous zero-shot settings and demonstrate its generalization capabilities
qualitatively on curated, large-scale, publicly sourced data.
\\ ( https://arxiv.org/abs/2401.13219 ,  2733kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13262 (*cross-listing*)
Date: Wed, 24 Jan 2024 07:09:32 GMT   (149kb,D)

Title: Designing Redistribution Mechanisms for Reducing Transaction Fees in
  Blockchains
Authors: Sankarshan Damle and Manisha Padala and Sujit Gujar
Categories: cs.GT cs.AI cs.CR
Comments: Full Paper (AAMAS '24)
\\
  Blockchains deploy Transaction Fee Mechanisms (TFMs) to determine which user
transactions to include in blocks and determine their payments (i.e.,
transaction fees). Increasing demand and scarce block resources have led to
high user transaction fees. As these blockchains are a public resource, it may
be preferable to reduce these transaction fees. To this end, we introduce
Transaction Fee Redistribution Mechanisms (TFRMs) -- redistributing VCG
payments collected from such TFM as rebates to minimize transaction fees.
Classic redistribution mechanisms (RMs) achieve this while ensuring Allocative
Efficiency (AE) and User Incentive Compatibility (UIC). Our first result shows
the non-triviality of applying RM in TFMs. More concretely, we prove that it is
impossible to reduce transaction fees when (i) transactions that are not
confirmed do not receive rebates and (ii) the miner can strategically
manipulate the mechanism. Driven by this, we propose \emph{Robust} TFRM
(\textsf{R-TFRM}): a mechanism that compromises on an honest miner's individual
rationality to guarantee strictly positive rebates to the users. We then
introduce \emph{robust} and \emph{rational} TFRM (\textsf{R}$^2$\textsf{-TFRM})
that uses trusted on-chain randomness that additionally guarantees miner's
individual rationality (in expectation) and strictly positive rebates. Our
results show that TFRMs provide a promising new direction for reducing
transaction fees in public blockchains.
\\ ( https://arxiv.org/abs/2401.13262 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13270 (*cross-listing*)
Date: Wed, 24 Jan 2024 07:22:05 GMT   (2186kb,D)

Title: Audio-Infused Automatic Image Colorization by Exploiting Audio Scene
  Semantics
Authors: Pengcheng Zhao, Yanxiang Chen, Yang Zhao, Wei Jia, Zhao Zhang,
  Ronggang Wang and Richang Hong
Categories: cs.CV cs.AI
\\
  Automatic image colorization is inherently an ill-posed problem with
uncertainty, which requires an accurate semantic understanding of scenes to
estimate reasonable colors for grayscale images. Although recent
interaction-based methods have achieved impressive performance, it is still a
very difficult task to infer realistic and accurate colors for automatic
colorization. To reduce the difficulty of semantic understanding of grayscale
scenes, this paper tries to utilize corresponding audio, which naturally
contains extra semantic information about the same scene. Specifically, a novel
audio-infused automatic image colorization (AIAIC) network is proposed, which
consists of three stages. First, we take color image semantics as a bridge and
pretrain a colorization network guided by color image semantics. Second, the
natural co-occurrence of audio and video is utilized to learn the color
semantic correlations between audio and visual scenes. Third, the implicit
audio semantic representation is fed into the pretrained network to finally
realize the audio-guided colorization. The whole process is trained in a
self-supervised manner without human annotation. In addition, an audiovisual
colorization dataset is established for training and testing. Experiments
demonstrate that audio guidance can effectively improve the performance of
automatic colorization, especially for some scenes that are difficult to
understand only from visual modality.
\\ ( https://arxiv.org/abs/2401.13270 ,  2186kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13311 (*cross-listing*)
Date: Wed, 24 Jan 2024 09:07:11 GMT   (30288kb,D)

Title: ConTextual: Evaluating Context-Sensitive Text-Rich Visual Reasoning in
  Large Multimodal Models
Authors: Rohan Wadhawan, Hritik Bansal, Kai-Wei Chang, Nanyun Peng
Categories: cs.CV cs.AI cs.LG
\\
  Recent advancements in AI have led to the development of large multimodal
models (LMMs) capable of processing complex tasks involving joint reasoning
over text and visual content in the image (e.g., navigating maps in public
places). This paper introduces ConTextual, a novel benchmark comprising
instructions designed explicitly to evaluate LMMs' ability to perform
context-sensitive text-rich visual reasoning. ConTextual emphasizes diverse
real-world scenarios (e.g., time-reading, navigation, shopping and more)
demanding a deeper understanding of the interactions between textual and visual
elements. Our findings reveal a significant performance gap of 30.8% between
the best-performing LMM, GPT-4V(ision), and human capabilities using human
evaluation indicating substantial room for improvement in context-sensitive
text-rich visual reasoning. Notably, while GPT-4V excelled in abstract
categories like meme and quote interpretation, its overall performance still
lagged behind humans. In addition to human evaluations, we also employed
automatic evaluation metrics using GPT-4, uncovering similar trends in
performance disparities. We also perform a fine-grained evaluation across
diverse visual contexts and provide qualitative analysis which provides a
robust framework for future advancements in the LMM design.
https://con-textual.github.io/
\\ ( https://arxiv.org/abs/2401.13311 ,  30288kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13315 (*cross-listing*)
Date: Wed, 24 Jan 2024 09:14:33 GMT   (3117kb,D)

Title: Deep Learning for Improved Polyp Detection from Synthetic Narrow-Band
  Imaging
Authors: Mathias Ramm Haugland, Hemin Ali Qadir, Ilangko Balasingham
Categories: eess.IV cs.AI cs.CV
DOI: 10.1117/12.2653048
\\
  To cope with the growing prevalence of colorectal cancer (CRC), screening
programs for polyp detection and removal have proven their usefulness.
Colonoscopy is considered the best-performing procedure for CRC screening. To
ease the examination, deep learning based methods for automatic polyp detection
have been developed for conventional white-light imaging (WLI). Compared with
WLI, narrow-band imaging (NBI) can improve polyp classification during
colonoscopy but requires special equipment. We propose a CycleGAN-based
framework to convert images captured with regular WLI to synthetic NBI (SNBI)
as a pre-processing method for improving object detection on WLI when NBI is
unavailable. This paper first shows that better results for polyp detection can
be achieved on NBI compared to a relatively similar dataset of WLI. Secondly,
experimental results demonstrate that our proposed modality translation can
achieve improved polyp detection on SNBI images generated from WLI compared to
the original WLI. This is because our WLI-to-SNBI translation model can enhance
the observation of polyp surface patterns in the generated SNBI images.
\\ ( https://arxiv.org/abs/2401.13315 ,  3117kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13324 (*cross-listing*)
Date: Wed, 24 Jan 2024 09:39:39 GMT   (2521kb)

Title: Information That Matters: Exploring Information Needs of People Affected
  by Algorithmic Decisions
Authors: Timoth\'ee Schmude, Laura Koesten, Torsten M\"oller, Sebastian
  Tschiatschek
Categories: cs.HC cs.AI
Comments: 21 pages excl. references, 3 figures. Supplementary material is
  provided. Manuscript submitted for review
\\
  Explanations of AI systems rarely address the information needs of people
affected by algorithmic decision-making (ADM). This gap between conveyed
information and information that matters to affected stakeholders can impede
understanding and adherence to regulatory frameworks such as the AI Act. To
address this gap, we present the "XAI Novice Question Bank": A catalog of
affected stakeholders' information needs in two ADM use cases (employment
prediction and health monitoring), covering the categories data, system
context, system usage, and system specifications. Information needs were
gathered in an interview study where participants received explanations in
response to their inquiries. Participants further reported their understanding
and decision confidence, showing that while confidence tended to increase after
receiving explanations, participants also met understanding challenges, such as
being unable to tell why their understanding felt incomplete. Explanations
further influenced participants' perceptions of the systems' risks and
benefits, which they confirmed or changed depending on the use case. When risks
were perceived as high, participants expressed particular interest in
explanations about intention, such as why and to what end a system was put in
place. With this work, we aim to support the inclusion of affected stakeholders
into explainability by contributing an overview of information and challenges
relevant to them when deciding on the adoption of ADM systems. We close by
summarizing our findings in a list of six key implications that inform the
design of future explanations for affected stakeholder audiences.
\\ ( https://arxiv.org/abs/2401.13324 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13335 (*cross-listing*)
Date: Wed, 24 Jan 2024 09:59:48 GMT   (1960kb,D)

Title: Full Bayesian Significance Testing for Neural Networks
Authors: Zehua Liu, Zimeng Li, Jingyuan Wang, Yue He
Categories: stat.ML cs.AI cs.LG
Comments: Published as a conference paper at AAAI 2024
\\
  Significance testing aims to determine whether a proposition about the
population distribution is the truth or not given observations. However,
traditional significance testing often needs to derive the distribution of the
testing statistic, failing to deal with complex nonlinear relationships. In
this paper, we propose to conduct Full Bayesian Significance Testing for neural
networks, called \textit{n}FBST, to overcome the limitation in relationship
characterization of traditional approaches. A Bayesian neural network is
utilized to fit the nonlinear and multi-dimensional relationships with small
errors and avoid hard theoretical derivation by computing the evidence value.
Besides, \textit{n}FBST can test not only global significance but also local
and instance-wise significance, which previous testing methods don't focus on.
Moreover, \textit{n}FBST is a general framework that can be extended based on
the measures selected, such as Grad-\textit{n}FBST, LRP-\textit{n}FBST,
DeepLIFT-\textit{n}FBST, LIME-\textit{n}FBST. A range of experiments on both
simulated and real data are conducted to show the advantages of our method.
\\ ( https://arxiv.org/abs/2401.13335 ,  1960kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13346 (*cross-listing*)
Date: Wed, 24 Jan 2024 10:17:59 GMT   (1277kb,D)

Title: Past, Present, Future: A Comprehensive Exploration of AI Use Cases in
  the UMBRELLA IoT Testbed
Authors: Peizheng Li, Ioannis Mavromatis, Aftab Khan
Categories: cs.NI cs.AI
Comments: 6 pgaes, 4 figures. This work has been accepted by PerCom TrustSense
  workshop 2024
\\
  UMBRELLA is a large-scale, open-access Internet of Things (IoT) ecosystem
incorporating over 200 multi-sensor multi-wireless nodes, 20 collaborative
robots, and edge-intelligence-enabled devices. This paper provides a guide to
the implemented and prospective artificial intelligence (AI) capabilities of
UMBRELLA in real-world IoT systems. Four existing UMBRELLA applications are
presented in detail: 1) An automated streetlight monitoring for detecting
issues and triggering maintenance alerts; 2) A Digital twin of building
environments providing enhanced air quality sensing with reduced cost; 3) A
large-scale Federated Learning framework for reducing communication overhead;
and 4) An intrusion detection for containerised applications identifying
malicious activities. Additionally, the potential of UMBRELLA is outlined for
future smart city and multi-robot crowdsensing applications enhanced by
semantic communications and multi-agent planning. Finally, to realise the above
use-cases we discuss the need for a tailored MLOps platform to automate
UMBRELLA model pipelines and establish trust.
\\ ( https://arxiv.org/abs/2401.13346 ,  1277kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13432 (*cross-listing*)
Date: Wed, 24 Jan 2024 13:03:28 GMT   (14616kb,D)

Title: Semi-Supervised Coupled Thin-Plate Spline Model for Rotation Correction
  and Beyond
Authors: Lang Nie, Chunyu Lin, Kang Liao, Shuaicheng Liu, Yao Zhao
Categories: cs.CV cs.AI
\\
  Thin-plate spline (TPS) is a principal warp that allows for representing
elastic, nonlinear transformation with control point motions. With the increase
of control points, the warp becomes increasingly flexible but usually
encounters a bottleneck caused by undesired issues, e.g., content distortion.
In this paper, we explore generic applications of TPS in single-image-based
warping tasks, such as rotation correction, rectangling, and portrait
correction. To break this bottleneck, we propose the coupled thin-plate spline
model (CoupledTPS), which iteratively couples multiple TPS with limited control
points into a more flexible and powerful transformation. Concretely, we first
design an iterative search to predict new control points according to the
current latent condition. Then, we present the warping flow as a bridge for the
coupling of different TPS transformations, effectively eliminating
interpolation errors caused by multiple warps. Besides, in light of the
laborious annotation cost, we develop a semi-supervised learning scheme to
improve warping quality by exploiting unlabeled data. It is formulated through
dual transformation between the searched control points of unlabeled data and
its graphic augmentation, yielding an implicit correction consistency
constraint. Finally, we collect massive unlabeled data to exhibit the benefit
of our semi-supervised scheme in rotation correction. Extensive experiments
demonstrate the superiority and universality of CoupledTPS over the existing
state-of-the-art (SoTA) solutions for rotation correction and beyond. The code
and data will be available at https://github.com/nie-lang/CoupledTPS.
\\ ( https://arxiv.org/abs/2401.13432 ,  14616kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13462 (*cross-listing*)
Date: Wed, 24 Jan 2024 14:04:08 GMT   (35315kb,D)

Title: Growing from Exploration: A self-exploring framework for robots based on
  foundation models
Authors: Shoujie Li and Ran Yu and Tong Wu and JunWen Zhong and Xiao-Ping Zhang
  and Wenbo Ding
Categories: cs.RO cs.AI
Comments: 19 pages
\\
  Intelligent robot is the ultimate goal in the robotics field. Existing works
leverage learning-based or optimization-based methods to accomplish
human-defined tasks. However, the challenge of enabling robots to explore
various environments autonomously remains unresolved. In this work, we propose
a framework named GExp, which enables robots to explore and learn autonomously
without human intervention. To achieve this goal, we devise modules including
self-exploration, knowledge-base-building, and close-loop feedback based on
foundation models. Inspired by the way that infants interact with the world,
GExp encourages robots to understand and explore the environment with a series
of self-generated tasks. During the process of exploration, the robot will
acquire skills from beneficial experiences that are useful in the future. GExp
provides robots with the ability to solve complex tasks through
self-exploration. GExp work is independent of prior interactive knowledge and
human intervention, allowing it to adapt directly to different scenarios,
unlike previous studies that provided in-context examples as few-shot learning.
In addition, we propose a workflow of deploying the real-world robot system
with self-learned skills as an embodied assistant.
\\ ( https://arxiv.org/abs/2401.13462 ,  35315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13481 (*cross-listing*)
Date: Wed, 24 Jan 2024 14:29:39 GMT   (12538kb,D)

Title: How AI Ideas Affect the Creativity, Diversity, and Evolution of Human
  Ideas: Evidence From a Large, Dynamic Experiment
Authors: Joshua Ashkinaze, Julia Mendelsohn, Li Qiwei, Ceren Budak, Eric
  Gilbert
Categories: cs.CY cs.AI cs.CL cs.HC
\\
  Exposure to large language model output is rapidly increasing. How will
seeing AI-generated ideas affect human ideas? We conducted an experiment (800+
participants, 40+ countries) where participants viewed creative ideas that were
from ChatGPT or prior experimental participants and then brainstormed their own
idea. We varied the number of AI-generated examples (none, low, or high
exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic
experiment design -- ideas from prior participants in an experimental condition
are used as stimuli for future participants in the same experimental condition
-- mimics the interdependent process of cultural creation: creative ideas are
built upon prior ideas. Hence, we capture the compounding effects of having
LLMs 'in the culture loop'. We find that high AI exposure (but not low AI
exposure) did not affect the creativity of individual ideas but did increase
the average amount and rate of change of collective idea diversity. AI made
ideas different, not better. There were no main effects of disclosure. We also
found that self-reported creative people were less influenced by knowing an
idea was from AI, and that participants were more likely to knowingly adopt AI
ideas when the task was difficult. Our findings suggest that introducing AI
ideas into society may increase collective diversity but not individual
creativity.
\\ ( https://arxiv.org/abs/2401.13481 ,  12538kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13486 (*cross-listing*)
Date: Wed, 24 Jan 2024 14:34:59 GMT   (7931kb,D)

Title: Separable Physics-Informed Neural Networks for the solution of
  elasticity problems
Authors: Vasiliy A. Es'kin, Danil V. Davydov, Julia V. Gur'eva, Alexey O.
  Malkhanov, Mikhail E. Smorkalov
Categories: math.NA cs.AI cs.LG cs.NA physics.app-ph
MSC-class: 68T07, 65Z05, 65M99
ACM-class: I.2.1; I.2.7; J.2
\\
  A method for solving elasticity problems based on separable physics-informed
neural networks (SPINN) in conjunction with the deep energy method (DEM) is
presented. Numerical experiments have been carried out for a number of problems
showing that this method has a significantly higher convergence rate and
accuracy than the vanilla physics-informed neural networks (PINN) and even
SPINN based on a system of partial differential equations (PDEs). In addition,
using the SPINN in the framework of DEM approach it is possible to solve
problems of the linear theory of elasticity on complex geometries, which is
unachievable with the help of PINNs in frames of partial differential
equations. Considered problems are very close to the industrial problems in
terms of geometry, loading, and material parameters.
\\ ( https://arxiv.org/abs/2401.13486 ,  7931kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13498 (*cross-listing*)
Date: Wed, 24 Jan 2024 14:44:01 GMT   (4279kb,D)

Title: Expressive Acoustic Guitar Sound Synthesis with an Instrument-Specific
  Input Representation and Diffusion Outpainting
Authors: Hounsu Kim, Soonbeom Choi, Juhan Nam
Categories: cs.SD cs.AI cs.LG eess.AS eess.SP
Comments: Accepted to ICASSP 2024
\\
  Synthesizing performing guitar sound is a highly challenging task due to the
polyphony and high variability in expression. Recently, deep generative models
have shown promising results in synthesizing expressive polyphonic instrument
sounds from music scores, often using a generic MIDI input. In this work, we
propose an expressive acoustic guitar sound synthesis model with a customized
input representation to the instrument, which we call guitarroll. We implement
the proposed approach using diffusion-based outpainting which can generate
audio with long-term consistency. To overcome the lack of MIDI/audio-paired
datasets, we used not only an existing guitar dataset but also collected data
from a high quality sample-based guitar synthesizer. Through quantitative and
qualitative evaluations, we show that our proposed model has higher audio
quality than the baseline model and generates more realistic timbre sounds than
the previous leading work.
\\ ( https://arxiv.org/abs/2401.13498 ,  4279kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13555 (*cross-listing*)
Date: Wed, 24 Jan 2024 16:13:26 GMT   (13326kb,D)

Title: Benchmarking the Fairness of Image Upsampling Methods
Authors: Mike Laszkiewicz, Imant Daunhawer, Julia E. Vogt, Asja Fischer,
  Johannes Lederer
Categories: cs.CV cs.AI cs.LG
\\
  Recent years have witnessed a rapid development of deep generative models for
creating synthetic media, such as images and videos. While the practical
applications of these models in everyday tasks are enticing, it is crucial to
assess the inherent risks regarding their fairness. In this work, we introduce
a comprehensive framework for benchmarking the performance and fairness of
conditional generative models. We develop a set of
metrics$\unicode{x2013}$inspired by their supervised fairness
counterparts$\unicode{x2013}$to evaluate the models on their fairness and
diversity. Focusing on the specific application of image upsampling, we create
a benchmark covering a wide variety of modern upsampling methods. As part of
the benchmark, we introduce UnfairFace, a subset of FairFace that replicates
the racial distribution of common large-scale face datasets. Our empirical
study highlights the importance of using an unbiased training set and reveals
variations in how the algorithms respond to dataset imbalances. Alarmingly, we
find that none of the considered methods produces statistically fair and
diverse results.
\\ ( https://arxiv.org/abs/2401.13555 ,  13326kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13611 (*cross-listing*)
Date: Wed, 24 Jan 2024 17:31:07 GMT   (198kb)

Title: Non-Intrusive Speech Intelligibility Prediction for Hearing-Impaired
  Users using Intermediate ASR Features and Human Memory Models
Authors: Rhiannon Mogridge, George Close, Robert Sutherland, Thomas Hain, Jon
  Barker, Stefan Goetze, Anton Ragni
Categories: cs.SD cs.AI eess.AS
Comments: Accepted paper. IEEE International Conference on Acoustics Speech and
  Signal Processing (ICASSP), Seoul, Korea, April 2024
\\
  Neural networks have been successfully used for non-intrusive speech
intelligibility prediction. Recently, the use of feature representations
sourced from intermediate layers of pre-trained self-supervised and
weakly-supervised models has been found to be particularly useful for this
task. This work combines the use of Whisper ASR decoder layer representations
as neural network input features with an exemplar-based, psychologically
motivated model of human memory to predict human intelligibility ratings for
hearing-aid users. Substantial performance improvement over an established
intrusive HASPI baseline system is found, including on enhancement systems and
listeners unseen in the training data, with a root mean squared error of 25.3
compared with the baseline of 28.7.
\\ ( https://arxiv.org/abs/2401.13611 ,  198kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13613 (*cross-listing*)
Date: Wed, 24 Jan 2024 17:35:38 GMT   (1541kb)

Title: Enhancing Image Retrieval : A Comprehensive Study on Photo Search using
  the CLIP Mode
Authors: Naresh Kumar Lahajal and Harini S
Categories: cs.CV cs.AI
\\
  Photo search, the task of retrieving images based on textual queries, has
witnessed significant advancements with the introduction of CLIP (Contrastive
Language-Image Pretraining) model. CLIP leverages a vision-language pre
training approach, wherein it learns a shared representation space for images
and text, enabling cross-modal understanding. This model demonstrates the
capability to understand the semantic relationships between diverse image and
text pairs, allowing for efficient and accurate retrieval of images based on
natural language queries. By training on a large-scale dataset containing
images and their associated textual descriptions, CLIP achieves remarkable
generalization, providing a powerful tool for tasks such as zero-shot learning
and few-shot classification. This abstract summarizes the foundational
principles of CLIP and highlights its potential impact on advancing the field
of photo search, fostering a seamless integration of natural language
understanding and computer vision for improved information retrieval in
multimedia applications
\\ ( https://arxiv.org/abs/2401.13613 ,  1541kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13641 (*cross-listing*)
Date: Wed, 24 Jan 2024 18:10:39 GMT   (20715kb,D)

Title: How Good is ChatGPT at Face Biometrics? A First Look into Recognition,
  Soft Biometrics, and Explainability
Authors: Ivan DeAndres-Tame, Ruben Tolosana, Ruben Vera-Rodriguez, Aythami
  Morales, Julian Fierrez, Javier Ortega-Garcia
Categories: cs.CV cs.AI cs.CY cs.LG
\\
  Large Language Models (LLMs) such as GPT developed by OpenAI, have already
shown astonishing results, introducing quick changes in our society. This has
been intensified by the release of ChatGPT which allows anyone to interact in a
simple conversational way with LLMs, without any experience in the field
needed. As a result, ChatGPT has been rapidly applied to many different tasks
such as code- and song-writer, education, virtual assistants, etc., showing
impressive results for tasks for which it was not trained (zero-shot learning).
  The present study aims to explore the ability of ChatGPT, based on the recent
GPT-4 multimodal LLM, for the task of face biometrics. In particular, we
analyze the ability of ChatGPT to perform tasks such as face verification,
soft-biometrics estimation, and explainability of the results. ChatGPT could be
very valuable to further increase the explainability and transparency of the
automatic decisions in human scenarios. Experiments are carried out in order to
evaluate the performance and robustness of ChatGPT, using popular public
benchmarks and comparing the results with state-of-the-art methods in the
field. The results achieved in this study show the potential of LLMs such as
ChatGPT for face biometrics, especially to enhance explainability. For
reproducibility reasons, we release all the code in GitHub.
\\ ( https://arxiv.org/abs/2401.13641 ,  20715kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13146 (*cross-listing*)
Date: Tue, 23 Jan 2024 23:46:01 GMT   (260kb,D)

Title: Locality enhanced dynamic biasing and sampling strategies for contextual
  ASR
Authors: Md Asif Jalal, Pablo Peso Parada, George Pavlidis, Vasileios
  Moschopoulos, Karthikeyan Saravanan, Chrysovalantis-Giorgos Kontoulis, Jisi
  Zhang, Anastasios Drosou, Gil Ho Lee, Jungin Lee, Seokyeong Jung
Categories: eess.AS cs.CL cs.SD
Comments: Accepted for IEEE ASRU 2023
\\
  Automatic Speech Recognition (ASR) still face challenges when recognizing
time-variant rare-phrases. Contextual biasing (CB) modules bias ASR model
towards such contextually-relevant phrases. During training, a list of biasing
phrases are selected from a large pool of phrases following a sampling
strategy. In this work we firstly analyse different sampling strategies to
provide insights into the training of CB for ASR with correlation plots between
the bias embeddings among various training stages. Secondly, we introduce a
neighbourhood attention (NA) that localizes self attention (SA) to the nearest
neighbouring frames to further refine the CB output. The results show that this
proposed approach provides on average a 25.84% relative WER improvement on
LibriSpeech sets and rare-word evaluation compared to the baseline.
\\ ( https://arxiv.org/abs/2401.13146 ,  260kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13313 (*cross-listing*)
Date: Wed, 24 Jan 2024 09:09:37 GMT   (5848kb,D)

Title: InstructDoc: A Dataset for Zero-Shot Generalization of Visual Document
  Understanding with Instructions
Authors: Ryota Tanaka, Taichi Iki, Kyosuke Nishida, Kuniko Saito, Jun Suzuki
Categories: cs.CV cs.CL
Comments: Accepted by AAAI2024; project page:
  https://github.com/nttmdlab-nlp/InstructDoc
\\
  We study the problem of completing various visual document understanding
(VDU) tasks, e.g., question answering and information extraction, on real-world
documents through human-written instructions. To this end, we propose
InstructDoc, the first large-scale collection of 30 publicly available VDU
datasets, each with diverse instructions in a unified format, which covers a
wide range of 12 tasks and includes open document types/formats. Furthermore,
to enhance the generalization performance on VDU tasks, we design a new
instruction-based document reading and understanding model, InstructDr, that
connects document images, image encoders, and large language models (LLMs)
through a trainable bridging module. Experiments demonstrate that InstructDr
can effectively adapt to new VDU datasets, tasks, and domains via given
instructions and outperforms existing multimodal LLMs and ChatGPT without
specific training.
\\ ( https://arxiv.org/abs/2401.13313 ,  5848kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13478 (*cross-listing*)
Date: Wed, 24 Jan 2024 14:23:12 GMT   (340kb,D)

Title: SciMMIR: Benchmarking Scientific Multi-modal Information Retrieval
Authors: Siwei Wu, Yizhi Li, Kang Zhu, Ge Zhang, Yiming Liang, Kaijing Ma,
  Chenghao Xiao, Haoran Zhang, Bohao Yang, Wenhu Chen, Wenhao Huang, Noura Al
  Moubayed, Jie Fu, Chenghua Lin
Categories: cs.IR cs.CL cs.CV cs.MM
\\
  Multi-modal information retrieval (MMIR) is a rapidly evolving field, where
significant progress, particularly in image-text pairing, has been made through
advanced representation learning and cross-modality alignment research.
However, current benchmarks for evaluating MMIR performance in image-text
pairing within the scientific domain show a notable gap, where chart and table
images described in scholarly language usually do not play a significant role.
To bridge this gap, we develop a specialised scientific MMIR (SciMMIR)
benchmark by leveraging open-access paper collections to extract data relevant
to the scientific domain. This benchmark comprises 530K meticulously curated
image-text pairs, extracted from figures and tables with detailed captions in
scientific documents. We further annotate the image-text pairs with two-level
subset-subcategory hierarchy annotations to facilitate a more comprehensive
evaluation of the baselines. We conducted zero-shot and fine-tuning evaluations
on prominent multi-modal image-captioning and visual language models, such as
CLIP and BLIP. Our analysis offers critical insights for MMIR in the scientific
domain, including the impact of pre-training and fine-tuning settings and the
influence of the visual and textual encoders. All our data and checkpoints are
publicly available at https://github.com/Wusiwei0410/SciMMIR.
\\ ( https://arxiv.org/abs/2401.13478 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13020 (*cross-listing*)
Date: Tue, 23 Jan 2024 17:52:49 GMT   (2045kb,D)

Title: A Safe Reinforcement Learning Algorithm for Supervisory Control of Power
  Plants
Authors: Yixuan Sun, Sami Khairy, Richard B. Vilim, Rui Hu, Akshay J. Dave
Categories: cs.SY cs.LG
\\
  Traditional control theory-based methods require tailored engineering for
each system and constant fine-tuning. In power plant control, one often needs
to obtain a precise representation of the system dynamics and carefully design
the control scheme accordingly. Model-free Reinforcement learning (RL) has
emerged as a promising solution for control tasks due to its ability to learn
from trial-and-error interactions with the environment. It eliminates the need
for explicitly modeling the environment's dynamics, which is potentially
inaccurate. However, the direct imposition of state constraints in power plant
control raises challenges for standard RL methods. To address this, we propose
a chance-constrained RL algorithm based on Proximal Policy Optimization for
supervisory control. Our method employs Lagrangian relaxation to convert the
constrained optimization problem into an unconstrained objective, where
trainable Lagrange multipliers enforce the state constraints. Our approach
achieves the smallest distance of violation and violation rate in a load-follow
maneuver for an advanced Nuclear Power Plant design.
\\ ( https://arxiv.org/abs/2401.13020 ,  2045kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13045 (*cross-listing*)
Date: Tue, 23 Jan 2024 19:02:13 GMT   (308kb)

Title: Assessment of Sports Concussion in Female Athletes: A Role for
  Neuroinformatics?
Authors: Rachel Edelstein, Sterling Gutterman, Benjamin Newman, John Darrell
  Van Horn
Categories: stat.ML cs.LG stat.AP stat.ME
\\
  Over the past decade, the intricacies of sports-related concussions among
female athletes have become readily apparent. Traditional clinical methods for
diagnosing concussions suffer limitations when applied to female athletes,
often failing to capture subtle changes in brain structure and function.
Advanced neuroinformatics techniques and machine learning models have become
invaluable assets in this endeavor. While these technologies have been
extensively employed in understanding concussion in male athletes, there
remains a significant gap in our comprehension of their effectiveness for
female athletes. With its remarkable data analysis capacity, machine learning
offers a promising avenue to bridge this deficit. By harnessing the power of
machine learning, researchers can link observed phenotypic neuroimaging data to
sex-specific biological mechanisms, unraveling the mysteries of concussions in
female athletes. Furthermore, embedding methods within machine learning enable
examining brain architecture and its alterations beyond the conventional
anatomical reference frame. In turn, allows researchers to gain deeper insights
into the dynamics of concussions, treatment responses, and recovery processes.
To guarantee that female athletes receive the optimal care they deserve,
researchers must employ advanced neuroimaging techniques and sophisticated
machine-learning models. These tools enable an in-depth investigation of the
underlying mechanisms responsible for concussion symptoms stemming from
neuronal dysfunction in female athletes. This paper endeavors to address the
crucial issue of sex differences in multimodal neuroimaging experimental design
and machine learning approaches within female athlete populations, ultimately
ensuring that they receive the tailored care they require when facing the
challenges of concussions.
\\ ( https://arxiv.org/abs/2401.13045 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13054 (*cross-listing*)
Date: Tue, 23 Jan 2024 19:26:24 GMT   (69kb,D)

Title: Frustrated Random Walks: A Fast Method to Compute Node Distances on
  Hypergraphs
Authors: Enzhi Li, Bilal Fadlallah
Categories: cs.SI cs.DM cs.LG
Comments: 11 pages, 5 figures
\\
  A hypergraph is a generalization of a graph that arises naturally when
attribute-sharing among entities is considered. Although a hypergraph can be
converted into a graph by expanding its hyperedges into fully connected
subgraphs, going the reverse way is computationally complex and NP-complete. We
therefore hypothesize that a hypergraph contains more information than a graph.
In addition, it is more convenient to manipulate a hypergraph directly, rather
than expand it into a graph. An open problem in hypergraphs is how to
accurately and efficiently calculate their node distances. Estimating node
distances enables us to find a node's nearest neighbors, and perform label
propagation on hypergraphs using a K-nearest neighbors (KNN) approach. In this
paper, we propose a novel approach based on random walks to achieve label
propagation on hypergraphs. We estimate node distances as the expected hitting
times of random walks. We note that simple random walks (SRW) cannot accurately
describe highly complex real-world hypergraphs, which motivates us to introduce
frustrated random walks (FRW) to better describe them. We further benchmark our
method against DeepWalk, and show that while the latter can achieve comparable
results, FRW has a distinct computational advantage in cases where the number
of targets is fairly small. For such cases, we show that FRW runs in
significantly shorter time than DeepWalk. Finally, we analyze the time
complexity of our method, and show that for large and sparse hypergraphs, the
complexity is approximately linear, rendering it superior to the DeepWalk
alternative.
\\ ( https://arxiv.org/abs/2401.13054 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13177 (*cross-listing*)
Date: Wed, 24 Jan 2024 01:50:29 GMT   (440kb,D)

Title: Deep Learning Model Reuse in the HuggingFace Community: Challenges,
  Benefit and Trends
Authors: Mina Taraghi, Gianolli Dorcelus, Armstrong Foundjem, Florian Tambon,
  Foutse Khomh
Categories: cs.SE cs.CY cs.LG
Comments: Accepted by IEEE SANER 2024
\\
  The ubiquity of large-scale Pre-Trained Models (PTMs) is on the rise,
sparking interest in model hubs, and dedicated platforms for hosting PTMs.
Despite this trend, a comprehensive exploration of the challenges that users
encounter and how the community leverages PTMs remains lacking. To address this
gap, we conducted an extensive mixed-methods empirical study by focusing on
discussion forums and the model hub of HuggingFace, the largest public model
hub. Based on our qualitative analysis, we present a taxonomy of the challenges
and benefits associated with PTM reuse within this community. We then conduct a
quantitative study to track model-type trends and model documentation evolution
over time. Our findings highlight prevalent challenges such as limited guidance
for beginner users, struggles with model output comprehensibility in training
or inference, and a lack of model understanding. We also identified interesting
trends among models where some models maintain high upload rates despite a
decline in topics related to them. Additionally, we found that despite the
introduction of model documentation tools, its quantity has not increased over
time, leading to difficulties in model comprehension and selection among users.
Our study sheds light on new challenges in reusing PTMs that were not reported
before and we provide recommendations for various stakeholders involved in PTM
reuse.
\\ ( https://arxiv.org/abs/2401.13177 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13343 (*cross-listing*)
Date: Wed, 24 Jan 2024 10:12:43 GMT   (3224kb,D)

Title: Lessons on Datasets and Paradigms in Machine Learning for Symbolic
  Computation: A Case Study on CAD
Authors: Tereso del R\'io and Matthew England
Categories: cs.SC cs.LG
MSC-class: 68W30, 68T05, 03C10
ACM-class: I.2.6; I.1.0
\\
  Symbolic Computation algorithms and their implementation in computer algebra
systems often contain choices which do not affect the correctness of the output
but can significantly impact the resources required: such choices can benefit
from having them made separately for each problem via a machine learning model.
This study reports lessons on such use of machine learning in symbolic
computation, in particular on the importance of analysing datasets prior to
machine learning and on the different machine learning paradigms that may be
utilised. We present results for a particular case study, the selection of
variable ordering for cylindrical algebraic decomposition, but expect that the
lessons learned are applicable to other decisions in symbolic computation.
  We utilise an existing dataset of examples derived from applications which
was found to be imbalanced with respect to the variable ordering decision. We
introduce an augmentation technique for polynomial systems problems that allows
us to balance and further augment the dataset, improving the machine learning
results by 28\% and 38\% on average, respectively. We then demonstrate how the
existing machine learning methodology used for the problem $-$ classification
$-$ might be recast into the regression paradigm. While this does not have a
radical change on the performance, it does widen the scope in which the
methodology can be applied to make choices.
\\ ( https://arxiv.org/abs/2401.13343 ,  3224kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13410 (*cross-listing*)
Date: Wed, 24 Jan 2024 12:11:41 GMT   (5471kb,D)

Title: How to Forget Clients in Federated Online Learning to Rank?
Authors: Shuyi Wang, Bing Liu, Guido Zuccon
Categories: cs.CR cs.IR cs.LG
Comments: Accepted in ECIR 2024
\\
  Data protection legislation like the European Union's General Data Protection
Regulation (GDPR) establishes the \textit{right to be forgotten}: a user
(client) can request contributions made using their data to be removed from
learned models. In this paper, we study how to remove the contributions made by
a client participating in a Federated Online Learning to Rank (FOLTR) system.
In a FOLTR system, a ranker is learned by aggregating local updates to the
global ranking model. Local updates are learned in an online manner at a
client-level using queries and implicit interactions that have occurred within
that specific client. By doing so, each client's local data is not shared with
other clients or with a centralised search service, while at the same time
clients can benefit from an effective global ranking model learned from
contributions of each client in the federation.
  In this paper, we study an effective and efficient unlearning method that can
remove a client's contribution without compromising the overall ranker
effectiveness and without needing to retrain the global ranker from scratch. A
key challenge is how to measure whether the model has unlearned the
contributions from the client $c^*$ that has requested removal. For this, we
instruct $c^*$ to perform a poisoning attack (add noise to this client updates)
and then we measure whether the impact of the attack is lessened when the
unlearning process has taken place. Through experiments on four datasets, we
demonstrate the effectiveness and efficiency of the unlearning strategy under
different combinations of parameter settings.
\\ ( https://arxiv.org/abs/2401.13410 ,  5471kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13421 (*cross-listing*)
Date: Wed, 24 Jan 2024 12:32:08 GMT   (19kb,D)

Title: Federated learning with distributed fixed design quantum chips and
  quantum channels
Authors: Ammar Daskin
Categories: quant-ph cs.DC cs.LG
\\
  The privacy in classical federated learning can be breached through the use
of local gradient results by using engineered queries from the clients.
However, quantum communication channels are considered more secure because the
use of measurements in the data causes some loss of information, which can be
detected. Therefore, the quantum version of federated learning can be used to
provide more privacy. Additionally, sending an $N$ dimensional data vector
through a quantum channel requires sending $\log N$ entangled qubits, which can
provide exponential efficiency if the data vector is obtained as quantum
states.
  In this paper, we propose a quantum federated learning model where fixed
design quantum chips are operated based on the quantum states sent by a
centralized server. Based on the coming superposition states, the clients
compute and then send their local gradients as quantum states to the server,
where they are aggregated to update parameters. Since the server does not send
model parameters, but instead sends the operator as a quantum state, the
clients are not required to share the model. This allows for the creation of
asynchronous learning models. In addition, the model as a quantum state is fed
into client-side chips directly; therefore, it does not require measurements on
the upcoming quantum state to obtain model parameters in order to compute
gradients. This can provide efficiency over the models where parameter vector
is sent via classical or quantum channels and local gradients are obtained
through the obtained values of these parameters.
\\ ( https://arxiv.org/abs/2401.13421 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13429 (*cross-listing*)
Date: Wed, 24 Jan 2024 12:58:08 GMT   (35kb)

Title: Detection of Correlated Random Vectors
Authors: Dor Elimelech and Wasim Huleihel
Categories: cs.IT cs.LG math.IT math.ST stat.TH
Comments: 34 pages
\\
  In this paper, we investigate the problem of deciding whether two standard
normal random vectors $\mathsf{X}\in\mathbb{R}^{n}$ and
$\mathsf{Y}\in\mathbb{R}^{n}$ are correlated or not. This is formulated as a
hypothesis testing problem, where under the null hypothesis, these vectors are
statistically independent, while under the alternative, $\mathsf{X}$ and a
randomly and uniformly permuted version of $\mathsf{Y}$, are correlated with
correlation $\rho$. We analyze the thresholds at which optimal testing is
information-theoretically impossible and possible, as a function of $n$ and
$\rho$. To derive our information-theoretic lower bounds, we develop a novel
technique for evaluating the second moment of the likelihood ratio using an
orthogonal polynomials expansion, which among other things, reveals a
surprising connection to integer partition functions. We also study a
multi-dimensional generalization of the above setting, where rather than two
vectors we observe two databases/matrices, and furthermore allow for partial
correlations between these two.
\\ ( https://arxiv.org/abs/2401.13429 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13511 (*cross-listing*)
Date: Wed, 24 Jan 2024 15:09:12 GMT   (34318kb,D)

Title: Tissue Cross-Section and Pen Marking Segmentation in Whole Slide Images
Authors: Ruben T. Lucassen, Willeke A. M. Blokx, Mitko Veta
Categories: eess.IV cs.CV cs.LG
Comments: 6 pages, 3 figures
\\
  Tissue segmentation is a routine preprocessing step to reduce the
computational cost of whole slide image (WSI) analysis by excluding background
regions. Traditional image processing techniques are commonly used for tissue
segmentation, but often require manual adjustments to parameter values for
atypical cases, fail to exclude all slide and scanning artifacts from the
background, and are unable to segment adipose tissue. Pen marking artifacts in
particular can be a potential source of bias for subsequent analyses if not
removed. In addition, several applications require the separation of individual
cross-sections, which can be challenging due to tissue fragmentation and
adjacent positioning. To address these problems, we develop a convolutional
neural network for tissue and pen marking segmentation using a dataset of 200
H&E stained WSIs. For separating tissue cross-sections, we propose a novel
post-processing method based on clustering predicted centroid locations of the
cross-sections in a 2D histogram. On an independent test set, the model
achieved a mean Dice score of 0.981$\pm$0.033 for tissue segmentation and a
mean Dice score of 0.912$\pm$0.090 for pen marking segmentation. The mean
absolute difference between the number of annotated and separated
cross-sections was 0.075$\pm$0.350. Our results demonstrate that the proposed
model can accurately segment H&E stained tissue cross-sections and pen markings
in WSIs while being robust to many common slide and scanning artifacts. The
model with trained model parameters and post-processing method are made
publicly available as a Python package called SlideSegmenter.
\\ ( https://arxiv.org/abs/2401.13511 ,  34318kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13536 (*cross-listing*)
Date: Wed, 24 Jan 2024 15:46:25 GMT   (2762kb,D)

Title: Finetuning Foundation Models for Joint Analysis Optimization
Authors: Matthias Vig and Nicole Hartman and Lukas Heinrich
Categories: hep-ex cs.LG hep-ph physics.data-an
Comments: 13 pages, 12 figures
\\
  In this work we demonstrate that significant gains in performance and data
efficiency can be achieved in High Energy Physics (HEP) by moving beyond the
standard paradigm of sequential optimization or reconstruction and analysis
components. We conceptually connect HEP reconstruction and analysis to modern
machine learning workflows such as pretraining, finetuning, domain adaptation
and high-dimensional embedding spaces and quantify the gains in the example
usecase of searches of heavy resonances decaying via an intermediate di-Higgs
system to four $b$-jets.
\\ ( https://arxiv.org/abs/2401.13536 ,  2762kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13537 (*cross-listing*)
Date: Wed, 24 Jan 2024 15:46:32 GMT   (647kb,D)

Title: Masked Particle Modeling on Sets: Towards Self-Supervised High Energy
  Physics Foundation Models
Authors: Lukas Heinrich and Michael Kagan and Samuel Klein and Matthew Leigh
  and Tobias Golling and John Andrew Raine and Margarita Osadchy
Categories: hep-ph cs.LG hep-ex physics.data-an
\\
  We propose \textit{masked particle modeling} (MPM) as a self-supervised
method for learning generic, transferable, and reusable representations on
unordered sets of inputs for use in high energy physics (HEP) scientific data.
This work provides a novel scheme to perform masked modeling based pre-training
to learn permutation invariant functions on sets. More generally, this work
provides a step towards building large foundation models for HEP that can be
generically pre-trained with self-supervised learning and later fine-tuned for
a variety of down-stream tasks. In MPM, particles in a set are masked and the
training objective is to recover their identity, as defined by a discretized
token representation of a pre-trained vector quantized variational autoencoder.
We study the efficacy of the method in samples of high energy jets at collider
physics experiments, including studies on the impact of discretization,
permutation invariance, and ordering. We also study the fine-tuning capability
of the model, showing that it can be adapted to tasks such as supervised and
weakly supervised jet classification, and that the model can transfer
efficiently with small fine-tuning data sets to new classes and new data
domains.
\\ ( https://arxiv.org/abs/2401.13537 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13570 (*cross-listing*)
Date: Wed, 24 Jan 2024 16:31:50 GMT   (11138kb,D)

Title: Guided Diffusion for Fast Inverse Design of Density-based Mechanical
  Metamaterials
Authors: Yanyan Yang, Lili Wang, Xiaoya Zhai, Kai Chen, Wenming Wu, Yunkai
  Zhao, Ligang Liu, Xiao-Ming Fu
Categories: cs.CE cs.LG
Comments: 13 pages, 6 figures
\\
  Mechanical metamaterial is a synthetic material that can possess
extraordinary physical characteristics, such as abnormal elasticity, stiffness,
and stability, by carefully designing its internal structure. To make
metamaterials contain delicate local structures with unique mechanical
properties, it is a potential method to represent them through high-resolution
voxels. However, it brings a substantial computational burden. To this end,
this paper proposes a fast inverse design method, whose core is an advanced
deep generative AI algorithm, to generate voxel-based mechanical metamaterials.
Specifically, we use the self-conditioned diffusion model, capable of
generating a microstructure with a resolution of $128^3$ to approach the
specified homogenized tensor matrix in just 3 seconds. Accordingly, this rapid
reverse design tool facilitates the exploration of extreme metamaterials, the
sequence interpolation in metamaterials, and the generation of diverse
microstructures for multi-scale design. This flexible and adaptive generative
tool is of great value in structural engineering or other mechanical systems
and can stimulate more subsequent research.
\\ ( https://arxiv.org/abs/2401.13570 ,  11138kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13575 (*cross-listing*)
Date: Wed, 24 Jan 2024 16:40:30 GMT   (5134kb,D)

Title: CNN architecture extraction on edge GPU
Authors: Peter Horvath, Lukasz Chmielewski, Leo Weissbart, Lejla Batina, Yuval
  Yarom
Categories: cs.CR cs.LG
Comments: Will appear at the AIHWS 2024 workshop at ACNS 2024
Report-no: AIHWS008
\\
  Neural networks have become popular due to their versatility and
state-of-the-art results in many applications, such as image classification,
natural language processing, speech recognition, forecasting, etc. These
applications are also used in resource-constrained environments such as
embedded devices. In this work, the susceptibility of neural network
implementations to reverse engineering is explored on the NVIDIA Jetson Nano
microcomputer via side-channel analysis. To this end, an architecture
extraction attack is presented. In the attack, 15 popular convolutional neural
network architectures (EfficientNets, MobileNets, NasNet, etc.) are implemented
on the GPU of Jetson Nano and the electromagnetic radiation of the GPU is
analyzed during the inference operation of the neural networks. The results of
the analysis show that neural network architectures are easily distinguishable
using deep learning-based side-channel analysis.
\\ ( https://arxiv.org/abs/2401.13575 ,  5134kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13624 (*cross-listing*)
Date: Wed, 24 Jan 2024 17:54:55 GMT   (75kb,D)

Title: Can overfitted deep neural networks in adversarial training generalize?
  -- An approximation viewpoint
Authors: Zhongjie Shi, Fanghui Liu, Yuan Cao and Johan A.K. Suykens
Categories: stat.ML cs.LG
\\
  Adversarial training is a widely used method to improve the robustness of
deep neural networks (DNNs) over adversarial perturbations. However, it is
empirically observed that adversarial training on over-parameterized networks
often suffers from the \textit{robust overfitting}: it can achieve almost zero
adversarial training error while the robust generalization performance is not
promising. In this paper, we provide a theoretical understanding of the
question of whether overfitted DNNs in adversarial training can generalize from
an approximation viewpoint. Specifically, our main results are summarized into
three folds: i) For classification, we prove by construction the existence of
infinitely many adversarial training classifiers on over-parameterized DNNs
that obtain arbitrarily small adversarial training error (overfitting), whereas
achieving good robust generalization error under certain conditions concerning
the data quality, well separated, and perturbation level. ii) Linear
over-parameterization (meaning that the number of parameters is only slightly
larger than the sample size) is enough to ensure such existence if the target
function is smooth enough. iii) For regression, our results demonstrate that
there also exist infinitely many overfitted DNNs with linear
over-parameterization in adversarial training that can achieve almost optimal
rates of convergence for the standard generalization error. Overall, our
analysis points out that robust overfitting can be avoided but the required
model capacity will depend on the smoothness of the target function, while a
robust generalization gap is inevitable. We hope our analysis will give a
better understanding of the mathematical foundations of robustness in DNNs from
an approximation view.
\\ ( https://arxiv.org/abs/2401.13624 ,  75kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2307.02637
replaced with revised version Wed, 24 Jan 2024 16:36:01 GMT   (5295kb,D)

Title: Surge Routing: Event-informed Multiagent Reinforcement Learning for
  Autonomous Rideshare
Authors: Daniel Garces and Stephanie Gil
Categories: cs.AI cs.MA cs.RO
Comments: 10 pages, 7 figures, 4 tables, 23rd International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS 2024)
\\ ( https://arxiv.org/abs/2307.02637 ,  5295kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06082
replaced with revised version Wed, 24 Jan 2024 15:10:07 GMT   (6298kb,D)

Title: VELMA: Verbalization Embodiment of LLM Agents for Vision and Language
  Navigation in Street View
Authors: Raphael Schumann and Wanrong Zhu and Weixi Feng and Tsu-Jui Fu and
  Stefan Riezler and William Yang Wang
Categories: cs.AI cs.CL cs.CV
Comments: Accepted at AAAI 2024
\\ ( https://arxiv.org/abs/2307.06082 ,  6298kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08535
replaced with revised version Wed, 24 Jan 2024 06:07:20 GMT   (223kb,D)

Title: Formally Specifying the High-Level Behavior of LLM-Based Agents
Authors: Maxwell Crouse, Ibrahim Abdelaziz, Ramon Astudillo, Kinjal Basu, Soham
  Dan, Sadhana Kumaravel, Achille Fokoue, Pavan Kapanipathi, Salim Roukos, Luis
  Lastras
Categories: cs.AI cs.CL
Comments: Preprint under review
\\ ( https://arxiv.org/abs/2310.08535 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13361
replaced with revised version Wed, 24 Jan 2024 09:52:39 GMT   (679kb,D)

Title: Applying Large Language Models to Power Systems: Potential Security
  Threats
Authors: Jiaqi Ruan, Gaoqi Liang, Huan Zhao, Guolong Liu, Xianzhuo Sun, Jing
  Qiu, Zhao Xu, Fushuan Wen, Zhao Yang Dong
Categories: cs.AI cs.HC cs.SY eess.SY
\\ ( https://arxiv.org/abs/2311.13361 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06717
replaced with revised version Tue, 23 Jan 2024 21:56:31 GMT   (5764kb,D)

Title: Privacy Issues in Large Language Models: A Survey
Authors: Seth Neel and Peter Chang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.06717 ,  5764kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07448
replaced with revised version Wed, 24 Jan 2024 01:48:00 GMT   (23820kb,D)

Title: Formal Logic Enabled Personalized Federated Learning Through Property
  Inference
Authors: Ziyan An, Taylor T. Johnson, Meiyi Ma
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.07448 ,  23820kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08517
replaced with revised version Wed, 24 Jan 2024 09:55:37 GMT   (3769kb,D)

Title: Supporting Student Decisions on Learning Recommendations: An LLM-Based
  Chatbot with Knowledge Graph Contextualization for Conversational
  Explainability and Mentoring
Authors: Hasan Abu-Rasheed, Mohamad Hussam Abdulsalam, Christian Weber, Madjid
  Fathi
Categories: cs.AI cs.CL cs.HC
\\ ( https://arxiv.org/abs/2401.08517 ,  3769kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12435
replaced with revised version Wed, 24 Jan 2024 02:31:26 GMT   (8274kb)

Title: Quantitative Analysis of Molecular Transport in the Extracellular Space
  Using Physics-Informed Neural Network
Authors: Jiayi Xie, Hongfeng Li, Jin Cheng, Qingrui Cai, Hanbo Tan, Lingyun Zu,
  Xiaobo Qu, and Hongbin Han
Categories: cs.AI cs.LG math.AP
\\ ( https://arxiv.org/abs/2401.12435 ,  8274kb)
------------------------------------------------------------------------------
\\
arXiv:2112.03203
replaced with revised version Wed, 24 Jan 2024 13:47:03 GMT   (530kb,D)

Title: A New Sentence Extraction Strategy for Unsupervised Extractive
  Summarization Methods
Authors: Dehao Tao, Yingzhu Xiong, Zhongliang Yang, and Yongfeng Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2112.03203 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2202.12312
replaced with revised version Tue, 23 Jan 2024 22:09:07 GMT   (3851kb,D)

Title: Oolong: Investigating What Makes Transfer Learning Hard with Controlled
  Studies
Authors: Zhengxuan Wu and Alex Tamkin and Isabel Papadimitriou
Categories: cs.CL
Comments: EMNLP 2023
\\ ( https://arxiv.org/abs/2202.12312 ,  3851kb)
------------------------------------------------------------------------------
\\
arXiv:2303.13716
replaced with revised version Tue, 23 Jan 2024 21:52:42 GMT   (351kb,D)

Title: ReCOGS: How Incidental Details of a Logical Form Overshadow an
  Evaluation of Semantic Interpretation
Authors: Zhengxuan Wu, Christopher D. Manning, Christopher Potts
Categories: cs.CL
Comments: TACL 2023
\\ ( https://arxiv.org/abs/2303.13716 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08809
replaced with revised version Tue, 23 Jan 2024 21:25:20 GMT   (3331kb,D)

Title: Interpretability at Scale: Identifying Causal Mechanisms in Alpaca
Authors: Zhengxuan Wu, Atticus Geiger, Christopher Potts, Noah D. Goodman
Categories: cs.CL
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2305.08809 ,  3331kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02272
replaced with revised version Wed, 24 Jan 2024 02:53:27 GMT   (197kb,D)

Title: OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and
  Inference of Large Language Models
Authors: Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park
Categories: cs.CL
Comments: Accepted at AAAI 2024 (oral presentation)
\\ ( https://arxiv.org/abs/2306.02272 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03268
replaced with revised version Wed, 24 Jan 2024 07:53:30 GMT   (2865kb,D)

Title: "Medium" LMs of Code in the Era of LLMs: Lessons From StackOverflow
Authors: Manisha Mukherjee, Vincent J. Hellendoorn
Categories: cs.CL cs.SE
\\ ( https://arxiv.org/abs/2306.03268 ,  2865kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08877
replaced with revised version Tue, 23 Jan 2024 20:55:48 GMT   (15791kb,D)

Title: Linguistic Binding in Diffusion Models: Enhancing Attribute
  Correspondence through Attention Map Alignment
Authors: Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav
  Goldberg, Gal Chechik
Categories: cs.CL cs.CV
Comments: Accepted to NeurIPS 2023 (oral). Our code is publicly available at
  https://github.com/RoyiRa/Syntax-Guided-Generation
\\ ( https://arxiv.org/abs/2306.08877 ,  15791kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12539
replaced with revised version Wed, 24 Jan 2024 01:09:01 GMT   (943kb,D)

Title: CALM : A Multi-task Benchmark for Comprehensive Assessment of Language
  Model Bias
Authors: Vipul Gupta, Pranav Narayanan Venkit, Hugo Lauren\c{c}on, Shomir
  Wilson, Rebecca J. Passonneau
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2308.12539 ,  943kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06657
replaced with revised version Tue, 23 Jan 2024 23:16:11 GMT   (4376kb,D)

Title: Statistical Rejection Sampling Improves Preference Optimization
Authors: Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh,
  Peter J. Liu, Jialu Liu
Categories: cs.CL
Comments: Accepted in ICLR 2024
\\ ( https://arxiv.org/abs/2309.06657 ,  4376kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08347
replaced with revised version Wed, 24 Jan 2024 04:53:13 GMT   (495kb,D)

Title: Reward Engineering for Generating Semi-structured Explanation
Authors: Jiuzhou Han, Wray Buntine, Ehsan Shareghi
Categories: cs.CL
Comments: Accepted to EACL2024; code is available at
  https://github.com/Jiuzhouh/Reward-Engineering-for-Generating-SEG
\\ ( https://arxiv.org/abs/2309.08347 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08565
replaced with revised version Wed, 24 Jan 2024 17:48:31 GMT   (8263kb,D)

Title: How Transferable are Attribute Controllers on Pretrained Multilingual
  Translation Models?
Authors: Danni Liu, Jan Niehues
Categories: cs.CL cs.AI
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2309.08565 ,  8263kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17249
replaced with revised version Wed, 24 Jan 2024 18:27:30 GMT   (6886kb,D)

Title: Batch Calibration: Rethinking Calibration for In-Context Learning and
  Prompt Engineering
Authors: Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen,
  Katherine Heller, Subhrajit Roy
Categories: cs.CL cs.AI cs.LG
Comments: ICLR 2024. 9 pages, 9 figures, 3 tables (22 pages, 11 figures, 11
  tables including references and appendices)
\\ ( https://arxiv.org/abs/2309.17249 ,  6886kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01749
replaced with revised version Wed, 24 Jan 2024 16:28:43 GMT   (105kb,D)

Title: Stack Attention: Improving the Ability of Transformers to Model
  Hierarchical Patterns
Authors: Brian DuSell and David Chiang
Categories: cs.CL
Comments: 20 pages, 4 figures. Published as a spotlight paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.01749 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02374
replaced with revised version Tue, 23 Jan 2024 21:27:14 GMT   (7586kb,D)

Title: Conversational Health Agents: A Personalized LLM-Powered Agent Framework
Authors: Mahyar Abbasian, Iman Azimi, Amir M. Rahmani, Ramesh Jain
Categories: cs.CL
Comments: 23 pages, 6 figures, 3 tables, journal paper
\\ ( https://arxiv.org/abs/2310.02374 ,  7586kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11803
replaced with revised version Tue, 23 Jan 2024 19:37:20 GMT   (915kb,D)

Title: NLP for Maternal Healthcare: Perspectives and Guiding Principles in the
  Age of LLMs
Authors: Maria Antoniak, Aakanksha Naik, Carla S. Alvarado, Lucy Lu Wang, Irene
  Y. Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.11803 ,  915kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02981
replaced with revised version Wed, 24 Jan 2024 18:16:34 GMT   (1099kb)

Title: Fine-tuning and Utilization Methods of Domain-specific LLMs
Authors: Cheonsu Jeong
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.02981 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06373
replaced with revised version Tue, 23 Jan 2024 22:46:12 GMT   (10941kb,D)

Title: How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to
  Challenge AI Safety by Humanizing LLMs
Authors: Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi
Categories: cs.CL cs.AI
Comments: 14 pages of the main text, qualitative examples of jailbreaks may be
  harmful in nature
\\ ( https://arxiv.org/abs/2401.06373 ,  10941kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10841
replaced with revised version Tue, 23 Jan 2024 20:05:30 GMT   (1462kb,D)

Title: Using LLMs to discover emerging coded antisemitic hate-speech in
  extremist social media
Authors: Dhanush Kikkisetti, Raza Ul Mustafa, Wendy Melillo, Roberto Corizzo,
  Zois Boukouvalas, Jeff Gill and Nathalie Japkowicz
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: 9 pages, 4 figures, 2 algorithms, 3 tables
\\ ( https://arxiv.org/abs/2401.10841 ,  1462kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11120
replaced with revised version Tue, 23 Jan 2024 19:43:06 GMT   (5796kb,D)

Title: Enhancing Large Language Models for Clinical Decision Support by
  Incorporating Clinical Practice Guidelines
Authors: David Oniani, Xizhi Wu, Shyam Visweswaran, Sumit Kapoor, Shravan
  Kooragayalu, Katelyn Polanska, Yanshan Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.11120 ,  5796kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12143
replaced with revised version Wed, 24 Jan 2024 16:07:00 GMT   (14162kb,D)

Title: Anisotropy Is Inherent to Self-Attention in Transformers
Authors: Nathan Godey and \'Eric de la Clergerie and Beno\^it Sagot
Categories: cs.CL
Comments: Proceedings of EACL 2024. A previous version of the paper, published
  as arXiv:2306.07656, was presented at ACL-SRW 2023 (non-archival)
\\ ( https://arxiv.org/abs/2401.12143 ,  14162kb)
------------------------------------------------------------------------------
\\
arXiv:2106.01135
replaced with revised version Wed, 24 Jan 2024 14:56:44 GMT   (88kb)

Title: MNL-Bandit with Knapsacks: a near-optimal algorithm
Authors: Abdellah Aznag, Vineet Goyal and Noemie Perivier
Categories: cs.LG cs.DS
Comments: Improved the regret bound/assumptions. Corrected the abstract
\\ ( https://arxiv.org/abs/2106.01135 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2106.07289
replaced with revised version Wed, 24 Jan 2024 09:35:38 GMT   (6287kb,D)

Title: Decentralized Personalized Federated Learning for Min-Max Problems
Authors: Ekaterina Borodich, Aleksandr Beznosikov, Abdurakhmon Sadiev, Vadim
  Sushko, Nikolay Savelyev, Martin Tak\'a\v{c}, Alexander Gasnikov
Categories: cs.LG cs.DC math.OC
Comments: 33 pages, 3 algorithms, 5 figures, 2 tables
\\ ( https://arxiv.org/abs/2106.07289 ,  6287kb)
------------------------------------------------------------------------------
\\
arXiv:2203.13883
replaced with revised version Wed, 24 Jan 2024 01:50:22 GMT   (423kb,D)

Title: Multi-modal Misinformation Detection: Approaches, Challenges and
  Opportunities
Authors: Sara Abdali, Sina shaham, Bhaskar Krishnamachari
Categories: cs.LG cs.AI cs.CV cs.CY cs.MM cs.SI
\\ ( https://arxiv.org/abs/2203.13883 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2206.06009
replaced with revised version Wed, 24 Jan 2024 15:23:09 GMT   (162kb,D)

Title: Relative Policy-Transition Optimization for Fast Policy Transfer
Authors: Jiawei Xu, Cheng Zhou, Yizheng Zhang, Baoxiang Wang, Lei Han
Categories: cs.LG cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2206.06009 ,  162kb)
------------------------------------------------------------------------------
\\
arXiv:2206.14359
replaced with revised version Tue, 23 Jan 2024 21:55:20 GMT   (4844kb,D)

Title: TE2Rules: Explaining Tree Ensembles using Rules
Authors: G Roshan Lal and Xiaotong Chen and Varun Mithal
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2206.14359 ,  4844kb)
------------------------------------------------------------------------------
\\
arXiv:2208.02389
replaced with revised version Tue, 23 Jan 2024 22:32:18 GMT   (7632kb,D)

Title: Risk-Aware Linear Bandits: Theory and Applications in Smart Order
  Routing
Authors: Jingwei Ji, Renyuan Xu, Ruihao Zhu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2208.02389 ,  7632kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10227
replaced with revised version Wed, 24 Jan 2024 11:38:25 GMT   (414kb,D)

Title: Adversarial Detection by Approximation of Ensemble Boundary
Authors: T. Windeatt
Categories: cs.LG cs.AI cs.CR cs.CV
Comments: 17 pages, 5 figures, 5 tables
\\ ( https://arxiv.org/abs/2211.10227 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2211.12343
replaced with revised version Wed, 24 Jan 2024 12:51:43 GMT   (22736kb,D)

Title: Diffusion Model Based Posterior Sampling for Noisy Linear Inverse
  Problems
Authors: Xiangming Meng and Yoshiyuki Kabashima
Categories: cs.LG cs.CV cs.IT math.IT stat.ML
Comments: Code is available at https://github.com/mengxiangming/dmps
\\ ( https://arxiv.org/abs/2211.12343 ,  22736kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08298
replaced with revised version Wed, 24 Jan 2024 02:58:56 GMT   (28174kb,D)

Title: Unleashing the Potential of Acquisition Functions in High-Dimensional
  Bayesian Optimization
Authors: Jiayu Zhao, Renyu Yang, Shenghao Qiu, Zheng Wang
Categories: cs.LG stat.ML
Comments: Accepted by Transactions on Machine Learning Research (TMLR)
\\ ( https://arxiv.org/abs/2302.08298 ,  28174kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13711
replaced with revised version Wed, 24 Jan 2024 13:44:31 GMT   (15508kb,D)

Title: Internal-Coordinate Density Modelling of Protein Structure: Covariance
  Matters
Authors: Marloes Arts, Jes Frellsen, Wouter Boomsma
Categories: cs.LG q-bio.BM
Comments: Pages: 10 main, 3 references, 8 appendix. Figures: 5 main, 6 appendix
\\ ( https://arxiv.org/abs/2302.13711 ,  15508kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04878
replaced with revised version Tue, 23 Jan 2024 22:13:16 GMT   (5749kb,D)

Title: DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep
  Neural Networks
Authors: Zohreh Aghababaeyan, Manel Abdellatif, Mahboubeh Dadkhah, Lionel
  Briand
Categories: cs.LG cs.PF cs.SE
\\ ( https://arxiv.org/abs/2303.04878 ,  5749kb)
------------------------------------------------------------------------------
\\
arXiv:2303.09599
replaced with revised version Wed, 24 Jan 2024 13:24:48 GMT   (1401kb)

Title: cito: An R package for training neural networks using torch
Authors: Christian Amesoeder, Florian Hartig, Maximilian Pichler
Categories: cs.LG
Comments: 12 pages, 4 figures, 2 tables
ACM-class: I.5.4
\\ ( https://arxiv.org/abs/2303.09599 ,  1401kb)
------------------------------------------------------------------------------
\\
arXiv:2303.15991
replaced with revised version Wed, 24 Jan 2024 06:03:32 GMT   (2635kb,D)

Title: Efficient Parallel Split Learning over Resource-constrained Wireless
  Edge Networks
Authors: Zheng Lin, Guangyu Zhu, Yiqin Deng, Xianhao Chen, Yue Gao, Kaibin
  Huang, Yuguang Fang
Categories: cs.LG
Comments: 15 pages, 13 figures
\\ ( https://arxiv.org/abs/2303.15991 ,  2635kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03468
replaced with revised version Wed, 24 Jan 2024 07:56:04 GMT   (3175kb,D)

Title: Toward Practical Entity Alignment Method Design: Insights from New
  Highly Heterogeneous Knowledge Graph Datasets
Authors: Xuhui Jiang, Chengjin Xu, Yinghan Shen, Yuanzhuo Wang, Fenglong Su,
  Fei Sun, Zixuan Li, Zhichao Shi, Jian Guo, Huawei Shen
Categories: cs.LG cs.AI
Comments: 12 pages, 6 figures
\\ ( https://arxiv.org/abs/2304.03468 ,  3175kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00557
replaced with revised version Tue, 23 Jan 2024 21:32:30 GMT   (9004kb,D)

Title: Collective Relational Inference for learning heterogeneous interactions
Authors: Zhichao Han, Olga Fink, David S. Kammer
Categories: cs.LG
Comments: Under review. Links to the supporting code can be found at the end of
  the main content
\\ ( https://arxiv.org/abs/2305.00557 ,  9004kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13998
replaced with revised version Tue, 23 Jan 2024 20:33:09 GMT   (1794kb,D)

Title: SMT 2.0: A Surrogate Modeling Toolbox with a focus on Hierarchical and
  Mixed Variables Gaussian Processes
Authors: Paul Saves and Remi Lafage and Nathalie Bartoli and Youssef Diouane
  and Jasper Bussemaker and Thierry Lefebvre and John T. Hwang and Joseph
  Morlier and Joaquim R. R. A. Martins
Categories: cs.LG math.OC stat.CO
Comments: 10.1016/j.advengsoft.2023.103571
Journal-ref: Advances in Engineering Software Volume 188, February 2024, 103571
DOI: 10.1016/j.advengsoft.2023.103571
\\ ( https://arxiv.org/abs/2305.13998 ,  1794kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15901
replaced with revised version Wed, 24 Jan 2024 14:13:09 GMT   (3176kb,D)

Title: Consistent Optimal Transport with Empirical Conditional Measures
Authors: Piyushi Manupriya, Rachit Keerti Das, Sayantan Biswas, Saketha Nath
  Jagarlapudi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.15901 ,  3176kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15936
replaced with revised version Tue, 23 Jan 2024 22:08:20 GMT   (254kb,D)

Title: Learning DAGs from Data with Few Root Causes
Authors: Panagiotis Misiakos, Chris Wendler, Markus P\"uschel
Categories: cs.LG cs.AI stat.ME
Comments: to be published in 37th Conference on Neural Information Processing
  Systems (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2305.15936 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09205
replaced with revised version Wed, 24 Jan 2024 18:32:49 GMT   (10433kb,D)

Title: Reward-Free Curricula for Training Robust World Models
Authors: Marc Rigter, Minqi Jiang, Ingmar Posner
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2306.09205 ,  10433kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09800
replaced with revised version Wed, 24 Jan 2024 10:33:18 GMT   (1981kb,D)

Title: $\pi2\text{vec}$: Policy Representations with Successor Features
Authors: Gianluca Scarpellini, Ksenia Konyushkova, Claudio Fantacci, Tom Le
  Paine, Yutian Chen, Misha Denil
Categories: cs.LG cs.RO
Comments: Accepted paper at ICLR2024
\\ ( https://arxiv.org/abs/2306.09800 ,  1981kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12194
replaced with revised version Wed, 24 Jan 2024 08:45:46 GMT   (5549kb)

Title: Split Learning in 6G Edge Networks
Authors: Zheng Lin, Guanqiao Qu, Xianhao Chen, and Kaibin Huang
Categories: cs.LG cs.DC cs.NI
Comments: 7 pages, 6 figures
\\ ( https://arxiv.org/abs/2306.12194 ,  5549kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15865
replaced with revised version Wed, 24 Jan 2024 17:55:05 GMT   (1902kb,D)

Title: Differentially Private Distributed Estimation and Learning
Authors: Marios Papachristou, M. Amin Rahimian
Categories: cs.LG cs.SI cs.SY eess.SY math.ST stat.AP stat.ML stat.TH
Comments: Additional experiments, comparison with related work, and extensions
  (dynamic networks, directed networks networks, heterogeneous privacy budgets)
\\ ( https://arxiv.org/abs/2306.15865 ,  1902kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08097
replaced with revised version Wed, 24 Jan 2024 02:37:10 GMT   (380kb,D)

Title: EasyTPP: Towards Open Benchmarking Temporal Point Processes
Authors: Siqiao Xue, Xiaoming Shi, Zhixuan Chu, Yan Wang, Hongyan Hao, Fan
  Zhou, Caigao Jiang, Chen Pan, James Y. Zhang, Qingsong Wen, Jun Zhou,
  Hongyuan Mei
Categories: cs.LG
Comments: ICLR 2024 camera ready
\\ ( https://arxiv.org/abs/2307.08097 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15398
replaced with revised version Wed, 24 Jan 2024 11:33:09 GMT   (150kb,D)

Title: The Initial Screening Order Problem
Authors: Jose M. Alvarez and Antonio Mastropietro and Salvatore Ruggieri
Categories: cs.LG cs.CY
\\ ( https://arxiv.org/abs/2307.15398 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15812
replaced with revised version Tue, 23 Jan 2024 20:44:17 GMT   (715kb,D)

Title: Peering Through Preferences: Unraveling Feedback Acquisition for
  Aligning Large Language Models
Authors: Hritik Bansal, John Dang, Aditya Grover
Categories: cs.LG cs.AI cs.CL
Comments: 31 pages, Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2308.15812 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00976
replaced with revised version Wed, 24 Jan 2024 04:41:27 GMT   (17780kb,D)

Title: Pure Message Passing Can Estimate Common Neighbor for Link Prediction
Authors: Kaiwen Dong, Zhichun Guo, Nitesh V. Chawla
Categories: cs.LG cs.IR cs.SI
Comments: preprint
\\ ( https://arxiv.org/abs/2309.00976 ,  17780kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14808
replaced with revised version Wed, 24 Jan 2024 01:22:59 GMT   (3211kb,D)

Title: Revisiting Softmax Masking: Stop Gradient for Enhancing Stability in
  Replay-based Continual Learning
Authors: Hoyong Kim, Minchan Kwon, Kangil Kim
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.14808 ,  3211kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17371
replaced with revised version Tue, 23 Jan 2024 19:37:29 GMT   (3719kb,D)

Title: Adversarial Imitation Learning from Visual Observations using Latent
  Information
Authors: Vittorio Giammarino, James Queeney, Ioannis Ch. Paschalidis
Categories: cs.LG cs.SY eess.SY stat.ML
\\ ( https://arxiv.org/abs/2309.17371 ,  3719kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03258
replaced with revised version Wed, 24 Jan 2024 15:52:08 GMT   (32964kb,D)

Title: Assessing Electricity Service Unfairness with Transfer Counterfactual
  Learning
Authors: Song Wei, Xiangrui Kong, Alinson Santos Xavier, Shixiang Zhu, Yao Xie,
  Feng Qiu
Categories: cs.LG stat.ME
Comments: The preliminary version titled "Detecting Electricity Service Equity
  Issues with Transfer Counterfactual Learning on Large-Scale Outage Datasets"
  is presented at NeurIPS 2023 Workshops on Causal Representation Learning
  (CRL) and Algorithmic Fairness through the Lens of Time (AFT); See v1
\\ ( https://arxiv.org/abs/2310.03258 ,  32964kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06343
replaced with revised version Wed, 24 Jan 2024 04:44:58 GMT   (9405kb,D)

Title: Boosting Continuous Control with Consistency Policy
Authors: Yuhui Chen, Haoran Li, Dongbin Zhao
Categories: cs.LG
Comments: 18 pages, 9 pages
\\ ( https://arxiv.org/abs/2310.06343 ,  9405kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10280
replaced with revised version Wed, 24 Jan 2024 06:35:43 GMT   (3554kb,D)

Title: Mimicking the Maestro: Exploring the Efficacy of a Virtual AI Teacher in
  Fine Motor Skill Acquisition
Authors: Hadar Mulian, Segev Shlomov, Lior Limonad, Alessia Noccaro, Silvia
  Buscaglione
Categories: cs.LG
Comments: arXiv admin note: The first version of this paper has been removed by
  arXiv administrators as the submitter did not have the right to agree to the
  license at the time of submission. This version resolves the rights issue,
  includes two additional authors, and is cleared to go public
\\ ( https://arxiv.org/abs/2310.10280 ,  3554kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18612
replaced with revised version Wed, 24 Jan 2024 11:19:11 GMT   (6193kb,D)

Title: Efficient kernel surrogates for neural network-based regression
Authors: Saad Qadeer, Andrew Engel, Amanda Howard, Adam Tsou, Max Vargas, Panos
  Stinis, and Tony Chiang
Categories: cs.LG
Comments: 35 pages. software used to reach results available upon request,
  approved for release by Pacific Northwest National Laboratory
Report-no: PNNL-SA-191858
MSC-class: 68T07, 65M99
\\ ( https://arxiv.org/abs/2310.18612 ,  6193kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10795
replaced with revised version Wed, 24 Jan 2024 02:00:05 GMT   (1937kb)

Title: How False Data Affects Machine Learning Models in Electrochemistry?
Authors: Krittapong Deshsorna, Luckhana Lawtrakul, Pawin Iamprasertkun
Categories: cs.LG physics.chem-ph
Comments: 40 pages, 11 figures
DOI: 10.1016/j.jpowsour.2024.234127
\\ ( https://arxiv.org/abs/2311.10795 ,  1937kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13541
replaced with revised version Wed, 24 Jan 2024 15:33:32 GMT   (2908kb,D)

Title: Linear Log-Normal Attention with Unbiased Concentration
Authors: Yury Nahshan, Joseph Kampeas and Emir Haleva
Categories: cs.LG cs.AI
Comments: 22 pages, 20 figures, 5 tables, submitted to ICLR2024
ACM-class: I.7.0; G.3
\\ ( https://arxiv.org/abs/2311.13541 ,  2908kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16093
replaced with revised version Wed, 24 Jan 2024 11:03:49 GMT   (4258kb,D)

Title: Visual cognition in multimodal large language models
Authors: Luca M. Schulze Buschoff, Elif Akata, Matthias Bethge, Eric Schulz
Categories: cs.LG
Comments: Changed title and main text
\\ ( https://arxiv.org/abs/2311.16093 ,  4258kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12784
replaced with revised version Wed, 24 Jan 2024 02:20:53 GMT   (7096kb,D)

Title: Fast Cell Library Characterization for Design Technology Co-Optimization
  Based on Graph Neural Networks
Authors: Tianliang Ma, Zhihui Deng, Xuguang Sun, Leilai Shao Kainlu Low
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.12784 ,  7096kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08318
replaced with revised version Wed, 24 Jan 2024 15:30:55 GMT   (1373kb,D)

Title: OpenDPD: An Open-Source End-to-End Learning & Benchmarking Framework for
  Wideband Power Amplifier Modeling and Digital Pre-Distortion
Authors: Yizhuo Wu, Gagan Deep Singh, Mohammadreza Beikmirza, Leo C. N. de
  Vreede, Morteza Alavi, Chang Gao
Categories: cs.LG eess.SP
Comments: To be published at the 2024 IEEE International Symposium on Circuits
  and Systems (ISCAS), Singapore
\\ ( https://arxiv.org/abs/2401.08318 ,  1373kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08534
replaced with revised version Wed, 24 Jan 2024 17:13:08 GMT   (6375kb,D)

Title: DiConStruct: Causal Concept-based Explanations through Black-Box
  Distillation
Authors: Ricardo Moreira, Jacopo Bono, M\'ario Cardoso, Pedro Saleiro, M\'ario
  A. T. Figueiredo, Pedro Bizarro
Categories: cs.LG cs.AI cs.HC
Comments: Accepted at Conference on Causal Learning and Reasoning (CLeaR 2024,
  https://www.cclear.cc/2024). To be published at Proceedings of Machine
  Learning Research (PMLR)
\\ ( https://arxiv.org/abs/2401.08534 ,  6375kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09793
replaced with revised version Wed, 24 Jan 2024 13:23:53 GMT   (1384kb,D)

Title: PatchAD: Patch-based MLP-Mixer for Time Series Anomaly Detection
Authors: Zhijie Zhong, Zhiwen Yu, Yiyuan Yang, Weizheng Wang, Kaixiang Yang
Categories: cs.LG
Comments: 13 pages, 16 figures, Under review
\\ ( https://arxiv.org/abs/2401.09793 ,  1384kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11648
replaced with revised version Wed, 24 Jan 2024 03:04:25 GMT   (7069kb,D)

Title: Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal
  Contrastive EHR Modelling with Hierarchical Regularisation
Authors: Heejoon Koo
Categories: cs.LG cs.AI cs.IR
Comments: Accepted to EACL 2024 (The 18th Conference of the European Chapter of
  the Association for Computational Linguistics)
\\ ( https://arxiv.org/abs/2401.11648 ,  7069kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11694
replaced with revised version Tue, 23 Jan 2024 20:06:38 GMT   (6016kb,D)

Title: Parametric Matrix Models
Authors: Patrick Cook, Danny Jammooa, Morten Hjorth-Jensen, Daniel D. Lee, Dean
  Lee
Categories: cs.LG cond-mat.dis-nn nucl-th physics.comp-ph quant-ph
\\ ( https://arxiv.org/abs/2401.11694 ,  6016kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11798
replaced with revised version Tue, 23 Jan 2024 22:25:56 GMT   (815kb,D)

Title: Knowledge Distillation on Spatial-Temporal Graph Convolutional Network
  for Traffic Prediction
Authors: Mohammad Izadi, Mehran Safayani, Abdolreza Mirzaei
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.11798 ,  815kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12233
replaced with revised version Wed, 24 Jan 2024 08:39:26 GMT   (5013kb,D)

Title: Memorization in Self-Supervised Learning Improves Downstream
  Generalization
Authors: Wenhao Wang, Muhammad Ahmad Kaleem, Adam Dziedzic, Michael Backes,
  Nicolas Papernot, Franziska Boenisch
Categories: cs.LG
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2401.12233 ,  5013kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12617
replaced with revised version Wed, 24 Jan 2024 12:49:24 GMT   (2266kb,D)

Title: The Joint Effect of Task Similarity and Overparameterization on
  Catastrophic Forgetting -- An Analytical Model
Authors: Daniel Goldfarb, Itay Evron, Nir Weinberger, Daniel Soudry, Paul Hand
Categories: cs.LG
Comments: Accepted to the Twelfth International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2401.12617 ,  2266kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12722
replaced with revised version Wed, 24 Jan 2024 04:43:05 GMT   (2715kb,D)

Title: Falcon: Fair Active Learning using Multi-armed Bandits
Authors: Ki Hyun Tae, Hantian Zhang, Jaeyoung Park, Kexin Rong, Steven Euijong
  Whang
Categories: cs.LG
Comments: Accepted to VLDB 2024
\\ ( https://arxiv.org/abs/2401.12722 ,  2715kb)
------------------------------------------------------------------------------
\\
arXiv:2302.14648
replaced with revised version Wed, 24 Jan 2024 08:45:26 GMT   (521kb,D)

Title: Digital Over-the-Air Federated Learning in Multi-Antenna Systems
Authors: Sihua Wang and Mingzhe Chen and Cong Shen and Changchuan Yin and
  Christopher G. Brinton
Categories: cs.IT cs.AI cs.LG math.IT
\\ ( https://arxiv.org/abs/2302.14648 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00527
replaced with revised version Wed, 24 Jan 2024 10:48:54 GMT   (418kb,D)

Title: Graph Neural Networks based Log Anomaly Detection and Explanation
Authors: Zhong Li, Jiayang Shi, Matthijs van Leeuwen
Categories: cs.SE cs.AI cs.LG
Comments: Technical Report (A short version was accepted by ICSE'24 poster
  track)
\\ ( https://arxiv.org/abs/2307.00527 ,  418kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12547
replaced with revised version Wed, 24 Jan 2024 03:22:32 GMT   (388kb,D)

Title: Knapsack: Connectedness, Path, and Shortest-Path
Authors: Palash Dey, Sudeshna Kolay, and Sipra Singh
Categories: cs.DS cs.AI
Comments: Accepted in LATIN 2024
\\ ( https://arxiv.org/abs/2307.12547 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16706
replaced with revised version Wed, 24 Jan 2024 05:45:27 GMT   (1136kb)

Title: Continuous-Time Distributed Dynamic Programming for Networked
  Multi-Agent Markov Decision Processes
Authors: Donghwan Lee, Han-Dong Lim, and Do Wan Kim
Categories: eess.SY cs.AI cs.SY
\\ ( https://arxiv.org/abs/2307.16706 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08917
replaced with revised version Wed, 24 Jan 2024 06:36:56 GMT   (38380kb,D)

Title: An Incremental Unified Framework for Small Defect Inspection
Authors: Jiaqi Tang, Hao Lu, Xiaogang Xu, Ruizheng Wu, Sixing Hu, Tong Zhang,
  Tsz Wa Cheng, Ming Ge, Ying-Cong Chen and Fugee Tsung
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.08917 ,  38380kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09442 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 21:56:34 GMT   (500kb)

Title: A Compact LSTM-SVM Fusion Model for Long-Duration Cardiovascular
  Diseases Detection
Authors: Siyang Wu
Categories: eess.SP cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.09442 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10369
replaced with revised version Wed, 24 Jan 2024 01:57:11 GMT   (37kb,D)

Title: Proportional Representation in Metric Spaces and Low-Distortion
  Committee Selection
Authors: Yusuf Hakan Kalayci and David Kempe and Vikram Kher
Categories: cs.GT cs.AI
Comments: 24 pages, Accepted to AAAI 24
\\ ( https://arxiv.org/abs/2312.10369 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00496
replaced with revised version Tue, 23 Jan 2024 23:30:57 GMT   (7124kb,D)

Title: SAR-RARP50: Segmentation of surgical instrumentation and Action
  Recognition on Robot-Assisted Radical Prostatectomy Challenge
Authors: Dimitrios Psychogyios, Emanuele Colleoni, Beatrice Van Amsterdam,
  Chih-Yang Li, Shu-Yu Huang, Yuchong Li, Fucang Jia, Baosheng Zou, Guotai
  Wang, Yang Liu, Maxence Boels, Jiayu Huo, Rachel Sparks, Prokar Dasgupta,
  Alejandro Granados, Sebastien Ourselin, Mengya Xu, An Wang, Yanan Wu, Long
  Bai, Hongliang Ren, Atsushi Yamada, Yuriko Harai, Yuto Ishikawa, Kazuyuki
  Hayashi, Jente Simoens, Pieter DeBacker, Francesco Cisternino, Gabriele
  Furnari, Alex Mottrie, Federica Ferraguti, Satoshi Kondo, Satoshi Kasai,
  Kousuke Hirasawa, Soohee Kim, Seung Hyun Lee, Kyu Eun Lee, Hyoun-Joong Kong,
  Kui Fu, Chao Li, Shan An, Stefanie Krell, Sebastian Bodenstedt, Nicolas
  Ayobi, Alejandra Perez, Santiago Rodriguez, Juanita Puentes, Pablo Arbelaez,
  Omid Mohareri, Danail Stoyanov
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.00496 ,  7124kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05535 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 02:58:54 GMT   (3000kb,D)

Title: Improving the Accuracy and Interpretability of Random Forests via Forest
  Pruning
Authors: Albert Dorador
Categories: stat.ML cs.AI cs.LG math.OC
\\ ( https://arxiv.org/abs/2401.05535 ,  3000kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05975
replaced with revised version Wed, 24 Jan 2024 03:19:00 GMT   (7743kb,D)

Title: Online Differentiable Clustering for Intent Learning in Recommendation
Authors: Yue Liu, Shihao Zhu, Jun Xia, Yingwei Ma, Jian Ma, Wenliang Zhong,
  Guannan Zhang, Kejun Zhang, Xinwang Liu
Categories: cs.IR cs.AI
Comments: 16 pages
\\ ( https://arxiv.org/abs/2401.05975 ,  7743kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06461
replaced with revised version Wed, 24 Jan 2024 14:57:42 GMT   (14545kb,D)

Title: Between Lines of Code: Unraveling the Distinct Patterns of Machine and
  Human Programmers
Authors: Yuling Shi, Hongyu Zhang, Chengcheng Wan, Xiaodong Gu
Categories: cs.SE cs.AI cs.CL
Comments: paper submitted to IEEE Transactions on Software Engineering, code
  available at https://github.com/YerbaPage/DetectCodeGPT
\\ ( https://arxiv.org/abs/2401.06461 ,  14545kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08396
replaced with revised version Wed, 24 Jan 2024 17:12:51 GMT   (8286kb,D)

Title: Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine
Authors: Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu, Justin M. Cheung,
  Robert Chen, Ronald M. Summers, Justin F. Rousseau, Peiyun Ni, Marc J
  Landsman, Sally L. Baxter, Subhi J. Al'Aref, Yijia Li, Michael F. Chiang,
  Yifan Peng, Zhiyong Lu
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.08396 ,  8286kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11792
replaced with revised version Wed, 24 Jan 2024 09:04:03 GMT   (10362kb,D)

Title: Safe and Generalized end-to-end Autonomous Driving System with
  Reinforcement Learning and Demonstrations
Authors: Zuojin Tang, Xiaoyu Chen, YongQiang Li, Jianyu Chen
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.11792 ,  10362kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08845
replaced with revised version Wed, 24 Jan 2024 04:41:01 GMT   (335kb,D)

Title: Large Language Models are Zero-Shot Rankers for Recommender Systems
Authors: Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian
  McAuley, Wayne Xin Zhao
Categories: cs.IR cs.CL
Comments: Accepted by ECIR 2024
\\ ( https://arxiv.org/abs/2305.08845 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07414 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 08:29:09 GMT   (361kb,D)

Title: PromptASR for contextualized ASR with controllable style
Authors: Xiaoyu Yang, Wei Kang, Zengwei Yao, Yifan Yang, Liyong Guo, Fangjun
  Kuang, Long Lin, Daniel Povey
Categories: eess.AS cs.CL cs.SD
Comments: Proc. ICASSP 2024
\\ ( https://arxiv.org/abs/2309.07414 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09084
replaced with revised version Wed, 24 Jan 2024 10:56:24 GMT   (141kb,D)

Title: Language Modeling on a SpiNNaker 2 Neuromorphic Chip
Authors: Khaleelulla Khan Nazeer, Mark Sch\"one, Rishav Mukherji, Bernhard
  Vogginger, Christian Mayr, David Kappel, Anand Subramoney
Categories: cs.NE cs.CL cs.ET cs.LG
\\ ( https://arxiv.org/abs/2312.09084 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2211.08262 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 20:32:20 GMT   (7043kb,D)

Title: A mixed-categorical correlation kernel for Gaussian process
Authors: P. Saves and Y. Diouane and N. Bartoli and T. Lefebvre and J. Morlier
Categories: math.OC cs.LG stat.ML
Comments: Published in Neurocomputing. 10.1016/j.neucom.2023.126472
Journal-ref: Neurocomputing (2023)
DOI: 10.1016/j.neucom.2023.126472
\\ ( https://arxiv.org/abs/2211.08262 ,  7043kb)
------------------------------------------------------------------------------
\\
arXiv:2301.02344
replaced with revised version Wed, 24 Jan 2024 17:49:12 GMT   (17081kb,D)

Title: TrojanPuzzle: Covertly Poisoning Code-Suggestion Models
Authors: Hojjat Aghakhani, Wei Dai, Andre Manoel, Xavier Fernandes, Anant
  Kharkar, Christopher Kruegel, Giovanni Vigna, David Evans, Ben Zorn, and
  Robert Sim
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2301.02344 ,  17081kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11824
replaced with revised version Wed, 24 Jan 2024 04:54:40 GMT   (1717kb,D)

Title: PECAN: A Deterministic Certified Defense Against Backdoor Attacks
Authors: Yuhao Zhang, Aws Albarghouthi, Loris D'Antoni
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2301.11824 ,  1717kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12838 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 00:39:57 GMT   (2511kb,D)

Title: A Multimodal Graph Neural Network Framework of Cancer Molecular Subtype
  Classification
Authors: Bingjun Li, Sheida Nabavi
Categories: q-bio.GN cs.LG
Comments: 18 pages, 4 figure
\\ ( https://arxiv.org/abs/2302.12838 ,  2511kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10728
replaced with revised version Tue, 23 Jan 2024 23:31:36 GMT   (14030kb,D)

Title: Training Deep Boltzmann Networks with Sparse Ising Machines
Authors: Shaila Niazi, Navid Anjum Aadit, Masoud Mohseni, Shuvro Chowdhury, Yao
  Qin, and Kerem Y. Camsari
Categories: cs.ET cs.LG cs.NE
\\ ( https://arxiv.org/abs/2303.10728 ,  14030kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02803
replaced with revised version Wed, 24 Jan 2024 09:47:27 GMT   (659kb,D)

Title: Tensor PCA from basis in tensor space
Authors: Claudio Turchetti, Laura Falaschetti
Categories: math.NA cs.CV cs.LG cs.NA
Comments: This version contains a new experiment better showing the
  potentiality of the paper and a corrected autor list. This work has been
  submitted to the IEEE for possible publication. Copyright may be transferred
  without notice, after which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2305.02803 ,  659kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07730 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 21:44:40 GMT   (7210kb,D)

Title: Learning in Inverse Optimization: Incenter Cost, Augmented Suboptimality
  Loss, and Algorithms
Authors: Pedro Zattoni Scroccaro, Bilge Atasoy, Peyman Mohajerin Esfahani
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2305.07730 ,  7210kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10623
replaced with revised version Wed, 24 Jan 2024 12:06:49 GMT   (1551kb,D)

Title: GaitPT: Skeletons Are All You Need For Gait Recognition
Authors: Andy Catruna, Adrian Cosma and Emilian Radoi
Categories: cs.CV cs.LG
MSC-class: 68U10
\\ ( https://arxiv.org/abs/2308.10623 ,  1551kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02292 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 11:10:07 GMT   (5548kb,D)

Title: Inferring effective couplings with Restricted Boltzmann Machines
Authors: Aur\'elien Decelle, Cyril Furtlehner, Alfonso De Jesus Navas G\'omez,
  Beatriz Seoane
Categories: cond-mat.dis-nn cond-mat.stat-mech cs.LG
Comments: 17 figures, 39 pages
\\ ( https://arxiv.org/abs/2309.02292 ,  5548kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03619
replaced with revised version Wed, 24 Jan 2024 13:37:11 GMT   (6434kb,D)

Title: Understanding Self-Supervised Learning of Speech Representation via
  Invariance and Redundancy Reduction
Authors: Yusuf Brima, Ulf Krumnack, Simone Pika and Gunther Heidemann
Categories: cs.SD cs.LG eess.AS
Comments: 13 pages, 5 figures, in submission to MDPI Information
\\ ( https://arxiv.org/abs/2309.03619 ,  6434kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07937 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 15:36:31 GMT   (132kb,D)

Title: Voxtlm: unified decoder-only models for consolidating speech
  recognition/synthesis and speech/text continuation tasks
Authors: Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang,
  Shinji Watanabe
Categories: eess.AS cs.LG cs.SD
\\ ( https://arxiv.org/abs/2309.07937 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15717 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 13:43:03 GMT   (408kb,D)

Title: Timbre-Trap: A Low-Resource Framework for Instrument-Agnostic Music
  Transcription
Authors: Frank Cwitkowitz, Kin Wai Cheuk, Woosung Choi, Marco A.
  Mart\'inez-Ram\'irez, Keisuke Toyama, Wei-Hsiang Liao, Yuki Mitsufuji
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2309.15717 ,  408kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07223
replaced with revised version Wed, 24 Jan 2024 08:11:02 GMT   (30947kb,D)

Title: Bidirectional recurrent imputation and abundance estimation of LULC
  classes with MODIS multispectral time series and geo-topographic and climatic
  data
Authors: Jos\'e Rodr\'iguez-Ortega (1 and 2), Rohaifa Khaldi (2), Domingo
  Alcaraz-Segura (3), Siham Tabik (1) ((1) Department of Computer Science and
  Artificial Intelligence, DaSCI, University of Granada, Granada, Spain, (2)
  LifeWatch-ERIC ICT Core, Seville, Spain, (3) Department of Botany, Faculty of
  Science, University of Granada, Granada, Spain)
Categories: cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2310.07223 ,  30947kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05836 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 15:54:17 GMT   (4078kb,D)

Title: UMedNeRF: Uncertainty-aware Single View Volumetric Rendering for Medical
  Neural Radiance Fields
Authors: Jing Hu, Qinrui Fan, Shu Hu, Siwei Lyu, Xi Wu, Xin Wang
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.05836 ,  4078kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14828 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 17:07:55 GMT   (3778kb,D)

Title: Deep Latent Force Models: ODE-based Process Convolutions for Bayesian
  Deep Learning
Authors: Thomas Baldwin-McDonald, Mauricio A. \'Alvarez
Categories: stat.ML cs.LG
Comments: 31 pages, 6 figures. Introduction and abstract updated. arXiv admin
  note: text overlap with arXiv:2106.05960
\\ ( https://arxiv.org/abs/2311.14828 ,  3778kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09493 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 17:18:39 GMT   (22142kb,D)

Title: Identifying Three-Dimensional Radiative Patterns Associated with Early
  Tropical Cyclone Intensification
Authors: Frederick Iat-Hin Tam, Tom Beucler, James H. Ruppert Jr
Categories: physics.ao-ph cs.LG
Comments: 12 pages, 4 figures (main text)
\\ ( https://arxiv.org/abs/2401.09493 ,  22142kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10451
replaced with revised version Wed, 24 Jan 2024 17:59:46 GMT   (2180kb,D)

Title: Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian
  Optimization Approach
Authors: Aron Brenner, Rahman Khorramfar, Dharik Mallapragada, Saurabh Amin
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2401.10451 ,  2180kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12509
replaced with revised version Wed, 24 Jan 2024 01:56:12 GMT   (1595kb)

Title: Digital cloning of online social networks for language-sensitive
  agent-based modeling of misinformation spread
Authors: Prateek Puri, Gabriel Hassler, Anton Shenk, Sai Katragadda
Categories: cs.SI cs.LG
\\ ( https://arxiv.org/abs/2401.12509 ,  1595kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12764 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 02:02:38 GMT   (36kb)

Title: Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving
  $O(1/k)$ Finite-Sample Complexity
Authors: Thinh T. Doan
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2401.12764 ,  36kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
