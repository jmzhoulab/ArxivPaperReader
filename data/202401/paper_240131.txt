Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月31日 15:45
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon 29 Jan 24 19:00:00 GMT  to  Tue 30 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.16580
Date: Mon, 29 Jan 2024 21:31:54 GMT   (138kb,D)

Title: Attention-based Reinforcement Learning for Combinatorial Optimization:
  Application to Job Shop Scheduling Problem
Authors: Jaejin Lee, Seho Kee, Mani Janakiram and George Runger
Categories: cs.AI
\\
  Job shop scheduling problems are one of the most important and challenging
combinatorial optimization problems that have been tackled mainly by exact or
approximate solution approaches. However, finding an exact solution can be
infeasible for real-world problems, and even with an approximate solution
approach, it can require a prohibitive amount of time to find a near-optimal
solution, and the found solutions are not applicable to new problems in
general. To address these challenges, we propose an attention-based
reinforcement learning method for the class of job shop scheduling problems by
integrating policy gradient reinforcement learning with a modified transformer
architecture. An important result is that our trained learners in the proposed
method can be reused to solve large-scale problems not used in training and
demonstrate that our approach outperforms the results of recent studies and
widely adopted heuristic rules.
\\ ( https://arxiv.org/abs/2401.16580 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16657
Date: Tue, 30 Jan 2024 01:22:18 GMT   (7099kb,D)

Title: Recovering Mental Representations from Large Language Models with Markov
  Chain Monte Carlo
Authors: Jian-Qiao Zhu and Haijiang Yan and Thomas L. Griffiths
Categories: cs.AI cs.CL
\\
  Simulating sampling algorithms with people has proven a useful method for
efficiently probing and understanding their mental representations. We propose
that the same methods can be used to study the representations of Large
Language Models (LLMs). While one can always directly prompt either humans or
LLMs to disclose their mental representations introspectively, we show that
increased efficiency can be achieved by using LLMs as elements of a sampling
algorithm. We explore the extent to which we recover human-like representations
when LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo
(MCMC). We found a significant increase in efficiency and performance using
adaptive sampling algorithms based on MCMC. We also highlight the potential of
our method to yield a more general method of conducting Bayesian inference
\textit{with} LLMs.
\\ ( https://arxiv.org/abs/2401.16657 ,  7099kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16744
Date: Tue, 30 Jan 2024 04:48:43 GMT   (1722kb,D)

Title: ShaRP: Explaining Rankings with Shapley Values
Authors: Venetia Pliatsika and Joao Fonseca and Tilun Wang and Julia
  Stoyanovich
Categories: cs.AI cs.CY
Comments: 8 pages
\\
  Algorithmic decisions in critical domains such as hiring, college admissions,
and lending are often based on rankings. Because of the impact these decisions
have on individuals, organizations, and population groups, there is a need to
understand them: to know whether the decisions are abiding by the law, to help
individuals improve their rankings, and to design better ranking procedures.
  In this paper, we present ShaRP (Shapley for Rankings and Preferences), a
framework that explains the contributions of features to different aspects of a
ranked outcome, and is based on Shapley values. Using ShaRP, we show that even
when the scoring function used by an algorithmic ranker is known and linear,
the weight of each feature does not correspond to its Shapley value
contribution. The contributions instead depend on the feature distributions,
and on the subtle local interactions between the scoring features. ShaRP builds
on the Quantitative Input Influence framework, and can compute the
contributions of features for multiple Quantities of Interest, including score,
rank, pair-wise preference, and top-k. Because it relies on black-box access to
the ranker, ShaRP can be used to explain both score-based and learned ranking
models. We show results of an extensive experimental validation of ShaRP using
real and synthetic datasets, showcasing its usefulness for qualitative
analysis.
\\ ( https://arxiv.org/abs/2401.16744 ,  1722kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17044
Date: Tue, 30 Jan 2024 14:26:04 GMT   (405kb,D)

Title: Scalable Mechanism Design for Multi-Agent Path Finding
Authors: Paul Friedrich, Yulun Zhang, Michael Curry, Ludwig Dierks, Stephen
  McAleer, Jiaoyang Li, Tuomas Sandholm, Sven Seuken
Categories: cs.AI cs.GT cs.MA
Comments: 12 pages, 5 figures. Submitted to IJCAI'24
\\
  Multi-Agent Path Finding (MAPF) involves determining paths for multiple
agents to travel simultaneously through a shared area toward particular goal
locations. This problem is computationally complex, especially when dealing
with large numbers of agents, as is common in realistic applications like
autonomous vehicle coordination. Finding an optimal solution is often
computationally infeasible, making the use of approximate algorithms essential.
Adding to the complexity, agents might act in a self-interested and strategic
way, possibly misrepresenting their goals to the MAPF algorithm if it benefits
them. Although the field of mechanism design offers tools to align incentives,
using these tools without careful consideration can fail when only having
access to approximately optimal outcomes. Since approximations are crucial for
scalable MAPF algorithms, this poses a significant challenge. In this work, we
introduce the problem of scalable mechanism design for MAPF and propose three
strategyproof mechanisms, two of which even use approximate MAPF algorithms. We
test our mechanisms on realistic MAPF domains with problem sizes ranging from
dozens to hundreds of agents. Our findings indicate that they improve welfare
beyond a simple baseline.
\\ ( https://arxiv.org/abs/2401.17044 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17045
Date: Tue, 30 Jan 2024 14:27:37 GMT   (44kb)

Title: Explaining Explanations in Probabilistic Logic Programming
Authors: Germ\'an Vidal
Categories: cs.AI cs.PL
\\
  The emergence of tools based on artificial intelligence has also led to the
need of producing explanations which are understandable by a human being. In
some approaches, the system is not transparent (often referred to as a "black
box"), making it difficult to generate appropriate explanations. In this work,
though, we consider probabilistic logic programming, a combination of logic
programming (for knowledge representation) and probability (to model
uncertainty). In this setting, one can say that models are interpretable, which
eases its understanding. However, given a particular query, the usual notion of
"explanation" is associated with a set of choices, one for each random variable
of the model. Unfortunately, this set does not have a causal structure and, in
fact, some of the choices are actually irrelevant to the considered query. In
order to overcome these shortcomings, we present an approach to explaining
explanations which is based on the definition of a query-driven inference
mechanism for probabilistic logic programs.
\\ ( https://arxiv.org/abs/2401.17045 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17159
Date: Tue, 30 Jan 2024 16:47:30 GMT   (1294kb,D)

Title: Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis
Authors: Zhengyang Lu, Stefan Siemer, Piyush Jha, Joel Day, Florin Manea, Vijay
  Ganesh
Categories: cs.AI cs.LO cs.SE
\\
  Modern SMT solvers, such as Z3, offer user-controllable strategies, enabling
users to tailor them for their unique set of instances, thus dramatically
enhancing solver performance for their use case. However, this approach of
strategy customization presents a significant challenge: handcrafting an
optimized strategy for a class of SMT instances remains a complex and demanding
task for both solver developers and users alike.
  In this paper, we address this problem of automatic SMT strategy synthesis
via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats
strategy synthesis as a sequential decision-making process, whose search tree
corresponds to the strategy space, and employs MCTS to navigate this vast
search space. The key innovations that enable our method to identify effective
strategies, while keeping costs low, are the ideas of layered and staged MCTS
search. These novel approaches allow for a deeper and more efficient
exploration of the strategy space, enabling us to synthesize more effective
strategies than the default ones in state-of-the-art (SOTA) SMT solvers. We
implement our method, dubbed Z3alpha, as part of the Z3 SMT solver. Through
extensive evaluations across 6 important SMT logics, Z3alpha demonstrates
superior performance compared to the SOTA synthesis tool FastSMT, the default
Z3 solver, and the CVC5 solver on most benchmarks. Remarkably, on a challenging
QF_BV benchmark set, Z3alpha solves 42.7% more instances than the default
strategy in the Z3 SMT solver.
\\ ( https://arxiv.org/abs/2401.17159 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16475
Date: Mon, 29 Jan 2024 19:00:01 GMT   (3097kb,D)

Title: InfoLossQA: Characterizing and Recovering Information Loss in Text
  Simplification
Authors: Jan Trienes, Sebastian Joseph, J\"org Schl\"otterer, Christin Seifert,
  Kyle Lo, Wei Xu, Byron C. Wallace, Junyi Jessy Li
Categories: cs.CL
\\
  Text simplification aims to make technical texts more accessible to laypeople
but often results in deletion of information and vagueness. This work proposes
InfoLossQA, a framework to characterize and recover simplification-induced
information loss in form of question-and-answer (QA) pairs. Building on the
theory of Question Under Discussion, the QA pairs are designed to help readers
deepen their knowledge of a text. We conduct a range of experiments with this
framework. First, we collect a dataset of 1,000 linguist-curated QA pairs
derived from 104 LLM simplifications of scientific abstracts of medical
studies. Our analyses of this data reveal that information loss occurs
frequently, and that the QA pairs give a high-level overview of what
information was lost. Second, we devise two methods for this task: end-to-end
prompting of open-source and commercial language models, and a natural language
inference pipeline. With a novel evaluation framework considering the
correctness of QA pairs and their linguistic suitability, our expert evaluation
reveals that models struggle to reliably identify information loss and applying
similar standards as humans at what constitutes information loss.
\\ ( https://arxiv.org/abs/2401.16475 ,  3097kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16541
Date: Mon, 29 Jan 2024 20:20:44 GMT   (176kb,D)

Title: GuReT: Distinguishing Guilt and Regret related Text
Authors: Sabur Butt, Fazlourrahman Balouchzahi, Abdul Gafar Manuel Meque, Maaz
  Amjad, Hector G. Ceballos Cancino, Grigori Sidorov, Alexander Gelbukh
Categories: cs.CL cs.AI
\\
  The intricate relationship between human decision-making and emotions,
particularly guilt and regret, has significant implications on behavior and
well-being. Yet, these emotions subtle distinctions and interplay are often
overlooked in computational models. This paper introduces a dataset tailored to
dissect the relationship between guilt and regret and their unique textual
markers, filling a notable gap in affective computing research. Our approach
treats guilt and regret recognition as a binary classification task and employs
three machine learning and six transformer-based deep learning techniques to
benchmark the newly created dataset. The study further implements innovative
reasoning methods like chain-of-thought and tree-of-thought to assess the
models interpretive logic. The results indicate a clear performance edge for
transformer-based models, achieving a 90.4% macro F1 score compared to the
85.3% scored by the best machine learning classifier, demonstrating their
superior capability in distinguishing complex emotional states.
\\ ( https://arxiv.org/abs/2401.16541 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16553
Date: Mon, 29 Jan 2024 20:44:10 GMT   (10251kb,D)

Title: SelectLLM: Can LLMs Select Important Instructions to Annotate?
Authors: Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang
Categories: cs.CL cs.AI
Comments: First Authors: Ritik Sachin Parkar and Jaehyung Kim | Second Author:
  Jong Inn Park | PI: Dongyeop Kang
\\
  Training large language models (LLMs) with a large and diverse instruction
dataset aligns the models to comprehend and follow human instructions. Recent
works have shown that using a small set of high-quality instructions can
outperform using large yet more noisy ones. Because instructions are unlabeled
and their responses are natural text, traditional active learning schemes with
the model's confidence cannot be directly applied to the selection of unlabeled
instructions. In this work, we propose a novel method for instruction
selection, called SelectLLM, that leverages LLMs for the selection of
high-quality instructions. Our high-level idea is to use LLMs to estimate the
usefulness and impactfulness of each instruction without the corresponding
labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing
the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to
multiple clusters, and then prompting LLMs to choose high-quality instructions
within each cluster. SelectLLM showed comparable or slightly better performance
on the popular instruction benchmarks, compared to the recent state-of-the-art
selection methods. All code and data are publicly available
(https://github.com/minnesotanlp/select-llm).
\\ ( https://arxiv.org/abs/2401.16553 ,  10251kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16561
Date: Mon, 29 Jan 2024 20:58:43 GMT   (192kb)

Title: Multi-class Regret Detection in Hindi Devanagari Script
Authors: Renuka Sharma, Sushama Nagpal, Sangeeta Sabharwal, Sabur Butt
Categories: cs.CL cs.AI
\\
  The number of Hindi speakers on social media has increased dramatically in
recent years. Regret is a common emotional experience in our everyday life.
Many speakers on social media, share their regretful experiences and opinions
regularly. It might cause a re-evaluation of one's choices and a desire to make
a different option if given the chance. As a result, knowing the source of
regret is critical for investigating its impact on behavior and
decision-making. This study focuses on regret and how it is expressed,
specifically in Hindi, on various social media platforms. In our study, we
present a novel dataset from three different sources, where each sentence has
been manually classified into one of three classes "Regret by action", "Regret
by inaction", and "No regret". Next, we use this dataset to investigate the
linguistic expressions of regret in Hindi text and also identify the textual
domains that are most frequently associated with regret. Our findings indicate
that individuals on social media platforms frequently express regret for both
past inactions and actions, particularly within the domain of interpersonal
relationships. We use a pre-trained BERT model to generate word embeddings for
the Hindi dataset and also compare deep learning models with conventional
machine learning models in order to demonstrate accuracy. Our results show that
BERT embedding with CNN consistently surpassed other models. This described the
effectiveness of BERT for conveying the context and meaning of words in the
regret domain.
\\ ( https://arxiv.org/abs/2401.16561 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16575
Date: Mon, 29 Jan 2024 21:22:23 GMT   (10195kb,D)

Title: Beyond Image-Text Matching: Verb Understanding in Multimodal
  Transformers Using Guided Masking
Authors: Ivana Be\v{n}ov\'a, Jana Ko\v{s}eck\'a, Michal Gregor, Martin Tamajka,
  Marcel Vesel\'y, Mari\'an \v{S}imko
Categories: cs.CL cs.CV
Comments: 9 pages of text, 11 pages total, 7 figures, 3 tables, preprint
\\
  The dominant probing approaches rely on the zero-shot performance of
image-text matching tasks to gain a finer-grained understanding of the
representations learned by recent multimodal image-language transformer models.
The evaluation is carried out on carefully curated datasets focusing on
counting, relations, attributes, and others. This work introduces an
alternative probing strategy called guided masking. The proposed approach
ablates different modalities using masking and assesses the model's ability to
predict the masked word with high accuracy. We focus on studying multimodal
models that consider regions of interest (ROI) features obtained by object
detectors as input tokens. We probe the understanding of verbs using guided
masking on ViLBERT, LXMERT, UNITER, and VisualBERT and show that these models
can predict the correct verb with high accuracy. This contrasts with previous
conclusions drawn from image-text matching probing techniques that frequently
fail in situations requiring verb understanding. The code for all experiments
will be publicly available https://github.com/ivana-13/guided_masking.
\\ ( https://arxiv.org/abs/2401.16575 ,  10195kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16577
Date: Mon, 29 Jan 2024 21:24:10 GMT   (2125kb,D)

Title: LLMs as On-demand Customizable Service
Authors: Souvika Sarkar, Mohammad Fakhruddin Babar, Monowar Hasan, Shubhra
  Kanti Karmaker (Santu)
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have demonstrated remarkable language
understanding and generation capabilities. However, training, deploying, and
accessing these models pose notable challenges, including resource-intensive
demands, extended training durations, and scalability issues. To address these
issues, we introduce a concept of hierarchical, distributed LLM architecture
that aims at enhancing the accessibility and deployability of LLMs across
heterogeneous computing platforms, including general-purpose computers (e.g.,
laptops) and IoT-style devices (e.g., embedded systems). By introducing a
"layered" approach, the proposed architecture enables on-demand accessibility
to LLMs as a customizable service. This approach also ensures optimal
trade-offs between the available computational resources and the user's
application needs. We envision that the concept of hierarchical LLM will
empower extensive, crowd-sourced user bases to harness the capabilities of
LLMs, thereby fostering advancements in AI technology in general.
\\ ( https://arxiv.org/abs/2401.16577 ,  2125kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16578
Date: Mon, 29 Jan 2024 21:24:43 GMT   (1755kb,D)

Title: Leveraging Professional Radiologists' Expertise to Enhance LLMs'
  Evaluation for Radiology Reports
Authors: Qingqing Zhu, Xiuying Chen, Qiao Jin, Benjamin Hou, Tejas Sudharshan
  Mathai, Pritam Mukherjee, Xin Gao, Ronald M Summers, Zhiyong Lu
Categories: cs.CL cs.AI
\\
  In radiology, Artificial Intelligence (AI) has significantly advanced report
generation, but automatic evaluation of these AI-produced reports remains
challenging. Current metrics, such as Conventional Natural Language Generation
(NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic
intricacies of clinical contexts or overemphasize clinical details, undermining
report clarity. To overcome these issues, our proposed method synergizes the
expertise of professional radiologists with Large Language Models (LLMs), like
GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain
of Thought (CoT) reasoning, our approach aligns LLM evaluations with
radiologist standards, enabling detailed comparisons between human and AI
generated reports. This is further enhanced by a Regression model that
aggregates sentence evaluation scores. Experimental results show that our
''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the
METEOR metric by 0.19, while our ''Regressed GPT-4'' model shows even greater
alignment with expert evaluations, exceeding the best existing metric by a 0.35
margin. Moreover, the robustness of our explanations has been validated through
a thorough iterative strategy. We plan to publicly release annotations from
radiology experts, setting a new standard for accuracy in future assessments.
This underscores the potential of our approach in enhancing the quality
assessment of AI-driven medical reports.
\\ ( https://arxiv.org/abs/2401.16578 ,  1755kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16582
Date: Mon, 29 Jan 2024 21:33:08 GMT   (35994kb,D)

Title: Massively Multilingual Text Translation For Low-Resource Languages
Authors: Zhong Zhou
Categories: cs.CL
Comments: 191 pages. Published on Dec 20, 2023. Ph.D. thesis @ Language
  Technology Institute, School of Computer Science, Carnegie Mellon University.
  https://doi.org/10.1184/R1/24992787.v1
\\
  Translation into severely low-resource languages has both the cultural goal
of saving and reviving those languages and the humanitarian goal of assisting
the everyday needs of local communities that are accelerated by the recent
COVID-19 pandemic. In many humanitarian efforts, translation into severely
low-resource languages often does not require a universal translation engine,
but a dedicated text-specific translation engine. For example, healthcare
records, hygienic procedures, government communication, emergency procedures
and religious texts are all limited texts. While generic translation engines
for all languages do not exist, translation of multilingually known limited
texts into new, low-resource languages may be possible and reduce human
translation effort. We attempt to leverage translation resources from
rich-resource languages to efficiently produce best possible translation
quality for well known texts, which are available in multiple languages, in a
new, low-resource language. To reach this goal, we argue that in translating a
closed text into low-resource languages, generalization to out-of-domain texts
is not necessary, but generalization to new languages is. Performance gain
comes from massive source parallelism by careful choice of close-by language
families, style-consistent corpus-level paraphrases within the same language
and strategic adaptation of existing large pretrained multilingual models to
the domain first and then to the language. Such performance gain makes it
possible for machine translation systems to collaborate with human translators
to expedite the translation process into new, low-resource languages.
\\ ( https://arxiv.org/abs/2401.16582 ,  35994kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16587
Date: Mon, 29 Jan 2024 21:43:27 GMT   (7173kb,D)

Title: A Linguistic Comparison between Human and ChatGPT-Generated
  Conversations
Authors: Morgan Sandler, Hyesun Choung, Arun Ross, Prabu David
Categories: cs.CL cs.AI cs.CY
Comments: Preprint. Pending review and feedback from ICPRAI2024
\\
  This study explores linguistic differences between human and LLM-generated
dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the
EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word
Count (LIWC) analysis, comparing ChatGPT-generated conversations with human
conversations across 118 linguistic categories. Results show greater
variability and authenticity in human dialogues, but ChatGPT excels in
categories such as social processes, analytical style, cognition, attentional
focus, and positive emotional tone, reinforcing recent findings of LLMs being
"more human than human." However, no significant difference was found in
positive or negative affect between ChatGPT and human dialogues. Classifier
analysis of dialogue embeddings indicates implicit coding of the valence of
affect despite no explicit mention of affect in the conversations. The research
also contributes a novel, companion ChatGPT-generated dataset of conversations
between two independent chatbots, which were designed to replicate a corpus of
human conversations available for open access and used widely in AI research on
language modeling. Our findings increase understanding of ChatGPT's linguistic
capabilities and inform ongoing efforts to distinguish between human and
LLM-generated text, which is critical in detecting AI-generated fakes,
misinformation, and disinformation.
\\ ( https://arxiv.org/abs/2401.16587 ,  7173kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16589
Date: Mon, 29 Jan 2024 21:44:27 GMT   (9266kb,D)

Title: ToPro: Token-Level Prompt Decomposition for Cross-Lingual Sequence
  Labeling Tasks
Authors: Bolei Ma, Ercong Nie, Shuzhou Yuan, Helmut Schmid, Michael F\"arber,
  Frauke Kreuter and Hinrich Sch\"utze
Categories: cs.CL
Comments: EACL 2024
\\
  Prompt-based methods have been successfully applied to multilingual
pretrained language models for zero-shot cross-lingual understanding. However,
most previous studies primarily focused on sentence-level classification tasks,
and only a few considered token-level labeling tasks such as Named Entity
Recognition (NER) and Part-of-Speech (POS) tagging. In this paper, we propose
Token-Level Prompt Decomposition (ToPro), which facilitates the prompt-based
method for token-level sequence labeling tasks. The ToPro method decomposes an
input sentence into single tokens and applies one prompt template to each
token. Our experiments on multilingual NER and POS tagging datasets demonstrate
that ToPro-based fine-tuning outperforms Vanilla fine-tuning and Prompt-Tuning
in zero-shot cross-lingual transfer, especially for languages that are
typologically different from the source language English. Our method also
attains state-of-the-art performance when employed with the mT5 model. Besides,
our exploratory study in multilingual large language models shows that ToPro
performs much better than the current in-context learning method. Overall, the
performance improvements show that ToPro could potentially serve as a novel and
simple benchmarking method for sequence labeling tasks.
\\ ( https://arxiv.org/abs/2401.16589 ,  9266kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16638
Date: Tue, 30 Jan 2024 00:23:29 GMT   (1022kb,D)

Title: Breaking Free Transformer Models: Task-specific Context Attribution
  Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs
Authors: Stepan Tytarenko, Mohammad Ruhul Amin
Categories: cs.CL cs.AI
Comments: 8 pages, 3 figures, 5 tables, To be published in 2024 AAAI workshop
  on Responsible Language Models (ReLM)
ACM-class: I.2.7; I.2.4
\\
  Fine-tuning large pre-trained language models (LLMs) on particular datasets
is a commonly employed strategy in Natural Language Processing (NLP)
classification tasks. However, this approach usually results in a loss of
models generalizability. In this paper, we present a framework that allows for
maintaining generalizability, and enhances the performance on the downstream
task by utilizing task-specific context attribution. We show that a linear
transformation of the text representation from any transformer model using the
task-specific concept operator results in a projection onto the latent concept
space, referred to as context attribution in this paper. The specific concept
operator is optimized during the supervised learning stage via novel loss
functions. The proposed framework demonstrates that context attribution of the
text representation for each task objective can improve the capacity of the
discriminator function and thus achieve better performance for the
classification task. Experimental results on three datasets, namely HateXplain,
IMDB reviews, and Social Media Attributions, illustrate that the proposed model
attains superior accuracy and generalizability. Specifically, for the
non-fine-tuned BERT on the HateXplain dataset, we observe 8% improvement in
accuracy and 10% improvement in F1-score. Whereas for the IMDB dataset,
fine-tuned state-of-the-art XLNet is outperformed by 1% for both accuracy and
F1-score. Furthermore, in an out-of-domain cross-dataset test, DistilBERT
fine-tuned on the IMDB dataset in conjunction with the proposed model improves
the F1-score on the HateXplain dataset by 7%. For the Social Media Attributions
dataset of YouTube comments, we observe 5.2% increase in F1-metric. The
proposed framework is implemented with PyTorch and provided open-source on
GitHub.
\\ ( https://arxiv.org/abs/2401.16638 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16640
Date: Tue, 30 Jan 2024 00:25:54 GMT   (2405kb,D)

Title: TeenyTinyLlama: open-source tiny language models trained in Brazilian
  Portuguese
Authors: Nicholas Kluge Corr\^ea, Sophia Falk, Shiza Fatimah, Aniket Sen,
  Nythamar de Oliveira
Categories: cs.CL cs.LG
Comments: 21 pages, 5 figures
\\
  Large language models (LLMs) have significantly advanced natural language
processing, but their progress has yet to be equal across languages. While most
LLMs are trained in high-resource languages like English, multilingual models
generally underperform monolingual ones. Additionally, aspects of their
multilingual foundation sometimes restrict the byproducts they produce, like
computational demands and licensing regimes. In this study, we document the
development of open-foundation models tailored for use in low-resource
settings, their limitations, and their benefits. This is the TeenyTinyLlama
pair: two compact models for Brazilian Portuguese text generation. We release
them under the permissive Apache 2.0 license on GitHub and Hugging Face for
community use and further development. See
https://github.com/Nkluge-correa/TeenyTinyLlama
\\ ( https://arxiv.org/abs/2401.16640 ,  2405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16646
Date: Tue, 30 Jan 2024 00:40:49 GMT   (600kb,D)

Title: Incoherent Probability Judgments in Large Language Models
Authors: Jian-Qiao Zhu and Thomas L. Griffiths
Categories: cs.CL cs.AI
\\
  Autoregressive Large Language Models (LLMs) trained for next-word prediction
have demonstrated remarkable proficiency at producing coherent text. But are
they equally adept at forming coherent probability judgments? We use
probabilistic identities and repeated judgments to assess the coherence of
probability judgments made by LLMs. Our results show that the judgments
produced by these models are often incoherent, displaying human-like systematic
deviations from the rules of probability theory. Moreover, when prompted to
judge the same event, the mean-variance relationship of probability judgments
produced by LLMs shows an inverted-U-shaped like that seen in humans. We
propose that these deviations from rationality can be explained by linking
autoregressive LLMs to implicit Bayesian inference and drawing parallels with
the Bayesian Sampler model of human probability judgments.
\\ ( https://arxiv.org/abs/2401.16646 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16656
Date: Tue, 30 Jan 2024 01:19:25 GMT   (105kb,D)

Title: Gradient-Based Language Model Red Teaming
Authors: Nevan Wichers, Carson Denison, Ahmad Beirami
Categories: cs.CL
Comments: EACL 2024 main conference
\\
  Red teaming is a common strategy for identifying weaknesses in generative
language models (LMs), where adversarial prompts are produced that trigger an
LM to generate unsafe responses. Red teaming is instrumental for both model
alignment and evaluation, but is labor-intensive and difficult to scale when
done by humans. In this paper, we present Gradient-Based Red Teaming (GBRT), a
red teaming method for automatically generating diverse prompts that are likely
to cause an LM to output unsafe responses. GBRT is a form of prompt learning,
trained by scoring an LM response with a safety classifier and then
backpropagating through the frozen safety classifier and LM to update the
prompt. To improve the coherence of input prompts, we introduce two variants
that add a realism loss and fine-tune a pretrained model to generate the
prompts instead of learning the prompts directly. Our experiments show that
GBRT is more effective at finding prompts that trigger an LM to generate unsafe
responses than a strong reinforcement learning-based red teaming approach, and
succeeds even when the LM has been fine-tuned to produce safer outputs.
\\ ( https://arxiv.org/abs/2401.16656 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16658
Date: Tue, 30 Jan 2024 01:22:18 GMT   (35kb)

Title: OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on
  E-Branchformer
Authors: Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan,
  Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang,
  Jee-weon Jung, Shinji Watanabe
Categories: cs.CL eess.AS
Comments: Project webpage: https://www.wavlab.org/activities/2024/owsm/
\\
  Recent studies have advocated for fully open foundation models to promote
transparency and open science. As an initial step, the Open Whisper-style
Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data
and open-source toolkits. With the aim of reproducing Whisper, the previous
OWSM v1 through v3 models were still based on Transformer, which might lead to
inferior performance compared to other state-of-the-art speech encoders. In
this work, we aim to improve the performance and efficiency of OWSM without
extra training data. We present E-Branchformer based OWSM v3.1 models at two
scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based
speech model that has been made publicly available. It outperforms the previous
OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to
25% faster inference speed. We publicly release the data preparation scripts,
pre-trained models and training logs.
\\ ( https://arxiv.org/abs/2401.16658 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16678
Date: Tue, 30 Jan 2024 01:57:17 GMT   (77kb,D)

Title: The Detection and Understanding of Fictional Discourse
Authors: Andrew Piper, Haiqi Zhou
Categories: cs.CL cs.LG
\\
  In this paper, we present a variety of classification experiments related to
the task of fictional discourse detection. We utilize a diverse array of
datasets, including contemporary professionally published fiction, historical
fiction from the Hathi Trust, fanfiction, stories from Reddit, folk tales,
GPT-generated stories, and anglophone world literature. Additionally, we
introduce a new feature set of word "supersenses" that facilitate the goal of
semantic generalization. The detection of fictional discourse can help enrich
our knowledge of large cultural heritage archives and assist with the process
of understanding the distinctive qualities of fictional storytelling more
broadly.
\\ ( https://arxiv.org/abs/2401.16678 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16727
Date: Tue, 30 Jan 2024 03:51:44 GMT   (8352kb,D)

Title: Recent Advances in Hate Speech Moderation: Multimodality and the Role of
  Large Models
Authors: Ming Shan Hee, Shivam Sharma, Rui Cao, Palash Nandi, Preslav Nakov,
  Tanmoy Chakraborty, Roy Ka-Wei Lee
Categories: cs.CL
\\
  In the evolving landscape of online communication, moderating hate speech
(HS) presents an intricate challenge, compounded by the multimodal nature of
digital content. This comprehensive survey delves into the recent strides in HS
moderation, spotlighting the burgeoning role of large language models (LLMs)
and large multimodal models (LMMs). Our exploration begins with a thorough
analysis of current literature, revealing the nuanced interplay between
textual, visual, and auditory elements in propagating HS. We uncover a notable
trend towards integrating these modalities, primarily due to the complexity and
subtlety with which HS is disseminated. A significant emphasis is placed on the
advances facilitated by LLMs and LMMs, which have begun to redefine the
boundaries of detection and moderation capabilities. We identify existing gaps
in research, particularly in the context of underrepresented languages and
cultures, and the need for solutions to handle low-resource settings. The
survey concludes with a forward-looking perspective, outlining potential
avenues for future research, including the exploration of novel AI
methodologies, the ethical governance of AI in moderation, and the development
of more nuanced, context-aware systems. This comprehensive overview aims to
catalyze further research and foster a collaborative effort towards more
sophisticated, responsible, and human-centric approaches to HS moderation in
the digital era.\footnote{ \textcolor{red}{WARNING: This paper contains
offensive examples.
\\ ( https://arxiv.org/abs/2401.16727 ,  8352kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16731
Date: Tue, 30 Jan 2024 04:06:25 GMT   (1294kb,D)

Title: Towards Generating Informative Textual Description for Neurons in
  Language Models
Authors: Shrayani Mondal, Rishabh Garodia, Arbaaz Qureshi, Taesung Lee and
  Youngja Park
Categories: cs.CL cs.AI
Comments: AAAI 2024
\\
  Recent developments in transformer-based language models have allowed them to
capture a wide variety of world knowledge that can be adapted to downstream
tasks with limited resources. However, what pieces of information are
understood in these models is unclear, and neuron-level contributions in
identifying them are largely unknown. Conventional approaches in neuron
explainability either depend on a finite set of pre-defined descriptors or
require manual annotations for training a secondary model that can then explain
the neurons of the primary model. In this paper, we take BERT as an example and
we try to remove these constraints and propose a novel and scalable framework
that ties textual descriptions to neurons. We leverage the potential of
generative language models to discover human-interpretable descriptors present
in a dataset and use an unsupervised approach to explain neurons with these
descriptors. Through various qualitative and quantitative analyses, we
demonstrate the effectiveness of this framework in generating useful
data-specific descriptors with little human involvement in identifying the
neurons that encode these descriptors. In particular, our experiment shows that
the proposed approach achieves 75% precision@2, and 50% recall@2
\\ ( https://arxiv.org/abs/2401.16731 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16736
Date: Tue, 30 Jan 2024 04:29:48 GMT   (110kb,D)

Title: Engineering A Large Language Model From Scratch
Authors: Abiodun Finbarrs Oketunji
Categories: cs.CL cs.CY cs.LG cs.SE
MSC-class: I.2.7
ACM-class: I.2.7
DOI: 10.13140/RG.2.2.28532.73600; 10.5281/zenodo.10581937
\\
  The proliferation of deep learning in natural language processing (NLP) has
led to the development and release of innovative technologies capable of
understanding and generating human language with remarkable proficiency.
Atinuke, a Transformer-based neural network, optimises performance across
various language tasks by utilising a unique configuration. The architecture
interweaves layers for processing sequential data with attention mechanisms to
draw meaningful affinities between inputs and outputs. Due to the configuration
of its topology and hyperparameter tuning, it can emulate human-like language
by extracting features and learning complex mappings. Atinuke is modular,
extensible, and integrates seamlessly with existing machine learning pipelines.
Advanced matrix operations like softmax, embeddings, and multi-head attention
enable nuanced handling of textual, acoustic, and visual signals. By unifying
modern deep learning techniques with software design principles and
mathematical theory, the system achieves state-of-the-art results on natural
language tasks whilst remaining interpretable and robust.
\\ ( https://arxiv.org/abs/2401.16736 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16745
Date: Tue, 30 Jan 2024 04:50:28 GMT   (7191kb,D)

Title: MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large
  Language Models
Authors: Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li,
  Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong
Categories: cs.CL
Comments: Code and data are available at
  https://github.com/KwanWaiChung/MT-Eval
\\
  Large language models (LLMs) are increasingly relied upon for complex
multi-turn conversations across diverse real-world applications. However,
existing benchmarks predominantly focus on single-turn evaluations, overlooking
the models' capabilities in multi-turn interactions. To address this gap, we
introduce MT-Eval, a comprehensive benchmark designed to evaluate multi-turn
conversational abilities. By analyzing human-LLM conversations, we categorize
interaction patterns into four types: recollection, expansion, refinement, and
follow-up. We construct multi-turn queries for each category either by
augmenting existing datasets or by creating new examples with GPT-4 to avoid
data leakage. To study the factors impacting multi-turn abilities, we create
single-turn versions of the 1170 multi-turn queries and compare performance.
Our evaluation of 11 well-known LLMs shows that while closed-source models
generally surpass open-source ones, certain open-source models exceed
GPT-3.5-Turbo in specific tasks. We observe significant performance degradation
in multi-turn settings compared to single-turn settings in most models, which
is not correlated with the models' fundamental capabilities. Moreover, we
identify the distance to relevant content and susceptibility to error
propagation as the key factors influencing multi-turn performance. MT-Eval is
released publicly to encourage future research towards more robust
conversational models.
\\ ( https://arxiv.org/abs/2401.16745 ,  7191kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16748
Date: Tue, 30 Jan 2024 04:56:55 GMT   (877kb,D)

Title: Detecting Racist Text in Bengali: An Ensemble Deep Learning Framework
Authors: S. S. Saruar, Nusrat, Sadia
Categories: cs.CL
\\
  Racism is an alarming phenomenon in our country as well as all over the
world. Every day we have come across some racist comments in our daily life and
virtual life. Though we can eradicate this racism from virtual life (such as
Social Media). In this paper, we have tried to detect those racist comments
with NLP and deep learning techniques. We have built a novel dataset in the
Bengali Language. Further, we annotated the dataset and conducted data label
validation. After extensive utilization of deep learning methodologies, we have
successfully achieved text detection with an impressive accuracy rate of
87.94\% using the Ensemble approach. We have applied RNN and LSTM models using
BERT Embeddings. However, the MCNN-LSTM model performed highest among all those
models. Lastly, the Ensemble approach has been followed to combine all the
model results to increase overall performance.
\\ ( https://arxiv.org/abs/2401.16748 ,  877kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16788
Date: Tue, 30 Jan 2024 07:03:32 GMT   (7879kb,D)

Title: Can Large Language Models be Trusted for Evaluation? Scalable
  Meta-Evaluation of LLMs as Evaluators via Agent Debate
Authors: Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu
Categories: cs.CL cs.AI
\\
  Despite the utility of Large Language Models (LLMs) across a wide range of
tasks and scenarios, developing a method for reliably evaluating LLMs across
varied contexts continues to be challenging. Modern evaluation approaches often
use LLMs to assess responses generated by LLMs. However, the meta-evaluation
conducted to assess the effectiveness of these LLMs as evaluators is typically
constrained by the coverage of existing benchmarks or requires extensive human
annotation. This underscores the urgency of methods for scalable
meta-evaluation that can effectively, reliably, and efficiently evaluate the
performance of LLMs as evaluators across diverse tasks and scenarios,
particularly in potentially new, user-defined scenarios. To fill this gap, we
propose ScaleEval, an agent-debate-assisted meta-evaluation framework that
leverages the capabilities of multiple communicative LLM agents. This framework
supports multi-round discussions to assist human annotators in discerning the
most capable LLMs as evaluators, which significantly eases their workload in
cases that used to require large-scale annotations during meta-evaluation. We
release the code for our framework, which is publicly available at:
\url{https://github.com/GAIR-NLP/scaleeval}.
\\ ( https://arxiv.org/abs/2401.16788 ,  7879kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16818
Date: Tue, 30 Jan 2024 08:45:08 GMT   (591kb)

Title: H2O-Danube-1.8B Technical Report
Authors: Philipp Singer, Pascal Pfeiffer, Yauhen Babakhin, Maximilian Jeblick,
  Nischay Dhankhar, Gabor Fodor, Sri Satish Ambati
Categories: cs.CL cs.LG
\\
  We present H2O-Danube-1.8B, a 1.8B language model trained on 1T tokens
following the core principles of LLama 2 and Mistral. We leverage and refine
various techniques for pre-training large language models. Although our model
is trained on significantly fewer total tokens compared to reference models of
similar size, it exhibits highly competitive metrics across a multitude of
benchmarks. We additionally release a chat model trained with supervised
fine-tuning followed by direct preference optimization. We make H2O-Danube-1.8B
openly available under Apache 2.0 license further democratizing LLMs to a wider
audience economically.
\\ ( https://arxiv.org/abs/2401.16818 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16862
Date: Tue, 30 Jan 2024 10:05:03 GMT   (983kb,D)

Title: State Value Generation with Prompt Learning and Self-Training for
  Low-Resource Dialogue State Tracking
Authors: Ming Gu, Yan Yang, Chengcai Chen, Zhou Yu
Categories: cs.CL
Comments: Accepted by ACML 2023
\\
  Recently, low-resource dialogue state tracking (DST) has received increasing
attention. First obtaining state values then based on values to generate slot
types has made great progress in this task. However, obtaining state values is
still an under-studied problem. Existing extraction-based approaches cannot
capture values that require the understanding of context and are not
generalizable either. To address these issues, we propose a novel State VAlue
Generation based framework (SVAG), decomposing DST into state value generation
and domain slot generation. Specifically, we propose to generate state values
and use self-training to further improve state value generation. Moreover, we
design an estimator aiming at detecting incomplete generation and incorrect
generation for pseudo-labeled data selection during self-training. Experimental
results on the MultiWOZ 2.1 dataset show that our method which has only less
than 1 billion parameters achieves state-of-the-art performance under the data
ratio settings of 5%, 10%, and 25% when limited to models under 100 billion
parameters. Compared to models with more than 100 billion parameters, SVAG
still reaches competitive results.
\\ ( https://arxiv.org/abs/2401.16862 ,  983kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16895
Date: Tue, 30 Jan 2024 11:04:36 GMT   (7857kb,D)

Title: Cross-Lingual Transfer from Related Languages: Treating Low-Resource
  Maltese as Multilingual Code-Switching
Authors: Kurt Micallef, Nizar Habash, Claudia Borg, Fadhl Eryani, Houda Bouamor
Categories: cs.CL
Comments: EACL 2024 camera-ready version
\\
  Although multilingual language models exhibit impressive cross-lingual
transfer capabilities on unseen languages, the performance on downstream tasks
is impacted when there is a script disparity with the languages used in the
multilingual model's pre-training data. Using transliteration offers a
straightforward yet effective means to align the script of a resource-rich
language with a target language, thereby enhancing cross-lingual transfer
capabilities. However, for mixed languages, this approach is suboptimal, since
only a subset of the language benefits from the cross-lingual transfer while
the remainder is impeded. In this work, we focus on Maltese, a Semitic
language, with substantial influences from Arabic, Italian, and English, and
notably written in Latin script. We present a novel dataset annotated with
word-level etymology. We use this dataset to train a classifier that enables us
to make informed decisions regarding the appropriate processing of each token
in the Maltese language. We contrast indiscriminate transliteration or
translation to mixing processing pipelines that only transliterate words of
Arabic origin, thereby resulting in text with a mixture of scripts. We
fine-tune the processed data on four downstream tasks and show that conditional
transliteration based on word etymology yields the best results, surpassing
fine-tuning with raw Maltese or Maltese processed with non-selective pipelines.
\\ ( https://arxiv.org/abs/2401.16895 ,  7857kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16960
Date: Tue, 30 Jan 2024 12:41:04 GMT   (4672kb,D)

Title: Two Heads Are Better Than One: Integrating Knowledge from Knowledge
  Graphs and Large Language Models for Entity Alignment
Authors: Linyao Yang and Hongyang Chen and Xiao Wang and Jing Yang and Fei-Yue
  Wang and Han Liu
Categories: cs.CL cs.AI
\\
  Entity alignment, which is a prerequisite for creating a more comprehensive
Knowledge Graph (KG), involves pinpointing equivalent entities across disparate
KGs. Contemporary methods for entity alignment have predominantly utilized
knowledge embedding models to procure entity embeddings that encapsulate
various similarities-structural, relational, and attributive. These embeddings
are then integrated through attention-based information fusion mechanisms.
Despite this progress, effectively harnessing multifaceted information remains
challenging due to inherent heterogeneity. Moreover, while Large Language
Models (LLMs) have exhibited exceptional performance across diverse downstream
tasks by implicitly capturing entity semantics, this implicit knowledge has yet
to be exploited for entity alignment. In this study, we propose a Large
Language Model-enhanced Entity Alignment framework (LLMEA), integrating
structural knowledge from KGs with semantic knowledge from LLMs to enhance
entity alignment. Specifically, LLMEA identifies candidate alignments for a
given entity by considering both embedding similarities between entities across
KGs and edit distances to a virtual equivalent entity. It then engages an LLM
iteratively, posing multiple multi-choice questions to draw upon the LLM's
inference capability. The final prediction of the equivalent entity is derived
from the LLM's output. Experiments conducted on three public datasets reveal
that LLMEA surpasses leading baseline models. Additional ablation studies
underscore the efficacy of our proposed framework.
\\ ( https://arxiv.org/abs/2401.16960 ,  4672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16968
Date: Tue, 30 Jan 2024 12:49:40 GMT   (8153kb,D)

Title: Distinguishing Fictional Voices: a Study of Authorship Verification
  Models for Quotation Attribution
Authors: Gaspard Michel, Elena V. Epure, Romain Hennequin, Christophe Cerisara
Categories: cs.CL
Comments: Accepted at EACL 2024's workshop LaTeCH-CLfL
\\
  Recent approaches to automatically detect the speaker of an utterance of
direct speech often disregard general information about characters in favor of
local information found in the context, such as surrounding mentions of
entities. In this work, we explore stylistic representations of characters
built by encoding their quotes with off-the-shelf pretrained Authorship
Verification models in a large corpus of English novels (the Project Dialogism
Novel Corpus). Results suggest that the combination of stylistic and topical
information captured in some of these models accurately distinguish characters
among each other, but does not necessarily improve over semantic-only models
when attributing quotes. However, these results vary across novels and more
investigation of stylometric models particularly tailored for literary texts
and the study of characters should be conducted.
\\ ( https://arxiv.org/abs/2401.16968 ,  8153kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17039
Date: Tue, 30 Jan 2024 14:18:31 GMT   (2970kb,D)

Title: Taking Action Towards Graceful Interaction: The Effects of Performing
  Actions on Modelling Policies for Instruction Clarification Requests
Authors: Brielen Madureira, David Schlangen
Categories: cs.CL
Comments: Accepted to UnImplicit workshop at EACL 2024
\\
  Clarification requests are a mechanism to help solve communication problems,
e.g. due to ambiguity or underspecification, in instruction-following
interactions. Despite their importance, even skilful models struggle with
producing or interpreting such repair acts. In this work, we test three
hypotheses concerning the effects of action taking as an auxiliary task in
modelling iCR policies. Contrary to initial expectations, we conclude that its
contribution to learning an iCR policy is limited, but some information can
still be extracted from prediction uncertainty. We present further evidence
that even well-motivated, Transformer-based models fail to learn good policies
for when to ask Instruction CRs (iCRs), while the task of determining what to
ask about can be more successfully modelled. Considering the implications of
these findings, we further discuss the shortcomings of the data-driven paradigm
for learning meta-communication acts.
\\ ( https://arxiv.org/abs/2401.17039 ,  2970kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17043
Date: Tue, 30 Jan 2024 14:25:32 GMT   (1448kb,D)

Title: CRUD-RAG: A Comprehensive Chinese Benchmark for Retrieval-Augmented
  Generation of Large Language Models
Authors: Yuanjie Lyu, Zhiyu Li, Simin Niu, Feiyu Xiong, Bo Tang, Wenjin Wang,
  Hao Wu, Huanyong Liu, Tong Xu, Enhong Chen
Categories: cs.CL
Comments: 26 Pages
\\
  Retrieval-Augmented Generation (RAG) is a technique that enhances the
capabilities of large language models (LLMs) by incorporating external
knowledge sources. This method addresses common LLM limitations, including
outdated information and the tendency to produce inaccurate "hallucinated"
content. However, the evaluation of RAG systems is challenging, as existing
benchmarks are limited in scope and diversity. Most of the current benchmarks
predominantly assess question-answering applications, overlooking the broader
spectrum of situations where RAG could prove advantageous. Moreover, they only
evaluate the performance of the LLM component of the RAG pipeline in the
experiments, and neglect the influence of the retrieval component and the
external knowledge database. To address these issues, this paper constructs a
large-scale and more comprehensive benchmark, and evaluates all the components
of RAG systems in various RAG application scenarios. Specifically, we have
categorized the range of RAG applications into four distinct types-Create,
Read, Update, and Delete (CRUD), each representing a unique use case. "Create"
refers to scenarios requiring the generation of original, varied content.
"Read" involves responding to intricate questions in knowledge-intensive
situations. "Update" focuses on revising and rectifying inaccuracies or
inconsistencies in pre-existing texts. "Delete" pertains to the task of
summarizing extensive texts into more concise forms. For each of these CRUD
categories, we have developed comprehensive datasets to evaluate the
performance of RAG systems. We also analyze the effects of various components
of the RAG system, such as the retriever, the context length, the knowledge
base construction, and the LLM. Finally, we provide useful insights for
optimizing the RAG technology for different scenarios.
\\ ( https://arxiv.org/abs/2401.17043 ,  1448kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17072
Date: Tue, 30 Jan 2024 14:52:50 GMT   (7817kb,D)

Title: SemScore: Automated Evaluation of Instruction-Tuned LLMs based on
  Semantic Textual Similarity
Authors: Ansar Aynetdinov, Alan Akbik
Categories: cs.CL
\\
  Instruction-tuned Large Language Models (LLMs) have recently showcased
remarkable advancements in their ability to generate fitting responses to
natural language instructions. However, many current works rely on manual
evaluation to judge the quality of generated responses. Since such manual
evaluation is time-consuming, it does not easily scale to the evaluation of
multiple models and model variants. In this short paper, we propose a
straightforward but remarkably effective evaluation metric called SemScore, in
which we directly compare model outputs to gold target responses using semantic
textual similarity (STS). We conduct a comparative evaluation of the model
outputs of 12 prominent instruction-tuned LLMs using 8 widely-used evaluation
metrics for text generation. We find that our proposed SemScore metric
outperforms all other, in many cases more complex, evaluation metrics in terms
of correlation to human evaluation. These findings indicate the utility of our
proposed metric for the evaluation of instruction-tuned LLMs.
\\ ( https://arxiv.org/abs/2401.17072 ,  7817kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17092
Date: Tue, 30 Jan 2024 15:18:29 GMT   (20336kb,D)

Title: NNOSE: Nearest Neighbor Occupational Skill Extraction
Authors: Mike Zhang and Rob van der Goot and Min-Yen Kan and Barbara Plank
Categories: cs.CL
Comments: Accepted at EACL 2024 Main
\\
  The labor market is changing rapidly, prompting increased interest in the
automatic extraction of occupational skills from text. With the advent of
English benchmark job description datasets, there is a need for systems that
handle their diversity well. We tackle the complexity in occupational skill
datasets tasks -- combining and leveraging multiple datasets for skill
extraction, to identify rarely observed skills within a dataset, and overcoming
the scarcity of skills across datasets. In particular, we investigate the
retrieval-augmentation of language models, employing an external datastore for
retrieving similar skills in a dataset-unifying manner. Our proposed method,
\textbf{N}earest \textbf{N}eighbor \textbf{O}ccupational \textbf{S}kill
\textbf{E}xtraction (NNOSE) effectively leverages multiple datasets by
retrieving neighboring skills from other datasets in the datastore. This
improves skill extraction \emph{without} additional fine-tuning. Crucially, we
observe a performance gain in predicting infrequent patterns, with substantial
gains of up to 30\% span-F1 in cross-dataset settings.
\\ ( https://arxiv.org/abs/2401.17092 ,  20336kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17099
Date: Tue, 30 Jan 2024 15:30:03 GMT   (572kb,D)

Title: MT-Ranker: Reference-free machine translation evaluation by inter-system
  ranking
Authors: Ibraheem Muhammad Moosa, Rui Zhang, Wenpeng Yin
Categories: cs.CL
Comments: 18 pages, 4 figures, to be published in ICLR'24, Code available at
  https://github.com/ibraheem-moosa/mt-ranker
ACM-class: I.2.7
\\
  Traditionally, Machine Translation (MT) Evaluation has been treated as a
regression problem -- producing an absolute translation-quality score. This
approach has two limitations: i) the scores lack interpretability, and human
annotators struggle with giving consistent scores; ii) most scoring methods are
based on (reference, translation) pairs, limiting their applicability in
real-world scenarios where references are absent. In practice, we often care
about whether a new MT system is better or worse than some competitors. In
addition, reference-free MT evaluation is increasingly practical and necessary.
Unfortunately, these two practical considerations have yet to be jointly
explored. In this work, we formulate the reference-free MT evaluation into a
pairwise ranking problem. Given the source sentence and a pair of translations,
our system predicts which translation is better. In addition to proposing this
new formulation, we further show that this new paradigm can demonstrate
superior correlation with human judgments by merely using indirect supervision
from natural language inference and weak supervision from our synthetic data.
In the context of reference-free evaluation, MT-Ranker, trained without any
human annotations, achieves state-of-the-art results on the WMT Shared Metrics
Task benchmarks DARR20, MQM20, and MQM21. On a more challenging benchmark,
ACES, which contains fine-grained evaluation criteria such as addition,
omission, and mistranslation errors, MT-Ranker marks state-of-the-art against
reference-free as well as reference-based baselines.
\\ ( https://arxiv.org/abs/2401.17099 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17167
Date: Tue, 30 Jan 2024 16:52:56 GMT   (2996kb,D)

Title: Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool
  Utilization in Real-World Complex Scenarios
Authors: Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen
  Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng
  Xu, Qun Liu
Categories: cs.CL
\\
  The recent trend of using Large Language Models (LLMs) as intelligent agents
in real-world applications underscores the necessity for comprehensive
evaluations of their capabilities, particularly in complex scenarios involving
planning, creating, and using tools. However, existing benchmarks typically
focus on simple synthesized queries that do not reflect real-world complexity,
thereby offering limited perspectives in evaluating tool utilization. To
address this issue, we present UltraTool, a novel benchmark designed to improve
and evaluate LLMs' ability in tool utilization within real-world scenarios.
UltraTool focuses on the entire process of using tools - from planning and
creating to applying them in complex tasks. It emphasizes real-world
complexities, demanding accurate, multi-step planning for effective
problem-solving. A key feature of UltraTool is its independent evaluation of
planning with natural language, which happens before tool usage and simplifies
the task solving by mapping out the intermediate steps. Thus, unlike previous
work, it eliminates the restriction of pre-defined toolset during planning.
Through extensive experiments on various LLMs, we offer novel insights into the
evaluation of capabilities of LLMs in tool utilization, thereby contributing a
fresh perspective to this rapidly evolving field. The benchmark is publicly
available at https://github.com/JoeYing1019/UltraTool.
\\ ( https://arxiv.org/abs/2401.17167 ,  2996kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17169
Date: Tue, 30 Jan 2024 16:56:54 GMT   (106kb,D)

Title: Conditional and Modal Reasoning in Large Language Models
Authors: Wesley H. Holliday and Matthew Mandelkern
Categories: cs.CL cs.AI cs.LO
Comments: 11 pages, 16 figures. Code and data available at
  https://github.com/wesholliday/llm-logic
MSC-class: 68T50, 03B65
ACM-class: I.2.7
\\
  The reasoning abilities of large language models (LLMs) are the topic of a
growing body of research in artificial intelligence and cognitive science. In
this paper, we probe the extent to which a dozen LLMs are able to distinguish
logically correct inferences from logically fallacious ones. We focus on
inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob
has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must
have a king'). These inference patterns have been of special interest to
logicians, philosophers, and linguists, since they plausibly play a central
role in human reasoning. Assessing LLMs on these inference patterns is thus
highly relevant to the question of how much the reasoning abilities of LLMs
match those of humans. Among the LLMs we tested, all but GPT-4 often make basic
mistakes with conditionals. Moreover, even GPT-4 displays logically
inconsistent judgments across inference patterns involving epistemic modals.
\\ ( https://arxiv.org/abs/2401.17169 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17181
Date: Tue, 30 Jan 2024 17:11:56 GMT   (135kb,D)

Title: Transfer Learning for Text Diffusion Models
Authors: Kehang Han, Kathleen Kenealy, Aditya Barua, Noah Fiedel, Noah Constant
Categories: cs.CL
\\
  In this report, we explore the potential for text diffusion to replace
autoregressive (AR) decoding for the training and deployment of large language
models (LLMs). We are particularly interested to see whether pretrained AR
models can be transformed into text diffusion models through a lightweight
adaptation procedure we call ``AR2Diff''. We begin by establishing a strong
baseline setup for training text diffusion models. Comparing across multiple
architectures and pretraining objectives, we find that training a decoder-only
model with a prefix LM objective is best or near-best across several tasks.
Building on this finding, we test various transfer learning setups for text
diffusion models. On machine translation, we find that text diffusion
underperforms the standard AR approach. However, on code synthesis and
extractive QA, we find diffusion models trained from scratch outperform AR
models in many cases. We also observe quality gains from AR2Diff -- adapting AR
models to use diffusion decoding. These results are promising given that text
diffusion is relatively underexplored and can be significantly faster than AR
decoding for long text generation.
\\ ( https://arxiv.org/abs/2401.17181 ,  135kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17196
Date: Tue, 30 Jan 2024 17:30:44 GMT   (1329kb,D)

Title: Single Word Change is All You Need: Designing Attacks and Defenses for
  Text Classifiers
Authors: Lei Xu, Sarah Alnegheimish, Laure Berti-Equille, Alfredo
  Cuesta-Infante, Kalyan Veeramachaneni
Categories: cs.CL
\\
  In text classification, creating an adversarial example means subtly
perturbing a few words in a sentence without changing its meaning, causing it
to be misclassified by a classifier. A concerning observation is that a
significant portion of adversarial examples generated by existing methods
change only one word. This single-word perturbation vulnerability represents a
significant weakness in classifiers, which malicious users can exploit to
efficiently create a multitude of adversarial examples. This paper studies this
problem and makes the following key contributions: (1) We introduce a novel
metric \r{ho} to quantitatively assess a classifier's robustness against
single-word perturbation. (2) We present the SP-Attack, designed to exploit the
single-word perturbation vulnerability, achieving a higher attack success rate,
better preserving sentence meaning, while reducing computation costs compared
to state-of-the-art adversarial methods. (3) We propose SP-Defense, which aims
to improve \r{ho} by applying data augmentation in learning. Experimental
results on 4 datasets and BERT and distilBERT classifiers show that SP-Defense
improves \r{ho} by 14.6% and 13.9% and decreases the attack success rate of
SP-Attack by 30.4% and 21.2% on two classifiers respectively, and decreases the
attack success rate of existing attack methods that involve multiple-word
perturbations.
\\ ( https://arxiv.org/abs/2401.17196 ,  1329kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17206
Date: Tue, 30 Jan 2024 17:47:07 GMT   (1660kb,D)

Title: Gazetteer-Enhanced Bangla Named Entity Recognition with BanglaBERT
  Semantic Embeddings K-Means-Infused CRF Model
Authors: Niloy Farhan, Saman Sarker Joy, Tafseer Binte Mannan, Farig Sadeque
Categories: cs.CL
\\
  Named Entity Recognition (NER) is a sub-task of Natural Language Processing
(NLP) that distinguishes entities from unorganized text into predefined
categorization. In recent years, a lot of Bangla NLP subtasks have received
quite a lot of attention; but Named Entity Recognition in Bangla still lags
behind. In this research, we explored the existing state of research in Bangla
Named Entity Recognition. We tried to figure out the limitations that current
techniques and datasets face, and we would like to address these limitations in
our research. Additionally, We developed a Gazetteer that has the ability to
significantly boost the performance of NER. We also proposed a new NER solution
by taking advantage of state-of-the-art NLP tools that outperform conventional
techniques.
\\ ( https://arxiv.org/abs/2401.17206 ,  1660kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17228
Date: Tue, 30 Jan 2024 18:15:25 GMT   (5419kb,D)

Title: Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding
  Space using Contrastive Learning
Authors: Jeongwoo Park, Enrico Liscio, Pradeep K. Murukannaiah
Categories: cs.CL
Comments: To appear in Findings of EACL 2024
\\
  Recent advances in NLP show that language models retain a discernible level
of knowledge in deontological ethics and moral norms. However, existing works
often treat morality as binary, ranging from right to wrong. This simplistic
view does not capture the nuances of moral judgment. Pluralist moral
philosophers argue that human morality can be deconstructed into a finite
number of elements, respecting individual differences in moral judgment. In
line with this view, we build a pluralist moral sentence embedding space via a
state-of-the-art contrastive learning approach. We systematically investigate
the embedding space by studying the emergence of relationships among moral
elements, both quantitatively and qualitatively. Our results show that a
pluralist approach to morality can be captured in an embedding space. However,
moral pluralism is challenging to deduce via self-supervision alone and
requires a supervised approach with human labels.
\\ ( https://arxiv.org/abs/2401.17228 ,  5419kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17244
Date: Tue, 30 Jan 2024 18:37:45 GMT   (1477kb,D)

Title: LLaMP: Large Language Model Made Powerful for High-fidelity Materials
  Knowledge Retrieval and Distillation
Authors: Yuan Chiang, Chia-Hong Chou, Janosh Riebesell
Categories: cs.CL cond-mat.mtrl-sci cs.AI
Comments: 22 pages, 4 figures
\\
  Reducing hallucination of Large Language Models (LLMs) is imperative for use
in the sciences where reproducibility is crucial. However, LLMs inherently lack
long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to
fine-tune them on domain-specific literature and data. Here we introduce LLaMP,
a multimodal retrieval-augmented generation (RAG) framework of multiple
data-aware reasoning-and-acting (ReAct) agents that dynamically interact with
computational and experimental data on Materials Project (MP). Without
fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various
modalities of materials science concepts, fetch relevant data stores on the
fly, process higher-order data (such as crystal structures and elastic
tensors), and summarize multi-step procedures for solid-state synthesis. We
show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge,
reducing a 5.21% MAPE on frequently-documented bandgaps and a significant
1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from
mixed data sources. Additionally, LLaMP substantially reduces the hallucinated
volumetric strain in a diamond cubic silicon structure from 66.3% to 0. The
proposed framework offers an intuitive and nearly hallucination-free approach
to exploring materials informatics and establishes a pathway for knowledge
distillation and fine-tuning other language models. We envision the framework
as a valuable component for scientific hypotheses and a foundation for future
autonomous laboratories where multiple LLM agents communicate and cooperate
with robotics to drive material synthesis and chemical reactions without
hard-coded human logic and intervention.
\\ ( https://arxiv.org/abs/2401.17244 ,  1477kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17256
Date: Tue, 30 Jan 2024 18:48:37 GMT   (560kb,D)

Title: Weak-to-Strong Jailbreaking on Large Language Models
Authors: Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang
  Wang, William Yang Wang
Categories: cs.CL
\\
  Although significant efforts have been dedicated to aligning large language
models (LLMs), red-teaming reports suggest that these carefully aligned LLMs
could still be jailbroken through adversarial prompts, tuning, or decoding.
Upon examining the jailbreaking vulnerability of aligned LLMs, we observe that
the decoding distributions of jailbroken and aligned models differ only in the
initial generations. This observation motivates us to propose the
weak-to-strong jailbreaking attack, where adversaries can utilize smaller
unsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly
larger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally
decode two smaller LLMs once, which involves minimal computation and latency
compared to decoding the larger LLMs. The efficacy of this attack is
demonstrated through experiments conducted on five models from three different
organizations. Our study reveals a previously unnoticed yet efficient way of
jailbreaking, exposing an urgent safety issue that needs to be considered when
aligning LLMs. As an initial attempt, we propose a defense strategy to protect
against such attacks, but creating more advanced defenses remains challenging.
The code for replicating the method is available at
https://github.com/XuandongZhao/weak-to-strong
\\ ( https://arxiv.org/abs/2401.17256 ,  560kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17268
Date: Tue, 30 Jan 2024 18:58:43 GMT   (821kb,D)

Title: Weaver: Foundation Models for Creative Writing
Authors: Tiannan Wang, Jiamin Chen, Qingrui Jia, Shuai Wang, Ruoyu Fang, Huilin
  Wang, Zhaowei Gao, Chunzhao Xie, Chuou Xu, Jihong Dai, Yibin Liu, Jialong Wu,
  Shengwei Ding, Long Li, Zhiwei Huang, Xinle Deng, Teng Yu, Gangan Ma, Han
  Xiao, Zixin Chen, Danjun Xiang, Yunxia Wang, Yuanyuan Zhu, Yi Xiao, Jing
  Wang, Yiru Wang, Siran Ding, Jiayang Huang, Jiayi Xu, Yilihamu Tayier, Zhenyu
  Hu, Yuan Gao, Chengfeng Zheng, Yueshu Ye, Yihang Li, Lei Wan, Xinyue Jiang,
  Yujie Wang, Siyu Cheng, Zhule Song, Xiangru Tang, Xiaohua Xu, Ningyu Zhang,
  Huajun Chen, Yuchen Eleanor Jiang, and Wangchunshu Zhou
Categories: cs.CL cs.AI cs.LG
\\
  This work introduces Weaver, our first family of large language models (LLMs)
dedicated to content creation. Weaver is pre-trained on a carefully selected
corpus that focuses on improving the writing capabilities of large language
models. We then fine-tune Weaver for creative and professional writing purposes
and align it to the preference of professional writers using a suit of novel
methods for instruction data synthesis and LLM alignment, making it able to
produce more human-like texts and follow more diverse instructions for content
creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver
Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for
different applications and can be dynamically dispatched by a routing agent
according to query complexity to balance response quality and computation cost.
Evaluation on a carefully curated benchmark for assessing the writing
capabilities of LLMs shows Weaver models of all sizes outperform generalist
LLMs several times larger than them. Notably, our most-capable Weaver Ultra
model surpasses GPT-4, a state-of-the-art generalist LLM, on various writing
scenarios, demonstrating the advantage of training specialized LLMs for writing
purposes. Moreover, Weaver natively supports retrieval-augmented generation
(RAG) and function calling (tool usage). We present various use cases of these
abilities for improving AI-assisted writing systems, including integration of
external knowledge bases, tools, or APIs, and providing personalized writing
assistance. Furthermore, we discuss and summarize a guideline and best
practices for pre-training and fine-tuning domain-specific LLMs.
\\ ( https://arxiv.org/abs/2401.17268 ,  821kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16426
Date: Wed, 29 Nov 2023 09:32:56 GMT   (19kb)

Title: Informal Safety Guarantees for Simulated Optimizers Through
  Extrapolation from Partial Simulations
Authors: Luke Marks
Categories: cs.LG
Comments: 17 pages, 0 figures
ACM-class: I.2.0
\\
  Self-supervised learning is the backbone of state of the art language
modeling. It has been argued that training with predictive loss on a
self-supervised dataset causes simulators: entities that internally represent
possible configurations of real-world systems. Under this assumption, a
mathematical model for simulators is built based in the Cartesian frames model
of embedded agents, which is extended to multi-agent worlds through scaling a
two-dimensional frame to arbitrary dimensions, where literature prior chooses
to instead use operations on frames. This variant leveraging scaling
dimensionality is named the Cartesian object, and is used to represent
simulations (where individual simulacra are the agents and devices in that
object). Around the Cartesian object, functions like token selection and
simulation complexity are accounted for in formalizing the behavior of a
simulator, and used to show (through the L\"obian obstacle) that a proof of
alignment between simulacra by inspection of design is impossible in the
simulator context. Following this, a scheme is proposed and termed Partial
Simulation Extrapolation aimed at circumventing the L\"obian obstacle through
the evaluation of low-complexity simulations.
\\ ( https://arxiv.org/abs/2401.16426 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16438
Date: Fri, 26 Jan 2024 21:51:49 GMT   (100kb,D)

Title: Do deep neural networks utilize the weight space efficiently?
Authors: Onur Can Koyun, Beh\c{c}et U\u{g}ur T\"oreyin
Categories: cs.LG cs.AI
\\
  Deep learning models like Transformers and Convolutional Neural Networks
(CNNs) have revolutionized various domains, but their parameter-intensive
nature hampers deployment in resource-constrained settings. In this paper, we
introduce a novel concept utilizes column space and row space of weight
matrices, which allows for a substantial reduction in model parameters without
compromising performance. Leveraging this paradigm, we achieve
parameter-efficient deep learning models.. Our approach applies to both
Bottleneck and Attention layers, effectively halving the parameters while
incurring only minor performance degradation. Extensive experiments conducted
on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of
our method, showcasing competitive performance when compared to traditional
models. This approach not only addresses the pressing demand for parameter
efficient deep learning solutions but also holds great promise for practical
deployment in real-world scenarios.
\\ ( https://arxiv.org/abs/2401.16438 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16439
Date: Sat, 27 Jan 2024 06:38:31 GMT   (29kb)

Title: Polynomial time auditing of statistical subgroup fairness for Gaussian
  data
Authors: Daniel Hsu, Jizhou Huang, Brendan Juba
Categories: cs.LG cs.CC cs.CY
\\
  We study the problem of auditing classifiers with the notion of statistical
subgroup fairness. Kearns et al. (2018) has shown that the problem of auditing
combinatorial subgroups fairness is as hard as agnostic learning. Essentially
all work on remedying statistical measures of discrimination against subgroups
assumes access to an oracle for this problem, despite the fact that no
efficient algorithms are known for it. If we assume the data distribution is
Gaussian, or even merely log-concave, then a recent line of work has discovered
efficient agnostic learning algorithms for halfspaces. Unfortunately, the
boosting-style reductions given by Kearns et al. required the agnostic learning
algorithm to succeed on reweighted distributions that may not be log-concave,
even if the original data distribution was. In this work, we give positive and
negative results on auditing for the Gaussian distribution: On the positive
side, we an alternative approach to leverage these advances in agnostic
learning and thereby obtain the first polynomial-time approximation scheme
(PTAS) for auditing nontrivial combinatorial subgroup fairness: we show how to
audit statistical notions of fairness over homogeneous halfspace subgroups when
the features are Gaussian. On the negative side, we find that under
cryptographic assumptions, no polynomial-time algorithm can guarantee any
nontrivial auditing, even under Gaussian feature distributions, for general
halfspace subgroups.
\\ ( https://arxiv.org/abs/2401.16439 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16440
Date: Sat, 27 Jan 2024 09:29:11 GMT   (24602kb,D)

Title: Beyond Eviction Prediction: Leveraging Local Spatiotemporal Public
  Records to Inform Action
Authors: Tasfia Mashiat, Alex DiChristofano, Patrick J. Fowler, Sanmay Das
Categories: cs.LG cs.AI
\\
  There has been considerable recent interest in scoring properties on the
basis of eviction risk. The success of methods for eviction prediction is
typically evaluated using different measures of predictive accuracy. However,
the underlying goal of such prediction is to direct appropriate assistance to
households that may be at greater risk so they remain stably housed. Thus, we
must ask the question of how useful such predictions are in targeting outreach
efforts - informing action. In this paper, we investigate this question using a
novel dataset that matches information on properties, evictions, and owners. We
perform an eviction prediction task to produce risk scores and then use these
risk scores to plan targeted outreach policies. We show that the risk scores
are, in fact, useful, enabling a theoretical team of caseworkers to reach more
eviction-prone properties in the same amount of time, compared to outreach
policies that are either neighborhood-based or focus on buildings with a recent
history of evictions. We also discuss the importance of neighborhood and
ownership features in both risk prediction and targeted outreach.
\\ ( https://arxiv.org/abs/2401.16440 ,  24602kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16441
Date: Sat, 27 Jan 2024 13:29:17 GMT   (2450kb,D)

Title: FaKnow: A Unified Library for Fake News Detection
Authors: Yiyuan Zhu, Yongjun Li, Jialiang Wang, Ming Gao, Jiali Wei
Categories: cs.LG cs.AI cs.CL
\\
  Over the past years, a large number of fake news detection algorithms based
on deep learning have emerged. However, they are often developed under
different frameworks, each mandating distinct utilization methodologies,
consequently hindering reproducibility. Additionally, a substantial amount of
redundancy characterizes the code development of such fake news detection
models. To address these concerns, we propose FaKnow, a unified and
comprehensive fake news detection algorithm library. It encompasses a variety
of widely used fake news detection models, categorized as content-based and
social context-based approaches. This library covers the full spectrum of the
model training and evaluation process, effectively organizing the data, models,
and training procedures within a unified framework. Furthermore, it furnishes a
series of auxiliary functionalities and tools, including visualization, and
logging. Our work contributes to the standardization and unification of fake
news detection research, concurrently facilitating the endeavors of researchers
in this field. The open-source code and documentation can be accessed at
https://github.com/NPURG/FaKnow and https://faknow.readthedocs.io,
respectively.
\\ ( https://arxiv.org/abs/2401.16441 ,  2450kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16449
Date: Sun, 28 Jan 2024 20:38:58 GMT   (4925kb,D)

Title: AI in Energy Digital Twining: A Reinforcement Learning-based Adaptive
  Digital Twin Model for Green Cities
Authors: Lal Verda Cakir, Kubra Duran, Craig Thomson, Matthew Broadbent, and
  Berk Canberk
Categories: cs.LG
\\
  Digital Twins (DT) have become crucial to achieve sustainable and effective
smart urban solutions. However, current DT modelling techniques cannot support
the dynamicity of these smart city environments. This is caused by the lack of
right-time data capturing in traditional approaches, resulting in inaccurate
modelling and high resource and energy consumption challenges. To fill this
gap, we explore spatiotemporal graphs and propose the Reinforcement
Learning-based Adaptive Twining (RL-AT) mechanism with Deep Q Networks (DQN).
By doing so, our study contributes to advancing Green Cities and showcases
tangible benefits in accuracy, synchronisation, resource optimization, and
energy efficiency. As a result, we note the spatiotemporal graphs are able to
offer a consistent accuracy and 55% higher querying performance when
implemented using graph databases. In addition, our model demonstrates
right-time data capturing with 20% lower overhead and 25% lower energy
consumption.
\\ ( https://arxiv.org/abs/2401.16449 ,  4925kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16452
Date: Mon, 29 Jan 2024 06:05:14 GMT   (778kb,D)

Title: Context-Former: Stitching via Latent Conditioned Sequence Modeling
Authors: Ziqi Zhang, Jingzehua Xu, Zifeng Zhuang, Jinxin Liu, Donglin wang
Categories: cs.LG cs.AI
\\
  Offline reinforcement learning (RL) algorithms can improve the decision
making via stitching sub-optimal trajectories to obtain more optimal ones. This
capability is a crucial factor in enabling RL to learn policies that are
superior to the behavioral policy. On the other hand, Decision Transformer (DT)
abstracts the decision-making as sequence modeling, showcasing competitive
performance on offline RL benchmarks, however, recent studies demonstrate that
DT lacks of stitching capability, thus exploit stitching capability for DT is
vital to further improve its performance. In order to endow stitching
capability to DT, we abstract trajectory stitching as expert matching and
introduce our approach, ContextFormer, which integrates contextual
information-based imitation learning (IL) and sequence modeling to stitch
sub-optimal trajectory fragments by emulating the representations of a limited
number of expert trajectories. To validate our claim, we conduct experiments
from two perspectives: 1) We conduct extensive experiments on D4RL benchmarks
under the settings of IL, and experimental results demonstrate ContextFormer
can achieve competitive performance in multi-IL settings. 2) More importantly,
we conduct a comparison of ContextFormer with diverse competitive DT variants
using identical training datasets. The experimental results unveiled
ContextFormer's superiority, as it outperformed all other variants, showcasing
its remarkable performance.
\\ ( https://arxiv.org/abs/2401.16452 ,  778kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16453
Date: Mon, 29 Jan 2024 06:17:23 GMT   (1191kb)

Title: Hybrid Transformer and Spatial-Temporal Self-Supervised Learning for
  Long-term Traffic Prediction
Authors: Wang Zhu, Doudou Zhang, Baichao Long, Jianli Xiao
Categories: cs.LG cs.AI
Comments: 22 pages, 10 figures
\\
  Long-term traffic prediction has always been a challenging task due to its
dynamic temporal dependencies and complex spatial dependencies. In this paper,
we propose a model that combines hybrid Transformer and spatio-temporal
self-supervised learning. The model enhances its robustness by applying
adaptive data augmentation techniques at the sequence-level and graph-level of
the traffic data. It utilizes Transformer to overcome the limitations of
recurrent neural networks in capturing long-term sequences, and employs
Chebyshev polynomial graph convolution to capture complex spatial dependencies.
Furthermore, considering the impact of spatio-temporal heterogeneity on traffic
speed, we design two self-supervised learning tasks to model the temporal and
spatial heterogeneity, thereby improving the accuracy and generalization
ability of the model. Experimental evaluations are conducted on two real-world
datasets, PeMS04 and PeMS08, and the results are visualized and analyzed,
demonstrating the superior performance of the proposed model.
\\ ( https://arxiv.org/abs/2401.16453 ,  1191kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16457
Date: Mon, 29 Jan 2024 09:15:50 GMT   (1159kb,D)

Title: Effective Controllable Bias Mitigation for Classification and Retrieval
  using Gate Adapters
Authors: Shahed Masoudian, Cornelia Volaucnik, Markus Schedl, Shahed Masoudian
Categories: cs.LG cs.AI cs.CY
\\
  Bias mitigation of Language Models has been the topic of many studies with a
recent focus on learning separate modules like adapters for on-demand
debiasing. Besides optimizing for a modularized debiased model, it is often
critical in practice to control the degree of bias reduction at inference time,
e.g., in order to tune for a desired performance-fairness trade-off in search
results or to control the strength of debiasing in classification tasks. In
this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular
gating mechanism with adjustable sensitivity parameters, which allows for a
gradual transition from the biased state of the model to the fully debiased
version at inference time. We demonstrate ConGater performance by (1)
conducting adversarial debiasing experiments with three different models on
three classification tasks with four protected attributes, and (2) reducing the
bias of search results through fairness list-wise regularization to enable
adjusting a trade-off between performance and fairness metrics. Our experiments
on the classification tasks show that compared to baselines of the same
caliber, ConGater can maintain higher task performance while containing less
information regarding the attributes. Our results on the retrieval task show
that the fully debiased ConGater can achieve the same fairness performance
while maintaining more than twice as high task performance than recent strong
baselines. Overall, besides strong performance ConGater enables the continuous
transitioning between biased and debiased states of models, enhancing
personalization of use and interpretability through controllability.
\\ ( https://arxiv.org/abs/2401.16457 ,  1159kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16462
Date: Mon, 29 Jan 2024 14:38:44 GMT   (4530kb,D)

Title: Supervised Contrastive Learning based Dual-Mixer Model for Remaining
  Useful Life Prediction
Authors: En Fu, Yanyan Hu, Kaixiang Peng and Yuxin Chu
Categories: cs.LG cs.AI
\\
  The problem of the Remaining Useful Life (RUL) prediction, aiming at
providing an accurate estimate of the remaining time from the current
predicting moment to the complete failure of the device, has gained significant
attention from researchers in recent years. In this paper, to overcome the
shortcomings of rigid combination for temporal and spatial features in most
existing RUL prediction approaches, a spatial-temporal homogeneous feature
extractor, named Dual-Mixer model, is firstly proposed. Flexible layer-wise
progressive feature fusion is employed to ensure the homogeneity of
spatial-temporal features and enhance the prediction accuracy. Secondly, the
Feature Space Global Relationship Invariance (FSGRI) training method is
introduced based on supervised contrastive learning. This method maintains the
consistency of relationships among sample features with their degradation
patterns during model training, simplifying the subsequently regression task in
the output layer and improving the model's performance in RUL prediction.
Finally, the effectiveness of the proposed method is validated through
comparisons with other latest research works on the C-MAPSS dataset. The
Dual-Mixer model demonstrates superiority across most metrics, while the FSGRI
training method shows an average improvement of 7.00% and 2.41% in RMSE and
MAPE, respectively, for all baseline models. Our experiments and model code are
publicly available at https://github.com/fuen1590/PhmDeepLearningProjects.
\\ ( https://arxiv.org/abs/2401.16462 ,  4530kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16497
Date: Mon, 29 Jan 2024 19:11:03 GMT   (1667kb)

Title: A Discriminative Bayesian Gaussian Process Latent Variable Model for
  High-Dimensional Data
Authors: Navid Ziaei, Behzad Nazari, Ali Yousefi
Categories: cs.LG
Comments: 33 pages, 3 figures
ACM-class: I.5.1; G.3
\\
  Extracting meaningful information from high-dimensional data poses a
formidable modeling challenge, particularly when the data is obscured by noise
or represented through different modalities. In this research, we propose a
novel non-parametric modeling approach, leveraging the Gaussian Process (GP),
to characterize high-dimensional data by mapping it to a latent low-dimensional
manifold. This model, named the Latent Discriminative Generative Decoder
(LDGD), utilizes both the data (or its features) and associated labels (such as
category or stimulus) in the manifold discovery process. To infer the latent
variables, we derive a Bayesian solution, allowing LDGD to effectively capture
inherent uncertainties in the data while enhancing the model's predictive
accuracy and robustness. We demonstrate the application of LDGD on both
synthetic and benchmark datasets. Not only does LDGD infer the manifold
accurately, but its prediction accuracy in anticipating labels surpasses
state-of-the-art approaches. We have introduced inducing points to reduce the
computational complexity of Gaussian Processes (GPs) for large datasets. This
enhancement facilitates batch training, allowing for more efficient processing
and scalability in handling extensive data collections. Additionally, we
illustrate that LDGD achieves higher accuracy in predicting labels and operates
effectively with a limited training dataset, underscoring its efficiency and
effectiveness in scenarios where data availability is constrained. These
attributes set the stage for the development of non-parametric modeling
approaches in the analysis of high-dimensional data; especially in fields where
data are both high-dimensional and complex.
\\ ( https://arxiv.org/abs/2401.16497 ,  1667kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16501
Date: Mon, 29 Jan 2024 19:17:42 GMT   (3011kb)

Title: AFSD-Physics: Exploring the governing equations of temperature evolution
  during additive friction stir deposition by a human-AI teaming approach
Authors: Tony Shi, Mason Ma, Jiajie Wu, Chase Post, Elijah Charles, Tony
  Schmitz
Categories: cs.LG cond-mat.mtrl-sci cs.AI
\\
  This paper presents a modeling effort to explore the underlying physics of
temperature evolution during additive friction stir deposition (AFSD) by a
human-AI teaming approach. AFSD is an emerging solid-state additive
manufacturing technology that deposits materials without melting. However, both
process modeling and modeling of the AFSD tool are at an early stage. In this
paper, a human-AI teaming approach is proposed to combine models based on first
principles with AI. The resulting human-informed machine learning method,
denoted as AFSD-Physics, can effectively learn the governing equations of
temperature evolution at the tool and the build from in-process measurements.
Experiments are designed and conducted to collect in-process measurements for
the deposition of aluminum 7075 with a total of 30 layers. The acquired
governing equations are physically interpretable models with low computational
cost and high accuracy. Model predictions show good agreement with the
measurements. Experimental validation with new process parameters demonstrates
the model's generalizability and potential for use in tool temperature control
and process optimization.
\\ ( https://arxiv.org/abs/2401.16501 ,  3011kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16520
Date: Mon, 29 Jan 2024 19:50:50 GMT   (7131kb,D)

Title: MT-HCCAR: Multi-Task Deep Learning with Hierarchical Classification and
  Attention-based Regression for Cloud Property Retrieval
Authors: Xingyan Li, Andrew M. Sayer, Ian T. Carroll, Xin Huang, Jianwu Wang
Categories: cs.LG cs.CV eess.SP
Comments: 14 pages, 3 figures, submitted to 40th IEEE International Conference
  on Data Engineering (ICDE 2024, website:
  https://icde2024.github.io/CFP_research.html)
MSC-class: 68T07
ACM-class: I.2.6
\\
  In the realm of Earth science, effective cloud property retrieval,
encompassing cloud masking, cloud phase classification, and cloud optical
thickness (COT) prediction, remains pivotal. Traditional methodologies
necessitate distinct models for each sensor instrument due to their unique
spectral characteristics. Recent strides in Earth Science research have
embraced machine learning and deep learning techniques to extract features from
satellite datasets' spectral observations. However, prevailing approaches lack
novel architectures accounting for hierarchical relationships among retrieval
tasks. Moreover, considering the spectral diversity among existing sensors, the
development of models with robust generalization capabilities over different
sensor datasets is imperative. Surprisingly, there is a dearth of methodologies
addressing the selection of an optimal model for diverse datasets. In response,
this paper introduces MT-HCCAR, an end-to-end deep learning model employing
multi-task learning to simultaneously tackle cloud masking, cloud phase
retrieval (classification tasks), and COT prediction (a regression task). The
MT-HCCAR integrates a hierarchical classification network (HC) and a
classification-assisted attention-based regression network (CAR), enhancing
precision and robustness in cloud labeling and COT prediction. Additionally, a
comprehensive model selection method rooted in K-fold cross-validation, one
standard error rule, and two introduced performance scores is proposed to
select the optimal model over three simulated satellite datasets OCI, VIIRS,
and ABI. The experiments comparing MT-HCCAR with baseline methods, the ablation
studies, and the model selection affirm the superiority and the generalization
capabilities of MT-HCCAR.
\\ ( https://arxiv.org/abs/2401.16520 ,  7131kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16521
Date: Mon, 29 Jan 2024 19:51:50 GMT   (17kb)

Title: Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity
  Analysis Methods for Time-Series Deep Learning Models
Authors: Zhengguang Wang
Categories: cs.LG cs.AI
\\
  This work undertakes studies to evaluate Interpretability Methods for
Time-Series Deep Learning. Sensitivity analysis assesses how input changes
affect the output, constituting a key component of interpretation. Among the
post-hoc interpretation methods such as back-propagation, perturbation, and
approximation, my work will investigate perturbation-based sensitivity Analysis
methods on modern Transformer models to benchmark their performances.
Specifically, my work answers three research questions: 1) Do different
sensitivity analysis (SA) methods yield comparable outputs and attribute
importance rankings? 2) Using the same sensitivity analysis method, do
different Deep Learning (DL) models impact the output of the sensitivity
analysis? 3) How well do the results from sensitivity analysis methods align
with the ground truth?
\\ ( https://arxiv.org/abs/2401.16521 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16537
Date: Mon, 29 Jan 2024 20:18:51 GMT   (623kb)

Title: Efficient Observation Time Window Segmentation for Administrative Data
  Machine Learning
Authors: Musa Taib, Geoffrey G. Messier
Categories: cs.LG cs.CY
\\
  Utilizing administrative data to predict outcomes is an important application
area of machine learning, particularly in healthcare. Most administrative data
records are timestamped and the pattern of records over time is a key input for
machine learning models. This paper explores how best to divide the observation
window of a machine learning model into time segments or "bins". A
computationally efficient process is presented that identifies which data
features benefit most from smaller, higher resolution time segments. Results
generated on healthcare and housing/homelessness administrative data
demonstrate that optimizing the time bin size of these high priority features
while using a single time bin for the other features achieves machine learning
models that are simpler and quicker to train. This approach also achieves
similar and sometimes better performance than more complex models that default
to representing all data features with the same time resolution.
\\ ( https://arxiv.org/abs/2401.16537 ,  623kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16549
Date: Mon, 29 Jan 2024 20:37:03 GMT   (1506kb)

Title: Deep Learning for Multi-Label Learning: A Comprehensive Survey
Authors: Adane Nega Tarekegn, Mohib Ullah, Faouzi Alaya Cheikh
Categories: cs.LG cs.CV
Comments: 21 pages, 12 figures, 5 tables. This paper is submitted to IEEE
  Transactions on Knowledge and Data Engineering and it is currently under
  review
\\
  Multi-label learning is a rapidly growing research area that aims to predict
multiple labels from a single input data point. In the era of big data, tasks
involving multi-label classification (MLC) or ranking present significant and
intricate challenges, capturing considerable attention in diverse domains.
Inherent difficulties in MLC include dealing with high-dimensional data,
addressing label correlations, and handling partial labels, for which
conventional methods prove ineffective. Recent years have witnessed a notable
increase in adopting deep learning (DL) techniques to address these challenges
more effectively in MLC. Notably, there is a burgeoning effort to harness the
robust learning capabilities of DL for improved modelling of label dependencies
and other challenges in MLC. However, it is noteworthy that comprehensive
studies specifically dedicated to DL for multi-label learning are limited.
Thus, this survey aims to thoroughly review recent progress in DL for
multi-label learning, along with a summary of open research problems in MLC.
The review consolidates existing research efforts in DL for MLC,including deep
neural networks, transformers, autoencoders, and convolutional and recurrent
architectures. Finally, the study presents a comparative analysis of the
existing methods to provide insightful observations and stimulate future
research directions in this domain.
\\ ( https://arxiv.org/abs/2401.16549 ,  1506kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16569
Date: Mon, 29 Jan 2024 21:08:33 GMT   (4696kb)

Title: Autoencoder-Based Domain Learning for Semantic Communication with
  Conceptual Spaces
Authors: Dylan Wheeler and Balasubramaniam Natarajan
Categories: cs.LG cs.AI cs.CL eess.IV
Comments: 6 pages, 5 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\
  Communication with the goal of accurately conveying meaning, rather than
accurately transmitting symbols, has become an area of growing interest. This
paradigm, termed semantic communication, typically leverages modern
developments in artificial intelligence and machine learning to improve the
efficiency and robustness of communication systems. However, a standard model
for capturing and quantifying the details of "meaning" is lacking, with many
leading approaches to semantic communication adopting a black-box framework
with little understanding of what exactly the model is learning. One solution
is to utilize the conceptual spaces framework, which models meaning explicitly
in a geometric manner. Though prior work studying semantic communication with
conceptual spaces has shown promising results, these previous attempts involve
hand-crafting a conceptual space model, severely limiting the scalability and
practicality of the approach. In this work, we develop a framework for learning
a domain of a conceptual space model using only the raw data with high-level
property labels. In experiments using the MNIST and CelebA datasets, we show
that the domains learned using the framework maintain semantic similarity
relations and possess interpretable dimensions.
\\ ( https://arxiv.org/abs/2401.16569 ,  4696kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16594
Date: Mon, 29 Jan 2024 21:51:27 GMT   (1446kb,D)

Title: Consistent algorithms for multi-label classification with macro-at-$k$
  metrics
Authors: Erik Schultheis, Wojciech Kot{\l}owski, Marek Wydmuch, Rohit Babbar,
  Strom Borman, Krzysztof Dembczy\'nski
Categories: cs.LG
Comments: This is the authors' version of the work accepted to ICLR 2024
\\
  We consider the optimization of complex performance metrics in multi-label
classification under the population utility framework. We mainly focus on
metrics linearly decomposable into a sum of binary classification utilities
applied separately to each label with an additional requirement of exactly $k$
labels predicted for each instance. These "macro-at-$k$" metrics possess
desired properties for extreme classification problems with long tail labels.
Unfortunately, the at-$k$ constraint couples the otherwise independent binary
classification tasks, leading to a much more challenging optimization problem
than standard macro-averages. We provide a statistical framework to study this
problem, prove the existence and the form of the optimal classifier, and
propose a statistically consistent and practical learning algorithm based on
the Frank-Wolfe method. Interestingly, our main results concern even more
general metrics being non-linear functions of label-wise confusion matrices.
Empirical results provide evidence for the competitive performance of the
proposed approach.
\\ ( https://arxiv.org/abs/2401.16594 ,  1446kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16635
Date: Tue, 30 Jan 2024 00:17:37 GMT   (100kb,D)

Title: Improving Reinforcement Learning from Human Feedback with Efficient
  Reward Model Ensemble
Authors: Shun Zhang, Zhenfang Chen, Sunli Chen, Yikang Shen, Zhiqing Sun,
  Chuang Gan
Categories: cs.LG cs.AI cs.CL
\\
  Reinforcement Learning from Human Feedback (RLHF) is a widely adopted
approach for aligning large language models with human values. However, RLHF
relies on a reward model that is trained with a limited amount of human
preference data, which could lead to inaccurate predictions. As a result, RLHF
may produce outputs that are misaligned with human values. To mitigate this
issue, we contribute a reward ensemble method that allows the reward model to
make more accurate predictions. As using an ensemble of large language
model-based reward models can be computationally and resource-expensive, we
explore efficient ensemble methods including linear-layer ensemble and
LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy
Optimization with our ensembled reward models, and verify that our ensemble
methods help improve the alignment performance of RLHF outputs.
\\ ( https://arxiv.org/abs/2401.16635 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16645
Date: Tue, 30 Jan 2024 00:37:57 GMT   (4594kb,D)

Title: Speeding up and reducing memory usage for scientific machine learning
  via mixed precision
Authors: Joel Hayford, Jacob Goldman-Wetzler, Eric Wang, and Lu Lu
Categories: cs.LG
Comments: 25 pages, 7 figures
\\
  Scientific machine learning (SciML) has emerged as a versatile approach to
address complex computational science and engineering problems. Within this
field, physics-informed neural networks (PINNs) and deep operator networks
(DeepONets) stand out as the leading techniques for solving partial
differential equations by incorporating both physical equations and
experimental data. However, training PINNs and DeepONets requires significant
computational resources, including long computational times and large amounts
of memory. In search of computational efficiency, training neural networks
using half precision (float16) rather than the conventional single (float32) or
double (float64) precision has gained substantial interest, given the inherent
benefits of reduced computational time and memory consumed. However, we find
that float16 cannot be applied to SciML methods, because of gradient divergence
at the start of training, weight updates going to zero, and the inability to
converge to a local minima. To overcome these limitations, we explore mixed
precision, which is an approach that combines the float16 and float32 numerical
formats to reduce memory usage and increase computational speed. Our
experiments showcase that mixed precision training not only substantially
decreases training times and memory demands but also maintains model accuracy.
We also reinforce our empirical observations with a theoretical analysis. The
research has broad implications for SciML in various computational
applications.
\\ ( https://arxiv.org/abs/2401.16645 ,  4594kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16649
Date: Tue, 30 Jan 2024 00:43:41 GMT   (4344kb,D)

Title: Using Motion Forecasting for Behavior-Based Virtual Reality (VR)
  Authentication
Authors: Mingjun Li, Natasha Kholgade Banerjee, Sean Banerjee
Categories: cs.LG cs.CR
Comments: AIxVR 2024 Best Paper Award
\\
  Task-based behavioral biometric authentication of users interacting in
virtual reality (VR) environments enables seamless continuous authentication by
using only the motion trajectories of the person's body as a unique signature.
Deep learning-based approaches for behavioral biometrics show high accuracy
when using complete or near complete portions of the user trajectory, but show
lower performance when using smaller segments from the start of the task. Thus,
any systems designed with existing techniques are vulnerable while waiting for
future segments of motion trajectories to become available. In this work, we
present the first approach that predicts future user behavior using
Transformer-based forecasting and using the forecasted trajectory to perform
user authentication. Our work leverages the notion that given the current
trajectory of a user in a task-based environment we can predict the future
trajectory of the user as they are unlikely to dramatically shift their
behavior since it would preclude the user from successfully completing their
task goal. Using the publicly available 41-subject ball throwing dataset of
Miller et al. we show improvement in user authentication when using forecasted
data. When compared to no forecasting, our approach reduces the authentication
equal error rate (EER) by an average of 23.85% and a maximum reduction of
36.14%.
\\ ( https://arxiv.org/abs/2401.16649 ,  4344kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16650
Date: Tue, 30 Jan 2024 00:48:26 GMT   (1519kb,D)

Title: Augmenting Replay in World Models for Continual Reinforcement Learning
Authors: Luke Yang, Levin Kuhlmann, Gideon Kowadlo
Categories: cs.LG cs.AI
ACM-class: I.2.6; I.5.0; I.5.1
\\
  In continual RL, the environment of a reinforcement learning (RL) agent
undergoes change. A successful system should appropriately balance the
conflicting requirements of retaining agent performance on already learned
tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out
buffer is commonly used to enhance learning in such settings but requires
significant memory. We explore the application of an augmentation to this
buffer which alleviates the memory constraints, and use it with a world model
model-based reinforcement learning algorithm, to evaluate its effectiveness in
facilitating continual learning. We evaluate the effectiveness of our method in
Procgen and Atari RL benchmarks and show that the distribution matching
augmentation to the replay-buffer used in the context of latent world models
can successfully prevent catastrophic forgetting with significantly reduced
computational overhead. Yet, we also find such a solution to not be entirely
infallible, and other failure modes such as the opposite -- lacking plasticity
and being unable to learn a new task -- to be a potential limitation in
continual learning systems.
\\ ( https://arxiv.org/abs/2401.16650 ,  1519kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16661
Date: Tue, 30 Jan 2024 01:24:43 GMT   (2550kb)

Title: Generalization of LiNGAM that allows confounding
Authors: Joe Suzuki and Tian-Le Yang
Categories: cs.LG cs.IT math.IT math.ST stat.TH
\\
  LiNGAM determines the variable order from cause to effect using additive
noise models, but it faces challenges with confounding. Previous methods
maintained LiNGAM's fundamental structure while trying to identify and address
variables affected by confounding. As a result, these methods required
significant computational resources regardless of the presence of confounding,
and they did not ensure the detection of all confounding types. In contrast,
this paper enhances LiNGAM by introducing LiNGAM-MMI, a method that quantifies
the magnitude of confounding using KL divergence and arranges the variables to
minimize its impact. This method efficiently achieves a globally optimal
variable order through the shortest path problem formulation. LiNGAM-MMI
processes data as efficiently as traditional LiNGAM in scenarios without
confounding while effectively addressing confounding situations. Our
experimental results suggest that LiNGAM-MMI more accurately determines the
correct variable order, both in the presence and absence of confounding.
\\ ( https://arxiv.org/abs/2401.16661 ,  2550kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16664
Date: Tue, 30 Jan 2024 01:28:48 GMT   (279kb)

Title: Fast Dual-Regularized Autoencoder for Sparse Biological Data
Authors: Aleksandar Poleksic
Categories: cs.LG
MSC-class: 92C42
ACM-class: J.3
\\
  Relationship inference from sparse data is an important task with
applications ranging from product recommendation to drug discovery. A recently
proposed linear model for sparse matrix completion has demonstrated surprising
advantage in speed and accuracy over more sophisticated recommender systems
algorithms. Here we extend the linear model to develop a shallow autoencoder
for the dual neighborhood-regularized matrix completion problem. We demonstrate
the speed and accuracy advantage of our approach over the existing
state-of-the-art in predicting drug-target interactions and drug-disease
associations.
\\ ( https://arxiv.org/abs/2401.16664 ,  279kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16669
Date: Tue, 30 Jan 2024 01:34:43 GMT   (1407kb)

Title: Is Artificial Intelligence Providing the Second Revolution for Weather
  Forecasting?
Authors: Fenghua Ling, Lin Ouyang, Boufeniza Redouane Larbi, Jing-Jia Luo, Tao
  Han, Xiaohui Zhong, Lei Bai
Categories: cs.LG cs.AI physics.ao-ph physics.geo-ph
\\
  The rapid advancement of artificial intelligence technologies, particularly
in recent years, has led to the emergence of several large parameter artificial
intelligence weather forecast models. These models represent a significant
breakthrough, overcoming the limitations of traditional numerical weather
prediction models and indicating a potential second revolution for weather
forecast. This study explores the evolution of these advanced artificial
intelligence forecast models, and based on the identified commonalities,
proposes the "Three Large Rules" for their development. We discuss the
potential of artificial intelligence in revolutionizing numerical weather
prediction, briefly outlining the underlying reasons for this potential.
Additionally, we explore key areas for future development prospects for large
artificial intelligence weather forecast models, integrating the entire
numerical prediction process. Through an example that combines a large
artificial intelligence model with ocean wave forecasting, we illustrate how
forecasters can adapt and leverage the advanced artificial intelligence model.
While acknowledging the high accuracy, computational efficiency, and ease of
deployment of large artificial intelligence forecast models, we emphasize the
irreplaceable values of traditional numerical forecasts. We believe that the
optimal future of weather forecasting lies in achieving a seamless integration
of artificial intelligence and traditional numerical models. Such a synthesis
is anticipated to offer a more comprehensive and reliable approach for future
weather forecasting.
\\ ( https://arxiv.org/abs/2401.16669 ,  1407kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16685
Date: Tue, 30 Jan 2024 02:16:19 GMT   (4868kb,D)

Title: Communication-Efficient Multimodal Federated Learning: Joint Modality
  and Client Selection
Authors: Liangqi Yuan, Dong-Jun Han, Su Wang, Devesh Upadhyay, Christopher G.
  Brinton
Categories: cs.LG cs.DC
Comments: arXiv admin note: text overlap with arXiv:2310.07048
\\
  Multimodal federated learning (FL) aims to enrich model training in FL
settings where clients are collecting measurements across multiple modalities.
However, key challenges to multimodal FL remain unaddressed, particularly in
heterogeneous network settings where: (i) the set of modalities collected by
each client will be diverse, and (ii) communication limitations prevent clients
from uploading all their locally trained modality models to the server. In this
paper, we propose multimodal Federated learning with joint Modality and Client
selection (mmFedMC), a new FL methodology that can tackle the above-mentioned
challenges in multimodal settings. The joint selection algorithm incorporates
two main components: (a) A modality selection methodology for each client,
which weighs (i) the impact of the modality, gauged by Shapley value analysis,
(ii) the modality model size as a gauge of communication overhead, against
(iii) the frequency of modality model updates, denoted recency, to enhance
generalizability. (b) A client selection strategy for the server based on the
local loss of modality model at each client. Experiments on five real-world
datasets demonstrate the ability of mmFedMC to achieve comparable accuracy to
several baselines while reducing the communication overhead by over 20x. A demo
video of our methodology is available at https://liangqiy.com/mmfedmc/.
\\ ( https://arxiv.org/abs/2401.16685 ,  4868kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16692
Date: Tue, 30 Jan 2024 02:38:23 GMT   (236kb,D)

Title: Calibration-then-Calculation: A Variance Reduced Metric Framework in
  Deep Click-Through Rate Prediction Models
Authors: Yewen Fan, Nian Si, Xiangchen Song, Kun Zhang
Categories: cs.LG
\\
  Deep learning has been widely adopted across various fields, but there has
been little focus on evaluating the performance of deep learning pipelines.
With the increased use of large datasets and complex models, it has become
common to run the training process only once and compare the result to previous
benchmarks. However, this procedure can lead to imprecise comparisons due to
the variance in neural network evaluation metrics. The metric variance comes
from the randomness inherent in the training process of deep learning
pipelines. Traditional solutions such as running the training process multiple
times are usually not feasible in deep learning due to computational
limitations. In this paper, we propose a new metric framework, Calibrated Loss
Metric, that addresses this issue by reducing the variance in its vanilla
counterpart. As a result, the new metric has a higher accuracy to detect
effective modeling improvement. Our approach is supported by theoretical
justifications and extensive experimental validations in the context of Deep
Click-Through Rate Prediction Models.
\\ ( https://arxiv.org/abs/2401.16692 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16694
Date: Tue, 30 Jan 2024 02:41:05 GMT   (2852kb,D)

Title: EdgeOL: Efficient in-situ Online Learning on Edge Devices
Authors: Sheng Li, Geng Yuan, Yawen Wu, Yue Dai, Chao Wu, Alex K. Jones,
  Jingtong Hu, Yanzhi Wang, Xulong Tang
Categories: cs.LG cs.CV cs.DC
\\
  Emerging applications, such as robot-assisted eldercare and object
recognition, generally employ deep learning neural networks (DNNs) models and
naturally require: i) handling streaming-in inference requests and ii) adapting
to possible deployment scenario changes. Online model fine-tuning is widely
adopted to satisfy these needs. However, fine-tuning involves significant
energy consumption, making it challenging to deploy on edge devices. In this
paper, we propose EdgeOL, an edge online learning framework that optimizes
inference accuracy, fine-tuning execution time, and energy efficiency through
both inter-tuning and intra-tuning optimizations. Experimental results show
that, on average, EdgeOL reduces overall fine-tuning execution time by 82%,
energy consumption by 74%, and improves average inference accuracy by 1.70%
over the immediate online learning strategy.
\\ ( https://arxiv.org/abs/2401.16694 ,  2852kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16708
Date: Tue, 30 Jan 2024 03:12:19 GMT   (432kb,D)

Title: Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible
  Cluster Shapes
Authors: Yung-Peng Hsu, Hung-Hsuan Chen
Categories: cs.LG cs.AI
\\
  This paper introduces the multivariate beta mixture model (MBMM), a new
probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes
because of the flexible probability density function of the multivariate beta
distribution. We introduce the properties of MBMM, describe the parameter
learning procedure, and present the experimental results, showing that MBMM
fits diverse cluster shapes on synthetic and real datasets. The code is
released anonymously at \url{https://github.com/hhchen1105/mbmm/}.
\\ ( https://arxiv.org/abs/2401.16708 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16720
Date: Tue, 30 Jan 2024 03:34:27 GMT   (400kb,D)

Title: SmartFRZ: An Efficient Training Framework using Attention-Based Layer
  Freezing
Authors: Sheng Li, Geng Yuan, Yue Dai, Youtao Zhang, Yanzhi Wang, Xulong Tang
Categories: cs.LG cs.CV
\\
  There has been a proliferation of artificial intelligence applications, where
model training is key to promising high-quality services for these
applications. However, the model training process is both time-intensive and
energy-intensive, inevitably affecting the user's demand for application
efficiency. Layer freezing, an efficient model training technique, has been
proposed to improve training efficiency. Although existing layer freezing
methods demonstrate the great potential to reduce model training costs, they
still remain shortcomings such as lacking generalizability and compromised
accuracy. For instance, existing layer freezing methods either require the
freeze configurations to be manually defined before training, which does not
apply to different networks, or use heuristic freezing criteria that is hard to
guarantee decent accuracy in different scenarios. Therefore, there lacks a
generic and smart layer freezing method that can automatically perform
``in-situation'' layer freezing for different networks during training
processes. To this end, we propose a generic and efficient training framework
(SmartFRZ). The core proposed technique in SmartFRZ is attention-guided layer
freezing, which can automatically select the appropriate layers to freeze
without compromising accuracy. Experimental results show that SmartFRZ
effectively reduces the amount of computation in training and achieves
significant training acceleration, and outperforms the state-of-the-art layer
freezing approaches.
\\ ( https://arxiv.org/abs/2401.16720 ,  400kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16729
Date: Tue, 30 Jan 2024 04:01:18 GMT   (720kb,D)

Title: Widely Linear Matched Filter: A Lynchpin towards the Interpretability of
  Complex-valued CNNs
Authors: Qingchen Wang, Zhe Li, Zdenka Babic, Wei Deng, Ljubi\v{s}a
  Stankovi\'c, Danilo P. Mandic
Categories: cs.LG
\\
  A recent study on the interpretability of real-valued convolutional neural
networks (CNNs) \cite{Stankovic_Mandic_2023CNN} has revealed a direct and
physically meaningful link with the task of finding features in data through
matched filters. However, applying this paradigm to illuminate the
interpretability of complex-valued CNNs meets a formidable obstacle: the
extension of matched filtering to a general class of noncircular complex-valued
data, referred to here as the widely linear matched filter (WLMF), has been
only implicit in the literature. To this end, to establish the interpretability
of the operation of complex-valued CNNs, we introduce a general WLMF paradigm,
provide its solution and undertake analysis of its performance. For rigor, our
WLMF solution is derived without imposing any assumption on the probability
density of noise. The theoretical advantages of the WLMF over its standard
strictly linear counterpart (SLMF) are provided in terms of their output
signal-to-noise-ratios (SNRs), with WLMF consistently exhibiting enhanced SNR.
Moreover, the lower bound on the SNR gain of WLMF is derived, together with
condition to attain this bound. This serves to revisit the
convolution-activation-pooling chain in complex-valued CNNs through the lens of
matched filtering, which reveals the potential of WLMFs to provide physical
interpretability and enhance explainability of general complex-valued CNNs.
Simulations demonstrate the agreement between the theoretical and numerical
results.
\\ ( https://arxiv.org/abs/2401.16729 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16754
Date: Tue, 30 Jan 2024 05:22:45 GMT   (3390kb,D)

Title: AI Oversight and Human Mistakes: Evidence from Centre Court
Authors: David Almog, Romain Gauriot, Lionel Page, Daniel Martin
Categories: cs.LG cs.CY econ.GN q-fin.EC
\\
  Powered by the increasing predictive capabilities of machine learning
algorithms, artificial intelligence (AI) systems have begun to be used to
overrule human mistakes in many settings. We provide the first field evidence
this AI oversight carries psychological costs that can impact human
decision-making. We investigate one of the highest visibility settings in which
AI oversight has occurred: the Hawk-Eye review of umpires in top tennis
tournaments. We find that umpires lowered their overall mistake rate after the
introduction of Hawk-Eye review, in line with rational inattention given
psychological costs of being overruled by AI. We also find that umpires
increased the rate at which they called balls in, which produced a shift from
making Type II errors (calling a ball out when in) to Type I errors (calling a
ball in when out). We structurally estimate the psychological costs of being
overruled by AI using a model of rational inattentive umpires, and our results
suggest that because of these costs, umpires cared twice as much about Type II
errors under AI oversight.
\\ ( https://arxiv.org/abs/2401.16754 ,  3390kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16755
Date: Tue, 30 Jan 2024 05:25:02 GMT   (392kb,D)

Title: Diffusion model for relational inference
Authors: Shuhan Zheng, Ziqiang Li, Kantaro Fujiwara, Gouhei Tanaka
Categories: cs.LG cs.AI
\\
  Dynamical behaviors of complex interacting systems, including brain
activities, financial price movements, and physical collective phenomena, are
associated with underlying interactions between the system's components. The
issue of uncovering interaction relations in such systems using observable
dynamics is called relational inference. In this study, we propose a Diffusion
model for Relational Inference (DiffRI), inspired by a self-supervised method
for probabilistic time series imputation. DiffRI learns to infer the
probability of the presence of connections between components through
conditional diffusion modeling. Experiments on both simulated and quasi-real
datasets show that DiffRI is highly competent compared with other
state-of-the-art models in discovering ground truth interactions in an
unsupervised manner. Our code will be made public soon.
\\ ( https://arxiv.org/abs/2401.16755 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16757
Date: Tue, 30 Jan 2024 05:29:49 GMT   (3924kb,D)

Title: SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond
  the Memory Budget
Authors: Kun Wang, Jiani Cao, Zimu Zhou and Zhenjiang Li
Categories: cs.LG cs.AI cs.DC
Comments: 14 pages, 19 figures, accepted by IEEE Transactions on Mobile
  Computing
DOI: 10.1109/TMC.2024.3355764
\\
  Executing deep neural networks (DNNs) on edge artificial intelligence (AI)
devices enables various autonomous mobile computing applications. However, the
memory budget of edge AI devices restricts the number and complexity of DNNs
allowed in such applications. Existing solutions, such as model compression or
cloud offloading, reduce the memory footprint of DNN inference at the cost of
decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN
into blocks and swap them in and out in order, such that large DNNs can execute
within a small memory budget. Nevertheless, naive swapping on edge AI devices
induces significant delays due to the redundant memory operations in the DNN
development ecosystem for edge AI devices. To this end, we develop SwapNet, an
efficient DNN block swapping middleware for edge AI devices. We systematically
eliminate the unnecessary memory operations during block swapping while
retaining compatible with the deep learning frameworks, GPU backends, and
hardware architectures of edge AI devices. We further showcase the utility of
SwapNet via a multi-DNN scheduling scheme. Evaluations on eleven DNN inference
tasks in three applications demonstrate that SwapNet achieves almost the same
latency as the case with sufficient memory even when DNNs demand 2.32x to 5.81x
memory beyond the available budget. The design of SwapNet also provides novel
and feasible insights for deploying large language models (LLMs) on edge AI
devices in the future.
\\ ( https://arxiv.org/abs/2401.16757 ,  3924kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16760
Date: Tue, 30 Jan 2024 05:42:54 GMT   (2818kb,D)

Title: One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware
  Quantization Training
Authors: Lianbo Ma, Yuee Zhou, Jianlun Ma, Guo Yu, Qing Li
Categories: cs.LG
Comments: 9 pages, 13 figures,accepted by AAAI-24
\\
  Weight quantization is an effective technique to compress deep neural
networks for their deployment on edge devices with limited resources.
Traditional loss-aware quantization methods commonly use the quantized gradient
to replace the full-precision gradient. However, we discover that the gradient
error will lead to an unexpected zig-zagging-like issue in the gradient descent
learning procedures, where the gradient directions rapidly oscillate or
zig-zag, and such issue seriously slows down the model convergence.
Accordingly, this paper proposes a one-step forward and backtrack way for
loss-aware quantization to get more accurate and stable gradient direction to
defy this issue. During the gradient descent learning, a one-step forward
search is designed to find the trial gradient of the next-step, which is
adopted to adjust the gradient of current step towards the direction of fast
convergence. After that, we backtrack the current step to update the
full-precision and quantized weights through the current-step gradient and the
trial gradient. A series of theoretical analysis and experiments on benchmark
deep models have demonstrated the effectiveness and competitiveness of the
proposed method, and our method especially outperforms others on the
convergence performance.
\\ ( https://arxiv.org/abs/2401.16760 ,  2818kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16766
Date: Tue, 30 Jan 2024 06:06:57 GMT   (1289kb,D)

Title: Detection and Recovery Against Deep Neural Network Fault Injection
  Attacks Based on Contrastive Learning
Authors: Chenan Wang, Pu Zhao, Siyue Wang, Xue Lin
Categories: cs.LG cs.AI cs.CR cs.CV
Comments: Published in AdvML 2021
\\
  Deep Neural Network (DNN) models when implemented on executing devices as the
inference engines are susceptible to Fault Injection Attacks (FIAs) that
manipulate model parameters to disrupt inference execution with disastrous
performance. This work introduces Contrastive Learning (CL) of visual
representations i.e., a self-supervised learning approach into the deep
learning training and inference pipeline to implement DNN inference engines
with self-resilience under FIAs. Our proposed CL based FIA Detection and
Recovery (CFDR) framework features (i) real-time detection with only a single
batch of testing data and (ii) fast recovery effective even with only a small
amount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on
multiple types of FIAs, our CFDR shows promising detection and recovery
effectiveness.
\\ ( https://arxiv.org/abs/2401.16766 ,  1289kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16771
Date: Tue, 30 Jan 2024 06:20:08 GMT   (2172kb)

Title: MolPLA: A Molecular Pretraining Framework for Learning Cores, R-Groups
  and their Linker Joints
Authors: Mogan Gim, Jueon Park, Soyon Park, Sanghoon Lee, Seungheun Baek,
  Junhyun Lee, Ngoc-Quang Nguyen, Jaewoo Kang
Categories: cs.LG
\\
  Molecular core structures and R-groups are essential concepts in drug
development. Integration of these concepts with conventional graph pre-training
approaches can promote deeper understanding in molecules. We propose MolPLA, a
novel pre-training framework that employs masked graph contrastive learning in
understanding the underlying decomposable parts inmolecules that implicate
their core structure and peripheral R-groups. Furthermore, we formulate an
additional framework that grants MolPLA the ability to help chemists find
replaceable R-groups in lead optimization scenarios. Experimental results on
molecular property prediction show that MolPLA exhibits predictability
comparable to current state-of-the-art models. Qualitative analysis implicate
that MolPLA is capable of distinguishing core and R-group sub-structures,
identifying decomposable regions in molecules and contributing to lead
optimization scenarios by rationally suggesting R-group replacements given
various query core templates. The code implementation for MolPLA and its
pre-trained model checkpoint is available at https://github.com/dmis-lab/MolPLA
\\ ( https://arxiv.org/abs/2401.16771 ,  2172kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16772
Date: Tue, 30 Jan 2024 06:22:19 GMT   (4312kb,D)

Title: Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator
Authors: Ryoma Furuyama, Daiki Kuyoshi and Satoshi Yamane
Categories: cs.LG cs.AI
Comments: 9 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:2001.06808
ACM-class: I.2.6
\\
  Imitation learning is often used in addition to reinforcement learning in
environments where reward design is difficult or where the reward is sparse,
but it is difficult to be able to imitate well in unknown states from a small
amount of expert data and sampling data. Supervised learning methods such as
Behavioral Cloning do not require sampling data, but usually suffer from
distribution shift. The methods based on reinforcement learning, such as
inverse reinforcement learning and Generative Adversarial imitation learning
(GAIL), can learn from only a few expert data. However, they often need to
interact with the environment. Soft Q imitation learning (SQIL) addressed the
problems, and it was shown that it could learn efficiently by combining
Behavioral Cloning and soft Q-learning with constant rewards. In order to make
this algorithm more robust to distribution shift, we propose more efficient and
robust algorithm by adding to this method a reward function based on
adversarial inverse reinforcement learning that rewards the agent for
performing actions in status similar to the demo. We call this algorithm
Discriminator Soft Q Imitation Learning (DSQIL). We evaluated it on MuJoCo
environments.
\\ ( https://arxiv.org/abs/2401.16772 ,  4312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16775
Date: Tue, 30 Jan 2024 06:27:11 GMT   (2741kb,D)

Title: Activity Detection for Massive Connectivity in Cell-free Networks with
  Unknown Large-scale Fading, Channel Statistics, Noise Variance, and Activity
  Probability: A Bayesian Approach
Authors: Hao Zhang, Qingfeng Lin, Yang Li, Lei Cheng, Yik-Chung Wu
Categories: cs.LG eess.SP
Comments: 16 pages, 9 figures
MSC-class: 68T01
\\
  Activity detection is an important task in the next generation grant-free
multiple access. While there are a number of existing algorithms designed for
this purpose, they mostly require precise information about the network, such
as large-scale fading coefficients, small-scale fading channel statistics,
noise variance at the access points, and user activity probability. Acquiring
these information would take a significant overhead and their estimated values
might not be accurate. This problem is even more severe in cell-free networks
as there are many of these parameters to be acquired. Therefore, this paper
sets out to investigate the activity detection problem without the
above-mentioned information. In order to handle so many unknown parameters,
this paper employs the Bayesian approach, where the unknown variables are
endowed with prior distributions which effectively act as regularizations.
Together with the likelihood function, a maximum a posteriori (MAP) estimator
and a variational inference algorithm are derived. Extensive simulations
demonstrate that the proposed methods, even without the knowledge of these
system parameters, perform better than existing state-of-the-art methods, such
as covariance-based and approximate message passing methods.
\\ ( https://arxiv.org/abs/2401.16775 ,  2741kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16777
Date: Tue, 30 Jan 2024 06:35:52 GMT   (7087kb,D)

Title: Addressing Distribution Shift in Time Series Forecasting with Instance
  Normalization Flows
Authors: Wei Fan, Shun Zheng, Pengyang Wang, Rui Xie, Jiang Bian, Yanjie Fu
Categories: cs.LG
Comments: 17 pages
\\
  Due to non-stationarity of time series, the distribution shift problem
largely hinders the performance of time series forecasting. Existing solutions
either fail for the shifts beyond simple statistics or the limited
compatibility with forecasting models. In this paper, we propose a general
decoupled formulation for time series forecasting, with no reliance on fixed
statistics and no restriction on forecasting architectures. Then, we make such
a formulation formalized into a bi-level optimization problem, to enable the
joint learning of the transformation (outer loop) and forecasting (inner loop).
Moreover, the special requirements of expressiveness and bi-direction for the
transformation motivate us to propose instance normalization flows (IN-Flow), a
novel invertible network for time series transformation. Extensive experiments
demonstrate our method consistently outperforms state-of-the-art baselines on
both synthetic and real-world data.
\\ ( https://arxiv.org/abs/2401.16777 ,  7087kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16784
Date: Tue, 30 Jan 2024 06:51:24 GMT   (15303kb,D)

Title: Graph Fairness Learning under Distribution Shifts
Authors: Yibo Li, Xiao Wang, Yujie Xing, Shaohua Fan, Ruijia Wang, Yaoqi Liu,
  and Chuan Shi
Categories: cs.LG cs.AI cs.SI
Comments: Accepted by WWW 2024
\\
  Graph neural networks (GNNs) have achieved remarkable performance on
graph-structured data. However, GNNs may inherit prejudice from the training
data and make discriminatory predictions based on sensitive attributes, such as
gender and race. Recently, there has been an increasing interest in ensuring
fairness on GNNs, but all of them are under the assumption that the training
and testing data are under the same distribution, i.e., training data and
testing data are from the same graph. Will graph fairness performance decrease
under distribution shifts? How does distribution shifts affect graph fairness
learning? All these open questions are largely unexplored from a theoretical
perspective. To answer these questions, we first theoretically identify the
factors that determine bias on a graph. Subsequently, we explore the factors
influencing fairness on testing graphs, with a noteworthy factor being the
representation distances of certain groups between the training and testing
graph. Motivated by our theoretical analysis, we propose our framework
FatraGNN. Specifically, to guarantee fairness performance on unknown testing
graphs, we propose a graph generator to produce numerous graphs with
significant bias and under different distributions. Then we minimize the
representation distances for each certain group between the training graph and
generated graphs. This empowers our model to achieve high classification and
fairness performance even on generated graphs with significant bias, thereby
effectively handling unknown testing graphs. Experiments on real-world and
semi-synthetic datasets demonstrate the effectiveness of our model in terms of
both accuracy and fairness.
\\ ( https://arxiv.org/abs/2401.16784 ,  15303kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16785
Date: Tue, 30 Jan 2024 06:53:59 GMT   (962kb,D)

Title: Enhancing Efficiency and Robustness in Support Vector Regression with
  HawkEye Loss
Authors: Mushir Akhtar, M. Tanveer, and Mohd. Arshad
Categories: cs.LG
\\
  Support vector regression (SVR) has garnered significant popularity over the
past two decades owing to its wide range of applications across various fields.
Despite its versatility, SVR encounters challenges when confronted with
outliers and noise, primarily due to the use of the $\varepsilon$-insensitive
loss function. To address this limitation, SVR with bounded loss functions has
emerged as an appealing alternative, offering enhanced generalization
performance and robustness. Notably, recent developments focus on designing
bounded loss functions with smooth characteristics, facilitating the adoption
of gradient-based optimization algorithms. However, it's crucial to highlight
that these bounded and smooth loss functions do not possess an insensitive
zone. In this paper, we address the aforementioned constraints by introducing a
novel symmetric loss function named the HawkEye loss function. It is worth
noting that the HawkEye loss function stands out as the first loss function in
SVR literature to be bounded, smooth, and simultaneously possess an insensitive
zone. Leveraging this breakthrough, we integrate the HawkEye loss function into
the least squares framework of SVR and yield a new fast and robust model termed
HE-LSSVR. The optimization problem inherent to HE-LSSVR is addressed by
harnessing the adaptive moment estimation (Adam) algorithm, known for its
adaptive learning rate and efficacy in handling large-scale problems. To our
knowledge, this is the first time Adam has been employed to solve an SVR
problem. To empirically validate the proposed HE-LSSVR model, we evaluate it on
UCI, synthetic, and time series datasets. The experimental outcomes
unequivocally reveal the superiority of the HE-LSSVR model both in terms of its
remarkable generalization performance and its efficiency in training time.
\\ ( https://arxiv.org/abs/2401.16785 ,  962kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16791
Date: Tue, 30 Jan 2024 07:09:48 GMT   (2832kb,D)

Title: Accelerated Cloud for Artificial Intelligence (ACAI)
Authors: Dachi Chen, Weitian Ding, Chen Liang, Chang Xu, Junwei Zhang, Majd
  Sakr
Categories: cs.LG
\\
  Training an effective Machine learning (ML) model is an iterative process
that requires effort in multiple dimensions. Vertically, a single pipeline
typically includes an initial ETL (Extract, Transform, Load) of raw datasets, a
model training stage, and an evaluation stage where the practitioners obtain
statistics of the model performance. Horizontally, many such pipelines may be
required to find the best model within a search space of model configurations.
Many practitioners resort to maintaining logs manually and writing simple glue
code to automate the workflow. However, carrying out this process on the cloud
is not a trivial task in terms of resource provisioning, data management, and
bookkeeping of job histories to make sure the results are reproducible. We
propose an end-to-end cloud-based machine learning platform, Accelerated Cloud
for AI (ACAI), to help improve the productivity of ML practitioners. ACAI
achieves this goal by enabling cloud-based storage of indexed, labeled, and
searchable data, as well as automatic resource provisioning, job scheduling,
and experiment tracking. Specifically, ACAI provides practitioners (1) a data
lake for storing versioned datasets and their corresponding metadata, and (2)
an execution engine for executing ML jobs on the cloud with automatic resource
provisioning (auto-provision), logging and provenance tracking. To evaluate
ACAI, we test the efficacy of our auto-provisioner on the MNIST handwritten
digit classification task, and we study the usability of our system using
experiments and interviews. We show that our auto-provisioner produces a 1.7x
speed-up and 39% cost reduction, and our system reduces experiment time for ML
scientists by 20% on typical ML use cases.
\\ ( https://arxiv.org/abs/2401.16791 ,  2832kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16795
Date: Tue, 30 Jan 2024 07:16:09 GMT   (555kb,D)

Title: Performance Insights-based AI-driven Football Transfer Fee Prediction
Authors: Daniil Sulimov
Categories: cs.LG cs.AI
MSC-class: 68T99
\\
  We developed an artificial intelligence approach to predict the transfer fee
of a football player. This model can help clubs make better decisions about
which players to buy and sell, which can lead to improved performance and
increased club budgets. Having collected data on player performance, transfer
fees, and other factors that might affect a player's value, we then used this
data to train a machine learning model that can accurately predict a player's
impact on the game. We further passed the obtained results as one of the
features to the predictor of transfer fees. The model can help clubs identify
players who are undervalued and who could be sold for a profit. It can also
help clubs avoid overpaying for players. We believe that our model can be a
valuable tool for football clubs. It can help them make better decisions about
player recruitment and transfers.
\\ ( https://arxiv.org/abs/2401.16795 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16796
Date: Tue, 30 Jan 2024 07:19:36 GMT   (3871kb,D)

Title: Learnable Prompt as Pseudo-Imputation: Reassessing the Necessity of
  Traditional EHR Data Imputation in Downstream Clinical Prediction
Authors: Weibin Liao, Yinghao Zhu, Zixiang Wang, Xu Chu, Yasha Wang, Liantao Ma
Categories: cs.LG
\\
  Analyzing the health status of patients based on Electronic Health Records
(EHR) is a fundamental research problem in medical informatics. The presence of
extensive missing values in EHR makes it challenging for deep neural networks
to directly model the patient's health status based on EHR. Existing deep
learning training protocols require the use of statistical information or
imputation models to reconstruct missing values; however, the protocols inject
non-realistic data into downstream EHR analysis models, significantly limiting
model performance. This paper introduces Learnable Prompt as Pseudo Imputation
(PAI) as a new training protocol. PAI no longer introduces any imputed data but
constructs a learnable prompt to model the implicit preferences of the
downstream model for missing values, resulting in a significant performance
improvement for all EHR analysis models. Additionally, our experiments show
that PAI exhibits higher robustness in situations of data insufficiency and
high missing rates. More importantly, in a real-world application involving
cross-institutional data with zero-shot evaluation, PAI demonstrates stronger
model generalization capabilities for non-overlapping features.
\\ ( https://arxiv.org/abs/2401.16796 ,  3871kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16800
Date: Tue, 30 Jan 2024 07:31:51 GMT   (683kb,D)

Title: Online Algorithm for Node Feature Forecasting in Temporal Graphs
Authors: Aniq Ur Rahman, Justin P. Coon
Categories: cs.LG cs.DM cs.SY eess.SY
Comments: 19 pages, 25 equations, 9 figures, 13 tables, 3 Algorithms
\\
  In this paper, we propose an online algorithm "mspace" for forecasting node
features in temporal graphs, which adeptly captures spatial cross-correlation
among different nodes as well as the temporal autocorrelation within a node.
The algorithm can be used for both probabilistic and deterministic multi-step
forecasting, making it applicable for estimation and generation tasks.
Comparative evaluations against various baselines, including graph neural
network (GNN) based models and classical Kalman filters, demonstrate that
mspace performs at par with the state-of-the-art and even surpasses them on
some datasets. Importantly, mspace demonstrates consistent robustness across
datasets with varying training sizes, a notable advantage over GNN-based
methods requiring abundant training samples to learn the spatiotemporal trends
in the data effectively. Therefore, employing mspace is advantageous in
scenarios where the training sample availability is limited. Additionally, we
establish theoretical bounds on multi-step forecasting error of mspace and show
that it scales as $O(q)$ for $q$-step forecast.
\\ ( https://arxiv.org/abs/2401.16800 ,  683kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16808
Date: Tue, 30 Jan 2024 08:11:36 GMT   (1345kb,D)

Title: Encoding Temporal Statistical-space Priors via Augmented Representation
Authors: Insu Choi, Woosung Koh, Gimin Kang, Yuntae Jang, Woo Chang Kim
Categories: cs.LG cs.AI
Comments: pre-print
\\
  Modeling time series data remains a pervasive issue as the temporal dimension
is inherent to numerous domains. Despite significant strides in time series
forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and
lack of data continue challenging practitioners. In response, we leverage a
simple representation augmentation technique to overcome these challenges. Our
augmented representation acts as a statistical-space prior encoded at each time
step. In response, we name our method Statistical-space Augmented
Representation (SSAR). The underlying high-dimensional data-generating process
inspires our representation augmentation. We rigorously examine the empirical
generalization performance on two data sets with two downstream temporal
learning algorithms. Our approach significantly beats all five up-to-date
baselines. Moreover, the highly modular nature of our approach can easily be
applied to various settings. Lastly, fully-fledged theoretical perspectives are
available throughout the writing for a clear and rigorous understanding.
\\ ( https://arxiv.org/abs/2401.16808 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16836
Date: Tue, 30 Jan 2024 09:22:37 GMT   (94kb,D)

Title: Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition
Authors: Juefei Chen, Longxiu Huang, and Yimin Wei
Categories: cs.LG cs.NA math.NA
\\
  Nonnegative Matrix Factorization (NMF) is an important unsupervised learning
method to extract meaningful features from data. To address the NMF problem
within a polynomial time framework, researchers have introduced a separability
assumption, which has recently evolved into the concept of coseparability. This
advancement offers a more efficient core representation for the original data.
However, in the real world, the data is more natural to be represented as a
multi-dimensional array, such as images or videos. The NMF's application to
high-dimensional data involves vectorization, which risks losing essential
multi-dimensional correlations. To retain these inherent correlations in the
data, we turn to tensors (multidimensional arrays) and leverage the tensor
t-product. This approach extends the coseparable NMF to the tensor setting,
creating what we term coseparable Nonnegative Tensor Factorization (NTF). In
this work, we provide an alternating index selection method to select the
coseparable core. Furthermore, we validate the t-CUR sampling theory and
integrate it with the tensor Discrete Empirical Interpolation Method (t-DEIM)
to introduce an alternative, randomized index selection process. These methods
have been tested on both synthetic and facial analysis datasets. The results
demonstrate the efficiency of coseparable NTF when compared to coseparable NMF.
\\ ( https://arxiv.org/abs/2401.16836 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16843
Date: Tue, 30 Jan 2024 09:34:15 GMT   (1701kb,D)

Title: Evaluating ML-Based Anomaly Detection Across Datasets of Varied
  Integrity: A Case Study
Authors: Adrian Pekar and Richard Jozsa
Categories: cs.LG cs.NI
\\
  Cybersecurity remains a critical challenge in the digital age, with network
traffic flow anomaly detection being a key pivotal instrument in the fight
against cyber threats. In this study, we address the prevalent issue of data
integrity in network traffic datasets, which are instrumental in developing
machine learning (ML) models for anomaly detection. We introduce two refined
versions of the CICIDS-2017 dataset, NFS-2023-nTE and NFS-2023-TE, processed
using NFStream to ensure methodologically sound flow expiration and labeling.
Our research contrasts the performance of the Random Forest (RF) algorithm
across the original CICIDS-2017, its refined counterparts WTMC-2021 and
CRiSIS-2022, and our NFStream-generated datasets, in both binary and
multi-class classification contexts. We observe that the RF model exhibits
exceptional robustness, achieving consistent high-performance metrics
irrespective of the underlying dataset quality, which prompts a critical
discussion on the actual impact of data integrity on ML efficacy. Our study
underscores the importance of continual refinement and methodological rigor in
dataset generation for network security research. As the landscape of network
threats evolves, so must the tools and techniques used to detect and analyze
them.
\\ ( https://arxiv.org/abs/2401.16843 ,  1701kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16852
Date: Tue, 30 Jan 2024 09:55:14 GMT   (2563kb,D)

Title: Checkmating One, by Using Many: Combining Mixture of Experts with MCTS
  to Improve in Chess
Authors: Felix Helfenstein, Jannis Bl\"uml, Johannes Czech and Kristian
  Kersting
Categories: cs.LG
Comments: Code available under https://github.com/HelpstoneX/CrazyAra
\\
  This paper presents a new approach that integrates deep learning with
computational chess, using both the Mixture of Experts (MoE) method and
Monte-Carlo Tree Search (MCTS). Our methodology employs a suite of specialized
models, each designed to respond to specific changes in the game's input data.
This results in a framework with sparsely activated models, which provides
significant computational benefits. Our framework combines the MoE method with
MCTS, in order to align it with the strategic phases of chess, thus departing
from the conventional ``one-for-all'' model. Instead, we utilize distinct game
phase definitions to effectively distribute computational tasks across multiple
expert neural networks. Our empirical research shows a substantial improvement
in playing strength, surpassing the traditional single-model framework. This
validates the efficacy of our integrated approach and highlights the potential
of incorporating expert knowledge and strategic principles into neural network
design. The fusion of MoE and MCTS offers a promising avenue for advancing
machine learning architectures.
\\ ( https://arxiv.org/abs/2401.16852 ,  2563kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16914
Date: Tue, 30 Jan 2024 11:25:49 GMT   (15801kb,D)

Title: Energy-conserving equivariant GNN for elasticity of lattice architected
  metamaterials
Authors: Ivan Grega, Ilyes Batatia, G\'abor Cs\'anyi, Sri Karlapati, Vikram S.
  Deshpande
Categories: cs.LG cond-mat.mtrl-sci
Comments: International Conference on Learning Representations 2024
\\
  Lattices are architected metamaterials whose properties strongly depend on
their geometrical design. The analogy between lattices and graphs enables the
use of graph neural networks (GNNs) as a faster surrogate model compared to
traditional methods such as finite element modelling. In this work we present a
higher-order GNN model trained to predict the fourth-order stiffness tensor of
periodic strut-based lattices. The key features of the model are (i) SE(3)
equivariance, and (ii) consistency with the thermodynamic law of conservation
of energy. We compare the model to non-equivariant models based on a number of
error metrics and demonstrate the benefits of the encoded equivariance and
energy conservation in terms of predictive performance and reduced training
requirements.
\\ ( https://arxiv.org/abs/2401.16914 ,  15801kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16936
Date: Tue, 30 Jan 2024 12:03:40 GMT   (8839kb,D)

Title: Multi-modal Representation Learning for Cross-modal Prediction of
  Continuous Weather Patterns from Discrete Low-Dimensional Data
Authors: Alif Bin Abdul Qayyum, Xihaier Luo, Nathan M. Urban, Xiaoning Qian,
  Byung-Jun Yoon
Categories: cs.LG cs.CV
\\
  World is looking for clean and renewable energy sources that do not pollute
the environment, in an attempt to reduce greenhouse gas emissions that
contribute to global warming. Wind energy has significant potential to not only
reduce greenhouse emission, but also meet the ever increasing demand for
energy. To enable the effective utilization of wind energy, addressing the
following three challenges in wind data analysis is crucial. Firstly, improving
data resolution in various climate conditions to ensure an ample supply of
information for assessing potential energy resources. Secondly, implementing
dimensionality reduction techniques for data collected from sensors/simulations
to efficiently manage and store large datasets. Thirdly, extrapolating wind
data from one spatial specification to another, particularly in cases where
data acquisition may be impractical or costly. We propose a deep learning based
approach to achieve multi-modal continuous resolution wind data prediction from
discontinuous wind data, along with data dimensionality reduction.
\\ ( https://arxiv.org/abs/2401.16936 ,  8839kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16945
Date: Tue, 30 Jan 2024 12:19:09 GMT   (667kb,D)

Title: Online Resource Allocation with Non-Stationary Customers
Authors: Xiaoyue Zhang, Hanzhang Qin, Mabel C. Chou
Categories: cs.LG math.OC
\\
  We propose a novel algorithm for online resource allocation with
non-stationary customer arrivals and unknown click-through rates. We assume
multiple types of customers arrive in a nonstationary stochastic fashion, with
unknown arrival rates in each period, and that customers' click-through rates
are unknown and can only be learned online. By leveraging results from the
stochastic contextual bandit with knapsack and online matching with adversarial
arrivals, we develop an online scheme to allocate the resources to
nonstationary customers. We prove that under mild conditions, our scheme
achieves a ``best-of-both-world'' result: the scheme has a sublinear regret
when the customer arrivals are near-stationary, and enjoys an optimal
competitive ratio under general (non-stationary) customer arrival
distributions. Finally, we conduct extensive numerical experiments to show our
approach generates near-optimal revenues for all different customer scenarios.
\\ ( https://arxiv.org/abs/2401.16945 ,  667kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16974
Date: Tue, 30 Jan 2024 12:57:52 GMT   (2437kb,D)

Title: CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement
  Learning
Authors: Andreas W.M. Sauter, Nicol\`o Botteghi, Erman Acar, Aske Plaat
Categories: cs.LG cs.AI
Comments: To be published In Proc. of the 23rd International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS 2024), Auckland, New Zealand,
  May 6 - 10, 2024, IFAAMAS
ACM-class: I.2.6; I.2.8
\\
  Causal discovery is the challenging task of inferring causal structure from
data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive
observations alone are not enough to distinguish correlation from causation,
there has been a recent push to incorporate interventions into machine learning
research. Reinforcement learning provides a convenient framework for such an
active approach to learning. This paper presents CORE, a deep reinforcement
learning-based approach for causal discovery and intervention planning. CORE
learns to sequentially reconstruct causal graphs from data while learning to
perform informative interventions. Our results demonstrate that CORE
generalizes to unseen graphs and efficiently uncovers causal structures.
Furthermore, CORE scales to larger graphs with up to 10 variables and
outperforms existing approaches in structure estimation accuracy and sample
efficiency. All relevant code and supplementary material can be found at
https://github.com/sa-and/CORE
\\ ( https://arxiv.org/abs/2401.16974 ,  2437kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17013
Date: Tue, 30 Jan 2024 13:49:03 GMT   (7706kb,D)

Title: Evaluation of Out-of-Distribution Detection Performance on Autonomous
  Driving Datasets
Authors: Jens Henriksson, Christian Berger, Stig Ursing, Markus Borg
Categories: cs.LG cs.CV
Comments: Preprint to 2023 IEEE International Conference On Artificial
  Intelligence Testing
\\
  Safety measures need to be systemically investigated to what extent they
evaluate the intended performance of Deep Neural Networks (DNNs) for critical
applications. Due to a lack of verification methods for high-dimensional DNNs,
a trade-off is needed between accepted performance and handling of
out-of-distribution (OOD) samples.
  This work evaluates rejecting outputs from semantic segmentation DNNs by
applying a Mahalanobis distance (MD) based on the most probable
class-conditional Gaussian distribution for the predicted class as an OOD
score. The evaluation follows three DNNs trained on the Cityscapes dataset and
tested on four automotive datasets and finds that classification risk can
drastically be reduced at the cost of pixel coverage, even when applied on
unseen datasets. The applicability of our findings will support legitimizing
safety measures and motivate their usage when arguing for safe usage of DNNs in
automotive perception.
\\ ( https://arxiv.org/abs/2401.17013 ,  7706kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17027
Date: Tue, 30 Jan 2024 14:02:49 GMT   (2228kb,D)

Title: Heterogeneous treatment effect estimation with subpopulation
  identification for personalized medicine in opioid use disorder
Authors: Seungyeon Lee, Ruoqi Liu, Wenyu Song, Ping Zhang
Categories: cs.LG
Comments: 2023 IEEE International Conference on Data Mining (ICDM)
\\
  Deep learning models have demonstrated promising results in estimating
treatment effects (TEE). However, most of them overlook the variations in
treatment outcomes among subgroups with distinct characteristics. This
limitation hinders their ability to provide accurate estimations and treatment
recommendations for specific subgroups. In this study, we introduce a novel
neural network-based framework, named SubgroupTE, which incorporates subgroup
identification and treatment effect estimation. SubgroupTE identifies diverse
subgroups and simultaneously estimates treatment effects for each subgroup,
improving the treatment effect estimation by considering the heterogeneity of
treatment responses. Comparative experiments on synthetic data show that
SubgroupTE outperforms existing models in treatment effect estimation.
Furthermore, experiments on a real-world dataset related to opioid use disorder
(OUD) demonstrate the potential of our approach to enhance personalized
treatment recommendations for OUD patients.
\\ ( https://arxiv.org/abs/2401.17027 ,  2228kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17035
Date: Tue, 30 Jan 2024 14:12:39 GMT   (173kb)

Title: Robust Kernel Sparse Subspace Clustering
Authors: Ivica Kopriva
Categories: cs.LG
Comments: 5 pages, 2 tables
ACM-class: I.5.3; G.2.2; I.4.6
\\
  Kernel methods are applied to many problems in pattern recognition, including
subspace clustering (SC). That way, nonlinear problems in the input data space
become linear in mapped high-dimensional feature space. Thereby,
computationally tractable nonlinear algorithms are enabled through implicit
mapping by the virtue of kernel trick. However, kernelization of linear
algorithms is possible only if square of the Froebenious norm of the error term
is used in related optimization problem. That, however, implies normal
distribution of the error. That is not appropriate for non-Gaussian errors such
as gross sparse corruptions that are modeled by -norm. Herein, to the best of
our knowledge, we propose for the first time robust kernel sparse SC (RKSSC)
algorithm for data with gross sparse corruptions. The concept, in principle,
can be applied to other SC algorithms to achieve robustness to the presence of
such type of corruption. We validated proposed approach on two well-known
datasets with linear robust SSC algorithm as a baseline model. According to
Wilcoxon test, clustering performance obtained by the RKSSC algorithm is
statistically significantly better than corresponding performance obtained by
the robust SSC algorithm. MATLAB code of proposed RKSSC algorithm is posted on
https://github.com/ikopriva/RKSSC.
\\ ( https://arxiv.org/abs/2401.17035 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17036
Date: Tue, 30 Jan 2024 14:16:02 GMT   (1027kb,D)

Title: Intrinsic Data Constraints and Upper Bounds in Binary Classification
  Performance
Authors: Fei Jing, Zi-Ke Zhang and Qingpeng Zhang
Categories: cs.LG cs.DS physics.data-an
Comments: 48 pages, 11 figures
\\
  The structure of data organization is widely recognized as having a
substantial influence on the efficacy of machine learning algorithms,
particularly in binary classification tasks. Our research provides a
theoretical framework suggesting that the maximum potential of binary
classifiers on a given dataset is primarily constrained by the inherent
qualities of the data. Through both theoretical reasoning and empirical
examination, we employed standard objective functions, evaluative metrics, and
binary classifiers to arrive at two principal conclusions. Firstly, we show
that the theoretical upper bound of binary classification performance on actual
datasets can be theoretically attained. This upper boundary represents a
calculable equilibrium between the learning loss and the metric of evaluation.
Secondly, we have computed the precise upper bounds for three commonly used
evaluation metrics, uncovering a fundamental uniformity with our overarching
thesis: the upper bound is intricately linked to the dataset's characteristics,
independent of the classifier in use. Additionally, our subsequent analysis
uncovers a detailed relationship between the upper limit of performance and the
level of class overlap within the binary classification data. This relationship
is instrumental for pinpointing the most effective feature subsets for use in
feature engineering.
\\ ( https://arxiv.org/abs/2401.17036 ,  1027kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17037
Date: Tue, 30 Jan 2024 14:16:06 GMT   (371kb,D)

Title: Bayesian Optimization with Noise-Free Observations: Improved Regret
  Bounds via Random Exploration
Authors: Hwanwoo Kim and Daniel Sanz-Alonso
Categories: cs.LG cs.NA math.NA stat.ML
\\
  This paper studies Bayesian optimization with noise-free observations. We
introduce new algorithms rooted in scattered data approximation that rely on a
random exploration step to ensure that the fill-distance of query points decays
at a near-optimal rate. Our algorithms retain the ease of implementation of the
classical GP-UCB algorithm and satisfy cumulative regret bounds that nearly
match those conjectured in arXiv:2002.05096, hence solving a COLT open problem.
Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian
optimization strategies in several examples.
\\ ( https://arxiv.org/abs/2401.17037 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17042
Date: Tue, 30 Jan 2024 14:23:01 GMT   (10935kb,D)

Title: Forecasting VIX using Bayesian Deep Learning
Authors: H\'ector J. Hort\'ua and Andr\'es Mora-Valencia
Categories: cs.LG
\\
  Recently, deep learning techniques are gradually replacing traditional
statistical and machine learning models as the first choice for price
forecasting tasks. In this paper, we leverage probabilistic deep learning for
inferring the volatility index VIX. We employ the probabilistic counterpart of
WaveNet, Temporal Convolutional Network (TCN), and Transformers. We show that
TCN outperforms all models with an RMSE around 0.189. In addition, it has been
well known that modern neural networks provide inaccurate uncertainty
estimates. For solving this problem, we use the standard deviation scaling to
calibrate the networks. Furthermore, we found out that MNF with Gaussian prior
outperforms Reparameterization Trick and Flipout models in terms of precision
and uncertainty predictions. Finally, we claim that MNF with Cauchy and
LogUniform prior distributions yield well calibrated TCN and WaveNet networks
being the former that best infer the VIX values.
\\ ( https://arxiv.org/abs/2401.17042 ,  10935kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17052
Date: Tue, 30 Jan 2024 14:33:18 GMT   (1320kb,D)

Title: Making Parametric Anomaly Detection on Tabular Data Non-Parametric Again
Authors: Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Li\^en Doan
Categories: cs.LG
\\
  Deep learning for tabular data has garnered increasing attention in recent
years, yet employing deep models for structured data remains challenging. While
these models excel with unstructured data, their efficacy with structured data
has been limited. Recent research has introduced retrieval-augmented models to
address this gap, demonstrating promising results in supervised tasks such as
classification and regression. In this work, we investigate using
retrieval-augmented models for anomaly detection on tabular data. We propose a
reconstruction-based approach in which a transformer model learns to
reconstruct masked features of \textit{normal} samples. We test the
effectiveness of KNN-based and attention-based modules to select relevant
samples to help in the reconstruction process of the target sample. Our
experiments on a benchmark of 31 tabular datasets reveal that augmenting this
reconstruction-based anomaly detection (AD) method with non-parametric
relationships via retrieval modules may significantly boost performance.
\\ ( https://arxiv.org/abs/2401.17052 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17062
Date: Tue, 30 Jan 2024 14:41:28 GMT   (1389kb,D)

Title: Outline of an Independent Systematic Blackbox Test for ML-based Systems
Authors: Hans-Werner Wiesbrock and J\"urgen Gro{\ss}mann
Categories: cs.LG
\\
  This article proposes a test procedure that can be used to test ML models and
ML-based systems independently of the actual training process. In this way, the
typical quality statements such as accuracy and precision of these models and
system can be verified independently, taking into account their black box
character and the immanent stochastic properties of ML models and their
training data. The article presents first results from a set of test
experiments and suggest extensions to existing test methods reflecting the
stochastic nature of ML models and ML-based systems.
\\ ( https://arxiv.org/abs/2401.17062 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17095
Date: Tue, 30 Jan 2024 15:21:50 GMT   (25072kb,D)

Title: Traffic estimation in unobserved network locations using data-driven
  macroscopic models
Authors: Pablo Guarda, Sean Qian
Categories: cs.LG cs.AI
Comments: 34 pages, 28 figures, 6 tables
\\
  This paper leverages macroscopic models and multi-source spatiotemporal data
collected from automatic traffic counters and probe vehicles to accurately
estimate traffic flow and travel time in links where these measurements are
unavailable. This problem is critical in transportation planning applications
where the sensor coverage is low and the planned interventions have
network-wide impacts. The proposed model, named the Macroscopic Traffic
Estimator (MaTE), can perform network-wide estimations of traffic flow and
travel time only using the set of observed measurements of these quantities.
Because MaTE is grounded in macroscopic flow theory, all parameters and
variables are interpretable. The estimated traffic flow satisfies fundamental
flow conservation constraints and exhibits an increasing monotonic relationship
with the estimated travel time. Using logit-based stochastic traffic assignment
as the principle for routing flow behavior makes the model fully differentiable
with respect to the model parameters. This property facilitates the application
of computational graphs to learn parameters from vast amounts of spatiotemporal
data. We also integrate neural networks and polynomial kernel functions to
capture link flow interactions and enrich the mapping of traffic flows into
travel times. MaTE also adds a destination choice model and a trip generation
model that uses historical data on the number of trips generated by location.
Experiments on synthetic data show that the model can accurately estimate
travel time and traffic flow in out-of-sample links. Results obtained using
real-world multi-source data from a large-scale transportation network suggest
that MaTE outperforms data-driven benchmarks, especially in travel time
estimation. The estimated parameters of MaTE are also informative about the
hourly change in travel demand and supply characteristics of the transportation
network.
\\ ( https://arxiv.org/abs/2401.17095 ,  25072kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17118
Date: Tue, 30 Jan 2024 15:53:07 GMT   (4782kb,D)

Title: Explainable data-driven modeling via mixture of experts: towards
  effective blending of grey and black-box models
Authors: Jessica Leoni, Valentina Breschi, Simone Formentin, Mara Tanelli
Categories: cs.LG cs.SY eess.SY
Comments: Submitted to Automatica
\\
  Traditional models grounded in first principles often struggle with accuracy
as the system's complexity increases. Conversely, machine learning approaches,
while powerful, face challenges in interpretability and in handling physical
constraints. Efforts to combine these models often often stumble upon
difficulties in finding a balance between accuracy and complexity. To address
these issues, we propose a comprehensive framework based on a "mixture of
experts" rationale. This approach enables the data-based fusion of diverse
local models, leveraging the full potential of first-principle-based priors.
Our solution allows independent training of experts, drawing on techniques from
both machine learning and system identification, and it supports both
collaborative and competitive learning paradigms. To enhance interpretability,
we penalize abrupt variations in the expert's combination. Experimental results
validate the effectiveness of our approach in producing an interpretable
combination of models closely resembling the target phenomena.
\\ ( https://arxiv.org/abs/2401.17118 ,  4782kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17123
Date: Mon, 29 Jan 2024 18:53:34 GMT   (60742kb,D)

Title: Unsupervised Discovery of Steerable Factors When Graph Deep Generative
  Models Are Entangled
Authors: Shengchao Liu, Chengpeng Wang, Jiarui Lu, Weili Nie, Hanchen Wang,
  Zhuoxinran Li, Bolei Zhou, Jian Tang
Categories: cs.LG cs.AI q-bio.QM
\\
  Deep generative models (DGMs) have been widely developed for graph data.
However, much less investigation has been carried out on understanding the
latent space of such pretrained graph DGMs. These understandings possess the
potential to provide constructive guidelines for crucial tasks, such as graph
controllable generation. Thus in this work, we are interested in studying this
problem and propose GraphCG, a method for the unsupervised discovery of
steerable factors in the latent space of pretrained graph DGMs. We first
examine the representation space of three pretrained graph DGMs with six
disentanglement metrics, and we observe that the pretrained representation
space is entangled. Motivated by this observation, GraphCG learns the steerable
factors via maximizing the mutual information between semantic-rich directions,
where the controlled graph moving along the same direction will share the same
steerable factors. We quantitatively verify that GraphCG outperforms four
competitive baselines on two graph DGMs pretrained on two molecule datasets.
Additionally, we qualitatively illustrate seven steerable factors learned by
GraphCG on five pretrained DGMs over five graph datasets, including two for
molecules and three for point clouds.
\\ ( https://arxiv.org/abs/2401.17123 ,  60742kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17124
Date: Mon, 29 Jan 2024 16:01:38 GMT   (342kb,D)

Title: Spectral Co-Distillation for Personalized Federated Learning
Authors: Zihan Chen, Howard H. Yang, Tony Q.S. Quek, Kai Fong Ernest Chong
Categories: cs.LG cs.NI
Comments: 13 pages, NeurIPS 2023. Code at
  https://github.com/jimmyc96/spectral-dis-FL
\\
  Personalized federated learning (PFL) has been widely investigated to address
the challenge of data heterogeneity, especially when a single generic model is
inadequate in satisfying the diverse performance requirements of local clients
simultaneously. Existing PFL methods are inherently based on the idea that the
relations between the generic global and personalized local models are captured
by the similarity of model weights. Such a similarity is primarily based on
either partitioning the model architecture into generic versus personalized
components, or modeling client relationships via model weights. To better
capture similar (yet distinct) generic versus personalized model
representations, we propose \textit{spectral distillation}, a novel
distillation method based on model spectrum information. Building upon spectral
distillation, we also introduce a co-distillation framework that establishes a
two-way bridge between generic and personalized model training. Moreover, to
utilize the local idle time in conventional PFL, we propose a wait-free local
training protocol. Through extensive experiments on multiple datasets over
diverse heterogeneous data settings, we demonstrate the outperformance and
efficacy of our proposed spectral co-distillation method, as well as our
wait-free training protocol.
\\ ( https://arxiv.org/abs/2401.17124 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17127
Date: Tue, 30 Jan 2024 16:00:14 GMT   (214kb,D)

Title: Personalized Differential Privacy for Ridge Regression
Authors: Krishna Acharya, Franziska Boenisch, Rakshit Naidu, Juba Ziani
Categories: cs.LG cs.CR cs.CY
Comments: 30 pages
\\
  The increased application of machine learning (ML) in sensitive domains
requires protecting the training data through privacy frameworks, such as
differential privacy (DP). DP requires to specify a uniform privacy level
$\varepsilon$ that expresses the maximum privacy loss that each data point in
the entire dataset is willing to tolerate. Yet, in practice, different data
points often have different privacy requirements. Having to set one uniform
privacy level is usually too restrictive, often forcing a learner to guarantee
the stringent privacy requirement, at a large cost to accuracy. To overcome
this limitation, we introduce our novel Personalized-DP Output Perturbation
method (PDP-OP) that enables to train Ridge regression models with individual
per data point privacy levels. We provide rigorous privacy proofs for our
PDP-OP as well as accuracy guarantees for the resulting model. This work is the
first to provide such theoretical accuracy guarantees when it comes to
personalized DP in machine learning, whereas previous work only provided
empirical evaluations. We empirically evaluate PDP-OP on synthetic and real
datasets and with diverse privacy distributions. We show that by enabling each
data point to specify their own privacy requirement, we can significantly
improve the privacy-accuracy trade-offs in DP. We also show that PDP-OP
outperforms the personalized privacy techniques of Jorgensen et al. (2015).
\\ ( https://arxiv.org/abs/2401.17127 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17139
Date: Tue, 30 Jan 2024 16:19:55 GMT   (197kb,D)

Title: Large Language Model Evaluation via Matrix Entropy
Authors: Lai Wei, Zhiquan Tan, Chenghai Li, Jindong Wang, Weiran Huang
Categories: cs.LG cs.AI cs.CL cs.IT math.IT
\\
  Large language models (LLMs) have revolutionized the field of natural
language processing, extending their strong capabilities into multi-modal
domains. Thus, it is vital to define proper and diversified metrics for the
evaluation of LLMs.
  In this paper, we introduce matrix entropy, a novel metric rooted in
information theory and geometry principles to quantify the data compression
proficiency in LLMs. It reflects the model's ability to extract relevant
information and eliminate unnecessary elements, thereby providing insight into
the language model's intrinsic capability. Specifically, we demonstrate its
applicability in both single-modal (language) and multi-modal settings. For
language models, our findings reveal that the matrix entropy of representations
follows a scaling law type reduction when the model scales up, serving as a
complement to the traditional loss scaling law. For the multi-modal setting, we
also propose an evaluation method based on matrix entropy for assessing
alignment quality and we find that modern large multi-modal models exhibit
great alignment performance.
\\ ( https://arxiv.org/abs/2401.17139 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17173
Date: Tue, 30 Jan 2024 17:04:47 GMT   (607kb,D)

Title: Zero-Shot Reinforcement Learning via Function Encoders
Authors: Tyler Ingebrand, Amy Zhang, Ufuk Topcu
Categories: cs.LG cs.AI
\\
  Although reinforcement learning (RL) can solve many challenging sequential
decision making problems, achieving zero-shot transfer across related tasks
remains a challenge. The difficulty lies in finding a good representation for
the current task so that the agent understands how it relates to previously
seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a
representation learning algorithm which represents a function as a weighted
combination of learned, non-linear basis functions. By using a function encoder
to represent the reward function or the transition function, the agent has
information on how the current task relates to previously seen tasks via a
coherent vector representation. Thus, the agent is able to achieve transfer
between related tasks at run time with no additional training. We demonstrate
state-of-the-art data efficiency, asymptotic performance, and training
stability in three RL fields by augmenting basic RL algorithms with a function
encoder task representation.
\\ ( https://arxiv.org/abs/2401.17173 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17178
Date: Tue, 30 Jan 2024 17:11:04 GMT   (8125kb,D)

Title: GraphViz2Vec: A Structure-aware Feature Generation Model to Improve
  Classification in GNNs
Authors: Shraban Kumar Chatterjee, Suman Kundu
Categories: cs.LG cs.AI cs.SI
\\
  GNNs are widely used to solve various tasks including node classification and
link prediction. Most of the GNN architectures assume the initial embedding to
be random or generated from popular distributions. These initial embeddings
require multiple layers of transformation to converge into a meaningful latent
representation. While number of layers allow accumulation of larger
neighbourhood of a node it also introduce the problem of over-smoothing. In
addition, GNNs are inept at representing structural information. For example,
the output embedding of a node does not capture its triangles participation. In
this paper, we presented a novel feature extraction methodology GraphViz2Vec
that can capture the structural information of a node's local neighbourhood to
create meaningful initial embeddings for a GNN model. These initial embeddings
helps existing models achieve state-of-the-art results in various
classification tasks. Further, these initial embeddings help the model to
produce desired results with only two layers which in turn reduce the problem
of over-smoothing. The initial encoding of a node is obtained from an image
classification model trained on multiple energy diagrams of its local
neighbourhood. These energy diagrams are generated with the induced sub-graph
of the nodes traversed by multiple random walks. The generated encodings
increase the performance of existing models on classification tasks (with a
mean increase of $4.65\%$ and $2.58\%$ for the node and link classification
tasks, respectively), with some models achieving state-of-the-art results.
\\ ( https://arxiv.org/abs/2401.17178 ,  8125kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17200
Date: Tue, 30 Jan 2024 17:33:35 GMT   (1966kb,D)

Title: NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble
  Techniques
Authors: Weronika Hryniewska-Guzik, Bartosz Sawicki, Przemys{\l}aw Biecek
Categories: cs.LG cs.AI cs.CV
\\
  This paper presents a comprehensive comparative analysis of explainable
artificial intelligence (XAI) ensembling methods. Our research brings three
significant contributions. Firstly, we introduce a novel ensembling method,
NormEnsembleXAI, that leverages minimum, maximum, and average functions in
conjunction with normalization techniques to enhance interpretability.
Secondly, we offer insights into the strengths and weaknesses of XAI ensemble
methods. Lastly, we provide a library, facilitating the practical
implementation of XAI ensembling, thus promoting the adoption of transparent
and interpretable deep learning models.
\\ ( https://arxiv.org/abs/2401.17200 ,  1966kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17263
Date: Tue, 30 Jan 2024 18:56:08 GMT   (1578kb,D)

Title: Robust Prompt Optimization for Defending Language Models Against
  Jailbreaking Attacks
Authors: Andy Zhou and Bo Li and Haohan Wang
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: code available at https://github.com/andyz245/rpo
\\
  Despite advances in AI alignment, language models (LM) remain vulnerable to
adversarial attacks or jailbreaking, in which adversaries modify input prompts
to induce harmful behavior. While some defenses have been proposed, they focus
on narrow threat models and fall short of a strong defense, which we posit
should be effective, universal, and practical. To achieve this, we propose the
first adversarial objective for defending LMs against jailbreaking attacks and
an algorithm, robust prompt optimization (RPO), that uses gradient-based token
optimization to enforce harmless outputs. This results in an easily accessible
suffix that significantly improves robustness to both jailbreaks seen during
optimization and unknown, held-out jailbreaks, reducing the attack success rate
on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find
that RPO has a minor effect on normal LM use, is successful under adaptive
attacks, and can transfer to black-box models, reducing the success rate of the
strongest attack on GPT-4 from 92% to 6%.
\\ ( https://arxiv.org/abs/2401.17263 ,  1578kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17267
Date: Tue, 30 Jan 2024 18:57:08 GMT   (758kb)

Title: ReacLLaMA: Merging chemical and textual information in chemical
  reactivity AI models
Authors: Aline Hartgers, Ramil Nugmanov, Kostiantyn Chernichenko, Joerg Kurt
  Wegner
Categories: cs.LG q-bio.QM
\\
  Chemical reactivity models are developed to predict chemical reaction
outcomes in the form of classification (success/failure) or regression (product
yield) tasks. The vast majority of the reported models are trained solely on
chemical information such as reactants, products, reagents, and solvents, but
not on the details of a synthetic protocol. Herein incorporation of procedural
text with the aim to augment the Graphormer reactivity model and improve its
accuracy is presented. Two major approaches are used: training an adapter
Graphormer model that is provided with a GPT-2-derived latent representation of
the text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a
dataset with the LLaMA 2 model followed by training the Graphormer on an
extended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the
discernment of unpromising reactions, thereby providing more accurate models
with improved specificity.
\\ ( https://arxiv.org/abs/2401.17267 ,  758kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2210.01595 (*cross-listing*)
Date: Tue, 4 Oct 2022 13:18:15 GMT   (5486kb,D)

Title: FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast
  Fourier Convolutions
Authors: Bruno Berenguel-Baeta, Jesus Bermudez-Cameo and Jose J. Guerrero
Categories: cs.CV cs.AI cs.RO
Comments: 7 pages, 5 figures, 3 tables
\\
  In this work we present FreDSNet, a deep learning solution which obtains
semantic 3D understanding of indoor environments from single panoramas.
Omnidirectional images reveal task-specific advantages when addressing scene
understanding problems due to the 360-degree contextual information about the
entire environment they provide. However, the inherent characteristics of the
omnidirectional images add additional problems to obtain an accurate detection
and segmentation of objects or a good depth estimation. To overcome these
problems, we exploit convolutions in the frequential domain obtaining a wider
receptive field in each convolutional layer. These convolutions allow to
leverage the whole context information from omnidirectional images. FreDSNet is
the first network that jointly provides monocular depth estimation and semantic
segmentation from a single panoramic image exploiting fast Fourier
convolutions. Our experiments show that FreDSNet has similar performance as
specific state of the art methods for semantic segmentation and depth
estimation. FreDSNet code is publicly available in
https://github.com/Sbrunoberenguel/FreDSNet
\\ ( https://arxiv.org/abs/2210.01595 ,  5486kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16443 (*cross-listing*)
Date: Sat, 27 Jan 2024 19:15:24 GMT   (6312kb,D)

Title: Evaluating Deep Networks for Detecting User Familiarity with VR from
  Hand Interactions
Authors: Mingjun Li, Numan Zafar, Natasha Kholgade Banerjee, Sean Banerjee
Categories: cs.HC cs.AI cs.LG
Comments: AIxVR 2024 poster paper
\\
  As VR devices become more prevalent in the consumer space, VR applications
are likely to be increasingly used by users unfamiliar with VR. Detecting the
familiarity level of a user with VR as an interaction medium provides the
potential of providing on-demand training for acclimatization and prevents the
user from being burdened by the VR environment in accomplishing their tasks. In
this work, we present preliminary results of using deep classifiers to conduct
automatic detection of familiarity with VR by using hand tracking of the user
as they interact with a numeric passcode entry panel to unlock a VR door. We
use a VR door as we envision it to the first point of entry to collaborative
virtual spaces, such as meeting rooms, offices, or clinics. Users who are
unfamiliar with VR will have used their hands to open doors with passcode entry
panels in the real world. Thus, while the user may not be familiar with VR,
they would be familiar with the task of opening the door. Using a pilot dataset
consisting of 7 users familiar with VR, and 7 not familiar with VR, we acquire
highest accuracy of 88.03\% when 6 test users, 3 familiar and 3 not familiar,
are evaluated with classifiers trained using data from the remaining 8 users.
Our results indicate potential for using user movement data to detect
familiarity for the simple yet important task of secure passcode-based access.
\\ ( https://arxiv.org/abs/2401.16443 ,  6312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16444 (*cross-listing*)
Date: Sun, 28 Jan 2024 05:05:57 GMT   (20208kb,D)

Title: Enhancing Human Experience in Human-Agent Collaboration: A
  Human-Centered Modeling Approach Based on Positive Human Gain
Authors: Yiming Gao, Feiyu Liu, Liang Wang, Zhenjie Lian, Dehua Zheng, Weixuan
  Wang, Wenjin Yang, Siqin Li, Xianliang Wang, Wenhui Chen, Jing Dai, Qiang Fu,
  Wei Yang, Lanxiao Huang, Wei Liu
Categories: cs.HC cs.AI
Comments: Accepted at ICLR 2024. arXiv admin note: text overlap with
  arXiv:2304.11632
\\
  Existing game AI research mainly focuses on enhancing agents' abilities to
win games, but this does not inherently make humans have a better experience
when collaborating with these agents. For example, agents may dominate the
collaboration and exhibit unintended or detrimental behaviors, leading to poor
experiences for their human partners. In other words, most game AI agents are
modeled in a "self-centered" manner. In this paper, we propose a
"human-centered" modeling scheme for collaborative agents that aims to enhance
the experience of humans. Specifically, we model the experience of humans as
the goals they expect to achieve during the task. We expect that agents should
learn to enhance the extent to which humans achieve these goals while
maintaining agents' original abilities (e.g., winning games). To achieve this,
we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG
approach introduces a "baseline", which corresponds to the extent to which
humans primitively achieve their goals, and encourages agents to learn
behaviors that can effectively enhance humans in achieving their goals better.
We evaluate the RLHG agent in the popular Multi-player Online Battle Arena
(MOBA) game, Honor of Kings, by conducting real-world human-agent tests. Both
objective performance and subjective preference results show that the RLHG
agent provides participants better gaming experience.
\\ ( https://arxiv.org/abs/2401.16444 ,  20208kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16448 (*cross-listing*)
Date: Sun, 28 Jan 2024 19:45:25 GMT   (1070kb,D)

Title: LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware
  Debugging
Authors: Weimin Fu, Kaichen Yang, Raj Gautam Dutta, Xiaolong Guo, Gang Qu
Categories: cs.AR cs.AI
Comments: 6 pages. 1 figure
Journal-ref: 2023 Asian Hardware Oriented Security and Trust Symposium
  (AsianHOST), Tianjin, China, 2023, pp. 1-6
DOI: 10.1109/AsianHOST59942.2023.10409307
\\
  This paper presents LLM4SecHW, a novel framework for hardware debugging that
leverages domain specific Large Language Model (LLM). Despite the success of
LLMs in automating various software development tasks, their application in the
hardware security domain has been limited due to the constraints of commercial
LLMs and the scarcity of domain specific data. To address these challenges, we
propose a unique approach to compile a dataset of open source hardware design
defects and their remediation steps, utilizing version control data. This
dataset provides a substantial foundation for training machine learning models
for hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this
dataset, enabling the identification and rectification of bugs in hardware
designs. This pioneering approach offers a reference workflow for the
application of fine tuning domain specific LLMs in other research areas. We
evaluate the performance of our proposed system on various open source hardware
designs, demonstrating its efficacy in accurately identifying and correcting
defects. Our work brings a new perspective on automating the quality control
process in hardware design.
\\ ( https://arxiv.org/abs/2401.16448 ,  1070kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16450 (*cross-listing*)
Date: Sun, 28 Jan 2024 22:49:33 GMT   (801kb,D)

Title: ACCESS: Prompt Engineering for Automated Web Accessibility Violation
  Corrections
Authors: Calista Huang, Alyssa Ma, Suchir Vyasamudri, Eugenie Puype, Sayem
  Kamal, Juan Belza Garcia, Salar Cheema, Michael Lutz
Categories: cs.HC cs.AI cs.SE
Comments: 11 pages, 6 figures
\\
  With the increasing need for inclusive and user-friendly technology, web
accessibility is crucial to ensuring equal access to online content for
individuals with disabilities, including visual, auditory, cognitive, or motor
impairments. Despite the existence of accessibility guidelines and standards
such as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility
Initiative (W3C), over 90\% of websites still fail to meet the necessary
accessibility requirements. For web users with disabilities, there exists a
need for a tool to automatically fix web page accessibility errors. While
research has demonstrated methods to find and target accessibility errors, no
research has focused on effectively correcting such violations. This paper
presents a novel approach to correcting accessibility violations on the web by
modifying the document object model (DOM) in real time with foundation models.
Leveraging accessibility error information, large language models (LLMs), and
prompt engineering techniques, we achieved greater than a 51\% reduction in
accessibility violation errors after corrections on our novel benchmark:
ACCESS. Our work demonstrates a valuable approach toward the direction of
inclusive web content, and provides directions for future research to explore
advanced methods to automate web accessibility.
\\ ( https://arxiv.org/abs/2401.16450 ,  801kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16454 (*cross-listing*)
Date: Mon, 29 Jan 2024 06:57:02 GMT   (1409kb,D)

Title: KAUCUS: Knowledge Augmented User Simulators for Training Language Model
  Assistants
Authors: Kaustubh D. Dhole
Categories: cs.HC cs.AI cs.CL cs.IR
Comments: Simulation of Conversational Intelligence in Chat, EACL 2024
ACM-class: I.2.7; H.3.3
\\
  An effective multi-turn instruction-following assistant can be developed by
creating a simulator that can generate useful interaction data. Apart from
relying on its intrinsic weights, an ideal user simulator should also be able
to bootstrap external knowledge rapidly in its raw form to simulate the
multifarious diversity of text available over the internet. Previous user
simulators generally lacked diversity, were mostly closed domain, and
necessitated rigid schema making them inefficient to rapidly scale to
incorporate external knowledge. In this regard, we introduce, Kaucus, a
Knowledge-Augmented User Simulator framework, to outline a process of creating
diverse user simulators, that can seamlessly exploit external knowledge as well
as benefit downstream assistant model training. Through two GPT-J based
simulators viz., a Retrieval Augmented Simulator and a Summary Controlled
Simulator we generate diverse simulator-assistant interactions. Through reward
and preference model-based evaluations, we find that these interactions serve
as useful training data and create more helpful downstream assistants. We also
find that incorporating knowledge through retrieval augmentation or summary
control helps create better assistants.
\\ ( https://arxiv.org/abs/2401.16454 ,  1409kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16458 (*cross-listing*)
Date: Mon, 29 Jan 2024 10:11:05 GMT   (1715kb,D)

Title: Credit Risk Meets Large Language Models: Building a Risk Indicator from
  Loan Descriptions in P2P Lending
Authors: Mario Sanz-Guerrero, Javier Arroyo
Categories: q-fin.RM cs.AI cs.CL cs.LG
\\
  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,
linking borrowers with lenders through online platforms. However, P2P lending
faces the challenge of information asymmetry, as lenders often lack sufficient
data to assess the creditworthiness of borrowers. This paper proposes a novel
approach to address this issue by leveraging the textual descriptions provided
by borrowers during the loan application process. Our methodology involves
processing these textual descriptions using a Large Language Model (LLM), a
powerful tool capable of discerning patterns and semantics within the text.
Transfer learning is applied to adapt the LLM to the specific task at hand.
  Our results derived from the analysis of the Lending Club dataset show that
the risk score generated by BERT, a widely used LLM, significantly improves the
performance of credit risk classifiers. However, the inherent opacity of
LLM-based systems, coupled with uncertainties about potential biases,
underscores critical considerations for regulatory frameworks and engenders
trust-related concerns among end-users, opening new avenues for future research
in the dynamic landscape of P2P lending and artificial intelligence.
\\ ( https://arxiv.org/abs/2401.16458 ,  1715kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16461 (*cross-listing*)
Date: Mon, 29 Jan 2024 11:09:45 GMT   (1112kb,D)

Title: Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents
Authors: Sz-Ting Tzeng, Nirav Ajmeri, Munindar P. Singh
Categories: cs.MA cs.AI cs.LG
Comments: 12 pages, 11 figures, 5 tables (and supplementary material with code
  availability and additional results), accepted at AAMAS 2024
\\
  A multiagent system can be viewed as a society of autonomous agents, whose
interactions can be effectively regulated via social norms. In general, the
norms of a society are not hardcoded but emerge from the agents' interactions.
Specifically, how the agents in a society react to each other's behavior and
respond to the reactions of others determines which norms emerge in the
society. We think of these reactions by an agent to the satisfactory or
unsatisfactory behaviors of another agent as communications from the first
agent to the second agent. Understanding these communications is a kind of
social intelligence: these communications provide natural drivers for norm
emergence by pushing agents toward certain behaviors, which can become
established as norms. Whereas it is well-known that sanctioning can lead to the
emergence of norms, we posit that a broader kind of social intelligence can
prove more effective in promoting cooperation in a multiagent system.
  Accordingly, we develop Nest, a framework that models social intelligence in
the form of a wider variety of communications and understanding of them than in
previous work. To evaluate Nest, we develop a simulated pandemic environment
and conduct simulation experiments to compare Nest with baselines considering a
combination of three kinds of social communication: sanction, tell, and hint.
  We find that societies formed of Nest agents achieve norms faster; moreover,
Nest agents effectively avoid undesirable consequences, which are negative
sanctions and deviation from goals, and yield higher satisfaction for
themselves than baseline agents despite requiring only an equivalent amount of
information.
\\ ( https://arxiv.org/abs/2401.16461 ,  1112kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16467 (*cross-listing*)
Date: Mon, 29 Jan 2024 18:45:30 GMT   (498kb,D)

Title: ReGAL: Refactoring Programs to Discover Generalizable Abstractions
Authors: Elias Stengel-Eskin, Archiki Prasad, Mohit Bansal
Categories: cs.SE cs.AI cs.CL cs.LG
Comments: 18 pages; First two authors contributed equally; Code:
  https://github.com/esteng/regal_program_learning
\\
  While large language models (LLMs) are increasingly being used for program
synthesis, they lack the global view needed to develop useful abstractions;
they generally predict programs one at a time, often repeating the same
functionality. Generating redundant code from scratch is both inefficient and
error-prone. To address this, we propose Refactoring for Generalizable
Abstraction Learning (ReGAL), a gradient-free method for learning a library of
reusable functions via code refactorization, i.e. restructuring code without
changing its execution output. ReGAL learns from a small set of existing
programs, iteratively verifying and refining its abstractions via execution. We
find that the shared function libraries discovered by ReGAL make programs
easier to predict across diverse domains. On three datasets (LOGO graphics
generation, Date reasoning, and TextCraft, a Minecraft-based text game), both
open-source and proprietary LLMs improve in accuracy when predicting programs
with ReGAL functions. For CodeLlama-13B, ReGAL results in absolute accuracy
increases of 11.5% on graphics, 26.1% on date understanding, and 8.1% on
TextCraft, outperforming GPT-3.5 in two of three domains. Our analysis reveals
ReGAL's abstractions encapsulate frequently-used subroutines as well as
environment dynamics.
\\ ( https://arxiv.org/abs/2401.16467 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16618 (*cross-listing*)
Date: Mon, 29 Jan 2024 23:14:15 GMT   (4971kb,D)

Title: A comparison of RL-based and PID controllers for 6-DOF swimming robots:
  hybrid underwater object tracking
Authors: Faraz Lotfi, Khalil Virji, Nicholas Dudek, and Gregory Dudek
Categories: cs.RO cs.AI
\\
  In this paper, we present an exploration and assessment of employing a
centralized deep Q-network (DQN) controller as a substitute for the prevalent
use of PID controllers in the context of 6DOF swimming robots. Our primary
focus centers on illustrating this transition with the specific case of
underwater object tracking. DQN offers advantages such as data efficiency and
off-policy learning, while remaining simpler to implement than other
reinforcement learning methods. Given the absence of a dynamic model for our
robot, we propose an RL agent to control this multi-input-multi-output (MIMO)
system, where a centralized controller may offer more robust control than
distinct PIDs. Our approach involves initially using classical controllers for
safe exploration, then gradually shifting to DQN to take full control of the
robot.
  We divide the underwater tracking task into vision and control modules. We
use established methods for vision-based tracking and introduce a centralized
DQN controller. By transmitting bounding box data from the vision module to the
control module, we enable adaptation to various objects and effortless vision
system replacement. Furthermore, dealing with low-dimensional data facilitates
cost-effective online learning for the controller. Our experiments, conducted
within a Unity-based simulator, validate the effectiveness of a centralized RL
agent over separated PID controllers, showcasing the applicability of our
framework for training the underwater RL agent and improved performance
compared to traditional control methods. The code for both real and simulation
implementations is at https://github.com/FARAZLOTFI/underwater-object-tracking.
\\ ( https://arxiv.org/abs/2401.16618 ,  4971kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16633 (*cross-listing*)
Date: Tue, 30 Jan 2024 00:06:16 GMT   (291kb)

Title: I came, I saw, I certified: some perspectives on the safety assurance of
  cyber-physical systems
Authors: Mithila Sivakumar, Alvine B. Belle, Kimya Khakzad Shahandashti,
  Oluwafemi Odu, Hadi Hemmati, Segla Kpodjedo, Song Wang, Opeyemi O. Adesina
Categories: cs.SE cs.AI
\\
  The execution failure of cyber-physical systems (e.g., autonomous driving
systems, unmanned aerial systems, and robotic systems) could result in the loss
of life, severe injuries, large-scale environmental damage, property
destruction, and major economic loss. Hence, such systems usually require a
strong justification that they will effectively support critical requirements
(e.g., safety, security, and reliability) for which they were designed. Thus,
it is often mandatory to develop compelling assurance cases to support that
justification and allow regulatory bodies to certify such systems. In such
contexts, detecting assurance deficits, relying on patterns to improve the
structure of assurance cases, improving existing assurance case notations, and
(semi-)automating the generation of assurance cases are key to develop
compelling assurance cases and foster consumer acceptance. We therefore explore
challenges related to such assurance enablers and outline some potential
directions that could be explored to tackle them.
\\ ( https://arxiv.org/abs/2401.16633 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16672 (*cross-listing*)
Date: Tue, 30 Jan 2024 01:45:03 GMT   (5056kb)

Title: AutoIE: An Automated Framework for Information Extraction from
  Scientific Literature
Authors: Yangyang Liu, Shoubin Li
Categories: cs.IR cs.AI cs.CE
\\
  In the rapidly evolving field of scientific research, efficiently extracting
key information from the burgeoning volume of scientific papers remains a
formidable challenge. This paper introduces an innovative framework designed to
automate the extraction of vital data from scientific PDF documents, enabling
researchers to discern future research trajectories more readily. AutoIE
uniquely integrates four novel components: (1) A multi-semantic feature
fusion-based approach for PDF document layout analysis; (2) Advanced functional
block recognition in scientific texts; (3) A synergistic technique for
extracting and correlating information on molecular sieve synthesis; (4) An
online learning paradigm tailored for molecular sieve literature. Our SBERT
model achieves high Marco F1 scores of 87.19 and 89.65 on CoNLL04 and ADE
datasets. In addition, a practical application of AutoIE in the petrochemical
molecular sieve synthesis domain demonstrates its efficacy, evidenced by an
impressive 78\% accuracy rate. This research paves the way for enhanced data
management and interpretation in molecular sieve synthesis. It is a valuable
asset for seasoned experts and newcomers in this specialized field.
\\ ( https://arxiv.org/abs/2401.16672 ,  5056kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16742 (*cross-listing*)
Date: Tue, 30 Jan 2024 04:40:49 GMT   (2942kb,D)

Title: Generative AI-based closed-loop fMRI system
Authors: Mikihiro Kasahara, Taiki Oka, Vincent Taschereau-Dumouchel, Mitsuo
  Kawato, Hiroki Takakura, Aurelio Cortese
Categories: cs.HC cs.AI cs.CR cs.LG
\\
  While generative AI is now widespread and useful in society, there are
potential risks of misuse, e.g., unconsciously influencing cognitive processes
or decision-making. Although this causes a security problem in the cognitive
domain, there has been no research about neural and computational mechanisms
counteracting the impact of malicious generative AI in humans. We propose
DecNefGAN, a novel framework that combines a generative adversarial system and
a neural reinforcement model. More specifically, DecNefGAN bridges human and
generative AI in a closed-loop system, with the AI creating stimuli that induce
specific mental states, thus exerting external control over neural activity.
The objective of the human is the opposite, to compete and reach an orthogonal
mental state. This framework can contribute to elucidating how the human brain
responds to and counteracts the potential influence of generative AI.
\\ ( https://arxiv.org/abs/2401.16742 ,  2942kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16765 (*cross-listing*)
Date: Tue, 30 Jan 2024 06:04:04 GMT   (294kb,D)

Title: A Cross-Language Investigation into Jailbreak Attacks in Large Language
  Models
Authors: Jie Li, Yi Liu, Chongyang Liu, Ling Shi, Xiaoning Ren, Yaowen Zheng,
  Yang Liu, Yinxing Xue
Categories: cs.CR cs.AI
\\
  Large Language Models (LLMs) have become increasingly popular for their
advanced text generation capabilities across various domains. However, like any
software, they face security challenges, including the risk of 'jailbreak'
attacks that manipulate LLMs to produce prohibited content. A particularly
underexplored area is the Multilingual Jailbreak attack, where malicious
questions are translated into various languages to evade safety filters.
Currently, there is a lack of comprehensive empirical studies addressing this
specific threat.
  To address this research gap, we conducted an extensive empirical study on
Multilingual Jailbreak attacks. We developed a novel semantic-preserving
algorithm to create a multilingual jailbreak dataset and conducted an
exhaustive evaluation on both widely-used open-source and commercial LLMs,
including GPT-4 and LLaMa. Additionally, we performed interpretability analysis
to uncover patterns in Multilingual Jailbreak attacks and implemented a
fine-tuning mitigation method. Our findings reveal that our mitigation strategy
significantly enhances model defense, reducing the attack success rate by
96.2%. This study provides valuable insights into understanding and mitigating
Multilingual Jailbreak attacks.
\\ ( https://arxiv.org/abs/2401.16765 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16807 (*cross-listing*)
Date: Tue, 30 Jan 2024 08:07:28 GMT   (448kb,D)

Title: Detecting LLM-Assisted Writing in Scientific Communication: Are We There
  Yet?
Authors: Teddy Lazebnik, Ariel Rosenfeld
Categories: cs.IR cs.AI
\\
  Large Language Models (LLMs), exemplified by ChatGPT, have significantly
reshaped text generation, particularly in the realm of writing assistance.
While ethical considerations underscore the importance of transparently
acknowledging LLM use, especially in scientific communication, genuine
acknowledgment remains infrequent. A potential avenue to encourage accurate
acknowledging of LLM-assisted writing involves employing automated detectors.
Our evaluation of four cutting-edge LLM-generated text detectors reveals their
suboptimal performance compared to a simple ad-hoc detector designed to
identify abrupt writing style changes around the time of LLM proliferation. We
contend that the development of specialized detectors exclusively dedicated to
LLM-assisted writing detection is necessary. Such detectors could play a
crucial role in fostering more authentic recognition of LLM involvement in
scientific communication, addressing the current challenges in acknowledgment
practices.
\\ ( https://arxiv.org/abs/2401.16807 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16867 (*cross-listing*)
Date: Tue, 30 Jan 2024 10:17:46 GMT   (2422kb,D)

Title: A Tournament of Transformation Models: B-Spline-based vs. Mesh-based
  Multi-Objective Deformable Image Registration
Authors: Georgios Andreadis, Joas I. Mulder, Anton Bouter, Peter A. N. Bosman,
  Tanja Alderliesten
Categories: cs.CV cs.AI cs.NE
Comments: Pre-print for the SPIE Medical Imaging: Image Processing Conference
\\
  The transformation model is an essential component of any deformable image
registration approach. It provides a representation of physical deformations
between images, thereby defining the range and realism of registrations that
can be found. Two types of transformation models have emerged as popular
choices: B-spline models and mesh models. Although both models have been
investigated in detail, a direct comparison has not yet been made, since the
models are optimized using very different optimization methods in practice.
B-spline models are predominantly optimized using gradient-descent methods,
while mesh models are typically optimized using finite-element method solvers
or evolutionary algorithms. Multi-objective optimization methods, which aim to
find a diverse set of high-quality trade-off registrations, are increasingly
acknowledged to be important in deformable image registration. Since these
methods search for a diverse set of registrations, they can provide a more
complete picture of the capabilities of different transformation models, making
them suitable for a comparison of models. In this work, we conduct the first
direct comparison between B-spline and mesh transformation models, by
optimizing both models with the same state-of-the-art multi-objective
optimization method, the Multi-Objective Real-Valued Gene-pool Optimal Mixing
Evolutionary Algorithm (MO-RV-GOMEA). The combination with B-spline
transformation models, moreover, is novel. We experimentally compare both
models on two different registration problems that are both based on pelvic CT
scans of cervical cancer patients, featuring large deformations. Our results,
on three cervical cancer patients, indicate that the choice of transformation
model can have a profound impact on the diversity and quality of achieved
registration outcomes.
\\ ( https://arxiv.org/abs/2401.16867 ,  2422kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16889 (*cross-listing*)
Date: Tue, 30 Jan 2024 10:48:43 GMT   (9930kb,D)

Title: Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal
  Locomotion Control
Authors: Zhongyu Li, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth,
  Koushil Sreenath
Categories: cs.RO cs.AI cs.SY eess.SY
\\
  This paper presents a comprehensive study on using deep reinforcement
learning (RL) to create dynamic locomotion controllers for bipedal robots.
Going beyond focusing on a single locomotion skill, we develop a general
control solution that can be used for a range of dynamic bipedal skills, from
periodic walking and running to aperiodic jumping and standing. Our RL-based
controller incorporates a novel dual-history architecture, utilizing both a
long-term and short-term input/output (I/O) history of the robot. This control
architecture, when trained through the proposed end-to-end RL approach,
consistently outperforms other methods across a diverse range of skills in both
simulation and the real world.The study also delves into the adaptivity and
robustness introduced by the proposed RL system in developing locomotion
controllers. We demonstrate that the proposed architecture can adapt to both
time-invariant dynamics shifts and time-variant changes, such as contact
events, by effectively using the robot's I/O history. Additionally, we identify
task randomization as another key source of robustness, fostering better task
generalization and compliance to disturbances. The resulting control policies
can be successfully deployed on Cassie, a torque-controlled human-sized bipedal
robot. This work pushes the limits of agility for bipedal robots through
extensive real-world experiments. We demonstrate a diverse range of locomotion
skills, including: robust standing, versatile walking, fast running with a
demonstration of a 400-meter dash, and a diverse set of jumping skills, such as
standing long jumps and high jumps.
\\ ( https://arxiv.org/abs/2401.16889 ,  9930kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16982 (*cross-listing*)
Date: Tue, 30 Jan 2024 13:10:33 GMT   (8690kb,D)

Title: ActDroid: An active learning framework for Android malware detection
Authors: Ali Muzaffar, Hani Ragab Hassen, Hind Zantout, Michael A Lones
Categories: cs.CR cs.AI
\\
  The growing popularity of Android requires malware detection systems that can
keep up with the pace of new software being released. According to a recent
study, a new piece of malware appears online every 12 seconds. To address this,
we treat Android malware detection as a streaming data problem and explore the
use of active online learning as a means of mitigating the problem of labelling
applications in a timely and cost-effective manner. Our resulting framework
achieves accuracies of up to 96\%, requires as little of 24\% of the training
data to be labelled, and compensates for concept drift that occurs between the
release and labelling of an application. We also consider the broader
practicalities of online learning within Android malware detection, and
systematically explore the trade-offs between using different static, dynamic
and hybrid feature sets to classify malware.
\\ ( https://arxiv.org/abs/2401.16982 ,  8690kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17010 (*cross-listing*)
Date: Tue, 30 Jan 2024 13:46:49 GMT   (522kb)

Title: Finetuning Large Language Models for Vulnerability Detection
Authors: Alexey Shestov, Anton Cheshkov, Rodion Levichev, Ravil Mussabayev,
  Pavel Zadorozhny, Evgeny Maslov, Chibirev Vadim, Egor Bulychev
Categories: cs.CR cs.AI cs.LG
\\
  This paper presents the results of finetuning large language models (LLMs)
for the task of detecting vulnerabilities in source code. We leverage
WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and
adapt it for vulnerability detection through further finetuning. To accelerate
training, we modify WizardCoder's training procedure, also we investigate
optimal training regimes. For the imbalanced dataset with many more negative
examples than positive, we also explore different techniques to improve
classification performance. The finetuned WizardCoder model achieves
improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability
datasets over CodeBERT-like model, demonstrating the effectiveness of adapting
pretrained LLMs for vulnerability detection in source code. The key
contributions are finetuning the state-of-the-art code LLM, WizardCoder,
increasing its training speed without the performance harm, optimizing the
training procedure and regimes, handling class imbalance, and improving
performance on difficult vulnerability detection datasets. This demonstrates
the potential for transfer learning by finetuning large pretrained language
models for specialized source code analysis tasks.
\\ ( https://arxiv.org/abs/2401.17010 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17050 (*cross-listing*)
Date: Tue, 30 Jan 2024 14:32:25 GMT   (2906kb,D)

Title: ViTree: Single-path Neural Tree for Step-wise Interpretable Fine-grained
  Visual Categorization
Authors: Danning Lao, Qi Liu, Jiazi Bu, Junchi Yan, Wei Shen
Categories: cs.CV cs.AI
\\
  As computer vision continues to advance and finds widespread applications
across various domains, the need for interpretability in deep learning models
becomes paramount. Existing methods often resort to post-hoc techniques or
prototypes to explain the decision-making process, which can be indirect and
lack intrinsic illustration. In this research, we introduce ViTree, a novel
approach for fine-grained visual categorization that combines the popular
vision transformer as a feature extraction backbone with neural decision trees.
By traversing the tree paths, ViTree effectively selects patches from
transformer-processed features to highlight informative local regions, thereby
refining representations in a step-wise manner. Unlike previous tree-based
models that rely on soft distributions or ensembles of paths, ViTree selects a
single tree path, offering a clearer and simpler decision-making process. This
patch and path selectivity enhances model interpretability of ViTree, enabling
better insights into the model's inner workings. Remarkably, extensive
experimentation validates that this streamlined approach surpasses various
strong competitors and achieves state-of-the-art performance while maintaining
exceptional interpretability which is proved by multi-perspective methods. Code
can be found at https://github.com/SJTU-DeepVisionLab/ViTree.
\\ ( https://arxiv.org/abs/2401.17050 ,  2906kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17053 (*cross-listing*)
Date: Tue, 30 Jan 2024 14:34:19 GMT   (7793kb,D)

Title: BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane
  Extrapolation
Authors: Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang,
  Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, and Pan Ji
Categories: cs.CV cs.AI cs.GR
Comments: Video: https://www.youtube.com/watch?v=PxIBtd6G0mA
\\
  We present BlockFusion, a diffusion-based model that generates 3D scenes as
unit blocks and seamlessly incorporates new blocks to extend the scene.
BlockFusion is trained using datasets of 3D blocks that are randomly cropped
from complete 3D scene meshes. Through per-block fitting, all training blocks
are converted into the hybrid neural fields: with a tri-plane containing the
geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the
signed distance values. A variational auto-encoder is employed to compress the
tri-planes into the latent tri-plane space, on which the denoising diffusion
process is performed. Diffusion applied to the latent representations allows
for high-quality and diverse 3D scene generation. To expand a scene during
generation, one needs only to append empty blocks to overlap with the current
scene and extrapolate existing latent tri-planes to populate new blocks. The
extrapolation is done by conditioning the generation process with the feature
samples from the overlapping tri-planes during the denoising iterations. Latent
tri-plane extrapolation produces semantically and geometrically meaningful
transitions that harmoniously blend with the existing scene. A 2D layout
conditioning mechanism is used to control the placement and arrangement of
scene elements. Experimental results indicate that BlockFusion is capable of
generating diverse, geometrically consistent and unbounded large 3D scenes with
unprecedented high-quality shapes in both indoor and outdoor scenarios.
\\ ( https://arxiv.org/abs/2401.17053 ,  7793kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17129 (*cross-listing*)
Date: Mon, 29 Jan 2024 06:05:23 GMT   (4108kb,D)

Title: Enhanced Sound Event Localization and Detection in Real 360-degree
  audio-visual soundscapes
Authors: Adrian S. Roman, Baladithya Balamurugan, Rithik Pothuganti
Categories: cs.SD cs.AI eess.AS
\\
  This technical report details our work towards building an enhanced
audio-visual sound event localization and detection (SELD) network. We build on
top of the audio-only SELDnet23 model and adapt it to be audio-visual by
merging both audio and video information prior to the gated recurrent unit
(GRU) of the audio-only network. Our model leverages YOLO and DETIC object
detectors. We also build a framework that implements audio-visual data
augmentation and audio-visual synthetic data generation. We deliver an
audio-visual SELDnet system that outperforms the existing audio-visual SELD
baseline.
\\ ( https://arxiv.org/abs/2401.17129 ,  4108kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17133 (*cross-listing*)
Date: Tue, 30 Jan 2024 16:07:44 GMT   (340kb,D)

Title: A Proactive and Dual Prevention Mechanism against Illegal Song Covers
  empowered by Singing Voice Conversion
Authors: Guangke Chen, Yedi Zhang, Fu Song, Ting Wang, Xiaoning Du, Yang Liu
Categories: cs.SD cs.AI cs.CR cs.LG cs.MM eess.AS
\\
  Singing voice conversion (SVC) automates song covers by converting one
singer's singing voice into another target singer's singing voice with the
original lyrics and melody. However, it raises serious concerns about copyright
and civil right infringements to multiple entities. This work proposes
SongBsAb, the first proactive approach to mitigate unauthorized SVC-based
illegal song covers. SongBsAb introduces human-imperceptible perturbations to
singing voices before releasing them, so that when they are used, the
generation process of SVC will be interfered, resulting in unexpected singing
voices. SongBsAb features a dual prevention effect by causing both (singer)
identity disruption and lyric disruption, namely, the SVC-covered singing voice
neither imitates the target singer nor preserves the original lyrics. To
improve the imperceptibility of perturbations, we refine a psychoacoustic
model-based loss with the backing track as an additional masker, a unique
accompanying element for singing voices compared to ordinary speech voices. To
enhance the transferability, we propose to utilize a frame-level interaction
reduction-based loss. We demonstrate the prevention effectiveness, utility, and
robustness of SongBsAb on three SVC models and two datasets using both
objective and human study-based subjective metrics. Our work fosters an
emerging research direction for mitigating illegal automated song covers.
\\ ( https://arxiv.org/abs/2401.17133 ,  340kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17186 (*cross-listing*)
Date: Tue, 30 Jan 2024 17:14:05 GMT   (1731kb,D)

Title: Embracing Language Inclusivity and Diversity in CLIP through Continual
  Language Learning
Authors: Bang Yang, Yong Dai, Xuxin Cheng, Yaowei Li, Asif Raza, Yuexian Zou
Categories: cs.CV cs.AI cs.IR
Comments: Accepted by AAAI'2024, 15 pages (with appendix), 7 figures, 10 tables
\\
  While vision-language pre-trained models (VL-PTMs) have advanced multimodal
research in recent years, their mastery in a few languages like English
restricts their applicability in broader communities. To this end, there is an
increasing interest in developing multilingual VL models via a joint-learning
setup, which, however, could be unrealistic due to expensive costs and data
availability. In this work, we propose to extend VL-PTMs' language capacity by
continual language learning (CLL), where a model needs to update its linguistic
knowledge incrementally without suffering from catastrophic forgetting (CF). We
begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP,
a prevailing VL-PTM that has acquired image-English text alignment.
Specifically, CLL-CLIP contains an expandable token embedding layer to handle
linguistic differences. It solely trains token embeddings to improve memory
stability and is optimized under cross-modal and cross-lingual objectives to
learn the alignment between images and multilingual texts. To alleviate CF
raised by covariate shift and lexical overlap, we further propose a novel
approach that ensures the identical distribution of all token embeddings during
initialization and regularizes token embedding learning during training. We
construct a CLL benchmark covering 36 languages based on MSCOCO and XM3600
datasets and then evaluate multilingual image-text retrieval performance.
Extensive experiments verify the effectiveness of CLL-CLIP and show that our
approach can boost CLL-CLIP, e.g., by 6.7% in text-to-image average Recall@1 on
XM3600, and improve various state-of-the-art methods consistently. Our code and
data are available at \url{https://github.com/yangbang18/CLFM}.
\\ ( https://arxiv.org/abs/2401.17186 ,  1731kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17188 (*cross-listing*)
Date: Tue, 30 Jan 2024 17:17:43 GMT   (1843kb,D)

Title: Nested Construction of Polar Codes via Transformers
Authors: Sravan Kumar Ankireddy, S Ashwin Hebbar, Heping Wan, Joonyoung Cho,
  Charlie Zhang
Categories: cs.IT cs.AI math.IT
Comments: 7 pages; 8 figures
\\
  Tailoring polar code construction for decoding algorithms beyond successive
cancellation has remained a topic of significant interest in the field.
However, despite the inherent nested structure of polar codes, the use of
sequence models in polar code construction is understudied. In this work, we
propose using a sequence modeling framework to iteratively construct a polar
code for any given length and rate under various channel conditions.
Simulations show that polar codes designed via sequential modeling using
transformers outperform both 5G-NR sequence and Density Evolution based
approaches for both AWGN and Rayleigh fading channels.
\\ ( https://arxiv.org/abs/2401.17188 ,  1843kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17221 (*cross-listing*)
Date: Tue, 30 Jan 2024 18:09:11 GMT   (30867kb,D)

Title: MouSi: Poly-Visual-Expert Vision-Language Models
Authors: Xiaoran Fan, Tao Ji, Changhao Jiang, Shuo Li, Senjie Jin, Sirui Song,
  Junke Wang, Boyang Hong, Lu Chen, Guodong Zheng, Ming Zhang, Caishuang Huang,
  Rui Zheng, Zhiheng Xi, Yuhao Zhou, Shihan Dou, Junjie Ye, Hang Yan, Tao Gui,
  Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Current large vision-language models (VLMs) often encounter challenges such
as insufficient capabilities of a single visual component and excessively long
visual tokens. These issues can limit the model's effectiveness in accurately
interpreting complex visual information and over-lengthy contextual
information. Addressing these challenges is crucial for enhancing the
performance and applicability of VLMs. This paper proposes the use of ensemble
experts technique to synergizes the capabilities of individual visual encoders,
including those skilled in image-text matching, OCR, image segmentation, etc.
This technique introduces a fusion network to unify the processing of outputs
from different visual experts, while bridging the gap between image encoders
and pre-trained LLMs. In addition, we explore different positional encoding
schemes to alleviate the waste of positional encoding caused by lengthy image
feature sequences, effectively addressing the issue of position overflow and
length limitations. For instance, in our implementation, this technique
significantly reduces the positional occupancy in models like SAM, from a
substantial 4096 to a more efficient and manageable 64 or even down to 1.
Experimental results demonstrate that VLMs with multiple experts exhibit
consistently superior performance over isolated visual encoders and mark a
significant performance boost as more experts are integrated. We have
open-sourced the training code used in this report. All of these resources can
be found on our project website.
\\ ( https://arxiv.org/abs/2401.17221 ,  30867kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17230 (*cross-listing*)
Date: Tue, 30 Jan 2024 18:18:27 GMT   (419kb,D)

Title: ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible
  recipes, self-supervised front-ends, and off-the-shelf models
Authors: Jee-weon Jung, Wangyou Zhang, Jiatong Shi, Zakaria Aldeneh, Takuya
  Higuchi, Barry-John Theobald, Ahmed Hussen Abdelaziz, Shinji Watanabe
Categories: cs.SD cs.AI eess.AS
Comments: 5 pages, 3 figures, 7 tables
\\
  This paper introduces ESPnet-SPK, a toolkit designed with several objectives
for training speaker embedding extractors. First, we provide an open-source
platform for researchers in the speaker recognition community to effortlessly
build models. We provide several models, ranging from x-vector to recent
SKA-TDNN. Through the modularized architecture design, variants can be
developed easily. We also aspire to bridge developed models with other domains,
facilitating the broad research community to effortlessly incorporate
state-of-the-art embedding extractors. Pre-trained embedding extractors can be
accessed in an off-the-shelf manner and we demonstrate the toolkit's
versatility by showcasing its integration with two tasks. Another goal is to
integrate with diverse self-supervised learning features. We release a
reproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O
evaluation protocol using WavLM-Large with ECAPA-TDNN.
\\ ( https://arxiv.org/abs/2401.17230 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17264 (*cross-listing*)
Date: Tue, 30 Jan 2024 18:56:22 GMT   (436kb,D)

Title: Proactive Detection of Voice Cloning with Localized Watermarking
Authors: Robin San Roman, Pierre Fernandez, Alexandre D\'efossez, Teddy Furon,
  Tuan Tran, Hady Elsahar
Categories: cs.SD cs.AI cs.CR
Comments: Code at https://github.com/facebookresearch/audioseal
\\
  In the rapidly evolving field of speech generative models, there is a
pressing need to ensure audio authenticity against the risks of voice cloning.
We present AudioSeal, the first audio watermarking technique designed
specifically for localized detection of AI-generated speech. AudioSeal employs
a generator/detector architecture trained jointly with a localization loss to
enable localized watermark detection up to the sample level, and a novel
perceptual loss inspired by auditory masking, that enables AudioSeal to achieve
better imperceptibility. AudioSeal achieves state-of-the-art performance in
terms of robustness to real life audio manipulations and imperceptibility based
on automatic and human evaluation metrics. Additionally, AudioSeal is designed
with a fast, single-pass detector, that significantly surpasses existing models
in speed - achieving detection up to two orders of magnitude faster, making it
ideal for large-scale and real-time applications.
\\ ( https://arxiv.org/abs/2401.17264 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16429 (*cross-listing*)
Date: Fri, 19 Jan 2024 14:30:35 GMT   (4246kb)

Title: Combining topic modelling and citation network analysis to study case
  law from the European Court on Human Rights on the right to respect for
  private and family life
Authors: M. Mohammadi, L. M. Bruijn, M. Wieling, M. Vols
Categories: cs.IR cs.CL cs.DL cs.LG
\\
  As legal case law databases such as HUDOC continue to grow rapidly, it has
become essential for legal researchers to find efficient methods to handle such
large-scale data sets. Such case law databases usually consist of the textual
content of cases together with the citations between them. This paper focuses
on case law from the European Court of Human Rights on Article 8 of the
European Convention of Human Rights, the right to respect private and family
life, home and correspondence. In this study, we demonstrate and compare the
potential of topic modelling and citation network to find and organize case law
on Article 8 based on their general themes and citation patterns, respectively.
Additionally, we explore whether combining these two techniques leads to better
results compared to the application of only one of the methods. We evaluate the
effectiveness of the combined method on a unique manually collected and
annotated dataset of Aricle 8 case law on evictions. The results of our
experiments show that our combined (text and citation-based) approach provides
the best results in finding and grouping case law, providing scholars with an
effective way to extract and analyse relevant cases on a specific issue.
\\ ( https://arxiv.org/abs/2401.16429 ,  4246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16430 (*cross-listing*)
Date: Sat, 20 Jan 2024 01:34:50 GMT   (2339kb,D)

Title: An Information Retrieval and Extraction Tool for Covid-19 Related Papers
Authors: Marcos V. L. Pivetta
Categories: cs.IR cs.CL
\\
  Background: The COVID-19 pandemic has caused severe impacts on health systems
worldwide. Its critical nature and the increased interest of individuals and
organizations to develop countermeasures to the problem has led to a surge of
new studies in scientific journals. Objetive: We sought to develop a tool that
incorporates, in a novel way, aspects of Information Retrieval (IR) and
Extraction (IE) applied to the COVID-19 Open Research Dataset (CORD-19). The
main focus of this paper is to provide researchers with a better search tool
for COVID-19 related papers, helping them find reference papers and hightlight
relevant entities in text. Method: We applied Latent Dirichlet Allocation (LDA)
to model, based on research aspects, the topics of all English abstracts in
CORD-19. Relevant named entities of each abstract were extracted and linked to
the corresponding UMLS concept. Regular expressions and the K-Nearest Neighbors
algorithm were used to rank relevant papers. Results: Our tool has shown the
potential to assist researchers by automating a topic-based search of CORD-19
papers. Nonetheless, we identified that more fine-tuned topic modeling
parameters and increased accuracy of the research aspect classifier model could
lead to a more accurate and reliable tool. Conclusion: We emphasize the need of
new automated tools to help researchers find relevant COVID-19 documents, in
addition to automatically extracting useful information contained in them. Our
work suggests that combining different algorithms and models could lead to new
ways of browsing COVID-19 paper data.
\\ ( https://arxiv.org/abs/2401.16430 ,  2339kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16558 (*cross-listing*)
Date: Mon, 29 Jan 2024 20:50:28 GMT   (599kb,D)

Title: Diverse, but Divisive: LLMs Can Exaggerate Gender Differences in Opinion
  Related to Harms of Misinformation
Authors: Terrence Neumann, Sooyong Lee, Maria De-Arteaga, Sina Fazelpour,
  Matthew Lease
Categories: cs.CY cs.CL
Comments: Under Review
\\
  The pervasive spread of misinformation and disinformation poses a significant
threat to society. Professional fact-checkers play a key role in addressing
this threat, but the vast scale of the problem forces them to prioritize their
limited resources. This prioritization may consider a range of factors, such as
varying risks of harm posed to specific groups of people. In this work, we
investigate potential implications of using a large language model (LLM) to
facilitate such prioritization. Because fact-checking impacts a wide range of
diverse segments of society, it is important that diverse views are represented
in the claim prioritization process. This paper examines whether a LLM can
reflect the views of various groups when assessing the harms of misinformation,
focusing on gender as a primary variable. We pose two central questions: (1) To
what extent do prompts with explicit gender references reflect gender
differences in opinion in the United States on topics of social relevance? and
(2) To what extent do gender-neutral prompts align with gendered viewpoints on
those topics? To analyze these questions, we present the TopicMisinfo dataset,
containing 160 fact-checked claims from diverse topics, supplemented by nearly
1600 human annotations with subjective perceptions and annotator demographics.
Analyzing responses to gender-specific and neutral prompts, we find that GPT
3.5-Turbo reflects empirically observed gender differences in opinion but
amplifies the extent of these differences. These findings illuminate AI's
complex role in moderating online communication, with implications for
fact-checkers, algorithm designers, and the use of crowd-workers as annotators.
We also release the TopicMisinfo dataset to support continuing research in the
community.
\\ ( https://arxiv.org/abs/2401.16558 ,  599kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16659 (*cross-listing*)
Date: Tue, 30 Jan 2024 01:24:18 GMT   (448kb,D)

Title: History-Aware Conversational Dense Retrieval
Authors: Fengran Mo, Chen Qu, Kelong Mao, Tianyu Zhu, Zhan Su, Kaiyu Huang,
  Jian-Yun Nie
Categories: cs.IR cs.CL
\\
  Conversational search facilitates complex information retrieval by enabling
multi-turn interactions between users and the system. Supporting such
interactions requires a comprehensive understanding of the conversational
inputs to formulate a good search query based on historical information. In
particular, the search query should include the relevant information from the
previous conversation turns. However, current approaches for conversational
dense retrieval primarily rely on fine-tuning a pre-trained ad-hoc retriever
using the whole conversational search session, which can be lengthy and noisy.
Moreover, existing approaches are limited by the amount of manual supervision
signals in the existing datasets. To address the aforementioned issues, we
propose a History-Aware Conversational Dense Retrieval (HAConvDR) system, which
incorporates two ideas: context-denoised query reformulation and automatic
mining of supervision signals based on the actual impact of historical turns.
Experiments on two public conversational search datasets demonstrate the
improved history modeling capability of HAConvDR, in particular for long
conversations with topic shifts.
\\ ( https://arxiv.org/abs/2401.16659 ,  448kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16713 (*cross-listing*)
Date: Tue, 30 Jan 2024 03:17:45 GMT   (142kb,D)

Title: Prospects for inconsistency detection using large language models and
  sheaves
Authors: Steve Huntsman, Michael Robinson, Ludmilla Huntsman
Categories: cs.CY cs.CL math.AT
\\
  We demonstrate that large language models can produce reasonable numerical
ratings of the logical consistency of claims. We also outline a mathematical
approach based on sheaf theory for lifting such ratings to hypertexts such as
laws, jurisprudence, and social media and evaluating their consistency
globally. This approach is a promising avenue to increasing consistency in and
of government, as well as to combating mis- and disinformation and related
ills.
\\ ( https://arxiv.org/abs/2401.16713 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17093 (*cross-listing*)
Date: Tue, 30 Jan 2024 15:20:26 GMT   (2782kb,D)

Title: StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis
Authors: Zecheng Tang, Chenfei Wu, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu
  Liu, Zhengyuan Yang, Lijuan Wang, Zicheng Liu, Juntao Li, Nan Duan
Categories: cs.CV cs.CL
\\
  To leverage LLMs for visual synthesis, traditional methods convert raster
image information into discrete grid tokens through specialized visual modules,
while disrupting the model's ability to capture the true semantic
representation of visual scenes. This paper posits that an alternative
representation of images, vector graphics, can effectively surmount this
limitation by enabling a more natural and semantically coherent segmentation of
the image information. Thus, we introduce StrokeNUWA, a pioneering work
exploring a better visual representation ''stroke tokens'' on vector graphics,
which is inherently visual semantics rich, naturally compatible with LLMs, and
highly compressed. Equipped with stroke tokens, StrokeNUWA can significantly
surpass traditional LLM-based and optimization-based methods across various
metrics in the vector graphic generation task. Besides, StrokeNUWA achieves up
to a 94x speedup in inference over the speed of prior methods with an
exceptional SVG code compression ratio of 6.9%.
\\ ( https://arxiv.org/abs/2401.17093 ,  2782kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16432 (*cross-listing*)
Date: Thu, 25 Jan 2024 08:44:22 GMT   (4461kb,D)

Title: Improving conversion rate prediction via self-supervised pre-training in
  online advertising
Authors: Alex Shtoff, Yohay Kaplan, Ariel Raviv
Categories: cs.IR cs.LG stat.ML
DOI: 10.1109/BigData59044.2023.10386162
\\
  The task of predicting conversion rates (CVR) lies at the heart of online
advertising systems aiming to optimize bids to meet advertiser performance
requirements. Even with the recent rise of deep neural networks, these
predictions are often made by factorization machines (FM), especially in
commercial settings where inference latency is key. These models are trained
using the logistic regression framework on labeled tabular data formed from
past user activity that is relevant to the task at hand.
  Many advertisers only care about click-attributed conversions. A major
challenge in training models that predict conversions-given-clicks comes from
data sparsity - clicks are rare, conversions attributed to clicks are even
rarer. However, mitigating sparsity by adding conversions that are not
click-attributed to the training set impairs model calibration. Since
calibration is critical to achieving advertiser goals, this is infeasible.
  In this work we use the well-known idea of self-supervised pre-training, and
use an auxiliary auto-encoder model trained on all conversion events, both
click-attributed and not, as a feature extractor to enrich the main CVR
prediction model. Since the main model does not train on non click-attributed
conversions, this does not impair calibration. We adapt the basic
self-supervised pre-training idea to our online advertising setup by using a
loss function designed for tabular data, facilitating continual learning by
ensuring auto-encoder stability, and incorporating a neural network into a
large-scale real-time ad auction that ranks tens of thousands of ads, under
strict latency constraints, and without incurring a major engineering cost. We
show improvements both offline, during training, and in an online A/B test.
Following its success in A/B tests, our solution is now fully deployed to the
Yahoo native advertising system.
\\ ( https://arxiv.org/abs/2401.16432 ,  4461kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16433 (*cross-listing*)
Date: Thu, 25 Jan 2024 19:40:55 GMT   (18312kb,D)

Title: Within-basket Recommendation via Neural Pattern Associator
Authors: Kai Luo, Tianshu Shen, Lan Yao, Ga Wu, Aaron Liblong, Istvan
  Fehervari, Ruijian An, Jawad Ahmed, Harshit Mishra, Charu Pujari
Categories: cs.IR cs.LG
Comments: 13 pages, 9 figures
\\
  Within-basket recommendation (WBR) refers to the task of recommending items
to the end of completing a non-empty shopping basket during a shopping session.
While the latest innovations in this space demonstrate remarkable performance
improvement on benchmark datasets, they often overlook the complexity of user
behaviors in practice, such as 1) co-existence of multiple shopping intentions,
2) multi-granularity of such intentions, and 3) interleaving behavior
(switching intentions) in a shopping session. This paper presents Neural
Pattern Associator (NPA), a deep item-association-mining model that explicitly
models the aforementioned factors. Specifically, inspired by vector
quantization, the NPA model learns to encode common user intentions (or
item-combination patterns) as quantized representations (a.k.a. codebook),
which permits identification of users's shopping intentions via
attention-driven lookup during the reasoning phase. This yields coherent and
self-interpretable recommendations. We evaluated the proposed NPA model across
multiple extensive datasets, encompassing the domains of grocery e-commerce
(shopping basket completion) and music (playlist extension), where our
quantitative evaluations show that the NPA model significantly outperforms a
wide range of existing WBR solutions, reflecting the benefit of explicitly
modeling complex user intentions.
\\ ( https://arxiv.org/abs/2401.16433 ,  18312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16434 (*cross-listing*)
Date: Fri, 26 Jan 2024 09:12:39 GMT   (1574kb)

Title: A novel ANROA based control approach for grid-tied multi-functional
  solar energy conversion system
Authors: Dinanath Prasad, Narendra Kumar, Rakhi Sharma, Hasmat Malik, Fausto
  Pedro Garc\'ia M\'arquez, Jes\'us Mar\'ia Pinar P\'erez
Categories: eess.SY cs.LG cs.SY eess.SP
Comments: The paper was published in Energy Reports journal (ELSEVIER). Cite
  as: Prasad, D., Kumar, N., Sharma, R., Malik, H., M\'arquez, F. P. G., &
  Pinar-P\'erez, J. M. (2023). A novel ANROA based control approach for
  grid-tied multi-functional solar energy conversion system. Energy Reports, 9,
  2044-2057
Journal-ref: Energy Reports (2023) Elsevier
DOI: 10.1016/j.egyr.2023.01.039
\\
  An adaptive control approach for a three-phase grid-interfaced solar
photovoltaic system based on the new Neuro-Fuzzy Inference System with Rain
Optimization Algorithm (ANROA) methodology is proposed and discussed in this
manuscript. This method incorporates an Adaptive Neuro-fuzzy Inference System
(ANFIS) with a Rain Optimization Algorithm (ROA). The ANFIS controller has
excellent maximum tracking capability because it includes features of both
neural and fuzzy techniques. The ROA technique is in charge of controlling the
voltage source converter switching. Avoiding power quality problems including
voltage fluctuations, harmonics, and flickers as well as unbalanced loads and
reactive power usage is the major goal. Besides, the proposed method performs
at zero voltage regulation and unity power factor modes. The suggested control
approach has been modeled and simulated, and its performance has been assessed
using existing alternative methods. A statistical analysis of proposed and
existing techniques has been also presented and discussed. The results of the
simulations demonstrate that, when compared to alternative approaches, the
suggested strategy may properly and effectively identify the best global
solutions. Furthermore, the system's robustness has been studied by using
MATLAB/SIMULINK environment and experimentally by Field Programmable Gate
Arrays Controller (FPGA)-based Hardware-in-Loop (HLL).
\\ ( https://arxiv.org/abs/2401.16434 ,  1574kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16437 (*cross-listing*)
Date: Fri, 26 Jan 2024 21:47:39 GMT   (16104kb,D)

Title: A Benchmark Dataset for Tornado Detection and Prediction using
  Full-Resolution Polarimetric Weather Radar Data
Authors: Mark S. Veillette, James M. Kurdzo, Phillip M. Stepanian, John Y. N.
  Cho, Siddharth Samsi and Joseph McDonald
Categories: physics.ao-ph cs.LG
Comments: 37 pages, 15 Figures, 2 Tables
\\
  Weather radar is the primary tool used by forecasters to detect and warn for
tornadoes in near-real time. In order to assist forecasters in warning the
public, several algorithms have been developed to automatically detect tornadic
signatures in weather radar observations. Recently, Machine Learning (ML)
algorithms, which learn directly from large amounts of labeled data, have been
shown to be highly effective for this purpose. Since tornadoes are extremely
rare events within the corpus of all available radar observations, the
selection and design of training datasets for ML applications is critical for
the performance, robustness, and ultimate acceptance of ML algorithms. This
study introduces a new benchmark dataset, TorNet to support development of ML
algorithms in tornado detection and prediction. TorNet contains
full-resolution, polarimetric, Level-II WSR-88D data sampled from 10 years of
reported storm events. A number of ML baselines for tornado detection are
developed and compared, including a novel deep learning (DL) architecture
capable of processing raw radar imagery without the need for manual feature
extraction required for existing ML algorithms. Despite not benefiting from
manual feature engineering or other preprocessing, the DL model shows increased
detection performance compared to non-DL and operational baselines. The TorNet
dataset, as well as source code and model weights of the DL baseline trained in
this work, are made freely available.
\\ ( https://arxiv.org/abs/2401.16437 ,  16104kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16445 (*cross-listing*)
Date: Sun, 28 Jan 2024 06:06:59 GMT   (1127kb,D)

Title: OMPGPT: A Generative Pre-trained Transformer Model for OpenMP
Authors: Le Chen, Arijit Bhattacharjee, Nesreen Ahmed, Niranjan Hasabnis, Gal
  Oren, Vy Vo, Ali Jannesari
Categories: cs.SE cs.DC cs.LG
\\
  Large language models (LLMs), as epitomized by models like ChatGPT, have
revolutionized the field of natural language processing (NLP). Along with this
trend, code-based large language models such as StarCoder, WizardCoder, and
CodeLlama have emerged, trained extensively on vast repositories of code data.
Yet, inherent in their design, these models primarily focus on generative tasks
like code generation, code completion, and comment generation, and general
support for multiple programming languages. While the generic abilities of code
LLMs are useful for many programmers, the area of high-performance computing
(HPC) has a narrower set of requirements that make a smaller and more
domain-specific LM a smarter choice. This paper introduces OMPGPT, a novel
model meticulously designed to harness the inherent strengths of language
models for OpenMP pragma generation. Furthermore, we adopt and adapt prompt
engineering techniques from the NLP domain to create chain-of-OMP, an
innovative strategy designed to enhance OMPGPT's effectiveness. Our extensive
evaluations demonstrate that OMPGPT outperforms existing large language models
specialized in OpenMP tasks and maintains a notably smaller size, aligning it
more closely with the typical hardware constraints of HPC environments. We
consider our contribution as a pivotal bridge, connecting the advantage of
language models with the specific demands of HPC tasks. The success of OMPGPT
lays a solid foundation, suggesting its potential applicability and
adaptability to a wider range of HPC tasks, thereby opening new avenues in the
field of computational efficiency and effectiveness.
\\ ( https://arxiv.org/abs/2401.16445 ,  1127kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16464 (*cross-listing*)
Date: Mon, 29 Jan 2024 16:10:05 GMT   (11469kb,D)

Title: Towards Regret Free Slot Allocation in Billboard Advertisement
Authors: Dildar Ali, Suman Banerjee, Yamuna Prasad
Categories: cs.IR cs.DB cs.LG cs.MA
Comments: 37 Pages
\\
  Creating and maximizing influence among the customers is one of the central
goals of an advertiser, and hence, remains an active area of research in recent
times. In this advertisement technique, the advertisers approach an influence
provider for a specific number of views of their content on a payment basis.
Now, if the influence provider can provide the required number of views or
more, he will receive the full, else a partial payment. In the context of an
influence provider, it is a loss for him if he offers more or less views. This
is formalized as 'Regret', and naturally, in the context of the influence
provider, the goal will be to minimize this quantity. In this paper, we solve
this problem in the context of billboard advertisement and pose it as a
discrete optimization problem. We propose four efficient solution approaches
for this problem and analyze them to understand their time and space
complexity. We implement all the solution methodologies with real-life datasets
and compare the obtained results with the existing solution approaches from the
literature. We observe that the proposed solutions lead to less regret while
taking less computational time.
\\ ( https://arxiv.org/abs/2401.16464 ,  11469kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16468 (*cross-listing*)
Date: Mon, 29 Jan 2024 18:53:33 GMT   (39012kb,D)

Title: High-Quality Image Restoration Following Human Instructions
Authors: Marcos V. Conde, Gregor Geigle, Radu Timofte
Categories: cs.CV cs.LG eess.IV
\\
  Image restoration is a fundamental problem that involves recovering a
high-quality clean image from its degraded observation. All-In-One image
restoration models can effectively restore images from various types and levels
of degradation using degradation-specific information as prompts to guide the
restoration model. In this work, we present the first approach that uses
human-written instructions to guide the image restoration model. Given natural
language prompts, our model can recover high-quality images from their degraded
counterparts, considering multiple degradation types. Our method, InstructIR,
achieves state-of-the-art results on several restoration tasks including image
denoising, deraining, deblurring, dehazing, and (low-light) image enhancement.
InstructIR improves +1dB over previous all-in-one restoration methods.
Moreover, our dataset and results represent a novel benchmark for new research
on text-guided image restoration and enhancement. Our code, datasets and models
are available at: https://github.com/mv-lab/InstructIR
\\ ( https://arxiv.org/abs/2401.16468 ,  39012kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16492 (*cross-listing*)
Date: Mon, 29 Jan 2024 19:06:08 GMT   (3365kb,D)

Title: GPU Cluster Scheduling for Network-Sensitive Deep Learning
Authors: Aakash Sharma, Vivek M. Bhasi, Sonali Singh, George Kesidis, Mahmut T.
  Kandemir, Chita R. Das
Categories: cs.PF cs.DC cs.LG
\\
  We propose a novel GPU-cluster scheduler for distributed DL (DDL) workloads
that enables proximity based consolidation of GPU resources based on the DDL
jobs' sensitivities to the anticipated communication-network delays. Our
scheduler consists of three major components: (i) a classical delay scheduling
algorithm to facilitate job placement and consolidation; (ii) a
network-sensitive job preemption strategy; and (iii) an "auto-tuner" mechanism
to optimize delay timers for effective delay scheduling. Additionally, to
enable a cost-effective methodology for large-scale experiments, we develop a
data-driven DDL cluster simulation platform. Employing the simulation platform
we compare against several state-of-the-art alternatives on real-world workload
traces to demonstrate the benefits of our design. Our scheduler can provide
improvement of up to 69% in end-to-end Makespan for training all jobs compared
to the prevailing consolidation-based scheduling methods, while reducing the
average job completion time by up to 83% and minimizing the communication
overheads by up to 98% under congested networking conditions.
\\ ( https://arxiv.org/abs/2401.16492 ,  3365kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16611 (*cross-listing*)
Date: Mon, 29 Jan 2024 22:44:28 GMT   (1498kb,D)

Title: Accelerating superconductor discovery through tempered deep learning of
  the electron-phonon spectral function
Authors: Jason B. Gibson, Ajinkya C. Hire, Philip M. Dee, Oscar Barrera,
  Benjamin Geisler, Peter J. Hirschfeld, Richard G. Hennig
Categories: cond-mat.supr-con cond-mat.mtrl-sci cs.LG
Comments: 12 pages, 5 figures, 1 table
\\
  Integrating deep learning with the search for new electron-phonon
superconductors represents a burgeoning field of research, where the primary
challenge lies in the computational intensity of calculating the
electron-phonon spectral function, $\alpha^2F(\omega)$, the essential
ingredient of Midgal-Eliashberg theory of superconductivity. To overcome this
challenge, we adopt a two-step approach. First, we compute $\alpha^2F(\omega)$
for 818 dynamically stable materials. We then train a deep-learning model to
predict $\alpha^2F(\omega)$, using an unconventional training strategy to
temper the model's overfitting, enhancing predictions. Specifically, we train a
Bootstrapped Ensemble of Tempered Equivariant graph neural NETworks (BETE-NET),
obtaining an MAE of 0.21, 45 K, and 43 K for the Eliashberg moments derived
from $\alpha^2F(\omega)$: $\lambda$, $\omega_{\log}$, and $\omega_{2}$,
respectively, yielding an MAE of 2.5 K for the critical temperature, $T_c$.
Further, we incorporate domain knowledge of the site-projected phonon density
of states to impose inductive bias into the model's node attributes and enhance
predictions. This methodological innovation decreases the MAE to 0.18, 29 K,
and 28 K, respectively, yielding an MAE of 2.1 K for $T_c$. We illustrate the
practical application of our model in high-throughput screening for high-$T_c$
materials. The model demonstrates an average precision nearly five times higher
than random screening, highlighting the potential of ML in accelerating
superconductor discovery. BETE-NET accelerates the search for high-$T_c$
superconductors while setting a precedent for applying ML in materials
discovery, particularly when data is limited.
\\ ( https://arxiv.org/abs/2401.16611 ,  1498kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16612 (*cross-listing*)
Date: Mon, 29 Jan 2024 22:52:57 GMT   (503kb,D)

Title: Learning a Gaussian Mixture for Sparsity Regularization in Inverse
  Problems
Authors: Giovanni S. Alberti, Luca Ratti, Matteo Santacesaria, Silvia Sciutto
Categories: stat.ML cs.LG
\\
  In inverse problems, it is widely recognized that the incorporation of a
sparsity prior yields a regularization effect on the solution. This approach is
grounded on the a priori assumption that the unknown can be appropriately
represented in a basis with a limited number of significant components, while
most coefficients are close to zero. This occurrence is frequently observed in
real-world scenarios, such as with piecewise smooth signals. In this study, we
propose a probabilistic sparsity prior formulated as a mixture of degenerate
Gaussians, capable of modeling sparsity with respect to a generic basis. Under
this premise, we design a neural network that can be interpreted as the Bayes
estimator for linear inverse problems. Additionally, we put forth both a
supervised and an unsupervised training strategy to estimate the parameters of
this network. To evaluate the effectiveness of our approach, we conduct a
numerical comparison with commonly employed sparsity-promoting regularization
techniques, namely LASSO, group LASSO, iterative hard thresholding, and sparse
coding/dictionary learning. Notably, our reconstructions consistently exhibit
lower mean square error values across all $1$D datasets utilized for the
comparisons, even in cases where the datasets significantly deviate from a
Gaussian mixture model.
\\ ( https://arxiv.org/abs/2401.16612 ,  503kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16613 (*cross-listing*)
Date: Mon, 29 Jan 2024 23:00:15 GMT   (35kb)

Title: Algebraic Complexity and Neurovariety of Linear Convolutional Networks
Authors: Vahid Shahverdi
Categories: math.AG cs.LG
MSC-class: 68T07, 14E99, 14J99, 14P10, 90C23
\\
  In this paper, we study linear convolutional networks with one-dimensional
filters and arbitrary strides. The neuromanifold of such a network is a
semialgebraic set, represented by a space of polynomials admitting specific
factorizations. Introducing a recursive algorithm, we generate polynomial
equations whose common zero locus corresponds to the Zariski closure of the
corresponding neuromanifold. Furthermore, we explore the algebraic complexity
of training these networks employing tools from metric algebraic geometry. Our
findings reveal that the number of all complex critical points in the
optimization of such a network is equal to the generic Euclidean distance
degree of a Segre variety. Notably, this count significantly surpasses the
number of critical points encountered in the training of a fully connected
linear network with the same number of parameters.
\\ ( https://arxiv.org/abs/2401.16613 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16634 (*cross-listing*)
Date: Tue, 30 Jan 2024 00:14:13 GMT   (4947kb,D)

Title: The Why, When, and How to Use Active Learning in Large-Data-Driven 3D
  Object Detection for Safe Autonomous Driving: An Empirical Exploration
Authors: Ross Greer, Bj{\o}rk Antoniussen, Mathias V. Andersen, Andreas
  M{\o}gelmose, and Mohan M. Trivedi
Categories: cs.CV cs.LG
\\
  Active learning strategies for 3D object detection in autonomous driving
datasets may help to address challenges of data imbalance, redundancy, and
high-dimensional data. We demonstrate the effectiveness of entropy querying to
select informative samples, aiming to reduce annotation costs and improve model
performance. We experiment using the BEVFusion model for 3D object detection on
the nuScenes dataset, comparing active learning to random sampling and
demonstrating that entropy querying outperforms in most cases. The method is
particularly effective in reducing the performance gap between majority and
minority classes. Class-specific analysis reveals efficient allocation of
annotated resources for limited data budgets, emphasizing the importance of
selecting diverse and informative data for model training. Our findings suggest
that entropy querying is a promising strategy for selecting data that enhances
model learning in resource-constrained environments.
\\ ( https://arxiv.org/abs/2401.16634 ,  4947kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16655 (*cross-listing*)
Date: Tue, 30 Jan 2024 01:18:41 GMT   (13kb)

Title: Rademacher Complexity of Neural ODEs via Chen-Fliess Series
Authors: Joshua Hanson, Maxim Raginsky
Categories: stat.ML cs.LG cs.SY eess.SY math.OC
Comments: 14 pages; final version to appear in L4DC 2024
\\
  We show how continuous-depth neural ODE models can be framed as single-layer,
infinite-width nets using the Chen--Fliess series expansion for nonlinear ODEs.
In this net, the output ''weights'' are taken from the signature of the control
input -- a tool used to represent infinite-dimensional paths as a sequence of
tensors -- which comprises iterated integrals of the control input over a
simplex. The ''features'' are taken to be iterated Lie derivatives of the
output function with respect to the vector fields in the controlled ODE model.
The main result of this work applies this framework to derive compact
expressions for the Rademacher complexity of ODE models that map an initial
condition to a scalar output at some terminal time. The result leverages the
straightforward analysis afforded by single-layer architectures. We conclude
with some examples instantiating the bound for some specific systems and
discuss potential follow-up work.
\\ ( https://arxiv.org/abs/2401.16655 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16677 (*cross-listing*)
Date: Tue, 30 Jan 2024 01:55:34 GMT   (961kb,D)

Title: T3: Transparent Tracking & Triggering for Fine-grained Overlap of
  Compute & Collectives
Authors: Suchita Pati, Shaizeen Aga, Mahzabeen Islam, Nuwan Jayasena and
  Matthew D. Sinclair
Categories: cs.AR cs.DC cs.LG
Comments: To appear at the International Conference on Architectural Support
  for Programming Languages and Operating Systems (ASPLOS) 2024
ACM-class: C.2.4; C.1.2
\\
  Large Language Models increasingly rely on distributed techniques for their
training and inference. These techniques require communication across devices
which can reduce scaling efficiency as the number of devices increases. While
some distributed techniques can overlap, and thus, hide this communication with
independent computations, techniques such as Tensor Parallelism (TP) inherently
serialize communication with model execution. One approach to hide this
serialized communication is to interleave it with the producer operation (of
the communicated data) in a fine-grained manner. However, this fine-grained
interleaving of communication and computation in software can be difficult.
Furthermore, as with any concurrent execution, it requires compute and memory
resources to be shared between computation and communication, causing resource
contention that reduces overlapping efficacy.
  To overcome these challenges, we propose T3 which applies hardware-software
co-design to transparently overlap serialized communication while minimizing
resource contention with compute. T3 transparently fuses producer operations
with the subsequent communication via a simple configuration of the producer's
output address space and requires minor software changes. At the hardware
level, T3 adds a lightweight track and trigger mechanism to orchestrate the
producer's compute, and communication. It further uses compute-enhanced
memories for communication's attendant compute. As a result, T3 reduces
resource contention, and efficiently overlaps serialized communication with
computation. For important Transformer models like T-NLG, T3 speeds up
communication-heavy sublayers by 30% geomean (max 47%) and reduces data
movement by 22% geomean (max 36%). Furthermore, T3's benefits persist as models
scale: geomean 29% for sublayers in $\sim$500-billion parameter models, PALM
and MT-NLG.
\\ ( https://arxiv.org/abs/2401.16677 ,  961kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16683 (*cross-listing*)
Date: Tue, 30 Jan 2024 02:13:02 GMT   (13685kb,D)

Title: Polynomial Chaos Expansions on Principal Geodesic Grassmannian
  Submanifolds for Surrogate Modeling and Uncertainty Quantification
Authors: Dimitris G. Giovanis, Dimitrios Loukrezis, Ioannis G. Kevrekidis,
  Michael D. Shields
Categories: stat.ML cs.LG math.DS
Comments: 50 pages, 17 figures
\\
  In this work we introduce a manifold learning-based surrogate modeling
framework for uncertainty quantification in high-dimensional stochastic
systems. Our first goal is to perform data mining on the available simulation
data to identify a set of low-dimensional (latent) descriptors that efficiently
parameterize the response of the high-dimensional computational model. To this
end, we employ Principal Geodesic Analysis on the Grassmann manifold of the
response to identify a set of disjoint principal geodesic submanifolds, of
possibly different dimension, that captures the variation in the data. Since
operations on the Grassmann require the data to be concentrated, we propose an
adaptive algorithm based on Riemanniann K-means and the minimization of the
sample Frechet variance on the Grassmann manifold to identify "local" principal
geodesic submanifolds that represent different system behavior across the
parameter space. Polynomial chaos expansion is then used to construct a mapping
between the random input parameters and the projection of the response on these
local principal geodesic submanifolds. The method is demonstrated on four test
cases, a toy-example that involves points on a hypersphere, a Lotka-Volterra
dynamical system, a continuous-flow stirred-tank chemical reactor system, and a
two-dimensional Rayleigh-Benard convection problem
\\ ( https://arxiv.org/abs/2401.16683 ,  13685kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16687 (*cross-listing*)
Date: Tue, 30 Jan 2024 02:18:30 GMT   (8258kb,D)

Title: Revisiting Gradient Pruning: A Dual Realization for Defending against
  Gradient Attacks
Authors: Lulu Xue, Shengshan Hu, Ruizhi Zhao, Leo Yu Zhang, Shengqing Hu,
  Lichao Sun, Dezhong Yao
Categories: cs.CR cs.LG
\\
  Collaborative learning (CL) is a distributed learning framework that aims to
protect user privacy by allowing users to jointly train a model by sharing
their gradient updates only. However, gradient inversion attacks (GIAs), which
recover users' training data from shared gradients, impose severe privacy
threats to CL. Existing defense methods adopt different techniques, e.g.,
differential privacy, cryptography, and perturbation defenses, to defend
against the GIAs. Nevertheless, all current defense methods suffer from a poor
trade-off between privacy, utility, and efficiency. To mitigate the weaknesses
of existing solutions, we propose a novel defense method, Dual Gradient Pruning
(DGP), based on gradient pruning, which can improve communication efficiency
while preserving the utility and privacy of CL. Specifically, DGP slightly
changes gradient pruning with a stronger privacy guarantee. And DGP can also
significantly improve communication efficiency with a theoretical analysis of
its convergence and generalization. Our extensive experiments show that DGP can
effectively defend against the most powerful GIAs and reduce the communication
cost without sacrificing the model's utility.
\\ ( https://arxiv.org/abs/2401.16687 ,  8258kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16719 (*cross-listing*)
Date: Tue, 30 Jan 2024 03:34:25 GMT   (2980kb,D)

Title: OptiState: State Estimation of Legged Robots using Gated Networks with
  Transformer-based Vision and Kalman Filtering
Authors: Alexander Schperberg, Yusuke Tanaka, Saviz Mowlavi, Feng Xu, Bharathan
  Balaji, Dennis Hong
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: Accepted to the 2024 IEEE International Conference on Robotics and
  Automation (ICRA), May 13-17, in Yokohama, Japan. 7 pages, 5 figures, 1 table
\\
  State estimation for legged robots is challenging due to their highly dynamic
motion and limitations imposed by sensor accuracy. By integrating Kalman
filtering, optimization, and learning-based modalities, we propose a hybrid
solution that combines proprioception and exteroceptive information for
estimating the state of the robot's trunk. Leveraging joint encoder and IMU
measurements, our Kalman filter is enhanced through a single-rigid body model
that incorporates ground reaction force control outputs from convex Model
Predictive Control optimization. The estimation is further refined through
Gated Recurrent Units, which also considers semantic insights and robot height
from a Vision Transformer autoencoder applied on depth images. This framework
not only furnishes accurate robot state estimates, including uncertainty
evaluations, but can minimize the nonlinear errors that arise from sensor
measurements and model simplifications through learning. The proposed
methodology is evaluated in hardware using a quadruped robot on various
terrains, yielding a 65% improvement on the Root Mean Squared Error compared to
our VIO SLAM baseline. Code example: https://github.com/AlexS28/OptiState
\\ ( https://arxiv.org/abs/2401.16719 ,  2980kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16776 (*cross-listing*)
Date: Tue, 30 Jan 2024 06:29:41 GMT   (1857kb,D)

Title: Leveraging Nested MLMC for Sequential Neural Posterior Estimation with
  Intractable Likelihoods
Authors: Xiliang Yang, Yifei Xiong, Zhijian He
Categories: stat.CO cs.LG stat.ML
Comments: 28 pages, 4 figures
\\
  Sequential neural posterior estimation (SNPE) techniques have been recently
proposed for dealing with simulation-based models with intractable likelihoods.
They are devoted to learning the posterior from adaptively proposed simulations
using neural network-based conditional density estimators. As a SNPE technique,
the automatic posterior transformation (APT) method proposed by Greenberg et
al. (2019) performs notably and scales to high dimensional data. However, the
APT method bears the computation of an expectation of the logarithm of an
intractable normalizing constant, i.e., a nested expectation. Although atomic
APT was proposed to solve this by discretizing the normalizing constant, it
remains challenging to analyze the convergence of learning. In this paper, we
propose a nested APT method to estimate the involved nested expectation
instead. This facilitates establishing the convergence analysis. Since the
nested estimators for the loss function and its gradient are biased, we make
use of unbiased multi-level Monte Carlo (MLMC) estimators for debiasing. To
further reduce the excessive variance of the unbiased estimators, this paper
also develops some truncated MLMC estimators by taking account of the trade-off
between the bias and the average cost. Numerical experiments for approximating
complex posteriors with multimodal in moderate dimensions are provided.
\\ ( https://arxiv.org/abs/2401.16776 ,  1857kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16782 (*cross-listing*)
Date: Tue, 30 Jan 2024 06:43:40 GMT   (3604kb,D)

Title: A Literature Review on Fetus Brain Motion Correction in MRI
Authors: Haoran Zhang, Yun Wang
Categories: eess.IV cs.LG
Comments: 8 pages, 2 figures
\\
  This paper provides a comprehensive review of the latest advancements in
fetal motion correction in MRI. We delve into various contemporary
methodologies and technological advancements aimed at overcoming these
challenges. It includes traditional 3D fetal MRI correction methods like Slice
to Volume Registration (SVR), deep learning-based techniques such as
Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) Networks,
Transformers, Generative Adversarial Networks (GANs) and most recent
advancements of Diffusion Models. The insights derived from this literature
review reflect a thorough understanding of both the technical intricacies and
practical implications of fetal motion in MRI studies, offering a reasoned
perspective on potential solutions and future improvements in this field.
\\ ( https://arxiv.org/abs/2401.16782 ,  3604kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16803 (*cross-listing*)
Date: Tue, 30 Jan 2024 07:50:32 GMT   (1399kb,D)

Title: PBSCSR: The Piano Bootleg Score Composer Style Recognition Dataset
Authors: Arhan Jain, Alec Bunn, and TJ Tsai
Categories: cs.SD cs.LG eess.AS
Comments: 15 pages, 4 figures
\\
  This article motivates, describes, and presents the PBSCSR dataset for
studying composer style recognition of piano sheet music. Our overarching goal
was to create a dataset for studying composer style recognition that is "as
accessible as MNIST and as challenging as ImageNet." To achieve this goal, we
sample fixed-length bootleg score fragments from piano sheet music images on
IMSLP. The dataset itself contains 40,000 62x64 bootleg score images for a
9-way classification task, 100,000 62x64 bootleg score images for a 100-way
classification task, and 29,310 unlabeled variable-length bootleg score images
for pretraining. The labeled data is presented in a form that mirrors MNIST
images, in order to make it extremely easy to visualize, manipulate, and train
models in an efficient manner. Additionally, we include relevant metadata to
allow access to the underlying raw sheet music images and other related data on
IMSLP. We describe several research tasks that could be studied with the
dataset, including variations of composer style recognition in a few-shot or
zero-shot setting. For tasks that have previously proposed models, we release
code and baseline results for future works to compare against. We also discuss
open research questions that the PBSCSR data is especially well suited to
facilitate research on and areas of fruitful exploration in future work.
\\ ( https://arxiv.org/abs/2401.16803 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16832 (*cross-listing*)
Date: Tue, 30 Jan 2024 09:19:50 GMT   (129kb,D)

Title: Analysis of Knowledge Tracing performance on synthesised student data
Authors: Panagiotis Pagonis and Kai Hartung and Di Wu and Munir Georges and
  S\"oren Gr\"ottrup
Categories: cs.CY cs.LG stat.ML
Comments: Accepted at AI4AI Education workshop 2023 (
  https://sme.uni-bamberg.de/ai4ai/ )
\\
  Knowledge Tracing (KT) aims to predict the future performance of students by
tracking the development of their knowledge states. Despite all the recent
progress made in this field, the application of KT models in education systems
is still restricted from the data perspectives: 1) limited access to real life
data due to data protection concerns, 2) lack of diversity in public datasets,
3) noises in benchmark datasets such as duplicate records. To resolve these
problems, we simulated student data with three statistical strategies based on
public datasets and tested their performance on two KT baselines. While we
observe only minor performance improvement with additional synthetic data, our
work shows that using only synthetic data for training can lead to similar
performance as real data.
\\ ( https://arxiv.org/abs/2401.16832 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16876 (*cross-listing*)
Date: Tue, 30 Jan 2024 10:29:31 GMT   (2358kb,D)

Title: Zero-shot Classification using Hyperdimensional Computing
Authors: Samuele Ruffino, Geethan Karunaratne, Michael Hersche, Luca Benini,
  Abu Sebastian and Abbas Rahimi
Categories: cs.CV cs.LG
Comments: This is the extended version of a paper accepted in the Design,
  Automation, and Test in Europe Conference (DATE), 2024
\\
  Classification based on Zero-shot Learning (ZSL) is the ability of a model to
classify inputs into novel classes on which the model has not previously seen
any training examples. Providing an auxiliary descriptor in the form of a set
of attributes describing the new classes involved in the ZSL-based
classification is one of the favored approaches to solving this challenging
task. In this work, inspired by Hyperdimensional Computing (HDC), we propose
the use of stationary binary codebooks of symbol-like distributed
representations inside an attribute encoder to compactly represent a
computationally simple end-to-end trainable model, which we name
Hyperdimensional Computing Zero-shot Classifier~(HDC-ZSC). It consists of a
trainable image encoder, an attribute encoder based on HDC, and a similarity
kernel. We show that HDC-ZSC can be used to first perform zero-shot attribute
extraction tasks and, can later be repurposed for Zero-shot Classification
tasks with minimal architectural changes and minimal model retraining. HDC-ZSC
achieves Pareto optimal results with a 63.8% top-1 classification accuracy on
the CUB-200 dataset by having only 26.6 million trainable parameters. Compared
to two other state-of-the-art non-generative approaches, HDC-ZSC achieves 4.3%
and 9.9% better accuracy, while they require more than 1.85x and 1.72x
parameters compared to HDC-ZSC, respectively.
\\ ( https://arxiv.org/abs/2401.16876 ,  2358kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16920 (*cross-listing*)
Date: Tue, 30 Jan 2024 11:42:52 GMT   (2407kb,D)

Title: Sparse Portfolio Selection via Topological Data Analysis based
  Clustering
Authors: Anubha Goel, Damir Filipovi\'c, Puneet Pasricha
Categories: q-fin.PM cs.LG q-fin.ST
\\
  This paper uses topological data analysis (TDA) tools and introduces a
data-driven clustering-based stock selection strategy tailored for sparse
portfolio construction. Our asset selection strategy exploits the topological
features of stock price movements to select a subset of topologically similar
(different) assets for a sparse index tracking (Markowitz) portfolio. We
introduce new distance measures, which serve as an input to the clustering
algorithm, on the space of persistence diagrams and landscapes that consider
the time component of a time series. We conduct an empirical analysis on the
S\&P index from 2009 to 2020, including a study on the COVID-19 data to
validate the robustness of our methodology. Our strategy to integrate TDA with
the clustering algorithm significantly enhanced the performance of sparse
portfolios across various performance measures in diverse market scenarios.
\\ ( https://arxiv.org/abs/2401.16920 ,  2407kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16937 (*cross-listing*)
Date: Tue, 30 Jan 2024 12:04:56 GMT   (10751kb,D)

Title: Segmentation and Characterization of Macerated Fibers and Vessels Using
  Deep Learning
Authors: Saqib Qamar, Abu Imran Baba, St\'ephane Verger, Magnus Andersson
Categories: cs.CV cs.LG
Comments: 7 figures
ACM-class: I.5.1
\\
  Purpose: Wood comprises different cell types, such as fibers and vessels,
defining its properties. Studying their shape, size, and arrangement in
microscopic images is crucial for understanding wood samples. Typically, this
involves macerating (soaking) samples in a solution to separate cells, then
spreading them on slides for imaging with a microscope that covers a wide area,
capturing thousands of cells. However, these cells often cluster and overlap in
images, making the segmentation difficult and time-consuming using standard
image-processing methods. Results: In this work, we develop an automatic deep
learning segmentation approach that utilizes the one-stage YOLOv8 model for
fast and accurate fiber and vessel segmentation and characterization in
microscopy images. The model can analyze 32640 x 25920 pixels images and
demonstrate effective cell detection and segmentation, achieving a mAP_0.5-0.95
of 78 %. To assess the model's robustness, we examined fibers from a
genetically modified tree line known for longer fibers. The outcomes were
comparable to previous manual measurements. Additionally, we created a
user-friendly web application for image analysis and provided the code for use
on Google Colab. Conclusion: By leveraging YOLOv8's advances, this work
provides a deep learning solution to enable efficient quantification and
analysis of wood cells suitable for practical applications.
\\ ( https://arxiv.org/abs/2401.16937 ,  10751kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16981 (*cross-listing*)
Date: Tue, 30 Jan 2024 13:07:24 GMT   (1220kb)

Title: Selection of gamma events from IACT images with deep learning methods
Authors: E. O. Gres, A. P. Kryukov, A. P. Demichev, J. J. Dubenskaya, S. P.
  Polyakov, A. A. Vlaskina, D. P. Zhurov
Categories: astro-ph.IM astro-ph.HE cs.LG
Comments: Version of article, submitted to journal
Journal-ref: Moscow Univ. Phys. 78, 1 (2023) 45-51
DOI: 10.3103/S002713492307010X
\\
  Imaging Atmospheric Cherenkov Telescopes (IACTs) of gamma ray observatory
TAIGA detect the Extesnive Air Showers (EASs) originating from the cosmic or
gamma rays interactions with the atmosphere. Thereby, telescopes obtain images
of the EASs. The ability to segregate gamma rays images from the hadronic
cosmic ray background is one of the main features of this type of detectors.
However, in actual IACT observations simultaneous observation of the background
and the source of gamma ray is needed. This observation mode (called wobbling)
modifies images of events, which affects the quality of selection by neural
networks.
  Thus, in this work, the results of the application of neural networks (NN)
for image classification task on Monte Carlo (MC) images of TAIGA-IACTs are
presented. The wobbling mode is considered together with the image adaptation
for adequate analysis by NNs. Simultaneously, we explore several neural network
structures that classify events both directly from images or through Hillas
parameters extracted from images. In addition, by employing NNs, MC simulation
data are used to evaluate the quality of the segregation of rare gamma events
with the account of all necessary image modifications.
\\ ( https://arxiv.org/abs/2401.16981 ,  1220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16985 (*cross-listing*)
Date: Tue, 30 Jan 2024 13:14:54 GMT   (2602kb,D)

Title: Multiple Yield Curve Modeling and Forecasting using Deep Learning
Authors: Ronald Richman, Salvatore Scognamiglio
Categories: stat.ML cs.LG
\\
  This manuscript introduces deep learning models that simultaneously describe
the dynamics of several yield curves. We aim to learn the dependence structure
among the different yield curves induced by the globalization of financial
markets and exploit it to produce more accurate forecasts. By combining the
self-attention mechanism and nonparametric quantile regression, our model
generates both point and interval forecasts of future yields. The architecture
is designed to avoid quantile crossing issues affecting multiple quantile
regression models. Numerical experiments conducted on two different datasets
confirm the effectiveness of our approach. Finally, we explore potential
extensions and enhancements by incorporating deep ensemble methods and transfer
learning mechanisms.
\\ ( https://arxiv.org/abs/2401.16985 ,  2602kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16986 (*cross-listing*)
Date: Tue, 30 Jan 2024 13:15:59 GMT   (880kb,D)

Title: Causal Machine Learning for Cost-Effective Allocation of Development Aid
Authors: Milan Kuzmanovic, Dennis Frauen, Tobias Hatt, Stefan Feuerriegel
Categories: stat.ML cs.LG
\\
  The Sustainable Development Goals (SDGs) of the United Nations provide a
blueprint of a better future by 'leaving no one behind', and, to achieve the
SDGs by 2030, poor countries require immense volumes of development aid. In
this paper, we develop a causal machine learning framework for predicting
heterogeneous treatment effects of aid disbursements to inform effective aid
allocation. Specifically, our framework comprises three components: (i) a
balancing autoencoder that uses representation learning to embed
high-dimensional country characteristics while addressing treatment selection
bias; (ii) a counterfactual generator to compute counterfactual outcomes for
varying aid volumes to address small sample-size settings; and (iii) an
inference model that is used to predict heterogeneous treatment-response
curves. We demonstrate the effectiveness of our framework using data with
official development aid earmarked to end HIV/AIDS in 105 countries, amounting
to more than USD 5.2 billion. For this, we first show that our framework
successfully computes heterogeneous treatment-response curves using
semi-synthetic data. Then, we demonstrate our framework using real-world HIV
data. Our framework points to large opportunities for a more effective aid
allocation, suggesting that the total number of new HIV infections could be
reduced by up to 3.3% (~50,000 cases) compared to the current allocation
practice.
\\ ( https://arxiv.org/abs/2401.16986 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17029 (*cross-listing*)
Date: Tue, 30 Jan 2024 14:06:09 GMT   (1251kb,D)

Title: LADDER: Revisiting the Cosmic Distance Ladder with Deep Learning
  Approaches and Exploring its Applications
Authors: Rahul Shah, Soumadeep Saha, Purba Mukherjee, Utpal Garain, Supratik
  Pal
Categories: astro-ph.CO astro-ph.IM cs.LG
Comments: 11 pages, 4 figures, 3 tables. Comments are welcome
\\
  We investigate the prospect of reconstructing the ``cosmic distance ladder''
of the Universe using a novel deep learning framework called LADDER - Learning
Algorithm for Deep Distance Estimation and Reconstruction. LADDER is trained on
the apparent magnitude data from the Pantheon Type Ia supernovae compilation,
incorporating the full covariance information among data points, to produce
predictions along with corresponding errors. After employing several validation
tests with a number of deep learning models, we pick LADDER as the best
performing one. We then demonstrate applications of our method in the
cosmological context, that include serving as a model-independent tool for
consistency checks for other datasets like baryon acoustic oscillations,
calibration of high-redshift datasets such as gamma ray bursts, use as a
model-independent mock catalog generator for future probes, etc. Our analysis
advocates for interesting yet cautious consideration of machine learning
applications in these contexts.
\\ ( https://arxiv.org/abs/2401.17029 ,  1251kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17032 (*cross-listing*)
Date: Tue, 30 Jan 2024 14:09:35 GMT   (1410kb,D)

Title: M2CURL: Sample-Efficient Multimodal Reinforcement Learning via
  Self-Supervised Representation Learning for Robotic Manipulation
Authors: Fotios Lygerakis, Vedant Dave, Elmar Rueckert
Categories: cs.RO cs.CV cs.LG
Comments: Project website: https://sites.google.com/view/M2CURL/home
\\
  One of the most critical aspects of multimodal Reinforcement Learning (RL) is
the effective integration of different observation modalities. Having robust
and accurate representations derived from these modalities is key to enhancing
the robustness and sample efficiency of RL algorithms. However, learning
representations in RL settings for visuotactile data poses significant
challenges, particularly due to the high dimensionality of the data and the
complexity involved in correlating visual and tactile inputs with the dynamic
environment and task objectives. To address these challenges, we propose
Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our
approach employs a novel multimodal self-supervised learning technique that
learns efficient representations and contributes to faster convergence of RL
algorithms. Our method is agnostic to the RL algorithm, thus enabling its
integration with any available RL algorithm. We evaluate M2CURL on the Tactile
Gym 2 simulator and we show that it significantly enhances the learning
efficiency in different manipulation tasks. This is evidenced by faster
convergence rates and higher cumulative rewards per episode, compared to
standard RL algorithms without our representation learning approach.
\\ ( https://arxiv.org/abs/2401.17032 ,  1410kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17077 (*cross-listing*)
Date: Tue, 30 Jan 2024 14:57:32 GMT   (1287kb,D)

Title: Dynamical Survival Analysis with Controlled Latent States
Authors: Linus Bleistein, Van-Tuan Nguyen, Adeline Fermanian, Agathe Guilloux
Categories: stat.ML cs.LG
Comments: 41 pages, 27 figures
\\
  We consider the task of learning individual-specific intensities of counting
processes from a set of static variables and irregularly sampled time series.
We introduce a novel modelization approach in which the intensity is the
solution to a controlled differential equation. We first design a neural
estimator by building on neural controlled differential equations. In a second
time, we show that our model can be linearized in the signature space under
sufficient regularity conditions, yielding a signature-based estimator which we
call CoxSig. We provide theoretical learning guarantees for both estimators,
before showcasing the performance of our models on a vast array of simulated
and real-world datasets from finance, predictive maintenance and food supply
chain management.
\\ ( https://arxiv.org/abs/2401.17077 ,  1287kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17098 (*cross-listing*)
Date: Tue, 30 Jan 2024 15:29:32 GMT   (1218kb)

Title: CharNet: Generalized Approach for High-Complexity Character
  Classification
Authors: Boris Kriuk
Categories: cs.CV cs.LG
Comments: 8 pages, 14 figures, preprint
\\
  Handwritten character recognition (HCR) is a challenging problem for machine
learning researchers. Unlike printed text data, handwritten character datasets
have more variation due to human-introduced bias. With numerous unique
character classes present, some data, such as Logographic Scripts or
Sino-Korean character sequences, bring new complications to the HCR problem.
The classification task on such datasets requires the model to learn
high-complexity details of the images that share similar features. With recent
advances in computational resource availability and further computer vision
theory development, some research teams have effectively addressed the arising
challenges. Although known for achieving high efficiency, many common
approaches are still not generalizable and use dataset-specific solutions to
achieve better results. Due to complex structure and high computing demands,
existing methods frequently prevent the solutions from gaining popularity. This
paper proposes a straightforward, generalizable, and highly effective approach
(CharNet) for detailed character image classification and compares its
performance to that of existing approaches.
\\ ( https://arxiv.org/abs/2401.17098 ,  1218kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17109 (*cross-listing*)
Date: Tue, 30 Jan 2024 15:45:30 GMT   (55180kb,D)

Title: Evaluation in Neural Style Transfer: A Review
Authors: Eleftherios Ioannou and Steve Maddock
Categories: cs.CV cs.LG cs.NE
Comments: 21 pages, 8 figures
\\
  The field of Neural Style Transfer (NST) has witnessed remarkable progress in
the past few years, with approaches being able to synthesize artistic and
photorealistic images and videos of exceptional quality. To evaluate such
results, a diverse landscape of evaluation methods and metrics is used,
including authors' opinions based on side-by-side comparisons, human evaluation
studies that quantify the subjective judgements of participants, and a
multitude of quantitative computational metrics which objectively assess the
different aspects of an algorithm's performance. However, there is no consensus
regarding the most suitable and effective evaluation procedure that can
guarantee the reliability of the results. In this review, we provide an
in-depth analysis of existing evaluation techniques, identify the
inconsistencies and limitations of current evaluation methods, and give
recommendations for standardized evaluation practices. We believe that the
development of a robust evaluation framework will not only enable more
meaningful and fairer comparisons among NST methods but will also enhance the
comprehension and interpretation of research findings in the field.
\\ ( https://arxiv.org/abs/2401.17109 ,  55180kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17116 (*cross-listing*)
Date: Tue, 30 Jan 2024 15:50:06 GMT   (309kb,D)

Title: Quantum error mitigation and correction mediated by Yang-Baxter equation
  and artificial neural network
Authors: Sahil Gulania, Yuri Alexeev, Stephen K. Gray, Bo Peng, Niranjan Govind
Categories: quant-ph cond-mat.soft cs.LG physics.comp-ph
\\
  Quantum computing shows great potential, but errors pose a significant
challenge. This study explores new strategies for mitigating quantum errors
using artificial neural networks (ANN) and the Yang-Baxter equation (YBE).
Unlike traditional error correction methods, which are computationally
intensive, we investigate artificial error mitigation. The manuscript
introduces the basics of quantum error sources and explores the potential of
using classical computation for error mitigation. The Yang-Baxter equation
plays a crucial role, allowing us to compress time dynamics simulations into
constant-depth circuits. By introducing controlled noise through the YBE, we
enhance the dataset for error mitigation. We train an ANN model on partial data
from quantum simulations, demonstrating its effectiveness in correcting errors
in time-evolving quantum states.
\\ ( https://arxiv.org/abs/2401.17116 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17136 (*cross-listing*)
Date: Tue, 30 Jan 2024 16:15:55 GMT   (2234kb,D)

Title: Systematically Assessing the Security Risks of AI/ML-enabled Connected
  Healthcare Systems
Authors: Mohammed Elnawawy, Mohammadreza Hallajiyan, Gargi Mitra, Shahrear
  Iqbal and Karthik Pattabiraman
Categories: cs.CR cs.CY cs.LG
Comments: 13 pages, 5 figures, 3 tables
\\
  The adoption of machine-learning-enabled systems in the healthcare domain is
on the rise. While the use of ML in healthcare has several benefits, it also
expands the threat surface of medical systems. We show that the use of ML in
medical systems, particularly connected systems that involve interfacing the ML
engine with multiple peripheral devices, has security risks that might cause
life-threatening damage to a patient's health in case of adversarial
interventions. These new risks arise due to security vulnerabilities in the
peripheral devices and communication channels. We present a case study where we
demonstrate an attack on an ML-enabled blood glucose monitoring system by
introducing adversarial data points during inference. We show that an adversary
can achieve this by exploiting a known vulnerability in the Bluetooth
communication channel connecting the glucose meter with the ML-enabled app. We
further show that state-of-the-art risk assessment techniques are not adequate
for identifying and assessing these new risks. Our study highlights the need
for novel risk analysis methods for analyzing the security of AI-enabled
connected health devices.
\\ ( https://arxiv.org/abs/2401.17136 ,  2234kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17172 (*cross-listing*)
Date: Tue, 30 Jan 2024 17:00:22 GMT   (3825kb,D)

Title: Learning Domain-Independent Green's Function For Elliptic Partial
  Differential Equations
Authors: Pawan Negi, Maggie Cheng, Mahesh Krishnamurthy, Wenjun Ying, Shuwang
  Li
Categories: physics.comp-ph cs.LG cs.NA math.NA
DOI: 10.1016/j.cma.2024.116779
\\
  Green's function characterizes a partial differential equation (PDE) and maps
its solution in the entire domain as integrals. Finding the analytical form of
Green's function is a non-trivial exercise, especially for a PDE defined on a
complex domain or a PDE with variable coefficients. In this paper, we propose a
novel boundary integral network to learn the domain-independent Green's
function, referred to as BIN-G. We evaluate the Green's function in the BIN-G
using a radial basis function (RBF) kernel-based neural network. We train the
BIN-G by minimizing the residual of the PDE and the mean squared errors of the
solutions to the boundary integral equations for prescribed test functions. By
leveraging the symmetry of the Green's function and controlling refinements of
the RBF kernel near the singularity of the Green function, we demonstrate that
our numerical scheme enables fast training and accurate evaluation of the
Green's function for PDEs with variable coefficients. The learned Green's
function is independent of the domain geometries, forcing terms, and boundary
conditions in the boundary integral formulation. Numerical experiments verify
the desired properties of the method and the expected accuracy for the
two-dimensional Poisson and Helmholtz equations with variable coefficients.
\\ ( https://arxiv.org/abs/2401.17172 ,  3825kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17174 (*cross-listing*)
Date: Tue, 30 Jan 2024 17:06:25 GMT   (82kb,D)

Title: A large dataset curation and benchmark for drug target interaction
Authors: Alex Golts, Vadim Ratner, Yoel Shoshan, Moshe Raboh, Sagi Polaczek,
  Michal Ozery-Flato, Daniel Shats, Liam Hazan, Sivan Ravid, Efrat Hexter
Categories: q-bio.BM cs.LG
\\
  Bioactivity data plays a key role in drug discovery and repurposing. The
resource-demanding nature of \textit{in vitro} and \textit{in vivo}
experiments, as well as the recent advances in data-driven computational
biochemistry research, highlight the importance of \textit{in silico} drug
target interaction (DTI) prediction approaches. While numerous large public
bioactivity data sources exist, research in the field could benefit from better
standardization of existing data resources. At present, different research
works that share similar goals are often difficult to compare properly because
of different choices of data sources and train/validation/test split
strategies. Additionally, many works are based on small data subsets, leading
to results and insights of possible limited validity. In this paper we propose
a way to standardize and represent efficiently a very large dataset curated
from multiple public sources, split the data into train, validation and test
sets based on different meaningful strategies, and provide a concrete
evaluation protocol to accomplish a benchmark. We analyze the proposed data
curation, prove its usefulness and validate the proposed benchmark through
experimental studies based on an existing neural network model.
\\ ( https://arxiv.org/abs/2401.17174 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17177 (*cross-listing*)
Date: Tue, 30 Jan 2024 17:10:42 GMT   (781kb,D)

Title: Data-Driven Discovery of PDEs via the Adjoint Method
Authors: Mohsen Sadr, Tony Tohme, Kamal Youcef-Toumi
Categories: math.OC cs.LG
\\
  In this work, we present an adjoint-based method for discovering the
underlying governing partial differential equations (PDEs) given data. The idea
is to consider a parameterized PDE in a general form, and formulate the
optimization problem that minimizes the error of PDE solution from data. Using
variational calculus, we obtain an evolution equation for the Lagrange
multipliers (adjoint equations) allowing us to compute the gradient of the
objective function with respect to the parameters of PDEs given data in a
straightforward manner. In particular, for a family of parameterized and
nonlinear PDEs, we show how the corresponding adjoint equations can be derived.
Here, we show that given smooth data set, the proposed adjoint method can
recover the true PDE up to machine accuracy. However, in the presence of noise,
the accuracy of the adjoint method becomes comparable to the famous PDE
Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy
et al., 2017). Even though the presented adjoint method relies on
forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to
the analytic expressions for gradients of the cost function with respect to
each PDE parameter.
\\ ( https://arxiv.org/abs/2401.17177 ,  781kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17205 (*cross-listing*)
Date: Tue, 30 Jan 2024 17:45:47 GMT   (1509kb,D)

Title: Adaptive Experiment Design with Synthetic Controls
Authors: Alihan H\"uy\"uk, Zhaozhi Qian, Mihaela van der Schaar
Categories: stat.ML cs.LG
Comments: Proceedings of the 27th International Conference on Artificial
  Intelligence and Statistics
\\
  Clinical trials are typically run in order to understand the effects of a new
treatment on a given population of patients. However, patients in large
populations rarely respond the same way to the same treatment. This
heterogeneity in patient responses necessitates trials that investigate effects
on multiple subpopulations - especially when a treatment has marginal or no
benefit for the overall population but might have significant benefit for a
particular subpopulation. Motivated by this need, we propose Syntax, an
exploratory trial design that identifies subpopulations with positive treatment
effect among many subpopulations. Syntax is sample efficient as it (i) recruits
and allocates patients adaptively and (ii) estimates treatment effects by
forming synthetic controls for each subpopulation that combines control samples
from other subpopulations. We validate the performance of Syntax and provide
insights into when it might have an advantage over conventional trial designs
through experiments.
\\ ( https://arxiv.org/abs/2401.17205 ,  1509kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17231 (*cross-listing*)
Date: Tue, 30 Jan 2024 18:18:41 GMT   (13523kb,D)

Title: ReAlnet: Achieving More Human Brain-Like Vision via Human Neural
  Representational Alignment
Authors: Zitong Lu, Yile Wang and Julie D. Golomb
Categories: cs.CV cs.LG cs.NE q-bio.NC
\\
  Despite the remarkable strides made in artificial intelligence, current
object recognition models still lag behind in emulating the mechanism of visual
information processing in human brains. Recent studies have highlighted the
potential of using neural data to mimic brain processing; however, these often
reply on invasive neural recordings from non-human subjects, leaving a critical
gap in our understanding of human visual perception and the development of more
human brain-like vision models. Addressing this gap, we present, for the first
time, "Re(presentational)Al(ignment)net", a vision model aligned with human
brain activity based on non-invasive EEG recordings, demonstrating a
significantly higher similarity to human brain representations. Our innovative
image-to-brain multi-layer encoding alignment framework not only optimizes
multiple layers of the model, marking a substantial leap in neural alignment,
but also enables the model to efficiently learn and mimic human brain's visual
representational patterns across object categories and different neural data
modalities. Furthermore, we discover that alignment with human brain
representations improves the model's adversarial robustness. Our findings
suggest that ReAlnet sets a new precedent in the field, bridging the gap
between artificial and human vision, and paving the way for more brain-like
artificial intelligence systems.
\\ ( https://arxiv.org/abs/2401.17231 ,  13523kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17269 (*cross-listing*)
Date: Tue, 30 Jan 2024 18:58:46 GMT   (1264kb,D)

Title: Effect of Weight Quantization on Learning Models by Typical Case
  Analysis
Authors: Shuhei Kashiwamura, Ayaka Sakata, Masaaki Imaizumi
Categories: stat.ML cs.LG
\\
  This paper examines the quantization methods used in large-scale data
analysis models and their hyperparameter choices. The recent surge in data
analysis scale has significantly increased computational resource requirements.
To address this, quantizing model weights has become a prevalent practice in
data analysis applications such as deep learning. Quantization is particularly
vital for deploying large models on devices with limited computational
resources. However, the selection of quantization hyperparameters, like the
number of bits and value range for weight quantization, remains an
underexplored area. In this study, we employ the typical case analysis from
statistical physics, specifically the replica method, to explore the impact of
hyperparameters on the quantization of simple learning models. Our analysis
yields three key findings: (i) an unstable hyperparameter phase, known as
replica symmetry breaking, occurs with a small number of bits and a large
quantization width; (ii) there is an optimal quantization width that minimizes
error; and (iii) quantization delays the onset of overparameterization, helping
to mitigate overfitting as indicated by the double descent phenomenon. We also
discover that non-uniform quantization can enhance stability. Additionally, we
develop an approximate message-passing algorithm to validate our theoretical
results.
\\ ( https://arxiv.org/abs/2401.17269 ,  1264kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2206.01900
replaced with revised version Tue, 30 Jan 2024 13:33:41 GMT   (2057kb,D)

Title: Estimating counterfactual treatment outcomes over time in complex
  multi-agent scenarios
Authors: Keisuke Fujii, Koh Takeuchi, Atsushi Kuribayashi, Naoya Takeishi,
  Yoshinobu Kawahara, Kazuya Takeda
Categories: cs.AI cs.LG cs.MA stat.ME stat.ML
Comments: 16 pages, 11 figures. Accepted in IEEE Transactions on Neural
  Networks and Learning Systems
\\ ( https://arxiv.org/abs/2206.01900 ,  2057kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05727
replaced with revised version Tue, 30 Jan 2024 07:27:32 GMT   (7703kb)

Title: An Open-Source Knowledge Graph Ecosystem for the Life Sciences
Authors: Tiffany J. Callahan, Ignacio J. Tripodi, Adrianne L. Stefanski, Luca
  Cappelletti, Sanya B. Taneja, Jordan M. Wyrwa, Elena Casiraghi, Nicolas A.
  Matentzoglu, Justin Reese, Jonathan C. Silverstein, Charles Tapley Hoyt,
  Richard D. Boyce, Scott A. Malec, Deepak R. Unni, Marcin P. Joachimiak, Peter
  N. Robinson, Christopher J. Mungall, Emanuele Cavalleri, Tommaso Fontana,
  Giorgio Valentini, Marco Mesiti, Lucas A. Gillenwater, Brook Santangelo,
  Nicole A. Vasilevsky, Robert Hoehndorf, Tellen D. Bennett, Patrick B. Ryan,
  George Hripcsak, Michael G. Kahn, Michael Bada, William A. Baumgartner Jr,
  Lawrence E. Hunter
Categories: cs.AI cs.CE
\\ ( https://arxiv.org/abs/2307.05727 ,  7703kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15706
replaced with revised version Tue, 30 Jan 2024 08:05:17 GMT   (526kb,D)

Title: Solving the flexible job-shop scheduling problem through an enhanced
  deep reinforcement learning approach
Authors: Imanol Echeverria, Maialen Murua, Roberto Santana
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.15706 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17807
replaced with revised version Tue, 30 Jan 2024 06:52:10 GMT   (174kb,D)

Title: Clover: Closed-Loop Verifiable Code Generation
Authors: Chuyue Sun, Ying Sheng, Oded Padon, Clark Barrett
Categories: cs.AI cs.LG cs.SE
\\ ( https://arxiv.org/abs/2310.17807 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11482
replaced with revised version Tue, 30 Jan 2024 01:15:59 GMT   (862kb,D)

Title: Meta Prompting for AGI Systems
Authors: Yifan Zhang
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.11482 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17227
replaced with revised version Tue, 30 Jan 2024 18:53:30 GMT   (12569kb,D)

Title: War and Peace (WarAgent): Large Language Model-based Multi-Agent
  Simulation of World Wars
Authors: Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang
  Ge, Libby Hemphill, Yongfeng Zhang
Categories: cs.AI cs.CL cs.CY
Comments: 47 pages, 9 figures, 5 tables
\\ ( https://arxiv.org/abs/2311.17227 ,  12569kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05934
replaced with revised version Tue, 30 Jan 2024 11:58:10 GMT   (527kb,D)

Title: Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
Authors: Oded Ovadia, Menachem Brief, Moshik Mishaeli, Oren Elisha
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2312.05934 ,  527kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15443
replaced with revised version Tue, 30 Jan 2024 04:43:27 GMT   (1096kb,D)

Title: DiffuserLite: Towards Real-time Diffusion Planning
Authors: Zibin Dong, Jianye Hao, Yifu Yuan, Fei Ni, Yitian Wang, Pengyi Li and
  Yan Zheng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.15443 ,  1096kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15911
replaced with revised version Tue, 30 Jan 2024 07:00:44 GMT   (45kb)

Title: Distribution-consistency Structural Causal Models
Authors: Heyang Gong, Chaochao Lu, Yu Zhang
Categories: cs.AI math.ST stat.TH
\\ ( https://arxiv.org/abs/2401.15911 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2301.07535
replaced with revised version Tue, 30 Jan 2024 15:32:17 GMT   (12026kb,D)

Title: News and Load: A Quantitative Exploration of Natural Language Processing
  Applications for Forecasting Day-ahead Electricity System Demand
Authors: Yun Bai, Simon Camal, Andrea Michiorri
Categories: cs.CL cs.AI cs.CY
Comments: 13 pages, 8 figures, 13 tables
\\ ( https://arxiv.org/abs/2301.07535 ,  12026kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11501
replaced with revised version Mon, 29 Jan 2024 23:09:49 GMT   (7261kb,D)

Title: Lost in Translationese? Reducing Translation Effect Using Abstract
  Meaning Representation
Authors: Shira Wein, Nathan Schneider
Categories: cs.CL
Comments: EACL 2024 Camera-ready
\\ ( https://arxiv.org/abs/2304.11501 ,  7261kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10163
replaced with revised version Tue, 30 Jan 2024 03:58:19 GMT   (1085kb,D)

Title: Large Language Models Leverage External Knowledge to Extend Clinical
  Insight Beyond Language Boundaries
Authors: Jiageng Wu, Xian Wu, Zhaopeng Qiu, Minghui Li, Yingying Zhang, Yefeng
  Zheng, Changzheng Yuan and Jie Yang
Categories: cs.CL cs.AI cs.CY
\\ ( https://arxiv.org/abs/2305.10163 ,  1085kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11789
replaced with revised version Tue, 30 Jan 2024 06:52:43 GMT   (426kb,D)

Title: Solving NLP Problems through Human-System Collaboration: A
  Discussion-based Approach
Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki
Categories: cs.CL
Comments: EACL2024 Findings
\\ ( https://arxiv.org/abs/2305.11789 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11863
replaced with revised version Tue, 30 Jan 2024 18:31:45 GMT   (35113kb,D)

Title: Scaling laws for language encoding models in fMRI
Authors: Richard Antonello, Aditya Vaidya, and Alexander G. Huth
Categories: cs.CL cs.AI
Comments: Accepted to the Thirty-seventh Annual Conference on Neural
  Information Processing Systems (NeurIPS 2023). Please cite NeurIPS version
\\ ( https://arxiv.org/abs/2305.11863 ,  35113kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13009
replaced with revised version Tue, 30 Jan 2024 11:52:51 GMT   (297kb,D)

Title: Textually Pretrained Speech Language Models
Authors: Michael Hassid, Tal Remez, Tu Anh Nguyen, Itai Gat, Alexis Conneau,
  Felix Kreuk, Jade Copet, Alexandre Defossez, Gabriel Synnaeve, Emmanuel
  Dupoux, Roy Schwartz, Yossi Adi
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2305.13009 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11489
replaced with revised version Tue, 30 Jan 2024 12:11:45 GMT   (6571kb,D)

Title: Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs
  for Fact-aware Language Modeling
Authors: Linyao Yang and Hongyang Chen and Zhao Li and Xiao Ding and Xindong Wu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2306.11489 ,  6571kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04823
replaced with revised version Tue, 30 Jan 2024 00:00:57 GMT   (1592kb)

Title: Evaluating the Generation Capabilities of Large Chinese Language Models
Authors: Hui Zeng, Jingyuan Xue, Meng Hao, Chen Sun, Bin Ning, Na Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.04823 ,  1592kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11696
replaced with revised version Tue, 30 Jan 2024 07:11:20 GMT   (12629kb,D)

Title: Efficient Benchmarking of Language Models
Authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor,
  Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen
Categories: cs.CL cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2308.11696 ,  12629kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05973
replaced with revised version Mon, 29 Jan 2024 22:19:12 GMT   (1516kb,D)

Title: Circuit Breaking: Removing Model Behaviors with Targeted Ablation
Authors: Maximilian Li, Xander Davies, Max Nadeau
Categories: cs.CL cs.LG
Journal-ref: Workshop on Challenges in Deployable Generative AI at
  International Conference on Machine Learning (ICML), Honolulu, Hawaii, USA.
  2023
\\ ( https://arxiv.org/abs/2309.05973 ,  1516kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07900
replaced with revised version Tue, 30 Jan 2024 18:26:03 GMT   (188kb,D)

Title: Ambiguity-Aware In-Context Learning with Large Language Models
Authors: Lingyu Gao, Aditi Chaudhary, Krishna Srinivasan, Kazuma Hashimoto,
  Karthik Raman, Michael Bendersky
Categories: cs.CL cs.IR
Comments: 15 pages in total
\\ ( https://arxiv.org/abs/2309.07900 ,  188kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08345
replaced with revised version Tue, 30 Jan 2024 02:40:44 GMT   (627kb,D)

Title: Data Distribution Bottlenecks in Grounding Language Models to Knowledge
  Bases
Authors: Yiheng Shu, Zhiwei Yu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2309.08345 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08648
replaced with revised version Mon, 29 Jan 2024 22:44:41 GMT   (5633kb,D)

Title: MAPLE: Mobile App Prediction Leveraging Large Language Model Embeddings
Authors: Yonchanok Khaokaew, Hao Xue, Flora D. Salim
Categories: cs.CL cs.AI
DOI: 10.1145/3643514
\\ ( https://arxiv.org/abs/2309.08648 ,  5633kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00863
replaced with revised version Tue, 30 Jan 2024 04:04:32 GMT   (8695kb,D)

Title: Syllable-level lyrics generation from melody exploiting character-level
  language model
Authors: Zhe Zhang, Karol Lasocki, Yi Yu, Atsuhiro Takasu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.00863 ,  8695kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09499
replaced with revised version Tue, 30 Jan 2024 09:04:06 GMT   (242kb,D)

Title: One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language
  Models
Authors: Hang Shao, Bei Liu, Yanmin Qian
Categories: cs.CL cs.AI
Comments: Accepted by ICASSP2024
\\ ( https://arxiv.org/abs/2310.09499 ,  242kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04507
replaced with revised version Tue, 30 Jan 2024 08:01:42 GMT   (8267kb,D)

Title: Conversation Understanding using Relational Temporal Graph Neural
  Networks with Auxiliary Cross-Modality Interaction
Authors: Cam-Van Thi Nguyen, Anh-Tuan Mai, The-Son Le, Hai-Dang Kieu, Duc-Trong
  Le
Categories: cs.CL cs.MM
Comments: EMNLP 2023
Journal-ref: The 2023 Conference on Empirical Methods in Natural Language
  Processing
DOI: 10.18653/v1/2023.emnlp-main.937
\\ ( https://arxiv.org/abs/2311.04507 ,  8267kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16267
replaced with revised version Tue, 30 Jan 2024 08:00:07 GMT   (597kb,D)

Title: Novel Preprocessing Technique for Data Embedding in Engineering Code
  Generation Using Large Language Model
Authors: Yu-Chen Lin, Akhilesh Kumar, Norman Chang, Wenliang Zhang, Muhammad
  Zakir, Rucha Apte, Haiyang He, Chao Wang, Jyh-Shing Roger Jang
Categories: cs.CL cs.SE
\\ ( https://arxiv.org/abs/2311.16267 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03731
replaced with revised version Tue, 30 Jan 2024 03:42:53 GMT   (586kb,D)

Title: MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs
Authors: Xingtong Yu, Chang Zhou, Yuan Fang, Xinming Zhang
Categories: cs.CL cs.LG
Comments: Accepted by WWW2024
\\ ( https://arxiv.org/abs/2312.03731 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07228
replaced with revised version Mon, 29 Jan 2024 21:34:27 GMT   (830kb)

Title: Toxic language detection: a systematic review of Arabic datasets
Authors: Imene Bensalem, Paolo Rosso, Hanane Zitouni
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.07228 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11509
replaced with revised version Tue, 30 Jan 2024 04:15:00 GMT   (1052kb,D)

Title: Toward a Reinforcement-Learning-Based System for Adjusting Medication to
  Minimize Speech Disfluency
Authors: Pavlos Constas, Vikram Rawal, Matthew Honorio Oliveira, Andreas
  Constas, Aditya Khan, Kaison Cheung, Najma Sultani, Carrie Chen, Micol
  Altomare, Michael Akzam, Jiacheng Chen, Vhea He, Lauren Altomare, Heraa
  Murqi, Asad Khan, Nimit Amikumar Bhanshali, Youssef Rachad, Michael Guerzhoy
Categories: cs.CL cs.LG eess.AS
Comments: In Proc. Machine Learning for Cognitive and Mental Health Workshop
  (ML4CMH) at AAAI 2024
\\ ( https://arxiv.org/abs/2312.11509 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12141
replaced with revised version Tue, 30 Jan 2024 12:19:09 GMT   (2065kb,D)

Title: Locating Factual Knowledge in Large Language Models: Exploring the
  Residual Stream and Analyzing Subvalues in Vocabulary Space
Authors: Zeping Yu, Sophia Ananiadou
Categories: cs.CL cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2312.12141 ,  2065kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14185
replaced with revised version Tue, 30 Jan 2024 17:02:20 GMT   (2795kb,D)

Title: Auto311: A Confidence-guided Automated System for Non-emergency Calls
Authors: Zirong Chen, Xutong Sun, Yuanhe Li, Meiyi Ma
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: Accepted by AAAI-2024, Sub-Track: Social Impacts
\\ ( https://arxiv.org/abs/2312.14185 ,  2795kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00595
replaced with revised version Tue, 30 Jan 2024 13:38:35 GMT   (1452kb,D)

Title: State of What Art? A Call for Multi-Prompt LLM Evaluation
Authors: Moran Mizrahi, Guy Kaplan, Dan Malkin, Rotem Dror, Dafna Shahaf,
  Gabriel Stanovsky
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.00595 ,  1452kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02777
replaced with revised version Tue, 30 Jan 2024 07:02:30 GMT   (1980kb,D)

Title: From LLM to Conversational Agent: A Memory Enhanced Architecture with
  Fine-Tuning of Large Language Models
Authors: Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, Ming Cui
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.02777 ,  1980kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08406
replaced with revised version Tue, 30 Jan 2024 13:55:34 GMT   (1300kb,D)

Title: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on
  Agriculture
Authors: Angels Balaguer, Vinamra Benara, Renato Luiz de Freitas Cunha, Roberto
  de M. Estev\~ao Filho, Todd Hendry, Daniel Holstein, Jennifer Marsman, Nick
  Mecklenburg, Sara Malvar, Leonardo O. Nunes, Rafael Padilha, Morris Sharp,
  Bruno Silva, Swati Sharma, Vijay Aski, Ranveer Chandra
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.08406 ,  1300kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09003
replaced with revised version Tue, 30 Jan 2024 15:29:10 GMT   (91kb,D)

Title: Augmenting Math Word Problems via Iterative Question Composing
Authors: Haoxiong Liu, Yifan Zhang, Yifan Luo, Andrew Chi-Chih Yao
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.09003 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09785
replaced with revised version Tue, 30 Jan 2024 07:20:17 GMT   (96kb,D)

Title: Instant Answering in E-Commerce Buyer-Seller Messaging using
  Message-to-Question Reformulation
Authors: Besnik Fetahu, Tejas Mehta, Qun Song, Nikhita Vedula, Oleg Rokhlenko,
  Shervin Malmasi
Categories: cs.CL
Comments: Accepted at ECIR 2024
\\ ( https://arxiv.org/abs/2401.09785 ,  96kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12585
replaced with revised version Tue, 30 Jan 2024 16:37:52 GMT   (9049kb,D)

Title: SLANG: New Concept Comprehension of Large Language Models
Authors: Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.12585 ,  9049kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12973
replaced with revised version Tue, 30 Jan 2024 18:59:34 GMT   (2541kb,D)

Title: In-Context Language Learning: Architectures and Algorithms
Authors: Ekin Aky\"urek, Bailin Wang, Yoon Kim, Jacob Andreas
Categories: cs.CL cs.LG
Comments: Fixes a typo in the title, and adds additional references
\\ ( https://arxiv.org/abs/2401.12973 ,  2541kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14360
replaced with revised version Mon, 29 Jan 2024 19:22:20 GMT   (7304kb,D)

Title: A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis
  on Noisy Bangla Texts
Authors: Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir
  Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar
Categories: cs.CL
Comments: Accepted in The 9th Workshop on Noisy and User-generated Text
  (W-NUT), 18th Conference of the European Chapter of the Association for
  Computational Linguistics (EACL 2024)
MSC-class: 68T50 (Primary)
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2401.14360 ,  7304kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14698
replaced with revised version Tue, 30 Jan 2024 05:36:06 GMT   (5757kb,D)

Title: Under the Surface: Tracking the Artifactuality of LLM-Generated Data
Authors: Debarati Das, Karin De Langis, Anna Martin-Boyle, Jaehyung Kim, Minhwa
  Lee, Zae Myung Kim, Shirley Anugrah Hayati, Risako Owan, Bin Hu, Ritik
  Parkar, Ryan Koo, Jonginn Park, Aahan Tyagi, Libby Ferland, Sanjali Roy,
  Vincent Liu, and Dongyeop Kang
Categories: cs.CL cs.AI
Comments: Core Authors: Debarati Das, Karin De Langis, Anna Martin-Boyle,
  Jaehyung Kim, Minhwa Lee and Zae Myung Kim | Project lead : Debarati Das | PI
  : Dongyeop Kang
\\ ( https://arxiv.org/abs/2401.14698 ,  5757kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15328
replaced with revised version Tue, 30 Jan 2024 06:08:33 GMT   (154kb,D)

Title: Equipping Language Models with Tool Use Capability for Tabular Data
  Analysis in Finance
Authors: Adrian Theuma and Ehsan Shareghi
Categories: cs.CL
Comments: Accepted to EACL2024; code, model and dataset are available at
  https://raven-lm.github.io
\\ ( https://arxiv.org/abs/2401.15328 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15378
replaced with revised version Tue, 30 Jan 2024 05:36:32 GMT   (524kb)

Title: A RAG-based Question Answering System Proposal for Understanding Islam:
  MufassirQAS LLM
Authors: Ahmet Yusuf Alan, Enis Karaarslan, Omer Aydin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.15378 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2204.02521
replaced with revised version Tue, 30 Jan 2024 06:18:38 GMT   (1499kb)

Title: Optimal service resource management strategy for IoT-based health
  information system considering value co-creation of users
Authors: Ji Fang, Vincent CS Lee, Haiyan Wang
Categories: cs.LG math.OC
Comments: Fang, J., Lee, V.C.S. and Wang, H. (2024), "Optimal service resource
  management strategy for IoT-based health information system considering value
  co-creation of users", Industrial Management & Data Systems, Vol.
  ahead-of-print No. ahead-of-print. https://doi.org/10.1108/IMDS-03-2023-0173
DOI: 10.1108/IMDS-03-2023-0173
\\ ( https://arxiv.org/abs/2204.02521 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2205.01362
replaced with revised version Tue, 30 Jan 2024 13:08:40 GMT   (115kb)

Title: TracInAD: Measuring Influence for Anomaly Detection
Authors: Hugo Thimonier, Fabrice Popineau, Arpad Rimmel, Bich-Li\^en Doan and
  Fabrice Daniel
Categories: cs.LG
Journal-ref: 2022 International Joint Conference on Neural Networks (IJCNN)
DOI: 10.1109/IJCNN55064.2022.9892058
\\ ( https://arxiv.org/abs/2205.01362 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2212.10789
replaced with revised version Mon, 29 Jan 2024 22:09:49 GMT   (9001kb,D)

Title: Multi-modal Molecule Structure-text Model for Text-based Retrieval and
  Editing
Authors: Shengchao Liu, Weili Nie, Chengpeng Wang, Jiarui Lu, Zhuoran Qiao,
  Ling Liu, Jian Tang, Chaowei Xiao, Anima Anandkumar
Categories: cs.LG cs.CL q-bio.QM stat.ML
\\ ( https://arxiv.org/abs/2212.10789 ,  9001kb)
------------------------------------------------------------------------------
\\
arXiv:2302.00736
replaced with revised version Tue, 30 Jan 2024 10:34:35 GMT   (1337kb,D)

Title: Approximating the Shapley Value without Marginal Contributions
Authors: Patrick Kolpaczki, Viktor Bengs, Maximilian Muschalik, Eyke
  H\"ullermeier
Categories: cs.LG cs.GT
\\ ( https://arxiv.org/abs/2302.00736 ,  1337kb)
------------------------------------------------------------------------------
\\
arXiv:2303.14623
replaced with revised version Mon, 29 Jan 2024 19:18:42 GMT   (910kb,D)

Title: Inverse Reinforcement Learning without Reinforcement Learning
Authors: Gokul Swamy, Sanjiban Choudhury, J. Andrew Bagnell, Zhiwei Steven Wu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.14623 ,  910kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16376
replaced with revised version Mon, 29 Jan 2024 23:45:17 GMT   (1560kb,D)

Title: A Unified Learning Model for Estimating Fiber Orientation Distribution
  Functions on Heterogeneous Multi-shell Diffusion-weighted MRI
Authors: Tianyuan Yao, Nancy Newlin, Praitayini Kanakaraj, Vishwesh nath, Leon
  Y Cai, Karthik Ramadass, Kurt Schilling, Bennett A. Landman, Yuankai Huo
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.16376 ,  1560kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17043
replaced with revised version Tue, 30 Jan 2024 00:43:40 GMT   (2447kb,D)

Title: Federated Learning for Heterogeneous Bandits with Unobserved Contexts
Authors: Jiabin Lin and Shana Moothedath
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2303.17043 ,  2447kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03923
replaced with revised version Tue, 30 Jan 2024 12:24:42 GMT   (986kb,D)

Title: Active Continual Learning: On Balancing Knowledge Retention and
  Learnability
Authors: Thuy-Trang Vu, Shahram Khadivi, Mahsa Ghorbanali, Dinh Phung and
  Gholamreza Haffari
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2305.03923 ,  986kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13795
replaced with revised version Mon, 29 Jan 2024 20:05:18 GMT   (5180kb,D)

Title: Proximal Policy Gradient Arborescence for Quality Diversity
  Reinforcement Learning
Authors: Sumeet Batra, Bryon Tjanaka, Matthew C. Fontaine, Aleksei Petrenko,
  Stefanos Nikolaidis, Gaurav Sukhatme
Categories: cs.LG cs.AI
Comments: Accepted as a spotlight paper at ICLR 2024
\\ ( https://arxiv.org/abs/2305.13795 ,  5180kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16284
replaced with revised version Tue, 30 Jan 2024 02:02:06 GMT   (593kb,D)

Title: DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent
  Method
Authors: Ahmed Khaled and Konstantin Mishchenko and Chi Jin
Categories: cs.LG math.OC stat.ML
Comments: 22 pages, 1 table, 4 figures
\\ ( https://arxiv.org/abs/2305.16284 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01685
replaced with revised version Tue, 30 Jan 2024 06:47:17 GMT   (3918kb,D)

Title: MKOR: Momentum-Enabled Kronecker-Factor-Based Optimizer Using Rank-1
  Updates
Authors: Mohammad Mozaffari, Sikan Li, Zhao Zhang, Maryam Mehri Dehnavi
Categories: cs.LG cs.AI cs.CV math.OC
Comments: Published at 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2306.01685 ,  3918kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07735
replaced with revised version Tue, 30 Jan 2024 14:46:57 GMT   (1766kb,D)

Title: Discrete Graph Auto-Encoder
Authors: Yoann Boget, Magda Gregorova, Alexandros Kalousis
Categories: cs.LG
Comments: Thoroughly revised the paper originally titled "Vector-Quantized
  Graph Auto-Encoder. Implemented comprehensive modifications across all
  sections. Incorporated additional experiments to enhance the study.
  Maintained the fundamental structure and essence of the original work,
  ensuring it remains a continuation of the same project
\\ ( https://arxiv.org/abs/2306.07735 ,  1766kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11560
replaced with revised version Tue, 30 Jan 2024 12:55:08 GMT   (950kb,D)

Title: MILD: Modeling the Instance Learning Dynamics for Learning with Noisy
  Labels
Authors: Chuanyang Hu, Shipeng Yan, Zhitong Gao, Xuming He
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2306.11560 ,  950kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11714
replaced with revised version Tue, 30 Jan 2024 16:24:51 GMT   (43kb)

Title: Convergence of SGD for Training Neural Networks with Sliced Wasserstein
  Losses
Authors: Eloi Tanguy
Categories: cs.LG math.OC math.PR
\\ ( https://arxiv.org/abs/2307.11714 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14355
replaced with revised version Tue, 30 Jan 2024 13:27:39 GMT   (10942kb,D)

Title: TransGNN: Harnessing the Collaborative Power of Transformers and Graph
  Neural Networks for Recommender Systems
Authors: Peiyan Zhang, Yuchen Yan, Chaozhuo Li, Senzhang Wang, Xing Xie,
  Sunghun Kim
Categories: cs.LG cs.IR
ACM-class: H.3.3
\\ ( https://arxiv.org/abs/2308.14355 ,  10942kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00203
replaced with revised version Tue, 30 Jan 2024 02:55:08 GMT   (367kb,D)

Title: Data-Driven Projection for Reducing Dimensionality of Linear Programs:
  Generalization Bound and Learning Methods
Authors: Shinsaku Sakaue, Taihei Oki
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.00203 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01745
replaced with revised version Mon, 29 Jan 2024 19:01:02 GMT   (16376kb,D)

Title: Benchmarking Autoregressive Conditional Diffusion Models for Turbulent
  Flow Simulation
Authors: Georg Kohl, Li-Wei Chen, Nils Thuerey
Categories: cs.LG physics.flu-dyn
Comments: Source code available at
  https://github.com/tum-pbs/autoreg-pde-diffusion and further information and
  videos at https://ge.in.tum.de/publications/2023-acdm-kohl
\\ ( https://arxiv.org/abs/2309.01745 ,  16376kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02351
replaced with revised version Mon, 29 Jan 2024 20:33:48 GMT   (2098kb,D)

Title: Exact Inference for Continuous-Time Gaussian Process Dynamics
Authors: Katharina Ensinger, Nicholas Tagliapietra, Sebastian Ziesche,
  Sebastian Trimpe
Categories: cs.LG stat.ML
Comments: Accepted at The 38th Annual AAAI Conference on Artificial
  Intelligence. 2024
\\ ( https://arxiv.org/abs/2309.02351 ,  2098kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02873
replaced with revised version Mon, 29 Jan 2024 20:25:52 GMT   (3117kb,D)

Title: Learning Hybrid Dynamics Models With Simulator-Informed Latent States
Authors: Katharina Ensinger, Sebastian Ziesche, Sebastian Trimpe
Categories: cs.LG
Comments: Accepted at The 38th Annual AAAI Conference on Artificial
  Intelligence, 2024
\\ ( https://arxiv.org/abs/2309.02873 ,  3117kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04284
replaced with revised version Tue, 30 Jan 2024 07:31:41 GMT   (446kb,D)

Title: Viewing the process of generating counterfactuals as a source of
  knowledge
Authors: Vincent Lemaire, Nathan Le Boudec, Victor Guyomard and Fran\c{c}oise
  Fessant
Categories: cs.LG
Comments: 8 pages
\\ ( https://arxiv.org/abs/2309.04284 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17401
replaced with revised version Tue, 30 Jan 2024 16:24:01 GMT   (1461kb,D)

Title: Adversarial Machine Learning in Latent Representations of Neural
  Networks
Authors: Milin Zhang, Mohammad Abdi and Francesco Restuccia
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.17401 ,  1461kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00489
replaced with revised version Tue, 30 Jan 2024 16:31:21 GMT   (3223kb,D)

Title: Interpretable Imitation Learning with Dynamic Causal Relations
Authors: Tianxiang Zhao, Wenchao Yu, Suhang Wang, Lu Wang, Xiang Zhang, Yuncong
  Chen, Yanchi Liu, Wei Cheng, Haifeng Chen
Categories: cs.LG
Comments: Accepted by WSDM 2024 as an oral paper
DOI: 10.1145/3616855.3635827
\\ ( https://arxiv.org/abs/2310.00489 ,  3223kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04987
replaced with revised version Tue, 30 Jan 2024 10:05:11 GMT   (152kb,D)

Title: Data-centric Graph Learning: A Survey
Authors: Yuxin Guo, Deyu Bo, Cheng Yang, Zhiyuan Lu, Zhongjian Zhang, Jixi Liu,
  Yufei Peng, Chuan Shi
Categories: cs.LG cs.SI
Comments: 20 pages
\\ ( https://arxiv.org/abs/2310.04987 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10012
replaced with revised version Tue, 30 Jan 2024 04:52:01 GMT   (7735kb,D)

Title: Ring-A-Bell! How Reliable are Concept Removal Methods for Diffusion
  Models?
Authors: Yu-Lin Tsai, Chia-Yi Hsu, Chulin Xie, Chih-Hsun Lin, Jia-You Chen, Bo
  Li, Pin-Yu Chen, Chia-Mu Yu, Chun-Ying Huang
Categories: cs.LG
Comments: This paper has already been accepted by ICLR 2024. The final version
  will be uploaded after the camera-ready submission
\\ ( https://arxiv.org/abs/2310.10012 ,  7735kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13139
replaced with revised version Tue, 30 Jan 2024 15:58:30 GMT   (29kb)

Title: Graph Neural Networks with polynomial activations have limited
  expressivity
Authors: Sammy Khalife
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.13139 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14336
replaced with revised version Tue, 30 Jan 2024 03:21:30 GMT   (4794kb,D)

Title: Learning Interpretable Rules for Scalable Data Representation and
  Classification
Authors: Zhuo Wang, Wei Zhang, Ning Liu, Jianyong Wang
Categories: cs.LG cs.AI
Comments: Accepted by IEEE TPAMI in October 2023; Interpretable ML;
  Neuro-Symbolic AI; Preliminary conference version (NeurIPS 2021) available at
  arXiv:2109.15103
Journal-ref: IEEE Transactions on Pattern Analysis and Machine Intelligence (
  Volume: 46, Issue: 2, February 2024)
DOI: 10.1109/TPAMI.2023.3328881
\\ ( https://arxiv.org/abs/2310.14336 ,  4794kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14526
replaced with revised version Tue, 30 Jan 2024 02:35:51 GMT   (601kb,D)

Title: Towards a Pretrained Model for Restless Bandits via Multi-arm
  Generalization
Authors: Yunfan Zhao, Nikhil Behari, Edward Hughes, Edwin Zhang, Dheeraj
  Nagaraj, Karl Tuyls, Aparna Taneja, Milind Tambe
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.14526 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02771
replaced with revised version Tue, 30 Jan 2024 12:34:15 GMT   (16602kb,D)

Title: Powerformer: A Section-adaptive Transformer for Power Flow Adjustment
Authors: Kaixuan Chen and Wei Luo and Shunyu Liu and Yaoquan Wei and Yihe Zhou
  and Yunpeng Qing and Quan Zhang and Jie Song and Mingli Song
Categories: cs.LG cs.SY eess.SY
Comments: 8 figures
\\ ( https://arxiv.org/abs/2401.02771 ,  16602kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09071
replaced with revised version Tue, 30 Jan 2024 14:01:18 GMT   (400kb,D)

Title: Rethinking Spectral Graph Neural Networks with Spatially Adaptive
  Filtering
Authors: Jingwei Guo, Kaizhu Huang, Xinping Yi, Zixian Su, and Rui Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.09071 ,  400kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10337
replaced with revised version Tue, 30 Jan 2024 11:40:07 GMT   (933kb,D)

Title: Noise Contrastive Estimation-based Matching Framework for Low-Resource
  Security Attack Pattern Recognition
Authors: Tu Nguyen, Nedim \v{S}rndi\'c, Alexander Neth
Categories: cs.LG cs.AI cs.CL cs.CR
Comments: accepted at EACL 2024, in ARR October 2023
\\ ( https://arxiv.org/abs/2401.10337 ,  933kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12007
replaced with revised version Tue, 30 Jan 2024 03:10:15 GMT   (606kb,D)

Title: Tensor-view Topological Graph Neural Network
Authors: Tao Wen, Elynn Chen, Yuzhou Chen
Categories: cs.LG cs.AI
Comments: Accepted at AISTATS 2024
\\ ( https://arxiv.org/abs/2401.12007 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12648
replaced with revised version Tue, 30 Jan 2024 12:04:30 GMT   (1311kb,D)

Title: Consistency Enhancement-Based Deep Multiview Clustering via Contrastive
  Learning
Authors: Hao Yang, Hua Mao, Wai Lok Woo, Jie Chen and Xi Peng
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2401.12648 ,  1311kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14424
replaced with revised version Tue, 30 Jan 2024 09:27:21 GMT   (1612kb,D)

Title: Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo
  Tree Search
Authors: Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Meilan
  Hao, Shu Wei, Yusong Deng
Categories: cs.LG cs.AI
Comments: 24 pages
\\ ( https://arxiv.org/abs/2401.14424 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15935
replaced with revised version Tue, 30 Jan 2024 07:54:24 GMT   (6131kb,D)

Title: Self-Supervised Learning in Event Sequences: A Comparative Study and
  Hybrid Approach of Generative Modeling and Contrastive Learning
Authors: Viktor Moskvoretskii, Dmitry Osin, Egor Shvetsov, Igor Udovichenko,
  Maxim Zhelnin, Andrey Dukhovny, Anna Zhimerikina, Albert Efimov, Evgeny
  Burnaev
Categories: cs.LG cs.AI
Comments: 11 pages, 9 figures
\\ ( https://arxiv.org/abs/2401.15935 ,  6131kb)
------------------------------------------------------------------------------
\\
arXiv:2208.06233
replaced with revised version Tue, 30 Jan 2024 09:28:35 GMT   (6610kb,D)

Title: Dynamic Sensor Matching based on Geomagnetic Inertial Navigation
Authors: Simone M\"uller and Dieter Kranzlm\"uller
Categories: cs.RO cs.AI cs.CV
Comments: Page 16-25
Journal-ref: Journal of WSCG, 2022, Vol.30., No.1-2, ISSN 1213-6972
DOI: 10.24132/JWSCG.2022.3
\\ ( https://arxiv.org/abs/2208.06233 ,  6610kb)
------------------------------------------------------------------------------
\\
arXiv:2210.14991
replaced with revised version Mon, 29 Jan 2024 21:25:02 GMT   (2601kb,D)

Title: Reachability Verification Based Reliability Assessment for Deep
  Reinforcement Learning Controlled Robotics and Autonomous Systems
Authors: Yi Dong, Xingyu Zhao, Sen Wang, Xiaowei Huang
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2210.14991 ,  2601kb)
------------------------------------------------------------------------------
\\
arXiv:2303.18136
replaced with revised version Tue, 30 Jan 2024 08:46:38 GMT   (1393kb,D)

Title: Machine-learned Adversarial Attacks against Fault Prediction Systems in
  Smart Electrical Grids
Authors: Carmelo Ardito, Yashar Deldjoo, Tommaso Di Noia, Eugenio Di Sciascio,
  Fatemeh Nazary, Giovanni Servedio
Categories: cs.CR cs.AI cs.LG
Comments: Accepted in AdvML@KDD'22
\\ ( https://arxiv.org/abs/2303.18136 ,  1393kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05284
replaced with revised version Tue, 30 Jan 2024 04:49:16 GMT   (1304kb,D)

Title: Simple and Controllable Music Generation
Authors: Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel
  Synnaeve, Yossi Adi, Alexandre D\'efossez
Categories: cs.SD cs.AI cs.LG eess.AS
Comments: Published at Neurips 2023
\\ ( https://arxiv.org/abs/2306.05284 ,  1304kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09267
replaced with revised version Tue, 30 Jan 2024 18:15:47 GMT   (591kb)

Title: Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?
Authors: Dimitrios Ioannidis, Jeremy Kepner, Andrew Bowne, Harriet S. Bryant
Categories: cs.CY cs.AI cs.DL cs.LG cs.SE
Comments: 40 pages, 100+ references, to appear in Fordham Law Review
\\ ( https://arxiv.org/abs/2306.09267 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05697
replaced with revised version Tue, 30 Jan 2024 16:30:43 GMT   (287kb,D)

Title: SSLRec: A Self-Supervised Learning Framework for Recommendation
Authors: Xubin Ren, Lianghao Xia, Yuhao Yang, Wei Wei, Tianle Wang, Xuheng Cai
  and Chao Huang
Categories: cs.IR cs.AI
Comments: Published as a WSDM'24 full paper (oral presentation)
DOI: 10.1145/3616855.3635814
\\ ( https://arxiv.org/abs/2308.05697 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15427
replaced with revised version Tue, 30 Jan 2024 03:44:02 GMT   (485kb,D)

Title: Complementing Onboard Sensors with Satellite Map: A New Perspective for
  HD Map Construction
Authors: Wenjie Gao, Jiawei Fu, Yanqing Shen, Haodong Jing, Shitao Chen,
  Nanning Zheng
Categories: cs.CV cs.AI
Comments: Accepted by ICRA 2024
\\ ( https://arxiv.org/abs/2308.15427 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16505
replaced with revised version Tue, 30 Jan 2024 03:17:26 GMT   (4096kb,D)

Title: Recommender AI Agent: Integrating Large Language Models for Interactive
  Recommendations
Authors: Xu Huang, Jianxun Lian, Yuxuan Lei, Jing Yao, Defu Lian, Xing Xie
Categories: cs.IR cs.AI
Comments: 18 pages, 17 figures, 7 tables
\\ ( https://arxiv.org/abs/2308.16505 ,  4096kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01750 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 17:59:18 GMT   (17kb)

Title: On CNF formulas irredundant with respect to unit clause propagation
Authors: Petr Savick\'y
Categories: math.CO cs.AI
Comments: 19 pages, improvements for better readability
\\ ( https://arxiv.org/abs/2309.01750 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02169
replaced with revised version Tue, 30 Jan 2024 16:02:41 GMT   (0kb,I)

Title: Dual Relation Alignment for Composed Image Retrieval
Authors: Xintong Jiang, Yaxiong Wang, Yujiao Wu, Meng Wang, Xueming Qian
Categories: cs.CV cs.AI
Comments: there are method changes in our model, hence the new architecture and
  experiments differs from the original one, which is the reason for withdraw
\\ ( https://arxiv.org/abs/2309.02169 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12570
replaced with revised version Tue, 30 Jan 2024 15:56:47 GMT   (5851kb,D)

Title: Creativity Support in the Age of Large Language Models: An Empirical
  Study Involving Emerging Writers
Authors: Tuhin Chakrabarty, Vishakh Padmakumar, Faeze Brahman, Smaranda Muresan
Categories: cs.HC cs.AI cs.CL cs.CY
\\ ( https://arxiv.org/abs/2309.12570 ,  5851kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14720
replaced with revised version Tue, 30 Jan 2024 05:11:59 GMT   (961kb,D)

Title: Perceptions and Detection of AI Use in Manuscript Preparation for
  Academic Journals
Authors: Nir Chemaya and Daniel Martin
Categories: cs.CY cs.AI econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2311.14720 ,  961kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00506
replaced with revised version Tue, 30 Jan 2024 14:19:33 GMT   (11119kb)

Title: Generative AI enhances individual creativity but reduces the collective
  diversity of novel content
Authors: Anil R. Doshi and Oliver P. Hauser
Categories: cs.HC cs.AI econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2312.00506 ,  11119kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15964
replaced with revised version Tue, 30 Jan 2024 04:55:29 GMT   (0kb,I)

Title: Semantic Guidance Tuning for Text-To-Image Diffusion Models
Authors: Hyun Kang, Dohae Lee, Myungjin Shin, In-Kwon Lee
Categories: cs.CV cs.AI
Comments: Rework is being done
\\ ( https://arxiv.org/abs/2312.15964 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17163
replaced with revised version Mon, 29 Jan 2024 21:30:33 GMT   (6510kb,D)

Title: FENet: Focusing Enhanced Network for Lane Detection
Authors: Liman Wang, Hanyang Zhong
Categories: cs.CV cs.AI
Comments: 12 pages including appendix. The Code is available at
  https://github.com/HanyangZhong/FENet
\\ ( https://arxiv.org/abs/2312.17163 ,  6510kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02383
replaced with revised version Mon, 29 Jan 2024 21:32:39 GMT   (2461kb)

Title: Survey of 3D Human Body Pose and Shape Estimation Methods for
  Contemporary Dance Applications
Authors: Darshan Venkatrayappa, Alain Tremeau, Damien Muselet, Philippe
  Colantoni
Categories: cs.CV cs.AI
Comments: arXiv admin note: text overlap with arXiv:2008.09062 by other authors
\\ ( https://arxiv.org/abs/2401.02383 ,  2461kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04478 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 09:29:47 GMT   (484kb,D)

Title: TwinBooster: Synergising Large Language Models with Barlow Twins and
  Gradient Boosting for Enhanced Molecular Property Prediction
Authors: Maximilian G. Schuh, Davide Boldini, Stephan A. Sieber
Categories: q-bio.BM cs.AI cs.CL cs.LG
Comments: 13(+9) pages(+appendix), 5 figures, 11 tables
\\ ( https://arxiv.org/abs/2401.04478 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05925
replaced with revised version Tue, 30 Jan 2024 12:46:04 GMT   (16268kb,D)

Title: CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with
  Dual Feature Fusion
Authors: Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan
Categories: cs.CV cs.AI
Comments: 9 pages, 8 figures, correct writing details
\\ ( https://arxiv.org/abs/2401.05925 ,  16268kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10746 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 16:32:14 GMT   (405kb,D)

Title: A Systematic Evaluation of Euclidean Alignment with Deep Learning for
  EEG Decoding
Authors: Bruna Junqueira, Bruno Aristimunha, Sylvain Chevallier, Raphael Y. de
  Camargo
Categories: eess.SP cs.AI cs.LG
Comments: 14 pages and 10 figures
ACM-class: I.5.1; I.6.3; I.2.6
\\ ( https://arxiv.org/abs/2401.10746 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13802
replaced with revised version Tue, 30 Jan 2024 06:10:29 GMT   (12329kb,D)

Title: Investigating the Efficacy of Large Language Models for Code Clone
  Detection
Authors: Mohamad Khajezade, Jie JW Wu, Fatemeh Hendijani Fard, Gema
  Rodr\'iguez-P\'erez, Mohamed Sami Shehata
Categories: cs.SE cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.13802 ,  12329kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14066
replaced with revised version Tue, 30 Jan 2024 05:58:09 GMT   (23495kb,D)

Title: CreativeSynth: Creative Blending and Synthesis of Visual Arts based on
  Multimodal Diffusion
Authors: Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li,
  Chongyang Ma, Xiu Li, Changsheng Xu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.14066 ,  23495kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15803
replaced with revised version Tue, 30 Jan 2024 15:57:22 GMT   (20319kb,D)

Title: GarchingSim: An Autonomous Driving Simulator with Photorealistic Scenes
  and Minimalist Workflow
Authors: Liguo Zhou, Yinglei Song, Yichao Gao, Zhou Yu, Michael Sodamin,
  Hongshen Liu, Liang Ma, Lian Liu, Hao Liu, Yang Liu, Haichuan Li, Guang Chen,
  Alois Knoll
Categories: cs.RO cs.AI cs.CV cs.SY eess.SY
\\ ( https://arxiv.org/abs/2401.15803 ,  20319kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16251
replaced with revised version Tue, 30 Jan 2024 04:57:20 GMT   (2891kb,D)

Title: Cross-silo Federated Learning with Record-level Personalized
  Differential Privacy
Authors: Junxu Liu, Jian Lou, Li Xiong, Jinfei Liu, Xiaofeng Meng
Categories: cs.CR cs.AI cs.LG
Comments: 12 pages, 7 figures, under review
\\ ( https://arxiv.org/abs/2401.16251 ,  2891kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00281
replaced with revised version Mon, 29 Jan 2024 22:41:48 GMT   (7307kb,D)

Title: Localization vs. Semantics: Visual Representations in Unimodal and
  Multimodal Models
Authors: Zhuowan Li, Cihang Xie, Benjamin Van Durme, Alan Yuille
Categories: cs.CV cs.CL
Comments: Accepted to EACL 2024. Code is released at
  https://github.com/Lizw14/visual_probing
\\ ( https://arxiv.org/abs/2212.00281 ,  7307kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09649
replaced with revised version Tue, 30 Jan 2024 06:31:56 GMT   (16510kb,D)

Title: ReactGenie: A Development Framework for Complex Multimodal Interactions
  Using Large Language Models
Authors: Jackie Junrui Yang, Yingtian Shi, Yuhan Zhang, Karina Li, Daniel Wan
  Rosli, Anisha Jain, Shuning Zhang, Tianshi Li, James A. Landay, Monica S. Lam
Categories: cs.HC cs.CL
\\ ( https://arxiv.org/abs/2306.09649 ,  16510kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17972
replaced with revised version Tue, 30 Jan 2024 16:19:48 GMT   (171kb,D)

Title: Self-Infilling Code Generation
Authors: Lin Zheng, Jianbo Yuan, Zhi Zhang, Hongxia Yang, Lingpeng Kong
Categories: cs.PL cs.CL cs.LG
Comments: Code available at https://github.com/LZhengisme/self-infilling
\\ ( https://arxiv.org/abs/2311.17972 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08676
replaced with revised version Tue, 30 Jan 2024 14:11:29 GMT   (1242kb,D)

Title: SEF-VC: Speaker Embedding Free Zero-Shot Voice Conversion with Cross
  Attention
Authors: Junjie Li, Yiwei Guo, Xie Chen, Kai Yu
Categories: cs.SD cs.CL eess.AS
Comments: 5 pages, 2 figures, accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2312.08676 ,  1242kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06071
replaced with revised version Tue, 30 Jan 2024 12:08:21 GMT   (4081kb,D)

Title: GroundingGPT:Language Enhanced Multi-modal Grounding Model
Authors: Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou,
  Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2401.06071 ,  4081kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10893
replaced with revised version Tue, 30 Jan 2024 03:14:11 GMT   (563kb)

Title: Location Sensitive Embedding for Knowledge Graph Reasoning
Authors: Deepak Banerjee, Anjali Ishaan
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2401.10893 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2211.14297 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 03:16:53 GMT   (85kb)

Title: Doubly robust nearest neighbors in factor models
Authors: Raaz Dwivedi, Katherine Tian, Sabina Tomkins, Predrag Klasnja, Susan
  Murphy, Devavrat Shah
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2211.14297 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05369 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 14:30:19 GMT   (177kb,D)

Title: Data-dependent Generalization Bounds via Variable-Size Compressibility
Authors: Milad Sefidgaran and Abdellatif Zaidi
Categories: stat.ML cs.IT cs.LG math.IT
\\ ( https://arxiv.org/abs/2303.05369 ,  177kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09157 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 23:56:47 GMT   (7828kb,D)

Title: Neural networks for geospatial data
Authors: Wentao Zhan, Abhirup Datta
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2304.09157 ,  7828kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14176 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 11:54:34 GMT   (3468kb,D)

Title: Exploring the flavor structure of quarks and leptons with reinforcement
  learning
Authors: Satsuki Nishimura, Coh Miyao, Hajime Otsuka
Categories: hep-ph cs.LG hep-th
Comments: 45 pages, 15 figures, v3: published version
Report-no: KYUSHU-HET-257
Journal-ref: J. High Energ. Phys. 2023, 21 (2023)
DOI: 10.1007/JHEP12(2023)021
\\ ( https://arxiv.org/abs/2304.14176 ,  3468kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14604 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 05:14:02 GMT   (3688kb,D)

Title: Deep Neural-network Prior for Orbit Recovery from Method of Moments
Authors: Yuehaw Khoo, Sounak Paul and Nir Sharon
Categories: stat.ME cs.CV cs.LG cs.NA math.NA
Journal-ref: J. Comput. Appl. Math. 115782 (2024)
DOI: 10.1016/j.cam.2024.115782
\\ ( https://arxiv.org/abs/2304.14604 ,  3688kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06272
replaced with revised version Tue, 30 Jan 2024 16:32:48 GMT   (328kb,D)

Title: FedPDD: A Privacy-preserving Double Distillation Framework for
  Cross-silo Federated Recommendation
Authors: Sheng Wan, Dashan Gao, Hanlin Gu, Daning Hu
Categories: cs.IR cs.CR cs.DC cs.LG
Comments: Accepted by IJCNN2023
\\ ( https://arxiv.org/abs/2305.06272 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15357 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 05:12:01 GMT   (1325kb,D)

Title: Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image
  Super-Resolution
Authors: Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2305.15357 ,  1325kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01433 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 15:40:06 GMT   (6135kb,D)

Title: Blind Audio Bandwidth Extension: A Diffusion-Based Zero-Shot Approach
Authors: Eloi Moliner, Filip Elvander, Vesa V\"alim\"aki
Categories: eess.AS cs.LG cs.SD
Comments: Submitted to IEEE/ACM Transactions on Audio, Speech and Language
  Processing
\\ ( https://arxiv.org/abs/2306.01433 ,  6135kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00238 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 02:56:59 GMT   (5017kb,D)

Title: Unified Transfer Learning Models in High-Dimensional Linear Regression
Authors: Shuo Shuo Liu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2307.00238 ,  5017kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00673 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 13:21:24 GMT   (3172kb,D)

Title: ENN: A Neural Network with DCT Adaptive Activation Functions
Authors: Marc Martinez-Gost, Ana P\'erez-Neira, Miguel \'Angel Lagunas
Categories: eess.SP cs.LG cs.NE
Comments: Paper accepted in IEEE Journal of Selected Topics in Signal
  Processing (JSTSP) Special Series on AI in Signal & Data Science - Toward
  Explainable, Reliable, and Sustainable Machine Learning
\\ ( https://arxiv.org/abs/2307.00673 ,  3172kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11515
replaced with revised version Tue, 30 Jan 2024 03:03:39 GMT   (297kb,D)

Title: Towards Differential Privacy in Sequential Recommendation: A Noisy Graph
  Neural Network Approach
Authors: Wentao Hu, Hui Fang
Categories: cs.CR cs.IR cs.LG
Comments: Accepted by ACM Transactions on Knowledge Discovery from Data
\\ ( https://arxiv.org/abs/2309.11515 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12813
replaced with revised version Tue, 30 Jan 2024 04:20:27 GMT   (144kb,D)

Title: Automatically Testing Functional Properties of Code Translation Models
Authors: Hasan Ferit Eniser, Valentin W\"ustholz, Maria Christakis
Categories: cs.SE cs.LG cs.PL
Comments: 13 pages including appendix and references
\\ ( https://arxiv.org/abs/2309.12813 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10434 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 11:10:00 GMT   (4297kb,D)

Title: Equivariant Matrix Function Neural Networks
Authors: Ilyes Batatia, Lars L. Schaaf, Huajie Chen, G\'abor Cs\'anyi,
  Christoph Ortner, Felix A. Faber
Categories: stat.ML cond-mat.mtrl-sci cs.LG physics.chem-ph
Comments: International Conference on Learning Representations, 2024
\\ ( https://arxiv.org/abs/2310.10434 ,  4297kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18897 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 18:35:26 GMT   (36121kb,D)

Title: Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary
  Differential Equations for Compressible Navier--Stokes Equations
Authors: Shinhoo Kang, Emil M. Constantinescu
Categories: physics.flu-dyn cs.LG cs.NA math.NA
Comments: 17 figures, 2 tables, 27 pages
MSC-class: 68T07, 76M10
\\ ( https://arxiv.org/abs/2310.18897 ,  36121kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14057
replaced with revised version Tue, 30 Jan 2024 17:52:23 GMT   (300kb,D)

Title: Weighted least-squares approximation with determinantal point processes
  and generalized volume sampling
Authors: Anthony Nouy and Bertrand Michel
Categories: math.NA cs.LG cs.NA math.ST stat.TH
Comments: In this revised version, conjecture (13) on DPP and (16) on volume
  sampling have been modified, including a convexity requirement. Proofs of
  propositions 5.4 and 5.12 have been modified accordingly. Remarks 5.5 and 5.6
  have been added to discuss alternatives to conjecture (13) on DPP
\\ ( https://arxiv.org/abs/2312.14057 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15282 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 15:53:01 GMT   (5331kb,D)

Title: Causal Forecasting for Pricing
Authors: Douglas Schultz, Johannes Stephan, Julian Sieber, Trudie Yeh, Manuel
  Kunz, Patrick Doupe, Tim Januschowski
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2312.15282 ,  5331kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15910
replaced with revised version Tue, 30 Jan 2024 06:39:00 GMT   (4943kb,D)

Title: Reinforcement Unlearning
Authors: Dayong Ye, Tianqing Zhu, Congcong Zhu, Derui Wang, Minhui Xue, Sheng
  Shen, Wanlei Zhou
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.15910 ,  4943kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01145 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 08:21:25 GMT   (2591kb,D)

Title: HAAQI-Net: A non-intrusive neural music quality assessment model for
  hearing aids
Authors: Dyah A. M. G. Wisnu, Epri Pratiwi, Stefano Rini, Ryandhimas E.
  Zezario, Hsin-Min Wang, Yu Tsao
Categories: eess.AS cs.LG cs.SD
\\ ( https://arxiv.org/abs/2401.01145 ,  2591kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02847
replaced with revised version Tue, 30 Jan 2024 10:32:16 GMT   (46322kb,D)

Title: Generating Non-Stationary Textures using Self-Rectification
Authors: Yang Zhou, Rongjun Xiao, Dani Lischinski, Daniel Cohen-Or, Hui Huang
Categories: cs.CV cs.GR cs.LG
Comments: Project page: https://github.com/xiaorongjun000/Self-Rectification
\\ ( https://arxiv.org/abs/2401.02847 ,  46322kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08876
replaced with revised version Tue, 30 Jan 2024 03:44:23 GMT   (5142kb,D)

Title: Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image
  Labeling
Authors: Dongping Zhang, Angelos Chatzimparmpas, Negar Kamali, and Jessica
  Hullman
Categories: cs.HC cs.CV cs.LG
Comments: 28 pages, 11 figures, 8 tables. Accepted by ACM CHI 2024
\\ ( https://arxiv.org/abs/2401.08876 ,  5142kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10107 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 08:53:00 GMT   (3513kb)

Title: Comparison analysis between standard polysomnographic data and
  in-ear-EEG signals: A preliminary study
Authors: Gianpaolo Palo, Luigi Fiorillo, Giuliana Monachino, Michal Bechny,
  Mark Melnykowycz, Athina Tzovara, Valentina Agostini, and Francesca Dalia
  Faraci
Categories: eess.SP cs.LG physics.med-ph
Comments: 12 figures, 1 table
\\ ( https://arxiv.org/abs/2401.10107 ,  3513kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14423
replaced with revised version Tue, 30 Jan 2024 07:06:21 GMT   (11883kb,D)

Title: Prompt Design and Engineering: Introduction and Advanced Methods
Authors: Xavier Amatriain
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2401.14423 ,  11883kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15139 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 17:57:12 GMT   (224kb,D)

Title: FDR-Controlled Portfolio Optimization for Sparse Financial Index
  Tracking
Authors: Jasin Machkour, Daniel P. Palomar, Michael Muma
Categories: q-fin.PM cs.LG stat.ME stat.ML
\\ ( https://arxiv.org/abs/2401.15139 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15771 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 05:01:03 GMT   (104kb,D)

Title: Bayesian Nonparametrics Meets Data-Driven Robust Optimization
Authors: Nicola Bariletto, Nhat Ho
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2401.15771 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16356 (*cross-listing*)
replaced with revised version Tue, 30 Jan 2024 09:56:48 GMT   (0kb,I)

Title: cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and
  Glitch Generation
Authors: Tom Dooney, Lyana Curier, Daniel Tan, Melissa Lopez, Chris Van Den
  Broeck, Stefano Bromuri
Categories: physics.ins-det cs.LG gr-qc
Comments: Paper to be internally reviewed within collaboration. It will be
  resubmitted after passing the internal rewiew
\\ ( https://arxiv.org/abs/2401.16356 ,  0kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
