Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月17日 20:18
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri 12 Jan 24 19:00:00 GMT  to  Tue 16 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.06781
Date: Thu, 4 Jan 2024 13:27:50 GMT   (2506kb,D)

Title: PokerGPT: An End-to-End Lightweight Solver for Multi-Player Texas
  Hold'em via Large Language Model
Authors: Chenghao Huang, Yanbo Cao, Yinlong Wen, Tao Zhou, Yanru Zhang
Categories: cs.AI cs.CL
\\
  Poker, also known as Texas Hold'em, has always been a typical research target
within imperfect information games (IIGs). IIGs have long served as a measure
of artificial intelligence (AI) development. Representative prior works, such
as DeepStack and Libratus heavily rely on counterfactual regret minimization
(CFR) to tackle heads-up no-limit Poker. However, it is challenging for
subsequent researchers to learn CFR from previous models and apply it to other
real-world applications due to the expensive computational cost of CFR
iterations. Additionally, CFR is difficult to apply to multi-player games due
to the exponential growth of the game tree size. In this work, we introduce
PokerGPT, an end-to-end solver for playing Texas Hold'em with arbitrary number
of players and gaining high win rates, established on a lightweight large
language model (LLM). PokerGPT only requires simple textual information of
Poker games for generating decision-making advice, thus guaranteeing the
convenient interaction between AI and humans. We mainly transform a set of
textual records acquired from real games into prompts, and use them to
fine-tune a lightweight pre-trained LLM using reinforcement learning human
feedback technique. To improve fine-tuning performance, we conduct prompt
engineering on raw data, including filtering useful information, selecting
behaviors of players with high win rates, and further processing them into
textual instruction using multiple prompt engineering techniques. Through the
experiments, we demonstrate that PokerGPT outperforms previous approaches in
terms of win rate, model size, training time, and response speed, indicating
the great potential of LLMs in solving IIGs.
\\ ( https://arxiv.org/abs/2401.06781 ,  2506kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06793
Date: Mon, 8 Jan 2024 09:28:55 GMT   (361kb)

Title: Greedy Algorithm for Inference of Decision Trees from Decision Rule
  Systems
Authors: Kerven Durdymyradov and Mikhail Moshkov
Categories: cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2305.01721,
  arXiv:2302.07063
\\
  Decision trees and decision rule systems play important roles as classifiers,
knowledge representation tools, and algorithms. They are easily interpretable
models for data analysis, making them widely used and studied in computer
science. Understanding the relationships between these two models is an
important task in this field. There are well-known methods for converting
decision trees into systems of decision rules. In this paper, we consider the
inverse transformation problem, which is not so simple. Instead of constructing
an entire decision tree, our study focuses on a greedy polynomial time
algorithm that simulates the operation of a decision tree on a given tuple of
attribute values.
\\ ( https://arxiv.org/abs/2401.06793 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06801
Date: Wed, 10 Jan 2024 05:32:20 GMT   (425kb)

Title: Graph-of-Thought: Utilizing Large Language Models to Solve Complex and
  Dynamic Business Problems
Authors: Ye Li
Categories: cs.AI
Comments: Keywords: Graph-of-Thought (GoT), Workflow Automation, Large Language
  Models (LLMs), Task Execution, Data-Driven Decision Making, Complexity
  Management
\\
  This paper presents Graph-of-Thought (GoT), a new model for workflow
automation that enhances the flexibility and efficiency of Large Language
Models (LLMs) in complex task execution. GoT advances beyond traditional linear
and tree-like cognitive models with a graph structure that enables dynamic path
selection. The open-source engine GoTFlow demonstrates the practical
application of GoT, facilitating automated, data-driven decision-making across
various domains. Despite challenges in complexity and transparency, GoTFlow's
potential for improving business processes is significant, promising
advancements in both efficiency and decision quality with continuous
development.
\\ ( https://arxiv.org/abs/2401.06801 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06810
Date: Thu, 11 Jan 2024 04:23:08 GMT   (899kb,D)

Title: TONE: A 3-Tiered ONtology for Emotion analysis
Authors: Srishti Gupta, Piyush Kumar Garg, Sourav Kumar Dandapat
Categories: cs.AI
\\
  Emotions have played an important part in many sectors, including psychology,
medicine, mental health, computer science, and so on, and categorizing them has
proven extremely useful in separating one emotion from another. Emotions can be
classified using the following two methods: (1) The supervised method's
efficiency is strongly dependent on the size and domain of the data collected.
A categorization established using relevant data from one domain may not work
well in another. (2) An unsupervised method that uses either domain expertise
or a knowledge base of emotion types already exists. Though this second
approach provides a suitable and generic categorization of emotions and is
cost-effective, the literature doesn't possess a publicly available knowledge
base that can be directly applied to any emotion categorization-related task.
This pushes us to create a knowledge base that can be used for emotion
classification across domains, and ontology is often used for this purpose. In
this study, we provide TONE, an emotion-based ontology that effectively creates
an emotional hierarchy based on Dr. Gerrod Parrot's group of emotions. In
addition to ontology development, we introduce a semi-automated vocabulary
construction process to generate a detailed collection of terms for emotions at
each tier of the hierarchy. We also demonstrate automated methods for
establishing three sorts of dependencies in order to develop linkages between
different emotions. Our human and automatic evaluation results show the
ontology's quality. Furthermore, we describe three distinct use cases that
demonstrate the applicability of our ontology.
\\ ( https://arxiv.org/abs/2401.06810 ,  899kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06925
Date: Fri, 12 Jan 2024 23:14:34 GMT   (32kb)

Title: Modeling Latent Selection with Structural Causal Models
Authors: Leihao Chen, Onno Zoeter, Joris M. Mooij
Categories: cs.AI cs.LG math.ST stat.ME stat.ML stat.TH
\\
  Selection bias is ubiquitous in real-world data, and can lead to misleading
results if not dealt with properly. We introduce a conditioning operation on
Structural Causal Models (SCMs) to model latent selection from a causal
perspective. We show that the conditioning operation transforms an SCM with the
presence of an explicit latent selection mechanism into an SCM without such
selection mechanism, which partially encodes the causal semantics of the
selected subpopulation according to the original SCM. Furthermore, we show that
this conditioning operation preserves the simplicity, acyclicity, and linearity
of SCMs, and commutes with marginalization. Thanks to these properties,
combined with marginalization and intervention, the conditioning operation
offers a valuable tool for conducting causal reasoning tasks within causal
models where latent details have been abstracted away. We demonstrate by
example how classical results of causal inference can be generalized to include
selection bias and how the conditioning operation helps with modeling of
real-world problems.
\\ ( https://arxiv.org/abs/2401.06925 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06979
Date: Sat, 13 Jan 2024 05:01:14 GMT   (3635kb)

Title: Distance-aware Attention Reshaping: Enhance Generalization of Neural
  Solver for Large-scale Vehicle Routing Problems
Authors: Yang Wang and Ya-Hui Jia and Wei-Neng Chen and Yi Mei
Categories: cs.AI cs.LG
\\
  Neural solvers based on attention mechanism have demonstrated remarkable
effectiveness in solving vehicle routing problems. However, in the
generalization process from small scale to large scale, we find a phenomenon of
the dispersion of attention scores in existing neural solvers, which leads to
poor performance. To address this issue, this paper proposes a distance-aware
attention reshaping method, assisting neural solvers in solving large-scale
vehicle routing problems. Specifically, without the need for additional
training, we utilize the Euclidean distance information between current nodes
to adjust attention scores. This enables a neural solver trained on small-scale
instances to make rational choices when solving a large-scale problem.
Experimental results show that the proposed method significantly outperforms
existing state-of-the-art neural solvers on the large-scale CVRPLib dataset.
\\ ( https://arxiv.org/abs/2401.06979 ,  3635kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07056
Date: Sat, 13 Jan 2024 12:09:49 GMT   (9251kb,D)

Title: Aquarium: A Comprehensive Framework for Exploring Predator-Prey Dynamics
  through Multi-Agent Reinforcement Learning Algorithms
Authors: Michael K\"olle, Yannick Erpelding, Fabian Ritz, Thomy Phan, Steffen
  Illium and Claudia Linnhoff-Popien
Categories: cs.AI cs.MA
Comments: Accepted at ICAART
\\
  Recent advances in Multi-Agent Reinforcement Learning have prompted the
modeling of intricate interactions between agents in simulated environments. In
particular, the predator-prey dynamics have captured substantial interest and
various simulations been tailored to unique requirements. To prevent further
time-intensive developments, we introduce Aquarium, a comprehensive Multi-Agent
Reinforcement Learning environment for predator-prey interaction, enabling the
study of emergent behavior. Aquarium is open source and offers a seamless
integration of the PettingZoo framework, allowing a quick start with proven
algorithm implementations. It features physics-based agent movement on a
two-dimensional, edge-wrapping plane. The agent-environment interaction
(observations, actions, rewards) and the environment settings (agent speed,
prey reproduction, predator starvation, and others) are fully customizable.
Besides a resource-efficient visualization, Aquarium supports to record video
files, providing a visual comprehension of agent behavior. To demonstrate the
environment's capabilities, we conduct preliminary studies which use PPO to
train multiple prey agents to evade a predator. In accordance to the
literature, we find Individual Learning to result in worse performance than
Parameter Sharing, which significantly improves coordination and
sample-efficiency.
\\ ( https://arxiv.org/abs/2401.07056 ,  9251kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07115
Date: Sat, 13 Jan 2024 16:41:40 GMT   (1642kb)

Title: Open Models, Closed Minds? On Agents Capabilities in Mimicking Human
  Personalities through Open Large Language Models
Authors: Lucio La Cava, Davide Costa, Andrea Tagarelli
Categories: cs.AI cs.CL cs.CY cs.HC physics.soc-ph
\\
  The emergence of unveiling human-like behaviors in Large Language Models
(LLMs) has led to a closer connection between NLP and human psychology, leading
to a proliferation of computational agents. Scholars have been studying the
inherent personalities displayed by LLM agents and attempting to incorporate
human traits and behaviors into them. However, these efforts have primarily
focused on commercially-licensed LLMs, neglecting the widespread use and
notable advancements seen in Open LLMs. This work aims to address this gap by
conducting a comprehensive examination of the ability of agents to emulate
human personalities using Open LLMs. To achieve this, we generate a set of ten
LLM Agents based on the most representative Open models and subject them to a
series of assessments concerning the Myers-Briggs Type Indicator (MBTI) test.
Our approach involves evaluating the intrinsic personality traits of Open LLM
agents and determining the extent to which these agents can mimic human
personalities when conditioned by specific personalities and roles. Our
findings unveil that: $(i)$ each Open LLM agent showcases distinct human
personalities; $(ii)$ personality-conditioned prompting produces varying
effects on the agents, with only few successfully mirroring the imposed
personality, while most of them being ``closed-minded'' (i.e., they retain
their intrinsic traits); $(iii)$ combining role and personality conditioning
can enhance the agents' ability to mimic human personalities; and $(iv)$
personalities typically associated with the role of teacher tend to be emulated
with greater accuracy. Our work represents a step up in understanding the dense
relationship between NLP and human psychology through the lens of Open LLMs.
\\ ( https://arxiv.org/abs/2401.07115 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07314
Date: Sun, 14 Jan 2024 15:34:48 GMT   (1961kb,D)

Title: MapGPT: Map-Guided Prompting for Unified Vision-and-Language Navigation
Authors: Jiaqi Chen, Bingqian Lin, Ran Xu, Zhenhua Chai, Xiaodan Liang,
  Kwan-Yee K. Wong
Categories: cs.AI cs.CV cs.RO
\\
  Embodied agents equipped with GPT as their brain have exhibited extraordinary
thinking and decision-making abilities across various tasks. However, existing
zero-shot agents for vision-and-language navigation (VLN) only prompt the GPT
to handle excessive environmental information and select potential locations
within localized environments, without constructing an effective
''global-view'' (e.g., a commonly-used map) for the agent to understand the
overall environment. In this work, we present a novel map-guided GPT-based
path-planning agent, dubbed MapGPT, for the zero-shot VLN task. Specifically,
we convert a topological map constructed online into prompts to encourage
map-guided global exploration, and require the agent to explicitly output and
update multi-step path planning to avoid getting stuck in local exploration.
Extensive experiments demonstrate that our MapGPT is effective, achieving
impressive performance on both the R2R and REVERIE datasets (38.8% and 28.4%
success rate, respectively) and showcasing the newly emerged global thinking
and path planning capabilities of the GPT model. Unlike previous VLN agents,
which require separate parameters fine-tuning or specific prompt design to
accommodate various instruction styles across different datasets, our MapGPT is
more unified as it can adapt to different instruction styles seamlessly, which
is the first of its kind in this field.
\\ ( https://arxiv.org/abs/2401.07314 ,  1961kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07324
Date: Sun, 14 Jan 2024 16:17:07 GMT   (1568kb,D)

Title: Small LLMs Are Weak Tool Learners: A Multi-LLM Agent
Authors: Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan,
  Hehong Chen, Ji Zhang, Fei Huang
Categories: cs.AI cs.CL
Comments: On progress
\\
  Large Language Model (LLM) agents significantly extend the capabilities of
standalone LLMs, empowering them to interact with external tools (e.g., APIs,
functions) and complete complex tasks in a self-directed fashion. The challenge
of tool use demands that LLMs not only understand user queries and generate
answers but also excel in task planning, memory management, tool invocation,
and result summarization. While traditional approaches focus on training a
single LLM with all these capabilities, performance limitations become
apparent, particularly with smaller models. Moreover, the entire LLM may
require retraining when tools are updated. To overcome these challenges, we
propose a novel strategy that decomposes the aforementioned capabilities into a
planner, caller, and summarizer. Each component is implemented by a single LLM
that focuses on a specific capability and collaborates with other components to
accomplish the task. This modular framework facilitates individual updates and
the potential use of smaller LLMs for building each capability. To effectively
train this framework, we introduce a two-stage training paradigm. First, we
fine-tune a backbone LLM on the entire dataset without discriminating
sub-tasks, providing the model with a comprehensive understanding of the task.
Second, the fine-tuned LLM is used to instantiate the planner, caller, and
summarizer respectively, which are continually fine-tuned on respective
sub-tasks. Evaluation across various tool-use benchmarks illustrates that our
proposed multi-LLM framework surpasses the traditional single-LLM approach,
highlighting its efficacy and advantages in tool learning.
\\ ( https://arxiv.org/abs/2401.07324 ,  1568kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07359
Date: Sun, 14 Jan 2024 20:14:07 GMT   (39kb)

Title: Reliability and Interpretability in Science and Deep Learning
Authors: Luigi Scorzato
Categories: cs.AI cs.LG physics.hist-ph
\\
  In recent years, the question of the reliability of Machine Learning (ML)
methods has acquired significant importance, and the analysis of the associated
uncertainties has motivated a growing amount of research. However, most of
these studies have applied standard error analysis to ML models, and in
particular Deep Neural Network (DNN) models, which represent a rather
significant departure from standard scientific modelling. It is therefore
necessary to integrate the standard error analysis with a deeper
epistemological analysis of the possible differences between DNN models and
standard scientific modelling and the possible implications of these
differences in the assessment of reliability. This article offers several
contributions. First, it emphasises the ubiquitous role of model assumptions
(both in ML and traditional Science) against the illusion of theory-free
science. Secondly, model assumptions are analysed from the point of view of
their (epistemic) complexity, which is shown to be language-independent. It is
argued that the high epistemic complexity of DNN models hinders the estimate of
their reliability and also their prospect of long-term progress. Some potential
ways forward are suggested. Thirdly, this article identifies the close relation
between a model's epistemic complexity and its interpretability, as introduced
in the context of responsible AI. This clarifies in which sense, and to what
extent, the lack of understanding of a model (black-box problem) impacts its
interpretability in a way that is independent of individual skills. It also
clarifies how interpretability is a precondition for assessing the reliability
of any model, which cannot be based on statistical analysis alone. This article
focuses on the comparison between traditional scientific models and DNN models.
But, Random Forest and Logistic Regression models are also briefly considered.
\\ ( https://arxiv.org/abs/2401.07359 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07426
Date: Mon, 15 Jan 2024 02:25:00 GMT   (1125kb,D)

Title: Generalized Planning for the Abstraction and Reasoning Corpus
Authors: Chao Lei, Nir Lipovetzky, Krista A. Ehinger
Categories: cs.AI
Comments: Accepted at AAAI 2024 (extended version)
\\
  The Abstraction and Reasoning Corpus (ARC) is a general artificial
intelligence benchmark that poses difficulties for pure machine learning
methods due to its requirement for fluid intelligence with a focus on reasoning
and abstraction. In this work, we introduce an ARC solver, Generalized Planning
for Abstract Reasoning (GPAR). It casts an ARC problem as a generalized
planning (GP) problem, where a solution is formalized as a planning program
with pointers. We express each ARC problem using the standard Planning Domain
Definition Language (PDDL) coupled with external functions representing
object-centric abstractions. We show how to scale up GP solvers via domain
knowledge specific to ARC in the form of restrictions over the actions model,
predicates, arguments and valid structure of planning programs. Our experiments
demonstrate that GPAR outperforms the state-of-the-art solvers on the
object-centric tasks of the ARC, showing the effectiveness of GP and the
expressiveness of PDDL to model ARC problems. The challenges provided by the
ARC benchmark motivate research to advance existing GP solvers and understand
new relations with other planning computational models. Code is available at
github.com/you68681/GPAR.
\\ ( https://arxiv.org/abs/2401.07426 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07448
Date: Mon, 15 Jan 2024 03:25:37 GMT   (15417kb,D)

Title: Formal Logic Enabled Personalized Federated Learning Through Property
  Inference
Authors: Ziyan An, Taylor T. Johnson, Meiyi Ma
Categories: cs.AI cs.LG
\\
  Recent advancements in federated learning (FL) have greatly facilitated the
development of decentralized collaborative applications, particularly in the
domain of Artificial Intelligence of Things (AIoT). However, a critical aspect
missing from the current research landscape is the ability to enable
data-driven client models with symbolic reasoning capabilities. Specifically,
the inherent heterogeneity of participating client devices poses a significant
challenge, as each client exhibits unique logic reasoning properties. Failing
to consider these device-specific specifications can result in critical
properties being missed in the client predictions, leading to suboptimal
performance. In this work, we propose a new training paradigm that leverages
temporal logic reasoning to address this issue. Our approach involves enhancing
the training process by incorporating mechanically generated logic expressions
for each FL client. Additionally, we introduce the concept of aggregation
clusters and develop a partitioning algorithm to effectively group clients
based on the alignment of their temporal reasoning properties. We evaluate the
proposed method on two tasks: a real-world traffic volume prediction task
consisting of sensory data from fifteen states and a smart city multi-task
prediction utilizing synthetic data. The evaluation results exhibit clear
improvements, with performance accuracy improved by up to 54% across all
sequential prediction models.
\\ ( https://arxiv.org/abs/2401.07448 ,  15417kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07656
Date: Mon, 15 Jan 2024 12:52:56 GMT   (185kb)

Title: Learning Explainable and Better Performing Representations of POMDP
  Strategies
Authors: Alexander Bork, Debraj Chakraborty, Kush Grover, Jan Kretinsky,
  Stefanie Mohr
Categories: cs.AI cs.LG cs.LO
Comments: Technical report for the submission to TACAS 24
\\
  Strategies for partially observable Markov decision processes (POMDP)
typically require memory. One way to represent this memory is via automata. We
present a method to learn an automaton representation of a strategy using the
L*-algorithm. Compared to the tabular representation of a strategy, the
resulting automaton is dramatically smaller and thus also more explainable.
Moreover, in the learning process, our heuristics may even improve the
strategy's performance. In contrast to approaches that synthesize an automaton
directly from the POMDP thereby solving it, our approach is incomparably more
scalable.
\\ ( https://arxiv.org/abs/2401.07656 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07710
Date: Mon, 15 Jan 2024 14:26:44 GMT   (311kb)

Title: Go-Explore for Residential Energy Management
Authors: Junlin Lu, Patrick Mannion, Karl Mason
Categories: cs.AI cs.LG cs.SY eess.SY
\\
  Reinforcement learning is commonly applied in residential energy management,
particularly for optimizing energy costs. However, RL agents often face
challenges when dealing with deceptive and sparse rewards in the energy control
domain, especially with stochastic rewards. In such situations, thorough
exploration becomes crucial for learning an optimal policy. Unfortunately, the
exploration mechanism can be misled by deceptive reward signals, making
thorough exploration difficult. Go-Explore is a family of algorithms which
combines planning methods and reinforcement learning methods to achieve
efficient exploration. We use the Go-Explore algorithm to solve the cost-saving
task in residential energy management problems and achieve an improvement of up
to 19.84\% compared to the well-known reinforcement learning algorithms.
\\ ( https://arxiv.org/abs/2401.07710 ,  311kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07722
Date: Mon, 15 Jan 2024 14:36:59 GMT   (409kb,D)

Title: Inferring Preferences from Demonstrations in Multi-Objective Residential
  Energy Management
Authors: Junlin Lu, Patrick Mannion, Karl Mason
Categories: cs.AI
\\
  It is often challenging for a user to articulate their preferences accurately
in multi-objective decision-making problems. Demonstration-based preference
inference (DemoPI) is a promising approach to mitigate this problem.
Understanding the behaviours and values of energy customers is an example of a
scenario where preference inference can be used to gain insights into the
values of energy customers with multiple objectives, e.g. cost and comfort. In
this work, we applied the state-of-art DemoPI method, i.e., the dynamic
weight-based preference inference (DWPI) algorithm in a multi-objective
residential energy consumption setting to infer preferences from energy
consumption demonstrations by simulated users following a rule-based approach.
According to our experimental results, the DWPI model achieves accurate
demonstration-based preference inferring in three scenarios. These advancements
enhance the usability and effectiveness of multi-objective reinforcement
learning (MORL) in energy management, enabling more intuitive and user-friendly
preference specifications, and opening the door for DWPI to be applied in
real-world settings.
\\ ( https://arxiv.org/abs/2401.07722 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07744
Date: Mon, 15 Jan 2024 14:56:04 GMT   (901kb,D)

Title: Combining Machine Learning and Ontology: A Systematic Literature Review
Authors: Sarah Ghidalia, Ouassila Labbani Narsis, Aur\'elie Bertaux, Christophe
  Nicolle
Categories: cs.AI cs.LG
\\
  Motivated by the desire to explore the process of combining inductive and
deductive reasoning, we conducted a systematic literature review of articles
that investigate the integration of machine learning and ontologies. The
objective was to identify diverse techniques that incorporate both inductive
reasoning (performed by machine learning) and deductive reasoning (performed by
ontologies) into artificial intelligence systems. Our review, which included
the analysis of 128 studies, allowed us to identify three main categories of
hybridization between machine learning and ontologies: learning-enhanced
ontologies, semantic data mining, and learning and reasoning systems. We
provide a comprehensive examination of all these categories, emphasizing the
various machine learning algorithms utilized in the studies. Furthermore, we
compared our classification with similar recent work in the field of hybrid AI
and neuro-symbolic approaches.
\\ ( https://arxiv.org/abs/2401.07744 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07764
Date: Mon, 15 Jan 2024 15:20:59 GMT   (5247kb,D)

Title: When Large Language Model Agents Meet 6G Networks: Perception,
  Grounding, and Alignment
Authors: Minrui Xu, Niyato Dusit, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu
  Han, Dong In Kim, and Khaled B. Letaief
Categories: cs.AI cs.NI
\\
  AI agents based on multimodal large language models (LLMs) are expected to
revolutionize human-computer interaction and offer more personalized assistant
services across various domains like healthcare, education, manufacturing, and
entertainment. Deploying LLM agents in 6G networks enables users to access
previously expensive AI assistant services via mobile devices democratically,
thereby reducing interaction latency and better preserving user privacy.
Nevertheless, the limited capacity of mobile devices constrains the
effectiveness of deploying and executing local LLMs, which necessitates
offloading complex tasks to global LLMs running on edge servers during
long-horizon interactions. In this article, we propose a split learning system
for LLM agents in 6G networks leveraging the collaboration between mobile
devices and edge servers, where multiple LLMs with different roles are
distributed across mobile devices and edge servers to perform user-agent
interactive tasks collaboratively. In the proposed system, LLM agents are split
into perception, grounding, and alignment modules, facilitating inter-module
communications to meet extended user requirements on 6G network functions,
including integrated sensing and communication, digital twins, and
task-oriented communications. Furthermore, we introduce a novel model caching
algorithm for LLMs within the proposed system to improve model utilization in
context, thus reducing network costs of the collaborative mobile and edge LLM
agents.
\\ ( https://arxiv.org/abs/2401.07764 ,  5247kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07871
Date: Mon, 15 Jan 2024 18:06:59 GMT   (8117kb,D)

Title: Explainable Predictive Maintenance: A Survey of Current Methods,
  Challenges and Opportunities
Authors: Logan Cummins, Alex Sommers, Somayeh Bakhtiari Ramezani, Sudip Mittal,
  Joseph Jabour, Maria Seale, Shahram Rahimi
Categories: cs.AI cs.HC cs.LG
\\
  Predictive maintenance is a well studied collection of techniques that aims
to prolong the life of a mechanical system by using artificial intelligence and
machine learning to predict the optimal time to perform maintenance. The
methods allow maintainers of systems and hardware to reduce financial and time
costs of upkeep. As these methods are adopted for more serious and potentially
life-threatening applications, the human operators need trust the predictive
system. This attracts the field of Explainable AI (XAI) to introduce
explainability and interpretability into the predictive system. XAI brings
methods to the field of predictive maintenance that can amplify trust in the
users while maintaining well-performing systems. This survey on explainable
predictive maintenance (XPM) discusses and presents the current methods of XAI
as applied to predictive maintenance while following the Preferred Reporting
Items for Systematic Reviews and Meta-Analyses (PRISMA) 2020 guidelines. We
categorize the different XPM methods into groups that follow the XAI
literature. Additionally, we include current challenges and a discussion on
future research directions in XPM.
\\ ( https://arxiv.org/abs/2401.07871 ,  8117kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07890
Date: Mon, 15 Jan 2024 18:43:48 GMT   (1006kb,D)

Title: A Strategy for Implementing description Temporal Dynamic Algorithms in
  Dynamic Knowledge Graphs by SPIN
Authors: Alireza Shahbazi, Seyyed Ahmad Mirsanei, Malikeh Haj Khan Mirzaye
  Sarraf and Behrouz Minaei Bidgoli
Categories: cs.AI cs.LO
\\
  Planning and reasoning about actions and processes, in addition to reasoning
about propositions, are important issues in recent logical and computer science
studies. The widespread use of actions in everyday life such as IoT, semantic
web services, etc., and the limitations and issues in the action formalisms are
two factors that lead us to study about how actions are represented.
  Since 2007, there was some ideas to integrate Description Logic (DL) and
action formalisms for representing both static and dynamic knowledge. In
meanwhile, time is an important factor in dynamic situations, and actions
change states over time. In this study, on the one hand, we examined related
logical structures such as extensions of description logics (DLs), temporal
formalisms, and action formalisms. On the other hand, we analyzed possible
tools for designing and developing the Knowledge and Action Base (KAB).
  For representation and reasoning about actions, we embedded actions into DLs
(such as Dynamic-ALC and its extensions). We propose a terminable algorithm for
action projection, planning, checking the satisfiability, consistency,
realizability, and executability, and also querying from KAB. Actions in this
framework were modeled with SPIN and added to state space. This framework has
also been implemented as a plugin for the Prot\'eg\'e ontology editor.
  During the last two decades, various algorithms have been presented, but due
to the high computational complexity, we face many problems in implementing
dynamic ontologies. In addition, an algorithm to detect the inconsistency of
actions' effects was not explicitly stated. In the proposed strategy, the
interactions of actions with other parts of modeled knowledge, and a method to
check consistency between the effects of actions are presented. With this
framework, the ramification problem can be well handled in future works.
\\ ( https://arxiv.org/abs/2401.07890 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07964
Date: Mon, 15 Jan 2024 21:06:20 GMT   (29kb)

Title: AI-as-exploration: Navigating intelligence space
Authors: Dimitri Coelho Mollo
Categories: cs.AI cs.CL
\\
  Artificial Intelligence is a field that lives many lives, and the term has
come to encompass a motley collection of scientific and commercial endeavours.
In this paper, I articulate the contours of a rather neglected but central
scientific role that AI has to play, which I dub `AI-as-exploration'.The basic
thrust of AI-as-exploration is that of creating and studying systems that can
reveal candidate building blocks of intelligence that may differ from the forms
of human and animal intelligence we are familiar with. In other words, I
suggest that AI is one of the best tools we have for exploring intelligence
space, namely the space of possible intelligent systems. I illustrate the value
of AI-as-exploration by focusing on a specific case study, i.e., recent work on
the capacity to combine novel and invented concepts in humans and Large
Language Models. I show that the latter, despite showing human-level accuracy
in such a task, most probably solve it in ways radically different, but no less
relevant to intelligence research, to those hypothesised for humans.
\\ ( https://arxiv.org/abs/2401.07964 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08008
Date: Mon, 15 Jan 2024 23:28:55 GMT   (2937kb,D)

Title: Analysing the Needs of Homeless People Using Feature Selection and
  Mining Association Rules
Authors: Jos\'e M. Alcalde-Llergo, Carlos Garc\'ia-Mart\'inez, Manuel
  Vaquero-Abell\'an, Pilar Aparicio-Mart\'inez and Enrique Yeguas-Bol\'ivar
Categories: cs.AI cs.CY
Comments: 6 pages, 4 figures, 4 tables, MetroXRAINE 2022
Journal-ref: 2022 IEEE International Conference on Metrology for Extended
  Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE), Rome,
  Italy, 2022, pp. 568-573
DOI: 10.1109/MetroXRAINE54828.2022.9967612
\\
  Homelessness is a social and health problem with great repercussions in
Europe. Many non-governmental organisations help homeless people by collecting
and analysing large amounts of information about them. However, these tasks are
not always easy to perform, and hinder other of the organisations duties. The
SINTECH project was created to tackle this issue proposing two different tools:
a mobile application to quickly and easily collect data; and a software based
on artificial intelligence which obtains interesting information from the
collected data. The first one has been distributed to some Spanish
organisations which are using it to conduct surveys of homeless people. The
second tool implements different feature selection and association rules mining
methods. These artificial intelligence techniques have allowed us to identify
the most relevant features and some interesting association rules from
previously collected homeless data.
\\ ( https://arxiv.org/abs/2401.08008 ,  2937kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08025
Date: Tue, 16 Jan 2024 00:46:29 GMT   (3598kb,D)

Title: Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using
  Self-Imagination
Authors: Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, Eric Nyberg
Categories: cs.AI cs.CL cs.LG
Comments: 10 pages, 6 figures
\\
  The potential of Vision-Language Models (\textsc{vlm}s) often remains
underutilized in handling complex text-based problems, particularly when these
problems could benefit from visual representation. Resonating with humans'
ability to solve complex text-based problems by (1) creating a visual diagram
from the problem and (2) deducing what steps they need to take to solve it, we
propose \textsc{Self-Imagine}. We leverage a single Vision-Language Model
(\textsc{vlm}) to generate a structured representation of the question using
HTML, then render the HTML as an image, and finally use the same \vlm to answer
the question using both the question and the image. Our approach does not
require any additional training data or training. We evaluate our approach in
three mathematics tasks and nine general-purpose reasoning tasks using
state-of-the-art \textsc{vlm}. Our approach boosts the performance of
\textsc{vlm} on all math tasks (\gsm: +4.62\%; \asdiv: +4.49\%; \svamp:
+9.30\%) and the majority of the general-purpose reasoning tasks by 0.4\% to
13.20\% while achieving comparable performance in other tasks.
  Code and data at https://github.com/snat1505027/self-imagine .
\\ ( https://arxiv.org/abs/2401.08025 ,  3598kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08189
Date: Tue, 16 Jan 2024 08:04:50 GMT   (175kb,D)

Title: PRewrite: Prompt Rewriting with Reinforcement Learning
Authors: Weize Kong and Spurthi Amba Hombaiah and Mingyang Zhang and Qiaozhu
  Mei and Michael Bendersky
Categories: cs.AI cs.CL cs.LG
\\
  Prompt engineering is critical for the development of LLM-based applications.
However, it is usually done manually in a "trial and error" fashion. This
manual procedure can be time consuming, ineffective, and the generated prompts
are, in a lot of cases, sub-optimal. Even for the prompts which seemingly work
well, there is always a lingering question: can the prompts be made better with
further modifications?
  To address these questions, in this paper, we investigate prompt engineering
automation. We consider a specific use case scenario in which developers/users
have drafted initial prompts, but lack the time/expertise to optimize them. We
propose PRewrite, an automated tool to rewrite these drafts and to generate
highly effective new prompts. PRewrite is based on the Reinforcement Learning
(RL) framework which allows for end-to-end optimization and our design allows
the RL search to happen in a large action space. The automated tool leverages
manually crafted prompts as starting points which makes the rewriting procedure
more guided and efficient. The generated prompts are human readable, and
self-explanatory, unlike some of those in previous works. We conducted
extensive experiments on diverse datasets and found that the prompts generated
with this new method not only outperform professionally crafted prompts, but
also prompts generated with other previously proposed methods.
\\ ( https://arxiv.org/abs/2401.08189 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08460
Date: Tue, 16 Jan 2024 16:09:56 GMT   (6889kb,D)

Title: Reinforcement Learning for Conversational Question Answering over
  Knowledge Graph
Authors: Mi Wu
Categories: cs.AI cs.CL
\\
  Conversational question answering (ConvQA) over law knowledge bases (KBs)
involves answering multi-turn natural language questions about law and hope to
find answers in the law knowledge base. Despite many methods have been
proposed. Existing law knowledge base ConvQA model assume that the input
question is clear and can perfectly reflect user's intention. However, in real
world, the input questions are noisy and inexplict. This makes the model hard
to find the correct answer in the law knowledge bases. In this paper, we try to
use reinforcement learning to solve this problem. The reinforcement learning
agent can automatically learn how to find the answer based on the input
question and the conversation history, even when the input question is
inexplicit. We test the proposed method on several real world datasets and the
results show the effectivenss of the proposed model.
\\ ( https://arxiv.org/abs/2401.08460 ,  6889kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08461
Date: Tue, 16 Jan 2024 16:11:35 GMT   (2311kb,D)

Title: Decentralised Emergence of Robust and Adaptive Linguistic Conventions in
  Populations of Autonomous Agents Grounded in Continuous Worlds
Authors: J\'er\^ome Botoko Ekila, Jens Nevens, Lara Verheyen, Katrien Beuls,
  Paul Van Eecke
Categories: cs.AI cs.CL cs.NE
\\
  This paper introduces a methodology through which a population of autonomous
agents can establish a linguistic convention that enables them to refer to
arbitrary entities that they observe in their environment. The linguistic
convention emerges in a decentralised manner through local communicative
interactions between pairs of agents drawn from the population. The convention
consists of symbolic labels (word forms) associated to concept representations
(word meanings) that are grounded in a continuous feature space. The concept
representations of each agent are individually constructed yet compatible on a
communicative level. Through a range of experiments, we show (i) that the
methodology enables a population to converge on a communicatively effective,
coherent and human-interpretable linguistic convention, (ii) that it is
naturally robust against sensor defects in individual agents, (iii) that it can
effectively deal with noisy observations, uncalibrated sensors and
heteromorphic populations, (iv) that the method is adequate for continual
learning, and (v) that the convention self-adapts to changes in the environment
and communicative needs of the agents.
\\ ( https://arxiv.org/abs/2401.08461 ,  2311kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08517
Date: Tue, 16 Jan 2024 17:31:35 GMT   (671kb)

Title: Supporting Student Decisions on Learning Recommendations: An LLM-Based
  Chatbot with Knowledge Graph Contextualization for Conversational
  Explainability and Mentoring
Authors: Hasan Abu-Rasheed, Mohamad Hussam Abdulsalam, Christian Weber, Madjid
  Fathi
Categories: cs.AI cs.CL cs.HC
\\
  Student commitment towards a learning recommendation is not separable from
their understanding of the reasons it was recommended to them; and their
ability to modify it based on that understanding. Among explainability
approaches, chatbots offer the potential to engage the student in a
conversation, similar to a discussion with a peer or a mentor. The capabilities
of chatbots, however, are still not sufficient to replace a human mentor,
despite the advancements of generative AI (GenAI) and large language models
(LLM). Therefore, we propose an approach to utilize chatbots as mediators of
the conversation and sources of limited and controlled generation of
explanations, to harvest the potential of LLMs while reducing their potential
risks at the same time. The proposed LLM-based chatbot supports students in
understanding learning-paths recommendations. We use a knowledge graph (KG) as
a human-curated source of information, to regulate the LLM's output through
defining its prompt's context. A group chat approach is developed to connect
students with human mentors, either on demand or in cases that exceed the
chatbot's pre-defined tasks. We evaluate the chatbot with a user study, to
provide a proof-of-concept and highlight the potential requirements and
limitations of utilizing chatbots in conversational explainability.
\\ ( https://arxiv.org/abs/2401.08517 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08525
Date: Tue, 16 Jan 2024 17:43:42 GMT   (4249kb,D)

Title: GATS: Gather-Attend-Scatter
Authors: Konrad Zolna, Serkan Cabi, Yutian Chen, Eric Lau, Claudio Fantacci,
  Jurgis Pasukonis, Jost Tobias Springenberg, Sergio Gomez Colmenarejo
Categories: cs.AI cs.CV cs.LG cs.RO
\\
  As the AI community increasingly adopts large-scale models, it is crucial to
develop general and flexible tools to integrate them. We introduce
Gather-Attend-Scatter (GATS), a novel module that enables seamless combination
of pretrained foundation models, both trainable and frozen, into larger
multimodal networks. GATS empowers AI systems to process and generate
information across multiple modalities at different rates. In contrast to
traditional fine-tuning, GATS allows for the original component models to
remain frozen, avoiding the risk of them losing important knowledge acquired
during the pretraining phase. We demonstrate the utility and versatility of
GATS with a few experiments across games, robotics, and multimodal input-output
systems.
\\ ( https://arxiv.org/abs/2401.08525 ,  4249kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06772
Date: Fri, 1 Dec 2023 20:45:06 GMT   (592kb)

Title: Semantic Segment Based Semantic Parsing for Question Answering over
  Knowledge Graphs
Authors: Sijia Wei, Wenwen Zhang, Qisong Li, Jiang Zhao
Categories: cs.CL
\\
  In this paper, we introduce a novel method named "graph-to-segment" for
question answering over knowledge graphs, focusing on understanding question
utterances. This method centers on semantic parsing, a key approach for
interpreting these utterances. Our primary challenge lies in comprehending
implicit entities, relationships, and complex constraints like time,
ordinality, and aggregation within questions, contextualized by the knowledge
graph. Our framework employs a combination of rule-based and neural-based
techniques to parse and construct highly accurate and comprehensive semantic
segment sequences. These sequences form semantic query graphs, effectively
representing question utterances. We approach question semantic parsing as a
sequence generation task, utilizing an encoder-decoder neural network to
transform natural language questions into semantic segments. Moreover, to
enhance the parsing of implicit entities and relations, we incorporate a graph
neural network that leverages the context of the knowledge graph to better
understand question representations. Our experimental evaluations on two
datasets demonstrate the effectiveness and superior performance of our model in
semantic parsing for question answering.
\\ ( https://arxiv.org/abs/2401.06772 ,  592kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06774
Date: Sat, 9 Dec 2023 19:35:40 GMT   (841kb,D)

Title: Two Directions for Clinical Data Generation with Large Language Models:
  Data-to-Label and Label-to-Data
Authors: Rumeng Li, Xun Wang, Hong Yu
Categories: cs.CL
Comments: Appear in EMNLP2023 Findings
\\
  Large language models (LLMs) can generate natural language texts for various
domains and tasks, but their potential for clinical text mining, a domain with
scarce, sensitive, and imbalanced medical data, is underexplored. We
investigate whether LLMs can augment clinical data for detecting Alzheimer's
Disease (AD)-related signs and symptoms from electronic health records (EHRs),
a challenging task that requires high expertise. We create a novel pragmatic
taxonomy for AD sign and symptom progression based on expert knowledge, which
guides LLMs to generate synthetic data following two different directions:
"data-to-label", which labels sentences from a public EHR collection with
AD-related signs and symptoms; and "label-to-data", which generates sentences
with AD-related signs and symptoms based on the label definition. We train a
system to detect AD-related signs and symptoms from EHRs, using three datasets:
(1) a gold dataset annotated by human experts on longitudinal EHRs of AD
patients; (2) a silver dataset created by the data-to-label method; and (3) a
bronze dataset created by the label-to-data method. We find that using the
silver and bronze datasets improves the system performance, outperforming the
system using only the gold dataset. This shows that LLMs can generate synthetic
clinical data for a complex task by incorporating expert knowledge, and our
label-to-data method can produce datasets that are free of sensitive
information, while maintaining acceptable quality.
\\ ( https://arxiv.org/abs/2401.06774 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06775
Date: Tue, 12 Dec 2023 20:54:51 GMT   (4889kb,D)

Title: Large language models in healthcare and medical domain: A review
Authors: Zabir Al Nazi, Wei Peng
Categories: cs.CL cs.AI
\\
  The deployment of large language models (LLMs) within the healthcare sector
has sparked both enthusiasm and apprehension. These models exhibit the
remarkable capability to provide proficient responses to free-text queries,
demonstrating a nuanced understanding of professional medical knowledge. This
comprehensive survey delves into the functionalities of existing LLMs designed
for healthcare applications, elucidating the trajectory of their development,
starting from traditional Pretrained Language Models (PLMs) to the present
state of LLMs in healthcare sector. First, we explore the potential of LLMs to
amplify the efficiency and effectiveness of diverse healthcare applications,
particularly focusing on clinical language understanding tasks. These tasks
encompass a wide spectrum, ranging from named entity recognition and relation
extraction to natural language inference, multi-modal medical applications,
document classification, and question-answering. Additionally, we conduct an
extensive comparison of the most recent state-of-the-art LLMs in the healthcare
domain, while also assessing the utilization of various open-source LLMs and
highlighting their significance in healthcare applications. Furthermore, we
present the essential performance metrics employed to evaluate LLMs in the
biomedical domain, shedding light on their effectiveness and limitations.
Finally, we summarize the prominent challenges and constraints faced by large
language models in the healthcare sector, offering a holistic perspective on
their potential benefits and shortcomings. This review provides a comprehensive
exploration of the current landscape of LLMs in healthcare, addressing their
role in transforming medical applications and the areas that warrant further
research and development.
\\ ( https://arxiv.org/abs/2401.06775 ,  4889kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06782
Date: Sat, 6 Jan 2024 02:35:49 GMT   (261kb,D)

Title: Semantic Similarity Matching for Patent Documents Using Ensemble
  BERT-related Model and Novel Text Processing Method
Authors: Liqiang Yu, Bo Liu, Qunwei Lin, Xinyu Zhao, Chang Che
Categories: cs.CL cs.AI
Comments: It accepted by The 6th International Conference on Machine Learning
  and Machine Intelligence (MLMI 2023)
\\
  In the realm of patent document analysis, assessing semantic similarity
between phrases presents a significant challenge, notably amplifying the
inherent complexities of Cooperative Patent Classification (CPC) research.
Firstly, this study addresses these challenges, recognizing early CPC work
while acknowledging past struggles with language barriers and document
intricacy. Secondly, it underscores the persisting difficulties of CPC
research.
  To overcome these challenges and bolster the CPC system, This paper presents
two key innovations. Firstly, it introduces an ensemble approach that
incorporates four BERT-related models, enhancing semantic similarity accuracy
through weighted averaging. Secondly, a novel text preprocessing method
tailored for patent documents is introduced, featuring a distinctive input
structure with token scoring that aids in capturing semantic relationships
during CPC context training, utilizing BCELoss. Our experimental findings
conclusively establish the effectiveness of both our Ensemble Model and novel
text processing strategies when deployed on the U.S. Patent Phrase to Phrase
Matching dataset.
\\ ( https://arxiv.org/abs/2401.06782 ,  261kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06783
Date: Sat, 6 Jan 2024 09:13:34 GMT   (307kb,D)

Title: MultiSiam: A Multiple Input Siamese Network For Social Media Text
  Classification And Duplicate Text Detection
Authors: Sudhanshu Bhoi, Swapnil Markhedkar, Shruti Phadke, and Prashant
  Agrawal
Categories: cs.CL cs.AI cs.LG cs.SI
\\
  Social media accounts post increasingly similar content, creating a chaotic
experience across platforms, which makes accessing desired information
difficult. These posts can be organized by categorizing and grouping duplicates
across social handles and accounts. There can be more than one duplicate of a
post, however, a conventional Siamese neural network only considers a pair of
inputs for duplicate text detection. In this paper, we first propose a
multiple-input Siamese network, MultiSiam. This condensed network is then used
to propose another model, SMCD (Social Media Classification and Duplication
Model) to perform both duplicate text grouping and categorization. The
MultiSiam network, just like the Siamese, can be used in multiple applications
by changing the sub-network appropriately.
\\ ( https://arxiv.org/abs/2401.06783 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06785
Date: Sat, 6 Jan 2024 14:00:12 GMT   (187kb,D)

Title: Human-Instruction-Free LLM Self-Alignment with Limited Samples
Authors: Hongyi Guo, Yuanshun Yao, Wei Shen, Jiaheng Wei, Xiaoying Zhang,
  Zhaoran Wang, Yang Liu
Categories: cs.CL cs.AI
\\
  Aligning large language models (LLMs) with human values is a vital task for
LLM practitioners. Current alignment techniques have several limitations: (1)
requiring a large amount of annotated data; (2) demanding heavy human
involvement; (3) lacking a systematic mechanism to continuously improve. In
this work, we study aligning LLMs to a new domain with limited samples (e.g. <
100). We propose an algorithm that can self-align LLMs iteratively without
active human involvement. Unlike existing works, our algorithm relies on
neither human-crafted instructions nor labeled rewards, significantly reducing
human involvement. In addition, our algorithm can self-improve the alignment
continuously. The key idea is to first retrieve high-quality samples related to
the target domain and use them as In-context Learning examples to generate more
samples. Then we use the self-generated samples to finetune the LLM
iteratively. We show that our method can unlock the LLMs' self-generalization
ability to perform alignment with near-zero human supervision. We test our
algorithm on three benchmarks in safety, truthfulness, and
instruction-following, and show good performance in alignment, domain
adaptability, and scalability.
\\ ( https://arxiv.org/abs/2401.06785 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06787
Date: Sun, 7 Jan 2024 04:58:59 GMT   (1063kb)

Title: Deep Learning Based Cyberbullying Detection in Bangla Language
Authors: Sristy Shidul Nath, Razuan Karim and Mahdi H. Miraz
Categories: cs.CL cs.AI cs.LG cs.SI
Journal-ref: Annals of Emerging Technologies in Computing (AETiC), Print ISSN:
  2516-0281, Online ISSN: 2516-029X, pp. 50-65, Vol. 8, No. 1, 1st January
  2024, Available: http://aetic.theiaer.org/archive/v8/v8n1/p5.html
DOI: 10.33166/AETiC.2024.01.005
\\
  The Internet is currently the largest platform for global communication
including expressions of opinions, reviews, contents, images, videos and so
forth. Moreover, social media has now become a very broad and highly engaging
platform due to its immense popularity and swift adoption trend. Increased
social networking, however, also has detrimental impacts on the society leading
to a range of unwanted phenomena, such as online assault, intimidation, digital
bullying, criminality and trolling. Hence, cyberbullying has become a pervasive
and worrying problem that poses considerable psychological and emotional harm
to the people, particularly amongst the teens and the young adults. In order to
lessen its negative effects and provide victims with prompt support, a great
deal of research to identify cyberbullying instances at various online
platforms is emerging. In comparison to other languages, Bangla (also known as
Bengali) has fewer research studies in this domain. This study demonstrates a
deep learning strategy for identifying cyberbullying in Bengali, using a
dataset of 12282 versatile comments from multiple social media sites. In this
study, a two-layer bidirectional long short-term memory (Bi-LSTM) model has
been built to identify cyberbullying, using a variety of optimisers as well as
5-fold cross validation. To evaluate the functionality and efficacy of the
proposed system, rigorous assessment and validation procedures have been
employed throughout the project. The results of this study reveals that the
proposed model's accuracy, using momentum-based stochastic gradient descent
(SGD) optimiser, is 94.46%. It also reflects a higher accuracy of 95.08% and a
F1 score of 95.23% using Adam optimiser as well as a better accuracy of 94.31%
in 5-fold cross validation.
\\ ( https://arxiv.org/abs/2401.06787 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06790
Date: Mon, 8 Jan 2024 00:27:16 GMT   (3125kb,D)

Title: Using Zero-shot Prompting in the Automatic Creation and Expansion of
  Topic Taxonomies for Tagging Retail Banking Transactions
Authors: Daniel de S. Moraes, Pedro T. C. Santos, Polyana B. da Costa, Matheus
  A. S. Pinto, Ivan de J. P. Pinto, \'Alvaro M. G. da Veiga, Sergio Colcher,
  Antonio J. G. Busson, Rafael H. Rocha, Rennan Gaio, Rafael Miceli, Gabriela
  Tourinho, Marcos Rabaioli, Leandro Santos, Fellipe Marques, David Favaro
Categories: cs.CL cs.AI
\\
  This work presents an unsupervised method for automatically constructing and
expanding topic taxonomies by using instruction-based fine-tuned LLMs (Large
Language Models). We apply topic modeling and keyword extraction techniques to
create initial topic taxonomies and LLMs to post-process the resulting terms
and create a hierarchy. To expand an existing taxonomy with new terms, we use
zero-shot prompting to find out where to add new nodes, which, to our
knowledge, is the first work to present such an approach to taxonomy tasks. We
use the resulting taxonomies to assign tags that characterize merchants from a
retail bank dataset. To evaluate our work, we asked 12 volunteers to answer a
two-part form in which we first assessed the quality of the taxonomies created
and then the tags assigned to merchants based on that taxonomy. The evaluation
revealed a coherence rate exceeding 90% for the chosen taxonomies, while the
average coherence for merchant tagging surpassed 80%.
\\ ( https://arxiv.org/abs/2401.06790 ,  3125kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06792
Date: Mon, 8 Jan 2024 03:52:40 GMT   (568kb,D)

Title: LightHouse: A Survey of AGI Hallucination
Authors: Feng Wang
Categories: cs.CL cs.AI
\\
  With the development of artificial intelligence, large-scale models have
become increasingly intelligent. However, numerous studies indicate that
hallucinations within these large models are a bottleneck hindering the
development of AI research. In the pursuit of achieving strong artificial
intelligence, a significant volume of research effort is being invested in the
AGI (Artificial General Intelligence) hallucination research. Previous
explorations have been conducted in researching hallucinations within LLMs
(Large Language Models). As for multimodal AGI, research on hallucinations is
still in an early stage. To further the progress of research in the domain of
hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,
summarizing the current work on AGI hallucinations and proposing some
directions for future research.
\\ ( https://arxiv.org/abs/2401.06792 ,  568kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06795
Date: Mon, 8 Jan 2024 18:42:55 GMT   (2529kb,D)

Title: AI and Generative AI for Research Discovery and Summarization
Authors: Mark Glickman and Yi Zhang
Categories: cs.CL cs.AI cs.LG
Comments: 29 pages, 9 figures
\\
  AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.
\\ ( https://arxiv.org/abs/2401.06795 ,  2529kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06796
Date: Tue, 9 Jan 2024 01:49:41 GMT   (168kb)

Title: AI Hallucinations: A Misnomer Worth Clarifying
Authors: Negar Maleki, Balaji Padmanabhan, Kaushik Dutta
Categories: cs.CL cs.AI
\\
  As large language models continue to advance in Artificial Intelligence (AI),
text generation systems have been shown to suffer from a problematic phenomenon
termed often as "hallucination." However, with AI's increasing presence across
various domains including medicine, concerns have arisen regarding the use of
the term itself. In this study, we conducted a systematic review to identify
papers defining "AI hallucination" across fourteen databases. We present and
analyze definitions obtained across all databases, categorize them based on
their applications, and extract key points within each category. Our results
highlight a lack of consistency in how the term is used, but also help identify
several alternative terms in the literature. We discuss implications of these
and call for a more unified effort to bring consistency to an important
contemporary AI issue that can affect multiple domains significantly.
\\ ( https://arxiv.org/abs/2401.06796 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06799
Date: Tue, 9 Jan 2024 10:15:59 GMT   (5656kb,D)

Title: Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt
  Learning with Data-Dependent Prior
Authors: Youngjae Cho, HeeSun Bae, Seungjae Shin, Yeo Dong Youn, Weonyoung Joo,
  Il-Chul Moon
Categories: cs.CL cs.LG
Comments: Accepted to AAAI-2024
\\
  Recent Vision-Language Pretrained (VLP) models have become the backbone for
many downstream tasks, but they are utilized as frozen model without learning.
Prompt learning is a method to improve the pre-trained VLP model by adding a
learnable context vector to the inputs of the text encoder. In a few-shot
learning scenario of the downstream task, MLE training can lead the context
vector to over-fit dominant image features in the training data. This
overfitting can potentially harm the generalization ability, especially in the
presence of a distribution shift between the training and test dataset. This
paper presents a Bayesian-based framework of prompt learning, which could
alleviate the overfitting issues on few-shot learning application and increase
the adaptability of prompts on unseen instances. Specifically, modeling
data-dependent prior enhances the adaptability of text features for both seen
and unseen image features without the trade-off of performance between them.
Based on the Bayesian framework, we utilize the Wasserstein Gradient Flow in
the estimation of our target posterior distribution, which enables our prompt
to be flexible in capturing the complex modes of image features. We demonstrate
the effectiveness of our method on benchmark datasets for several experiments
by showing statistically significant improvements on performance compared to
existing methods. The code is available at https://github.com/youngjae-cho/APP.
\\ ( https://arxiv.org/abs/2401.06799 ,  5656kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06800
Date: Wed, 10 Jan 2024 02:57:20 GMT   (3073kb,D)

Title: Reinforcement Learning for Optimizing RAG for Domain Chatbots
Authors: Mandar Kulkarni, Praveen Tangarajan, Kyung Kim, Anusua Trivedi
Categories: cs.CL cs.AI
\\
  With the advent of Large Language Models (LLM), conversational assistants
have become prevalent for domain use cases. LLMs acquire the ability to
contextual question answering through training, and Retrieval Augmented
Generation (RAG) further enables the bot to answer domain-specific questions.
This paper describes a RAG-based approach for building a chatbot that answers
user's queries using Frequently Asked Questions (FAQ) data. We train an
in-house retrieval embedding model using infoNCE loss, and experimental results
demonstrate that the in-house model works significantly better than the
well-known general-purpose public embedding model, both in terms of retrieval
accuracy and Out-of-Domain (OOD) query detection. As an LLM, we use an open
API-based paid ChatGPT model. We noticed that a previously retrieved-context
could be used to generate an answer for specific patterns/sequences of queries
(e.g., follow-up queries). Hence, there is a scope to optimize the number of
LLM tokens and cost. Assuming a fixed retrieval model and an LLM, we optimize
the number of LLM tokens using Reinforcement Learning (RL). Specifically, we
propose a policy-based model external to the RAG, which interacts with the RAG
pipeline through policy actions and updates the policy to optimize the cost.
The policy model can perform two actions: to fetch FAQ context or skip
retrieval. We use the open API-based GPT-4 as the reward model. We then train a
policy model using policy gradient on multiple training chat sessions. As a
policy model, we experimented with a public gpt-2 model and an in-house BERT
model. With the proposed RL-based optimization combined with similarity
threshold, we are able to achieve significant cost savings while getting a
slightly improved accuracy. Though we demonstrate results for the FAQ chatbot,
the proposed RL approach is generic and can be experimented with any existing
RAG pipeline.
\\ ( https://arxiv.org/abs/2401.06800 ,  3073kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06802
Date: Wed, 10 Jan 2024 05:50:34 GMT   (173kb,D)

Title: Hierarchical Knowledge Distillation on Text Graph for Data-limited
  Attribute Inference
Authors: Quan Li, Shixiong Jing, Lingwei Chen
Categories: cs.CL cs.LG cs.SI
Comments: 11 pages, 2 figures, uses log_2022.sty
\\
  The popularization of social media increases user engagements and generates a
large amount of user-oriented data. Among them, text data (e.g., tweets, blogs)
significantly attracts researchers and speculators to infer user attributes
(e.g., age, gender, location) for fulfilling their intents. Generally, this
line of work casts attribute inference as a text classification problem, and
starts to leverage graph neural networks (GNNs) to utilize higher-level
representations of source texts. However, these text graphs are constructed
over words, suffering from high memory consumption and ineffectiveness on few
labeled texts. To address this challenge, we design a text-graph-based few-shot
learning model for attribute inferences on social media text data. Our model
first constructs and refines a text graph using manifold learning and message
passing, which offers a better trade-off between expressiveness and complexity.
Afterwards, to further use cross-domain texts and unlabeled texts to improve
few-shot performance, a hierarchical knowledge distillation is devised over
text graph to optimize the problem, which derives better text representations,
and advances model generalization ability. Experiments on social media datasets
demonstrate the state-of-the-art performance of our model on attribute
inferences with considerably fewer labeled texts.
\\ ( https://arxiv.org/abs/2401.06802 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06803
Date: Wed, 10 Jan 2024 09:56:36 GMT   (4491kb,D)

Title: Generative AI Meets Semantic Communication: Evolution and Revolution of
  Communication Tasks
Authors: Eleonora Grassucci, Jihong Park, Sergio Barbarossa, Seong-Lyun Kim,
  Jinho Choi, Danilo Comminiello
Categories: cs.CL cs.LG
Comments: Under consideration in IEEE Network Special Issue "The Interplay
  Between Generative AI and 5G-Advanced toward 6G"
\\
  While deep generative models are showing exciting abilities in computer
vision and natural language processing, their adoption in communication
frameworks is still far underestimated. These methods are demonstrated to
evolve solutions to classic communication problems such as denoising,
restoration, or compression. Nevertheless, generative models can unveil their
real potential in semantic communication frameworks, in which the receiver is
not asked to recover the sequence of bits used to encode the transmitted
(semantic) message, but only to regenerate content that is semantically
consistent with the transmitted message. Disclosing generative models
capabilities in semantic communication paves the way for a paradigm shift with
respect to conventional communication systems, which has great potential to
reduce the amount of data traffic and offers a revolutionary versatility to
novel tasks and applications that were not even conceivable a few years ago. In
this paper, we present a unified perspective of deep generative models in
semantic communication and we unveil their revolutionary role in future
communication frameworks, enabling emerging applications and tasks. Finally, we
analyze the challenges and opportunities to face to develop generative models
specifically tailored for communication systems.
\\ ( https://arxiv.org/abs/2401.06803 ,  4491kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06804
Date: Wed, 10 Jan 2024 13:39:49 GMT   (454kb)

Title: ChatGPT, Let us Chat Sign Language: Experiments, Architectural Elements,
  Challenges and Research Directions
Authors: Nada Shahin and Leila Ismail
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\
  ChatGPT is a language model based on Generative AI. Existing research work on
ChatGPT focused on its use in various domains. However, its potential for Sign
Language Translation (SLT) is yet to be explored. This paper addresses this
void. Therefore, we present GPT's evolution aiming a retrospective analysis of
the improvements to its architecture for SLT. We explore ChatGPT's capabilities
in translating different sign languages in paving the way to better
accessibility for deaf and hard-of-hearing community. Our experimental results
indicate that ChatGPT can accurately translate from English to American (ASL),
Australian (AUSLAN), and British (BSL) sign languages and from Arabic Sign
Language (ArSL) to English with only one prompt iteration. However, the model
failed to translate from Arabic to ArSL and ASL, AUSLAN, and BSL to Arabic.
Consequently, we present challenges and derive insights for future research
directions.
\\ ( https://arxiv.org/abs/2401.06804 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06805
Date: Wed, 10 Jan 2024 15:29:21 GMT   (2460kb,D)

Title: Exploring the Reasoning Abilities of Multimodal Large Language Models
  (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning
Authors: Yiqi Wang, Wentao Chen, Xiaotian Han, Xudong Lin, Haiteng Zhao,
  Yongfei Liu, Bohan Zhai, Jianbo Yuan, Quanzeng You, Hongxia Yang
Categories: cs.CL cs.AI
\\
  Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence
(AGI) with abstract reasoning ability is the goal of next-generation AI. Recent
advancements in Large Language Models (LLMs), along with the emerging field of
Multimodal Large Language Models (MLLMs), have demonstrated impressive
capabilities across a wide range of multimodal tasks and applications.
Particularly, various MLLMs, each with distinct model architectures, training
data, and training stages, have been evaluated across a broad range of MLLM
benchmarks. These studies have, to varying degrees, revealed different aspects
of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs
have not been systematically investigated. In this survey, we comprehensively
review the existing evaluation protocols of multimodal reasoning, categorize
and illustrate the frontiers of MLLMs, introduce recent trends in applications
of MLLMs on reasoning-intensive tasks, and finally discuss current practices
and future directions. We believe our survey establishes a solid base and sheds
light on this important topic, multimodal reasoning.
\\ ( https://arxiv.org/abs/2401.06805 ,  2460kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06806
Date: Wed, 10 Jan 2024 18:39:46 GMT   (36kb)

Title: AugSumm: towards generalizable speech summarization using synthetic
  labels from large language model
Authors: Jee-weon Jung, Roshan Sharma, William Chen, Bhiksha Raj, Shinji
  Watanabe
Categories: cs.CL cs.AI
Comments: This work has been submitted to the IEEE ICASSP for possible
  publication. Copyright may be transferred without notice, after which this
  version may no longer be accessible. 5 pages
\\
  Abstractive speech summarization (SSUM) aims to generate human-like summaries
from speech. Given variations in information captured and phrasing, recordings
can be summarized in multiple ways. Therefore, it is more reasonable to
consider a probabilistic distribution of all potential summaries rather than a
single summary. However, conventional SSUM models are mostly trained and
evaluated with a single ground-truth (GT) human-annotated deterministic summary
for every recording. Generating multiple human references would be ideal to
better represent the distribution statistically, but is impractical because
annotation is expensive. We tackle this challenge by proposing AugSumm, a
method to leverage large language models (LLMs) as a proxy for human annotators
to generate augmented summaries for training and evaluation. First, we explore
prompting strategies to generate synthetic summaries from ChatGPT. We validate
the quality of synthetic summaries using multiple metrics including human
evaluation, where we find that summaries generated using AugSumm are perceived
as more valid to humans. Second, we develop methods to utilize synthetic
summaries in training and evaluation. Experiments on How2 demonstrate that
pre-training on synthetic summaries and fine-tuning on GT summaries improves
ROUGE-L by 1 point on both GT and AugSumm-based test sets. AugSumm summaries
are available at https://github.com/Jungjee/AugSumm.
\\ ( https://arxiv.org/abs/2401.06806 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06807
Date: Wed, 10 Jan 2024 19:06:35 GMT   (13369kb,D)

Title: An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue
  Assistant
Authors: Mohit Tomar, Abhisek Tiwari, Tulika Saha, Prince Jha, Sriparna Saha
Categories: cs.CL cs.AI
\\
  In recent times, there has been an increasing awareness about imminent
environmental challenges, resulting in people showing a stronger dedication to
taking care of the environment and nurturing green life. The current $19.6
billion indoor gardening industry, reflective of this growing sentiment, not
only signifies a monetary value but also speaks of a profound human desire to
reconnect with the natural world. However, several recent surveys cast a
revealing light on the fate of plants within our care, with more than half
succumbing primarily due to the silent menace of improper care. Thus, the need
for accessible expertise capable of assisting and guiding individuals through
the intricacies of plant care has become paramount more than ever. In this
work, we make the very first attempt at building a plant care assistant, which
aims to assist people with plant(-ing) concerns through conversations. We
propose a plant care conversational dataset named Plantational, which contains
around 1K dialogues between users and plant care experts. Our end-to-end
proposed approach is two-fold : (i) We first benchmark the dataset with the
help of various large language models (LLMs) and visual language model (VLM) by
studying the impact of instruction tuning (zero-shot and few-shot prompting)
and fine-tuning techniques on this task; (ii) finally, we build EcoSage, a
multi-modal plant care assisting dialogue generation framework, incorporating
an adapter-based modality infusion using a gated mechanism. We performed an
extensive examination (both automated and manual evaluation) of the performance
exhibited by various LLMs and VLM in the generation of the domain-specific
dialogue responses to underscore the respective strengths and weaknesses of
these diverse models.
\\ ( https://arxiv.org/abs/2401.06807 ,  13369kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06808
Date: Wed, 10 Jan 2024 22:12:34 GMT   (4836kb,D)

Title: Grounded learning for compositional vector semantics
Authors: Martha Lewis
Categories: cs.CL cs.AI cs.NE
DOI: 10.1007/978-3-031-41862-4_10
\\
  Categorical compositional distributional semantics is an approach to
modelling language that combines the success of vector-based models of meaning
with the compositional power of formal semantics. However, this approach was
developed without an eye to cognitive plausibility. Vector representations of
concepts and concept binding are also of interest in cognitive science, and
have been proposed as a way of representing concepts within a biologically
plausible spiking neural network. This work proposes a way for compositional
distributional semantics to be implemented within a spiking neural network
architecture, with the potential to address problems in concept binding, and
give a small implementation. We also describe a means of training word
representations using labelled images.
\\ ( https://arxiv.org/abs/2401.06808 ,  4836kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06816
Date: Thu, 11 Jan 2024 16:34:09 GMT   (1894kb)

Title: When ChatGPT is gone: Creativity reverts and homogeneity persists
Authors: Qinghan Liu, Yiyong Zhou, Jihao Huang, Guiquan Li
Categories: cs.CL cs.AI cs.HC
Comments: 30pages,6figures
\\
  ChatGPT has been evidenced to enhance human performance in creative tasks.
Yet, it is still unclear if this boosting effect sustains with and without
ChatGPT. In a pre-registered seven-day lab experiment and a follow-up survey
after 30 days of experiment completion, we examined the impacts of ChatGPT
presence and absence on sustained creativity using a text dataset of 3302
creative ideas and 427 creative solutions from 61 college students.
Participants in the treatment group used ChatGPT in creative tasks, while those
in the control group completed the tasks by themselves. The findings show that
although the boosting effect of ChatGPT was consistently observed over a
five-day creative journey, human creative performance reverted to baseline when
ChatGPT was down on the 7th and the 30th day. More critically, the use of
ChatGPT in creative tasks resulted in increasingly homogenized contents, and
this homogenization effect persisted even when ChatGPT was absence. These
findings pose a challenge to the prevailing argument that ChatGPT can enhance
human creativity. In fact, generative AI like ChatGPT lends to human with a
temporary rise in creative performance but boxes human creative capability in
the long run, highlighting the imperative for cautious generative AI
integration in creative endeavors.
\\ ( https://arxiv.org/abs/2401.06816 ,  1894kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06817
Date: Thu, 11 Jan 2024 16:44:59 GMT   (4539kb,D)

Title: Analyzing Regional Impacts of Climate Change using Natural Language
  Processing Techniques
Authors: Tanwi Mallick, John Murphy, Joshua David Bergerson, Duane R. Verner,
  John K Hutchison, Leslie-Anne Levy
Categories: cs.CL cs.LG
\\
  Understanding the multifaceted effects of climate change across diverse
geographic locations is crucial for timely adaptation and the development of
effective mitigation strategies. As the volume of scientific literature on this
topic continues to grow exponentially, manually reviewing these documents has
become an immensely challenging task. Utilizing Natural Language Processing
(NLP) techniques to analyze this wealth of information presents an efficient
and scalable solution. By gathering extensive amounts of peer-reviewed articles
and studies, we can extract and process critical information about the effects
of climate change in specific regions. We employ BERT (Bidirectional Encoder
Representations from Transformers) for Named Entity Recognition (NER), which
enables us to efficiently identify specific geographies within the climate
literature. This, in turn, facilitates location-specific analyses. We conduct
region-specific climate trend analyses to pinpoint the predominant themes or
concerns related to climate change within a particular area, trace the temporal
progression of these identified issues, and evaluate their frequency, severity,
and potential development over time. These in-depth examinations of
location-specific climate data enable the creation of more customized
policy-making, adaptation, and mitigation strategies, addressing each region's
unique challenges and providing more effective solutions rooted in data-driven
insights. This approach, founded on a thorough exploration of scientific texts,
offers actionable insights to a wide range of stakeholders, from policymakers
to engineers to environmentalists. By proactively understanding these impacts,
societies are better positioned to prepare, allocate resources wisely, and
design tailored strategies to cope with future climate conditions, ensuring a
more resilient future for all.
\\ ( https://arxiv.org/abs/2401.06817 ,  4539kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06824
Date: Fri, 12 Jan 2024 00:50:04 GMT   (170kb,D)

Title: Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation
  Engineering
Authors: Tianlong Li, Xiaoqing Zheng, Xuanjing Huang
Categories: cs.CL cs.AI
Comments: Work in progress
\\
  Getting large language models (LLMs) to refuse to answer hostile toxicity
questions is a core issue under the theme of LLMs security. Previous approaches
have used prompts engineering to jailbreak LLMs and answer some toxicity
questions. These approaches can easily fail after the model manufacturer makes
additional fine-tuning to the model. To promote the further understanding of
model jailbreaking by researchers, we are inspired by Representation
Engineering to propose a jailbreaking method that does not require elaborate
construction prompts, is not affected by model fine-tuning, and can be widely
applied to any open-source LLMs in a pluggable manner. We have evaluated this
method on multiple mainstream LLMs on carefully supplemented toxicity datasets,
and the experimental results demonstrate the significant effectiveness of our
approach. After being surprised by some interesting jailbreaking cases, we did
extensive in-depth research to explore the techniques behind this method.
\\ ( https://arxiv.org/abs/2401.06824 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06829
Date: Fri, 12 Jan 2024 09:39:50 GMT   (870kb,D)

Title: Cross-Attention Watermarking of Large Language Models
Authors: Folco Bertini Baldassini, Huy H. Nguyen, Ching-Chung Chang, Isao
  Echizen
Categories: cs.CL cs.AI
Comments: 5 pages, 3 figures. Accepted to ICASSP 2024
\\
  A new approach to linguistic watermarking of language models is presented in
which information is imperceptibly inserted into the output text while
preserving its readability and original meaning. A cross-attention mechanism is
used to embed watermarks in the text during inference. Two methods using
cross-attention are presented that minimize the effect of watermarking on the
performance of a pretrained model. Exploration of different training strategies
for optimizing the watermarking and of the challenges and implications of
applying this approach in real-world scenarios clarified the tradeoff between
watermark robustness and text quality. Watermark selection substantially
affects the generated output for high entropy sentences. This proactive
watermarking approach has potential application in future model development.
\\ ( https://arxiv.org/abs/2401.06829 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06831
Date: Fri, 12 Jan 2024 10:29:48 GMT   (19kb)

Title: A Survey on the Applications of Frontier AI, Foundation Models, and
  Large Language Models to Intelligent Transportation Systems
Authors: Mohamed R. Shoaib, Heba M. Emara, Jun Zhao
Categories: cs.CL cs.AI
Comments: This paper appears in International Conference on Computer and
  Applications (ICCA) 2023
\\
  This survey paper explores the transformative influence of frontier AI,
foundation models, and Large Language Models (LLMs) in the realm of Intelligent
Transportation Systems (ITS), emphasizing their integral role in advancing
transportation intelligence, optimizing traffic management, and contributing to
the realization of smart cities. Frontier AI refers to the forefront of AI
technology, encompassing the latest advancements, innovations, and experimental
techniques in the field, especially AI foundation models and LLMs. Foundation
models, like GPT-4, are large, general-purpose AI models that provide a base
for a wide range of applications. They are characterized by their versatility
and scalability. LLMs are obtained from finetuning foundation models with a
specific focus on processing and generating natural language. They excel in
tasks like language understanding, text generation, translation, and
summarization. By leveraging vast textual data, including traffic reports and
social media interactions, LLMs extract critical insights, fostering the
evolution of ITS. The survey navigates the dynamic synergy between LLMs and
ITS, delving into applications in traffic management, integration into
autonomous vehicles, and their role in shaping smart cities. It provides
insights into ongoing research, innovations, and emerging trends, aiming to
inspire collaboration at the intersection of language, intelligence, and
mobility for safer, more efficient, and sustainable transportation systems. The
paper further surveys interactions between LLMs and various aspects of ITS,
exploring roles in traffic management, facilitating autonomous vehicles, and
contributing to smart city development, while addressing challenges brought by
frontier AI and foundation models. This paper offers valuable inspiration for
future research and innovation in the transformative domain of intelligent
transportation.
\\ ( https://arxiv.org/abs/2401.06831 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06832
Date: Fri, 12 Jan 2024 13:44:48 GMT   (294kb)

Title: XLS-R Deep Learning Model for Multilingual ASR on Low- Resource
  Languages: Indonesian, Javanese, and Sundanese
Authors: Panji Arisaputra, Alif Tri Handoyo and Amalia Zahra
Categories: cs.CL cs.LG cs.SD eess.AS
\\
  This research paper focuses on the development and evaluation of Automatic
Speech Recognition (ASR) technology using the XLS-R 300m model. The study aims
to improve ASR performance in converting spoken language into written text,
specifically for Indonesian, Javanese, and Sundanese languages. The paper
discusses the testing procedures, datasets used, and methodology employed in
training and evaluating the ASR systems. The results show that the XLS-R 300m
model achieves competitive Word Error Rate (WER) measurements, with a slight
compromise in performance for Javanese and Sundanese languages. The integration
of a 5-gram KenLM language model significantly reduces WER and enhances ASR
accuracy. The research contributes to the advancement of ASR technology by
addressing linguistic diversity and improving performance across various
languages. The findings provide insights into optimizing ASR accuracy and
applicability for diverse linguistic contexts.
\\ ( https://arxiv.org/abs/2401.06832 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06836
Date: Fri, 12 Jan 2024 16:42:10 GMT   (341kb,D)

Title: Enhancing the Emotional Generation Capability of Large Language Models
  via Emotional Chain-of-Thought
Authors: Zaijing Li, Gongwei Chen, Rui Shao, Dongmei Jiang, and Liqiang Nie
Categories: cs.CL cs.AI
\\
  The Emotional Generation is a subset of emotional intelligence, which aims to
output an emotional response based on emotional conditions as input. Emotion
generation has a wide range of applications, including emotion chat, emotional
visual caption, and emotional rewriting. However, it faces challenges such as a
lack of interpretability and poor evaluability. In this paper, we propose the
Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that
enhances the performance of Large Language Models (LLMs) on various emotional
generation tasks by aligning with human emotional intelligence guidelines. To
assess the reliability of ECoT, we propose an automated model-based evaluation
method called EGS. Extensive experimental results demonstrate the effectiveness
of ECoT and EGS. Further,we discuss the promise of LLMs in the field of
sentiment analysis and present key insights into the LLMs with the ECoT in
emotional generation tasks.
\\ ( https://arxiv.org/abs/2401.06836 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06837
Date: Fri, 12 Jan 2024 17:43:51 GMT   (7513kb,D)

Title: Structsum Generation for Faster Text Comprehension
Authors: Parag Jain, Andreea Marzoca, Francesco Piccinno
Categories: cs.CL cs.AI
\\
  We consider the task of generating structured representations of text using
large language models (LLMs). We focus on tables and mind maps as
representative modalities. Tables are more organized way of representing data,
while mind maps provide a visually dynamic and flexible approach, particularly
suitable for sparse content. Despite the effectiveness of LLMs on different
tasks, we show that current models struggle with generating structured outputs.
In response, we present effective prompting strategies for both of these tasks.
We introduce a taxonomy of problems around factuality, global and local
structure, common to both modalities and propose a set of critiques to tackle
these issues resulting in an absolute improvement in accuracy of +37pp (79%)
for mind maps and +15pp (78%) for tables. To evaluate semantic coverage of
generated structured representations we propose Auto-QA, and we verify the
adequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of
structured representations via a text comprehension user study. The results
show a significant reduction in comprehension time compared to text when using
table (42.9%) and mind map (31.9%), without loss in accuracy.
\\ ( https://arxiv.org/abs/2401.06837 ,  7513kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06838
Date: Fri, 12 Jan 2024 18:03:54 GMT   (6647kb,D)

Title: MAPO: Advancing Multilingual Reasoning through Multilingual
  Alignment-as-Preference Optimization
Authors: Shuaijie She, Shujian Huang, Wei Zou, Wenhao Zhu, Xiang Liu, Xiang
  Geng, Jiajun Chen
Categories: cs.CL
Comments: The project will be available at https://github.com/NJUNLP/MAPO
\\
  Though reasoning abilities are considered language-agnostic, existing LLMs
exhibit inconsistent reasoning abilities across different languages, e.g.,
reasoning in a pivot language is superior to other languages due to the
imbalance of multilingual training data.To enhance reasoning abilities in
non-pivot languages, we propose an alignment-as-preference optimization
framework. Specifically, we adopt an open-source translation model to estimate
the consistency between answers in non-pivot and pivot languages. We further
adopt the answer consistency as the preference for DPO or PPO thus optimizing
the lesser reasoning. Experiments show that our method significantly improves
the model's multilingual reasoning, with better reasoning consistency across
languages. Our framework achieved a 13.7% accuracy improvement on out-of-domain
datasets MSVAMP while preserving the competitive performance on MGSM. Moreover,
we find that iterative DPO is helpful for further alignment and improvement of
the model's multilingual mathematical reasoning ability, further pushing the
improvement to 16.7%
\\ ( https://arxiv.org/abs/2401.06838 ,  6647kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06853
Date: Fri, 12 Jan 2024 19:00:26 GMT   (7033kb,D)

Title: Large Language Models Can Learn Temporal Reasoning
Authors: Siheng Xiong, Ali Payani, Ramana Kompella, Faramarz Fekri
Categories: cs.CL
\\
  Large language models (LLMs) learn temporal concepts from the co-occurrence
of related tokens in a sequence. Compared with conventional text generation,
temporal reasoning, which reaches a conclusion based on mathematical, logical
and commonsense knowledge, is more challenging. In this paper, we propose
TempGraph-LLM, a new paradigm towards text-based temporal reasoning. To be
specific, we first teach LLMs to translate the context into a temporal graph. A
synthetic dataset, which is fully controllable and requires minimal
supervision, is constructed for pre-training on this task. We prove in
experiments that LLMs benefit from the pre-training on other tasks. On top of
that, we guide LLMs to perform symbolic reasoning with the strategies of Chain
of Thoughts (CoTs) bootstrapping and special data augmentation. We observe that
CoTs with symbolic reasoning bring more consistent and reliable results than
those using free text.
\\ ( https://arxiv.org/abs/2401.06853 ,  7033kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06855
Date: Fri, 12 Jan 2024 19:02:48 GMT   (1410kb,D)

Title: Fine-grained Hallucination Detection and Editing for Language Models
Authors: Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham
  Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi
Categories: cs.CL
\\
  Large language models (LMs) are prone to generate diverse factually incorrect
statements, which are widely called hallucinations. Current approaches
predominantly focus on coarse-grained automatic hallucination detection or
editing, overlooking nuanced error levels. In this paper, we propose a novel
task -- automatic fine-grained hallucination detection -- and present a
comprehensive taxonomy encompassing six hierarchically defined types of
hallucination. To facilitate evaluation, we introduce a new benchmark that
includes fine-grained human judgments on two LM outputs across various domains.
Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in
60% and 75% of their outputs, respectively, and a majority of these
hallucinations fall into categories that have been underexplored. As an initial
step to address this, we train FAVA, a retrieval-augmented LM by carefully
designing synthetic data generations to detect and correct fine-grained
hallucinations. On our benchmark, our automatic and human evaluations show that
FAVA significantly outperforms ChatGPT on fine-grained hallucination detection
by a large margin though a large room for future improvement still exists.
FAVA's suggested edits also improve the factuality of LM-generated text,
resulting in 5-10% FActScore improvements.
\\ ( https://arxiv.org/abs/2401.06855 ,  1410kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06866
Date: Fri, 12 Jan 2024 19:40:11 GMT   (22672kb,D)

Title: Health-LLM: Large Language Models for Health Prediction via Wearable
  Sensor Data
Authors: Yubin Kim, Xuhai Xu, Daniel McDuff, Cynthia Breazeal, Hae Won Park
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) are capable of many natural language tasks, yet
they are far from perfect. In health applications, grounding and interpreting
domain-specific and non-linguistic data is important. This paper investigates
the capacity of LLMs to deliver multi-modal health predictions based on
contextual information (e.g. user demographics, health knowledge) and
physiological data (e.g. resting heart rate, sleep minutes). We present a
comprehensive evaluation of eight state-of-the-art LLMs with diverse prompting
and fine-tuning techniques on six public health datasets (PM-Data, LifeSnaps,
GLOBEM, AW_FB, MIT-BIH & MIMIC-III). Our experiments cover thirteen consumer
health prediction tasks in mental health, activity, metabolic, sleep, and
cardiac assessment. Our fine-tuned model, Health-Alpaca exhibits comparable
performance to larger models (GPT-3.5 and GPT-4), achieving the best
performance in 5 out of 13 tasks. Ablation studies highlight the effectiveness
of context enhancement strategies, and generalization capability of the
fine-tuned models across training datasets and the size of training samples.
Notably, we observe that our context enhancement can yield up to 23.8%
improvement in performance. While constructing contextually rich prompts
(combining user context, health knowledge and temporal information) exhibits
synergistic improvement, the inclusion of health knowledge context in prompts
significantly enhances overall performance.
\\ ( https://arxiv.org/abs/2401.06866 ,  22672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06877
Date: Fri, 12 Jan 2024 20:08:39 GMT   (358kb,D)

Title: Promptly Predicting Structures: The Return of Inference
Authors: Maitrey Mehta, Valentina Pyatkin, Vivek Srikumar
Categories: cs.CL
Comments: 19 pages, 13 figures
\\
  Prompt-based methods have been used extensively across NLP to build zero- and
few-shot label predictors. Many NLP tasks are naturally structured: that is,
their outputs consist of multiple labels which constrain each other. Annotating
data for such tasks can be cumbersome. Can the promise of the prompt-based
paradigm be extended to such structured outputs? In this paper, we present a
framework for constructing zero- and few-shot linguistic structure predictors.
Our key insight is that we can use structural constraints -- and combinatorial
inference derived from them -- to filter out inconsistent structures predicted
by large language models. We instantiated this framework on two structured
prediction tasks, and five datasets. Across all cases, our results show that
enforcing consistency not only constructs structurally valid outputs, but also
improves performance over the unconstrained variants.
\\ ( https://arxiv.org/abs/2401.06877 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06915
Date: Fri, 12 Jan 2024 22:19:22 GMT   (2190kb,D)

Title: DocFinQA: A Long-Context Financial Reasoning Dataset
Authors: Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Chris Tanner
Categories: cs.CL cs.AI
Comments: 13 pages
\\
  Research in quantitative reasoning within the financial domain indeed
necessitates the use of realistic tasks and data, primarily because of the
significant impact of decisions made in business and finance. Financial
professionals often interact with documents hundreds of pages long, but most
research datasets drastically reduce this context length. To address this, we
introduce a long-document financial QA task. We augment 7,621 questions from
the existing FinQA dataset with full-document context, extending the average
context length for each question from under 700 words in FinQA to 123k words in
DocFinQA. We conduct extensive experiments of retrieval-based QA pipelines and
long-context language models on the augmented data. Our results show that
DocFinQA provides challenges for even the strongest, state-of-the-art systems.
\\ ( https://arxiv.org/abs/2401.06915 ,  2190kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06920
Date: Fri, 12 Jan 2024 22:27:25 GMT   (30kb,D)

Title: Comparing GPT-4 and Open-Source Language Models in Misinformation
  Mitigation
Authors: Tyler Vergho, Jean-Francois Godbout, Reihaneh Rabbany, Kellin Pelrine
Categories: cs.CL
\\
  Recent large language models (LLMs) have been shown to be effective for
misinformation detection. However, the choice of LLMs for experiments varies
widely, leading to uncertain conclusions. In particular, GPT-4 is known to be
strong in this domain, but it is closed source, potentially expensive, and can
show instability between different versions. Meanwhile, alternative LLMs have
given mixed results. In this work, we show that Zephyr-7b presents a
consistently viable alternative, overcoming key limitations of commonly used
approaches like Llama-2 and GPT-3.5. This provides the research community with
a solid open-source option and shows open-source models are gradually catching
up on this task. We then highlight how GPT-3.5 exhibits unstable performance,
such that this very widely used model could provide misleading results in
misinformation detection. Finally, we validate new tools including approaches
to structured output and the latest version of GPT-4 (Turbo), showing they do
not compromise performance, thus unlocking them for future research and
potentially enabling more complex pipelines for misinformation mitigation.
\\ ( https://arxiv.org/abs/2401.06920 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06930
Date: Fri, 12 Jan 2024 23:33:01 GMT   (1660kb,D)

Title: PizzaCommonSense: Learning to Model Commonsense Reasoning about
  Intermediate Steps in Cooking Recipes
Authors: Aissatou Diallo, Antonis Bikakis, Luke Dickens, Anthony Hunter, Rob
  Miller
Categories: cs.CL
Comments: The data is available at:
  https://github.com/adiallo07/PizzaCommonsense
\\
  Decoding the core of procedural texts, exemplified by cooking recipes, is
crucial for intelligent reasoning and instruction automation. Procedural texts
can be comprehensively defined as a sequential chain of steps to accomplish a
task employing resources. From a cooking perspective, these instructions can be
interpreted as a series of modifications to a food preparation, which initially
comprises a set of ingredients. These changes involve transformations of
comestible resources. For a model to effectively reason about cooking recipes,
it must accurately discern and understand the inputs and outputs of
intermediate steps within the recipe. Aiming to address this, we present a new
corpus of cooking recipes enriched with descriptions of intermediate steps of
the recipes that explicate the input and output for each step. We discuss the
data collection process, investigate and provide baseline models based on T5
and GPT-3.5. This work presents a challenging task and insight into commonsense
reasoning and procedural text generation.
\\ ( https://arxiv.org/abs/2401.06930 ,  1660kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06935
Date: Sat, 13 Jan 2024 00:08:23 GMT   (585kb,D)

Title: MiTTenS: A Dataset for Evaluating Misgendering in Translation
Authors: Kevin Robinson, Sneha Kudugunta, Romina Stella, Sunipa Dev, Jasmijn
  Bastings
Categories: cs.CL cs.CY
Comments: GitHub repository https://github.com/google-research-datasets/mittens
\\
  Misgendering is the act of referring to someone in a way that does not
reflect their gender identity. Translation systems, including foundation models
capable of translation, can produce errors that result in misgendering harms.
To measure the extent of such potential harms when translating into and out of
English, we introduce a dataset, MiTTenS, covering 26 languages from a variety
of language families and scripts, including several traditionally
underpresented in digital resources. The dataset is constructed with
handcrafted passages that target known failure patterns, longer synthetically
generated passages, and natural passages sourced from multiple domains. We
demonstrate the usefulness of the dataset by evaluating both dedicated neural
machine translation systems and foundation models, and show that all systems
exhibit errors resulting in misgendering harms, even in high resource
languages.
\\ ( https://arxiv.org/abs/2401.06935 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06945
Date: Sat, 13 Jan 2024 01:22:15 GMT   (829kb,D)

Title: Knowledge-Centric Templatic Views of Documents
Authors: Isabel Cachola, Silviu Cucerzan, Allen Herring, Vuksan Mijovic, Erik
  Oveson, Sujay Kumar Jauhar
Categories: cs.CL
\\
  Authors seeking to communicate with broader audiences often compose their
ideas about the same underlying knowledge in different documents and formats --
for example, as slide decks, newsletters, reports, brochures, etc. Prior work
in document generation has generally considered the creation of each separate
format to be different a task, developing independent methods for generation
and evaluation. This approach is suboptimal for the advancement of AI-supported
content authoring from both research and application perspectives because it
leads to fragmented learning processes, redundancy in models and methods, and
disjointed evaluation. Thus, in our work, we consider each of these documents
to be templatic views of the same underlying knowledge, and we aim to unify the
generation and evaluation of these templatic views of documents. We begin by
introducing an LLM-powered method to extract the most important information
from an input document and represent this information in a structured format.
We show that this unified representation can be used to generate multiple
templatic views with no supervision and with very little guidance, improving
over strong baselines. We additionally introduce a unified evaluation method
that is template agnostic, and can be adapted to building document generators
for heterogeneous downstream applications. Finally, we conduct a human
evaluation, which shows that humans prefer 82% of the downstream documents
generated with our method. Furthermore, the newly proposed evaluation metric
correlates more highly with human judgement than prior metrics, while providing
a unified evaluation method.
\\ ( https://arxiv.org/abs/2401.06945 ,  829kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06947
Date: Sat, 13 Jan 2024 01:46:20 GMT   (7167kb,D)

Title: Parameter-Efficient Detoxification with Contrastive Decoding
Authors: Tong Niu, Caiming Xiong, Semih Yavuz, Yingbo Zhou
Categories: cs.CL cs.AI
\\
  The field of natural language generation has witnessed significant
advancements in recent years, including the development of controllable text
generation techniques. However, controlling the attributes of the generated
text remains a challenge, especially when aiming to avoid undesirable behavior
such as toxicity. In this work, we introduce Detoxification Generator
(DETOXIGEN), an inference-time algorithm that steers the generation away from
unwanted styles. DETOXIGEN is an ensemble of a pre-trained language model
(generator) and a detoxifier. The detoxifier is trained intentionally on the
toxic data representative of the undesirable attribute, encouraging it to
generate text in that style exclusively. During the actual generation, we use
the trained detoxifier to produce undesirable tokens for the generator to
contrast against at each decoding step. This approach directly informs the
generator to avoid generating tokens that the detoxifier considers highly
likely. We evaluate DETOXIGEN on the commonly used REALTOXICITYPROMPTS
benchmark (Gehman et al., 2020) with various language models as generators. We
find that it significantly outperforms previous approaches in detoxification
metrics while not compromising on the generation quality. Moreover, the
detoxifier is obtained by soft prompt-tuning using the same backbone language
model as the generator. Hence, DETOXIGEN requires only a tiny amount of extra
weights from the virtual tokens of the detoxifier to be loaded into GPU memory
while decoding, making it a promising lightweight, practical, and
parameter-efficient detoxification strategy.
\\ ( https://arxiv.org/abs/2401.06947 ,  7167kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06951
Date: Sat, 13 Jan 2024 02:11:20 GMT   (4255kb,D)

Title: E^2-LLM: Efficient and Extreme Length Extension of Large Language Models
Authors: Jiaheng Liu, Zhiqi Bai, Yuanxing Zhang, Chenchen Zhang, Yu Zhang, Ge
  Zhang, Jiakai Wang, Haoran Que, Yukang Chen, Wenbo Su, Tiezheng Ge, Jie Fu,
  Wenhu Chen, Bo Zheng
Categories: cs.CL cs.AI
\\
  Typically, training LLMs with long context sizes is computationally
expensive, requiring extensive training hours and GPU resources. Existing
long-context extension methods usually need additional training procedures to
support corresponding long-context windows, where the long-context training
data (e.g., 32k) is needed, and high GPU training costs are assumed. To address
the aforementioned issues, we propose an Efficient and Extreme length extension
method for Large Language Models, called E 2 -LLM, with only one training
procedure and dramatically reduced computation cost, which also removes the
need to collect long-context data. Concretely, first, the training data of our
E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost
greatly. Second, the training procedure on the short training context window is
performed only once time, and we can support different evaluation context
windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,
we introduce two different augmentation methods on the scale and position index
parameters for different samples in training. It aims to make the model more
robust to the different relative differences when directly interpolating the
arbitrary context length at inference. Comprehensive experimental results on
multiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on
challenging long-context tasks.
\\ ( https://arxiv.org/abs/2401.06951 ,  4255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06954
Date: Sat, 13 Jan 2024 02:20:17 GMT   (8554kb,D)

Title: Bridging the Preference Gap between Retrievers and LLMs
Authors: Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei and
  Michael Bendersky
Categories: cs.CL
\\
  Large Language Models (LLMs) have demonstrated superior results across a wide
range of tasks, while retrieval has long been established as an effective means
of obtaining task-relevant information for humans. Retrieval-augmented
Generation (RAG) are known for their effectiveness in knowledge-intensive tasks
by locating relevant information and placing it within the context window of
the LLM. However, the relationship between retrievers and LLMs is still
under-investigated. Most existing work treats the retriever and the LLM as
independent components and leaves a gap between retrieving human-friendly
information and assembling a LLM-friendly context. In this work, we examine a
novel bridge model, validate the ranking and selection assumptions in
retrievers in the context of RAG, and propose a training framework that chains
together supervised and reinforcement learning to learn a bridge model.
Empirical results demonstrate the effectiveness of our method in both
question-answering and personalized generation tasks.
\\ ( https://arxiv.org/abs/2401.06954 ,  8554kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06961
Date: Sat, 13 Jan 2024 03:18:16 GMT   (1153kb,D)

Title: CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs'
  Mathematical Reasoning Capabilities
Authors: Yujun Mao, Yoon Kim, Yilun Zhou
Categories: cs.CL cs.AI cs.LG
Comments: Project website at https://yujunmao1.github.io/CHAMP/
\\
  Recent large language models (LLMs) have shown indications of mathematical
reasoning ability. However it has not been clear how they would fare on more
challenging competition-level problems. And while self-generated verbalizations
of intermediate reasoning steps (i.e., chain-of-thought prompting) have been
shown to be helpful, whether LLMs can make use of helpful side information such
as problem-specific hints has not been investigated before. In this paper, we
propose a challenging benchmark dataset for enabling such analyses. The Concept
and Hint-Annotated Math Problems (CHAMP) consists of high school math
competition problems, annotated with concepts, or general math facts, and
hints, or problem-specific tricks. These annotations allow us to explore the
effects of additional information, such as relevant hints, misleading concepts,
or related problems. This benchmark is difficult, with the best model only
scoring 58.1% in standard settings. With concepts and hints, performance
sometimes improves, indicating that some models can make use of such side
information. We further annotate model-generated solutions for their
correctness. Using this corpus, we find that models often arrive at the correct
final answer through wrong reasoning steps. In addition, we test whether models
are able to verify these solutions, and find that most models struggle. The
dataset and code are available on the project website.
\\ ( https://arxiv.org/abs/2401.06961 ,  1153kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06980
Date: Sat, 13 Jan 2024 05:01:47 GMT   (189kb,D)

Title: Joint Unsupervised and Supervised Training for Automatic Speech
  Recognition via Bilevel Optimization
Authors: A F M Saif, Xiaodong Cui, Han Shen, Songtao Lu, Brian Kingsbury,
  Tianyi Chen
Categories: cs.CL
Comments: This paper has been accepted in ICASSP-2024 conference
\\
  In this paper, we present a novel bilevel optimization-based training
approach to training acoustic models for automatic speech recognition (ASR)
tasks that we term {bi-level joint unsupervised and supervised training
(BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an
unsupervised loss and a supervised loss respectively, leveraging recent
advances in penalty-based bilevel optimization to solve this challenging ASR
problem with affordable complexity and rigorous convergence guarantees.} To
evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2
datasets have been conducted. BL-JUST achieves superior performance over the
commonly used pre-training followed by fine-tuning strategy.
\\ ( https://arxiv.org/abs/2401.06980 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07004
Date: Sat, 13 Jan 2024 07:57:01 GMT   (7728kb,D)

Title: Extending LLMs' Context Window with 100 Samples
Authors: Yikai Zhang, Junlong Li, Pengfei Liu
Categories: cs.CL
\\
  Large Language Models (LLMs) are known to have limited extrapolation ability
beyond their pre-trained context window, constraining their application in
downstream tasks with lengthy inputs. Recent studies have sought to extend
LLMs' context window by modifying rotary position embedding (RoPE), a popular
position encoding method adopted by well-known LLMs such as LLaMA, PaLM, and
GPT-NeoX. However, prior works like Position Interpolation (PI) and YaRN are
resource-intensive and lack comparative experiments to assess their
applicability. In this work, we identify the inherent need for LLMs' attention
entropy (i.e. the information entropy of attention scores) to maintain
stability and introduce a novel extension to RoPE which combines adjusting
RoPE's base frequency and scaling the attention logits to help LLMs efficiently
adapt to a larger context window. We validate the superiority of our method in
both fine-tuning performance and robustness across different context window
sizes on various context-demanding tasks. Notably, our method extends the
context window of LLaMA-2-7B-Chat to 16,384 with only 100 samples and 6
training steps, showcasing extraordinary efficiency. Finally, we also explore
how data compositions and training curricula affect context window extension
for specific downstream tasks, suggesting fine-tuning LLMs with lengthy
conversations as a good starting point. We release our code and SFT data at
https://github.com/GAIR-NLP/Entropy-ABF.
\\ ( https://arxiv.org/abs/2401.07004 ,  7728kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07009
Date: Sat, 13 Jan 2024 08:27:24 GMT   (347kb,D)

Title: Joint Extraction of Uyghur Medicine Knowledge with Edge Computing
Authors: Fan Lu, Quan Qi, Huaibin Qin
Categories: cs.CL cs.AI
Comments: 11 pages,6 figures,Has been accepted by Tsinghua Science and
  Technology
\\
  Medical knowledge extraction methods based on edge computing deploy deep
learning models on edge devices to achieve localized entity and relation
extraction. This approach avoids transferring substantial sensitive data to
cloud data centers, effectively safeguarding the privacy of healthcare
services. However, existing relation extraction methods mainly employ a
sequential pipeline approach, which classifies relations between determined
entities after entity recognition. This mode faces challenges such as error
propagation between tasks, insufficient consideration of dependencies between
the two subtasks, and the neglect of interrelations between different relations
within a sentence. To address these challenges, a joint extraction model with
parameter sharing in edge computing is proposed, named CoEx-Bert. This model
leverages shared parameterization between two models to jointly extract
entities and relations. Specifically, CoEx-Bert employs two models, each
separately sharing hidden layer parameters, and combines these two loss
functions for joint backpropagation to optimize the model parameters.
Additionally, it effectively resolves the issue of entity overlapping when
extracting knowledge from unstructured Uyghur medical texts by considering
contextual relations. Finally, this model is deployed on edge devices for
real-time extraction and inference of Uyghur medical knowledge. Experimental
results demonstrate that CoEx-Bert outperforms existing state-of-the-art
methods, achieving accuracy, recall, and F1 scores of 90.65\%, 92.45\%, and
91.54\%, respectively, in the Uyghur traditional medical literature dataset.
These improvements represent a 6.45\% increase in accuracy, a 9.45\% increase
in recall, and a 7.95\% increase in F1 score compared to the baseline.
\\ ( https://arxiv.org/abs/2401.07009 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07013
Date: Sat, 13 Jan 2024 08:43:32 GMT   (359kb,D)

Title: Knowledge Distillation for Closed-Source Language Models
Authors: Hongzhan Chen, Xiaojun Quan, Hehong Chen, Ming Yan and Ji Zhang
Categories: cs.CL
\\
  Closed-source language models such as GPT-4 have achieved remarkable
performance. Many recent studies focus on enhancing the capabilities of smaller
models through knowledge distillation from closed-source language models.
However, due to the incapability to directly access the weights, hidden states,
and output distributions of these closed-source models, the distillation can
only be performed by fine-tuning smaller models with data samples generated by
closed-source language models, which constrains the effectiveness of knowledge
distillation. In this paper, we propose to estimate the output distributions of
closed-source language models within a Bayesian estimation framework, involving
both prior and posterior estimation. The prior estimation aims to derive a
prior distribution by utilizing the corpus generated by closed-source language
models, while the posterior estimation employs a proxy model to update the
prior distribution and derive a posterior distribution. By leveraging the
estimated output distribution of closed-source language models, traditional
knowledge distillation can be executed. Experimental results demonstrate that
our method surpasses the performance of current models directly fine-tuned on
data generated by closed-source language models.
\\ ( https://arxiv.org/abs/2401.07013 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07037
Date: Sat, 13 Jan 2024 10:53:53 GMT   (1126kb,D)

Title: xCoT: Cross-lingual Instruction Tuning for Cross-lingual
  Chain-of-Thought Reasoning
Authors: Linzheng Chai, Jian Yang, Tao Sun, Hongcheng Guo, Jiaheng Liu, Bing
  Wang, Xiannian Liang, Jiaqi Bai, Tongliang Li, Qiyao Peng, Zhoujun Li
Categories: cs.CL cs.AI
Comments: 11 pages
\\
  Chain-of-thought (CoT) has emerged as a powerful technique to elicit
reasoning in large language models and improve a variety of downstream tasks.
CoT mainly demonstrates excellent performance in English, but its usage in
low-resource languages is constrained due to poor language generalization. To
bridge the gap among different languages, we propose a cross-lingual
instruction fine-tuning framework (xCOT) to transfer knowledge from
high-resource languages to low-resource languages. Specifically, the
multilingual instruction training data (xCOT-INSTRUCT) is created to encourage
the semantic alignment of multiple languages. We introduce cross-lingual
in-context few-shot learning (xICL)) to accelerate multilingual agreement in
instruction tuning, where some fragments of source languages in examples are
randomly substituted by their counterpart translations of target languages.
During multilingual instruction tuning, we adopt the randomly online CoT
strategy to enhance the multilingual reasoning ability of the large language
model by first translating the query to another language and then answering in
English. To further facilitate the language transfer, we leverage the
high-resource CoT to supervise the training of low-resource languages with
cross-lingual distillation. Experimental results on previous benchmarks
demonstrate the superior performance of xCoT in reducing the gap among
different languages, highlighting its potential to reduce the cross-lingual
gap.
\\ ( https://arxiv.org/abs/2401.07037 ,  1126kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07078
Date: Sat, 13 Jan 2024 13:46:14 GMT   (2531kb,D)

Title: PUB: A Pragmatics Understanding Benchmark for Assessing LLMs' Pragmatics
  Capabilities
Authors: Settaluri Lakshmi Sravanthi, Meet Doshi, Tankala Pavan Kalyan, Rudra
  Murthy, Pushpak Bhattacharyya, Raj Dabre
Categories: cs.CL
\\
  LLMs have demonstrated remarkable capability for understanding semantics, but
they often struggle with understanding pragmatics. To demonstrate this fact, we
release a Pragmatics Understanding Benchmark (PUB) dataset consisting of
fourteen tasks in four pragmatics phenomena, namely, Implicature,
Presupposition, Reference, and Deixis. We curated high-quality test sets for
each task, consisting of Multiple Choice Question Answers (MCQA). PUB includes
a total of 28k data points, 6.1k of which have been created by us, and the rest
are adapted from existing datasets. We evaluated nine models varying in the
number of parameters and type of training. Our study indicates that fine-tuning
for instruction-following and chat significantly enhances the pragmatics
capabilities of smaller language models. However, for larger models, the base
versions perform comparably with their chat-adapted counterparts. Additionally,
there is a noticeable performance gap between human capabilities and model
capabilities. Furthermore, unlike the consistent performance of humans across
various tasks, the models demonstrate variability in their proficiency, with
performance levels fluctuating due to different hints and the complexities of
tasks within the same dataset. Overall, the benchmark aims to provide a
comprehensive evaluation of LLM's ability to handle real-world language tasks
that require pragmatic reasoning.
\\ ( https://arxiv.org/abs/2401.07078 ,  2531kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07098
Date: Sat, 13 Jan 2024 15:18:44 GMT   (10204kb,D)

Title: A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ
  Generation using GPT
Authors: Subhankar Maity, Aniket Deroy, Sudeshna Sarkar
Categories: cs.CL
Comments: Accepted at ECIR 2024(short paper)
\\
  We introduce a multi-stage prompting approach (MSP) for the generation of
multiple choice questions (MCQs), harnessing the capabilities of GPT models
such as text-davinci-003 and GPT-4, renowned for their excellence across
various NLP tasks. Our approach incorporates the innovative concept of
chain-of-thought prompting, a progressive technique in which the GPT model is
provided with a series of interconnected cues to guide the MCQ generation
process. Automated evaluations consistently demonstrate the superiority of our
proposed MSP method over the traditional single-stage prompting (SSP) baseline,
resulting in the production of high-quality distractors. Furthermore, the
one-shot MSP technique enhances automatic evaluation results, contributing to
improved distractor generation in multiple languages, including English,
German, Bengali, and Hindi. In human evaluations, questions generated using our
approach exhibit superior levels of grammaticality, answerability, and
difficulty, highlighting its efficacy in various languages.
\\ ( https://arxiv.org/abs/2401.07098 ,  10204kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07103
Date: Sat, 13 Jan 2024 15:59:09 GMT   (7300kb,D)

Title: Leveraging Large Language Models for NLG Evaluation: A Survey
Authors: Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, Chongyang Tao
Categories: cs.CL
Comments: 19pages
\\
  In the rapidly evolving domain of Natural Language Generation (NLG)
evaluation, introducing Large Language Models (LLMs) has opened new avenues for
assessing generated content quality, e.g., coherence, creativity, and context
relevance. This survey aims to provide a thorough overview of leveraging LLMs
for NLG evaluation, a burgeoning area that lacks a systematic analysis. We
propose a coherent taxonomy for organizing existing LLM-based evaluation
metrics, offering a structured framework to understand and compare these
methods. Our detailed exploration includes critically assessing various
LLM-based methodologies, as well as comparing their strengths and limitations
in evaluating NLG outputs. By discussing unresolved challenges, including bias,
robustness, domain-specificity, and unified evaluation, this survey seeks to
offer insights to researchers and advocate for fairer and more advanced NLG
evaluation techniques.
\\ ( https://arxiv.org/abs/2401.07103 ,  7300kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07105
Date: Sat, 13 Jan 2024 16:09:49 GMT   (8352kb,D)

Title: Graph Language Models
Authors: Moritz Plenz, Anette Frank
Categories: cs.CL cs.AI cs.LG
Comments: 9 pages, 4 figures, 5 tables
ACM-class: I.2.0; I.2.4; I.2.7
\\
  While Language Models have become workhorses for NLP, their interplay with
textual knowledge graphs (KGs) - structured memories of general or domain
knowledge - is actively researched. Current embedding methodologies for such
graphs typically either (i) linearize graphs for embedding them using
sequential Language Models (LMs), which underutilize structural information, or
(ii) use Graph Neural Networks (GNNs) to preserve graph structure, while GNNs
cannot represent textual features as well as a pre-trained LM could. In this
work we introduce a novel language model, the Graph Language Model (GLM), that
integrates the strengths of both approaches, while mitigating their weaknesses.
The GLM parameters are initialized from a pretrained LM, to facilitate nuanced
understanding of individual concepts and triplets. Simultaneously, its
architectural design incorporates graph biases, thereby promoting effective
knowledge distribution within the graph. Empirical evaluations on relation
classification tasks on ConceptNet subgraphs reveal that GLM embeddings surpass
both LM- and GNN-based baselines in supervised and zero-shot settings.
\\ ( https://arxiv.org/abs/2401.07105 ,  8352kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07128
Date: Sat, 13 Jan 2024 18:09:05 GMT   (9030kb,D)

Title: EHRAgent: Code Empowers Large Language Models for Complex Tabular
  Reasoning on Electronic Health Records
Authors: Wenqi Shi, Ran Xu, Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu, Yuanda
  Zhu, Joyce Ho, Carl Yang, May D. Wang
Categories: cs.CL cs.AI
Comments: Work in Progress
\\
  Large language models (LLMs) have demonstrated exceptional capabilities in
planning and tool utilization as autonomous agents, but few have been developed
for medical problem-solving. We propose EHRAgent1, an LLM agent empowered with
a code interface, to autonomously generate and execute code for complex
clinical tasks within electronic health records (EHRs). First, we formulate an
EHR question-answering task into a tool-use planning process, efficiently
decomposing a complicated task into a sequence of manageable actions. By
integrating interactive coding and execution feedback, EHRAgent learns from
error messages and improves the originally generated code through iterations.
Furthermore, we enhance the LLM agent by incorporating long-term memory, which
allows EHRAgent to effectively select and build upon the most relevant
successful cases from past experiences. Experiments on two real-world EHR
datasets show that EHRAgent outperforms the strongest LLM agent baseline by
36.48% and 12.41%, respectively. EHRAgent leverages the emerging few-shot
learning capabilities of LLMs, enabling autonomous code generation and
execution to tackle complex clinical tasks with minimal demonstrations.
\\ ( https://arxiv.org/abs/2401.07128 ,  9030kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07190
Date: Sun, 14 Jan 2024 03:16:49 GMT   (7344kb,D)

Title: Inroads to a Structured Data Natural Language Bijection and the role of
  LLM annotation
Authors: Blake Vente
Categories: cs.CL
Comments: Graduate Coursework
\\
  This work finds limited evidence supporting the theory that using multiple
tasks with sequence-to-sequence transformer language models can improve
performance on some metrics. In particular, the multi-task generalist t5-small
outperforms the specialist t5-small with a $F_1$ of $0.771$ up from $0.692$,
which may point to underlying cross-task knowledge generalization. This further
suggests that even with the same network, "re-using" the same data in a
different way may lead to higher performance in some metrics. However, the
inverse task alone is likely only an optimization strategy, since it does not
yield a significant general improvement at the model sizes explored in this
work. Also, adding $\approx 4500$ LLM annotated records (interlaced with the
$12800$ WebNLG training records) does not substantially change automatic metric
performance compared to the same t5-small model without the synthetic data.
This may be due to a learning capacity bottleneck on account of model size, and
decreases observed may be due to distributional differences in the corpora.
Future research using larger models or human evaluation is required to more
fully explain the mechanisms contributing to performance on these tasks.
\\ ( https://arxiv.org/abs/2401.07190 ,  7344kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07237
Date: Sun, 14 Jan 2024 09:34:42 GMT   (8612kb,D)

Title: Distilling Event Sequence Knowledge From Large Language Models
Authors: Somin Wadhwa, Oktie Hassanzadeh, Debarun Bhattacharjya, Ken Barker,
  Jian Ni
Categories: cs.CL cs.AI
Comments: Under Review
\\
  Event sequence models have been found to be highly effective in the analysis
and prediction of events. Building such models requires availability of
abundant high-quality event sequence data. In certain applications, however,
clean structured event sequences are not available, and automated sequence
extraction results in data that is too noisy and incomplete. In this work, we
explore the use of Large Language Models (LLMs) to generate event sequences
that can effectively be used for probabilistic event model construction. This
can be viewed as a mechanism of distilling event sequence knowledge from LLMs.
Our approach relies on a Knowledge Graph (KG) of event concepts with partial
causal relations to guide the generative language model for causal event
sequence generation. We show that our approach can generate high-quality event
sequences, filling a knowledge gap in the input KG. Furthermore, we explore how
the generated sequences can be leveraged to discover useful and more complex
structured knowledge from pattern mining and probabilistic event models. We
release our sequence generation code and evaluation framework, as well as
corpus of event sequence data.
\\ ( https://arxiv.org/abs/2401.07237 ,  8612kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07284
Date: Sun, 14 Jan 2024 13:11:31 GMT   (734kb,D)

Title: Improving Domain Adaptation through Extended-Text Reading Comprehension
Authors: Ting Jiang, Shaohan Huang, Shengyue Luo, Zihan Zhang, Haizhen Huang,
  Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang
Categories: cs.CL
Comments: Work in Progress
\\
  To enhance the domain-specific capabilities of large language models,
continued pre-training on a domain-specific corpus is a prevalent method.
Recent work demonstrates that adapting models using reading comprehension data
formatted by regex-based patterns can significantly improve performance on
domain-specific tasks. However, regex-based patterns are incapable of parsing
raw corpora using domain-specific knowledge. Furthermore, the question and
answer pairs are extracted directly from the corpus in predefined formats
offers limited context. To address this limitation, we improve reading
comprehension via LLM and clustering. LLM focuses on leveraging domain
knowledge within the corpus to refine comprehension stage, while clustering
supplies relevant knowledge by extending the context to enrich reading stage.
Additionally, our method incorporates parameter-efficient fine-tuning to
improve the efficiency of domain adaptation. In comparison to AdaptLLM, our
method achieves an improvement exceeding 5% in domain-specific tasks. Our code
will available at https://github.com/microsoft/LMOps.
\\ ( https://arxiv.org/abs/2401.07284 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07286
Date: Sun, 14 Jan 2024 13:24:30 GMT   (564kb,D)

Title: CANDLE: Iterative Conceptualization and Instantiation Distillation from
  Large Language Models for Commonsense Reasoning
Authors: Weiqi Wang, Tianqing Fang, Chunyang Li, Haochen Shi, Wenxuan Ding,
  Baixuan Xu, Zhaowei Wang, Jiaxin Bai, Xin Liu, Jiayang Cheng, Chunkit Chan,
  Yangqiu Song
Categories: cs.CL
\\
  The sequential process of conceptualization and instantiation is essential to
generalizable commonsense reasoning as it allows the application of existing
knowledge to unfamiliar scenarios. However, existing works tend to undervalue
the step of instantiation and heavily rely on pre-built concept taxonomies and
human annotations to collect both types of knowledge, resulting in a lack of
instantiated knowledge to complete reasoning, high cost, and limited
scalability. To tackle these challenges, we introduce CANDLE, a distillation
framework that iteratively performs contextualized conceptualization and
instantiation over commonsense knowledge bases by instructing large language
models to generate both types of knowledge with critic filtering. By applying
CANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six
million conceptualizations and instantiated commonsense knowledge triples. Both
types of knowledge are firmly rooted in the original ATOMIC dataset, and
intrinsic evaluations demonstrate their exceptional quality and diversity.
Empirical results indicate that distilling CANDLE on student models provides
benefits across four downstream tasks. Our code, data, and models are publicly
available at https://github.com/HKUST-KnowComp/CANDLE.
\\ ( https://arxiv.org/abs/2401.07286 ,  564kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07301
Date: Sun, 14 Jan 2024 14:29:07 GMT   (246kb,D)

Title: Small Language Model Can Self-correct
Authors: Haixia Han, Jiaqing Liang, Jie Shi, Qianyu He, Yanghua Xiao
Categories: cs.CL cs.AI
\\
  Generative Language Models (LMs) such as ChatGPT have exhibited remarkable
performance across various downstream tasks. Nevertheless, one of their most
prominent drawbacks is generating inaccurate or false information with a
confident tone. Previous studies have devised sophisticated pipelines and
prompts to induce large LMs to exhibit the capability for self-correction.
However, large LMs are explicitly prompted to verify and modify its answers
separately rather than completing all steps spontaneously like humans.
Moreover, these complex prompts are extremely challenging for small LMs to
follow. In this paper, we introduce the \underline{I}ntrinsic
\underline{S}elf-\underline{C}orrection (ISC) in generative language models,
aiming to correct the initial output of LMs in a self-triggered manner, even
for those small LMs with 6 billion parameters. Specifically, we devise a
pipeline for constructing self-correction data and propose Partial Answer
Masking (PAM), aiming to endow the model with the capability for intrinsic
self-correction through fine-tuning. We conduct experiments using LMs with
parameters sizes ranging from 6 billion to 13 billion in two tasks, including
commonsense reasoning and factual knowledge reasoning. Our experiments
demonstrate that the outputs generated using ISC outperform those generated
without self-correction. We believe that the output quality of even small LMs
can be further improved by empowering them with the ability to intrinsic
self-correct.
\\ ( https://arxiv.org/abs/2401.07301 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07310
Date: Sun, 14 Jan 2024 15:15:58 GMT   (2160kb,D)

Title: Harnessing Large Language Models Over Transformer Models for Detecting
  Bengali Depressive Social Media Text: A Comprehensive Study
Authors: Ahmadul Karim Chowdhury, Md. Saidur Rahman Sujon, Md. Shirajus Salekin
  Shafi, Tasin Ahmmad, Sifat Ahmed, Khan Md Hasib, Faisal Muhammad Shah
Categories: cs.CL
\\
  In an era where the silent struggle of underdiagnosed depression pervades
globally, our research delves into the crucial link between mental health and
social media. This work focuses on early detection of depression, particularly
in extroverted social media users, using LLMs such as GPT 3.5, GPT 4 and our
proposed GPT 3.5 fine-tuned model DepGPT, as well as advanced Deep learning
models(LSTM, Bi-LSTM, GRU, BiGRU) and Transformer models(BERT, BanglaBERT,
SahajBERT, BanglaBERT-Base). The study categorized Reddit and X datasets into
"Depressive" and "Non-Depressive" segments, translated into Bengali by native
speakers with expertise in mental health, resulting in the creation of the
Bengali Social Media Depressive Dataset (BSMDD). Our work provides full
architecture details for each model and a methodical way to assess their
performance in Bengali depressive text categorization using zero-shot and
few-shot learning techniques. Our work demonstrates the superiority of
SahajBERT and Bi-LSTM with FastText embeddings in their respective domains also
tackles explainability issues with transformer models and emphasizes the
effectiveness of LLMs, especially DepGPT, demonstrating flexibility and
competence in a range of learning contexts. According to the experiment
results, the proposed model, DepGPT, outperformed not only Alpaca Lora 7B in
zero-shot and few-shot scenarios but also every other model, achieving a
near-perfect accuracy of 0.9796 and an F1-score of 0.9804, high recall, and
exceptional precision. Although competitive, GPT-3.5 Turbo and Alpaca Lora 7B
show relatively poorer effectiveness in zero-shot and few-shot situations. The
work emphasizes the effectiveness and flexibility of LLMs in a variety of
linguistic circumstances, providing insightful information about the complex
field of depression detection models.
\\ ( https://arxiv.org/abs/2401.07310 ,  2160kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07333
Date: Sun, 14 Jan 2024 17:43:55 GMT   (3289kb,D)

Title: ELLA-V: Stable Neural Codec Language Modeling with Alignment-guided
  Sequence Reordering
Authors: Yakun Song, Zhuo Chen, Xiaofei Wang, Ziyang Ma, Xie Chen
Categories: cs.CL cs.AI cs.SD eess.AS
Comments: Working in progress
\\
  The language model (LM) approach based on acoustic and linguistic prompts,
such as VALL-E, has achieved remarkable progress in the field of zero-shot
audio generation. However, existing methods still have some limitations: 1)
repetitions, transpositions, and omissions in the output synthesized speech due
to limited alignment constraints between audio and phoneme tokens; 2)
challenges of fine-grained control over the synthesized speech with
autoregressive (AR) language model; 3) infinite silence generation due to the
nature of AR-based decoding, especially under the greedy strategy. To alleviate
these issues, we propose ELLA-V, a simple but efficient LM-based zero-shot
text-to-speech (TTS) framework, which enables fine-grained control over
synthesized audio at the phoneme level. The key to ELLA-V is interleaving
sequences of acoustic and phoneme tokens, where phoneme tokens appear ahead of
the corresponding acoustic tokens. The experimental findings reveal that our
model outperforms VALL-E in terms of accuracy and delivers more stable results
using both greedy and sampling-based decoding strategies. The code of ELLA-V
will be open-sourced after cleanups. Audio samples are available at
https://ereboas.github.io/ELLAV/.
\\ ( https://arxiv.org/abs/2401.07333 ,  3289kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07340
Date: Sun, 14 Jan 2024 18:15:06 GMT   (1003kb)

Title: The Afterlives of Shakespeare and Company in Online Social Readership
Authors: Maria Antoniak, David Mimno, Rosamond Thalken, Melanie Walsh, Matthew
  Wilkens, Gregory Yauney
Categories: cs.CL
\\
  The growth of social reading platforms such as Goodreads and LibraryThing
enables us to analyze reading activity at very large scale and in remarkable
detail. But twenty-first century systems give us a perspective only on
contemporary readers. Meanwhile, the digitization of the lending library
records of Shakespeare and Company provides a window into the reading activity
of an earlier, smaller community in interwar Paris. In this article, we explore
the extent to which we can make comparisons between the Shakespeare and Company
and Goodreads communities. By quantifying similarities and differences, we can
identify patterns in how works have risen or fallen in popularity across these
datasets. We can also measure differences in how works are received by
measuring similarities and differences in co-reading patterns. Finally, by
examining the complete networks of co-readership, we can observe changes in the
overall structures of literary reception.
\\ ( https://arxiv.org/abs/2401.07340 ,  1003kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07360
Date: Sun, 14 Jan 2024 20:14:35 GMT   (48kb,D)

Title: Promptformer: Prompted Conformer Transducer for ASR
Authors: Sergio Duarte-Torres, Arunasish Sen, Aman Rana, Lukas Drude, Alejandro
  Gomez-Alanis, Andreas Schwarz, Leif R\"adel, Volker Leutnant
Categories: cs.CL cs.SD eess.AS
\\
  Context cues carry information which can improve multi-turn interactions in
automatic speech recognition (ASR) systems. In this paper, we introduce a novel
mechanism inspired by hyper-prompting to fuse textual context with acoustic
representations in the attention mechanism. Results on a test set with
multi-turn interactions show that our method achieves 5.9% relative word error
rate reduction (rWERR) over a strong baseline. We show that our method does not
degrade in the absence of context and leads to improvements even if the model
is trained without context. We further show that leveraging a pre-trained
sentence-piece model for context embedding generation can outperform an
external BERT model.
\\ ( https://arxiv.org/abs/2401.07360 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07363
Date: Sun, 14 Jan 2024 20:35:33 GMT   (7209kb,D)

Title: PersonalityChat: Conversation Distillation for Personalized Dialog
  Modeling with Facts and Traits
Authors: Ehsan Lotfi, Maxime De Bruyn, Jeska Buhmann, Walter Daelemans
Categories: cs.CL
Comments: GEM workshop @ EMNLP23
\\
  The new wave of Large Language Models (LLM) has offered an efficient tool to
curate sizeable conversational datasets. So far studies have mainly focused on
task-oriented or generic open-domain dialogs, and have not fully explored the
ability of LLMs in following complicated prompts. In this work, we focus on
personalization, and employ LLMs to curate a dataset which is difficult and
costly to crowd-source: PersonalityChat is a synthetic conversational dataset
based upon the popular PersonaChat dataset, but conditioned on both personas
and (Big-5) personality traits. Evaluating models fine-tuned on this dataset,
we show that the personality trait labels can be used for trait-based
personalization of generative dialogue models. We also perform a head-to-head
comparison between PersonalityChat and PersonaChat, and show that training on
the distilled dataset results in more fluent and coherent dialog agents in the
small-model regime.
\\ ( https://arxiv.org/abs/2401.07363 ,  7209kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07367
Date: Sun, 14 Jan 2024 21:00:52 GMT   (232kb,D)

Title: Active Learning for NLP with Large Language Models
Authors: Xuesong Wang
Categories: cs.CL
Comments: init report
\\
  Human annotation of training samples is expensive, laborious, and sometimes
challenging, especially for Natural Language Processing (NLP) tasks. To reduce
the labeling cost and enhance the sample efficiency, Active Learning (AL)
technique can be used to label as few samples as possible to reach a reasonable
or similar results. To reduce even more costs and with the significant advances
of Large Language Models (LLMs), LLMs can be a good candidate to annotate
samples. This work investigates the accuracy and cost of using LLMs (GPT-3.5
and GPT-4) to label samples on 3 different datasets. A consistency-based
strategy is proposed to select samples that are potentially incorrectly labeled
so that human annotations can be used for those samples in AL settings, and we
call it mixed annotation strategy. Then we test performance of AL under two
different settings: (1) using human annotations only; (2) using the proposed
mixed annotation strategy. The accuracy of AL models under 3 AL query
strategies are reported on 3 text classification datasets, i.e., AG's News,
TREC-6, and Rotten Tomatoes. On AG's News and Rotten Tomatoes, the models
trained with the mixed annotation strategy achieves similar or better results
compared to that with human annotations. The method reveals great potentials of
LLMs as annotators in terms of accuracy and cost efficiency in active learning
settings.
\\ ( https://arxiv.org/abs/2401.07367 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07382
Date: Sun, 14 Jan 2024 22:05:11 GMT   (505kb,D)

Title: DRLC: Reinforcement Learning with Dense Rewards from LLM Critic
Authors: Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, Lei
  Meng
Categories: cs.CL cs.AI
\\
  Reinforcement learning (RL) can align language models with non-differentiable
reward signals, such as human preferences. However, a major challenge arises
from the sparsity of these reward signals - typically, there is only one reward
for the entire generation. This sparsity of rewards can lead to inefficient and
unstable learning. In this paper, we introduce a novel framework leveraging the
critique ability of LLMs to produce dense rewards throughout the learning
process. Our approach incorporates a critic language model alongside the policy
model. This critic is prompted with the task description, question, policy
model's output, and environment's reward signal as input, and provides token or
span-level dense rewards that reflect the quality of each segment of the
output. We assess our approach on three text generation tasks: sentiment
control, language model detoxification, and summarization. Experimental results
show that incorporating artificial dense rewards in training yields consistent
performance gains over the PPO baseline with holistic rewards. Furthermore, in
a setting where the same model serves as both policy and critic, we demonstrate
that "self-critique" rewards also boost learning efficiency.
\\ ( https://arxiv.org/abs/2401.07382 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07414
Date: Mon, 15 Jan 2024 01:40:39 GMT   (238kb,D)

Title: Leveraging the power of transformers for guilt detection in text
Authors: Abdul Gafar Manuel Meque, Jason Angel, Grigori Sidorov, Alexander
  Gelbukh
Categories: cs.CL
\\
  In recent years, language models and deep learning techniques have
revolutionized natural language processing tasks, including emotion detection.
However, the specific emotion of guilt has received limited attention in this
field. In this research, we explore the applicability of three
transformer-based language models for detecting guilt in text and compare their
performance for general emotion detection and guilt detection. Our proposed
model outformed BERT and RoBERTa models by two and one points respectively.
Additionally, we analyze the challenges in developing accurate guilt-detection
models and evaluate our model's effectiveness in detecting related emotions
like "shame" through qualitative analysis of results.
\\ ( https://arxiv.org/abs/2401.07414 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07441
Date: Mon, 15 Jan 2024 03:00:39 GMT   (1694kb,D)

Title: Stability Analysis of ChatGPT-based Sentiment Analysis in AI Quality
  Assurance
Authors: Tinghui Ouyang, AprilPyone MaungMaung, Koichi Konishi, Yoshiki Seo,
  and Isao Echizen
Categories: cs.CL
\\
  In the era of large AI models, the complex architecture and vast parameters
present substantial challenges for effective AI quality management (AIQM), e.g.
large language model (LLM). This paper focuses on investigating the quality
assurance of a specific LLM-based AI product--a ChatGPT-based sentiment
analysis system. The study delves into stability issues related to both the
operation and robustness of the expansive AI model on which ChatGPT is based.
Experimental analysis is conducted using benchmark datasets for sentiment
analysis. The results reveal that the constructed ChatGPT-based sentiment
analysis system exhibits uncertainty, which is attributed to various
operational factors. It demonstrated that the system also exhibits stability
issues in handling conventional small text attacks involving robustness.
\\ ( https://arxiv.org/abs/2401.07441 ,  1694kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07447
Date: Mon, 15 Jan 2024 03:23:24 GMT   (431kb)

Title: Taec: a Manually annotated text dataset for trait and phenotype
  extraction and entity linking in wheat breeding literature
Authors: Claire N\'edellec, Clara Sauvion, Robert Bossy, Mariya Borovikova,
  Louise Del\'eger
Categories: cs.CL cs.AI
Comments: 17 pages
\\
  Wheat varieties show a large diversity of traits and phenotypes. Linking them
to genetic variability is essential for shorter and more efficient wheat
breeding programs. Newly desirable wheat variety traits include disease
resistance to reduce pesticide use, adaptation to climate change, resistance to
heat and drought stresses, or low gluten content of grains. Wheat breeding
experiments are documented by a large body of scientific literature and
observational data obtained in-field and under controlled conditions. The
cross-referencing of complementary information from the literature and
observational data is essential to the study of the genotype-phenotype
relationship and to the improvement of wheat selection. The scientific
literature on genetic marker-assisted selection describes much information
about the genotype-phenotype relationship. However, the variety of expressions
used to refer to traits and phenotype values in scientific articles is a hinder
to finding information and cross-referencing it. When trained adequately by
annotated examples, recent text mining methods perform highly in named entity
recognition and linking in the scientific domain. While several corpora contain
annotations of human and animal phenotypes, currently, no corpus is available
for training and evaluating named entity recognition and entity-linking methods
in plant phenotype literature. The Triticum aestivum trait Corpus is a new gold
standard for traits and phenotypes of wheat. It consists of 540 PubMed
references fully annotated for trait, phenotype, and species named entities
using the Wheat Trait and Phenotype Ontology and the species taxonomy of the
National Center for Biotechnology Information. A study of the performance of
tools trained on the Triticum aestivum trait Corpus shows that the corpus is
suitable for the training and evaluation of named entity recognition and
linking.
\\ ( https://arxiv.org/abs/2401.07447 ,  431kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07453
Date: Mon, 15 Jan 2024 03:57:15 GMT   (1497kb,D)

Title: Model Editing at Scale leads to Gradual and Catastrophic Forgetting
Authors: Akshat Gupta, Anurag Rao, Gopala Anumanchipalli
Categories: cs.CL cs.AI cs.IR
\\
  Editing knowledge in large language models is an attractive capability to
have which allows us to correct incorrectly learnt facts during pre-training,
as well as update the model with an ever-growing list of new facts. While
existing model editing techniques have shown promise, they are usually
evaluated using metrics for reliability, specificity and generalization over
one or few edits. We argue that for model editing to have practical utility, we
must be able to make multiple edits to the same model. With this in mind, we
evaluate the current model editing methods at scale, focusing on two state of
the art methods: ROME and MEMIT. We find that as the model is edited
sequentially with multiple facts, it continually forgets previously edited
facts and the ability to perform downstream tasks. This forgetting happens in
two phases -- an initial gradual but progressive forgetting phase followed by
abrupt or catastrophic forgetting phase. Both gradual and catastrophic
forgetting limit the usefulness of model editing methods at scale -- the former
making model editing less effective as multiple edits are made to the model
while the latter caps the scalability of such model editing methods. Our
analysis also highlights other key limitations of ROME and MEMIT at scale. With
our work, we push for the development and evaluation of model editing methods
keeping scalability in mind.
\\ ( https://arxiv.org/abs/2401.07453 ,  1497kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07456
Date: Mon, 15 Jan 2024 04:04:26 GMT   (16420kb,D)

Title: Only Send What You Need: Learning to Communicate Efficiently in
  Federated Multilingual Machine Translation
Authors: Yun-Wei Chu, Dong-Jun Han, Christopher G. Brinton
Categories: cs.CL cs.AI
\\
  Federated learning (FL) is a promising approach for solving multilingual
tasks, potentially enabling clients with their own language-specific data to
collaboratively construct a high-quality neural machine translation (NMT)
model. However, communication constraints in practical network systems present
challenges for exchanging large-scale NMT engines between FL parties. In this
paper, we propose a meta-learning-based adaptive parameter selection
methodology, MetaSend, that improves the communication efficiency of model
transmissions from clients during FL-based multilingual NMT training. Our
approach learns a dynamic threshold for filtering parameters prior to
transmission without compromising the NMT model quality, based on the tensor
deviations of clients between different FL rounds. Through experiments on two
NMT datasets with different language distributions, we demonstrate that
MetaSend obtains substantial improvements over baselines in translation quality
in the presence of a limited communication budget.
\\ ( https://arxiv.org/abs/2401.07456 ,  16420kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07470
Date: Mon, 15 Jan 2024 04:58:50 GMT   (791kb)

Title: Utilizing deep learning models for the identification of enhancers and
  super-enhancers based on genomic and epigenomic features
Authors: Zahra Ahani, Moein Shahiki Tash, Yoel Ledo Mezquita and Jason Angel
Categories: cs.CL cs.AI
Comments: 13 pages, 7 figures, 6 Tables
\\
  This paper provides an extensive examination of a sizable dataset of English
tweets focusing on nine widely recognized cryptocurrencies, specifically
Cardano, Binance, Bitcoin, Dogecoin, Ethereum, Fantom, Matic, Shiba, and
Ripple. Our primary objective was to conduct a psycholinguistic and emotion
analysis of social media content associated with these cryptocurrencies. To
enable investigators to make more informed decisions. The study involved
comparing linguistic characteristics across the diverse digital coins, shedding
light on the distinctive linguistic patterns that emerge within each coin's
community. To achieve this, we utilized advanced text analysis techniques.
Additionally, our work unveiled an intriguing Understanding of the interplay
between these digital assets within the cryptocurrency community. By examining
which coin pairs are mentioned together most frequently in the dataset, we
established correlations between different cryptocurrencies. To ensure the
reliability of our findings, we initially gathered a total of 832,559 tweets
from Twitter. These tweets underwent a rigorous preprocessing stage, resulting
in a refined dataset of 115,899 tweets that were used for our analysis.
Overall, our research offers valuable Perception into the linguistic nuances of
various digital coins' online communities and provides a deeper understanding
of their interactions in the cryptocurrency space.
\\ ( https://arxiv.org/abs/2401.07470 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07475
Date: Mon, 15 Jan 2024 05:06:17 GMT   (731kb,D)

Title: GWPT: A Green Word-Embedding-based POS Tagger
Authors: Chengwei Wei, Runqi Pang, C.-C. Jay Kuo
Categories: cs.CL
\\
  As a fundamental tool for natural language processing (NLP), the
part-of-speech (POS) tagger assigns the POS label to each word in a sentence. A
novel lightweight POS tagger based on word embeddings is proposed and named
GWPT (green word-embedding-based POS tagger) in this work. Following the green
learning (GL) methodology, GWPT contains three modules in cascade: 1)
representation learning, 2) feature learning, and 3) decision learning modules.
The main novelty of GWPT lies in representation learning. It uses
non-contextual or contextual word embeddings, partitions embedding dimension
indices into low-, medium-, and high-frequency sets, and represents them with
different N-grams. It is shown by experimental results that GWPT offers
state-of-the-art accuracies with fewer model parameters and significantly lower
computational complexity in both training and inference as compared with
deep-learning-based methods.
\\ ( https://arxiv.org/abs/2401.07475 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07510
Date: Mon, 15 Jan 2024 07:21:16 GMT   (1123kb)

Title: Developing ChatGPT for Biology and Medicine: A Complete Review of
  Biomedical Question Answering
Authors: Qing Li, Lei Li, Yu Li
Categories: cs.CL cs.AI
Comments: 50 pages, 3 figures, Biophysics Reports
MSC-class: cs.CL
ACM-class: I.2.1
\\
  ChatGPT explores a strategic blueprint of question answering (QA) in
delivering medical diagnosis, treatment recommendations, and other healthcare
support. This is achieved through the increasing incorporation of medical
domain data via natural language processing (NLP) and multimodal paradigms. By
transitioning the distribution of text, images, videos, and other modalities
from the general domain to the medical domain, these techniques have expedited
the progress of medical domain question answering (MDQA). They bridge the gap
between human natural language and sophisticated medical domain knowledge or
expert manual annotations, handling large-scale, diverse, unbalanced, or even
unlabeled data analysis scenarios in medical contexts. Central to our focus is
the utilizing of language models and multimodal paradigms for medical question
answering, aiming to guide the research community in selecting appropriate
mechanisms for their specific medical research requirements. Specialized tasks
such as unimodal-related question answering, reading comprehension, reasoning,
diagnosis, relation extraction, probability modeling, and others, as well as
multimodal-related tasks like vision question answering, image caption,
cross-modal retrieval, report summarization, and generation, are discussed in
detail. Each section delves into the intricate specifics of the respective
method under consideration. This paper highlights the structures and
advancements of medical domain explorations against general domain methods,
emphasizing their applications across different tasks and datasets. It also
outlines current challenges and opportunities for future medical domain
research, paving the way for continued innovation and application in this
rapidly evolving field.
\\ ( https://arxiv.org/abs/2401.07510 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07518
Date: Mon, 15 Jan 2024 07:48:42 GMT   (1761kb,D)

Title: Survey of Natural Language Processing for Education: Taxonomy,
  Systematic Review, and Future Trends
Authors: Yunshi Lan, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Weining Qian,
  Aoying Zhou
Categories: cs.CL cs.AI
\\
  Natural Language Processing (NLP) aims to analyze the text via techniques in
the computer science field. It serves the applications in healthcare, commerce,
and education domains. Particularly, NLP has been applied to the education
domain to help teaching and learning. In this survey, we review recent advances
in NLP with a focus on solving problems related to the education domain. In
detail, we begin with introducing the relevant background. Then, we present the
taxonomy of NLP in the education domain. Next, we illustrate the task
definition, challenges, and corresponding techniques based on the above
taxonomy. After that, we showcase some off-the-shelf demonstrations in this
domain and conclude with future directions.
\\ ( https://arxiv.org/abs/2401.07518 ,  1761kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07525
Date: Mon, 15 Jan 2024 07:57:58 GMT   (327kb,D)

Title: TAROT: A Hierarchical Framework with Multitask Co-Pretraining on
  Semi-Structured Data towards Effective Person-Job Fit
Authors: Yihan Cao, Xu Chen, Lun Du, Hao Chen, Qiang Fu, Shi Han, Yushu Du,
  Yanbin Kang, Guangming Lu, Zi Li
Categories: cs.CL cs.AI
Comments: ICASSP 2024 camera ready. 5 pages, 1 figure, 3 tables
\\
  Person-job fit is an essential part of online recruitment platforms in
serving various downstream applications like Job Search and Candidate
Recommendation. Recently, pretrained large language models have further
enhanced the effectiveness by leveraging richer textual information in user
profiles and job descriptions apart from user behavior features and job
metadata. However, the general domain-oriented design struggles to capture the
unique structural information within user profiles and job descriptions,
leading to a loss of latent semantic correlations. We propose TAROT, a
hierarchical multitask co-pretraining framework, to better utilize structural
and semantic information for informative text embeddings. TAROT targets
semi-structured text in profiles and jobs, and it is co-pretained with
multi-grained pretraining tasks to constrain the acquired semantic information
at each level. Experiments on a real-world LinkedIn dataset show significant
performance improvements, proving its effectiveness in person-job fit tasks.
\\ ( https://arxiv.org/abs/2401.07525 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07526
Date: Mon, 15 Jan 2024 08:08:24 GMT   (315kb,D)

Title: Editing Arbitrary Propositions in LLMs without Subject Labels
Authors: Itai Feigenbaum, Devansh Arpit, Huan Wang, Shelby Heinecke, Juan
  Carlos Niebles, Weiran Yao, Caiming Xiong, Silvio Savarese
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Model (LLM) editing modifies factual information in LLMs.
Locate-and-Edit (L\&E) methods accomplish this by finding where relevant
information is stored within the neural network, and editing the weights at
that location. The goal of editing is to modify the response of an LLM to a
proposition independently of its phrasing, while not modifying its response to
other related propositions. Existing methods are limited to binary
propositions, which represent straightforward binary relations between a
subject and an object. Furthermore, existing methods rely on semantic subject
labels, which may not be available or even be well-defined in practice. In this
paper, we show that both of these issues can be effectively skirted with a
simple and fast localization method called Gradient Tracing (GT). This
localization method allows editing arbitrary propositions instead of just
binary ones, and does so without the need for subject labels. As propositions
always have a truth value, our experiments prompt an LLM as a boolean
classifier, and edit its T/F response to propositions. Our method applies GT
for location tracing, and then edit the model at that location using a mild
variant of Rank-One Model Editing (ROME). On datasets of binary propositions
derived from the CounterFact dataset, we show that our method -- without access
to subject labels -- performs close to state-of-the-art L\&E methods which has
access subject labels. We then introduce a new dataset, Factual Accuracy
Classification Test (FACT), which includes non-binary propositions and for
which subject labels are not generally applicable, and therefore is beyond the
scope of existing L\&E methods. Nevertheless, we show that with our method
editing is possible on FACT.
\\ ( https://arxiv.org/abs/2401.07526 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07544
Date: Mon, 15 Jan 2024 09:09:14 GMT   (491kb,D)

Title: See the Unseen: Better Context-Consistent Knowledge-Editing by Noises
Authors: Youcheng Huang, Wenqiang Lei, Zheng Zhang, Jiancheng Lv, Shuicheng Yan
Categories: cs.CL
\\
  Knowledge-editing updates knowledge of large language models (LLMs) and
contributes to the interpretability and application of LLMs. However, knowledge
applying is context-consistent: LLMs can recall the same knowledge in different
contexts. Existing works ignore this property and the editing lacks
generalization. In this paper, we empirically find that the effects of
different contexts upon LLMs in recalling the same knowledge follow a
Gaussian-like distribution. We then sample Gaussian noises to simulate the
effects of different contexts when updating LLMs. By such, we can make LLMs see
the unseen contexts where the edited knowledge will be applied, therefore
improving the editing generalization. Experimental results on three LLMs
demonstrate the effectiveness of our methods and also distinguish our methods
from the others of fine-tuning LLMs by noises.
\\ ( https://arxiv.org/abs/2401.07544 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07575
Date: Mon, 15 Jan 2024 10:18:08 GMT   (528kb,D)

Title: Cascaded Cross-Modal Transformer for Audio-Textual Classification
Authors: Nicolae-Catalin Ristea, Andrei Anghel, Radu Tudor Ionescu
Categories: cs.CL cs.LG cs.SD eess.AS
\\
  Speech classification tasks often require powerful language understanding
models to grasp useful features, which becomes problematic when limited
training data is available. To attain superior classification performance, we
propose to harness the inherent value of multimodal representations by
transcribing speech using automatic speech recognition (ASR) models and
translating the transcripts into different languages via pretrained translation
models. We thus obtain an audio-textual (multimodal) representation for each
data sample. Subsequently, we combine language-specific Bidirectional Encoder
Representations from Transformers (BERT) with Wav2Vec2.0 audio features via a
novel cascaded cross-modal transformer (CCMT). Our model is based on two
cascaded transformer blocks. The first one combines text-specific features from
distinct languages, while the second one combines acoustic features with
multilingual features previously learned by the first transformer block. We
employed our system in the Requests Sub-Challenge of the ACM Multimedia 2023
Computational Paralinguistics Challenge. CCMT was declared the winning
solution, obtaining an unweighted average recall (UAR) of 65.41% and 85.87% for
complaint and request detection, respectively. Moreover, we applied our
framework on the Speech Commands v2 and HarperValleyBank dialog data sets,
surpassing previous studies reporting results on these benchmarks. Our code is
freely available for download at: https://github.com/ristea/ccmt.
\\ ( https://arxiv.org/abs/2401.07575 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07598
Date: Mon, 15 Jan 2024 11:06:43 GMT   (2900kb,D)

Title: MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of
  Large Language Models
Authors: Divyanshu Aggarwal, Ashutosh Sathe and Sunayana Sitaram
Categories: cs.CL
Comments: 23 pages, 23 figures, 14 tables
\\
  Parameter efficient finetuning has emerged as a viable solution for improving
the performance of Large Language Models without requiring massive resources
and compute. Prior work on multilingual evaluation has shown that there is a
large gap between the performance of LLMs on English and other languages.
Further, there is also a large gap between the performance of smaller
open-source models and larger LLMs. Finetuning can be an effective way to
bridge this gap and make language models more equitable. In this work, we
finetune the LLaMA-7B and Mistral-7B models on synthetic multilingual
instruction tuning data to determine its effect on model performance on five
downstream tasks covering twenty three languages in all. Additionally, we
experiment with various parameters, such as rank for low-rank adaptation and
values of quantisation to determine their effects on downstream performance and
find that higher rank and higher quantisation values benefit low-resource
languages. We find that parameter efficient finetuning of smaller open source
models sometimes bridges the gap between the performance of these models and
the larger ones, however, English performance can take a hit. We also find that
finetuning sometimes improves performance on low-resource languages, while
degrading performance on high-resource languages.
\\ ( https://arxiv.org/abs/2401.07598 ,  2900kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07683
Date: Mon, 15 Jan 2024 13:51:00 GMT   (493kb,D)

Title: Assisted Knowledge Graph Authoring: Human-Supervised Knowledge Graph
  Construction from Natural Language
Authors: Marcel Gohsen and Benno Stein
Categories: cs.CL
Comments: accepted at CHIIR 2024
DOI: 10.1145/3627508.3638340
\\
  Encyclopedic knowledge graphs, such as Wikidata, host an extensive repository
of millions of knowledge statements. However, domain-specific knowledge from
fields such as history, physics, or medicine is significantly underrepresented
in those graphs. Although few domain-specific knowledge graphs exist (e.g.,
Pubmed for medicine), developing specialized retrieval applications for many
domains still requires constructing knowledge graphs from scratch. To
facilitate knowledge graph construction, we introduce WAKA: a Web application
that allows domain experts to create knowledge graphs through the medium with
which they are most familiar: natural language.
\\ ( https://arxiv.org/abs/2401.07683 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07702
Date: Mon, 15 Jan 2024 14:19:47 GMT   (9003kb,D)

Title: Prompting open-source and commercial language models for grammatical
  error correction of English learner text
Authors: Christopher Davis, Andrew Caines, {\O}istein Andersen, Shiva
  Taslimipoor, Helen Yannakoudakis, Zheng Yuan, Christopher Bryant, Marek Rei,
  Paula Buttery
Categories: cs.CL
Comments: 8 pages with appendices
\\
  Thanks to recent advances in generative AI, we are able to prompt large
language models (LLMs) to produce texts which are fluent and grammatical. In
addition, it has been shown that we can elicit attempts at grammatical error
correction (GEC) from LLMs when prompted with ungrammatical input sentences. We
evaluate how well LLMs can perform at GEC by measuring their performance on
established benchmark datasets. We go beyond previous studies, which only
examined GPT* models on a selection of English GEC datasets, by evaluating
seven open-source and three commercial LLMs on four established GEC benchmarks.
We investigate model performance and report results against individual error
types. Our results indicate that LLMs do not always outperform supervised
English GEC models except in specific contexts -- namely commercial LLMs on
benchmarks annotated with fluency corrections as opposed to minimal edits. We
find that several open-source models outperform commercial ones on minimal edit
benchmarks, and that in some settings zero-shot prompting is just as
competitive as few-shot prompting.
\\ ( https://arxiv.org/abs/2401.07702 ,  9003kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07760
Date: Mon, 15 Jan 2024 15:11:15 GMT   (283kb,D)

Title: On the importance of Data Scale in Pretraining Arabic Language Models
Authors: Abbas Ghaddar, Philippe Langlais, Mehdi Rezagholizadeh, Boxing Chen
Categories: cs.CL
\\
  Pretraining monolingual language models have been proven to be vital for
performance in Arabic Natural Language Processing (NLP) tasks. In this paper,
we conduct a comprehensive study on the role of data in Arabic Pretrained
Language Models (PLMs). More precisely, we reassess the performance of a suite
of state-of-the-art Arabic PLMs by retraining them on massive-scale,
high-quality Arabic corpora. We have significantly improved the performance of
the leading Arabic encoder-only BERT-base and encoder-decoder T5-base models on
the ALUE and ORCA leaderboards, thereby reporting state-of-the-art results in
their respective model categories. In addition, our analysis strongly suggests
that pretraining data by far is the primary contributor to performance,
surpassing other factors. Our models and source code are publicly available at
https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/JABER-PyTorch.
\\ ( https://arxiv.org/abs/2401.07760 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07777
Date: Mon, 15 Jan 2024 15:40:16 GMT   (884kb,D)

Title: Quantum Transfer Learning for Acceptability Judgements
Authors: Giuseppe Buonaiuto, Raffaele Guarasci, Aniello Minutolo, Giuseppe De
  Pietro, Massimo Esposito
Categories: cs.CL physics.comp-ph quant-ph
\\
  Hybrid quantum-classical classifiers promise to positively impact critical
aspects of natural language processing tasks, particularly
classification-related ones. Among the possibilities currently investigated,
quantum transfer learning, i.e., using a quantum circuit for fine-tuning
pre-trained classical models for a specific task, is attracting significant
attention as a potential platform for proving quantum advantage.
  This work shows potential advantages, both in terms of performance and
expressiveness, of quantum transfer learning algorithms trained on embedding
vectors extracted from a large language model to perform classification on a
classical Linguistics task: acceptability judgments. Acceptability judgment is
the ability to determine whether a sentence is considered natural and
well-formed by a native speaker. The approach has been tested on sentences
extracted from ItaCoLa, a corpus that collects Italian sentences labeled with
their acceptability judgment. The evaluation phase shows results for the
quantum transfer learning pipeline comparable to state-of-the-art classical
transfer learning algorithms, proving current quantum computers' capabilities
to tackle NLP tasks for ready-to-use applications. Furthermore, a qualitative
linguistic analysis, aided by explainable AI methods, reveals the capabilities
of quantum transfer learning algorithms to correctly classify complex and more
structured sentences, compared to their classical counterpart. This finding
sets the ground for a quantifiable quantum advantage in NLP in the near future.
\\ ( https://arxiv.org/abs/2401.07777 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07793
Date: Mon, 15 Jan 2024 16:00:50 GMT   (319kb,D)

Title: Flexibly Scaling Large Language Models Contexts Through Extensible
  Tokenization
Authors: Ninglu Shao and Shitao Xiao and Zheng Liu and Peitian Zhang
Categories: cs.CL
\\
  Large language models (LLMs) are in need of sufficient contexts to handle
many critical applications, such as retrieval augmented generation and few-shot
learning. However, due to the constrained window size, the LLMs can only access
to the information within a limited context. Although the size of context
window can be extended by fine-tuning, it will result in a substantial cost in
both training and inference stage. In this paper, we present Extensible
Tokenization as an alternative method which realizes the flexible scaling of
LLMs' context. Extensible Tokenization stands as a midware in between of the
tokenized context and the LLM, which transforms the raw token embeddings into
the extensible embeddings. Such embeddings provide a more compact
representation for the long context, on top of which the LLM is able to
perceive more information with the same context window. Extensible Tokenization
is also featured by its flexibility: the scaling factor can be flexibly
determined within a feasible scope, leading to the extension of an arbitrary
context length at the inference time. Besides, Extensible Tokenization is
introduced as a drop-in component, which can be seamlessly plugged into not
only the LLM itself and but also its fine-tuned derivatives, bringing in the
extended contextual information while fully preserving the LLM's existing
capabilities. We perform comprehensive experiments on long-context language
modeling and understanding tasks, which verify Extensible Tokenization as an
effective, efficient, flexible, and compatible method to extend LLM's context.
Our model and source code will be made publicly available.
\\ ( https://arxiv.org/abs/2401.07793 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07810
Date: Mon, 15 Jan 2024 16:31:18 GMT   (7640kb,D)

Title: Consolidating Strategies for Countering Hate Speech Using Persuasive
  Dialogues
Authors: Sougata Saha and Rohini Srihari
Categories: cs.CL cs.AI
\\
  Hateful comments are prevalent on social media platforms. Although tools for
automatically detecting, flagging, and blocking such false, offensive, and
harmful content online have lately matured, such reactive and brute force
methods alone provide short-term and superficial remedies while the
perpetrators persist. With the public availability of large language models
which can generate articulate synthetic and engaging content at scale, there
are concerns about the rapid growth of dissemination of such malicious content
on the web. There is now a need to focus on deeper, long-term solutions that
involve engaging with the human perpetrator behind the source of the content to
change their viewpoint or at least bring down the rhetoric using persuasive
means. To do that, we propose defining and experimenting with controllable
strategies for generating counter-arguments to hateful comments in online
conversations. We experiment with controlling response generation using
features based on (i) argument structure and reasoning-based Walton argument
schemes, (ii) counter-argument speech acts, and (iii) human
characteristics-based qualities such as Big-5 personality traits and human
values. Using automatic and human evaluations, we determine the best
combination of features that generate fluent, argumentative, and logically
sound arguments for countering hate. We further share the developed
computational models for automatically annotating text with such features, and
a silver-standard annotated version of an existing hate speech dialog corpora.
\\ ( https://arxiv.org/abs/2401.07810 ,  7640kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07812
Date: Mon, 15 Jan 2024 16:35:52 GMT   (1302kb,D)

Title: Wikidata as a seed for Web Extraction
Authors: Kunpeng Guo, Dennis Diefenbach, Antoine Gourru, Christophe Gravier
Categories: cs.CL
DOI: 10.1145/3543507
\\
  Wikidata has grown to a knowledge graph with an impressive size. To date, it
contains more than 17 billion triples collecting information about people,
places, films, stars, publications, proteins, and many more. On the other side,
most of the information on the Web is not published in highly structured data
repositories like Wikidata, but rather as unstructured and semi-structured
content, more concretely in HTML pages containing text and tables. Finding,
monitoring, and organizing this data in a knowledge graph is requiring
considerable work from human editors. The volume and complexity of the data
make this task difficult and time-consuming. In this work, we present a
framework that is able to identify and extract new facts that are published
under multiple Web domains so that they can be proposed for validation by
Wikidata editors. The framework is relying on question-answering technologies.
We take inspiration from ideas that are used to extract facts from textual
collections and adapt them to extract facts from Web pages. For achieving this,
we demonstrate that language models can be adapted to extract facts not only
from textual collections but also from Web pages. By exploiting the information
already contained in Wikidata the proposed framework can be trained without the
need for any additional learning signals and can extract new facts for a wide
range of properties and domains. Following this path, Wikidata can be used as a
seed to extract facts on the Web. Our experiments show that we can achieve a
mean performance of 84.07 at F1-score. Moreover, our estimations show that we
can potentially extract millions of facts that can be proposed for human
validation. The goal is to help editors in their daily tasks and contribute to
the completion of the Wikidata knowledge graph.
\\ ( https://arxiv.org/abs/2401.07812 ,  1302kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07817
Date: Mon, 15 Jan 2024 16:39:10 GMT   (898kb,D)

Title: Question Translation Training for Better Multilingual Reasoning
Authors: Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen,
  Alexandra Birch
Categories: cs.CL
\\
  Large language models show compelling performance on reasoning tasks but they
tend to perform much worse in languages other than English. This is
unsurprising given that their training data largely consists of English text
and instructions. A typical solution is to translate instruction data into all
languages of interest, and then train on the resulting multilingual data, which
is called translate-training. This approach not only incurs high cost, but also
results in poorly translated data due to the non-standard formatting of
chain-of-thought and mathematical reasoning instructions. In this paper, we
explore the benefits of question alignment, where we train the model to
translate reasoning questions into English by finetuning on X-English question
data. In this way we perform targetted, in-domain language alignment which
makes best use of English instruction data to unlock the LLMs' multilingual
reasoning abilities. Experimental results on LLaMA2-13B show that question
alignment leads to consistent improvements over the translate-training
approach: an average improvement of 11.3\% and 16.1\% accuracy across ten
languages on the MGSM and MSVAMP maths reasoning benchmarks (The project will
be available at: https://github.com/NJUNLP/QAlign).
\\ ( https://arxiv.org/abs/2401.07817 ,  898kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07847
Date: Mon, 15 Jan 2024 17:23:02 GMT   (128kb,D)

Title: Milestones in Bengali Sentiment Analysis leveraging Transformer-models:
  Fundamentals, Challenges and Future Directions
Authors: Saptarshi Sengupta, Shreya Ghosh, Prasenjit Mitra, Tarikul Islam
  Tamiti
Categories: cs.CL
\\
  Sentiment Analysis (SA) refers to the task of associating a view polarity
(usually, positive, negative, or neutral; or even fine-grained such as slightly
angry, sad, etc.) to a given text, essentially breaking it down to a supervised
(since we have the view labels apriori) classification task. Although heavily
studied in resource-rich languages such as English thus pushing the SOTA by
leaps and bounds, owing to the arrival of the Transformer architecture, the
same cannot be said for resource-poor languages such as Bengali (BN). For a
language spoken by roughly 300 million people, the technology enabling them to
run trials on their favored tongue is severely lacking. In this paper, we
analyze the SOTA for SA in Bengali, particularly, Transformer-based models. We
discuss available datasets, their drawbacks, the nuances associated with
Bengali i.e. what makes this a challenging language to apply SA on, and finally
provide insights for future direction to mitigate the limitations in the field.
\\ ( https://arxiv.org/abs/2401.07847 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07851
Date: Mon, 15 Jan 2024 17:26:50 GMT   (2017kb,D)

Title: Unlocking Efficiency in Large Language Model Inference: A Comprehensive
  Survey of Speculative Decoding
Authors: Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge,
  Tianyu Liu, Wenjie Li, Zhifang Sui
Categories: cs.CL
\\
  To mitigate the high inference latency stemming from autoregressive decoding
in Large Language Models (LLMs), Speculative Decoding has emerged as a novel
decoding paradigm for LLM inference. In each decoding step, this method first
efficiently drafts several future tokens and then verifies them in parallel.
Unlike autoregressive decoding, Speculative Decoding facilitates the
simultaneous decoding of multiple tokens per step, thereby accelerating
inference. This paper presents a comprehensive overview and analysis of this
promising decoding paradigm. We begin by providing a formal definition and
formulation of Speculative Decoding. Then, we organize in-depth discussions on
its key facets, including current leading techniques, the challenges faced, and
potential future directions in this field. We aim for this work to serve as a
catalyst for further research on Speculative Decoding, ultimately contributing
to more efficient LLM inference.
\\ ( https://arxiv.org/abs/2401.07851 ,  2017kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07867
Date: Mon, 15 Jan 2024 17:57:41 GMT   (8365kb,D)

Title: Authorship Obfuscation in Multilingual Machine-Generated Text Detection
Authors: Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel
  Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko,
  Maria Bielikova
Categories: cs.CL
\\
  High-quality text generation capability of latest Large Language Models
(LLMs) causes concerns about their misuse (e.g., in massive generation/spread
of disinformation). Machine-generated text (MGT) detection is important to cope
with such threats. However, it is susceptible to authorship obfuscation (AO)
methods, such as paraphrasing, which can cause MGTs to evade detection. So far,
this was evaluated only in monolingual settings. Thus, the susceptibility of
recently proposed multilingual detectors is still unknown. We fill this gap by
comprehensively benchmarking the performance of 10 well-known AO methods,
attacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10
$\times$ 37 $\times$ 11 = 4,070 combinations). We also evaluate the effect of
data augmentation on adversarial robustness using obfuscated texts. The results
indicate that all tested AO methods can cause detection evasion in all tested
languages, where homoglyph attacks are especially successful.
\\ ( https://arxiv.org/abs/2401.07867 ,  8365kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07870
Date: Mon, 15 Jan 2024 18:04:29 GMT   (314kb,D)

Title: JumpCoder: Go Beyond Autoregressive Coder via Online Modification
Authors: Mouxiang Chen, Hao Tian, Zhongxin Liu, Xiaoxue Ren, Jianling Sun
Categories: cs.CL cs.AI cs.SE
\\
  While existing code large language models (code LLMs) exhibit impressive
capabilities in code generation, their autoregressive sequential generation
inherently lacks reversibility. This limitation hinders them from timely
correcting previous missing statements during coding as humans do, often
leading to error propagation and suboptimal performance. We introduce
JumpCoder, a novel modelagnostic framework that enables online modification and
non-sequential generation to augment the code LLMs. The key idea behind
JumpCoder is to insert new code into the currently generated code when
necessary during generation, which is achieved through an auxiliary infilling
model that works in tandem with the code LLM. Since identifying the best infill
position beforehand is intractable, we adopt an infill-first, judge-later
strategy, which experiments with filling at the $k$ most critical positions
following the generation of each line, and uses an Abstract Syntax Tree (AST)
parser alongside the Generation Model Scoring to effectively judge the validity
of each potential infill. Extensive experiments using six state-of-the-art code
LLMs across multiple benchmarks consistently indicate significant improvements
over all baselines. Notably, JumpCoder assists code LLMs in achieving up to a
3.6% increase in Pass@1 for Python, 6.3% for Java, and 3.7% for C++ in the
multilingual HumanEval benchmarks. Our code is public at
https://github.com/Keytoyze/JumpCoder.
\\ ( https://arxiv.org/abs/2401.07870 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07872
Date: Mon, 15 Jan 2024 18:07:21 GMT   (8691kb,D)

Title: The What, Why, and How of Context Length Extension Techniques in Large
  Language Models -- A Detailed Survey
Authors: Saurav Pawar, S.M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija
  Jain, Aman Chadha, Amitava Das
Categories: cs.CL
\\
  The advent of Large Language Models (LLMs) represents a notable breakthrough
in Natural Language Processing (NLP), contributing to substantial progress in
both text comprehension and generation. However, amidst these advancements, it
is noteworthy that LLMs often face a limitation in terms of context length
extrapolation. Understanding and extending the context length for LLMs is
crucial in enhancing their performance across various NLP applications. In this
survey paper, we delve into the multifaceted aspects of exploring why it is
essential, and the potential transformations that superior techniques could
bring to NLP applications. We study the inherent challenges associated with
extending context length and present an organized overview of the existing
strategies employed by researchers. Additionally, we discuss the intricacies of
evaluating context extension techniques and highlight the open challenges that
researchers face in this domain. Furthermore, we explore whether there is a
consensus within the research community regarding evaluation standards and
identify areas where further agreement is needed. This comprehensive survey
aims to serve as a valuable resource for researchers, guiding them through the
nuances of context length extension techniques and fostering discussions on
future advancements in this evolving field.
\\ ( https://arxiv.org/abs/2401.07872 ,  8691kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07877
Date: Mon, 15 Jan 2024 18:12:01 GMT   (405kb)

Title: EMBRE: Entity-aware Masking for Biomedical Relation Extraction
Authors: Mingjie Li and Karin Verspoor
Categories: cs.CL cs.AI
Comments: 5 pages, 1 figure
\\
  Information extraction techniques, including named entity recognition (NER)
and relation extraction (RE), are crucial in many domains to support making
sense of vast amounts of unstructured text data by identifying and connecting
relevant information. Such techniques can assist researchers in extracting
valuable insights. In this paper, we introduce the Entity-aware Masking for
Biomedical Relation Extraction (EMBRE) method for biomedical relation
extraction, as applied in the context of the BioRED challenge Task 1, in which
human-annotated entities are provided as input. Specifically, we integrate
entity knowledge into a deep neural network by pretraining the backbone model
with an entity masking objective. We randomly mask named entities for each
instance and let the model identify the masked entity along with its type. In
this way, the model is capable of learning more specific knowledge and more
robust representations. Then, we utilize the pre-trained model as our backbone
to encode language representations and feed these representations into two
multilayer perceptron (MLPs) to predict the logits for relation and novelty,
respectively. The experimental results demonstrate that our proposed method can
improve the performances of entity pair, relation and novelty extraction over
our baseline.
\\ ( https://arxiv.org/abs/2401.07877 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07897
Date: Mon, 15 Jan 2024 18:53:15 GMT   (37kb)

Title: The Pitfalls of Defining Hallucination
Authors: Kees van Deemter
Categories: cs.CL
Comments: Accepted for publication in Computational Linguistics on 30 Dec.
  2023. (9 Pages.)
\\
  Despite impressive advances in Natural Language Generation (NLG) and Large
Language Models (LLMs), researchers are still unclear about important aspects
of NLG evaluation. To substantiate this claim, I examine current
classifications of hallucination and omission in Data-text NLG, and I propose a
logic-based synthesis of these classfications. I conclude by highlighting some
remaining limitations of all current thinking about hallucination and by
discussing implications for LLMs.
\\ ( https://arxiv.org/abs/2401.07897 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07923
Date: Mon, 15 Jan 2024 19:21:08 GMT   (8042kb,D)

Title: Word Boundary Information Isn't Useful for Encoder Language Models
Authors: Edward Gow-Smith, Dylan Phelps, Harish Tayyar Madabushi, Carolina
  Scarton, Aline Villavicencio
Categories: cs.CL
Comments: Preprint
\\
  All existing transformer-based approaches to NLP using subword tokenisation
algorithms encode whitespace (word boundary information) through the use of
special space symbols (such as \#\# or \_) forming part of tokens. These
symbols have been shown to a) lead to reduced morphological validity of
tokenisations, and b) give substantial vocabulary redundancy. As such, removing
these symbols has been shown to have a beneficial effect on the processing of
morphologically complex words for transformer encoders in the pretrain-finetune
paradigm. In this work, we explore whether word boundary information is at all
useful to such models. In particular, we train transformer encoders across four
different training scales, and investigate several alternative approaches to
including word boundary information, evaluating on a range of tasks across
different domains and problem set-ups: GLUE (for sentence-level
classification), NER (for token-level classification), and two classification
datasets involving complex words (Superbizarre and FLOTA). Overall, through an
extensive experimental setup that includes the pre-training of 29 models, we
find no substantial improvements from our alternative approaches, suggesting
that modifying tokenisers to remove word boundary information isn't leading to
a loss of useful information.
\\ ( https://arxiv.org/abs/2401.07923 ,  8042kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07927
Date: Mon, 15 Jan 2024 19:39:15 GMT   (183kb,D)

Title: Can Large Language Models Explain Themselves?
Authors: Andreas Madsen, Sarath Chandar, Siva Reddy
Categories: cs.CL cs.AI cs.LG
\\
  Instruction-tuned large language models (LLMs) excel at many tasks, and will
even provide explanations for their behavior. Since these models are directly
accessible to the public, there is a risk that convincing and wrong
explanations can lead to unsupported confidence in LLMs. Therefore,
interpretability-faithfulness of self-explanations is an important
consideration for AI Safety. Assessing the interpretability-faithfulness of
these explanations, termed self-explanations, is challenging as the models are
too complex for humans to annotate what is a correct explanation. To address
this, we propose employing self-consistency checks as a measure of
faithfulness. For example, if an LLM says a set of words is important for
making a prediction, then it should not be able to make the same prediction
without these words. While self-consistency checks are a common approach to
faithfulness, they have not previously been applied to LLM's self-explanations.
We apply self-consistency checks to three types of self-explanations:
counterfactuals, importance measures, and redactions. Our work demonstrate that
faithfulness is both task and model dependent, e.g., for sentiment
classification, counterfactual explanations are more faithful for Llama2,
importance measures for Mistral, and redaction for Falcon 40B. Finally, our
findings are robust to prompt-variations.
\\ ( https://arxiv.org/abs/2401.07927 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07928
Date: Mon, 15 Jan 2024 19:39:29 GMT   (364kb)

Title: A Lexicon for Studying Radicalization in Incel Communities
Authors: Emily Klein and Jennifer Golbeck
Categories: cs.CL
Comments: 6 pages, 1 figure
\\
  Incels are an extremist online community of men who believe in an ideology
rooted in misogyny, racism, the glorification of violence, and dehumanization.
In their online forums, they use an extensive, evolving cryptolect - a set of
ingroup terms that have meaning within the group, reflect the ideology,
demonstrate membership in the community, and are difficult for outsiders to
understand. This paper presents a lexicon with terms and definitions for common
incel root words, prefixes, and affixes. The lexicon is text-based for use in
automated analysis and is derived via a Qualitative Content Analysis of the
most frequent incel words, their structure, and their meaning on five of the
most active incel communities from 2016 to 2023. This lexicon will support
future work examining radicalization and deradicalization/disengagement within
the community.
\\ ( https://arxiv.org/abs/2401.07928 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07944
Date: Mon, 15 Jan 2024 20:17:31 GMT   (357kb)

Title: SemEval-2017 Task 4: Sentiment Analysis in Twitter using BERT
Authors: Rupak Kumar Das, Dr. Ted Pedersen
Categories: cs.CL
\\
  This paper uses the BERT model, which is a transformer-based architecture, to
solve task 4A, English Language, Sentiment Analysis in Twitter of SemEval2017.
BERT is a very powerful large language model for classification tasks when the
amount of training data is small. For this experiment, we have used the
BERT{\textsubscript{\tiny BASE}} model, which has 12 hidden layers. This model
provides better accuracy, precision, recall, and f1 score than the Naive Bayes
baseline model. It performs better in binary classification subtasks than the
multi-class classification subtasks. We also considered all kinds of ethical
issues during this experiment, as Twitter data contains personal and sensible
information. The dataset and code used in our experiment can be found in this
GitHub repository.
\\ ( https://arxiv.org/abs/2401.07944 ,  357kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07950
Date: Mon, 15 Jan 2024 20:22:21 GMT   (2519kb,D)

Title: SciGLM: Training Scientific Language Models with Self-Reflective
  Instruction Annotation and Tuning
Authors: Dan Zhang and Ziniu Hu and Sining Zhoubian and Zhengxiao Du and Kaiyu
  Yang and Zihan Wang and Yisong Yue and Yuxiao Dong and Jie Tang
Categories: cs.CL
Comments: 20 pages
\\
  \label{sec:abstract} Large Language Models (LLMs) have shown promise in
assisting scientific discovery. However, such applications are currently
limited by LLMs' deficiencies in understanding intricate scientific concepts,
deriving symbolic equations, and solving advanced numerical calculations. To
bridge these gaps, we introduce SciGLM, a suite of scientific language models
able to conduct college-level scientific reasoning. Central to our approach is
a novel self-reflective instruction annotation framework to address the data
scarcity challenge in the science domain. This framework leverages existing
LLMs to generate step-by-step reasoning for unlabelled scientific questions,
followed by a process of self-reflective critic-and-revise. Applying this
framework, we curated SciInstruct, a diverse and high-quality dataset
encompassing mathematics, physics, chemistry, and formal proofs. We fine-tuned
the ChatGLM family of language models with SciInstruct, enhancing their
capabilities in scientific and mathematical reasoning. Remarkably, SciGLM
consistently improves both the base model (ChatGLM3-6B-Base) and larger-scale
models (12B and 32B), without sacrificing the language understanding
capabilities of the base model. This makes SciGLM a suitable foundational model
to facilitate diverse scientific discovery tasks. For the benefit of the wider
research community, we release SciInstruct, SciGLM, alongside a self-reflective
framework and fine-tuning code at \url{https://github.com/THUDM/SciGLM}.
\\ ( https://arxiv.org/abs/2401.07950 ,  2519kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07955
Date: Mon, 15 Jan 2024 20:42:16 GMT   (393kb,D)

Title: A Study on Large Language Models' Limitations in Multiple-Choice
  Question Answering
Authors: Aisha Khatun and Daniel G. Brown
Categories: cs.CL cs.AI cs.LG
\\
  The widespread adoption of Large Language Models (LLMs) has become
commonplace, particularly with the emergence of open-source models. More
importantly, smaller models are well-suited for integration into consumer
devices and are frequently employed either as standalone solutions or as
subroutines in various AI tasks. Despite their ubiquitous use, there is no
systematic analysis of their specific capabilities and limitations. In this
study, we tackle one of the most widely used tasks - answering Multiple Choice
Question (MCQ). We analyze 26 small open-source models and find that 65% of the
models do not understand the task, only 4 models properly select an answer from
the given choices, and only 5 of these models are choice order independent.
These results are rather alarming given the extensive use of MCQ tests with
these models. We recommend exercising caution and testing task understanding
before using MCQ to evaluate LLMs in any field whatsoever.
\\ ( https://arxiv.org/abs/2401.07955 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07977
Date: Mon, 15 Jan 2024 21:43:46 GMT   (64kb)

Title: Leveraging External Knowledge Resources to Enable Domain-Specific
  Comprehension
Authors: Saptarshi Sengupta, Connor Heaton, Prasenjit Mitra, Soumalya Sarkar
Categories: cs.CL
\\
  Machine Reading Comprehension (MRC) has been a long-standing problem in NLP
and, with the recent introduction of the BERT family of transformer based
language models, it has come a long way to getting solved. Unfortunately,
however, when BERT variants trained on general text corpora are applied to
domain-specific text, their performance inevitably degrades on account of the
domain shift i.e. genre/subject matter discrepancy between the training and
downstream application data. Knowledge graphs act as reservoirs for either open
or closed domain information and prior studies have shown that they can be used
to improve the performance of general-purpose transformers in domain-specific
applications. Building on existing work, we introduce a method using
Multi-Layer Perceptrons (MLPs) for aligning and integrating embeddings
extracted from knowledge graphs with the embeddings spaces of pre-trained
language models (LMs). We fuse the aligned embeddings with open-domain LMs BERT
and RoBERTa, and fine-tune them for two MRC tasks namely span detection
(COVID-QA) and multiple-choice questions (PubMedQA). On the COVID-QA dataset,
we see that our approach allows these models to perform similar to their
domain-specific counterparts, Bio/Sci-BERT, as evidenced by the Exact Match
(EM) metric. With regards to PubMedQA, we observe an overall improvement in
accuracy while the F1 stays relatively the same over the domain-specific
models.
\\ ( https://arxiv.org/abs/2401.07977 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08026
Date: Tue, 16 Jan 2024 00:47:36 GMT   (7951kb,D)

Title: JustiLM: Few-shot Justification Generation for Explainable Fact-Checking
  of Real-world Claims
Authors: Fengzhu Zeng, Wei Gao
Categories: cs.CL
Comments: Accepted in TACL. This is a pre-MIT Press publication version
\\
  Justification is an explanation that supports the veracity assigned to a
claim in fact-checking. However, the task of justification generation is
previously oversimplified as summarization of fact-check article authored by
fact-checkers. Therefore, we propose a realistic approach to generate
justification based on retrieved evidence. We present a new benchmark dataset
called ExClaim for \underline{Ex}plainable fact-checking of real-world
\underline{Claim}s, and introduce JustiLM, a novel few-shot
\underline{Justi}fication generation based on retrieval-augmented
\underline{L}anguage \underline{M}odel by using fact-check articles as
auxiliary resource during training only. Experiments show that JustiLM achieves
promising performance in justification generation compared to strong baselines,
and can also enhance veracity classification with a straightforward extension.
\\ ( https://arxiv.org/abs/2401.08026 ,  7951kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08038
Date: Tue, 16 Jan 2024 01:27:26 GMT   (1486kb,D)

Title: Calpric: Inclusive and Fine-grain Labeling of Privacy Policies with
  Crowdsourcing and Active Learning
Authors: Wenjun Qiu, David Lie, and Lisa Austin
Categories: cs.CL cs.CR cs.HC cs.LG
Comments: published at USENIX Security 2023; associated website:
  https://www.usenix.org/conference/usenixsecurity23/presentation/qiu
\\
  A significant challenge to training accurate deep learning models on privacy
policies is the cost and difficulty of obtaining a large and comprehensive set
of training data. To address these challenges, we present Calpric , which
combines automatic text selection and segmentation, active learning and the use
of crowdsourced annotators to generate a large, balanced training set for
privacy policies at low cost. Automated text selection and segmentation
simplifies the labeling task, enabling untrained annotators from crowdsourcing
platforms, like Amazon's Mechanical Turk, to be competitive with trained
annotators, such as law students, and also reduces inter-annotator agreement,
which decreases labeling cost. Having reliable labels for training enables the
use of active learning, which uses fewer training samples to efficiently cover
the input space, further reducing cost and improving class and data category
balance in the data set. The combination of these techniques allows Calpric to
produce models that are accurate over a wider range of data categories, and
provide more detailed, fine-grain labels than previous work. Our crowdsourcing
process enables Calpric to attain reliable labeled data at a cost of roughly
$0.92-$1.71 per labeled text segment. Calpric 's training process also
generates a labeled data set of 16K privacy policy text segments across 9 Data
categories with balanced positive and negative samples.
\\ ( https://arxiv.org/abs/2401.08038 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08046
Date: Tue, 16 Jan 2024 01:58:36 GMT   (631kb,D)

Title: Enhancing Robustness of LLM-Synthetic Text Detectors for Academic
  Writing: A Comprehensive Analysis
Authors: Zhicheng Dou, Yuchen Guo, Ching-Chun Chang, Huy H. Nguyen, Isao
  Echizen
Categories: cs.CL cs.AI
\\
  The emergence of large language models (LLMs), such as Generative Pre-trained
Transformer 4 (GPT-4) used by ChatGPT, has profoundly impacted the academic and
broader community. While these models offer numerous advantages in terms of
revolutionizing work and study methods, they have also garnered significant
attention due to their potential negative consequences. One example is
generating academic reports or papers with little to no human contribution.
Consequently, researchers have focused on developing detectors to address the
misuse of LLMs. However, most existing methods prioritize achieving higher
accuracy on restricted datasets, neglecting the crucial aspect of
generalizability. This limitation hinders their practical application in
real-life scenarios where reliability is paramount. In this paper, we present a
comprehensive analysis of the impact of prompts on the text generated by LLMs
and highlight the potential lack of robustness in one of the current
state-of-the-art GPT detectors. To mitigate these issues concerning the misuse
of LLMs in academic writing, we propose a reference-based Siamese detector
named Synthetic-Siamese which takes a pair of texts, one as the inquiry and the
other as the reference. Our method effectively addresses the lack of robustness
of previous detectors (OpenAI detector and DetectGPT) and significantly
improves the baseline performances in realistic academic writing scenarios by
approximately 67% to 95%.
\\ ( https://arxiv.org/abs/2401.08046 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08047
Date: Tue, 16 Jan 2024 02:00:17 GMT   (4265kb,D)

Title: Incremental Extractive Opinion Summarization Using Cover Trees
Authors: Somnath Basu Roy Chowdhury, Nicholas Monath, Avinava Dubey, Manzil
  Zaheer, Andrew McCallum, Amr Ahmed, Snigdha Chaturvedi
Categories: cs.CL cs.LG
Comments: Work in progress
\\
  Extractive opinion summarization involves automatically producing a summary
of text about an entity (e.g., a product's reviews) by extracting
representative sentences that capture prevalent opinions in the review set.
Typically, in online marketplaces user reviews accrue over time, and opinion
summaries need to be updated periodically to provide customers with up-to-date
information. In this work, we study the task of extractive opinion
summarization in an incremental setting, where the underlying review set
evolves over time. Many of the state-of-the-art extractive opinion
summarization approaches are centrality-based, such as CentroidRank.
CentroidRank performs extractive summarization by selecting a subset of review
sentences closest to the centroid in the representation space as the summary.
However, these methods are not capable of operating efficiently in an
incremental setting, where reviews arrive one at a time. In this paper, we
present an efficient algorithm for accurately computing the CentroidRank
summaries in an incremental setting. Our approach, CoverSumm, relies on
indexing review representations in a cover tree and maintaining a reservoir of
candidate summary review sentences. CoverSumm's efficacy is supported by a
theoretical and empirical analysis of running time. Empirically, on a diverse
collection of data (both real and synthetically created to illustrate scaling
considerations), we demonstrate that CoverSumm is up to 25x faster than
baseline methods, and capable of adapting to nuanced changes in data
distribution. We also conduct human evaluations of the generated summaries and
find that CoverSumm is capable of producing informative summaries consistent
with the underlying review set.
\\ ( https://arxiv.org/abs/2401.08047 ,  4265kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08088
Date: Tue, 16 Jan 2024 03:28:26 GMT   (211kb,D)

Title: Enhancing Document-level Translation of Large Language Model via
  Translation Mixed-instructions
Authors: Yachao Li, Junhui Li, Jing Jiang and Min Zhang
Categories: cs.CL
Comments: under review
\\
  Existing large language models (LLMs) for machine translation are typically
fine-tuned on sentence-level translation instructions and achieve satisfactory
performance at the sentence level. However, when applied to document-level
translation, these models face a significant challenge, particularly when
dealing with documents containing over 512 tokens. This challenge arises from
the issue of sentence-level coverage, where subsequent sentences in the
document remain untranslated. As a result, the document-level translation
capability of LLMs fine-tuned on sentence-level translation instructions is
significantly limited. We conjecture that the primary cause of LLMs' weak
document-level translation performance is the absence of document-to-document
mapping ability. To address the issue, we propose an approach that combines
sentence-level and document-level translation instructions of varying lengths
to fine-tune LLMs. Our proposed translation mixed-instructions enable LLMs
(Llama-2~7B and 13B) to maintain consistent translation performance from the
sentence level to documents containing as many as 2048 tokens. Extensive
experimental results show that the proposed approach significantly enhances the
document-level translation capabilities of LLMs on 10 language pairs,
effectively mitigating the sentence-level coverage issue in document-level
translation. Experimentation on discourse phenomena has demonstrated that our
document-level translation approach significantly improves translation quality,
both in terms of BLEU score and discourse coherence.
\\ ( https://arxiv.org/abs/2401.08088 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08089
Date: Tue, 16 Jan 2024 03:28:29 GMT   (760kb,D)

Title: A Study on Training and Developing Large Language Models for Behavior
  Tree Generation
Authors: Fu Li, Xueying Wang, Bin Li, Yunlong Wu, Yanzhen Wang and Xiaodong Yi
Categories: cs.CL cs.AI cs.RO
\\
  This paper presents an innovative exploration of the application potential of
large language models (LLM) in addressing the challenging task of automatically
generating behavior trees (BTs) for complex tasks. The conventional manual BT
generation method is inefficient and heavily reliant on domain expertise. On
the other hand, existing automatic BT generation technologies encounter
bottlenecks related to task complexity, model adaptability, and reliability. In
order to overcome these challenges, we propose a novel methodology that
leverages the robust representation and reasoning abilities of LLMs. The core
contribution of this paper lies in the design of a BT generation framework
based on LLM, which encompasses the entire process, from data synthesis and
model training to application developing and data verification. Synthetic data
is introduced to train the BT generation model (BTGen model), enhancing its
understanding and adaptability to various complex tasks, thereby significantly
improving its overall performance. In order to ensure the effectiveness and
executability of the generated BTs, we emphasize the importance of data
verification and introduce a multilevel verification strategy. Additionally, we
explore a range of agent design and development schemes with LLM as the central
element. We hope that the work in this paper may provide a reference for the
researchers who are interested in BT generation based on LLMs.
\\ ( https://arxiv.org/abs/2401.08089 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08190
Date: Tue, 16 Jan 2024 08:08:01 GMT   (937kb,D)

Title: MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible
  Pipeline
Authors: Minpeng Liao, Wei Luo, Chengxi Li, Jing Wu, Kai Fan
Categories: cs.CL
Comments: Work in progress
\\
  Large language models (LLMs) have seen considerable advancements in natural
language understanding tasks, yet there remains a gap to bridge before
attaining true artificial general intelligence, especially concerning
shortcomings in mathematical reasoning capabilities. We postulate that the
inherent nature of LLM training, which focuses on predicting probabilities of
next token, presents challenges in effectively modeling mathematical reasoning
that demands exact calculations, both from data-driven and theoretical
standpoints. In this paper, we address this challenge by enriching the data
landscape and introducing a novel math dataset, enhanced with a capability to
utilize a Python code interpreter. This dataset is derived from GSM8K and MATH
and has been further refined through a combination of GPT-4 annotations, human
review, and self-training processes, where the errors in the original GSM8K
training set have been fixed. Additionally, we propose a tentative, easily
replicable protocol for the fine-tuning of math-specific LLMs, which has led to
a significant improvement in the performance of a 7B-parameter LLM on the GSM8K
and MATH datasets. We are committed to advancing the field of mathematical
reasoning in LLMs and, to that end, we have made the model checkpoints and will
make the dataset publicly available. We hope this will facilitate further
research and development within the community.
\\ ( https://arxiv.org/abs/2401.08190 ,  937kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08255
Date: Tue, 16 Jan 2024 10:14:27 GMT   (152kb,D)

Title: A Generative Adversarial Attack for Multilingual Text Classifiers
Authors: Tom Roth, Inigo Jauregi Unanue, Alsharif Abuadbba, Massimo Piccardi
Categories: cs.CL cs.AI
Comments: AAAI-24 Workshop on Artificial Intelligence for Cyber Security (AICS)
\\
  Current adversarial attack algorithms, where an adversary changes a text to
fool a victim model, have been repeatedly shown to be effective against text
classifiers. These attacks, however, generally assume that the victim model is
monolingual and cannot be used to target multilingual victim models, a
significant limitation given the increased use of these models. For this
reason, in this work we propose an approach to fine-tune a multilingual
paraphrase model with an adversarial objective so that it becomes able to
generate effective adversarial examples against multilingual classifiers. The
training objective incorporates a set of pre-trained models to ensure text
quality and language consistency of the generated text. In addition, all the
models are suitably connected to the generator by vocabulary-mapping matrices,
allowing for full end-to-end differentiability of the overall training
pipeline. The experimental validation over two multilingual datasets and five
languages has shown the effectiveness of the proposed approach compared to
existing baselines, particularly in terms of query efficiency. We also provide
a detailed analysis of the generated attacks and discuss limitations and
opportunities for future research.
\\ ( https://arxiv.org/abs/2401.08255 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08273
Date: Tue, 16 Jan 2024 10:53:11 GMT   (8564kb,D)

Title: Large Language Models are Null-Shot Learners
Authors: Pittawat Taveekitworachai, Febri Abdullah, Ruck Thawonmas
Categories: cs.CL cs.AI cs.LG
Comments: 24 pages
\\
  This paper presents null-shot prompting. Null-shot prompting exploits
hallucination in large language models (LLMs) by instructing LLMs to utilize
information from the "Examples" section that never exists within the provided
context to perform a task. While reducing hallucination is crucial and
non-negligible for daily and critical uses of LLMs, we propose that in the
current landscape in which these LLMs still hallucinate, it is possible, in
fact, to exploit hallucination to increase performance in performing tasks
compared to standard zero-shot prompting. Experiments with six LLMs show
improvements in performance across the majority of eight datasets, including
reading comprehension, arithmetic reasoning, and closed-book question
answering. The observed inconsistency in increased relative performance across
LLMs also potentially indicates a different degree of inherent hallucination in
each model. These differences show that it is possible to utilize null-shot
prompting as a way to detect degrees of hallucination in LLMs using existing
benchmarking datasets. We also perform ablation studies, including
experimenting with a modified version of null-shot prompting that incorporates
ideas from zero-shot chain-of-thought prompting, which shows different trends
of results.
\\ ( https://arxiv.org/abs/2401.08273 ,  8564kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08294
Date: Tue, 16 Jan 2024 11:39:09 GMT   (417kb,D)

Title: Inferflow: an Efficient and Highly Configurable Inference Engine for
  Large Language Models
Authors: Shuming Shi, Enbo Zhao, Deng Cai, Leyang Cui, Xinting Huang, Huayang
  Li
Categories: cs.CL
Comments: Technical report of Inferflow
\\
  We present Inferflow, an efficient and highly configurable inference engine
for large language models (LLMs). With Inferflow, users can serve most of the
common transformer models by simply modifying some lines in corresponding
configuration files, without writing a single line of source code. Compared
with most existing inference engines, Inferflow has some key features. First,
by implementing a modular framework of atomic build-blocks and technologies,
Inferflow is compositionally generalizable to new models. Second, 3.5-bit
quantization is introduced in Inferflow as a tradeoff between 3-bit and 4-bit
quantization. Third, hybrid model partitioning for multi-GPU inference is
introduced in Inferflow to better balance inference speed and throughput than
the existing partition-by-layer and partition-by-tensor strategies.
\\ ( https://arxiv.org/abs/2401.08294 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08295
Date: Tue, 16 Jan 2024 11:45:03 GMT   (1649kb,D)

Title: DAPT: A Dual Attention Framework for Parameter-Efficient Continual
  Learning of Large Language Models
Authors: Weixiang Zhao, Shilong Wang, Yulin Hu, Yanyan Zhao, Bing Qin, Xuanyu
  Zhang, Qing Yang, Dongliang Xu, Wanxiang Che
Categories: cs.CL
Comments: work in progress
\\
  The continual learning (CL) ability is vital for deploying large language
models (LLMs) in the dynamic world. Based on parameter-efficient tuning (PET),
existing methods devise the learning module and the selection module to handle
the challenges of catastrophic forgetting (CF) and knowledge transfer (KT) in
CL. The learning module allocates separate PET blocks for each continually
emerged task and the selection module function to choose the correct one for
the input at testing time. However, there are limitations in their deigns of
both modules and they ignore the potential of aligning the two module to
address CF and KT simultaneously. To this end, we propose a novel Dual
Attention Framework , to align the PET learning and selection via the Dual
Attentive Learning\&Selection module. Extensive Experiments on two CL
benchmarks demonstrate the superiority of DAPT to resist CF and facilitate KT
at the same time. Moreover, DAPT exhibits the superiority when we scale it to
different model sizes (from 770M to 11B) and unseen tasks.
\\ ( https://arxiv.org/abs/2401.08295 ,  1649kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08309
Date: Tue, 16 Jan 2024 12:10:49 GMT   (24891kb,D)

Title: Anchor function: a type of benchmark functions for studying language
  models
Authors: Zhongwang Zhang, Zhiwei Wang, Junjie Yao, Zhangchen Zhou, Xiaolong Li,
  Weinan E, Zhi-Qin John Xu
Categories: cs.CL cs.LG
\\
  Understanding transformer-based language models is becoming increasingly
crucial, particularly as they play pivotal roles in advancing towards
artificial general intelligence. However, language model research faces
significant challenges, especially for academic research groups with
constrained resources. These challenges include complex data structures,
unknown target functions, high computational costs and memory requirements, and
a lack of interpretability in the inference process, etc. Drawing a parallel to
the use of simple models in scientific research, we propose the concept of an
anchor function. This is a type of benchmark function designed for studying
language models in learning tasks that follow an "anchor-key" pattern. By
utilizing the concept of an anchor function, we can construct a series of
functions to simulate various language tasks. The anchor function plays a role
analogous to that of mice in diabetes research, particularly suitable for
academic research. We demonstrate the utility of the anchor function with an
example, revealing two basic operations by attention structures in language
models: shifting tokens and broadcasting one token from one position to many
positions. These operations are also commonly observed in large language
models. The anchor function framework, therefore, opens up a series of valuable
and accessible research questions for further exploration, especially for
theoretical study.
\\ ( https://arxiv.org/abs/2401.08309 ,  24891kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08315
Date: Tue, 16 Jan 2024 12:30:56 GMT   (8841kb,D)

Title: Application of LLM Agents in Recruitment: A Novel Framework for Resume
  Screening
Authors: Chengguang Gan, Qinghao Zhang, Tatsunori Mori
Categories: cs.CL
Comments: Under review, 14 pages, 10 figures
\\
  The automation of resume screening is a crucial aspect of the recruitment
process in organizations. Automated resume screening systems often encompass a
range of natural language processing (NLP) tasks. The advent of Large Language
Models (LLMs) has notably enhanced the efficacy of these systems, showcasing
their robust generalization abilities across diverse language-related tasks.
Accompanying these developments are various agents based on LLMs, which
facilitate their application in practical scenarios. This paper introduces a
novel LLM-based agent framework for resume screening, aimed at enhancing
efficiency and time management in recruitment processes. Our framework is
distinct in its ability to efficiently summarize and grade each resume from a
large dataset. Moreover, it utilizes LLM agents for decision-making,
determining which candidates receive job offers, or which ones to bring in for
interviews. To evaluate our framework, we constructed a dataset from actual
resumes and conducted simulate a resume screening process. Subsequently, the
outcomes of the simulation experiment were compared and subjected to detailed
analysis. The results demonstrate that our automated resume screening framework
is 11 times faster than traditional manual methods. Furthermore, by fine-tuning
the LLMs, we observed a significant improvement in the F1 score, reaching
87.73\%, during the resume sentence classification phase. In the resume
summarization and grading phase, our fine-tuned model surpassed the baseline
performance of the GPT-3.5 model. Analysis of the decision-making efficacy of
the LLM agents in the final offer stage further underscores the potential of
LLM agents in transforming resume screening processes.
\\ ( https://arxiv.org/abs/2401.08315 ,  8841kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08326
Date: Tue, 16 Jan 2024 12:45:15 GMT   (8728kb,D)

Title: RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large
  Language Models in Tool Learning
Authors: Junjie Ye, Yilong Wu, Songyang Gao, Sixian Li, Guanyu Li, Xiaoran Fan,
  Qi Zhang, Tao Gui, Xuanjing Huang
Categories: cs.CL cs.AI
\\
  Tool learning has generated widespread interest as a vital means of
interaction between Large Language Models (LLMs) and the physical world.
Current research predominantly emphasizes LLMs' capacity to utilize tools in
well-structured environments while overlooking their stability when confronted
with the inevitable noise of the real world. To bridge this gap, we introduce
RoTBench, a multi-level benchmark for evaluating the robustness of LLMs in tool
learning. Specifically, we establish five external environments, each featuring
varying levels of noise (i.e., Clean, Slight, Medium, Heavy, and Union),
providing an in-depth analysis of the model's resilience across three critical
phases: tool selection, parameter identification, and content filling.
Experiments involving six widely-used models underscore the urgent necessity
for enhancing the robustness of LLMs in tool learning. For instance, the
performance of GPT-4 even drops significantly from 80.00 to 58.10 when there is
no substantial change in manual accuracy. More surprisingly, the noise
correction capability inherent in the GPT family paradoxically impedes its
adaptability in the face of mild noise. In light of these findings, we propose
RoTTuning, a strategy that enriches the diversity of training environments to
bolster the robustness of LLMs in tool learning. The code and data are
available at https://github.com/Junjie-Ye/RoTBench.
\\ ( https://arxiv.org/abs/2401.08326 ,  8728kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08350
Date: Tue, 16 Jan 2024 13:30:09 GMT   (8916kb,D)

Title: Salute the Classic: Revisiting Challenges of Machine Translation in the
  Age of Large Language Models
Authors: Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu, Derek F. Wong,
  Shuming Shi, Zhaopeng Tu
Categories: cs.CL
Comments: 17 pages
\\
  The evolution of Neural Machine Translation (NMT) has been significantly
influenced by six core challenges (Koehn and Knowles, 2017), which have acted
as benchmarks for progress in this field. This study revisits these challenges,
offering insights into their ongoing relevance in the context of advanced Large
Language Models (LLMs): domain mismatch, amount of parallel data, rare word
prediction, translation of long sentences, attention model as word alignment,
and sub-optimal beam search. Our empirical findings indicate that LLMs
effectively lessen the reliance on parallel data for major languages in the
pretraining phase. Additionally, the LLM-based translation system significantly
enhances the translation of long sentences that contain approximately 80 words
and shows the capability to translate documents of up to 512 words. However,
despite these significant improvements, the challenges of domain mismatch and
prediction of rare words persist. While the challenges of word alignment and
beam search, specifically associated with NMT, may not apply to LLMs, we
identify three new challenges for LLMs in translation tasks: inference
efficiency, translation of low-resource languages in the pretraining phase, and
human-aligned evaluation. The datasets and models are released at
https://github.com/pangjh3/LLM4MT.
\\ ( https://arxiv.org/abs/2401.08350 ,  8916kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08358
Date: Tue, 16 Jan 2024 13:36:07 GMT   (10580kb,D)

Title: Hallucination Detection and Hallucination Mitigation: An Investigation
Authors: Junliang Luo, Tianyu Li, Di Wu, Michael Jenkin, Steve Liu, Gregory
  Dudek
Categories: cs.CL cs.AI
\\
  Large language models (LLMs), including ChatGPT, Bard, and Llama, have
achieved remarkable successes over the last two years in a range of different
applications. In spite of these successes, there exist concerns that limit the
wide application of LLMs. A key problem is the problem of hallucination.
Hallucination refers to the fact that in addition to correct responses, LLMs
can also generate seemingly correct but factually incorrect responses. This
report aims to present a comprehensive review of the current literature on both
hallucination detection and hallucination mitigation. We hope that this report
can serve as a good reference for both engineers and researchers who are
interested in LLMs and applying them to real world tasks.
\\ ( https://arxiv.org/abs/2401.08358 ,  10580kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08367
Date: Tue, 16 Jan 2024 13:52:25 GMT   (231kb)

Title: Morphology and Syntax of the Tamil Language
Authors: Kengatharaiyer Sarveswaran
Categories: cs.CL
Comments: 45 pages
\\
  This paper provides an overview of the morphology and syntax of the Tamil
language, focusing on its contemporary usage. The paper also highlights the
complexity and richness of Tamil in terms of its morphological and syntactic
features, which will be useful for linguists analysing the language and
conducting comparative studies. In addition, the paper will be useful for those
developing computational resources for the Tamil language. It is proven as a
rule-based morphological analyser cum generator and a computational grammar for
Tamil have already been developed based on this paper. To enhance accessibility
for a broader audience, the analysis is conducted without relying on any
specific grammatical formalism.
\\ ( https://arxiv.org/abs/2401.08367 ,  231kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08374
Date: Tue, 16 Jan 2024 14:00:28 GMT   (964kb,D)

Title: Cross-lingual neural fuzzy matching for exploiting target-language
  monolingual corpora in computer-aided translation
Authors: Miquel Espl\`a-Gomis, V\'ictor M. S\'anchez-Cartagena, Juan Antonio
  P\'erez-Ortiz, Felipe S\'anchez-Mart\'inez
Categories: cs.CL
Journal-ref: In Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing (pp. 7532-7543)
DOI: 10.18653/v1/2022.emnlp-main.511
\\
  Computer-aided translation (CAT) tools based on translation memories (MT)
play a prominent role in the translation workflow of professional translators.
However, the reduced availability of in-domain TMs, as compared to in-domain
monolingual corpora, limits its adoption for a number of translation tasks. In
this paper, we introduce a novel neural approach aimed at overcoming this
limitation by exploiting not only TMs, but also in-domain target-language (TL)
monolingual corpora, and still enabling a similar functionality to that offered
by conventional TM-based CAT tools. Our approach relies on cross-lingual
sentence embeddings to retrieve translation proposals from TL monolingual
corpora, and on a neural model to estimate their post-editing effort. The paper
presents an automatic evaluation of these techniques on four language pairs
that shows that our approach can successfully exploit monolingual texts in a
TM-based CAT environment, increasing the amount of useful translation
proposals, and that our neural model for estimating the post-editing effort
enables the combination of translation proposals obtained from monolingual
corpora and from TMs in the usual way. A human evaluation performed on a single
language pair confirms the results of the automatic evaluation and seems to
indicate that the translation proposals retrieved with our approach are more
useful than what the automatic evaluation shows.
\\ ( https://arxiv.org/abs/2401.08374 ,  964kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08406
Date: Tue, 16 Jan 2024 14:44:47 GMT   (1300kb,D)

Title: RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on
  Agriculture
Authors: Aman Gupta, Anup Shirgaonkar, Angels de Luis Balaguer, Bruno Silva,
  Daniel Holstein, Dawei Li, Jennifer Marsman, Leonardo O. Nunes, Mahsa
  Rouzbahman, Morris Sharp, Nick Mecklenburg, Rafael Padilha, Ranveer Chandra,
  Renato Luiz de Freitas Cunha, Roberto de M. Estev\~ao Filho, Ryan Tsang, Sara
  Malvar, Swati Sharma, Todd Hendry, Vijay Aski, Vijetha Vijayendran, Vinamra
  Benara
Categories: cs.CL cs.LG
\\
  There are two common ways in which developers are incorporating proprietary
and domain-specific data when building applications of Large Language Models
(LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the
prompt with the external data, while fine-Tuning incorporates the additional
knowledge into the model itself. However, the pros and cons of both approaches
are not well understood. In this paper, we propose a pipeline for fine-tuning
and RAG, and present the tradeoffs of both for multiple popular LLMs, including
Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages,
including extracting information from PDFs, generating questions and answers,
using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We
propose metrics to assess the performance of different stages of the RAG and
fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset.
Agriculture as an industry has not seen much penetration of AI, and we study a
potentially disruptive application - what if we could provide location-specific
insights to a farmer? Our results show the effectiveness of our dataset
generation pipeline in capturing geographic-specific knowledge, and the
quantitative and qualitative benefits of RAG and fine-tuning. We see an
accuracy increase of over 6 p.p. when fine-tuning the model and this is
cumulative with RAG, which increases accuracy by 5 p.p. further. In one
particular experiment, we also demonstrate that the fine-tuned model leverages
information from across geographies to answer specific questions, increasing
answer similarity from 47% to 72%. Overall, the results point to how systems
built using LLMs can be adapted to respond and incorporate knowledge across a
dimension that is critical for a specific industry, paving the way for further
applications of LLMs in other industrial domains.
\\ ( https://arxiv.org/abs/2401.08406 ,  1300kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08417
Date: Tue, 16 Jan 2024 15:04:51 GMT   (180kb,D)

Title: Contrastive Preference Optimization: Pushing the Boundaries of LLM
  Performance in Machine Translation
Authors: Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen,
  Benjamin Van Durme, Kenton Murray, Young Jin Kim
Categories: cs.CL
\\
  Moderate-sized large language models (LLMs) -- those with 7B or 13B
parameters -- exhibit promising machine translation (MT) performance. However,
even the top-performing 13B LLM-based translation models, like ALMA, does not
match the performance of state-of-the-art conventional encoder-decoder
translation models or larger-scale LLMs such as GPT-4. In this study, we bridge
this performance gap. We first assess the shortcomings of supervised
fine-tuning for LLMs in the MT task, emphasizing the quality issues present in
the reference data, despite being human-generated. Then, in contrast to SFT
which mimics reference translations, we introduce Contrastive Preference
Optimization (CPO), a novel approach that trains models to avoid generating
adequate but not perfect translations. Applying CPO to ALMA models with only
22K parallel sentences and 12M parameters yields significant improvements. The
resulting model, called ALMA-R, can match or exceed the performance of the WMT
competition winners and GPT-4 on WMT'21, WMT'22 and WMT'23 test datasets.
\\ ( https://arxiv.org/abs/2401.08417 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08420
Date: Tue, 16 Jan 2024 15:07:09 GMT   (9651kb,D)

Title: Ask the experts: sourcing high-quality datasets for nutritional
  counselling through Human-AI collaboration
Authors: Simone Balloccu, Ehud Reiter, Vivek Kumar, Diego Reforgiato Recupero
  and Daniele Riboni
Categories: cs.CL
\\
  Large Language Models (LLMs), with their flexible generation abilities, can
be powerful data sources in domains with few or no available corpora. However,
problems like hallucinations and biases limit such applications. In this case
study, we pick nutrition counselling, a domain lacking any public resource, and
show that high-quality datasets can be gathered by combining LLMs,
crowd-workers and nutrition experts. We first crowd-source and cluster a novel
dataset of diet-related issues, then work with experts to prompt ChatGPT into
producing related supportive text. Finally, we let the experts evaluate the
safety of the generated text. We release HAI-coaching, the first
expert-annotated nutrition counselling dataset containing ~2.4K dietary
struggles from crowd workers, and ~97K related supportive texts generated by
ChatGPT. Extensive analysis shows that ChatGPT while producing highly fluent
and human-like text, also manifests harmful behaviours, especially in sensitive
topics like mental health, making it unsuitable for unsupervised use.
\\ ( https://arxiv.org/abs/2401.08420 ,  9651kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08429
Date: Tue, 16 Jan 2024 15:16:34 GMT   (10297kb)

Title: Machine Translation with Large Language Models: Prompt Engineering for
  Persian, English, and Russian Directions
Authors: Nooshin Pourkamali, Shler Ebrahim Sharifi
Categories: cs.CL cs.AI cs.HC cs.LG
Comments: 34 pages, 46 figures
MSC-class: ACM-class: I.2.2
ACM-class: I.2.2
\\
  Generative large language models (LLMs) have demonstrated exceptional
proficiency in various natural language processing (NLP) tasks, including
machine translation, question answering, text summarization, and natural
language understanding.
  To further enhance the performance of LLMs in machine translation, we
conducted an investigation into two popular prompting methods and their
combination, focusing on cross-language combinations of Persian, English, and
Russian. We employed n-shot feeding and tailored prompting frameworks. Our
findings indicate that multilingual LLMs like PaLM exhibit human-like machine
translation outputs, enabling superior fine-tuning of desired translation
nuances in accordance with style guidelines and linguistic considerations.
These models also excel in processing and applying prompts. However, the choice
of language model, machine translation task, and the specific source and target
languages necessitate certain considerations when adopting prompting frameworks
and utilizing n-shot in-context learning.
  Furthermore, we identified errors and limitations inherent in popular LLMs as
machine translation tools and categorized them based on various linguistic
metrics. This typology of errors provides valuable insights for utilizing LLMs
effectively and offers methods for designing prompts for in-context learning.
Our report aims to contribute to the advancement of machine translation with
LLMs by improving both the accuracy and reliability of evaluation metrics.
\\ ( https://arxiv.org/abs/2401.08429 ,  10297kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08438
Date: Sat, 6 Jan 2024 03:59:59 GMT   (855kb,D)

Title: CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language
  Models
Authors: Yaojia Lv, Haojie Pan, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin
Categories: cs.CL cs.AI cs.LG
\\
  Cognitive dynamics are pivotal to advance human understanding of the world.
Recent advancements in large language models (LLMs) reveal their potential for
cognitive simulation. However, these LLM-based cognitive studies primarily
focus on static modeling, overlooking the dynamic nature of cognition. To
bridge this gap, we propose the concept of the cognitive dynamics of LLMs and
present a corresponding task with the inspiration of longitudinal studies.
Towards the task, we develop CogBench, a novel benchmark to assess the
cognitive dynamics of LLMs and validate it through participant surveys. We also
design two evaluation metrics for CogBench, including Authenticity and
Rationality. Recognizing the inherent static nature of LLMs, we introduce
CogGPT for the task, which features an innovative iterative cognitive mechanism
aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate
the superiority of CogGPT over existing methods, particularly in its ability to
facilitate role-specific cognitive dynamics under continuous information flows.
\\ ( https://arxiv.org/abs/2401.08438 ,  855kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08491
Date: Tue, 16 Jan 2024 16:49:39 GMT   (1220kb,D)

Title: Contrastive Perplexity for Controlled Generation: An Application in
  Detoxifying Large Language Models
Authors: Tassilo Klein, Moin Nabi
Categories: cs.CL cs.LG
\\
  The generation of undesirable and factually incorrect content of large
language models poses a significant challenge and remains largely an unsolved
issue. This paper studies the integration of a contrastive learning objective
for fine-tuning LLMs for implicit knowledge editing and controlled text
generation. Optimizing the training objective entails aligning text
perplexities in a contrastive fashion. To facilitate training the model in a
self-supervised fashion, we leverage an off-the-shelf LLM for training data
generation. We showcase applicability in the domain of detoxification. Herein,
the proposed approach leads to a significant decrease in the generation of
toxic content while preserving general utility for downstream tasks such as
commonsense reasoning and reading comprehension. The proposed approach is
conceptually simple but empirically powerful.
\\ ( https://arxiv.org/abs/2401.08491 ,  1220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08495
Date: Tue, 16 Jan 2024 16:52:00 GMT   (64kb,D)

Title: The Effect of Group Status on the Variability of Group Representations
  in LLM-generated Text
Authors: Messi H.J. Lee, Jacob M. Montgomery, Calvin K. Lai
Categories: cs.CL
Comments: Presented at the Socially Responsible Language Modelling Research
  (SoLaR) Workshop at NeurIPS 2023
\\
  Large Language Models (LLMs) have become pervasive in everyday life, yet
their inner workings remain opaque. While scholarly efforts have demonstrated
LLMs' propensity to reproduce biases in their training data, they have
primarily focused on the association of social groups with stereotypic
attributes. In this paper, we extend this line of inquiry to investigate a bias
akin to the social-psychological phenomenon where socially dominant groups are
perceived to be less homogeneous than socially subordinate groups as it is
reproduced by LLMs. We had ChatGPT, a state-of-the-art LLM, generate a
diversity of texts about intersectional group identities and compared text
homogeneity. We consistently find that LLMs portray African, Asian, and
Hispanic Americans as more homogeneous than White Americans. They also portray
women as more homogeneous than men, but these differences are small. Finally,
we find that the effect of gender differs across racial/ethnic groups such that
the effect of gender is consistent within African and Hispanic Americans but
not within Asian and White Americans. We speculate possible sources of this
bias in LLMs and posit that the bias has the potential to amplify biases in
future LLM training and to reinforce stereotypes.
\\ ( https://arxiv.org/abs/2401.08495 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08508
Date: Tue, 16 Jan 2024 17:11:11 GMT   (861kb,D)

Title: EmoLLMs: A Series of Emotional Large Language Models and Annotation
  Tools for Comprehensive Affective Analysis
Authors: Zhiwei Liu, Kailai Yang, Tianlin Zhang, Qianqian Xie, Zeping Yu,
  Sophia Ananiadou
Categories: cs.CL
Comments: Work in progress
\\
  Sentiment analysis and emotion detection are important research topics in
natural language processing (NLP) and benefit many downstream tasks. With the
widespread application of LLMs, researchers have started exploring the
application of LLMs based on instruction-tuning in the field of sentiment
analysis. However, these models only focus on single aspects of affective
classification tasks (e.g. sentimental polarity or categorical emotions), and
overlook the regression tasks (e.g. sentiment strength or emotion intensity),
which leads to poor performance in downstream tasks. The main reason is the
lack of comprehensive affective instruction tuning datasets and evaluation
benchmarks, which cover various affective classification and regression tasks.
Moreover, although emotional information is useful for downstream tasks,
existing downstream datasets lack high-quality and comprehensive affective
annotations. In this paper, we propose EmoLLMs, the first series of
open-sourced instruction-following LLMs for comprehensive affective analysis
based on fine-tuning various LLMs with instruction data, the first multi-task
affective analysis instruction dataset (AAID) with 234K data samples based on
various classification and regression tasks to support LLM instruction tuning,
and a comprehensive affective evaluation benchmark (AEB) with 14 tasks from
various sources and domains to test the generalization ability of LLMs. We
propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various
affective instruction tasks. We compare our model with a variety of LLMs on
AEB, where our models outperform all other open-sourced LLMs, and surpass
ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve
the ChatGPT-level and GPT-4-level generalization capabilities on affective
analysis tasks, and demonstrates our models can be used as affective annotation
tools.
\\ ( https://arxiv.org/abs/2401.08508 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08511
Date: Tue, 16 Jan 2024 17:15:08 GMT   (469kb,D)

Title: The Gaps between Pre-train and Downstream Settings in Bias Evaluation
  and Debiasing
Authors: Masahiro Kaneko, Danushka Bollegala, Timothy Baldwin
Categories: cs.CL
\\
  The output tendencies of Pre-trained Language Models (PLM) vary markedly
before and after Fine-Tuning (FT) due to the updates to the model parameters.
These divergences in output tendencies result in a gap in the social biases of
PLMs. For example, there exits a low correlation between intrinsic bias scores
of a PLM and its extrinsic bias scores under FT-based debiasing methods.
Additionally, applying FT-based debiasing methods to a PLM leads to a decline
in performance in downstream tasks. On the other hand, PLMs trained on large
datasets can learn without parameter updates via In-Context Learning (ICL)
using prompts. ICL induces smaller changes to PLMs compared to FT-based
debiasing methods. Therefore, we hypothesize that the gap observed in
pre-trained and FT models does not hold true for debiasing methods that use
ICL. In this study, we demonstrate that ICL-based debiasing methods show a
higher correlation between intrinsic and extrinsic bias scores compared to
FT-based methods. Moreover, the performance degradation due to debiasing is
also lower in the ICL case compared to that in the FT case.
\\ ( https://arxiv.org/abs/2401.08511 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08537
Date: Tue, 16 Jan 2024 17:59:54 GMT   (1054kb)

Title: Spatial Entity Resolution between Restaurant Locations and
  Transportation Destinations in Southeast Asia
Authors: Emily Gao, Dominic Widdows
Categories: cs.CL
Journal-ref: 6th International Conference on Geospatial Information Systems
  Theory, Applications, and Management. GISTAM 2020, Prague, Czech Republic,
  May 7-9, 2020
\\
  As a tech company, Grab has expanded from transportation to food delivery,
aiming to serve Southeast Asia with hyperlocalized applications. Information
about places as transportation destinations can help to improve our knowledge
about places as restaurants, so long as the spatial entity resolution problem
between these datasets can be solved. In this project, we attempted to
recognize identical place entities from databases of Points-of-Interest (POI)
and GrabFood restaurants, using their spatial and textual attributes, i.e.,
latitude, longitude, place name, and street address.
  Distance metrics were calculated for these attributes and fed to tree-based
classifiers. POI-restaurant matching was conducted separately for Singapore,
Philippines, Indonesia, and Malaysia. Experimental estimates demonstrate that a
matching POI can be found for over 35% of restaurants in these countries. As
part of these estimates, test datasets were manually created, and RandomForest,
AdaBoost, Gradient Boosting, and XGBoost perform well, with most accuracy,
precision, and recall scores close to or higher than 90% for matched vs.
unmatched classification. To the authors' knowledge, there are no previous
published scientific papers devoted to matching of spatial entities for the
Southeast Asia region.
\\ ( https://arxiv.org/abs/2401.08537 ,  1054kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08565
Date: Tue, 16 Jan 2024 18:49:55 GMT   (6441kb,D)

Title: Tuning Language Models by Proxy
Authors: Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi,
  Noah A. Smith
Categories: cs.CL
Comments: 21 pages
\\
  Despite the general capabilities of large pretrained language models, they
consistently benefit from further adaptation to better achieve desired
behaviors. However, tuning these models has become increasingly
resource-intensive, or impossible when model weights are private. We introduce
proxy-tuning, a lightweight decoding-time algorithm that operates on top of
black-box LMs to achieve the result of directly tuning the model, but by
accessing only its prediction over the output vocabulary. Our method instead
tunes a smaller LM, then applies the difference between the predictions of the
small tuned and untuned LMs to shift the original predictions of the base model
in the direction of tuning, while retaining the benefits of larger scale
pretraining. In experiments, when we apply proxy-tuning to Llama2-70B using
proxies of only 7B size, we can close 88% of the gap between Llama2-70B and its
truly-tuned chat version, when evaluated across knowledge, reasoning, and
safety benchmarks. Interestingly, when tested on TruthfulQA, proxy-tuned models
are actually more truthful than directly tuned models, possibly because
decoding-time guidance better retains the model's factual knowledge. We then
demonstrate the generality of proxy-tuning by applying it for domain adaptation
on code, and task-specific finetuning on question-answering and math problems.
Our work demonstrates the promise of using small tuned LMs to efficiently
customize large, potentially proprietary LMs through decoding-time guidance.
\\ ( https://arxiv.org/abs/2401.08565 ,  6441kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08574
Date: Tue, 16 Jan 2024 18:58:37 GMT   (8442kb,D)

Title: Deductive Closure Training of Language Models for Coherence, Accuracy,
  and Updatability
Authors: Afra Feyza Aky\"urek, Ekin Aky\"urek, Leshem Choshen, Derry Wijaya and
  Jacob Andreas
Categories: cs.CL
\\
  While language models (LMs) can sometimes generate factually correct text and
estimate truth values of individual claims, these generally do not reflect a
globally coherent, manipulable model of the world. As a consequence, current
LMs also generate incorrect or nonsensical content, and are difficult to edit
and bring up to date. We present a method called Deductive Closure Training
(DCT) that uses LMs themselves to identify implications of (and contradictions
within) the text that they generate, yielding an efficient self-supervised
procedure for improving LM factuality. Given a collection of seed documents,
DCT prompts LMs to generate additional text implied by these documents, reason
globally about the correctness of this generated text, and finally fine-tune on
text inferred to be correct. Given seed documents from a trusted source, DCT
provides a tool for supervised model updating; if seed documents are sampled
from the LM itself, DCT enables fully unsupervised fine-tuning for improved
coherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets,
supervised DCT improves LM fact verification and text generation accuracy by
3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%.
These results show that LMs' reasoning capabilities during inference can be
leveraged during training to improve their reliability.
\\ ( https://arxiv.org/abs/2401.08574 ,  8442kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06821
Date: Thu, 11 Jan 2024 21:04:28 GMT   (544kb,D)

Title: Surrogate Neural Networks Local Stability for Aircraft Predictive
  Maintenance
Authors: M\'elanie Ducoffe, Guillaume Pov\'eda, Audrey Galametz, Ryma
  Boumazouza, Marion-C\'ecile Martin, Julien Baris, Derk Daverschot and Eugene
  O'Higgins
Categories: cs.LG cs.AI
\\
  Surrogate Neural Networks (NN) now routinely serve as substitutes for
computationally demanding simulations (e.g., finite element). They enable
faster analyses in industrial applications e.g., manufacturing processes,
performance assessment. The verification of surrogate models is a critical step
to assess their robustness under different scenarios. We explore the
combination of empirical and formal methods in one NN verification pipeline. We
showcase its efficiency on an industrial use case of aircraft predictive
maintenance. We assess the local stability of surrogate NN designed to predict
the stress sustained by an aircraft part from external loads. Our contribution
lies in the complete verification of the surrogate models that possess a
high-dimensional input and output space, thus accommodating multi-objective
constraints. We also demonstrate the pipeline effectiveness in substantially
decreasing the runtime needed to assess the targeted property.
\\ ( https://arxiv.org/abs/2401.06821 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06826
Date: Fri, 12 Jan 2024 02:48:51 GMT   (755kb,D)

Title: Direct Distillation between Different Domains
Authors: Jialiang Tang, Shuo Chen, Gang Niu, Hongyuan Zhu, Joey Tianyi Zhou,
  Chen Gong, Masashi Sugiyama
Categories: cs.LG cs.AI cs.CV
\\
  Knowledge Distillation (KD) aims to learn a compact student network using
knowledge from a large pre-trained teacher network, where both networks are
trained on data from the same distribution. However, in practical applications,
the student network may be required to perform in a new scenario (i.e., the
target domain), which usually exhibits significant differences from the known
scenario of the teacher network (i.e., the source domain). The traditional
domain adaptation techniques can be integrated with KD in a two-stage process
to bridge the domain gap, but the ultimate reliability of two-stage approaches
tends to be limited due to the high computational consumption and the
additional errors accumulated from both stages. To solve this problem, we
propose a new one-stage method dubbed ``Direct Distillation between Different
Domains" (4Ds). We first design a learnable adapter based on the Fourier
transform to separate the domain-invariant knowledge from the domain-specific
knowledge. Then, we build a fusion-activation mechanism to transfer the
valuable domain-invariant knowledge to the student network, while
simultaneously encouraging the adapter within the teacher network to learn the
domain-specific knowledge of the target data. As a result, the teacher network
can effectively transfer categorical knowledge that aligns with the target
domain of the student network. Intensive experiments on various benchmark
datasets demonstrate that our proposed 4Ds method successfully produces
reliable student networks and outperforms state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2401.06826 ,  755kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06868
Date: Fri, 12 Jan 2024 19:46:29 GMT   (706kb,D)

Title: Multicriteria decision support employing adaptive prediction in a
  tensor-based feature representation
Authors: Betania Silva Carneiro Campello, Leonardo Tomazeli Duarte, Jo\~ao
  Marcos Travassos Romano
Categories: cs.LG cs.AI eess.SP
\\
  Multicriteria decision analysis (MCDA) is a widely used tool to support
decisions in which a set of alternatives should be ranked or classified based
on multiple criteria. Recent studies in MCDA have shown the relevance of
considering not only current evaluations of each criterion but also past data.
Past-data-based approaches carry new challenges, especially in time-varying
environments. This study deals with this challenge via essential tools of
signal processing, such as tensorial representations and adaptive prediction.
More specifically, we structure the criteria' past data as a tensor and, by
applying adaptive prediction, we compose signals with these prediction values
of the criteria. Besides, we transform the prediction in the time domain into a
most favorable decision making domain, called the feature domain. We present a
novel extension of the MCDA method PROMETHEE II, aimed at addressing the tensor
in the feature domain to obtain a ranking of alternatives. Numerical
experiments were performed using real-world time series, and our approach is
compared with other existing strategies. The results highlight the relevance
and efficiency of our proposal, especially for nonstationary time series.
\\ ( https://arxiv.org/abs/2401.06868 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06890
Date: Fri, 12 Jan 2024 20:53:35 GMT   (618kb,D)

Title: An Axiomatic Approach to Model-Agnostic Concept Explanations
Authors: Zhili Feng, Michal Moshkovitz, Dotan Di Castro, J. Zico Kolter
Categories: cs.LG
\\
  Concept explanation is a popular approach for examining how
human-interpretable concepts impact the predictions of a model. However, most
existing methods for concept explanations are tailored to specific models. To
address this issue, this paper focuses on model-agnostic measures.
Specifically, we propose an approach to concept explanations that satisfy three
natural axioms: linearity, recursivity, and similarity. We then establish
connections with previous concept explanation methods, offering insight into
their varying semantic meanings. Experimentally, we demonstrate the utility of
the new method by applying it in different scenarios: for model selection,
optimizer selection, and model improvement using a kind of prompt editing for
zero-shot vision language models.
\\ ( https://arxiv.org/abs/2401.06890 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06898
Date: Fri, 12 Jan 2024 21:32:04 GMT   (523kb,D)

Title: Always-Sparse Training by Growing Connections with Guided Stochastic
  Exploration
Authors: Mike Heddes, Narayan Srinivasa, Tony Givargis, Alexandru Nicolau
Categories: cs.LG
\\
  The excessive computational requirements of modern artificial neural networks
(ANNs) are posing limitations on the machines that can run them. Sparsification
of ANNs is often motivated by time, memory and energy savings only during model
inference, yielding no benefits during training. A growing body of work is now
focusing on providing the benefits of model sparsification also during
training. While these methods greatly improve the training efficiency, the
training algorithms yielding the most accurate models still materialize the
dense weights, or compute dense gradients during training. We propose an
efficient, always-sparse training algorithm with excellent scaling to larger
and sparser models, supported by its linear time complexity with respect to the
model width during training and inference. Moreover, our guided stochastic
exploration algorithm improves over the accuracy of previous sparse training
methods. We evaluate our method on CIFAR-10/100 and ImageNet using ResNet, VGG,
and ViT models, and compare it against a range of sparsification methods.
\\ ( https://arxiv.org/abs/2401.06898 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06899
Date: Fri, 12 Jan 2024 21:33:17 GMT   (53kb)

Title: Analyses and Concerns in Precision Medicine: A Statistical Perspective
Authors: Xiaofei Chen
Categories: cs.LG stat.AP
\\
  This article explores the critical role of statistical analysis in precision
medicine. It discusses how personalized healthcare is enhanced by statistical
methods that interpret complex, multidimensional datasets, focusing on
predictive modeling, machine learning algorithms, and data visualization
techniques. The paper addresses challenges in data integration and
interpretation, particularly with diverse data sources like electronic health
records (EHRs) and genomic data. It also delves into ethical considerations
such as patient privacy and data security. In addition, the paper highlights
the evolution of statistical analysis in medicine, core statistical
methodologies in precision medicine, and future directions in the field,
emphasizing the integration of artificial intelligence (AI) and machine
learning (ML).
\\ ( https://arxiv.org/abs/2401.06899 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06922
Date: Fri, 12 Jan 2024 22:43:07 GMT   (3363kb,D)

Title: Open RAN LSTM Traffic Prediction and Slice Management using Deep
  Reinforcement Learning
Authors: Fatemeh Lotfi, Fatemeh Afghah
Categories: cs.LG cs.AI cs.NI cs.SY eess.SY stat.ML
Comments: Accepted to publish in the IEEE Asilomar Conference on Signals,
  Systems, and Computers, 2023
\\
  With emerging applications such as autonomous driving, smart cities, and
smart factories, network slicing has become an essential component of 5G and
beyond networks as a means of catering to a service-aware network. However,
managing different network slices while maintaining quality of services (QoS)
is a challenge in a dynamic environment. To address this issue, this paper
leverages the heterogeneous experiences of distributed units (DUs) in ORAN
systems and introduces a novel approach to ORAN slicing xApp using distributed
deep reinforcement learning (DDRL). Additionally, to enhance the
decision-making performance of the RL agent, a prediction rApp based on long
short-term memory (LSTM) is incorporated to provide additional information from
the dynamic environment to the xApp. Simulation results demonstrate significant
improvements in network performance, particularly in reducing QoS violations.
This emphasizes the importance of using the prediction rApp and distributed
actors' information jointly as part of a dynamic xApp.
\\ ( https://arxiv.org/abs/2401.06922 ,  3363kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06923
Date: Fri, 12 Jan 2024 22:51:48 GMT   (1359kb,D)

Title: Minimally Supervised Learning using Topological Projections in
  Self-Organizing Maps
Authors: Zimeng Lyu, Alexander Ororbia, Rui Li, Travis Desell
Categories: cs.LG cs.NE
\\
  Parameter prediction is essential for many applications, facilitating
insightful interpretation and decision-making. However, in many real life
domains, such as power systems, medicine, and engineering, it can be very
expensive to acquire ground truth labels for certain datasets as they may
require extensive and expensive laboratory testing. In this work, we introduce
a semi-supervised learning approach based on topological projections in
self-organizing maps (SOMs), which significantly reduces the required number of
labeled data points to perform parameter prediction, effectively exploiting
information contained in large unlabeled datasets. Our proposed method first
trains SOMs on unlabeled data and then a minimal number of available labeled
data points are ultimately assigned to key best matching units (BMU). The
values estimated for newly-encountered data points are computed utilizing the
average of the $n$ closest labeled data points in the SOM's U-matrix in tandem
with a topological shortest path distance calculation scheme. Our results
indicate that the proposed semi-supervised model significantly outperforms
traditional regression techniques, including linear and polynomial regression,
Gaussian process regression, K-nearest neighbors, as well as various deep
neural network models.
\\ ( https://arxiv.org/abs/2401.06923 ,  1359kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06936
Date: Sat, 13 Jan 2024 00:11:02 GMT   (2078kb,D)

Title: Accelerated Sampling of Rare Events using a Neural Network Bias
  Potential
Authors: Xinru Hua, Rasool Ahmad, Jose Blanchet, Wei Cai
Categories: cs.LG physics.comp-ph
\\
  In the field of computational physics and material science, the efficient
sampling of rare events occurring at atomic scale is crucial. It aids in
understanding mechanisms behind a wide range of important phenomena, including
protein folding, conformal changes, chemical reactions and materials diffusion
and deformation. Traditional simulation methods, such as Molecular Dynamics and
Monte Carlo, often prove inefficient in capturing the timescale of these rare
events by brute force. In this paper, we introduce a practical approach by
combining the idea of importance sampling with deep neural networks (DNNs) that
enhance the sampling of these rare events. In particular, we approximate the
variance-free bias potential function with DNNs which is trained to maximize
the probability of rare event transition under the importance potential
function. This method is easily scalable to high-dimensional problems and
provides robust statistical guarantees on the accuracy of the estimated
probability of rare event transition. Furthermore, our algorithm can actively
generate and learn from any successful samples, which is a novel improvement
over existing methods. Using a 2D system as a test bed, we provide comparisons
between results obtained from different training strategies, traditional Monte
Carlo sampling and numerically solved optimal bias potential function under
different temperatures. Our numerical results demonstrate the efficacy of the
DNN-based importance sampling of rare events.
\\ ( https://arxiv.org/abs/2401.06936 ,  2078kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06952
Date: Sat, 13 Jan 2024 02:14:35 GMT   (593kb,D)

Title: Reinforcement Learning for Scalable Train Timetable Rescheduling with
  Graph Representation
Authors: Peng Yue, Yaochu Jin, Xuewu Dai, Zhenhua Feng, Dongliang Cui
Categories: cs.LG cs.AI
\\
  Train timetable rescheduling (TTR) aims to promptly restore the original
operation of trains after unexpected disturbances or disruptions. Currently,
this work is still done manually by train dispatchers, which is challenging to
maintain performance under various problem instances. To mitigate this issue,
this study proposes a reinforcement learning-based approach to TTR, which makes
the following contributions compared to existing work. First, we design a
simple directed graph to represent the TTR problem, enabling the automatic
extraction of informative states through graph neural networks. Second, we
reformulate the construction process of TTR's solution, not only decoupling the
decision model from the problem size but also ensuring the generated scheme's
feasibility. Third, we design a learning curriculum for our model to handle the
scenarios with different levels of delay. Finally, a simple local search method
is proposed to assist the learned decision model, which can significantly
improve solution quality with little additional computation cost, further
enhancing the practical value of our method. Extensive experimental results
demonstrate the effectiveness of our method. The learned decision model can
achieve better performance for various problems with varying degrees of train
delay and different scales when compared to handcrafted rules and
state-of-the-art solvers.
\\ ( https://arxiv.org/abs/2401.06952 ,  593kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06953
Date: Sat, 13 Jan 2024 02:15:41 GMT   (2118kb,D)

Title: FedDriveScore: Federated Scoring Driving Behavior with a Mixture of
  Metric Distributions
Authors: Lin Lu
Categories: cs.LG cs.DC
\\
  Scoring the driving performance of various drivers on a unified scale, based
on how safe or economical they drive on their daily trips, is essential for the
driver profile task. Connected vehicles provide the opportunity to collect
real-world driving data, which is advantageous for constructing scoring models.
However, the lack of pre-labeled scores impede the use of supervised regression
models and the data privacy issues hinder the way of traditionally
data-centralized learning on the cloud side for model training. To address
them, an unsupervised scoring method is presented without the need for labels
while still preserving fairness and objectiveness compared to subjective
scoring strategies. Subsequently, a federated learning framework based on
vehicle-cloud collaboration is proposed as a privacy-friendly alternative to
centralized learning. This framework includes a consistently federated version
of the scoring method to reduce the performance degradation of the global
scoring model caused by the statistical heterogeneous challenge of local data.
Theoretical and experimental analysis demonstrate that our federated scoring
model is consistent with the utility of the centrally learned counterpart and
is effective in evaluating driving performance.
\\ ( https://arxiv.org/abs/2401.06953 ,  2118kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06970
Date: Sat, 13 Jan 2024 03:53:47 GMT   (212kb,D)

Title: TemporalAugmenter: An Ensemble Recurrent Based Deep Learning Approach
  for Signal Classification
Authors: Nelly Elsayed, Constantinos L. Zekios, Navid Asadizanjani, Zag ElSayed
Categories: cs.LG cs.HC eess.SP
Comments: 9 pages, 5 figures, 9 tables, under review process
\\
  Ensemble modeling has been widely used to solve complex problems as it helps
to improve overall performance and generalization. In this paper, we propose a
novel TemporalAugmenter approach based on ensemble modeling for augmenting the
temporal information capturing for long-term and short-term dependencies in
data integration of two variations of recurrent neural networks in two learning
streams to obtain the maximum possible temporal extraction. Thus, the proposed
model augments the extraction of temporal dependencies. In addition, the
proposed approach reduces the preprocessing and prior stages of feature
extraction, which reduces the required energy to process the models built upon
the proposed TemporalAugmenter approach, contributing towards green AI.
Moreover, the proposed model can be simply integrated into various domains
including industrial, medical, and human-computer interaction applications. Our
proposed approach empirically evaluated the speech emotion recognition,
electrocardiogram signal, and signal quality examination tasks as three
different signals with varying complexity and different temporal dependency
features.
\\ ( https://arxiv.org/abs/2401.06970 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06989
Date: Sat, 13 Jan 2024 06:17:17 GMT   (5560kb,D)

Title: Gradient Coreset for Federated Learning
Authors: Durga Sivasubramanian, Lokesh Nagalapatti, Rishabh Iyer, Ganesh
  Ramakrishnan
Categories: cs.LG
Comments: Accepted at WACV-24
\\
  Federated Learning (FL) is used to learn machine learning models with data
that is partitioned across multiple clients, including resource-constrained
edge devices. It is therefore important to devise solutions that are efficient
in terms of compute, communication, and energy consumption, while ensuring
compliance with the FL framework's privacy requirements. Conventional
approaches to these problems select a weighted subset of the training dataset,
known as coreset, and learn by fitting models on it. Such coreset selection
approaches are also known to be robust to data noise. However, these approaches
rely on the overall statistics of the training data and are not easily
extendable to the FL setup.
  In this paper, we propose an algorithm called Gradient based Coreset for
Robust and Efficient Federated Learning (GCFL) that selects a coreset at each
client, only every $K$ communication rounds and derives updates only from it,
assuming the availability of a small validation dataset at the server. We
demonstrate that our coreset selection technique is highly effective in
accounting for noise in clients' data. We conduct experiments using four
real-world datasets and show that GCFL is (1) more compute and energy efficient
than FL, (2) robust to various kinds of noise in both the feature space and
labels, (3) preserves the privacy of the validation dataset, and (4) introduces
a small communication overhead but achieves significant gains in performance,
particularly in cases when the clients' data is noisy.
\\ ( https://arxiv.org/abs/2401.06989 ,  5560kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07012
Date: Sat, 13 Jan 2024 08:38:54 GMT   (872kb)

Title: An ADRC-Incorporated Stochastic Gradient Descent Algorithm for Latent
  Factor Analysis
Authors: Jinli Li and Ye Yuan
Categories: cs.LG cs.SY eess.SY stat.ML
\\
  High-dimensional and incomplete (HDI) matrix contains many complex
interactions between numerous nodes. A stochastic gradient descent (SGD)-based
latent factor analysis (LFA) model is remarkably effective in extracting
valuable information from an HDI matrix. However, such a model commonly
encounters the problem of slow convergence because a standard SGD algorithm
only considers the current learning error to compute the stochastic gradient
without considering the historical and future state of the learning error. To
address this critical issue, this paper innovatively proposes an
ADRC-incorporated SGD (ADS) algorithm by refining the instance learning error
by considering the historical and future state by following the principle of an
ADRC controller. With it, an ADS-based LFA model is further achieved for fast
and accurate latent factor analysis on an HDI matrix. Empirical studies on two
HDI datasets demonstrate that the proposed model outperforms the
state-of-the-art LFA models in terms of computational efficiency and accuracy
for predicting the missing data of an HDI matrix.
\\ ( https://arxiv.org/abs/2401.07012 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07022
Date: Sat, 13 Jan 2024 09:27:37 GMT   (13030kb,D)

Title: Edge-Enabled Anomaly Detection and Information Completion for Social
  Network Knowledge Graphs
Authors: Fan Lu, Quan Qi, Huaibin Qin
Categories: cs.LG cs.AI cs.CL
Comments: 20 pages, 6 figures, Has been accepted by Wireless Network
\\
  In the rapidly advancing information era, various human behaviors are being
precisely recorded in the form of data, including identity information,
criminal records, and communication data. Law enforcement agencies can
effectively maintain social security and precisely combat criminal activities
by analyzing the aforementioned data. In comparison to traditional data
analysis methods, deep learning models, relying on the robust computational
power in cloud centers, exhibit higher accuracy in extracting data features and
inferring data. However, within the architecture of cloud centers, the
transmission of data from end devices introduces significant latency, hindering
real-time inference of data. Furthermore, low-latency edge computing
architectures face limitations in direct deployment due to relatively weak
computing and storage capacities of nodes. To address these challenges, a
lightweight distributed knowledge graph completion architecture is proposed.
Firstly, we introduce a lightweight distributed knowledge graph completion
architecture that utilizes knowledge graph embedding for data analysis.
Subsequently, to filter out substandard data, a personnel data quality
assessment method named PDQA is proposed. Lastly, we present a model pruning
algorithm that significantly reduces the model size while maximizing
performance, enabling lightweight deployment. In experiments, we compare the
effects of 11 advanced models on completing the knowledge graph of public
security personnel information. The results indicate that the RotatE model
outperforms other models significantly in knowledge graph completion, with the
pruned model size reduced by 70\%, and hits@10 reaching 86.97\%.}
\\ ( https://arxiv.org/abs/2401.07022 ,  13030kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07044
Date: Sat, 13 Jan 2024 11:13:06 GMT   (13356kb,D)

Title: BP(\lambda): Online Learning via Synthetic Gradients
Authors: Joseph Pemberton and Rui Ponte Costa
Categories: cs.LG
Comments: 24 pages, 7 figures
MSC-class: 68T07
\\
  Training recurrent neural networks typically relies on backpropagation
through time (BPTT). BPTT depends on forward and backward passes to be
completed, rendering the network locked to these computations before loss
gradients are available. Recently, Jaderberg et al. proposed synthetic
gradients to alleviate the need for full BPTT. In their implementation
synthetic gradients are learned through a mixture of backpropagated gradients
and bootstrapped synthetic gradients, analogous to the temporal difference (TD)
algorithm in Reinforcement Learning (RL). However, as in TD learning, heavy use
of bootstrapping can result in bias which leads to poor synthetic gradient
estimates. Inspired by the accumulate $\mathrm{TD}(\lambda)$ in RL, we propose
a fully online method for learning synthetic gradients which avoids the use of
BPTT altogether: accumulate $BP(\lambda)$. As in accumulate
$\mathrm{TD}(\lambda)$, we show analytically that accumulate
$\mathrm{BP}(\lambda)$ can control the level of bias by using a mixture of
temporal difference errors and recursively defined eligibility traces. We next
demonstrate empirically that our model outperforms the original implementation
for learning synthetic gradients in a variety of tasks, and is particularly
suited for capturing longer timescales. Finally, building on recent work we
reflect on accumulate $\mathrm{BP}(\lambda)$ as a principle for learning in
biological circuits. In summary, inspired by RL principles we introduce an
algorithm capable of bias-free online learning via synthetic gradients.
\\ ( https://arxiv.org/abs/2401.07044 ,  13356kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07051
Date: Sat, 13 Jan 2024 11:43:25 GMT   (3610kb,D)

Title: COIN: Chance-Constrained Imitation Learning for Uncertainty-aware
  Adaptive Resource Oversubscription Policy
Authors: Lu Wang, Mayukh Das, Fangkai Yang, Chao Duo, Bo Qiao, Hang Dong, Si
  Qin, Chetan Bansal, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
Categories: cs.LG cs.AI
Comments: 9 pages, 4 figures
\\
  We address the challenge of learning safe and robust decision policies in
presence of uncertainty in context of the real scientific problem of adaptive
resource oversubscription to enhance resource efficiency while ensuring safety
against resource congestion risk.
  Traditional supervised prediction or forecasting models are ineffective in
learning adaptive policies whereas standard online optimization or
reinforcement learning is difficult to deploy on real systems. Offline methods
such as imitation learning (IL) are ideal since we can directly leverage
historical resource usage telemetry. But, the underlying aleatoric uncertainty
in such telemetry is a critical bottleneck.
  We solve this with our proposed novel chance-constrained imitation learning
framework, which ensures implicit safety against uncertainty in a principled
manner via a combination of stochastic (chance) constraints on resource
congestion risk and ensemble value functions. This leads to substantial
($\approx 3-4\times$) improvement in resource efficiency and safety in many
oversubscription scenarios, including resource management in cloud services.
\\ ( https://arxiv.org/abs/2401.07051 ,  3610kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07062
Date: Sat, 13 Jan 2024 12:33:04 GMT   (197kb,D)

Title: Dirichlet-Based Prediction Calibration for Learning with Noisy Labels
Authors: Chen-Chen Zong, Ye-Wen Wang, Ming-Kun Xie, Sheng-Jun Huang
Categories: cs.LG cs.AI
\\
  Learning with noisy labels can significantly hinder the generalization
performance of deep neural networks (DNNs). Existing approaches address this
issue through loss correction or example selection methods. However, these
methods often rely on the model's predictions obtained from the softmax
function, which can be over-confident and unreliable. In this study, we
identify the translation invariance of the softmax function as the underlying
cause of this problem and propose the \textit{Dirichlet-based Prediction
Calibration} (DPC) method as a solution. Our method introduces a calibrated
softmax function that breaks the translation invariance by incorporating a
suitable constant in the exponent term, enabling more reliable model
predictions. To ensure stable model training, we leverage a Dirichlet
distribution to assign probabilities to predicted labels and introduce a novel
evidence deep learning (EDL) loss. The proposed loss function encourages
positive and sufficiently large logits for the given label, while penalizing
negative and small logits for other labels, leading to more distinct logits and
facilitating better example selection based on a large-margin criterion.
Through extensive experiments on diverse benchmark datasets, we demonstrate
that DPC achieves state-of-the-art performance. The code is available at
https://github.com/chenchenzong/DPC.
\\ ( https://arxiv.org/abs/2401.07062 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07065
Date: Sat, 13 Jan 2024 12:49:56 GMT   (420kb)

Title: Tensor Graph Convolutional Network for Dynamic Graph Representation
  Learning
Authors: Ling Wang, Ye Yuan
Categories: cs.LG cs.AI
Comments: 6 pages, 3 figures
\\
  Dynamic graphs (DG) describe dynamic interactions between entities in many
practical scenarios. Most existing DG representation learning models combine
graph convolutional network and sequence neural network, which model
spatial-temporal dependencies through two different types of neural networks.
However, this hybrid design cannot well capture the spatial-temporal continuity
of a DG. In this paper, we propose a tensor graph convolutional network to
learn DG representations in one convolution framework based on the tensor
product with the following two-fold ideas: a) representing the information of
DG by tensor form; b) adopting tensor product to design a tensor graph
convolutional network modeling spatial-temporal feature simultaneously.
Experiments on real-world DG datasets demonstrate that our model obtains
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2401.07065 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07066
Date: Sat, 13 Jan 2024 12:56:04 GMT   (5635kb,D)

Title: Classification of Volatile Organic Compounds by Differential Mobility
  Spectrometry Based on Continuity of Alpha Curves
Authors: Anton Rauhameri, Angelo Robi\~nos, Osmo Anttalainen, Timo Salpavaara,
  Jussi Rantala, Veikko Surakka, Pasi Kallio, Antti Vehkaoja, Philipp M\"uller
Categories: cs.LG
\\
  Background: Classification of volatile organic compounds (VOCs) is of
interest in many fields. Examples include but are not limited to medicine,
detection of explosives, and food quality control. Measurements collected with
electronic noses can be used for classification and analysis of VOCs. One type
of electronic noses that has seen considerable development in recent years is
Differential Mobility Spectrometry (DMS). DMS yields measurements that are
visualized as dispersion plots that contain traces, also known as alpha curves.
Current methods used for analyzing DMS dispersion plots do not usually utilize
the information stored in the continuity of these traces, which suggests that
alternative approaches should be investigated.
  Results: In this work, for the first time, dispersion plots were interpreted
as a series of measurements evolving sequentially. Thus, it was hypothesized
that time-series classification algorithms can be effective for classification
and analysis of dispersion plots. An extensive dataset of 900 dispersion plots
for five chemicals measured at five flow rates and two concentrations was
collected. The data was used to analyze the classification performance of six
algorithms. According to our hypothesis, the highest classification accuracy of
88\% was achieved by a Long-Short Term Memory neural network, which supports
our hypothesis.
  Significance: A new concept for approaching classification tasks of
dispersion plots is presented and compared with other well-known classification
algorithms. This creates a new angle of view for analysis and classification of
the dispersion plots. In addition, a new dataset of dispersion plots is openly
shared to public.
\\ ( https://arxiv.org/abs/2401.07066 ,  5635kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07085
Date: Sat, 13 Jan 2024 14:21:46 GMT   (1259kb,D)

Title: When Does Feature Learning Happen? Perspective from an Analytically
  Solvable Model
Authors: Yizhou Xu and Liu Ziyin
Categories: cs.LG cs.AI
\\
  We identify and solve a hidden-layer model that is analytically tractable at
any finite width and whose limits exhibit both the kernel phase and the feature
learning phase. We analyze the phase diagram of this model in all possible
limits of common hyperparameters including width, layer-wise learning rates,
scale of output, and scale of initialization. We apply our result to analyze
how and when feature learning happens in both infinite and finite-width models.
Three prototype mechanisms of feature learning are identified: (1) learning by
alignment, (2) learning by disalignment, and (3) learning by rescaling. In
sharp contrast, neither of these mechanisms is present when the model is in the
kernel regime. This discovery explains why large initialization often leads to
worse performance. Lastly, we empirically demonstrate that discoveries we made
for this analytical model also appear in nonlinear networks in real tasks.
\\ ( https://arxiv.org/abs/2401.07085 ,  1259kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07091
Date: Sat, 13 Jan 2024 14:59:12 GMT   (487kb,D)

Title: Optimization of Inter-group Criteria for Clustering with Minimum Size
  Constraints
Authors: Eduardo S. Laber and Lucas Murtinho
Categories: cs.LG cs.DS
Comments: Presented at Neurips 2023. 20 pages, 5 figures
\\
  Internal measures that are used to assess the quality of a clustering usually
take into account intra-group and/or inter-group criteria. There are many
papers in the literature that propose algorithms with provable approximation
guarantees for optimizing the former. However, the optimization of inter-group
criteria is much less understood.
  Here, we contribute to the state-of-the-art of this literature by devising
algorithms with provable guarantees for the maximization of two natural
inter-group criteria, namely the minimum spacing and the minimum spanning tree
spacing. The former is the minimum distance between points in different groups
while the latter captures separability through the cost of the minimum spanning
tree that connects all groups. We obtain results for both the unrestricted
case, in which no constraint on the clusters is imposed, and for the
constrained case where each group is required to have a minimum number of
points. Our constraint is motivated by the fact that the popular Single
Linkage, which optimizes both criteria in the unrestricted case, produces
clusterings with many tiny groups.
  To complement our work, we present an empirical study with 10 real datasets,
providing evidence that our methods work very well in practical settings.
\\ ( https://arxiv.org/abs/2401.07091 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07145
Date: Sat, 13 Jan 2024 19:30:34 GMT   (18067kb)

Title: Scalable and Efficient Methods for Uncertainty Estimation and Reduction
  in Deep Learning
Authors: Soyed Tuhin Ahmed
Categories: cs.LG cs.AI
\\
  Neural networks (NNs) can achieved high performance in various fields such as
computer vision, and natural language processing. However, deploying NNs in
resource-constrained safety-critical systems has challenges due to uncertainty
in the prediction caused by out-of-distribution data, and hardware
non-idealities. To address the challenges of deploying NNs in
resource-constrained safety-critical systems, this paper summarizes the (4th
year) PhD thesis work that explores scalable and efficient methods for
uncertainty estimation and reduction in deep learning, with a focus on
Computation-in-Memory (CIM) using emerging resistive non-volatile memories. We
tackle the inherent uncertainties arising from out-of-distribution inputs and
hardware non-idealities, crucial in maintaining functional safety in automated
decision-making systems. Our approach encompasses problem-aware training
algorithms, novel NN topologies, and hardware co-design solutions, including
dropout-based \emph{binary} Bayesian Neural Networks leveraging spintronic
devices and variational inference techniques. These innovations significantly
enhance OOD data detection, inference accuracy, and energy efficiency, thereby
contributing to the reliability and robustness of NN implementations.
\\ ( https://arxiv.org/abs/2401.07145 ,  18067kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07159
Date: Sat, 13 Jan 2024 21:00:21 GMT   (498kb,D)

Title: Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized
  Large Language Models
Authors: Zhengxin Zhang, Dan Zhao, Xupeng Miao, Gabriele Oliaro, Qing Li, Yong
  Jiang, and Zhihao Jia
Categories: cs.LG cs.AI
ACM-class: I.2.7
\\
  Finetuning large language models (LLMs) has been empirically effective on a
variety of downstream tasks. Existing approaches to finetuning an LLM either
focus on parameter-efficient finetuning, which only updates a small number of
trainable parameters, or attempt to reduce the memory footprint during the
training phase of the finetuning. Typically, the memory footprint during
finetuning stems from three contributors: model weights, optimizer states, and
intermediate activations. However, existing works still require considerable
memory and none can simultaneously mitigate memory footprint for all three
sources. In this paper, we present Quantized Side Tuing (QST), which enables
memory-efficient and fast finetuning of LLMs by operating through a dual-stage
process. First, QST quantizes an LLM's model weights into 4-bit to reduce the
memory footprint of the LLM's original weights; QST also introduces a side
network separated from the LLM, which utilizes the hidden states of the LLM to
make task-specific predictions. Using a separate side network avoids performing
backpropagation through the LLM, thus reducing the memory requirement of the
intermediate activations. Furthermore, QST leverages several low-rank adaptors
and gradient-free downsample modules to significantly reduce the trainable
parameters, so as to save the memory footprint of the optimizer states.
Experiments show that QST can reduce the total memory footprint by up to 2.3
$\times$ and speed up the finetuning process by up to 3 $\times$ while
achieving competent performance compared with the state-of-the-art. When it
comes to full finetuning, QST can reduce the total memory footprint up to 7
$\times$.
\\ ( https://arxiv.org/abs/2401.07159 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07175
Date: Sat, 13 Jan 2024 23:51:42 GMT   (12286kb,D)

Title: Domain Adaptation for Sustainable Soil Management using Causal and
  Contrastive Constraint Minimization
Authors: Somya Sharma, Swati Sharma, Rafael Padilha, Emre Kiciman, Ranveer
  Chandra
Categories: cs.LG
Comments: Neurips workshop on Tackling Climate Change 2023
\\
  Monitoring organic matter is pivotal for maintaining soil health and can help
inform sustainable soil management practices. While sensor-based soil
information offers higher-fidelity and reliable insights into organic matter
changes, sampling and measuring sensor data is cost-prohibitive. We propose a
multi-modal, scalable framework that can estimate organic matter from remote
sensing data, a more readily available data source while leveraging sparse soil
information for improving generalization. Using the sensor data, we preserve
underlying causal relations among sensor attributes and organic matter.
Simultaneously we leverage inherent structure in the data and train the model
to discriminate among domains using contrastive learning. This causal and
contrastive constraint minimization ensures improved generalization and
adaptation to other domains. We also shed light on the interpretability of the
framework by identifying attributes that are important for improving
generalization. Identifying these key soil attributes that affect organic
matter will aid in efforts to standardize data collection efforts.
\\ ( https://arxiv.org/abs/2401.07175 ,  12286kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07181
Date: Sun, 14 Jan 2024 01:09:48 GMT   (3603kb,D)

Title: Reinforcement Learning from LLM Feedback to Counteract Goal
  Misgeneralization
Authors: Houda Nait El Barj, Theophile Sautory
Categories: cs.LG
\\
  We introduce a method to address goal misgeneralization in reinforcement
learning (RL), leveraging Large Language Model (LLM) feedback during training.
Goal misgeneralization, a type of robustness failure in RL occurs when an agent
retains its capabilities out-of-distribution yet pursues a proxy rather than
the intended one. Our approach utilizes LLMs to analyze an RL agent's policies
during training and identify potential failure scenarios. The RL agent is then
deployed in these scenarios, and a reward model is learnt through the LLM
preferences and feedback. This LLM-informed reward model is used to further
train the RL agent on the original dataset. We apply our method to a maze
navigation task, and show marked improvements in goal generalization,
especially in cases where true and proxy goals are somewhat distinguishable and
behavioral biases are pronounced. This study demonstrates how the LLM, despite
its lack of task proficiency, can efficiently supervise RL agents, providing
scalable oversight and valuable insights for enhancing goal-directed learning
in RL through the use of LLMs.
\\ ( https://arxiv.org/abs/2401.07181 ,  3603kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07231
Date: Sun, 14 Jan 2024 08:32:32 GMT   (226kb,D)

Title: Use of Prior Knowledge to Discover Causal Additive Models with
  Unobserved Variables and its Application to Time Series Data
Authors: Takashi Nicholas Maeda, Shimizu Shohei
Categories: cs.LG stat.ME stat.ML
\\
  This paper proposes two methods for causal additive models with unobserved
variables (CAM-UV). CAM-UV assumes that the causal functions take the form of
generalized additive models and that latent confounders are present. First, we
propose a method that leverages prior knowledge for efficient causal discovery.
Then, we propose an extension of this method for inferring causality in time
series data. The original CAM-UV algorithm differs from other existing causal
function models in that it does not seek the causal order between observed
variables, but rather aims to identify the causes for each observed variable.
Therefore, the first proposed method in this paper utilizes prior knowledge,
such as understanding that certain variables cannot be causes of specific
others. Moreover, by incorporating the prior knowledge that causes precedes
their effects in time, we extend the first algorithm to the second method for
causal discovery in time series data. We validate the first proposed method by
using simulated data to demonstrate that the accuracy of causal discovery
increases as more prior knowledge is accumulated. Additionally, we test the
second proposed method by comparing it with existing time series causal
discovery methods, using both simulated data and real-world data.
\\ ( https://arxiv.org/abs/2401.07231 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07234
Date: Sun, 14 Jan 2024 09:15:10 GMT   (2683kb,D)

Title: The Effects of Data Imbalance Under a Federated Learning Approach for
  Credit Risk Forecasting
Authors: Shuyao Zhang, Jordan Tay, Pedro Baiz
Categories: cs.LG cs.AI
\\
  Credit risk forecasting plays a crucial role for commercial banks and other
financial institutions in granting loans to customers and minimise the
potential loss. However, traditional machine learning methods require the
sharing of sensitive client information with an external server to build a
global model, potentially posing a risk of security threats and privacy
leakage. A newly developed privacy-preserving distributed machine learning
technique known as Federated Learning (FL) allows the training of a global
model without the necessity of accessing private local data directly. This
investigation examined the feasibility of federated learning in credit risk
assessment and showed the effects of data imbalance on model performance. Two
neural network architectures, Multilayer Perceptron (MLP) and Long Short-Term
Memory (LSTM), and one tree ensemble architecture, Extreme Gradient Boosting
(XGBoost), were explored across three different datasets under various
scenarios involving different numbers of clients and data distribution
configurations. We demonstrate that federated models consistently outperform
local models on non-dominant clients with smaller datasets. This trend is
especially pronounced in highly imbalanced data scenarios, yielding a
remarkable average improvement of 17.92% in model performance. However, for
dominant clients (clients with more data), federated models may not exhibit
superior performance, suggesting the need for special incentives for this type
of clients to encourage their participation.
\\ ( https://arxiv.org/abs/2401.07234 ,  2683kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07249
Date: Sun, 14 Jan 2024 10:51:30 GMT   (1547kb,D)

Title: Imputation with Inter-Series Information from Prototypes for Irregular
  Sampled Time Series
Authors: Zhihao Yu, Xu Chu, Liantao Ma, Yasha Wang, Wenwu Zhu
Categories: cs.LG
\\
  Irregularly sampled time series are ubiquitous, presenting significant
challenges for analysis due to missing values. Despite existing methods address
imputation, they predominantly focus on leveraging intra-series information,
neglecting the potential benefits that inter-series information could provide,
such as reducing uncertainty and memorization effect. To bridge this gap, we
propose PRIME, a Prototype Recurrent Imputation ModEl, which integrates both
intra-series and inter-series information for imputing missing values in
irregularly sampled time series. Our framework comprises a prototype memory
module for learning inter-series information, a bidirectional gated recurrent
unit utilizing prototype information for imputation, and an attentive
prototypical refinement module for adjusting imputations. We conducted
extensive experiments on three datasets, and the results underscore PRIME's
superiority over the state-of-the-art models by up to 26% relative improvement
on mean square error.
\\ ( https://arxiv.org/abs/2401.07249 ,  1547kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07250
Date: Sun, 14 Jan 2024 10:53:36 GMT   (7920kb,D)

Title: Stabilizing Sharpness-aware Minimization Through A Simple
  Renormalization Strategy
Authors: Chengli Tan, Jiangshe Zhang, Junmin Liu, Yicheng Wang, Yunda Hao
Categories: cs.LG cs.AI
Comments: 31 pages
\\
  Recently, sharpness-aware minimization (SAM) has attracted a lot of attention
because of its surprising effectiveness in improving generalization
performance.However, training neural networks with SAM can be highly unstable
since the loss does not decrease along the direction of the exact gradient at
the current point, but instead follows the direction of a surrogate gradient
evaluated at another point nearby. To address this issue, we propose a simple
renormalization strategy, dubbed StableSAM, so that the norm of the surrogate
gradient maintains the same as that of the exact gradient. Our strategy is easy
to implement and flexible enough to integrate with SAM and its variants, almost
at no computational cost. With elementary tools from convex optimization and
learning theory, we also conduct a theoretical analysis of sharpness-aware
training, revealing that compared to stochastic gradient descent (SGD), the
effectiveness of SAM is only assured in a limited regime of learning rate. In
contrast, we show how StableSAM extends this regime of learning rate and when
it can consistently perform better than SAM with minor modification. Finally,
we demonstrate the improved performance of StableSAM on several representative
data sets and tasks.
\\ ( https://arxiv.org/abs/2401.07250 ,  7920kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07263
Date: Sun, 14 Jan 2024 11:45:05 GMT   (7972kb,D)

Title: BET: Explaining Deep Reinforcement Learning through The Error-Prone
  Decisions
Authors: Xiao Liu, Jie Zhao, Wubing Chen, Mao Tan, Yongxing Su
Categories: cs.LG cs.AI
Comments: This is an early version of a paper that submitted to IJCAI 2024 8
  pages, 4 figures and 1 table
\\
  Despite the impressive capabilities of Deep Reinforcement Learning (DRL)
agents in many challenging scenarios, their black-box decision-making process
significantly limits their deployment in safety-sensitive domains. Several
previous self-interpretable works focus on revealing the critical states of the
agent's decision. However, they cannot pinpoint the error-prone states. To
address this issue, we propose a novel self-interpretable structure, named
Backbone Extract Tree (BET), to better explain the agent's behavior by identify
the error-prone states. At a high level, BET hypothesizes that states in which
the agent consistently executes uniform decisions exhibit a reduced propensity
for errors. To effectively model this phenomenon, BET expresses these states
within neighborhoods, each defined by a curated set of representative states.
Therefore, states positioned at a greater distance from these representative
benchmarks are more prone to error. We evaluate BET in various popular RL
environments and show its superiority over existing self-interpretable models
in terms of explanation fidelity. Furthermore, we demonstrate a use case for
providing explanations for the agents in StarCraft II, a sophisticated
multi-agent cooperative game. To the best of our knowledge, we are the first to
explain such a complex scenarios using a fully transparent structure.
\\ ( https://arxiv.org/abs/2401.07263 ,  7972kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07364
Date: Sun, 14 Jan 2024 20:41:36 GMT   (426kb,D)

Title: PDE Generalization of In-Context Operator Networks: A Study on 1D Scalar
  Nonlinear Conservation Laws
Authors: Liu Yang, Stanley J. Osher
Categories: cs.LG cs.AI cs.NA math.NA
\\
  Can we build a single large model for a wide range of PDE-related scientific
learning tasks? Can this model generalize to new PDEs, even of new forms,
without any fine-tuning? In-context operator learning and the corresponding
model In-Context Operator Networks (ICON) [1] represent an initial exploration
of these questions. The capability of ICON regarding the first question has
been demonstrated in [1]. In this paper, we explore the second question by
investigating the generalization capabilities of ICON for conservation laws, a
family of PDEs with temporal evolution. We show the positive answer to the
second question, i.e., ICON can generalize well to some PDEs with new forms
without any fine-tuning. We also show how to broaden the range of problems that
ICON can address, by transforming functions and equations to ICON's capability
scope. We believe that the progress in this paper is a significant step towards
the goal of training a foundation model for PDE-related tasks under the
in-context operator learning framework.
\\ ( https://arxiv.org/abs/2401.07364 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07369
Date: Sun, 14 Jan 2024 21:10:59 GMT   (4368kb,D)

Title: CoVO-MPC: Theoretical Analysis of Sampling-based MPC and Optimal
  Covariance Design
Authors: Zeji Yi, Chaoyi Pan, Guanqi He, Guannan Qu, Guanya Shi
Categories: cs.LG cs.RO
Comments: 32 pages, 4 figures
\\
  Sampling-based Model Predictive Control (MPC) has been a practical and
effective approach in many domains, notably model-based reinforcement learning,
thanks to its flexibility and parallelizability. Despite its appealing
empirical performance, the theoretical understanding, particularly in terms of
convergence analysis and hyperparameter tuning, remains absent. In this paper,
we characterize the convergence property of a widely used sampling-based MPC
method, Model Predictive Path Integral Control (MPPI). We show that MPPI enjoys
at least linear convergence rates when the optimization is quadratic, which
covers time-varying LQR systems. We then extend to more general nonlinear
systems. Our theoretical analysis directly leads to a novel sampling-based MPC
algorithm, CoVariance-Optimal MPC (CoVo-MPC) that optimally schedules the
sampling covariance to optimize the convergence rate. Empirically, CoVo-MPC
significantly outperforms standard MPPI by 43-54% in both simulations and
real-world quadrotor agile control tasks. Videos and Appendices are available
at \url{https://lecar-lab.github.io/CoVO-MPC/}.
\\ ( https://arxiv.org/abs/2401.07369 ,  4368kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07387
Date: Sun, 14 Jan 2024 22:46:53 GMT   (30596kb,D)

Title: Optimising network interactions through device agnostic models
Authors: Luca Manneschi, Ian T. Vidamour, Kilian D. Stenning, Jack C. Gartside,
  Charles Swindells, Guru Venkat, David Griffin, Susan Stepney, Will R.
  Branford, Thomas Hayward, Matt O Ellis, Eleni Vasilaki
Categories: cs.LG cs.AI cs.ET cs.NE
\\
  Physically implemented neural networks hold the potential to achieve the
performance of deep learning models by exploiting the innate physical
properties of devices as computational tools. This exploration of physical
processes for computation requires to also consider their intrinsic dynamics,
which can serve as valuable resources to process information. However, existing
computational methods are unable to extend the success of deep learning
techniques to parameters influencing device dynamics, which often lack a
precise mathematical description. In this work, we formulate a universal
framework to optimise interactions with dynamic physical systems in a fully
data-driven fashion. The framework adopts neural stochastic differential
equations as differentiable digital twins, effectively capturing both
deterministic and stochastic behaviours of devices. Employing differentiation
through the trained models provides the essential mathematical estimates for
optimizing a physical neural network, harnessing the intrinsic temporal
computation abilities of its physical nodes. To accurately model real devices'
behaviours, we formulated neural-SDE variants that can operate under a variety
of experimental settings. Our work demonstrates the framework's applicability
through simulations and physical implementations of interacting dynamic
devices, while highlighting the importance of accurately capturing system
stochasticity for the successful deployment of a physically defined neural
network.
\\ ( https://arxiv.org/abs/2401.07387 ,  30596kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07389
Date: Sun, 14 Jan 2024 23:19:53 GMT   (2531kb,D)

Title: A Rapid Review of Clustering Algorithms
Authors: Hui Yin, Amir Aryani, Stephen Petrie, Aishwarya Nambissan, Aland
  Astudillo, Shengyuan Cao
Categories: cs.LG cs.AI
Comments: 25 pages, 7 figures, 3 tables
MSC-class: 68-02
ACM-class: I.2.0
\\
  Clustering algorithms aim to organize data into groups or clusters based on
the inherent patterns and similarities within the data. They play an important
role in today's life, such as in marketing and e-commerce, healthcare, data
organization and analysis, and social media. Numerous clustering algorithms
exist, with ongoing developments introducing new ones. Each algorithm possesses
its own set of strengths and weaknesses, and as of now, there is no universally
applicable algorithm for all tasks. In this work, we analyzed existing
clustering algorithms and classify mainstream algorithms across five different
dimensions: underlying principles and characteristics, data point assignment to
clusters, dataset capacity, predefined cluster numbers and application area.
This classification facilitates researchers in understanding clustering
algorithms from various perspectives and helps them identify algorithms
suitable for solving specific tasks. Finally, we discussed the current trends
and potential future directions in clustering algorithms. We also identified
and discussed open challenges and unresolved issues in the field.
\\ ( https://arxiv.org/abs/2401.07389 ,  2531kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07390
Date: Sun, 14 Jan 2024 23:25:44 GMT   (276kb,D)

Title: Knee or ROC
Authors: Veronica Wendt, Byunggu Yu, Caleb Kelly, and Junwhan Kim
Categories: cs.LG cs.CV
Comments: 9 pages
\\
  Self-attention transformers have demonstrated accuracy for image
classification with smaller data sets. However, a limitation is that tests
to-date are based upon single class image detection with known representation
of image populations. For instances where the input image classes may be
greater than one and test sets that lack full information on representation of
image populations, accuracy calculations must adapt. The Receiver Operating
Characteristic (ROC) accuracy thresh-old can address the instances of
multi-class input images. However, this approach is unsuitable in instances
where image population representation is unknown. We consider calculating
accuracy using the knee method to determine threshold values on an ad-hoc
basis. Results of ROC curve and knee thresholds for a multi-class data set,
created from CIFAR-10 images, are discussed for multi-class image detection.
\\ ( https://arxiv.org/abs/2401.07390 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07395
Date: Mon, 15 Jan 2024 00:06:24 GMT   (8598kb,D)

Title: Harnessing the Power of Beta Scoring in Deep Active Learning for
  Multi-Label Text Classification
Authors: Wei Tan, Ngoc Dang Nguyen, Lan Du, Wray Buntine
Categories: cs.LG cs.AI
Comments: 7 pages AAAI 2024
\\
  Within the scope of natural language processing, the domain of multi-label
text classification is uniquely challenging due to its expansive and uneven
label distribution. The complexity deepens due to the demand for an extensive
set of annotated data for training an advanced deep learning model, especially
in specialized fields where the labeling task can be labor-intensive and often
requires domain-specific knowledge. Addressing these challenges, our study
introduces a novel deep active learning strategy, capitalizing on the Beta
family of proper scoring rules within the Expected Loss Reduction framework. It
computes the expected increase in scores using the Beta Scoring Rules, which
are then transformed into sample vector representations. These vector
representations guide the diverse selection of informative samples, directly
linking this process to the model's expected proper score. Comprehensive
evaluations across both synthetic and real datasets reveal our method's
capability to often outperform established acquisition techniques in
multi-label text classification, presenting encouraging outcomes across various
architectural and dataset scenarios.
\\ ( https://arxiv.org/abs/2401.07395 ,  8598kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07468
Date: Mon, 15 Jan 2024 04:51:34 GMT   (2440kb,D)

Title: CarSpeedNet: A Deep Neural Network-based Car Speed Estimation from
  Smartphone Accelerometer
Authors: Barak Or
Categories: cs.LG cs.AI
Comments: 8 pages
\\
  In this study, a novel deep neural network (DNN) architecture, CarSpeedNet,
is introduced to estimate car speed using three-axis accelerometer data from
smartphones. Utilizing 13 hours of data collected from smartphones mounted in
vehicles navigating through various regions in Israel, the CarSpeedNet
effectively learns the relationship between measured smartphone acceleration
and car speed. Ground truth speed data was obtained at 1[Hz] from the GPS
receiver in the smartphones. The proposed model enables high-frequency speed
estimation, incorporating historical inputs. Our trained model demonstrates
exceptional accuracy in car speed estimation, achieving a precision of less
than 0.72[m/s] during an extended driving test, solely relying on smartphone
accelerometer data without any connectivity to the car.
\\ ( https://arxiv.org/abs/2401.07468 ,  2440kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07482
Date: Mon, 15 Jan 2024 05:32:35 GMT   (22399kb)

Title: A Contrast Based Feature Selection Algorithm for High-dimensional Data
  set in Machine Learning
Authors: Chunxu Cao, Qiang Zhang
Categories: cs.LG
\\
  Feature selection is an important process in machine learning and knowledge
discovery. By selecting the most informative features and eliminating
irrelevant ones, the performance of learning algorithms can be improved and the
extraction of meaningful patterns and insights from data can be facilitated.
However, most existing feature selection methods, when applied to large
datasets, encountered the bottleneck of high computation costs. To address this
problem, we propose a novel filter feature selection method, ContrastFS, which
selects discriminative features based on the discrepancies features shown
between different classes. We introduce a dimensionless quantity as a surrogate
representation to summarize the distributional individuality of certain
classes, based on this quantity we evaluate features and study the correlation
among them. We validate effectiveness and efficiency of our approach on several
widely studied benchmark datasets, results show that the new method performs
favorably with negligible computation in comparison with other state-of-the-art
feature selection methods.
\\ ( https://arxiv.org/abs/2401.07482 ,  22399kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07488
Date: Mon, 15 Jan 2024 06:10:10 GMT   (1211kb)

Title: Feature Selection via Maximizing Distances between Class Conditional
  Distributions
Authors: Chunxu Cao, Qiang Zhang
Categories: cs.LG
\\
  For many data-intensive tasks, feature selection is an important
preprocessing step. However, most existing methods do not directly and
intuitively explore the intrinsic discriminative information of features. We
propose a novel feature selection framework based on the distance between class
conditional distributions, measured by integral probability metrics (IPMs). Our
framework directly explores the discriminative information of features in the
sense of distributions for supervised classification. We analyze the
theoretical and practical aspects of IPMs for feature selection, construct
criteria based on IPMs. We propose several variant feature selection methods of
our framework based on the 1-Wasserstein distance and implement them on real
datasets from different domains. Experimental results show that our framework
can outperform state-of-the-art methods in terms of classification accuracy and
robustness to perturbations.
\\ ( https://arxiv.org/abs/2401.07488 ,  1211kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07494
Date: Mon, 15 Jan 2024 06:26:53 GMT   (11427kb,D)

Title: Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks
Authors: Zihao Wang, P S Pravin, Zhe Wu
Categories: cs.LG cs.CE cs.SY eess.SY
\\
  Computational efficiency and adversarial robustness are critical factors in
real-world engineering applications. Yet, conventional neural networks often
fall short in addressing both simultaneously, or even separately. Drawing
insights from natural physical systems and existing literature, it is known
that an input convex architecture enhances computational efficiency, while a
Lipschitz-constrained architecture bolsters adversarial robustness. By
leveraging the strengths of convexity and Lipschitz continuity, we develop a
novel network architecture, termed Input Convex Lipschitz Recurrent Neural
Network. This model outperforms existing recurrent units across a spectrum of
engineering tasks in terms of computational efficiency and adversarial
robustness. These tasks encompass a benchmark MNIST image classification,
real-world solar irradiance prediction for Solar PV system planning at LHT
Holdings in Singapore, and real-time Model Predictive Control optimization for
a chemical reactor.
\\ ( https://arxiv.org/abs/2401.07494 ,  11427kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07516
Date: Mon, 15 Jan 2024 07:35:29 GMT   (325kb,D)

Title: Temporal Link Prediction Using Graph Embedding Dynamics
Authors: Sanaz Hasanzadeh Fard, Mohammad Ghassemi
Categories: cs.LG cs.SI
\\
  Graphs are a powerful representation tool in machine learning applications,
with link prediction being a key task in graph learning. Temporal link
prediction in dynamic networks is of particular interest due to its potential
for solving complex scientific and real-world problems. Traditional approaches
to temporal link prediction have focused on finding the aggregation of dynamics
of the network as a unified output. In this study, we propose a novel
perspective on temporal link prediction by defining nodes as Newtonian objects
and incorporating the concept of velocity to predict network dynamics. By
computing more specific dynamics of each node, rather than overall dynamics, we
improve both accuracy and explainability in predicting future connections. We
demonstrate the effectiveness of our approach using two datasets, including 17
years of co-authorship data from PubMed. Experimental results show that our
temporal graph embedding dynamics approach improves downstream classification
models' ability to predict future collaboration efficacy in co-authorship
networks by 17.34% (AUROC improvement relative to the baseline model).
Furthermore, our approach offers an interpretable layer over traditional
approaches to address the temporal link prediction problem.
\\ ( https://arxiv.org/abs/2401.07516 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07540
Date: Mon, 15 Jan 2024 09:01:31 GMT   (1637kb,D)

Title: Study Features via Exploring Distribution Structure
Authors: Chunxu Cao, Qiang Zhang
Categories: cs.LG
\\
  In this paper, we present a novel framework for data redundancy measurement
based on probabilistic modeling of datasets, and a new criterion for redundancy
detection that is resilient to noise. We also develop new methods for data
redundancy reduction using both deterministic and stochastic optimization
techniques. Our framework is flexible and can handle different types of
features, and our experiments on benchmark datasets demonstrate the
effectiveness of our methods. We provide a new perspective on feature
selection, and propose effective and robust approaches for both supervised and
unsupervised learning problems.
\\ ( https://arxiv.org/abs/2401.07540 ,  1637kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07551
Date: Mon, 15 Jan 2024 09:27:46 GMT   (1071kb,D)

Title: Robust Semi-Supervised Learning for Self-learning Open-World Classes
Authors: Wenjuan Xi, Xin Song, Weili Guo, Yang Yang
Categories: cs.LG
Journal-ref: 2023 IEEE International Conference on Data Mining (ICDM)
DOI: 10.1109/ICDM58522.2023.00075
\\
  Existing semi-supervised learning (SSL) methods assume that labeled and
unlabeled data share the same class space. However, in real-world applications,
unlabeled data always contain classes not present in the labeled set, which may
cause classification performance degradation of known classes. Therefore,
open-world SSL approaches are researched to handle the presence of multiple
unknown classes in the unlabeled data, which aims to accurately classify known
classes while fine-grained distinguishing different unknown classes. To address
this challenge, in this paper, we propose an open-world SSL method for
Self-learning Open-world Classes (SSOC), which can explicitly self-learn
multiple unknown classes. Specifically, SSOC first defines class center tokens
for both known and unknown classes and autonomously learns token
representations according to all samples with the cross-attention mechanism. To
effectively discover novel classes, SSOC further designs a pairwise similarity
loss in addition to the entropy loss, which can wisely exploit the information
available in unlabeled data from instances' predictions and relationships.
Extensive experiments demonstrate that SSOC outperforms the state-of-the-art
baselines on multiple popular classification benchmarks. Specifically, on the
ImageNet-100 dataset with a novel ratio of 90%, SSOC achieves a remarkable 22%
improvement.
\\ ( https://arxiv.org/abs/2401.07551 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07553
Date: Mon, 15 Jan 2024 09:37:03 GMT   (3770kb,D)

Title: Safe Reinforcement Learning with Free-form Natural Language Constraints
  and Pre-Trained Language Models
Authors: Xingzhou Lou, Junge Zhang, Ziyan Wang, Kaiqi Huang, Yali Du
Categories: cs.LG cs.CL
\\
  Safe reinforcement learning (RL) agents accomplish given tasks while adhering
to specific constraints. Employing constraints expressed via
easily-understandable human language offers considerable potential for
real-world applications due to its accessibility and non-reliance on domain
expertise. Previous safe RL methods with natural language constraints typically
adopt a recurrent neural network, which leads to limited capabilities when
dealing with various forms of human language input. Furthermore, these methods
often require a ground-truth cost function, necessitating domain expertise for
the conversion of language constraints into a well-defined cost function that
determines constraint violation. To address these issues, we proposes to use
pre-trained language models (LM) to facilitate RL agents' comprehension of
natural language constraints and allow them to infer costs for safe policy
learning. Through the use of pre-trained LMs and the elimination of the need
for a ground-truth cost, our method enhances safe policy learning under a
diverse set of human-derived free-form natural language constraints.
Experiments on grid-world navigation and robot control show that the proposed
method can achieve strong performance while adhering to given constraints. The
usage of pre-trained LMs allows our method to comprehend complicated
constraints and learn safe policies without the need for ground-truth cost at
any stage of training or evaluation. Extensive ablation studies are conducted
to demonstrate the efficacy of each part of our method.
\\ ( https://arxiv.org/abs/2401.07553 ,  3770kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07578
Date: Mon, 15 Jan 2024 10:26:13 GMT   (547kb,D)

Title: Confounded Budgeted Causal Bandits
Authors: Fateme Jamshidi, Jalal Etesami, Negar Kiyavash
Categories: cs.LG
\\
  We study the problem of learning 'good' interventions in a stochastic
environment modeled by its underlying causal graph. Good interventions refer to
interventions that maximize rewards. Specifically, we consider the setting of a
pre-specified budget constraint, where interventions can have non-uniform
costs. We show that this problem can be formulated as maximizing the expected
reward for a stochastic multi-armed bandit with side information. We propose an
algorithm to minimize the cumulative regret in general causal graphs. This
algorithm trades off observations and interventions based on their costs to
achieve the optimal reward. This algorithm generalizes the state-of-the-art
methods by allowing non-uniform costs and hidden confounders in the causal
graph. Furthermore, we develop an algorithm to minimize the simple regret in
the budgeted setting with non-uniform costs and also general causal graphs. We
provide theoretical guarantees, including both upper and lower bounds, as well
as empirical evaluations of our algorithms. Our empirical results showcase that
our algorithms outperform the state of the art.
\\ ( https://arxiv.org/abs/2401.07578 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07595
Date: Mon, 15 Jan 2024 11:04:47 GMT   (1497kb,D)

Title: E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy
Authors: Oliver T. Unke and Hartmut Maennel
Categories: cs.LG cs.AI physics.chem-ph
\\
  This work introduces E3x, a software package for building neural networks
that are equivariant with respect to the Euclidean group $\mathrm{E}(3)$,
consisting of translations, rotations, and reflections of three-dimensional
space. Compared to ordinary neural networks, $\mathrm{E}(3)$-equivariant models
promise benefits whenever input and/or output data are quantities associated
with three-dimensional objects. This is because the numeric values of such
quantities (e.g. positions) typically depend on the chosen coordinate system.
Under transformations of the reference frame, the values change predictably,
but the underlying rules can be difficult to learn for ordinary machine
learning models. With built-in $\mathrm{E}(3)$-equivariance, neural networks
are guaranteed to satisfy the relevant transformation rules exactly, resulting
in superior data efficiency and accuracy. The code for E3x is available from
https://github.com/google-research/e3x.
\\ ( https://arxiv.org/abs/2401.07595 ,  1497kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07606
Date: Mon, 15 Jan 2024 11:30:11 GMT   (37kb)

Title: RedEx: Beyond Fixed Representation Methods via Convex Optimization
Authors: Amit Daniely, Mariano Schain and Gilad Yehudai
Categories: cs.LG math.OC stat.ML
\\
  Optimizing Neural networks is a difficult task which is still not well
understood. On the other hand, fixed representation methods such as kernels and
random features have provable optimization guarantees but inferior performance
due to their inherent inability to learn the representations. In this paper, we
aim at bridging this gap by presenting a novel architecture called RedEx
(Reduced Expander Extractor) that is as expressive as neural networks and can
also be trained in a layer-wise fashion via a convex program with semi-definite
constraints and optimization guarantees. We also show that RedEx provably
surpasses fixed representation methods, in the sense that it can efficiently
learn a family of target functions which fixed representation methods cannot.
\\ ( https://arxiv.org/abs/2401.07606 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07639
Date: Mon, 15 Jan 2024 12:32:07 GMT   (7174kb,D)

Title: Compute-Efficient Active Learning
Authors: G\'abor N\'emeth, Tam\'as Matuszka
Categories: cs.LG
Comments: Accepted at NeurIPS 2023 Workshop on Adaptive Experimental Design and
  Active Learning in the Real World
\\
  Active learning, a powerful paradigm in machine learning, aims at reducing
labeling costs by selecting the most informative samples from an unlabeled
dataset. However, the traditional active learning process often demands
extensive computational resources, hindering scalability and efficiency. In
this paper, we address this critical issue by presenting a novel method
designed to alleviate the computational burden associated with active learning
on massive datasets. To achieve this goal, we introduce a simple, yet effective
method-agnostic framework that outlines how to strategically choose and
annotate data points, optimizing the process for efficiency while maintaining
model performance. Through case studies, we demonstrate the effectiveness of
our proposed method in reducing computational costs while maintaining or, in
some cases, even surpassing baseline model outcomes. Code is available at
https://github.com/aimotive/Compute-Efficient-Active-Learning.
\\ ( https://arxiv.org/abs/2401.07639 ,  7174kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07657
Date: Mon, 15 Jan 2024 12:53:58 GMT   (244kb,D)

Title: Empirical Evidence for the Fragment level Understanding on Drug
  Molecular Structure of LLMs
Authors: Xiuyuan Hu, Guoqing Liu, Yang Zhao, Hao Zhang
Categories: cs.LG cs.CE q-bio.BM
Comments: Accepted by AAAI 2024 workshop: Large Language Models for Biological
  Discoveries (LLMs4Bio)
\\
  AI for drug discovery has been a research hotspot in recent years, and
SMILES-based language models has been increasingly applied in drug molecular
design. However, no work has explored whether and how language models
understand the chemical spatial structure from 1D sequences. In this work, we
pre-train a transformer model on chemical language and fine-tune it toward drug
design objectives, and investigate the correspondence between high-frequency
SMILES substrings and molecular fragments. The results indicate that language
models can understand chemical structures from the perspective of molecular
fragments, and the structural knowledge learned through fine-tuning is
reflected in the high-frequency SMILES substrings generated by the model.
\\ ( https://arxiv.org/abs/2401.07657 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07697
Date: Mon, 15 Jan 2024 14:14:16 GMT   (306kb,D)

Title: Data vs. Model Machine Learning Fairness Testing: An Empirical Study
Authors: Arumoy Shome and Luis Cruz and Arie van Deursen
Categories: cs.LG cs.CY cs.SE
\\
  Although several fairness definitions and bias mitigation techniques exist in
the literature, all existing solutions evaluate fairness of Machine Learning
(ML) systems after the training stage. In this paper, we take the first steps
towards evaluating a more holistic approach by testing for fairness both before
and after model training. We evaluate the effectiveness of the proposed
approach and position it within the ML development lifecycle, using an
empirical analysis of the relationship between model dependent and independent
fairness metrics. The study uses 2 fairness metrics, 4 ML algorithms, 5
real-world datasets and 1600 fairness evaluation cycles. We find a linear
relationship between data and model fairness metrics when the distribution and
the size of the training data changes. Our results indicate that testing for
fairness prior to training can be a ``cheap'' and effective means of catching a
biased data collection process early; detecting data drifts in production
systems and minimising execution of full training cycles thus reducing
development time and costs.
\\ ( https://arxiv.org/abs/2401.07697 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07711
Date: Mon, 15 Jan 2024 14:27:03 GMT   (92kb,D)

Title: Efficient Nonparametric Tensor Decomposition for Binary and Count Data
Authors: Zerui Tao, Toshihisa Tanaka, Qibin Zhao
Categories: cs.LG stat.ML
Comments: AAAI-24
\\
  In numerous applications, binary reactions or event counts are observed and
stored within high-order tensors. Tensor decompositions (TDs) serve as a
powerful tool to handle such high-dimensional and sparse data. However, many
traditional TDs are explicitly or implicitly designed based on the Gaussian
distribution, which is unsuitable for discrete data. Moreover, most TDs rely on
predefined multi-linear structures, such as CP and Tucker formats. Therefore,
they may not be effective enough to handle complex real-world datasets. To
address these issues, we propose ENTED, an \underline{E}fficient
\underline{N}onparametric \underline{TE}nsor \underline{D}ecomposition for
binary and count tensors. Specifically, we first employ a nonparametric
Gaussian process (GP) to replace traditional multi-linear structures. Next, we
utilize the \pg augmentation which provides a unified framework to establish
conjugate models for binary and count distributions. Finally, to address the
computational issue of GPs, we enhance the model by incorporating sparse
orthogonal variational inference of inducing points, which offers a more
effective covariance approximation within GPs and stochastic natural gradient
updates for nonparametric models. We evaluate our model on several real-world
tensor completion tasks, considering binary and count datasets. The results
manifest both better performance and computational advantages of the proposed
model.
\\ ( https://arxiv.org/abs/2401.07711 ,  92kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07756
Date: Mon, 15 Jan 2024 15:09:47 GMT   (56kb,D)

Title: Joint Probability Selection and Power Allocation for Federated Learning
Authors: Ouiame Marnissi, Hajar EL Hammouti, El Houcine Bergou
Categories: cs.LG
\\
  In this paper, we study the performance of federated learning over wireless
networks, where devices with a limited energy budget train a machine learning
model. The federated learning performance depends on the selection of the
clients participating in the learning at each round. Most existing studies
suggest deterministic approaches for the client selection, resulting in
challenging optimization problems that are usually solved using heuristics, and
therefore without guarantees on the quality of the final solution. We formulate
a new probabilistic approach to jointly select clients and allocate power
optimally so that the expected number of participating clients is maximized. To
solve the problem, a new alternating algorithm is proposed, where at each step,
the closed-form solutions for user selection probabilities and power
allocations are obtained. Our numerical results show that the proposed approach
achieves a significant performance in terms of energy consumption, completion
time and accuracy as compared to the studied benchmarks.
\\ ( https://arxiv.org/abs/2401.07756 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07788
Date: Mon, 15 Jan 2024 15:54:54 GMT   (3859kb,D)

Title: Activations and Gradients Compression for Model-Parallel Training
Authors: Mikhail Rudakov, Aleksandr Beznosikov, Yaroslav Kholodov, Alexander
  Gasnikov
Categories: cs.LG cs.DC math.OC
Comments: 17 pages, 6 figures, 5 tables, Doklady Rossijskoj akademii nauk:
  https://journals.rcsi.science/2686-9543/article/view/247111
DOI: 10.31857/S2686954323601562
\\
  Large neural networks require enormous computational clusters of machines.
Model-parallel training, when the model architecture is partitioned
sequentially between workers, is a popular approach for training modern models.
Information compression can be applied to decrease workers communication time,
as it is often a bottleneck in such systems. This work explores how
simultaneous compression of activations and gradients in model-parallel
distributed training setup affects convergence. We analyze compression methods
such as quantization and TopK compression, and also experiment with error
compensation techniques. Moreover, we employ TopK with AQ-SGD per-batch error
feedback approach. We conduct experiments on image classification and language
model fine-tuning tasks. Our findings demonstrate that gradients require milder
compression rates than activations. We observe that $K=10\%$ is the lowest TopK
compression level, which does not harm model convergence severely. Experiments
also show that models trained with TopK perform well only when compression is
also applied during inference. We find that error feedback techniques do not
improve model-parallel training compared to plain compression, but allow model
inference without compression with almost no quality drop. Finally, when
applied with the AQ-SGD approach, TopK stronger than with $ K=30\%$ worsens
model performance significantly.
\\ ( https://arxiv.org/abs/2401.07788 ,  3859kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07844
Date: Mon, 15 Jan 2024 17:20:17 GMT   (69kb)

Title: The ODE Method for Stochastic Approximation and Reinforcement Learning
  with Markovian Noise
Authors: Shuze Liu, Shuhang Chen, Shangtong Zhang
Categories: cs.LG cs.AI
\\
  Stochastic approximation is a class of algorithms that update a vector
iteratively, incrementally, and stochastically, including, e.g., stochastic
gradient descent and temporal difference learning. One fundamental challenge in
analyzing a stochastic approximation algorithm is to establish its stability,
i.e., to show that the stochastic vector iterates are bounded almost surely. In
this paper, we extend the celebrated Borkar-Meyn theorem for stability from the
Martingale difference noise setting to the Markovian noise setting, which
greatly improves its applicability in reinforcement learning, especially in
those off-policy reinforcement learning algorithms with linear function
approximation and eligibility traces. Central to our analysis is the
diminishing asymptotic rate of change of a few functions, which is implied by
both a form of strong law of large numbers and a commonly used V4 Lyapunov
drift condition and trivially holds if the Markov chain is finite and
irreducible.
\\ ( https://arxiv.org/abs/2401.07844 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07874
Date: Mon, 15 Jan 2024 18:08:31 GMT   (1014kb,D)

Title: Do stable neural networks exist for classification problems? -- A new
  view on stability in AI
Authors: Z. N. D. Liu, A. C. Hansen
Categories: cs.LG math.FA
\\
  In deep learning (DL) the instability phenomenon is widespread and well
documented, most commonly using the classical measure of stability, the
Lipschitz constant. While a small Lipchitz constant is traditionally viewed as
guarantying stability, it does not capture the instability phenomenon in DL for
classification well. The reason is that a classification function -- which is
the target function to be approximated -- is necessarily discontinuous, thus
having an 'infinite' Lipchitz constant. As a result, the classical approach
will deem every classification function unstable, yet basic classification
functions a la 'is there a cat in the image?' will typically be locally very
'flat' -- and thus locally stable -- except at the decision boundary. The lack
of an appropriate measure of stability hinders a rigorous theory for stability
in DL, and consequently, there are no proper approximation theoretic results
that can guarantee the existence of stable networks for classification
functions. In this paper we introduce a novel stability measure
$\mathscr{S}(f)$, for any classification function $f$, appropriate to study the
stability of discontinuous functions and their approximations. We further prove
two approximation theorems: First, for any $\epsilon > 0$ and any
classification function $f$ on a \emph{compact set}, there is a neural network
(NN) $\psi$, such that $\psi - f \neq 0$ only on a set of measure $< \epsilon$,
moreover, $\mathscr{S}(\psi) \geq \mathscr{S}(f) - \epsilon$ (as accurate and
stable as $f$ up to $\epsilon$). Second, for any classification function $f$
and $\epsilon > 0$, there exists a NN $\psi$ such that $\psi = f$ on the set of
points that are at least $\epsilon$ away from the decision boundary.
\\ ( https://arxiv.org/abs/2401.07874 ,  1014kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07883
Date: Mon, 15 Jan 2024 18:25:18 GMT   (1608kb,D)

Title: The Chronicles of RAG: The Retriever, the Chunk and the Generator
Authors: Paulo Finardi, Leonardo Avila, Rodrigo Castaldoni, Pedro Gengo, Celio
  Larcher, Marcos Piau, Pablo Costa, Vinicius Carid\'a
Categories: cs.LG cs.AI cs.CL cs.IR
Comments: 16 pages, 15 figures, 9 tables
\\
  Retrieval Augmented Generation (RAG) has become one of the most popular
paradigms for enabling LLMs to access external data, and also as a mechanism
for grounding to mitigate against hallucinations. When implementing RAG you can
face several challenges like effective integration of retrieval models,
efficient representation learning, data diversity, computational efficiency
optimization, evaluation, and quality of text generation. Given all these
challenges, every day a new technique to improve RAG appears, making it
unfeasible to experiment with all combinations for your problem. In this
context, this paper presents good practices to implement, optimize, and
evaluate RAG for the Brazilian Portuguese language, focusing on the
establishment of a simple pipeline for inference and experiments. We explored a
diverse set of methods to answer questions about the first Harry Potter book.
To generate the answers we used the OpenAI's gpt-4, gpt-4-1106-preview,
gpt-3.5-turbo-1106, and Google's Gemini Pro. Focusing on the quality of the
retriever, our approach achieved an improvement of MRR@10 by 35.4% compared to
the baseline. When optimizing the input size in the application, we observed
that it is possible to further enhance it by 2.4%. Finally, we present the
complete architecture of the RAG with our recommendations. As result, we moved
from a baseline of 57.88% to a maximum relative score of 98.61%.
\\ ( https://arxiv.org/abs/2401.07883 ,  1608kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07886
Date: Mon, 15 Jan 2024 18:28:17 GMT   (542kb,D)

Title: Learned Best-Effort LLM Serving
Authors: Siddharth Jha, Coleman Hooper, Xiaoxuan Liu, Sehoon Kim, Kurt Keutzer
Categories: cs.LG cs.AI cs.CL cs.DC
\\
  Many applications must provide low-latency LLM service to users or risk
unacceptable user experience. However, over-provisioning resources to serve
fluctuating request patterns is often prohibitively expensive. In this work, we
present a best-effort serving system that employs deep reinforcement learning
to adjust service quality based on the task distribution and system load. Our
best-effort system can maintain availability with over 10x higher client
request rates, serves above 96% of peak performance 4.1x more often, and serves
above 98% of peak performance 2.3x more often than static serving on
unpredictable workloads. Our learned router is robust to shifts in both the
arrival and task distribution. Compared to static serving, learned best-effort
serving allows for cost-efficient serving through increased hardware utility.
Additionally, we argue that learned best-effort LLM serving is applicable in
wide variety of settings and provides application developers great flexibility
to meet their specific needs.
\\ ( https://arxiv.org/abs/2401.07886 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07889
Date: Mon, 15 Jan 2024 18:39:13 GMT   (670kb)

Title: Machine Learning Techniques to Identify Hand Gestures amidst Forearm
  Muscle Signals
Authors: Ryan Cho, Sunil Patel, Kyu Taek Cho, and Jaejin Hwang
Categories: cs.LG cs.AI eess.SP
Comments: 21 pages, 7 figures
\\
  This study investigated the use of forearm EMG data for distinguishing eight
hand gestures, employing the Neural Network and Random Forest algorithms on
data from ten participants. The Neural Network achieved 97 percent accuracy
with 1000-millisecond windows, while the Random Forest achieved 85 percent
accuracy with 200-millisecond windows. Larger window sizes improved gesture
classification due to increased temporal resolution. The Random Forest
exhibited faster processing at 92 milliseconds, compared to the Neural
Network's 124 milliseconds. In conclusion, the study identified a Neural
Network with a 1000-millisecond stream as the most accurate (97 percent), and a
Random Forest with a 200-millisecond stream as the most efficient (85 percent).
Future research should focus on increasing sample size, incorporating more hand
gestures, and exploring different feature extraction methods and modeling
algorithms to enhance system accuracy and efficiency.
\\ ( https://arxiv.org/abs/2401.07889 ,  670kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07936
Date: Mon, 15 Jan 2024 19:53:35 GMT   (17984kb,D)

Title: A Globally Convergent Algorithm for Neural Network Parameter
  Optimization Based on Difference-of-Convex Functions
Authors: Daniel Tschernutter, Mathias Kraus, Stefan Feuerriegel
Categories: cs.LG cs.NE math.OC
Comments: accepted by TMLR
\\
  We propose an algorithm for optimizing the parameters of single hidden layer
neural networks. Specifically, we derive a blockwise difference-of-convex (DC)
functions representation of the objective function. Based on the latter, we
propose a block coordinate descent (BCD) approach that we combine with a
tailored difference-of-convex functions algorithm (DCA). We prove global
convergence of the proposed algorithm. Furthermore, we mathematically analyze
the convergence rate of parameters and the convergence rate in value (i.e., the
training loss). We give conditions under which our algorithm converges linearly
or even faster depending on the local shape of the loss function. We confirm
our theoretical derivations numerically and compare our algorithm against
state-of-the-art gradient-based solvers in terms of both training loss and test
loss.
\\ ( https://arxiv.org/abs/2401.07936 ,  17984kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07958
Date: Mon, 15 Jan 2024 20:54:20 GMT   (7451kb,D)

Title: GD-CAF: Graph Dual-stream Convolutional Attention Fusion for
  Precipitation Nowcasting
Authors: Lorand Vatamany, Siamak Mehrkanoon
Categories: cs.LG cs.CV
Comments: 13 pages, 13 figures
ACM-class: I.2; I.5
\\
  Accurate precipitation nowcasting is essential for various purposes,
including flood prediction, disaster management, optimizing agricultural
activities, managing transportation routes and renewable energy. While several
studies have addressed this challenging task from a sequence-to-sequence
perspective, most of them have focused on a single area without considering the
existing correlation between multiple disjoint regions. In this paper, we
formulate precipitation nowcasting as a spatiotemporal graph sequence
nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional
Attention Fusion (GD-CAF), a novel approach designed to learn from historical
spatiotemporal graph of precipitation maps and nowcast future time step ahead
precipitation at different spatial locations. GD-CAF consists of
spatio-temporal convolutional attention as well as gated fusion modules which
are equipped with depthwise-separable convolutional operations. This
enhancement enables the model to directly process the high-dimensional
spatiotemporal graph of precipitation maps and exploits higher-order
correlations between the data dimensions. We evaluate our model on seven years
of precipitation maps across Europe and its neighboring areas collected from
the ERA5 dataset, provided by Copernicus. The model receives a fully connected
graph in which each node represents historical observations from a specific
region on the map. Consequently, each node contains a 3D tensor with time,
height, and width dimensions. Experimental results demonstrate that the
proposed GD-CAF model outperforms the other examined models. Furthermore, the
averaged seasonal spatial and temporal attention scores over the test set are
visualized to provide additional insights about the strongest connections
between different regions or time steps. These visualizations shed light on the
decision-making process of our model.
\\ ( https://arxiv.org/abs/2401.07958 ,  7451kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07991
Date: Mon, 15 Jan 2024 22:31:15 GMT   (475kb,D)

Title: Robustness Against Adversarial Attacks via Learning Confined Adversarial
  Polytopes
Authors: Shayan Mohajer Hamidi, Linfeng Ye
Categories: cs.LG cs.CR
Comments: The paper has been accepted in ICASSP 2024
\\
  Deep neural networks (DNNs) could be deceived by generating
human-imperceptible perturbations of clean samples. Therefore, enhancing the
robustness of DNNs against adversarial attacks is a crucial task. In this
paper, we aim to train robust DNNs by limiting the set of outputs reachable via
a norm-bounded perturbation added to a clean sample. We refer to this set as
adversarial polytope, and each clean sample has a respective adversarial
polytope. Indeed, if the respective polytopes for all the samples are compact
such that they do not intersect the decision boundaries of the DNN, then the
DNN is robust against adversarial samples. Hence, the inner-working of our
algorithm is based on learning \textbf{c}onfined \textbf{a}dversarial
\textbf{p}olytopes (CAP). By conducting a thorough set of experiments, we
demonstrate the effectiveness of CAP over existing adversarial robustness
methods in improving the robustness of models against state-of-the-art attacks
including AutoAttack.
\\ ( https://arxiv.org/abs/2401.07991 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07993
Date: Mon, 15 Jan 2024 22:36:11 GMT   (36562kb,D)

Title: Carrying over algorithm in transformers
Authors: Jorrit Kruthoff
Categories: cs.LG cs.AI
Comments: Comments welcome!
\\
  Addition is perhaps one of the simplest arithmetic tasks one can think of and
is usually performed using the carrying over algorithm. This algorithm consists
of two tasks: adding digits in the same position and carrying over a one
whenever necessary. We study how transformer models implement this algorithm
and how the two aforementioned tasks are allocated to different parts of the
network. We first focus on two-layer encoder-only models and show that the
carrying over algorithm is implemented in a modular fashion. The first layer is
mostly responsible for adding digits in the same position. The second layer
first decides, in the attention, which positions need a carried one or not, and
then performs the carrying of the one in the final MLP. We provide a simple way
of precisely identifying which neurons are responsible for that task. This
implementation of the carrying over algorithm occurs across a range of
hyperparameters for two as well as three-layer models. For small decoder-only
models, we observe the same implementation and provide suggestive evidence for
its existence in three 7B large language models.
\\ ( https://arxiv.org/abs/2401.07993 ,  36562kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08002
Date: Mon, 15 Jan 2024 23:10:22 GMT   (5428kb)

Title: Discovery of Generalizable TBI Phenotypes Using Multivariate Time-Series
  Clustering
Authors: Hamid Ghaderi, Brandon Foreman, Chandan K. Reddy, Vignesh Subbian
Categories: cs.LG q-bio.QM stat.AP
Comments: 25 pages, 10 figures, 4 tables, submitted to Computers in Biology and
  Medicine
\\
  Traumatic Brain Injury (TBI) presents a broad spectrum of clinical
presentations and outcomes due to its inherent heterogeneity, leading to
diverse recovery trajectories and varied therapeutic responses. While many
studies have delved into TBI phenotyping for distinct patient populations,
identifying TBI phenotypes that consistently generalize across various settings
and populations remains a critical research gap. Our research addresses this by
employing multivariate time-series clustering to unveil TBI's dynamic
intricates. Utilizing a self-supervised learning-based approach to clustering
multivariate time-Series data with missing values (SLAC-Time), we analyzed both
the research-centric TRACK-TBI and the real-world MIMIC-IV datasets.
Remarkably, the optimal hyperparameters of SLAC-Time and the ideal number of
clusters remained consistent across these datasets, underscoring SLAC-Time's
stability across heterogeneous datasets. Our analysis revealed three
generalizable TBI phenotypes ({\alpha}, \b{eta}, and {\gamma}), each exhibiting
distinct non-temporal features during emergency department visits, and temporal
feature profiles throughout ICU stays. Specifically, phenotype {\alpha}
represents mild TBI with a remarkably consistent clinical presentation. In
contrast, phenotype \b{eta} signifies severe TBI with diverse clinical
manifestations, and phenotype {\gamma} represents a moderate TBI profile in
terms of severity and clinical diversity. Age is a significant determinant of
TBI outcomes, with older cohorts recording higher mortality rates. Importantly,
while certain features varied by age, the core characteristics of TBI
manifestations tied to each phenotype remain consistent across diverse
populations.
\\ ( https://arxiv.org/abs/2401.08002 ,  5428kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08016
Date: Mon, 15 Jan 2024 23:58:21 GMT   (3064kb,D)

Title: Contextual Bandits with Stage-wise Constraints
Authors: Aldo Pacchiano, Mohammad Ghavamzadeh, Peter Bartlett
Categories: cs.LG stat.ML
Comments: 53 pages. arXiv admin note: text overlap with arXiv:2006.10185
\\
  We study contextual bandits in the presence of a stage-wise constraint (a
constraint at each round), when the constraint must be satisfied both with high
probability and in expectation. Obviously the setting where the constraint is
in expectation is a relaxation of the one with high probability. We start with
the linear case where both the contextual bandit problem (reward function) and
the stage-wise constraint (cost function) are linear. In each of the high
probability and in expectation settings, we propose an upper-confidence bound
algorithm for the problem and prove a $T$-round regret bound for it. Our
algorithms balance exploration and constraint satisfaction using a novel idea
that scales the radii of the reward and cost confidence sets with different
scaling factors. We also prove a lower-bound for this constrained problem, show
how our algorithms and analyses can be extended to multiple constraints, and
provide simulations to validate our theoretical results. In the high
probability setting, we describe the minimum requirements for the action set in
order for our algorithm to be tractable. In the setting that the constraint is
in expectation, we further specialize our results to multi-armed bandits and
propose a computationally efficient algorithm for this setting with regret
analysis. Finally, we extend our results to the case where the reward and cost
functions are both non-linear. We propose an algorithm for this case and prove
a regret bound for it that characterize the function class complexity by the
eluder dimension.
\\ ( https://arxiv.org/abs/2401.08016 ,  3064kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08061
Date: Tue, 16 Jan 2024 02:42:45 GMT   (1218kb,D)

Title: Augmenting Ground-Level PM2.5 Prediction via Kriging-Based Pseudo-Label
  Generation
Authors: Lei Duan, Ziyang Jiang, David Carlson
Categories: cs.LG cs.CV
Comments: 8 pages, 4 figures, NeurIPS 2023 Workshop: Tackling Climate Change
  with Machine Learning
\\
  Fusing abundant satellite data with sparse ground measurements constitutes a
major challenge in climate modeling. To address this, we propose a strategy to
augment the training dataset by introducing unlabeled satellite images paired
with pseudo-labels generated through a spatial interpolation technique known as
ordinary kriging, thereby making full use of the available satellite data
resources. We show that the proposed data augmentation strategy helps enhance
the performance of the state-of-the-art convolutional neural network-random
forest (CNN-RF) model by a reasonable amount, resulting in a noteworthy
improvement in spatial correlation and a reduction in prediction error.
\\ ( https://arxiv.org/abs/2401.08061 ,  1218kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08077
Date: Tue, 16 Jan 2024 03:03:39 GMT   (688kb)

Title: Transformer-based approach for Ethereum Price Prediction Using
  Crosscurrency correlation and Sentiment Analysis
Authors: Shubham Singh, Mayur Bhat
Categories: cs.LG cs.AI q-fin.PR
Comments: 12 pages
\\
  The research delves into the capabilities of a transformer-based neural
network for Ethereum cryptocurrency price forecasting. The experiment runs
around the hypothesis that cryptocurrency prices are strongly correlated with
other cryptocurrencies and the sentiments around the cryptocurrency. The model
employs a transformer architecture for several setups from single-feature
scenarios to complex configurations incorporating volume, sentiment, and
correlated cryptocurrency prices. Despite a smaller dataset and less complex
architecture, the transformer model surpasses ANN and MLP counterparts on some
parameters. The conclusion presents a hypothesis on the illusion of causality
in cryptocurrency price movements driven by sentiments.
\\ ( https://arxiv.org/abs/2401.08077 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08081
Date: Tue, 16 Jan 2024 03:15:52 GMT   (523kb,D)

Title: Predicting Next Useful Location With Context-Awareness: The
  State-Of-The-Art
Authors: Alireza Nezhadettehad, Arkady Zaslavsky, Rakib Abdur, Siraj Ahmed
  Shaikh, Seng W. Loke, Guang-Li Huang, Alireza Hassani
Categories: cs.LG cs.SI
\\
  Predicting the future location of mobile objects reinforces location-aware
services with proactive intelligence and helps businesses and decision-makers
with better planning and near real-time scheduling in different applications
such as traffic congestion control, location-aware advertisements, and
monitoring public health and well-being. The recent developments in the
smartphone and location sensors technology and the prevalence of using
location-based social networks alongside the improvements in artificial
intelligence and machine learning techniques provide an excellent opportunity
to exploit massive amounts of historical and real-time contextual information
to recognise mobility patterns and achieve more accurate and intelligent
predictions. This survey provides a comprehensive overview of the next useful
location prediction problem with context-awareness. First, we explain the
concepts of context and context-awareness and define the next location
prediction problem. Then we analyse nearly thirty studies in this field
concerning the prediction method, the challenges addressed, the datasets and
metrics used for training and evaluating the model, and the types of context
incorporated. Finally, we discuss the advantages and disadvantages of different
approaches, focusing on the usefulness of the predicted location and
identifying the open challenges and future work on this subject by introducing
two potential use cases of next location prediction in the automotive industry.
\\ ( https://arxiv.org/abs/2401.08081 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08092
Date: Tue, 16 Jan 2024 03:35:26 GMT   (22912kb,D)

Title: A Survey of Resource-efficient LLM and Multimodal Foundation Models
Authors: Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng
  Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, Qiyang Zhang, Zhenyan
  Lu, Li Zhang, Shangguang Wang, Yuanchun Li, Yunxin Liu, Xin Jin, Xuanzhe Liu
Categories: cs.LG cs.AI cs.DC
\\
  Large foundation models, including large language models (LLMs), vision
transformers (ViTs), diffusion, and LLM-based multimodal models, are
revolutionizing the entire machine learning lifecycle, from training to
deployment. However, the substantial advancements in versatility and
performance these models offer come at a significant cost in terms of hardware
resources. To support the growth of these large models in a scalable and
environmentally sustainable way, there has been a considerable focus on
developing resource-efficient strategies. This survey delves into the critical
importance of such research, examining both algorithmic and systemic aspects.
It offers a comprehensive analysis and valuable insights gleaned from existing
literature, encompassing a broad array of topics from cutting-edge model
architectures and training/serving algorithms to practical system designs and
implementations. The goal of this survey is to provide an overarching
understanding of how current approaches are tackling the resource challenges
posed by large foundation models and to potentially inspire future
breakthroughs in this field.
\\ ( https://arxiv.org/abs/2401.08092 ,  22912kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08119
Date: Tue, 16 Jan 2024 05:23:34 GMT   (863kb,D)

Title: SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic
  Spatio-Temporal Traffic Forecasting
Authors: Lequan Lin, Dai Shi, Andi Han, Junbin Gao
Categories: cs.LG
\\
  Traffic forecasting, a crucial application of spatio-temporal graph (STG)
learning, has traditionally relied on deterministic models for accurate point
estimations. Yet, these models fall short of identifying latent risks of
unexpected volatility in future observations. To address this gap,
probabilistic methods, especially variants of diffusion models, have emerged as
uncertainty-aware solutions. However, existing diffusion methods typically
focus on generating separate future time series for individual sensors in the
traffic network, resulting in insufficient involvement of spatial network
characteristics in the probabilistic learning process. To better leverage
spatial dependencies and systematic patterns inherent in traffic data, we
propose SpecSTG, a novel spectral diffusion framework. Our method generates the
Fourier representation of future time series, transforming the learning process
into the spectral domain enriched with spatial information. Additionally, our
approach incorporates a fast spectral graph convolution designed for Fourier
input, alleviating the computational burden associated with existing models.
Numerical experiments show that SpecSTG achieves outstanding performance with
traffic flow and traffic speed datasets compared to state-of-the-art baselines.
The source code for SpecSTG is available at
https://anonymous.4open.science/r/SpecSTG.
\\ ( https://arxiv.org/abs/2401.08119 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08121
Date: Tue, 16 Jan 2024 05:28:12 GMT   (9643kb,D)

Title: CycLight: learning traffic signal cooperation with a cycle-level
  strategy
Authors: Gengyue Han, Xiaohan Liu, Xianyue Peng, Hao Wang, Yu Han
Categories: cs.LG cs.AI cs.SY eess.SY
\\
  This study introduces CycLight, a novel cycle-level deep reinforcement
learning (RL) approach for network-level adaptive traffic signal control
(NATSC) systems. Unlike most traditional RL-based traffic controllers that
focus on step-by-step decision making, CycLight adopts a cycle-level strategy,
optimizing cycle length and splits simultaneously using Parameterized Deep
Q-Networks (PDQN) algorithm. This cycle-level approach effectively reduces the
computational burden associated with frequent data communication, meanwhile
enhancing the practicality and safety of real-world applications. A
decentralized framework is formulated for multi-agent cooperation, while
attention mechanism is integrated to accurately assess the impact of the
surroundings on the current intersection. CycLight is tested in a large
synthetic traffic grid using the microscopic traffic simulation tool, SUMO.
Experimental results not only demonstrate the superiority of CycLight over
other state-of-the-art approaches but also showcase its robustness against
information transmission delays.
\\ ( https://arxiv.org/abs/2401.08121 ,  9643kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08135
Date: Tue, 16 Jan 2024 06:01:02 GMT   (522kb,D)

Title: Machine Learning-Based Malicious Vehicle Detection for Security Threats
  and Attacks in Vehicle Ad-hoc Network (VANET) Communications
Authors: Thanh Nguyen Canh and Xiem HoangVan
Categories: cs.LG cs.CR cs.NI
Comments: In the 2023 RIVF International Conference on Computing and
  Communication Technologies, Hanoi, Vietnam
\\
  With the rapid growth of Vehicle Ad-hoc Network (VANET) as a promising
technology for efficient and reliable communication among vehicles and
infrastructure, the security and integrity of VANET communications has become a
critical concern. One of the significant threats to VANET is the presence of
blackhole attacks, where malicious nodes disrupt the network's functionality
and compromise data confidentiality, integrity, and availability. In this
paper, we propose a machine learning-based approach for blackhole detection in
VANET. To achieve this task, we first create a comprehensive dataset comprising
normal and malicious traffic flows. Afterward, we study and define a promising
set of features to discriminate the blackhole attacks. Finally, we evaluate
various machine learning algorithms, including Gradient Boosting, Random
Forest, Support Vector Machines, k-Nearest Neighbors, Gaussian Naive Bayes, and
Logistic Regression. Experimental results demonstrate the effectiveness of
these algorithms in distinguishing between normal and malicious nodes. Our
findings also highlight the potential of machine learning based approach in
enhancing the security of VANET by detecting and mitigating blackhole attacks.
\\ ( https://arxiv.org/abs/2401.08135 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08139
Date: Tue, 16 Jan 2024 06:18:11 GMT   (9488kb,D)

Title: Transferring Core Knowledge via Learngenes
Authors: Fu Feng, Jing Wang and Xin Geng
Categories: cs.LG cs.NE
\\
  The pre-training paradigm fine-tunes the models trained on large-scale
datasets to downstream tasks with enhanced performance. It transfers all
knowledge to downstream tasks without discriminating which part is necessary or
unnecessary, which may lead to negative transfer. In comparison, knowledge
transfer in nature is much more efficient. When passing genetic information to
descendants, ancestors encode only the essential knowledge into genes, which
act as the medium. Inspired by that, we adopt a recent concept called
``learngene'' and refine its structures by mimicking the structures of natural
genes. We propose the Genetic Transfer Learning (GTL) -- a framework to copy
the evolutionary process of organisms into neural networks. GTL trains a
population of networks, selects superior learngenes by tournaments, performs
learngene mutations, and passes the learngenes to next generations. Finally, we
successfully extract the learngenes of VGG11 and ResNet12. We show that the
learngenes bring the descendant networks instincts and strong learning ability:
with 20% parameters, the learngenes bring 12% and 16% improvements of accuracy
on CIFAR-FS and miniImageNet. Besides, the learngenes have the scalability and
adaptability on the downstream structure of networks and datasets. Overall, we
offer a novel insight that transferring core knowledge via learngenes may be
sufficient and efficient for neural networks.
\\ ( https://arxiv.org/abs/2401.08139 ,  9488kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08147
Date: Tue, 16 Jan 2024 06:40:24 GMT   (651kb)

Title: Machine Learning on Dynamic Graphs: A Survey on Applications
Authors: Sanaz Hasanzadeh Fard
Categories: cs.LG cs.SI
\\
  Dynamic graph learning has gained significant attention as it offers a
powerful means to model intricate interactions among entities across various
real-world and scientific domains. Notably, graphs serve as effective
representations for diverse networks such as transportation, brain, social, and
internet networks. Furthermore, the rapid advancements in machine learning have
expanded the scope of dynamic graph applications beyond the aforementioned
domains. In this paper, we present a review of lesser-explored applications of
dynamic graph learning. This study revealed the potential of machine learning
on dynamic graphs in addressing challenges across diverse domains, including
those with limited levels of association with the field.
\\ ( https://arxiv.org/abs/2401.08147 ,  651kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08197
Date: Tue, 16 Jan 2024 08:25:29 GMT   (1729kb,D)

Title: Matrix Completion with Hypergraphs:Sharp Thresholds and Efficient
  Algorithms
Authors: Zhongtian Ma, Qiaosheng Zhang and Zhen Wang
Categories: cs.LG cs.IT eess.SP math.IT
\\
  This paper considers the problem of completing a rating matrix based on
sub-sampled matrix entries as well as observed social graphs and hypergraphs.
We show that there exists a \emph{sharp threshold} on the sample probability
for the task of exactly completing the rating matrix -- the task is achievable
when the sample probability is above the threshold, and is impossible otherwise
-- demonstrating a phase transition phenomenon. The threshold can be expressed
as a function of the ``quality'' of hypergraphs, enabling us to \emph{quantify}
the amount of reduction in sample probability due to the exploitation of
hypergraphs. This also highlights the usefulness of hypergraphs in the matrix
completion problem. En route to discovering the sharp threshold, we develop a
computationally efficient matrix completion algorithm that effectively exploits
the observed graphs and hypergraphs. Theoretical analyses show that our
algorithm succeeds with high probability as long as the sample probability
exceeds the aforementioned threshold, and this theoretical result is further
validated by synthetic experiments. Moreover, our experiments on a real social
network dataset (with both graphs and hypergraphs) show that our algorithm
outperforms other state-of-the-art matrix completion algorithms.
\\ ( https://arxiv.org/abs/2401.08197 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08221
Date: Tue, 16 Jan 2024 09:15:43 GMT   (574kb,D)

Title: Towards Causal Relationship in Indefinite Data: Baseline Model and New
  Datasets
Authors: Hang Chen, Xinyu Yang, Keqing Du
Categories: cs.LG cs.AI
Comments: If you are interested in the two new datasets, pls contact us by
  email
\\
  Integrating deep learning and causal discovery has encouraged us to spot that
learning causal structures and representations in dialogue and video is full of
challenges. We defined These data forms as "Indefinite Data", characterized by
multi-structure data and multi-value representations. Unlike existing adaptable
data forms, Indefinite Data still faces gaps in datasets and methods. To
address the dataset gap, we release two high-quality datasets - Causalogue and
Causaction, containing text dialogue samples and video action samples with
causal annotations respectively. Moreover, the method gap arises from the
coexistence of multi-structure data and multi-value representations, breaking
the assumptions of all current methods and rendering them infeasible on
Indefinite Data. To this end, we propose a probabilistic framework as a
baseline, incorporating three designed highlights for this gap: 1) establishing
Causation Condition of representations using the independence of noise terms
under non-fixed causal structures, 2) treating causal strength as a latent
variable and measuring the reconstruction loss in the correlation space, and 3)
estimating the effects of latent confounders. These highpoints make the
probabilistic model capable of overcoming challenges brought by the coexistence
of multi-structure data and multi-value representations and pave the way for
the extension of latent confounders. Comprehensive experiments have evaluated
baseline results of causal structures, causal representations, and confounding
disentanglement.
\\ ( https://arxiv.org/abs/2401.08221 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08225
Date: Tue, 16 Jan 2024 09:22:38 GMT   (454kb,D)

Title: Efficient and Mathematically Robust Operations for Certified Neural
  Networks Inference
Authors: Fabien Geyer, Johannes Freitag, Tobias Schulz, Sascha Uhrig
Categories: cs.LG cs.AR
Journal-ref: 6th Workshop on Accelerated Machine Learning (AccML) at HiPEAC
  2024
\\
  In recent years, machine learning (ML) and neural networks (NNs) have gained
widespread use and attention across various domains, particularly in
transportation for achieving autonomy, including the emergence of flying taxis
for urban air mobility (UAM). However, concerns about certification have come
up, compelling the development of standardized processes encompassing the
entire ML and NN pipeline. This paper delves into the inference stage and the
requisite hardware, highlighting the challenges associated with IEEE 754
floating-point arithmetic and proposing alternative number representations. By
evaluating diverse summation and dot product algorithms, we aim to mitigate
issues related to non-associativity. Additionally, our exploration of
fixed-point arithmetic reveals its advantages over floating-point methods,
demonstrating significant hardware efficiencies. Employing an empirical
approach, we ascertain the optimal bit-width necessary to attain an acceptable
level of accuracy, considering the inherent complexity of bit-width
optimization.
\\ ( https://arxiv.org/abs/2401.08225 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08233
Date: Tue, 16 Jan 2024 09:34:17 GMT   (1066kb)

Title: Enhancing Wind Speed and Wind Power Forecasting Using Shape-Wise Feature
  Engineering: A Novel Approach for Improved Accuracy and Robustness
Authors: Mulomba Mukendi Christian, Yun Seon Kim, Hyebong Choi, Jaeyoung Lee,
  SongHee You
Categories: cs.LG
Journal-ref: International Journal of Advanced Culture Technology Vol.11 No.4
  393-405 (2023)
DOI: 10.17703/IJACT.2023.11.4.393
\\
  Accurate prediction of wind speed and power is vital for enhancing the
efficiency of wind energy systems. Numerous solutions have been implemented to
date, demonstrating their potential to improve forecasting. Among these, deep
learning is perceived as a revolutionary approach in the field. However,
despite their effectiveness, the noise present in the collected data remains a
significant challenge. This noise has the potential to diminish the performance
of these algorithms, leading to inaccurate predictions. In response to this,
this study explores a novel feature engineering approach. This approach
involves altering the data input shape in both Convolutional Neural
Network-Long Short-Term Memory (CNN-LSTM) and Autoregressive models for various
forecasting horizons. The results reveal substantial enhancements in model
resilience against noise resulting from step increases in data. The approach
could achieve an impressive 83% accuracy in predicting unseen data up to the
24th steps. Furthermore, this method consistently provides high accuracy for
short, mid, and long-term forecasts, outperforming the performance of
individual models. These findings pave the way for further research on noise
reduction strategies at different forecasting horizons through shape-wise
feature engineering.
\\ ( https://arxiv.org/abs/2401.08233 ,  1066kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08245
Date: Tue, 16 Jan 2024 09:59:36 GMT   (232kb,D)

Title: Optimizing $k$ in $k$NN Graphs with Graph Learning Perspective
Authors: Asuka Tamaru, Junya Hara, Hiroshi Higashi, Yuichi Tanaka, Antonio
  Ortega
Categories: cs.LG eess.SP
\\
  In this paper, we propose a method, based on graph signal processing, to
optimize the choice of $k$ in $k$-nearest neighbor graphs ($k$NNGs). $k$NN is
one of the most popular approaches and is widely used in machine learning and
signal processing. The parameter $k$ represents the number of neighbors that
are connected to the target node; however, its appropriate selection is still a
challenging problem. Therefore, most $k$NNGs use ad hoc selection methods for
$k$. In the proposed method, we assume that a different $k$ can be chosen for
each node. We formulate a discrete optimization problem to seek the best $k$
with a constraint on the sum of distances of the connected nodes. The optimal
$k$ values are efficiently obtained without solving a complex optimization.
Furthermore, we reveal that the proposed method is closely related to existing
graph learning methods. In experiments on real datasets, we demonstrate that
the $k$NNGs obtained with our method are sparse and can determine an
appropriate variable number of edges per node. We validate the effectiveness of
the proposed method for point cloud denoising, comparing our denoising
performance with achievable graph construction methods that can be scaled to
typical point cloud sizes (e.g., thousands of nodes).
\\ ( https://arxiv.org/abs/2401.08245 ,  232kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08281
Date: Tue, 16 Jan 2024 11:12:36 GMT   (304kb,D)

Title: The Faiss library
Authors: Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson
  and Gergely Szilvasy and Pierre-Emmanuel Mazar\'e and Maria Lomeli and Lucas
  Hosseini and Herv\'e J\'egou
Categories: cs.LG cs.CV cs.SE
\\
  Vector databases manage large collections of embedding vectors. As AI
applications are growing rapidly, so are the number of embeddings that need to
be stored and indexed. The Faiss library is dedicated to vector similarity
search, a core functionality of vector databases. Faiss is a toolkit of
indexing methods and related primitives used to search, cluster, compress and
transform vectors. This paper first describes the tradeoff space of vector
search, then the design principles of Faiss in terms of structure, approach to
optimization and interfacing. We benchmark key features of the library and
discuss a few selected applications to highlight its broad applicability.
\\ ( https://arxiv.org/abs/2401.08281 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08318
Date: Tue, 16 Jan 2024 12:36:17 GMT   (736kb,D)

Title: OpenDPD: An Open-Source End-to-End Learning & Benchmarking Framework for
  Wideband Power Amplifier Modeling and Digital Pre-Distortion
Authors: Yizhuo Wu, Gagan Deep Singh, Mohammadreza Beikmirza, Leo de Vreede,
  Morteza Alavi, Chang Gao
Categories: cs.LG eess.SP
Comments: To be published at the 2024 IEEE International Symposium on Circuits
  and Systems (ISCAS), Singapore
\\
  With the rise in communication capacity, deep neural networks (DNN) for
digital pre-distortion (DPD) to correct non-linearity in wideband power
amplifiers (PAs) have become prominent. Yet, there is a void in open-source and
measurement-setup-independent platforms for fast DPD exploration and objective
DPD model comparison. This paper presents an open-source framework, OpenDPD,
crafted in PyTorch, with an associated dataset for PA modeling and DPD
learning. We introduce a Dense Gated Recurrent Unit (DGRU)-DPD, trained via a
novel end-to-end learning architecture, outperforming previous DPD models on a
digital PA DPA in the new digital transmitter (DTX) architecture with
unconventional transfer characteristics compared to analog PAs. Measurements
show our DGRU-DPD achieves an ACPR of -44.69/-44.47 dBc and an EVM of -35.22 dB
for 200 MHz OFDM signals. OpenDPD code, datasets, and documentation are
publicly available at https://github.com/lab-emi/OpenDPD.
\\ ( https://arxiv.org/abs/2401.08318 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08327
Date: Tue, 16 Jan 2024 12:45:15 GMT   (3798kb,D)

Title: Learn What You Need in Personalized Federated Learning
Authors: Kexin Lv, Rui Ye, Xiaolin Huang, Jie Yang and Siheng Chen
Categories: cs.LG
\\
  Personalized federated learning aims to address data heterogeneity across
local clients in federated learning. However, current methods blindly
incorporate either full model parameters or predefined partial parameters in
personalized federated learning. They fail to customize the collaboration
manner according to each local client's data characteristics, causing
unpleasant aggregation results. To address this essential issue, we propose
$\textit{Learn2pFed}$, a novel algorithm-unrolling-based personalized federated
learning framework, enabling each client to adaptively select which part of its
local model parameters should participate in collaborative training. The key
novelty of the proposed $\textit{Learn2pFed}$ is to optimize each local model
parameter's degree of participant in collaboration as learnable parameters via
algorithm unrolling methods. This approach brings two benefits: 1)
mathmatically determining the participation degree of local model parameters in
the federated collaboration, and 2) obtaining more stable and improved
solutions. Extensive experiments on various tasks, including regression,
forecasting, and image classification, demonstrate that $\textit{Learn2pFed}$
significantly outperforms previous personalized federated learning methods.
\\ ( https://arxiv.org/abs/2401.08327 ,  3798kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08330
Date: Tue, 16 Jan 2024 12:49:10 GMT   (8944kb,D)

Title: Boosting Gradient Ascent for Continuous DR-submodular Maximization
Authors: Qixin Zhang, Zongqi Wan, Zengde Deng, Zaiyi Chen, Xiaoming Sun, Jialin
  Zhang and Yu Yang
Categories: cs.LG cs.AI math.OC
Comments: 74 pages, 6 figures and 9 tables. An extended version of Stochastic
  Continuous Submodular Maximization: Boosting via Non-oblivious Function (ICML
  2022)
\\
  Projected Gradient Ascent (PGA) is the most commonly used optimization scheme
in machine learning and operations research areas. Nevertheless, numerous
studies and examples have shown that the PGA methods may fail to achieve the
tight approximation ratio for continuous DR-submodular maximization problems.
To address this challenge, we present a boosting technique in this paper, which
can efficiently improve the approximation guarantee of the standard PGA to
\emph{optimal} with only small modifications on the objective function. The
fundamental idea of our boosting technique is to exploit non-oblivious search
to derive a novel auxiliary function $F$, whose stationary points are excellent
approximations to the global maximum of the original DR-submodular objective
$f$. Specifically, when $f$ is monotone and $\gamma$-weakly DR-submodular, we
propose an auxiliary function $F$ whose stationary points can provide a better
$(1-e^{-\gamma})$-approximation than the
$(\gamma^2/(1+\gamma^2))$-approximation guaranteed by the stationary points of
$f$ itself. Similarly, for the non-monotone case, we devise another auxiliary
function $F$ whose stationary points can achieve an optimal
$\frac{1-\min_{\boldsymbol{x}\in\mathcal{C}}\|\boldsymbol{x}\|_{\infty}}{4}$-approximation
guarantee where $\mathcal{C}$ is a convex constraint set. In contrast, the
stationary points of the original non-monotone DR-submodular function can be
arbitrarily bad~\citep{chen2023continuous}. Furthermore, we demonstrate the
scalability of our boosting technique on four problems. In all of these four
problems, our resulting variants of boosting PGA algorithm beat the previous
standard PGA in several aspects such as approximation ratio and efficiency.
Finally, we corroborate our theoretical findings with numerical experiments,
which demonstrate the effectiveness of our boosting PGA methods.
\\ ( https://arxiv.org/abs/2401.08330 ,  8944kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08348
Date: Tue, 16 Jan 2024 13:29:30 GMT   (5220kb,D)

Title: We don't need no labels: Estimating post-deployment model performance
  under covariate shift without ground truth
Authors: Jakub Bia{\l}ek, Wojtek Kuberski, Nikolaos Perrakis
Categories: cs.LG
MSC-class: 62G05
\\
  The performance of machine learning models often degrades after deployment
due to data distribution shifts. In many use cases, it is impossible to
calculate the post-deployment performance because labels are unavailable or
significantly delayed. Proxy methods for evaluating model performance
stability, like drift detection techniques, do not properly quantify data
distribution shift impact. As a solution, we propose a robust and accurate
performance estimation method for evaluating ML classification models on
unlabeled data that accurately quantifies the impact of covariate shift on
model performance. We call it multi-calibrated confidence-based performance
estimation (M-CBPE). It is model and data-type agnostic and works for any
performance metric. It does not require access to the monitored model - it uses
the model predictions and probability estimates. M-CBPE does not need user
input on the nature of the covariate shift as it fully learns from the data. We
evaluate it with over 600 dataset-model pairs from US census data and compare
it with multiple benchmarks using several evaluation metrics. Results show that
M-CBPE is the best method to estimate the performance of classification models
in any evaluation context.
\\ ( https://arxiv.org/abs/2401.08348 ,  5220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08351
Date: Tue, 16 Jan 2024 13:30:37 GMT   (799kb,D)

Title: Personalized Federated Learning of Probabilistic Models: A PAC-Bayesian
  Approach
Authors: Mahrokh Ghoddousi Boroujeni, Andreas Krause, Giancarlo Ferrari Trecate
Categories: cs.LG cs.CR
\\
  Federated learning aims to infer a shared model from private and
decentralized data stored locally by multiple clients. Personalized federated
learning (PFL) goes one step further by adapting the global model to each
client, enhancing the model's fit for different clients. A significant level of
personalization is required for highly heterogeneous clients, but can be
challenging to achieve especially when they have small datasets. To address
this problem, we propose a PFL algorithm named PAC-PFL for learning
probabilistic models within a PAC-Bayesian framework that utilizes differential
privacy to handle data-dependent priors. Our algorithm collaboratively learns a
shared hyper-posterior and regards each client's posterior inference as the
personalization step. By establishing and minimizing a generalization bound on
the average true risk of clients, PAC-PFL effectively combats over-fitting.
PACPFL achieves accurate and well-calibrated predictions, supported by
experiments on a dataset of photovoltaic panel power generation, FEMNIST
dataset (Caldas et al., 2019), and Dirichlet-partitioned EMNIST dataset (Cohen
et al., 2017).
\\ ( https://arxiv.org/abs/2401.08351 ,  799kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08364
Date: Tue, 16 Jan 2024 13:46:10 GMT   (14184kb,D)

Title: Weighted Spectral Filters for Kernel Interpolation on Spheres: Estimates
  of Prediction Accuracy for Noisy Data
Authors: Xiaotong Liu, Jinxin Wang, Di Wang and Shao-Bo Lin
Categories: cs.LG
\\
  Spherical radial-basis-based kernel interpolation abounds in image sciences
including geophysical image reconstruction, climate trends description and
image rendering due to its excellent spatial localization property and perfect
approximation performance. However, in dealing with noisy data, kernel
interpolation frequently behaves not so well due to the large condition number
of the kernel matrix and instability of the interpolation process. In this
paper, we introduce a weighted spectral filter approach to reduce the condition
number of the kernel matrix and then stabilize kernel interpolation. The main
building blocks of the proposed method are the well developed spherical
positive quadrature rules and high-pass spectral filters. Using a recently
developed integral operator approach for spherical data analysis, we
theoretically demonstrate that the proposed weighted spectral filter approach
succeeds in breaking through the bottleneck of kernel interpolation, especially
in fitting noisy data. We provide optimal approximation rates of the new method
to show that our approach does not compromise the predicting accuracy.
Furthermore, we conduct both toy simulations and two real-world data
experiments with synthetically added noise in geophysical image reconstruction
and climate image processing to verify our theoretical assertions and show the
feasibility of the weighted spectral filter approach.
\\ ( https://arxiv.org/abs/2401.08364 ,  14184kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08383
Date: Tue, 16 Jan 2024 14:16:47 GMT   (21624kb,D)

Title: Exploiting Inter-Layer Expert Affinity for Accelerating
  Mixture-of-Experts Model Inference
Authors: Jinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar
  K. (DK) Panda
Categories: cs.LG cs.AI cs.DC
\\
  In large language models like the Generative Pre-trained Transformer, the
Mixture of Experts paradigm has emerged as a powerful technique for enhancing
model expressiveness and accuracy. However, deploying GPT MoE models for
parallel inference on distributed systems presents significant challenges,
primarily due to the extensive Alltoall communication required for expert
routing and aggregation. This communication bottleneck exacerbates the already
complex computational landscape, hindering the efficient utilization of
high-performance computing resources. In this paper, we propose a lightweight
optimization technique called ExFlow, to largely accelerate the inference of
these MoE models. We take a new perspective on alleviating the communication
overhead by exploiting the inter-layer expert affinity. Unlike previous
methods, our solution can be directly applied to pre-trained MoE models without
any fine-tuning or accuracy degradation. By proposing a context-coherent expert
parallelism on distributed systems, our design only uses one Alltoall
communication to deliver the same functionality while previous methods all
require two Alltoalls. By carefully examining the conditional probability in
tokens' routing across multiple layers, we proved that pre-trained GPT MoE
models implicitly exhibit a strong inter-layer expert affinity. We then design
an efficient integer programming model to capture such features and show that
by properly placing the experts on corresponding GPUs, we can reduce up to 67%
cross-GPU routing latency. Our solution beats the cutting-edge MoE
implementations with experts from 8 to 64, with up to 2.2x improvement in
inference throughput. We further provide a detailed study of how the model
implicitly acquires this expert affinity at the very early training stage and
how this affinity evolves and stabilizes during training.
\\ ( https://arxiv.org/abs/2401.08383 ,  21624kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08386
Date: Tue, 16 Jan 2024 14:19:28 GMT   (760kb,D)

Title: Deep Learning-based Group Causal Inference in Multivariate Time-series
Authors: Wasim Ahmad, Maha Shadaydeh, Joachim Denzler
Categories: cs.LG cs.AI
Comments: Accepted in AAAI24 (AI4TS)
\\
  Causal inference in a nonlinear system of multivariate timeseries is
instrumental in disentangling the intricate web of relationships among
variables, enabling us to make more accurate predictions and gain deeper
insights into real-world complex systems. Causality methods typically identify
the causal structure of a multivariate system by considering the cause-effect
relationship of each pair of variables while ignoring the collective effect of
a group of variables or interactions involving more than two-time series
variables. In this work, we test model invariance by group-level interventions
on the trained deep networks to infer causal direction in groups of variables,
such as climate and ecosystem, brain networks, etc. Extensive testing with
synthetic and real-world time series data shows a significant improvement of
our method over other applied group causality methods and provides us insights
into real-world time series. The code for our method can be found
at:https://github.com/wasimahmadpk/gCause.
\\ ( https://arxiv.org/abs/2401.08386 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08426
Date: Tue, 16 Jan 2024 15:11:29 GMT   (2076kb,D)

Title: Three ways that non-differentiability affects neural network training
Authors: Siddharth Krishna Kumar
Categories: cs.LG cs.CV
\\
  This paper investigates how non-differentiability affects three different
aspects of the neural network training process. We first analyze fully
connected neural networks with ReLU activations, for which we show that the
continuously differentiable neural networks converge faster than
non-differentiable neural networks. Next, we analyze the problem of $L_{1}$
regularization and show that the solutions produced by deep learning solvers
are incorrect and counter-intuitive even for the $L_{1}$ penalized linear
model. Finally, we analyze the Edge of Stability problem, where we show that
all convex, non-smooth, Lipschitz continuous functions display unstable
convergence, and provide an example of a result derived using twice
differentiable functions which fails in the once differentiable setting. More
generally, our results suggest that accounting for the non-linearity of neural
networks in the training process is essential for us to develop better
algorithms, and to get a better understanding of the training process in
general.
\\ ( https://arxiv.org/abs/2401.08426 ,  2076kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08464
Date: Tue, 16 Jan 2024 16:16:42 GMT   (3492kb,D)

Title: Enhancing Evolving Domain Generalization through Dynamic Latent
  Representations
Authors: Binghui Xie, Yongqiang Chen, Jiaqi Wang, Kaiwen Zhou, Bo Han, Wei
  Meng, James Cheng
Categories: cs.LG
Comments: Accepted By AAAI 2024
\\
  Domain generalization is a critical challenge for machine learning systems.
Prior domain generalization methods focus on extracting domain-invariant
features across several stationary domains to enable generalization to new
domains. However, in non-stationary tasks where new domains evolve in an
underlying continuous structure, such as time, merely extracting the invariant
features is insufficient for generalization to the evolving new domains.
Nevertheless, it is non-trivial to learn both evolving and invariant features
within a single model due to their conflicts. To bridge this gap, we build
causal models to characterize the distribution shifts concerning the two
patterns, and propose to learn both dynamic and invariant features via a new
framework called Mutual Information-Based Sequential Autoencoders (MISTS).
MISTS adopts information theoretic constraints onto sequential autoencoders to
disentangle the dynamic and invariant features, and leverage a domain adaptive
classifier to make predictions based on both evolving and invariant
information. Our experimental results on both synthetic and real-world datasets
demonstrate that MISTS succeeds in capturing both evolving and invariant
information, and present promising results in evolving domain generalization
tasks.
\\ ( https://arxiv.org/abs/2401.08464 ,  3492kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08478
Date: Tue, 16 Jan 2024 16:28:32 GMT   (25268kb,D)

Title: Solving Continual Offline Reinforcement Learning with Decision
  Transformer
Authors: Kaixin Huang, Li Shen, Chen Zhao, Chun Yuan, Dacheng Tao
Categories: cs.LG cs.AI
Comments: 11 pages, 6 figures
\\
  Continuous offline reinforcement learning (CORL) combines continuous and
offline reinforcement learning, enabling agents to learn multiple tasks from
static datasets without forgetting prior tasks. However, CORL faces challenges
in balancing stability and plasticity. Existing methods, employing Actor-Critic
structures and experience replay (ER), suffer from distribution shifts, low
efficiency, and weak knowledge-sharing. We aim to investigate whether Decision
Transformer (DT), another offline RL paradigm, can serve as a more suitable
offline continuous learner to address these issues. We first compare AC-based
offline algorithms with DT in the CORL framework. DT offers advantages in
learning efficiency, distribution shift mitigation, and zero-shot
generalization but exacerbates the forgetting problem during supervised
parameter updates. We introduce multi-head DT (MH-DT) and low-rank adaptation
DT (LoRA-DT) to mitigate DT's forgetting problem. MH-DT stores task-specific
knowledge using multiple heads, facilitating knowledge sharing with common
components. It employs distillation and selective rehearsal to enhance current
task learning when a replay buffer is available. In buffer-unavailable
scenarios, LoRA-DT merges less influential weights and fine-tunes DT's decisive
MLP layer to adapt to the current task. Extensive experiments on MoJuCo and
Meta-World benchmarks demonstrate that our methods outperform SOTA CORL
baselines and showcase enhanced learning capabilities and superior memory
efficiency.
\\ ( https://arxiv.org/abs/2401.08478 ,  25268kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08500
Date: Tue, 16 Jan 2024 17:00:36 GMT   (358kb,D)

Title: Code Generation with AlphaCodium: From Prompt Engineering to Flow
  Engineering
Authors: Tal Ridnik, Dedy Kredo, Itamar Friedman
Categories: cs.LG cs.CL cs.SE
\\
  Code generation problems differ from common natural language problems - they
require matching the exact syntax of the target language, identifying happy
paths and edge cases, paying attention to numerous small details in the problem
spec, and addressing other code-specific issues and requirements. Hence, many
of the optimizations and tricks that have been successful in natural language
generation may not be effective for code tasks. In this work, we propose a new
approach to code generation by LLMs, which we call AlphaCodium - a test-based,
multi-stage, code-oriented iterative flow, that improves the performances of
LLMs on code problems. We tested AlphaCodium on a challenging code generation
dataset called CodeContests, which includes competitive programming problems
from platforms such as Codeforces. The proposed flow consistently and
significantly improves results. On the validation set, for example, GPT-4
accuracy (pass@5) increased from 19% with a single well-designed direct prompt
to 44% with the AlphaCodium flow. Many of the principles and best practices
acquired in this work, we believe, are broadly applicable to general code
generation tasks. Full implementation is available at:
https://github.com/Codium-ai/AlphaCodium
\\ ( https://arxiv.org/abs/2401.08500 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08505
Date: Tue, 16 Jan 2024 17:07:22 GMT   (1844kb,D)

Title: Harnessing Orthogonality to Train Low-Rank Neural Networks
Authors: Daniel Coquelin, Katharina Fl\"ugel, Marie Weiel, Nicholas Kiefer,
  Charlotte Debus, Achim Streit and Markus G\"otz
Categories: cs.LG cs.AI
\\
  This study explores the learning dynamics of neural networks by analyzing the
singular value decomposition (SVD) of their weights throughout training. Our
investigation reveals that an orthogonal basis within each multidimensional
weight's SVD representation stabilizes during training. Building upon this, we
introduce Orthogonality-Informed Adaptive Low-Rank (OIALR) training, a novel
training method exploiting the intrinsic orthogonality of neural networks.
OIALR seamlessly integrates into existing training workflows with minimal
accuracy loss, as demonstrated by benchmarking on various datasets and
well-established network architectures. With appropriate hyperparameter tuning,
OIALR can surpass conventional training setups, including those of
state-of-the-art models.
\\ ( https://arxiv.org/abs/2401.08505 ,  1844kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08513
Date: Tue, 16 Jan 2024 17:21:33 GMT   (4035kb,D)

Title: X Hacking: The Threat of Misguided AutoML
Authors: Rahul Sharma, Sergey Redyuk, Sumantrak Mukherjee, Andrea Sipka,
  Sebastian Vollmer, David Selby
Categories: cs.LG cs.CR
Comments: 13 pages, 8 figures, plus supplementary materials
\\
  Explainable AI (XAI) and interpretable machine learning methods help to build
trust in model predictions and derived insights, yet also present a perverse
incentive for analysts to manipulate XAI metrics to support pre-specified
conclusions. This paper introduces the concept of X-hacking, a form of
p-hacking applied to XAI metrics such as Shap values. We show how an automated
machine learning pipeline can be used to search for 'defensible' models that
produce a desired explanation while maintaining superior predictive performance
to a common baseline. We formulate the trade-off between explanation and
accuracy as a multi-objective optimization problem and illustrate the
feasibility and severity of X-hacking empirically on familiar real-world
datasets. Finally, we suggest possible methods for detection and prevention,
and discuss ethical implications for the credibility and reproducibility of XAI
research.
\\ ( https://arxiv.org/abs/2401.08513 ,  4035kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08514
Date: Tue, 16 Jan 2024 17:23:23 GMT   (2608kb,D)

Title: Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN
  Expressiveness
Authors: Bohang Zhang, Jingchu Gai, Yiheng Du, Qiwei Ye, Di He, Liwei Wang
Categories: cs.LG cs.DM cs.DS math.CO
Comments: 73 pages, 9 figures, 9 tables; Extended from ICLR 2024 (Oral
  Presentation). This version polishes all proofs for better readability
\\
  Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in
the graph learning community. So far, GNN expressiveness has been primarily
assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an
expressivity measure has notable limitations: it is inherently coarse,
qualitative, and may not well reflect practical requirements (e.g., the ability
to encode substructures). In this paper, we introduce a unified framework for
quantitatively studying the expressiveness of GNN architectures, addressing all
the above limitations. Specifically, we identify a fundamental expressivity
measure termed homomorphism expressivity, which quantifies the ability of GNN
models to count graphs under homomorphism. Homomorphism expressivity offers a
complete and practical assessment tool: the completeness enables direct
expressivity comparisons between GNN models, while the practicality allows for
understanding concrete GNN abilities such as subgraph counting. By examining
four classes of prominent GNNs as case studies, we derive simple, unified, and
elegant descriptions of their homomorphism expressivity for both invariant and
equivariant settings. Our results provide novel insights into a series of
previous work, unify the landscape of different subareas in the community, and
settle several open questions. Empirically, extensive experiments on both
synthetic and real-world tasks verify our theory, showing that the practical
performance of GNN models aligns well with the proposed metric.
\\ ( https://arxiv.org/abs/2401.08514 ,  2608kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08519
Date: Tue, 16 Jan 2024 17:31:54 GMT   (2304kb,D)

Title: From Graphs to Hypergraphs: Hypergraph Projection and its Remediation
Authors: Yanbang Wang, Jon Kleinberg
Categories: cs.LG cs.IR cs.SI
Comments: Accepted at ICLR 2024
\\
  We study the implications of the modeling choice to use a graph, instead of a
hypergraph, to represent real-world interconnected systems whose constituent
relationships are of higher order by nature. Such a modeling choice typically
involves an underlying projection process that maps the original hypergraph
onto a graph, and is common in graph-based analysis. While hypergraph
projection can potentially lead to loss of higher-order relations, there exists
very limited studies on the consequences of doing so, as well as its
remediation. This work fills this gap by doing two things: (1) we develop
analysis based on graph and set theory, showing two ubiquitous patterns of
hyperedges that are root to structural information loss in all hypergraph
projections; we also quantify the combinatorial impossibility of recovering the
lost higher-order structures if no extra help is provided; (2) we still seek to
recover the lost higher-order structures in hypergraph projection, and in light
of (1)'s findings we propose to relax the problem into a learning-based
setting. Under this setting, we develop a learning-based hypergraph
reconstruction method based on an important statistic of hyperedge
distributions that we find. Our reconstruction method is evaluated on 8
real-world datasets under different settings, and exhibits consistently good
performance. We also demonstrate benefits of the reconstructed hypergraphs via
use cases of protein rankings and link predictions.
\\ ( https://arxiv.org/abs/2401.08519 ,  2304kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08534
Date: Tue, 16 Jan 2024 17:54:02 GMT   (34624kb,D)

Title: DiConStruct: Causal Concept-based Explanations through Black-Box
  Distillation
Authors: Ricardo Moreira, Jacopo Bono, M\'ario Cardoso, Pedro Saleiro, M\'ario
  A. T. Figueiredo, Pedro Bizarro
Categories: cs.LG cs.AI cs.HC
Comments: Accepted at Conference on Causal Learning and Reasoning (CLeaR 2024,
  https://www.cclear.cc/2024). To be published at Proceedings of Machine
  Learning Research (PMLR)
\\
  Model interpretability plays a central role in human-AI decision-making
systems. Ideally, explanations should be expressed using human-interpretable
semantic concepts. Moreover, the causal relations between these concepts should
be captured by the explainer to allow for reasoning about the explanations.
Lastly, explanation methods should be efficient and not compromise the
performance of the predictive task. Despite the rapid advances in AI
explainability in recent years, as far as we know to date, no method fulfills
these three properties. Indeed, mainstream methods for local concept
explainability do not produce causal explanations and incur a trade-off between
explainability and prediction performance. We present DiConStruct, an
explanation method that is both concept-based and causal, with the goal of
creating more interpretable local explanations in the form of structural causal
models and concept attributions. Our explainer works as a distillation model to
any black-box machine learning model by approximating its predictions while
producing the respective explanations. Because of this, DiConStruct generates
explanations efficiently while not impacting the black-box prediction task. We
validate our method on an image dataset and a tabular dataset, showing that
DiConStruct approximates the black-box models with higher fidelity than other
concept explainability baselines, while providing explanations that include the
causal relations between the concepts.
\\ ( https://arxiv.org/abs/2401.08534 ,  34624kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08552
Date: Tue, 16 Jan 2024 18:27:37 GMT   (707kb,D)

Title: Explaining Time Series via Contrastive and Locally Sparse Perturbations
Authors: Zichuan Liu, Yingying Zhang, Tianchun Wang, Zefan Wang, Dongsheng Luo,
  Mengnan Du, Min Wu, Yi Wang, Chunlin Chen, Lunting Fan, Qingsong Wen
Categories: cs.LG cs.AI
\\
  Explaining multivariate time series is a compound challenge, as it requires
identifying important locations in the time series and matching complex
temporal patterns. Although previous saliency-based methods addressed the
challenges, their perturbation may not alleviate the distribution shift issue,
which is inevitable especially in heterogeneous samples. We present ContraLSP,
a locally sparse model that introduces counterfactual samples to build
uninformative perturbations but keeps distribution using contrastive learning.
Furthermore, we incorporate sample-specific sparse gates to generate more
binary-skewed and smooth masks, which easily integrate temporal trends and
select the salient features parsimoniously. Empirical studies on both synthetic
and real-world datasets show that ContraLSP outperforms state-of-the-art
models, demonstrating a substantial improvement in explanation quality for time
series data. The code is available for review:
https://anonymous.4open.science/r/ContraLSP-1146/
\\ ( https://arxiv.org/abs/2401.08552 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08567
Date: Tue, 16 Jan 2024 18:52:27 GMT   (7208kb,D)

Title: Connect, Collapse, Corrupt: Learning Cross-Modal Tasks with Uni-Modal
  Data
Authors: Yuhui Zhang, Elaine Sui, Serena Yeung-Levy
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: Published at ICLR 2024
\\
  Building cross-modal applications is challenging due to limited paired
multi-modal data. Recent works have shown that leveraging a pre-trained
multi-modal contrastive representation space enables cross-modal tasks to be
learned from uni-modal data. This is based on the assumption that contrastive
optimization makes embeddings from different modalities interchangeable.
However, this assumption is under-explored due to the poorly understood
geometry of the multi-modal contrastive space, where a modality gap exists. In
our study, we provide a theoretical explanation of this space's geometry and
introduce a three-step method, $C^3$ (Connect, Collapse, Corrupt), to bridge
the modality gap, enhancing the interchangeability of embeddings. Our $C^3$
method significantly improves cross-modal learning from uni-modal data,
achieving state-of-the-art results on zero-shot image / audio / video
captioning and text-to-image generation.
\\ ( https://arxiv.org/abs/2401.08567 ,  7208kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.06777 (*cross-listing*)
Date: Thu, 21 Dec 2023 00:56:51 GMT   (422kb,D)

Title: Multimodal Neuroimaging Attention-Based architecture for Cognitive
  Decline Prediction
Authors: Jamie Vo, Naeha Sharif and Ghulam Mubashar Hassan
Categories: eess.IV cs.AI
\\
  The early detection of Alzheimer's Disease is imperative to ensure early
treatment and improve patient outcomes. There has consequently been extenstive
research into detecting AD and its intermediate phase, mild cognitive
impairment (MCI). However, there is very small literature in predicting the
conversion to AD and MCI from normal cognitive condition. Recently, multiple
studies have applied convolutional neural networks (CNN) which integrate
Magnetic Resonance Imaging (MRI) and Positron Emission Tomography (PET) to
classify MCI and AD. However, in these works, the fusion of MRI and PET
features are simply achieved through concatenation, resulting in a lack of
cross-modal interactions. In this paper, we propose a novel multimodal
neuroimaging attention-based CNN architecture, MNA-net, to predict whether
cognitively normal (CN) individuals will develop MCI or AD within a period of
10 years. To address the lack of interactions across neuroimaging modalities
seen in previous works, MNA-net utilises attention mechanisms to form shared
representations of the MRI and PET images. The proposed MNA-net is tested in
OASIS-3 dataset and is able to predict CN individuals who converted to MCI or
AD with an accuracy of 83%, true negative rate of 80%, and true positive rate
of 86%. The new state of the art results improved by 5% and 10% for accuracy
and true negative rate by the use of attention mechanism. These results
demonstrate the potential of the proposed model to predict cognitive impairment
and attention based mechanisms in the fusion of different neuroimaging
modalities to improve the prediction of cognitive decline.
\\ ( https://arxiv.org/abs/2401.06777 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06780 (*cross-listing*)
Date: Tue, 2 Jan 2024 12:46:02 GMT   (3062kb,D)

Title: HA-HI: Synergising fMRI and DTI through Hierarchical Alignments and
  Hierarchical Interactions for Mild Cognitive Impairment Diagnosis
Authors: Xiongri Shen, Zhenxi Song, Linling Li, Min Zhang, Lingyan Liang
  Honghai Liu, Demao Deng, Zhiguo Zhang
Categories: eess.IV cs.AI cs.CV
\\
  Early diagnosis of mild cognitive impairment (MCI) and subjective cognitive
decline (SCD) utilizing multi-modal magnetic resonance imaging (MRI) is a
pivotal area of research. While various regional and connectivity features from
functional MRI (fMRI) and diffusion tensor imaging (DTI) have been employed to
develop diagnosis models, most studies integrate these features without
adequately addressing their alignment and interactions. This limits the
potential to fully exploit the synergistic contributions of combined features
and modalities. To solve this gap, our study introduces a novel Hierarchical
Alignments and Hierarchical Interactions (HA-HI) method for MCI and SCD
classification, leveraging the combined strengths of fMRI and DTI. HA-HI
efficiently learns significant MCI- or SCD- related regional and connectivity
features by aligning various feature types and hierarchically maximizing their
interactions. Furthermore, to enhance the interpretability of our approach, we
have developed the Synergistic Activation Map (SAM) technique, revealing the
critical brain regions and connections that are indicative of MCI/SCD.
Comprehensive evaluations on the ADNI dataset and our self-collected data
demonstrate that HA-HI outperforms other existing methods in diagnosing MCI and
SCD, making it a potentially vital and interpretable tool for early detection.
The implementation of this method is publicly accessible at
https://github.com/ICI-BCI/Dual-MRI-HA-HI.git.
\\ ( https://arxiv.org/abs/2401.06780 ,  3062kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06786 (*cross-listing*)
Date: Fri, 10 Nov 2023 01:49:57 GMT   (1458kb,D)

Title: CloudEval-YAML: A Practical Benchmark for Cloud Configuration Generation
Authors: Yifei Xu, Yuning Chen, Xumiao Zhang, Xianshang Lin, Pan Hu, Yunfei Ma,
  Songwu Lu, Wan Du, Zhuoqing Mao, Ennan Zhai, Dennis Cai
Categories: cs.DC cs.AI
\\
  Among the thriving ecosystem of cloud computing and the proliferation of
Large Language Model (LLM)-based code generation tools, there is a lack of
benchmarking for code generation in cloud-native applications. In response to
this need, we present CloudEval-YAML, a practical benchmark for cloud
configuration generation. CloudEval-YAML tackles the diversity challenge by
focusing on YAML, the de facto standard of numerous cloud-native tools. We
develop the CloudEval-YAML benchmark with practicality in mind: the dataset
consists of hand-written problems with unit tests targeting practical
scenarios. We further enhanced the dataset to meet practical needs by
rephrasing questions in a concise, abbreviated, and bilingual manner. The
dataset consists of 1011 problems that take more than 1200 human hours to
complete. To improve practicality during evaluation, we build a scalable
evaluation platform for CloudEval-YAML that achieves a 20 times speedup over a
single machine. To the best of our knowledge, the CloudEval-YAML dataset is the
first hand-written dataset targeting cloud-native applications. We present an
in-depth evaluation of 12 LLMs, leading to a deeper understanding of the
problems and LLMs, as well as effective methods to improve task performance and
reduce cost.
\\ ( https://arxiv.org/abs/2401.06786 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06788 (*cross-listing*)
Date: Sun, 7 Jan 2024 14:20:52 GMT   (81kb,D)

Title: The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in
  CNVSRC 2023
Authors: He Wang, Pengcheng Guo, Wei Chen, Pan Zhou, Lei Xie
Categories: eess.AS cs.AI cs.SD
Comments: 2 pages
\\
  This paper delineates the visual speech recognition (VSR) system introduced
by the NPU-ASLP-LiAuto (Team 237) in the first Chinese Continuous Visual Speech
Recognition Challenge (CNVSRC) 2023, engaging in the fixed and open tracks of
Single-Speaker VSR Task, and the open track of Multi-Speaker VSR Task. In terms
of data processing, we leverage the lip motion extractor from the baseline1 to
produce multi-scale video data. Besides, various augmentation techniques are
applied during training, encompassing speed perturbation, random rotation,
horizontal flipping, and color transformation. The VSR model adopts an
end-to-end architecture with joint CTC/attention loss, comprising a ResNet3D
visual frontend, an E-Branchformer encoder, and a Transformer decoder.
Experiments show that our system achieves 34.76% CER for the Single-Speaker
Task and 41.06% CER for the Multi-Speaker Task after multi-system fusion,
ranking first place in all three tracks we participate.
\\ ( https://arxiv.org/abs/2401.06788 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06789 (*cross-listing*)
Date: Sun, 7 Jan 2024 16:35:30 GMT   (643kb)

Title: Information Retrieval and Classification of Real-Time Multi-Source
  Hurricane Evacuation Notices
Authors: Tingting Zhao, Shubo Tian, Jordan Daly, Melissa Geiger, Minna Jia,
  Jinfeng Zhang
Categories: cs.IR cs.AI cs.CL cs.LG
\\
  For an approaching disaster, the tracking of time-sensitive critical
information such as hurricane evacuation notices is challenging in the United
States. These notices are issued and distributed rapidly by numerous local
authorities that may spread across multiple states. They often undergo frequent
updates and are distributed through diverse online portals lacking standard
formats. In this study, we developed an approach to timely detect and track the
locally issued hurricane evacuation notices. The text data were collected
mainly with a spatially targeted web scraping method. They were manually
labeled and then classified using natural language processing techniques with
deep learning models. The classification of mandatory evacuation notices
achieved a high accuracy (recall = 96%). We used Hurricane Ian (2022) to
illustrate how real-time evacuation notices extracted from local government
sources could be redistributed with a Web GIS system. Our method applied to
future hurricanes provides live data for situation awareness to higher-level
government agencies and news media. The archived data helps scholars to study
government responses toward weather warnings and individual behaviors
influenced by evacuation history. The framework may be applied to other types
of disasters for rapid and targeted retrieval, classification, redistribution,
and archiving of real-time government orders and notifications.
\\ ( https://arxiv.org/abs/2401.06789 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06791 (*cross-listing*)
Date: Mon, 8 Jan 2024 03:35:02 GMT   (361kb,D)

Title: A Span-based Model for Extracting Overlapping PICO Entities from RCT
  Publications
Authors: Gongbo Zhang, Yiliang Zhou, Yan Hu, Hua Xu, Chunhua Weng, Yifan Peng
Categories: cs.IR cs.AI cs.CL
\\
  Objectives Extraction of PICO (Populations, Interventions, Comparison, and
Outcomes) entities is fundamental to evidence retrieval. We present a novel
method PICOX to extract overlapping PICO entities.
  Materials and Methods PICOX first identifies entities by assessing whether a
word marks the beginning or conclusion of an entity. Then it uses a multi-label
classifier to assign one or more PICO labels to a span candidate. PICOX was
evaluated using one of the best-performing baselines, EBM-NLP, and three more
datasets, i.e., PICO-Corpus, and RCT publications on Alzheimer's Disease or
COVID-19, using entity-level precision, recall, and F1 scores.
  Results PICOX achieved superior precision, recall, and F1 scores across the
board, with the micro F1 score improving from 45.05 to 50.87 (p << 0.01). On
the PICO-Corpus, PICOX obtained higher recall and F1 scores than the baseline
and improved the micro recall score from 56.66 to 67.33. On the COVID-19
dataset, PICOX also outperformed the baseline and improved the micro F1 score
from 77.10 to 80.32. On the AD dataset, PICOX demonstrated comparable F1 scores
with higher precision when compared to the baseline.
  Conclusion PICOX excels in identifying overlapping entities and consistently
surpasses a leading baseline across multiple datasets. Ablation studies reveal
that its data augmentation strategy effectively minimizes false positives and
improves precision.
\\ ( https://arxiv.org/abs/2401.06791 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06811 (*cross-listing*)
Date: Thu, 11 Jan 2024 06:09:15 GMT   (149kb,D)

Title: UniRQR: A Unified Model for Retrieval Decision, Query, and Response
  Generation in Internet-Based Knowledge Dialogue Systems
Authors: Zhongtian Hu, Yangqi Chen, Meng Zhao, Ronghan Li, Lifang Wang
Categories: cs.IR cs.AI cs.CL
\\
  Knowledge-based dialogue systems with internet retrieval have recently
attracted considerable attention from researchers. The dialogue systems
overcome a major limitation of traditional knowledge dialogue systems, where
the timeliness of knowledge cannot be assured, hence providing greater
practical application value. Knowledge-based dialogue systems with internet
retrieval can be typically segmented into three tasks: Retrieval Decision,
Query Generation, and Response Generation. However, many of studies assumed
that all conversations require external knowledge to continue, neglecting the
critical step of determining when retrieval is necessary. This assumption often
leads to an over-dependence on external knowledge, even when it may not be
required. Our work addresses this oversight by employing a single unified model
facilitated by prompt and multi-task learning approaches. This model not only
decides whether retrieval is necessary but also generates retrieval queries and
responses. By integrating these functions, our system leverages the full
potential of pre-trained models and reduces the complexity and costs associated
with deploying multiple models. We conducted extensive experiments to
investigate the mutual enhancement among the three tasks in our system. What is
more, the experiment results on the Wizint and Dusinc datasets not only
demonstrate that our unified model surpasses the baseline performance for
individual tasks, but also reveal that it achieves comparable results when
contrasted with SOTA systems that deploy separate, specialized models for each
task.
\\ ( https://arxiv.org/abs/2401.06811 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06827 (*cross-listing*)
Date: Fri, 12 Jan 2024 04:54:01 GMT   (437kb,D)

Title: APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning
Authors: Guiming Cao, Kaize Shi, Hong Fu, Huaiwen Zhang and Guandong Xu
Categories: cs.CV cs.AI cs.CL
Comments: 7 pages,3 figures
\\
  Pre-trained Vision-Language (V-L) models set the benchmark for generalization
to downstream tasks among the noteworthy contenders. Many characteristics of
the V-L model have been explored in existing research including the challenge
of the sensitivity to text input and the tuning process across multi-modal
prompts. With the advanced utilization of the V-L model like CLIP, recent
approaches deploy learnable prompts instead of hand-craft prompts to boost the
generalization performance and address the aforementioned challenges. Inspired
by layer-wise training, which is wildly used in image fusion, we note that
using a sequential training process to adapt different modalities branches of
CLIP efficiently facilitates the improvement of generalization. In the context
of addressing the multi-modal prompting challenge, we propose Token-wise
Adaptive for Multi-modal Prompt Learning (APLe) for tuning both modalities
prompts, vision and language, as tokens in a sequential manner. APLe addresses
the challenges in V-L models to promote prompt learning across both modalities,
which indicates a competitive generalization performance in line with the
state-of-the-art. Preeminently, APLe shows robustness and favourable
performance in prompt-length experiments with an absolute advantage in adopting
the V-L models.
\\ ( https://arxiv.org/abs/2401.06827 ,  437kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06830 (*cross-listing*)
Date: Fri, 12 Jan 2024 10:14:10 GMT   (63kb,D)

Title: RecSys Challenge 2023: From data preparation to prediction, a simple,
  efficient, robust and scalable solution
Authors: Maxime Manderlier and Fabian Lecron
Categories: cs.IR cs.AI cs.LG
\\
  The RecSys Challenge 2023, presented by ShareChat, consists to predict if an
user will install an application on his smartphone after having seen
advertising impressions in ShareChat & Moj apps. This paper presents the
solution of 'Team UMONS' to this challenge, giving accurate results (our best
score is 6.622686) with a relatively small model that can be easily implemented
in different production configurations. Our solution scales well when
increasing the dataset size and can be used with datasets containing missing
values.
\\ ( https://arxiv.org/abs/2401.06830 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06833 (*cross-listing*)
Date: Fri, 12 Jan 2024 15:25:51 GMT   (1613kb,D)

Title: A hierarchical control framework for autonomous decision-making systems:
  Integrating HMDP and MPC
Authors: Xue-Fang Wang, Jingjing Jiang, Wen-Hua Chen
Categories: cs.SY cs.AI cs.RO eess.SY
Comments: 11 pages, 14 figures, submitted to Automatica
\\
  This paper proposes a comprehensive hierarchical control framework for
autonomous decision-making arising in robotics and autonomous systems. In a
typical hierarchical control architecture, high-level decision making is often
characterised by discrete state and decision/control sets. However, a rational
decision is usually affected by not only the discrete states of the autonomous
system, but also the underlying continuous dynamics even the evolution of its
operational environment. This paper proposes a holistic and comprehensive
design process and framework for this type of challenging problems, from new
modelling and design problem formulation to control design and stability
analysis. It addresses the intricate interplay between traditional continuous
systems dynamics utilized at the low levels for control design and discrete
Markov decision processes (MDP) for facilitating high-level decision making. We
model the decision making system in complex environments as a hybrid system
consisting of a controlled MDP and autonomous (i.e. uncontrolled) continuous
dynamics. Consequently, the new formulation is called as hybrid Markov decision
process (HMDP). The design problem is formulated with a focus on ensuring both
safety and optimality while taking into account the influence of both the
discrete and continuous state variables of different levels. With the help of
the model predictive control (MPC) concept, a decision maker design scheme is
proposed for the proposed hybrid decision making model. By carefully designing
key ingredients involved in this scheme, it is shown that the recursive
feasibility and stability of the proposed autonomous decision making scheme are
guaranteed. The proposed framework is applied to develop an autonomous lane
changing system for intelligent vehicles.
\\ ( https://arxiv.org/abs/2401.06833 ,  1613kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06883 (*cross-listing*)
Date: Fri, 12 Jan 2024 20:27:55 GMT   (666kb)

Title: Scaling While Privacy Preserving: A Comprehensive Synthetic Tabular Data
  Generation and Evaluation in Learning Analytics
Authors: Qinyi Liu, Mohammad Khalil, Ronas Shakya, and Jelena Jovanovic
Categories: cs.CR cs.AI
\\
  Privacy poses a significant obstacle to the progress of learning analytics
(LA), presenting challenges like inadequate anonymization and data misuse that
current solutions struggle to address. Synthetic data emerges as a potential
remedy, offering robust privacy protection. However, prior LA research on
synthetic data lacks thorough evaluation, essential for assessing the delicate
balance between privacy and data utility. Synthetic data must not only enhance
privacy but also remain practical for data analytics. Moreover, diverse LA
scenarios come with varying privacy and utility needs, making the selection of
an appropriate synthetic data approach a pressing challenge. To address these
gaps, we propose a comprehensive evaluation of synthetic data, which
encompasses three dimensions of synthetic data quality, namely resemblance,
utility, and privacy. We apply this evaluation to three distinct LA datasets,
using three different synthetic data generation methods. Our results show that
synthetic data can maintain similar utility (i.e., predictive performance) as
real data, while preserving privacy. Furthermore, considering different privacy
and data utility requirements in different LA scenarios, we make customized
recommendations for synthetic data generation. This paper not only presents a
comprehensive evaluation of synthetic data but also illustrates its potential
in mitigating privacy concerns within the field of LA, thus contributing to a
wider application of synthetic data in LA and promoting a better practice for
open science.
\\ ( https://arxiv.org/abs/2401.06883 ,  666kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06946 (*cross-listing*)
Date: Sat, 13 Jan 2024 01:22:20 GMT   (3185kb)

Title: 3D Object Detection and High-Resolution Traffic Parameters Extraction
  Using Low-Resolution LiDAR Data
Authors: Linlin Zhang, Xiang Yu, Armstrong Aboah, Yaw Adu-Gyamfi
Categories: cs.CV cs.AI cs.LG
Comments: 19 pages, 11 figures. This paper has been submitted for consideration
  for presentation at the 103rd Annual Meeting of the Transportation Research
  Board, January 2024
\\
  Traffic volume data collection is a crucial aspect of transportation
engineering and urban planning, as it provides vital insights into traffic
patterns, congestion, and infrastructure efficiency. Traditional manual methods
of traffic data collection are both time-consuming and costly. However, the
emergence of modern technologies, particularly Light Detection and Ranging
(LiDAR), has revolutionized the process by enabling efficient and accurate data
collection. Despite the benefits of using LiDAR for traffic data collection,
previous studies have identified two major limitations that have impeded its
widespread adoption. These are the need for multiple LiDAR systems to obtain
complete point cloud information of objects of interest, as well as the
labor-intensive process of annotating 3D bounding boxes for object detection
tasks. In response to these challenges, the current study proposes an
innovative framework that alleviates the need for multiple LiDAR systems and
simplifies the laborious 3D annotation process. To achieve this goal, the study
employed a single LiDAR system, that aims at reducing the data acquisition cost
and addressed its accompanying limitation of missing point cloud information by
developing a Point Cloud Completion (PCC) framework to fill in missing point
cloud information using point density. Furthermore, we also used zero-shot
learning techniques to detect vehicles and pedestrians, as well as proposed a
unique framework for extracting low to high features from the object of
interest, such as height, acceleration, and speed. Using the 2D bounding box
detection and extracted height information, this study is able to generate 3D
bounding boxes automatically without human intervention.
\\ ( https://arxiv.org/abs/2401.06946 ,  3185kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06949 (*cross-listing*)
Date: Sat, 13 Jan 2024 02:03:28 GMT   (12378kb,D)

Title: ORGANA: A Robotic Assistant for Automated Chemistry Experimentation and
  Characterization
Authors: Kourosh Darvish, Marta Skreta, Yuchi Zhao, Naruki Yoshikawa, Sagnik
  Som, Miroslav Bogdanovic, Yang Cao, Han Hao, Haoping Xu, Al\'an Aspuru-Guzik,
  Animesh Garg, Florian Shkurti
Categories: cs.RO cs.AI
\\
  Chemistry experimentation is often resource- and labor-intensive. Despite the
many benefits incurred by the integration of advanced and special-purpose lab
equipment, many aspects of experimentation are still manually conducted by
chemists, for example, polishing an electrode in electrochemistry experiments.
Traditional lab automation infrastructure faces challenges when it comes to
flexibly adapting to new chemistry experiments. To address this issue, we
propose a human-friendly and flexible robotic system, ORGANA, that automates a
diverse set of chemistry experiments. It is capable of interacting with
chemists in the lab through natural language, using Large Language Models
(LLMs). ORGANA keeps scientists informed by providing timely reports that
incorporate statistical analyses. Additionally, it actively engages with users
when necessary for disambiguation or troubleshooting. ORGANA can reason over
user input to derive experiment goals, and plan long sequences of both
high-level tasks and low-level robot actions while using feedback from the
visual perception of the environment. It also supports scheduling and parallel
execution for experiments that require resource allocation and coordination
between multiple robots and experiment stations. We show that ORGANA
successfully conducts a diverse set of chemistry experiments, including
solubility assessment, pH measurement, recrystallization, and electrochemistry
experiments. For the latter, we show that ORGANA robustly executes a
long-horizon plan, comprising 19 steps executed in parallel, to characterize
the electrochemical properties of quinone derivatives, a class of molecules
used in rechargeable flow batteries. Our user study indicates that ORGANA
significantly improves many aspects of user experience while reducing their
physical workload. More details about ORGANA can be found at
https://ac-rad.github.io/organa/.
\\ ( https://arxiv.org/abs/2401.06949 ,  12378kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06960 (*cross-listing*)
Date: Sat, 13 Jan 2024 03:17:57 GMT   (8588kb,D)

Title: Transformer for Object Re-Identification: A Survey
Authors: Mang Ye, Shuoyi Chen, Chenyue Li, Wei-Shi Zheng, David Crandall, Bo Du
Categories: cs.CV cs.AI
\\
  Object Re-Identification (Re-ID) aims to identify and retrieve specific
objects from varying viewpoints. For a prolonged period, this field has been
predominantly driven by deep convolutional neural networks. In recent years,
the Transformer has witnessed remarkable advancements in computer vision,
prompting an increasing body of research to delve into the application of
Transformer in Re-ID. This paper provides a comprehensive review and in-depth
analysis of the Transformer-based Re-ID. In categorizing existing works into
Image/Video-Based Re-ID, Re-ID with limited data/annotations, Cross-Modal
Re-ID, and Special Re-ID Scenarios, we thoroughly elucidate the advantages
demonstrated by the Transformer in addressing a multitude of challenges across
these domains. Considering the trending unsupervised Re-ID, we propose a new
Transformer baseline, UntransReID, achieving state-of-the-art performance on
both single-/cross modal tasks. Besides, this survey also covers a wide range
of Re-ID research objects, including progress in animal Re-ID. Given the
diversity of species in animal Re-ID, we devise a standardized experimental
benchmark and conduct extensive experiments to explore the applicability of
Transformer for this task to facilitate future research. Finally, we discuss
some important yet under-investigated open issues in the big foundation model
era, we believe it will serve as a new handbook for researchers in this field.
\\ ( https://arxiv.org/abs/2401.06960 ,  8588kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06977 (*cross-listing*)
Date: Sat, 13 Jan 2024 04:42:48 GMT   (131kb,D)

Title: Singing the Body Electric: The Impact of Robot Embodiment on User
  Expectations
Authors: Nathaniel Dennler, Stefanos Nikolaidis, Maja Matari\'c
Categories: cs.RO cs.AI cs.HC
Comments: Presented at the RSS Workshop on Social Intelligence in Humans and
  Robots, 2023
\\
  Users develop mental models of robots to conceptualize what kind of
interactions they can have with those robots. The conceptualizations are often
formed before interactions with the robot and are based only on observing the
robot's physical design. As a result, understanding conceptualizations formed
from physical design is necessary to understand how users intend to interact
with the robot. We propose to use multimodal features of robot embodiments to
predict what kinds of expectations users will have about a given robot's social
and physical capabilities. We show that using such features provides
information about general mental models of the robots that generalize across
socially interactive robots. We describe how these models can be incorporated
into interaction design and physical design for researchers working with
socially interactive robots.
\\ ( https://arxiv.org/abs/2401.06977 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06992 (*cross-listing*)
Date: Sat, 13 Jan 2024 06:34:32 GMT   (1334kb,D)

Title: Progressive Feature Fusion Network for Enhancing Image Quality
  Assessment
Authors: Kaiqun Wu, Xiaoling Jiang, Rui Yu, Yonggang Luo, Tian Jiang, Xi Wu,
  Peng Wei
Categories: cs.CV cs.AI
Comments: Data Compression Conference
\\
  Image compression has been applied in the fields of image storage and video
broadcasting. However, it's formidably tough to distinguish the subtle quality
differences between those distorted images generated by different algorithms.
In this paper, we propose a new image quality assessment framework to decide
which image is better in an image group. To capture the subtle differences, a
fine-grained network is adopted to acquire multi-scale features. Subsequently,
we design a cross subtract block for separating and gathering the information
within positive and negative image pairs. Enabling image comparison in feature
space. After that, a progressive feature fusion block is designed, which fuses
multi-scale features in a novel progressive way. Hierarchical spatial 2D
features can thus be processed gradually. Experimental results show that
compared with the current mainstream image quality assessment methods, the
proposed network can achieve more accurate image quality assessment and ranks
second in the benchmark of CLIC in the image perceptual model track.
\\ ( https://arxiv.org/abs/2401.06992 ,  1334kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07014 (*cross-listing*)
Date: Sat, 13 Jan 2024 08:45:41 GMT   (33894kb,D)

Title: Weak Labeling for Cropland Mapping in Africa
Authors: Gilles Quentin Hacheme, Akram Zaytar, Girmaw Abebe Tadesse, Caleb
  Robinson, Rahul Dodhia, Juan M. Lavista Ferres, Stephen Wood
Categories: cs.CV cs.AI
Comments: 5 pages
\\
  Cropland mapping can play a vital role in addressing environmental,
agricultural, and food security challenges. However, in the context of Africa,
practical applications are often hindered by the limited availability of
high-resolution cropland maps. Such maps typically require extensive human
labeling, thereby creating a scalability bottleneck. To address this, we
propose an approach that utilizes unsupervised object clustering to refine
existing weak labels, such as those obtained from global cropland maps. The
refined labels, in conjunction with sparse human annotations, serve as training
data for a semantic segmentation network designed to identify cropland areas.
We conduct experiments to demonstrate the benefits of the improved weak labels
generated by our method. In a scenario where we train our model with only 33
human-annotated labels, the F_1 score for the cropland category increases from
0.53 to 0.84 when we add the mined negative labels.
\\ ( https://arxiv.org/abs/2401.07014 ,  33894kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07031 (*cross-listing*)
Date: Sat, 13 Jan 2024 10:19:26 GMT   (2870kb,D)

Title: Code Security Vulnerability Repair Using Reinforcement Learning with
  Large Language Models
Authors: Nafis Tanveer Islam, Peyman Najafirad
Categories: cs.CR cs.AI cs.SE
\\
  With the recent advancement of Large Language Models (LLMs), generating
functionally correct code has become less complicated for a wide array of
developers. While using LLMs has sped up the functional development process, it
poses a heavy risk to code security. Code generation with proper security
measures using LLM is a significantly more challenging task than functional
code generation. Security measures may include adding a pair of lines of code
with the original code, consisting of null pointer checking or prepared
statements for SQL injection prevention. Currently, available code repair LLMs
generate code repair by supervised fine-tuning, where the model looks at
cross-entropy loss. However, the original and repaired codes are mostly similar
in functionality and syntactically, except for a few (1-2) lines, which act as
security measures. This imbalance between the lines needed for security
measures and the functional code enforces the supervised fine-tuned model to
prioritize generating functional code without adding proper security measures,
which also benefits the model by resulting in minimal loss. Therefore, in this
work, for security hardening and strengthening of generated code from LLMs, we
propose a reinforcement learning-based method for program-specific repair with
the combination of semantic and syntactic reward mechanisms that focus heavily
on adding security and functional measures in the code, respectively.
\\ ( https://arxiv.org/abs/2401.07031 ,  2870kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07042 (*cross-listing*)
Date: Sat, 13 Jan 2024 11:05:24 GMT   (328kb)

Title: GEML: A Grammar-based Evolutionary Machine Learning Approach for
  Design-Pattern Detection
Authors: Rafael Barbudo and Aurora Ram\'irez and Francisco Servant and Jos\'e
  Ra\'ul Romero
Categories: cs.SE cs.AI
Comments: 27 pages, 18 tables, 10 figures, journal paper
MSC-class: 68W50
ACM-class: D.2.7; I.2.8
Journal-ref: Journal of Systems and Software, Volume 175, May 2021, 110919
DOI: 10.1016/j.jss.2021.110919
\\
  Design patterns (DPs) are recognised as a good practice in software
development. However, the lack of appropriate documentation often hampers
traceability, and their benefits are blurred among thousands of lines of code.
Automatic methods for DP detection have become relevant but are usually based
on the rigid analysis of either software metrics or specific properties of the
source code. We propose GEML, a novel detection approach based on evolutionary
machine learning using software properties of diverse nature. Firstly, GEML
makes use of an evolutionary algorithm to extract those characteristics that
better describe the DP, formulated in terms of human-readable rules, whose
syntax is conformant with a context-free grammar. Secondly, a rule-based
classifier is built to predict whether new code contains a hidden DP
implementation. GEML has been validated over five DPs taken from a public
repository recurrently adopted by machine learning studies. Then, we increase
this number up to 15 diverse DPs, showing its effectiveness and robustness in
terms of detection capability. An initial parameter study served to tune a
parameter setup whose performance guarantees the general applicability of this
approach without the need to adjust complex parameters to a specific pattern.
Finally, a demonstration tool is also provided.
\\ ( https://arxiv.org/abs/2401.07042 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07043 (*cross-listing*)
Date: Sat, 13 Jan 2024 11:08:45 GMT   (2612kb,D)

Title: Quantum Advantage Actor-Critic for Reinforcement Learning
Authors: Michael K\"olle, Mohamad Hgog, Fabian Ritz, Philipp Altmann,
  Maximilian Zorn, Jonas Stein, Claudia Linnhoff-Popien
Categories: quant-ph cs.AI cs.LG
Comments: Accepted at ICAART 24
\\
  Quantum computing offers efficient encapsulation of high-dimensional states.
In this work, we propose a novel quantum reinforcement learning approach that
combines the Advantage Actor-Critic algorithm with variational quantum circuits
by substituting parts of the classical components. This approach addresses
reinforcement learning's scalability concerns while maintaining high
performance. We empirically test multiple quantum Advantage Actor-Critic
configurations with the well known Cart Pole environment to evaluate our
approach in control tasks with continuous state spaces. Our results indicate
that the hybrid strategy of using either a quantum actor or quantum critic with
classical post-processing yields a substantial performance increase compared to
pure classical and pure quantum variants with similar parameter counts. They
further reveal the limits of current quantum approaches due to the hardware
constraints of noisy intermediate-scale quantum computers, suggesting further
research to scale hybrid approaches for larger and more complex control tasks.
\\ ( https://arxiv.org/abs/2401.07043 ,  2612kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07054 (*cross-listing*)
Date: Sat, 13 Jan 2024 11:55:54 GMT   (2525kb,D)

Title: A Reinforcement Learning Environment for Directed Quantum Circuit
  Synthesis
Authors: Michael K\"olle, Tom Schubert, Philipp Altmann, Maximilian Zorn, Jonas
  Stein, Claudia Linnhoff-Popien
Categories: quant-ph cs.AI
\\
  With recent advancements in quantum computing technology, optimizing quantum
circuits and ensuring reliable quantum state preparation have become
increasingly vital. Traditional methods often demand extensive expertise and
manual calculations, posing challenges as quantum circuits grow in qubit- and
gate-count. Therefore, harnessing machine learning techniques to handle the
growing variety of gate-to-qubit combinations is a promising approach. In this
work, we introduce a comprehensive reinforcement learning environment for
quantum circuit synthesis, where circuits are constructed utilizing gates from
the the Clifford+T gate set to prepare specific target states. Our experiments
focus on exploring the relationship between the depth of synthesized quantum
circuits and the circuit depths used for target initialization, as well as
qubit count. We organize the environment configurations into multiple
evaluation levels and include a range of well-known quantum states for
benchmarking purposes. We also lay baselines for evaluating the environment
using Proximal Policy Optimization. By applying the trained agents to benchmark
tests, we demonstrated their ability to reliably design minimal quantum
circuits for a selection of 2-qubit Bell states.
\\ ( https://arxiv.org/abs/2401.07054 ,  2525kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07058 (*cross-listing*)
Date: Sat, 13 Jan 2024 12:19:01 GMT   (2697kb,D)

Title: Does More Advice Help? The Effects of Second Opinions in AI-Assisted
  Decision Making
Authors: Zhuoran Lu, Dakuo Wang, Ming Yin
Categories: cs.HC cs.AI
\\
  AI assistance in decision-making has become popular, yet people's
inappropriate reliance on AI often leads to unsatisfactory human-AI
collaboration performance. In this paper, through three pre-registered,
randomized human subject experiments, we explore whether and how the provision
of {second opinions} may affect decision-makers' behavior and performance in
AI-assisted decision-making. We find that if both the AI model's decision
recommendation and a second opinion are always presented together,
decision-makers reduce their over-reliance on AI while increase their
under-reliance on AI, regardless whether the second opinion is generated by a
peer or another AI model. However, if decision-makers have the control to
decide when to solicit a peer's second opinion, we find that their active
solicitations of second opinions have the potential to mitigate over-reliance
on AI without inducing increased under-reliance in some cases. We conclude by
discussing the implications of our findings for promoting effective human-AI
collaborations in decision-making.
\\ ( https://arxiv.org/abs/2401.07058 ,  2697kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07072 (*cross-listing*)
Date: Sat, 13 Jan 2024 13:14:29 GMT   (997kb,D)

Title: InterEvo-TR: Interactive Evolutionary Test Generation With Readability
  Assessment
Authors: Pedro Delgado-P\'erez and Aurora Ram\'irez and Kevin J. Valle-G\'omez
  and Inmaculada Medina-Bulo and Jos\'e Ra\'ul Romero
Categories: cs.SE cs.AI
Comments: 17 pages, 10 figures, 5 tables, journal paper
MSC-class: 68T20
ACM-class: D.2.5; I.2.8
Journal-ref: IEEE Transactions on Software Engineering (Volume: 49, Issue: 4,
  01 April 2023)
DOI: 10.1109/TSE.2022.3227418
\\
  Automated test case generation has proven to be useful to reduce the usually
high expenses of software testing. However, several studies have also noted the
skepticism of testers regarding the comprehension of generated test suites when
compared to manually designed ones. This fact suggests that involving testers
in the test generation process could be helpful to increase their acceptance of
automatically-produced test suites. In this paper, we propose incorporating
interactive readability assessments made by a tester into EvoSuite, a
widely-known evolutionary test generation tool. Our approach, InterEvo-TR,
interacts with the tester at different moments during the search and shows
different test cases covering the same coverage target for their subjective
evaluation. The design of such an interactive approach involves a schedule of
interaction, a method to diversify the selected targets, a plan to save and
handle the readability values, and some mechanisms to customize the level of
engagement in the revision, among other aspects. To analyze the potential and
practicability of our proposal, we conduct a controlled experiment in which 39
participants, including academics, professional developers, and student
collaborators, interact with InterEvo-TR. Our results show that the strategy to
select and present intermediate results is effective for the purpose of
readability assessment. Furthermore, the participants' actions and responses to
a questionnaire allowed us to analyze the aspects influencing test code
readability and the benefits and limitations of an interactive approach in the
context of test case generation, paving the way for future developments based
on interactivity.
\\ ( https://arxiv.org/abs/2401.07072 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07102 (*cross-listing*)
Date: Sat, 13 Jan 2024 15:57:54 GMT   (496kb,D)

Title: Evolving Code with A Large Language Model
Authors: Erik Hemberg, Stephen Moskal, Una-May O'Reilly
Categories: cs.NE cs.AI
Comments: 34 pages, 9 figures, 6 Tables
ACM-class: I.2.8
\\
  Algorithms that use Large Language Models (LLMs) to evolve code arrived on
the Genetic Programming (GP) scene very recently. We present LLM GP, a
formalized LLM-based evolutionary algorithm designed to evolve code. Like GP,
it uses evolutionary operators, but its designs and implementations of those
operators radically differ from GP's because they enlist an LLM, using
prompting and the LLM's pre-trained pattern matching and sequence completion
capability. We also present a demonstration-level variant of LLM GP and share
its code. By addressing algorithms that range from the formal to hands-on, we
cover design and LLM-usage considerations as well as the scientific challenges
that arise when using an LLM for genetic programming.
\\ ( https://arxiv.org/abs/2401.07102 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07118 (*cross-listing*)
Date: Sat, 13 Jan 2024 16:57:40 GMT   (1596kb,D)

Title: Exploring of Discrete and Continuous Input Control for AI-enhanced
  Assistive Robotic Arms
Authors: Max Pascher and Kevin Zinta and Jens Gerken
Categories: cs.HC cs.AI cs.RO
Comments: Companion of the 2024 ACM/IEEE International Conference on
  Human-Robot Interaction
DOI: 10.1145/3610978.3640626
\\
  Robotic arms, integral in domestic care for individuals with motor
impairments, enable them to perform Activities of Daily Living (ADLs)
independently, reducing dependence on human caregivers. These collaborative
robots require users to manage multiple Degrees-of-Freedom (DoFs) for tasks
like grasping and manipulating objects. Conventional input devices, typically
limited to two DoFs, necessitate frequent and complex mode switches to control
individual DoFs. Modern adaptive controls with feed-forward multi-modal
feedback reduce the overall task completion time, number of mode switches, and
cognitive load. Despite the variety of input devices available, their
effectiveness in adaptive settings with assistive robotics has yet to be
thoroughly assessed. This study explores three different input devices by
integrating them into an established XR framework for assistive robotics,
evaluating them and providing empirical insights through a preliminary study
for future developments.
\\ ( https://arxiv.org/abs/2401.07118 ,  1596kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07139 (*cross-listing*)
Date: Sat, 13 Jan 2024 18:56:18 GMT   (6727kb,D)

Title: Deep Blind Super-Resolution for Satellite Video
Authors: Yi Xiao and Qiangqiang Yuan and Qiang Zhang and Liangpei Zhang
Categories: cs.CV cs.AI eess.IV
Comments: Published in IEEE TGRS
Journal-ref: IEEE Transactions on Geoscience and Remote Sensing, vol. 61, pp.
  1-16, 2023, Art no. 5516316
DOI: 10.1109/TGRS.2023.3291822
\\
  Recent efforts have witnessed remarkable progress in Satellite Video
Super-Resolution (SVSR). However, most SVSR methods usually assume the
degradation is fixed and known, e.g., bicubic downsampling, which makes them
vulnerable in real-world scenes with multiple and unknown degradations. To
alleviate this issue, blind SR has thus become a research hotspot.
Nevertheless, existing approaches are mainly engaged in blur kernel estimation
while losing sight of another critical aspect for VSR tasks: temporal
compensation, especially compensating for blurry and smooth pixels with vital
sharpness from severely degraded satellite videos. Therefore, this paper
proposes a practical Blind SVSR algorithm (BSVSR) to explore more sharp cues by
considering the pixel-wise blur levels in a coarse-to-fine manner.
Specifically, we employed multi-scale deformable convolution to coarsely
aggregate the temporal redundancy into adjacent frames by window-slid
progressive fusion. Then the adjacent features are finely merged into
mid-feature using deformable attention, which measures the blur levels of
pixels and assigns more weights to the informative pixels, thus inspiring the
representation of sharpness. Moreover, we devise a pyramid spatial
transformation module to adjust the solution space of sharp mid-feature,
resulting in flexible feature adaptation in multi-level domains. Quantitative
and qualitative evaluations on both simulated and real-world satellite videos
demonstrate that our BSVSR performs favorably against state-of-the-art
non-blind and blind SR models. Code will be available at
https://github.com/XY-boy/Blind-Satellite-VSR
\\ ( https://arxiv.org/abs/2401.07139 ,  6727kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07179 (*cross-listing*)
Date: Sun, 14 Jan 2024 00:33:30 GMT   (306kb,D)

Title: Forecasting GDP in Europe with Textual Data
Authors: Luca Barbaglia, Sergio Consoli, Sebastiano Manzan
Categories: cs.CE cs.AI cs.CL
Comments: 34 pages, 6 figures, published in Journal of Applied Econometrics
  (Early view)
MSC-class: 91B62, 91B84, 91B86
DOI: 10.1002/jae.3027
\\
  We evaluate the informational content of news-based sentiment indicators for
forecasting Gross Domestic Product (GDP) and other macroeconomic variables of
the five major European economies. Our data set includes over 27 million
articles for 26 major newspapers in 5 different languages. The evidence
indicates that these sentiment indicators are significant predictors to
forecast macroeconomic variables and their predictive content is robust to
controlling for other indicators available to forecasters in real-time.
\\ ( https://arxiv.org/abs/2401.07179 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07220 (*cross-listing*)
Date: Sun, 14 Jan 2024 07:33:14 GMT   (6454kb)

Title: Application of 2D Homography for High Resolution Traffic Data Collection
  using CCTV Cameras
Authors: Linlin Zhang, Xiang Yu, Abdulateef Daud, Abdul Rashid Mussah, Yaw
  Adu-Gyamfi
Categories: cs.CV cs.AI
Comments: 25 pages, 9 figures, this paper was submitted for consideration for
  presentation at the 102nd Annual Meeting of the Transportation Research
  Board, January 2023
\\
  Traffic cameras remain the primary source data for surveillance activities
such as congestion and incident monitoring. To date, State agencies continue to
rely on manual effort to extract data from networked cameras due to limitations
of the current automatic vision systems including requirements for complex
camera calibration and inability to generate high resolution data. This study
implements a three-stage video analytics framework for extracting
high-resolution traffic data such vehicle counts, speed, and acceleration from
infrastructure-mounted CCTV cameras. The key components of the framework
include object recognition, perspective transformation, and vehicle trajectory
reconstruction for traffic data collection. First, a state-of-the-art vehicle
recognition model is implemented to detect and classify vehicles. Next, to
correct for camera distortion and reduce partial occlusion, an algorithm
inspired by two-point linear perspective is utilized to extracts the region of
interest (ROI) automatically, while a 2D homography technique transforms the
CCTV view to bird's-eye view (BEV). Cameras are calibrated with a two-layer
matrix system to enable the extraction of speed and acceleration by converting
image coordinates to real-world measurements. Individual vehicle trajectories
are constructed and compared in BEV using two time-space-feature-based object
trackers, namely Motpy and BYTETrack. The results of the current study showed
about +/- 4.5% error rate for directional traffic counts, less than 10% MSE for
speed bias between camera estimates in comparison to estimates from probe data
sources. Extracting high-resolution data from traffic cameras has several
implications, ranging from improvements in traffic management and identify
dangerous driving behavior, high-risk areas for accidents, and other safety
concerns, enabling proactive measures to reduce accidents and fatalities.
\\ ( https://arxiv.org/abs/2401.07220 ,  6454kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07271 (*cross-listing*)
Date: Sun, 14 Jan 2024 12:02:39 GMT   (5866kb,D)

Title: SpineCLUE: Automatic Vertebrae Identification Using Contrastive Learning
  and Uncertainty Estimation
Authors: Sheng Zhang, Minheng Chen, Junxian Wu, Ziyue Zhang, Tonglong Li, Cheng
  Xue, Youyong Kong
Categories: cs.CV cs.AI
\\
  Vertebrae identification in arbitrary fields-of-view plays a crucial role in
diagnosing spine disease. Most spine CT contain only local regions, such as the
neck, chest, and abdomen. Therefore, identification should not depend on
specific vertebrae or a particular number of vertebrae being visible. Existing
methods at the spine-level are unable to meet this challenge. In this paper, we
propose a three-stage method to address the challenges in 3D CT vertebrae
identification at vertebrae-level. By sequentially performing the tasks of
vertebrae localization, segmentation, and identification, the anatomical prior
information of the vertebrae is effectively utilized throughout the process.
Specifically, we introduce a dual-factor density clustering algorithm to
acquire localization information for individual vertebra, thereby facilitating
subsequent segmentation and identification processes. In addition, to tackle
the issue of interclass similarity and intra-class variability, we pre-train
our identification network by using a supervised contrastive learning method.
To further optimize the identification results, we estimated the uncertainty of
the classification network and utilized the message fusion module to combine
the uncertainty scores, while aggregating global information about the spine.
Our method achieves state-of-the-art results on the VerSe19 and VerSe20
challenge benchmarks. Additionally, our approach demonstrates outstanding
generalization performance on an collected dataset containing a wide range of
abnormal cases.
\\ ( https://arxiv.org/abs/2401.07271 ,  5866kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07278 (*cross-listing*)
Date: Sun, 14 Jan 2024 12:22:34 GMT   (8710kb,D)

Title: Semi-supervised Semantic Segmentation using Redesigned Self-Training for
  White Blood Cel
Authors: Vinh Quoc Luu, Duy Khanh Le, Huy Thanh Nguyen, Minh Thanh Nguyen,
  Thinh Tien Nguyen, Vinh Quang Dinh
Categories: cs.CV cs.AI
\\
  Artificial Intelligence (AI) in healthcare, especially in white blood cell
cancer diagnosis, is hindered by two primary challenges: the lack of
large-scale labeled datasets for white blood cell (WBC) segmentation and
outdated segmentation methods. To address the first challenge, a
semi-supervised learning framework should be brought to efficiently annotate
the large dataset. In this work, we address this issue by proposing a novel
self-training pipeline with the incorporation of FixMatch. We discover that by
incorporating FixMatch in the self-training pipeline, the performance improves
in the majority of cases. Our performance achieved the best performance with
the self-training scheme with consistency on DeepLab-V3 architecture and
ResNet-50, reaching 90.69%, 87.37%, and 76.49% on Zheng 1, Zheng 2, and LISC
datasets, respectively.
\\ ( https://arxiv.org/abs/2401.07278 ,  8710kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07336 (*cross-listing*)
Date: Sun, 14 Jan 2024 17:56:36 GMT   (2086kb)

Title: Construction and Evaluation of Mandarin Multimodal Emotional Speech
  Database
Authors: Zhu Ting, Li Liangqi, Duan Shufei, Zhang Xueying, Xiao Zhongzhe, Jia
  Hairng, Liang Huizhi
Categories: eess.AS cs.AI cs.SD eess.SP
\\
  A multi-modal emotional speech Mandarin database including articulatory
kinematics, acoustics, glottal and facial micro-expressions is designed and
established, which is described in detail from the aspects of corpus design,
subject selection, recording details and data processing. Where signals are
labeled with discrete emotion labels (neutral, happy, pleasant, indifferent,
angry, sad, grief) and dimensional emotion labels (pleasure, arousal,
dominance). In this paper, the validity of dimension annotation is verified by
statistical analysis of dimension annotation data. The SCL-90 scale data of
annotators are verified and combined with PAD annotation data for analysis, so
as to explore the internal relationship between the outlier phenomenon in
annotation and the psychological state of annotators. In order to verify the
speech quality and emotion discrimination of the database, this paper uses 3
basic models of SVM, CNN and DNN to calculate the recognition rate of these
seven emotions. The results show that the average recognition rate of seven
emotions is about 82% when using acoustic data alone. When using glottal data
alone, the average recognition rate is about 72%. Using kinematics data alone,
the average recognition rate also reaches 55.7%. Therefore, the database is of
high quality and can be used as an important source for speech analysis
research, especially for the task of multimodal emotional speech analysis.
\\ ( https://arxiv.org/abs/2401.07336 ,  2086kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07348 (*cross-listing*)
Date: Sun, 14 Jan 2024 19:16:29 GMT   (573kb)

Title: Generative AI in EU Law: Liability, Privacy, Intellectual Property, and
  Cybersecurity
Authors: Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato,
  Luciano Floridi
Categories: cs.CY cs.AI
\\
  The advent of Generative AI, particularly through Large Language Models
(LLMs) like ChatGPT and its successors, marks a paradigm shift in the AI
landscape. Advanced LLMs exhibit multimodality, handling diverse data formats,
thereby broadening their application scope. However, the complexity and
emergent autonomy of these models introduce challenges in predictability and
legal compliance. This paper delves into the legal and regulatory implications
of Generative AI and LLMs in the European Union context, analyzing aspects of
liability, privacy, intellectual property, and cybersecurity. It critically
examines the adequacy of the existing and proposed EU legislation, including
the Artificial Intelligence Act (AIA) draft, in addressing the unique
challenges posed by Generative AI in general and LLMs in particular. The paper
identifies potential gaps and shortcomings in the legislative framework and
proposes recommendations to ensure the safe and compliant deployment of
generative models, ensuring they align with the EU's evolving digital landscape
and legal standards.
\\ ( https://arxiv.org/abs/2401.07348 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07353 (*cross-listing*)
Date: Sun, 14 Jan 2024 19:40:32 GMT   (6896kb,D)

Title: Towards Engineering Fair and Equitable Software Systems for Managing
  Low-Altitude Airspace Authorizations
Authors: Usman Gohar, Michael C. Hunter, Agnieszka Marczak-Czajka, Robyn R.
  Lutz, Myra B. Cohen, Jane Cleland-Huang
Categories: cs.SE cs.AI cs.LG
Journal-ref: ICSE-SEIS 2024
DOI: 10.1145/3639475.3640103
\\
  Small Unmanned Aircraft Systems (sUAS) have gained widespread adoption across
a diverse range of applications. This has introduced operational complexities
within shared airspaces and an increase in reported incidents, raising safety
concerns. In response, the U.S. Federal Aviation Administration (FAA) is
developing a UAS Traffic Management (UTM) system to control access to airspace
based on an sUAS's predicted ability to safely complete its mission. However, a
fully automated system capable of swiftly approving or denying flight requests
can be prone to bias and must consider safety, transparency, and fairness to
diverse stakeholders. In this paper, we present an initial study that explores
stakeholders' perspectives on factors that should be considered in an automated
system. Results indicate flight characteristics and environmental conditions
were perceived as most important but pilot and drone capabilities should also
be considered. Further, several respondents indicated an aversion to any
AI-supported automation, highlighting the need for full transparency in
automated decision-making. Results provide a societal perspective on the
challenges of automating UTM flight authorization decisions and help frame the
ongoing design of a solution acceptable to the broader sUAS community.
\\ ( https://arxiv.org/abs/2401.07353 ,  6896kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07378 (*cross-listing*)
Date: Sun, 14 Jan 2024 21:42:18 GMT   (6995kb,D)

Title: Efficient approximation of Earth Mover's Distance Based on Nearest
  Neighbor Search
Authors: Guangyu Meng, Ruyu Zhou, Liu Liu, Peixian Liang, Fang Liu, Danny Chen,
  Michael Niemier, X.Sharon Hu
Categories: cs.CV cs.AI
\\
  Earth Mover's Distance (EMD) is an important similarity measure between two
distributions, used in computer vision and many other application domains.
However, its exact calculation is computationally and memory intensive, which
hinders its scalability and applicability for large-scale problems. Various
approximate EMD algorithms have been proposed to reduce computational costs,
but they suffer lower accuracy and may require additional memory usage or
manual parameter tuning. In this paper, we present a novel approach, NNS-EMD,
to approximate EMD using Nearest Neighbor Search (NNS), in order to achieve
high accuracy, low time complexity, and high memory efficiency. The NNS
operation reduces the number of data points compared in each NNS iteration and
offers opportunities for parallel processing. We further accelerate NNS-EMD via
vectorization on GPU, which is especially beneficial for large datasets. We
compare NNS-EMD with both the exact EMD and state-of-the-art approximate EMD
algorithms on image classification and retrieval tasks. We also apply NNS-EMD
to calculate transport mapping and realize color transfer between images.
NNS-EMD can be 44x to 135x faster than the exact EMD implementation, and
achieves superior accuracy, speedup, and memory efficiency over existing
approximate EMD methods.
\\ ( https://arxiv.org/abs/2401.07378 ,  6995kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07379 (*cross-listing*)
Date: Sun, 14 Jan 2024 21:43:10 GMT   (7059kb,D)

Title: Inference of dynamical gene regulatory networks from single-cell data
  with physics informed neural networks
Authors: Maria Mircea, Diego Garlaschelli, Stefan Semrau
Categories: q-bio.QM cs.AI cs.LG q-bio.MN
Comments: 25 pages, 8 figures
\\
  One of the main goals of developmental biology is to reveal the gene
regulatory networks (GRNs) underlying the robust differentiation of multipotent
progenitors into precisely specified cell types. Most existing methods to infer
GRNs from experimental data have limited predictive power as the inferred GRNs
merely reflect gene expression similarity or correlation. Here, we demonstrate,
how physics-informed neural networks (PINNs) can be used to infer the
parameters of predictive, dynamical GRNs that provide mechanistic understanding
of biological processes. Specifically we study GRNs that exhibit bifurcation
behavior and can therefore model cell differentiation. We show that PINNs
outperform regular feed-forward neural networks on the parameter inference task
and analyze two relevant experimental scenarios: 1. a system with cell
communication for which gene expression trajectories are available and 2.
snapshot measurements of a cell population in which cell communication is
absent. Our analysis will inform the design of future experiments to be
analyzed with PINNs and provides a starting point to explore this powerful
class of neural network models further.
\\ ( https://arxiv.org/abs/2401.07379 ,  7059kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07445 (*cross-listing*)
Date: Mon, 15 Jan 2024 03:12:21 GMT   (1289kb,D)

Title: GACE: Learning Graph-Based Cross-Page Ads Embedding For Click-Through
  Rate Prediction
Authors: Haowen Wang, Yuliang Du, Congyun Jin, Yujiao Li, Yingbo Wang, Tao Sun,
  Piqi Qin, Cong Fan
Categories: cs.IR cs.AI cs.LG stat.ME
Comments: 15 pages, 3 figures
DOI: 10.1007/978-981-99-8184-7_33
\\
  Predicting click-through rate (CTR) is the core task of many ads online
recommendation systems, which helps improve user experience and increase
platform revenue. In this type of recommendation system, we often encounter two
main problems: the joint usage of multi-page historical advertising data and
the cold start of new ads. In this paper, we proposed GACE, a graph-based
cross-page ads embedding generation method. It can warm up and generate the
representation embedding of cold-start and existing ads across various pages.
Specifically, we carefully build linkages and a weighted undirected graph model
considering semantic and page-type attributes to guide the direction of feature
fusion and generation. We designed a variational auto-encoding task as
pre-training module and generated embedding representations for new and old ads
based on this task. The results evaluated in the public dataset AliEC from
RecBole and the real-world industry dataset from Alipay show that our GACE
method is significantly superior to the SOTA method. In the online A/B test,
the click-through rate on three real-world pages from Alipay has increased by
3.6%, 2.13%, and 3.02%, respectively. Especially in the cold-start task, the
CTR increased by 9.96%, 7.51%, and 8.97%, respectively.
\\ ( https://arxiv.org/abs/2401.07445 ,  1289kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07450 (*cross-listing*)
Date: Mon, 15 Jan 2024 03:38:57 GMT   (8587kb,D)

Title: Hierarchical Fashion Design with Multi-stage Diffusion Models
Authors: Zhifeng Xie, Hao li, Huiming Ding, Mengtian Li, Ying Cao
Categories: cs.CV cs.AI
\\
  Cross-modal fashion synthesis and editing offer intelligent support to
fashion designers by enabling the automatic generation and local modification
of design drafts.While current diffusion models demonstrate commendable
stability and controllability in image synthesis,they still face significant
challenges in generating fashion design from abstract design elements and
fine-grained editing.Abstract sensory expressions, \eg office, business, and
party, form the high-level design concepts, while measurable aspects like
sleeve length, collar type, and pant length are considered the low-level
attributes of clothing.Controlling and editing fashion images using lengthy
text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a
novel fashion design method using the shared multi-stage diffusion model
encompassing high-level design concepts and low-level clothing attributes in a
hierarchical structure.Specifically, we categorized the input text into
different levels and fed them in different time step to the diffusion model
according to the criteria of professional clothing designers.HieraFashDiff
allows designers to add low-level attributes after high-level prompts for
interactive editing incrementally.In addition, we design a differentiable loss
function in the sampling process with a mask to keep non-edit
areas.Comprehensive experiments performed on our newly conducted Hierarchical
fashion dataset,demonstrate that our proposed method outperforms other
state-of-the-art competitors.
\\ ( https://arxiv.org/abs/2401.07450 ,  8587kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07466 (*cross-listing*)
Date: Mon, 15 Jan 2024 04:45:27 GMT   (6987kb,D)

Title: Your Instructions Are Not Always Helpful: Assessing the Efficacy of
  Instruction Fine-tuning for Software Vulnerability Detection
Authors: Imam Nur Bani Yusuf, Lingxiao Jiang
Categories: cs.SE cs.AI
\\
  Software, while beneficial, poses potential cybersecurity risks due to
inherent vulnerabilities. Detecting these vulnerabilities is crucial, and deep
learning has shown promise as an effective tool for this task due to its
ability to perform well without extensive feature engineering. However, a
challenge in deploying deep learning for vulnerability detection is the limited
availability of training data. Recent research highlights the deep learning
efficacy in diverse tasks. This success is attributed to instruction
fine-tuning, a technique that remains under-explored in the context of
vulnerability detection. This paper investigates the capability of models,
specifically a recent language model, to generalize beyond the programming
languages used in their training data. It also examines the role of natural
language instructions in enhancing this generalization. Our study evaluates the
model performance on a real-world dataset to predict vulnerable code. We
present key insights and lessons learned, contributing to understanding the
deep learning application in software vulnerability detection.
\\ ( https://arxiv.org/abs/2401.07466 ,  6987kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07489 (*cross-listing*)
Date: Mon, 15 Jan 2024 06:12:22 GMT   (685kb,D)

Title: The Principle of Minimum Pressure Gradient: An Alternative Basis for
  Physics-Informed Learning of Incompressible Fluid Mechanics
Authors: Hussam Alhussein, Mohammed Daqaq
Categories: physics.flu-dyn cs.AI
\\
  Recent advances in the application of physics-informed learning into the
field of fluid mechanics have been predominantly grounded in the Newtonian
framework, primarly leveraging Navier-Stokes Equation or one of its various
derivative to train a neural network. Here, we propose an alternative approach
based on variational methods. The proposed approach uses the principle of
minimum pressure gradient combined with the continuity constraint to train a
neural network and predict the flow field in incompressible fluids. We describe
the underlying principles of the proposed approach, then use a demonstrative
example to illustrate its implementation and show that it reduces the
computational time per training epoch when compared to the conventional
approach.
\\ ( https://arxiv.org/abs/2401.07489 ,  685kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07519 (*cross-listing*)
Date: Mon, 15 Jan 2024 07:50:18 GMT   (38576kb,D)

Title: InstantID: Zero-shot Identity-Preserving Generation in Seconds
Authors: Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin and Anthony Chen
Categories: cs.CV cs.AI
Comments: Technical Report, project page available at
  https://instantid.github.io/
\\
  There has been significant progress in personalized image synthesis with
methods such as Textual Inversion, DreamBooth, and LoRA. Yet, their real-world
applicability is hindered by high storage demands, lengthy fine-tuning
processes, and the need for multiple reference images. Conversely, existing ID
embedding-based methods, while requiring only a single forward inference, face
challenges: they either necessitate extensive fine-tuning across numerous model
parameters, lack compatibility with community pre-trained models, or fail to
maintain high face fidelity. Addressing these limitations, we introduce
InstantID, a powerful diffusion model-based solution. Our plug-and-play module
adeptly handles image personalization in various styles using just a single
facial image, while ensuring high fidelity. To achieve this, we design a novel
IdentityNet by imposing strong semantic and weak spatial conditions,
integrating facial and landmark images with textual prompts to steer the image
generation. InstantID demonstrates exceptional performance and efficiency,
proving highly beneficial in real-world applications where identity
preservation is paramount. Moreover, our work seamlessly integrates with
popular pre-trained text-to-image diffusion models like SD1.5 and SDXL, serving
as an adaptable plugin. Our codes and pre-trained checkpoints will be available
at https://github.com/InstantID/InstantID.
\\ ( https://arxiv.org/abs/2401.07519 ,  38576kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07532 (*cross-listing*)
Date: Mon, 15 Jan 2024 08:41:01 GMT   (1364kb,D)

Title: Multi-view MidiVAE: Fusing Track- and Bar-view Representations for Long
  Multi-track Symbolic Music Generation
Authors: Zhiwei Lin, Jun Chen, Boshi Tang, Binzhu Sha, Jing Yang, Yaolong Ju,
  Fan Fan, Shiyin Kang, Zhiyong Wu, Helen Meng
Categories: cs.SD cs.AI eess.AS
Comments: Accepted by ICASSP 2024
\\
  Variational Autoencoders (VAEs) constitute a crucial component of neural
symbolic music generation, among which some works have yielded outstanding
results and attracted considerable attention. Nevertheless, previous VAEs still
encounter issues with overly long feature sequences and generated results lack
contextual coherence, thus the challenge of modeling long multi-track symbolic
music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as
one of the pioneers in VAE methods that effectively model and generate long
multi-track symbolic music. The Multi-view MidiVAE utilizes the two-dimensional
(2-D) representation, OctupleMIDI, to capture relationships among notes while
reducing the feature sequences length. Moreover, we focus on instrumental
characteristics and harmony as well as global and local information about the
musical composition by employing a hybrid variational encoding-decoding
strategy to integrate both Track- and Bar-view MidiVAE features. Objective and
subjective experimental results on the CocoChorales dataset demonstrate that,
compared to the baseline, Multi-view MidiVAE exhibits significant improvements
in terms of modeling long multi-track symbolic music.
\\ ( https://arxiv.org/abs/2401.07532 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07543 (*cross-listing*)
Date: Mon, 15 Jan 2024 09:07:28 GMT   (18741kb,D)

Title: Must: Maximizing Latent Capacity of Spatial Transcriptomics Data
Authors: Zelin Zang, Liangyu Li, Yongjie Xu, Chenrui Duan, Kai Wang, Yang You,
  Yi Sun, Stan Z. Li
Categories: cs.CE cs.AI
Comments: 30 pages and 6 figures, plus 27 pages and 14 figures in appendices
\\
  Spatial transcriptomics (ST) technologies have revolutionized the study of
gene expression patterns in tissues by providing multimodality data in
transcriptomic, spatial, and morphological, offering opportunities for
understanding tissue biology beyond transcriptomics. However, we identify the
modality bias phenomenon in ST data species, i.e., the inconsistent
contribution of different modalities to the labels leads to a tendency for the
analysis methods to retain the information of the dominant modality. How to
mitigate the adverse effects of modality bias to satisfy various downstream
tasks remains a fundamental challenge. This paper introduces Multiple-modality
Structure Transformation, named MuST, a novel methodology to tackle the
challenge. MuST integrates the multi-modality information contained in the ST
data effectively into a uniform latent space to provide a foundation for all
the downstream tasks. It learns intrinsic local structures by topology
discovery strategy and topology fusion loss function to solve the
inconsistencies among different modalities. Thus, these topology-based and deep
learning techniques provide a solid foundation for a variety of analytical
tasks while coordinating different modalities. The effectiveness of MuST is
assessed by performance metrics and biological significance. The results show
that it outperforms existing state-of-the-art methods with clear advantages in
the precision of identifying and preserving structures of tissues and
biomarkers. MuST offers a versatile toolkit for the intricate analysis of
complex biological systems.
\\ ( https://arxiv.org/abs/2401.07543 ,  18741kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07586 (*cross-listing*)
Date: Mon, 15 Jan 2024 10:46:01 GMT   (551kb,D)

Title: Curriculum for Crowd Counting -- Is it Worthy?
Authors: Muhammad Asif Khan, Hamid Menouar, Ridha Hamila
Categories: cs.CV cs.AI
Comments: Accepted version of the paper in 19th International Conference on
  Computer Vision Theory and Applications (VISAPP), Rome, Italy, 27-19 February
  2024
\\
  Recent advances in deep learning techniques have achieved remarkable
performance in several computer vision problems. A notably intuitive technique
called Curriculum Learning (CL) has been introduced recently for training deep
learning models. Surprisingly, curriculum learning achieves significantly
improved results in some tasks but marginal or no improvement in others. Hence,
there is still a debate about its adoption as a standard method to train
supervised learning models. In this work, we investigate the impact of
curriculum learning in crowd counting using the density estimation method. We
performed detailed investigations by conducting 112 experiments using six
different CL settings using eight different crowd models. Our experiments show
that curriculum learning improves the model learning performance and shortens
the convergence time.
\\ ( https://arxiv.org/abs/2401.07586 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07591 (*cross-listing*)
Date: Mon, 15 Jan 2024 10:54:35 GMT   (15165kb,D)

Title: Multimodal Crowd Counting with Pix2Pix GANs
Authors: Muhammad Asif Khan, Hamid Menouar, Ridha Hamila
Categories: cs.CV cs.AI
Comments: Accepted version of the paper in 19th International Conference on
  Computer Vision Theory and Applications (VISAPP), Rome, Italy, 27-29 Feb,
  2024,
\\
  Most state-of-the-art crowd counting methods use color (RGB) images to learn
the density map of the crowd. However, these methods often struggle to achieve
higher accuracy in densely crowded scenes with poor illumination. Recently,
some studies have reported improvement in the accuracy of crowd counting models
using a combination of RGB and thermal images. Although multimodal data can
lead to better predictions, multimodal data might not be always available
beforehand. In this paper, we propose the use of generative adversarial
networks (GANs) to automatically generate thermal infrared (TIR) images from
color (RGB) images and use both to train crowd counting models to achieve
higher accuracy. We use a Pix2Pix GAN network first to translate RGB images to
TIR images. Our experiments on several state-of-the-art crowd counting models
and benchmark crowd datasets report significant improvement in accuracy.
\\ ( https://arxiv.org/abs/2401.07591 ,  15165kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07603 (*cross-listing*)
Date: Mon, 15 Jan 2024 11:20:34 GMT   (21938kb,D)

Title: Multi-task robot data for dual-arm fine manipulation
Authors: Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi
Categories: cs.RO cs.AI
Comments: 10 pages, The dataset is available at
  https://sites.google.com/view/multi-task-fine
\\
  In the field of robotic manipulation, deep imitation learning is recognized
as a promising approach for acquiring manipulation skills. Additionally,
learning from diverse robot datasets is considered a viable method to achieve
versatility and adaptability. In such research, by learning various tasks,
robots achieved generality across multiple objects. However, such multi-task
robot datasets have mainly focused on single-arm tasks that are relatively
imprecise, not addressing the fine-grained object manipulation that robots are
expected to perform in the real world. This paper introduces a dataset of
diverse object manipulations that includes dual-arm tasks and/or tasks
requiring fine manipulation. To this end, we have generated dataset with 224k
episodes (150 hours, 1,104 language instructions) which includes dual-arm fine
tasks such as bowl-moving, pencil-case opening or banana-peeling, and this data
is publicly available. Additionally, this dataset includes visual attention
signals as well as dual-action labels, a signal that separates actions into a
robust reaching trajectory and precise interaction with objects, and language
instructions to achieve robust and precise object manipulation. We applied the
dataset to our Dual-Action and Attention (DAA), a model designed for
fine-grained dual arm manipulation tasks and robust against covariate shifts.
The model was tested with over 7k total trials in real robot manipulation
tasks, demonstrating its capability in fine manipulation.
\\ ( https://arxiv.org/abs/2401.07603 ,  21938kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07612 (*cross-listing*)
Date: Mon, 15 Jan 2024 11:44:18 GMT   (543kb)

Title: Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks
  Against LLM-Integrated Applications
Authors: Xuchen Suo
Categories: cs.CR cs.AI
\\
  The critical challenge of prompt injection attacks in Large Language Models
(LLMs) integrated applications, a growing concern in the Artificial
Intelligence (AI) field. Such attacks, which manipulate LLMs through natural
language inputs, pose a significant threat to the security of these
applications. Traditional defense strategies, including output and input
filtering, as well as delimiter use, have proven inadequate. This paper
introduces the 'Signed-Prompt' method as a novel solution. The study involves
signing sensitive instructions within command segments by authorized users,
enabling the LLM to discern trusted instruction sources. The paper presents a
comprehensive analysis of prompt injection attack patterns, followed by a
detailed explanation of the Signed-Prompt concept, including its basic
architecture and implementation through both prompt engineering and fine-tuning
of LLMs. Experiments demonstrate the effectiveness of the Signed-Prompt method,
showing substantial resistance to various types of prompt injection attacks,
thus validating its potential as a robust defense strategy in AI security.
\\ ( https://arxiv.org/abs/2401.07612 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07655 (*cross-listing*)
Date: Mon, 15 Jan 2024 12:51:13 GMT   (1959kb,D)

Title: MLAD: A Unified Model for Multi-system Log Anomaly Detection
Authors: Runqiang Zang, Hongcheng Guo, Jian Yang, Jiaheng Liu, Zhoujun Li,
  Tieqiao Zheng, Xu Shi, Liangfan Zheng, Bo Zhang
Categories: cs.SE cs.AI cs.LG
\\
  In spite of the rapid advancements in unsupervised log anomaly detection
techniques, the current mainstream models still necessitate specific training
for individual system datasets, resulting in costly procedures and limited
scalability due to dataset size, thereby leading to performance bottlenecks.
Furthermore, numerous models lack cognitive reasoning capabilities, posing
challenges in direct transferability to similar systems for effective anomaly
detection. Additionally, akin to reconstruction networks, these models often
encounter the "identical shortcut" predicament, wherein the majority of system
logs are classified as normal, erroneously predicting normal classes when
confronted with rare anomaly logs due to reconstruction errors.
  To address the aforementioned issues, we propose MLAD, a novel anomaly
detection model that incorporates semantic relational reasoning across multiple
systems. Specifically, we employ Sentence-bert to capture the similarities
between log sequences and convert them into highly-dimensional learnable
semantic vectors. Subsequently, we revamp the formulas of the Attention layer
to discern the significance of each keyword in the sequence and model the
overall distribution of the multi-system dataset through appropriate vector
space diffusion. Lastly, we employ a Gaussian mixture model to highlight the
uncertainty of rare words pertaining to the "identical shortcut" problem,
optimizing the vector space of the samples using the maximum expectation model.
Experiments on three real-world datasets demonstrate the superiority of MLAD.
\\ ( https://arxiv.org/abs/2401.07655 ,  1959kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07709 (*cross-listing*)
Date: Mon, 15 Jan 2024 14:25:54 GMT   (4419kb,D)

Title: Towards Efficient Diffusion-Based Image Editing with Instant Attention
  Masks
Authors: Siyu Zou, Jiji Tang, Yiyi Zhou, Jing He, Chaoyi Zhao, Rongsheng Zhang,
  Zhipeng Hu, Xiaoshuai Sun
Categories: cs.CV cs.AI
Comments: Accepted by AAAI2024
\\
  Diffusion-based Image Editing (DIE) is an emerging research hot-spot, which
often applies a semantic mask to control the target area for diffusion-based
editing. However, most existing solutions obtain these masks via manual
operations or off-line processing, greatly reducing their efficiency. In this
paper, we propose a novel and efficient image editing method for Text-to-Image
(T2I) diffusion models, termed Instant Diffusion Editing(InstDiffEdit). In
particular, InstDiffEdit aims to employ the cross-modal attention ability of
existing diffusion models to achieve instant mask guidance during the diffusion
steps. To reduce the noise of attention maps and realize the full automatics,
we equip InstDiffEdit with a training-free refinement scheme to adaptively
aggregate the attention distributions for the automatic yet accurate mask
generation. Meanwhile, to supplement the existing evaluations of DIE, we
propose a new benchmark called Editing-Mask to examine the mask accuracy and
local editing ability of existing methods. To validate InstDiffEdit, we also
conduct extensive experiments on ImageNet and Imagen, and compare it with a
bunch of the SOTA methods. The experimental results show that InstDiffEdit not
only outperforms the SOTA methods in both image quality and editing results,
but also has a much faster inference speed, i.e., +5 to +6 times. Our code
available at https://anonymous.4open.science/r/InstDiffEdit-C306/
\\ ( https://arxiv.org/abs/2401.07709 ,  4419kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07729 (*cross-listing*)
Date: Mon, 15 Jan 2024 14:43:40 GMT   (1365kb,D)

Title: SSL-Interactions: Pretext Tasks for Interactive Trajectory Prediction
Authors: Prarthana Bhattacharyya, Chengjie Huang and Krzysztof Czarnecki
Categories: cs.CV cs.AI cs.RO
Comments: 13 pages, 5 figures, submitted to IV-2024
\\
  This paper addresses motion forecasting in multi-agent environments, pivotal
for ensuring safety of autonomous vehicles. Traditional as well as recent
data-driven marginal trajectory prediction methods struggle to properly learn
non-linear agent-to-agent interactions. We present SSL-Interactions that
proposes pretext tasks to enhance interaction modeling for trajectory
prediction. We introduce four interaction-aware pretext tasks to encapsulate
various aspects of agent interactions: range gap prediction, closest distance
prediction, direction of movement prediction, and type of interaction
prediction. We further propose an approach to curate interaction-heavy
scenarios from datasets. This curated data has two advantages: it provides a
stronger learning signal to the interaction model, and facilitates generation
of pseudo-labels for interaction-centric pretext tasks. We also propose three
new metrics specifically designed to evaluate predictions in interactive
scenes. Our empirical evaluations indicate SSL-Interactions outperforms
state-of-the-art motion forecasting methods quantitatively with up to 8%
improvement, and qualitatively, for interaction-heavy scenarios.
\\ ( https://arxiv.org/abs/2401.07729 ,  1365kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07796 (*cross-listing*)
Date: Mon, 15 Jan 2024 16:04:46 GMT   (2600kb,D)

Title: Fusing Echocardiography Images and Medical Records for Continuous
  Patient Stratification
Authors: Nathan Painchaud, Pierre-Yves Courand, Pierre-Marc Jodoin, Nicolas
  Duchateau, Olivier Bernard
Categories: cs.CV cs.AI cs.LG
Comments: 10 pages, submitted to IEEE TMI
\\
  Deep learning now enables automatic and robust extraction of cardiac function
descriptors from echocardiographic sequences, such as ejection fraction or
strain. These descriptors provide fine-grained information that physicians
consider, in conjunction with more global variables from the clinical record,
to assess patients' condition. Drawing on novel transformer models applied to
tabular data (e.g., variables from electronic health records), we propose a
method that considers all descriptors extracted from medical records and
echocardiograms to learn the representation of a difficult-to-characterize
cardiovascular pathology, namely hypertension. Our method first projects each
variable into its own representation space using modality-specific approaches.
These standardized representations of multimodal data are then fed to a
transformer encoder, which learns to merge them into a comprehensive
representation of the patient through a pretext task of predicting a clinical
rating. This pretext task is formulated as an ordinal classification to enforce
a pathological continuum in the representation space. We observe the major
trends along this continuum for a cohort of 239 hypertensive patients to
describe, with unprecedented gradation, the effect of hypertension on a number
of cardiac function descriptors. Our analysis shows that i) pretrained weights
from a foundation model allow to reach good performance (83% accuracy) even
with limited data (less than 200 training samples), ii) trends across the
population are reproducible between trainings, and iii) for descriptors whose
interactions with hypertension are well documented, patterns are consistent
with prior physiological knowledge.
\\ ( https://arxiv.org/abs/2401.07796 ,  2600kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07836 (*cross-listing*)
Date: Mon, 15 Jan 2024 17:06:02 GMT   (59kb)

Title: Two Types of AI Existential Risk: Decisive and Accumulative
Authors: Atoosa Kasirzadeh
Categories: cs.CY cs.AI cs.LG
\\
  The conventional discourse on existential risks (x-risks) from AI typically
focuses on abrupt, dire events caused by advanced AI systems, particularly
those that might achieve or surpass human-level intelligence. These events have
severe consequences that either lead to human extinction or irreversibly
cripple human civilization to a point beyond recovery. This discourse, however,
often neglects the serious possibility of AI x-risks manifesting incrementally
through a series of smaller yet interconnected disruptions, gradually crossing
critical thresholds over time. This paper contrasts the conventional "decisive
AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the
former envisions an overt AI takeover pathway, characterized by scenarios like
uncontrollable superintelligence, the latter suggests a different causal
pathway to existential catastrophes. This involves a gradual accumulation of
critical AI-induced threats such as severe vulnerabilities and systemic erosion
of econopolitical structures. The accumulative hypothesis suggests a boiling
frog scenario where incremental AI risks slowly converge, undermining
resilience until a triggering event results in irreversible collapse. Through
systems analysis, this paper examines the distinct assumptions differentiating
these two hypotheses. It is then argued that the accumulative view reconciles
seemingly incompatible perspectives on AI risks. The implications of
differentiating between these causal pathways -- the decisive and the
accumulative -- for the governance of AI risks as well as long-term AI safety
are discussed.
\\ ( https://arxiv.org/abs/2401.07836 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07862 (*cross-listing*)
Date: Mon, 15 Jan 2024 17:52:15 GMT   (3678kb,D)

Title: Adaptive Neural-Operator Backstepping Control of a Benchmark Hyperbolic
  PDE
Authors: Maxence Lamarque, Luke Bhan, Yuanyuan Shi, Miroslav Krstic
Categories: eess.SY cs.AI cs.LG cs.SY math.DS math.OC
Comments: 16.5 pages, 3 figures
\\
  To stabilize PDEs, feedback controllers require gain kernel functions, which
are themselves governed by PDEs. Furthermore, these gain-kernel PDEs depend on
the PDE plants' functional coefficients. The functional coefficients in PDE
plants are often unknown. This requires an adaptive approach to PDE control,
i.e., an estimation of the plant coefficients conducted concurrently with
control, where a separate PDE for the gain kernel must be solved at each
timestep upon the update in the plant coefficient function estimate. Solving a
PDE at each timestep is computationally expensive and a barrier to the
implementation of real-time adaptive control of PDEs. Recently, results in
neural operator (NO) approximations of functional mappings have been introduced
into PDE control, for replacing the computation of the gain kernel with a
neural network that is trained, once offline, and reused in real-time for rapid
solution of the PDEs. In this paper, we present the first result on applying
NOs in adaptive PDE control, presented for a benchmark 1-D hyperbolic PDE with
recirculation. We establish global stabilization via Lyapunov analysis, in the
plant and parameter error states, and also present an alternative approach, via
passive identifiers, which avoids the strong assumptions on kernel
differentiability. We then present numerical simulations demonstrating
stability and observe speedups up to three orders of magnitude, highlighting
the real-time efficacy of neural operators in adaptive control. Our code
(Github) is made publicly available for future researchers.
\\ ( https://arxiv.org/abs/2401.07862 ,  3678kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07868 (*cross-listing*)
Date: Mon, 15 Jan 2024 18:01:59 GMT   (1698kb,D)

Title: Consolidating Trees of Robotic Plans Generated Using Large Language
  Models to Improve Reliability
Authors: Md Sadman Sakib and Yu Sun
Categories: cs.RO cs.AI cs.CL
\\
  The inherent probabilistic nature of Large Language Models (LLMs) introduces
an element of unpredictability, raising concerns about potential discrepancies
in their output. This paper introduces an innovative approach aims to generate
correct and optimal robotic task plans for diverse real-world demands and
scenarios. LLMs have been used to generate task plans, but they are unreliable
and may contain wrong, questionable, or high-cost steps. The proposed approach
uses LLM to generate a number of task plans as trees and amalgamates them into
a graph by removing questionable paths. Then an optimal task tree can be
retrieved to circumvent questionable and high-cost nodes, thereby improving
planning accuracy and execution efficiency. The approach is further improved by
incorporating a large knowledge network. Leveraging GPT-4 further, the
high-level task plan is converted into a low-level Planning Domain Definition
Language (PDDL) plan executable by a robot. Evaluation results highlight the
superior accuracy and efficiency of our approach compared to previous
methodologies in the field of task planning.
\\ ( https://arxiv.org/abs/2401.07868 ,  1698kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07931 (*cross-listing*)
Date: Mon, 15 Jan 2024 19:47:14 GMT   (2732kb,D)

Title: Vertical Federated Image Segmentation
Authors: Paul K. Mandal, Cole Leo
Categories: cs.CV cs.AI cs.DC cs.LG
Comments: 8 pages, 5 figures
ACM-class: C.2.4; I.2.8; I.4; I.4.8
\\
  With the popularization of AI solutions for image based problems, there has
been a growing concern for both data privacy and acquisition. In a large number
of cases, information is located on separate data silos and it can be difficult
for a developer to consolidate all of it in a fashion that is appropriate for
machine learning model development. Alongside this, a portion of these
localized data regions may not have access to a labelled ground truth. This
indicates that they have the capacity to reach conclusions numerically, but are
not able to assign classifications amid a lack of pertinent information. Such a
determination is often negligible, especially when attempting to develop image
based solutions that often necessitate this capability. With this being the
case, we propose an innovative vertical federated learning (VFL) model
architecture that can operate under this common set of conditions. This is the
first (and currently the only) implementation of a system that can work under
the constraints of a VFL environment and perform image segmentation while
maintaining nominal accuracies. We achieved this by utilizing an FCN that
boasts the ability to operate on federates that lack labelled data and
privately share the respective weights with a central server, that of which
hosts the necessary features for classification. Tests were conducted on the
CamVid dataset in order to determine the impact of heavy feature compression
required for the transfer of information between federates, as well as to reach
nominal conclusions about the overall performance metrics when working under
such constraints.
\\ ( https://arxiv.org/abs/2401.07931 ,  2732kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07969 (*cross-listing*)
Date: Mon, 15 Jan 2024 21:23:23 GMT   (540kb,D)

Title: Simulated Autopoiesis in Liquid Automata
Authors: Steve Battle
Categories: nlin.CG cs.AI
Comments: 12 pages
\\
  We present a novel form of Liquid Automata, using this to simulate
autopoiesis, whereby living machines self-organise in the physical realm. This
simulation is based on an earlier Cellular Automaton described by Francisco
Varela. The basis of Liquid Automata is a particle simulation with additional
rules about how particles are transformed on collision with other particles.
Unlike cellular automata, there is no fixed grid or time-step, only particles
moving about and colliding with each other in a continuous space/time.
\\ ( https://arxiv.org/abs/2401.07969 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08003 (*cross-listing*)
Date: Mon, 15 Jan 2024 23:10:50 GMT   (4810kb,D)

Title: Jewelry Recognition via Encoder-Decoder Models
Authors: Jos\'e M. Alcalde-Llergo, Enrique Yeguas-Bol\'ivar, Andrea Zingoni and
  Alejandro Fuerte-Jurado
Categories: cs.CV cs.AI
Comments: 6 pages, 5 figures, MetroXRAINE 2023 Conference
Journal-ref: 2023 IEEE International Conference on Metrology for Extended
  Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE),
  Milano, Italy, 2023, pp. 116-121
\\
  Jewelry recognition is a complex task due to the different styles and designs
of accessories. Precise descriptions of the various accessories is something
that today can only be achieved by experts in the field of jewelry. In this
work, we propose an approach for jewelry recognition using computer vision
techniques and image captioning, trying to simulate this expert human behavior
of analyzing accessories. The proposed methodology consist on using different
image captioning models to detect the jewels from an image and generate a
natural language description of the accessory. Then, this description is also
utilized to classify the accessories at different levels of detail. The
generated caption includes details such as the type of jewel, color, material,
and design. To demonstrate the effectiveness of the proposed method in
accurately recognizing different types of jewels, a dataset consisting of
images of accessories belonging to jewelry stores in C\'ordoba (Spain) has been
created. After testing the different image captioning architectures designed,
the final model achieves a captioning accuracy of 95\%. The proposed
methodology has the potential to be used in various applications such as
jewelry e-commerce, inventory management or automatic jewels recognition to
analyze people's tastes and social status.
\\ ( https://arxiv.org/abs/2401.08003 ,  4810kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08014 (*cross-listing*)
Date: Mon, 15 Jan 2024 23:52:35 GMT   (1425kb,D)

Title: Convolutional Neural Network Compression via Dynamic Parameter Rank
  Pruning
Authors: Manish Sharma, Jamison Heard, Eli Saber, Panos P. Markopoulos
Categories: cs.CV cs.AI
Comments: 11 pages, 6 figures
\\
  While Convolutional Neural Networks (CNNs) excel at learning complex
latent-space representations, their over-parameterization can lead to
overfitting and reduced performance, particularly with limited data. This,
alongside their high computational and memory demands, limits the applicability
of CNNs for edge deployment. Low-rank matrix approximation has emerged as a
promising approach to reduce CNN parameters, but its application presents
challenges including rank selection and performance loss. To address these
issues, we propose an efficient training method for CNN compression via dynamic
parameter rank pruning. Our approach integrates efficient matrix factorization
and novel regularization techniques, forming a robust framework for dynamic
rank reduction and model compression. We use Singular Value Decomposition (SVD)
to model low-rank convolutional filters and dense weight matrices and we
achieve model compression by training the SVD factors with back-propagation in
an end-to-end way. We evaluate our method on an array of modern CNNs, including
ResNet-18, ResNet-20, and ResNet-32, and datasets like CIFAR-10, CIFAR-100, and
ImageNet (2012), showcasing its applicability in computer vision. Our
experiments show that the proposed method can yield substantial storage savings
while maintaining or even enhancing classification performance.
\\ ( https://arxiv.org/abs/2401.08014 ,  1425kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08066 (*cross-listing*)
Date: Tue, 16 Jan 2024 02:49:52 GMT   (3179kb,D)

Title: Achieve Fairness without Demographics for Dermatological Disease
  Diagnosis
Authors: Ching-Hao Chiu, Yu-Jen Chen, Yawen Wu, Yiyu Shi, Tsung-Yi Ho
Categories: cs.CV cs.AI
\\
  In medical image diagnosis, fairness has become increasingly crucial. Without
bias mitigation, deploying unfair AI would harm the interests of the
underprivileged population and potentially tear society apart. Recent research
addresses prediction biases in deep learning models concerning demographic
groups (e.g., gender, age, and race) by utilizing demographic (sensitive
attribute) information during training. However, many sensitive attributes
naturally exist in dermatological disease images. If the trained model only
targets fairness for a specific attribute, it remains unfair for other
attributes. Moreover, training a model that can accommodate multiple sensitive
attributes is impractical due to privacy concerns. To overcome this, we propose
a method enabling fair predictions for sensitive attributes during the testing
phase without using such information during training. Inspired by prior work
highlighting the impact of feature entanglement on fairness, we enhance the
model features by capturing the features related to the sensitive and target
attributes and regularizing the feature entanglement between corresponding
classes. This ensures that the model can only classify based on the features
related to the target attribute without relying on features associated with
sensitive attributes, thereby improving fairness and accuracy. Additionally, we
use disease masks from the Segment Anything Model (SAM) to enhance the quality
of the learned feature. Experimental results demonstrate that the proposed
method can improve fairness in classification compared to state-of-the-art
methods in two dermatological disease datasets.
\\ ( https://arxiv.org/abs/2401.08066 ,  3179kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08095 (*cross-listing*)
Date: Tue, 16 Jan 2024 03:39:35 GMT   (20027kb,D)

Title: DurFlex-EVC: Duration-Flexible Emotional Voice Conversion with Parallel
  Generation
Authors: Hyoung-Seok Oh, Sang-Hoon Lee, Deok-Hyun Cho, Seong-Whan Lee
Categories: cs.SD cs.AI eess.AS
Comments: 13 pages, 9 figures, 8 tables
\\
  Emotional voice conversion (EVC) seeks to modify the emotional tone of a
speaker's voice while preserving the original linguistic content and the
speaker's unique vocal characteristics. Recent advancements in EVC have
involved the simultaneous modeling of pitch and duration, utilizing the
potential of sequence-to-sequence (seq2seq) models. To enhance reliability and
efficiency in conversion, this study shifts focus towards parallel speech
generation. We introduce Duration-Flexible EVC (DurFlex-EVC), which integrates
a style autoencoder and unit aligner. Traditional models, while incorporating
self-supervised learning (SSL) representations that contain both linguistic and
paralinguistic information, have neglected this dual nature, leading to reduced
controllability. Addressing this issue, we implement cross-attention to
synchronize these representations with various emotions. Additionally, a style
autoencoder is developed for the disentanglement and manipulation of style
elements. The efficacy of our approach is validated through both subjective and
objective evaluations, establishing its superiority over existing models in the
field.
\\ ( https://arxiv.org/abs/2401.08095 ,  20027kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08097 (*cross-listing*)
Date: Tue, 16 Jan 2024 03:43:33 GMT   (2868kb,D)

Title: A Study of Fairness Concerns in AI-based Mobile App Reviews
Authors: Ali Rezaei Nasab, Maedeh Dashti, Mojtaba Shahin, Mansooreh Zahedi,
  Hourieh Khalajzadeh, Chetan Arora, Peng Liang
Categories: cs.SE cs.AI cs.CY
Comments: 15 pages, 4 images, 2 tables, Manuscript submitted to a Journal
  (2024)
\\
  With the growing application of AI-based systems in our lives and society,
there is a rising need to ensure that AI-based systems are developed and used
in a responsible way. Fairness is one of the socio-technical concerns that must
be addressed in AI-based systems for this purpose. Unfair AI-based systems,
particularly, unfair AI-based mobile apps, can pose difficulties for a
significant proportion of the global populace. This paper aims to deeply
analyze fairness concerns in AI-based app reviews. We first manually
constructed a ground-truth dataset including a statistical sample of fairness
and non-fairness reviews. Leveraging the ground-truth dataset, we then
developed and evaluated a set of machine learning and deep learning classifiers
that distinguish fairness reviews from non-fairness reviews. Our experiments
show that our best-performing classifier can detect fairness reviews with a
precision of 94%. We then applied the best-performing classifier on
approximately 9.5M reviews collected from 108 AI-based apps and identified
around 92K fairness reviews. While the fairness reviews appear in 23 app
categories, we found that the 'communication' and 'social' app categories have
the highest percentage of fairness reviews. Next, applying the K-means
clustering technique to the 92K fairness reviews, followed by manual analysis,
led to the identification of six distinct types of fairness concerns (e.g.,
'receiving different quality of features and services in different platforms
and devices' and 'lack of transparency and fairness in dealing with
user-generated content'). Finally, the manual analysis of 2,248 app owners'
responses to the fairness reviews identified six root causes (e.g., 'copyright
issues', 'external factors', 'development cost') that app owners report to
justify fairness concerns.
\\ ( https://arxiv.org/abs/2401.08097 ,  2868kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08099 (*cross-listing*)
Date: Tue, 16 Jan 2024 03:59:07 GMT   (24559kb,D)

Title: Inpainting Normal Maps for Lightstage data
Authors: Hancheng Zuo and Bernard Tiddeman
Categories: cs.CV cs.AI cs.GR
Comments: 8 pages, 4 figures, CGVC Conference, The Eurographics Association
ACM-class: I.2.6; I.4.5
Journal-ref: Computer Graphics and Visual Computing (CGVC), 2023, pp. 45-52
DOI: 10.2312/cgvc.20231190
\\
  This study introduces a novel method for inpainting normal maps using a
generative adversarial network (GAN). Normal maps, often derived from a
lightstage, are crucial in performance capture but can have obscured areas due
to movement (e.g., by arms, hair, or props). Inpainting fills these missing
areas with plausible data. Our approach extends previous general image
inpainting techniques, employing a bow tie-like generator network and a
discriminator network, with alternating training phases. The generator aims to
synthesize images aligning with the ground truth and deceive the discriminator,
which differentiates between real and processed images. Periodically, the
discriminator undergoes retraining to enhance its ability to identify processed
images. Importantly, our method adapts to the unique characteristics of normal
map data, necessitating modifications to the loss function. We utilize a cosine
loss instead of mean squared error loss for generator training. Limited
training data availability, even with synthetic datasets, demands significant
augmentation, considering the specific nature of the input data. This includes
appropriate image flipping and in-plane rotations to accurately alter normal
vectors. Throughout training, we monitored key metrics such as average loss,
Structural Similarity Index Measure (SSIM), and Peak Signal-to-Noise Ratio
(PSNR) for the generator, along with average loss and accuracy for the
discriminator. Our findings suggest that the proposed model effectively
generates high-quality, realistic inpainted normal maps, suitable for
performance capture applications. These results establish a foundation for
future research, potentially involving more advanced networks and comparisons
with inpainting of source images used to create the normal maps.
\\ ( https://arxiv.org/abs/2401.08099 ,  24559kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08100 (*cross-listing*)
Date: Tue, 16 Jan 2024 04:01:49 GMT   (11647kb,D)

Title: KTVIC: A Vietnamese Image Captioning Dataset on the Life Domain
Authors: Anh-Cuong Pham, Van-Quang Nguyen, Thi-Hong Vuong, Quang-Thuy Ha
Categories: cs.CV cs.AI
\\
  Image captioning is a crucial task with applications in a wide range of
domains, including healthcare and education. Despite extensive research on
English image captioning datasets, the availability of such datasets for
Vietnamese remains limited, with only two existing datasets. In this study, we
introduce KTVIC, a comprehensive Vietnamese Image Captioning dataset focused on
the life domain, covering a wide range of daily activities. This dataset
comprises 4,327 images and 21,635 Vietnamese captions, serving as a valuable
resource for advancing image captioning in the Vietnamese language. We conduct
experiments using various deep neural networks as the baselines on our dataset,
evaluating them using the standard image captioning metrics, including BLEU,
METEOR, CIDEr, and ROUGE. Our findings underscore the effectiveness of the
proposed dataset and its potential contributions to the field of image
captioning in the Vietnamese context.
\\ ( https://arxiv.org/abs/2401.08100 ,  11647kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08103 (*cross-listing*)
Date: Tue, 16 Jan 2024 04:14:23 GMT   (214kb,D)

Title: Resolving Ethics Trade-offs in Implementing Responsible AI
Authors: Conrad Sanderson, Emma Schleiger, David Douglas, Petra Kuhnert,
  Qinghua Lu
Categories: cs.CY cs.AI
ACM-class: K.4.1
\\
  While the operationalisation of high-level AI ethics principles into
practical AI/ML systems has made progress, there is still a theory-practice gap
in managing tensions between the underlying AI ethics aspects. We cover five
approaches for addressing the tensions via trade-offs, ranging from rudimentary
to complex. The approaches differ in the types of considered context, scope,
methods for measuring contexts, and degree of justification. None of the
approaches is likely to be appropriate for all organisations, systems, or
applications. To address this, we propose a framework which consists of: (i)
proactive identification of tensions, (ii) prioritisation and weighting of
ethics aspects, (iii) justification and documentation of trade-off decisions.
The proposed framework aims to facilitate the implementation of well-rounded
AI/ML systems that are appropriate for potential regulatory requirements.
\\ ( https://arxiv.org/abs/2401.08103 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08105 (*cross-listing*)
Date: Tue, 16 Jan 2024 04:16:46 GMT   (6012kb,D)

Title: Hardware Acceleration for Real-Time Wildfire Detection Onboard Drone
  Networks
Authors: Austin Briley, Fatemeh Afghah
Categories: cs.CV cs.AI eess.IV
Comments: 6 pages, 7 figures, NETROBOTICS conference submission
\\
  Early wildfire detection in remote and forest areas is crucial for minimizing
devastation and preserving ecosystems. Autonomous drones offer agile access to
remote, challenging terrains, equipped with advanced imaging technology that
delivers both high-temporal and detailed spatial resolution, making them
valuable assets in the early detection and monitoring of wildfires. However,
the limited computation and battery resources of Unmanned Aerial Vehicles
(UAVs) pose significant challenges in implementing robust and efficient image
classification models. Current works in this domain often operate offline,
emphasizing the need for solutions that can perform inference in real time,
given the constraints of UAVs. To address these challenges, this paper aims to
develop a real-time image classification and fire segmentation model. It
presents a comprehensive investigation into hardware acceleration using the
Jetson Nano P3450 and the implications of TensorRT, NVIDIA's high-performance
deep-learning inference library, on fire classification accuracy and speed. The
study includes implementations of Quantization Aware Training (QAT), Automatic
Mixed Precision (AMP), and post-training mechanisms, comparing them against the
latest baselines for fire segmentation and classification. All experiments
utilize the FLAME dataset - an image dataset collected by low-altitude drones
during a prescribed forest fire. This work contributes to the ongoing efforts
to enable real-time, on-board wildfire detection capabilities for UAVs,
addressing speed and the computational and energy constraints of these crucial
monitoring systems. The results show a 13% increase in classification speed
compared to similar models without hardware optimization. Comparatively, loss
and accuracy are within 1.225% of the original values.
\\ ( https://arxiv.org/abs/2401.08105 ,  6012kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08115 (*cross-listing*)
Date: Tue, 16 Jan 2024 05:05:08 GMT   (48178kb,D)

Title: No-Clean-Reference Image Super-Resolution: Application to Electron
  Microscopy
Authors: Mohammad Khateri, Morteza Ghahremani, Alejandra Sierra, and Jussi
  Tohka
Categories: cs.CV cs.AI
Comments: 14 pages, 12 figures, and 2 tables
\\
  The inability to acquire clean high-resolution (HR) electron microscopy (EM)
images over a large brain tissue volume hampers many neuroscience studies. To
address this challenge, we propose a deep-learning-based image super-resolution
(SR) approach to computationally reconstruct clean HR 3D-EM with a large field
of view (FoV) from noisy low-resolution (LR) acquisition. Our contributions are
I) Investigating training with no-clean references for $\ell_2$ and $\ell_1$
loss functions; II) Introducing a novel network architecture, named EMSR, for
enhancing the resolution of LR EM images while reducing inherent noise; and,
III) Comparing different training strategies including using acquired LR and HR
image pairs, i.e., real pairs with no-clean references contaminated with real
corruptions, the pairs of synthetic LR and acquired HR, as well as acquired LR
and denoised HR pairs. Experiments with nine brain datasets showed that
training with real pairs can produce high-quality super-resolved results,
demonstrating the feasibility of training with non-clean references for both
loss functions. Additionally, comparable results were observed, both visually
and numerically, when employing denoised and noisy references for training.
Moreover, utilizing the network trained with synthetically generated LR images
from HR counterparts proved effective in yielding satisfactory SR results, even
in certain cases, outperforming training with real pairs. The proposed SR
network was compared quantitatively and qualitatively with several established
SR techniques, showcasing either the superiority or competitiveness of the
proposed method in mitigating noise while recovering fine details.
\\ ( https://arxiv.org/abs/2401.08115 ,  48178kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08117 (*cross-listing*)
Date: Tue, 16 Jan 2024 05:10:50 GMT   (12763kb,D)

Title: E2HQV: High-Quality Video Generation from Event Camera via
  Theory-Inspired Model-Aided Deep Learning
Authors: Qiang Qu, Yiran Shen, Xiaoming Chen, Yuk Ying Chung, and Tongliang Liu
Categories: cs.CV cs.AI cs.MM
Comments: Accepted in AAAI2024
\\
  The bio-inspired event cameras or dynamic vision sensors are capable of
asynchronously capturing per-pixel brightness changes (called event-streams) in
high temporal resolution and high dynamic range. However, the non-structural
spatial-temporal event-streams make it challenging for providing intuitive
visualization with rich semantic information for human vision. It calls for
events-to-video (E2V) solutions which take event-streams as input and generate
high quality video frames for intuitive visualization. However, current
solutions are predominantly data-driven without considering the prior knowledge
of the underlying statistics relating event-streams and video frames. It highly
relies on the non-linearity and generalization capability of the deep neural
networks, thus, is struggling on reconstructing detailed textures when the
scenes are complex. In this work, we propose \textbf{E2HQV}, a novel E2V
paradigm designed to produce high-quality video frames from events. This
approach leverages a model-aided deep learning framework, underpinned by a
theory-inspired E2V model, which is meticulously derived from the fundamental
imaging principles of event cameras. To deal with the issue of state-reset in
the recurrent components of E2HQV, we also design a temporal shift embedding
module to further improve the quality of the video frames. Comprehensive
evaluations on the real world event camera datasets validate our approach, with
E2HQV, notably outperforming state-of-the-art approaches, e.g., surpassing the
second best by over 40\% for some evaluation metrics.
\\ ( https://arxiv.org/abs/2401.08117 ,  12763kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08138 (*cross-listing*)
Date: Tue, 16 Jan 2024 06:16:33 GMT   (119kb,D)

Title: LLMs for Test Input Generation for Semantic Caches
Authors: Zafaryab Rasool, Scott Barnett, David Willie, Stefanus Kurniawan,
  Sherwin Balugo, Srikanth Thudumu, Mohamed Abdelrazek
Categories: cs.SE cs.AI
Comments: Accepted in International Conference on AI Engineering Software
  Engineering (CAIN 2024)
\\
  Large language models (LLMs) enable state-of-the-art semantic capabilities to
be added to software systems such as semantic search of unstructured documents
and text generation. However, these models are computationally expensive. At
scale, the cost of serving thousands of users increases massively affecting
also user experience. To address this problem, semantic caches are used to
check for answers to similar queries (that may have been phrased differently)
without hitting the LLM service. Due to the nature of these semantic cache
techniques that rely on query embeddings, there is a high chance of errors
impacting user confidence in the system. Adopting semantic cache techniques
usually requires testing the effectiveness of a semantic cache (accurate cache
hits and misses) which requires a labelled test set of similar queries and
responses which is often unavailable. In this paper, we present VaryGen, an
approach for using LLMs for test input generation that produces similar
questions from unstructured text documents. Our novel approach uses the
reasoning capabilities of LLMs to 1) adapt queries to the domain, 2) synthesise
subtle variations to queries, and 3) evaluate the synthesised test dataset. We
evaluated our approach in the domain of a student question and answer system by
qualitatively analysing 100 generated queries and result pairs, and conducting
an empirical case study with an open source semantic cache. Our results show
that query pairs satisfy human expectations of similarity and our generated
data demonstrates failure cases of a semantic cache. Additionally, we also
evaluate our approach on Qasper dataset. This work is an important first step
into test input generation for semantic applications and presents
considerations for practitioners when calibrating a semantic cache.
\\ ( https://arxiv.org/abs/2401.08138 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08185 (*cross-listing*)
Date: Tue, 16 Jan 2024 08:01:09 GMT   (415kb)

Title: DPAFNet:Dual Path Attention Fusion Network for Single Image Deraining
Authors: Bingcai Wei
Categories: cs.CV cs.AI eess.IV
\\
  Rainy weather will have a significant impact on the regular operation of the
imaging system. Based on this premise, image rain removal has always been a
popular branch of low-level visual tasks, especially methods using deep neural
networks. However, most neural networks are but-branched, such as only using
convolutional neural networks or Transformers, which is unfavourable for the
multidimensional fusion of image features. In order to solve this problem, this
paper proposes a dual-branch attention fusion network. Firstly, a two-branch
network structure is proposed. Secondly, an attention fusion module is proposed
to selectively fuse the features extracted by the two branches rather than
simply adding them. Finally, complete ablation experiments and sufficient
comparison experiments prove the rationality and effectiveness of the proposed
method.
\\ ( https://arxiv.org/abs/2401.08185 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08194 (*cross-listing*)
Date: Tue, 16 Jan 2024 08:16:10 GMT   (16482kb,D)

Title: End-to-End Optimized Image Compression with the Frequency-Oriented
  Transform
Authors: Yuefeng Zhang and Kai Lin
Categories: cs.CV cs.AI cs.MM
Comments: 25 pages, accepted by MVAP
\\
  Image compression constitutes a significant challenge amidst the era of
information explosion. Recent studies employing deep learning methods have
demonstrated the superior performance of learning-based image compression
methods over traditional codecs. However, an inherent challenge associated with
these methods lies in their lack of interpretability. Following an analysis of
the varying degrees of compression degradation across different frequency
bands, we propose the end-to-end optimized image compression model facilitated
by the frequency-oriented transform. The proposed end-to-end image compression
model consists of four components: spatial sampling, frequency-oriented
transform, entropy estimation, and frequency-aware fusion. The
frequency-oriented transform separates the original image signal into distinct
frequency bands, aligning with the human-interpretable concept. Leveraging the
non-overlapping hypothesis, the model enables scalable coding through the
selective transmission of arbitrary frequency components. Extensive experiments
are conducted to demonstrate that our model outperforms all traditional codecs
including next-generation standard H.266/VVC on MS-SSIM metric. Moreover,
visual analysis tasks (i.e., object detection and semantic segmentation) are
conducted to verify the proposed compression method could preserve semantic
fidelity besides signal-level precision.
\\ ( https://arxiv.org/abs/2401.08194 ,  16482kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08261 (*cross-listing*)
Date: Tue, 16 Jan 2024 10:32:13 GMT   (2313kb,D)

Title: Probabilistically Robust Watermarking of Neural Networks
Authors: Mikhail Pautov, Nikita Bogdanov, Stanislav Pyatkin, Oleg Rogov, Ivan
  Oseledets
Categories: cs.CR cs.AI
\\
  As deep learning (DL) models are widely and effectively used in Machine
Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in
DL watermarking techniques that can be used to confirm the ownership of a
particular model. Unfortunately, these methods usually produce watermarks
susceptible to model stealing attacks. In our research, we introduce a novel
trigger set-based watermarking approach that demonstrates resilience against
functionality stealing attacks, particularly those involving extraction and
distillation. Our approach does not require additional model training and can
be applied to any model architecture. The key idea of our method is to compute
the trigger set, which is transferable between the source model and the set of
proxy models with a high probability. In our experimental study, we show that
if the probability of the set being transferable is reasonably high, it can be
effectively used for ownership verification of the stolen model. We evaluate
our method on multiple benchmarks and show that our approach outperforms
current state-of-the-art watermarking techniques in all considered experimental
setups.
\\ ( https://arxiv.org/abs/2401.08261 ,  2313kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08268 (*cross-listing*)
Date: Tue, 16 Jan 2024 10:41:33 GMT   (145kb,D)

Title: An Explainable Proxy Model for Multiabel Audio Segmentation
Authors: Th\'eo Mariotte and Antonio Almud\'evar and Marie Tahon and Alsonfo
  Ortega
Categories: eess.AS cs.AI cs.LG cs.SD eess.SP
Comments: Accepted at ICASSP 2024
Report-no: AA001
\\
  Audio signal segmentation is a key task for automatic audio indexing. It
consists of detecting the boundaries of class-homogeneous segments in the
signal. In many applications, explainable AI is a vital process for
transparency of decision-making with machine learning. In this paper, we
propose an explainable multilabel segmentation model that solves speech
activity (SAD), music (MD), noise (ND), and overlapped speech detection (OSD)
simultaneously. This proxy uses the non-negative matrix factorization (NMF) to
map the embedding used for the segmentation to the frequency domain.
Experiments conducted on two datasets show similar performances as the
pre-trained black box model while showing strong explainability features.
Specifically, the frequency bins used for the decision can be easily identified
at both the segment level (local explanations) and global level (class
prototypes).
\\ ( https://arxiv.org/abs/2401.08268 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08376 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:07:48 GMT   (983kb,D)

Title: KADEL: Knowledge-Aware Denoising Learning for Commit Message Generation
Authors: Wei Tao, Yucheng Zhou, Yanlin Wang, Hongyu Zhang, Haofen Wang,
  Wenqiang Zhang
Categories: cs.SE cs.AI
Comments: Accepted to ACM Transactions on Software Engineering and Methodology
  2024 (TOSEM'24)
\\
  Commit messages are natural language descriptions of code changes, which are
important for software evolution such as code understanding and maintenance.
However, previous methods are trained on the entire dataset without considering
the fact that a portion of commit messages adhere to good practice (i.e.,
good-practice commits), while the rest do not. On the basis of our empirical
study, we discover that training on good-practice commits significantly
contributes to the commit message generation. Motivated by this finding, we
propose a novel knowledge-aware denoising learning method called KADEL.
Considering that good-practice commits constitute only a small proportion of
the dataset, we align the remaining training samples with these good-practice
commits. To achieve this, we propose a model that learns the commit knowledge
by training on good-practice commits. This knowledge model enables
supplementing more information for training samples that do not conform to good
practice. However, since the supplementary information may contain noise or
prediction errors, we propose a dynamic denoising training method. This method
composes a distribution-aware confidence function and a dynamic distribution
list, which enhances the effectiveness of the training process. Experimental
results on the whole MCMD dataset demonstrate that our method overall achieves
state-of-the-art performance compared with previous methods. Our source code
and data are available at https://github.com/DeepSoftwareAnalytics/KADEL
\\ ( https://arxiv.org/abs/2401.08376 ,  983kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08396 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:41:20 GMT   (6405kb)

Title: Hidden Flaws Behind Expert-Level Accuracy of GPT-4 Vision in Medicine
Authors: Qiao Jin, Fangyuan Chen, Yiliang Zhou, Ziyang Xu, Justin M. Cheung,
  Robert Chen, Ronald M. Summers, Justin F. Rousseau, Peiyun Ni, Marc J
  Landsman, Sally L. Baxter, Subhi J. Al'Aref, Yijia Li, Michael F. Chiang,
  Yifan Peng, Zhiyong Lu
Categories: cs.CV cs.AI cs.CL
\\
  Recent studies indicate that Generative Pre-trained Transformer 4 with Vision
(GPT-4V) outperforms human physicians in medical challenge tasks. However,
these evaluations primarily focused on the accuracy of multi-choice questions
alone. Our study extends the current scope by conducting a comprehensive
analysis of GPT-4V's rationales of image comprehension, recall of medical
knowledge, and step-by-step multimodal reasoning when solving New England
Journal of Medicine (NEJM) Image Challenges - an imaging quiz designed to test
the knowledge and diagnostic capabilities of medical professionals. Evaluation
results confirmed that GPT-4V outperforms human physicians regarding
multi-choice accuracy (88.0% vs. 77.0%, p=0.034). GPT-4V also performs well in
cases where physicians incorrectly answer, with over 80% accuracy. However, we
discovered that GPT-4V frequently presents flawed rationales in cases where it
makes the correct final choices (27.3%), most prominent in image comprehension
(21.6%). Regardless of GPT-4V's high accuracy in multi-choice questions, our
findings emphasize the necessity for further in-depth evaluations of its
rationales before integrating such models into clinical workflows.
\\ ( https://arxiv.org/abs/2401.08396 ,  6405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08397 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:41:20 GMT   (2164kb,D)

Title: A Micro Architectural Events Aware Real-Time Embedded System Fault
  Injector
Authors: Enrico Magliano, Alessio Carpegna, Alessadro Savino, Stefano Di Carlo
Categories: cs.AR cs.AI
\\
  In contemporary times, the increasing complexity of the system poses
significant challenges to the reliability, trustworthiness, and security of the
SACRES. Key issues include the susceptibility to phenomena such as
instantaneous voltage spikes, electromagnetic interference, neutron strikes,
and out-of-range temperatures. These factors can induce switch state changes in
transistors, resulting in bit-flipping, soft errors, and transient corruption
of stored data in memory. The occurrence of soft errors, in turn, may lead to
system faults that can propel the system into a hazardous state. Particularly
in critical sectors like automotive, avionics, or aerospace, such malfunctions
can have real-world implications, potentially causing harm to individuals.
  This paper introduces a novel fault injector designed to facilitate the
monitoring, aggregation, and examination of micro-architectural events. This is
achieved by harnessing the microprocessor's PMU and the debugging interface,
specifically focusing on ensuring the repeatability of fault injections. The
fault injection methodology targets bit-flipping within the memory system,
affecting CPU registers and RAM. The outcomes of these fault injections enable
a thorough analysis of the impact of soft errors and establish a robust
correlation between the identified faults and the essential timing
predictability demanded by SACRES.
\\ ( https://arxiv.org/abs/2401.08397 ,  2164kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08405 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:44:13 GMT   (36kb)

Title: Interrogating AI: Characterizing Emergent Playful Interactions with
  ChatGPT
Authors: Mohammad Ronagh Nikghalb, Jinghui Cheng
Categories: cs.HC cs.AI
Comments: 17 pages
\\
  In an era of AI's growing capabilities and influences, recent advancements
are reshaping HCI and CSCW's view of AI as mere tools. Playful interactions
with AI systems naturally emerged as a way for users to make sense of the
ever-changing technology. However, these emergent and playful interactions are
underexamined. We target this gap by investigating playful interactions
exhibited by users of a recently trending powerful AI technology, ChatGPT.
Through a thematic analysis of 372 user-generated posts on the ChatGPT
subreddit, we found that a substantial portion of user discourse revolves
around playful interactions. The analysis further allowed us to construct a
preliminary taxonomy to describe these interactions, categorizing them into six
types: reflecting, jesting, imitating, challenging, tricking, and contriving;
each included sub-categories. Overall, this study contributes to the field of
HCI and CSCW by illuminating the multifaceted nature of playful interactions
with AI, underlining their significance in shaping the human-AI relationship.
\\ ( https://arxiv.org/abs/2401.08405 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08425 (*cross-listing*)
Date: Tue, 16 Jan 2024 15:11:18 GMT   (6338kb,D)

Title: U-DIADS-Bib: a full and few-shot pixel-precise dataset for document
  layout analysis of ancient manuscripts
Authors: Silvia Zottin, Axel De Nardin, Emanuela Colombi, Claudio Piciarelli,
  Filippo Pavan, Gian Luca Foresti
Categories: cs.CV cs.AI cs.LG
Comments: Neural Comput & Applic (2024)
DOI: 10.1007/s00521-023-09356-5
\\
  Document Layout Analysis, which is the task of identifying different semantic
regions inside of a document page, is a subject of great interest for both
computer scientists and humanities scholars as it represents a fundamental step
towards further analysis tasks for the former and a powerful tool to improve
and facilitate the study of the documents for the latter. However, many of the
works currently present in the literature, especially when it comes to the
available datasets, fail to meet the needs of both worlds and, in particular,
tend to lean towards the needs and common practices of the computer science
side, leading to resources that are not representative of the humanities real
needs. For this reason, the present paper introduces U-DIADS-Bib, a novel,
pixel-precise, non-overlapping and noiseless document layout analysis dataset
developed in close collaboration between specialists in the fields of computer
vision and humanities. Furthermore, we propose a novel, computer-aided,
segmentation pipeline in order to alleviate the burden represented by the
time-consuming process of manual annotation, necessary for the generation of
the ground truth segmentation maps. Finally, we present a standardized few-shot
version of the dataset (U-DIADS-BibFS), with the aim of encouraging the
development of models and solutions able to address this task with as few
samples as possible, which would allow for more effective use in a real-world
scenario, where collecting a large number of segmentations is not always
feasible.
\\ ( https://arxiv.org/abs/2401.08425 ,  6338kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08458 (*cross-listing*)
Date: Tue, 16 Jan 2024 16:07:53 GMT   (794kb,D)

Title: Security and Privacy Issues and Solutions in Federated Learning for
  Digital Healthcare
Authors: Hyejun Jeong, Tai-Myoung Chung
Categories: cs.CR cs.AI
Journal-ref: International Conference on Future Data and Security Engineering
  (2022) 316-331
DOI: 10.1007/978-981-19-8069-5_21
\\
  The advent of Federated Learning has enabled the creation of a
high-performing model as if it had been trained on a considerable amount of
data. A multitude of participants and a server cooperatively train a model
without the need for data disclosure or collection. The healthcare industry,
where security and privacy are paramount, can substantially benefit from this
new learning paradigm, as data collection is no longer feasible due to
stringent data policies. Nonetheless, unaddressed challenges and insufficient
attack mitigation are hampering its adoption. Attack surfaces differ from
traditional centralized learning in that the server and clients communicate
between each round of training. In this paper, we thus present vulnerabilities,
attacks, and defenses based on the widened attack surfaces, as well as suggest
promising new research directions toward a more robust FL.
\\ ( https://arxiv.org/abs/2401.08458 ,  794kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08527 (*cross-listing*)
Date: Tue, 16 Jan 2024 17:45:01 GMT   (7647kb,D)

Title: MICA: Towards Explainable Skin Lesion Diagnosis via Multi-Level
  Image-Concept Alignment
Authors: Yequan Bie, Luyang Luo, Hao Chen
Categories: cs.CV cs.AI
\\
  Black-box deep learning approaches have showcased significant potential in
the realm of medical image analysis. However, the stringent trustworthiness
requirements intrinsic to the medical field have catalyzed research into the
utilization of Explainable Artificial Intelligence (XAI), with a particular
focus on concept-based methods. Existing concept-based methods predominantly
apply concept annotations from a single perspective (e.g., global level),
neglecting the nuanced semantic relationships between sub-regions and concepts
embedded within medical images. This leads to underutilization of the valuable
medical information and may cause models to fall short in harmoniously
balancing interpretability and performance when employing inherently
interpretable architectures such as Concept Bottlenecks. To mitigate these
shortcomings, we propose a multi-modal explainable disease diagnosis framework
that meticulously aligns medical images and clinical-related concepts
semantically at multiple strata, encompassing the image level, token level, and
concept level. Moreover, our method allows for model intervention and offers
both textual and visual explanations in terms of human-interpretable concepts.
Experimental results on three skin image datasets demonstrate that our method,
while preserving model interpretability, attains high performance and label
efficiency for concept detection and disease diagnosis.
\\ ( https://arxiv.org/abs/2401.08527 ,  7647kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08577 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:59:45 GMT   (16121kb,D)

Title: MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in
  3D World
Authors: Yining Hong, Zishuo Zheng, Peihao Chen, Yian Wang, Junyan Li, Chuang
  Gan
Categories: cs.CV cs.AI cs.CL cs.LG cs.RO
Comments: Project page: https://vis-www.cs.umass.edu/multiply
\\
  Human beings possess the capability to multiply a melange of multisensory
cues while actively exploring and interacting with the 3D world. Current
multi-modal large language models, however, passively absorb sensory data as
inputs, lacking the capacity to actively interact with the objects in the 3D
environment and dynamically collect their multisensory information. To usher in
the study of this area, we propose MultiPLY, a multisensory embodied large
language model that could incorporate multisensory interactive data, including
visual, audio, tactile, and thermal information into large language models,
thereby establishing the correlation among words, actions, and percepts. To
this end, we first collect Multisensory Universe, a large-scale multisensory
interaction dataset comprising 500k data by deploying an LLM-powered embodied
agent to engage with the 3D environment. To perform instruction tuning with
pre-trained LLM on such generated data, we first encode the 3D scene as
abstracted object-centric representations and then introduce action tokens
denoting that the embodied agent takes certain actions within the environment,
as well as state tokens that represent the multisensory state observations of
the agent at each time step. In the inference time, MultiPLY could generate
action tokens, instructing the agent to take the action in the environment and
obtain the next multisensory state observation. The observation is then
appended back to the LLM via state tokens to generate subsequent text or action
tokens. We demonstrate that MultiPLY outperforms baselines by a large margin
through a diverse set of embodied tasks involving object retrieval, tool use,
multisensory captioning, and task decomposition.
\\ ( https://arxiv.org/abs/2401.08577 ,  16121kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06437 (*cross-listing*)
Date: Fri, 12 Jan 2024 08:07:52 GMT   (935kb,D)

Title: 3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp
  Features and Parametric Control?
Authors: Zeqing Yuan, Haoxuan Lan, Qiang Zou, Junbo Zhao
Categories: cs.GR cs.AI cs.CL
Comments: 10 pages, 6 figures
\\
  Recent advancements in implicit 3D representations and generative models have
markedly propelled the field of 3D object generation forward. However, it
remains a significant challenge to accurately model geometries with defined
sharp features under parametric controls, which is crucial in fields like
industrial design and manufacturing. To bridge this gap, we introduce a
framework that employs Large Language Models (LLMs) to generate text-driven 3D
shapes, manipulating 3D software via program synthesis. We present 3D-PreMise,
a dataset specifically tailored for 3D parametric modeling of industrial
shapes, designed to explore state-of-the-art LLMs within our proposed pipeline.
Our work reveals effective generation strategies and delves into the
self-correction capabilities of LLMs using a visual interface. Our work
highlights both the potential and limitations of LLMs in 3D parametric modeling
for industrial applications.
\\ ( https://arxiv.org/abs/2401.06437 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07106 (*cross-listing*)
Date: Sat, 13 Jan 2024 16:13:45 GMT   (268kb,D)

Title: Directed Regular and Context-Free Languages
Authors: Moses Ganardi, Irmak Saglam, Georg Zetzsche
Categories: cs.FL cs.CL
\\
  We study the problem of deciding whether a given language is directed. A
language $L$ is \emph{directed} if every pair of words in $L$ have a common
(scattered) superword in $L$. Deciding directedness is a fundamental problem in
connection with ideal decompositions of downward closed sets. Another
motivation is that deciding whether two \emph{directed} context-free languages
have the same downward closures can be decided in polynomial time, whereas for
general context-free languages, this problem is known to be coNEXP-complete.
  We show that the directedness problem for regular languages, given as NFAs,
belongs to $AC^1$, and thus polynomial time. Moreover, it is NL-complete for
fixed alphabet sizes. Furthermore, we show that for context-free languages, the
directedness problem is PSPACE-complete.
\\ ( https://arxiv.org/abs/2401.07106 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07123 (*cross-listing*)
Date: Sat, 13 Jan 2024 17:30:57 GMT   (1615kb,D)

Title: One Agent Too Many: User Perspectives on Approaches to Multi-agent
  Conversational AI
Authors: Christopher Clarke, Karthik Krishnamurthy, Walter Talamonti, Yiping
  Kang, Lingjia Tang, Jason Mars
Categories: cs.HC cs.CL
\\
  Conversational agents have been gaining increasing popularity in recent
years. Influenced by the widespread adoption of task-oriented agents such as
Apple Siri and Amazon Alexa, these agents are being deployed into various
applications to enhance user experience. Although these agents promote "ask me
anything" functionality, they are typically built to focus on a single or
finite set of expertise. Given that complex tasks often require more than one
expertise, this results in the users needing to learn and adopt multiple
agents. One approach to alleviate this is to abstract the orchestration of
agents in the background. However, this removes the option of choice and
flexibility, potentially harming the ability to complete tasks. In this paper,
we explore these different interaction experiences (one agent for all) vs (user
choice of agents) for conversational AI. We design prototypes for each,
systematically evaluating their ability to facilitate task completion. Through
a series of conducted user studies, we show that users have a significant
preference for abstracting agent orchestration in both system usability and
system performance. Additionally, we demonstrate that this mode of interaction
is able to provide quality responses that are rated within 1% of human-selected
answers.
\\ ( https://arxiv.org/abs/2401.07123 ,  1615kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07529 (*cross-listing*)
Date: Mon, 15 Jan 2024 08:19:22 GMT   (2762kb,D)

Title: MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of
  Multimodal Large Language Models in Perception
Authors: Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yu Wang, Yanfeng
  Wang
Categories: cs.CV cs.CL
\\
  Multimodal Large Language Models (MLLMs) have shown their remarkable
abilities in visual perception and understanding recently. However, how to
comprehensively evaluate the capabilities of MLLMs remains a challenge. Most of
the existing benchmarks predominantly focus on assessing perception, cognition,
and reasoning, neglecting the abilities of self-awareness, referring to the
model's recognition of its own capability boundary. In our study, we focus on
self-awareness in image perception and introduce the knowledge quadrant for
MLLMs, which clearly defines the knowns and unknowns in perception. Based on
this, we propose a novel benchmark specifically designed to evaluate the
Self-Aware capabilities in Perception for MLLMs(MM-SAP). MM-SAP encompasses
three distinct sub-datasets, each focusing on different aspects of
self-awareness. We evaluated eight well-known MLLMs using MM-SAP, analyzing
their self-awareness and providing detailed insights. Code and data are
available at https://github.com/YHWmz/MM-SAP
\\ ( https://arxiv.org/abs/2401.07529 ,  2762kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07572 (*cross-listing*)
Date: Mon, 15 Jan 2024 10:16:44 GMT   (2081kb,D)

Title: Exploiting GPT-4 Vision for Zero-shot Point Cloud Understanding
Authors: Qi Sun, Xiao Cui, Wengang Zhou and Houqiang Li
Categories: cs.CV cs.CL
\\
  In this study, we tackle the challenge of classifying the object category in
point clouds, which previous works like PointCLIP struggle to address due to
the inherent limitations of the CLIP architecture. Our approach leverages GPT-4
Vision (GPT-4V) to overcome these challenges by employing its advanced
generative abilities, enabling a more adaptive and robust classification
process. We adapt the application of GPT-4V to process complex 3D data,
enabling it to achieve zero-shot recognition capabilities without altering the
underlying model architecture. Our methodology also includes a systematic
strategy for point cloud image visualization, mitigating domain gap and
enhancing GPT-4V's efficiency. Experimental validation demonstrates our
approach's superiority in diverse scenarios, setting a new benchmark in
zero-shot point cloud classification.
\\ ( https://arxiv.org/abs/2401.07572 ,  2081kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07967 (*cross-listing*)
Date: Mon, 15 Jan 2024 21:10:19 GMT   (164kb,D)

Title: MCMChaos: Improvising Rap Music with MCMC Methods and Chaos Theory
Authors: Robert G. Kimelman
Categories: cs.SD cs.CL cs.HC eess.AS
\\
  A novel freestyle rap software, MCMChaos 0.0.1, based on rap music
transcriptions created in previous research is presented. The software has
three different versions, each making use of different mathematical simulation
methods: collapsed gibbs sampler and lorenz attractor simulation. As far as we
know, these simulation methods have never been used in rap music generation
before. The software implements Python Text-to-Speech processing (pyttxs) to
convert text wrangled from the MCFlow corpus into English speech. In each
version, values simulated from each respective mathematical model alter the
rate of speech, volume, and (in the multiple voice case) the voice of the
text-to-speech engine on a line-by-line basis. The user of the software is
presented with a real-time graphical user interface (GUI) which instantaneously
changes the initial values read into the mathematical simulation methods.
Future research might attempt to allow for more user control and autonomy.
\\ ( https://arxiv.org/abs/2401.07967 ,  164kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07994 (*cross-listing*)
Date: Mon, 15 Jan 2024 22:36:31 GMT   (184kb,D)

Title: A Novel Approach for Automatic Program Repair using Round-Trip
  Translation with Large Language Models
Authors: Fernando Vallecillos Ruiz and Anastasiia Grishina and Max Hort and
  Leon Moonen
Categories: cs.SE cs.CL cs.LG
\\
  Research shows that grammatical mistakes in a sentence can be corrected by
translating it to another language and back using neural machine translation
with language models. We investigate whether this correction capability of
Large Language Models (LLMs) extends to Automatic Program Repair (APR). Current
generative models for APR are pre-trained on source code and fine-tuned for
repair. This paper proposes bypassing the fine-tuning step and using Round-Trip
Translation (RTT): translation of code from one programming language to another
programming or natural language, and back. We hypothesize that RTT with LLMs
restores the most commonly seen patterns in code during pre-training, i.e.,
performs a regression toward the mean, which removes bugs as they are a form of
noise w.r.t. the more frequent, natural, bug-free code in the training data. To
test this hypothesis, we employ eight recent LLMs pre-trained on code,
including the latest GPT versions, and four common program repair benchmarks in
Java. We find that RTT with English as an intermediate language repaired 101 of
164 bugs with GPT-4 on the HumanEval-Java dataset. Moreover, 46 of these are
unique bugs that are not repaired by other LLMs fine-tuned for APR. Our
findings highlight the viability of round-trip translation with LLMs as a
technique for automated program repair and its potential for research in
software engineering.
  Keywords: automated program repair, large language model, machine translation
\\ ( https://arxiv.org/abs/2401.07994 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08206 (*cross-listing*)
Date: Tue, 16 Jan 2024 08:44:29 GMT   (2469kb,D)

Title: Generative Multi-Modal Knowledge Retrieval with Large Language Models
Authors: Xinwei Long, Jiali Zeng, Fandong Meng, Zhiyuan Ma, Kaiyan Zhang, Bowen
  Zhou, Jie Zhou
Categories: cs.IR cs.CL
Comments: Accepted to AAAI 2024
\\
  Knowledge retrieval with multi-modal queries plays a crucial role in
supporting knowledge-intensive multi-modal applications. However, existing
methods face challenges in terms of their effectiveness and training
efficiency, especially when it comes to training and integrating multiple
retrievers to handle multi-modal queries. In this paper, we propose an
innovative end-to-end generative framework for multi-modal knowledge retrieval.
Our framework takes advantage of the fact that large language models (LLMs) can
effectively serve as virtual knowledge bases, even when trained with limited
data. We retrieve knowledge via a two-step process: 1) generating knowledge
clues related to the queries, and 2) obtaining the relevant document by
searching databases using the knowledge clue. In particular, we first introduce
an object-aware prefix-tuning technique to guide multi-grained visual learning.
Then, we align multi-grained visual features into the textual feature space of
the LLM, employing the LLM to capture cross-modal interactions. Subsequently,
we construct instruction data with a unified format for model training.
Finally, we propose the knowledge-guided generation strategy to impose prior
constraints in the decoding steps, thereby promoting the generation of
distinctive knowledge clues. Through experiments conducted on three benchmarks,
we demonstrate significant improvements ranging from 3.0% to 14.6% across all
evaluation metrics when compared to strong baselines.
\\ ( https://arxiv.org/abs/2401.08206 ,  2469kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08276 (*cross-listing*)
Date: Tue, 16 Jan 2024 10:58:07 GMT   (430kb,D)

Title: AesBench: An Expert Benchmark for Multimodal Large Language Models on
  Image Aesthetics Perception
Authors: Yipo Huang, Quan Yuan, Xiangfei Sheng, Zhichao Yang, Haoning Wu,
  Pengfei Chen, Yuzhe Yang, Leida Li, Weisi Lin
Categories: cs.CV cs.CL
\\
  With collective endeavors, multimodal large language models (MLLMs) are
undergoing a flourishing development. However, their performances on image
aesthetics perception remain indeterminate, which is highly desired in
real-world applications. An obvious obstacle lies in the absence of a specific
benchmark to evaluate the effectiveness of MLLMs on aesthetic perception. This
blind groping may impede the further development of more advanced MLLMs with
aesthetic perception capacity. To address this dilemma, we propose AesBench, an
expert benchmark aiming to comprehensively evaluate the aesthetic perception
capacities of MLLMs through elaborate design across dual facets. (1) We
construct an Expert-labeled Aesthetics Perception Database (EAPD), which
features diversified image contents and high-quality annotations provided by
professional aesthetic experts. (2) We propose a set of integrative criteria to
measure the aesthetic perception abilities of MLLMs from four perspectives,
including Perception (AesP), Empathy (AesE), Assessment (AesA) and
Interpretation (AesI). Extensive experimental results underscore that the
current MLLMs only possess rudimentary aesthetic perception ability, and there
is still a significant gap between MLLMs and humans. We hope this work can
inspire the community to engage in deeper explorations on the aesthetic
potentials of MLLMs. Source data will be available at
https://github.com/yipoh/AesBench.
\\ ( https://arxiv.org/abs/2401.08276 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08392 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:33:09 GMT   (5733kb,D)

Title: DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language
  Models
Authors: Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, Yi Yang
Categories: cs.CV cs.CL
\\
  The field of AI agents is advancing at an unprecedented rate due to the
capabilities of large language models (LLMs). However, LLM-driven visual agents
mainly focus on solving tasks for the image modality, which limits their
ability to understand the dynamic nature of the real world, making it still far
from real-life applications, e.g., guiding students in laboratory experiments
and identifying their mistakes. Considering the video modality better reflects
the ever-changing and perceptually intensive nature of real-world scenarios, we
devise DoraemonGPT, a comprehensive and conceptually elegant system driven by
LLMs to handle dynamic video tasks. Given a video with a question/task,
DoraemonGPT begins by converting the input video with massive content into a
symbolic memory that stores \textit{task-related} attributes. This structured
representation allows for spatial-temporal querying and reasoning by sub-task
tools, resulting in concise and relevant intermediate results. Recognizing that
LLMs have limited internal knowledge when it comes to specialized domains
(e.g., analyzing the scientific principles underlying experiments), we
incorporate plug-and-play tools to assess external knowledge and address tasks
across different domains. Moreover, we introduce a novel LLM-driven planner
based on Monte Carlo Tree Search to efficiently explore the large planning
space for scheduling various tools. The planner iteratively finds feasible
solutions by backpropagating the result's reward, and multiple solutions can be
summarized into an improved final answer. We extensively evaluate DoraemonGPT
in dynamic scenes and provide in-the-wild showcases demonstrating its ability
to handle more complex questions than previous studies.
\\ ( https://arxiv.org/abs/2401.08392 ,  5733kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17816 (*cross-listing*)
Date: Wed, 29 Nov 2023 17:10:49 GMT   (2804kb,D)

Title: Fixed point actions from convolutional neural networks
Authors: Kieran Holland, Andreas Ipp, David I. M\"uller, Urs Wenger
Categories: hep-lat cs.LG stat.ML
Comments: 9 pages, 5 figures; Proceedings of the 40th International Symposium
  on Lattice Field Theory (Lattice 2023)
\\
  Lattice gauge-equivariant convolutional neural networks (L-CNNs) can be used
to form arbitrarily shaped Wilson loops and can approximate any gauge-covariant
or gauge-invariant function on the lattice. Here we use L-CNNs to describe
fixed point (FP) actions which are based on renormalization group
transformations. FP actions are classically perfect, i.e., they have no lattice
artifacts on classical gauge-field configurations satisfying the equations of
motion, and therefore possess scale invariant instanton solutions. FP actions
are tree-level Symanzik-improved to all orders in the lattice spacing and can
produce physical predictions with very small lattice artifacts even on coarse
lattices. We find that L-CNNs are much more accurate at parametrizing the FP
action compared to older approaches. They may therefore provide a way to
circumvent critical slowing down and topological freezing towards the continuum
limit.
\\ ( https://arxiv.org/abs/2311.17816 ,  2804kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06481 (*cross-listing*)
Date: Fri, 12 Jan 2024 10:03:00 GMT   (1953kb,D)

Title: Machine learning a fixed point action for SU(3) gauge theory with a
  gauge equivariant convolutional neural network
Authors: Kieran Holland, Andreas Ipp, David I. M\"uller, Urs Wenger
Categories: hep-lat cs.LG stat.ML
Comments: 22 pages, 15 figures, 6 tables
\\
  Fixed point lattice actions are designed to have continuum classical
properties unaffected by discretization effects and reduced lattice artifacts
at the quantum level. They provide a possible way to extract continuum physics
with coarser lattices, thereby allowing to circumvent problems with critical
slowing down and topological freezing toward the continuum limit. A crucial
ingredient for practical applications is to find an accurate and compact
parametrization of a fixed point action, since many of its properties are only
implicitly defined. Here we use machine learning methods to revisit the
question of how to parametrize fixed point actions. In particular, we obtain a
fixed point action for four-dimensional SU(3) gauge theory using convolutional
neural networks with exact gauge invariance. The large operator space allows us
to find superior parametrizations compared to previous studies, a necessary
first step for future Monte Carlo simulations.
\\ ( https://arxiv.org/abs/2401.06481 ,  1953kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06771 (*cross-listing*)
Date: Sun, 24 Sep 2023 06:44:51 GMT   (763kb)

Title: Curiosity as a Self-Supervised Method to Improve Exploration in De novo
  Drug Design
Authors: Mohamed-Amine Chadi, Hajar Mousannif, Ahmed Aamouche
Categories: q-bio.QM cs.LG q-bio.BM
DOI: 10.1109/ICITRI59340.2023.10249596
\\
  In recent years, deep learning has demonstrated promising results in de novo
drug design. However, the proposed techniques still lack an efficient
exploration of the large chemical space. Most of these methods explore a small
fragment of the chemical space of known drugs, if the desired molecules were
not found, the process ends. In this work, we introduce a curiosity-driven
method to force the model to navigate many parts of the chemical space,
therefore, achieving higher desirability and diversity as well. At first, we
train a recurrent neural network-based general molecular generator (G), then we
fine-tune G to maximize curiosity and desirability. We define curiosity as the
Tanimoto similarity between two generated molecules, a first molecule generated
by G, and a second one generated by a copy of G (Gcopy). We only backpropagate
the loss through G while keeping Gcopy unchanged. We benchmarked our approach
against two desirable chemical properties related to drug-likeness and showed
that the discovered chemical space can be significantly expanded, thus,
discovering a higher number of desirable molecules with more diversity and
potentially easier to synthesize. All Code and data used in this paper are
available at https://github.com/amine179/Curiosity-RL-for-Drug-Design.
\\ ( https://arxiv.org/abs/2401.06771 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06779 (*cross-listing*)
Date: Mon, 25 Dec 2023 04:04:47 GMT   (2590kb,D)

Title: VAE for Modified 1-Hot Generative Materials Modeling, A Step Towards
  Inverse Material Design
Authors: Khalid El-Awady
Categories: cond-mat.mtrl-sci cs.LG
Comments: 8 pages, 8 figures
\\
  We investigate the construction of generative models capable of encoding
physical constraints that can be hard to express explicitly. For the problem of
inverse material design, where one seeks to design a material with a prescribed
set of properties, a significant challenge is ensuring synthetic viability of a
proposed new material. We encode an implicit dataset relationships, namely that
certain materials can be decomposed into other ones in the dataset, and present
a VAE model capable of preserving this property in the latent space and
generating new samples with the same. This is particularly useful in sequential
inverse material design, an emergent research area that seeks to design a
material with specific properties by sequentially adding (or removing) elements
using policies trained through deep reinforcement learning.
\\ ( https://arxiv.org/abs/2401.06779 ,  2590kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06820 (*cross-listing*)
Date: Thu, 11 Jan 2024 20:17:44 GMT   (282kb,D)

Title: QCQP-Net: Reliably Learning Feasible Alternating Current Optimal Power
  Flow Solutions Under Constraints
Authors: Sihan Zeng, Youngdae Kim, Yuxuan Ren, Kibaek Kim
Categories: math.OC cs.LG
\\
  At the heart of power system operations, alternating current optimal power
flow (ACOPF) studies the generation of electric power in the most economical
way under network-wide load requirement, and can be formulated as a highly
structured non-convex quadratically constrained quadratic program (QCQP).
Optimization-based solutions to ACOPF (such as ADMM or interior-point method),
as the classic approach, require large amount of computation and cannot meet
the need to repeatedly solve the problem as load requirement frequently
changes. On the other hand, learning-based methods that directly predict the
ACOPF solution given the load input incur little computational cost but often
generates infeasible solutions (i.e. violate the constraints of ACOPF). In this
work, we combine the best of both worlds -- we propose an innovated framework
for learning ACOPF, where the input load is mapped to the ACOPF solution
through a neural network in a computationally efficient and reliable manner.
Key to our innovation is a specific-purpose "activation function" defined
implicitly by a QCQP and a novel loss, which enforce constraint satisfaction.
We show through numerical simulations that our proposed method achieves
superior feasibility rate and generation cost in situations where the existing
learning-based approaches fail.
\\ ( https://arxiv.org/abs/2401.06820 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06823 (*cross-listing*)
Date: Thu, 11 Jan 2024 23:59:37 GMT   (946kb,D)

Title: Interpretable deep learning in single-cell omics
Authors: Manoj M Wagle, Siqu Long, Carissa Chen, Chunlei Liu, Pengyi Yang
Categories: q-bio.GN cs.LG
\\
  Recent developments in single-cell omics technologies have enabled the
quantification of molecular profiles in individual cells at an unparalleled
resolution. Deep learning, a rapidly evolving sub-field of machine learning,
has instilled a significant interest in single-cell omics research due to its
remarkable success in analysing heterogeneous high-dimensional single-cell
omics data. Nevertheless, the inherent multi-layer nonlinear architecture of
deep learning models often makes them `black boxes' as the reasoning behind
predictions is often unknown and not transparent to the user. This has
stimulated an increasing body of research for addressing the lack of
interpretability in deep learning models, especially in single-cell omics data
analyses, where the identification and understanding of molecular regulators
are crucial for interpreting model predictions and directing downstream
experimental validations. In this work, we introduce the basics of single-cell
omics technologies and the concept of interpretable deep learning. This is
followed by a review of the recent interpretable deep learning models applied
to various single-cell omics research. Lastly, we highlight the current
limitations and discuss potential future directions. We anticipate this review
to bring together the single-cell and machine learning research communities to
foster future development and application of interpretable deep learning in
single-cell omics research.
\\ ( https://arxiv.org/abs/2401.06823 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06834 (*cross-listing*)
Date: Fri, 12 Jan 2024 15:45:56 GMT   (3957kb,D)

Title: Optimization of Discrete Parameters Using the Adaptive Gradient Method
  and Directed Evolution
Authors: Andrei Beinarovich, Sergey Stepanov, Alexander Zaslavsky
Categories: math.OC cs.LG cs.NE
Comments: 19 pages, 12 figures
\\
  The problem is considered of optimizing discrete parameters in the presence
of constraints. We use the stochastic sigmoid with temperature and put forward
the new adaptive gradient method CONGA. The search for an optimal solution is
carried out by a population of individuals. Each of them varies according to
gradients of the 'environment' and is characterized by two temperature
parameters with different annealing schedules. Unadapted individuals die, and
optimal ones interbreed, the result is directed evolutionary dynamics. The
proposed method is illustrated using the well-known combinatorial problem for
optimal packing of a backpack (0-1 KP).
\\ ( https://arxiv.org/abs/2401.06834 ,  3957kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06839 (*cross-listing*)
Date: Fri, 12 Jan 2024 19:00:00 GMT   (2706kb,D)

Title: Inferring Stellar Parameters from Iodine-Imprinted Keck/HIRES Spectra
  with Machine Learning
Authors: Jude Gussman and Malena Rice
Categories: astro-ph.EP astro-ph.SR cs.LG
Comments: 7 pages, 3 figures, accepted to ApJL
\\
  The properties of exoplanet host stars are traditionally characterized
through a detailed forward-modeling analysis of high-resolution spectra.
However, many exoplanet radial velocity surveys employ iodine-cell-calibrated
spectrographs, such that the vast majority of spectra obtained include an
imprinted forest of iodine absorption lines. For surveys that use iodine cells,
iodine-free "template" spectra must be separately obtained for precise stellar
characterization. These template spectra often require extensive additional
observing time to obtain, and they are not always feasible to obtain for faint
stars. In this paper, we demonstrate that machine learning methods can be
applied to infer stellar parameters and chemical abundances from
iodine-imprinted spectra with high accuracy and precision. The methods
presented in this work are broadly applicable to any iodine-cell-calibrated
spectrograph. We make publicly available our spectroscopic pipeline, the Cannon
HIRES Iodine Pipeline (CHIP), which derives stellar parameters and 15 chemical
abundances from iodine-imprinted spectra of FGK stars and which has been set up
for ease of use with Keck/HIRES spectra. Our proof-of-concept offers an
efficient new avenue to rapidly estimate a large number of stellar parameters
even in the absence of an iodine-free template spectrum.
\\ ( https://arxiv.org/abs/2401.06839 ,  2706kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06864 (*cross-listing*)
Date: Fri, 12 Jan 2024 19:35:54 GMT   (3391kb,D)

Title: Deep Learning With DAGs
Authors: Sourabh Balgi, Adel Daoud, Jose M. Pe\~na, Geoffrey T. Wodtke and
  Jesse Zhou
Categories: stat.ML cs.LG econ.EM stat.ME
\\
  Social science theories often postulate causal relationships among a set of
variables or events. Although directed acyclic graphs (DAGs) are increasingly
used to represent these theories, their full potential has not yet been
realized in practice. As non-parametric causal models, DAGs require no
assumptions about the functional form of the hypothesized relationships.
Nevertheless, to simplify the task of empirical evaluation, researchers tend to
invoke such assumptions anyway, even though they are typically arbitrary and do
not reflect any theoretical content or prior knowledge. Moreover, functional
form assumptions can engender bias, whenever they fail to accurately capture
the complexity of the causal system under investigation. In this article, we
introduce causal-graphical normalizing flows (cGNFs), a novel approach to
causal inference that leverages deep neural networks to empirically evaluate
theories represented as DAGs. Unlike conventional approaches, cGNFs model the
full joint distribution of the data according to a DAG supplied by the analyst,
without relying on stringent assumptions about functional form. In this way,
the method allows for flexible, semi-parametric estimation of any causal
estimand that can be identified from the DAG, including total effects,
conditional effects, direct and indirect effects, and path-specific effects. We
illustrate the method with a reanalysis of Blau and Duncan's (1967) model of
status attainment and Zhou's (2019) model of conditional versus controlled
mobility. To facilitate adoption, we provide open-source software together with
a series of online tutorials for implementing cGNFs. The article concludes with
a discussion of current limitations and directions for future development.
\\ ( https://arxiv.org/abs/2401.06864 ,  3391kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06885 (*cross-listing*)
Date: Fri, 12 Jan 2024 20:32:38 GMT   (2058kb)

Title: Accelerating Neural Networks for Large Language Models and Graph
  Processing with Silicon Photonics
Authors: Salma Afifi, Febin Sunny, Mahdi Nikdast, Sudeep Pasricha
Categories: cs.AR cs.LG
\\
  In the rapidly evolving landscape of artificial intelligence, large language
models (LLMs) and graph processing have emerged as transformative technologies
for natural language processing (NLP), computer vision, and graph-structured
data applications. However, the complex structures of these models pose
challenges for acceleration on conventional electronic platforms. In this
paper, we describe novel hardware accelerators based on silicon photonics to
accelerate transformer neural networks that are used in LLMs and graph neural
networks for graph data processing. Our analysis demonstrates that both
hardware accelerators achieve at least 10.2x throughput improvement and 3.8x
better energy efficiency over multiple state-of-the-art electronic hardware
accelerators designed for LLMs and graph processing.
\\ ( https://arxiv.org/abs/2401.06885 ,  2058kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06913 (*cross-listing*)
Date: Fri, 12 Jan 2024 21:59:01 GMT   (12212kb,D)

Title: Microphone Conversion: Mitigating Device Variability in Sound Event
  Classification
Authors: Myeonghoon Ryu, Hongseok Oh, Suji Lee and Han Park
Categories: cs.SD cs.LG cs.MM eess.AS
Comments: Accepted to ICASSP 2024
\\
  In this study, we introduce a new augmentation technique to enhance the
resilience of sound event classification (SEC) systems against device
variability through the use of CycleGAN. We also present a unique dataset to
evaluate this method. As SEC systems become increasingly common, it is crucial
that they work well with audio from diverse recording devices. Our method
addresses limited device diversity in training data by enabling unpaired
training to transform input spectrograms as if they are recorded on a different
device. Our experiments show that our approach outperforms existing methods in
generalization by 5.2% - 11.5% in weighted f1 score. Additionally, it surpasses
the current methods in adaptability across diverse recording devices by
achieving a 6.5% - 12.8% improvement in weighted f1 score.
\\ ( https://arxiv.org/abs/2401.06913 ,  12212kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06967 (*cross-listing*)
Date: Sat, 13 Jan 2024 03:41:54 GMT   (374kb)

Title: NHANES-GCP: Leveraging the Google Cloud Platform and BigQuery ML for
  reproducible machine learning with data from the National Health and
  Nutrition Examination Survey
Authors: B. Ross Katz, Abdul Khan, James York-Winegar, and Alexander J. Titus
Categories: q-bio.QM cs.LG stat.AP
Comments: 7 pages, 1 figure
\\
  Summary: NHANES, the National Health and Nutrition Examination Survey, is a
program of studies led by the Centers for Disease Control and Prevention (CDC)
designed to assess the health and nutritional status of adults and children in
the United States (U.S.). NHANES data is frequently used by biostatisticians
and clinical scientists to study health trends across the U.S., but every
analysis requires extensive data management and cleaning before use and this
repetitive data engineering collectively costs valuable research time and
decreases the reproducibility of analyses. Here, we introduce NHANES-GCP, a
Cloud Development Kit for Terraform (CDKTF) Infrastructure-as-Code (IaC) and
Data Build Tool (dbt) resources built on the Google Cloud Platform (GCP) that
automates the data engineering and management aspects of working with NHANES
data. With current GCP pricing, NHANES-GCP costs less than $2 to run and less
than $15/yr of ongoing costs for hosting the NHANES data, all while providing
researchers with clean data tables that can readily be integrated for
large-scale analyses. We provide examples of leveraging BigQuery ML to carry
out the process of selecting data, integrating data, training machine learning
and statistical models, and generating results all from a single SQL-like
query. NHANES-GCP is designed to enhance the reproducibility of analyses and
create a well-engineered NHANES data resource for statistics, machine learning,
and fine-tuning Large Language Models (LLMs).
  Availability and implementation" NHANES-GCP is available at
https://github.com/In-Vivo-Group/NHANES-GCP
\\ ( https://arxiv.org/abs/2401.06967 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07003 (*cross-listing*)
Date: Sat, 13 Jan 2024 07:26:47 GMT   (5882kb,D)

Title: Deep Neural Network Solutions for Oscillatory Fredholm Integral
  Equations
Authors: Jie Jiang and Yuesheng Xu
Categories: math.NA cs.LG cs.NA
\\
  We studied the use of deep neural networks (DNNs) in the numerical solution
of the oscillatory Fredholm integral equation of the second kind. It is known
that the solution of the equation exhibits certain oscillatory behaviors due to
the oscillation of the kernel. It was pointed out recently that standard DNNs
favour low frequency functions, and as a result, they often produce poor
approximation for functions containing high frequency components. We addressed
this issue in this study. We first developed a numerical method for solving the
equation with DNNs as an approximate solution by designing a numerical
quadrature that tailors to computing oscillatory integrals involving DNNs. We
proved that the error of the DNN approximate solution of the equation is
bounded by the training loss and the quadrature error. We then proposed a
multi-grade deep learning (MGDL) model to overcome the spectral bias issue of
neural networks. Numerical experiments demonstrate that the MGDL model is
effective in extracting multiscale information of the oscillatory solution and
overcoming the spectral bias issue from which a standard DNN model suffers.
\\ ( https://arxiv.org/abs/2401.07003 ,  5882kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07039 (*cross-listing*)
Date: Sat, 13 Jan 2024 10:56:34 GMT   (1065kb,D)

Title: Quantum Generative Diffusion Model
Authors: Chuangtao Chen and Qinglin Zhao
Categories: quant-ph cs.LG
Comments: Comments are welcome. 11 pages, 11 figures
\\
  This paper introduces the Quantum Generative Diffusion Model (QGDM), a fully
quantum-mechanical model for generating quantum state ensembles, inspired by
Denoising Diffusion Probabilistic Models. QGDM features a diffusion process
that introduces timestep-dependent noise into quantum states, paired with a
denoising mechanism trained to reverse this contamination. This model
efficiently evolves a completely mixed state into a target quantum state
post-training. Our comparative analysis with Quantum Generative Adversarial
Networks demonstrates QGDM's superiority, with fidelity metrics exceeding 0.99
in numerical simulations involving up to 4 qubits. Additionally, we present a
Resource-Efficient version of QGDM (RE-QGDM), which minimizes the need for
auxiliary qubits while maintaining impressive generative capabilities for tasks
involving up to 8 qubits. These results showcase the proposed models' potential
for tackling challenging quantum generation problems.
\\ ( https://arxiv.org/abs/2401.07039 ,  1065kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07087 (*cross-listing*)
Date: Sat, 13 Jan 2024 14:34:18 GMT   (28486kb,D)

Title: Exploring Adversarial Attacks against Latent Diffusion Model from the
  Perspective of Adversarial Transferability
Authors: Junxi Chen, Junhao Dong, Xiaohua Xie
Categories: cs.CV cs.CR cs.LG
Comments: 24 pages, 13 figures
\\
  Recently, many studies utilized adversarial examples (AEs) to raise the cost
of malicious image editing and copyright violation powered by latent diffusion
models (LDMs). Despite their successes, a few have studied the surrogate model
they used to generate AEs. In this paper, from the perspective of adversarial
transferability, we investigate how the surrogate model's property influences
the performance of AEs for LDMs. Specifically, we view the time-step sampling
in the Monte-Carlo-based (MC-based) adversarial attack as selecting surrogate
models. We find that the smoothness of surrogate models at different time steps
differs, and we substantially improve the performance of the MC-based AEs by
selecting smoother surrogate models. In the light of the theoretical framework
on adversarial transferability in image classification, we also conduct a
theoretical analysis to explain why smooth surrogate models can also boost AEs
for LDMs.
\\ ( https://arxiv.org/abs/2401.07087 ,  28486kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07119 (*cross-listing*)
Date: Sat, 13 Jan 2024 17:08:09 GMT   (1089kb,D)

Title: Curator: Efficient Indexing for Multi-Tenant Vector Databases
Authors: Yicheng Jin, Yongji Wu, Wenjun Hu, Bruce M. Maggs, Xiao Zhang, Danyang
  Zhuo
Categories: cs.DB cs.DC cs.IR cs.LG
\\
  Vector databases have emerged as key enablers for bridging intelligent
applications with unstructured data, providing generic search and management
support for embedding vectors extracted from the raw unstructured data. As
multiple data users can share the same database infrastructure, multi-tenancy
support for vector databases is increasingly desirable. This hinges on an
efficient filtered search operation, i.e., only querying the vectors accessible
to a particular tenant. Multi-tenancy in vector databases is currently achieved
by building either a single, shared index among all tenants, or a per-tenant
index. The former optimizes for memory efficiency at the expense of search
performance, while the latter does the opposite. Instead, this paper presents
Curator, an in-memory vector index design tailored for multi-tenant queries
that simultaneously achieves the two conflicting goals, low memory overhead and
high performance for queries, vector insertion, and deletion. Curator indexes
each tenant's vectors with a tenant-specific clustering tree and encodes these
trees compactly as sub-trees of a shared clustering tree. Each tenant's
clustering tree adapts dynamically to its unique vector distribution, while
maintaining a low per-tenant memory footprint. Our evaluation, based on two
widely used data sets, confirms that Curator delivers search performance on par
with per-tenant indexing, while maintaining memory consumption at the same
level as metadata filtering on a single, shared index.
\\ ( https://arxiv.org/abs/2401.07119 ,  1089kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07124 (*cross-listing*)
Date: Sat, 13 Jan 2024 17:31:12 GMT   (803kb)

Title: Concrete Surface Crack Detection with Convolutional-based Deep Learning
  Models
Authors: Sara Shomal Zadeh, Sina Aalipour birgani, Meisam Khorshidi, Farhad
  Kooban
Categories: cs.CV cs.LG eess.IV
Comments: 11 pages, 3 figures, Journal paper
Journal-ref: International Journal of Novel Research in Civil Structural and
  Earth Sciences, Vol. 10, Issue 3, (2023) pp: (25-35)
DOI: 10.5281/zenodo.10061654
\\
  Effective crack detection is pivotal for the structural health monitoring and
inspection of buildings. This task presents a formidable challenge to computer
vision techniques due to the inherently subtle nature of cracks, which often
exhibit low-level features that can be easily confounded with background
textures, foreign objects, or irregularities in construction. Furthermore, the
presence of issues like non-uniform lighting and construction irregularities
poses significant hurdles for autonomous crack detection during building
inspection and monitoring. Convolutional neural networks (CNNs) have emerged as
a promising framework for crack detection, offering high levels of accuracy and
precision. Additionally, the ability to adapt pre-trained networks through
transfer learning provides a valuable tool for users, eliminating the need for
an in-depth understanding of algorithm intricacies. Nevertheless, it is
imperative to acknowledge the limitations and considerations when deploying
CNNs, particularly in contexts where the outcomes carry immense significance,
such as crack detection in buildings. In this paper, our approach to surface
crack detection involves the utilization of various deep-learning models.
Specifically, we employ fine-tuning techniques on pre-trained deep learning
architectures: VGG19, ResNet50, Inception V3, and EfficientNetV2. These models
are chosen for their established performance and versatility in image analysis
tasks. We compare deep learning models using precision, recall, and F1 scores.
\\ ( https://arxiv.org/abs/2401.07124 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07154 (*cross-listing*)
Date: Sat, 13 Jan 2024 20:03:11 GMT   (539kb,D)

Title: Discovering Command and Control Channels Using Reinforcement Learning
Authors: Cheng Wang, Akshay Kakkar, Christopher Redino, Abdul Rahman, Ajinsyam
  S, Ryan Clark, Daniel Radke, Tyler Cody, Lanxiao Huang, Edward Bowen
Categories: cs.CR cs.LG
Comments: SoutheastCon 2023. IEEE, 2023
DOI: 10.1109/SoutheastCon51012.2023.10115173
\\
  Command and control (C2) paths for issuing commands to malware are sometimes
the only indicators of its existence within networks. Identifying potential C2
channels is often a manually driven process that involves a deep understanding
of cyber tradecraft. Efforts to improve discovery of these channels through
using a reinforcement learning (RL) based approach that learns to automatically
carry out C2 attack campaigns on large networks, where multiple defense layers
are in place serves to drive efficiency for network operators. In this paper,
we model C2 traffic flow as a three-stage process and formulate it as a Markov
decision process (MDP) with the objective to maximize the number of valuable
hosts whose data is exfiltrated. The approach also specifically models payload
and defense mechanisms such as firewalls which is a novel contribution. The
attack paths learned by the RL agent can in turn help the blue team identify
high-priority vulnerabilities and develop improved defense strategies. The
method is evaluated on a large network with more than a thousand hosts and the
results demonstrate that the agent can effectively learn attack paths while
avoiding firewalls.
\\ ( https://arxiv.org/abs/2401.07154 ,  539kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07174 (*cross-listing*)
Date: Sat, 13 Jan 2024 23:38:10 GMT   (6203kb,D)

Title: On the (In)Compatibility between Group Fairness and Individual Fairness
Authors: Shizhou Xu and Thomas Strohmer
Categories: math.ST cs.CY cs.LG stat.ML stat.TH
Comments: 32 pages, 3 figures
\\
  We study the compatibility between the optimal statistical parity solutions
and individual fairness. While individual fairness seeks to treat similar
individuals similarly, optimal statistical parity aims to provide similar
treatment to individuals who share relative similarity within their respective
sensitive groups. The two fairness perspectives, while both desirable from a
fairness perspective, often come into conflict in applications. Our goal in
this work is to analyze the existence of this conflict and its potential
solution. In particular, we establish sufficient (sharp) conditions for the
compatibility between the optimal (post-processing) statistical parity $L^2$
learning and the ($K$-Lipschitz or $(\epsilon,\delta)$) individual fairness
requirements. Furthermore, when there exists a conflict between the two, we
first relax the former to the Pareto frontier (or equivalently the optimal
trade-off) between $L^2$ error and statistical disparity, and then analyze the
compatibility between the frontier and the individual fairness requirements.
Our analysis identifies regions along the Pareto frontier that satisfy
individual fairness requirements. (Lastly, we provide individual fairness
guarantees for the composition of a trained model and the optimal
post-processing step so that one can determine the compatibility of the
post-processed model.) This provides practitioners with a valuable approach to
attain Pareto optimality for statistical parity while adhering to the
constraints of individual fairness.
\\ ( https://arxiv.org/abs/2401.07174 ,  6203kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07187 (*cross-listing*)
Date: Sun, 14 Jan 2024 02:30:19 GMT   (70kb)

Title: A Survey on Statistical Theory of Deep Learning: Approximation, Training
  Dynamics, and Generative Models
Authors: Namjoon Suh and Guang Cheng
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 33 pages, no figures,Invited for review in Annual Review of
  Statistics and Its Application (In review)
\\
  In this article, we review the literature on statistical theories of neural
networks from three perspectives. In the first part, results on excess risks
for neural networks are reviewed in the nonparametric framework of regression
or classification. These results rely on explicit constructions of neural
networks, leading to fast convergence rates of excess risks, in that tools from
the approximation theory are adopted. Through these constructions, the width
and depth of the networks can be expressed in terms of sample size, data
dimension, and function smoothness. Nonetheless, their underlying analysis only
applies to the global minimizer in the highly non-convex landscape of deep
neural networks. This motivates us to review the training dynamics of neural
networks in the second part. Specifically, we review papers that attempt to
answer ``how the neural network trained via gradient-based methods finds the
solution that can generalize well on unseen data.'' In particular, two
well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm,
and Mean-Field (MF) paradigm. In the last part, we review the most recent
theoretical advancements in generative models including Generative Adversarial
Networks (GANs), diffusion models, and in-context learning (ICL) in the Large
Language Models (LLMs). The former two models are known to be the main pillars
of the modern generative AI era, while ICL is a strong capability of LLMs in
learning from a few examples in the context. Finally, we conclude the paper by
suggesting several promising directions for deep learning theory.
\\ ( https://arxiv.org/abs/2401.07187 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07200 (*cross-listing*)
Date: Sun, 14 Jan 2024 04:37:17 GMT   (12141kb,D)

Title: Exploring Compressed Image Representation as a Perceptual Proxy: A Study
Authors: Chen-Hsiu Huang and Ja-Ling Wu
Categories: cs.CV cs.LG eess.IV
\\
  We propose an end-to-end learned image compression codec wherein the analysis
transform is jointly trained with an object classification task. This study
affirms that the compressed latent representation can predict human perceptual
distance judgments with an accuracy comparable to a custom-tailored DNN-based
quality metric. We further investigate various neural encoders and demonstrate
the effectiveness of employing the analysis transform as a perceptual loss
network for image tasks beyond quality judgments. Our experiments show that the
off-the-shelf neural encoder proves proficient in perceptual modeling without
needing an additional VGG network. We expect this research to serve as a
valuable reference developing of a semantic-aware and coding-efficient neural
encoder.
\\ ( https://arxiv.org/abs/2401.07200 ,  12141kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07205 (*cross-listing*)
Date: Sun, 14 Jan 2024 05:06:42 GMT   (6139kb,D)

Title: Crafter: Facial Feature Crafting against Inversion-based Identity Theft
  on Deep Models
Authors: Shiming Wang, Zhe Ji, Liyao Xiang, Hao Zhang, Xinbing Wang, Chenghu
  Zhou, Bo Li
Categories: cs.CR cs.CV cs.LG
\\
  With the increased capabilities at the edge (e.g., mobile device) and more
stringent privacy requirement, it becomes a recent trend for deep
learning-enabled applications to pre-process sensitive raw data at the edge and
transmit the features to the backend cloud for further processing. A typical
application is to run machine learning (ML) services on facial images collected
from different individuals. To prevent identity theft, conventional methods
commonly rely on an adversarial game-based approach to shed the identity
information from the feature. However, such methods can not defend against
adaptive attacks, in which an attacker takes a countermove against a known
defence strategy. We propose Crafter, a feature crafting mechanism deployed at
the edge, to protect the identity information from adaptive model inversion
attacks while ensuring the ML tasks are properly carried out in the cloud. The
key defence strategy is to mislead the attacker to a non-private prior from
which the attacker gains little about the private identity. In this case, the
crafted features act like poison training samples for attackers with adaptive
model updates. Experimental results indicate that Crafter successfully defends
both basic and possible adaptive attacks, which can not be achieved by
state-of-the-art adversarial game-based methods.
\\ ( https://arxiv.org/abs/2401.07205 ,  6139kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07206 (*cross-listing*)
Date: Sun, 14 Jan 2024 05:38:10 GMT   (520kb,D)

Title: Probabilistic Reduced-Dimensional Vector Autoregressive Modeling with
  Oblique Projections
Authors: Yanfang Mo and S. Joe Qin
Categories: stat.ML cs.LG cs.SY eess.SY
Comments: 16pages, 5 figures
\\
  In this paper, we propose a probabilistic reduced-dimensional vector
autoregressive (PredVAR) model to extract low-dimensional dynamics from
high-dimensional noisy data. The model utilizes an oblique projection to
partition the measurement space into a subspace that accommodates the
reduced-dimensional dynamics and a complementary static subspace. An optimal
oblique decomposition is derived for the best predictability regarding
prediction error covariance. Building on this, we develop an iterative PredVAR
algorithm using maximum likelihood and the expectation-maximization (EM)
framework. This algorithm alternately updates the estimates of the latent
dynamics and optimal oblique projection, yielding dynamic latent variables with
rank-ordered predictability and an explicit latent VAR model that is consistent
with the outer projection model. The superior performance and efficiency of the
proposed approach are demonstrated using data sets from a synthesized Lorenz
system and an industrial process from Eastman Chemical.
\\ ( https://arxiv.org/abs/2401.07206 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07232 (*cross-listing*)
Date: Sun, 14 Jan 2024 08:32:41 GMT   (4406kb,D)

Title: Polariton lattices as binarized neuromorphic networks
Authors: Evgeny Sedov and Alexey Kavokin
Categories: cond-mat.dis-nn cs.LG cs.NE physics.app-ph
\\
  We introduce a novel neuromorphic network architecture based on a lattice of
exciton-polariton condensates, intricately interconnected and energized through
non-resonant optical pumping. The network employs a binary framework, where
each neuron, facilitated by the spatial coherence of pairwise coupled
condensates, performs binary operations. This coherence, emerging from the
ballistic propagation of polaritons, ensures efficient, network-wide
communication. The binary neuron switching mechanism, driven by the nonlinear
repulsion through the excitonic component of polaritons, offers computational
efficiency and scalability advantages over continuous weight neural networks.
Our network enables parallel processing, enhancing computational speed compared
to sequential or pulse-coded binary systems. The system's performance was
evaluated using the MNIST dataset for handwritten digit recognition, showcasing
the potential to outperform existing polaritonic neuromorphic systems, as
demonstrated by its impressive predicted classification accuracy of up to
97.5%.
\\ ( https://arxiv.org/abs/2401.07232 ,  4406kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07298 (*cross-listing*)
Date: Sun, 14 Jan 2024 14:14:19 GMT   (47358kb,D)

Title: Efficient Frameworks for Generalized Low-Rank Matrix Bandit Problems
Authors: Yue Kang, Cho-Jui Hsieh, Thomas C. M. Lee
Categories: stat.ML cs.LG
Comments: Revision of the paper accepted by NeurIPS 2022
\\
  In the stochastic contextual low-rank matrix bandit problem, the expected
reward of an action is given by the inner product between the action's feature
matrix and some fixed, but initially unknown $d_1$ by $d_2$ matrix $\Theta^*$
with rank $r \ll \{d_1, d_2\}$, and an agent sequentially takes actions based
on past experience to maximize the cumulative reward. In this paper, we study
the generalized low-rank matrix bandit problem, which has been recently
proposed in \cite{lu2021low} under the Generalized Linear Model (GLM)
framework. To overcome the computational infeasibility and theoretical restrain
of existing algorithms on this problem, we first propose the G-ESTT framework
that modifies the idea from \cite{jun2019bilinear} by using Stein's method on
the subspace estimation and then leverage the estimated subspaces via a
regularization idea. Furthermore, we remarkably improve the efficiency of
G-ESTT by using a novel exclusion idea on the estimated subspace instead, and
propose the G-ESTS framework. We also show that G-ESTT can achieve the
$\tilde{O}(\sqrt{(d_1+d_2)MrT})$ bound of regret while G-ESTS can achineve the
$\tilde{O}(\sqrt{(d_1+d_2)^{3/2}Mr^{3/2}T})$ bound of regret under mild
assumption up to logarithm terms, where $M$ is some problem dependent value.
Under a reasonable assumption that $M = O((d_1+d_2)^2)$ in our problem setting,
the regret of G-ESTT is consistent with the current best regret of
$\tilde{O}((d_1+d_2)^{3/2} \sqrt{rT}/D_{rr})$~\citep{lu2021low} ($D_{rr}$ will
be defined later). For completeness, we conduct experiments to illustrate that
our proposed algorithms, especially G-ESTS, are also computationally tractable
and consistently outperform other state-of-the-art (generalized) linear matrix
bandit methods based on a suite of simulations.
\\ ( https://arxiv.org/abs/2401.07298 ,  47358kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07342 (*cross-listing*)
Date: Sun, 14 Jan 2024 18:27:37 GMT   (734kb,D)

Title: Who Said What? An Automated Approach to Analyzing Speech in Preschool
  Classrooms
Authors: Anchen Sun, Juan J Londono, Batya Elbaum, Luis Estrada, Roberto Jose
  Lazo, Laura Vitale, Hugo Gonzalez Villasanti, Riccardo Fusaroli, Lynn K
  Perry, Daniel S Messinger
Categories: eess.AS cs.LG
Comments: 7 pages, 3 figures, 3 tables
\\
  Young children spend substantial portions of their waking hours in noisy
preschool classrooms. In these environments, children's vocal interactions with
teachers are critical contributors to their language outcomes, but manually
transcribing these interactions is prohibitive. Using audio from child- and
teacher-worn recorders, we propose an automated framework that uses open source
software both to classify speakers (ALICE) and to transcribe their utterances
(Whisper). We compare results from our framework to those from a human expert
for 110 minutes of classroom recordings, including 85 minutes from child-word
microphones (n=4 children) and 25 minutes from teacher-worn microphones (n=2
teachers). The overall proportion of agreement, that is, the proportion of
correctly classified teacher and child utterances, was .76, with an
error-corrected kappa of .50 and a weighted F1 of .76. The word error rate for
both teacher and child transcriptions was .15, meaning that 15% of words would
need to be deleted, added, or changed to equate the Whisper and expert
transcriptions. Moreover, speech features such as the mean length of utterances
in words, the proportion of teacher and child utterances that were questions,
and the proportion of utterances that were responded to within 2.5 seconds were
similar when calculated separately from expert and automated transcriptions.
The results suggest substantial progress in analyzing classroom speech that may
support children's language development. Future research using natural language
processing is underway to improve speaker classification and to analyze results
from the application of the automated it framework to a larger dataset
containing classroom recordings from 13 children and 4 teachers observed on 17
occasions over one year.
\\ ( https://arxiv.org/abs/2401.07342 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07371 (*cross-listing*)
Date: Sun, 14 Jan 2024 21:22:22 GMT   (610kb)

Title: A Data-driven Resilience Framework of Directionality Configuration based
  on Topological Credentials in Road Networks
Authors: H M Imran Kays, Khondhaker Al Momin, K.K. "Muralee" Muraleetharan,
  Arif Mohaimin Sadri
Categories: cs.SI cs.LG stat.AP
Comments: 103rd Transportation Research Board (TRB) Annual Meeting
\\
  Roadway reconfiguration is a crucial aspect of transportation planning,
aiming to enhance traffic flow, reduce congestion, and improve overall road
network performance with existing infrastructure and resources. This paper
presents a novel roadway reconfiguration technique by integrating optimization
based Brute Force search approach and decision support framework to rank
various roadway configurations for better performance. The proposed framework
incorporates a multi-criteria decision analysis (MCDA) approach, combining
input from generated scenarios during the optimization process. By utilizing
data from optimization, the model identifies total betweenness centrality
(TBC), system travel time (STT), and total link traffic flow (TLTF) as the most
influential decision variables. The developed framework leverages graph theory
to model the transportation network topology and apply network science metrics
as well as stochastic user equilibrium traffic assignment to assess the impact
of each roadway configuration on the overall network performance. To rank the
roadway configurations, the framework employs machine learning algorithms, such
as ridge regression, to determine the optimal weights for each criterion (i.e.,
TBC, STT, TLTF). Moreover, the network-based analysis ensures that the selected
configurations not only optimize individual roadway segments but also enhance
system-level efficiency, which is particularly helpful as the increasing
frequency and intensity of natural disasters and other disruptive events
underscore the critical need for resilient transportation networks. By
integrating multi-criteria decision analysis, machine learning, and network
science metrics, the proposed framework would enable transportation planners to
make informed and data-driven decisions, leading to more sustainable,
efficient, and resilient roadway configurations.
\\ ( https://arxiv.org/abs/2401.07371 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07398 (*cross-listing*)
Date: Mon, 15 Jan 2024 00:27:41 GMT   (9424kb,D)

Title: Cross Domain Early Crop Mapping using CropGAN and CNN Classifier
Authors: Yiqun Wang, Hui Huang, Radu State
Categories: cs.CV cs.LG eess.IV
\\
  Driven by abundant satellite imagery, machine learning-based approaches have
recently been promoted to generate high-resolution crop cultivation maps to
support many agricultural applications. One of the major challenges faced by
these approaches is the limited availability of ground truth labels. In the
absence of ground truth, existing work usually adopts the "direct transfer
strategy" that trains a classifier using historical labels collected from other
regions and then applies the trained model to the target region. Unfortunately,
the spectral features of crops exhibit inter-region and inter-annual
variability due to changes in soil composition, climate conditions, and crop
progress, the resultant models perform poorly on new and unseen regions or
years. This paper presents the Crop Generative Adversarial Network (CropGAN) to
address the above cross-domain issue. Our approach does not need labels from
the target domain. Instead, it learns a mapping function to transform the
spectral features of the target domain to the source domain (with labels) while
preserving their local structure. The classifier trained by the source domain
data can be directly applied to the transformed data to produce high-accuracy
early crop maps of the target domain. Comprehensive experiments across various
regions and years demonstrate the benefits and effectiveness of the proposed
approach. Compared with the widely adopted direct transfer strategy, the F1
score after applying the proposed CropGAN is improved by 13.13% - 50.98%
\\ ( https://arxiv.org/abs/2401.07398 ,  9424kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07463 (*cross-listing*)
Date: Mon, 15 Jan 2024 04:20:39 GMT   (3840kb,D)

Title: Consistency of semi-supervised learning, stochastic tug-of-war games,
  and the p-Laplacian
Authors: Jeff Calder, Nadejda Drenska
Categories: math.ST cs.LG cs.NA math.AP math.NA math.PR stat.TH
MSC-class: 91A05, 68T05, 68Q32, 35D40, 35J60, 65N06
\\
  In this paper we give a broad overview of the intersection of partial
differential equations (PDEs) and graph-based semi-supervised learning. The
overview is focused on a large body of recent work on PDE continuum limits of
graph-based learning, which have been used to prove well-posedness of
semi-supervised learning algorithms in the large data limit. We highlight some
interesting research directions revolving around consistency of graph-based
semi-supervised learning, and present some new results on the consistency of
p-Laplacian semi-supervised learning using the stochastic tug-of-war game
interpretation of the p-Laplacian. We also present the results of some
numerical experiments that illustrate our results and suggest directions for
future work.
\\ ( https://arxiv.org/abs/2401.07463 ,  3840kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07464 (*cross-listing*)
Date: Mon, 15 Jan 2024 04:38:06 GMT   (317kb,D)

Title: Quantum Privacy Aggregation of Teacher Ensembles (QPATE) for
  Privacy-preserving Quantum Machine Learning
Authors: William Watkins, Heehwan Wang, Sangyoon Bae, Huan-Hsin Tseng, Jiook
  Cha, Samuel Yen-Chi Chen, Shinjae Yoo
Categories: quant-ph cs.CR cs.LG
\\
  The utility of machine learning has rapidly expanded in the last two decades
and presents an ethical challenge. Papernot et. al. developed a technique,
known as Private Aggregation of Teacher Ensembles (PATE) to enable federated
learning in which multiple teacher models are trained on disjoint datasets.
This study is the first to apply PATE to an ensemble of quantum neural networks
(QNN) to pave a new way of ensuring privacy in quantum machine learning (QML)
models.
\\ ( https://arxiv.org/abs/2401.07464 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07465 (*cross-listing*)
Date: Mon, 15 Jan 2024 04:43:37 GMT   (3943kb,D)

Title: Power Flow Analysis Using Deep Neural Networks in Three-Phase Unbalanced
  Smart Distribution Grids
Authors: Deepak Tiwari, Mehdi Jabbari Zideh, Veeru Talreja, Vishal Verma,
  Sarika K. Solanki, and Jignesh Solanki
Categories: eess.SY cs.LG cs.NE cs.SY
\\
  Most power systems' approaches are currently tending towards stochastic and
probabilistic methods due to the high variability of renewable sources and the
stochastic nature of loads. Conventional power flow (PF) approaches such as
forward-backward sweep (FBS) and Newton-Raphson require a high number of
iterations to solve non-linear PF equations making them computationally very
intensive. PF is the most important study performed by utility, required in all
stages of the power system, especially in operations and planning. This paper
discusses the applications of deep learning (DL) to predict PF solutions for
three-phase unbalanced power distribution grids. Three deep neural networks
(DNNs); Radial Basis Function Network (RBFnet), Multi-Layer Perceptron (MLP),
and Convolutional Neural Network (CNN), are proposed in this paper to predict
PF solutions. The PF problem is formulated as a multi-output regression model
where two or more output values are predicted based on the inputs. The training
and testing data are generated through the OpenDSS-MATLAB COM interface. These
methods are completely data-driven where the training relies on reducing the
mismatch at each node without the need for the knowledge of the system. The
novelty of the proposed methodology is that the models can accurately predict
the PF solutions for the unbalanced distribution grids with mutual coupling and
are robust to different R/X ratios, topology changes as well as generation and
load variability introduced by the integration of distributed energy resources
(DERs) and electric vehicles (EVs). To test the efficacy of the DNN models,
they are applied to IEEE 4-node and 123-node test cases, and the American
Electric Power (AEP) feeder model. The PF results for RBFnet, MLP, and CNN
models are discussed in this paper demonstrating that all three DNN models
provide highly accurate results in predicting PF solutions.
\\ ( https://arxiv.org/abs/2401.07465 ,  3943kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07496 (*cross-listing*)
Date: Mon, 15 Jan 2024 06:30:06 GMT   (317kb,D)

Title: Low-Rank Gradient Compression with Error Feedback for MIMO Wireless
  Federated Learning
Authors: Mingzhao Guo, Dongzhu Liu, Osvaldo Simeone, Dingzhu Wen
Categories: cs.IT cs.LG eess.SP math.IT
Comments: 5 pages, 3 figures, 27 references, submitted
\\
  This paper presents a novel approach to enhance the communication efficiency
of federated learning (FL) in multiple input and multiple output (MIMO)
wireless systems. The proposed method centers on a low-rank matrix
factorization strategy for local gradient compression based on alternating
least squares, along with over-the-air computation and error feedback. The
proposed protocol, termed over-the-air low-rank compression (Ota-LC), is
demonstrated to have lower computation cost and lower communication overhead as
compared to existing benchmarks while guaranteeing the same inference
performance. As an example, when targeting a test accuracy of 80% on the
Cifar-10 dataset, Ota-LC achieves a reduction in total communication costs of
at least 30% when contrasted with benchmark schemes, while also reducing the
computational complexity order by a factor equal to the sum of the dimension of
the gradients.
\\ ( https://arxiv.org/abs/2401.07496 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07503 (*cross-listing*)
Date: Mon, 15 Jan 2024 07:06:36 GMT   (862kb,D)

Title: PolMERLIN: Self-Supervised Polarimetric Complex SAR Image Despeckling
  with Masked Networks
Authors: Shunya Kato, Masaki Saito, Katsuhiko Ishiguro, Sol Cummings
Categories: cs.CV cs.LG
Comments: To appear on IEEE Geoscience and Remote Sensing Letters
\\
  Despeckling is a crucial noise reduction task in improving the quality of
synthetic aperture radar (SAR) images. Directly obtaining noise-free SAR images
is a challenging task that has hindered the development of accurate despeckling
algorithms. The advent of deep learning has facilitated the study of denoising
models that learn from only noisy SAR images. However, existing methods deal
solely with single-polarization images and cannot handle the multi-polarization
images captured by modern satellites. In this work, we present an extension of
the existing model for generating single-polarization SAR images to handle
multi-polarization SAR images. Specifically, we propose a novel self-supervised
despeckling approach called channel masking, which exploits the relationship
between polarizations. Additionally, we utilize a spatial masking method that
addresses pixel-to-pixel correlations to further enhance the performance of our
approach. By effectively incorporating multiple polarization information, our
method surpasses current state-of-the-art methods in quantitative evaluation in
both synthetic and real-world scenarios.
\\ ( https://arxiv.org/abs/2401.07503 ,  862kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07506 (*cross-listing*)
Date: Mon, 15 Jan 2024 07:13:43 GMT   (209kb,D)

Title: SeMaScore : a new evaluation metric for automatic speech recognition
  tasks
Authors: Zitha Sasindran, Harsha Yelchuri, T. V. Prabhakar
Categories: eess.AS cs.LG cs.SD
\\
  In this study, we present SeMaScore, generated using a segment-wise mapping
and scoring algorithm that serves as an evaluation metric for automatic speech
recognition tasks. SeMaScore leverages both the error rate and a more robust
similarity score. We show that our algorithm's score generation improves upon
the state-of-the-art BERTscore. Our experimental results show that SeMaScore
corresponds well with expert human assessments, signal-to-noise ratio levels,
and other natural language metrics. We outperform BERTscore by 41x in metric
computation speed. Overall, we demonstrate that SeMaScore serves as a more
dependable evaluation metric, particularly in real-world situations involving
atypical speech patterns.
\\ ( https://arxiv.org/abs/2401.07506 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07528 (*cross-listing*)
Date: Mon, 15 Jan 2024 08:14:23 GMT   (3633kb)

Title: Automatic characterization of boulders on planetary surfaces from
  high-resolution satellite images
Authors: Nils C. Prieur, Brian Amaro, Emiliano Gonzalez, Hannah Kerner, Sergei
  Medvedev, Lior Rubanenko, Stephanie C. Werner, Zhiyong Xiao8, Dmitry
  Zastrozhnov, Mathieu G. A. Lap\^otre
Categories: astro-ph.EP astro-ph.IM cs.LG eess.IV
DOI: 10.1029/2023JE008013
\\
  Boulders form from a variety of geological processes, which their size,
shape, and orientation may help us better understand. Furthermore, they
represent potential hazards to spacecraft landing that need to be
characterized. However, mapping individual boulders across vast areas is
extremely labor-intensive, often limiting the extent over which they are
characterized and the statistical robustness of obtained boulder morphometrics.
To automate boulder characterization, we use an instance segmentation neural
network, Mask R-CNN, to detect and outline boulders in high-resolution
satellite images. Our neural network, BoulderNet, was trained from a dataset of
> 33,000 boulders in > 750 image tiles from Earth, the Moon, and Mars.
BoulderNet not only correctly detects the majority of boulders in images, but
it identifies the outline of boulders with high fidelity, achieving average
precision and recall values of 72% and 64% relative to manually digitized
boulders from the test dataset, when only detections with
intersection-over-union ratios > 50% are considered valid. These values are
similar to those obtained by human mappers. On Earth, equivalent boulder
diameters, aspect ratios, and orientations extracted from predictions were
benchmarked against ground measurements and yield values within 15%, 0.20, and
20 degrees of their ground-truth values, respectively. BoulderNet achieves
better boulder detection and characterization performance relative to existing
methods, providing a versatile open-source tool to characterize entire boulder
fields on planetary surfaces.
\\ ( https://arxiv.org/abs/2401.07528 ,  3633kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07558 (*cross-listing*)
Date: Mon, 15 Jan 2024 09:50:27 GMT   (2426kb,D)

Title: FedRFQ: Prototype-Based Federated Learning with Reduced Redundancy,
  Minimal Failure, and Enhanced Quality
Authors: Biwei Yan, Hongliang Zhang, Minghui Xu, Dongxiao Yu, Xiuzhen Cheng
Categories: cs.DC cs.LG
\\
  Federated learning is a powerful technique that enables collaborative
learning among different clients. Prototype-based federated learning is a
specific approach that improves the performance of local models under non-IID
(non-Independently and Identically Distributed) settings by integrating class
prototypes. However, prototype-based federated learning faces several
challenges, such as prototype redundancy and prototype failure, which limit its
accuracy. It is also susceptible to poisoning attacks and server malfunctions,
which can degrade the prototype quality. To address these issues, we propose
FedRFQ, a prototype-based federated learning approach that aims to reduce
redundancy, minimize failures, and improve \underline{q}uality. FedRFQ
leverages a SoftPool mechanism, which effectively mitigates prototype
redundancy and prototype failure on non-IID data. Furthermore, we introduce the
BFT-detect, a BFT (Byzantine Fault Tolerance) detectable aggregation algorithm,
to ensure the security of FedRFQ against poisoning attacks and server
malfunctions. Finally, we conduct experiments on three different datasets,
namely MNIST, FEMNIST, and CIFAR-10, and the results demonstrate that FedRFQ
outperforms existing baselines in terms of accuracy when handling non-IID data.
\\ ( https://arxiv.org/abs/2401.07558 ,  2426kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07604 (*cross-listing*)
Date: Mon, 15 Jan 2024 11:21:25 GMT   (8520kb,D)

Title: Data Assimilation using ERA5, ASOS, and the U-STN model for Weather
  Forecasting over the UK
Authors: Wenqi Wang, Jacob Bieker, Rossella Arcucci, C\'esar Quilodr\'an-Casas
Categories: physics.ao-ph cs.LG
Comments: Accepted in the Tackling Climate Change with Machine Learning
  Workshop@NeurIPS 2023, 8 pages, 2 figures
\\
  In recent years, the convergence of data-driven machine learning models with
Data Assimilation (DA) offers a promising avenue for enhancing weather
forecasting. This study delves into this emerging trend, presenting our
methodologies and outcomes. We harnessed the UK's local ERA5 850 hPa
temperature data and refined the U-STN12 global weather forecasting model,
tailoring its predictions to the UK's climate nuances. From the ASOS network,
we sourced T2m data, representing ground observations across the UK. We
employed the advanced kriging method with a polynomial drift term for
consistent spatial resolution. Furthermore, Gaussian noise was superimposed on
the ERA5 T850 data, setting the stage for ensuing multi-time step synthetic
observations. Probing into the assimilation impacts, the ASOS T2m data was
integrated with the ERA5 T850 dataset. Our insights reveal that while global
forecast models can adapt to specific regions, incorporating atmospheric data
in DA significantly bolsters model accuracy. Conversely, the direct
assimilation of surface temperature data tends to mitigate this enhancement,
tempering the model's predictive prowess.
\\ ( https://arxiv.org/abs/2401.07604 ,  8520kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07627 (*cross-listing*)
Date: Mon, 15 Jan 2024 12:07:52 GMT   (35kb)

Title: Cost-sensitive Feature Selection for Support Vector Machines
Authors: Sandra Ben\'itez-Pe\~na and Rafael Blanquero and Emilio Carrizosa and
  Pepa Ram\'irez-Cobo
Categories: stat.ML cs.LG
Journal-ref: S. Ben\'itez-Pe\~na, R. Blanquero, E. Carrizosa, P.
  Ram\'irez-Cobo. Cost-sensitive Feature Selection for Support Vector Machines,
  Computers & Operations Research, Volume 106, 2019, Pages 169-178
DOI: 10.1016/j.cor.2018.03.005
\\
  Feature Selection is a crucial procedure in Data Science tasks such as
Classification, since it identifies the relevant variables, making thus the
classification procedures more interpretable, cheaper in terms of measurement
and more effective by reducing noise and data overfit. The relevance of
features in a classification procedure is linked to the fact that
misclassifications costs are frequently asymmetric, since false positive and
false negative cases may have very different consequences. However,
off-the-shelf Feature Selection procedures seldom take into account such
cost-sensitivity of errors.
  In this paper we propose a mathematical-optimization-based Feature Selection
procedure embedded in one of the most popular classification procedures,
namely, Support Vector Machines, accommodating asymmetric misclassification
costs. The key idea is to replace the traditional margin maximization by
minimizing the number of features selected, but imposing upper bounds on the
false positive and negative rates. The problem is written as an integer linear
problem plus a quadratic convex problem for Support Vector Machines with both
linear and radial kernels.
  The reported numerical experience demonstrates the usefulness of the proposed
Feature Selection procedure. Indeed, our results on benchmark data sets show
that a substantial decrease of the number of features is obtained, whilst the
desired trade-off between false positive and false negative rates is achieved.
\\ ( https://arxiv.org/abs/2401.07627 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07646 (*cross-listing*)
Date: Mon, 15 Jan 2024 12:42:15 GMT   (1950kb)

Title: Multifractal-spectral features enhance classification of anomalous
  diffusion
Authors: Henrik Seckler, Ralf Metzler, Damian G. Kelty-Stephen, Madhur Mangalam
Categories: nlin.AO cs.LG
Comments: 23 pages, 6 figures
\\
  Anomalous diffusion processes pose a unique challenge in classification and
characterization. Previously (Mangalam et al., 2023, Physical Review Research
5, 023144), we established a framework for understanding anomalous diffusion
using multifractal formalism. The present study delves into the potential of
multifractal spectral features for effectively distinguishing anomalous
diffusion trajectories from five widely used models: fractional Brownian
motion, scaled Brownian motion, continuous time random walk, annealed transient
time motion, and L\'evy walk. To accomplish this, we generate extensive
datasets comprising $10^6$ trajectories from these five anomalous diffusion
models and extract multiple multifractal spectra from each trajectory. Our
investigation entails a thorough analysis of neural network performance,
encompassing features derived from varying numbers of spectra. Furthermore, we
explore the integration of multifractal spectra into traditional feature
datasets, enabling us to assess their impact comprehensively. To ensure a
statistically meaningful comparison, we categorize features into concept groups
and train neural networks using features from each designated group. Notably,
several feature groups demonstrate similar levels of accuracy, with the highest
performance observed in groups utilizing moving-window characteristics and
$p$-variation features. Multifractal spectral features, particularly those
derived from three spectra involving different timescales and cutoffs, closely
follow, highlighting their robust discriminatory potential. Remarkably, a
neural network exclusively trained on features from a single multifractal
spectrum exhibits commendable performance, surpassing other feature groups. Our
findings underscore the diverse and potent efficacy of multifractal spectral
features in enhancing classification of anomalous diffusion.
\\ ( https://arxiv.org/abs/2401.07646 ,  1950kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07671 (*cross-listing*)
Date: Mon, 15 Jan 2024 13:35:21 GMT   (1276kb,D)

Title: CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory
  Architectures
Authors: Rebecca Pelke, Jose Cubero-Cascante, Nils Bosbach, Felix Staudigl,
  Rainer Leupers, Jan Moritz Joseph
Categories: cs.AR cs.ET cs.LG
\\
  The demand for efficient machine learning (ML) accelerators is growing
rapidly, driving the development of novel computing concepts such as resistive
random access memory (RRAM)-based tiled computing-in-memory (CIM)
architectures. CIM allows to compute within the memory unit, resulting in
faster data processing and reduced power consumption. Efficient compiler
algorithms are essential to exploit the potential of tiled CIM architectures.
While conventional ML compilers focus on code generation for CPUs, GPUs, and
other von Neumann architectures, adaptations are needed to cover CIM
architectures. Cross-layer scheduling is a promising approach, as it enhances
the utilization of CIM cores, thereby accelerating computations. Although
similar concepts are implicitly used in previous work, there is a lack of clear
and quantifiable algorithmic definitions for cross-layer scheduling for tiled
CIM architectures. To close this gap, we present CLSA-CIM, a cross-layer
scheduling algorithm for tiled CIM architectures. We integrate CLSA-CIM with
existing weight-mapping strategies and compare performance against
state-of-the-art (SOTA) scheduling algorithms. CLSA-CIM improves the
utilization by up to 17.9 x , resulting in an overall speedup increase of up to
29.2 x compared to SOTA.
\\ ( https://arxiv.org/abs/2401.07671 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07694 (*cross-listing*)
Date: Mon, 15 Jan 2024 14:04:50 GMT   (426kb,D)

Title: Stochastic optimization with arbitrary recurrent data sampling
Authors: William G. Powell and Hanbaek Lyu
Categories: math.OC cs.LG stat.ML
Comments: 41 pages, 3 figures, 1 table
\\
  For obtaining optimal first-order convergence guarantee for stochastic
optimization, it is necessary to use a recurrent data sampling algorithm that
samples every data point with sufficient frequency. Most commonly used data
sampling algorithms (e.g., i.i.d., MCMC, random reshuffling) are indeed
recurrent under mild assumptions. In this work, we show that for a particular
class of stochastic optimization algorithms, we do not need any other property
(e.g., independence, exponential mixing, and reshuffling) than recurrence in
data sampling algorithms to guarantee the optimal rate of first-order
convergence. Namely, using regularized versions of Minimization by Incremental
Surrogate Optimization (MISO), we show that for non-convex and possibly
non-smooth objective functions, the expected optimality gap converges at an
optimal rate $O(n^{-1/2})$ under general recurrent sampling schemes.
Furthermore, the implied constant depends explicitly on the `speed of
recurrence', measured by the expected amount of time to visit a given data
point either averaged (`target time') or supremized (`hitting time') over the
current location. We demonstrate theoretically and empirically that convergence
can be accelerated by selecting sampling algorithms that cover the data set
most effectively. We discuss applications of our general framework to
decentralized optimization and distributed non-negative matrix factorization.
\\ ( https://arxiv.org/abs/2401.07694 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07733 (*cross-listing*)
Date: Mon, 15 Jan 2024 14:45:18 GMT   (2200kb,D)

Title: Conformal Approach To Gaussian Process Surrogate Evaluation With
  Coverage Guarantees
Authors: Edgar Jaber (EDF R&D PRISME, CB, LISN), Vincent Blot (The State of the
  Art AI company, LISN), Nicolas Brunel (The State of the Art AI company,
  ENSIIE), Vincent Chabridon (EDF R&D PRISME, SINCLAIR AI Lab), Emmanuel Remy
  (EDF R&D PRISME), Bertrand Iooss (EDF R&D PRISME, IMT, SINCLAIR AI Lab, GdR
  MASCOT-NUM), Didier Lucor (LISN), Mathilde Mougeot (CB, ENSIIE), Alessandro
  Leite (LISN)
Categories: stat.ML cs.LG
\\
  Gaussian processes (GPs) are a Bayesian machine learning approach widely used
to construct surrogate models for the uncertainty quantification of computer
simulation codes in industrial applications. It provides both a mean predictor
and an estimate of the posterior prediction variance, the latter being used to
produce Bayesian credibility intervals. Interpreting these intervals relies on
the Gaussianity of the simulation model as well as the well-specification of
the priors which are not always appropriate. We propose to address this issue
with the help of conformal prediction. In the present work, a method for
building adaptive cross-conformal prediction intervals is proposed by weighting
the non-conformity score with the posterior standard deviation of the GP. The
resulting conformal prediction intervals exhibit a level of adaptivity akin to
Bayesian credibility sets and display a significant correlation with the
surrogate model local approximation error, while being free from the underlying
model assumptions and having frequentist coverage guarantees. These estimators
can thus be used for evaluating the quality of a GP surrogate model and can
assist a decision-maker in the choice of the best prior for the specific
application of the GP. The performance of the method is illustrated through a
panel of numerical examples based on various reference databases. Moreover, the
potential applicability of the method is demonstrated in the context of
surrogate modeling of an expensive-to-evaluate simulator of the clogging
phenomenon in steam generators of nuclear reactors.
\\ ( https://arxiv.org/abs/2401.07733 ,  2200kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07769 (*cross-listing*)
Date: Mon, 15 Jan 2024 15:27:24 GMT   (1773kb,D)

Title: Deep Evolutional Instant Interest Network for CTR Prediction in
  Trigger-Induced Recommendation
Authors: Zhibo Xiao, Luwei Yang, Tao Zhang, Wen Jiang, Wei Ning and Yujiu Yang
Categories: cs.IR cs.LG
Comments: 7 pages, 3 figures, reviewing of the 17th ACM International
  Conference on Web Search and Data Mining
\\
  The recommendation has been playing a key role in many industries, e.g.,
e-commerce, streaming media, social media, etc. Recently, a new recommendation
scenario, called Trigger-Induced Recommendation (TIR), where users are able to
explicitly express their instant interests via trigger items, is emerging as an
essential role in many e-commerce platforms, e.g., Alibaba.com and Amazon.
Without explicitly modeling the user's instant interest, traditional
recommendation methods usually obtain sub-optimal results in TIR. Even though
there are a few methods considering the trigger and target items simultaneously
to solve this problem, they still haven't taken into account temporal
information of user behaviors, the dynamic change of user instant interest when
the user scrolls down and the interactions between the trigger and target
items. To tackle these problems, we propose a novel method -- Deep Evolutional
Instant Interest Network (DEI2N), for click-through rate prediction in TIR
scenarios. Specifically, we design a User Instant Interest Modeling Layer to
predict the dynamic change of the intensity of instant interest when the user
scrolls down. Temporal information is utilized in user behavior modeling.
Moreover, an Interaction Layer is introduced to learn better interactions
between the trigger and target items. We evaluate our method on several offline
and real-world industrial datasets. Experimental results show that our proposed
DEI2N outperforms state-of-the-art baselines. In addition, online A/B testing
demonstrates the superiority over the existing baseline in real-world
production environments.
\\ ( https://arxiv.org/abs/2401.07769 ,  1773kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07787 (*cross-listing*)
Date: Mon, 15 Jan 2024 15:53:13 GMT   (17924kb)

Title: Improving OCR Quality in 19th Century Historical Documents Using a
  Combined Machine Learning Based Approach
Authors: David Fleischhacker, Wolfgang Goederle, Roman Kern
Categories: cs.CV cs.LG
Comments: 29 pages, 23 figures, 7 tables
\\
  This paper addresses a major challenge to historical research on the 19th
century. Large quantities of sources have become digitally available for the
first time, while extraction techniques are lagging behind. Therefore, we
researched machine learning (ML) models to recognise and extract complex data
structures in a high-value historical primary source, the Schematismus. It
records every single person in the Habsburg civil service above a certain
hierarchical level between 1702 and 1918 and documents the genesis of the
central administration over two centuries. Its complex and intricate structure
as well as its enormous size have so far made any more comprehensive analysis
of the administrative and social structure of the later Habsburg Empire on the
basis of this source impossible. We pursued two central objectives: Primarily,
the improvement of the OCR quality, for which we considered an improved
structure recognition to be essential; in the further course, it turned out
that this also made the extraction of the data structure possible. We chose
Faster R-CNN as base for the ML architecture for structure recognition. In
order to obtain the required amount of training data quickly and economically,
we synthesised Hof- und Staatsschematismus-style data, which we used to train
our model. The model was then fine-tuned with a smaller set of manually
annotated historical source data. We then used Tesseract-OCR, which was further
optimised for the style of our documents, to complete the combined structure
extraction and OCR process. Results show a significant decrease in the two
standard parameters of OCR-performance, WER and CER (where lower values are
better). Combined structure detection and fine-tuned OCR improved CER and WER
values by remarkable 71.98 percent (CER) respectively 52.49 percent (WER).
\\ ( https://arxiv.org/abs/2401.07787 ,  17924kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07809 (*cross-listing*)
Date: Mon, 15 Jan 2024 16:30:12 GMT   (436kb,D)

Title: Optimal Data Splitting in Distributed Optimization for Machine Learning
Authors: Daniil Medyakov, Gleb Molodtsov, Aleksandr Beznosikov, Alexander
  Gasnikov
Categories: math.OC cs.LG
Comments: 17 pages, 2 figures, Doklady Rossijskoj akademii nauk:
  https://journals.rcsi.science/2686-9543/article/view/247131
DOI: 10.31857/S2686954323601665
\\
  The distributed optimization problem has become increasingly relevant
recently. It has a lot of advantages such as processing a large amount of data
in less time compared to non-distributed methods. However, most distributed
approaches suffer from a significant bottleneck - the cost of communications.
Therefore, a large amount of research has recently been directed at solving
this problem. One such approach uses local data similarity. In particular,
there exists an algorithm provably optimally exploiting the similarity
property. But this result, as well as results from other works solve the
communication bottleneck by focusing only on the fact that communication is
significantly more expensive than local computing and does not take into
account the various capacities of network devices and the different
relationship between communication time and local computing expenses. We
consider this setup and the objective of this study is to achieve an optimal
ratio of distributed data between the server and local machines for any costs
of communications and local computations. The running times of the network are
compared between uniform and optimal distributions. The superior theoretical
performance of our solutions is experimentally validated.
\\ ( https://arxiv.org/abs/2401.07809 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07888 (*cross-listing*)
Date: Mon, 15 Jan 2024 18:32:53 GMT   (1186kb,D)

Title: Multifidelity domain decomposition-based physics-informed neural
  networks for time-dependent problems
Authors: Alexander Heinlein, Amanda A. Howard, Damien Beecroft, Panos Stinis
Categories: math.NA cs.LG cs.NA
MSC-class: 65M22, 65M55, 68T07
\\
  Multiscale problems are challenging for neural network-based discretizations
of differential equations, such as physics-informed neural networks (PINNs).
This can be (partly) attributed to the so-called spectral bias of neural
networks. To improve the performance of PINNs for time-dependent problems, a
combination of multifidelity stacking PINNs and domain decomposition-based
finite basis PINNs are employed. In particular, to learn the high-fidelity part
of the multifidelity model, a domain decomposition in time is employed. The
performance is investigated for a pendulum and a two-frequency problem as well
as the Allen-Cahn equation. It can be observed that the domain decomposition
approach clearly improves the PINN and stacking PINN approaches.
\\ ( https://arxiv.org/abs/2401.07888 ,  1186kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07937 (*cross-listing*)
Date: Mon, 15 Jan 2024 19:57:07 GMT   (40658kb,D)

Title: Integrate Any Omics: Towards genome-wide data integration for patient
  stratification
Authors: Shihao Ma, Andy G.X. Zeng, Benjamin Haibe-Kains, Anna Goldenberg, John
  E Dick and Bo Wang
Categories: q-bio.GN cs.LG q-bio.QM
\\
  High-throughput omics profiling advancements have greatly enhanced cancer
patient stratification. However, incomplete data in multi-omics integration
presents a significant challenge, as traditional methods like sample exclusion
or imputation often compromise biological diversity and dependencies.
Furthermore, the critical task of accurately classifying new patients with
partial omics data into existing subtypes is commonly overlooked. To address
these issues, we introduce IntegrAO (Integrate Any Omics), an unsupervised
framework for integrating incomplete multi-omics data and classifying new
samples. IntegrAO first combines partially overlapping patient graphs from
diverse omics sources and utilizes graph neural networks to produce unified
patient embeddings. Our systematic evaluation across five cancer cohorts
involving six omics modalities demonstrates IntegrAO's robustness to missing
data and its accuracy in classifying new samples with partial profiles. An
acute myeloid leukemia case study further validates its capability to uncover
biological and clinical heterogeneity in incomplete datasets. IntegrAO's
ability to handle heterogeneous and incomplete data makes it an essential tool
for precision oncology, offering a holistic approach to patient
characterization.
\\ ( https://arxiv.org/abs/2401.07937 ,  40658kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07957 (*cross-listing*)
Date: Mon, 15 Jan 2024 20:47:24 GMT   (6803kb)

Title: Machine Perceptual Quality: Evaluating the Impact of Severe Lossy
  Compression on Audio and Image Models
Authors: Dan Jacobellis, Daniel Cummings, Neeraja J. Yadwadkar
Categories: eess.IV cs.CV cs.LG cs.SD eess.AS
Comments: 10 pages; abridged version published in IEEE Data Compression
  Conference 2024
\\
  In the field of neural data compression, the prevailing focus has been on
optimizing algorithms for either classical distortion metrics, such as PSNR or
SSIM, or human perceptual quality. With increasing amounts of data consumed by
machines rather than humans, a new paradigm of machine-oriented
compression$\unicode{x2013}$which prioritizes the retention of features salient
for machine perception over traditional human-centric
criteria$\unicode{x2013}$has emerged, creating several new challenges to the
development, evaluation, and deployment of systems utilizing lossy compression.
In particular, it is unclear how different approaches to lossy compression will
affect the performance of downstream machine perception tasks. To address this
under-explored area, we evaluate various perception
models$\unicode{x2013}$including image classification, image segmentation,
speech recognition, and music source separation$\unicode{x2013}$under severe
lossy compression. We utilize several popular codecs spanning conventional,
neural, and generative compression architectures. Our results indicate three
key findings: (1) using generative compression, it is feasible to leverage
highly compressed data while incurring a negligible impact on machine
perceptual quality; (2) machine perceptual quality correlates strongly with
deep similarity metrics, indicating a crucial role of these metrics in the
development of machine-oriented codecs; and (3) using lossy compressed
datasets, (e.g. ImageNet) for pre-training can lead to counter-intuitive
scenarios where lossy compression increases machine perceptual quality rather
than degrading it. To encourage engagement on this growing area of research,
our code and experiments are available at:
https://github.com/danjacobellis/MPQ.
\\ ( https://arxiv.org/abs/2401.07957 ,  6803kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07961 (*cross-listing*)
Date: Mon, 15 Jan 2024 20:57:50 GMT   (4405kb,D)

Title: Solution of the Probabilistic Lambert Problem: Connections with Optimal
  Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs
Authors: Alexis M.H. Teter, Iman Nodozi, Abhishek Halder
Categories: math.OC cs.LG cs.SY eess.SY math-ph math.MP stat.ML
\\
  Lambert's problem concerns with transferring a spacecraft from a given
initial to a given terminal position within prescribed flight time via velocity
control subject to a gravitational force field. We consider a probabilistic
variant of the Lambert problem where the knowledge of the endpoint constraints
in position vectors are replaced by the knowledge of their respective joint
probability density functions. We show that the Lambert problem with endpoint
joint probability density constraints is a generalized optimal mass transport
(OMT) problem, thereby connecting this classical astrodynamics problem with a
burgeoning area of research in modern stochastic control and stochastic machine
learning. This newfound connection allows us to rigorously establish the
existence and uniqueness of solution for the probabilistic Lambert problem. The
same connection also helps to numerically solve the probabilistic Lambert
problem via diffusion regularization, i.e., by leveraging further connection of
the OMT with the Schr\"odinger bridge problem (SBP). This also shows that the
probabilistic Lambert problem with additive dynamic process noise is in fact a
generalized SBP, and can be solved numerically using the so-called
Schr\"odinger factors, as we do in this work. We explain how the resulting
analysis leads to solving a boundary-coupled system of reaction-diffusion PDEs
where the nonlinear gravitational potential appears as the reaction rate. We
propose novel algorithms for the same, and present illustrative numerical
results. Our analysis and the algorithmic framework are nonparametric, i.e., we
make neither statistical (e.g., Gaussian, first few moments, mixture or
exponential family, finite dimensionality of the sufficient statistic) nor
dynamical (e.g., Taylor series) approximations.
\\ ( https://arxiv.org/abs/2401.07961 ,  4405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07990 (*cross-listing*)
Date: Mon, 15 Jan 2024 22:29:23 GMT   (45792kb,D)

Title: How does self-supervised pretraining improve robustness against noisy
  labels across various medical image classification datasets?
Authors: Bidur Khanal, Binod Bhattarai, Bishesh Khanal, Cristian Linte
Categories: eess.IV cs.CV cs.LG
\\
  Noisy labels can significantly impact medical image classification,
particularly in deep learning, by corrupting learned features. Self-supervised
pretraining, which doesn't rely on labeled data, can enhance robustness against
noisy labels. However, this robustness varies based on factors like the number
of classes, dataset complexity, and training size. In medical images, subtle
inter-class differences and modality-specific characteristics add complexity.
Previous research hasn't comprehensively explored the interplay between
self-supervised learning and robustness against noisy labels in medical image
classification, considering all these factors. In this study, we address three
key questions: i) How does label noise impact various medical image
classification datasets? ii) Which types of medical image datasets are more
challenging to learn and more affected by label noise? iii) How do different
self-supervised pretraining methods enhance robustness across various medical
image datasets? Our results show that DermNet, among five datasets (Fetal
plane, DermNet, COVID-DU-Ex, MURA, NCT-CRC-HE-100K), is the most challenging
but exhibits greater robustness against noisy labels. Additionally, contrastive
learning stands out among the eight self-supervised methods as the most
effective approach to enhance robustness against noisy labels.
\\ ( https://arxiv.org/abs/2401.07990 ,  45792kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08032 (*cross-listing*)
Date: Tue, 16 Jan 2024 01:03:39 GMT   (2420kb,D)

Title: Structure-based out-of-distribution (OOD) materials property prediction:
  a benchmark study
Authors: Sadman Sadeed Omee and Nihang Fu and Rongzhi Dong and Ming Hu and
  Jianjun Hu
Categories: cond-mat.mtrl-sci cs.LG
Comments: 21 pages
\\
  In real-world material research, machine learning (ML) models are usually
expected to predict and discover novel exceptional materials that deviate from
the known materials. It is thus a pressing question to provide an objective
evaluation of ML model performances in property prediction of
out-of-distribution (OOD) materials that are different from the training set
distribution. Traditional performance evaluation of materials property
prediction models through random splitting of the dataset frequently results in
artificially high performance assessments due to the inherent redundancy of
typical material datasets. Here we present a comprehensive benchmark study of
structure-based graph neural networks (GNNs) for extrapolative OOD materials
property prediction. We formulate five different categories of OOD ML problems
for three benchmark datasets from the MatBench study. Our extensive experiments
show that current state-of-the-art GNN algorithms significantly underperform
for the OOD property prediction tasks on average compared to their baselines in
the MatBench study, demonstrating a crucial generalization gap in realistic
material prediction tasks. We further examine the latent physical spaces of
these GNN models and identify the sources of CGCNN, ALIGNN, and DeeperGATGNN's
significantly more robust OOD performance than those of the current best models
in the MatBench study (coGN and coNGN), and provide insights to improve their
performance.
\\ ( https://arxiv.org/abs/2401.08032 ,  2420kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08035 (*cross-listing*)
Date: Tue, 16 Jan 2024 01:08:19 GMT   (424kb,D)

Title: BanglaNet: Bangla Handwritten Character Recognition using Ensembling of
  Convolutional Neural Network
Authors: Chandrika Saha, Md. Mostafijur Rahman
Categories: cs.CV cs.LG
\\
  Handwritten character recognition is a crucial task because of its abundant
applications. The recognition task of Bangla handwritten characters is
especially challenging because of the cursive nature of Bangla characters and
the presence of compound characters with more than one way of writing. In this
paper, a classification model based on the ensembling of several Convolutional
Neural Networks (CNN), namely, BanglaNet is proposed to classify Bangla basic
characters, compound characters, numerals, and modifiers. Three different
models based on the idea of state-of-the-art CNN models like Inception, ResNet,
and DenseNet have been trained with both augmented and non-augmented inputs.
Finally, all these models are averaged or ensembled to get the finishing model.
Rigorous experimentation on three benchmark Bangla handwritten characters
datasets, namely, CMATERdb, BanglaLekha-Isolated, and Ekush has exhibited
significant recognition accuracies compared to some recent CNN-based research.
The top-1 recognition accuracies obtained are 98.40%, 97.65%, and 97.32%, and
the top-3 accuracies are 99.79%, 99.74%, and 99.56% for CMATERdb,
BanglaLekha-Isolated, and Ekush datasets respectively.
\\ ( https://arxiv.org/abs/2401.08035 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08150 (*cross-listing*)
Date: Tue, 16 Jan 2024 06:47:43 GMT   (193kb,D)

Title: Differentially Private Sliced Inverse Regression: Minimax Optimality and
  Algorithm
Authors: Xintao Xia, Linjun Zhang, Zhanrui Cai
Categories: stat.ML cs.CR cs.LG math.ST stat.TH
\\
  Privacy preservation has become a critical concern in high-dimensional data
analysis due to the growing prevalence of data-driven applications. Proposed by
Li (1991), sliced inverse regression has emerged as a widely utilized
statistical technique for reducing covariate dimensionality while maintaining
sufficient statistical information. In this paper, we propose optimally
differentially private algorithms specifically designed to address privacy
concerns in the context of sufficient dimension reduction. We proceed to
establish lower bounds for differentially private sliced inverse regression in
both the low and high-dimensional settings. Moreover, we develop differentially
private algorithms that achieve the minimax lower bounds up to logarithmic
factors. Through a combination of simulations and real data analysis, we
illustrate the efficacy of these differentially private algorithms in
safeguarding privacy while preserving vital information within the reduced
dimension space. As a natural extension, we can readily offer analogous lower
and upper bounds for differentially private sparse principal component
analysis, a topic that may also be of potential interest to the statistical and
machine learning community.
\\ ( https://arxiv.org/abs/2401.08150 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08169 (*cross-listing*)
Date: Tue, 16 Jan 2024 07:18:47 GMT   (1339kb,D)

Title: Statistical Test for Attention Map in Vision Transformer
Authors: Tomohiro Shiraishi, Daiki Miwa, Teruyuki Katsuoka, Vo Nguyen Le Duy,
  Koichi Taji, Ichiro Takeuchi
Categories: stat.ML cs.LG
Comments: 42pages, 17figures
\\
  The Vision Transformer (ViT) demonstrates exceptional performance in various
computer vision tasks. Attention is crucial for ViT to capture complex
wide-ranging relationships among image patches, allowing the model to weigh the
importance of image patches and aiding our understanding of the decision-making
process. However, when utilizing the attention of ViT as evidence in
high-stakes decision-making tasks such as medical diagnostics, a challenge
arises due to the potential of attention mechanisms erroneously focusing on
irrelevant regions. In this study, we propose a statistical test for ViT's
attentions, enabling us to use the attentions as reliable quantitative evidence
indicators for ViT's decision-making with a rigorously controlled error rate.
Using the framework called selective inference, we quantify the statistical
significance of attentions in the form of p-values, which enables the
theoretically grounded quantification of the false positive detection
probability of attentions. We demonstrate the validity and the effectiveness of
the proposed method through numerical experiments and applications to brain
image diagnoses.
\\ ( https://arxiv.org/abs/2401.08169 ,  1339kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08216 (*cross-listing*)
Date: Tue, 16 Jan 2024 09:02:34 GMT   (3745kb,D)

Title: Towards Efficient and Certified Recovery from Poisoning Attacks in
  Federated Learning
Authors: Yu Jiang, Jiyuan Shen, Ziyao Liu, Chee Wei Tan, Kwok-Yan Lam
Categories: cs.CR cs.LG
\\
  Federated learning (FL) is vulnerable to poisoning attacks, where malicious
clients manipulate their updates to affect the global model. Although various
methods exist for detecting those clients in FL, identifying malicious clients
requires sufficient model updates, and hence by the time malicious clients are
detected, FL models have been already poisoned. Thus, a method is needed to
recover an accurate global model after malicious clients are identified.
Current recovery methods rely on (i) all historical information from
participating FL clients and (ii) the initial model unaffected by the malicious
clients, leading to a high demand for storage and computational resources. In
this paper, we show that highly effective recovery can still be achieved based
on (i) selective historical information rather than all historical information
and (ii) a historical model that has not been significantly affected by
malicious clients rather than the initial model. In this scenario, while
maintaining comparable recovery performance, we can accelerate the recovery
speed and decrease memory consumption. Following this concept, we introduce
Crab, an efficient and certified recovery method, which relies on selective
information storage and adaptive model rollback. Theoretically, we demonstrate
that the difference between the global model recovered by Crab and the one
recovered by train-from-scratch can be bounded under certain assumptions. Our
empirical evaluation, conducted across three datasets over multiple machine
learning models, and a variety of untargeted and targeted poisoning attacks
reveals that Crab is both accurate and efficient, and consistently outperforms
previous approaches in terms of both recovery speed and memory consumption.
\\ ( https://arxiv.org/abs/2401.08216 ,  3745kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08224 (*cross-listing*)
Date: Tue, 16 Jan 2024 09:22:12 GMT   (774kb,D)

Title: Differentially Private Estimation of CATE in Adaptive Experiment
Authors: Jiachun Li, David Simchi-Levi and Kaining Shi
Categories: stat.ME cs.CR cs.LG
\\
  Adaptive experiment is widely adopted to estimate conditional average
treatment effect (CATE) in clinical trials and many other scenarios. While the
primary goal in experiment is to maximize estimation accuracy, due to the
imperative of social welfare, it's also crucial to provide treatment with
superior outcomes to patients, which is measured by regret in contextual bandit
framework. These two objectives often lead to contrast optimal allocation
mechanism. Furthermore, privacy concerns arise in clinical scenarios containing
sensitive data like patients health records. Therefore, it's essential for the
treatment allocation mechanism to incorporate robust privacy protection
measures. In this paper, we investigate the tradeoff between loss of social
welfare and statistical power in contextual bandit experiment. We propose a
matched upper and lower bound for the multi-objective optimization problem, and
then adopt the concept of Pareto optimality to mathematically characterize the
optimality condition. Furthermore, we propose differentially private algorithms
which still matches the lower bound, showing that privacy is "almost free".
Additionally, we derive the asymptotic normality of the estimator, which is
essential in statistical inference and hypothesis testing.
\\ ( https://arxiv.org/abs/2401.08224 ,  774kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08260 (*cross-listing*)
Date: Tue, 16 Jan 2024 10:31:27 GMT   (335kb,D)

Title: Fast Kernel Summation in High Dimensions via Slicing and Fourier
  Transforms
Authors: Johannes Hertrich
Categories: math.NA cs.LG cs.NA
\\
  Kernel-based methods are heavily used in machine learning. However, they
suffer from $O(N^2)$ complexity in the number $N$ of considered data points. In
this paper, we propose an approximation procedure, which reduces this
complexity to $O(N)$. Our approach is based on two ideas. First, we prove that
any radial kernel with analytic basis function can be represented as sliced
version of some one-dimensional kernel and derive an analytic formula for the
one-dimensional counterpart. It turns out that the relation between one- and
$d$-dimensional kernels is given by a generalized Riemann-Liouville fractional
integral. Hence, we can reduce the $d$-dimensional kernel summation to a
one-dimensional setting. Second, for solving these one-dimensional problems
efficiently, we apply fast Fourier summations on non-equispaced data, a sorting
algorithm or a combination of both. Due to its practical importance we pay
special attention to the Gaussian kernel, where we show a dimension-independent
error bound and represent its one-dimensional counterpart via a closed-form
Fourier transform. We provide a run time comparison and error estimate of our
fast kernel summations.
\\ ( https://arxiv.org/abs/2401.08260 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08272 (*cross-listing*)
Date: Tue, 16 Jan 2024 10:51:55 GMT   (22982kb,D)

Title: Siamese Content-based Search Engine for a More Transparent Skin and
  Breast Cancer Diagnosis through Histological Imaging
Authors: Zahra Tabatabaei, Adri\'an Colomer, JAvier Oliver Moll, Valery Naranjo
Categories: cs.CV cs.IR cs.LG
\\
  Computer Aid Diagnosis (CAD) has developed digital pathology with Deep
Learning (DL)-based tools to assist pathologists in decision-making.
Content-Based Histopathological Image Retrieval (CBHIR) is a novel tool to seek
highly correlated patches in terms of similarity in histopathological features.
In this work, we proposed two CBHIR approaches on breast (Breast-twins) and
skin cancer (Skin-twins) data sets for robust and accurate patch-level
retrieval, integrating a custom-built Siamese network as a feature extractor.
The proposed Siamese network is able to generalize for unseen images by
focusing on the similar histopathological features of the input pairs. The
proposed CBHIR approaches are evaluated on the Breast (public) and Skin
(private) data sets with top K accuracy. Finding the optimum amount of K is
challenging, but also, as much as K increases, the dissimilarity between the
query and the returned images increases which might mislead the pathologists.
To the best of the author's belief, this paper is tackling this issue for the
first time on histopathological images by evaluating the top first retrieved
images. The Breast-twins model achieves 70% of the F1score at the top first,
which exceeds the other state-of-the-art methods at a higher amount of K such
as 5 and 400. Skin-twins overpasses the recently proposed Convolutional Auto
Encoder (CAE) by 67%, increasing the precision. Besides, the Skin-twins model
tackles the challenges of Spitzoid Tumors of Uncertain Malignant Potential
(STUMP) to assist pathologists with retrieving top K images and their
corresponding labels. So, this approach can offer a more explainable CAD tool
to pathologists in terms of transparency, trustworthiness, or reliability among
other characteristics.
\\ ( https://arxiv.org/abs/2401.08272 ,  22982kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08301 (*cross-listing*)
Date: Tue, 16 Jan 2024 11:54:32 GMT   (863kb)

Title: Sum Throughput Maximization in Multi-BD Symbiotic Radio NOMA Network
  Assisted by Active-STAR-RIS
Authors: Rahman Saadat Yeganeh, Mohammad Javad Omidi, Farshad Zeinali, Mohammad
  Robatmili, Mohammad Ghavami
Categories: eess.SP cs.LG cs.SY eess.SY
Comments: This article will be submitted to the Transactions journal
\\
  In this paper, we employ active simultaneously transmitting and reflecting
reconfigurable intelligent surface (ASRIS) to aid in establishing and enhancing
communication within a commensal symbiotic radio (CSR) network. Unlike
traditional RIS, ASRIS not only ensures coverage in an omni directional manner
but also amplifies received signals, consequently elevating overall network
performance. in the first phase, base station (BS) with active massive MIMO
antennas, send ambient signal to SBDs. In the first phase, the BS transmits
ambient signals to the symbiotic backscatter devices (SBDs), and after
harvesting the energy and modulating their information onto the signal carrier,
the SBDs send Backscatter signals back to the BS. In this scheme, we employ the
Backscatter Relay system to facilitate the transmission of information from the
SBDs to the symbiotic User Equipments (SUEs) with the assistance of the BS. In
the second phase, the BS transmits information signals to the SUEs after
eliminating interference using the Successive Interference Cancellation (SIC)
method. ASRIS is employed to establish communication among SUEs lacking a line
of sight (LoS) and to amplify power signals for SUEs with a LoS connection to
the BS. It is worth noting that we use NOMA for multiple access in all network.
  The main goal of this paper is to maximize the sum throughput between all
users. To achieve this, we formulate an optimization problem with variables
including active beamforming coefficients at the BS and ASRIS, as well as the
phase adjustments of ASRIS and scheduling parameters between the first and
second phases. To model this optimization problem, we employ three deep
reinforcement learning (DRL) methods, namely PPO, TD3, and A3C. Finally, the
mentioned methods are simulated and compared with each other.
\\ ( https://arxiv.org/abs/2401.08301 ,  863kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08307 (*cross-listing*)
Date: Tue, 16 Jan 2024 12:08:31 GMT   (8436kb,D)

Title: On Quantum Natural Policy Gradients
Authors: Andr\'e Sequeira and Luis Paulo Santos and Luis Soares Barbosa
Categories: quant-ph cs.LG
\\
  This research delves into the role of the quantum Fisher Information Matrix
(FIM) in enhancing the performance of Parameterized Quantum Circuit (PQC)-based
reinforcement learning agents. While previous studies have highlighted the
effectiveness of PQC-based policies preconditioned with the quantum FIM in
contextual bandits, its impact in broader reinforcement learning contexts, such
as Markov Decision Processes, is less clear. Through a detailed analysis of
L\"owner inequalities between quantum and classical FIMs, this study uncovers
the nuanced distinctions and implications of using each type of FIM. Our
results indicate that a PQC-based agent using the quantum FIM without
additional insights typically incurs a larger approximation error and does not
guarantee improved performance compared to the classical FIM. Empirical
evaluations in classic control benchmarks suggest even though quantum FIM
preconditioning outperforms standard gradient ascent, in general it is not
superior to classical FIM preconditioning.
\\ ( https://arxiv.org/abs/2401.08307 ,  8436kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08375 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:07:36 GMT   (185kb,D)

Title: Sparse PCA with False Discovery Rate Controlled Variable Selection
Authors: Jasin Machkour, Arnaud Breloy, Michael Muma, Daniel P. Palomar,
  Fr\'ed\'eric Pascal
Categories: stat.ML cs.LG
Comments: Published in ICASSP 2024 - 2024 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP), scheduled for 14-19 April
  2024 in Seoul, Korea
\\
  Sparse principal component analysis (PCA) aims at mapping large dimensional
data to a linear subspace of lower dimension. By imposing loading vectors to be
sparse, it performs the double duty of dimension reduction and variable
selection. Sparse PCA algorithms are usually expressed as a trade-off between
explained variance and sparsity of the loading vectors (i.e., number of
selected variables). As a high explained variance is not necessarily synonymous
with relevant information, these methods are prone to select irrelevant
variables. To overcome this issue, we propose an alternative formulation of
sparse PCA driven by the false discovery rate (FDR). We then leverage the
Terminating-Random Experiments (T-Rex) selector to automatically determine an
FDR-controlled support of the loading vectors. A major advantage of the
resulting T-Rex PCA is that no sparsity parameter tuning is required. Numerical
experiments and a stock market data example demonstrate a significant
performance improvement.
\\ ( https://arxiv.org/abs/2401.08375 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08381 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:11:54 GMT   (3925kb,D)

Title: Robotic Imitation of Human Actions
Authors: Josua Spisak, Matthias Kerzel, Stefan Wermter
Categories: cs.RO cs.LG
\\
  Imitation can allow us to quickly gain an understanding of a new task.
Through a demonstration, we can gain direct knowledge about which actions need
to be performed and which goals they have. In this paper, we introduce a new
approach to imitation learning that tackles the challenges of a robot imitating
a human, such as the change in perspective and body schema. Our approach can
use a single human demonstration to abstract information about the demonstrated
task, and use that information to generalise and replicate it. We facilitate
this ability by a new integration of two state-of-the-art methods: a diffusion
action segmentation model to abstract temporal information from the
demonstration and an open vocabulary object detector for spatial information.
Furthermore, we refine the abstracted information and use symbolic reasoning to
create an action plan utilising inverse kinematics, to allow the robot to
imitate the demonstrated action.
\\ ( https://arxiv.org/abs/2401.08381 ,  3925kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08404 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:44:06 GMT   (1533kb)

Title: Training and Comparison of nnU-Net and DeepMedic Methods for
  Autosegmentation of Pediatric Brain Tumors
Authors: Arastoo Vossough, Nastaran Khalili, Ariana M. Familiar, Deep Gandhi,
  Karthik Viswanathan, Wenxin Tu, Debanjan Haldar, Sina Bagheri, Hannah
  Anderson, Shuvanjan Haldar, Phillip B. Storm, Adam Resnick, Jeffrey B. Ware,
  Ali Nabavizadeh, Anahita Fathi Kazerooni
Categories: eess.IV cs.CV cs.LG physics.med-ph
\\
  Brain tumors are the most common solid tumors and the leading cause of
cancer-related death among children. Tumor segmentation is essential in
surgical and treatment planning, and response assessment and monitoring.
However, manual segmentation is time-consuming and has high inter-operator
variability, underscoring the need for more efficient methods. We compared two
deep learning-based 3D segmentation models, DeepMedic and nnU-Net, after
training with pediatric-specific multi-institutional brain tumor data using
based on multi-parametric MRI scans.Multi-parametric preoperative MRI scans of
339 pediatric patients (n=293 internal and n=46 external cohorts) with a
variety of tumor subtypes, were preprocessed and manually segmented into four
tumor subregions, i.e., enhancing tumor (ET), non-enhancing tumor (NET), cystic
components (CC), and peritumoral edema (ED). After training, performance of the
two models on internal and external test sets was evaluated using Dice scores,
sensitivity, and Hausdorff distance with reference to ground truth manual
segmentations. Dice score for nnU-Net internal test sets was (mean +/- SD
(median)) 0.9+/-0.07 (0.94) for WT, 0.77+/-0.29 for ET, 0.66+/-0.32 for NET,
0.71+/-0.33 for CC, and 0.71+/-0.40 for ED, respectively. For DeepMedic the
Dice scores were 0.82+/-0.16 for WT, 0.66+/-0.32 for ET, 0.48+/-0.27, for NET,
0.48+/-0.36 for CC, and 0.19+/-0.33 for ED, respectively. Dice scores were
significantly higher for nnU-Net (p<=0.01). External validation of the trained
nnU-Net model on the multi-institutional BraTS-PEDs 2023 dataset revealed high
generalization capability in segmentation of whole tumor and tumor core with
Dice scores of 0.87+/-0.13 (0.91) and 0.83+/-0.18 (0.89), respectively.
Pediatric-specific data trained nnU-Net model is superior to DeepMedic for
whole tumor and subregion segmentation of pediatric brain tumors.
\\ ( https://arxiv.org/abs/2401.08404 ,  1533kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08409 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:49:26 GMT   (1837kb,D)

Title: Faster ISNet for Background Bias Mitigation on Deep Neural Networks
Authors: Pedro R. A. S. Bassi, Sergio Decherchi and Andrea Cavalli
Categories: eess.IV cs.CV cs.CY cs.LG
\\
  Image background features can constitute background bias (spurious
correlations) and impact deep classifiers decisions, causing shortcut learning
(Clever Hans effect) and reducing the generalization skill on real-world data.
The concept of optimizing Layer-wise Relevance Propagation (LRP) heatmaps, to
improve classifier behavior, was recently introduced by a neural network
architecture named ISNet. It minimizes background relevance in LRP maps, to
mitigate the influence of image background features on deep classifiers
decisions, hindering shortcut learning and improving generalization. For each
training image, the original ISNet produces one heatmap per possible class in
the classification task, hence, its training time scales linearly with the
number of classes. Here, we introduce reformulated architectures that allow the
training time to become independent from this number, rendering the
optimization process much faster. We challenged the enhanced models utilizing
the MNIST dataset with synthetic background bias, and COVID-19 detection in
chest X-rays, an application that is prone to shortcut learning due to
background bias. The trained models minimized background attention and hindered
shortcut learning, while retaining high accuracy. Considering external
(out-of-distribution) test datasets, they consistently proved more accurate
than multiple state-of-the-art deep neural network architectures, including a
dedicated image semantic segmenter followed by a classifier. The architectures
presented here represent a potentially massive improvement in training speed
over the original ISNet, thus introducing LRP optimization into a gamut of
applications that could not be feasibly handled by the original model.
\\ ( https://arxiv.org/abs/2401.08409 ,  1837kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08414 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:58:21 GMT   (12149kb,D)

Title: Enhancing Dynamical System Modeling through Interpretable Machine
  Learning Augmentations: A Case Study in Cathodic Electrophoretic Deposition
Authors: Christian Jacobsen, Jiayuan Dong, Mehdi Khalloufi, Xun Huan, Karthik
  Duraisamy, Maryam Akram, Wanjiao Liu
Categories: physics.comp-ph cs.LG
\\
  We introduce a comprehensive data-driven framework aimed at enhancing the
modeling of physical systems, employing inference techniques and machine
learning enhancements. As a demonstrative application, we pursue the modeling
of cathodic electrophoretic deposition (EPD), commonly known as e-coating. Our
approach illustrates a systematic procedure for enhancing physical models by
identifying their limitations through inference on experimental data and
introducing adaptable model enhancements to address these shortcomings. We
begin by tackling the issue of model parameter identifiability, which reveals
aspects of the model that require improvement. To address generalizability , we
introduce modifications which also enhance identifiability. However, these
modifications do not fully capture essential experimental behaviors. To
overcome this limitation, we incorporate interpretable yet flexible
augmentations into the baseline model. These augmentations are parameterized by
simple fully-connected neural networks (FNNs), and we leverage machine learning
tools, particularly Neural Ordinary Differential Equations (Neural ODEs), to
learn these augmentations. Our simulations demonstrate that the machine
learning-augmented model more accurately captures observed behaviors and
improves predictive accuracy. Nevertheless, we contend that while the model
updates offer superior performance and capture the relevant physics, we can
reduce off-line computational costs by eliminating certain dynamics without
compromising accuracy or interpretability in downstream predictions of
quantities of interest, particularly film thickness predictions. The entire
process outlined here provides a structured approach to leverage data-driven
methods. Firstly, it helps us comprehend the root causes of model inaccuracies,
and secondly, it offers a principled method for enhancing model performance.
\\ ( https://arxiv.org/abs/2401.08414 ,  12149kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08415 (*cross-listing*)
Date: Tue, 16 Jan 2024 14:59:37 GMT   (427kb,D)

Title: From Coarse to Fine: Efficient Training for Audio Spectrogram
  Transformers
Authors: Jiu Feng, Mehmet Hamza Erol, Joon Son Chung, Arda Senocak
Categories: cs.SD cs.LG eess.AS
Comments: ICASSP 2024
\\
  Transformers have become central to recent advances in audio classification.
However, training an audio spectrogram transformer, e.g. AST, from scratch can
be resource and time-intensive. Furthermore, the complexity of transformers
heavily depends on the input audio spectrogram size. In this work, we aim to
optimize AST training by linking to the resolution in the time-axis. We
introduce multi-phase training of audio spectrogram transformers by connecting
the seminal idea of coarse-to-fine with transformer models. To achieve this, we
propose a set of methods for temporal compression. By employing one of these
methods, the transformer model learns from lower-resolution (coarse) data in
the initial phases, and then is fine-tuned with high-resolution data later in a
curriculum learning strategy. Experimental results demonstrate that the
proposed training mechanism for AST leads to improved (or on-par) performance
with faster convergence, i.e. requiring fewer computational resources and less
time. This approach is also generalizable to other AST-based methods regardless
of their learning paradigms.
\\ ( https://arxiv.org/abs/2401.08415 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08468 (*cross-listing*)
Date: Tue, 16 Jan 2024 16:18:17 GMT   (1400kb,D)

Title: Keep or toss? A nonparametric score to evaluate solutions for noisy ICA
Authors: Syamantak Kumar, Purnamrita Sarkar, Peter Bickel, and Derek Bean
Categories: math.ST cs.LG eess.SP stat.TH
\\
  In this paper, we propose a non-parametric score to evaluate the quality of
the solution to an iterative algorithm for Independent Component Analysis (ICA)
with arbitrary Gaussian noise. The novelty of this score stems from the fact
that it just assumes a finite second moment of the data and uses the
characteristic function to evaluate the quality of the estimated mixing matrix
without any knowledge of the parameters of the noise distribution. We also
provide a new characteristic function-based contrast function for ICA and
propose a fixed point iteration to optimize the corresponding objective
function. Finally, we propose a theoretical framework to obtain sufficient
conditions for the local and global optima of a family of contrast functions
for ICA. This framework uses quasi-orthogonalization inherently, and our
results extend the classical analysis of cumulant-based objective functions to
noisy ICA. We demonstrate the efficacy of our algorithms via experimental
results on simulated datasets.
\\ ( https://arxiv.org/abs/2401.08468 ,  1400kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08469 (*cross-listing*)
Date: Tue, 16 Jan 2024 16:18:42 GMT   (1631kb,D)

Title: Explanations of Classifiers Enhance Medical Image Segmentation via
  End-to-end Pre-training
Authors: Jiamin Chen and Xuhong Li and Yanwu Xu and Mengnan Du and Haoyi Xiong
Categories: eess.IV cs.CV cs.LG
\\
  Medical image segmentation aims to identify and locate abnormal structures in
medical images, such as chest radiographs, using deep neural networks. These
networks require a large number of annotated images with fine-grained masks for
the regions of interest, making pre-training strategies based on classification
datasets essential for sample efficiency. Based on a large-scale medical image
classification dataset, our work collects explanations from well-trained
classifiers to generate pseudo labels of segmentation tasks. Specifically, we
offer a case study on chest radiographs and train image classifiers on the
CheXpert dataset to identify 14 pathological observations in radiology. We then
use Integrated Gradients (IG) method to distill and boost the explanations
obtained from the classifiers, generating massive diagnosis-oriented
localization labels (DoLL). These DoLL-annotated images are used for
pre-training the model before fine-tuning it for downstream segmentation tasks,
including COVID-19 infectious areas, lungs, heart, and clavicles. Our method
outperforms other baselines, showcasing significant advantages in model
performance and training efficiency across various segmentation settings.
\\ ( https://arxiv.org/abs/2401.08469 ,  1631kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08522 (*cross-listing*)
Date: Tue, 16 Jan 2024 17:33:54 GMT   (372kb,D)

Title: Video Quality Assessment Based on Swin TransformerV2 and Coarse to Fine
  Strategy
Authors: Zihao Yu, Fengbin Guan, Yiting Lu, Xin Li, Zhibo Chen
Categories: cs.CV cs.LG eess.IV
\\
  The objective of non-reference video quality assessment is to evaluate the
quality of distorted video without access to reference high-definition
references. In this study, we introduce an enhanced spatial perception module,
pre-trained on multiple image quality assessment datasets, and a lightweight
temporal fusion module to address the no-reference visual quality assessment
(NR-VQA) task. This model implements Swin Transformer V2 as a local-level
spatial feature extractor and fuses these multi-stage representations through a
series of transformer layers. Furthermore, a temporal transformer is utilized
for spatiotemporal feature fusion across the video. To accommodate compressed
videos of varying bitrates, we incorporate a coarse-to-fine contrastive
strategy to enrich the model's capability to discriminate features from videos
of different bitrates. This is an expanded version of the one-page abstract.
\\ ( https://arxiv.org/abs/2401.08522 ,  372kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08544 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:11:14 GMT   (14139kb)

Title: N-Adaptive Ritz Method: A Neural Network Enriched Partition of Unity for
  Boundary Value Problems
Authors: Jonghyuk Baek and Yanran Wang and J. S. Chen
Categories: math.NA cs.LG cs.NA
Comments: 66 pages, 41 figures, 7 tables
\\
  Conventional finite element methods are known to be tedious in adaptive
refinements due to their conformal regularity requirements. Further, the
enrichment functions for adaptive refinements are often not readily available
in general applications. This work introduces a novel neural network-enriched
Partition of Unity (NN-PU) approach for solving boundary value problems via
artificial neural networks with a potential energy-based loss function
minimization. The flexibility and adaptivity of the NN function space are
utilized to capture complex solution patterns that the conventional Galerkin
methods fail to capture. The NN enrichment is constructed by combining
pre-trained feature-encoded NN blocks with an additional untrained NN block.
The pre-trained NN blocks learn specific local features during the offline
stage, enabling efficient enrichment of the approximation space during the
online stage through the Ritz-type energy minimization. The NN enrichment is
introduced under the Partition of Unity (PU) framework, ensuring convergence of
the proposed method. The proposed NN-PU approximation and feature-encoded
transfer learning forms an adaptive approximation framework, termed the
neural-refinement (n-refinement), for solving boundary value problems.
Demonstrated by solving various elasticity problems, the proposed method offers
accurate solutions while notably reducing the computational cost compared to
the conventional adaptive refinement in the mesh-based methods.
\\ ( https://arxiv.org/abs/2401.08544 ,  14139kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08559 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:39:15 GMT   (13366kb,D)

Title: Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation
Authors: Mathis Petrovich, Or Litany, Umar Iqbal, Michael J. Black, G\"ul
  Varol, Xue Bin Peng, Davis Rempe
Categories: cs.CV cs.GR cs.LG
Comments: Project page: https://mathis.petrovich.fr/stmc
\\
  Recent advances in generative modeling have led to promising progress on
synthesizing 3D human motion from text, with methods that can generate
character animations from short prompts and specified durations. However, using
a single text prompt as input lacks the fine-grained control needed by
animators, such as composing multiple actions and defining precise durations
for parts of the motion. To address this, we introduce the new problem of
timeline control for text-driven motion synthesis, which provides an intuitive,
yet fine-grained, input interface for users. Instead of a single prompt, users
can specify a multi-track timeline of multiple prompts organized in temporal
intervals that may overlap. This enables specifying the exact timings of each
action and composing multiple actions in sequence or at overlapping intervals.
To generate composite animations from a multi-track timeline, we propose a new
test-time denoising method. This method can be integrated with any pre-trained
motion diffusion model to synthesize realistic motions that accurately reflect
the timeline. At every step of denoising, our method processes each timeline
interval (text prompt) individually, subsequently aggregating the predictions
with consideration for the specific body parts engaged in each action.
Experimental comparisons and ablations validate that our method produces
realistic motions that respect the semantics and timing of given text prompts.
Our code and models are publicly available at https://mathis.petrovich.fr/stmc.
\\ ( https://arxiv.org/abs/2401.08559 ,  13366kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08564 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:49:08 GMT   (7298kb,D)

Title: ADVENT: Attack/Anomaly Detection in VANETs
Authors: Hamideh Baharlouei, Adetokunbo Makanju, Nur Zincir-Heywood
Categories: cs.CR cs.LG
\\
  In the domain of Vehicular Ad hoc Networks (VANETs), where the imperative of
having a real-world malicious detector capable of detecting attacks in
real-time and unveiling their perpetrators is crucial, our study introduces a
system with this goal. This system is designed for real-time detection of
malicious behavior, addressing the critical need to first identify the onset of
attacks and subsequently the responsible actors. Prior work in this area have
never addressed both requirements, which we believe are necessary for real
world deployment, simultaneously. By seamlessly integrating statistical and
machine learning techniques, the proposed system prioritizes simplicity and
efficiency. It excels in swiftly detecting attack onsets with a remarkable
F1-score of 99.66%, subsequently identifying malicious vehicles with an average
F1-score of approximately 97.85%. Incorporating federated learning in both
stages enhances privacy and improves the efficiency of malicious node
detection, effectively reducing the false negative rate.
\\ ( https://arxiv.org/abs/2401.08564 ,  7298kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08573 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:58:36 GMT   (11566kb,D)

Title: Benchmarking the Robustness of Image Watermarks
Authors: Bang An, Mucong Ding, Tahseen Rabbani, Aakriti Agrawal, Yuancheng Xu,
  Chenghao Deng, Sicheng Zhu, Abdirisak Mohamed, Yuxin Wen, Tom Goldstein,
  Furong Huang
Categories: cs.CV cs.CR cs.LG
\\
  This paper investigates the weaknesses of image watermarking techniques. We
present WAVES (Watermark Analysis Via Enhanced Stress-testing), a novel
benchmark for assessing watermark robustness, overcoming the limitations of
current evaluation methods.WAVES integrates detection and identification tasks,
and establishes a standardized evaluation protocol comprised of a diverse range
of stress tests. The attacks in WAVES range from traditional image distortions
to advanced and novel variations of adversarial, diffusive, and embedding-based
attacks. We introduce a normalized score of attack potency which incorporates
several widely used image quality metrics and allows us to produce of an
ordered ranking of attacks. Our comprehensive evaluation over reveals
previously undetected vulnerabilities of several modern watermarking
algorithms. WAVES is envisioned as a toolkit for the future development of
robust watermarking systems.
\\ ( https://arxiv.org/abs/2401.08573 ,  11566kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2303.17503
replaced with revised version Mon, 15 Jan 2024 13:12:36 GMT   (583kb,D)

Title: Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement
  Learning
Authors: Sotetsu Koyamada, Shinri Okano, Soichiro Nishimori, Yu Murata, Keigo
  Habara, Haruka Kita, Shin Ishii
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2303.17503 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10345
replaced with revised version Sun, 14 Jan 2024 08:01:24 GMT   (7105kb,D)

Title: Do as I can, not as I get
Authors: Shangfei Zheng, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Wei
  Chen, and Lei Zhao
Categories: cs.AI cs.IR
\\ ( https://arxiv.org/abs/2306.10345 ,  7105kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09552
replaced with revised version Sun, 14 Jan 2024 01:12:43 GMT   (1549kb,D)

Title: CB-Whisper: Contextual Biasing Whisper using Open-Vocabulary
  Keyword-Spotting
Authors: Yuang Li, Yinglu Li, Min Zhang, Chang Su, Mengxin Ren, Xiaosong Qiao,
  Xiaofeng Zhao, Mengyao Piao, Jiawei Yu, Xinglin Lv, Miaomiao Ma, Yanqing
  Zhao, Hao Yang
Categories: cs.AI cs.CL
Comments: 5 pages, 2 figures
\\ ( https://arxiv.org/abs/2309.09552 ,  1549kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06114
replaced with revised version Sat, 13 Jan 2024 00:42:24 GMT   (36104kb,D)

Title: Learning Interactive Real-World Simulators
Authors: Mengjiao Yang, Yilun Du, Kamyar Ghasemipour, Jonathan Tompson, Leslie
  Kaelbling, Dale Schuurmans, Pieter Abbeel
Categories: cs.AI
Comments: https://universal-simulator.github.io
\\ ( https://arxiv.org/abs/2310.06114 ,  36104kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09605
replaced with revised version Sat, 13 Jan 2024 10:25:26 GMT   (7051kb,D)

Title: Penetrative AI: Making LLMs Comprehend the Physical World
Authors: Huatao Xu, Liying Han, Qirui Yang, Mo Li, Mani Srivastava
Categories: cs.AI cs.LG
Comments: Published in HotMobile 2024
\\ ( https://arxiv.org/abs/2310.09605 ,  7051kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17072
replaced with revised version Mon, 15 Jan 2024 07:02:44 GMT   (11701kb,D)

Title: IMMP++: Isometric Motion Manifold Primitives with Parametric Curve
  Models
Authors: Yonghyeon Lee
Categories: cs.AI cs.LG cs.RO
Comments: 8 pages. This work has been submitted to the IEEE for possible
  publication
\\ ( https://arxiv.org/abs/2310.17072 ,  11701kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01520
replaced with revised version Mon, 15 Jan 2024 09:03:19 GMT   (894kb,D)

Title: Entropy and the Kullback-Leibler Divergence for Bayesian Networks:
  Computational Complexity and Efficient Implementation
Authors: Marco Scutari
Categories: cs.AI cs.LG stat.CO stat.ML
Comments: 32 pages, 4 figures
Journal-ref: Algorithms (2024), 17(1), 24
\\ ( https://arxiv.org/abs/2312.01520 ,  894kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03006
replaced with revised version Sun, 14 Jan 2024 20:22:12 GMT   (652kb,D)

Title: Multi-Weight Ranking for Multi-Criteria Decision Making
Authors: Andreas H Hamel and Daniel Kostner
Categories: cs.AI cs.LG math.ST stat.TH
Comments: 20 pages, 12 figures
MSC-class: 62G30, 90C29
\\ ( https://arxiv.org/abs/2312.03006 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05379
replaced with revised version Sun, 14 Jan 2024 10:23:09 GMT   (48kb,D)

Title: Exploring Parity Challenges in Reinforcement Learning through Curriculum
  Learning with Noisy Labels
Authors: Bei Zhou, Soren Riis
Categories: cs.AI
\\ ( https://arxiv.org/abs/2312.05379 ,  48kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01841
replaced with revised version Mon, 15 Jan 2024 20:45:21 GMT   (3359kb,D)

Title: Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov
  Decision Processes
Authors: Baiting Luo, Yunuo Zhang, Abhishek Dubey, Ayan Mukhopadhyay
Categories: cs.AI cs.LG
Comments: Accepted for publication at the International Conference on
  Autonomous Agents and MultiAgent Systems (AAMAS), 2024
\\ ( https://arxiv.org/abs/2401.01841 ,  3359kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04374
replaced with revised version Sat, 13 Jan 2024 06:00:18 GMT   (6393kb,D)

Title: Towards Explainable Artificial Intelligence (XAI): A Data Mining
  Perspective
Authors: Haoyi Xiong and Xuhong Li and Xiaofei Zhang and Jiamin Chen and Xinhao
  Sun and Yuchen Li and Zeyi Sun and Mengnan Du
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.04374 ,  6393kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04812
replaced with revised version Sat, 13 Jan 2024 21:18:46 GMT   (3379kb,D)

Title: Sample-and-Bound for Non-Convex Optimization
Authors: Yaoguang Zhai, Zhizhen Qin, Sicun Gao
Categories: cs.AI
Comments: Published at AAAI 2024. Code is available at
  https://github.com/aaucsd/MCIR
\\ ( https://arxiv.org/abs/2401.04812 ,  3379kb)
------------------------------------------------------------------------------
\\
arXiv:2102.01223
replaced with revised version Tue, 16 Jan 2024 11:50:49 GMT   (10578kb,D)

Title: Inducing Meaningful Units from Character Sequences with Dynamic Capacity
  Slot Attention
Authors: Melika Behjati and James Henderson
Categories: cs.CL cs.LG
Comments: Accepted to TMLR 2023
\\ ( https://arxiv.org/abs/2102.01223 ,  10578kb)
------------------------------------------------------------------------------
\\
arXiv:2105.00815
replaced with revised version Sun, 14 Jan 2024 13:32:18 GMT   (3250kb,D)

Title: Representation Learning for Weakly Supervised Relation Extraction
Authors: Zhuang Li
Categories: cs.CL
Comments: Master Research Thesis of the Australian National University, 60
  pages
\\ ( https://arxiv.org/abs/2105.00815 ,  3250kb)
------------------------------------------------------------------------------
\\
arXiv:2206.13288
replaced with revised version Sun, 14 Jan 2024 13:25:01 GMT   (689kb,D)

Title: Discovering Salient Neurons in Deep NLP Models
Authors: Nadir Durrani and Fahim Dalvi and Hassan Sajjad
Categories: cs.CL
\\ ( https://arxiv.org/abs/2206.13288 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2209.11326
replaced with revised version Fri, 12 Jan 2024 20:19:20 GMT   (16463kb,D)

Title: Towards Faithful Model Explanation in NLP: A Survey
Authors: Qing Lyu, Marianna Apidianaki, Chris Callison-Burch
Categories: cs.CL
Comments: Added acknowledgements; Accepted to the Computational Linguistics
  Journal (June 2024 issue)
\\ ( https://arxiv.org/abs/2209.11326 ,  16463kb)
------------------------------------------------------------------------------
\\
arXiv:2210.08604
replaced with revised version Sat, 13 Jan 2024 22:46:01 GMT   (6984kb,D)

Title: NormSAGE: Multi-Lingual Multi-Cultural Norm Discovery from Conversations
  On-the-Fly
Authors: Yi R. Fung, Tuhin Chakraborty, Hao Guo, Owen Rambow, Smaranda Muresan,
  Heng Ji
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2210.08604 ,  6984kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13709
replaced with revised version Sun, 14 Jan 2024 11:38:28 GMT   (258kb,D)

Title: Undesirable Biases in NLP: Addressing Challenges of Measurement
Authors: Oskar van der Wal, Dominik Bachmann, Alina Leidinger, Leendert van
  Maanen, Willem Zuidema, Katrin Schulz
Categories: cs.CL cs.AI
Journal-ref: Journal of Artificial Intelligence Research, 79, 1-40 (2024)
DOI: 10.1613/jair.1.15195
\\ ( https://arxiv.org/abs/2211.13709 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04511
replaced with revised version Sat, 13 Jan 2024 04:44:21 GMT   (4859kb,D)

Title: A Large-Scale Analysis of Persian Tweets Regarding Covid-19 Vaccination
Authors: Taha ShabaniMirzaei, Houmaan Chamani, Amirhossein Abaskohi, Zhivar
  Sourati Hassan Zadeh, Behnam Bahrak
Categories: cs.CL cs.SI
Comments: 10 figures
ACM-class: I.2.7
Journal-ref: Soc. Netw. Anal. Min. 13, 148 (2023)
DOI: 10.1007/s13278-023-01154-0
\\ ( https://arxiv.org/abs/2302.04511 ,  4859kb)
------------------------------------------------------------------------------
\\
arXiv:2303.13310
replaced with revised version Tue, 16 Jan 2024 16:24:36 GMT   (112kb,D)

Title: SwissBERT: The Multilingual Language Model for Switzerland
Authors: Jannis Vamvas and Johannes Gra\"en and Rico Sennrich
Categories: cs.CL
Comments: SwissText 2023 [v3: Changed template because the proceedings moved to
  a different publisher. Same content.]
\\ ( https://arxiv.org/abs/2303.13310 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10005
replaced with revised version Tue, 16 Jan 2024 05:43:20 GMT   (814kb,D)

Title: DinoSR: Self-Distillation and Online Clustering for Self-supervised
  Speech Representation Learning
Authors: Alexander H. Liu, Heng-Jui Chang, Michael Auli, Wei-Ning Hsu, James R.
  Glass
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.10005 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11554
replaced with revised version Mon, 15 Jan 2024 23:52:21 GMT   (1130kb,D)

Title: ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via
  Tool Embeddings
Authors: Shibo Hao, Tianyang Liu, Zhen Wang, Zhiting Hu
Categories: cs.CL cs.LG
Comments: NeurIPS 2023 (oral). Code: https://github.com/Ber666/ToolkenGPT
\\ ( https://arxiv.org/abs/2305.11554 ,  1130kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17547
replaced with revised version Tue, 16 Jan 2024 08:27:38 GMT   (310kb,D)

Title: Translatotron 3: Speech to Speech Translation with Monolingual Data
Authors: Eliya Nachmani, Alon Levkovitch, Yifan Ding, Chulayuth Asawaroengchai,
  Heiga Zen, Michelle Tadmor Ramanovich
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: To appear in ICASSP 2024
\\ ( https://arxiv.org/abs/2305.17547 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17727
replaced with revised version Tue, 16 Jan 2024 09:07:37 GMT   (1994kb,D)

Title: Learning a Structural Causal Model for Intuition Reasoning in
  Conversation
Authors: Hang Chen, Bingyu Liao, Jing Luo, Wenjing Zhu, Xinyu Yang
Categories: cs.CL
Journal-ref: IEEE Transactions on Knowledge and Data Engineering early access
  (2024) 1-14
DOI: 10.1109/TKDE.2024.3352575
\\ ( https://arxiv.org/abs/2305.17727 ,  1994kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05323
replaced with revised version Mon, 15 Jan 2024 11:05:23 GMT   (1063kb)

Title: Advancing Italian Biomedical Information Extraction with
  Transformers-based Models: Methodological Insights and Multicenter Practical
  Application
Authors: Claudio Crema, Tommaso Mario Buonocore, Silvia Fostinelli, Enea
  Parimbelli, Federico Verde, Cira Fundar\`o, Marina Manera, Matteo Cotta
  Ramusino, Marco Capelli, Alfredo Costa, Giuliano Binetti, Riccardo Bellazzi
  and Alberto Redolfi
Categories: cs.CL cs.AI cs.LG
Comments: 2 figures, 6 tables, Supplementary Notes included
ACM-class: I.2.7; J.3
Journal-ref: Journal of Biomedical Informatics, Volume 148, 2023, 104557, ISSN
  1532-0464
DOI: 10.1016/j.jbi.2023.104557
\\ ( https://arxiv.org/abs/2306.05323 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10443
replaced with revised version Sat, 13 Jan 2024 15:24:59 GMT   (788kb)

Title: Integrating a Heterogeneous Graph with Entity-aware Self-attention using
  Relative Position Labels for Reading Comprehension Model
Authors: Shima Foolad and Kourosh Kiani
Categories: cs.CL cs.LG
Comments: submitted for Multimedia Tools and Applications Journal
\\ ( https://arxiv.org/abs/2307.10443 ,  788kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16349
replaced with revised version Tue, 16 Jan 2024 16:51:33 GMT   (7783kb,D)

Title: Human Feedback is not Gold Standard
Authors: Tom Hosking, Phil Blunsom, Max Bartolo
Categories: cs.CL
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2309.16349 ,  7783kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16540
replaced with revised version Tue, 16 Jan 2024 15:36:40 GMT   (609kb,D)

Title: Unsupervised Pretraining for Fact Verification by Language Model
  Distillation
Authors: Adri\'an Bazaga and Pietro Li\`o and Gos Micklem
Categories: cs.CL cs.LG stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2309.16540 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00100
replaced with revised version Sat, 13 Jan 2024 15:44:00 GMT   (350kb,D)

Title: Multilingual Natural Language Processing Model for Radiology Reports --
  The Summary is all you need!
Authors: Mariana Lindo, Ana Sofia Santos, Andr\'e Ferreira, Jianning Li, Gijs
  Luijten, Gustavo Correia, Moon Kim, Benedikt Michael Schaarschmidt, Cornelius
  Deuschl, Johannes Haubold, Jens Kleesiek, Jan Egger and Victor Alves
Categories: cs.CL cs.AI
Comments: 10 pages, 1 figure, 3 tables
\\ ( https://arxiv.org/abs/2310.00100 ,  350kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05378
replaced with revised version Sun, 14 Jan 2024 09:29:18 GMT   (596kb)

Title: Transcending the Attention Paradigm: Representation Learning from
  Geospatial Social Media Data
Authors: Nick DiSanto, Anthony Corso, Benjamin Sanders, Gavin Harding
Categories: cs.CL cs.SI
\\ ( https://arxiv.org/abs/2310.05378 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05628
replaced with revised version Tue, 16 Jan 2024 14:02:07 GMT   (497kb,D)

Title: Glitter or Gold? Deriving Structured Insights from Sustainability
  Reports via Large Language Models
Authors: Marco Bronzini, Carlo Nicolini, Bruno Lepri, Andrea Passerini, Jacopo
  Staiano
Categories: cs.CL cs.CE cs.CY
\\ ( https://arxiv.org/abs/2310.05628 ,  497kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05782
replaced with revised version Sat, 13 Jan 2024 11:37:57 GMT   (3435kb,D)

Title: Aligning Language Models with Human Preferences via a Bayesian Approach
Authors: Jiashuo Wang, Haozhao Wang, Shichao Sun, Wenjie Li
Categories: cs.CL
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.05782 ,  3435kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08279
replaced with revised version Mon, 15 Jan 2024 04:22:59 GMT   (358kb,D)

Title: Can Text-based Knowledge Graph Completion Benefit From Zero-Shot Large
  Language Models?
Authors: Rui Yang, Li Fang, Yi Zhou
Categories: cs.CL cs.AI
Comments: new versionv
\\ ( https://arxiv.org/abs/2310.08279 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10803
replaced with revised version Tue, 16 Jan 2024 05:54:49 GMT   (1022kb,D)

Title: SD-HuBERT: Sentence-Level Self-Distillation Induces Syllabic
  Organization in HuBERT
Authors: Cheol Jun Cho, Abdelrahman Mohamed, Shang-Wen Li, Alan W Black and
  Gopala K. Anumanchipalli
Categories: cs.CL eess.AS
\\ ( https://arxiv.org/abs/2310.10803 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11454
replaced with revised version Tue, 16 Jan 2024 18:59:22 GMT   (187kb,D)

Title: VeRA: Vector-based Random Matrix Adaptation
Authors: Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano
Categories: cs.CL
Comments: Accepted at ICLR 2024, website: https://dkopi.github.io/vera
\\ ( https://arxiv.org/abs/2310.11454 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12798
replaced with revised version Mon, 15 Jan 2024 03:02:01 GMT   (971kb,D)

Title: MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and
  Uni-Modal Adapter
Authors: Zhiyuan Liu, Sihang Li, Yanchen Luo, Hao Fei, Yixin Cao, Kenji
  Kawaguchi, Xiang Wang, Tat-Seng Chua
Categories: cs.CL cs.MM
Comments: EMNLP main conference. 9 pages
\\ ( https://arxiv.org/abs/2310.12798 ,  971kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18341
replaced with revised version Sun, 14 Jan 2024 13:29:15 GMT   (2828kb)

Title: CXR-LLAVA: a multimodal large language model for interpreting chest
  X-ray images
Authors: Seowoo Lee, Jiwon Youn, Hyungjin Kim, Mansu Kim, Soon Ho Yoon
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.18341 ,  2828kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19680
replaced with revised version Sat, 13 Jan 2024 15:39:10 GMT   (733kb,D)

Title: Integrating Pre-trained Language Model into Neural Machine Translation
Authors: Soon-Jae Hwang, Chang-Sung Jeong
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.19680 ,  733kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03220
replaced with revised version Tue, 16 Jan 2024 07:12:32 GMT   (1737kb,D)

Title: ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic
  Decision-Making with AI Agents
Authors: Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang,
  Tao Ge, Furu Wei
Categories: cs.CL cs.AI cs.GT
\\ ( https://arxiv.org/abs/2311.03220 ,  1737kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04076
replaced with revised version Mon, 15 Jan 2024 17:52:31 GMT   (2539kb,D)

Title: Do LLMs exhibit human-like response biases? A case study in survey
  design
Authors: Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar,
  Graham Neubig
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.04076 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04131
replaced with revised version Sat, 13 Jan 2024 20:07:22 GMT   (15389kb,D)

Title: Locating Cross-Task Sequence Continuation Circuits in Transformers
Authors: Michael Lan, Fazl Barez
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.04131 ,  15389kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08724
replaced with revised version Sun, 14 Jan 2024 23:44:15 GMT   (319kb)

Title: Knowledge Graph Construction in Power Distribution Networks
Authors: Xiang Li, Che Wang, Bing Li, Hao Chen, Sizhe Li
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.08724 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09812
replaced with revised version Sun, 14 Jan 2024 06:32:09 GMT   (4604kb,D)

Title: Large Language Models for Propaganda Span Annotation
Authors: Maram Hasanain, Fatema Ahmed, Firoj Alam
Categories: cs.CL
Comments: propaganda, span detection, disinformation, misinformation, fake
  news, LLMs, GPT-4
MSC-class: 68T50
ACM-class: F.2.2; I.2.7
\\ ( https://arxiv.org/abs/2311.09812 ,  4604kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12275
replaced with revised version Sun, 14 Jan 2024 04:31:56 GMT   (2586kb,D)

Title: Enabling On-Device Large Language Model Personalization with
  Self-Supervised Data Selection and Synthesis
Authors: Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei
  Zhou, Jingtong Hu, Yiyu Shi
Categories: cs.CL
Comments: 6 pages, 3 figures, 3 tables
\\ ( https://arxiv.org/abs/2311.12275 ,  2586kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13708
replaced with revised version Mon, 15 Jan 2024 00:14:58 GMT   (349kb)

Title: Dynamic Fault Analysis in Substations Based on Knowledge Graphs
Authors: Weiwei Li, Xing Liu, Wei Wang, Lu Chen, Sizhe Li, Hui Fan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.13708 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15565
replaced with revised version Sat, 13 Jan 2024 15:09:41 GMT   (746kb)

Title: Evaluating the Efficacy of Hybrid Deep Learning Models in Distinguishing
  AI-Generated Text
Authors: Abiodun Finbarrs Oketunji
Categories: cs.CL cs.AI cs.LG
MSC-class: I.2.7
ACM-class: I.2.7
DOI: 10.5281/zenodo.10251778
\\ ( https://arxiv.org/abs/2311.15565 ,  746kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16989
replaced with revised version Mon, 15 Jan 2024 09:55:05 GMT   (817kb,D)

Title: ChatGPT's One-year Anniversary: Are Open-Source Large Language Models
  Catching up?
Authors: Hailin Chen, Fangkai Jiao, Xingxuan Li, Chengwei Qin, Mathieu Ravaut,
  Ruochen Zhao, Caiming Xiong, Shafiq Joty
Categories: cs.CL
Comments: version v4, included latest top-performing open-sourced LLMs
\\ ( https://arxiv.org/abs/2311.16989 ,  817kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04350
replaced with revised version Tue, 16 Jan 2024 12:07:27 GMT   (579kb,D)

Title: CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language
  Models
Authors: Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal,
  Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner,
  Mrinmaya Sachan, Bernhard Sch\"olkopf
Categories: cs.CL cs.AI cs.LG
Comments: NeurIPS 2023; updated with CLadder dataset v1.5
\\ ( https://arxiv.org/abs/2312.04350 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06355
replaced with revised version Tue, 16 Jan 2024 13:35:20 GMT   (4524kb)

Title: Linguistic and Structural Basis of Engineering Design Knowledge
Authors: L. Siddharth, Jianxi Luo
Categories: cs.CL cs.DL cs.IR
\\ ( https://arxiv.org/abs/2312.06355 ,  4524kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09211
replaced with revised version Sat, 13 Jan 2024 13:52:16 GMT   (696kb,D)

Title: Mitigating Outlier Activations in Low-Precision Fine-Tuning of Language
  Models
Authors: Alireza Ghaffari, Justin Yu, Mahsa Ghazvini Nejad, Masoud Asgharian,
  Boxing Chen, Vahid Partovi Nia
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.09211 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10048
replaced with revised version Sun, 14 Jan 2024 23:04:14 GMT   (518kb)

Title: Knowledge Graph Enhanced Aspect-Level Sentiment Analysis
Authors: Kavita Sharma, Ritu Patel, Sunita Iyer
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.10048 ,  518kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12108
replaced with revised version Tue, 16 Jan 2024 10:03:57 GMT   (209kb,D)

Title: Knowledge Graph Error Detection with Contrastive Confidence Adaption
Authors: Xiangyu Liu and Yang Liu and Wei Hu
Categories: cs.CL cs.AI
Comments: Accepted in the 38th AAAI Conference on Artificial Intelligence (AAAI
  2024)
\\ ( https://arxiv.org/abs/2312.12108 ,  209kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14033
replaced with revised version Mon, 15 Jan 2024 03:18:25 GMT   (4275kb,D)

Title: T-Eval: Evaluating the Tool Utilization Capability of Large Language
  Models Step by Step
Authors: Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao
  Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, Feng Zhao
Categories: cs.CL
Comments: Project: https://open-compass.github.io/T-Eval
\\ ( https://arxiv.org/abs/2312.14033 ,  4275kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17267
replaced with revised version Mon, 15 Jan 2024 08:09:00 GMT   (649kb,D)

Title: Improving Low-resource Prompt-based Relation Representation with
  Multi-view Decoupling Learning
Authors: Chenghao Fan, Wei Wei, Xiaoye Qu, Zhenyi Lu, Wenfeng Xie, Yu Cheng,
  Dangyang Chen
Categories: cs.CL cs.AI
Comments: Accepted to AAAI 2024
\\ ( https://arxiv.org/abs/2312.17267 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17482
replaced with revised version Tue, 16 Jan 2024 16:03:31 GMT   (8950kb,D)

Title: MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining
Authors: Jacob Portes, Alex Trott, Sam Havens, Daniel King, Abhinav Venigalla,
  Moin Nadeem, Nikhil Sardana, Daya Khudia, Jonathan Frankle
Categories: cs.CL cs.LG
Comments: 10 pages, 4 figures in main text. 25 pages total
Journal-ref: NeurIPS 2023
\\ ( https://arxiv.org/abs/2312.17482 ,  8950kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00165
replaced with revised version Sat, 13 Jan 2024 05:56:17 GMT   (3112kb,D)

Title: Mitigating the Impact of False Negatives in Dense Retrieval with
  Contrastive Confidence Regularization
Authors: Shiqi Wang, Yeqin Zhang and Cam-Tu Nguyen
Categories: cs.CL
Comments: Accepted by AAAI24
\\ ( https://arxiv.org/abs/2401.00165 ,  3112kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00741
replaced with revised version Sun, 14 Jan 2024 15:06:18 GMT   (9910kb,D)

Title: ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of
  Large Language Models in Real-world Scenarios
Authors: Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian
  Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.00741 ,  9910kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01326
replaced with revised version Mon, 15 Jan 2024 13:39:38 GMT   (1333kb,D)

Title: An Autoregressive Text-to-Graph Framework for Joint Entity and Relation
  Extraction
Authors: Urchade Zaratiana, Nadi Tomeh, Pierre Holat, Thierry Charnois
Categories: cs.CL cs.AI cs.LG
Comments: AAAI 2024 (camera ready version)
\\ ( https://arxiv.org/abs/2401.01326 ,  1333kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02987
replaced with revised version Mon, 15 Jan 2024 18:50:17 GMT   (10096kb,D)

Title: Has Your Pretrained Model Improved? A Multi-head Posterior Based
  Approach
Authors: Prince Aboagye, Yan Zheng, Junpeng Wang, Uday Singh Saini, Xin Dai,
  Michael Yeh, Yujie Fan, Zhongfang Zhuang, Shubham Jain, Liang Wang and Wei
  Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.02987 ,  10096kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03642
replaced with revised version Tue, 16 Jan 2024 01:05:59 GMT   (258kb,D)

Title: A Content-Based Novelty Measure for Scholarly Publications: A Proof of
  Concept
Authors: Haining Wang
Categories: cs.CL cs.DL
Comments: Accepted for publication in the proceedings of iConference2024
\\ ( https://arxiv.org/abs/2401.03642 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04658
replaced with revised version Mon, 15 Jan 2024 14:57:29 GMT   (623kb,D)

Title: Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence
  Lengths in Large Language Models
Authors: Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong
Categories: cs.CL cs.AI
Comments: Technical Report. Yiran Zhong is the corresponding author. The source
  code is available at https://github.com/OpenNLPLab/lightning-attention
\\ ( https://arxiv.org/abs/2401.04658 ,  623kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04925
replaced with revised version Tue, 16 Jan 2024 17:40:14 GMT   (411kb,D)

Title: The Impact of Reasoning Step Length on Large Language Models
Authors: Mingyu Jin, Qinkai Yu, Dong shu, Haiyan Zhao, Wenyue Hua, Yanda Meng,
  Yongfeng Zhang, Mengnan Du
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.04925 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05561
replaced with revised version Sat, 13 Jan 2024 17:57:06 GMT   (1499kb,D)

Title: TrustLLM: Trustworthiness in Large Language Models
Authors: Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie
  Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin
  Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao,
  Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan
  Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal,
  James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang
  Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang
  He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu
  Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen,
  Tianming Liu, Tianyi Zhou, Willian Wang, Xiang Li, Xiangliang Zhang, Xiao
  Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, et al. (3
  additional authors not shown)
Categories: cs.CL
Comments: This work is still under work and we welcome your contribution
\\ ( https://arxiv.org/abs/2401.05561 ,  1499kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05596
replaced with revised version Tue, 16 Jan 2024 14:42:45 GMT   (850kb)

Title: POMP: Probability-driven Meta-graph Prompter for LLMs in Low-resource
  Unsupervised Neural Machine Translation
Authors: Shilong Pan, Zhiliang Tian, Liang Ding, Zhen Huang, Zhihua Wen,
  Dongsheng Li
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.05596 ,  850kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06400
replaced with revised version Tue, 16 Jan 2024 06:01:48 GMT   (5781kb,D)

Title: Generalizing Visual Question Answering from Synthetic to Human-Written
  Questions via a Chain of QA with a Large Language Model
Authors: Taehee Kim, Yeongjae Cho, Heejun Shin, Yohan Jo, Dongmyung Shin
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2401.06400 ,  5781kb)
------------------------------------------------------------------------------
\\
arXiv:1809.04564
replaced with revised version Mon, 15 Jan 2024 14:55:29 GMT   (132kb,D)

Title: On the Generalization of Stochastic Gradient Descent with Momentum
Authors: Ali Ramezani-Kebrya, Kimon Antonakopoulos, Volkan Cevher, Ashish
  Khisti, Ben Liang
Categories: cs.LG stat.ML
Comments: 56 pages, 14 figures. To appear in the Journal of Machine Learning
  Research (JMLR)
\\ ( https://arxiv.org/abs/1809.04564 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:1902.01453
replaced with revised version Sat, 13 Jan 2024 19:33:12 GMT   (9148kb,D)

Title: PVNet: A LRCN Architecture for Spatio-Temporal Photovoltaic
  PowerForecasting from Numerical Weather Prediction
Authors: Johan Mathe, Nina Miolane, Nicolas Sebastien, Jeremie Lequeux
Categories: cs.LG cs.AI
Comments: 8 pages
\\ ( https://arxiv.org/abs/1902.01453 ,  9148kb)
------------------------------------------------------------------------------
\\
arXiv:2002.12410
replaced with revised version Sun, 14 Jan 2024 16:36:09 GMT   (932kb,D)

Title: On Biased Compression for Distributed Learning
Authors: Aleksandr Beznosikov and Samuel Horv\'ath and Peter Richt\'arik and
  Mher Safaryan
Categories: cs.LG cs.DC math.OC stat.ML
Comments: 50 pages, 9 figures, 5 tables, 22 theorems and lemmas, 7 new
  compression operators, 1 algorithm
Journal-ref: Journal of Machine Learning Research 2023:
  https://www.jmlr.org/papers/v24/21-1548.html
\\ ( https://arxiv.org/abs/2002.12410 ,  932kb)
------------------------------------------------------------------------------
\\
arXiv:2008.05825
replaced with revised version Sun, 14 Jan 2024 14:45:36 GMT   (3384kb,D)

Title: Unifying supervised learning and VAEs -- coverage, systematics and
  goodness-of-fit in normalizing-flow based neural network models for
  astro-particle reconstructions
Authors: Thorsten Gl\"usenkamp
Categories: cs.LG astro-ph.HE astro-ph.IM hep-ex stat.ML
\\ ( https://arxiv.org/abs/2008.05825 ,  3384kb)
------------------------------------------------------------------------------
\\
arXiv:2009.04614
replaced with revised version Tue, 16 Jan 2024 02:54:58 GMT   (4180kb,D)

Title: End-to-end Kernel Learning via Generative Random Fourier Features
Authors: Kun Fang, Fanghui Liu, Xiaolin Huang and Jie Yang
Categories: cs.LG stat.ML
Comments: Accepted by Pattern Recognition
DOI: 10.1016/j.patcog.2022.109057
\\ ( https://arxiv.org/abs/2009.04614 ,  4180kb)
------------------------------------------------------------------------------
\\
arXiv:2101.04645
replaced with revised version Sun, 14 Jan 2024 17:28:57 GMT   (841kb,D)

Title: Double-Adversarial Activation Anomaly Detection: Adversarial
  Autoencoders are Anomaly Generators
Authors: J.-P. Schulze, P. Sperl, K. B\"ottinger
Categories: cs.LG cs.CR
Comments: Accepted at IJCNN 2022
DOI: 10.1109/IJCNN55064.2022.9892896
\\ ( https://arxiv.org/abs/2101.04645 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2104.00032
replaced with revised version Mon, 15 Jan 2024 08:33:14 GMT   (20097kb,D)

Title: Convolutional Dynamic Alignment Networks for Interpretable
  Classifications
Authors: Moritz B\"ohle and Mario Fritz and Bernt Schiele
Categories: cs.LG
Comments: Published at CVRP 2021 (oral)
\\ ( https://arxiv.org/abs/2104.00032 ,  20097kb)
------------------------------------------------------------------------------
\\
arXiv:2107.00116
replaced with revised version Mon, 15 Jan 2024 20:05:14 GMT   (185kb,D)

Title: On the Benefits of Inducing Local Lipschitzness for Robust Generative
  Adversarial Imitation Learning
Authors: Farzan Memarian, Abolfazl Hashemi, Scott Niekum, Ufuk Topcu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2107.00116 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2110.10083
replaced with revised version Mon, 15 Jan 2024 18:49:36 GMT   (8853kb,D)

Title: Contrastive Active Inference
Authors: Pietro Mazzaglia and Tim Verbelen and Bart Dhoedt
Categories: cs.LG
Comments: Accepted as a conference paper at 35th Conference on Neural
  Information Processing Systems (NeurIPS 2021)
\\ ( https://arxiv.org/abs/2110.10083 ,  8853kb)
------------------------------------------------------------------------------
\\
arXiv:2111.13322
replaced with revised version Sun, 14 Jan 2024 12:30:15 GMT   (641kb,D)

Title: Random-reshuffled SARAH does not need a full gradient computations
Authors: Aleksandr Beznosikov and Martin Tak\'a\v{c}
Categories: cs.LG math.OC
Comments: 20 pages, 2 algorithms, 5 figures, 3 tables
DOI: 10.1007/s11590-023-02081-x
\\ ( https://arxiv.org/abs/2111.13322 ,  641kb)
------------------------------------------------------------------------------
\\
arXiv:2204.05104
replaced with revised version Mon, 15 Jan 2024 10:48:19 GMT   (2013kb,D)

Title: Self-Supervised Graph Neural Network for Multi-Source Domain Adaptation
Authors: Jin Yuan, Feng Hou, Yangzhou Du, Zhongchao Shi, Xin Geng, Jianping
  Fan, Yong Rui
Categories: cs.LG
\\ ( https://arxiv.org/abs/2204.05104 ,  2013kb)
------------------------------------------------------------------------------
\\
arXiv:2205.03990
replaced with revised version Sun, 14 Jan 2024 01:45:10 GMT   (23389kb,D)

Title: Multi-resolution partial differential equations preserved learning
  framework for spatiotemporal dynamics
Authors: Xin-Yang Liu and Min Zhu and Lu Lu and Hao Sun and Jian-Xun Wang
Categories: cs.LG physics.comp-ph
Comments: 51 pages, 27 figures
Journal-ref: Commun Phys 7, 31 (2024)
DOI: 10.1038/s42005-024-01521-z
\\ ( https://arxiv.org/abs/2205.03990 ,  23389kb)
------------------------------------------------------------------------------
\\
arXiv:2205.12787
replaced with revised version Sun, 14 Jan 2024 12:12:06 GMT   (442kb,D)

Title: Impartial Games: A Challenge for Reinforcement Learning
Authors: Bei Zhou and S{\o}ren Riis
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2205.12787 ,  442kb)
------------------------------------------------------------------------------
\\
arXiv:2205.13114
replaced with revised version Tue, 16 Jan 2024 01:34:20 GMT   (99kb,D)

Title: Contextual Pandora's Box
Authors: Alexia Atsidakou, Constantine Caramanis, Evangelia Gergatsouli,
  Orestis Papadigenopoulos, Christos Tzamos
Categories: cs.LG
\\ ( https://arxiv.org/abs/2205.13114 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2206.11030
replaced with revised version Mon, 15 Jan 2024 12:13:57 GMT   (3688kb)

Title: KeyCLD: Learning Constrained Lagrangian Dynamics in Keypoint Coordinates
  from Images
Authors: Rembert Daems, Jeroen Taets, Francis wyffels and Guillaume Crevecoeur
Categories: cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2206.11030 ,  3688kb)
------------------------------------------------------------------------------
\\
arXiv:2207.10199
replaced with revised version Mon, 15 Jan 2024 09:23:39 GMT   (133kb,D)

Title: Provably tuning the ElasticNet across instances
Authors: Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, Ameet
  Talwalkar
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2207.10199 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2209.02935
replaced with revised version Sat, 13 Jan 2024 05:55:55 GMT   (997kb,D)

Title: Normalised clustering accuracy: An asymmetric external cluster validity
  measure
Authors: Marek Gagolewski
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2209.02935 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07225
replaced with revised version Sun, 14 Jan 2024 10:55:39 GMT   (4871kb,D)

Title: MIXRTs: Toward Interpretable Multi-Agent Reinforcement Learning via
  Mixing Recurrent Soft Decision Trees
Authors: Zichuan Liu, Yuanyang Zhu, Zhi Wang, Yang Gao, Chunlin Chen
Categories: cs.LG cs.MA
\\ ( https://arxiv.org/abs/2209.07225 ,  4871kb)
------------------------------------------------------------------------------
\\
arXiv:2211.02658
replaced with revised version Sat, 13 Jan 2024 18:33:56 GMT   (11789kb,D)

Title: Dealing with Drift of Adaptation Spaces in Learning-based Self-Adaptive
  Systems using Lifelong Self-Adaptation
Authors: Omid Gheibi and Danny Weyns
Categories: cs.LG cs.AI cs.NE cs.SE
DOI: 10.1145/3636428
\\ ( https://arxiv.org/abs/2211.02658 ,  11789kb)
------------------------------------------------------------------------------
\\
arXiv:2212.03597
replaced with revised version Sun, 14 Jan 2024 22:14:26 GMT   (462kb,D)

Title: DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and
  Training Efficiency via Efficient Data Sampling and Routing
Authors: Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Connor Holmes,
  Cheng Li, Yuxiong He
Categories: cs.LG cs.AI
Comments: Published in AAAI 2024 Main Technical Track. Equal contribution by
  the first 3 authors. Code has been released as a part of
  https://github.com/microsoft/DeepSpeed. Part of this paper is from our
  previous arxiv report (arXiv:2211.11586)
\\ ( https://arxiv.org/abs/2212.03597 ,  462kb)
------------------------------------------------------------------------------
\\
arXiv:2212.08949
replaced with revised version Tue, 16 Jan 2024 06:59:29 GMT   (187kb,D)

Title: Managing Temporal Resolution in Continuous Value Estimation: A
  Fundamental Trade-off
Authors: Zichen Zhang, Johannes Kirschner, Junxi Zhang, Francesco Zanini, Alex
  Ayoub, Masood Dehghan, Dale Schuurmans
Categories: cs.LG cs.SY eess.SY stat.ML
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2212.08949 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2301.09734
replaced with revised version Mon, 15 Jan 2024 15:04:10 GMT   (10562kb,D)

Title: Topological Learning in Multi-Class Data Sets
Authors: Christopher Griffin and Trevor Karn and Benjamin Apple
Categories: cs.LG physics.data-an
Comments: 16 pages, 18 figures. This is a revision of v2
\\ ( https://arxiv.org/abs/2301.09734 ,  10562kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11578
replaced with revised version Mon, 15 Jan 2024 22:22:24 GMT   (14357kb,D)

Title: Learning to Unlearn: Instance-wise Unlearning for Pre-trained
  Classifiers
Authors: Sungmin Cha, Sungjun Cho, Dasol Hwang, Honglak Lee, Taesup Moon, and
  Moontae Lee
Categories: cs.LG
Comments: AAAI 2024 camera ready version
\\ ( https://arxiv.org/abs/2301.11578 ,  14357kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12755
replaced with revised version Mon, 15 Jan 2024 15:52:46 GMT   (215kb,D)

Title: Efficient Node Selection in Private Personalized Decentralized Learning
Authors: Edvin Listo Zec, Johan \"Ostman, Olof Mogren, Daniel Gillblad
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.12755 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01463
replaced with revised version Mon, 15 Jan 2024 17:27:40 GMT   (378kb,D)

Title: Gradient Descent with Linearly Correlated Noise: Theory and Applications
  to Differential Privacy
Authors: Anastasia Koloskova, Ryan McKenna, Zachary Charles, Keith Rush,
  Brendan McMahan
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.01463 ,  378kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04925
replaced with revised version Sun, 14 Jan 2024 08:18:25 GMT   (49kb)

Title: Information Theoretic Lower Bounds for Information Theoretic Upper
  Bounds
Authors: Roi Livni
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2302.04925 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02561
replaced with revised version Tue, 16 Jan 2024 17:06:42 GMT   (0kb,I)

Title: CAMEL: Curvature-Augmented Manifold Embedding and Learning
Authors: Nan Xu, Yongming Liu
Categories: cs.LG stat.ML
Comments: The original results reported in the original manuscript cannot be
  reproduced by a valid code from the first author. The first author stated
  that the original code was lost during a computer crash and cannot be
  retrieved. The corresponding author decided to withdraw the manuscript for
  further evaluation before resubmission
\\ ( https://arxiv.org/abs/2303.02561 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2303.03374
replaced with revised version Mon, 15 Jan 2024 19:12:13 GMT   (644kb,D)

Title: To Stay or Not to Stay in the Pre-train Basin: Insights on Ensembling in
  Transfer Learning
Authors: Ildus Sadrtdinov, Dmitrii Pozdeev, Dmitry Vetrov, Ekaterina Lobacheva
Categories: cs.LG stat.ML
Comments: Published in NeurIPS 2023. First two authors contributed equally
\\ ( https://arxiv.org/abs/2303.03374 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11593
replaced with revised version Sun, 14 Jan 2024 00:18:44 GMT   (2792kb)

Title: Difficulty in chirality recognition for Transformer architectures
  learning chemical structures from string
Authors: Yasuhiro Yoshikai, Tadahaya Mizuno, Shumpei Nemoto, Hiroyuki Kusuhara
Categories: cs.LG cs.CL physics.chem-ph q-bio.BM
Comments: 29 pages, 6 figures
ACM-class: J.2; I.2.7
\\ ( https://arxiv.org/abs/2303.11593 ,  2792kb)
------------------------------------------------------------------------------
\\
arXiv:2303.13506
replaced with revised version Sat, 13 Jan 2024 23:51:39 GMT   (1758kb,D)

Title: The Quantization Model of Neural Scaling
Authors: Eric J. Michaud, Ziming Liu, Uzay Girit, Max Tegmark
Categories: cs.LG cond-mat.dis-nn
Comments: 24 pages, 18 figures, NeurIPS 2023
\\ ( https://arxiv.org/abs/2303.13506 ,  1758kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09221
replaced with revised version Fri, 12 Jan 2024 23:41:44 GMT   (19kb)

Title: Convergence of stochastic gradient descent under a local Lojasiewicz
  condition for deep neural networks
Authors: Jing An and Jianfeng Lu
Categories: cs.LG math.OC stat.ML
Comments: v2 fixed several mistakes. Some parts have been rewritten
\\ ( https://arxiv.org/abs/2304.09221 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10818
replaced with revised version Tue, 16 Jan 2024 10:03:54 GMT   (8758kb,D)

Title: Diffusion Language Models Generation Can Be Halted Early
Authors: Sofia Maria Lo Cicero Vaina, Nikita Balagansky, Daniil Gavrilov
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2305.10818 ,  8758kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11417
replaced with revised version Tue, 16 Jan 2024 16:29:09 GMT   (37kb)

Title: Complexity of Deep Neural Networks from the Perspective of Functional
  Equivalence
Authors: Guohao Shen
Categories: cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2305.11417 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15944
replaced with revised version Tue, 16 Jan 2024 10:53:05 GMT   (3059kb,D)

Title: How to Turn Your Knowledge Graph Embeddings into Generative Models
Authors: Lorenzo Loconte, Nicola Di Mauro, Robert Peharz, Antonio Vergari
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2305.15944 ,  3059kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16215
replaced with revised version Tue, 16 Jan 2024 15:02:57 GMT   (1015kb,D)

Title: Koopman Kernel Regression
Authors: Petar Bevanda, Max Beier, Armin Lederer, Stefan Sosnowski, Eyke
  H\"ullermeier, Sandra Hirche
Categories: cs.LG cs.SY eess.SY math.DS stat.ML
Comments: Accepted to the thirty-seventh Conference on Neural Information
  Processing Systems (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2305.16215 ,  1015kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16284
replaced with revised version Tue, 16 Jan 2024 15:56:09 GMT   (562kb,D)

Title: DoWG Unleashed: An Efficient Universal Parameter-Free Gradient Descent
  Method
Authors: Ahmed Khaled and Konstantin Mishchenko and Chi Jin
Categories: cs.LG math.OC stat.ML
Comments: 22 pages, 1 table, 4 figures
\\ ( https://arxiv.org/abs/2305.16284 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16501
replaced with revised version Mon, 15 Jan 2024 16:39:52 GMT   (64kb)

Title: Strategic Classification under Unknown Personalized Manipulation
Authors: Han Shao, Avrim Blum, Omar Montasser
Categories: cs.LG cs.GT
\\ ( https://arxiv.org/abs/2305.16501 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18342
replaced with revised version Sun, 14 Jan 2024 11:52:11 GMT   (1282kb,D)

Title: Neural Task Synthesis for Visual Programming
Authors: Victor-Alexandru P\u{a}durean, Georgios Tzannetos, Adish Singla
Categories: cs.LG cs.AI cs.CL cs.CY cs.PL
Comments: Published in Transactions on Machine Learning Research (TMLR) 2024
\\ ( https://arxiv.org/abs/2305.18342 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18455
replaced with revised version Mon, 15 Jan 2024 07:51:23 GMT   (25615kb,D)

Title: Diff-Instruct: A Universal Approach for Transferring Knowledge From
  Pre-trained Diffusion Models
Authors: Weijian Luo and Tianyang Hu and Shifeng Zhang and Jiacheng Sun and
  Zhenguo Li and Zhihua Zhang
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2305.18455 ,  25615kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19706
replaced with revised version Mon, 15 Jan 2024 13:06:10 GMT   (870kb,D)

Title: Necessary and Sufficient Conditions for Optimal Decision Trees using
  Dynamic Programming
Authors: Jacobus G. M. van der Linden, Mathijs M. de Weerdt, Emir Demirovi\'c
Categories: cs.LG cs.AI cs.DS
MSC-class: 68T09, 68T20, 90C39
ACM-class: I.2.8; I.2.6
\\ ( https://arxiv.org/abs/2305.19706 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00196
replaced with revised version Tue, 16 Jan 2024 05:42:06 GMT   (2205kb,D)

Title: Restless Bandits with Average Reward: Breaking the Uniform Global
  Attractor Assumption
Authors: Yige Hong, Qiaomin Xie, Yudong Chen, Weina Wang
Categories: cs.LG math.OC math.PR stat.ML
Comments: NeurIPS 2023. 35 pages, 8 figures
MSC-class: 90C40
ACM-class: G.3; I.6
\\ ( https://arxiv.org/abs/2306.00196 ,  2205kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01222
replaced with revised version Sat, 13 Jan 2024 02:01:31 GMT   (960kb,D)

Title: Scaling Up Semi-supervised Learning with Unconstrained Unlabelled Data
Authors: Shuvendu Roy, Ali Etemad
Categories: cs.LG cs.CV
Comments: Accepted in AAAI Conference on Artificial Intelligence (AAAI-24)
\\ ( https://arxiv.org/abs/2306.01222 ,  960kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03072
replaced with revised version Mon, 15 Jan 2024 13:58:51 GMT   (15287kb,D)

Title: Explore to Generalize in Zero-Shot RL
Authors: Ev Zisselman, Itai Lavie, Daniel Soudry, Aviv Tamar
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.03072 ,  15287kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03266
replaced with revised version Sun, 14 Jan 2024 14:02:53 GMT   (131kb,D)

Title: Extending the Design Space of Graph Neural Networks by Rethinking
  Folklore Weisfeiler-Lehman
Authors: Jiarui Feng, Lecheng Kong, Hao Liu, Dacheng Tao, Fuhai Li, Muhan
  Zhang, Yixin Chen
Categories: cs.LG stat.ML
Comments: Accepted to NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.03266 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06446
replaced with revised version Sat, 13 Jan 2024 03:03:51 GMT   (7648kb,D)

Title: ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient
  Vision Transformer
Authors: Haoran You, Huihong Shi, Yipin Guo, Yingyan Lin
Categories: cs.LG cs.AI
Comments: Accepted by NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.06446 ,  7648kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09467
replaced with revised version Tue, 16 Jan 2024 11:10:28 GMT   (1967kb,D)

Title: AQuA: A Benchmarking Tool for Label Quality Assessment
Authors: Mononito Goswami, Vedant Sanil, Arjun Choudhry, Arvind Srinivasan,
  Chalisa Udompanyawit, Artur Dubrawski
Categories: cs.LG
Comments: Accepted at the 37th Conference on Neural Information Processing
  Systems (NeurIPS 2023) Track on Datasets and Benchmarks. Source code can be
  found at www.github.com/autonlab/aqua/
\\ ( https://arxiv.org/abs/2306.09467 ,  1967kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09526
replaced with revised version Mon, 15 Jan 2024 04:37:48 GMT   (1275kb,D)

Title: Residual Q-Learning: Offline and Online Policy Customization without
  Value
Authors: Chenran Li, Chen Tang, Haruki Nishimura, Jean Mercat, Masayoshi
  Tomizuka, Wei Zhan
Categories: cs.LG cs.AI
Comments: Accepted by 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023). The first two authors contributed equally
\\ ( https://arxiv.org/abs/2306.09526 ,  1275kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09666
replaced with revised version Mon, 15 Jan 2024 12:54:05 GMT   (689kb,D)

Title: A Smooth Binary Mechanism for Efficient Private Continual Observation
Authors: Joel Daniel Andersson, Rasmus Pagh
Categories: cs.LG cs.CR cs.DS
Comments: Appeared at NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.09666 ,  689kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10506
replaced with revised version Sun, 14 Jan 2024 14:05:58 GMT   (3262kb,D)

Title: Fast Conditional Mixing of MCMC Algorithms for Non-log-concave
  Distributions
Authors: Xiang Cheng, Bohan Wang, Jingzhao Zhang, Yusong Zhu
Categories: cs.LG math.PR
Comments: Camera ready version
\\ ( https://arxiv.org/abs/2306.10506 ,  3262kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11589
replaced with revised version Tue, 16 Jan 2024 03:46:39 GMT   (1991kb,D)

Title: Sampling from Gaussian Process Posteriors using Stochastic Gradient
  Descent
Authors: Jihao Andreas Lin and Javier Antor\'an and Shreyas Padhy and David
  Janz and Jos\'e Miguel Hern\'andez-Lobato and Alexander Terenin
Categories: cs.LG stat.ML
Journal-ref: Advances in Neural Information Processing Systems, 2023
\\ ( https://arxiv.org/abs/2306.11589 ,  1991kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12974
replaced with revised version Mon, 15 Jan 2024 03:11:24 GMT   (2338kb,D)

Title: Adaptive Bernstein Change Detector for High-Dimensional Data Streams
Authors: Marco Heyden, Edouard Fouch\'e, Vadim Arzamasov, Tanja Fenn, Florian
  Kalinke, Klemens B\"ohm
Categories: cs.LG
MSC-class: 68T05
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2306.12974 ,  2338kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17361
replaced with revised version Sat, 13 Jan 2024 01:44:30 GMT   (6624kb,D)

Title: iSCAN: Identifying Causal Mechanism Shifts among Nonlinear Additive
  Noise Models
Authors: Tianyu Chen, Kevin Bello, Bryon Aragam, Pradeep Ravikumar
Categories: cs.LG cs.AI stat.AP stat.ME stat.ML
Comments: 36 pages, 18 figures. Published at NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.17361 ,  6624kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00859
replaced with revised version Sat, 13 Jan 2024 12:03:25 GMT   (0kb,I)

Title: CardiGraphormer: Unveiling the Power of Self-Supervised Learning in
  Revolutionizing Drug Discovery
Authors: Abhijit Gupta
Categories: cs.LG q-bio.QM stat.AP stat.ML
Comments: Disagreement with my PhD supervisor about open sourcing the
  publication and affiliation
\\ ( https://arxiv.org/abs/2307.00859 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02251
replaced with revised version Tue, 16 Jan 2024 03:38:44 GMT   (2185kb,D)

Title: RanPAC: Random Projections and Pre-trained Models for Continual Learning
Authors: Mark D. McDonnell, Dong Gong, Amin Parveneh, Ehsan Abbasnejad, Anton
  van den Hengel
Categories: cs.LG cs.CV
Comments: 32 pages, 11 figures
Journal-ref: 37th Annual Conference on Neural Information Processing Systems
  (NeurIPS 2023), Dec 2023, New Orleans, United States
\\ ( https://arxiv.org/abs/2307.02251 ,  2185kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03393
replaced with revised version Tue, 16 Jan 2024 16:12:14 GMT   (769kb,D)

Title: Exploring the Potential of Large Language Models (LLMs) in Learning on
  Graphs
Authors: Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei,
  Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang
Categories: cs.LG cs.AI
Comments: To be appear on SIGKDD Explorations
\\ ( https://arxiv.org/abs/2307.03393 ,  769kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13304
replaced with revised version Mon, 15 Jan 2024 21:54:28 GMT   (108kb,D)

Title: QuIP: 2-Bit Quantization of Large Language Models With Guarantees
Authors: Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2307.13304 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13716
replaced with revised version Tue, 16 Jan 2024 12:03:03 GMT   (27349kb,D)

Title: FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on
  Staged Reinforcement Learning
Authors: Leiming Chen, Cihao Dong, Sibo Qiao, Ziling Huang, Kai Wang, Yuming
  Nie, Zhaoxiang Hou, Cheewei Tan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2307.13716 ,  27349kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16149
replaced with revised version Sat, 13 Jan 2024 08:00:03 GMT   (12220kb,D)

Title: A Novel DDPM-based Ensemble Approach for Energy Theft Detection in Smart
  Grids
Authors: Xun Yuan and Yang Yang and Asif Iqbal and Prosanta Gope and Biplab
  Sikdar
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2307.16149 ,  12220kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07272
replaced with revised version Tue, 16 Jan 2024 03:22:15 GMT   (1227kb,D)

Title: Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt
  Generation for Few-shot Learning
Authors: Chengzhengxu Li, Xiaoming Liu, Yichen Wang, Duyi Li, Yu Lan, Chao Shen
Categories: cs.LG cs.CL
Comments: AAAI 2024 Main Track
\\ ( https://arxiv.org/abs/2308.07272 ,  1227kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11272
replaced with revised version Sun, 14 Jan 2024 04:46:49 GMT   (21726kb,D)

Title: FoX: Formation-aware exploration in multi-agent reinforcement learning
Authors: Yonghyeon Jo, Sunwoo Lee, Junghyuk Yeom, Seungyul Han
Categories: cs.LG
Comments: 8 pages main, 5 pages appendix with reference. 10 figures, accepeted
  by AAAI 2024
MSC-class: Machine Learning (ML) - ML: Reinforcement Learning, Secondary
  Subject Areas: Multiagent Systems (MAS) - MAS: Multiagent Learning
\\ ( https://arxiv.org/abs/2308.11272 ,  21726kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16900
replaced with revised version Mon, 15 Jan 2024 14:07:55 GMT   (21753kb,D)

Title: Learning to Taste: A Multimodal Wine Dataset
Authors: Thoranna Bender, Simon Moe S{\o}rensen, Alireza Kashani, K. Eldjarn
  Hjorleifsson, Grethe Hyldig, S{\o}ren Hauberg, Serge Belongie and Frederik
  Warburg
Categories: cs.LG
Comments: Accepted to NeurIPS 2023. See project page:
  https://thoranna.github.io/learning_to_taste/
\\ ( https://arxiv.org/abs/2308.16900 ,  21753kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01816
replaced with revised version Mon, 15 Jan 2024 09:01:23 GMT   (1225kb,D)

Title: Adaptive Model Pruning and Personalization for Federated Learning over
  Wireless Networks
Authors: Xiaonan Liu and Tharmalingam Ratnarajah and Mathini Sellathurai and
  Yonina C. Eldar
Categories: cs.LG cs.NI
Comments: arXiv admin note: text overlap with arXiv:2305.09042
\\ ( https://arxiv.org/abs/2309.01816 ,  1225kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12252
replaced with revised version Tue, 16 Jan 2024 16:56:11 GMT   (573kb,D)

Title: Parallelizing non-linear sequential models over the sequence length
Authors: Yi Heng Lim, Qi Zhu, Joshua Selfridge, Muhammad Firmansyah Kasim
Categories: cs.LG cs.DC physics.comp-ph
\\ ( https://arxiv.org/abs/2309.12252 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12689
replaced with revised version Mon, 15 Jan 2024 09:42:52 GMT   (1750kb,D)

Title: AMPLIFY:Attention-based Mixup for Performance Improvement and Label
  Smoothing in Transformer
Authors: Leixin Yang, Yu Xiang
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2309.12689 ,  1750kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13915
replaced with revised version Sun, 14 Jan 2024 23:46:34 GMT   (413kb,D)

Title: Sample Complexity of Neural Policy Mirror Descent for Policy
  Optimization on Low-Dimensional Manifolds
Authors: Zhenghao Xu, Xiang Ji, Minshuo Chen, Mengdi Wang, Tuo Zhao
Categories: cs.LG stat.ML
DOI: 10.48550/arXiv.2309.13915
\\ ( https://arxiv.org/abs/2309.13915 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17011
replaced with revised version Mon, 15 Jan 2024 09:02:16 GMT   (974kb,D)

Title: Feature Interaction Aware Automated Data Representation Transformation
Authors: Ehtesamul Azim, Dongjie Wang, Kunpeng Liu, Wei Zhang, Yanjie Fu
Categories: cs.LG
Comments: Accepted to SIAM Conference on Data Mining(SDM) 2024
\\ ( https://arxiv.org/abs/2309.17011 ,  974kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06872
replaced with revised version Tue, 16 Jan 2024 00:20:23 GMT   (29739kb,D)

Title: On sparse regression, Lp-regularization, and automated model discovery
Authors: Jeremy A. McCulloch, Skyler R. St. Pierre, Kevin Linka, Ellen Kuhl
Categories: cs.LG
Comments: 35 pages, 15 figures, 2 tables, 62 references
MSC-class: 65, 74
ACM-class: I.6; J.2
\\ ( https://arxiv.org/abs/2310.06872 ,  29739kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07985
replaced with revised version Sat, 13 Jan 2024 01:32:56 GMT   (1458kb,D)

Title: Neural Combinatorial Optimization with Heavy Decoder: Toward Large Scale
  Generalization
Authors: Fu Luo, Xi Lin, Fei Liu, Qingfu Zhang, Zhenkun Wang
Categories: cs.LG cs.NE math.OC
Comments: Accepted at NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.07985 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08661
replaced with revised version Fri, 12 Jan 2024 20:26:31 GMT   (422kb,D)

Title: Counting and Algorithmic Generalization with Transformers
Authors: Simon Ouellette, Rolf Pfister, Hansueli Jud
Categories: cs.LG
Comments: Applied AAAI 2024 reviewer comments. We clarified notation in the
  main algorithm pseudo-code (alg. 1). Removed superfluous experiments on
  Universal Transformers which did not yield interesting results and added
  confusion to the main insights of the paper. The paper is now more concise
  and straight to the point. Clarified our main contributions
\\ ( https://arxiv.org/abs/2310.08661 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11952
replaced with revised version Sun, 14 Jan 2024 13:22:30 GMT   (836kb,D)

Title: Recasting Continual Learning as Sequence Modeling
Authors: Soochan Lee, Jaehyeon Son, Gunhee Kim
Categories: cs.LG
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.11952 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12248
replaced with revised version Mon, 15 Jan 2024 20:36:44 GMT   (65kb,D)

Title: A PAC Learning Algorithm for LTL and Omega-regular Objectives in MDPs
Authors: Mateo Perez, Fabio Somenzi, Ashutosh Trivedi
Categories: cs.LG cs.LO
\\ ( https://arxiv.org/abs/2310.12248 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14053
replaced with revised version Tue, 16 Jan 2024 14:03:10 GMT   (3388kb,D)

Title: Beyond Accuracy: Evaluating Self-Consistency of Code Large Language
  Models with IdentityChain
Authors: Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail
  Kaiser, Suman Jana, Baishakhi Ray
Categories: cs.LG cs.CL cs.SE
Comments: ICLR 2024
MSC-class: 68
ACM-class: I.2; D.2
\\ ( https://arxiv.org/abs/2310.14053 ,  3388kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14753
replaced with revised version Mon, 15 Jan 2024 02:55:06 GMT   (2878kb,D)

Title: Rethinking Tokenizer and Decoder in Masked Graph Modeling for Molecules
Authors: Zhiyuan Liu, Yaorui Shi, An Zhang, Enzhi Zhang, Kenji Kawaguchi, Xiang
  Wang, Tat-Seng Chua
Categories: cs.LG
Comments: NeurIPS 2023. 10 pages
\\ ( https://arxiv.org/abs/2310.14753 ,  2878kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16221
replaced with revised version Mon, 15 Jan 2024 10:34:18 GMT   (1348kb,D)

Title: Hierarchical Randomized Smoothing
Authors: Yan Scholten, Jan Schuchardt, Aleksandar Bojchevski, Stephan
  G\"unnemann
Categories: cs.LG cs.AI cs.CV stat.ML
\\ ( https://arxiv.org/abs/2310.16221 ,  1348kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16652
replaced with revised version Fri, 12 Jan 2024 19:30:15 GMT   (561kb,D)

Title: How Robust is Federated Learning to Communication Error? A Comparison
  Study Between Uplink and Downlink Channels
Authors: Linping Qu, Shenghui Song, Chi-Ying Tsui, and Yuyi Mao
Categories: cs.LG
Comments: Accepted by IEEE WCNC 2024
\\ ( https://arxiv.org/abs/2310.16652 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19273
replaced with revised version Tue, 16 Jan 2024 12:38:15 GMT   (3014kb,D)

Title: The Memory Perturbation Equation: Understanding Model's Sensitivity to
  Data
Authors: Peter Nickl, Lu Xu, Dharmesh Tailor, Thomas M\"ollenhoff, Mohammad
  Emtiyaz Khan
Categories: cs.LG cs.AI stat.ML
Comments: 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
\\ ( https://arxiv.org/abs/2310.19273 ,  3014kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01644
replaced with revised version Tue, 16 Jan 2024 00:21:43 GMT   (5460kb,D)

Title: Should Under-parameterized Student Networks Copy or Average Teacher
  Weights?
Authors: Berfin \c{S}im\c{s}ek, Amire Bendjeddou, Wulfram Gerstner, Johanni
  Brea
Categories: cs.LG cs.NE stat.ML
Comments: 41 pages, presented at NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.01644 ,  5460kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02332
replaced with revised version Fri, 12 Jan 2024 22:44:04 GMT   (2526kb,D)

Title: Multimodal Machine Learning in Image-Based and Clinical Biomedicine:
  Survey and Prospects
Authors: Elisa Warner, Joonsang Lee, William Hsu, Tanveer Syeda-Mahmood,
  Charles Kahn, Olivier Gevaert and Arvind Rao
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2311.02332 ,  2526kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03351
replaced with revised version Sat, 13 Jan 2024 13:10:51 GMT   (13885kb,D)

Title: Uni-O4: Unifying Online and Offline Deep Reinforcement Learning with
  Multi-Step On-Policy Optimization
Authors: Kun Lei, Zhengmao He, Chenhao Lu, Kaizhe Hu, Yang Gao, Huazhe Xu
Categories: cs.LG cs.RO
Comments: Our website: https://lei-kun.github.io/uni-o4/
\\ ( https://arxiv.org/abs/2311.03351 ,  13885kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09620
replaced with revised version Tue, 16 Jan 2024 12:26:08 GMT   (2539kb,D)

Title: GAIA: Delving into Gradient-based Attribution Abnormality for
  Out-of-distribution Detection
Authors: Jinggang Chen, Junjie Li, Xiaoyang Qu, Jianzong Wang, Jiguang Wan,
  Jing Xiao
Categories: cs.LG
Comments: Accepted by NeurIPS2023
\\ ( https://arxiv.org/abs/2311.09620 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11798
replaced with revised version Fri, 12 Jan 2024 23:49:45 GMT   (8920kb,D)

Title: Operator Learning for Continuous Spatial-Temporal Model with
  Gradient-Based and Derivative-Free Optimization Methods
Authors: Chuanqi Chen, Jin-Long Wu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2311.11798 ,  8920kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13326
replaced with revised version Sat, 13 Jan 2024 03:53:24 GMT   (8646kb,D)

Title: Curriculum Learning and Imitation Learning for Model-free Control on
  Financial Time-series
Authors: Woosung Koh, Insu Choi, Yuntae Jang, Gimin Kang, Woo Chang Kim
Categories: cs.LG cs.AI q-fin.PM
Comments: AAAI 2024 AI4TS Workshop Oral
\\ ( https://arxiv.org/abs/2311.13326 ,  8646kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16522
replaced with revised version Mon, 15 Jan 2024 04:50:47 GMT   (320kb)

Title: Dynamic Fault Characteristics Evaluation in Power Grid
Authors: Hao Pei, Si Lin, Chuanfu Li, Che Wang, Haoming Chen, Sizhe Li
Categories: cs.LG cs.CL eess.SP
\\ ( https://arxiv.org/abs/2311.16522 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02102
replaced with revised version Sun, 14 Jan 2024 21:05:13 GMT   (1181kb,D)

Title: Mitigating Data Injection Attacks on Federated Learning
Authors: Or Shalom, Amir Leshem, Waheed U. Bajwa
Categories: cs.LG cs.CR eess.SP
Comments: This work will be presented at IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP) 2024
\\ ( https://arxiv.org/abs/2312.02102 ,  1181kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02708
replaced with revised version Mon, 15 Jan 2024 10:07:36 GMT   (13963kb,D)

Title: Provable Adversarial Robustness for Group Equivariant Tasks: Graphs,
  Point Clouds, Molecules, and More
Authors: Jan Schuchardt, Yan Scholten, Stephan G\"unnemann
Categories: cs.LG cs.CR stat.ML
Comments: Accepted at NeurIPS 2023
\\ ( https://arxiv.org/abs/2312.02708 ,  13963kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04234
replaced with revised version Sat, 13 Jan 2024 06:24:39 GMT   (2816kb,D)

Title: Graph Convolutions Enrich the Self-Attention in Transformers!
Authors: Jeongwhan Choi, Hyowon Wi, Jayoung Kim, Yehjin Shin, Kookjin Lee,
  Nathaniel Trask, Noseong Park
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.04234 ,  2816kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05910
replaced with revised version Sun, 14 Jan 2024 08:37:23 GMT   (1305kb,D)

Title: Ensemble Kalman Filtering Meets Gaussian Process SSM for Non-Mean-Field
  and Online Inference
Authors: Zhidi Lin and Yiyong Sun and Feng Yin and Alexandre Hoang Thi\'ery
Categories: cs.LG eess.SP stat.ML
Comments: Gaussian process, state-space model, ensemble Kalman filter, online
  learning, variational inference. (19 pages, 10 figures)
\\ ( https://arxiv.org/abs/2312.05910 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14698
replaced with revised version Mon, 15 Jan 2024 21:12:03 GMT   (2735kb,D)

Title: Time-changed normalizing flows for accurate SDE modeling
Authors: Naoufal El Bekri and Lucas Drumetz and Franck Vermet
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2312.14698 ,  2735kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15551
replaced with revised version Tue, 16 Jan 2024 18:57:58 GMT   (212kb,D)

Title: Leveraging Public Representations for Private Transfer Learning
Authors: Pratiksha Thaker, Amrith Setlur, Zhiwei Steven Wu, Virginia Smith
Categories: cs.LG cs.CR stat.ML
\\ ( https://arxiv.org/abs/2312.15551 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15965
replaced with revised version Mon, 15 Jan 2024 05:42:09 GMT   (8406kb,D)

Title: Efficient Reinforcemen Learning with Decoupling Exploration and
  Utilization
Authors: Jingpu Yang, Qirui Zhao, Helin Wang, Yuxiao Huang, Zirui Song, Miao
  Fang
Categories: cs.LG
Comments: Update some figures
\\ ( https://arxiv.org/abs/2312.15965 ,  8406kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16895
replaced with revised version Tue, 16 Jan 2024 11:33:28 GMT   (879kb,D)

Title: RLPlanner: Reinforcement Learning based Floorplanning for Chiplets with
  Fast Thermal Analysis
Authors: Yuanyuan Duan, Xingchen Liu, Zhiping Yu, Hanming Wu, Leilai Shao and
  Xiaolei Zhu
Categories: cs.LG cs.AR
\\ ( https://arxiv.org/abs/2312.16895 ,  879kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00773
replaced with revised version Sat, 13 Jan 2024 05:39:36 GMT   (3767kb,D)

Title: Unsupervised Outlier Detection using Random Subspace and Subsampling
  Ensembles of Dirichlet Process Mixtures
Authors: Dongwook Kim, Juyeon Park, Hee Cheol Chung, Seonghyun Jeong
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2401.00773 ,  3767kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00828
replaced with revised version Sat, 13 Jan 2024 15:46:09 GMT   (261kb,D)

Title: Multi-Lattice Sampling of Quantum Field Theories via Neural
  Operator-based Flows
Authors: B\'alint M\'at\'e, Fran\c{c}ois Fleuret
Categories: cs.LG hep-lat stat.ML
\\ ( https://arxiv.org/abs/2401.00828 ,  261kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02333
replaced with revised version Tue, 16 Jan 2024 17:18:35 GMT   (872kb,D)

Title: Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models
Authors: Uday Allu, Biddwan Ahmed, Vishesh Tripathi
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2401.02333 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02810
replaced with revised version Mon, 15 Jan 2024 13:10:12 GMT   (16042kb,D)

Title: Physics-Informed Neural Networks for High-Frequency and Multi-Scale
  Problems using Transfer Learning
Authors: Abdul Hannan Mustajab, Hao Lyu, Zarghaam Rizvi, Frank Wuttke
Categories: cs.LG cs.AI cs.NA math.NA
Comments: 18 pages
\\ ( https://arxiv.org/abs/2401.02810 ,  16042kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05831
replaced with revised version Mon, 15 Jan 2024 10:16:12 GMT   (726kb,D)

Title: Silhouette Aggregation: From Micro to Macro
Authors: Georgios Vardakas and John Pavlopoulos and Aristidis Likas
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.05831 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06604
replaced with revised version Mon, 15 Jan 2024 14:39:10 GMT   (2030kb,D)

Title: Identifying Policy Gradient Subspaces
Authors: Jan Schneider, Pierre Schumacher, Simon Guist, Le Chen, Daniel
  H\"aufle, Bernhard Sch\"olkopf, Dieter B\"uchler
Categories: cs.LG
Comments: 21 pages, 11 figures
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2401.06604 ,  2030kb)
------------------------------------------------------------------------------
\\
arXiv:1905.09610
replaced with revised version Mon, 15 Jan 2024 17:16:12 GMT   (44kb)

Title: Hypothetical answers to continuous queries over data streams
Authors: Lu\'is Cruz-Filipe, Gra\c{c}a Gaspar, Isabel Nunes
Categories: cs.PL cs.AI
\\ ( https://arxiv.org/abs/1905.09610 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2202.12883
replaced with revised version Mon, 15 Jan 2024 22:14:36 GMT   (4871kb,D)

Title: Human Detection of Political Speech Deepfakes across Transcripts, Audio,
  and Video
Authors: Matthew Groh, Aruna Sankaranarayanan, Nikhil Singh, Dong Young Kim,
  Andrew Lippman, Rosalind Picard
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2202.12883 ,  4871kb)
------------------------------------------------------------------------------
\\
arXiv:2204.07703
replaced with revised version Sun, 14 Jan 2024 09:54:30 GMT   (1240kb,D)

Title: TeleGraph: A Benchmark Dataset for Hierarchical Link Prediction
Authors: Min Zhou, Bisheng Li, Menglin Yang, Lujia Pan
Categories: cs.SI cs.AI
Comments: Accepted by GLB 2022 @TheWebConf 2022;Data and codes are available at
  https://github.com/huawei-noah/benchmark/tree/main/TeleGraph
\\ ( https://arxiv.org/abs/2204.07703 ,  1240kb)
------------------------------------------------------------------------------
\\
arXiv:2207.11230
replaced with revised version Tue, 16 Jan 2024 08:01:23 GMT   (5863kb,D)

Title: You Actually Look Twice At it (YALTAi): using an object detection
  approach instead of region segmentation within the Kraken engine
Authors: Thibault Cl\'erice (ENC, CJM, HiSoMA, UJML, ALMAnaCH)
Categories: cs.CV cs.AI
Comments: Journal of Data Mining and Digital Humanities, 2023, Historical
  Documents and automatic text recognition
DOI: 10.46298/jdmdh.9806
\\ ( https://arxiv.org/abs/2207.11230 ,  5863kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07003
replaced with revised version Fri, 12 Jan 2024 23:30:59 GMT   (21795kb,D)

Title: Vision-aided UAV navigation and dynamic obstacle avoidance using
  gradient-based B-spline trajectory optimization
Authors: Zhefan Xu, Yumeng Xiu, Xiaoyang Zhan, Baihan Chen, Kenji Shimada
Categories: cs.RO cs.AI
Journal-ref: 2023 IEEE International Conference on Robotics and Automation
  (ICRA)
DOI: 10.1109/ICRA48891.2023.10160638
\\ ( https://arxiv.org/abs/2209.07003 ,  21795kb)
------------------------------------------------------------------------------
\\
arXiv:2209.08258
replaced with revised version Fri, 12 Jan 2024 23:37:35 GMT   (28121kb,D)

Title: A real-time dynamic obstacle tracking and mapping system for UAV
  navigation and collision avoidance with an RGB-D camera
Authors: Zhefan Xu, Xiaoyang Zhan, Baihan Chen, Yumeng Xiu, Chenhao Yang, and
  Kenji Shimada
Categories: cs.RO cs.AI
Journal-ref: 2023 IEEE International Conference on Robotics and Automation
  (ICRA)
DOI: 10.1109/ICRA48891.2023.10161194
\\ ( https://arxiv.org/abs/2209.08258 ,  28121kb)
------------------------------------------------------------------------------
\\
arXiv:2210.10537
replaced with revised version Mon, 15 Jan 2024 01:44:53 GMT   (0kb,I)

Title: Online LiDAR-Camera Extrinsic Parameters Self-checking
Authors: Pengjin Wei, Guohang Yan, Yikang Li, Kun Fang, Jie Yang, Wei Liu
Categories: cs.CV cs.AI
Comments: There are some errors in the methodology section of the paper, which
  is currently being revised
\\ ( https://arxiv.org/abs/2210.10537 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2301.08422
replaced with revised version Fri, 12 Jan 2024 23:53:40 GMT   (8768kb,D)

Title: A vision-based autonomous UAV inspection framework for unknown tunnel
  construction sites with dynamic obstacles
Authors: Zhefan Xu, Baihan Chen, Xiaoyang Zhan, Yumeng Xiu, Christopher Suzuki,
  Kenji Shimada
Categories: cs.RO cs.AI
Comments: 8 pages, 8 figures
Journal-ref: IEEE Robotics and Automation Letters, Volume: 8, Issue: 8, June
  2023. Page(s): 4983 - 4990
DOI: 10.1109/LRA.2023.3290415
\\ ( https://arxiv.org/abs/2301.08422 ,  8768kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02209 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 13:28:46 GMT   (4043kb,D)

Title: Large-scale Online Ridesharing: The Effect of Assignment Optimality on
  System Performance
Authors: David Fiedler, Michal \v{C}ertick\'y, Javier Alonso-Mora, Michal
  P\v{e}chou\v{c}ek and Michal \v{C}\'ap
Categories: math.OC cs.AI
Comments: Accepted Manuscript version. Currently published online in the
  Journal of Intelligent Transportation Systems:
  https://www.tandfonline.com/doi/abs/10.1080/15472450.2022.2121651
Journal-ref: (2022) Large-scale online ridesharing: the effect of assignment
  optimality on system performance, Journal of Intelligent Transportation
  Systems
DOI: 10.1080/15472450.2022.2121651
\\ ( https://arxiv.org/abs/2305.02209 ,  4043kb)
------------------------------------------------------------------------------
\\
arXiv:2305.06435 (*cross-listing*)
replaced with revised version Sat, 13 Jan 2024 07:31:26 GMT   (812kb,D)

Title: Phase transitions in the mini-batch size for sparse and dense two-layer
  neural networks
Authors: Raffaele Marino and Federico Ricci-Tersenghi
Categories: cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG
Comments: 15 pages, 8 figures
Journal-ref: Machine Learning: Science and Technology (2024)
DOI: 10.1088/2632-2153/ad1de6
\\ ( https://arxiv.org/abs/2305.06435 ,  812kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07522
replaced with revised version Sat, 13 Jan 2024 09:13:25 GMT   (17073kb,D)

Title: SPADE: Sparse Pillar-based 3D Object Detection Accelerator for
  Autonomous Driving
Authors: Minjae Lee, Seongmin Park, Hyungmin Kim, Minyong Yoon, Janghwan Lee,
  Jun Won Choi, Nam Sung Kim, Mingu Kang, Jungwook Choi
Categories: cs.AR cs.AI
Comments: 14 pages, 15 figures
\\ ( https://arxiv.org/abs/2305.07522 ,  17073kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16494
replaced with revised version Thu, 11 Jan 2024 21:19:42 GMT   (21191kb,D)

Title: Diffusion-Based Adversarial Sample Generation for Improved Stealthiness
  and Controllability
Authors: Haotian Xue, Alexandre Araujo, Bin Hu, Yongxin Chen
Categories: cs.CV cs.AI
Comments: Accepted as a conference paper in NeurIPS'2023. Code repo:
  https://github.com/xavihart/Diff-PGD
\\ ( https://arxiv.org/abs/2305.16494 ,  21191kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17402
replaced with revised version Fri, 12 Jan 2024 23:04:46 GMT   (269kb,D)

Title: Learning and Collusion in Multi-unit Auctions
Authors: Simina Br\^anzei and Mahsa Derakhshan and Negin Golrezaei and Yanjun
  Han
Categories: cs.GT cs.AI cs.LG
Comments: 40 pages, 1 figure. In Proceedings of NeurIPS 2023
\\ ( https://arxiv.org/abs/2305.17402 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19556
replaced with revised version Tue, 16 Jan 2024 03:26:22 GMT   (2039kb,D)

Title: Exploring Phonetic Context-Aware Lip-Sync For Talking Face Generation
Authors: Se Jin Park, Minsu Kim, Jeongsoo Choi, Yong Man Ro
Categories: cs.CV cs.AI cs.SD eess.AS eess.IV
Comments: Accepted at AAAI 2024
\\ ( https://arxiv.org/abs/2305.19556 ,  2039kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03988
replaced with revised version Sun, 14 Jan 2024 00:29:39 GMT   (42168kb,D)

Title: Learn the Force We Can: Enabling Sparse Motion Control in Multi-Object
  Video Generation
Authors: Aram Davtyan and Paolo Favaro
Categories: cs.CV cs.AI
Comments: Accepted to AAAI 2024. Project website:
  https://araachie.github.io/yoda
\\ ( https://arxiv.org/abs/2306.03988 ,  42168kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14448
replaced with revised version Mon, 15 Jan 2024 07:45:02 GMT   (27907kb,D)

Title: Progressive Energy-Based Cooperative Learning for Multi-Domain
  Image-to-Image Translation
Authors: Weinan Song, Yaxuan Zhu, Lei He, Yingnian Wu, and Jianwen Xie
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2306.14448 ,  27907kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14685
replaced with revised version Mon, 15 Jan 2024 15:20:29 GMT   (26365kb,D)

Title: DiffSketcher: Text Guided Vector Sketch Synthesis through Latent
  Diffusion Models
Authors: Ximing Xing, Chuang Wang, Haitao Zhou, Jing Zhang, Qian Yu, Dong Xu
Categories: cs.CV cs.AI
Comments: Accepted by NIPS 2023. Project page:
  https://ximinng.github.io/DiffSketcher-project/
\\ ( https://arxiv.org/abs/2306.14685 ,  26365kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16834 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 11:02:09 GMT   (3686kb)

Title: Intelligence of Astronomical Optical Telescope: Present Status and
  Future Perspectives
Authors: Kang Huang, Tianzhu Hu, Jingyi Cai, Xiushan Pang, Yonghui Hou, Yong
  Zhang, Huaiqing Wang, Xiangqun Cui
Categories: astro-ph.IM cs.AI
Comments: 41 pages, 10 figure, for questions or comments, please email
  tzhu@niaot.ac.cn
ACM-class: J.7
\\ ( https://arxiv.org/abs/2306.16834 ,  3686kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08927
replaced with revised version Sat, 13 Jan 2024 07:39:35 GMT   (21550kb,D)

Title: Multi-Stage Cable Routing through Hierarchical Imitation Learning
Authors: Jianlan Luo, Charles Xu, Xinyang Geng, Gilbert Feng, Kuan Fang, Liam
  Tan, Stefan Schaal, Sergey Levine
Categories: cs.RO cs.AI
Comments: T-RO 2024
\\ ( https://arxiv.org/abs/2307.08927 ,  21550kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12542
replaced with revised version Mon, 15 Jan 2024 16:18:13 GMT   (397kb,D)

Title: Client-Level Differential Privacy via Adaptive Intermediary in Federated
  Medical Imaging
Authors: Meirui Jiang, Yuan Zhong, Anjie Le, Xiaoxiao Li, Qi Dou
Categories: cs.CV cs.AI
Comments: Accepted by 26th International Conference on Medical Image Computing
  and Computer Assisted Intervention (MICCAI'23)
\\ ( https://arxiv.org/abs/2307.12542 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15220
replaced with revised version Sat, 13 Jan 2024 13:56:32 GMT   (47949kb,D)

Title: Learning Multi-modal Representations by Watching Hundreds of Surgical
  Video Lectures
Authors: Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Pietro
  Mascagni, Nassir Navab, Nicolas Padoy
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2307.15220 ,  47949kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05027 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 07:38:39 GMT   (1683kb,D)

Title: VoiceFlow: Efficient Text-to-Speech with Rectified Flow Matching
Authors: Yiwei Guo, Chenpeng Du, Ziyang Ma, Xie Chen, Kai Yu
Categories: eess.AS cs.AI cs.HC cs.SD
Comments: 4 figure, 5 pages, accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2309.05027 ,  1683kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09677 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 14:17:47 GMT   (77kb,D)

Title: Single and Few-step Diffusion for Generative Speech Enhancement
Authors: Bunlong Lay, Jean-Marie Lemercier, Julius Richter, Timo Gerkmann
Categories: eess.AS cs.AI cs.LG cs.SD
Comments: copyright 2023 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
\\ ( https://arxiv.org/abs/2309.09677 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12678 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 20:10:40 GMT   (1760kb,D)

Title: QAL-BP: An Augmented Lagrangian Quantum Approach for Bin Packing
Authors: Lorenzo Cellini, Antonio Macaluso, Michele Lombardi
Categories: quant-ph cs.AI math.OC
Comments: 18 pages, 8 figures, 1 table
\\ ( https://arxiv.org/abs/2309.12678 ,  1760kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07932
replaced with revised version Tue, 16 Jan 2024 04:39:59 GMT   (7300kb,D)

Title: What Matters to You? Towards Visual Representation Alignment for Robot
  Learning
Authors: Ran Tian, Chenfeng Xu, Masayoshi Tomizuka, Jitendra Malik, Andrea
  Bajcsy
Categories: cs.RO cs.AI cs.CV
\\ ( https://arxiv.org/abs/2310.07932 ,  7300kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20501
replaced with revised version Sun, 14 Jan 2024 14:41:06 GMT   (1345kb,D)

Title: LLMs may Dominate Information Access: Neural Retrievers are Biased
  Towards LLM-Generated Texts
Authors: Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu,
  Xiao Zhang, Gang Wang and Jun Xu
Categories: cs.IR cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.20501 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01017
replaced with revised version Tue, 16 Jan 2024 18:02:27 GMT   (39838kb,D)

Title: Learning Unsupervised World Models for Autonomous Driving via Discrete
  Diffusion
Authors: Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, Raquel
  Urtasun
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2311.01017 ,  39838kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02794 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 01:18:50 GMT   (15792kb,D)

Title: Modelling Cellular Perturbations with the Sparse Additive Mechanism
  Shift Variational Autoencoder
Authors: Michael Bereket, Theofanis Karaletsos
Categories: stat.ML cs.AI cs.LG q-bio.QM
Comments: Presented at the 37th Conference on Neural Information Processing
  Systems (NeurIPS 2023) (Post-NeurIPS fixes: cosmetic fixes, updated
  references, added simulation to appendix)
\\ ( https://arxiv.org/abs/2311.02794 ,  15792kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05546 (*cross-listing*)
replaced with revised version Sat, 13 Jan 2024 10:59:54 GMT   (1707kb,D)

Title: Multi-Agent Quantum Reinforcement Learning using Evolutionary
  Optimization
Authors: Michael K\"olle, Felix Topp, Thomy Phan, Philipp Altmann, Jonas
  N\"u{\ss}lein, Claudia Linnhoff-Popien
Categories: quant-ph cs.AI cs.MA
\\ ( https://arxiv.org/abs/2311.05546 ,  1707kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07377
replaced with revised version Tue, 16 Jan 2024 00:50:05 GMT   (2012kb,D)

Title: Testing learning-enabled cyber-physical systems with Large-Language
  Models: A Formal Approach
Authors: Xi Zheng, Aloysius K. Mok, Ruzica Piskac, Yong Jae Lee, Bhaskar
  Krishnamachari, Dakai Zhu, Oleg Sokolsky, Insup Lee
Categories: cs.SE cs.AI cs.DC cs.RO
\\ ( https://arxiv.org/abs/2311.07377 ,  2012kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11101
replaced with revised version Sun, 14 Jan 2024 13:51:06 GMT   (40kb)

Title: $\varepsilon$-fractional Core Stability in Hedonic Games
Authors: Simone Fioravanti, Michele Flammini, Bojana Kodric and Giovanna
  Varricchio
Categories: cs.GT cs.AI
Comments: Accepted as poster at NeurIPS 2023
\\ ( https://arxiv.org/abs/2311.11101 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14084
replaced with revised version Mon, 15 Jan 2024 02:31:04 GMT   (1441kb,D)

Title: AI-Generated Images Introduce Invisible Relevance Bias to Text-Image
  Retrieval
Authors: Shicheng Xu, Danyang Hou, Liang Pang, Jingcheng Deng, Jun Xu, Huawei
  Shen, Xueqi Cheng
Categories: cs.IR cs.AI cs.CV
Comments: 11 pages
\\ ( https://arxiv.org/abs/2311.14084 ,  1441kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14656
replaced with revised version Tue, 16 Jan 2024 18:20:08 GMT   (9472kb,D)

Title: Charting New Territories: Exploring the Geographic and Geospatial
  Capabilities of Multimodal LLMs
Authors: Jonathan Roberts, Timo L\"uddecke, Rehan Sheikh, Kai Han, Samuel
  Albanie
Categories: cs.CV cs.AI
Comments: V3: Fixed typo in Fig.1; V2: Minor formatting changes and added
  missing subfigure captions
\\ ( https://arxiv.org/abs/2311.14656 ,  9472kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16167
replaced with revised version Mon, 15 Jan 2024 07:33:31 GMT   (7413kb,D)

Title: Moving Sampling Physics-informed Neural Networks induced by Moving Mesh
  PDE
Authors: Yu Yang, Qihong Yang, Yangtao Deng, Qiaolin He
Categories: math.NA cs.AI cs.LG cs.NA
\\ ( https://arxiv.org/abs/2311.16167 ,  7413kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16716
replaced with revised version Sun, 14 Jan 2024 09:17:00 GMT   (1364kb,D)

Title: GraphPro: Graph Pre-training and Prompt Learning for Recommendation
Authors: Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2311.16716 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18424
replaced with revised version Tue, 16 Jan 2024 13:12:17 GMT   (584kb)

Title: Investigating Collaborative Data Practices: a Case Study on Artificial
  Intelligence for Healthcare Research
Authors: Rafael Henkin, Elizabeth Remfry, Duncan J. Reynolds, Megan Clinch,
  Michael R. Barnes
Categories: cs.HC cs.AI cs.CY
Comments: 17 pages
\\ ( https://arxiv.org/abs/2311.18424 ,  584kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03365
replaced with revised version Tue, 16 Jan 2024 11:06:30 GMT   (500kb,D)

Title: Demand response for residential building heating: Effective Monte Carlo
  Tree Search control based on physics-informed neural networks
Authors: Fabio Pavirani, Gargya Gokhale, Bert Claessens, Chris Develder
Categories: eess.SY cs.AI cs.SY
\\ ( https://arxiv.org/abs/2312.03365 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06008
replaced with revised version Tue, 16 Jan 2024 13:09:47 GMT   (225kb)

Title: Guardians of Trust: Navigating Data Security in AIOps through Vendor
  Partnerships
Authors: Subhadip Kumar
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2312.06008 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07586
replaced with revised version Tue, 16 Jan 2024 04:36:23 GMT   (28249kb,D)

Title: Characteristic Guidance: Non-linear Correction for Diffusion Model at
  Large Guidance Scale
Authors: Candi Zheng, Yuan Lan
Categories: cs.CV cs.AI cs.LG physics.data-an
Comments: 8 pages, 8 figures
\\ ( https://arxiv.org/abs/2312.07586 ,  28249kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07899 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 21:22:46 GMT   (2160kb)

Title: Morphological Profiling for Drug Discovery in the Era of Deep Learning
Authors: Qiaosi Tang, Ranjala Ratnayake, Gustavo Seabra, Zhe Jiang, Ruogu Fang,
  Lina Cui, Yousong Ding, Tamer Kahveci, Jiang Bian, Chenglong Li, Hendrik
  Luesch, Yanjun Li
Categories: q-bio.QM cs.AI cs.CV cs.LG
Comments: 44 pages, 5 figure, 5 tables
\\ ( https://arxiv.org/abs/2312.07899 ,  2160kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10201
replaced with revised version Sat, 13 Jan 2024 06:05:33 GMT   (1240kb,D)

Title: CARAT: Contrastive Feature Reconstruction and Aggregation for
  Multi-Modal Multi-Label Emotion Recognition
Authors: Cheng Peng, Ke Chen, Lidan Shou, Gang Chen
Categories: cs.MM cs.AI
\\ ( https://arxiv.org/abs/2312.10201 ,  1240kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10251
replaced with revised version Sat, 13 Jan 2024 13:52:52 GMT   (21207kb,D)

Title: Advancing Surgical VQA with Scene Graph Knowledge
Authors: Kun Yuan, Manasi Kattel, Joel L. Lavanchy, Nassir Navab, Vinkle
  Srivastav, Nicolas Padoy
Categories: cs.CV cs.AI
Comments: Accept by IPCAI 2024
\\ ( https://arxiv.org/abs/2312.10251 ,  21207kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11658
replaced with revised version Mon, 15 Jan 2024 21:24:41 GMT   (275kb,D)

Title: Traces of Memorisation in Large Language Models for Code
Authors: Ali Al-Kaswan and Maliheh Izadi and Arie van Deursen
Categories: cs.CR cs.AI cs.SE
Comments: ICSE 2024 Research Track
DOI: 10.1145/3597503.3639133
\\ ( https://arxiv.org/abs/2312.11658 ,  275kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13845
replaced with revised version Mon, 15 Jan 2024 01:39:11 GMT   (118kb,D)

Title: Image Clustering using Restricted Boltzman Machine
Authors: Abraham Woubie, Enoch Solomon and Eyael Solomon Emiru
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.13845 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14856
replaced with revised version Sun, 14 Jan 2024 18:58:36 GMT   (102kb,D)

Title: Turbulence: Systematically and Automatically Testing Instruction-Tuned
  Large Language Models for Code
Authors: Shahin Honarvar, Mark van der Wilk, Alastair Donaldson
Categories: cs.SE cs.AI
Comments: Modified a typo in the conclusion section regarding the impact of
  temperature reduction on the diversity of errors
\\ ( https://arxiv.org/abs/2312.14856 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14972
replaced with revised version Mon, 15 Jan 2024 15:44:10 GMT   (775kb,D)

Title: A Trade-off Analysis of Replacing Proprietary LLMs with Open Source SLMs
  in Production
Authors: Chandra Irugalbandara, Ashish Mahendra, Roland Daynauth, Tharuka
  Kasthuri Arachchige, Krisztian Flautner, Lingjia Tang, Yiping Kang, Jason
  Mars
Categories: cs.SE cs.AI cs.LG
Comments: Updated title
\\ ( https://arxiv.org/abs/2312.14972 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15667
replaced with revised version Mon, 15 Jan 2024 09:06:53 GMT   (15228kb,D)

Title: TAPE: Leveraging Agent Topology for Cooperative Multi-Agent Policy
  Gradient
Authors: Xingzhou Lou, Junge Zhang, Timothy J. Norman, Kaiqi Huang, Yali Du
Categories: cs.MA cs.AI
\\ ( https://arxiv.org/abs/2312.15667 ,  15228kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02727
replaced with revised version Sat, 13 Jan 2024 09:29:13 GMT   (3704kb)

Title: Enhancing targeted transferability via feature space fine-tuning
Authors: Hui Zeng, Biwei Chen, and Anjie Peng
Categories: cs.CV cs.AI
Comments: 9 pages, 10 figures, accepted by 2024ICASSP
\\ ( https://arxiv.org/abs/2401.02727 ,  3704kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02941
replaced with revised version Sun, 14 Jan 2024 01:12:16 GMT   (5116kb,D)

Title: Unsupervised Federated Domain Adaptation for Segmentation of MRI Images
Authors: Navapat Nananukul, Hamid Soltanian-zadeh, Mohammad Rostami
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.02941 ,  5116kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04124
replaced with revised version Tue, 16 Jan 2024 06:11:00 GMT   (5952kb,D)

Title: MobileAgent: enhancing mobile control via human-machine interaction and
  SOP integration
Authors: Tinghe Ding
Categories: cs.HC cs.AI
Comments: agent, mobile control, SOP, human-machine interaction
\\ ( https://arxiv.org/abs/2401.04124 ,  5952kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04579 (*cross-listing*)
replaced with revised version Sat, 13 Jan 2024 14:48:18 GMT   (819kb)

Title: A Deep Network for Explainable Prediction of Non-Imaging Phenotypes
  using Anatomical Multi-View Data
Authors: Yuxiang Wei, Yuqian Chen, Tengfei Xue, Leo Zekelman, Nikos Makris,
  Yogesh Rathi, Weidong Cai, Fan Zhang, Lauren J. O' Donnell
Categories: q-bio.QM cs.AI eess.IV
Comments: 2023 The Medical Image Computing and Computer Assisted Intervention
  Society workshop
\\ ( https://arxiv.org/abs/2401.04579 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04846 (*cross-listing*)
replaced with revised version Sun, 14 Jan 2024 02:12:28 GMT   (18165kb,D)

Title: The inherent goodness of well educated intelligence
Authors: Michael E. Glinsky and Sharon Sievert
Categories: econ.TH cs.AI
Comments: 10 pages, 8 figures, 16 equations, to be submitted to Nature
\\ ( https://arxiv.org/abs/2401.04846 ,  18165kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06506
replaced with revised version Tue, 16 Jan 2024 05:44:45 GMT   (872kb,D)

Title: Frequency Masking for Universal Deepfake Detection
Authors: Chandler Timm Doloriel, Ngai-Man Cheung
Categories: cs.CV cs.AI
Comments: Accepted to IEEE ICASSP-2024
\\ ( https://arxiv.org/abs/2401.06506 ,  872kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16801
replaced with revised version Tue, 16 Jan 2024 15:56:35 GMT   (4400kb,D)

Title: Motion-Based Sign Language Video Summarization using Curvature and
  Torsion
Authors: Evangelos G. Sartinas, Emmanouil Z. Psarakis, Dimitrios I. Kosmopoulos
Categories: cs.CV cs.CL
Comments: This work is under consideration at Pattern Recognition Letters for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
MSC-class: 68T45, 68U10
ACM-class: I.4.9; I.5.4; I.2.7
\\ ( https://arxiv.org/abs/2305.16801 ,  4400kb)
------------------------------------------------------------------------------
\\
arXiv:2306.04746 (*cross-listing*)
replaced with revised version Sun, 14 Jan 2024 23:02:59 GMT   (154kb,D)

Title: Using Imperfect Surrogates for Downstream Inference: Design-based
  Supervised Learning for Social Science Applications of Large Language Models
Authors: Naoki Egami, Musashi Hinck, Brandon M. Stewart, Hanying Wei
Categories: stat.ME cs.CL cs.LG stat.ML
Comments: 37th Conference on Neural Information Processing Systems (NeurIPS
  2023)
\\ ( https://arxiv.org/abs/2306.04746 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04041
replaced with revised version Sat, 13 Jan 2024 03:02:12 GMT   (4499kb,D)

Title: Evaluation and Enhancement of Semantic Grounding in Large
  Vision-Language Models
Authors: Jiaying Lu, Jinmeng Rao, Kezhen Chen, Xiaoyuan Guo, Yawen Zhang,
  Baochen Sun, Carl Yang and Jie Yang
Categories: cs.CV cs.CL
Comments: This paper has been accepted to the AAAI'24 Workshop on Responsible
  Language Models (ReLM 2024)
\\ ( https://arxiv.org/abs/2309.04041 ,  4499kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14324 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 13:53:56 GMT   (381kb,D)

Title: Towards General-Purpose Text-Instruction-Guided Voice Conversion
Authors: Chun-Yi Kuan, Chen An Li, Tsu-Yuan Hsu, Tse-Yang Lin, Ho-Lam Chung,
  Kai-Wei Chang, Shuo-yiin Chang, Hung-yi Lee
Categories: eess.AS cs.CL cs.LG cs.SD
Comments: Accepted to ASRU 2023
\\ ( https://arxiv.org/abs/2309.14324 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10788 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 08:09:15 GMT   (1542kb,D)

Title: Self-Supervised Models of Speech Infer Universal Articulatory Kinematics
Authors: Cheol Jun Cho, Abdelrahman Mohamed, Alan W Black and Gopala K.
  Anumanchipalli
Categories: eess.AS cs.CL
\\ ( https://arxiv.org/abs/2310.10788 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01699
replaced with revised version Fri, 12 Jan 2024 22:09:09 GMT   (7306kb,D)

Title: WordArt Designer API: User-Driven Artistic Typography Synthesis with
  Large Language Models on ModelScope
Authors: Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang,
  Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng,
  Xuansong Xie, Jingren Zhou
Categories: cs.CV cs.CL cs.MM
Comments: Spotlight Paper at the Workshop on Machine Learning for Creativity
  and Design, 37th Conference on Neural Information Processing Systems (NeurIPS
  2023). 5 pages, 5 figures
\\ ( https://arxiv.org/abs/2401.01699 ,  7306kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02330
replaced with revised version Mon, 15 Jan 2024 05:24:20 GMT   (672kb,D)

Title: LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model
Authors: Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang
Categories: cs.CV cs.CL
Comments: technique report
\\ ( https://arxiv.org/abs/2401.02330 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06071
replaced with revised version Mon, 15 Jan 2024 06:29:17 GMT   (5130kb,D)

Title: LEGO:Language Enhanced Multi-modal Grounding Model
Authors: Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou,
  Junting Pan, Zefeng Li, Van Tu Vu, Zhida Huang, Tao Wang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2401.06071 ,  5130kb)
------------------------------------------------------------------------------
\\
arXiv:2010.12190
replaced with revised version Tue, 16 Jan 2024 02:34:11 GMT   (5270kb)

Title: Towards Robust Neural Networks via Orthogonal Diversity
Authors: Kun Fang, Qinghua Tao, Yingwen Wu, Tao Li, Jia Cai, Feipeng Cai,
  Xiaolin Huang and Jie Yang
Categories: cs.CV cs.LG
Comments: accepted by Pattern Recognition
DOI: 10.1016/j.patcog.2024.110281
\\ ( https://arxiv.org/abs/2010.12190 ,  5270kb)
------------------------------------------------------------------------------
\\
arXiv:2101.00009 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 17:26:13 GMT   (250kb,D)

Title: Adversarial Estimation of Riesz Representers
Authors: Victor Chernozhukov, Whitney Newey, Rahul Singh, Vasilis Syrgkanis
Categories: econ.EM cs.LG stat.ML
\\ ( https://arxiv.org/abs/2101.00009 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2103.01861
replaced with revised version Mon, 15 Jan 2024 15:52:39 GMT   (508kb,D)

Title: Follow Your Nose -- Which Code Smells are Worth Chasing?
Authors: Idan Amit, Nili Ben Ezra, Dror G. Feitelson
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2103.01861 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2109.10399 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 18:59:12 GMT   (11164kb,D)

Title: SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and
  Benchmarking
Authors: Soukayna Mouatadid, Paulo Orenstein, Genevieve Flaspohler, Miruna
  Oprescu, Judah Cohen, Franklyn Wang, Sean Knight, Maria Geogdzhayeva, Sam
  Levang, Ernest Fraenkel and Lester Mackey
Categories: physics.ao-ph cs.LG stat.ML
\\ ( https://arxiv.org/abs/2109.10399 ,  11164kb)
------------------------------------------------------------------------------
\\
arXiv:2109.13004 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 08:44:20 GMT   (14023kb,D)

Title: Optimising for Interpretability: Convolutional Dynamic Alignment
  Networks
Authors: Moritz B\"ohle, Mario Fritz, Bernt Schiele
Categories: stat.ML cs.CV cs.LG
Comments: Extension of "Convolutional Dynamic Alignment Networks for
  Interpretable Classifications" (B\"ohle et al., CVPR 2021). arXiv admin note:
  substantial text overlap with arXiv:2104.00032
Journal-ref: Published in IEEE Transactions on Pattern Analysis and Machine
  Intelligence (Volume 45, Issue: 6, 01 June 2023, Page(s): 7625 - 7638)
DOI: 10.1109/TPAMI.2022.3226041
\\ ( https://arxiv.org/abs/2109.13004 ,  14023kb)
------------------------------------------------------------------------------
\\
arXiv:2111.11802
replaced with revised version Tue, 16 Jan 2024 09:18:27 GMT   (3462kb,D)

Title: Pruning Self-attentions into Convolutional Layers in Single Path
Authors: Haoyu He, Jianfei Cai, Jing Liu, Zizheng Pan, Jing Zhang, Dacheng Tao,
  Bohan Zhuang
Categories: cs.CV cs.LG
Comments: Accepted by TPAMI 2024
\\ ( https://arxiv.org/abs/2111.11802 ,  3462kb)
------------------------------------------------------------------------------
\\
arXiv:2202.02423
replaced with revised version Mon, 15 Jan 2024 16:42:13 GMT   (1023kb,D)

Title: Improved Information Theoretic Generalization Bounds for Distributed and
  Federated Learning
Authors: L. P. Barnes, Alex Dytso, and H. V. Poor
Categories: cs.IT cs.LG math.IT
Comments: This version of the paper adds an assumption that was missing from
  Theorem 4 for loss functions of type (i). Thanks to Peyman Gholami for
  spotting this bug
DOI: 10.3390/e24091178
\\ ( https://arxiv.org/abs/2202.02423 ,  1023kb)
------------------------------------------------------------------------------
\\
arXiv:2205.00932
replaced with revised version Tue, 16 Jan 2024 02:00:14 GMT   (26240kb,D)

Title: Understanding CNNs from excitations
Authors: Zijian Ying, Qianmu Li, Zhichao Lian, Jun Hou, Tong Lin, Tao Wang
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2205.00932 ,  26240kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14855 (*cross-listing*)
replaced with revised version Sun, 14 Jan 2024 06:07:45 GMT   (49kb)

Title: Leave-one-out Singular Subspace Perturbation Analysis for Spectral
  Clustering
Authors: Anderson Y. Zhang, Harrison H. Zhou
Categories: math.ST cs.LG math.SP stat.TH
\\ ( https://arxiv.org/abs/2205.14855 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2206.00285 (*cross-listing*)
replaced with revised version Sun, 14 Jan 2024 17:18:32 GMT   (21390kb,D)

Title: Stochastic Gradient Methods with Preconditioned Updates
Authors: Abdurakhmon Sadiev, Aleksandr Beznosikov, Abdulla Jasem Almansoori,
  Dmitry Kamzolov, Rachael Tappenden, Martin Tak\'a\v{c}
Categories: math.OC cs.LG
Comments: 40 pages, 2 new algorithms, 20 figures, 4 tables
\\ ( https://arxiv.org/abs/2206.00285 ,  21390kb)
------------------------------------------------------------------------------
\\
arXiv:2206.08756 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 22:35:00 GMT   (314kb,D)

Title: Tensor-on-Tensor Regression: Riemannian Optimization,
  Over-parameterization, Statistical-computational Gap, and Their Interplay
Authors: Yuetian Luo and Anru R. Zhang
Categories: math.ST cs.LG math.OC stat.ME stat.ML stat.TH
\\ ( https://arxiv.org/abs/2206.08756 ,  314kb)
------------------------------------------------------------------------------
\\
arXiv:2206.09507 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 17:35:33 GMT   (120kb,D)

Title: Resource-Efficient Separation Transformer
Authors: Luca Della Libera, Cem Subakan, Mirco Ravanelli, Samuele Cornell,
  Fr\'ed\'eric Lepoutre, Fran\c{c}ois Grondin
Categories: eess.AS cs.LG cs.SD eess.SP
Comments: Accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2206.09507 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2207.08195 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 09:18:19 GMT   (1684kb,D)

Title: SPIRAL: A superlinearly convergent incremental proximal algorithm for
  nonconvex finite sum minimization
Authors: Pourya Behmandpoor, Puya Latafat, Andreas Themelis, Marc Moonen, and
  Panagiotis Patrinos
Categories: math.OC cs.LG
MSC-class: 90C06, 90C25, 90C26, 49J52, 49J53, 90C53
\\ ( https://arxiv.org/abs/2207.08195 ,  1684kb)
------------------------------------------------------------------------------
\\
arXiv:2207.14314 (*cross-listing*)
replaced with revised version Fri, 12 Jan 2024 22:59:47 GMT   (258kb,D)

Title: Supplementing Recurrent Neural Network Wave Functions with Symmetry and
  Annealing to Improve Accuracy
Authors: Mohamed Hibat-Allah, Roger G. Melko, Juan Carrasquilla
Categories: cond-mat.dis-nn cond-mat.str-el cs.LG physics.comp-ph
Comments: 11 pages, 4 figures, 1 table. Corrected typos. Originally published
  in Machine Learning and the Physical Sciences Workshop (NeurIPS 2021), see:
  https://ml4physicalsciences.github.io/2021/files/NeurIPS_ML4PS_2021_92.pdf.
  Our reproducibility code can be found at
  https://github.com/mhibatallah/RNNWavefunctions
Journal-ref: Machine Learning and the Physical Sciences, NeurIPS 2021
\\ ( https://arxiv.org/abs/2207.14314 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2208.01215 (*cross-listing*)
replaced with revised version Sat, 13 Jan 2024 01:42:49 GMT   (11458kb,D)

Title: NAPA: Intermediate-level Variational Native-pulse Ansatz for Variational
  Quantum Algorithms
Authors: Zhiding Liang, Jinglei Cheng, Hang Ren, Hanrui Wang, Fei Hua, Zhixin
  Song, Yongshan Ding, Fred Chong, Song Han, Xuehai Qian, Yiyu Shi
Categories: quant-ph cs.AR cs.LG
Comments: 13 pages, 13 figures
\\ ( https://arxiv.org/abs/2208.01215 ,  11458kb)
------------------------------------------------------------------------------
\\
arXiv:2208.10962 (*cross-listing*)
replaced with revised version Sat, 13 Jan 2024 12:04:15 GMT   (0kb,I)

Title: Prediction of good reaction coordinates and future evolution of MD
  trajectories using Regularized Sparse Autoencoders: A novel deep learning
  approach
Authors: Abhijit Gupta
Categories: physics.chem-ph cs.LG q-bio.QM stat.ME stat.ML
Comments: Disagreement with my PhD supervisor about open-sourcing the
  publication and affiliation
\\ ( https://arxiv.org/abs/2208.10962 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2209.14408
replaced with revised version Sun, 14 Jan 2024 16:40:15 GMT   (14102kb,D)

Title: RALACs: Action Recognition in Autonomous Vehicles using Interaction
  Encoding and Optical Flow
Authors: Eddy Zhou, Alex Zhuang, Alikasim Budhwani, Owen Leather, Rowan
  Dempster, Quanquan Li, Mohammad Al-Sharman, Derek Rayside, and William Melek
Categories: cs.CV cs.LG cs.RO
\\ ( https://arxiv.org/abs/2209.14408 ,  14102kb)
------------------------------------------------------------------------------
\\
arXiv:2210.07893 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 16:37:16 GMT   (1953kb,D)

Title: Numerically Stable Sparse Gaussian Processes via Minimum Separation
  using Cover Trees
Authors: Alexander Terenin, David R. Burt, Artem Artemev, Seth Flaxman, Mark
  van der Wilk, Carl Edward Rasmussen, and Hong Ge
Categories: stat.ML cs.LG
Journal-ref: Journal of Machine Learning Research, 2024
\\ ( https://arxiv.org/abs/2210.07893 ,  1953kb)
------------------------------------------------------------------------------
\\
arXiv:2210.16584 (*cross-listing*)
replaced with revised version Sat, 13 Jan 2024 12:00:45 GMT   (6113kb,D)

Title: Interpretable CNN-Multilevel Attention Transformer for Rapid Recognition
  of Pneumonia from Chest X-Ray Images
Authors: Shengchao Chen, Sufen Ren, Guanjun Wang, Mengxing Huang, and Chenyang
  Xue
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by the IEEE Journal of Biomedical and Health Informatic,
  doi: 10.1109/JBHI.2023.3247949
DOI: 10.1109/JBHI.2023.3247949
\\ ( https://arxiv.org/abs/2210.16584 ,  6113kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10747 (*cross-listing*)
replaced with revised version Sat, 13 Jan 2024 22:40:37 GMT   (1190kb,D)

Title: Exploring validation metrics for offline model-based optimisation with
  diffusion models
Authors: Christopher Beckham, Alexandre Piche, David Vazquez, Christopher Pal
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2211.10747 ,  1190kb)
------------------------------------------------------------------------------
\\
arXiv:2211.11691 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 14:46:57 GMT   (489kb,D)

Title: Deep Signature Algorithm for Multi-dimensional Path-Dependent Options
Authors: Erhan Bayraktar, Qi Feng, and Zhaoyu Zhang
Categories: q-fin.CP cs.LG q-fin.MF
Comments: 21 pages, 1 figure
MSC-class: 65C30, 60H35, 65M75
Journal-ref: SIAM Journal on Financial Mathematics. 2024
\\ ( https://arxiv.org/abs/2211.11691 ,  489kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14566
replaced with revised version Mon, 15 Jan 2024 06:09:01 GMT   (1787kb,D)

Title: Pontryagin Optimal Control via Neural Networks
Authors: Chengyang Gu, Hui Xiong and Yize Chen
Categories: eess.SY cs.LG cs.SY
Comments: In submission
\\ ( https://arxiv.org/abs/2212.14566 ,  1787kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10260 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 15:34:41 GMT   (7765kb,D)

Title: Learned Interferometric Imaging for the SPIDER Instrument
Authors: Matthijs Mars, Marta M. Betcke, Jason D. McEwen
Categories: astro-ph.IM cs.LG eess.IV
Comments: 21 pages, 14 figures
Journal-ref: RAS Techniques and Instruments, Volume 2, Issue 1, January 2023,
  Pages 760-778
DOI: 10.1093/rasti/rzad054
\\ ( https://arxiv.org/abs/2301.10260 ,  7765kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12024 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 13:49:40 GMT   (2477kb,D)

Title: Comparative Study of Coupling and Autoregressive Flows through Robust
  Statistical Tests
Authors: Andrea Coccaro and Marco Letizia and Humberto Reyes-Gonzalez and
  Riccardo Torre
Categories: stat.ML cs.LG hep-ex hep-ph
Comments: v2: Title changed; Substantially improved statistical approach,
  including uncertainties; Highly multimodal/Truncated gaussian benchmarks
  removed; 23 Pages, 2 Figures, 3 Tables
\\ ( https://arxiv.org/abs/2302.12024 ,  2477kb)
------------------------------------------------------------------------------
\\
arXiv:2303.14226 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 09:14:09 GMT   (1229kb,D)

Title: Synthetic Combinations: A Causal Inference Framework for Combinatorial
  Interventions
Authors: Abhineet Agarwal, Anish Agarwal, Suhas Vijaykumar
Categories: stat.ME cs.LG econ.EM stat.ML
\\ ( https://arxiv.org/abs/2303.14226 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2303.14822
replaced with revised version Tue, 16 Jan 2024 02:48:05 GMT   (2073kb,D)

Title: MGTBench: Benchmarking Machine-Generated Text Detection
Authors: Xinlei He and Xinyue Shen and Zeyuan Chen and Michael Backes and Yang
  Zhang
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2303.14822 ,  2073kb)
------------------------------------------------------------------------------
\\
arXiv:2304.04162
replaced with revised version Tue, 16 Jan 2024 07:11:42 GMT   (296kb,D)

Title: Design of Two-Level Incentive Mechanisms for Hierarchical Federated
  Learning
Authors: Shunfeng Chu, Jun Li, Kang Wei, Yuwen Qian, Kunlun Wang, Feng Shu and
  Wen Chen
Categories: cs.GT cs.DC cs.LG
\\ ( https://arxiv.org/abs/2304.04162 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05492
replaced with revised version Tue, 16 Jan 2024 18:37:59 GMT   (1486kb,D)

Title: Towards More Robust and Accurate Sequential Recommendation with
  Cascade-guided Adversarial Training
Authors: Juntao Tan, Shelby Heinecke, Zhiwei Liu, Yongjun Chen, Yongfeng Zhang,
  Huan Wang
Categories: cs.IR cs.LG
Comments: Accepted to present at SIAM International Conference on Data Mining
  (SDM24)
\\ ( https://arxiv.org/abs/2304.05492 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09373
replaced with revised version Mon, 15 Jan 2024 14:52:42 GMT   (37222kb,D)

Title: Multi-task convolutional neural network for image aesthetic assessment
Authors: Derya Soydaner, Johan Wagemans
Categories: cs.CV cs.LG
Journal-ref: IEEE Access, vol. 12, pp. 4716-4729, 2024
DOI: 10.1109/ACCESS.2024.3349961
\\ ( https://arxiv.org/abs/2305.09373 ,  37222kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14062 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 13:54:19 GMT   (32857kb,D)

Title: Amplitude-Independent Machine Learning for PPG through Visibility Graphs
  and Transfer Learning
Authors: Yuyang Miao, Harry J. Davies, Danilo P. Mandic
Categories: eess.SP cs.LG
\\ ( https://arxiv.org/abs/2305.14062 ,  32857kb)
------------------------------------------------------------------------------
\\
arXiv:2305.20004 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 04:15:25 GMT   (34546kb,D)

Title: Learning to solve Bayesian inverse problems: An amortized variational
  inference approach using Gaussian and Flow guides
Authors: Sharmila Karumuri and Ilias Bilionis
Categories: stat.ML cs.LG physics.data-an
\\ ( https://arxiv.org/abs/2305.20004 ,  34546kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00673 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 11:15:43 GMT   (3172kb,D)

Title: ENN: A Neural Network with DCT Adaptive Activation Functions
Authors: Marc Martinez-Gost, Ana P\'erez-Neira, Miguel \'Angel Lagunas
Categories: eess.SP cs.LG cs.NE
Comments: Paper accepted in IEEE Journal of Selected Topics in Signal
  Processing (JSTSP) Special Series on AI in Signal & Data Science - Toward
  Explainable, Reliable, and Sustainable Machine Learning
\\ ( https://arxiv.org/abs/2307.00673 ,  3172kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03590 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 03:27:31 GMT   (622kb,D)

Title: Accelerated Optimization Landscape of Linear-Quadratic Regulator
Authors: Lechen Feng and Yuan-Hua Ni
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2307.03590 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04748
replaced with revised version Mon, 15 Jan 2024 20:12:22 GMT   (1704kb,D)

Title: Fuzz4All: Universal Fuzzing with Large Language Models
Authors: Chunqiu Steven Xia, Matteo Paltenghi, Jia Le Tian, Michael Pradel,
  Lingming Zhang
Categories: cs.SE cs.LG
Comments: Accepted at ICSE 2024
\\ ( https://arxiv.org/abs/2308.04748 ,  1704kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05141
replaced with revised version Sat, 13 Jan 2024 11:40:54 GMT   (13335kb,D)

Title: Sound propagation in realistic interactive 3D scenes with parameterized
  sources using deep neural operators
Authors: Nikolas Borrel-Jensen, Somdatta Goswami, Allan P. Engsig-Karup, George
  Em Karniadakis, Cheol-Ho Jeong
Categories: cs.SD cs.LG eess.AS
Comments: 25 pages, 10 figures, 4 tables
\\ ( https://arxiv.org/abs/2308.05141 ,  13335kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06399 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 09:07:07 GMT   (456kb,D)

Title: Learning Bayesian Networks with Heterogeneous Agronomic Data Sets via
  Mixed-Effect Models and Hierarchical Clustering
Authors: Lorenzo Valleggi and Marco Scutari and Federico Mattia Stefanini
Categories: stat.ML cs.LG stat.AP
Comments: 34 pages, 6 figures
\\ ( https://arxiv.org/abs/2308.06399 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13983 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 09:50:37 GMT   (4444kb,D)

Title: Interpolation of mountain weather forecasts by machine learning
Authors: Kazuma Iwase and Tomoyuki Takenawa
Categories: physics.ao-ph cs.LG
Comments: 8 pages
\\ ( https://arxiv.org/abs/2308.13983 ,  4444kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06627 (*cross-listing*)
replaced with revised version Sun, 14 Jan 2024 20:27:00 GMT   (4483kb,D)

Title: A Sequentially Fair Mechanism for Multiple Sensitive Attributes
Authors: Fran\c{c}ois Hu and Philipp Ratz and Arthur Charpentier
Categories: stat.ML cs.CY cs.LG
\\ ( https://arxiv.org/abs/2309.06627 ,  4483kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07156 (*cross-listing*)
replaced with revised version Sun, 14 Jan 2024 14:33:19 GMT   (4536kb,D)

Title: Transparency in Sleep Staging: Deep Learning Method for EEG Sleep Stage
  Classification with Model Interpretability
Authors: Shivam Sharma, Suvadeep Maiti, S. Mythirayee, Srijithesh Rajendran,
  Raju Surampudi Bapi
Categories: eess.SP cs.LG
Comments: 12 pages, 9 figures, Under review at IEEE Journal of Biomedical and
  Health Informatics
\\ ( https://arxiv.org/abs/2309.07156 ,  4536kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11856 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 17:44:40 GMT   (6398kb,D)

Title: Activation Compression of Graph Neural Networks using Block-wise
  Quantization with Improved Variance Minimization
Authors: Sebastian Eliassen, Raghavendra Selvan
Categories: stat.ML cs.LG
Comments: Accepted to be presented at the International Conference on
  Acoustics, Speech and Signal Processing (ICASSP-2024). Source code at
  https://github.com/saintslab/i-Exact
\\ ( https://arxiv.org/abs/2309.11856 ,  6398kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16369
replaced with revised version Mon, 15 Jan 2024 14:40:23 GMT   (295kb,D)

Title: Bringing the Discussion of Minima Sharpness to the Audio Domain: a
  Filter-Normalised Evaluation for Acoustic Scene Classification
Authors: Manuel Milling, Andreas Triantafyllopoulos, Iosif Tsangko, Simon David
  Noel Rampp, Bj\"orn Wolfgang Schuller
Categories: cs.SD cs.LG eess.AS
Comments: This work has been submitted to the IEEE for possible publication
\\ ( https://arxiv.org/abs/2309.16369 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16672
replaced with revised version Mon, 15 Jan 2024 21:13:58 GMT   (7994kb,D)

Title: Learning to Transform for Generalizable Instance-wise Invariance
Authors: Utkarsh Singhal and Carlos Esteves and Ameesh Makadia and Stella X. Yu
Categories: cs.CV cs.LG
Comments: Accepted to ICCV 2023
\\ ( https://arxiv.org/abs/2309.16672 ,  7994kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04585 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 21:54:35 GMT   (47kb,D)

Title: Interventions Against Machine-Assisted Statistical Discrimination
Authors: John Y. Zhu
Categories: econ.TH cs.LG
\\ ( https://arxiv.org/abs/2310.04585 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07958
replaced with revised version Mon, 15 Jan 2024 04:21:08 GMT   (2517kb,D)

Title: Towards Causal Deep Learning for Vulnerability Detection
Authors: Md Mahbubur Rahman, Ira Ceka, Chengzhi Mao, Saikat Chakraborty,
  Baishakhi Ray, and Wei Le
Categories: cs.SE cs.CR cs.LG stat.ME
Comments: ICSE 2024, Camera Ready Version
DOI: 10.1145/3597503.3639170
\\ ( https://arxiv.org/abs/2310.07958 ,  2517kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12447 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 06:56:51 GMT   (876kb,D)

Title: Constrained Reweighting of Distributions: an Optimal Transport Approach
Authors: Abhisek Chakraborty, Anirban Bhattacharya, Debdeep Pati
Categories: stat.ML cs.LG
Comments: arXiv admin note: text overlap with arXiv:2303.10085
\\ ( https://arxiv.org/abs/2310.12447 ,  876kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13349 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 17:17:26 GMT   (21992kb,D)

Title: DeepFDR: A Deep Learning-based False Discovery Rate Control Method for
  Neuroimaging Data
Authors: Taehyo Kim, Hai Shu, Qiran Jia, Mony de Leon
Categories: stat.ML cs.CV cs.LG
\\ ( https://arxiv.org/abs/2310.13349 ,  21992kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20598
replaced with revised version Sat, 13 Jan 2024 23:20:08 GMT   (3005kb,D)

Title: Online Conversion with Switching Costs: Robust and Learning-Augmented
  Algorithms
Authors: Adam Lechowicz, Nicolas Christianson, Bo Sun, Noman Bashir, Mohammad
  Hajiesmaili, Adam Wierman, Prashant Shenoy
Categories: cs.DS cs.LG
Comments: Accepted to SIGMETRICS / Performance '24. 47 pages, 9 figures
\\ ( https://arxiv.org/abs/2310.20598 ,  3005kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01352 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 11:32:48 GMT   (9047kb,D)

Title: Deep learning based Image Compression for Microscopy Images: An
  Empirical Study
Authors: Yu Zhou, Jan Sollmann, Jianxu Chen
Categories: eess.IV cs.CV cs.LG
Comments: - Update github link; - correct the author name; - update the table
  (correct some errors during calculation); - update the implementation detail
  section and the discussion section
\\ ( https://arxiv.org/abs/2311.01352 ,  9047kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05559 (*cross-listing*)
replaced with revised version Sat, 13 Jan 2024 11:02:53 GMT   (3879kb,D)

Title: Disentangling Quantum and Classical Contributions in Hybrid Quantum
  Machine Learning Architectures
Authors: Michael K\"olle, Jonas Maurer, Philipp Altmann, Leo S\"unkel, Jonas
  Stein, Claudia Linnhoff-Popien
Categories: quant-ph cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.05559 ,  3879kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05739
replaced with revised version Mon, 15 Jan 2024 22:29:37 GMT   (1906kb,D)

Title: Deep Learning Architecture for Network-Efficiency at the Edge
Authors: Akrit Mudvari, Antero Vainio, Iason Ofeidis, Sasu Tarkoma, Leandros
  Tassiulas
Categories: cs.NI cs.LG
\\ ( https://arxiv.org/abs/2311.05739 ,  1906kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06748 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 08:35:30 GMT   (5508kb,D)

Title: How do Minimum-Norm Shallow Denoisers Look in Function Space?
Authors: Chen Zeno, Greg Ongie, Yaniv Blumenfeld, Nir Weinberger, Daniel Soudry
Categories: stat.ML cs.LG
Comments: Thirty-seventh Conference on Neural Information Processing Systems
\\ ( https://arxiv.org/abs/2311.06748 ,  5508kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12082 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 08:34:28 GMT   (11111kb,D)

Title: Tiny-VBF: Resource-Efficient Vision Transformer based Lightweight
  Beamformer for Ultrasound Single-Angle Plane Wave Imaging
Authors: Abdul Rahoof, Vivek Chaturvedi, Mahesh Raveendranatha Panicker, and
  Muhammad Shafique
Categories: eess.IV cs.LG
Comments: 6 pages, DATE 2024
\\ ( https://arxiv.org/abs/2311.12082 ,  11111kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18744 (*cross-listing*)
replaced with revised version Sun, 14 Jan 2024 01:35:59 GMT   (3961kb,D)

Title: $\mathbb{Z}_2\times \mathbb{Z}_2$ Equivariant Quantum Neural Networks:
  Benchmarking against Classical Neural Networks
Authors: Zhongtian Dong, Mar\c{c}al Comajoan Cara, Gopal Ramesh Dahale, Roy T.
  Forestano, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom Magorsch,
  Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu
Categories: quant-ph cs.LG hep-ph stat.ML
Comments: 7 pages, 5 figures
\\ ( https://arxiv.org/abs/2311.18744 ,  3961kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02683 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 16:17:57 GMT   (36kb,D)

Title: Diffusion-Based Speech Enhancement in Matched and Mismatched Conditions
  Using a Heun-Based Sampler
Authors: Philippe Gonzalez, Zheng-Hua Tan, Jan {\O}stergaard, Jesper Jensen,
  Tommy Sonne Alstr{\o}m, Tobias May
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2312.02683 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04432
replaced with revised version Tue, 16 Jan 2024 08:40:12 GMT   (227kb,D)

Title: FreqFed: A Frequency Analysis-Based Approach for Mitigating Poisoning
  Attacks in Federated Learning
Authors: Hossein Fereidooni, Alessandro Pegoraro, Phillip Rieger, Alexandra
  Dmitrienko, Ahmad-Reza Sadeghi
Categories: cs.CR cs.LG
Comments: To appear in the Network and Distributed System Security (NDSS)
  Symposium 2024. 16 pages, 8 figures, 12 tables, 1 algorithm, 3 equations
DOI: 10.14722/ndss.2024.23620
\\ ( https://arxiv.org/abs/2312.04432 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08723
replaced with revised version Tue, 16 Jan 2024 09:15:05 GMT   (380kb,D)

Title: StemGen: A music generation model that listens
Authors: Julian D. Parker, Janne Spijkervet, Katerina Kosta, Furkan Yesiler,
  Boris Kuznetsov, Ju-Chiang Wang, Matt Avent, Jitong Chen, Duc Le
Categories: cs.SD cs.LG eess.AS
Comments: Accepted for publication at ICASSP 2024
\\ ( https://arxiv.org/abs/2312.08723 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14507
replaced with revised version Mon, 15 Jan 2024 10:41:32 GMT   (1504kb,D)

Title: Unsupervised Harmonic Parameter Estimation Using Differentiable DSP and
  Spectral Optimal Transport
Authors: Bernardo Torres (S2A, IDS), Geoffroy Peeters (S2A, IDS), Ga\"el
  Richard (S2A, IDS)
Categories: cs.SD cs.LG eess.AS eess.SP
Comments: Accepted in ICASSP 2024
Journal-ref: IEEE International Conference on Acoustics, Speech and Signal
  Processing, Apr 2024, Seoul, South Korea
\\ ( https://arxiv.org/abs/2312.14507 ,  1504kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00744 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 14:31:50 GMT   (1580kb,D)

Title: Harmonizing Covariance and Expressiveness for Deep Hamiltonian
  Regression in Crystalline Material Research: a Hybrid Cascaded Regression
  Framework
Authors: Shi Yin, Xinyang Pan, Xudong Zhu, Tianyu Gao, Haochong Zhang, Feng Wu,
  Lixin He
Categories: physics.comp-ph cond-mat.mtrl-sci cs.LG
\\ ( https://arxiv.org/abs/2401.00744 ,  1580kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00910
replaced with revised version Tue, 16 Jan 2024 16:28:58 GMT   (5452kb,D)

Title: WoodScape Motion Segmentation for Autonomous Driving -- CVPR 2023 OmniCV
  Workshop Challenge
Authors: Saravanabalagi Ramachandran and Nathaniel Cibik and Ganesh Sistu and
  John McDonald
Categories: cs.CV cs.LG
Comments: CVPR 2023 OmniCV Workshop Challenge
\\ ( https://arxiv.org/abs/2401.00910 ,  5452kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04464
replaced with revised version Mon, 15 Jan 2024 16:12:45 GMT   (3398kb,D)

Title: PhilEO Bench: Evaluating Geo-Spatial Foundation Models
Authors: Casper Fibaek, Luke Camilleri, Andreas Luyts, Nikolaos Dionelis,
  Bertrand Le Saux
Categories: cs.CV cs.LG
Comments: 6 pages, 5 figures, Submitted to IGARSS 2024
\\ ( https://arxiv.org/abs/2401.04464 ,  3398kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05394 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 09:57:58 GMT   (2848kb,D)

Title: Iterative Regularization with k-support Norm: An Important Complement to
  Sparse Recovery
Authors: William de Vazelhes, Bhaskar Mukhoty, Xiao-Tong Yuan, Bin Gu
Categories: eess.SP cs.LG math.OC stat.ML
Comments: Accepted at AAAI 2024. Code at
  https://github.com/wdevazelhes/IRKSN_AAAI2024
\\ ( https://arxiv.org/abs/2401.05394 ,  2848kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05982 (*cross-listing*)
replaced with revised version Mon, 15 Jan 2024 15:15:34 GMT   (118kb)

Title: A tree-based varying coefficient model
Authors: Henning Zakrisson and Mathias Lindholm
Categories: stat.ML cs.LG
Comments: 23 pages, 6 figures
\\ ( https://arxiv.org/abs/2401.05982 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06362
replaced with revised version Tue, 16 Jan 2024 09:29:39 GMT   (9568kb,D)

Title: Attention, Distillation, and Tabularization: Towards Practical Neural
  Network-Based Prefetching
Authors: Pengmiao Zhang, Neelesh Gupta, Rajgopal Kannan, Viktor K. Prasanna
Categories: cs.NE cs.AR cs.LG cs.OS
\\ ( https://arxiv.org/abs/2401.06362 ,  9568kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
