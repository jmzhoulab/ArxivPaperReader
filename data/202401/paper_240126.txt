Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月26日 17:07
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed 24 Jan 24 19:00:00 GMT  to  Thu 25 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.13752
Date: Wed, 24 Jan 2024 19:12:38 GMT   (451kb)

Title: Explaining Image Classifiers
Authors: Hana Chockler and Joseph Y. Halpern
Categories: cs.AI
\\
  We focus on explaining image classifiers, taking the work of Mothilal et al.
[2021] (MMTS) as our point of departure. We observe that, although MMTS claim
to be using the definition of explanation proposed by Halpern [2016], they do
not quite do so. Roughly speaking, Halpern's definition has a necessity clause
and a sufficiency clause. MMTS replace the necessity clause by a requirement
that, as we show, implies it. Halpern's definition also allows agents to
restrict the set of options considered. While these difference may seem minor,
as we show, they can have a nontrivial impact on explanations. We also show
that, essentially without change, Halpern's definition can handle two issues
that have proved difficult for other approaches: explanations of absence (when,
for example, an image classifier for tumors outputs "no tumor") and
explanations of rare events (such as tumors).
\\ ( https://arxiv.org/abs/2401.13752 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13770
Date: Wed, 24 Jan 2024 19:37:10 GMT   (32kb)

Title: AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard
  Combinatorial Problems
Authors: Piyush Jha, Zhengyu Li, Zhengyang Lu, Curtis Bright, Vijay Ganesh
Categories: cs.AI math.CO
\\
  This paper introduces AlphaMapleSAT, a novel Monte Carlo Tree Search (MCTS)
based Cube-and-Conquer (CnC) SAT solving method aimed at efficiently solving
challenging combinatorial problems. Despite the tremendous success of CnC
solvers in solving a variety of hard combinatorial problems, the lookahead
cubing techniques at the heart of CnC have not evolved much for many years.
Part of the reason is the sheer difficulty of coming up with new cubing
techniques that are both low-cost and effective in partitioning input formulas
into sub-formulas, such that the overall runtime is minimized.
  Lookahead cubing techniques used by current state-of-the-art CnC solvers,
such as March, keep their cubing costs low by constraining the search for the
optimal splitting variables. By contrast, our key innovation is a
deductively-driven MCTS-based lookahead cubing technique, that performs a
deeper heuristic search to find effective cubes, while keeping the cubing cost
low. We perform an extensive comparison of AlphaMapleSAT against the March CnC
solver on challenging combinatorial problems such as the minimum Kochen-Specker
and Ramsey problems. We also perform ablation studies to verify the efficacy of
the MCTS heuristic search for the cubing problem. Results show up to 2.3x
speedup in parallel (and up to 27x in sequential) elapsed real time.
\\ ( https://arxiv.org/abs/2401.13770 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13883
Date: Thu, 25 Jan 2024 01:48:09 GMT   (660kb,D)

Title: Domain-Independent Dynamic Programming
Authors: Ryo Kuroiwa, J. Christopher Beck
Categories: cs.AI
Comments: Manuscript submitted to JACM
\\
  For combinatorial optimization problems, model-based paradigms such as
mixed-integer programming (MIP) and constraint programming (CP) aim to decouple
modeling and solving a problem: the `holy grail' of declarative problem
solving. We propose domain-independent dynamic programming (DIDP), a new
model-based paradigm based on dynamic programming (DP). While DP is not new, it
has typically been implemented as a problem-specific method. We introduce
Dynamic Programming Description Language (DyPDL), a formalism to define DP
models based on a state transition system, inspired by AI planning. We show
that heuristic search algorithms can be used to solve DyPDL models and propose
seven DIDP solvers. We experimentally compare our DIDP solvers with commercial
MIP and CP solvers (solving MIP and CP models, respectively) on common
benchmark instances of eleven combinatorial optimization problem classes. We
show that DIDP outperforms MIP in nine problem classes, CP also in nine problem
classes, and both MIP and CP in seven.
\\ ( https://arxiv.org/abs/2401.13883 ,  660kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13935
Date: Thu, 25 Jan 2024 04:28:39 GMT   (1230kb,D)

Title: A New Paradigm for Counterfactual Reasoning in Fairness and Recourse
Authors: Lucius E.J. Bynum, Joshua R. Loftus, Julia Stoyanovich
Categories: cs.AI cs.CY stat.ML
\\
  Counterfactuals and counterfactual reasoning underpin numerous techniques for
auditing and understanding artificial intelligence (AI) systems. The
traditional paradigm for counterfactual reasoning in this literature is the
interventional counterfactual, where hypothetical interventions are imagined
and simulated. For this reason, the starting point for causal reasoning about
legal protections and demographic data in AI is an imagined intervention on a
legally-protected characteristic, such as ethnicity, race, gender, disability,
age, etc. We ask, for example, what would have happened had your race been
different? An inherent limitation of this paradigm is that some demographic
interventions -- like interventions on race -- may not translate into the
formalisms of interventional counterfactuals. In this work, we explore a new
paradigm based instead on the backtracking counterfactual, where rather than
imagine hypothetical interventions on legally-protected characteristics, we
imagine alternate initial conditions while holding these characteristics fixed.
We ask instead, what would explain a counterfactual outcome for you as you
actually are or could be? This alternate framework allows us to address many of
the same social concerns, but to do so while asking fundamentally different
questions that do not rely on demographic interventions.
\\ ( https://arxiv.org/abs/2401.13935 ,  1230kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14086
Date: Thu, 25 Jan 2024 11:06:16 GMT   (2001kb,D)

Title: Generating Likely Counterfactuals Using Sum-Product Networks
Authors: Jiri Nemecek, Tomas Pevny, Jakub Marecek
Categories: cs.AI cs.LG math.OC
\\
  Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI
systems need to be explained. These decisions are often explainable only post
hoc, where counterfactual explanations are popular. The question of what
constitutes the best counterfactual explanation must consider multiple aspects,
where "distance from the sample" is the most common. We argue that this
requirement frequently leads to explanations that are unlikely and, therefore,
of limited value. Here, we present a system that provides high-likelihood
explanations. We show that the search for the most likely explanations
satisfying many common desiderata for counterfactual explanations can be
modeled using mixed-integer optimization (MIO). In the process, we propose an
MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the
likelihood of a counterfactual, which can be of independent interest. A
numerical comparison against several methods for generating counterfactual
explanations is provided.
\\ ( https://arxiv.org/abs/2401.14086 ,  2001kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14153
Date: Thu, 25 Jan 2024 13:05:06 GMT   (505kb,D)

Title: Agent-based Simulation with Netlogo to Evaluate AmI Scenarios
Authors: J. Carbo, N. Sanchez, J. M. Molina
Categories: cs.AI
DOI: 10.1057/jos.2016.10
\\
  In this paper an agent-based simulation is developed in order to evaluate an
AmI scenario based on agents. Many AmI applications are implemented through
agents but they are not compared to any other existing alternative in order to
evaluate the relative benefits of using them. The proposal simulation
environment developed in Netlogo analyse such benefits using two evaluation
criteria: First, measuring agent satisfaction of different types of desires
along the execution. Second, measuring time savings obtained through a correct
use of context information.
  So, here, a previously suggested agent architecture, an ontology and a
12-steps protocol to provide AmI services in airports, is evaluated using a
NetLogo simulation environment. The present work uses a NetLogo model
considering scalability problems of this application domain but using FIPA and
BDI extensions to be coherent with our previous works and our previous JADE
implementation of them.
  The NetLogo model presented simulates an airport with agent users passing
through several zones located in a specific order in a map: passport controls,
check-in counters of airline companies, boarding gates, different types of
shopping. Although initial data in simulations are generated randomly, and the
model is just an approximation of real-world airports, the definition of this
case of use of Ambient Intelligence through NetLogo agents opens an interesting
way to evaluate the benefits of using Ambient Intelligence, which is a
significant contribution to the final development of them.
\\ ( https://arxiv.org/abs/2401.14153 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14183
Date: Fri, 13 Oct 2023 22:09:52 GMT   (37251kb,D)

Title: Towards Autonomous Supply Chains: Definition, Characteristics,
  Conceptual Framework, and Autonomy Levels
Authors: Liming Xu and Stephen Mak and Yaniv Proselkov and Alexandra Brintrup
Categories: cs.AI cs.MA cs.SY eess.SY math.OC
Comments: This paper includes 20 pages and 8 figures
\\
  Recent global disruptions, such as the pandemic and geopolitical conflicts,
have profoundly exposed vulnerabilities in traditional supply chains, requiring
exploration of more resilient alternatives. Autonomous supply chains (ASCs)
have emerged as a potential solution, offering increased visibility,
flexibility, and resilience in turbulent trade environments. Despite
discussions in industry and academia over several years, ASCs lack
well-established theoretical foundations. This paper addresses this research
gap by presenting a formal definition of ASC along with its defining
characteristics and auxiliary concepts. We propose a layered conceptual
framework called the MIISI model. An illustrative case study focusing on the
meat supply chain demonstrates an initial ASC implementation based on this
conceptual model. Additionally, we introduce a seven-level supply chain
autonomy reference model, delineating a trajectory towards achieving a full
supply chain autonomy. Recognising that this work represents an initial
endeavour, we emphasise the need for continued exploration in this emerging
domain. We anticipate that this work will stimulate further research, both
theoretical and technical, and contribute to the continual evolution of ASCs.
\\ ( https://arxiv.org/abs/2401.14183 ,  37251kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13789
Date: Wed, 24 Jan 2024 20:17:11 GMT   (375kb,D)

Title: A Unified Approach to Emotion Detection and Task-Oriented Dialogue
  Modeling
Authors: Armand Stricker, Patrick Paroubek
Categories: cs.CL
Comments: Accepted @ IWSDS 2024
\\
  In current text-based task-oriented dialogue (TOD) systems, user emotion
detection (ED) is often overlooked or is typically treated as a separate and
independent task, requiring additional training. In contrast, our work
demonstrates that seamlessly unifying ED and TOD modeling brings about mutual
benefits, and is therefore an alternative to be considered. Our method consists
in augmenting SimpleToD, an end-to-end TOD system, by extending belief state
tracking to include ED, relying on a single language model. We evaluate our
approach using GPT-2 and Llama-2 on the EmoWOZ benchmark, a version of MultiWOZ
annotated with emotions. Our results reveal a general increase in performance
for ED and task results. Our findings also indicate that user emotions provide
useful contextual conditioning for system responses, and can be leveraged to
further refine responses in terms of empathy.
\\ ( https://arxiv.org/abs/2401.13789 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13810
Date: Wed, 24 Jan 2024 21:02:07 GMT   (591kb,D)

Title: Automated Root Causing of Cloud Incidents using In-Context Learning with
  GPT-4
Authors: Xuchao Zhang, Supriyo Ghosh, Chetan Bansal, Rujia Wang, Minghua Ma, Yu
  Kang, Saravan Rajmohan
Categories: cs.CL cs.SE
\\
  Root Cause Analysis (RCA) plays a pivotal role in the incident diagnosis
process for cloud services, requiring on-call engineers to identify the primary
issues and implement corrective actions to prevent future recurrences.
Improving the incident RCA process is vital for minimizing service downtime,
customer impact and manual toil. Recent advances in artificial intelligence
have introduced state-of-the-art Large Language Models (LLMs) like GPT-4, which
have proven effective in tackling various AIOps problems, ranging from code
authoring to incident management. Nonetheless, the GPT-4 model's immense size
presents challenges when trying to fine-tune it on user data because of the
significant GPU resource demand and the necessity for continuous model
fine-tuning with the emergence of new data. To address the high cost of
fine-tuning LLM, we propose an in-context learning approach for automated root
causing, which eliminates the need for fine-tuning. We conduct extensive study
over 100,000 production incidents, comparing several large language models
using multiple metrics. The results reveal that our in-context learning
approach outperforms the previous fine-tuned large language models such as
GPT-3 by an average of 24.8\% across all metrics, with an impressive 49.7\%
improvement over the zero-shot model. Moreover, human evaluation involving
actual incident owners demonstrates its superiority over the fine-tuned model,
achieving a 43.5\% improvement in correctness and an 8.7\% enhancement in
readability. The impressive results demonstrate the viability of utilizing a
vanilla GPT model for the RCA task, thereby avoiding the high computational and
maintenance costs associated with a fine-tuned model.
\\ ( https://arxiv.org/abs/2401.13810 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13849
Date: Wed, 24 Jan 2024 23:11:33 GMT   (1731kb,D)

Title: TPD: Enhancing Student Language Model Reasoning via Principle Discovery
  and Guidance
Authors: Haorui Wang (1), Rongzhi Zhang (1), Yinghao Li (1), Lingkai Kong (1),
  Yuchen Zhuang (1), Xiusi Chen (2), Chao Zhang (1) ((1) College of Computing,
  Georgia Institute of Technology, (2) Department of Computer Science,
  University of California, Los Angeles)
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have recently showcased remarkable reasoning
abilities. However, larger models often surpass their smaller counterparts in
reasoning tasks, posing the challenge of effectively transferring these
capabilities from larger models. Existing approaches heavily rely on extensive
fine-tuning data or continuous interactions with a superior teacher LLM during
inference. We introduce a principle-based teacher-student framework called
``Teaching via Principle Discovery'' (TPD) to address these limitations.
Inspired by human learning mechanisms, TPD mimics the interaction between a
teacher and a student using a principle-based approach. The teacher LLM
generates problem-solving instructions and corrective principles based on the
student LLM's errors. These principles guide the refinement of instructions and
the selection of instructive examples from a validation set. This enables the
student model to learn from both the teacher's guidance and its own mistakes.
Once the student model begins making inferences, TPD requires no further
intervention from the teacher LLM or humans. Through extensive experiments
across eight reasoning tasks, we demonstrate the effectiveness of TPD. Compared
to standard chain-of-thought prompting, TPD significantly improves the student
model's performance, achieving $6.2\%$ improvement on average.
\\ ( https://arxiv.org/abs/2401.13849 ,  1731kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13867
Date: Thu, 25 Jan 2024 00:34:16 GMT   (748kb)

Title: Unmasking and Quantifying Racial Bias of Large Language Models in
  Medical Report Generation
Authors: Yifan Yang, Xiaoyu Liu, Qiao Jin, Furong Huang, Zhiyong Lu
Categories: cs.CL
\\
  Large language models like GPT-3.5-turbo and GPT-4 hold promise for
healthcare professionals, but they may inadvertently inherit biases during
their training, potentially affecting their utility in medical applications.
Despite few attempts in the past, the precise impact and extent of these biases
remain uncertain. Through both qualitative and quantitative analyses, we find
that these models tend to project higher costs and longer hospitalizations for
White populations and exhibit optimistic views in challenging medical scenarios
with much higher survival rates. These biases, which mirror real-world
healthcare disparities, are evident in the generation of patient backgrounds,
the association of specific diseases with certain races, and disparities in
treatment recommendations, etc. Our findings underscore the critical need for
future research to address and mitigate biases in language models, especially
in critical healthcare applications, to ensure fair and accurate outcomes for
all patients.
\\ ( https://arxiv.org/abs/2401.13867 ,  748kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13887
Date: Thu, 25 Jan 2024 02:05:31 GMT   (1483kb)

Title: A comparative study of zero-shot inference with large language models
  and supervised modeling in breast cancer pathology classification
Authors: Madhumita Sushil, Travis Zack, Divneet Mandair, Zhiwei Zheng, Ahmed
  Wali, Yan-Ning Yu, Yuwei Quan, Atul J. Butte
Categories: cs.CL cs.LG
\\
  Although supervised machine learning is popular for information extraction
from clinical notes, creating large annotated datasets requires extensive
domain expertise and is time-consuming. Meanwhile, large language models (LLMs)
have demonstrated promising transfer learning capability. In this study, we
explored whether recent LLMs can reduce the need for large-scale data
annotations. We curated a manually-labeled dataset of 769 breast cancer
pathology reports, labeled with 13 categories, to compare zero-shot
classification capability of the GPT-4 model and the GPT-3.5 model with
supervised classification performance of three model architectures: random
forests classifier, long short-term memory networks with attention (LSTM-Att),
and the UCSF-BERT model. Across all 13 tasks, the GPT-4 model performed either
significantly better than or as well as the best supervised model, the LSTM-Att
model (average macro F1 score of 0.83 vs. 0.75). On tasks with high imbalance
between labels, the differences were more prominent. Frequent sources of GPT-4
errors included inferences from multiple samples and complex task design. On
complex tasks where large annotated datasets cannot be easily collected, LLMs
can reduce the burden of large-scale data labeling. However, if the use of LLMs
is prohibitive, the use of simpler supervised models with large annotated
datasets can provide comparable results. LLMs demonstrated the potential to
speed up the execution of clinical NLP studies by reducing the need for
curating large annotated datasets. This may result in an increase in the
utilization of NLP-based variables and outcomes in observational clinical
studies.
\\ ( https://arxiv.org/abs/2401.13887 ,  1483kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13905
Date: Thu, 25 Jan 2024 02:50:03 GMT   (814kb,D)

Title: Dynamic embedded topic models and change-point detection for exploring
  literary-historical hypotheses
Authors: Hale Sirin, Tom Lippincott
Categories: cs.CL
Comments: Accepted to LaTeCH@EACL2024
\\
  We present a novel combination of dynamic embedded topic models and
change-point detection to explore diachronic change of lexical semantic
modality in classical and early Christian Latin. We demonstrate several methods
for finding and characterizing patterns in the output, and relating them to
traditional scholarship in Comparative Literature and Classics. This simple
approach to unsupervised models of semantic change can be applied to any
suitable corpus, and we conclude with future directions and refinements aiming
to allow noisier, less-curated materials to meet that threshold.
\\ ( https://arxiv.org/abs/2401.13905 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13907
Date: Thu, 25 Jan 2024 02:54:53 GMT   (2364kb,D)

Title: No More Distractions: an Adaptive Up-Sampling Algorithm to Reduce Data
  Artifacts
Authors: Han Chen
Categories: cs.CL
\\
  Researchers recently found out that sometimes language models achieve high
accuracy on benchmark data set, but they can not generalize very well with even
little changes to the original data set. This is sometimes due to data
artifacts, model is learning the spurious correlation between tokens and
labels, instead of the semantics and logic. In this work, we analyzed SNLI data
and visualized such spurious correlations. We proposed an adaptive up-sampling
algorithm to correct the data artifacts, which is simple and effective, and
does not need human edits or annotation. We did an experiment applying the
algorithm to fix the data artifacts in SNLI data and the model trained with
corrected data performed significantly better than the model trained with raw
SNLI data, overall, as well as on the subset we corrected.
\\ ( https://arxiv.org/abs/2401.13907 ,  2364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13919
Date: Thu, 25 Jan 2024 03:33:18 GMT   (18186kb,D)

Title: WebVoyager: Building an End-to-End Web Agent with Large Multimodal
  Models
Authors: Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming
  Zhang, Zhenzhong Lan, Dong Yu
Categories: cs.CL cs.AI
\\
  The advancement of large language models (LLMs) leads to a new era marked by
the development of autonomous applications in the real world, which drives
innovation in the creation of advanced web-based agents. Existing web agents
typically only handle one input modality and are evaluated only in simplified
web simulators or static web snapshots, greatly limiting their applicability in
real-world scenarios. To bridge this gap, we introduce WebVoyager, an
innovative Large Multimodal Model (LMM) powered web agent that can complete
user instructions end-to-end by interacting with real-world websites. Moreover,
we propose a new evaluation protocol for web agents to address the challenges
of automatic evaluation of open-ended web agent tasks, leveraging the robust
multimodal comprehension capabilities of GPT-4V. We create a new benchmark by
gathering real-world tasks from 15 widely used websites to evaluate our agents.
We show that WebVoyager achieves a 55.7% task success rate, significantly
surpassing the performance of both GPT-4 (All Tools) and the WebVoyager
(text-only) setups, underscoring the exceptional capability of WebVoyager in
practical applications. We found that our proposed automatic evaluation
achieves 85.3% agreement with human judgment, paving the way for further
development of web agents in a real-world setting.
\\ ( https://arxiv.org/abs/2401.13919 ,  18186kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13927
Date: Thu, 25 Jan 2024 03:57:12 GMT   (4276kb,D)

Title: Adaptive Text Watermark for Large Language Models
Authors: Yepeng Liu, Yuheng Bu
Categories: cs.CL
\\
  The advancement of Large Language Models (LLMs) has led to increasing
concerns about the misuse of AI-generated text, and watermarking for
LLM-generated text has emerged as a potential solution. However, it is
challenging to generate high-quality watermarked text while maintaining strong
security, robustness, and the ability to detect watermarks without prior
knowledge of the prompt or model. This paper proposes an adaptive watermarking
strategy to address this problem. To improve the text quality and maintain
robustness, we adaptively add watermarking to token distributions with high
entropy measured using an auxiliary model and keep the low entropy token
distributions untouched. For the sake of security and to further minimize the
watermark's impact on text quality, instead of using a fixed green/red list
generated from a random secret key, which can be vulnerable to decryption and
forgery, we adaptively scale up the output logits in proportion based on the
semantic embedding of previously generated text using a well designed semantic
mapping model. Our experiments involving various LLMs demonstrate that our
approach achieves comparable robustness performance to existing watermark
methods. Additionally, the text generated by our method has perplexity
comparable to that of \emph{un-watermarked} LLMs while maintaining security
even under various attacks.
\\ ( https://arxiv.org/abs/2401.13927 ,  4276kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13979
Date: Thu, 25 Jan 2024 06:45:32 GMT   (8610kb,D)

Title: Leeroo Orchestrator: Elevating LLMs Performance Through Model
  Integration
Authors: Alireza Mohammadshahi, Ali Shaikh, Majid Yazdani
Categories: cs.CL cs.AI cs.LG
\\
  In this paper, we propose an architecture to harness the collective knowledge
of multiple trained LLMs to create a new state-of-the-art. At the core of this
framework is a LLM-based orchestrator that is adept at picking the right
underlying LLM experts for optimal task execution. Inspired by self-play in
reinforcement learning, we created a loop of query generation, orchestration,
and evaluation to generate training data for the orchestrator. Our evaluation
focused on the MMLU benchmark, employing models with 7B, 13B, and 34B
parameters available on Hugging Face. The results demonstrate new
state-of-the-art open-source models: Our Leeroo orchestrator achieves
performance on par with the Mixtral model while incurring only two-thirds of
its cost. Moreover, increasing the allowed cost surpasses Mixtral's accuracy by
over 5% at the same cost level, reaching an accuracy of 75.9%. Further
enhancements were observed when integrating GPT4 into the underlying model
pool. The Leeroo orchestrator nearly matches GPT4's performance at half the
cost and even exceeds GPT4's results with a 25% cost reduction. These findings
illustrate the potential of our architecture in creating state-of-the-art and
cost-effective LLMs by optimizing the synergy between multiple LLMs to achieve
superior performance outcomes.
\\ ( https://arxiv.org/abs/2401.13979 ,  8610kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13986
Date: Thu, 25 Jan 2024 07:04:30 GMT   (211kb,D)

Title: Towards Consistent Natural-Language Explanations via
  Explanation-Consistency Finetuning
Authors: Yanda Chen, Chandan Singh, Xiaodong Liu, Simiao Zuo, Bin Yu, He He,
  Jianfeng Gao
Categories: cs.CL cs.AI cs.LG
Comments: arXiv admin note: text overlap with arXiv:2307.08678
\\
  Large language models (LLMs) often generate convincing, fluent explanations.
However, different from humans, they often generate inconsistent explanations
on different inputs. For example, an LLM may generate the explanation "all
birds can fly" when answering the question "Can sparrows fly?" but meanwhile
answer "no" to the related question "Can penguins fly?". Explanations should be
consistent across related examples so that they allow a human to simulate the
LLM's decision process on multiple examples. We propose explanation-consistency
finetuning (EC-finetuning), a method that adapts LLMs to generate more
consistent natural-language explanations on related examples. EC-finetuning
involves finetuning LLMs on synthetic data that is carefully constructed to
contain consistent explanations. Across a variety of question-answering
datasets in various domains, EC-finetuning yields a 10.0% relative explanation
consistency improvement on four finetuning datasets, and generalizes to seven
out-of-distribution datasets not seen during finetuning (+4.5% relative). Code
is available at https://github.com/yandachen/explanation-consistency-finetuning .
\\ ( https://arxiv.org/abs/2401.13986 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13996
Date: Thu, 25 Jan 2024 07:47:49 GMT   (5367kb,D)

Title: Investigate-Consolidate-Exploit: A General Strategy for Inter-Task Agent
  Self-Evolution
Authors: Cheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin Cong, Yankai Lin,
  Yesai Wu, Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI
Comments: 18 pages, 5 figures
\\
  This paper introduces Investigate-Consolidate-Exploit (ICE), a novel strategy
for enhancing the adaptability and flexibility of AI agents through inter-task
self-evolution. Unlike existing methods focused on intra-task learning, ICE
promotes the transfer of knowledge between tasks for genuine self-evolution,
similar to human experience learning. The strategy dynamically investigates
planning and execution trajectories, consolidates them into simplified
workflows and pipelines, and exploits them for improved task execution. Our
experiments on the XAgent framework demonstrate ICE's effectiveness, reducing
API calls by as much as 80% and significantly decreasing the demand for the
model's capability. Specifically, when combined with GPT-3.5, ICE's performance
matches that of raw GPT-4 across various agent tasks. We argue that this
self-evolution approach represents a paradigm shift in agent design,
contributing to a more robust AI community and ecosystem, and moving a step
closer to full autonomy.
\\ ( https://arxiv.org/abs/2401.13996 ,  5367kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14003
Date: Thu, 25 Jan 2024 08:03:38 GMT   (291kb,D)

Title: ConstraintChecker: A Plugin for Large Language Models to Reason on
  Commonsense Knowledge Bases
Authors: Quyet V. Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, Yangqiu Song
Categories: cs.CL cs.AI
Comments: Proceedings of EACL 2024
\\
  Reasoning over Commonsense Knowledge Bases (CSKB), i.e. CSKB reasoning, has
been explored as a way to acquire new commonsense knowledge based on reference
knowledge in the original CSKBs and external prior knowledge. Despite the
advancement of Large Language Models (LLM) and prompt engineering techniques in
various reasoning tasks, they still struggle to deal with CSKB reasoning. One
of the problems is that it is hard for them to acquire explicit relational
constraints in CSKBs from only in-context exemplars, due to a lack of symbolic
reasoning capabilities (Bengio et al., 2021). To this end, we proposed
**ConstraintChecker**, a plugin over prompting techniques to provide and check
explicit constraints. When considering a new knowledge instance,
ConstraintChecker employs a rule-based module to produce a list of constraints,
then it uses a zero-shot learning module to check whether this knowledge
instance satisfies all constraints. The acquired constraint-checking result is
then aggregated with the output of the main prompting technique to produce the
final output. Experimental results on CSKB Reasoning benchmarks demonstrate the
effectiveness of our method by bringing consistent improvements over all
prompting methods. Codes and data are available at
\url{https://github.com/HKUST-KnowComp/ConstraintChecker}.
\\ ( https://arxiv.org/abs/2401.14003 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14011
Date: Thu, 25 Jan 2024 08:22:10 GMT   (4923kb,D)

Title: CMMU: A Benchmark for Chinese Multi-modal Multi-type Question
  Understanding and Reasoning
Authors: Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang,
  Qiannan Zhu, Hua Huang
Categories: cs.CL cs.AI cs.MM
\\
  Multi-modal large language models(MLLMs) have achieved remarkable progress
and demonstrated powerful knowledge comprehension and reasoning abilities.
However, the mastery of domain-specific knowledge, which is essential for
evaluating the intelligence of MLLMs, continues to be a challenge. Current
multi-modal benchmarks for domain-specific knowledge concentrate on
multiple-choice questions and are predominantly available in English, which
imposes limitations on the comprehensiveness of the evaluation. To this end, we
introduce CMMU, a novel benchmark for multi-modal and multi-type question
understanding and reasoning in Chinese. CMMU consists of 3,603 questions in 7
subjects, covering knowledge from primary to high school. The questions can be
categorized into 3 types: multiple-choice, multiple-response, and
fill-in-the-blank, bringing greater challenges to MLLMs. In addition, we
propose a rigorous evaluation strategy called ShiftCheck for assessing
multiple-choice questions. The strategy aims to reduce position bias, minimize
the influence of randomness on correctness, and perform a quantitative analysis
of position bias. We evaluate seven open-source MLLMs along with GPT4-V,
Gemini-Pro, and Qwen-VL-Plus. The results demonstrate that CMMU poses a
significant challenge to the recent MLLMs.
\\ ( https://arxiv.org/abs/2401.14011 ,  4923kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14016
Date: Thu, 25 Jan 2024 08:48:21 GMT   (1043kb,D)

Title: Towards Uncertainty-Aware Language Agent
Authors: Jiuzhou Han and Wray Buntine and Ehsan Shareghi
Categories: cs.CL
Comments: The code and data are at https://uala-agent.github.io. arXiv admin
  note: substantial text overlap with arXiv:2310.05915
\\
  While Language Agents have achieved promising success by placing Large
Language Models at the core of a more versatile design that dynamically
interacts with the external world, the existing approaches neglect the notion
of uncertainty during these interactions. We present the Uncertainty-Aware
Language Agent (UALA), a framework that orchestrates the interaction between
the agent and the external world using uncertainty quantification. Compared
with other well-known counterparts like ReAct, our extensive experiments across
3 representative tasks (HotpotQA, StrategyQA, MMLU) and various LLM sizes
demonstrates that UALA brings a significant improvement of performance, while
having a substantially lower reliance on the external world (i.e., reduced
number of tool calls and tokens). Our analyses provide various insights
including the great potential of UALA compared with agent fine-tuning, and
underscoring the unreliably of verbalised confidence of LLMs as a proxy for
uncertainty.
\\ ( https://arxiv.org/abs/2401.14016 ,  1043kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14019
Date: Thu, 25 Jan 2024 08:57:33 GMT   (8380kb,D)

Title: Unitxt: Flexible, Shareable and Reusable Data Preparation and Evaluation
  for Generative AI
Authors: Elron Bandel, Yotam Perlitz, Elad Venezian, Roni Friedman-Melamed,
  Ofir Arviv, Matan Orbach, Shachar Don-Yehyia, Dafna Sheinwald, Ariel Gera,
  Leshem Choshen, Michal Shmueli-Scheuer, Yoav Katz
Categories: cs.CL cs.AI
Comments: Submitted to NAACL demo track
\\
  In the dynamic landscape of generative NLP, traditional text processing
pipelines limit research flexibility and reproducibility, as they are tailored
to specific dataset, task, and model combinations. The escalating complexity,
involving system prompts, model-specific formats, instructions, and more, calls
for a shift to a structured, modular, and customizable solution. Addressing
this need, we present Unitxt, an innovative library for customizable textual
data preparation and evaluation tailored to generative language models. Unitxt
natively integrates with common libraries like HuggingFace and LM-eval-harness
and deconstructs processing flows into modular components, enabling easy
customization and sharing between practitioners. These components encompass
model-specific formats, task prompts, and many other comprehensive dataset
processing definitions. The Unitxt-Catalog centralizes these components,
fostering collaboration and exploration in modern textual data workflows.
Beyond being a tool, Unitxt is a community-driven platform, empowering users to
build, share, and advance their pipelines collaboratively. Join the Unitxt
community at https://github.com/IBM/unitxt!
\\ ( https://arxiv.org/abs/2401.14019 ,  8380kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14040
Date: Thu, 25 Jan 2024 09:36:58 GMT   (1180kb,D)

Title: (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection
Authors: Francesco Periti, Haim Dubossarsky, Nina Tahmasebi
Categories: cs.CL
Comments: Accepted to the Findings of EACL 2024
\\
  In the universe of Natural Language Processing, Transformer-based language
models like BERT and (Chat)GPT have emerged as lexical superheroes with great
power to solve open research problems. In this paper, we specifically focus on
the temporal problem of semantic change, and evaluate their ability to solve
two diachronic extensions of the Word-in-Context (WiC) task: TempoWiC and
HistoWiC. In particular, we investigate the potential of a novel, off-the-shelf
technology like ChatGPT (and GPT) 3.5 compared to BERT, which represents a
family of models that currently stand as the state-of-the-art for modeling
semantic change. Our experiments represent the first attempt to assess the use
of (Chat)GPT for studying semantic change. Our results indicate that ChatGPT
performs significantly worse than the foundational GPT version. Furthermore,
our results demonstrate that (Chat)GPT achieves slightly lower performance than
BERT in detecting long-term changes but performs significantly worse in
detecting short-term changes.
\\ ( https://arxiv.org/abs/2401.14040 ,  1180kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14043
Date: Thu, 25 Jan 2024 09:47:55 GMT   (204kb,D)

Title: Towards Goal-oriented Large Language Model Prompting: A Survey
Authors: Haochen Li, Jonathan Leung, Zhiqi Shen
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have shown prominent performance in various
downstream tasks in which prompt engineering plays a pivotal role in optimizing
LLMs' performance. This paper, not as an overview of current prompt engineering
methods, aims to highlight the limitation of designing prompts while holding an
anthropomorphic assumption that expects LLMs to think like humans. From our
review of 35 representative studies, we demonstrate that a goal-oriented prompt
formulation, which guides LLMs to follow established human logical thinking,
significantly improves the performance of LLMs. Furthermore, We introduce a
novel taxonomy that categorizes goal-oriented prompting methods into five
interconnected stages and we demonstrate the broad applicability of our
framework by summarizing ten applicable tasks. With four future directions
proposed, we hope to further emphasize and promote goal-oriented prompt
engineering.
\\ ( https://arxiv.org/abs/2401.14043 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14067
Date: Thu, 25 Jan 2024 10:43:00 GMT   (963kb)

Title: Ta'keed: The First Generative Fact-Checking System for Arabic Claims
Authors: Saud Althabiti, Mohammad Ammar Alsalka, and Eric Atwell
Categories: cs.CL cs.AI
Comments: 9 pages, conference paper
Journal-ref: VOLUME 14 NUMBER 01 2024
\\
  This paper introduces Ta'keed, an explainable Arabic automatic fact-checking
system. While existing research often focuses on classifying claims as "True"
or "False," there is a limited exploration of generating explanations for claim
credibility, particularly in Arabic. Ta'keed addresses this gap by assessing
claim truthfulness based on retrieved snippets, utilizing two main components:
information retrieval and LLM-based claim verification. We compiled the
ArFactEx, a testing gold-labelled dataset with manually justified references,
to evaluate the system. The initial model achieved a promising F1 score of 0.72
in the classification task. Meanwhile, the system's generated explanations are
compared with gold-standard explanations syntactically and semantically. The
study recommends evaluating using semantic similarities, resulting in an
average cosine similarity score of 0.76. Additionally, we explored the impact
of varying snippet quantities on claim classification accuracy, revealing a
potential correlation, with the model using the top seven hits outperforming
others with an F1 score of 0.77.
\\ ( https://arxiv.org/abs/2401.14067 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14109
Date: Thu, 25 Jan 2024 11:45:21 GMT   (1402kb,D)

Title: CompactifAI: Extreme Compression of Large Language Models using
  Quantum-Inspired Tensor Networks
Authors: Andrei Tomut, Saeed S. Jahromi, Sukhbinder Singh, Faysal Ishtiaq,
  Cesar Mu\~noz, Prabdeep Singh Bajaj, Ali Elborady, Gianni del Bimbo, Mehrazin
  Alizadeh, David Montero, Pablo Martin-Ramiro, Muhammad Ibrahim, Oussama
  Tahiri Alaoui, John Malcolm, Samuel Mugel, Roman Orus
Categories: cs.CL cs.AI cs.LG quant-ph
Comments: 4 pages, 3 figures
\\
  Large Language Models (LLMs) such as ChatGPT and LlaMA are advancing rapidly
in generative Artificial Intelligence (AI), but their immense size poses
significant challenges, such as huge training and inference costs, substantial
energy demands, and limitations for on-site deployment. Traditional compression
methods such as pruning, distillation, and low-rank approximation focus on
reducing the effective number of neurons in the network, while quantization
focuses on reducing the numerical precision of individual weights to reduce the
model size while keeping the number of neurons fixed. While these compression
methods have been relatively successful in practice, there's no compelling
reason to believe that truncating the number of neurons is an optimal strategy.
In this context, this paper introduces CompactifAI, an innovative LLM
compression approach using quantum-inspired Tensor Networks that focuses on the
model's correlation space instead, allowing for a more controlled, refined and
interpretable model compression. Our method is versatile and can be implemented
with - or on top of - other compression techniques. As a benchmark, we
demonstrate that CompactifAI alone enables compression of the LlaMA-2 7B model
to only $30\%$ of its original size while recovering over $90\%$ of the
original accuracy after a brief distributed retraining.
\\ ( https://arxiv.org/abs/2401.14109 ,  1402kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14113
Date: Thu, 25 Jan 2024 11:47:58 GMT   (300kb,D)

Title: On the Affinity, Rationality, and Diversity of Hierarchical Topic
  Modeling
Authors: Xiaobao Wu, Fengjun Pan, Thong Nguyen, Yichao Feng, Chaoqun Liu,
  Cong-Duy Nguyen, Anh Tuan Luu
Categories: cs.CL
Comments: Accepted to AAAI2024 conference
\\
  Hierarchical topic modeling aims to discover latent topics from a corpus and
organize them into a hierarchy to understand documents with desirable semantic
granularity. However, existing work struggles with producing topic hierarchies
of low affinity, rationality, and diversity, which hampers document
understanding. To overcome these challenges, we in this paper propose Transport
Plan and Context-aware Hierarchical Topic Model (TraCo). Instead of early
simple topic dependencies, we propose a transport plan dependency method. It
constrains dependencies to ensure their sparsity and balance, and also
regularizes topic hierarchy building with them. This improves affinity and
diversity of hierarchies. We further propose a context-aware disentangled
decoder. Rather than previously entangled decoding, it distributes different
semantic granularity to topics at different levels by disentangled decoding.
This facilitates the rationality of hierarchies. Experiments on benchmark
datasets demonstrate that our method surpasses state-of-the-art baselines,
effectively improving the affinity, rationality, and diversity of hierarchical
topic modeling with better performance on downstream tasks.
\\ ( https://arxiv.org/abs/2401.14113 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14135
Date: Thu, 25 Jan 2024 12:31:41 GMT   (8019kb,D)

Title: Convolutional Neural Networks can achieve binary bail judgement
  classification
Authors: Amit Barman, Devangan Roy, Debapriya Paul, Indranil Dutta, Shouvik
  Kumar Guha, Samir Karmakar, Sudip Kumar Naskar
Categories: cs.CL cs.CY cs.LG
Comments: Accepted on 20th International Conference on Natural Language
  Processing (ICON)
\\
  There is an evident lack of implementation of Machine Learning (ML) in the
legal domain in India, and any research that does take place in this domain is
usually based on data from the higher courts of law and works with English
data. The lower courts and data from the different regional languages of India
are often overlooked. In this paper, we deploy a Convolutional Neural Network
(CNN) architecture on a corpus of Hindi legal documents. We perform a bail
Prediction task with the help of a CNN model and achieve an overall accuracy of
93\% which is an improvement on the benchmark accuracy, set by Kapoor et al.
(2022), albeit in data from 20 districts of the Indian state of Uttar Pradesh.
\\ ( https://arxiv.org/abs/2401.14135 ,  8019kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14166
Date: Thu, 25 Jan 2024 13:20:47 GMT   (579kb,D)

Title: BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on
  Few-shot Inference via Debiased Domain Abstraction
Authors: Jiangmeng Li, Fei Song, Yifan Jin, Wenwen Qiang, Changwen Zheng,
  Fuchun Sun, Hui Xiong
Categories: cs.CL cs.AI
Comments: Accepted by ICLR2024
\\
  As a novel and effective fine-tuning paradigm based on large-scale
pre-trained language models (PLMs), prompt-tuning aims to reduce the gap
between downstream tasks and pre-training objectives. While prompt-tuning has
yielded continuous advancements in various tasks, such an approach still
remains a persistent defect: prompt-tuning methods fail to generalize to
specific few-shot patterns. From the perspective of distribution analyses, we
disclose that the intrinsic issues behind the phenomenon are the
over-multitudinous conceptual knowledge contained in PLMs and the abridged
knowledge for target downstream domains, which jointly result in that PLMs
mis-locate the knowledge distributions corresponding to the target domains in
the universal knowledge embedding space. To this end, we intuitively explore to
approximate the unabridged target domains of downstream tasks in a debiased
manner, and then abstract such domains to generate discriminative prompts,
thereby providing the de-ambiguous guidance for PLMs. Guided by such an
intuition, we propose a simple yet effective approach, namely BayesPrompt, to
learn prompts that contain the domain discriminative information against the
interference from domain-irrelevant knowledge. BayesPrompt primitively
leverages known distributions to approximate the debiased factual distributions
of target domains and further uniformly samples certain representative features
from the approximated distributions to generate the ultimate prompts for PLMs.
We provide theoretical insights with the connection to domain adaptation.
Empirically, our method achieves state-of-the-art performance on benchmarks.
\\ ( https://arxiv.org/abs/2401.14166 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14194
Date: Thu, 25 Jan 2024 14:07:34 GMT   (1168kb,D)

Title: Parameter-Efficient Conversational Recommender System as a Language
  Processing Task
Authors: Mathieu Ravaut, Hao Zhang, Lu Xu, Aixin Sun, Yong Liu
Categories: cs.CL
Comments: 9 pages, 4 figures, 7 tables, EACL 2024 conference
\\
  Conversational recommender systems (CRS) aim to recommend relevant items to
users by eliciting user preference through natural language conversation. Prior
work often utilizes external knowledge graphs for items' semantic information,
a language model for dialogue generation, and a recommendation module for
ranking relevant items. This combination of multiple components suffers from a
cumbersome training process, and leads to semantic misalignment issues between
dialogue generation and item recommendation. In this paper, we represent items
in natural language and formulate CRS as a natural language processing task.
Accordingly, we leverage the power of pre-trained language models to encode
items, understand user intent via conversation, perform item recommendation
through semantic matching, and generate dialogues. As a unified model, our
PECRS (Parameter-Efficient CRS), can be optimized in a single stage, without
relying on non-textual metadata such as a knowledge graph. Experiments on two
benchmark CRS datasets, ReDial and INSPIRED, demonstrate the effectiveness of
PECRS on recommendation and conversation. Our code is available at:
https://github.com/Ravoxsg/efficient_unified_crs.
\\ ( https://arxiv.org/abs/2401.14194 ,  1168kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14212
Date: Thu, 25 Jan 2024 14:53:30 GMT   (2339kb,D)

Title: Explicitly Representing Syntax Improves Sentence-to-layout Prediction of
  Unexpected Situations
Authors: Wolf Nuyts, Ruben Cartuyvels, Marie-Francine Moens
Categories: cs.CL
\\
  Recognizing visual entities in a natural language sentence and arranging them
in a 2D spatial layout require a compositional understanding of language and
space. This task of layout prediction is valuable in text-to-image synthesis as
it allows localized and controlled in-painting of the image. In this
comparative study it is shown that we can predict layouts from language
representations that implicitly or explicitly encode sentence syntax, if the
sentences mention similar entity-relationships to the ones seen during
training. To test compositional understanding, we collect a test set of
grammatically correct sentences and layouts describing compositions of entities
and relations that unlikely have been seen during training. Performance on this
test set substantially drops, showing that current models rely on correlations
in the training data and have difficulties in understanding the structure of
the input sentences. We propose a novel structural loss function that better
enforces the syntactic structure of the input sentence and show large
performance gains in the task of 2D spatial layout prediction conditioned on
text. The loss has the potential to be used in other generation tasks where a
tree-like structure underlies the conditioning modality. Code, trained models
and the USCOCO evaluation set will be made available via github.
\\ ( https://arxiv.org/abs/2401.14212 ,  2339kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14215
Date: Thu, 25 Jan 2024 14:54:33 GMT   (5819kb,D)

Title: Commonsense-augmented Memory Construction and Management in Long-term
  Conversations via Context-aware Persona Refinement
Authors: Hana Kim, Kai Tzu-iunn Ong, Seoyeon Kim, Dongha Lee, Jinyoung Yeo
Categories: cs.CL cs.AI
Comments: Accepted to EACL 2024
\\
  Memorizing and utilizing speakers' personas is a common practice for response
generation in long-term conversations. Yet, human-authored datasets often
provide uninformative persona sentences that hinder response quality. This
paper presents a novel framework that leverages commonsense-based persona
expansion to address such issues in long-term conversation. While prior work
focuses on not producing personas that contradict others, we focus on
transforming contradictory personas into sentences that contain rich speaker
information, by refining them based on their contextual backgrounds with
designed strategies. As the pioneer of persona expansion in multi-session
settings, our framework facilitates better response generation via human-like
persona refinement. The supplementary video of our work is available at
https://caffeine-15bbf.web.app/.
\\ ( https://arxiv.org/abs/2401.14215 ,  5819kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14228
Date: Thu, 25 Jan 2024 15:11:07 GMT   (385kb,D)

Title: Assessing the Portability of Parameter Matrices Trained by
  Parameter-Efficient Finetuning Methods
Authors: Mohammed Sabry and Anya Belz
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to Findings of EACL 2024. Camera ready version
\\
  As the cost of training ever larger language models has grown, so has the
interest in reusing previously learnt knowledge. Transfer learning methods have
shown how reusing non-task-specific knowledge can help in subsequent
task-specific learning. In this paper, we investigate the inverse: porting
whole functional modules that encode task-specific knowledge from one model to
another. We designed a study comprising 1,440 training/testing runs to test the
portability of modules trained by parameter-efficient finetuning (PEFT)
techniques, using sentiment analysis as an example task. We test portability in
a wide range of scenarios, involving different PEFT techniques and different
pretrained host models, among other dimensions. We compare the performance of
ported modules with that of equivalent modules trained (i) from scratch, and
(ii) from parameters sampled from the same distribution as the ported module.
We find that the ported modules far outperform the two alternatives tested, but
that there are interesting performance differences between the four PEFT
techniques. We conclude that task-specific knowledge in the form of
structurally modular sets of parameters as produced by PEFT techniques is
highly portable, but that degree of success depends on type of PEFT and on
differences between originating and receiving pretrained models.
\\ ( https://arxiv.org/abs/2401.14228 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14240
Date: Thu, 25 Jan 2024 15:28:07 GMT   (251kb,D)

Title: Enhanced Labeling Technique for Reddit Text and Fine-Tuned Longformer
  Models for Classifying Depression Severity in English and Luganda
Authors: Richard Kimera, Daniela N. Rim, Joseph Kirabira, Ubong Godwin Udomah,
  Heeyoul Choi
Categories: cs.CL cs.LG
Comments: In IEEE Proceedings of the 14th International Conference on ICT
  Convergence (ICTC), Jeju, Korea, October 2023
DOI: 10.1109/ICTC58733.2023.10393433
\\
  Depression is a global burden and one of the most challenging mental health
conditions to control. Experts can detect its severity early using the Beck
Depression Inventory (BDI) questionnaire, administer appropriate medication to
patients, and impede its progression. Due to the fear of potential
stigmatization, many patients turn to social media platforms like Reddit for
advice and assistance at various stages of their journey. This research
extracts text from Reddit to facilitate the diagnostic process. It employs a
proposed labeling approach to categorize the text and subsequently fine-tunes
the Longformer model. The model's performance is compared against baseline
models, including Naive Bayes, Random Forest, Support Vector Machines, and
Gradient Boosting. Our findings reveal that the Longformer model outperforms
the baseline models in both English (48%) and Luganda (45%) languages on a
custom-made dataset.
\\ ( https://arxiv.org/abs/2401.14240 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14242
Date: Thu, 25 Jan 2024 15:33:20 GMT   (692kb,D)

Title: Improving Natural Language Capability of Code Large Language Model
Authors: Wei Li and Daoguang Zan and Bei Guan and Ailun Yu and Xiaolin Chen and
  Yongji Wang
Categories: cs.CL
\\
  Code large language models (Code LLMs) have demonstrated remarkable
performance in code generation. Nonetheless, most existing works focus on
boosting code LLMs from the perspective of programming capabilities, while
their natural language capabilities receive less attention. To fill this gap,
we thus propose a novel framework, comprising two modules: AttentionExtractor,
which is responsible for extracting key phrases from the user's natural
language requirements, and AttentionCoder, which leverages these extracted
phrases to generate target code to solve the requirement. This framework
pioneers an innovative idea by seamlessly integrating code LLMs with
traditional natural language processing tools. To validate the effectiveness of
the framework, we craft a new code generation benchmark, called MultiNL-H,
covering five natural languages. Extensive experimental results demonstrate the
effectiveness of our proposed framework.
\\ ( https://arxiv.org/abs/2401.14242 ,  692kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14267
Date: Thu, 25 Jan 2024 16:01:49 GMT   (1049kb)

Title: Transformers and Cortical Waves: Encoders for Pulling In Context Across
  Time
Authors: Lyle Muller, Patricia S. Churchland, and Terrence J. Sejnowski
Categories: cs.CL cs.AI
Comments: 25 pages, 4 figures
\\
  The capabilities of transformer networks such as ChatGPT and other Large
Language Models (LLMs) have captured the world's attention. The crucial
computational mechanism underlying their performance relies on transforming a
complete input sequence - for example, all the words in a sentence into a long
"encoding vector" - that allows transformers to learn long-range temporal
dependencies in naturalistic sequences. Specifically, "self-attention" applied
to this encoding vector enhances temporal context in transformers by computing
associations between pairs of words in the input sequence. We suggest that
waves of neural activity, traveling across single cortical regions or across
multiple regions at the whole-brain scale, could implement a similar encoding
principle. By encapsulating recent input history into a single spatial pattern
at each moment in time, cortical waves may enable temporal context to be
extracted from sequences of sensory inputs, the same computational principle
used in transformers.
\\ ( https://arxiv.org/abs/2401.14267 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14280
Date: Thu, 25 Jan 2024 16:11:41 GMT   (7254kb,D)

Title: RomanSetu: Efficiently unlocking multilingual capabilities of Large
  Language Models models via Romanization
Authors: Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Ratish Puduppully,
  Anoop Kunchukuttan
Categories: cs.CL cs.AI
Comments: Work in progress
\\
  This study addresses the challenge of extending Large Language Models (LLMs)
to non-English languages, specifically those using non-Latin scripts. We
propose an innovative approach that utilizes the romanized form of text as an
interface for LLMs, hypothesizing that its frequent informal use and shared
tokens with English enhance cross-lingual alignment. Focusing on Hindi, we
demonstrate through Hindi-to-English translation and sentiment analysis tasks
that romanized text not only significantly improves inference efficiency due to
its lower fertility compared to native text but also achieves competitive
performance with limited pre-training. Additionally, our novel multi-script
prompting approach, which combines romanized and native texts, shows promise in
further enhancing task performance. These findings suggest the potential of
romanization in bridging the language gap for LLM applications, with future
work aimed at expanding this approach to more languages and tasks.
\\ ( https://arxiv.org/abs/2401.14280 ,  7254kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14295
Date: Thu, 25 Jan 2024 16:34:00 GMT   (2247kb,D)

Title: Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of
  Thoughts
Authors: Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger, Nils
  Blach, Piotr Nyczyk, Marcin Copik, Grzegorz Kwa\'sniewski, J\"urgen M\"uller,
  Lukas Gianinazzi, Ales Kubicek, Hubert Niewiadomski, Onur Mutlu, Torsten
  Hoefler
Categories: cs.CL cs.AI cs.LG
\\
  The field of natural language processing (NLP) has witnessed significant
progress in recent years, with a notable focus on improving large language
models' (LLM) performance through innovative prompting techniques. Among these,
prompt engineering coupled with structures has emerged as a promising paradigm,
with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts,
in which the overall LLM reasoning is guided by a structure such as a graph. As
illustrated with numerous examples, this paradigm significantly enhances the
LLM's capability to solve numerous tasks, ranging from logical or mathematical
reasoning to planning or creative writing. To facilitate the understanding of
this growing field and pave the way for future developments, we devise a
general blueprint for effective and efficient LLM reasoning schemes. For this,
we conduct an in-depth analysis of the prompt execution pipeline, clarifying
and clearly defining different concepts. We then build the first taxonomy of
structure-enhanced LLM reasoning schemes. We focus on identifying fundamental
classes of harnessed structures, and we analyze the representations of these
structures, algorithms executed with these structures, and many others. We
refer to these structures as reasoning topologies, because their representation
becomes to a degree spatial, as they are contained within the LLM context. Our
study compares existing prompting schemes using the proposed taxonomy,
discussing how certain design choices lead to different patterns in performance
and cost. We also outline theoretical underpinnings, relationships between
prompting and others parts of the LLM ecosystem such as knowledge bases, and
the associated research challenges. Our work will help to advance future prompt
engineering techniques.
\\ ( https://arxiv.org/abs/2401.14295 ,  2247kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14360
Date: Thu, 25 Jan 2024 18:06:19 GMT   (7304kb,D)

Title: A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis
  on Noisy Bengali Texts
Authors: Kazi Toufique Elahi, Tasnuva Binte Rahman, Shakil Shahriar, Samir
  Sarker, Md. Tanvir Rouf Shawon, G. M. Shahariar
Categories: cs.CL
Comments: Accepted in The 9th Workshop on Noisy and User-generated Text
  (W-NUT), 18th Conference of the European Chapter of the Association for
  Computational Linguistics (EACL 2024)
MSC-class: 68T50 (Primary)
ACM-class: I.2.7
\\
  While Bengali is considered a language with limited resources, sentiment
analysis has been a subject of extensive research in the literature.
Nevertheless, there is a scarcity of exploration into sentiment analysis
specifically in the realm of noisy Bengali texts. In this paper, we introduce a
dataset (NC-SentNoB) that we annotated manually to identify ten different types
of noise found in a pre-existing sentiment analysis dataset comprising of
around 15K noisy Bengali texts. At first, given an input noisy text, we
identify the noise type, addressing this as a multi-label classification task.
Then, we introduce baseline noise reduction methods to alleviate noise prior to
conducting sentiment analysis. Finally, we assess the performance of fine-tuned
sentiment analysis models with both noisy and noise-reduced texts to make
comparisons. The experimental findings indicate that the noise reduction
methods utilized are not satisfactory, highlighting the need for more suitable
noise reduction methods in future research endeavors. We have made the
implementation and dataset presented in this paper publicly available at
https://github.com/ktoufiquee/A-Comparative-Analysis-of-Noise-Reduction-Methods-in-Sentiment-Analysis-on-Noisy-Bengali-Texts
\\ ( https://arxiv.org/abs/2401.14360 ,  7304kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14367
Date: Thu, 25 Jan 2024 18:14:57 GMT   (425kb,D)

Title: Genie: Achieving Human Parity in Content-Grounded Datasets Generation
Authors: Asaf Yehudai, Boaz Carmeli, Yosi Mass, Ofir Arviv, Nathaniel Mills,
  Assaf Toledo, Eyal Shnarch, Leshem Choshen
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to ICLR24
\\
  The lack of high-quality data for content-grounded generation tasks has been
identified as a major obstacle to advancing these tasks. To address this gap,
we propose Genie, a novel method for automatically generating high-quality
content-grounded data. It consists of three stages: (a) Content Preparation,
(b) Generation: creating task-specific examples from the content (e.g.,
question-answer pairs or summaries). (c) Filtering mechanism aiming to ensure
the quality and faithfulness of the generated data. We showcase this
methodology by generating three large-scale synthetic data, making wishes, for
Long-Form Question-Answering (LFQA), summarization, and information extraction.
In a human evaluation, our generated data was found to be natural and of high
quality. Furthermore, we compare models trained on our data with models trained
on human-written data -- ELI5 and ASQA for LFQA and CNN-DailyMail for
Summarization. We show that our models are on par with or outperforming models
trained on human-generated data and consistently outperforming them in
faithfulness. Finally, we applied our method to create LFQA data within the
medical domain and compared a model trained on it with models trained on other
domains.
\\ ( https://arxiv.org/abs/2401.14367 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14373
Date: Thu, 25 Jan 2024 18:24:13 GMT   (93kb,D)

Title: TURNA: A Turkish Encoder-Decoder Language Model for Enhanced
  Understanding and Generation
Authors: G\"ok\c{c}e Uludo\u{g}an and Zeynep Yirmibe\c{s}o\u{g}lu Balal and
  Furkan Akkurt and Melik\c{s}ah T\"urker and Onur G\"ung\"or and Susan
  \"Usk\"udarl{\i}
Categories: cs.CL cs.AI cs.LG
\\
  The recent advances in natural language processing have predominantly favored
well-resourced English-centric models, resulting in a significant gap with
low-resource languages. In this work, we introduce the language model TURNA,
which is developed for the low-resource language Turkish and is capable of both
natural language understanding and generation tasks. TURNA is pretrained with
an encoder-decoder architecture based on the unified framework UL2 with a
diverse corpus that we specifically curated for this purpose. We evaluated
TURNA with three generation tasks and five understanding tasks for Turkish. The
results show that TURNA outperforms several multilingual models in both
understanding and generation tasks, and competes with monolingual Turkish
models in understanding tasks. TURNA is made available at
https://huggingface.co/boun-tabi-LMG/TURNA .
\\ ( https://arxiv.org/abs/2401.14373 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14400
Date: Thu, 25 Jan 2024 18:59:32 GMT   (41kb,D)

Title: Modular Adaptation of Multilingual Encoders to Written Swiss German
  Dialect
Authors: Jannis Vamvas, No\"emi Aepli, Rico Sennrich
Categories: cs.CL
Comments: First Workshop on Modular and Open Multilingual NLP (MOOMIN 2024)
\\
  Creating neural text encoders for written Swiss German is challenging due to
a dearth of training data combined with dialectal variation. In this paper, we
build on several existing multilingual encoders and adapt them to Swiss German
using continued pre-training. Evaluation on three diverse downstream tasks
shows that simply adding a Swiss German adapter to a modular encoder achieves
97.5% of fully monolithic adaptation performance. We further find that for the
task of retrieving Swiss German sentences given Standard German queries,
adapting a character-level model is more effective than the other adaptation
strategies. We release our code and the models trained for our experiments at
https://github.com/ZurichNLP/swiss-german-text-encoders
\\ ( https://arxiv.org/abs/2401.14400 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13713
Date: Wed, 24 Jan 2024 00:41:51 GMT   (3130kb,D)

Title: EMP: Effective Multidimensional Persistence for Graph Representation
  Learning
Authors: Ignacio Segovia-Dominguez, Yuzhou Chen, Cuneyt G. Akcora, Zhiwei Zhen,
  Murat Kantarcioglu, Yulia R. Gel, Baris Coskunuzer
Categories: cs.LG cs.AI cs.CG
Comments: arXiv admin note: text overlap with arXiv:2401.13157
Journal-ref: LoG 2023
\\
  Topological data analysis (TDA) is gaining prominence across a wide spectrum
of machine learning tasks that spans from manifold learning to graph
classification. A pivotal technique within TDA is persistent homology (PH),
which furnishes an exclusive topological imprint of data by tracing the
evolution of latent structures as a scale parameter changes. Present PH tools
are confined to analyzing data through a single filter parameter. However, many
scenarios necessitate the consideration of multiple relevant parameters to
attain finer insights into the data. We address this issue by introducing the
Effective Multidimensional Persistence (EMP) framework. This framework empowers
the exploration of data by simultaneously varying multiple scale parameters.
The framework integrates descriptor functions into the analysis process,
yielding a highly expressive data summary. It seamlessly integrates established
single PH summaries into multidimensional counterparts like EMP Landscapes,
Silhouettes, Images, and Surfaces. These summaries represent data's
multidimensional aspects as matrices and arrays, aligning effectively with
diverse ML models. We provide theoretical guarantees and stability proofs for
EMP summaries. We demonstrate EMP's utility in graph classification tasks,
showing its effectiveness. Results reveal that EMP enhances various single PH
descriptors, outperforming cutting-edge methods on multiple benchmark datasets.
\\ ( https://arxiv.org/abs/2401.13713 ,  3130kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13716
Date: Wed, 24 Jan 2024 08:14:20 GMT   (5346kb)

Title: Can I trust my fake data - A comprehensive quality assessment framework
  for synthetic tabular data in healthcare
Authors: Vibeke Binz Vallevik, Aleksandar Babic, Serena Elizabeth Marshall,
  Severin Elvatun, Helga Br{\o}gger, Sharmini Alagaratnam, Bj{\o}rn Edwin,
  Narasimha Raghavan Veeraragavan, Anne Kjersti Befring, Jan Franz Nyg{\aa}rd
Categories: cs.LG cs.AI
\\
  Ensuring safe adoption of AI tools in healthcare hinges on access to
sufficient data for training, testing and validation. In response to privacy
concerns and regulatory requirements, using synthetic data has been suggested.
Synthetic data is created by training a generator on real data to produce a
dataset with similar statistical properties. Competing metrics with differing
taxonomies for quality evaluation have been suggested, resulting in a complex
landscape. Optimising quality entails balancing considerations that make the
data fit for use, yet relevant dimensions are left out of existing frameworks.
We performed a comprehensive literature review on the use of quality evaluation
metrics on SD within the scope of tabular healthcare data and SD made using
deep generative methods. Based on this and the collective team experiences, we
developed a conceptual framework for quality assurance. The applicability was
benchmarked against a practical case from the Dutch National Cancer Registry.
We present a conceptual framework for quality assurance of SD for AI
applications in healthcare that aligns diverging taxonomies, expands on common
quality dimensions to include the dimensions of Fairness and Carbon footprint,
and proposes stages necessary to support real-life applications. Building trust
in synthetic data by increasing transparency and reducing the safety risk will
accelerate the development and uptake of trustworthy AI tools for the benefit
of patients. Despite the growing emphasis on algorithmic fairness and carbon
footprint, these metrics were scarce in the literature review. The overwhelming
focus was on statistical similarity using distance metrics while sequential
logic detection was scarce. A consensus-backed framework that includes all
relevant quality dimensions can provide assurance for safe and responsible
real-life applications of SD.
\\ ( https://arxiv.org/abs/2401.13716 ,  5346kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13744
Date: Wed, 24 Jan 2024 19:01:22 GMT   (3123kb,D)

Title: Conformal Prediction Sets Improve Human Decision Making
Authors: Jesse C. Cresswell, Yi Sui, Bhargava Kumar, No\"el Vouitsis
Categories: cs.LG cs.HC stat.ML
Comments: Code available at
  https://github.com/layer6ai-labs/hitl-conformal-prediction
\\
  In response to everyday queries, humans explicitly signal uncertainty and
offer alternative answers when they are unsure. Machine learning models that
output calibrated prediction sets through conformal prediction mimic this human
behaviour; larger sets signal greater uncertainty while providing alternatives.
In this work, we study the usefulness of conformal prediction sets as an aid
for human decision making by conducting a pre-registered randomized controlled
trial with conformal prediction sets provided to human subjects. With
statistical significance, we find that when humans are given conformal
prediction sets their accuracy on tasks improves compared to fixed-size
prediction sets with the same coverage guarantee. The results show that
quantifying model uncertainty with conformal prediction is helpful for
human-in-the-loop decision making and human-AI teams.
\\ ( https://arxiv.org/abs/2401.13744 ,  3123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13751
Date: Wed, 24 Jan 2024 19:12:37 GMT   (1709kb)

Title: A Systematic Approach to Robustness Modelling for Deep Convolutional
  Neural Networks
Authors: Charles Meyers, Mohammad Reza Saleh Sedghpour, Tommy L\"ofstedt, Erik
  Elmroth
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  Convolutional neural networks have shown to be widely applicable to a large
number of fields when large amounts of labelled data are available. The recent
trend has been to use models with increasingly larger sets of tunable
parameters to increase model accuracy, reduce model loss, or create more
adversarially robust models -- goals that are often at odds with one another.
In particular, recent theoretical work raises questions about the ability for
even larger models to generalize to data outside of the controlled train and
test sets. As such, we examine the role of the number of hidden layers in the
ResNet model, demonstrated on the MNIST, CIFAR10, CIFAR100 datasets. We test a
variety of parameters including the size of the model, the floating point
precision, and the noise level of both the training data and the model output.
To encapsulate the model's predictive power and computational cost, we provide
a method that uses induced failures to model the probability of failure as a
function of time and relate that to a novel metric that allows us to quickly
determine whether or not the cost of training a model outweighs the cost of
attacking it. Using this approach, we are able to approximate the expected
failure rate using a small number of specially crafted samples rather than
increasingly larger benchmark datasets. We demonstrate the efficacy of this
technique on both the MNIST and CIFAR10 datasets using 8-, 16-, 32-, and 64-bit
floating-point numbers, various data pre-processing techniques, and several
attacks on five configurations of the ResNet model. Then, using empirical
measurements, we examine the various trade-offs between cost, robustness,
latency, and reliability to find that larger models do not significantly aid in
adversarial robustness despite costing significantly more to train.
\\ ( https://arxiv.org/abs/2401.13751 ,  1709kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13756
Date: Wed, 24 Jan 2024 19:17:45 GMT   (214kb,D)

Title: NLICE: Synthetic Medical Record Generation for Effective Primary
  Healthcare Differential Diagnosis
Authors: Zaid Al-Ars, Obinna Agba, Zhuoran Guo, Christiaan Boerkamp, Ziyaad
  Jaber, Tareq Jaber
Categories: cs.LG
\\
  This paper offers a systematic method for creating medical knowledge-grounded
patient records for use in activities involving differential diagnosis.
Additionally, an assessment of machine learning models that can differentiate
between various conditions based on given symptoms is also provided. We use a
public disease-symptom data source called SymCat in combination with Synthea to
construct the patients records. In order to increase the expressive nature of
the synthetic data, we use a medically-standardized symptom modeling method
called NLICE to augment the synthetic data with additional contextual
information for each condition. In addition, Naive Bayes and Random Forest
models are evaluated and compared on the synthetic data. The paper shows how to
successfully construct SymCat-based and NLICE-based datasets. We also show
results for the effectiveness of using the datasets to train predictive disease
models. The SymCat-based dataset is able to train a Naive Bayes and Random
Forest model yielding a 58.8% and 57.1% Top-1 accuracy score, respectively. In
contrast, the NLICE-based dataset improves the results, with a Top-1 accuracy
of 82.0% and Top-5 accuracy values of more than 90% for both models. Our
proposed data generation approach solves a major barrier to the application of
artificial intelligence methods in the healthcare domain. Our novel NLICE
symptom modeling approach addresses the incomplete and insufficient information
problem in the current binary symptom representation approach. The NLICE code
is open sourced at https://github.com/guozhuoran918/NLICE.
\\ ( https://arxiv.org/abs/2401.13756 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13794
Date: Wed, 24 Jan 2024 20:24:32 GMT   (806kb)

Title: Traffic Pattern Classification in Smart Cities Using Deep Recurrent
  Neural Network
Authors: Ayad Ghany Ismaeel, Krishnadas Janardhanan, Manishankar Sankar,
  Yuvaraj Natarajan, Sarmad Nozad Mahmood, Sameer Alani, and Akram H. Shather
Categories: cs.LG
Comments: 18 pages, 6 figures, 3 tables
Journal-ref: sustainability 2023, 15, 14522
DOI: 10.3390/su151914522
\\
  This paper examines the use of deep recurrent neural networks to classify
traffic patterns in smart cities. We propose a novel approach to traffic
pattern classification based on deep recurrent neural networks, which can
effectively capture traffic patterns' dynamic and sequential features. The
proposed model combines convolutional and recurrent layers to extract features
from traffic pattern data and a SoftMax layer to classify traffic patterns.
Experimental results show that the proposed model outperforms existing methods
regarding accuracy, precision, recall, and F1 score. Furthermore, we provide an
in depth analysis of the results and discuss the implications of the proposed
model for smart cities. The results show that the proposed model can accurately
classify traffic patterns in smart cities with a precision of as high as 95%.
The proposed model is evaluated on a real world traffic pattern dataset and
compared with existing classification methods.
\\ ( https://arxiv.org/abs/2401.13794 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13796
Date: Wed, 24 Jan 2024 20:30:52 GMT   (87kb)

Title: Don't Push the Button! Exploring Data Leakage Risks in Machine Learning
  and Transfer Learning
Authors: Andrea Apicella, Francesco Isgr\`o, Roberto Prevete
Categories: cs.LG cs.AI
Comments: under rev
\\
  Machine Learning (ML) has revolutionized various domains, offering predictive
capabilities in several areas. However, with the increasing accessibility of ML
tools, many practitioners, lacking deep ML expertise, adopt a "push the button"
approach, utilizing user-friendly interfaces without a thorough understanding
of underlying algorithms. While this approach provides convenience, it raises
concerns about the reliability of outcomes, leading to challenges such as
incorrect performance evaluation. This paper addresses a critical issue in ML,
known as data leakage, where unintended information contaminates the training
data, impacting model performance evaluation. Users, due to a lack of
understanding, may inadvertently overlook crucial steps, leading to optimistic
performance estimates that may not hold in real-world scenarios. The
discrepancy between evaluated and actual performance on new data is a
significant concern. In particular, this paper categorizes data leakage in ML,
discussing how certain conditions can propagate through the ML workflow.
Furthermore, it explores the connection between data leakage and the specific
task being addressed, investigates its occurrence in Transfer Learning, and
compares standard inductive ML with transductive ML frameworks. The conclusion
summarizes key findings, emphasizing the importance of addressing data leakage
for robust and reliable ML applications.
\\ ( https://arxiv.org/abs/2401.13796 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13822
Date: Wed, 24 Jan 2024 21:47:13 GMT   (2660kb,D)

Title: Navigating Dataset Documentations in AI: A Large-Scale Analysis of
  Dataset Cards on Hugging Face
Authors: Xinyu Yang, Weixin Liang, James Zou
Categories: cs.LG cs.AI
Comments: Accepted to the main conference of ICLR 2024
\\
  Advances in machine learning are closely tied to the creation of datasets.
While data documentation is widely recognized as essential to the reliability,
reproducibility, and transparency of ML, we lack a systematic empirical
understanding of current dataset documentation practices. To shed light on this
question, here we take Hugging Face -- one of the largest platforms for sharing
and collaborating on ML models and datasets -- as a prominent case study. By
analyzing all 7,433 dataset documentation on Hugging Face, our investigation
provides an overview of the Hugging Face dataset ecosystem and insights into
dataset documentation practices, yielding 5 main findings: (1) The dataset card
completion rate shows marked heterogeneity correlated with dataset popularity.
(2) A granular examination of each section within the dataset card reveals that
the practitioners seem to prioritize Dataset Description and Dataset Structure
sections, while the Considerations for Using the Data section receives the
lowest proportion of content. (3) By analyzing the subsections within each
section and utilizing topic modeling to identify key topics, we uncover what is
discussed in each section, and underscore significant themes encompassing both
technical and social impacts, as well as limitations within the Considerations
for Using the Data section. (4) Our findings also highlight the need for
improved accessibility and reproducibility of datasets in the Usage sections.
(5) In addition, our human annotation evaluation emphasizes the pivotal role of
comprehensive dataset content in shaping individuals' perceptions of a dataset
card's overall quality. Overall, our study offers a unique perspective on
analyzing dataset documentation through large-scale data science analysis and
underlines the need for more thorough dataset documentation in machine learning
research.
\\ ( https://arxiv.org/abs/2401.13822 ,  2660kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13827
Date: Wed, 24 Jan 2024 21:57:55 GMT   (710kb,D)

Title: Traffic Learning and Proactive UAV Trajectory Planning for Data Uplink
  in Markovian IoT Models
Authors: Eslam Eldeeb, Mohammad Shehab and Hirley Alves
Categories: cs.LG cs.AI cs.NI
Journal-ref: IEEE Internet of Things Journal
DOI: 10.1109/JIOT.2023.3339514
\\
  The age of information (AoI) is used to measure the freshness of the data. In
IoT networks, the traditional resource management schemes rely on a message
exchange between the devices and the base station (BS) before communication
which causes high AoI, high energy consumption, and low reliability. Unmanned
aerial vehicles (UAVs) as flying BSs have many advantages in minimizing the
AoI, energy-saving, and throughput improvement. In this paper, we present a
novel learning-based framework that estimates the traffic arrival of IoT
devices based on Markovian events. The learning proceeds to optimize the
trajectory of multiple UAVs and their scheduling policy. First, the BS predicts
the future traffic of the devices. We compare two traffic predictors: the
forward algorithm (FA) and the long short-term memory (LSTM). Afterward, we
propose a deep reinforcement learning (DRL) approach to optimize the optimal
policy of each UAV. Finally, we manipulate the optimum reward function for the
proposed DRL approach. Simulation results show that the proposed algorithm
outperforms the random-walk (RW) baseline model regarding the AoI, scheduling
accuracy, and transmission power.
\\ ( https://arxiv.org/abs/2401.13827 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13835
Date: Wed, 24 Jan 2024 22:21:04 GMT   (1110kb,D)

Title: The Calibration Gap between Model and Human Confidence in Large Language
  Models
Authors: Mark Steyvers, Heliodoro Tejeda, Aakriti Kumar, Catarina Belem, Sheer
  Karny, Xinyue Hu, Lukas Mayer, Padhraic Smyth
Categories: cs.LG cs.AI cs.CL cs.HC
Comments: 27 pages, 10 figures
\\
  For large language models (LLMs) to be trusted by humans they need to be
well-calibrated in the sense that they can accurately assess and communicate
how likely it is that their predictions are correct. Recent work has focused on
the quality of internal LLM confidence assessments, but the question remains of
how well LLMs can communicate this internal model confidence to human users.
This paper explores the disparity between external human confidence in an LLM's
responses and the internal confidence of the model. Through experiments
involving multiple-choice questions, we systematically examine human users'
ability to discern the reliability of LLM outputs. Our study focuses on two key
areas: (1) assessing users' perception of true LLM confidence and (2)
investigating the impact of tailored explanations on this perception. The
research highlights that default explanations from LLMs often lead to user
overestimation of both the model's confidence and its' accuracy. By modifying
the explanations to more accurately reflect the LLM's internal confidence, we
observe a significant shift in user perception, aligning it more closely with
the model's actual confidence levels. This adjustment in explanatory approach
demonstrates potential for enhancing user trust and accuracy in assessing LLM
outputs. The findings underscore the importance of transparent communication of
confidence levels in LLMs, particularly in high-stakes applications where
understanding the reliability of AI-generated information is essential.
\\ ( https://arxiv.org/abs/2401.13835 ,  1110kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13843
Date: Wed, 24 Jan 2024 22:40:00 GMT   (55kb)

Title: Enumerating the k-fold configurations in multi-class classification
  problems
Authors: Attila Fazekas and Gyorgy Kovacs
Categories: cs.LG
\\
  K-fold cross-validation is a widely used tool for assessing classifier
performance. The reproducibility crisis faced by artificial intelligence partly
results from the irreproducibility of reported k-fold cross-validation-based
performance scores. Recently, we introduced numerical techniques to test the
consistency of claimed performance scores and experimental setups. In a crucial
use case, the method relies on the combinatorial enumeration of all k-fold
configurations, for which we proposed an algorithm in the binary classification
case.
\\ ( https://arxiv.org/abs/2401.13843 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13848
Date: Wed, 24 Jan 2024 23:11:11 GMT   (643kb,D)

Title: A V2X-based Privacy Preserving Federated Measuring and Learning System
Authors: Levente Alekszejenk\'o and Tadeusz Dobrowiecki
Categories: cs.LG cs.AI cs.CR stat.ML
Comments: 8 pages, 5 figures
MSC-class: 68T07, 68T42, 68P27, 68P25
ACM-class: I.2.6; I.2.11
\\
  Future autonomous vehicles (AVs) will use a variety of sensors that generate
a vast amount of data. Naturally, this data not only serves self-driving
algorithms; but can also assist other vehicles or the infrastructure in
real-time decision-making. Consequently, vehicles shall exchange their
measurement data over Vehicle-to-Everything (V2X) technologies. Moreover,
predicting the state of the road network might be beneficial too. With such a
prediction, we might mitigate road congestion, balance parking lot usage, or
optimize the traffic flow. That would decrease transportation costs as well as
reduce its environmental impact.
  In this paper, we propose a federated measurement and learning system that
provides real-time data to fellow vehicles over Vehicle-to-Vehicle (V2V)
communication while also operating a federated learning (FL) scheme over the
Vehicle-to-Network (V2N) link to create a predictive model of the
transportation network. As we are yet to have real-world AV data, we model it
with a non-IID (independent and identically distributed) dataset to evaluate
the capabilities of the proposed system in terms of performance and privacy.
Results indicate that the proposed FL scheme improves learning performance and
prevents eavesdropping at the aggregator server side.
\\ ( https://arxiv.org/abs/2401.13848 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13854
Date: Wed, 24 Jan 2024 23:35:29 GMT   (290kb,D)

Title: Embedding Attack Project (Work Report)
Authors: Jiameng Pu and Zafar Takhirov
Categories: cs.LG cs.CR
Comments: 13 pages, 5 figures
\\
  This report summarizes all the MIA experiments (Membership Inference Attacks)
of the Embedding Attack Project, including threat models, experimental setup,
experimental results, findings and discussion. Current results cover the
evaluation of two main MIA strategies (loss-based and embedding-based MIAs) on
6 AI models ranging from Computer Vision to Language Modelling. There are two
ongoing experiments on MIA defense and neighborhood-comparison embedding
attacks. These are ongoing projects.
  The current work on MIA and PIA can be summarized into six conclusions: (1)
Amount of overfitting is directly proportional to model's vulnerability; (2)
early embedding layers in the model are less susceptible to privacy leaks; (3)
Deeper model layers contain more membership information; (4) Models are more
vulnerable to MIA if both embeddings and corresponding training labels are
compromised; (5) it is possible to use pseudo-labels to increase the MIA
success; and (6) although MIA and PIA success rates are proportional, reducing
the MIA does not necessarily reduce the PIA.
\\ ( https://arxiv.org/abs/2401.13854 ,  290kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13858
Date: Wed, 24 Jan 2024 23:45:31 GMT   (10235kb,D)

Title: Inverse Molecular Design with Multi-Conditional Diffusion Guidance
Authors: Gang Liu, Jiaxin Xu, Tengfei Luo, Meng Jiang
Categories: cs.LG q-bio.BM
Comments: 20 pages, 8 figures, 7 tables
\\
  Inverse molecular design with diffusion models holds great potential for
advancements in material and drug discovery. Despite success in unconditional
molecule generation, integrating multiple properties such as synthetic score
and gas permeability as condition constraints into diffusion models remains
unexplored. We introduce multi-conditional diffusion guidance. The proposed
Transformer-based denoising model has a condition encoder that learns the
representations of numerical and categorical conditions. The denoising model,
consisting of a structure encoder-decoder, is trained for denoising under the
representation of conditions. The diffusion process becomes graph-dependent to
accurately estimate graph-related noise in molecules, unlike the previous
models that focus solely on the marginal distributions of atoms or bonds. We
extensively validate our model for multi-conditional polymer and small molecule
generation. Results demonstrate our superiority across metrics from
distribution learning to condition control for molecular properties. An inverse
polymer design task for gas separation with feedback from domain experts
further demonstrates its practical utility.
\\ ( https://arxiv.org/abs/2401.13858 ,  10235kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13872
Date: Thu, 25 Jan 2024 00:47:44 GMT   (1276kb,D)

Title: Edge Conditional Node Update Graph Neural Network for Multi-variate Time
  Series Anomaly Detection
Authors: Hayoung Jo and Seong-Whan Lee
Categories: cs.LG
\\
  With the rapid advancement in cyber-physical systems, the increasing number
of sensors has significantly complicated manual monitoring of system states.
Consequently, graph-based time-series anomaly detection methods have gained
attention due to their ability to explicitly represent relationships between
sensors. However, these methods often apply a uniform source node
representation across all connected target nodes, even when updating different
target node representations. Moreover, the graph attention mechanism, commonly
used to infer unknown graph structures, could constrain the diversity of source
node representations. In this paper, we introduce the Edge Conditional
Node-update Graph Neural Network (ECNU-GNN). Our model, equipped with an edge
conditional node update module, dynamically transforms source node
representations based on connected edges to represent target nodes aptly. We
validate performance on three real-world datasets: SWaT, WADI, and PSM. Our
model demonstrates 5.4%, 12.4%, and 6.0% higher performance, respectively,
compared to best F1 baseline models.
\\ ( https://arxiv.org/abs/2401.13872 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13898
Date: Thu, 25 Jan 2024 02:25:23 GMT   (221kb,D)

Title: Cross-Modal Prototype based Multimodal Federated Learning under Severely
  Missing Modality
Authors: Huy Q. Le, Chu Myaet Thwal, Yu Qiao, Ye Lin Tun, Minh N. H. Nguyen and
  Choong Seon Hong
Categories: cs.LG
Comments: 12 pages, 8 figures, 5 tables
\\
  Multimodal federated learning (MFL) has emerged as a decentralized machine
learning paradigm, allowing multiple clients with different modalities to
collaborate on training a machine learning model across diverse data sources
without sharing their private data. However, challenges, such as data
heterogeneity and severely missing modalities, pose crucial hindrances to the
robustness of MFL, significantly impacting the performance of global model. The
absence of a modality introduces misalignment during the local training phase,
stemming from zero-filling in the case of clients with missing modalities.
Consequently, achieving robust generalization in global model becomes
imperative, especially when dealing with clients that have incomplete data. In
this paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a
novel approach for MFL under severely missing modalities by conducting the
complete prototypes to provide diverse modality knowledge in modality-shared
level with the cross-modal regularization and modality-specific level with
cross-modal contrastive mechanism. Additionally, our approach introduces the
cross-modal alignment to provide regularization for modality-specific features,
thereby enhancing overall performance, particularly in scenarios involving
severely missing modalities. Through extensive experiments on three multimodal
datasets, we demonstrate the effectiveness of MFCPL in mitigating these
challenges and improving the overall performance.
\\ ( https://arxiv.org/abs/2401.13898 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13904
Date: Thu, 25 Jan 2024 02:48:44 GMT   (2008kb,D)

Title: Empowering Machines to Think Like Chemists: Unveiling Molecular
  Structure-Polarity Relationships with Hierarchical Symbolic Regression
Authors: Siyu Lou, Chengchun Liu, Yuntian Chen, Fanyang Mo
Categories: cs.LG cs.AI cs.DB stat.AP
Comments: 33 pages, 6 figures
\\
  Thin-layer chromatography (TLC) is a crucial technique in molecular polarity
analysis. Despite its importance, the interpretability of predictive models for
TLC, especially those driven by artificial intelligence, remains a challenge.
Current approaches, utilizing either high-dimensional molecular fingerprints or
domain-knowledge-driven feature engineering, often face a dilemma between
expressiveness and interpretability. To bridge this gap, we introduce
Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical
neural networks and symbolic regression. UHiSR automatically distills
chemical-intuitive polarity indices, and discovers interpretable equations that
link molecular structure to chromatographic behavior.
\\ ( https://arxiv.org/abs/2401.13904 ,  2008kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13912
Date: Thu, 25 Jan 2024 03:14:07 GMT   (281kb,D)

Title: A Survey of Deep Learning and Foundation Models for Time Series
  Forecasting
Authors: John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna,
  Subas Rana, I. Budak Arpinar, and Ninghao Liu
Categories: cs.LG
\\
  Deep Learning has been successfully applied to many application domains, yet
its advantages have been slow to emerge for time series forecasting. For
example, in the well-known Makridakis (M) Competitions, hybrids of traditional
statistical or machine learning techniques have only recently become the top
performers. With the recent architectural advances in deep learning being
applied to time series forecasting (e.g., encoder-decoders with attention,
transformers, and graph neural networks), deep learning has begun to show
significant advantages. Still, in the area of pandemic prediction, there remain
challenges for deep learning models: the time series is not long enough for
effective training, unawareness of accumulated scientific knowledge, and
interpretability of the model. To this end, the development of foundation
models (large deep learning models with extensive pre-training) allows models
to understand patterns and acquire knowledge that can be applied to new related
problems before extensive training data becomes available. Furthermore, there
is a vast amount of knowledge available that deep learning models can tap into,
including Knowledge Graphs and Large Language Models fine-tuned with scientific
domain knowledge. There is ongoing research examining how to utilize or inject
such knowledge into deep learning models. In this survey, several
state-of-the-art modeling techniques are reviewed, and suggestions for further
work are provided.
\\ ( https://arxiv.org/abs/2401.13912 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13913
Date: Thu, 25 Jan 2024 03:17:03 GMT   (1087kb,D)

Title: Spectral Clustering for Discrete Distributions
Authors: Zixiao Wang, Dong Qiao, Jicong Fan
Categories: cs.LG cs.AI stat.ML
\\
  Discrete distribution clustering (D2C) was often solved by Wasserstein
barycenter methods. These methods are under a common assumption that clusters
can be well represented by barycenters, which may not hold in many real
applications. In this work, we propose a simple yet effective framework based
on spectral clustering and distribution affinity measures (e.g., maximum mean
discrepancy and Wasserstein distance) for D2C. To improve the scalability, we
propose to use linear optimal transport to construct affinity matrices
efficiently on large datasets. We provide theoretical guarantees for the
success of the proposed methods in clustering distributions. Experiments on
synthetic and real data show that our methods outperform the baselines largely
in terms of both clustering accuracy and computational efficiency.
\\ ( https://arxiv.org/abs/2401.13913 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13920
Date: Thu, 25 Jan 2024 03:36:39 GMT   (5199kb,D)

Title: LocMoE: A Low-overhead MoE for Large Language Model Training
Authors: Jing Li, Zhijie Sun, Xuan He, Li Zeng, Yi Lin, Entong Li, Binfan
  Zheng, Rongqian Zhao, Xin Chen
Categories: cs.LG cs.AI cs.CL
\\
  The Mixtures-of-Experts (MoE) model is a widespread distributed and
integrated learning method for large language models (LLM), which is favored
due to its ability to sparsify and expand models efficiently. However, the
performance of MoE is limited by load imbalance and high latency of All-To-All
communication, along with relatively redundant computation owing to large
expert capacity. Load imbalance may result from existing routing policies that
consistently tend to select certain experts. The frequent inter-node
communication in the All-To-All procedure also significantly prolongs the
training time. To alleviate the above performance problems, we propose a novel
routing strategy that combines load balance and locality by converting partial
inter-node communication to that of intra-node. Notably, we elucidate that
there is a minimum threshold for expert capacity, calculated through the
maximal angular deviation between the gating weights of the experts and the
assigned tokens. We port these modifications on the PanGu-Sigma model based on
the MindSpore framework with multi-level routing and conduct experiments on
Ascend clusters. The experiment results demonstrate that the proposed LocMoE
reduces training time per epoch by 12.68% to 22.24% compared to classical
routers, such as hash router and switch router, without impacting the model
accuracy.
\\ ( https://arxiv.org/abs/2401.13920 ,  5199kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13923
Date: Thu, 25 Jan 2024 03:42:00 GMT   (3422kb,D)

Title: Towards 3D Molecule-Text Interpretation in Language Models
Authors: Sihang Li, Zhiyuan Liu, Yanchen Luo, Xiang Wang, Xiangnan He, Kenji
  Kawaguchi, Tat-Seng Chua, Qi Tian
Categories: cs.LG cs.IR q-bio.BM
\\
  Language Models (LMs) have greatly influenced diverse domains. However, their
inherent limitation in comprehending 3D molecular structures has considerably
constrained their potential in the biomolecular domain. To bridge this gap, we
focus on 3D molecule-text interpretation, and propose 3D-MoLM: 3D-Molecular
Language Modeling. Specifically, 3D-MoLM enables an LM to interpret and analyze
3D molecules by equipping the LM with a 3D molecular encoder. This integration
is achieved by a 3D molecule-text projector, bridging the 3D molecular
encoder's representation space and the LM's input space. Moreover, to enhance
3D-MoLM's ability of cross-modal molecular understanding and instruction
following, we meticulously curated a 3D molecule-centric instruction tuning
dataset -- 3D-MoIT. Through 3D molecule-text alignment and 3D molecule-centric
instruction tuning, 3D-MoLM establishes an integration of 3D molecular encoder
and LM. It significantly surpasses existing baselines on downstream tasks,
including molecule-text retrieval, molecule captioning, and more challenging
open-text molecular QA tasks, especially focusing on 3D-dependent properties.
\\ ( https://arxiv.org/abs/2401.13923 ,  3422kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13929
Date: Thu, 25 Jan 2024 04:03:32 GMT   (1234kb,D)

Title: Reinforcement Learning with Hidden Markov Models for Discovering
  Decision-Making Dynamics
Authors: Xingche Guo, Donglin Zeng, Yuanjia Wang
Categories: cs.LG stat.AP stat.ME stat.ML
\\
  Major depressive disorder (MDD) presents challenges in diagnosis and
treatment due to its complex and heterogeneous nature. Emerging evidence
indicates that reward processing abnormalities may serve as a behavioral marker
for MDD. To measure reward processing, patients perform computer-based
behavioral tasks that involve making choices or responding to stimulants that
are associated with different outcomes. Reinforcement learning (RL) models are
fitted to extract parameters that measure various aspects of reward processing
to characterize how patients make decisions in behavioral tasks. Recent
findings suggest the inadequacy of characterizing reward learning solely based
on a single RL model; instead, there may be a switching of decision-making
processes between multiple strategies. An important scientific question is how
the dynamics of learning strategies in decision-making affect the reward
learning ability of individuals with MDD. Motivated by the probabilistic reward
task (PRT) within the EMBARC study, we propose a novel RL-HMM framework for
analyzing reward-based decision-making. Our model accommodates learning
strategy switching between two distinct approaches under a hidden Markov model
(HMM): subjects making decisions based on the RL model or opting for random
choices. We account for continuous RL state space and allow time-varying
transition probabilities in the HMM. We introduce a computationally efficient
EM algorithm for parameter estimation and employ a nonparametric bootstrap for
inference. We apply our approach to the EMBARC study to show that MDD patients
are less engaged in RL compared to the healthy controls, and engagement is
associated with brain activities in the negative affect circuitry during an
emotional conflict task.
\\ ( https://arxiv.org/abs/2401.13929 ,  1234kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13968
Date: Thu, 25 Jan 2024 06:03:56 GMT   (6063kb,D)

Title: Dynamic Long-Term Time-Series Forecasting via Meta Transformer Networks
Authors: Muhammad Anwar Ma'sum, MD Rasel Sarkar, Mahardhika Pratama, Savitha
  Ramasamy, Sreenatha Anavatti, Lin Liu, Habibullah, Ryszard Kowalczyk
Categories: cs.LG cs.AI
Comments: Under Consideration in IEEE Transactions on Artificial Intelligence
\\
  A reliable long-term time-series forecaster is highly demanded in practice
but comes across many challenges such as low computational and memory
footprints as well as robustness against dynamic learning environments. This
paper proposes Meta-Transformer Networks (MANTRA) to deal with the dynamic
long-term time-series forecasting tasks. MANTRA relies on the concept of fast
and slow learners where a collection of fast learners learns different aspects
of data distributions while adapting quickly to changes. A slow learner tailors
suitable representations to fast learners. Fast adaptations to dynamic
environments are achieved using the universal representation transformer layers
producing task-adapted representations with a small number of parameters. Our
experiments using four datasets with different prediction lengths demonstrate
the advantage of our approach with at least $3\%$ improvements over the
baseline algorithms for both multivariate and univariate settings. Source codes
of MANTRA are publicly available in
\url{https://github.com/anwarmaxsum/MANTRA}.
\\ ( https://arxiv.org/abs/2401.13968 ,  6063kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13977
Date: Thu, 25 Jan 2024 06:37:48 GMT   (3455kb)

Title: Evaluating the Determinants of Mode Choice Using Statistical and Machine
  Learning Techniques in the Indian Megacity of Bengaluru
Authors: Tanmay Ghosh and Nithin Nagaraj
Categories: cs.LG econ.GN q-fin.EC
Comments: 65 pages, 26 figures
\\
  The decision making involved behind the mode choice is critical for
transportation planning. While statistical learning techniques like discrete
choice models have been used traditionally, machine learning (ML) models have
gained traction recently among the transportation planners due to their higher
predictive performance. However, the black box nature of ML models pose
significant interpretability challenges, limiting their practical application
in decision and policy making. This study utilised a dataset of $1350$
households belonging to low and low-middle income bracket in the city of
Bengaluru to investigate mode choice decision making behaviour using
Multinomial logit model and ML classifiers like decision trees, random forests,
extreme gradient boosting and support vector machines. In terms of accuracy,
random forest model performed the best ($0.788$ on training data and $0.605$ on
testing data) compared to all the other models. This research has adopted
modern interpretability techniques like feature importance and individual
conditional expectation plots to explain the decision making behaviour using ML
models. A higher travel costs significantly reduce the predicted probability of
bus usage compared to other modes (a $0.66\%$ and $0.34\%$ reduction using
Random Forests and XGBoost model for $10\%$ increase in travel cost). However,
reducing travel time by $10\%$ increases the preference for the metro ($0.16\%$
in Random Forests and 0.42% in XGBoost). This research augments the ongoing
research on mode choice analysis using machine learning techniques, which would
help in improving the understanding of the performance of these models with
real-world data in terms of both accuracy and interpretability.
\\ ( https://arxiv.org/abs/2401.13977 ,  3455kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13987
Date: Thu, 25 Jan 2024 07:05:42 GMT   (387kb,D)

Title: Cross-Domain Few-Shot Learning via Adaptive Transformer Networks
Authors: Naeem Paeedeh, Mahardhika Pratama, Muhammad Anwar Ma'sum, Wolfgang
  Mayer, Zehong Cao, Ryszard Kowlczyk
Categories: cs.LG cs.AI
Comments: Under Consideration in Knowledge-based Systems
\\
  Most few-shot learning works rely on the same domain assumption between the
base and the target tasks, hindering their practical applications. This paper
proposes an adaptive transformer network (ADAPTER), a simple but effective
solution for cross-domain few-shot learning where there exist large domain
shifts between the base task and the target task. ADAPTER is built upon the
idea of bidirectional cross-attention to learn transferable features between
the two domains. The proposed architecture is trained with DINO to produce
diverse, and less biased features to avoid the supervision collapse problem.
Furthermore, the label smoothing approach is proposed to improve the
consistency and reliability of the predictions by also considering the
predicted labels of the close samples in the embedding space. The performance
of ADAPTER is rigorously evaluated in the BSCD-FSL benchmarks in which it
outperforms prior arts with significant margins.
\\ ( https://arxiv.org/abs/2401.13987 ,  387kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14021
Date: Thu, 25 Jan 2024 09:06:44 GMT   (2010kb,D)

Title: Accelerating Retrieval-Augmented Language Model Serving with Speculation
Authors: Zhihao Zhang, Alan Zhu, Lijie Yang, Yihua Xu, Lanting Li, Phitchaya
  Mangpo Phothilimthana, Zhihao Jia
Categories: cs.LG cs.CL cs.IR
Comments: Preprint
\\
  Retrieval-augmented language models (RaLM) have demonstrated the potential to
solve knowledge-intensive natural language processing (NLP) tasks by combining
a non-parametric knowledge base with a parametric language model. Instead of
fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to
the latest data and better source attribution mechanisms. Among various RaLM
approaches, iterative RaLM delivers a better generation quality due to a more
frequent interaction between the retriever and the language model. Despite the
benefits, iterative RaLM usually encounters high overheads due to the frequent
retrieval step. To this end, we propose RaLMSpec, a speculation-inspired
framework that provides generic speed-up over iterative RaLM while preserving
the same model outputs through speculative retrieval and batched verification.
By further incorporating prefetching, optimal speculation stride scheduler, and
asynchronous verification, RaLMSpec can automatically exploit the acceleration
potential to the fullest. For naive iterative RaLM serving, extensive
evaluations over three language models on four downstream QA datasets
demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x,
1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever,
approximate dense retriever, and sparse retriever respectively compared with
the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to
7.59x and 2.45x when the retriever is an exact dense retriever and approximate
dense retriever, respectively, compared with the baseline.
\\ ( https://arxiv.org/abs/2401.14021 ,  2010kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14027
Date: Thu, 25 Jan 2024 09:18:51 GMT   (816kb,D)

Title: The Risk of Federated Learning to Skew Fine-Tuning Features and
  Underperform Out-of-Distribution Robustness
Authors: Mengyao Du, Miao Zhang, Yuwen Pu, Kai Xu, Shouling Ji, Quanjun Yin
Categories: cs.LG
Comments: 12 pages, 10 figures
\\
  To tackle the scarcity and privacy issues associated with domain-specific
datasets, the integration of federated learning in conjunction with fine-tuning
has emerged as a practical solution. However, our findings reveal that
federated learning has the risk of skewing fine-tuning features and
compromising the out-of-distribution robustness of the model. By introducing
three robustness indicators and conducting experiments across diverse robust
datasets, we elucidate these phenomena by scrutinizing the diversity,
transferability, and deviation within the model feature space. To mitigate the
negative impact of federated learning on model robustness, we introduce GNP, a
\underline{G}eneral \underline{N}oisy \underline{P}rojection-based robust
algorithm, ensuring no deterioration of accuracy on the target distribution.
Specifically, the key strategy for enhancing model robustness entails the
transfer of robustness from the pre-trained model to the fine-tuned model,
coupled with adding a small amount of Gaussian noise to augment the
representative capacity of the model. Comprehensive experimental results
demonstrate that our approach markedly enhances the robustness across diverse
scenarios, encompassing various parameter-efficient fine-tuning methods and
confronting different levels of data heterogeneity.
\\ ( https://arxiv.org/abs/2401.14027 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14031
Date: Thu, 25 Jan 2024 09:21:29 GMT   (3579kb,D)

Title: Sparse and Transferable Universal Singular Vectors Attack
Authors: Kseniia Kuvshinova, Olga Tsymboi, Ivan Oseledets
Categories: cs.LG cs.CR cs.CV
\\
  The research in the field of adversarial attacks and models' vulnerability is
one of the fundamental directions in modern machine learning. Recent studies
reveal the vulnerability phenomenon, and understanding the mechanisms behind
this is essential for improving neural network characteristics and
interpretability. In this paper, we propose a novel sparse universal white-box
adversarial attack. Our approach is based on truncated power iteration
providing sparsity to $(p,q)$-singular vectors of the hidden layers of Jacobian
matrices. Using the ImageNet benchmark validation subset, we analyze the
proposed method in various settings, achieving results comparable to dense
baselines with more than a 50% fooling rate while damaging only 5% of pixels
and utilizing 256 samples for perturbation fitting. We also show that our
algorithm admits higher attack magnitude without affecting the human ability to
solve the task. Furthermore, we investigate that the constructed perturbations
are highly transferable among different models without significantly decreasing
the fooling rate. Our findings demonstrate the vulnerability of
state-of-the-art models to sparse attacks and highlight the importance of
developing robust machine learning systems.
\\ ( https://arxiv.org/abs/2401.14031 ,  3579kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14033
Date: Thu, 25 Jan 2024 09:23:31 GMT   (178kb)

Title: Novel Quadratic Constraints for Extending LipSDP beyond Slope-Restricted
  Activations
Authors: Patricia Pauli, Aaron Havens, Alexandre Araujo, Siddharth Garg,
  Farshad Khorrami, Frank Allg\"ower, Bin Hu
Categories: cs.LG
Comments: accepted as a conference paper at ICLR 2024
\\
  Recently, semidefinite programming (SDP) techniques have shown great promise
in providing accurate Lipschitz bounds for neural networks. Specifically, the
LipSDP approach (Fazlyab et al., 2019) has received much attention and provides
the least conservative Lipschitz upper bounds that can be computed with
polynomial time guarantees. However, one main restriction of LipSDP is that its
formulation requires the activation functions to be slope-restricted on
$[0,1]$, preventing its further use for more general activation functions such
as GroupSort, MaxMin, and Householder. One can rewrite MaxMin activations for
example as residual ReLU networks. However, a direct application of LipSDP to
the resultant residual ReLU networks is conservative and even fails in
recovering the well-known fact that the MaxMin activation is 1-Lipschitz. Our
paper bridges this gap and extends LipSDP beyond slope-restricted activation
functions. To this end, we provide novel quadratic constraints for GroupSort,
MaxMin, and Householder activations via leveraging their underlying properties
such as sum preservation. Our proposed analysis is general and provides a
unified approach for estimating $\ell_2$ and $\ell_\infty$ Lipschitz bounds for
a rich class of neural network architectures, including non-residual and
residual neural networks and implicit models, with GroupSort, MaxMin, and
Householder activations. Finally, we illustrate the utility of our approach
with a variety of experiments and show that our proposed SDPs generate less
conservative Lipschitz bounds in comparison to existing approaches.
\\ ( https://arxiv.org/abs/2401.14033 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14065
Date: Thu, 25 Jan 2024 10:39:40 GMT   (8679kb)

Title: Novel application of Relief Algorithm in cascaded artificial neural
  network to predict wind speed for wind power resource assessment in India
Authors: Hasmat Malik, Amit Kumar Yadav, Fausto Pedro Garc\'ia M\'arquez,
  Jes\'us Mar\'ia Pinar-P\'erez
Categories: cs.LG cs.SY eess.SY
Comments: Malik, H., Yadav, A. K., M\'arquez, F. P. G., & Pinar-P\'erez, J. M.
  (2022). Novel application of Relief Algorithm in cascaded artificial neural
  network to predict wind speed for wind power resource assessment in India.
  Energy Strategy Reviews, 41, 100864
Journal-ref: Energy Strategy Reviews 2022. Vol 41, 100864
DOI: 10.1016/j.esr.2022.100864
\\
  Wind power generated by wind has non-schedule nature due to stochastic nature
of meteorological variable. Hence energy business and control of wind power
generation requires prediction of wind speed (WS) from few seconds to different
time steps in advance. To deal with prediction shortcomings, various WS
prediction methods have been used. Predictive data mining offers variety of
methods for WS predictions where artificial neural network (ANN) is one of the
reliable and accurate methods. It is observed from the result of this study
that ANN gives better accuracy in comparison conventional model. The accuracy
of WS prediction models is found to be dependent on input parameters and
architecture type algorithms utilized. So the selection of most relevant input
parameters is important research area in WS predicton field. The objective of
the paper is twofold: first extensive review of ANN for wind power and WS
prediction is carried out. Discussion and analysis of feature selection using
Relief Algorithm (RA) in WS prediction are considered for different Indian
sites. RA identify atmospheric pressure, solar radiation and relative humidity
are relevant input variables. Based on relevant input variables Cascade ANN
model is developed and prediction accuracy is evaluated. It is found that root
mean square error (RMSE) for comparison between predicted and measured WS for
training and testing wind speed are found to be 1.44 m/s and 1.49 m/s
respectively. The developed cascade ANN model can be used to predict wind speed
for sites where there are not WS measuring instruments are installed in India.
\\ ( https://arxiv.org/abs/2401.14065 ,  8679kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14069
Date: Thu, 25 Jan 2024 10:44:50 GMT   (20313kb,D)

Title: Neural Sinkhorn Gradient Flow
Authors: Huminhao Zhu, Fangyikang Wang, Chao Zhang, Hanbin Zhao, Hui Qian
Categories: cs.LG
\\
  Wasserstein Gradient Flows (WGF) with respect to specific functionals have
been widely used in the machine learning literature. Recently, neural networks
have been adopted to approximate certain intractable parts of the underlying
Wasserstein gradient flow and result in efficient inference procedures. In this
paper, we introduce the Neural Sinkhorn Gradient Flow (NSGF) model, which
parametrizes the time-varying velocity field of the Wasserstein gradient flow
w.r.t. the Sinkhorn divergence to the target distribution starting a given
source distribution. We utilize the velocity field matching training scheme in
NSGF, which only requires samples from the source and target distribution to
compute an empirical velocity field approximation. Our theoretical analyses
show that as the sample size increases to infinity, the mean-field limit of the
empirical approximation converges to the true underlying velocity field. To
further enhance model efficiency on high-dimensional tasks, a two-phase NSGF++
model is devised, which first follows the Sinkhorn flow to approach the image
manifold quickly ($\le 5$ NFEs) and then refines the samples along a simple
straight flow. Numerical experiments with synthetic and real-world benchmark
datasets support our theoretical results and demonstrate the effectiveness of
the proposed methods.
\\ ( https://arxiv.org/abs/2401.14069 ,  20313kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14081
Date: Thu, 25 Jan 2024 11:00:19 GMT   (1335kb,D)

Title: Accelerating Fractional PINNs using Operational Matrices of Derivative
Authors: Tayebeh Taheri, Alireza Afzal Aghaei, Kourosh Parand
Categories: cs.LG cs.NA math.NA
Comments: 19 pages, 11 figures
\\
  This paper presents a novel operational matrix method to accelerate the
training of fractional Physics-Informed Neural Networks (fPINNs). Our approach
involves a non-uniform discretization of the fractional Caputo operator,
facilitating swift computation of fractional derivatives within Caputo-type
fractional differential problems with $0<\alpha<1$. In this methodology, the
operational matrix is precomputed, and during the training phase, automatic
differentiation is replaced with a matrix-vector product. While our methodology
is compatible with any network, we particularly highlight its successful
implementation in PINNs, emphasizing the enhanced accuracy achieved when
utilizing the Legendre Neural Block (LNB) architecture. LNB incorporates
Legendre polynomials into the PINN structure, providing a significant boost in
accuracy. The effectiveness of our proposed method is validated across diverse
differential equations, including Delay Differential Equations (DDEs) and
Systems of Differential Algebraic Equations (DAEs). To demonstrate its
versatility, we extend the application of the method to systems of differential
equations, specifically addressing nonlinear Pantograph fractional-order
DDEs/DAEs. The results are supported by a comprehensive analysis of numerical
outcomes.
\\ ( https://arxiv.org/abs/2401.14081 ,  1335kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14107
Date: Thu, 25 Jan 2024 11:43:35 GMT   (157kb,D)

Title: Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement
Authors: Aaqib Saeed, Dimitris Spathis, Jungwoo Oh, Edward Choi, Ali Etemad
Categories: cs.LG eess.SP
\\
  Wearable technologies enable continuous monitoring of various health metrics,
such as physical activity, heart rate, sleep, and stress levels. A key
challenge with wearable data is obtaining quality labels. Unlike modalities
like video where the videos themselves can be effectively used to label objects
or events, wearable data do not contain obvious cues about the physical
manifestation of the users and usually require rich metadata. As a result,
label noise can become an increasingly thorny issue when labeling such data. In
this paper, we propose a novel solution to address noisy label learning,
entitled Few-Shot Human-in-the-Loop Refinement (FHLR). Our method initially
learns a seed model using weak labels. Next, it fine-tunes the seed model using
a handful of expert corrections. Finally, it achieves better generalizability
and robustness by merging the seed and fine-tuned models via weighted parameter
averaging. We evaluate our approach on four challenging tasks and datasets, and
compare it against eight competitive baselines designed to deal with noisy
labels. We show that FHLR achieves significantly better performance when
learning from noisy labels and achieves state-of-the-art by a large margin,
with up to 19% accuracy improvement under symmetric and asymmetric noise.
Notably, we find that FHLR is particularly robust to increased label noise,
unlike prior works that suffer from severe performance degradation. Our work
not only achieves better generalization in high-stakes health sensing
benchmarks but also sheds light on how noise affects commonly-used models.
\\ ( https://arxiv.org/abs/2401.14107 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14110
Date: Thu, 25 Jan 2024 11:46:01 GMT   (542kb,D)

Title: Towards Cheaper Inference in Deep Networks with Lower Bit-Width
  Accumulators
Authors: Yaniv Blumenfeld, Itay Hubara, Daniel Soudry
Categories: cs.LG cs.AI cs.AR
\\
  The majority of the research on the quantization of Deep Neural Networks
(DNNs) is focused on reducing the precision of tensors visible by high-level
frameworks (e.g., weights, activations, and gradients). However, current
hardware still relies on high-accuracy core operations. Most significant is the
operation of accumulating products. This high-precision accumulation operation
is gradually becoming the main computational bottleneck. This is because, so
far, the usage of low-precision accumulators led to a significant degradation
in performance. In this work, we present a simple method to train and fine-tune
high-end DNNs, to allow, for the first time, utilization of cheaper, $12$-bits
accumulators, with no significant degradation in accuracy. Lastly, we show that
as we decrease the accumulation precision further, using fine-grained gradient
approximations can improve the DNN accuracy.
\\ ( https://arxiv.org/abs/2401.14110 ,  542kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14112
Date: Thu, 25 Jan 2024 11:46:38 GMT   (2147kb,D)

Title: FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric
  Algorithm-System Co-Design
Authors: Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen
  Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji
  Ruwase, Yuxiong He, Shuaiwen Leon Song
Categories: cs.LG cs.AI cs.AR
\\
  Six-bit quantization (FP6) can effectively reduce the size of large language
models (LLMs) and preserve the model quality consistently across varied
applications. However, existing systems do not provide Tensor Core support for
FP6 quantization and struggle to achieve practical performance improvements
during LLM inference. It is challenging to support FP6 quantization on GPUs due
to (1) unfriendly memory access of model weights with irregular bit-width and
(2) high runtime overhead of weight de-quantization. To address these problems,
we propose TC-FPx, the first full-stack GPU kernel design scheme with unified
Tensor Core support of float-point weights for various quantization bit-width.
We integrate TC-FPx kernel into an existing inference system, providing new
end-to-end support (called FP6-LLM) for quantized LLM inference, where better
trade-offs between inference cost and model quality are achieved. Experiments
show that FP6-LLM enables the inference of LLaMA-70b using only a single GPU,
achieving 1.69x-2.65x higher normalized inference throughput than the FP16
baseline. The source code will be publicly available soon.
\\ ( https://arxiv.org/abs/2401.14112 ,  2147kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14131
Date: Thu, 25 Jan 2024 12:23:22 GMT   (151kb,D)

Title: Equivariant Manifold Neural ODEs and Differential Invariants
Authors: Emma Andersdotter, Fredrik Ohlsson
Categories: cs.LG math.DS
Comments: 17 pages, 3 figures
\\
  In this paper we develop a manifestly geometric framework for equivariant
manifold neural ordinary differential equations (NODEs), and use it to analyse
their modelling capabilities for symmetric data. First, we consider the action
of a Lie group $G$ on a smooth manifold $M$ and establish the equivalence
between equivariance of vector fields, symmetries of the corresponding Cauchy
problems, and equivariance of the associated NODEs. We also propose a novel
formulation of the equivariant NODEs in terms of the differential invariants of
the action of $G$ on $M$, based on Lie theory for symmetries of differential
equations, which provides an efficient parameterisation of the space of
equivariant vector fields in a way that is agnostic to both the manifold $M$
and the symmetry group $G$. Second, we construct augmented manifold NODEs,
through embeddings into equivariant flows, and show that they are universal
approximators of equivariant diffeomorphisms on any path-connected $M$.
Furthermore, we show that the augmented NODEs can be incorporated in the
geometric framework and parameterised using higher order differential
invariants. Finally, we consider the induced action of $G$ on different fields
on $M$ and show how it can be used to generalise previous work, on, e.g.,
continuous normalizing flows, to equivariant models in any geometry.
\\ ( https://arxiv.org/abs/2401.14131 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14151
Date: Thu, 25 Jan 2024 13:03:20 GMT   (31930kb,D)

Title: True Knowledge Comes from Practice: Aligning LLMs with Embodied
  Environments via Reinforcement Learning
Authors: Weihao Tan, Wentao Zhang, Shanqi Liu, Longtao Zheng, Xinrun Wang, Bo
  An
Categories: cs.LG cs.AI cs.CL
Comments: Accepted by ICLR2024
\\
  Despite the impressive performance across numerous tasks, large language
models (LLMs) often fail in solving simple decision-making tasks due to the
misalignment of the knowledge in LLMs with environments. On the contrary,
reinforcement learning (RL) agents learn policies from scratch, which makes
them always align with environments but difficult to incorporate prior
knowledge for efficient explorations. To narrow the gap, we propose TWOSOME, a
novel general online framework that deploys LLMs as decision-making agents to
efficiently interact and align with embodied environments via RL without
requiring any prepared datasets or prior knowledge of the environments.
Firstly, we query the joint probabilities of each valid action with LLMs to
form behavior policies. Then, to enhance the stability and robustness of the
policies, we propose two normalization methods and summarize four prompt design
principles. Finally, we design a novel parameter-efficient training
architecture where the actor and critic share one frozen LLM equipped with
low-rank adapters (LoRA) updated by PPO. We conduct extensive experiments to
evaluate TWOSOME. i) TWOSOME exhibits significantly better sample efficiency
and performance compared to the conventional RL method, PPO, and prompt tuning
method, SayCan, in both classical decision-making environment, Overcooked, and
simulated household environment, VirtualHome. ii) Benefiting from LLMs'
open-vocabulary feature, TWOSOME shows superior generalization ability to
unseen tasks. iii) Under our framework, there is no significant loss of the
LLMs' original ability during online PPO finetuning.
\\ ( https://arxiv.org/abs/2401.14151 ,  31930kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14155
Date: Thu, 25 Jan 2024 13:07:34 GMT   (1132kb,D)

Title: Alleviating Structural Distribution Shift in Graph Anomaly Detection
Authors: Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng,
  Yongdong Zhang
Categories: cs.LG cs.AI
Comments: Accepted to WSDM 2023
DOI: 10.1145/3539597.3570377
\\
  Graph anomaly detection (GAD) is a challenging binary classification problem
due to its different structural distribution between anomalies and normal nodes
-- abnormal nodes are a minority, therefore holding high heterophily and low
homophily compared to normal nodes. Furthermore, due to various time factors
and the annotation preferences of human experts, the heterophily and homophily
can change across training and testing data, which is called structural
distribution shift (SDS) in this paper. The mainstream methods are built on
graph neural networks (GNNs), benefiting the classification of normals from
aggregating homophilous neighbors, yet ignoring the SDS issue for anomalies and
suffering from poor generalization.
  This work solves the problem from a feature view. We observe that the degree
of SDS varies between anomalies and normal nodes. Hence to address the issue,
the key lies in resisting high heterophily for anomalies meanwhile benefiting
the learning of normals from homophily. We tease out the anomaly features on
which we constrain to mitigate the effect of heterophilous neighbors and make
them invariant. We term our proposed framework as Graph Decomposition Network
(GDN). Extensive experiments are conducted on two benchmark datasets, and the
proposed framework achieves a remarkable performance boost in GAD, especially
in an SDS environment where anomalies have largely different structural
distribution across training and testing environments. Codes are open-sourced
in https://github.com/blacksingular/wsdm_GDN.
\\ ( https://arxiv.org/abs/2401.14155 ,  1132kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14192
Date: Thu, 25 Jan 2024 14:03:15 GMT   (2715kb,D)

Title: How Can Large Language Models Understand Spatial-Temporal Data?
Authors: Lei Liu, Shuo Yu, Runze Wang, Zhenxun Ma, Yanming Shen
Categories: cs.LG cs.CL
\\
  While Large Language Models (LLMs) dominate tasks like natural language
processing and computer vision, harnessing their power for spatial-temporal
forecasting remains challenging. The disparity between sequential text and
complex spatial-temporal data hinders this application. To address this issue,
this paper introduces STG-LLM, an innovative approach empowering LLMs for
spatial-temporal forecasting. We tackle the data mismatch by proposing: 1)
STG-Tokenizer: This spatial-temporal graph tokenizer transforms intricate graph
data into concise tokens capturing both spatial and temporal relationships; 2)
STG-Adapter: This minimalistic adapter, consisting of linear encoding and
decoding layers, bridges the gap between tokenized data and LLM comprehension.
By fine-tuning only a small set of parameters, it can effectively grasp the
semantics of tokens generated by STG-Tokenizer, while preserving the original
natural language understanding capabilities of LLMs. Extensive experiments on
diverse spatial-temporal benchmark datasets show that STG-LLM successfully
unlocks LLM potential for spatial-temporal forecasting. Remarkably, our
approach achieves competitive performance on par with dedicated SOTA methods.
\\ ( https://arxiv.org/abs/2401.14192 ,  2715kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14199
Date: Thu, 25 Jan 2024 14:21:14 GMT   (551kb,D)

Title: MTRGL:Effective Temporal Correlation Discerning through Multi-modal
  Temporal Relational Graph Learning
Authors: Junwei Su, Shan Wu, Jinhui Li
Categories: cs.LG econ.GN q-fin.EC q-fin.TR
\\
  In this study, we explore the synergy of deep learning and financial market
applications, focusing on pair trading. This market-neutral strategy is
integral to quantitative finance and is apt for advanced deep-learning
techniques. A pivotal challenge in pair trading is discerning temporal
correlations among entities, necessitating the integration of diverse data
modalities. Addressing this, we introduce a novel framework, Multi-modal
Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and
discrete features into a temporal graph and employs a memory-based temporal
graph neural network. This approach reframes temporal correlation
identification as a temporal graph link prediction task, which has shown
empirical success. Our experiments on real-world datasets confirm the superior
performance of MTRGL, emphasizing its promise in refining automated pair
trading strategies.
\\ ( https://arxiv.org/abs/2401.14199 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14210
Date: Thu, 25 Jan 2024 14:48:08 GMT   (41766kb,D)

Title: At the junction between deep learning and statistics of extremes:
  formalizing the landslide hazard definition
Authors: Ashok Dahal, Rapha\"el Huser, Luigi Lombardo
Categories: cs.LG physics.geo-ph stat.AP stat.ML
\\
  The most adopted definition of landslide hazard combines spatial information
about landslide location (susceptibility), threat (intensity), and frequency
(return period). Only the first two elements are usually considered and
estimated when working over vast areas. Even then, separate models constitute
the standard, with frequency being rarely investigated. Frequency and intensity
are intertwined and depend on each other because larger events occur less
frequently and vice versa. However, due to the lack of multi-temporal
inventories and joint statistical models, modelling such properties via a
unified hazard model has always been challenging and has yet to be attempted.
Here, we develop a unified model to estimate landslide hazard at the slope unit
level to address such gaps. We employed deep learning, combined with a model
motivated by extreme-value theory to analyse an inventory of 30 years of
observed rainfall-triggered landslides in Nepal and assess landslide hazard for
multiple return periods. We also use our model to further explore landslide
hazard for the same return periods under different climate change scenarios up
to the end of the century. Our results show that the proposed model performs
excellently and can be used to model landslide hazard in a unified manner.
Geomorphologically, we find that under both climate change scenarios (SSP245
and SSP885), landslide hazard is likely to increase up to two times on average
in the lower Himalayan regions while remaining the same in the middle Himalayan
region whilst decreasing slightly in the upper Himalayan region areas.
\\ ( https://arxiv.org/abs/2401.14210 ,  41766kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14211
Date: Thu, 25 Jan 2024 14:49:15 GMT   (478kb,D)

Title: Communication-Efficient Federated Learning through Adaptive Weight
  Clustering and Server-Side Distillation
Authors: Vasileios Tsouvalas. Aaqib Saeed, Tanir Ozcelebi and Nirvana Meratnia
Categories: cs.LG cs.DC
Comments: 9 pages, 2 figures, Accepted on ICASSP 2024
\\
  Federated Learning (FL) is a promising technique for the collaborative
training of deep neural networks across multiple devices while preserving data
privacy. Despite its potential benefits, FL is hindered by excessive
communication costs due to repeated server-client communication during
training. To address this challenge, model compression techniques, such as
sparsification and weight clustering are applied, which often require modifying
the underlying model aggregation schemes or involve cumbersome hyperparameter
tuning, with the latter not only adjusts the model's compression rate but also
limits model's potential for continuous improvement over growing data. In this
paper, we propose FedCompress, a novel approach that combines dynamic weight
clustering and server-side knowledge distillation to reduce communication costs
while learning highly generalizable models. Through a comprehensive evaluation
on diverse public datasets, we demonstrate the efficacy of our approach
compared to baselines in terms of communication costs and inference speed. We
will make our implementation public upon acceptance.
\\ ( https://arxiv.org/abs/2401.14211 ,  478kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14226
Date: Thu, 25 Jan 2024 15:06:40 GMT   (7264kb,D)

Title: Sample Efficient Reinforcement Learning by Automatically Learning to
  Compose Subtasks
Authors: Shuai Han, Mehdi Dastani, Shihan Wang
Categories: cs.LG
\\
  Improving sample efficiency is central to Reinforcement Learning (RL),
especially in environments where the rewards are sparse. Some recent approaches
have proposed to specify reward functions as manually designed or learned
reward structures whose integrations in the RL algorithms are claimed to
significantly improve the learning efficiency. Manually designed reward
structures can suffer from inaccuracy and existing automatically learning
methods are often computationally intractable for complex tasks. The
integration of inaccurate or partial reward structures in RL algorithms fail to
learn optimal policies. In this work, we propose an RL algorithm that can
automatically structure the reward function for sample efficiency, given a set
of labels that signify subtasks. Given such minimal knowledge about the task,
we train a high-level policy that selects optimal sub-tasks in each state
together with a low-level policy that efficiently learns to complete each
sub-task. We evaluate our algorithm in a variety of sparse-reward environments.
The experiment results show that our approach significantly outperforms the
state-of-art baselines as the difficulty of the task increases.
\\ ( https://arxiv.org/abs/2401.14226 ,  7264kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14255
Date: Thu, 25 Jan 2024 15:45:28 GMT   (587kb,D)

Title: Interpretable Solutions for Breast Cancer Diagnosis with Grammatical
  Evolution and Data Augmentation
Authors: Yumnah Hasan, Allan de Lima, Fatemeh Amerehi, Darian Reyes Fernandez
  de Bulnes, Patrick Healy, and Conor Ryan
Categories: cs.LG cs.NE
\\
  Medical imaging diagnosis increasingly relies on Machine Learning (ML)
models. This is a task that is often hampered by severely imbalanced datasets,
where positive cases can be quite rare. Their use is further compromised by
their limited interpretability, which is becoming increasingly important. While
post-hoc interpretability techniques such as SHAP and LIME have been used with
some success on so-called black box models, the use of inherently
understandable models makes such endeavors more fruitful. This paper addresses
these issues by demonstrating how a relatively new synthetic data generation
technique, STEM, can be used to produce data to train models produced by
Grammatical Evolution (GE) that are inherently understandable. STEM is a
recently introduced combination of the Synthetic Minority Oversampling
Technique (SMOTE), Edited Nearest Neighbour (ENN), and Mixup; it has previously
been successfully used to tackle both between class and within class imbalance
issues. We test our technique on the Digital Database for Screening Mammography
(DDSM) and the Wiscon- sin Breast Cancer (WBC) datasets and compare Area Under
the Curve (AUC) results with an ensemble of the top three performing
classifiers from a set of eight standard ML classifiers with varying degrees of
interpretability. We demonstrate that the GE-derived models present the best
AUC while still maintaining interpretable solutions.
\\ ( https://arxiv.org/abs/2401.14255 ,  587kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14343
Date: Thu, 25 Jan 2024 17:43:39 GMT   (4929kb,D)

Title: Class-attribute Priors: Adapting Optimization to Heterogeneity and
  Fairness Objective
Authors: Xuechen Zhang, Mingchen Li, Jiasi Chen, Christos Thrampoulidis, Samet
  Oymak
Categories: cs.LG cs.CY stat.ML
Comments: 15 pages, 8 figures
\\
  Modern classification problems exhibit heterogeneities across individual
classes: Each class may have unique attributes, such as sample size, label
quality, or predictability (easy vs difficult), and variable importance at
test-time. Without care, these heterogeneities impede the learning process,
most notably, when optimizing fairness objectives. Confirming this, under a
gaussian mixture setting, we show that the optimal SVM classifier for balanced
accuracy needs to be adaptive to the class attributes. This motivates us to
propose CAP: An effective and general method that generates a class-specific
learning strategy (e.g. hyperparameter) based on the attributes of that class.
This way, optimization process better adapts to heterogeneities. CAP leads to
substantial improvements over the naive approach of assigning separate
hyperparameters to each class. We instantiate CAP for loss function design and
post-hoc logit adjustment, with emphasis on label-imbalanced problems. We show
that CAP is competitive with prior art and its flexibility unlocks clear
benefits for fairness objectives beyond balanced accuracy. Finally, we evaluate
CAP on problems with label noise as well as weighted test objectives to
showcase how CAP can jointly adapt to different heterogeneities.
\\ ( https://arxiv.org/abs/2401.14343 ,  4929kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14351
Date: Thu, 25 Jan 2024 17:55:07 GMT   (941kb,D)

Title: ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language
  Models
Authors: Yao Fu, Leyang Xue, Yeqi Huang, Andrei-Octavian Brabete, Dmitrii
  Ustiugov, Yuvraj Patel, Luo Mai
Categories: cs.LG cs.DC
\\
  This paper presents ServerlessLLM, a locality-enhanced serverless inference
system for Large Language Models (LLMs). ServerlessLLM exploits the substantial
capacity and bandwidth of storage and memory devices available on GPU servers,
thereby reducing costly remote checkpoint downloads and achieving efficient
checkpoint loading. ServerlessLLM achieves this through three main
contributions: (i) fast LLM checkpoint loading via a novel loading-optimized
checkpoint format design, coupled with an efficient multi-tier checkpoint
loading system; (ii) locality-driven LLM inference with live migration, which
allows ServerlessLLM to effectively achieve locality-driven server allocation
while preserving the low latency of ongoing LLM inference; and (iii)
locality-aware server allocation, enabling ServerlessLLM to evaluate the status
of each server in a cluster and effectively schedule model startup time to
capitalize on local checkpoint placement. Our comprehensive experiments, which
include microbenchmarks and real-world traces, show that ServerlessLLM
surpasses state-of-the-art systems by 10 - 200X in latency performance when
running various LLM inference workloads.
\\ ( https://arxiv.org/abs/2401.14351 ,  941kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14361
Date: Thu, 25 Jan 2024 18:07:50 GMT   (5159kb,D)

Title: MoE-Infinity: Activation-Aware Expert Offloading for Efficient MoE
  Serving
Authors: Leyang Xue, Yao Fu, Zhan Lu, Luo Mai, Mahesh Marina
Categories: cs.LG cs.PF
\\
  This paper presents MoE-Infinity, a cost-efficient mixture-of-expert (MoE)
serving system that realizes activation-aware expert offloading. MoE-Infinity
features sequence-level expert activation tracing, a new approach adept at
identifying sparse activations and capturing the temporal locality of MoE
inference. By analyzing these traces, MoE-Infinity performs novel
activation-aware expert prefetching and caching, substantially reducing the
latency overheads usually associated with offloading experts for improved cost
performance. Extensive experiments in a cluster show that MoE-Infinity
outperforms numerous existing systems and approaches, reducing latency by 4 -
20X and decreasing deployment costs by over 8X for various MoEs. MoE-Infinity's
source code is publicly available at https://github.com/TorchMoE/MoE-Infinity
\\ ( https://arxiv.org/abs/2401.14361 ,  5159kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14381
Date: Thu, 25 Jan 2024 18:36:10 GMT   (3462kb,D)

Title: Manifold GCN: Diffusion-based Convolutional Neural Network for
  Manifold-valued Graphs
Authors: Martin Hanik and Gabriele Steidl and Christoph von Tycowicz
Categories: cs.LG math.DG
MSC-class: 53Z50
ACM-class: I.2.4
\\
  We propose two graph neural network layers for graphs with features in a
Riemannian manifold. First, based on a manifold-valued graph diffusion
equation, we construct a diffusion layer that can be applied to an arbitrary
number of nodes and graph connectivity patterns. Second, we model a tangent
multilayer perceptron by transferring ideas from the vector neuron framework to
our general setting. Both layers are equivariant with respect to node
permutations and isometries of the feature manifold. These properties have been
shown to lead to a beneficial inductive bias in many deep learning tasks.
Numerical examples on synthetic data as well as on triangle meshes of the right
hippocampus to classify Alzheimer's disease demonstrate the very good
performance of our layers.
\\ ( https://arxiv.org/abs/2401.14381 ,  3462kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14388
Date: Thu, 25 Jan 2024 18:47:23 GMT   (268kb)

Title: Smooth Ranking SVM via Cutting-Plane Method
Authors: Erhan Can Ozcan, Berk G\"org\"ul\"u, Mustafa G. Baydogan, Ioannis Ch.
  Paschalidis
Categories: cs.LG
\\
  The most popular classification algorithms are designed to maximize
classification accuracy during training. However, this strategy may fail in the
presence of class imbalance since it is possible to train models with high
accuracy by overfitting to the majority class. On the other hand, the Area
Under the Curve (AUC) is a widely used metric to compare classification
performance of different algorithms when there is a class imbalance, and
various approaches focusing on the direct optimization of this metric during
training have been proposed. Among them, SVM-based formulations are especially
popular as this formulation allows incorporating different regularization
strategies easily. In this work, we develop a prototype learning approach that
relies on cutting-plane method, similar to Ranking SVM, to maximize AUC. Our
algorithm learns simpler models by iteratively introducing cutting planes, thus
overfitting is prevented in an unconventional way. Furthermore, it penalizes
the changes in the weights at each iteration to avoid large jumps that might be
observed in the test performance, thus facilitating a smooth learning process.
Based on the experiments conducted on 73 binary classification datasets, our
method yields the best test AUC in 25 datasets among its relevant competitors.
\\ ( https://arxiv.org/abs/2401.14388 ,  268kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.13672 (*cross-listing*)
Date: Tue, 7 Nov 2023 22:02:54 GMT   (37198kb,D)

Title: Transforming Agriculture with Intelligent Data Management and Insights
Authors: Yu Pan, Jianxin Sun, Hongfeng Yu, Geng Bai, Yufeng Ge, Joe Luck, Tala
  Awada
Categories: cs.DB cs.AI cs.IR
\\
  Modern agriculture faces grand challenges to meet increased demands for food,
fuel, feed, and fiber with population growth under the constraints of climate
change and dwindling natural resources. Data innovation is urgently required to
secure and improve the productivity, sustainability, and resilience of our
agroecosystems. As various sensors and Internet of Things (IoT) instrumentation
become more available, affordable, reliable, and stable, it has become possible
to conduct data collection, integration, and analysis at multiple temporal and
spatial scales, in real-time, and with high resolutions. At the same time, the
sheer amount of data poses a great challenge to data storage and analysis, and
the \textit{de facto} data management and analysis practices adopted by
scientists have become increasingly inefficient. Additionally, the data
generated from different disciplines, such as genomics, phenomics, environment,
agronomy, and socioeconomic, can be highly heterogeneous. That is, datasets
across disciplines often do not share the same ontology, modality, or format.
All of the above make it necessary to design a new data management
infrastructure that implements the principles of Findable, Accessible,
Interoperable, and Reusable (FAIR). In this paper, we propose Agriculture Data
Management and Analytics (ADMA), which satisfies the FAIR principles. Our new
data management infrastructure is intelligent by supporting semantic data
management across disciplines, interactive by providing various data
management/analysis portals such as web GUI, command line, and API, scalable by
utilizing the power of high-performance computing (HPC), extensible by allowing
users to load their own data analysis tools, trackable by keeping track of
different operations on each file, and open by using a rich set of mature open
source technologies.
\\ ( https://arxiv.org/abs/2401.13672 ,  37198kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13677 (*cross-listing*)
Date: Thu, 30 Nov 2023 12:09:14 GMT   (512kb,D)

Title: Process Mining for Unstructured Data: Challenges and Research Directions
Authors: Agnes Koschmider, Milda Aleknonyt\.e-Resch, Frederik Fonger, Christian
  Imenkamp, Arvid Lepsien, Kaan Apaydin, Maximilian Harms, Dominik Janssen,
  Dominic Langhammer, Tobias Ziolkowski, Yorck Zisgen
Categories: cs.DB cs.AI cs.LG
\\
  The application of process mining for unstructured data might significantly
elevate novel insights into disciplines where unstructured data is a common
data format. To efficiently analyze unstructured data by process mining and to
convey confidence into the analysis result, requires bridging multiple
challenges. The purpose of this paper is to discuss these challenges, present
initial solutions and describe future research directions. We hope that this
article lays the foundations for future collaboration on this topic.
\\ ( https://arxiv.org/abs/2401.13677 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13693 (*cross-listing*)
Date: Mon, 15 Jan 2024 10:58:30 GMT   (836kb,D)

Title: Challenge design roadmap
Authors: Hugo Jair Escalante Balderas, Isabelle Guyon (LISN, TAU), Addison
  Howard, Walter Reade, Sebastien Treguer (TAU)
Categories: cs.OH cs.AI cs.HC
Journal-ref: AI Competitions and Benchmarks: The Science Behind the Contests,
  In press
\\
  Challenges can be seen as a type of game that motivates participants to solve
serious tasks. As a result, competition organizers must develop effective game
rules. However, these rules have multiple objectives beyond making the game
enjoyable for participants. These objectives may include solving real-world
problems, advancing scientific or technical areas, making scientific
discoveries, and educating the public. In many ways, creating a challenge is
similar to launching a product. It requires the same level of excitement and
rigorous testing, and the goal is to attract ''customers'' in the form of
participants. The process begins with a solid plan, such as a competition
proposal that will eventually be submitted to an international conference and
subjected to peer review. Although peer review does not guarantee quality, it
does force organizers to consider the impact of their challenge, identify
potential oversights, and generally improve its quality. This chapter provides
guidelines for creating a strong plan for a challenge. The material draws on
the preparation guidelines from organizations such as Kaggle 1 , ChaLearn 2 and
Tailor 3 , as well as the NeurIPS proposal template, which some of the authors
contributed to.
\\ ( https://arxiv.org/abs/2401.13693 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13697 (*cross-listing*)
Date: Sat, 20 Jan 2024 04:46:43 GMT   (7207kb,D)

Title: Toward Robust Multimodal Learning using Multimodal Foundational Models
Authors: Xianbing Zhao, Soujanya Poria, Xuejiao Li, Yixin Chen, Buzhou Tang
Categories: cs.CV cs.AI cs.CL
Comments: Under Review
\\
  Existing multimodal sentiment analysis tasks are highly rely on the
assumption that the training and test sets are complete multimodal data, while
this assumption can be difficult to hold: the multimodal data are often
incomplete in real-world scenarios. Therefore, a robust multimodal model in
scenarios with randomly missing modalities is highly preferred. Recently,
CLIP-based multimodal foundational models have demonstrated impressive
performance on numerous multimodal tasks by learning the aligned cross-modal
semantics of image and text pairs, but the multimodal foundational models are
also unable to directly address scenarios involving modality absence. To
alleviate this issue, we propose a simple and effective framework, namely TRML,
Toward Robust Multimodal Learning using Multimodal Foundational Models. TRML
employs generated virtual modalities to replace missing modalities, and aligns
the semantic spaces between the generated and missing modalities. Concretely,
we design a missing modality inference module to generate virtual modaliites
and replace missing modalities. We also design a semantic matching learning
module to align semantic spaces generated and missing modalities. Under the
prompt of complete modality, our model captures the semantics of missing
modalities by leveraging the aligned cross-modal semantic space. Experiments
demonstrate the superiority of our approach on three multimodal sentiment
analysis benchmark datasets, CMU-MOSI, CMU-MOSEI, and MELD.
\\ ( https://arxiv.org/abs/2401.13697 ,  7207kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13699 (*cross-listing*)
Date: Mon, 22 Jan 2024 03:17:41 GMT   (14406kb,D)

Title: Generative AI-Driven Human Digital Twin in IoT-Healthcare: A
  Comprehensive Survey
Authors: Jiayuan Chen, You Shi, Changyan Yi, Hongyang Du, Jiawen Kang, Dusit
  Niyato
Categories: cs.HC cs.AI cs.LG
\\
  The Internet of things (IoT) can significantly enhance the quality of human
life, specifically in healthcare, attracting extensive attentions to
IoT-healthcare services. Meanwhile, the human digital twin (HDT) is proposed as
an innovative paradigm that can comprehensively characterize the replication of
the individual human body in the digital world and reflect its physical status
in real time. Naturally, HDT is envisioned to empower IoT-healthcare beyond the
application of healthcare monitoring by acting as a versatile and vivid human
digital testbed, simulating the outcomes and guiding the practical treatments.
However, successfully establishing HDT requires high-fidelity virtual modeling
and strong information interactions but possibly with scarce, biased and noisy
data. Fortunately, a recent popular technology called generative artificial
intelligence (GAI) may be a promising solution because it can leverage advanced
AI algorithms to automatically create, manipulate, and modify valuable while
diverse data. This survey particularly focuses on the implementation of
GAI-driven HDT in IoT-healthcare. We start by introducing the background of
IoT-healthcare and the potential of GAI-driven HDT. Then, we delve into the
fundamental techniques and present the overall framework of GAI-driven HDT.
After that, we explore the realization of GAI-driven HDT in detail, including
GAI-enabled data acquisition, communication, data management, digital modeling,
and data analysis. Besides, we discuss typical IoT-healthcare applications that
can be revolutionized by GAI-driven HDT, namely personalized health monitoring
and diagnosis, personalized prescription, and personalized rehabilitation.
Finally, we conclude this survey by highlighting some future research
directions.
\\ ( https://arxiv.org/abs/2401.13699 ,  14406kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13700 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:48:51 GMT   (36kb,D)

Title: Towards Automated Readable Proofs of Ruler and Compass Constructions
Authors: Vesna Marinkovi\'c (Faculty of Mathematics, University of Belgrade),
  Tijana \v{S}ukilovi\'c (Faculty of Mathematics, University of Belgrade),
  Filip Mari\'c (Faculty of Mathematics, University of Belgrade)
Categories: cs.LO cs.AI
Comments: In Proceedings ADG 2023, arXiv:2401.10725
ACM-class: F.4.1
Journal-ref: EPTCS 398, 2024, pp. 11-20
DOI: 10.4204/EPTCS.398.5
\\
  Although there are several systems that successfully generate construction
steps for ruler and compass construction problems, none of them provides
readable synthetic correctness proofs for generated constructions. In the
present work, we demonstrate how our triangle construction solver ArgoTriCS can
cooperate with automated theorem provers for first order logic and coherent
logic so that it generates construction correctness proofs, that are both
human-readable and formal (can be checked by interactive theorem provers such
as Coq or Isabelle/HOL). These proofs currently rely on many high-level lemmas
and our goal is to have them all formally shown from the basic axioms of
geometry.
\\ ( https://arxiv.org/abs/2401.13700 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13703 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:51:51 GMT   (914kb,D)

Title: Solving Some Geometry Problems of the N\'aboj 2023 Contest with
  Automated Deduction in GeoGebra Discovery
Authors: Amela Hota (The Private University College of Education of the Diocese
  of Linz, Austria), Zolt\'an Kov\'acs (The Private University College of
  Education of the Diocese of Linz, Austria), Alexander Vujic (The Private
  University College of Education of the Diocese of Linz, Austria)
Categories: math.HO cs.AI cs.CG cs.SC
Comments: In Proceedings ADG 2023, arXiv:2401.10725
Journal-ref: EPTCS 398, 2024, pp. 110-123
DOI: 10.4204/EPTCS.398.14
\\
  In this article, we solve some of the geometry problems of the N\'aboj 2023
competition with the help of a computer, using examples that the software tool
GeoGebra Discovery can calculate. In each case, the calculation requires
symbolic computations. We analyze the difficulty of feeding the problem into
the machine and set further goals to make the problems of this type of contests
even more tractable in the future.
\\ ( https://arxiv.org/abs/2401.13703 ,  914kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13704 (*cross-listing*)
Date: Mon, 22 Jan 2024 12:52:07 GMT   (501kb,D)

Title: Using Java Geometry Expert as Guide in the Preparations for Math
  Contests
Authors: Ines Ganglmayr (The Private University College of Education of the
  Diocese of Linz, Austria), Zolt\'an Kov\'acs (The Private University College
  of Education of the Diocese of Linz, Austria)
Categories: cs.CY cs.AI cs.CG cs.SC
Comments: In Proceedings ADG 2023, arXiv:2401.10725
Journal-ref: EPTCS 398, 2024, pp. 124-131
DOI: 10.4204/EPTCS.398.15
\\
  We give an insight into Java Geometry Expert (JGEX) in use in a school
context, focusing on the Austrian school system. JGEX can offer great support
in some classroom situations, especially for solving mathematical competition
tasks. Also, we discuss some limitations of the program.
\\ ( https://arxiv.org/abs/2401.13704 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13708 (*cross-listing*)
Date: Tue, 23 Jan 2024 12:59:40 GMT   (17687kb,D)

Title: Accelerating hyperbolic t-SNE
Authors: Martin Skrodzki, Hunter van Geffen, Nicolas F. Chaves-de-Plaza, Thomas
  H\"ollt, Elmar Eisemann, Klaus Hildebrandt
Categories: cs.HC cs.AI cs.LG q-bio.QM stat.ML
\\
  The need to understand the structure of hierarchical or high-dimensional data
is present in a variety of fields. Hyperbolic spaces have proven to be an
important tool for embedding computations and analysis tasks as their
non-linear nature lends itself well to tree or graph data. Subsequently, they
have also been used in the visualization of high-dimensional data, where they
exhibit increased embedding performance. However, none of the existing
dimensionality reduction methods for embedding into hyperbolic spaces scale
well with the size of the input data. That is because the embeddings are
computed via iterative optimization schemes and the computation cost of every
iteration is quadratic in the size of the input. Furthermore, due to the
non-linear nature of hyperbolic spaces, Euclidean acceleration structures
cannot directly be translated to the hyperbolic setting. This paper introduces
the first acceleration structure for hyperbolic embeddings, building upon a
polar quadtree. We compare our approach with existing methods and demonstrate
that it computes embeddings of similar quality in significantly less time.
Implementation and scripts for the experiments can be found at
https://graphics.tudelft.nl/accelerating-hyperbolic-tsne.
\\ ( https://arxiv.org/abs/2401.13708 ,  17687kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13722 (*cross-listing*)
Date: Wed, 24 Jan 2024 15:05:11 GMT   (457kb,D)

Title: Proactive Emotion Tracker: AI-Driven Continuous Mood and Emotion
  Monitoring
Authors: Mohammad Asif, Sudhakar Mishra, Ankush Sonker, Sanidhya Gupta, Somesh
  Kumar Maurya and Uma Shanker Tiwary
Categories: cs.HC cs.AI
\\
  This research project aims to tackle the growing mental health challenges in
today's digital age. It employs a modified pre-trained BERT model to detect
depressive text within social media and users' web browsing data, achieving an
impressive 93% test accuracy. Simultaneously, the project aims to incorporate
physiological signals from wearable devices, such as smartwatches and EEG
sensors, to provide long-term tracking and prognosis of mood disorders and
emotional states. This comprehensive approach holds promise for enhancing early
detection of depression and advancing overall mental health outcomes.
\\ ( https://arxiv.org/abs/2401.13722 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13758 (*cross-listing*)
Date: Wed, 24 Jan 2024 19:18:34 GMT   (20kb,D)

Title: Assumptions and Bounds in the Instrumental Variable Model
Authors: Thomas S. Richardson and James M. Robins
Categories: math.ST cs.AI stat.TH
Comments: 27 pages, 1 figure, 1 table. Proofs of Theorems 1 and 2 stated in
  Richardson and Robins (2014), arXiv:1410.0470
MSC-class: 62A01 (Primary) 62D20, 62H22 (Secondary)
\\
  In this note we give proofs for results relating to the Instrumental Variable
(IV) model with binary response $Y$ and binary treatment $X$, but with an
instrument $Z$ that takes $K$ states that were originally stated in Richardson
& Robins (2014), "ACE Bounds; SEMS with Equilibrium Conditions,"
arXiv:1410.0470.
\\ ( https://arxiv.org/abs/2401.13758 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13782 (*cross-listing*)
Date: Wed, 24 Jan 2024 20:05:49 GMT   (7319kb,D)

Title: Tweets to Citations: Unveiling the Impact of Social Media Influencers on
  AI Research Visibility
Authors: Iain Xie Weissburg, Mehir Arora, Liangming Pan, William Yang Wang
Categories: cs.DL cs.AI cs.CL cs.CV cs.LG cs.SI
\\
  As the number of accepted papers at AI and ML conferences reaches into the
thousands, it has become unclear how researchers access and read research
publications. In this paper, we investigate the role of social media
influencers in enhancing the visibility of machine learning research,
particularly the citation counts of papers they share. We have compiled a
comprehensive dataset of over 8,000 papers, spanning tweets from December 2018
to October 2023, alongside 1:1 matched controls based on publication year,
venue, and abstract topics. Our analysis reveals a significant increase in
citations for papers endorsed by these influencers, with median citation counts
2-3 times higher than those of the control group. Additionally, the study
delves into the geographic, gender, and institutional diversity of highlighted
authors. These findings highlight the expanding influence of social media in
scholarly communication and underscore the importance of an evolving ecosystem
in today's digital academic landscape.
\\ ( https://arxiv.org/abs/2401.13782 ,  7319kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13800 (*cross-listing*)
Date: Wed, 24 Jan 2024 20:41:25 GMT   (5491kb,D)

Title: Multi-Object Navigation in real environments using hybrid policies
Authors: Assem Sadek, Guillaume Bono, Boris Chidlovskii, Atilla Baskurt and
  Christian Wolf
Categories: cs.RO cs.AI
\\
  Navigation has been classically solved in robotics through the combination of
SLAM and planning. More recently, beyond waypoint planning, problems involving
significant components of (visual) high-level reasoning have been explored in
simulated environments, mostly addressed with large-scale machine learning, in
particular RL, offline-RL or imitation learning. These methods require the
agent to learn various skills like local planning, mapping objects and querying
the learned spatial representations. In contrast to simpler tasks like waypoint
planning (PointGoal), for these more complex tasks the current state-of-the-art
models have been thoroughly evaluated in simulation but, to our best knowledge,
not yet in real environments.
  In this work we focus on sim2real transfer. We target the challenging
Multi-Object Navigation (Multi-ON) task and port it to a physical environment
containing real replicas of the originally virtual Multi-ON objects. We
introduce a hybrid navigation method, which decomposes the problem into two
different skills: (1) waypoint navigation is addressed with classical SLAM
combined with a symbolic planner, whereas (2) exploration, semantic mapping and
goal retrieval are dealt with deep neural networks trained with a combination
of supervised learning and RL. We show the advantages of this approach compared
to end-to-end methods both in simulation and a real environment and outperform
the SOTA for this task.
\\ ( https://arxiv.org/abs/2401.13800 ,  5491kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13802 (*cross-listing*)
Date: Wed, 24 Jan 2024 20:43:36 GMT   (12329kb,D)

Title: Investigating the Efficacy of Large Language Models for Code Clone
  Detection
Authors: Mohamad Khajezade, Jie Wu, Fatemeh Hendijani Fard, Gema
  Rodr\'iguez-P\'erez, Mohamed Sami Shehata
Categories: cs.SE cs.AI cs.CL cs.LG
\\
  Large Language Models (LLMs) have demonstrated remarkable success in various
natural language processing and software engineering tasks, such as code
generation. The LLMs are mainly utilized in the prompt-based zero/few-shot
paradigm to guide the model in accomplishing the task. %\textbf{Goal:}
GPT-based models are one of the popular ones studied for tasks such as code
comment generation or test generation. These tasks are `generative' tasks.
However, there is limited research on the usage of LLMs for `non-generative'
tasks such as classification using the prompt-based paradigm. In this
preliminary exploratory study, we investigated the applicability of LLMs for
Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By
building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we
first investigated two different prompts using ChatGPT to detect
\textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a
zero-shot setting. We \textcolor{black}{then} conducted an analysis to
understand the strengths and weaknesses of ChatGPT in CCD. %\textbf{Results:}
ChatGPT surpasses the baselines in cross-language CCD
\textcolor{black}{attaining an F1-score of 0.877 } and achieves comparable
performance to fully fine-tuned models for mono-lingual CCD,
\textcolor{black}{with an F1-score of 0.878}. Also, the
\textcolor{black}{prompt and the} difficulty level of the problems has an
impact on the performance of ChatGPT. \textcolor{black}{Finally,} we provide
insights and future directions based on our initial analysis
\\ ( https://arxiv.org/abs/2401.13802 ,  12329kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13945 (*cross-listing*)
Date: Thu, 25 Jan 2024 05:00:46 GMT   (1458kb)

Title: General Automatic Solution Generation of Social Problems
Authors: Tong Niu, Haoyu Huang, Yu Du, Weihao Zhang, Luping Shi, Rong Zhao
Categories: cs.CY cs.AI cs.CE cs.MA
DOI: 10.1007/s11633-024-1396-5
\\
  Given the escalating intricacy and multifaceted nature of contemporary social
systems, manually generating solutions to address pertinent social issues has
become a formidable task. In response to this challenge, the rapid development
of artificial intelligence has spurred the exploration of computational
methodologies aimed at automatically generating solutions. However, current
methods for auto-generation of solutions mainly concentrate on local social
regulations that pertain to specific scenarios. Here, we report an automatic
social operating system (ASOS) designed for general social solution generation,
which is built upon agent-based models, enabling both global and local analyses
and regulations of social problems across spatial and temporal dimensions. ASOS
adopts a hypergraph with extensible social semantics for a comprehensive and
structured representation of social dynamics. It also incorporates a
generalized protocol for standardized hypergraph operations and a symbolic
hybrid framework that delivers interpretable solutions, yielding a balance
between regulatory efficacy and function viability. To demonstrate the
effectiveness of ASOS, we apply it to the domain of averting extreme events
within international oil futures markets. By generating a new trading role
supplemented by new mechanisms, ASOS can adeptly discern precarious market
conditions and make front-running interventions for non-profit purposes. This
study demonstrates that ASOS provides an efficient and systematic approach for
generating solutions for enhancing our society.
\\ ( https://arxiv.org/abs/2401.13945 ,  1458kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13974 (*cross-listing*)
Date: Thu, 25 Jan 2024 06:18:20 GMT   (2201kb,D)

Title: BootPIG: Bootstrapping Zero-shot Personalized Image Generation
  Capabilities in Pretrained Diffusion Models
Authors: Senthil Purushwalkam, Akash Gokul, Shafiq Joty, Nikhil Naik
Categories: cs.CV cs.AI cs.GR
\\
  Recent text-to-image generation models have demonstrated incredible success
in generating images that faithfully follow input prompts. However, the
requirement of using words to describe a desired concept provides limited
control over the appearance of the generated concepts. In this work, we address
this shortcoming by proposing an approach to enable personalization
capabilities in existing text-to-image diffusion models. We propose a novel
architecture (BootPIG) that allows a user to provide reference images of an
object in order to guide the appearance of a concept in the generated images.
  The proposed BootPIG architecture makes minimal modifications to a pretrained
text-to-image diffusion model and utilizes a separate UNet model to steer the
generations toward the desired appearance. We introduce a training procedure
that allows us to bootstrap personalization capabilities in the BootPIG
architecture using data generated from pretrained text-to-image models, LLM
chat agents, and image segmentation models. In contrast to existing methods
that require several days of pretraining, the BootPIG architecture can be
trained in approximately 1 hour. Experiments on the DreamBooth dataset
demonstrate that BootPIG outperforms existing zero-shot methods while being
comparable with test-time finetuning approaches. Through a user study, we
validate the preference for BootPIG generations over existing methods both in
maintaining fidelity to the reference object's appearance and aligning with
textual prompts.
\\ ( https://arxiv.org/abs/2401.13974 ,  2201kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13976 (*cross-listing*)
Date: Thu, 25 Jan 2024 06:34:49 GMT   (3795kb,D)

Title: Learning to Manipulate Artistic Images
Authors: Wei Guo, Yuqi Zhang, De Ma, Qian Zheng
Categories: cs.CV cs.AI
\\
  Recent advancement in computer vision has significantly lowered the barriers
to artistic creation. Exemplar-based image translation methods have attracted
much attention due to flexibility and controllability. However, these methods
hold assumptions regarding semantics or require semantic information as the
input, while accurate semantics is not easy to obtain in artistic images.
Besides, these methods suffer from cross-domain artifacts due to training data
prior and generate imprecise structure due to feature compression in the
spatial domain. In this paper, we propose an arbitrary Style Image Manipulation
Network (SIM-Net), which leverages semantic-free information as guidance and a
region transportation strategy in a self-supervised manner for image
generation. Our method balances computational efficiency and high resolution to
a certain extent. Moreover, our method facilitates zero-shot style image
manipulation. Both qualitative and quantitative experiments demonstrate the
superiority of our method over state-of-the-art methods.Code is available at
https://github.com/SnailForce/SIM-Net.
\\ ( https://arxiv.org/abs/2401.13976 ,  3795kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14032 (*cross-listing*)
Date: Thu, 25 Jan 2024 09:22:32 GMT   (9714kb,D)

Title: GauU-Scene: A Scene Reconstruction Benchmark on Large Scale 3D
  Reconstruction Dataset Using Gaussian Splatting
Authors: Butian Xiong, Zhuo Li, Zhen Li
Categories: cs.CV cs.AI
Comments: IJCAI2024 submit, 8 pages
\\
  We introduce a novel large-scale scene reconstruction benchmark using the
newly developed 3D representation approach, Gaussian Splatting, on our
expansive U-Scene dataset. U-Scene encompasses over one and a half square
kilometres, featuring a comprehensive RGB dataset coupled with LiDAR ground
truth. For data acquisition, we employed the Matrix 300 drone equipped with the
high-accuracy Zenmuse L1 LiDAR, enabling precise rooftop data collection. This
dataset, offers a unique blend of urban and academic environments for advanced
spatial analysis convers more than 1.5 km$^2$. Our evaluation of U-Scene with
Gaussian Splatting includes a detailed analysis across various novel
viewpoints. We also juxtapose these results with those derived from our
accurate point cloud dataset, highlighting significant differences that
underscore the importance of combine multi-modal information
\\ ( https://arxiv.org/abs/2401.14032 ,  9714kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14057 (*cross-listing*)
Date: Thu, 25 Jan 2024 10:29:07 GMT   (4863kb)

Title: Left/Right Brain, human motor control and the implications for robotics
Authors: Jarrad Rinaldo, Levin Kuhlmann, Jason Friedman, Gideon Kowadlo
Categories: cs.RO cs.AI cs.LG cs.NE q-bio.NC
ACM-class: I.2.6; I.2.9
\\
  Neural Network movement controllers promise a variety of advantages over
conventional control methods however they are not widely adopted due to their
inability to produce reliably precise movements. This research explores a
bilateral neural network architecture as a control system for motor tasks. We
aimed to achieve hemispheric specialisation similar to what is observed in
humans across different tasks; the dominant system (usually the right hand,
left hemisphere) excels at tasks involving coordination and efficiency of
movement, and the non-dominant system performs better at tasks requiring
positional stability. Specialisation was achieved by training the hemispheres
with different loss functions tailored toward the expected behaviour of the
respective hemispheres. We compared bilateral models with and without
specialised hemispheres, with and without inter-hemispheric connectivity
(representing the biological Corpus Callosum), and unilateral models with and
without specialisation. The models were trained and tested on two tasks common
in the human motor control literature: the random reach task, suited to the
dominant system, a model with better coordination, and the hold position task,
suited to the non-dominant system, a model with more stable movement. Each
system out-performed the non-favoured system in its preferred task. For both
tasks, a bilateral model outperforms the 'non-preferred' hand, and is as good
or better than the 'preferred' hand. The Corpus Callosum tends to improve
performance, but not always for the specialised models.
\\ ( https://arxiv.org/abs/2401.14057 ,  4863kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14066 (*cross-listing*)
Date: Thu, 25 Jan 2024 10:42:09 GMT   (23495kb,D)

Title: CreativeSynth: Creative Blending and Synthesis of Visual Arts based on
  Multimodal Diffusion
Authors: Nisha Huang, Weiming Dong, Yuxin Zhang, Fan Tang, Ronghui Li,
  Chongyang Ma, Xiu Li, Changsheng Xu
Categories: cs.CV cs.AI
\\
  Large-scale text-to-image generative models have made impressive strides,
showcasing their ability to synthesize a vast array of high-quality images.
However, adapting these models for artistic image editing presents two
significant challenges. Firstly, users struggle to craft textual prompts that
meticulously detail visual elements of the input image. Secondly, prevalent
models, when effecting modifications in specific zones, frequently disrupt the
overall artistic style, complicating the attainment of cohesive and
aesthetically unified artworks. To surmount these obstacles, we build the
innovative unified framework CreativeSynth, which is based on a diffusion model
with the ability to coordinate multimodal inputs and multitask in the field of
artistic image generation. By integrating multimodal features with customized
attention mechanisms, CreativeSynth facilitates the importation of real-world
semantic content into the domain of art through inversion and real-time style
transfer. This allows for the precise manipulation of image style and content
while maintaining the integrity of the original model parameters. Rigorous
qualitative and quantitative evaluations underscore that CreativeSynth excels
in enhancing artistic images' fidelity and preserves their innate aesthetic
essence. By bridging the gap between generative models and artistic finesse,
CreativeSynth becomes a custom digital palette.
\\ ( https://arxiv.org/abs/2401.14066 ,  23495kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14079 (*cross-listing*)
Date: Thu, 25 Jan 2024 10:56:58 GMT   (77kb,D)

Title: From Requirements to Architecture: An AI-Based Journey to
  Semi-Automatically Generate Software Architectures
Authors: Tobias Eisenreich, Sandro Speth, Stefan Wagner
Categories: cs.SE cs.AI
Comments: 4 pages, vision paper, submitted to the ICSE workshop Designing2024
ACM-class: D.2.2
\\
  Designing domain models and software architectures represents a significant
challenge in software development, as the resulting architectures play a vital
role in fulfilling the system's quality of service. Due to time pressure,
architects often model only one architecture based on their known limited
domain understanding, patterns, and experience instead of thoroughly analyzing
the domain and evaluating multiple candidates, selecting the best fitting.
Existing approaches try to generate domain models based on requirements, but
still require time-consuming manual effort to achieve good results. Therefore,
in this vision paper, we propose a method to generate software architecture
candidates semi-automatically based on requirements using artificial
intelligence techniques. We further envision an automatic evaluation and
trade-off analysis of the generated architecture candidates using, e.g., the
architecture trade-off analysis method combined with large language models and
quantitative analyses. To evaluate this approach, we aim to analyze the quality
of the generated architecture models and the efficiency and effectiveness of
our proposed process by conducting qualitative studies.
\\ ( https://arxiv.org/abs/2401.14079 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14089 (*cross-listing*)
Date: Thu, 25 Jan 2024 11:11:16 GMT   (3944kb,D)

Title: GQHAN: A Grover-inspired Quantum Hard Attention Network
Authors: Ren-Xin Zhao, Jinjing Shi and Xuelong Li
Categories: quant-ph cs.AI
\\
  Numerous current Quantum Machine Learning (QML) models exhibit an inadequacy
in discerning the significance of quantum data, resulting in diminished
efficacy when handling extensive quantum datasets. Hard Attention Mechanism
(HAM), anticipated to efficiently tackle the above QML bottlenecks, encounters
the substantial challenge of non-differentiability, consequently constraining
its extensive applicability. In response to the dilemma of HAM and QML, a
Grover-inspired Quantum Hard Attention Mechanism (GQHAM) consisting of a
Flexible Oracle (FO) and an Adaptive Diffusion Operator (ADO) is proposed.
Notably, the FO is designed to surmount the non-differentiable issue by
executing the activation or masking of Discrete Primitives (DPs) with Flexible
Control (FC) to weave various discrete destinies. Based on this, such discrete
choice can be visualized with a specially defined Quantum Hard Attention Score
(QHAS). Furthermore, a trainable ADO is devised to boost the generality and
flexibility of GQHAM. At last, a Grover-inspired Quantum Hard Attention Network
(GQHAN) based on QGHAM is constructed on PennyLane platform for Fashion MNIST
binary classification. Experimental findings demonstrate that GQHAN adeptly
surmounts the non-differentiability hurdle, surpassing the efficacy of extant
quantum soft self-attention mechanisms in accuracies and learning ability. In
noise experiments, GQHAN is robuster to bit-flip noise in accuracy and
amplitude damping noise in learning performance. Predictably, the proposal of
GQHAN enriches the Quantum Attention Mechanism (QAM), lays the foundation for
future quantum computers to process large-scale data, and promotes the
development of quantum computer vision.
\\ ( https://arxiv.org/abs/2401.14089 ,  3944kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14142 (*cross-listing*)
Date: Thu, 25 Jan 2024 12:46:37 GMT   (5233kb,D)

Title: Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept
  Intervention, and Conditional Interpretations
Authors: Xinyue Xu, Yi Qin, Lu Mi, Hao Wang, Xiaomeng Li
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: Accepted by ICLR 2024
\\
  Existing methods, such as concept bottleneck models (CBMs), have been
successful in providing concept-based interpretations for black-box deep
learning models. They typically work by predicting concepts given the input and
then predicting the final class label given the predicted concepts. However,
(1) they often fail to capture the high-order, nonlinear interaction between
concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not
help correct highly correlated concepts (e.g., "yellow belly"), leading to
suboptimal final accuracy; (2) they cannot naturally quantify the complex
conditional dependencies between different concepts and class labels (e.g., for
an image with the class label "Kentucky Warbler" and a concept "black bill",
what is the probability that the model correctly predicts another concept
"black crown"), therefore failing to provide deeper insight into how a
black-box model works. In response to these limitations, we propose
Energy-based Concept Bottleneck Models (ECBMs). Our ECBMs use a set of neural
networks to define the joint energy of candidate (input, concept, class)
tuples. With such a unified interface, prediction, concept correction, and
conditional dependency quantification are then represented as conditional
probabilities, which are generated by composing different energy functions. Our
ECBMs address both limitations of existing CBMs, providing higher accuracy and
richer concept interpretations. Empirical results show that our approach
outperforms the state-of-the-art on real-world datasets.
\\ ( https://arxiv.org/abs/2401.14142 ,  5233kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14171 (*cross-listing*)
Date: Thu, 25 Jan 2024 13:28:53 GMT   (1165kb,D)

Title: Predicting Hypoxia in Brain Tumors from Multiparametric MRI
Authors: Daniele Perlo and Georgia Kanli and Selma Boudissa and Olivier Keunen
Categories: eess.IV cs.AI
Comments: 7 pages, 2 figures
ACM-class: J.3; I.2.1
\\
  This research paper presents a novel approach to the prediction of hypoxia in
brain tumors, using multi-parametric Magnetic Resonance Imaging (MRI). Hypoxia,
a condition characterized by low oxygen levels, is a common feature of
malignant brain tumors associated with poor prognosis. Fluoromisonidazole
Positron Emission Tomography (FMISO PET) is a well-established method for
detecting hypoxia in vivo, but it is expensive and not widely available. Our
study proposes the use of MRI, a more accessible and cost-effective imaging
modality, to predict FMISO PET signals. We investigate deep learning models
(DL) trained on the ACRIN 6684 dataset, a resource that contains paired MRI and
FMISO PET images from patients with brain tumors. Our trained models
effectively learn the complex relationships between the MRI features and the
corresponding FMISO PET signals, thereby enabling the prediction of hypoxia
from MRI scans alone. The results show a strong correlation between the
predicted and actual FMISO PET signals, with an overall PSNR score above 29.6
and a SSIM score greater than 0.94, confirming MRI as a promising option for
hypoxia prediction in brain tumors. This approach could significantly improve
the accessibility of hypoxia detection in clinical settings, with the potential
for more timely and targeted treatments.
\\ ( https://arxiv.org/abs/2401.14171 ,  1165kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14174 (*cross-listing*)
Date: Thu, 25 Jan 2024 13:34:33 GMT   (247kb,D)

Title: The Boundaries of Tractability in Hierarchical Task Network Planning
Authors: Cornelius Brand, Robert Ganian, Fionn Mc Inerney, Simon Wietheger
Categories: cs.CC cs.AI
\\
  We study the complexity-theoretic boundaries of tractability for three
classical problems in the context of Hierarchical Task Network Planning: the
validation of a provided plan, whether an executable plan exists, and whether a
given state can be reached by some plan. We show that all three problems can be
solved in polynomial time on primitive task networks of constant partial order
width (and a generalization thereof), whereas for the latter two problems this
holds only under a provably necessary restriction to the state space. Next, we
obtain an algorithmic meta-theorem along with corresponding lower bounds to
identify tight conditions under which general polynomial-time solvability
results can be lifted from primitive to general task networks. Finally, we
enrich our investigation by analyzing the parameterized complexity of the three
considered problems, and show that (1) fixed-parameter tractability for all
three problems can be achieved by replacing the partial order width with the
vertex cover number of the network as the parameter, and (2) other classical
graph-theoretic parameters of the network (including treewidth, treedepth, and
the aforementioned partial order width) do not yield fixed-parameter
tractability for any of the three problems.
\\ ( https://arxiv.org/abs/2401.14174 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14176 (*cross-listing*)
Date: Thu, 25 Jan 2024 13:39:54 GMT   (469kb,D)

Title: Copilot Refinement: Addressing Code Smells in Copilot-Generated Python
  Code
Authors: Beiqi Zhang, Peng Liang, Qiong Feng, Yujia Fu, Zengyang Li
Categories: cs.SE cs.AI
\\
  As one of the most popular dynamic languages, Python experiences a decrease
in readability and maintainability when code smells are present. Recent
advancements in Large Language Models have sparked growing interest in
AI-enabled tools for both code generation and refactoring. GitHub Copilot is
one such tool that has gained widespread usage. Copilot Chat, released on
September 2023, functions as an interactive tool aims at facilitating natural
language-powered coding. However, limited attention has been given to
understanding code smells in Copilot-generated Python code and Copilot's
ability to fix the code smells it generates. To this end, we built a dataset
comprising 102 code smells in Copilot-generated Python code. Our aim is to
first explore the occurrence of code smells in Copilot-generated Python code
and then evaluate the effectiveness of Copilot in fixing these code smells
employing different prompts. The results show that 8 out of 10 types of Python
smells can be detected in Copilot-generated Python code, among which
Multiply-Nested Container is the most common one. For these code smells,
Copilot Chat achieves a highest fixing rate of 87.1%, showing promise in fixing
Python code smells generated by Copilot itself. Besides, the effectiveness of
Copilot Chat in fixing these smells can be improved with the provision of more
detailed prompts. However, using Copilot Chat to fix these smells might
introduce new code smells.
\\ ( https://arxiv.org/abs/2401.14176 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14185 (*cross-listing*)
Date: Thu, 25 Jan 2024 13:47:22 GMT   (8049kb,D)

Title: TDFNet: An Efficient Audio-Visual Speech Separation Model with Top-down
  Fusion
Authors: Samuel Pegg, Kai Li, Xiaolin Hu
Categories: cs.SD cs.AI eess.AS
Journal-ref: 2023 13th International Conference on Information Science and
  Technology (ICIST), Cairo, Egypt, 2023, pp. 243-252
DOI: 10.1109/ICIST59754.2023.10367130
\\
  Audio-visual speech separation has gained significant traction in recent
years due to its potential applications in various fields such as speech
recognition, diarization, scene analysis and assistive technologies. Designing
a lightweight audio-visual speech separation network is important for
low-latency applications, but existing methods often require higher
computational costs and more parameters to achieve better separation
performance. In this paper, we present an audio-visual speech separation model
called Top-Down-Fusion Net (TDFNet), a state-of-the-art (SOTA) model for
audio-visual speech separation, which builds upon the architecture of TDANet,
an audio-only speech separation method. TDANet serves as the architectural
foundation for the auditory and visual networks within TDFNet, offering an
efficient model with fewer parameters. On the LRS2-2Mix dataset, TDFNet
achieves a performance increase of up to 10\% across all performance metrics
compared with the previous SOTA method CTCNet. Remarkably, these results are
achieved using fewer parameters and only 28\% of the multiply-accumulate
operations (MACs) of CTCNet. In essence, our method presents a highly effective
and efficient solution to the challenges of speech separation within the
audio-visual domain, making significant strides in harnessing visual
information optimally.
\\ ( https://arxiv.org/abs/2401.14185 ,  8049kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14206 (*cross-listing*)
Date: Thu, 25 Jan 2024 14:40:58 GMT   (1584kb,D)

Title: Exploiting Liver CT scans in Colorectal Carcinoma genomics mutation
  classification
Authors: Daniele Perlo and Luca Berton and Alessia Delpiano and Francesca
  Menchini and Stefano Tibaldi and Marco Grosso and Paolo Fonio
Categories: eess.IV cs.AI
ACM-class: J.3; I.1.2
Journal-ref: 2022 IEEE International Conference on Big Data (Big Data)
DOI: 10.1109/BigData55660.2022.10020613
\\
  The liver is the most involved organ by distant metastasis in colon-rectal
cancer (CRC) patients and it comes necessary to be aware of the mutational
status of the lesions to correctly design the best individual treatment. So
far, efforts have been made in order to develop non-invasive and real-time
methods that permit the analysis of the whole tumor, using new artificial
intelligence tools to analyze the tumor's image obtained by Computed Tomography
(CT) scan. In order to address the current medical workflow, that is biopsy
analysis-based, we propose the first DeepLearning-based exploration, to our
knowledge, of such classification approach from the patient medical imaging. We
propose i) a solid pipeline for managing undersized datasets of available CT
scans and ii) a baseline study for genomics mutation diagnosis support for
preemptive patient follow-up. Our method is able to identify CRC RAS mutation
family from CT images with 0.73 F1 score.
\\ ( https://arxiv.org/abs/2401.14206 ,  1584kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14232 (*cross-listing*)
Date: Sun, 31 Dec 2023 21:49:03 GMT   (1739kb)

Title: AR-GAN: Generative Adversarial Network-Based Defense Method Against
  Adversarial Attacks on the Traffic Sign Classification System of Autonomous
  Vehicles
Authors: M Sabbir Salek, Abdullah Al Mamun, and Mashrur Chowdhury
Categories: cs.CV cs.AI cs.CR cs.LG
\\
  This study developed a generative adversarial network (GAN)-based defense
method for traffic sign classification in an autonomous vehicle (AV), referred
to as the attack-resilient GAN (AR-GAN). The novelty of the AR-GAN lies in (i)
assuming zero knowledge of adversarial attack models and samples and (ii)
providing consistently high traffic sign classification performance under
various adversarial attack types. The AR-GAN classification system consists of
a generator that denoises an image by reconstruction, and a classifier that
classifies the reconstructed image. The authors have tested the AR-GAN under
no-attack and under various adversarial attacks, such as Fast Gradient Sign
Method (FGSM), DeepFool, Carlini and Wagner (C&W), and Projected Gradient
Descent (PGD). The authors considered two forms of these attacks, i.e., (i)
black-box attacks (assuming the attackers possess no prior knowledge of the
classifier), and (ii) white-box attacks (assuming the attackers possess full
knowledge of the classifier). The classification performance of the AR-GAN was
compared with several benchmark adversarial defense methods. The results showed
that both the AR-GAN and the benchmark defense methods are resilient against
black-box attacks and could achieve similar classification performance to that
of the unperturbed images. However, for all the white-box attacks considered in
this study, the AR-GAN method outperformed the benchmark defense methods. In
addition, the AR-GAN was able to maintain its high classification performance
under varied white-box adversarial perturbation magnitudes, whereas the
performance of the other defense methods dropped abruptly at increased
perturbation magnitudes.
\\ ( https://arxiv.org/abs/2401.14232 ,  1739kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14257 (*cross-listing*)
Date: Thu, 25 Jan 2024 15:49:12 GMT   (9661kb,D)

Title: Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation
Authors: Minglin Chen and Longguang Wang and Weihao Yuan and Yukun Wang and Zhe
  Sheng and Yisheng He and Zilong Dong and Liefeng Bo and Yulan Guo
Categories: cs.CV cs.AI
Comments: 11 pages, 9 figures
\\
  Recently, text-to-3D approaches have achieved high-fidelity 3D content
generation using text description. However, the generated objects are
stochastic and lack fine-grained control. Sketches provide a cheap approach to
introduce such fine-grained control. Nevertheless, it is challenging to achieve
flexible control from these sketches due to their abstraction and ambiguity. In
this paper, we present a multi-view sketch-guided text-to-3D generation
framework (namely, Sketch2NeRF) to add sketch control to 3D generation.
Specifically, our method leverages pretrained 2D diffusion models (e.g., Stable
Diffusion and ControlNet) to supervise the optimization of a 3D scene
represented by a neural radiance field (NeRF). We propose a novel synchronized
generation and reconstruction method to effectively optimize the NeRF. In the
experiments, we collected two kinds of multi-view sketch datasets to evaluate
the proposed method. We demonstrate that our method can synthesize 3D
consistent contents with fine-grained sketch control while being high-fidelity
to text prompts. Extensive results show that our method achieves
state-of-the-art performance in terms of sketch similarity and text alignment.
\\ ( https://arxiv.org/abs/2401.14257 ,  9661kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14285 (*cross-listing*)
Date: Thu, 25 Jan 2024 16:18:11 GMT   (3883kb,D)

Title: POUR-Net: A Population-Prior-Aided Over-Under-Representation Network for
  Low-Count PET Attenuation Map Generation
Authors: Bo Zhou, Jun Hou, Tianqi Chen, Yinchi Zhou, Xiongchao Chen, Huidong
  Xie, Qiong Liu, Xueqi Guo, Yu-Jung Tsai, Vladimir Y. Panin, Takuya Toyonaga,
  James S. Duncan, Chi Liu
Categories: cs.CV cs.AI eess.IV
Comments: 10 pages, 5 figures
\\
  Low-dose PET offers a valuable means of minimizing radiation exposure in PET
imaging. However, the prevalent practice of employing additional CT scans for
generating attenuation maps (u-map) for PET attenuation correction
significantly elevates radiation doses. To address this concern and further
mitigate radiation exposure in low-dose PET exams, we propose POUR-Net - an
innovative population-prior-aided over-under-representation network that aims
for high-quality attenuation map generation from low-dose PET. First, POUR-Net
incorporates an over-under-representation network (OUR-Net) to facilitate
efficient feature extraction, encompassing both low-resolution abstracted and
fine-detail features, for assisting deep generation on the full-resolution
level. Second, complementing OUR-Net, a population prior generation machine
(PPGM) utilizing a comprehensive CT-derived u-map dataset, provides additional
prior information to aid OUR-Net generation. The integration of OUR-Net and
PPGM within a cascade framework enables iterative refinement of $\mu$-map
generation, resulting in the production of high-quality $\mu$-maps.
Experimental results underscore the effectiveness of POUR-Net, showing it as a
promising solution for accurate CT-free low-count PET attenuation correction,
which also surpasses the performance of previous baseline methods.
\\ ( https://arxiv.org/abs/2401.14285 ,  3883kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14292 (*cross-listing*)
Date: Thu, 25 Jan 2024 16:30:22 GMT   (7798kb,D)

Title: AST-2: Single and bi-layered 2-D acoustic soft tactile skin
Authors: Vishnu Rajendran, Simon Parsons and Amir Ghalamzan E
Categories: cs.RO cs.AI
\\
  This paper aims to present an innovative and cost-effective design for
Acoustic Soft Tactile (AST) Skin, with the primary goal of significantly
enhancing the accuracy of 2-D tactile feature estimation. The existing
challenge lies in achieving precise tactile feature estimation, especially
concerning contact geometry characteristics, using cost-effective solutions. We
hypothesise that by harnessing acoustic energy through dedicated acoustic
channels in 2 layers beneath the sensing surface and analysing amplitude
modulation, we can effectively decode interactions on the sensory surface,
thereby improving tactile feature estimation. Our approach involves the
distinct separation of hardware components responsible for emitting and
receiving acoustic signals, resulting in a modular and highly customizable skin
design. Practical tests demonstrate the effectiveness of this novel design,
achieving remarkable precision in estimating contact normal forces (MAE < 0.8
N), 2D contact localisation (MAE < 0.7 mm), and contact surface diameter (MAE <
0.3 mm). In conclusion, the AST skin, with its innovative design and modular
architecture, successfully addresses the challenge of tactile feature
estimation. The presented results showcase its ability to precisely estimate
various tactile features, making it a practical and cost-effective solution for
robotic applications.
\\ ( https://arxiv.org/abs/2401.14292 ,  7798kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14336 (*cross-listing*)
Date: Thu, 25 Jan 2024 17:34:34 GMT   (7297kb,D)

Title: Progressive Multi-task Anti-Noise Learning and Distilling Frameworks for
  Fine-grained Vehicle Recognition
Authors: Dichao Liu
Categories: cs.CV cs.AI
\\
  Fine-grained vehicle recognition (FGVR) is an essential fundamental
technology for intelligent transportation systems, but very difficult because
of its inherent intra-class variation. Most previous FGVR studies only focus on
the intra-class variation caused by different shooting angles, positions, etc.,
while the intra-class variation caused by image noise has received little
attention. This paper proposes a progressive multi-task anti-noise learning
(PMAL) framework and a progressive multi-task distilling (PMD) framework to
solve the intra-class variation problem in FGVR due to image noise. The PMAL
framework achieves high recognition accuracy by treating image denoising as an
additional task in image recognition and progressively forcing a model to learn
noise invariance. The PMD framework transfers the knowledge of the PMAL-trained
model into the original backbone network, which produces a model with about the
same recognition accuracy as the PMAL-trained model, but without any additional
overheads over the original backbone network. Combining the two frameworks, we
obtain models that significantly exceed previous state-of-the-art methods in
recognition accuracy on two widely-used, standard FGVR datasets, namely
Stanford Cars, and CompCars, as well as three additional surveillance
image-based vehicle-type classification datasets, namely Beijing Institute of
Technology (BIT)-Vehicle, Vehicle Type Image Data 2 (VTID2), and Vehicle Images
Dataset for Make Model Recognition (VIDMMR), without any additional overheads
over the original backbone networks. The source code is available at
https://github.com/Dichao-Liu/Anti-noise_FGVR
\\ ( https://arxiv.org/abs/2401.14336 ,  7297kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14362 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:08:53 GMT   (98kb)

Title: The Typing Cure: Experiences with Large Language Model Chatbots for
  Mental Health Support
Authors: Inhwa Song, Sachin R. Pendse, Neha Kumar, Munmun De Choudhury
Categories: cs.HC cs.AI cs.CY
Comments: The first two authors contributed equally to this work
\\
  People experiencing severe distress increasingly use Large Language Model
(LLM) chatbots as mental health support tools. Discussions on social media have
described how engagements were lifesaving for some, but evidence suggests that
general-purpose LLM chatbots also have notable risks that could endanger the
welfare of users if not designed responsibly. In this study, we investigate the
lived experiences of people who have used LLM chatbots for mental health
support. We build on interviews with 21 individuals from globally diverse
backgrounds to analyze how users create unique support roles for their
chatbots, fill in gaps in everyday care, and navigate associated cultural
limitations when seeking support from chatbots. We ground our analysis in
psychotherapy literature around effective support, and introduce the concept of
therapeutic alignment, or aligning AI with therapeutic values for mental health
contexts. Our study offers recommendations for how designers can approach the
ethical and effective use of LLM chatbots and other AI mental health support
tools in mental health care.
\\ ( https://arxiv.org/abs/2401.14362 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14371 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:20:37 GMT   (619kb,D)

Title: Efficient Optimisation of Physical Reservoir Computers using only a
  Delayed Input
Authors: Enrico Picco, Lina Jaurigue, Kathy L\"udge and Serge Massar
Categories: cs.ET cs.AI cs.NE physics.optics
\\
  We present an experimental validation of a recently proposed optimization
technique for reservoir computing, using an optoelectronic setup. Reservoir
computing is a robust framework for signal processing applications, and the
development of efficient optimization approaches remains a key challenge. The
technique we address leverages solely a delayed version of the input signal to
identify the optimal operational region of the reservoir, simplifying the
traditionally time-consuming task of hyperparameter tuning. We verify the
effectiveness of this approach on different benchmark tasks and reservoir
operating conditions.
\\ ( https://arxiv.org/abs/2401.14371 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14403 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:59:44 GMT   (9832kb,D)

Title: Adaptive Mobile Manipulation for Articulated Objects In the Open World
Authors: Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak
Categories: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY
Comments: Website at https://open-world-mobilemanip.github.io/
\\
  Deploying robots in open-ended unstructured environments such as homes has
been a long-standing research problem. However, robots are often studied only
in closed-off lab settings, and prior mobile manipulation work is restricted to
pick-move-place, which is arguably just the tip of the iceberg in this area. In
this paper, we introduce Open-World Mobile Manipulation System, a full-stack
approach to tackle realistic articulated object operation, e.g. real-world
doors, cabinets, drawers, and refrigerators in open-ended unstructured
environments. The robot utilizes an adaptive learning framework to initially
learns from a small set of data through behavior cloning, followed by learning
from online practice on novel objects that fall outside the training
distribution. We also develop a low-cost mobile manipulation hardware platform
capable of safe and autonomous online adaptation in unstructured environments
with a cost of around 20,000 USD. In our experiments we utilize 20 articulate
objects across 4 buildings in the CMU campus. With less than an hour of online
learning for each object, the system is able to increase success rate from 50%
of BC pre-training to 95% using online adaptation. Video results at
https://open-world-mobilemanip.github.io/
\\ ( https://arxiv.org/abs/2401.14403 ,  9832kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14405 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:59:58 GMT   (257kb,D)

Title: Multimodal Pathway: Improve Transformers with Irrelevant Data from Other
  Modalities
Authors: Yiyuan Zhang, Xiaohan Ding, Kaixiong Gong, Yixiao Ge, Ying Shan,
  Xiangyu Yue
Categories: cs.CV cs.AI cs.LG
Comments: The code and models are available at
  https://github.com/AILab-CVC/M2PT
\\
  We propose to improve transformers of a specific modality with irrelevant
data from other modalities, e.g., improve an ImageNet model with audio or point
cloud datasets. We would like to highlight that the data samples of the target
modality are irrelevant to the other modalities, which distinguishes our method
from other works utilizing paired (e.g., CLIP) or interleaved data of different
modalities. We propose a methodology named Multimodal Pathway - given a target
modality and a transformer designed for it, we use an auxiliary transformer
trained with data of another modality and construct pathways to connect
components of the two models so that data of the target modality can be
processed by both models. In this way, we utilize the universal
sequence-to-sequence modeling abilities of transformers obtained from two
modalities. As a concrete implementation, we use a modality-specific tokenizer
and task-specific head as usual but utilize the transformer blocks of the
auxiliary model via a proposed method named Cross-Modal Re-parameterization,
which exploits the auxiliary weights without any inference costs. On the image,
point cloud, video, and audio recognition tasks, we observe significant and
consistent performance improvements with irrelevant data from other modalities.
The code and models are available at https://github.com/AILab-CVC/M2PT.
\\ ( https://arxiv.org/abs/2401.14405 ,  257kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12255 (*cross-listing*)
Date: Sun, 21 Jan 2024 09:51:45 GMT   (3109kb,D)

Title: Instructional Fingerprinting of Large Language Models
Authors: Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang Wei Koh, Chaowei Xiao,
  Muhao Chen
Categories: cs.CR cs.AI cs.CL cs.LG
Comments: 30 pages
\\
  The exorbitant cost of training Large language models (LLMs) from scratch
makes it essential to fingerprint the models to protect intellectual property
via ownership authentication and to ensure downstream users and developers
comply with their license terms (e.g. restricting commercial use). In this
study, we present a pilot study on LLM fingerprinting as a form of very
lightweight instruction tuning. Model publisher specifies a confidential
private key and implants it as an instruction backdoor that causes the LLM to
generate specific text when the key is present. Results on 11 popularly-used
LLMs showed that this approach is lightweight and does not affect the normal
behavior of the model. It also prevents publisher overclaim, maintains
robustness against fingerprint guessing and parameter-efficient training, and
supports multi-stage fingerprinting akin to MIT License. Code is available in
https://cnut1648.github.io/Model-Fingerprint/.
\\ ( https://arxiv.org/abs/2401.12255 ,  3109kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14196 (*cross-listing*)
Date: Thu, 25 Jan 2024 14:17:53 GMT   (3001kb,D)

Title: DeepSeek-Coder: When the Large Language Model Meets Programming - The
  Rise of Code Intelligence
Authors: Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang,
  Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng
  Liang
Categories: cs.SE cs.CL cs.LG
\\
  The rapid development of large language models has revolutionized code
intelligence in software development. However, the predominance of
closed-source models has restricted extensive research and development. To
address this, we introduce the DeepSeek-Coder series, a range of open-source
code models with sizes from 1.3B to 33B, trained from scratch on 2 trillion
tokens. These models are pre-trained on a high-quality project-level code
corpus and employ a fill-in-the-blank task with a 16K window to enhance code
generation and infilling. Our extensive evaluations demonstrate that
DeepSeek-Coder not only achieves state-of-the-art performance among open-source
code models across multiple benchmarks but also surpasses existing
closed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models
are under a permissive license that allows for both research and unrestricted
commercial use.
\\ ( https://arxiv.org/abs/2401.14196 ,  3001kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13231 (*cross-listing*)
Date: Wed, 24 Jan 2024 05:03:05 GMT   (2417kb,D)

Title: DittoGym: Learning to Control Soft Shape-Shifting Robots
Authors: Suning Huang and Boyuan Chen and Huazhe Xu and Vincent Sitzmann
Categories: cs.RO cs.LG
\\
  Robot co-design, where the morphology of a robot is optimized jointly with a
learned policy to solve a specific task, is an emerging area of research. It
holds particular promise for soft robots, which are amenable to novel
manufacturing techniques that can realize learned morphologies and actuators.
Inspired by nature and recent novel robot designs, we propose to go a step
further and explore the novel reconfigurable robots, defined as robots that can
change their morphology within their lifetime. We formalize control of
reconfigurable soft robots as a high-dimensional reinforcement learning (RL)
problem. We unify morphology change, locomotion, and environment interaction in
the same action space, and introduce an appropriate, coarse-to-fine curriculum
that enables us to discover policies that accomplish fine-grained control of
the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark
for reconfigurable soft robots that require fine-grained morphology changes to
accomplish the tasks. Finally, we evaluate our proposed coarse-to-fine
algorithm on DittoGym and demonstrate robots that learn to change their
morphology several times within a sequence, uniquely enabled by our RL
algorithm. More results are available at https://dittogym.github.io.
\\ ( https://arxiv.org/abs/2401.13231 ,  2417kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13671 (*cross-listing*)
Date: Fri, 27 Oct 2023 11:29:34 GMT   (44kb,D)

Title: Determinants of renewable energy consumption in Madagascar: Evidence
  from feature selection algorithms
Authors: Franck Ramaharo and Fitiavana Randriamifidy
Categories: econ.GN cs.LG q-fin.EC
Comments: 21 pages, 4 tables, 1 figure
MSC-class: 91B74, 62J05
\\
  The aim of this note is to identify the factors influencing renewable energy
consumption in Madagascar. We tested 12 features covering macroeconomic,
financial, social, and environmental aspects, including economic growth,
domestic investment, foreign direct investment, financial development,
industrial development, inflation, income distribution, trade openness,
exchange rate, tourism development, environmental quality, and urbanization. To
assess their significance, we assumed a linear relationship between renewable
energy consumption and these features over the 1990-2021 period. Next, we
applied different machine learning feature selection algorithms classified as
filter-based (relative importance for linear regression, correlation method),
embedded (LASSO), and wrapper-based (best subset regression, stepwise
regression, recursive feature elimination, iterative predictor weighting
partial least squares, Boruta, simulated annealing, and genetic algorithms)
methods. Our analysis revealed that the five most influential drivers stem from
macroeconomic aspects. We found that domestic investment, foreign direct
investment, and inflation positively contribute to the adoption of renewable
energy sources. On the other hand, industrial development and trade openness
negatively affect renewable energy consumption in Madagascar.
\\ ( https://arxiv.org/abs/2401.13671 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13695 (*cross-listing*)
Date: Wed, 17 Jan 2024 22:21:07 GMT   (15421kb,D)

Title: Inverse analysis of granular flows using differentiable graph neural
  network simulator
Authors: Yongjin Choi, Krishna Kumar
Categories: physics.geo-ph cs.LG
ACM-class: I.6.8
\\
  Inverse problems in granular flows, such as landslides and debris flows,
involve estimating material parameters or boundary conditions based on target
runout profile. Traditional high-fidelity simulators for these inverse problems
are computationally demanding, restricting the number of simulations possible.
Additionally, their non-differentiable nature makes gradient-based optimization
methods, known for their efficiency in high-dimensional problems, inapplicable.
While machine learning-based surrogate models offer computational efficiency
and differentiability, they often struggle to generalize beyond their training
data due to their reliance on low-dimensional input-output mappings that fail
to capture the complete physics of granular flows. We propose a novel
differentiable graph neural network simulator (GNS) by combining reverse mode
automatic differentiation of graph neural networks with gradient-based
optimization for solving inverse problems. GNS learns the dynamics of granular
flow by representing the system as a graph and predicts the evolution of the
graph at the next time step, given the current state. The differentiable GNS
shows optimization capabilities beyond the training data. We demonstrate the
effectiveness of our method for inverse estimation across single and
multi-parameter optimization problems, including evaluating material properties
and boundary conditions for a target runout distance and designing baffle
locations to limit a landslide runout. Our proposed differentiable GNS
framework offers an orders of magnitude faster solution to these inverse
problems than the conventional finite difference approach to gradient-based
optimization.
\\ ( https://arxiv.org/abs/2401.13695 ,  15421kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13714 (*cross-listing*)
Date: Wed, 24 Jan 2024 04:21:41 GMT   (8260kb,D)

Title: Value-Driven Mixed-Precision Quantization for Patch-Based Inference on
  Microcontrollers
Authors: Wei Tao, Shenglin He, Kai Lu, Xiaoyang Qu, Guokuan Li, Jiguang Wan,
  Jianzong Wang, Jing Xiao
Categories: cs.CV cs.LG
Comments: Accepted by the 27th Design, Automation and Test in Europe Conference
  (DATE 2024)
\\
  Deploying neural networks on microcontroller units (MCUs) presents
substantial challenges due to their constrained computation and memory
resources. Previous researches have explored patch-based inference as a
strategy to conserve memory without sacrificing model accuracy. However, this
technique suffers from severe redundant computation overhead, leading to a
substantial increase in execution latency. A feasible solution to address this
issue is mixed-precision quantization, but it faces the challenges of accuracy
degradation and a time-consuming search time. In this paper, we propose
QuantMCU, a novel patch-based inference method that utilizes value-driven
mixed-precision quantization to reduce redundant computation. We first utilize
value-driven patch classification (VDPC) to maintain the model accuracy. VDPC
classifies patches into two classes based on whether they contain outlier
values. For patches containing outlier values, we apply 8-bit quantization to
the feature maps on the dataflow branches that follow. In addition, for patches
without outlier values, we utilize value-driven quantization search (VDQS) on
the feature maps of their following dataflow branches to reduce search time.
Specifically, VDQS introduces a novel quantization search metric that takes
into account both computation and accuracy, and it employs entropy as an
accuracy representation to avoid additional training. VDQS also adopts an
iterative approach to determine the bitwidth of each feature map to further
accelerate the search process. Experimental results on real-world MCU devices
show that QuantMCU can reduce computation by 2.2x on average while maintaining
comparable model accuracy compared to the state-of-the-art patch-based
inference methods.
\\ ( https://arxiv.org/abs/2401.13714 ,  8260kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13719 (*cross-listing*)
Date: Wed, 24 Jan 2024 09:51:03 GMT   (2726kb,D)

Title: Inference Attacks Against Face Recognition Model without Classification
  Layers
Authors: Yuanqing Huang, Huilong Chen, Yinggui Wang, Lei Wang
Categories: cs.CV cs.LG
\\
  Face recognition (FR) has been applied to nearly every aspect of daily life,
but it is always accompanied by the underlying risk of leaking private
information. At present, almost all attack models against FR rely heavily on
the presence of a classification layer. However, in practice, the FR model can
obtain complex features of the input via the model backbone, and then compare
it with the target for inference, which does not explicitly involve the outputs
of the classification layer adopting logit or other losses. In this work, we
advocate a novel inference attack composed of two stages for practical FR
models without a classification layer. The first stage is the membership
inference attack. Specifically, We analyze the distances between the
intermediate features and batch normalization (BN) parameters. The results
indicate that this distance is a critical metric for membership inference. We
thus design a simple but effective attack model that can determine whether a
face image is from the training dataset or not. The second stage is the model
inversion attack, where sensitive private data is reconstructed using a
pre-trained generative adversarial network (GAN) guided by the attack model in
the first stage. To the best of our knowledge, the proposed attack model is the
very first in the literature developed for FR models without a classification
layer. We illustrate the application of the proposed attack model in the
establishment of privacy-preserving FR techniques.
\\ ( https://arxiv.org/abs/2401.13719 ,  2726kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13721 (*cross-listing*)
Date: Wed, 24 Jan 2024 14:55:02 GMT   (40561kb,D)

Title: Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in
  Regression
Authors: Ismail Nejjar, Gaetan Frusque, Florent Forest, Olga Fink
Categories: cs.CV cs.LG
\\
  Unsupervised Domain Adaptation for Regression (UDAR) aims to adapt a model
from a labeled source domain to an unlabeled target domain for regression
tasks. Recent successful works in UDAR mostly focus on subspace alignment,
involving the alignment of a selected subspace within the entire feature space.
This contrasts with the feature alignment methods used for classification,
which aim at aligning the entire feature space and have proven effective but
are less so in regression settings. Specifically, while classification aims to
identify separate clusters across the entire embedding dimension, regression
induces less structure in the data representation, necessitating additional
guidance for efficient alignment. In this paper, we propose an effective method
for UDAR by incorporating guidance from uncertainty. Our approach serves a dual
purpose: providing a measure of confidence in predictions and acting as a
regularization of the embedding space. Specifically, we leverage the Deep
Evidential Learning framework, which outputs both predictions and uncertainties
for each input sample. We propose aligning the parameters of higher-order
evidential distributions between the source and target domains using
traditional alignment methods at the feature or posterior level. Additionally,
we propose to augment the feature space representation by mixing source samples
with pseudo-labeled target samples based on label similarity. This cross-domain
mixing strategy produces more realistic samples than random mixing and
introduces higher uncertainty, facilitating further alignment. We demonstrate
the effectiveness of our approach on four benchmarks for UDAR, on which we
outperform existing methods.
\\ ( https://arxiv.org/abs/2401.13721 ,  40561kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13726 (*cross-listing*)
Date: Wed, 24 Jan 2024 18:45:34 GMT   (8332kb,D)

Title: Supporting Sensemaking of Large Language Model Outputs at Scale
Authors: Katy Ilonka Gero, Chelse Swoopes, Ziwei Gu, Jonathan K. Kummerfeld,
  Elena L. Glassman
Categories: cs.HC cs.LG
Comments: 34 pages, 13 figures, conditionally accepted to ACM Conference on
  Human Factors in Computing Systems 2024
\\
  Large language models (LLMs) are capable of generating multiple responses to
a single prompt, yet little effort has been expended to help end-users or
system designers make use of this capability. In this paper, we explore how to
present many LLM responses at once. We design five features, which include both
pre-existing and novel methods for computing similarities and differences
across textual documents, as well as how to render their outputs. We report on
a controlled user study (n=24) and eight case studies evaluating these features
and how they support users in different tasks. We find that the features
support a wide variety of sensemaking tasks and even make tasks previously
considered to be too difficult by our participants now tractable. Finally, we
present design guidelines to inform future explorations of new LLM interfaces.
\\ ( https://arxiv.org/abs/2401.13726 ,  8332kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13769 (*cross-listing*)
Date: Wed, 24 Jan 2024 19:35:54 GMT   (1613kb,D)

Title: Multiview Graph Learning with Consensus Graph
Authors: Abdullah Karaaslanli, Selin Aviyente
Categories: eess.SP cs.LG
\\
  Graph topology inference, i.e., learning graphs from a given set of nodal
observations, is a significant task in many application domains. Existing
approaches are mostly limited to learning a single graph assuming that the
observed data is homogeneous. This is problematic because many modern datasets
are heterogeneous or mixed and involve multiple related graphs, i.e., multiview
graphs. Recent work proposing to learn multiview graphs ensures the similarity
of learned view graphs through pairwise regularization, where each pair of
views is encouraged to have similar structures. However, this approach cannot
infer the shared structure across views. In this work, we propose an
alternative method based on consensus regularization, where views are ensured
to be similar through a learned consensus graph representing the common
structure of the views. In particular, we propose an optimization problem,
where graph data is assumed to be smooth over the multiview graph and the
topology of the individual views and that of the consensus graph are learned,
simultaneously. Our optimization problem is designed to be general in the sense
that different regularization functions can be used depending on what the
shared structure across views is. Moreover, we propose two regularization
functions that extend fused and group graphical lasso to consensus based
regularization. Proposed multiview graph learning is evaluated on simulated
data and shown to have better performance than existing methods. It is also
employed to infer the functional brain connectivity networks of multiple
subjects from their electroencephalogram (EEG) recordings. The proposed method
reveals the structure shared by subjects as well as the characteristics unique
to each subject.
\\ ( https://arxiv.org/abs/2401.13769 ,  1613kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13779 (*cross-listing*)
Date: Wed, 24 Jan 2024 20:00:23 GMT   (227kb,D)

Title: Faster Convergence with Less Communication: Broadcast-Based Subgraph
  Sampling for Decentralized Learning over Wireless Networks
Authors: Daniel P\'erez Herrera, Zheng Chen, and Erik G. Larsson
Categories: cs.IT cs.DC cs.LG eess.SP math.IT
Comments: 11 pages, 5 figures, submitted for possible journal publication.
  arXiv admin note: text overlap with arXiv:2310.16106
\\
  Consensus-based decentralized stochastic gradient descent (D-SGD) is a widely
adopted algorithm for decentralized training of machine learning models across
networked agents. A crucial part of D-SGD is the consensus-based model
averaging, which heavily relies on information exchange and fusion among the
nodes. Specifically, for consensus averaging over wireless networks,
communication coordination is necessary to determine when and how a node can
access the channel and transmit (or receive) information to (or from) its
neighbors. In this work, we propose $\texttt{BASS}$, a broadcast-based subgraph
sampling method designed to accelerate the convergence of D-SGD while
considering the actual communication cost per iteration. $\texttt{BASS}$
creates a set of mixing matrix candidates that represent sparser subgraphs of
the base topology. In each consensus iteration, one mixing matrix is sampled,
leading to a specific scheduling decision that activates multiple
collision-free subsets of nodes. The sampling occurs in a probabilistic manner,
and the elements of the mixing matrices, along with their sampling
probabilities, are jointly optimized. Simulation results demonstrate that
$\texttt{BASS}$ enables faster convergence with fewer transmission slots
compared to existing link-based scheduling methods. In conclusion, the inherent
broadcasting nature of wireless channels offers intrinsic advantages in
accelerating the convergence of decentralized optimization and learning.
\\ ( https://arxiv.org/abs/2401.13779 ,  227kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13836 (*cross-listing*)
Date: Wed, 24 Jan 2024 22:27:04 GMT   (404kb,D)

Title: Machine learning for industrial sensing and control: A survey and
  practical perspective
Authors: Nathan P. Lawrence, Seshu Kumar Damarla, Jong Woo Kim, Aditya Tulsyan,
  Faraz Amjad, Kai Wang, Benoit Chachuat, Jong Min Lee, Biao Huang, R. Bhushan
  Gopaluni
Categories: eess.SY cs.LG cs.SY
Comments: 48 pages
Journal-ref: Control Engineering Practice 2024
DOI: 10.1016/j.conengprac.2024.105841
\\
  With the rise of deep learning, there has been renewed interest within the
process industries to utilize data on large-scale nonlinear sensing and control
problems. We identify key statistical and machine learning techniques that have
seen practical success in the process industries. To do so, we start with
hybrid modeling to provide a methodological framework underlying core
application areas: soft sensing, process optimization, and control. Soft
sensing contains a wealth of industrial applications of statistical and machine
learning methods. We quantitatively identify research trends, allowing insight
into the most successful techniques in practice.
  We consider two distinct flavors for data-driven optimization and control:
hybrid modeling in conjunction with mathematical programming techniques and
reinforcement learning. Throughout these application areas, we discuss their
respective industrial requirements and challenges.
  A common challenge is the interpretability and efficiency of purely
data-driven methods. This suggests a need to carefully balance deep learning
techniques with domain knowledge. As a result, we highlight ways prior
knowledge may be integrated into industrial machine learning applications. The
treatment of methods, problems, and applications presented here is poised to
inform and inspire practitioners and researchers to develop impactful
data-driven sensing, optimization, and control solutions in the process
industries.
\\ ( https://arxiv.org/abs/2401.13836 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13851 (*cross-listing*)
Date: Wed, 24 Jan 2024 23:18:33 GMT   (3855kb)

Title: Scaling NVIDIA's multi-speaker multi-lingual TTS systems with voice
  cloning to Indic Languages
Authors: Akshit Arora, Rohan Badlani, Sungwon Kim, Rafael Valle, Bryan
  Catanzaro
Categories: cs.SD cs.LG eess.AS
Comments: Presentation accepted at ICASSP 2024
\\
  In this paper, we describe the TTS models developed by NVIDIA for the
MMITS-VC (Multi-speaker, Multi-lingual Indic TTS with Voice Cloning) 2024
Challenge. In Tracks 1 and 2, we utilize RAD-MMM to perform few-shot TTS by
training additionally on 5 minutes of target speaker data. In Track 3, we
utilize P-Flow to perform zero-shot TTS by training on the challenge dataset as
well as external datasets. We use HiFi-GAN vocoders for all submissions.
RAD-MMM performs competitively on Tracks 1 and 2, while P-Flow ranks first on
Track 3, with mean opinion score (MOS) 4.4 and speaker similarity score (SMOS)
of 3.62.
\\ ( https://arxiv.org/abs/2401.13851 ,  3855kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13875 (*cross-listing*)
Date: Thu, 25 Jan 2024 01:09:09 GMT   (65kb)

Title: Is Temperature Sample Efficient for Softmax Gaussian Mixture of Experts?
Authors: Huy Nguyen, Pedram Akbarian, Nhat Ho
Categories: stat.ML cs.LG
Comments: 53 pages
\\
  Dense-to-sparse gating mixture of experts (MoE) has recently become an
effective alternative to a well-known sparse MoE. Rather than fixing the number
of activated experts as in the latter model, which could limit the
investigation of potential experts, the former model utilizes the temperature
to control the softmax weight distribution and the sparsity of the MoE during
training in order to stabilize the expert specialization. Nevertheless, while
there are previous attempts to theoretically comprehend the sparse MoE, a
comprehensive analysis of the dense-to-sparse gating MoE has remained elusive.
Therefore, we aim to explore the impacts of the dense-to-sparse gate on the
maximum likelihood estimation under the Gaussian MoE in this paper. We
demonstrate that due to interactions between the temperature and other model
parameters via some partial differential equations, the convergence rates of
parameter estimations are slower than any polynomial rates, and could be as
slow as $\mathcal{O}(1/\log(n))$, where $n$ denotes the sample size. To address
this issue, we propose using a novel activation dense-to-sparse gate, which
routes the output of a linear layer to an activation function before delivering
them to the softmax function. By imposing linearly independence conditions on
the activation function and its derivatives, we show that the parameter
estimation rates are significantly improved to polynomial rates.
\\ ( https://arxiv.org/abs/2401.13875 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13884 (*cross-listing*)
Date: Thu, 25 Jan 2024 02:01:53 GMT   (243kb,D)

Title: Constant Stepsize Q-learning: Distributional Convergence, Bias and
  Extrapolation
Authors: Yixuan Zhang and Qiaomin Xie
Categories: stat.ML cs.LG math.OC
Comments: 41 pages, 3 figures
\\
  Stochastic Approximation (SA) is a widely used algorithmic approach in
various fields, including optimization and reinforcement learning (RL). Among
RL algorithms, Q-learning is particularly popular due to its empirical success.
In this paper, we study asynchronous Q-learning with constant stepsize, which
is commonly used in practice for its fast convergence. By connecting the
constant stepsize Q-learning to a time-homogeneous Markov chain, we show the
distributional convergence of the iterates in Wasserstein distance and
establish its exponential convergence rate. We also establish a Central Limit
Theory for Q-learning iterates, demonstrating the asymptotic normality of the
averaged iterates. Moreover, we provide an explicit expansion of the asymptotic
bias of the averaged iterate in stepsize. Specifically, the bias is
proportional to the stepsize up to higher-order terms and we provide an
explicit expression for the linear coefficient. This precise characterization
of the bias allows the application of Richardson-Romberg (RR) extrapolation
technique to construct a new estimate that is provably closer to the optimal Q
function. Numerical results corroborate our theoretical finding on the
improvement of the RR extrapolation method.
\\ ( https://arxiv.org/abs/2401.13884 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13947 (*cross-listing*)
Date: Thu, 25 Jan 2024 05:05:55 GMT   (1261kb,D)

Title: Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy
  Trading
Authors: Chen Feng and Andrew L. Liu
Categories: eess.SY cs.LG cs.MA cs.SY
\\
  Utilizing distributed renewable and energy storage resources in local
distribution networks via peer-to-peer (P2P) energy trading has long been
touted as a solution to improve energy systems' resilience and sustainability.
Consumers and prosumers (those who have energy generation resources), however,
do not have the expertise to engage in repeated P2P trading, and the
zero-marginal costs of renewables present challenges in determining fair market
prices. To address these issues, we propose multi-agent reinforcement learning
(MARL) frameworks to help automate consumers' bidding and management of their
solar PV and energy storage resources, under a specific P2P clearing mechanism
that utilizes the so-called supply-demand ratio. In addition, we show how the
MARL frameworks can integrate physical network constraints to realize voltage
control, hence ensuring physical feasibility of the P2P energy trading and
paving way for real-world implementations.
\\ ( https://arxiv.org/abs/2401.13947 ,  1261kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13971 (*cross-listing*)
Date: Thu, 25 Jan 2024 06:06:31 GMT   (90kb,D)

Title: Stochastic Weakly Convex Optimization Beyond Lipschitz Continuity
Authors: Wenzhi Gao, Qi Deng
Categories: math.OC cs.LG
\\
  This paper considers stochastic weakly convex optimization without the
standard Lipschitz continuity assumption. Based on new adaptive regularization
(stepsize) strategies, we show that a wide class of stochastic algorithms,
including the stochastic subgradient method, preserve the $\mathcal{O} ( 1 /
\sqrt{K})$ convergence rate with constant failure rate. Our analyses rest on
rather weak assumptions: the Lipschitz parameter can be either bounded by a
general growth function of $\|x\|$ or locally estimated through independent
random samples.
\\ ( https://arxiv.org/abs/2401.13971 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14025 (*cross-listing*)
Date: Thu, 25 Jan 2024 09:17:19 GMT   (443kb,D)

Title: DNA Sequence Classification with Compressors
Authors: \c{S}\"ukr\"u Ozan
Categories: q-bio.GN cs.LG
\\
  Recent studies in DNA sequence classification have leveraged sophisticated
machine learning techniques, achieving notable accuracy in categorizing complex
genomic data. Among these, methods such as k-mer counting have proven effective
in distinguishing sequences from varied species like chimpanzees, dogs, and
humans, becoming a staple in contemporary genomic research. However, these
approaches often demand extensive computational resources, posing a challenge
in terms of scalability and efficiency. Addressing this issue, our study
introduces a novel adaptation of Jiang et al.'s compressor-based,
parameter-free classification method, specifically tailored for DNA sequence
analysis. This innovative approach utilizes a variety of compression
algorithms, such as Gzip, Brotli, and LZMA, to efficiently process and classify
genomic sequences. Not only does this method align with the current
state-of-the-art in terms of accuracy, but it also offers a more
resource-efficient alternative to traditional machine learning methods. Our
comprehensive evaluation demonstrates the proposed method's effectiveness in
accurately classifying DNA sequences from multiple species. We present a
detailed analysis of the performance of each algorithm used, highlighting the
strengths and limitations of our approach in various genomic contexts.
Furthermore, we discuss the broader implications of our findings for
bioinformatics, particularly in genomic data processing and analysis. The
results of our study pave the way for more efficient and scalable DNA sequence
classification methods, offering significant potential for advancements in
genomic research and applications.
\\ ( https://arxiv.org/abs/2401.14025 ,  443kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14029 (*cross-listing*)
Date: Thu, 25 Jan 2024 09:20:21 GMT   (2870kb,D)

Title: Towards a Systems Theory of Algorithms
Authors: Florian D\"orfler, Zhiyu He, Giuseppe Belgioioso, Saverio Bolognani,
  John Lygeros, Michael Muehlebach
Categories: math.OC cs.LG cs.SY eess.SY
\\
  Traditionally, numerical algorithms are seen as isolated pieces of code
confined to an {\em in silico} existence. However, this perspective is not
appropriate for many modern computational approaches in control, learning, or
optimization, wherein {\em in vivo} algorithms interact with their environment.
Examples of such {\em open} include various real-time optimization-based
control strategies, reinforcement learning, decision-making architectures,
online optimization, and many more. Further, even {\em closed} algorithms in
learning or optimization are increasingly abstracted in block diagrams with
interacting dynamic modules and pipelines. In this opinion paper, we state our
vision on a to-be-cultivated {\em systems theory of algorithms} and argue in
favour of viewing algorithms as open dynamical systems interacting with other
algorithms, physical systems, humans, or databases. Remarkably, the manifold
tools developed under the umbrella of systems theory also provide valuable
insights into this burgeoning paradigm shift and its accompanying challenges in
the algorithmic world. We survey various instances where the principles of
algorithmic systems theory are being developed and outline pertinent modeling,
analysis, and design challenges.
\\ ( https://arxiv.org/abs/2401.14029 ,  2870kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14074 (*cross-listing*)
Date: Thu, 25 Jan 2024 10:52:36 GMT   (2751kb,D)

Title: ProCNS: Progressive Prototype Calibration and Noise Suppression for
  Weakly-Supervised Medical Image Segmentation
Authors: Y. Liu, L. Lin, K. K. Y. Wong, X. Tang
Categories: cs.CV cs.LG
\\
  Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate
the conflict between annotation cost and model performance by adopting sparse
annotation formats (e.g., point, scribble, block, etc.). Typical approaches
attempt to exploit anatomy and topology priors to directly expand sparse
annotations into pseudo-labels. However, due to a lack of attention to the
ambiguous edges in medical images and insufficient exploration of sparse
supervision, existing approaches tend to generate erroneous and overconfident
pseudo proposals in noisy regions, leading to cumulative model error and
performance degradation. In this work, we propose a novel WSS approach, named
ProCNS, encompassing two synergistic modules devised with the principles of
progressive prototype calibration and noise suppression. Specifically, we
design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the
pair-wise affinities between spatial and semantic elements, providing our model
of interest with more reliable guidance. The affinities are derived from the
input images and the prototype-refined predictions. Meanwhile, we propose an
Adaptive Noise Perception and Masking (ANPM) module to obtain more enriched and
representative prototype representations, which adaptively identifies and masks
noisy regions within the pseudo proposals, reducing potential erroneous
interference during prototype computation. Furthermore, we generate specialized
soft pseudo-labels for the noisy regions identified by ANPM, providing
supplementary supervision. Extensive experiments on three medical image
segmentation tasks involving different modalities demonstrate that the proposed
framework significantly outperforms representative state-of-the-art methods
\\ ( https://arxiv.org/abs/2401.14074 ,  2751kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14090 (*cross-listing*)
Date: Thu, 25 Jan 2024 11:12:16 GMT   (316kb,D)

Title: A Modular Approach to Automatic Cyber Threat Attribution using Opinion
  Pools
Authors: Koen T.W. Teuwen
Categories: cs.CR cs.LG cs.SE
Comments: For source code see:
  https://github.com/Koen1999/modular-threat-attribution
Journal-ref: IEEE International Conference on Big Data (Big Data), Sorrento,
  Italy, 2023, pp. 3089-3098
DOI: 10.1109/BigData59044.2023.10386708
\\
  Cyber threat attribution can play an important role in increasing resilience
against digital threats. Recent research focuses on automating the threat
attribution process and on integrating it with other efforts, such as threat
hunting. To support increasing automation of the cyber threat attribution
process, this paper proposes a modular architecture as an alternative to
current monolithic automated approaches. The modular architecture can utilize
opinion pools to combine the output of concrete attributors. The proposed
solution increases the tractability of the threat attribution problem and
offers increased usability and interpretability, as opposed to monolithic
alternatives. In addition, a Pairing Aggregator is proposed as an aggregation
method that forms pairs of attributors based on distinct features to produce
intermediary results before finally producing a single Probability Mass
Function (PMF) as output. The Pairing Aggregator sequentially applies both the
logarithmic opinion pool and the linear opinion pool. An experimental
validation suggests that the modular approach does not result in decreased
performance and can even enhance precision and recall compared to monolithic
alternatives. The results also suggest that the Pairing Aggregator can improve
precision over the linear and logarithmic opinion pools. Furthermore, the
improved k-accuracy in the experiment suggests that forensic experts can
leverage the resulting PMF during their manual attribution processes to enhance
their efficiency.
\\ ( https://arxiv.org/abs/2401.14090 ,  316kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14093 (*cross-listing*)
Date: Thu, 25 Jan 2024 11:15:51 GMT   (405kb,D)

Title: McUDI: Model-Centric Unsupervised Degradation Indicator for Failure
  Prediction AIOps Solutions
Authors: Lorena Poenaru-Olaru, Luis Cruz, Jan Rellermeyer, Arie van Deursen
Categories: cs.SE cs.LG
\\
  Due to the continuous change in operational data, AIOps solutions suffer from
performance degradation over time. Although periodic retraining is the
state-of-the-art technique to preserve the failure prediction AIOps models'
performance over time, this technique requires a considerable amount of labeled
data to retrain. In AIOps obtaining label data is expensive since it requires
the availability of domain experts to intensively annotate it. In this paper,
we present McUDI, a model-centric unsupervised degradation indicator that is
capable of detecting the exact moment the AIOps model requires retraining as a
result of changes in data. We further show how employing McUDI in the
maintenance pipeline of AIOps solutions can reduce the number of samples that
require annotations with 30k for job failure prediction and 260k for disk
failure prediction while achieving similar performance with periodic
retraining.
\\ ( https://arxiv.org/abs/2401.14093 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14130 (*cross-listing*)
Date: Thu, 25 Jan 2024 12:18:46 GMT   (1605kb,D)

Title: Attention-based Efficient Classification for 3D MRI Image of Alzheimer's
  Disease
Authors: Yihao Lin, Ximeng Li, Yan Zhang, Jinshan Tang
Categories: eess.IV cs.CV cs.LG
\\
  Early diagnosis of Alzheimer Diagnostics (AD) is a challenging task due to
its subtle and complex clinical symptoms. Deep learning-assisted medical
diagnosis using image recognition techniques has become an important research
topic in this field. The features have to accurately capture main variations of
anatomical brain structures. However, time-consuming is expensive for feature
extraction by deep learning training. This study proposes a novel Alzheimer's
disease detection model based on Convolutional Neural Networks. The model
utilizes a pre-trained ResNet network as the backbone, incorporating
post-fusion algorithm for 3D medical images and attention mechanisms. The
experimental results indicate that the employed 2D fusion algorithm effectively
improves the model's training expense. And the introduced attention mechanism
accurately weights important regions in images, further enhancing the model's
diagnostic accuracy.
\\ ( https://arxiv.org/abs/2401.14130 ,  1605kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14184 (*cross-listing*)
Date: Thu, 25 Jan 2024 13:46:21 GMT   (726kb,D)

Title: Friendly Attacks to Improve Channel Coding Reliability
Authors: Anastasiia Kurmukova and Deniz Gunduz
Categories: cs.IT cs.LG math.IT
\\
  This paper introduces a novel approach called "friendly attack" aimed at
enhancing the performance of error correction channel codes. Inspired by the
concept of adversarial attacks, our method leverages the idea of introducing
slight perturbations to the neural network input, resulting in a substantial
impact on the network's performance. By introducing small perturbations to
fixed-point modulated codewords before transmission, we effectively improve the
decoder's performance without violating the input power constraint. The
perturbation design is accomplished by a modified iterative fast gradient
method. This study investigates various decoder architectures suitable for
computing gradients to obtain the desired perturbations. Specifically, we
consider belief propagation (BP) for LDPC codes; the error correcting code
transformer, BP and neural BP (NBP) for polar codes, and neural BCJR for
convolutional codes. We demonstrate that the proposed friendly attack method
can improve the reliability across different channels, modulations, codes, and
decoders. This method allows us to increase the reliability of communication
with a legacy receiver by simply modifying the transmitted codeword
appropriately.
\\ ( https://arxiv.org/abs/2401.14184 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14256 (*cross-listing*)
Date: Thu, 25 Jan 2024 15:47:18 GMT   (18040kb,D)

Title: Producing Plankton Classifiers that are Robust to Dataset Shift
Authors: Cheng Chen, Sreenath Kyathanahally, Marta Reyes, Stefanie Merkli, Ewa
  Merz, Emanuele Francazi, Marvin Hoege, Francesco Pomati, Marco Baity-Jesi
Categories: cs.CV cs.LG
\\
  Modern plankton high-throughput monitoring relies on deep learning
classifiers for species recognition in water ecosystems. Despite satisfactory
nominal performances, a significant challenge arises from Dataset Shift, which
causes performances to drop during deployment. In our study, we integrate the
ZooLake dataset with manually-annotated images from 10 independent days of
deployment, serving as test cells to benchmark Out-Of-Dataset (OOD)
performances. Our analysis reveals instances where classifiers, initially
performing well in In-Dataset conditions, encounter notable failures in
practical scenarios. For example, a MobileNet with a 92% nominal test accuracy
shows a 77% OOD accuracy. We systematically investigate conditions leading to
OOD performance drops and propose a preemptive assessment method to identify
potential pitfalls when classifying new data, and pinpoint features in OOD
images that adversely impact classification. We present a three-step pipeline:
(i) identifying OOD degradation compared to nominal test performance, (ii)
conducting a diagnostic analysis of degradation causes, and (iii) providing
solutions. We find that ensembles of BEiT vision transformers, with targeted
augmentations addressing OOD robustness, geometric ensembling, and
rotation-based test-time augmentation, constitute the most robust model, which
we call BEsT model. It achieves an 83% OOD accuracy, with errors concentrated
on container classes. Moreover, it exhibits lower sensitivity to dataset shift,
and reproduces well the plankton abundances. Our proposed pipeline is
applicable to generic plankton classifiers, contingent on the availability of
suitable test cells. By identifying critical shortcomings and offering
practical procedures to fortify models against dataset shift, our study
contributes to the development of more reliable plankton classification
technologies.
\\ ( https://arxiv.org/abs/2401.14256 ,  18040kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14283 (*cross-listing*)
Date: Thu, 25 Jan 2024 16:15:27 GMT   (4011kb,D)

Title: Information Leakage Detection through Approximate Bayes-optimal
  Prediction
Authors: Pritha Gupta, Marcel Wever, and Eyke H\"ullermeier
Categories: stat.ML cs.LG
Comments: Under submission in JMLR
MSC-class: 94A15, 62H30, 94A60
ACM-class: I.5.1; G.3; E.3
\\
  In today's data-driven world, the proliferation of publicly available
information intensifies the challenge of information leakage (IL), raising
security concerns. IL involves unintentionally exposing secret (sensitive)
information to unauthorized parties via systems' observable information.
Conventional statistical approaches, which estimate mutual information (MI)
between observable and secret information for detecting IL, face challenges
such as the curse of dimensionality, convergence, computational complexity, and
MI misestimation. Furthermore, emerging supervised machine learning (ML)
methods, though effective, are limited to binary system-sensitive information
and lack a comprehensive theoretical framework. To address these limitations,
we establish a theoretical framework using statistical learning theory and
information theory to accurately quantify and detect IL. We demonstrate that MI
can be accurately estimated by approximating the log-loss and accuracy of the
Bayes predictor. As the Bayes predictor is typically unknown in practice, we
propose to approximate it with the help of automated machine learning (AutoML).
First, we compare our MI estimation approaches against current baselines, using
synthetic data sets generated using the multivariate normal (MVN) distribution
with known MI. Second, we introduce a cut-off technique using one-sided
statistical tests to detect IL, employing the Holm-Bonferroni correction to
increase confidence in detection decisions. Our study evaluates IL detection
performance on real-world data sets, highlighting the effectiveness of the
Bayes predictor's log-loss estimation, and finds our proposed method to
effectively estimate MI on synthetic data sets and thus detect ILs accurately.
\\ ( https://arxiv.org/abs/2401.14283 ,  4011kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14289 (*cross-listing*)
Date: Wed, 24 Jan 2024 18:26:52 GMT   (269kb,D)

Title: Speech foundation models on intelligibility prediction for
  hearing-impaired listeners
Authors: Santiago Cuervo and Ricard Marxer
Categories: cs.SD cs.LG eess.AS
Comments: To be presented in ICASSP 2024
\\
  Speech foundation models (SFMs) have been benchmarked on many speech
processing tasks, often achieving state-of-the-art performance with minimal
adaptation. However, the SFM paradigm has been significantly less explored for
applications of interest to the speech perception community. In this paper we
present a systematic evaluation of 10 SFMs on one such application: Speech
intelligibility prediction. We focus on the non-intrusive setup of the Clarity
Prediction Challenge 2 (CPC2), where the task is to predict the percentage of
words correctly perceived by hearing-impaired listeners from speech-in-noise
recordings. We propose a simple method that learns a lightweight specialized
prediction head on top of frozen SFMs to approach the problem. Our results
reveal statistically significant differences in performance across SFMs. Our
method resulted in the winning submission in the CPC2, demonstrating its
promise for speech perception applications.
\\ ( https://arxiv.org/abs/2401.14289 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14296 (*cross-listing*)
Date: Thu, 25 Jan 2024 16:38:06 GMT   (145kb,D)

Title: "All of Me": Mining Users' Attributes from their Public Spotify
  Playlists
Authors: Pier Paolo Tricomi, Luca Pajola, Luca Pasa, Mauro Conti
Categories: cs.CR cs.LG cs.SI
\\
  In the age of digital music streaming, playlists on platforms like Spotify
have become an integral part of individuals' musical experiences. People create
and publicly share their own playlists to express their musical tastes, promote
the discovery of their favorite artists, and foster social connections. These
publicly accessible playlists transcend the boundaries of mere musical
preferences: they serve as sources of rich insights into users' attributes and
identities. For example, the musical preferences of elderly individuals may
lean more towards Frank Sinatra, while Billie Eilish remains a favored choice
among teenagers. These playlists thus become windows into the diverse and
evolving facets of one's musical identity.
  In this work, we investigate the relationship between Spotify users'
attributes and their public playlists. In particular, we focus on identifying
recurring musical characteristics associated with users' individual attributes,
such as demographics, habits, or personality traits. To this end, we conducted
an online survey involving 739 Spotify users, yielding a dataset of 10,286
publicly shared playlists encompassing over 200,000 unique songs and 55,000
artists. Through extensive statistical analyses, we first assess a deep
connection between a user's Spotify playlists and their real-life attributes.
For instance, we found individuals high in openness often create playlists
featuring a diverse array of artists, while female users prefer Pop and K-pop
music genres. Building upon these observed associations, we create accurate
predictive models for users' attributes, presenting a novel DeepSet application
that outperforms baselines in most of these users' attributes.
\\ ( https://arxiv.org/abs/2401.14296 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14332 (*cross-listing*)
Date: Thu, 25 Jan 2024 17:30:08 GMT   (212kb,D)

Title: SunBlock: Cloudless Protection for IoT Systems
Authors: Vadim Safronov, Anna Maria Mandalari, Daniel J. Dubois, David
  Choffnes, Hamed Haddadi
Categories: cs.CR cs.LG
Comments: This paper is accepted at Passive and Active Measurement (PAM)
  conference 2024
\\
  With an increasing number of Internet of Things (IoT) devices present in
homes, there is a rise in the number of potential information leakage channels
and their associated security threats and privacy risks. Despite a long history
of attacks on IoT devices in unprotected home networks, the problem of
accurate, rapid detection and prevention of such attacks remains open. Many
existing IoT protection solutions are cloud-based, sometimes ineffective, and
might share consumer data with unknown third parties. This paper investigates
the potential for effective IoT threat detection locally, on a home router,
using AI tools combined with classic rule-based traffic-filtering algorithms.
Our results show that with a slight rise of router hardware resources caused by
machine learning and traffic filtering logic, a typical home router
instrumented with our solution is able to effectively detect risks and protect
a typical home IoT network, equaling or outperforming existing popular
solutions, without any effects on benign IoT functionality, and without relying
on cloud services and third parties.
\\ ( https://arxiv.org/abs/2401.14332 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14340 (*cross-listing*)
Date: Thu, 25 Jan 2024 17:39:47 GMT   (896kb,D)

Title: Estimation of partially known Gaussian graphical models with score-based
  structural priors
Authors: Mart\'in Sevilla, Antonio Garc\'ia Marques, Santiago Segarra
Categories: stat.ML cs.LG
Comments: 15 pages, 5 figures
\\
  We propose a novel algorithm for the support estimation of partially known
Gaussian graphical models that incorporates prior information about the
underlying graph. In contrast to classical approaches that provide a point
estimate based on a maximum likelihood or a maximum a posteriori criterion
using (simple) priors on the precision matrix, we consider a prior on the graph
and rely on annealed Langevin diffusion to generate samples from the posterior
distribution. Since the Langevin sampler requires access to the score function
of the underlying graph prior, we use graph neural networks to effectively
estimate the score from a graph dataset (either available beforehand or
generated from a known distribution). Numerical experiments demonstrate the
benefits of our approach.
\\ ( https://arxiv.org/abs/2401.14340 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14379 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:30:46 GMT   (1012kb)

Title: UrbanGenAI: Reconstructing Urban Landscapes using Panoptic Segmentation
  and Diffusion Models
Authors: Timo Kapsalis
Categories: cs.CV cs.LG
Comments: 19 pages, 4 figures, 2 tables
\\
  In contemporary design practices, the integration of computer vision and
generative artificial intelligence (genAI) represents a transformative shift
towards more interactive and inclusive processes. These technologies offer new
dimensions of image analysis and generation, which are particularly relevant in
the context of urban landscape reconstruction. This paper presents a novel
workflow encapsulated within a prototype application, designed to leverage the
synergies between advanced image segmentation and diffusion models for a
comprehensive approach to urban design. Our methodology encompasses the
OneFormer model for detailed image segmentation and the Stable Diffusion XL
(SDXL) diffusion model, implemented through ControlNet, for generating images
from textual descriptions. Validation results indicated a high degree of
performance by the prototype application, showcasing significant accuracy in
both object detection and text-to-image generation. This was evidenced by
superior Intersection over Union (IoU) and CLIP scores across iterative
evaluations for various categories of urban landscape features. Preliminary
testing included utilising UrbanGenAI as an educational tool enhancing the
learning experience in design pedagogy, and as a participatory instrument
facilitating community-driven urban planning. Early results suggested that
UrbanGenAI not only advances the technical frontiers of urban landscape
reconstruction but also provides significant pedagogical and participatory
planning benefits. The ongoing development of UrbanGenAI aims to further
validate its effectiveness across broader contexts and integrate additional
features such as real-time feedback mechanisms and 3D modelling capabilities.
Keywords: generative AI; panoptic image segmentation; diffusion models; urban
landscape design; design pedagogy; co-design
\\ ( https://arxiv.org/abs/2401.14379 ,  1012kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14382 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:37:17 GMT   (1070kb,D)

Title: An Orthogonal Polynomial Kernel-Based Machine Learning Model for
  Differential-Algebraic Equations
Authors: Tayebeh Taheri, Alireza Afzal Aghaei, Kourosh Parand
Categories: math.NA cs.LG cs.NA
Comments: 17 pages, 5 figures
\\
  The recent introduction of the Least-Squares Support Vector Regression
(LS-SVR) algorithm for solving differential and integral equations has sparked
interest. In this study, we expand the application of this algorithm to address
systems of differential-algebraic equations (DAEs). Our work presents a novel
approach to solving general DAEs in an operator format by establishing
connections between the LS-SVR machine learning model, weighted residual
methods, and Legendre orthogonal polynomials. To assess the effectiveness of
our proposed method, we conduct simulations involving various DAE scenarios,
such as nonlinear systems, fractional-order derivatives, integro-differential,
and partial DAEs. Finally, we carry out comparisons between our proposed method
and currently established state-of-the-art approaches, demonstrating its
reliability and effectiveness.
\\ ( https://arxiv.org/abs/2401.14382 ,  1070kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14398 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:57:36 GMT   (48984kb,D)

Title: pix2gestalt: Amodal Segmentation by Synthesizing Wholes
Authors: Ege Ozguroglu, Ruoshi Liu, D\'idac Sur\'is, Dian Chen, Achal Dave,
  Pavel Tokmakov, Carl Vondrick
Categories: cs.CV cs.LG
Comments: Website: https://gestalt.cs.columbia.edu/
\\
  We introduce pix2gestalt, a framework for zero-shot amodal segmentation,
which learns to estimate the shape and appearance of whole objects that are
only partially visible behind occlusions. By capitalizing on large-scale
diffusion models and transferring their representations to this task, we learn
a conditional diffusion model for reconstructing whole objects in challenging
zero-shot cases, including examples that break natural and physical priors,
such as art. As training data, we use a synthetically curated dataset
containing occluded objects paired with their whole counterparts. Experiments
show that our approach outperforms supervised baselines on established
benchmarks. Our model can furthermore be used to significantly improve the
performance of existing object recognition and 3D reconstruction methods in the
presence of occlusions.
\\ ( https://arxiv.org/abs/2401.14398 ,  48984kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14404 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:59:57 GMT   (8316kb,D)

Title: Deconstructing Denoising Diffusion Models for Self-Supervised Learning
Authors: Xinlei Chen, Zhuang Liu, Saining Xie, Kaiming He
Categories: cs.CV cs.LG
Comments: Technical report, 10 pages
\\
  In this study, we examine the representation learning abilities of Denoising
Diffusion Models (DDM) that were originally purposed for image generation. Our
philosophy is to deconstruct a DDM, gradually transforming it into a classical
Denoising Autoencoder (DAE). This deconstructive procedure allows us to explore
how various components of modern DDMs influence self-supervised representation
learning. We observe that only a very few modern components are critical for
learning good representations, while many others are nonessential. Our study
ultimately arrives at an approach that is highly simplified and to a large
extent resembles a classical DAE. We hope our study will rekindle interest in a
family of classical methods within the realm of modern self-supervised
learning.
\\ ( https://arxiv.org/abs/2401.14404 ,  8316kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2208.09344
replaced with revised version Thu, 25 Jan 2024 18:16:25 GMT   (159kb,D)

Title: A note on incorrect inferences in non-binary qualitative probabilistic
  networks
Authors: Jack Storror Carter
Categories: cs.AI math.ST stat.ME stat.TH
Comments: 11 pages, 3 figures
\\ ( https://arxiv.org/abs/2208.09344 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16424
replaced with revised version Thu, 25 Jan 2024 11:25:09 GMT   (1841kb,D)

Title: Realistic Synthetic Financial Transactions for Anti-Money Laundering
  Models
Authors: Erik Altman, Jovan Blanu\v{s}a, Luc von Niederh\"ausern, B\'eni
  Egressy, Andreea Anghel, Kubilay Atasu
Categories: cs.AI cs.LG q-fin.CP
\\ ( https://arxiv.org/abs/2306.16424 ,  1841kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01154
replaced with revised version Thu, 25 Jan 2024 10:04:49 GMT   (1403kb,D)

Title: Arithmetic with Language Models: from Memorization to Computation
Authors: Davide Maltoni and Matteo Ferrara
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2308.01154 ,  1403kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03251
replaced with revised version Thu, 25 Jan 2024 08:34:05 GMT   (2166kb,D)

Title: Temporal Inductive Path Neural Network for Temporal Knowledge Graph
  Reasoning
Authors: Hao Dong, Pengyang Wang, Meng Xiao, Zhiyuan Ning, Pengfei Wang,
  Yuanchun Zhou
Categories: cs.AI cs.LG
Comments: Accepted to Artificial Intelligence
\\ ( https://arxiv.org/abs/2309.03251 ,  2166kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09404
replaced with revised version Thu, 25 Jan 2024 16:22:56 GMT   (3760kb,D)

Title: Promoting Research Collaboration with Open Data Driven Team
  Recommendation in Response to Call for Proposals
Authors: Siva Likitha Valluru, Biplav Srivastava, Sai Teja Paladi, Siwen Yan,
  Sriraam Natarajan
Categories: cs.AI
Comments: 9 pages, 2 figures, 3 tables, Accepted to The Thirty-Sixth Annual
  Conference on Innovative Applications of Artificial Intelligence
  (IAAI/AAAI-24)
ACM-class: H.3.3; I.2.7
\\ ( https://arxiv.org/abs/2309.09404 ,  3760kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01193
replaced with revised version Wed, 24 Jan 2024 21:34:11 GMT   (83kb,D)

Title: Contextual Confidence and Generative AI
Authors: Shrey Jain, Zo\"e Hitzig, Pamela Mishkin
Categories: cs.AI
\\ ( https://arxiv.org/abs/2311.01193 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11482
replaced with revised version Thu, 25 Jan 2024 13:54:42 GMT   (767kb,D)

Title: Meta Prompting for AGI Systems
Authors: Yifan Zhang
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.11482 ,  767kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05934
replaced with revised version Thu, 25 Jan 2024 08:37:45 GMT   (1099kb,D)

Title: Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs
Authors: Oded Ovadia, Menachem Brief, Moshik Mishaeli, Oren Elisha
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2312.05934 ,  1099kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11562
replaced with revised version Thu, 25 Jan 2024 11:20:16 GMT   (3873kb,D)

Title: A Survey of Reasoning with Foundation Models
Authors: Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu,
  Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai
  Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan,
  Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng Ann Heng,
  Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui
  Xiong, Qun Liu, Zhenguo Li
Categories: cs.AI cs.CL cs.CV cs.LG
Comments: 20 Figures, 160 Pages, 750+ References, Project Page
  https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models
\\ ( https://arxiv.org/abs/2312.11562 ,  3873kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14472
replaced with revised version Thu, 25 Jan 2024 14:35:05 GMT   (1567kb,D)

Title: Not All Tasks Are Equally Difficult: Multi-Task Deep Reinforcement
  Learning with Dynamic Depth Routing
Authors: Jinmin He, Kai Li, Yifan Zang, Haobo Fu, Qiang Fu, Junliang Xing, Jian
  Cheng
Categories: cs.AI
Comments: AAAI2024, with supplementary material
Journal-ref: 38th AAAI Conference on Artificial Intelligence (AAAI2024),
  Vancouver, BC, Canada, 2024
\\ ( https://arxiv.org/abs/2312.14472 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15643
replaced with revised version Thu, 25 Jan 2024 06:55:39 GMT   (643kb,D)

Title: Advancing Abductive Reasoning in Knowledge Graphs through Complex
  Logical Hypothesis Generation
Authors: Jiaxin Bai, Yicheng Wang, Tianshi Zheng, Yue Guo, Xin Liu, and Yangqiu
  Song
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2312.15643 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01623
replaced with revised version Thu, 25 Jan 2024 13:10:15 GMT   (248kb,D)

Title: Can AI Be as Creative as Humans?
Authors: Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb,
  Linjun Zhang, Weijie J Su, Zhun Deng, Michael Qizhe Xie, Hannah Brown, Kenji
  Kawaguchi
Categories: cs.AI cs.CL
Comments: The paper examines AI's creativity, introducing Relative and
  Statistical Creativity for theoretical and practical analysis, along with
  practical training guidelines. Project Page: ai-relative-creativity.github.io
\\ ( https://arxiv.org/abs/2401.01623 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2211.03818
replaced with revised version Thu, 25 Jan 2024 09:30:11 GMT   (2707kb,D)

Title: Retrieval augmentation of large language models for lay language
  generation
Authors: Yue Guo, Wei Qiu, Gondy Leroy, Sheng Wang, Trevor Cohen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2211.03818 ,  2707kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00509
replaced with revised version Thu, 25 Jan 2024 14:52:51 GMT   (618kb)

Title: CultureBERT: Measuring Corporate Culture With Transformer-Based Language
  Models
Authors: Sebastian Koch and Stefan Pasch
Categories: cs.CL
Comments: 23 pages, 9 figures
Journal-ref: 2023 IEEE International Conference on Big Data (BigData)
DOI: 10.1109/BigData59044.2023.10386765
\\ ( https://arxiv.org/abs/2212.00509 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16416
replaced with revised version Thu, 25 Jan 2024 04:02:23 GMT   (4590kb,D)

Title: Improving Large Language Models for Clinical Named Entity Recognition
  via Prompt Engineering
Authors: Yan Hu, Qingyu Chen, Jingcheng Du, Xueqing Peng, Vipina Kuttichi
  Keloth, Xu Zuo, Yujia Zhou, Zehan Li, Xiaoqian Jiang, Zhiyong Lu, Kirk
  Roberts, Hua Xu
Categories: cs.CL
Comments: 17 pages, 5 tables, 6 figure
\\ ( https://arxiv.org/abs/2303.16416 ,  4590kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04928
replaced with revised version Thu, 25 Jan 2024 15:23:34 GMT   (231kb,D)

Title: From Zero to Hero: Harnessing Transformers for Biomedical Named Entity
  Recognition in Zero- and Few-shot Contexts
Authors: Milo\v{s} Ko\v{s}prdi\'c, Nikola Prodanovi\'c, Adela Ljaji\'c, Bojana
  Ba\v{s}aragin and Nikola Milo\v{s}evi\'c
Categories: cs.CL cs.AI
Comments: Collaboration between Bayer Pharma R&D and Serbian Institute for
  Artificial Intelligence Research and Development
\\ ( https://arxiv.org/abs/2305.04928 ,  231kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14603
replaced with revised version Thu, 25 Jan 2024 18:15:31 GMT   (8551kb,D)

Title: OpenPI2.0: An Improved Dataset for Entity Tracking in Texts
Authors: Li Zhang, Hainiu Xu, Abhinav Kommula, Chris Callison-Burch, Niket
  Tandon
Categories: cs.CL
Comments: In EACL 2024
\\ ( https://arxiv.org/abs/2305.14603 ,  8551kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00789
replaced with revised version Thu, 25 Jan 2024 07:45:45 GMT   (319kb,D)

Title: Improved Cross-Lingual Transfer Learning For Automatic Speech
  Translation
Authors: Sameer Khurana, Nauman Dawalatabad, Antoine Laurent, Luis Vicente,
  Pablo Gimeno, Victoria Mingote, James Glass
Categories: cs.CL cs.AI eess.AS eess.SP
\\ ( https://arxiv.org/abs/2306.00789 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08302
replaced with revised version Thu, 25 Jan 2024 00:48:34 GMT   (6907kb,D)

Title: Unifying Large Language Models and Knowledge Graphs: A Roadmap
Authors: Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, Xindong Wu
Categories: cs.CL cs.AI
Comments: A short version of this paper was accepted by IEEE Transactions on
  Knowledge and Data Engineering (TKDE)
Journal-ref: IEEE Transactions on Knowledge and Data Engineering (TKDE) 2024
DOI: 10.1109/TKDE.2024.3352100
\\ ( https://arxiv.org/abs/2306.08302 ,  6907kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13588
replaced with revised version Thu, 25 Jan 2024 17:52:07 GMT   (8049kb,D)

Title: System-Level Natural Language Feedback
Authors: Weizhe Yuan, Kyunghyun Cho, Jason Weston
Categories: cs.CL cs.AI
Comments: Accepted by EACL 2024
\\ ( https://arxiv.org/abs/2306.13588 ,  8049kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14806
replaced with revised version Thu, 25 Jan 2024 10:26:14 GMT   (82kb,D)

Title: A Positive-Unlabeled Metric Learning Framework for Document-Level
  Relation Extraction with Incomplete Labeling
Authors: Ye Wang, Huazheng Pan, Tao Zhang, Wen Wu, Wenxin Hu
Categories: cs.CL cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2306.14806 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00162
replaced with revised version Thu, 25 Jan 2024 02:50:53 GMT   (6563kb,D)

Title: What do self-supervised speech models know about words?
Authors: Ankita Pasad, Chung-Ming Chien, Shane Settle, Karen Livescu
Categories: cs.CL cs.LG eess.AS
Comments: This is a pre-MIT Press publication version
\\ ( https://arxiv.org/abs/2307.00162 ,  6563kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16778
replaced with revised version Thu, 25 Jan 2024 12:48:10 GMT   (7885kb,D)

Title: KoBBQ: Korean Bias Benchmark for Question Answering
Authors: Jiho Jin, Jiseon Kim, Nayeon Lee, Haneul Yoo, Alice Oh, Hwaran Lee
Categories: cs.CL cs.AI
Comments: TACL 2024 (pre-MIT Press publication version)
\\ ( https://arxiv.org/abs/2307.16778 ,  7885kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06450
replaced with revised version Thu, 25 Jan 2024 09:42:40 GMT   (4542kb,D)

Title: ERNetCL: A novel emotion recognition network in textual conversation
  based on curriculum learning strategy
Authors: Jiang Li, Xiaoping Wang, Yingjian Liu, Zhigang Zeng
Categories: cs.CL
Comments: Accepted by Knowledge-Based Systems (KBS)
DOI: 10.1016/j.knosys.2024.111434
\\ ( https://arxiv.org/abs/2308.06450 ,  4542kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04661
replaced with revised version Thu, 25 Jan 2024 03:50:57 GMT   (310kb,D)

Title: Massive Editing for Large Language Models via Meta Learning
Authors: Chenmien Tan and Ge Zhang and Jie Fu
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.04661 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04928
replaced with revised version Wed, 24 Jan 2024 19:38:52 GMT   (6513kb,D)

Title: Leveraging Large Language Models for Collective Decision-Making
Authors: Marios Papachristou, Longqi Yang, Chin-Chia Hsu
Categories: cs.CL cs.AI cs.HC cs.SI
Comments: Comparison with baselines, requirements analysis, expand related work
\\ ( https://arxiv.org/abs/2311.04928 ,  6513kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13892
replaced with revised version Thu, 25 Jan 2024 15:36:44 GMT   (414kb,D)

Title: General Phrase Debiaser: Debiasing Masked Language Models at a
  Multi-Token Level
Authors: Bingkang Shi, Xiaodan Zhang, Dehan Kong, Yulei Wu, Zongzhen Liu,
  Honglei Lyu, Longtao Huang
Categories: cs.CL cs.AI
Comments: Accepted by ICASSP 2024 as mian conference paper
\\ ( https://arxiv.org/abs/2311.13892 ,  414kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14067
replaced with revised version Wed, 24 Jan 2024 20:56:59 GMT   (655kb,D)

Title: Enhancing Task-Oriented Dialogues with Chitchat: a Comparative Study
  Based on Lexical Diversity and Divergence
Authors: Armand Stricker, Patrick Paroubek
Categories: cs.CL
Comments: Accepted @ ASRU 2023 Code:
  https://github.com/armandstrickernlp/Task-Chitchat-Entropy
DOI: 10.1109/ASRU57964.2023.10389695
\\ ( https://arxiv.org/abs/2311.14067 ,  655kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09043
replaced with revised version Thu, 25 Jan 2024 17:57:02 GMT   (370kb,D)

Title: Topic Bias in Emotion Classification
Authors: Maximilian Wegge and Roman Klinger
Categories: cs.CL
Comments: accepted to W-NUT at EACL 2024
\\ ( https://arxiv.org/abs/2312.09043 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05561
replaced with revised version Thu, 25 Jan 2024 17:49:03 GMT   (1500kb,D)

Title: TrustLLM: Trustworthiness in Large Language Models
Authors: Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie
  Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin
  Liu, Yijue Wang, Zhikun Zhang, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao,
  Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan
  Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal,
  James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang
  Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang
  He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu
  Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen,
  Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao
  Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, et al. (3
  additional authors not shown)
Categories: cs.CL
Comments: This work is still under work and we welcome your contribution
\\ ( https://arxiv.org/abs/2401.05561 ,  1500kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08491
replaced with revised version Wed, 24 Jan 2024 23:04:02 GMT   (1220kb,D)

Title: Contrastive Perplexity for Controlled Generation: An Application in
  Detoxifying Large Language Models
Authors: Tassilo Klein, Moin Nabi
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.08491 ,  1220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10286
replaced with revised version Thu, 25 Jan 2024 07:46:34 GMT   (169kb,D)

Title: Top in Chinese Data Processing: English Code Models
Authors: Linghan Zheng, Hui Liu, Xiaojun Lin, Jiayuan Dong, Yue Sheng, Gang
  Shi, Zhiwei Liu, Hongwei Chen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.10286 ,  169kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12522
replaced with revised version Thu, 25 Jan 2024 14:02:03 GMT   (2934kb,D)

Title: BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language
  Models
Authors: Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming
  Lu, Rong Xiao
Categories: cs.CL cs.AI cs.LG
Comments: An appendix has been included. Source code at
  https://github.com/linfeng93/BiTA
\\ ( https://arxiv.org/abs/2401.12522 ,  2934kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12756
replaced with revised version Thu, 25 Jan 2024 14:32:36 GMT   (1560kb,D)

Title: What the Weight?! A Unified Framework for Zero-Shot Knowledge
  Composition
Authors: Carolin Holtermann, Markus Frohmann, Navid Rekabsaz, Anne Lauscher
Categories: cs.CL cs.AI
Comments: Accepted to Findings of the ACL: EACL 2024
\\ ( https://arxiv.org/abs/2401.12756 ,  1560kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13527
replaced with revised version Thu, 25 Jan 2024 17:24:52 GMT   (611kb,D)

Title: SpeechGPT-Gen: Scaling Chain-of-Information Speech Generation
Authors: Dong Zhang, Xin Zhang, Jun Zhan, Shimin Li, Yaqian Zhou, Xipeng Qiu
Categories: cs.CL cs.SD eess.AS
Comments: work in progress
\\ ( https://arxiv.org/abs/2401.13527 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13601
replaced with revised version Thu, 25 Jan 2024 03:46:15 GMT   (5256kb,D)

Title: MM-LLMs: Recent Advances in MultiModal Large Language Models
Authors: Duzhen Zhang, Yahan Yu, Chenxing Li, Jiahua Dong, Dan Su, Chenhui Chu,
  Dong Yu
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.13601 ,  5256kb)
------------------------------------------------------------------------------
\\
arXiv:2103.07295
replaced with revised version Thu, 25 Jan 2024 02:42:20 GMT   (13694kb,D)

Title: Adversarial Graph Disentanglement
Authors: Shuai Zheng, Zhenfeng Zhu, Zhizhe Liu, Jian Cheng, Yao Zhao
Categories: cs.LG cs.AI
Comments: Accepted by IEEE Transactions on Artificial Intelligence
\\ ( https://arxiv.org/abs/2103.07295 ,  13694kb)
------------------------------------------------------------------------------
\\
arXiv:2103.11856
replaced with revised version Thu, 25 Jan 2024 08:55:05 GMT   (129kb,D)

Title: A Link between Coding Theory and Cross-Validation with Applications
Authors: Tapio Pahikkala, Parisa Movahedi, Ileana Montoya, Havu Miikonen,
  Stephan Foldes, Antti Airola, Laszlo Major
Categories: cs.LG cs.IT math.CO math.IT
\\ ( https://arxiv.org/abs/2103.11856 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2110.03301
replaced with revised version Thu, 25 Jan 2024 13:26:37 GMT   (3189kb,D)

Title: EvadeDroid: A Practical Evasion Attack on Machine Learning for Black-box
  Android Malware Detection
Authors: Hamid Bostani and Veelasha Moonsamy
Categories: cs.LG cs.CR
Comments: The paper was accepted by Elsevier Computers & Security on 20
  December 2023
Journal-ref: Computers & Security, Volume 139, 2024
DOI: 10.1016/j.cose.2023.103676
\\ ( https://arxiv.org/abs/2110.03301 ,  3189kb)
------------------------------------------------------------------------------
\\
arXiv:2206.03183
replaced with revised version Thu, 25 Jan 2024 13:23:56 GMT   (7125kb,D)

Title: Risk Measures and Upper Probabilities: Coherence and Stratification
Authors: Christian Fr\"ohlich and Robert C. Williamson
Categories: cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2206.03183 ,  7125kb)
------------------------------------------------------------------------------
\\
arXiv:2210.14051
replaced with revised version Thu, 25 Jan 2024 13:23:05 GMT   (72kb,D)

Title: Bridging Distributional and Risk-sensitive Reinforcement Learning with
  Provable Regret Bounds
Authors: Hao Liang, Zhi-Quan Luo
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2210.14051 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2210.14080
replaced with revised version Thu, 25 Jan 2024 12:11:03 GMT   (2701kb,D)

Title: Learning Individual Treatment Effects under Heterogeneous Interference
  in Networks
Authors: Ziyu Zhao, Yuqi Bai, Kun Kuang, Ruoxuan Xiong, Fei Wu
Categories: cs.LG cs.AI cs.SI stat.ME
\\ ( https://arxiv.org/abs/2210.14080 ,  2701kb)
------------------------------------------------------------------------------
\\
arXiv:2301.02515
replaced with revised version Thu, 25 Jan 2024 05:41:47 GMT   (15031kb,D)

Title: GNN-based Passenger Request Prediction
Authors: Aqsa Ashraf Makhdomi and Iqra Altaf Gillani
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2301.02515 ,  15031kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06595
replaced with revised version Wed, 24 Jan 2024 21:50:39 GMT   (534kb,D)

Title: When Can We Track Significant Preference Shifts in Dueling Bandits?
Authors: Joe Suk and Arpit Agarwal
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2302.06595 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10295
replaced with revised version Thu, 25 Jan 2024 17:51:12 GMT   (24858kb,D)

Title: Correlation Clustering with Active Learning of Pairwise Similarities
Authors: Linus Aronsson, Morteza Haghir Chehreghani
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2302.10295 ,  24858kb)
------------------------------------------------------------------------------
\\
arXiv:2303.03106
replaced with revised version Thu, 25 Jan 2024 16:19:09 GMT   (610kb,D)

Title: Rotation Invariant Quantization for Model Compression
Authors: Joseph Kampeas, Yury Nahshan, Hanoch Kremer, Gil Lederman, Shira
  Zaloshinski, Zheng Li and Emir Haleva
Categories: cs.LG cs.AI cs.IT math.IT
Comments: 19 pages, 5 figures, submitted to ICML 2023
ACM-class: I.2.4; E.4
\\ ( https://arxiv.org/abs/2303.03106 ,  610kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11835
replaced with revised version Thu, 25 Jan 2024 09:35:25 GMT   (82kb)

Title: Lipschitz-bounded 1D convolutional neural networks using the Cayley
  transform and the controllability Gramian
Authors: Patricia Pauli, Ruigang Wang, Ian R. Manchester, Frank Allg\"ower
Categories: cs.LG cs.SY eess.SY stat.ML
Comments: Published as a conference paper at CDC 2023
\\ ( https://arxiv.org/abs/2303.11835 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10818
replaced with revised version Thu, 25 Jan 2024 15:15:17 GMT   (8760kb,D)

Title: Diffusion Language Models Generation Can Be Halted Early
Authors: Sofia Maria Lo Cicero Vaina, Nikita Balagansky, Daniil Gavrilov
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2305.10818 ,  8760kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13764
replaced with revised version Thu, 25 Jan 2024 17:39:19 GMT   (604kb,D)

Title: Mitigating Label Noise through Data Ambiguation
Authors: Julian Lienen, Eyke H\"ullermeier
Categories: cs.LG
Comments: Paper incl. appendix accepted at AAAI-2024 (cf. copyright remark on
  title page), 20 pages, 9 figures
\\ ( https://arxiv.org/abs/2305.13764 ,  604kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15277
replaced with revised version Thu, 25 Jan 2024 15:58:06 GMT   (8119kb,D)

Title: Successor-Predecessor Intrinsic Exploration
Authors: Changmin Yu, Neil Burgess, Maneesh Sahani, Samuel J. Gershman
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.15277 ,  8119kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01690
replaced with revised version Thu, 25 Jan 2024 12:41:44 GMT   (8333kb,D)

Title: Context selectivity with dynamic availability enables lifelong continual
  learning
Authors: Martin Barry, Wulfram Gerstner, Guillaume Bellec
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2306.01690 ,  8333kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12774
replaced with revised version Thu, 25 Jan 2024 11:17:25 GMT   (798kb,D)

Title: Pure Exploration in Bandits with Linear Constraints
Authors: Emil Carlsson, Debabrota Basu, Fredrik D. Johansson, Devdatt Dubhashi
Categories: cs.LG
Comments: Accepted to AISTATS 2024
\\ ( https://arxiv.org/abs/2306.12774 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13119
replaced with revised version Thu, 25 Jan 2024 02:44:52 GMT   (36kb)

Title: Adversarial Resilience in Sequential Prediction via Abstention
Authors: Surbhi Goel, Steve Hanneke, Shay Moran, Abhishek Shetty
Categories: cs.LG cs.DS stat.ML
\\ ( https://arxiv.org/abs/2306.13119 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03761
replaced with revised version Thu, 25 Jan 2024 18:45:31 GMT   (1052kb,D)

Title: DyEdgeGAT: Dynamic Edge via Graph Attention for Early Fault Detection in
  IIoT Systems
Authors: Mengjie Zhao and Olga Fink
Categories: cs.LG cs.AI
Comments: 16 pages, 8 figures
\\ ( https://arxiv.org/abs/2307.03761 ,  1052kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09437
replaced with revised version Thu, 25 Jan 2024 15:52:19 GMT   (18462kb,D)

Title: Grounded Object Centric Learning
Authors: Avinash Kori, Francesco Locatello, Fabio De Sousa Ribeiro, Francesca
  Toni, Ben Glocker
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2307.09437 ,  18462kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12243
replaced with revised version Thu, 25 Jan 2024 17:15:41 GMT   (8057kb,D)

Title: Multi-Objective Optimization for Sparse Deep Multi-Task Learning
Authors: S. S. Hotegni, M. Berkemeier, S. Peitz
Categories: cs.LG cs.AI math.OC
Comments: 12 pages, 7 figures
\\ ( https://arxiv.org/abs/2308.12243 ,  8057kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13265
replaced with revised version Thu, 25 Jan 2024 10:16:46 GMT   (3198kb,D)

Title: Heterogeneous Federated Learning via Personalized Generative Networks
Authors: Zahra Taghiyarrenani, Abdallah Alabdallah, Slawomir Nowaczyk, Sepideh
  Pashami
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.13265 ,  3198kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14104
replaced with revised version Thu, 25 Jan 2024 07:44:10 GMT   (1362kb,D)

Title: Towards Generalizable Neural Solvers for Vehicle Routing Problems via
  Ensemble with Transferrable Local Policy
Authors: Chengrui Gao, Haopu Shang, Ke Xue, Dong Li, Chao Qian
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.14104 ,  1362kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04160
replaced with revised version Thu, 25 Jan 2024 17:14:33 GMT   (1765kb,D)

Title: PRISM: Leveraging Prototype Patient Representations with
  Feature-Missing-Aware Calibration for EHR Data Sparsity Mitigation
Authors: Yinghao Zhu, Zixiang Wang, Long He, Shiyun Xie, Liantao Ma, Chengwei
  Pan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.04160 ,  1765kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02373
replaced with revised version Wed, 24 Jan 2024 22:02:53 GMT   (23328kb,D)

Title: Secure and Effective Data Appraisal for Machine Learning
Authors: Xu Ouyang, Changhong Yang, Felix Xiaozhu Lin, Yangfeng Ji
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2310.02373 ,  23328kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07799
replaced with revised version Thu, 25 Jan 2024 18:00:05 GMT   (844kb,D)

Title: Domain-invariant Clinical Representation Learning by Bridging Data
  Distribution Shift across EMR Datasets
Authors: Zhongji Zhang, Yuhang Wang, Yinghao Zhu, Xinyu Ma, Tianlong Wang,
  Chaohe Zhang, Yasha Wang, Liantao Ma
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.07799 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17944
replaced with revised version Thu, 25 Jan 2024 15:52:51 GMT   (1032kb,D)

Title: A Survey on Trustworthy Edge Intelligence: From Security and Reliability
  To Transparency and Sustainability
Authors: Xiaojie Wang, Beibei Wang, Yu Wu, Zhaolong Ning, Song Guo, and Fei
  Richard Yu
Categories: cs.LG
Comments: 25 pages, 6 figures, 8 tables
\\ ( https://arxiv.org/abs/2310.17944 ,  1032kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10467
replaced with revised version Thu, 25 Jan 2024 15:51:21 GMT   (58kb)

Title: TrojFST: Embedding Trojans in Few-shot Prompt Tuning
Authors: Mengxin Zheng, Jiaqi Xue, Xun Chen, YanShan Wang, Qian Lou, and Lei
  Jiang
Categories: cs.LG
Comments: 9 pages
\\ ( https://arxiv.org/abs/2312.10467 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11819
replaced with revised version Thu, 25 Jan 2024 02:46:06 GMT   (1373kb,D)

Title: An Adaptive Placement and Parallelism Framework for Accelerating RLHF
  Training
Authors: Youshao Xiao, Weichang Wu, Zhenglei Zhou, Fagui Mao, Shangchun Zhao,
  Lin Ju, Lei Liang, Xiaolu Zhang, Jun Zhou
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2312.11819 ,  1373kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16020
replaced with revised version Wed, 24 Jan 2024 19:20:19 GMT   (3793kb,D)

Title: Robust Neural Pruning with Gradient Sampling Optimization for Residual
  Neural Networks
Authors: Juyoung Yun
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.16020 ,  3793kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03233
replaced with revised version Thu, 25 Jan 2024 17:50:54 GMT   (246kb)

Title: Convergence Rate Maximization for Split Learning-based Control of EMG
  Prosthetic Devices
Authors: Matea Marinova, Daniel Denkovski, Hristijan Gjoreski, Zoran
  Hadzi-Velkov, Valentin Rakovic
Categories: cs.LG cs.AI eess.SP
Comments: 8 pages, 7 figures, corrected typos
\\ ( https://arxiv.org/abs/2401.03233 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05304
replaced with revised version Thu, 25 Jan 2024 05:14:26 GMT   (221kb,D)

Title: Can Probabilistic Feedback Drive User Impacts in Online Platforms?
Authors: Jessica Dai, Bailey Flanigan, Nika Haghtalab, Meena Jagadeesan, Chara
  Podimata
Categories: cs.LG cs.CY
Comments: Authors listed in alphabetical order. Accept as poster at AISTATS
  2024
\\ ( https://arxiv.org/abs/2401.05304 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08534
replaced with revised version Thu, 25 Jan 2024 15:06:40 GMT   (6375kb,D)

Title: DiConStruct: Causal Concept-based Explanations through Black-Box
  Distillation
Authors: Ricardo Moreira, Jacopo Bono, M\'ario Cardoso, Pedro Saleiro, M\'ario
  A. T. Figueiredo, Pedro Bizarro
Categories: cs.LG cs.AI cs.HC
Comments: Accepted at Conference on Causal Learning and Reasoning (CLeaR 2024,
  https://www.cclear.cc/2024). To be published at Proceedings of Machine
  Learning Research (PMLR)
\\ ( https://arxiv.org/abs/2401.08534 ,  6375kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08703
replaced with revised version Thu, 25 Jan 2024 11:09:38 GMT   (2493kb,D)

Title: Decoupled Prototype Learning for Reliable Test-Time Adaptation
Authors: Guowei Wang, Changxing Ding, Wentao Tan, Mingkui Tan
Categories: cs.LG
Comments: 12 pages, 5 figures
\\ ( https://arxiv.org/abs/2401.08703 ,  2493kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08895
replaced with revised version Thu, 25 Jan 2024 06:04:30 GMT   (1034kb,D)

Title: cedar: Composable and Optimized Machine Learning Input Data Pipelines
Authors: Mark Zhao, Emanuel Adamiak, Christos Kozyrakis
Categories: cs.LG cs.DC cs.PF
\\ ( https://arxiv.org/abs/2401.08895 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09750
replaced with revised version Thu, 25 Jan 2024 11:36:00 GMT   (11350kb,D)

Title: Exploration and Anti-Exploration with Distributional Random Network
  Distillation
Authors: Kai Yang, Jian Tao, Jiafei Lyu, Xiu Li
Categories: cs.LG
Comments: Submitted to ICML 2024
\\ ( https://arxiv.org/abs/2401.09750 ,  11350kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10809
replaced with revised version Wed, 24 Jan 2024 19:09:06 GMT   (1174kb,D)

Title: Neglected Hessian component explains mysteries in Sharpness
  regularization
Authors: Yann N. Dauphin, Atish Agarwala, Hossein Mobahi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.10809 ,  1174kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10895
replaced with revised version Thu, 25 Jan 2024 17:38:36 GMT   (47412kb,D)

Title: AI in Supply Chain Risk Assessment: A Systematic Literature Review and
  Bibliometric Analysis
Authors: Md Abrar Jahin, Saleh Akram Naife, Anik Kumar Saha, and M. F. Mridha
Categories: cs.LG cs.CE
\\ ( https://arxiv.org/abs/2401.10895 ,  47412kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12012
replaced with revised version Thu, 25 Jan 2024 17:27:10 GMT   (10103kb,D)

Title: TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for
  Lazy Clients
Authors: Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci
Categories: cs.LG cs.DC
Comments: Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
Journal-ref: Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
\\ ( https://arxiv.org/abs/2401.12012 ,  10103kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12236
replaced with revised version Thu, 25 Jan 2024 14:57:48 GMT   (50kb)

Title: The Surprising Harmfulness of Benign Overfitting for Adversarial
  Robustness
Authors: Yifan Hao, Tong Zhang
Categories: cs.LG cs.CR stat.ML
\\ ( https://arxiv.org/abs/2401.12236 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12689
replaced with revised version Thu, 25 Jan 2024 04:37:38 GMT   (7738kb,D)

Title: Energy-based Automated Model Evaluation
Authors: Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: ICLR2024 poster paper
\\ ( https://arxiv.org/abs/2401.12689 ,  7738kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12806
replaced with revised version Thu, 25 Jan 2024 12:53:39 GMT   (6185kb,D)

Title: Binary structured physics-informed neural networks for solving equations
  with rapidly changing solutions
Authors: Yanzhi Liu and Ruifan Wu and Ying Jiang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.12806 ,  6185kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13360
replaced with revised version Thu, 25 Jan 2024 04:55:08 GMT   (2949kb,D)

Title: Debiased Sample Selection for Combating Noisy Labels
Authors: Qi Wei, Lei Feng, Haobo Wang, Bo An
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.13360 ,  2949kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13530
replaced with revised version Thu, 25 Jan 2024 07:01:34 GMT   (58kb)

Title: Continuous-time Riemannian SGD and SVRG Flows on Wasserstein
  Probabilistic Space
Authors: Mingyang Yi, Bohan Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.13530 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13652
replaced with revised version Thu, 25 Jan 2024 02:10:47 GMT   (2952kb,D)

Title: Graph-Informed Neural Networks for Sparse Grid-Based Discontinuity
  Detectors
Authors: Francesco Della Santa and Sandra Pieraccini
Categories: cs.LG cs.AI cs.NA math.NA
MSC-class: 68T07, 03D32, 65D40
\\ ( https://arxiv.org/abs/2401.13652 ,  2952kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13657
replaced with revised version Thu, 25 Jan 2024 12:31:21 GMT   (368kb,D)

Title: Inadequacy of common stochastic neural networks for reliable clinical
  decision support
Authors: Adrian Lindenmeyer, Malte Blattmann, Stefan Franke, Thomas Neumuth,
  Daniel Schneider
Categories: cs.LG cs.AI
Comments: Keywords: probabilistic inference, uncertainty estimation,
  uncertainty quantification, epistemic uncertainty, clinical prognosis,
  electronic health records
\\ ( https://arxiv.org/abs/2401.13657 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2209.11812
replaced with revised version Thu, 25 Jan 2024 04:46:53 GMT   (1266kb,D)

Title: Explanations, Fairness, and Appropriate Reliance in Human-AI
  Decision-Making
Authors: Jakob Schoeffer, Maria De-Arteaga, Niklas Kuehl
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2209.11812 ,  1266kb)
------------------------------------------------------------------------------
\\
arXiv:2211.01839
replaced with revised version Thu, 25 Jan 2024 16:49:33 GMT   (447kb,D)

Title: HyperSound: Generating Implicit Neural Representations of Audio Signals
  with Hypernetworks
Authors: Filip Szatkowski, Karol J. Piczak, Przemys{\l}aw Spurek, Jacek Tabor,
  Tomasz Trzci\'nski
Categories: cs.SD cs.AI cs.LG cs.NE eess.AS
Comments: NeurIPS 2022 MetaLearn workshop
\\ ( https://arxiv.org/abs/2211.01839 ,  447kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01622 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 14:55:17 GMT   (13271kb,D)

Title: Private, fair and accurate: Training large-scale, privacy-preserving AI
  models in medical imaging
Authors: Soroosh Tayebi Arasteh, Alexander Ziller, Christiane Kuhl, Marcus
  Makowski, Sven Nebelung, Rickmer Braren, Daniel Rueckert, Daniel Truhn,
  Georgios Kaissis
Categories: eess.IV cs.AI cs.CR cs.CV cs.LG
Comments: To appear in Communications Medicine. 2024. Nature Portfolio
\\ ( https://arxiv.org/abs/2302.01622 ,  13271kb)
------------------------------------------------------------------------------
\\
arXiv:2303.13496
replaced with revised version Thu, 25 Jan 2024 03:20:12 GMT   (411kb,D)

Title: The effectiveness of MAE pre-pretraining for billion-scale pretraining
Authors: Mannat Singh, Quentin Duval, Kalyan Vasudev Alwala, Haoqi Fan, Vaibhav
  Aggarwal, Aaron Adcock, Armand Joulin, Piotr Doll\'ar, Christoph
  Feichtenhofer, Ross Girshick, Rohit Girdhar, Ishan Misra
Categories: cs.CV cs.AI cs.LG
Comments: ICCV 2023. Models available at
  https://github.com/facebookresearch/maws/
\\ ( https://arxiv.org/abs/2303.13496 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05207
replaced with revised version Thu, 25 Jan 2024 16:29:03 GMT   (5880kb,D)

Title: Facial Action Unit Detection Based on Multi-task Learning Strategy for
  Unlabeled Facial Images in the Wild
Authors: Ziqiao Shang, Bin Liu
Categories: cs.CV cs.AI cs.LG
Comments: 15 pages, 6 figure, submitted to Expert Systems with Applications
\\ ( https://arxiv.org/abs/2310.05207 ,  5880kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08543 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 01:46:03 GMT   (3467kb,D)

Title: 2D-RC: Two-Dimensional Neural Network Approach for OTFS Symbol Detection
Authors: Jiarui Xu, Karim Said, Lizhong Zheng, and Lingjia Liu
Categories: eess.SP cs.AI cs.LG
Comments: 15 pages, journal submission
\\ ( https://arxiv.org/abs/2311.08543 ,  3467kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15193
replaced with revised version Thu, 25 Jan 2024 06:32:22 GMT   (3348kb,D)

Title: IA-LSTM: Interaction-Aware LSTM for Pedestrian Trajectory Prediction
Authors: Yuehai Chen
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.15193 ,  3348kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18296
replaced with revised version Thu, 25 Jan 2024 01:18:28 GMT   (4378kb,D)

Title: Perceptual Group Tokenizer: Building Perception with Iterative Grouping
Authors: Zhiwei Deng, Ting Chen, Yang Li
Categories: cs.CV cs.AI
Comments: The International Conference on Learning Representations (ICLR) 2024
\\ ( https://arxiv.org/abs/2311.18296 ,  4378kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03365
replaced with revised version Thu, 25 Jan 2024 07:36:29 GMT   (500kb,D)

Title: Demand response for residential building heating: Effective Monte Carlo
  Tree Search control based on physics-informed neural networks
Authors: Fabio Pavirani, Gargya Gokhale, Bert Claessens, Chris Develder
Categories: eess.SY cs.AI cs.SY
\\ ( https://arxiv.org/abs/2312.03365 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05925
replaced with revised version Thu, 25 Jan 2024 11:51:22 GMT   (16269kb,D)

Title: CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians with
  Dual Feature Fusion
Authors: Bin Dou, Tianyu Zhang, Yongjia Ma, Zhaohui Wang, Zejian Yuan
Categories: cs.CV cs.AI
Comments: Correct writing details
\\ ( https://arxiv.org/abs/2401.05925 ,  16269kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08655
replaced with revised version Thu, 25 Jan 2024 02:29:00 GMT   (3873kb,D)

Title: SAiD: Speech-driven Blendshape Facial Animation with Diffusion
Authors: Inkyu Park, Jaewoong Cho
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
Comments: Fix bug related to the font size
\\ ( https://arxiv.org/abs/2401.08655 ,  3873kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10529
replaced with revised version Thu, 25 Jan 2024 04:11:57 GMT   (44887kb,D)

Title: Mementos: A Comprehensive Benchmark for Multimodal Large Language Model
  Reasoning over Image Sequences
Authors: Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong
  He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, Furong
  Huang
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 27 pages, 23 figures
\\ ( https://arxiv.org/abs/2401.10529 ,  44887kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13138
replaced with revised version Thu, 25 Jan 2024 09:35:06 GMT   (718kb,D)

Title: Visibility into AI Agents
Authors: Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond,
  Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt,
  Lennart Heim, Markus Anderljung
Categories: cs.CY cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2401.13138 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13324
replaced with revised version Thu, 25 Jan 2024 10:38:26 GMT   (2521kb)

Title: Information That Matters: Exploring Information Needs of People Affected
  by Algorithmic Decisions
Authors: Timoth\'ee Schmude, Laura Koesten, Torsten M\"oller, Sebastian
  Tschiatschek
Categories: cs.HC cs.AI
Comments: Main text: 21 pages, 3 figures. Supplementary material is provided.
  Manuscript submitted for review to IJHCS
\\ ( https://arxiv.org/abs/2401.13324 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01673 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 13:44:10 GMT   (2668kb,D)

Title: Disentanglement in a GAN for Unconditional Speech Synthesis
Authors: Matthew Baas and Herman Kamper
Categories: eess.AS cs.CL cs.SD
Comments: 12 pages, 5 tables, 4 figures. Accepted to IEEE TASLP. arXiv admin
  note: substantial text overlap with arXiv:2210.05271
\\ ( https://arxiv.org/abs/2307.01673 ,  2668kb)
------------------------------------------------------------------------------
\\
arXiv:2006.05956 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 09:42:25 GMT   (36kb)

Title: Gradient Flows for Regularized Stochastic Control Problems
Authors: David \v{S}i\v{s}ka and {\L}ukasz Szpruch
Categories: math.OC cs.LG math.PR
MSC-class: 93E20, 60H30, 37L40
\\ ( https://arxiv.org/abs/2006.05956 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2108.00473 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 15:15:45 GMT   (576kb,D)

Title: Derivative-free Alternating Projection Algorithms for General
  Nonconvex-Concave Minimax Problems
Authors: Zi Xu, Ziqi Wang, Jingjing Shen, Yuhong Dai
Categories: math.OC cs.LG stat.ML
\\ ( https://arxiv.org/abs/2108.00473 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2111.09790 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 04:39:53 GMT   (3018kb,D)

Title: MCCE: Monte Carlo sampling of realistic counterfactual explanations
Authors: Annabelle Redelmeier, Martin Jullum, Kjersti Aas, Anders L{\o}land
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2111.09790 ,  3018kb)
------------------------------------------------------------------------------
\\
arXiv:2208.02107 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 09:38:55 GMT   (2136kb,D)

Title: Convolutional Persistence Transforms
Authors: Elchanan Solomon, Paul Bendich
Categories: math.AT cs.LG
Comments: Updated paper with new results and proofs written more clearly
\\ ( https://arxiv.org/abs/2208.02107 ,  2136kb)
------------------------------------------------------------------------------
\\
arXiv:2211.12612 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 02:31:43 GMT   (1022kb)

Title: Transfer Learning for Contextual Multi-armed Bandits
Authors: Changxiao Cai, T. Tony Cai, Hongzhe Li
Categories: stat.ML cs.LG math.ST stat.TH
Comments: Accepted to the Annals of Statistics
\\ ( https://arxiv.org/abs/2211.12612 ,  1022kb)
------------------------------------------------------------------------------
\\
arXiv:2212.09437
replaced with revised version Thu, 25 Jan 2024 14:06:18 GMT   (5243kb,D)

Title: Machine Learning Systems are Bloated and Vulnerable
Authors: Huaifeng Zhang, Fahmi Abdulqadir Ahmed, Dyako Fatih, Akayou Kitessa,
  Mohannad Alhanahnah, Philipp Leitner, Ahmed Ali-Eldin
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2212.09437 ,  5243kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01757
replaced with revised version Wed, 24 Jan 2024 23:58:13 GMT   (904kb,D)

Title: RS-Del: Edit Distance Robustness Certificates for Sequence Classifiers
  via Randomized Deletion
Authors: Zhuoqun Huang, Neil G. Marchant, Keane Lucas, Lujo Bauer, Olga
  Ohrimenko and Benjamin I. P. Rubinstein
Categories: cs.CR cs.LG stat.ML
Comments: Final camera-ready version for NeurIPS 2023. 36 pages, 7 figures, 12
  tables. Includes 20 pages of appendices. Code available at
  https://github.com/Dovermore/randomized-deletion
\\ ( https://arxiv.org/abs/2302.01757 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03868 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 02:47:34 GMT   (11663kb,D)

Title: A Generalized Surface Loss for Reducing the Hausdorff Distance in
  Medical Imaging Segmentation
Authors: Adrian Celaya, Beatrice Riviere, and David Fuentes
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2302.03868 ,  11663kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04136
replaced with revised version Thu, 25 Jan 2024 10:31:28 GMT   (3401kb,D)

Title: Domain Randomization for Robust, Affordable and Effective Closed-loop
  Control of Soft Robots
Authors: Gabriele Tiboni, Andrea Protopapa, Tatiana Tommasi, Giuseppe Averta
Categories: cs.RO cs.LG
Comments: Presented as conference paper at IEEE/RSJ IROS 2023, Detroit, USA.
  Project website at https://andreaprotopapa.github.io/dr-soro/
\\ ( https://arxiv.org/abs/2303.04136 ,  3401kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03223
replaced with revised version Thu, 25 Jan 2024 15:36:24 GMT   (2556kb,D)

Title: Structural Group Unfairness: Measurement and Mitigation by means of the
  Effective Resistance
Authors: Adrian Arnaiz-Rodriguez, Georgina Curto, Nuria Oliver
Categories: cs.SI cs.LG
Comments: 19 pages, 7 figures
ACM-class: K.4.2; F.2.0; I.3
\\ ( https://arxiv.org/abs/2305.03223 ,  2556kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14486
replaced with revised version Wed, 24 Jan 2024 20:41:54 GMT   (3299kb,D)

Title: Point2SSM: Learning Morphological Variations of Anatomies from Point
  Cloud
Authors: Jadie Adams and Shireen Elhabian
Categories: cs.CV cs.LG
Comments: Accepted as a Spotlight presentation at ICLR 2024
\\ ( https://arxiv.org/abs/2305.14486 ,  3299kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03334 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 01:01:33 GMT   (118kb,D)

Title: Variational quantum regression algorithm with encoded data structure
Authors: C.-C. Joseph Wang and Ryan S. Bennink
Categories: quant-ph cs.LG
\\ ( https://arxiv.org/abs/2307.03334 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10895
replaced with revised version Thu, 25 Jan 2024 12:59:04 GMT   (27595kb,D)

Title: Variational Autoencoding of Dental Point Clouds
Authors: Johan Ziruo Ye, Thomas {\O}rkild, Peter Lempel S{\o}ndergaard,
  S{\o}ren Hauberg
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2307.10895 ,  27595kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06548 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 01:25:33 GMT   (40kb)

Title: Online Infinite-Dimensional Regression: Learning Linear Operators
Authors: Vinod Raman, Unique Subedi, Ambuj Tewari
Categories: stat.ML cs.LG
Comments: 21 pages, ALT 2024 Camera Ready
\\ ( https://arxiv.org/abs/2309.06548 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07159 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 16:35:00 GMT   (4548kb,D)

Title: A Strong and Simple Deep Learning Baseline for BCI MI Decoding
Authors: Yassine El Ouahidi, Vincent Gripon, Bastien Pasdeloup, Ghaith
  Bouallegue, Nicolas Farrugia and Giulia Lioi
Categories: eess.SP cs.LG q-bio.NC
\\ ( https://arxiv.org/abs/2309.07159 ,  4548kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08421 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 20:25:02 GMT   (0kb,I)

Title: MIML: Multiplex Image Machine Learning for High Precision Cell
  Classification via Mechanical Traits within Microfluidic Systems
Authors: Khayrul Islam, Ratul Paul, Shen Wang, and Yaling Liu
Categories: eess.IV cs.CV cs.LG q-bio.QM
Comments: major change
\\ ( https://arxiv.org/abs/2309.08421 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09543
replaced with revised version Thu, 25 Jan 2024 15:35:26 GMT   (3404kb,D)

Title: Benchmarking the Sim-to-Real Gap in Cloth Manipulation
Authors: David Blanco-Mulero, Oriol Barbany, Gokhan Alcan, Adri\`a Colom\'e,
  Carme Torras, Ville Kyrki
Categories: cs.RO cs.CV cs.LG
Comments: Accepted to IEEE Robotics and Automation Letters (RA-L). 8 pages, 6
  figures. Supplementary material available at
  https://sites.google.com/view/cloth-sim2real-benchmark
\\ ( https://arxiv.org/abs/2310.09543 ,  3404kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04234 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 03:00:54 GMT   (921kb)

Title: Leveraging sinusoidal representation networks to predict fMRI signals
  from EEG
Authors: Yamin Li, Ange Lou, Ziyuan Xu, Shiyu Wang, Catie Chang
Categories: eess.SP cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.04234 ,  921kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09852
replaced with revised version Thu, 25 Jan 2024 08:53:35 GMT   (1722kb,D)

Title: Short vs. Long-term Coordination of Drones: When Distributed
  Optimization Meets Deep Reinforcement Learning
Authors: Chuhao Qin and Evangelos Pournaras
Categories: cs.RO cs.LG cs.MA
Comments: 12 pages, 11 figures
\\ ( https://arxiv.org/abs/2311.09852 ,  1722kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00024
replaced with revised version Wed, 24 Jan 2024 20:52:07 GMT   (526kb,D)

Title: Can LLMs Patch Security Issues?
Authors: Kamel Alrashedy, Abdullah Aljasser
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.00024 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03311 (*cross-listing*)
replaced with revised version Wed, 24 Jan 2024 19:28:43 GMT   (217kb,D)

Title: On the Nystrom Approximation for Preconditioning in Kernel Machines
Authors: Amirhesam Abedsoltan, Parthe Pandit, Luis Rademacher, Mikhail Belkin
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2312.03311 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04402
replaced with revised version Thu, 25 Jan 2024 13:49:15 GMT   (25332kb,D)

Title: Semi-Supervised Active Learning for Semantic Segmentation in Unknown
  Environments Using Informative Path Planning
Authors: Julius R\"uckin, Federico Magistri, Cyrill Stachniss, Marija Popovi\'c
Categories: cs.RO cs.LG
Comments: 8 pages, 9 figures
\\ ( https://arxiv.org/abs/2312.04402 ,  25332kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00744 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 09:16:15 GMT   (1586kb,D)

Title: Harmonizing Covariance and Expressiveness for Deep Hamiltonian
  Regression in Crystalline Material Research: a Hybrid Cascaded Regression
  Framework
Authors: Shi Yin, Xinyang Pan, Xudong Zhu, Tianyu Gao, Haochong Zhang, Feng Wu,
  Lixin He
Categories: physics.comp-ph cond-mat.mtrl-sci cs.LG
\\ ( https://arxiv.org/abs/2401.00744 ,  1586kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08859
replaced with revised version Thu, 25 Jan 2024 16:34:22 GMT   (2767kb,D)

Title: Shabari: Delayed Decision-Making for Faster and Efficient Serverless
  Functions
Authors: Prasoon Sinha and Kostis Kaffes and Neeraja J. Yadwadkar
Categories: cs.DC cs.LG
Comments: 17 pages, 14 figures, update typo in manually entered arxiv title
\\ ( https://arxiv.org/abs/2401.08859 ,  2767kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09627 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 17:09:21 GMT   (2299kb)

Title: SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of
  Lumbar Spine MRI
Authors: Jiasong Chen, Linchen Qian, Linhai Ma, Timur Urakov, Weiyong Gu, Liang
  Liang
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.09627 ,  2299kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10949
replaced with revised version Wed, 24 Jan 2024 20:43:24 GMT   (112kb)

Title: The Synergy Between Optimal Transport Theory and Multi-Agent
  Reinforcement Learning
Authors: Ali Baheri and Mykel J. Kochenderfer
Categories: cs.MA cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2401.10949 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13536 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 16:47:18 GMT   (2767kb,D)

Title: Finetuning Foundation Models for Joint Analysis Optimization
Authors: Matthias Vigl and Nicole Hartman and Lukas Heinrich
Categories: hep-ex cs.LG hep-ph physics.data-an
Comments: 13 pages, 12 figures
\\ ( https://arxiv.org/abs/2401.13536 ,  2767kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13537 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 16:44:01 GMT   (660kb,D)

Title: Masked Particle Modeling on Sets: Towards Self-Supervised High Energy
  Physics Foundation Models
Authors: Lukas Heinrich and Tobias Golling and Michael Kagan and Samuel Klein
  and Matthew Leigh and Margarita Osadchy and John Andrew Raine
Categories: hep-ph cs.LG hep-ex physics.data-an
\\ ( https://arxiv.org/abs/2401.13537 ,  660kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
