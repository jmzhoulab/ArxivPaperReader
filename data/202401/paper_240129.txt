Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月29日 16:38
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu 25 Jan 24 19:00:00 GMT  to  Fri 26 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.14461
Date: Thu, 25 Jan 2024 19:00:25 GMT   (359kb,D)

Title: Marabou 2.0: A Versatile Formal Analyzer of Neural Networks
Authors: Haoze Wu, Omri Isac, Aleksandar Zelji\'c, Teruhiro Tagomori, Matthew
  Daggitt, Wen Kokke, Idan Refaeli, Guy Amir, Kyle Julian, Shahaf Bassan, Pei
  Huang, Ori Lahav, Min Wu, Min Zhang, Ekaterina Komendantskaya, Guy Katz, and
  Clark Barrett
Categories: cs.AI cs.LG cs.LO
Comments: In submission
\\
  This paper serves as a comprehensive system description of version 2.0 of the
Marabou framework for formal analysis of neural networks. We discuss the tool's
architectural design and highlight the major features and components introduced
since its initial release.
\\ ( https://arxiv.org/abs/2401.14461 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14511
Date: Thu, 25 Jan 2024 21:11:08 GMT   (47kb)

Title: Automated legal reasoning with discretion to act using s(LAW)
Authors: Joaqu\'in Arias, Mar Moreno-Rebato, Jos\'e A. Rodr\'iguez-Garc\'ia,
  Sascha Ossowski
Categories: cs.AI
ACM-class: I.2.1
Journal-ref: Artificial Intelligence and Law (2023)
DOI: 10.1007/s10506-023-09376-5
\\
  Automated legal reasoning and its application in smart contracts and
automated decisions are increasingly attracting interest. In this context,
ethical and legal concerns make it necessary for automated reasoners to justify
in human-understandable terms the advice given. Logic Programming, specially
Answer Set Programming, has a rich semantics and has been used to very
concisely express complex knowledge. However, modelling discretionality to act
and other vague concepts such as ambiguity cannot be expressed in top-down
execution models based on Prolog, and in bottom-up execution models based on
ASP the justifications are incomplete and/or not scalable. We propose to use
s(CASP), a top-down execution model for predicate ASP, to model vague concepts
following a set of patterns. We have implemented a framework, called s(LAW), to
model, reason, and justify the applicable legislation and validate it by
translating (and benchmarking) a representative use case, the criteria for the
admission of students in the "Comunidad de Madrid".
\\ ( https://arxiv.org/abs/2401.14511 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14636
Date: Fri, 26 Jan 2024 04:00:07 GMT   (845kb,D)

Title: Efficient Constraint Generation for Stochastic Shortest Path Problems
Authors: Johannes Schmalz, Felipe Trevizan
Categories: cs.AI
Comments: Extended version of AAAI 2024 paper
\\
  Current methods for solving Stochastic Shortest Path Problems (SSPs) find
states' costs-to-go by applying Bellman backups, where state-of-the-art methods
employ heuristics to select states to back up and prune. A fundamental
limitation of these algorithms is their need to compute the cost-to-go for
every applicable action during each state backup, leading to unnecessary
computation for actions identified as sub-optimal. We present new connections
between planning and operations research and, using this framework, we address
this issue of unnecessary computation by introducing an efficient version of
constraint generation for SSPs. This technique allows algorithms to ignore
sub-optimal actions and avoid computing their costs-to-go. We also apply our
novel technique to iLAO* resulting in a new algorithm, CG-iLAO*. Our
experiments show that CG-iLAO* ignores up to 57% of iLAO*'s actions and it
solves problems up to 8x and 3x faster than LRTDP and iLAO*.
\\ ( https://arxiv.org/abs/2401.14636 ,  845kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14743
Date: Fri, 26 Jan 2024 10:05:41 GMT   (2057kb)

Title: Synthetic Multimodal Dataset for Empowering Safety and Well-being in
  Home Environments
Authors: Takanori Ugai, Shusaku Egami, Swe Nwe Nwe Htun, Kouji Kozaki, Takahiro
  Kawamura, Ken Fukuda
Categories: cs.AI
Comments: 7 pages, 2 figures,4 tables
\\
  This paper presents a synthetic multimodal dataset of daily activities that
fuses video data from a 3D virtual space simulator with knowledge graphs
depicting the spatiotemporal context of the activities. The dataset is
developed for the Knowledge Graph Reasoning Challenge for Social Issues
(KGRC4SI), which focuses on identifying and addressing hazardous situations in
the home environment. The dataset is available to the public as a valuable
resource for researchers and practitioners developing innovative solutions
recognizing human behaviors to enhance safety and well-being in
\\ ( https://arxiv.org/abs/2401.14743 ,  2057kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14811
Date: Fri, 26 Jan 2024 12:18:29 GMT   (49kb)

Title: On the Limitations of Markovian Rewards to Express Multi-Objective,
  Risk-Sensitive, and Modal Tasks
Authors: Joar Skalse and Alessandro Abate
Categories: cs.AI cs.LG
Journal-ref: Proceedings of the Thirty-Ninth Conference on Uncertainty in
  Artificial Intelligence, PMLR 216:1974-1984, 2023
\\
  In this paper, we study the expressivity of scalar, Markovian reward
functions in Reinforcement Learning (RL), and identify several limitations to
what they can express. Specifically, we look at three classes of RL tasks;
multi-objective RL, risk-sensitive RL, and modal RL. For each class, we derive
necessary and sufficient conditions that describe when a problem in this class
can be expressed using a scalar, Markovian reward. Moreover, we find that
scalar, Markovian rewards are unable to express most of the instances in each
of these three classes. We thereby contribute to a more complete understanding
of what standard reward functions can and cannot express. In addition to this,
we also call attention to modal problems as a new class of problems, since they
have so far not been given any systematic treatment in the RL literature. We
also briefly outline some approaches for solving some of the problems we
discuss, by means of bespoke RL algorithms.
\\ ( https://arxiv.org/abs/2401.14811 ,  49kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14923
Date: Fri, 26 Jan 2024 14:59:48 GMT   (16254kb,D)

Title: Reinforcement Learning Interventions on Boundedly Rational Human Agents
  in Frictionful Tasks
Authors: Eura Nofshin, Siddharth Swaroop, Weiwei Pan, Susan Murphy, Finale
  Doshi-Velez
Categories: cs.AI cs.LG
Comments: In AAMAS 2024
\\
  Many important behavior changes are frictionful; they require individuals to
expend effort over a long period with little immediate gratification. Here, an
artificial intelligence (AI) agent can provide personalized interventions to
help individuals stick to their goals. In these settings, the AI agent must
personalize rapidly (before the individual disengages) and interpretably, to
help us understand the behavioral interventions. In this paper, we introduce
Behavior Model Reinforcement Learning (BMRL), a framework in which an AI agent
intervenes on the parameters of a Markov Decision Process (MDP) belonging to a
boundedly rational human agent. Our formulation of the human decision-maker as
a planning agent allows us to attribute undesirable human policies (ones that
do not lead to the goal) to their maladapted MDP parameters, such as an
extremely low discount factor. Furthermore, we propose a class of tractable
human models that captures fundamental behaviors in frictionful tasks.
Introducing a notion of MDP equivalence specific to BMRL, we theoretically and
empirically show that AI planning with our human models can lead to helpful
policies on a wide range of more complex, ground-truth humans.
\\ ( https://arxiv.org/abs/2401.14923 ,  16254kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14933
Date: Fri, 26 Jan 2024 15:11:31 GMT   (849kb)

Title: SSDOnt: an Ontology for representing Single-Subject Design Studies
Authors: Idoia Berges, Jes\'us Berm\'udez, Arantza Illarramendi
Categories: cs.AI
Comments: This document is the Accepted Manuscript version of a Published Work
  that appeared in final form in Methods of Information in Medicine 57(01/02) :
  55-61 (2018), copyright 2018 Schattauer. To access the final edited and
  published work see https://doi.org/10.3414/ME17-01-0109
Journal-ref: Methods of Information in Medicine 57(01/02) : 55-61 (2018)
DOI: 10.3414/ME17-01-0109
\\
  Background: Single-Subject Design is used in several areas such as education
and biomedicine. However, no suited formal vocabulary exists for annotating the
detailed configuration and the results of this type of research studies with
the appropriate granularity for looking for information about them. Therefore,
the search for those study designs relies heavily on a syntactical search on
the abstract, keywords or full text of the publications about the study, which
entails some limitations. Objective: To present SSDOnt, a specific purpose
ontology for describing and annotating single-subject design studies, so that
complex questions can be asked about them afterwards. Methods: The ontology was
developed following the NeOn methodology. Once the requirements of the ontology
were defined, a formal model was described in a Description Logic and later
implemented in the ontology language OWL 2 DL. Results: We show how the
ontology provides a reference model with a suitable terminology for the
annotation and searching of single-subject design studies and their main
components, such as the phases, the intervention types, the outcomes and the
results. Some mappings with terms of related ontologies have been established.
We show as proof-of-concept that classes in the ontology can be easily extended
to annotate more precise information about specific interventions and outcomes
such as those related to autism. Moreover, we provide examples of some types of
queries that can be posed to the ontology. Conclusions: SSDOnt has achieved the
purpose of covering the descriptions of the domain of single-subject research
studies.
\\ ( https://arxiv.org/abs/2401.14933 ,  849kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14440
Date: Thu, 25 Jan 2024 14:47:05 GMT   (7082kb,D)

Title: Semantic Sensitivities and Inconsistent Predictions: Measuring the
  Fragility of NLI Models
Authors: Erik Arakelyan, Zhaoqi Liu, Isabelle Augenstein
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: EACL 2024
\\
  Recent studies of the emergent capabilities of transformer-based Natural
Language Understanding (NLU) models have indicated that they have an
understanding of lexical and compositional semantics. We provide evidence that
suggests these claims should be taken with a grain of salt: we find that
state-of-the-art Natural Language Inference (NLI) models are sensitive towards
minor semantics preserving surface-form variations, which lead to sizable
inconsistent model decisions during inference. Notably, this behaviour differs
from valid and in-depth comprehension of compositional semantics, however does
neither emerge when evaluating model accuracy on standard benchmarks nor when
probing for syntactic, monotonic, and logically robust reasoning. We propose a
novel framework to measure the extent of semantic sensitivity. To this end, we
evaluate NLI models on adversarially generated examples containing minor
semantics-preserving surface-form input noise. This is achieved using
conditional text generation, with the explicit condition that the NLI model
predicts the relationship between the original and adversarial inputs as a
symmetric equivalence entailment. We systematically study the effects of the
phenomenon across NLI models for \emph{in-} and \emph{out-of} domain settings.
Our experiments show that semantic sensitivity causes performance degradations
of $12.92\%$ and $23.71\%$ average over \emph{in-} and \emph{out-of-} domain
settings, respectively. We further perform ablation studies, analysing this
phenomenon across models, datasets, and variations in inference and show that
semantic sensitivity can lead to major inconsistency within model predictions.
\\ ( https://arxiv.org/abs/2401.14440 ,  7082kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14490
Date: Thu, 25 Jan 2024 19:57:00 GMT   (1476kb,D)

Title: LongHealth: A Question Answering Benchmark with Long Clinical Documents
Authors: Lisa Adams, Felix Busch, Tianyu Han, Jean-Baptiste Excoffier, Matthieu
  Ortala, Alexander L\"oser, Hugo JWL. Aerts, Jakob Nikolas Kather, Daniel
  Truhn, Keno Bressem
Categories: cs.CL
Comments: 11 pages, 3 figures, 5 tables
\\
  Background: Recent advancements in large language models (LLMs) offer
potential benefits in healthcare, particularly in processing extensive patient
records. However, existing benchmarks do not fully assess LLMs' capability in
handling real-world, lengthy clinical data.
  Methods: We present the LongHealth benchmark, comprising 20 detailed
fictional patient cases across various diseases, with each case containing
5,090 to 6,754 words. The benchmark challenges LLMs with 400 multiple-choice
questions in three categories: information extraction, negation, and sorting,
challenging LLMs to extract and interpret information from large clinical
documents.
  Results: We evaluated nine open-source LLMs with a minimum of 16,000 tokens
and also included OpenAI's proprietary and cost-efficient GPT-3.5 Turbo for
comparison. The highest accuracy was observed for Mixtral-8x7B-Instruct-v0.1,
particularly in tasks focused on information retrieval from single and multiple
patient documents. However, all models struggled significantly in tasks
requiring the identification of missing information, highlighting a critical
area for improvement in clinical data interpretation.
  Conclusion: While LLMs show considerable potential for processing long
clinical documents, their current accuracy levels are insufficient for reliable
clinical use, especially in scenarios requiring the identification of missing
information. The LongHealth benchmark provides a more realistic assessment of
LLMs in a healthcare setting and highlights the need for further model
refinement for safe and effective clinical application.
  We make the benchmark and evaluation code publicly available.
\\ ( https://arxiv.org/abs/2401.14490 ,  1476kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14493
Date: Thu, 25 Jan 2024 20:11:04 GMT   (9345kb,D)

Title: K-QA: A Real-World Medical Q&A Benchmark
Authors: Itay Manes, Naama Ronn, David Cohen, Ran Ilan Ber, Zehavi
  Horowitz-Kugler, Gabriel Stanovsky
Categories: cs.CL cs.HC cs.LG
Comments: The data and the evaluation script are available at
  https://github.com/Itaymanes/K-QA. Results and model comparisons can be
  viewed at https://huggingface.co/spaces/Itaykhealth/K-QA
\\
  Ensuring the accuracy of responses provided by large language models (LLMs)
is crucial, particularly in clinical settings where incorrect information may
directly impact patient health. To address this challenge, we construct K-QA, a
dataset containing 1,212 patient questions originating from real-world
conversations held on K Health (an AI-driven clinical platform). We employ a
panel of in-house physicians to answer and manually decompose a subset of K-QA
into self-contained statements. Additionally, we formulate two NLI-based
evaluation metrics approximating recall and precision: (1) comprehensiveness,
measuring the percentage of essential clinical information in the generated
answer and (2) hallucination rate, measuring the number of statements from the
physician-curated response contradicted by the LLM answer. Finally, we use K-QA
along with these metrics to evaluate several state-of-the-art models, as well
as the effect of in-context learning and medically-oriented augmented retrieval
schemes developed by the authors. Our findings indicate that in-context
learning improves the comprehensiveness of the models, and augmented retrieval
is effective in reducing hallucinations. We make K-QA available to to the
community to spur research into medically accurate NLP applications.
\\ ( https://arxiv.org/abs/2401.14493 ,  9345kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14524
Date: Thu, 25 Jan 2024 21:34:53 GMT   (392kb)

Title: Evaluating GPT-3.5's Awareness and Summarization Abilities for European
  Constitutional Texts with Shared Topics
Authors: Candida M. Greco, A. Tagarelli
Categories: cs.CL cs.AI cs.CY cs.DL physics.soc-ph
\\
  Constitutions are foundational legal documents that underpin the governmental
and societal structures. As such, they are a reflection of a nation's cultural
and social uniqueness, but also contribute to establish topics of universal
importance, like citizens' rights and duties (RD). In this work, using the
renowned GPT-3.5, we leverage generative large language models to understand
constitutional passages that transcend national boundaries. A key contribution
of our study is the introduction of a novel application of abstractive
summarization on a multi-source collection of constitutional texts, with a
focus on European countries' constitution passages related to RD topics. Our
results show the meaningfulness of GPT-3.5 to produce informative, coherent and
faithful summaries capturing RD topics across European countries.
\\ ( https://arxiv.org/abs/2401.14524 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14526
Date: Thu, 25 Jan 2024 21:38:30 GMT   (6631kb,D)

Title: MEDs for PETs: Multilingual Euphemism Disambiguation for Potentially
  Euphemistic Terms
Authors: Patrick Lee, Alain Chirino Trujillo, Diana Cuevas Plancarte, Olumide
  Ebenezer Ojo, Xinyi Liu, Iyanuoluwa Shode, Yuan Zhao, Jing Peng, Anna Feldman
Categories: cs.CL
\\
  This study investigates the computational processing of euphemisms, a
universal linguistic phenomenon, across multiple languages. We train a
multilingual transformer model (XLM-RoBERTa) to disambiguate potentially
euphemistic terms (PETs) in multilingual and cross-lingual settings. In line
with current trends, we demonstrate that zero-shot learning across languages
takes place. We also show cases where multilingual models perform better on the
task compared to monolingual models by a statistically significant margin,
indicating that multilingual data presents additional opportunities for models
to learn about cross-lingual, computational properties of euphemisms. In a
follow-up analysis, we focus on universal euphemistic "categories" such as
death and bodily functions among others. We test to see whether cross-lingual
data of the same domain is more important than within-language data of other
domains to further understand the nature of the cross-lingual transfer.
\\ ( https://arxiv.org/abs/2401.14526 ,  6631kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14530
Date: Thu, 25 Jan 2024 21:49:32 GMT   (2607kb)

Title: Relative Value Biases in Large Language Models
Authors: William M. Hayes, Nicolas Yax, Stefano Palminteri
Categories: cs.CL cs.AI cs.LG
\\
  Studies of reinforcement learning in humans and animals have demonstrated a
preference for options that yielded relatively better outcomes in the past,
even when those options are associated with lower absolute reward. The present
study tested whether large language models would exhibit a similar bias. We had
gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between
pairs of options with the goal of maximizing payoffs. A complete record of
previous outcomes was included in each prompt. Both models exhibited relative
value decision biases similar to those observed in humans and animals. Making
relative comparisons among outcomes more explicit magnified the bias, whereas
prompting the models to estimate expected outcomes caused the bias to
disappear. These results have implications for the potential mechanisms that
contribute to context-dependent choice in human agents.
\\ ( https://arxiv.org/abs/2401.14530 ,  2607kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14556
Date: Thu, 25 Jan 2024 22:50:48 GMT   (9694kb,D)

Title: Do Not (Always) Look Right: Investigating the Capabilities of
  Decoder-Based Large Language Models for Sequence Labeling
Authors: David Duki\'c, Jan \v{S}najder
Categories: cs.CL
\\
  Pre-trained language models based on masked language modeling (MLM) objective
excel in natural language understanding (NLU) tasks. While fine-tuned MLM-based
encoders consistently outperform causal language modeling decoders of
comparable size, a recent trend of scaling decoder models to multiple billion
parameters resulted in large language models (LLMs), making them competitive
with MLM-based encoders. Although scale amplifies their prowess in NLU tasks,
LLMs fall short of SOTA results in information extraction (IE) tasks, many
framed as sequence labeling (SL). However, whether this is an intrinsic
limitation of LLMs or whether their SL performance can be improved remains
unclear. To address this, we explore strategies to enhance the SL performance
of "open" LLMs (Llama2 and Mistral) on IE tasks. We investigate bidirectional
information flow within groups of decoder blocks, applying layer-wise removal
or enforcement of the causal mask (CM) during LLM fine-tuning. This approach
yields performance gains competitive with SOTA SL models, matching or
outperforming the results of CM removal from all blocks. Our findings hold for
diverse SL tasks, proving that "open" LLMs with layer-dependent CM removal
outperform strong MLM-based encoders and instruction-tuned LLMs. However, we
observe no effect from CM removal on a small scale when maintaining an
equivalent model size, pre-training steps, and pre-training and fine-tuning
data.
\\ ( https://arxiv.org/abs/2401.14556 ,  9694kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14559
Date: Thu, 25 Jan 2024 23:02:54 GMT   (1650kb,D)

Title: Language Modelling Approaches to Adaptive Machine Translation
Authors: Yasmin Moslem
Categories: cs.CL cs.AI cs.HC cs.IR
Comments: PhD thesis
\\
  Consistency is a key requirement of high-quality translation. It is
especially important to adhere to pre-approved terminology and adapt to
corrected translations in domain-specific projects. Machine translation (MT)
has achieved significant progress in the area of domain adaptation. However,
in-domain data scarcity is common in translation settings, due to the lack of
specialised datasets and terminology, or inconsistency and inaccuracy of
available in-domain translations. In such scenarios where there is insufficient
in-domain data to fine-tune MT models, producing translations that are
consistent with the relevant context is challenging. While real-time adaptation
can make use of smaller amounts of in-domain data to improve the translation on
the fly, it remains challenging due to supported context limitations and
efficiency constraints. Large language models (LLMs) have recently shown
interesting capabilities of in-context learning, where they learn to replicate
certain input-output text generation patterns, without further fine-tuning.
Such capabilities have opened new horizons for domain-specific data
augmentation and real-time adaptive MT. This work attempts to address two main
relevant questions: 1) in scenarios involving human interaction and continuous
feedback, can we employ language models to improve the quality of adaptive MT
at inference time? and 2) in the absence of sufficient in-domain data, can we
use pre-trained large-scale language models to improve the process of MT domain
adaptation?
\\ ( https://arxiv.org/abs/2401.14559 ,  1650kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14569
Date: Thu, 25 Jan 2024 23:54:34 GMT   (1140kb,D)

Title: Detecting Structured Language Alternations in Historical Documents by
  Combining Language Identification with Fourier Analysis
Authors: Hale Sirin, Sabrina Li, Tom Lippincott
Categories: cs.CL
Comments: Accepted to LaTeCH@EACL2024
\\
  In this study, we present a generalizable workflow to identify documents in a
historic language with a nonstandard language and script combination,
Armeno-Turkish. We introduce the task of detecting distinct patterns of
multilinguality based on the frequency of structured language alternations
within a document.
\\ ( https://arxiv.org/abs/2401.14569 ,  1140kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14589
Date: Fri, 26 Jan 2024 01:35:50 GMT   (2937kb)

Title: Enhancing Diagnostic Accuracy through Multi-Agent Conversations: Using
  Large Language Models to Mitigate Cognitive Bias
Authors: Yu He Ke, Rui Yang, Sui An Lie, Taylor Xin Yi Lim, Hairil Rizal
  Abdullah, Daniel Shu Wei Ting, Nan Liu
Categories: cs.CL cs.AI
Comments: 22 pages, 3 figures
\\
  Background: Cognitive biases in clinical decision-making significantly
contribute to errors in diagnosis and suboptimal patient outcomes. Addressing
these biases presents a formidable challenge in the medical field. This study
explores the role of large language models (LLMs) in mitigating these biases
through the utilization of a multi-agent framework. We simulate the clinical
decision-making processes through multi-agent conversation and evaluate its
efficacy in improving diagnostic accuracy. Methods: A total of 16 published and
unpublished case reports where cognitive biases have resulted in misdiagnoses
were identified from the literature. In the multi-agent system, we leveraged
GPT-4 Turbo to facilitate interactions among four simulated agents to replicate
clinical team dynamics. Each agent has a distinct role: 1) To make the initial
and final diagnosis after considering the discussions, 2) The devil's advocate
and correct confirmation and anchoring bias, 3) The tutor and facilitator of
the discussion to reduce premature closure bias, and 4) To record and summarize
the findings. A total of 80 simulations were evaluated for the accuracy of
initial diagnosis, top differential diagnosis and final two differential
diagnoses. Findings: In a total of 80 responses evaluating both initial and
final diagnoses, the initial diagnosis had an accuracy of 0% (0/80), but
following multi-agent discussions, the accuracy for the top differential
diagnosis increased to 71.3% (57/80), and for the final two differential
diagnoses, to 80.0% (64/80). The system demonstrated an ability to reevaluate
and correct misconceptions, even in scenarios with misleading initial
investigations. Interpretation: The LLM-driven multi-agent conversation system
shows promise in enhancing diagnostic accuracy in diagnostically challenging
medical scenarios.
\\ ( https://arxiv.org/abs/2401.14589 ,  2937kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14616
Date: Fri, 26 Jan 2024 03:16:54 GMT   (590kb,D)

Title: Alternative Speech: Complementary Method to Counter-Narrative for Better
  Discourse
Authors: Seungyoon Lee, Dahyun Jung, Chanjun Park, Seolhwa Lee, Heuiseok Lim
Categories: cs.CL cs.AI
Comments: Accepted for The First Workshop on Data-Centric AI (DCAI) at ICDM
  2023
\\
  We introduce the concept of "Alternative Speech" as a new way to directly
combat hate speech and complement the limitations of counter-narrative. An
alternative speech provides practical alternatives to hate speech in real-world
scenarios by offering speech-level corrections to speakers while considering
the surrounding context and promoting speakers to reform. Further, an
alternative speech can combat hate speech alongside counter-narratives,
offering a useful tool to address social issues such as racial discrimination
and gender inequality. We propose the new concept and provide detailed
guidelines for constructing the necessary dataset. Through discussion, we
demonstrate that combining alternative speech and counter-narrative can be a
more effective strategy for combating hate speech by complementing specificity
and guiding capacity of counter-narrative. This paper presents another
perspective for dealing with hate speech, offering viable remedies to
complement the constraints of current approaches to mitigating harmful bias.
\\ ( https://arxiv.org/abs/2401.14616 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14624
Date: Fri, 26 Jan 2024 03:38:23 GMT   (8064kb,D)

Title: Query of CC: Unearthing Large Scale Domain-Specific Knowledge from
  Public Corpora
Authors: Zhaoye Fei, Yunfan Shao, Linyang Li, Zhiyuan Zeng, Hang Yan, Xipeng
  Qiu and Dahua Lin
Categories: cs.CL
\\
  Large language models have demonstrated remarkable potential in various
tasks, however, there remains a significant scarcity of open-source models and
data for specific domains. Previous works have primarily focused on manually
specifying resources and collecting high-quality data on specific domains,
which significantly consume time and effort. To address this limitation, we
propose an efficient data collection method~\textit{Query of CC} based on large
language models. This method bootstraps seed information through a large
language model and retrieves related data from public corpora. It not only
collects knowledge-related data for specific domains but unearths the data with
potential reasoning procedures. Through the application of this method, we have
curated a high-quality dataset called~\textsc{Knowledge Pile}, encompassing
four major domains, including stem and humanities sciences, among others.
Experimental results demonstrate that~\textsc{Knowledge Pile} significantly
improves the performance of large language models in mathematical and
knowledge-related reasoning ability tests. To facilitate academic sharing, we
open-source our dataset and code, providing valuable support to the academic
community.
\\ ( https://arxiv.org/abs/2401.14624 ,  8064kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14625
Date: Fri, 26 Jan 2024 03:42:45 GMT   (187kb)

Title: Toward Practical Automatic Speech Recognition and Post-Processing: a
  Call for Explainable Error Benchmark Guideline
Authors: Seonmin Koo, Chanjun Park, Jinsung Kim, Jaehyung Seo, Sugyeong Eo,
  Hyeonseok Moon, Heuiseok Lim
Categories: cs.CL
Comments: Accepted for Data-centric Machine Learning Research (DMLR) Workshop
  at ICML 2023
\\
  Automatic speech recognition (ASR) outcomes serve as input for downstream
tasks, substantially impacting the satisfaction level of end-users. Hence, the
diagnosis and enhancement of the vulnerabilities present in the ASR model bear
significant importance. However, traditional evaluation methodologies of ASR
systems generate a singular, composite quantitative metric, which fails to
provide comprehensive insight into specific vulnerabilities. This lack of
detail extends to the post-processing stage, resulting in further obfuscation
of potential weaknesses. Despite an ASR model's ability to recognize utterances
accurately, subpar readability can negatively affect user satisfaction, giving
rise to a trade-off between recognition accuracy and user-friendliness. To
effectively address this, it is imperative to consider both the speech-level,
crucial for recognition accuracy, and the text-level, critical for
user-friendliness. Consequently, we propose the development of an Error
Explainable Benchmark (EEB) dataset. This dataset, while considering both
speech- and text-level, enables a granular understanding of the model's
shortcomings. Our proposition provides a structured pathway for a more
`real-world-centric' evaluation, a marked shift away from abstracted,
traditional methods, allowing for the detection and rectification of nuanced
system weaknesses, ultimately aiming for an improved user experience.
\\ ( https://arxiv.org/abs/2401.14625 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14630
Date: Fri, 26 Jan 2024 03:49:55 GMT   (250kb,D)

Title: An Empirical Investigation of Domain Adaptation Ability for Chinese
  Spelling Check Models
Authors: Xi Wang, Ruoqing Zhao, Hongliang Dai, Piji Li
Categories: cs.CL cs.AI
Comments: ICASSP2024
\\
  Chinese Spelling Check (CSC) is a meaningful task in the area of Natural
Language Processing (NLP) which aims at detecting spelling errors in Chinese
texts and then correcting these errors. However, CSC models are based on
pretrained language models, which are trained on a general corpus.
Consequently, their performance may drop when confronted with downstream tasks
involving domain-specific terms. In this paper, we conduct a thorough
evaluation about the domain adaption ability of various typical CSC models by
building three new datasets encompassing rich domain-specific terms from the
financial, medical, and legal domains. Then we conduct empirical investigations
in the corresponding domain-specific test datasets to ascertain the
cross-domain adaptation ability of several typical CSC models. We also test the
performance of the popular large language model ChatGPT. As shown in our
experiments, the performances of the CSC models drop significantly in the new
domains.
\\ ( https://arxiv.org/abs/2401.14630 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14637
Date: Fri, 26 Jan 2024 04:08:50 GMT   (7316kb,D)

Title: T-Rex: Text-assisted Retrosynthesis Prediction
Authors: Yifeng Liu, Hanwen Xu, Tangqi Fang, Haocheng Xi, Zixuan Liu, Sheng
  Zhang, Hoifung Poon, Sheng Wang
Categories: cs.CL
\\
  As a fundamental task in computational chemistry, retrosynthesis prediction
aims to identify a set of reactants to synthesize a target molecule. Existing
template-free approaches only consider the graph structures of the target
molecule, which often cannot generalize well to rare reaction types and large
molecules. Here, we propose T-Rex, a text-assisted retrosynthesis prediction
approach that exploits pre-trained text language models, such as ChatGPT, to
assist the generation of reactants. T-Rex first exploits ChatGPT to generate a
description for the target molecule and rank candidate reaction centers based
both the description and the molecular graph. It then re-ranks these candidates
by querying the descriptions for each reactants and examines which group of
reactants can best synthesize the target molecule. We observed that T-Rex
substantially outperformed graph-based state-of-the-art approaches on two
datasets, indicating the effectiveness of considering text information. We
further found that T-Rex outperformed the variant that only use ChatGPT-based
description without the re-ranking step, demonstrate how our framework
outperformed a straightforward integration of ChatGPT and graph information.
Collectively, we show that text generated by pre-trained language models can
substantially improve retrosynthesis prediction, opening up new avenues for
exploiting ChatGPT to advance computational chemistry. And the codes can be
found at https://github.com/lauyikfung/T-Rex.
\\ ( https://arxiv.org/abs/2401.14637 ,  7316kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14640
Date: Fri, 26 Jan 2024 04:11:07 GMT   (944kb,D)

Title: Benchmarking Large Language Models in Complex Question Answering
  Attribution using Knowledge Graphs
Authors: Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Sheng Bi, Tongtong Wu and
  Jeff Z. Pan
Categories: cs.CL
Comments: 13 pages, 5 figures
\\
  The attribution of question answering is to provide citations for supporting
generated statements, and has attracted wide research attention. The current
methods for automatically evaluating the attribution, which are often based on
Large Language Models (LLMs), are still inadequate, particularly in recognizing
subtle differences between attributions, and complex relationships between
citations and statements. To compare these attribution evaluation methods and
develop new ones, we introduce a set of fine-grained categories (i.e.,
supportive, insufficient, contradictory and irrelevant) for measuring the
attribution, and develop a Complex Attributed Question Answering (CAQA)
benchmark by leveraging knowledge graphs (KGs) for automatically generating
attributions of different categories to question-answer pairs. Our analysis
reveals that existing evaluators perform poorly under fine-grained attribution
settings and exhibit weaknesses in complex citation-statement reasoning. Our
CAQA benchmark, validated with human annotations, emerges as a promising tool
for selecting and developing LLM attribution evaluators.
\\ ( https://arxiv.org/abs/2401.14640 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14654
Date: Fri, 26 Jan 2024 05:26:27 GMT   (128kb,D)

Title: A Korean Legal Judgment Prediction Dataset for Insurance Disputes
Authors: Alice Saebom Kwak, Cheonkam Jeong, Ji Weon Lim, and Byeongcheol Min
Categories: cs.CL cs.LG
Comments: 5 pages, 1 figure
\\
  This paper introduces a Korean legal judgment prediction (LJP) dataset for
insurance disputes. Successful LJP models on insurance disputes can benefit
insurance companies and their customers. It can save both sides' time and money
by allowing them to predict how the result would come out if they proceed to
the dispute mediation process. As is often the case with low-resource
languages, there is a limitation on the amount of data available for this
specific task. To mitigate this issue, we investigate how one can achieve a
good performance despite the limitation in data. In our experiment, we
demonstrate that Sentence Transformer Fine-tuning (SetFit, Tunstall et al.,
2022) is a good alternative to standard fine-tuning when training data are
limited. The models fine-tuned with the SetFit approach on our data show
similar performance to the Korean LJP benchmark models (Hwang et al., 2022)
despite the much smaller data size.
\\ ( https://arxiv.org/abs/2401.14654 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14656
Date: Fri, 26 Jan 2024 05:33:34 GMT   (2156kb,D)

Title: Scientific Large Language Models: A Survey on Biological & Chemical
  Domains
Authors: Qiang Zhang, Keyang Ding, Tianwen Lyv, Xinda Wang, Qingyu Yin, Yiwen
  Zhang, Jing Yu, Yuhao Wang, Xiaotong Li, Zhuoyi Xiang, Xiang Zhuang, Zeyuan
  Wang, Ming Qin, Mengyao Zhang, Jinlu Zhang, Jiyu Cui, Renjun Xu, Hongyang
  Chen, Xiaohui Fan, Huabin Xing, Huajun Chen
Categories: cs.CL
\\
  Large Language Models (LLMs) have emerged as a transformative power in
enhancing natural language comprehension, representing a significant stride
toward artificial general intelligence. The application of LLMs extends beyond
conventional linguistic boundaries, encompassing specialized linguistic systems
developed within various scientific disciplines. This growing interest has led
to the advent of scientific LLMs, a novel subclass specifically engineered for
facilitating scientific discovery. As a burgeoning area in the community of AI
for Science, scientific LLMs warrant comprehensive exploration. However, a
systematic and up-to-date survey introducing them is currently lacking. In this
paper, we endeavor to methodically delineate the concept of "scientific
language", whilst providing a thorough review of the latest advancements in
scientific LLMs. Given the expansive realm of scientific disciplines, our
analysis adopts a focused lens, concentrating on the biological and chemical
domains. This includes an in-depth examination of LLMs for textual knowledge,
small molecules, macromolecular proteins, genomic sequences, and their
combinations, analyzing them in terms of model architectures, capabilities,
datasets, and evaluation. Finally, we critically examine the prevailing
challenges and point out promising research directions along with the advances
of LLMs. By offering a comprehensive overview of technical developments in this
field, this survey aspires to be an invaluable resource for researchers
navigating the intricate landscape of scientific LLMs.
\\ ( https://arxiv.org/abs/2401.14656 ,  2156kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14680
Date: Fri, 26 Jan 2024 06:56:05 GMT   (974kb,D)

Title: MaLLaM -- Malaysia Large Language Model
Authors: Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan
Categories: cs.CL
\\
  Addressing the gap in Large Language Model pretrained from scratch with
Malaysian context, We trained models with 1.1 billion, 3 billion, and 5 billion
parameters on a substantial 349GB dataset, equivalent to 90 billion tokens
based on our pretrained Byte Pair Encoding (BPE) tokenizer for a single epoch.
MaLLaM contributes to enhanced natural language understanding and generation
tasks in the Malay language. Although trained on a smaller dataset of 90
billion tokens, our instruction-tuned MaLLaM models perform competitively. When
compared to ChatGPT3.5 and Malaysian Mistral, MaLLaM's instruction-tuned models
demonstrate notable proficiency, underscoring the effectiveness of our approach
in capturing and understanding the nuances of the Malaysian language. MaLLaM
models mark a significant contribution to the field, providing comprehensive
language representations grounded in Malaysian context. This endeavor aims to
pave the way for enhanced natural language understanding and generation tasks
specific to the linguistic nuances present in Malaysia. We discuss the training
methodology, dataset composition, and the potential impact of MaLLaM in
advancing the capabilities of large language models within the context of the
Malay language.
  All models released at
https://huggingface.co/collections/mesolitica/mallam-6577b59d1e0b436ae75f930f
\\ ( https://arxiv.org/abs/2401.14680 ,  974kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14681
Date: Fri, 26 Jan 2024 06:56:17 GMT   (7592kb,D)

Title: MasonTigers@LT-EDI-2024: An Ensemble Approach towards Detecting
  Homophobia and Transphobia in Social Media Comments
Authors: Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Al
  Nahian Bin Emran
Categories: cs.CL
\\
  In this paper, we describe our approaches and results for Task 2 of the
LT-EDI 2024 Workshop, aimed at detecting homophobia and/or transphobia across
ten languages. Our methodologies include monolingual transformers and ensemble
methods, capitalizing on the strengths of each to enhance the performance of
the models. The ensemble models worked well, placing our team, MasonTigers, in
the top five for eight of the ten languages, as measured by the macro F1 score.
Our work emphasizes the efficacy of ensemble methods in multilingual scenarios,
addressing the complexities of language-specific tasks.
\\ ( https://arxiv.org/abs/2401.14681 ,  7592kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14688
Date: Fri, 26 Jan 2024 07:17:50 GMT   (46890kb,D)

Title: Taiyi-Diffusion-XL: Advancing Bilingual Text-to-Image Generation with
  Large Vision-Language Model Support
Authors: Xiaojun Wu, Dixiang Zhang, Ruyi Gan, Junyu Lu, Ziwei Wu, Renliang Sun,
  Jiaxing Zhang, Pingjian Zhang, Yan Song
Categories: cs.CL
Comments: Taiyi-Diffusion-XL Tech Report
\\
  Recent advancements in text-to-image models have significantly enhanced image
generation capabilities, yet a notable gap of open-source models persists in
bilingual or Chinese language support. To address this need, we present
Taiyi-Diffusion-XL, a new Chinese and English bilingual text-to-image model
which is developed by extending the capabilities of CLIP and
Stable-Diffusion-XL through a process of bilingual continuous pre-training.
This approach includes the efficient expansion of vocabulary by integrating the
most frequently used Chinese characters into CLIP's tokenizer and embedding
layers, coupled with an absolute position encoding expansion. Additionally, we
enrich text prompts by large vision-language model, leading to better images
captions and possess higher visual quality. These enhancements are subsequently
applied to downstream text-to-image models. Our empirical results indicate that
the developed CLIP model excels in bilingual image-text retrieval.Furthermore,
the bilingual image generation capabilities of Taiyi-Diffusion-XL surpass
previous models. This research leads to the development and open-sourcing of
the Taiyi-Diffusion-XL model, representing a notable advancement in the field
of image generation, particularly for Chinese language applications. This
contribution is a step forward in addressing the need for more diverse language
support in multimodal research. The model and demonstration are made publicly
available at
\href{https://huggingface.co/IDEA-CCNL/Taiyi-Stable-Diffusion-XL-3.5B/}{this
https URL}, fostering further research and collaboration in this domain.
\\ ( https://arxiv.org/abs/2401.14688 ,  46890kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14698
Date: Fri, 26 Jan 2024 07:53:27 GMT   (5757kb,D)

Title: Under the Surface: Tracking the Artifactuality of LLM-Generated Data
Authors: Debarati Das, Karin De Langis, Anna Martin, Jaehyung Kim, Minhwa Lee,
  Zae Myung Kim, Shirley Hayati, Risako Owan, Bin Hu, Ritik Parkar, Ryan Koo,
  Jonginn Park, Aahan Tyagi, Libby Ferland, Sanjali Roy, Vincent Liu, and
  Dongyeop Kang
Categories: cs.CL cs.AI
Comments: Core Authors: Debarati Das, Karin De Langis, Anna Martin, Jaehyung
  Kim, Minhwa Lee and Zae Myung Kim | Project lead : Debarati Das | PI :
  Dongyeop Kang
\\
  This work delves into the expanding role of large language models (LLMs) in
generating artificial data. LLMs are increasingly employed to create a variety
of outputs, including annotations, preferences, instruction prompts, simulated
dialogues, and free text. As these forms of LLM-generated data often intersect
in their application, they exert mutual influence on each other and raise
significant concerns about the quality and diversity of the artificial data
incorporated into training cycles, leading to an artificial data ecosystem. To
the best of our knowledge, this is the first study to aggregate various types
of LLM-generated text data, from more tightly constrained data like "task
labels" to more lightly constrained "free-form text". We then stress test the
quality and implications of LLM-generated artificial data, comparing it with
human data across various existing benchmarks. Despite artificial data's
capability to match human performance, this paper reveals significant hidden
disparities, especially in complex tasks where LLMs often miss the nuanced
understanding of intrinsic human-generated content. This study critically
examines diverse LLM-generated data and emphasizes the need for ethical
practices in data creation and when using LLMs. It highlights the LLMs'
shortcomings in replicating human traits and behaviors, underscoring the
importance of addressing biases and artifacts produced in LLM-generated content
for future research and development. All data and code are available on our
project page.
\\ ( https://arxiv.org/abs/2401.14698 ,  5757kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14717
Date: Fri, 26 Jan 2024 08:59:07 GMT   (3233kb,D)

Title: Turn-taking and Backchannel Prediction with Acoustic and Large Language
  Model Fusion
Authors: Jinhan Wang, Long Chen, Aparna Khare, Anirudh Raju, Pranav Dheram, Di
  He, Minhua Wu, Andreas Stolcke, Venkatesh Ravichandran
Categories: cs.CL cs.AI cs.LG cs.SD eess.AS
Comments: To appear in IEEE ICASSP 2024
\\
  We propose an approach for continuous prediction of turn-taking and
backchanneling locations in spoken dialogue by fusing a neural acoustic model
with a large language model (LLM). Experiments on the Switchboard human-human
conversation dataset demonstrate that our approach consistently outperforms the
baseline models with single modality. We also develop a novel multi-task
instruction fine-tuning strategy to further benefit from LLM-encoded knowledge
for understanding the tasks and conversational contexts, leading to additional
improvements. Our approach demonstrates the potential of combined LLMs and
acoustic models for a more natural and conversational interaction between
humans and speech-enabled AI agents.
\\ ( https://arxiv.org/abs/2401.14717 ,  3233kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14777
Date: Fri, 26 Jan 2024 11:04:01 GMT   (170kb,D)

Title: Large Language Model Adaptation for Financial Sentiment Analysis
Authors: Pau Rodriguez Inserte, Mariam Nakhl\'e, Raheel Qader, Gaetan Caillaut
  and Jingshu Liu
Categories: cs.CL cs.AI
\\
  Natural language processing (NLP) has recently gained relevance within
financial institutions by providing highly valuable insights into companies and
markets' financial documents. However, the landscape of the financial domain
presents extra challenges for NLP, due to the complexity of the texts and the
use of specific terminology. Generalist language models tend to fall short in
tasks specifically tailored for finance, even when using large language models
(LLMs) with great natural language understanding and generative capabilities.
This paper presents a study on LLM adaptation methods targeted at the financial
domain and with high emphasis on financial sentiment analysis. To this purpose,
two foundation models with less than 1.5B parameters have been adapted using a
wide range of strategies. We show that through careful fine-tuning on both
financial documents and instructions, these foundation models can be adapted to
the target domain. Moreover, we observe that small LLMs have comparable
performance to larger scale models, while being more efficient in terms of
parameters and data. In addition to the models, we show how to generate
artificial instructions through LLMs to augment the number of samples of the
instruction dataset.
\\ ( https://arxiv.org/abs/2401.14777 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14818
Date: Fri, 26 Jan 2024 12:45:55 GMT   (11477kb,D)

Title: ChemDFM: Dialogue Foundation Model for Chemistry
Authors: Zihan Zhao, Da Ma, Lu Chen, Liangtai Sun, Zihao Li, Hongshen Xu,
  Zichen Zhu, Su Zhu, Shuai Fan, Guodong Shen, Xin Chen and Kai Yu
Categories: cs.CL cs.DL
Comments: 10 pages, 12 figures, 13 tables. Under Review
\\
  Large language models (LLMs) have established great success in the general
domain of natural language processing. Their emerging task generalization and
free-form dialogue capabilities can greatly help to design Chemical General
Intelligence (CGI) to assist real-world research in chemistry. However, the
existence of specialized language and knowledge in the field of chemistry, such
as the highly informative SMILES notation, hinders the performance of
general-domain LLMs in chemistry. To this end, we develop ChemDFM, the first
LLM towards CGI. ChemDFM-13B is trained on 34B tokens from chemical literature,
textbooks, and instructions as well as various data from the general domain.
Therefore, it can store, understand, and reason over chemical knowledge and
languages while still possessing advanced free-form language comprehension
capabilities. Extensive quantitative evaluation shows that ChemDFM can
significantly outperform the representative open-sourced LLMs. Moreover,
ChemDFM can also surpass GPT-4 on a great portion of chemical tasks, despite
the significant size difference. Further qualitative evaluations demonstrate
the efficiency and effectiveness of ChemDFM in real-world research scenarios.
We will open-source the ChemDFM model soon.
\\ ( https://arxiv.org/abs/2401.14818 ,  11477kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14869
Date: Fri, 26 Jan 2024 13:55:32 GMT   (8947kb,D)

Title: F-Eval: Asssessing Fundamental Abilities with Refined Evaluation Methods
Authors: Yu Sun, Keyu Chen, Shujie Wang, Qipeng Guo, Hang Yan, Xipeng Qiu,
  Xuanjing Huang, Dahua Lin
Categories: cs.CL
Comments: 21 pages, 7 figures
\\
  Large language models (LLMs) garner significant attention for their
unprecedented performance, leading to an increasing number of researches
evaluating LLMs. However, these evaluation benchmarks are limited to assessing
the instruction-following capabilities, overlooking the fundamental abilities
that emerge during the pre-training stage. Previous subjective evaluation
methods mainly reply on scoring by API models. However, in the absence of
references, large models have shown limited ability to discern subtle
differences. To bridge the gap, we propose F-Eval, a bilingual evaluation
benchmark to evaluate the fundamental abilities, including expression,
commonsense and logic. The tasks in F-Eval include multi-choice objective
tasks, open-ended objective tasks, reference-based subjective tasks and
reference-free subjective tasks. For reference-free subjective tasks, we devise
new evaluation methods, serving as alternatives to scoring by API models. We
conduct evaluations on 13 advanced LLMs. Results show that our evaluation
methods show higher correlation coefficients and larger distinction than other
evaluators. Additionally, we discuss the influence of different model sizes,
dimensions, and normalization methods. We anticipate that F-Eval will
facilitate the study of LLMs' fundamental abilities.
\\ ( https://arxiv.org/abs/2401.14869 ,  8947kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14931
Date: Fri, 26 Jan 2024 15:10:23 GMT   (637kb,D)

Title: Do LLMs Dream of Ontologies?
Authors: Marco Bombieri, Paolo Fiorini, Simone Paolo Ponzetto, Marco Rospocher
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have recently revolutionized automated text
understanding and generation. The performance of these models relies on the
high number of parameters of the underlying neural architectures, which allows
LLMs to memorize part of the vast quantity of data seen during the training.
This paper investigates whether and to what extent general-purpose pre-trained
LLMs have memorized information from known ontologies. Our results show that
LLMs partially know ontologies: they can, and do indeed, memorize concepts from
ontologies mentioned in the text, but the level of memorization of their
concepts seems to vary proportionally to their popularity on the Web, the
primary source of their training material. We additionally propose new metrics
to estimate the degree of memorization of ontological information in LLMs by
measuring the consistency of the output produced across different prompt
repetitions, query languages, and degrees of determinism.
\\ ( https://arxiv.org/abs/2401.14931 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15006
Date: Fri, 26 Jan 2024 17:07:08 GMT   (1668kb,D)

Title: Airavata: Introducing Hindi Instruction-tuned LLM
Authors: Jay Gala and Thanmay Jayakumar and Jaavid Aktar Husain and Aswanth
  Kumar M and Mohammed Safi Ur Rahman Khan and Diptesh Kanojia and Ratish
  Puduppully and Mitesh M. Khapra and Raj Dabre and Rudra Murthy and Anoop
  Kunchukuttan
Categories: cs.CL cs.AI
Comments: Work in progress
\\
  We announce the initial release of "Airavata," an instruction-tuned LLM for
Hindi. Airavata was created by fine-tuning OpenHathi with diverse,
instruction-tuning Hindi datasets to make it better suited for assistive tasks.
Along with the model, we also share the IndicInstruct dataset, which is a
collection of diverse instruction-tuning datasets to enable further research
for Indic LLMs. Additionally, we present evaluation benchmarks and a framework
for assessing LLM performance across tasks in Hindi. Currently, Airavata
supports Hindi, but we plan to expand this to all 22 scheduled Indic languages.
You can access all artifacts at https://ai4bharat.github.io/airavata.
\\ ( https://arxiv.org/abs/2401.15006 ,  1668kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15042
Date: Fri, 26 Jan 2024 18:12:25 GMT   (7869kb,D)

Title: PROXYQA: An Alternative Framework for Evaluating Long-Form Text
  Generation with Large Language Models
Authors: Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Xiaoguang Li,
  Yasheng Wang, Lifeng Shang, Qun Liu, Linqi Song
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have exhibited remarkable success in long-form
context comprehension tasks. However, their capacity to generate long contents,
such as reports and articles, remains insufficiently explored. Current
benchmarks do not adequately assess LLMs' ability to produce informative and
comprehensive content, necessitating a more rigorous evaluation approach. In
this study, we introduce \textsc{ProxyQA}, a framework for evaluating long-form
text generation, comprising in-depth human-curated \textit{meta-questions}
spanning various domains. Each meta-question contains corresponding
\textit{proxy-questions} with annotated answers. LLMs are prompted to generate
extensive content in response to these meta-questions. Utilizing an evaluator
and incorporating generated content as background context, \textsc{ProxyQA}
evaluates the quality of generated content based on the evaluator's performance
in answering the \textit{proxy-questions}. We examine multiple LLMs,
emphasizing \textsc{ProxyQA}'s demanding nature as a high-quality assessment
tool. Human evaluation demonstrates that evaluating through
\textit{proxy-questions} is a highly self-consistent and
human-criteria-correlated validation method. The dataset and leaderboard will
be available at \url{https://github.com/Namco0816/ProxyQA}.
\\ ( https://arxiv.org/abs/2401.15042 ,  7869kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15043
Date: Fri, 26 Jan 2024 18:13:57 GMT   (343kb,D)

Title: Health Text Simplification: An Annotated Corpus for Digestive Cancer
  Education and Novel Strategies for Reinforcement Learning
Authors: Md Mushfiqur Rahman, Mohammad Sabik Irbaz, Kai North, Michelle S.
  Williams, Marcos Zampieri, Kevin Lybarger
Categories: cs.CL cs.AI cs.LG
\\
  Objective: The reading level of health educational materials significantly
influences information understandability and accessibility, particularly for
minoritized populations. Many patient educational resources surpass the reading
level and complexity of widely accepted standards. There is a critical need for
high-performing text simplification models in health information to enhance
dissemination and literacy. This need is particularly acute in cancer
education, where effective prevention and screening education can substantially
reduce morbidity and mortality.
  Methods: We introduce Simplified Digestive Cancer (SimpleDC), a parallel
corpus of cancer education materials tailored for health text simplification
research. Utilizing SimpleDC alongside the existing Med-EASi corpus, we explore
Large Language Model (LLM)-based simplification methods, including fine-tuning,
reinforcement learning (RL), reinforcement learning with human feedback (RLHF),
domain adaptation, and prompt-based approaches. Our experimentation encompasses
Llama 2 and GPT-4. A novel RLHF reward function is introduced, featuring a
lightweight model adept at distinguishing between original and simplified
texts, thereby enhancing the model's effectiveness with unlabeled data.
  Results: Fine-tuned Llama 2 models demonstrated high performance across
various metrics. Our innovative RLHF reward function surpassed existing RL text
simplification reward functions in effectiveness. The results underscore that
RL/RLHF can augment fine-tuning, facilitating model training on unlabeled text
and improving performance. Additionally, these methods effectively adapt
out-of-domain text simplification models to targeted domains.
\\ ( https://arxiv.org/abs/2401.15043 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15050
Date: Fri, 26 Jan 2024 18:23:45 GMT   (4371kb,D)

Title: LongFin: A Multimodal Document Understanding Model for Long Financial
  Domain Documents
Authors: Ahmed Masry and Amir Hajian
Categories: cs.CL
Comments: Accepted at AAAI 2024 Workshop on AI in Finance for Social Impact
\\
  Document AI is a growing research field that focuses on the comprehension and
extraction of information from scanned and digital documents to make everyday
business operations more efficient. Numerous downstream tasks and datasets have
been introduced to facilitate the training of AI models capable of parsing and
extracting information from various document types such as receipts and scanned
forms. Despite these advancements, both existing datasets and models fail to
address critical challenges that arise in industrial contexts. Existing
datasets primarily comprise short documents consisting of a single page, while
existing models are constrained by a limited maximum length, often set at 512
tokens. Consequently, the practical application of these methods in financial
services, where documents can span multiple pages, is severely impeded. To
overcome these challenges, we introduce LongFin, a multimodal document AI model
capable of encoding up to 4K tokens. We also propose the LongForms dataset, a
comprehensive financial dataset that encapsulates several industrial challenges
in financial documents. Through an extensive evaluation, we demonstrate the
effectiveness of the LongFin model on the LongForms dataset, surpassing the
performance of existing public models while maintaining comparable results on
existing single-page benchmarks.
\\ ( https://arxiv.org/abs/2401.15050 ,  4371kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15068
Date: Fri, 26 Jan 2024 18:49:34 GMT   (77kb,D)

Title: Pairing Orthographically Variant Literary Words to Standard Equivalents
  Using Neural Edit Distance Models
Authors: Craig Messner and Tom Lippincott
Categories: cs.CL
Comments: Accepted to LaTeCH@EACL2024
\\
  We present a novel corpus consisting of orthographically variant words found
in works of 19th century U.S. literature annotated with their corresponding
"standard" word pair. We train a set of neural edit distance models to pair
these variants with their standard forms, and compare the performance of these
models to the performance of a set of neural edit distance models trained on a
corpus of orthographic errors made by L2 English learners. Finally, we analyze
the relative performance of these models in the light of different negative
training sample generation strategies, and offer concluding remarks on the
unique challenge literary orthographic variation poses to string pairing
methodologies.
\\ ( https://arxiv.org/abs/2401.15068 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14411
Date: Wed, 17 Jan 2024 19:10:09 GMT   (12278kb,D)

Title: Precision Mars Entry Navigation with Atmospheric Density Adaptation via
  Neural Networks
Authors: Felipe Giraldo-Grueso, Andrey A. Popov, Renato Zanetti
Categories: cs.LG cs.SY eess.SY stat.AP
\\
  Discrepancies between the true Martian atmospheric density and the onboard
density model can significantly impair the performance of spacecraft entry
navigation filters. This work introduces a new approach to online filtering for
Martian entry by using a neural network to estimate atmospheric density and
employing a consider analysis to account for the uncertainty in the estimate.
The network is trained on an exponential atmospheric density model, and its
parameters are dynamically adapted in real time to account for any mismatches
between the true and estimated densities. The adaptation of the network is
formulated as a maximum likelihood problem, leveraging the measurement
innovations of the filter to identify optimal network parameters. The
incorporation of a neural network enables the use of stochastic optimizers
known for their efficiency in the machine learning domain within the context of
the maximum likelihood approach. Performance comparisons against previous
approaches are conducted in various realistic Mars entry navigation scenarios,
resulting in superior estimation accuracy and precise alignment of the
estimated density with a broad selection of realistic Martian atmospheres
sampled from perturbed Mars-GRAM data.
\\ ( https://arxiv.org/abs/2401.14411 ,  12278kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14412
Date: Fri, 19 Jan 2024 23:48:04 GMT   (1971kb,D)

Title: Harnessing Neuron Stability to Improve DNN Verification
Authors: Hai Duong, Dong Xu, ThanhVu Nguyen, Matthew B. Dwyer
Categories: cs.LG cs.AI
Comments: VeriStable and experimental data are available at:
  https://github.com/veristable/veristable
\\
  Deep Neural Networks (DNN) have emerged as an effective approach to tackling
real-world problems. However, like human-written software, DNNs are susceptible
to bugs and attacks. This has generated significant interests in developing
effective and scalable DNN verification techniques and tools. In this paper, we
present VeriStable, a novel extension of recently proposed DPLL-based
constraint DNN verification approach. VeriStable leverages the insight that
while neuron behavior may be non-linear across the entire DNN input space, at
intermediate states computed during verification many neurons may be
constrained to have linear behavior - these neurons are stable. Efficiently
detecting stable neurons reduces combinatorial complexity without compromising
the precision of abstractions. Moreover, the structure of clauses arising in
DNN verification problems shares important characteristics with industrial SAT
benchmarks. We adapt and incorporate multi-threading and restart optimizations
targeting those characteristics to further optimize DPLL-based DNN
verification. We evaluate the effectiveness of VeriStable across a range of
challenging benchmarks including fully-connected feedforward networks (FNNs),
convolutional neural networks (CNNs) and residual networks (ResNets) applied to
the standard MNIST and CIFAR datasets. Preliminary results show that VeriStable
is competitive and outperforms state-of-the-art DNN verification tools,
including $\alpha$-$\beta$-CROWN and MN-BaB, the first and second performers of
the VNN-COMP, respectively.
\\ ( https://arxiv.org/abs/2401.14412 ,  1971kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14413
Date: Sat, 20 Jan 2024 16:41:25 GMT   (247kb,D)

Title: Aprendizado de m\'aquina aplicado na eletroqu\'imica
Authors: Carlos Eduardo do Egito Ara\'ujo and L\'ivia F. Sgobbi and Iwens
  Gervasio Sene Jr and Sergio Teixeira de Carvalho
Categories: cs.LG cs.AI
Comments: in Portuguese language
\\
  This systematic review focuses on analyzing the use of machine learning
techniques for identifying and quantifying analytes in various electrochemical
applications, presenting the available applications in the literature. Machine
learning is a tool that can facilitate the analysis and enhance the
understanding of processes involving various analytes. In electrochemical
biosensors, it increases the precision of medical diagnostics, improving the
identification of biomarkers and pathogens with high reliability. It can be
effectively used for the classification of complex chemical products; in
environmental monitoring, using low-cost sensors; in portable devices and
wearable systems; among others. Currently, the analysis of some analytes is
still performed manually, requiring the expertise of a specialist in the field
and thus hindering the generalization of results. In light of the advancements
in artificial intelligence today, this work proposes to carry out a systematic
review of the literature on the applications of artificial intelligence
techniques. A set of articles has been identified that address electrochemical
problems using machine learning techniques, more specifically, supervised
learning.
\\ ( https://arxiv.org/abs/2401.14413 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14417
Date: Mon, 22 Jan 2024 13:58:03 GMT   (417kb)

Title: Fuzzy Logic Function as a Post-hoc Explanator of the Nonlinear
  Classifier
Authors: Martin Klimo, Lubomir Kralik
Categories: cs.LG cs.AI
Journal-ref: Fuzzy Logic and Technology, and Aggregation Operators. EUSFLAT
  AGOP 2023 2023. LNCS, vol. 14069, pp. 431-442. Springer, Cham (2023)
DOI: 10.1007/978-3-031-39965-7_36
\\
  Pattern recognition systems implemented using deep neural networks achieve
better results than linear models. However, their drawback is the black box
property. This property means that one with no experience utilising nonlinear
systems may need help understanding the outcome of the decision. Such a
solution is unacceptable to the user responsible for the final decision. He
must not only believe in the decision but also understand it. Therefore,
recognisers must have an architecture that allows interpreters to interpret the
findings. The idea of post-hoc explainable classifiers is to design an
interpretable classifier parallel to the black box classifier, giving the same
decisions as the black box classifier. This paper shows that the explainable
classifier completes matching classification decisions with the black box
classifier on the MNIST and FashionMNIST databases if Zadeh`s fuzzy logic
function forms the classifier and DeconvNet importance gives the truth values.
Since the other tested significance measures achieved lower performance than
DeconvNet, it is the optimal transformation of the feature values to their
truth values as inputs to the fuzzy logic function for the databases and
recogniser architecture used.
\\ ( https://arxiv.org/abs/2401.14417 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14421
Date: Tue, 23 Jan 2024 22:21:07 GMT   (4875kb,D)

Title: Multi-Agent Based Transfer Learning for Data-Driven Air Traffic
  Applications
Authors: Chuhao Deng and Hong-Cheol Choi and Hyunsang Park and Inseok Hwang
Categories: cs.LG cs.MA cs.SY eess.SY stat.ML
Comments: 12 pages, 8 figures, submitted for IEEE Transactions on Intelligent
  Transportation System
\\
  Research in developing data-driven models for Air Traffic Management (ATM)
has gained a tremendous interest in recent years. However, data-driven models
are known to have long training time and require large datasets to achieve good
performance. To address the two issues, this paper proposes a Multi-Agent
Bidirectional Encoder Representations from Transformers (MA-BERT) model that
fully considers the multi-agent characteristic of the ATM system and learns air
traffic controllers' decisions, and a pre-training and fine-tuning transfer
learning framework. By pre-training the MA-BERT on a large dataset from a major
airport and then fine-tuning it to other airports and specific air traffic
applications, a large amount of the total training time can be saved. In
addition, for newly adopted procedures and constructed airports where no
historical data is available, this paper shows that the pre-trained MA-BERT can
achieve high performance by updating regularly with little data. The proposed
transfer learning framework and MA-BERT are tested with the automatic dependent
surveillance-broadcast data recorded in 3 airports in South Korea in 2019.
\\ ( https://arxiv.org/abs/2401.14421 ,  4875kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14422
Date: Wed, 24 Jan 2024 02:08:48 GMT   (1107kb,D)

Title: Location Agnostic Source-Free Domain Adaptive Learning to Predict Solar
  Power Generation
Authors: Md Shazid Islam, A S M Jahid Hasan, Md Saydur Rahman, Jubair Yusuf, Md
  Saiful Islam Sajol, Farhana Akter Tumpa
Categories: cs.LG
\\
  The prediction of solar power generation is a challenging task due to its
dependence on climatic characteristics that exhibit spatial and temporal
variability. The performance of a prediction model may vary across different
places due to changes in data distribution, resulting in a model that works
well in one region but not in others. Furthermore, as a consequence of global
warming, there is a notable acceleration in the alteration of weather patterns
on an annual basis. This phenomenon introduces the potential for diminished
efficacy of existing models, even within the same geographical region, as time
progresses. In this paper, a domain adaptive deep learning-based framework is
proposed to estimate solar power generation using weather features that can
solve the aforementioned challenges. A feed-forward deep convolutional network
model is trained for a known location dataset in a supervised manner and
utilized to predict the solar power of an unknown location later. This adaptive
data-driven approach exhibits notable advantages in terms of computing speed,
storage efficiency, and its ability to improve outcomes in scenarios where
state-of-the-art non-adaptive methods fail. Our method has shown an improvement
of $10.47 \%$, $7.44 \%$, $5.11\%$ in solar power prediction accuracy compared
to best performing non-adaptive method for California (CA), Florida (FL) and
New York (NY), respectively.
\\ ( https://arxiv.org/abs/2401.14422 ,  1107kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14424
Date: Wed, 24 Jan 2024 07:47:04 GMT   (1612kb,D)

Title: Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo
  Tree Search
Authors: Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Meilan
  Hao, Shu Wei, Yusong Deng
Categories: cs.LG cs.AI
Comments: 24 pages
\\
  Finding a concise and interpretable mathematical formula that accurately
describes the relationship between each variable and the predicted value in the
data is a crucial task in scientific research, as well as a significant
challenge in artificial intelligence. This problem is referred to as symbolic
regression, which is an NP-hard problem. Last year, a symbolic regression
method based on Monte Carlo Tree Search (MCTS) was proposed and sota was
obtained on multiple datasets. While this algorithm has shown considerable
improvement in recovering target expressions compared to previous methods, the
lack of guidance during the MCTS process severely hampers its search
efficiency. Recently, some algorithms have added a pre-trained policy network
to guide the search of MCTS, but the pre-trained policy network generalizes
poorly. To balance efficiency and generality, we propose SR-GPT combining ideas
from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines
MCTS with a Generative Pre-Trained Transformer (GPT). By using GPT to guide the
MCTS process, the search efficiency of MCTS is significantly improved. Next, we
utilize the MCTS results to further refine the GPT, enhancing its capabilities
and providing more accurate guidance for the MCTS process. MCTS and GPT are
coupled together and optimize each other until the target expression is
successfully determined. We conducted extensive evaluations of SR-GPT using 222
expressions sourced from over 10 different symbolic regression datasets. The
experimental results demonstrate that SR-GPT outperforms existing
state-of-the-art algorithms in accurately recovering symbolic expressions both
with and without added noise.
\\ ( https://arxiv.org/abs/2401.14424 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14426
Date: Wed, 24 Jan 2024 08:10:36 GMT   (745kb,D)

Title: M$^3$TN: Multi-gate Mixture-of-Experts based Multi-valued Treatment
  Network for Uplift Modeling
Authors: Zexu Sun, Xu Chen
Categories: cs.LG cs.AI stat.ME
Comments: ICASSP 2024
\\
  Uplift modeling is a technique used to predict the effect of a treatment
(e.g., discounts) on an individual's response. Although several methods have
been proposed for multi-valued treatment, they are extended from binary
treatment methods. There are still some limitations. Firstly, existing methods
calculate uplift based on predicted responses, which may not guarantee a
consistent uplift distribution between treatment and control groups. Moreover,
this may cause cumulative errors for multi-valued treatment. Secondly, the
model parameters become numerous with many prediction heads, leading to reduced
efficiency. To address these issues, we propose a novel \underline{M}ulti-gate
\underline{M}ixture-of-Experts based \underline{M}ulti-valued
\underline{T}reatment \underline{N}etwork (M$^3$TN). M$^3$TN consists of two
components: 1) a feature representation module with Multi-gate
Mixture-of-Experts to improve the efficiency; 2) a reparameterization module by
modeling uplift explicitly to improve the effectiveness. We also conduct
extensive experiments to demonstrate the effectiveness and efficiency of our
M$^3$TN.
\\ ( https://arxiv.org/abs/2401.14426 ,  745kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14429
Date: Wed, 24 Jan 2024 21:00:42 GMT   (4865kb)

Title: [Re] The Discriminative Kalman Filter for Bayesian Filtering with
  Nonlinear and Non-Gaussian Observation Models
Authors: Josue Casco-Rodriguez, Caleb Kemere, Richard G. Baraniuk
Categories: cs.LG cs.RO eess.SP stat.ML
\\
  Kalman filters provide a straightforward and interpretable means to estimate
hidden or latent variables, and have found numerous applications in control,
robotics, signal processing, and machine learning. One such application is
neural decoding for neuroprostheses. In 2020, Burkhart et al. thoroughly
evaluated their new version of the Kalman filter that leverages Bayes' theorem
to improve filter performance for highly non-linear or non-Gaussian observation
models. This work provides an open-source Python alternative to the authors'
MATLAB algorithm. Specifically, we reproduce their most salient results for
neuroscientific contexts and further examine the efficacy of their filter using
multiple random seeds and previously unused trials from the authors' dataset.
All experiments were performed offline on a single computer.
\\ ( https://arxiv.org/abs/2401.14429 ,  4865kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14439
Date: Thu, 25 Jan 2024 14:20:00 GMT   (1015kb,D)

Title: Incremental Affinity Propagation based on Cluster Consolidation and
  Stratification
Authors: Silvana Castano, Alfio Ferrara, Stefano Montanelli, Francesco Periti
Categories: cs.LG cs.NE
\\
  Modern data mining applications require to perform incremental clustering
over dynamic datasets by tracing temporal changes over the resulting clusters.
In this paper, we propose A-Posteriori affinity Propagation (APP), an
incremental extension of Affinity Propagation (AP) based on cluster
consolidation and cluster stratification to achieve faithfulness and
forgetfulness. APP enforces incremental clustering where i) new arriving
objects are dynamically consolidated into previous clusters without the need to
re-execute clustering over the entire dataset of objects, and ii) a faithful
sequence of clustering results is produced and maintained over time, while
allowing to forget obsolete clusters with decremental learning functionalities.
Four popular labeled datasets are used to test the performance of APP with
respect to benchmark clustering performances obtained by conventional AP and
Incremental Affinity Propagation based on Nearest neighbor Assignment (IAPNA)
algorithms. Experimental results show that APP achieves comparable clustering
performance while enforcing scalability at the same time.
\\ ( https://arxiv.org/abs/2401.14439 ,  1015kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14469
Date: Thu, 25 Jan 2024 19:05:53 GMT   (16561kb,D)

Title: Unveiling the Unseen: Identifiable Clusters in Trained Depthwise
  Convolutional Kernels
Authors: Zahra Babaiee, Peyman M. Kiasari, Daniela Rus, Radu Grosu
Categories: cs.LG cs.AI cs.CV cs.NE
\\
  Recent advances in depthwise-separable convolutional neural networks
(DS-CNNs) have led to novel architectures, that surpass the performance of
classical CNNs, by a considerable scalability and accuracy margin. This paper
reveals another striking property of DS-CNN architectures: discernible and
explainable patterns emerge in their trained depthwise convolutional kernels in
all layers. Through an extensive analysis of millions of trained filters, with
different sizes and from various models, we employed unsupervised clustering
with autoencoders, to categorize these filters. Astonishingly, the patterns
converged into a few main clusters, each resembling the difference of Gaussian
(DoG) functions, and their first and second-order derivatives. Notably, we were
able to classify over 95\% and 90\% of the filters from state-of-the-art
ConvNextV2 and ConvNeXt models, respectively. This finding is not merely a
technological curiosity; it echoes the foundational models neuroscientists have
long proposed for the vision systems of mammals. Our results thus deepen our
understanding of the emergent properties of trained DS-CNNs and provide a
bridge between artificial and biological visual processing systems. More
broadly, they pave the way for more interpretable and biologically-inspired
neural network designs in the future.
\\ ( https://arxiv.org/abs/2401.14469 ,  16561kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14483
Date: Thu, 25 Jan 2024 19:36:11 GMT   (315kb,D)

Title: Four Facets of Forecast Felicity: Calibration, Predictiveness,
  Randomness and Regret
Authors: Rabanus Derr and Robert C. Williamson
Categories: cs.LG stat.ML
\\
  Machine learning is about forecasting. Forecasts, however, obtain their
usefulness only through their evaluation. Machine learning has traditionally
focused on types of losses and their corresponding regret. Currently, the
machine learning community regained interest in calibration. In this work, we
show the conceptual equivalence of calibration and regret in evaluating
forecasts. We frame the evaluation problem as a game between a forecaster, a
gambler and nature. Putting intuitive restrictions on gambler and forecaster,
calibration and regret naturally fall out of the framework. In addition, this
game links evaluation of forecasts to randomness of outcomes. Random outcomes
with respect to forecasts are equivalent to good forecasts with respect to
outcomes. We call those dual aspects, calibration and regret, predictiveness
and randomness, the four facets of forecast felicity.
\\ ( https://arxiv.org/abs/2401.14483 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14488
Date: Thu, 25 Jan 2024 19:49:02 GMT   (2341kb,D)

Title: Scilab-RL: A software framework for efficient reinforcement learning and
  cognitive modeling research
Authors: Jan Dohmen, Frank R\"oder, Manfred Eppe
Categories: cs.LG cs.AI cs.RO
\\
  One problem with researching cognitive modeling and reinforcement learning
(RL) is that researchers spend too much time on setting up an appropriate
computational framework for their experiments. Many open source implementations
of current RL algorithms exist, but there is a lack of a modular suite of tools
combining different robotic simulators and platforms, data visualization,
hyperparameter optimization, and baseline experiments. To address this problem,
we present Scilab-RL, a software framework for efficient research in cognitive
modeling and reinforcement learning for robotic agents. The framework focuses
on goal-conditioned reinforcement learning using Stable Baselines 3 and the
OpenAI gym interface. It enables native possibilities for experiment
visualizations and hyperparameter optimization. We describe how these features
enable researchers to conduct experiments with minimal time effort, thus
maximizing research output.
\\ ( https://arxiv.org/abs/2401.14488 ,  2341kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14498
Date: Thu, 25 Jan 2024 20:29:07 GMT   (513kb,D)

Title: Predictive Analysis for Optimizing Port Operations
Authors: Aniruddha Rajendra Rao, Haiyan Wang, Chetan Gupta
Categories: cs.LG cs.SY eess.SY stat.AP stat.ML
Comments: 13 pages, 9 figures, 4 Tables. Submitted at IEEE IJCNN 2024
\\
  Maritime transport is a pivotal logistics mode for the long-distance and bulk
transportation of goods. However, the intricate planning involved in this mode
is often hindered by uncertainties, including weather conditions, cargo
diversity, and port dynamics, leading to increased costs. Consequently,
accurately estimating vessel total (stay) time at port and potential delays
becomes imperative for effective planning and scheduling in port operations.
This study aims to develop a port operation solution with competitive
prediction and classification capabilities for estimating vessel Total and
Delay times. This research addresses a significant gap in port analysis models
for vessel Stay and Delay times, offering a valuable contribution to the field
of maritime logistics. The proposed solution is designed to assist
decision-making in port environments and predict service delays. This is
demonstrated through a case study on Brazil ports. Additionally, feature
analysis is used to understand the key factors impacting maritime logistics,
enhancing the overall understanding of the complexities involved in port
operations.
\\ ( https://arxiv.org/abs/2401.14498 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14521
Date: Thu, 25 Jan 2024 21:26:49 GMT   (7672kb)

Title: Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological
  Modeling using the Mass-Conserving-Perceptron
Authors: Yuan-Heng Wang, Hoshin V. Gupta
Categories: cs.LG cs.AI
Comments: 50 pages, 7 Figures, 2 Tables, 1 Supplementary Material
\\
  We investigate the applicability of machine learning technologies to the
development of parsimonious, interpretable, catchment-scale hydrologic models
using directed-graph architectures based on the mass-conserving perceptron
(MCP) as the fundamental computational unit. Here, we focus on architectural
complexity (depth) at a single location, rather than universal applicability
(breadth) across large samples of catchments. The goal is to discover a minimal
representation (numbers of cell-states and flow paths) that represents the
dominant processes that can explain the input-state-output behaviors of a given
catchment, with particular emphasis given to simulating the full range (high,
medium, and low) of flow dynamics. We find that a HyMod-like architecture with
three cell-states and two major flow pathways achieves such a representation at
our study location, but that the additional incorporation of an input-bypass
mechanism significantly improves the timing and shape of the hydrograph, while
the inclusion of bi-directional groundwater mass exchanges significantly
enhances the simulation of baseflow. Overall, our results demonstrate the
importance of using multiple diagnostic metrics for model evaluation, while
highlighting the need for designing training metrics that are better suited to
extracting information across the full range of flow dynamics. Further, they
set the stage for interpretable regional-scale MCP-based hydrological modeling
(using large sample data) by using neural architecture search to determine
appropriate minimal representations for catchments in different hydroclimatic
regimes.
\\ ( https://arxiv.org/abs/2401.14521 ,  7672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14535
Date: Thu, 25 Jan 2024 22:01:07 GMT   (13537kb,D)

Title: CaRiNG: Learning Temporal Causal Representation under Non-Invertible
  Generation Process
Authors: Guangyi Chen, Yifan Shen, Zhenhao Chen, Xiangchen Song, Yuewen Sun,
  Weiran Yao, Xiao Liu, Kun Zhang
Categories: cs.LG cs.CV stat.ME
Comments: 22 pages, preprint
\\
  Identifying the underlying time-delayed latent causal processes in sequential
data is vital for grasping temporal dynamics and making downstream reasoning.
While some recent methods can robustly identify these latent causal variables,
they rely on strict assumptions about the invertible generation process from
latent variables to observed data. However, these assumptions are often hard to
satisfy in real-world applications containing information loss. For instance,
the visual perception process translates a 3D space into 2D images, or the
phenomenon of persistence of vision incorporates historical data into current
perceptions. To address this challenge, we establish an identifiability theory
that allows for the recovery of independent latent components even when they
come from a nonlinear and non-invertible mix. Using this theory as a
foundation, we propose a principled approach, CaRiNG, to learn the CAusal
RepresentatIon of Non-invertible Generative temporal data with identifiability
guarantees. Specifically, we utilize temporal context to recover lost latent
information and apply the conditions in our theory to guide the training
process. Through experiments conducted on synthetic datasets, we validate that
our CaRiNG method reliably identifies the causal process, even when the
generation process is non-invertible. Moreover, we demonstrate that our
approach considerably improves temporal understanding and reasoning in
practical applications.
\\ ( https://arxiv.org/abs/2401.14535 ,  13537kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14539
Date: Thu, 25 Jan 2024 22:09:28 GMT   (2931kb,D)

Title: Understanding Disparities in Post Hoc Machine Learning Explanation
Authors: Vishwali Mhasawade, Salman Rahman, Zoe Haskell-Craig, Rumi Chunara
Categories: cs.LG cs.CY stat.ML
\\
  Previous work has highlighted that existing post-hoc explanation methods
exhibit disparities in explanation fidelity (across 'race' and 'gender' as
sensitive attributes), and while a large body of work focuses on mitigating
these issues at the explanation metric level, the role of the data generating
process and black box model in relation to explanation disparities remains
largely unexplored. Accordingly, through both simulations as well as
experiments on a real-world dataset, we specifically assess challenges to
explanation disparities that originate from properties of the data: limited
sample size, covariate shift, concept shift, omitted variable bias, and
challenges based on model properties: inclusion of the sensitive attribute and
appropriate functional form. Through controlled simulation analyses, our study
demonstrates that increased covariate shift, concept shift, and omission of
covariates increase explanation disparities, with the effect pronounced higher
for neural network models that are better able to capture the underlying
functional form in comparison to linear models. We also observe consistent
findings regarding the effect of concept shift and omitted variable bias on
explanation disparities in the Adult income dataset. Overall, results indicate
that disparities in model explanations can also depend on data and model
properties. Based on this systematic investigation, we provide recommendations
for the design of explanation methods that mitigate undesirable disparities.
\\ ( https://arxiv.org/abs/2401.14539 ,  2931kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14544
Date: Thu, 25 Jan 2024 22:26:15 GMT   (11123kb,D)

Title: Bayesian Optimization through Gaussian Cox Process Models for
  Spatio-temporal Data
Authors: Yongsheng Mei, Mahdi Imani, Tian Lan
Categories: cs.LG math.FA math.PR
Comments: 2024 International Conference on Learning Representations (ICLR)
\\
  Bayesian optimization (BO) has established itself as a leading strategy for
efficiently optimizing expensive-to-evaluate functions. Existing BO methods
mostly rely on Gaussian process (GP) surrogate models and are not applicable to
(doubly-stochastic) Gaussian Cox processes, where the observation process is
modulated by a latent intensity function modeled as a GP. In this paper, we
propose a novel maximum a posteriori inference of Gaussian Cox processes. It
leverages the Laplace approximation and change of kernel technique to transform
the problem into a new reproducing kernel Hilbert space, where it becomes more
tractable computationally. It enables us to obtain both a functional posterior
of the latent intensity function and the covariance of the posterior, thus
extending existing works that often focus on specific link functions or
estimating the posterior mean. Using the result, we propose a BO framework
based on the Gaussian Cox process model and further develop a Nystr\"om
approximation for efficient computation. Extensive evaluations on various
synthetic and real-world datasets demonstrate significant improvement over
state-of-the-art inference solutions for Gaussian Cox processes, as well as
effective BO with a wide range of acquisition functions designed through the
underlying Gaussian Cox process model.
\\ ( https://arxiv.org/abs/2401.14544 ,  11123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14557
Date: Thu, 25 Jan 2024 22:54:39 GMT   (699kb,D)

Title: Extension of Recurrent Kernels to different Reservoir Computing
  topologies
Authors: Giuseppe Alessio D'Inverno, Jonathan Dong
Categories: cs.LG cs.NE
Comments: 8 pages
\\
  Reservoir Computing (RC) has become popular in recent years due to its fast
and efficient computational capabilities. Standard RC has been shown to be
equivalent in the asymptotic limit to Recurrent Kernels, which helps in
analyzing its expressive power. However, many well-established RC paradigms,
such as Leaky RC, Sparse RC, and Deep RC, are yet to be analyzed in such a way.
This study aims to fill this gap by providing an empirical analysis of the
equivalence of specific RC architectures with their corresponding Recurrent
Kernel formulation. We conduct a convergence study by varying the activation
function implemented in each architecture. Our study also sheds light on the
role of sparse connections in RC architectures and propose an optimal sparsity
level that depends on the reservoir size. Furthermore, our systematic analysis
shows that in Deep RC models, convergence is better achieved with successive
reservoirs of decreasing sizes.
\\ ( https://arxiv.org/abs/2401.14557 ,  699kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14578
Date: Fri, 26 Jan 2024 00:32:58 GMT   (1962kb)

Title: GOAt: Explaining Graph Neural Networks via Graph Output Attribution
Authors: Shengyao Lu, Keith G. Mills, Jiao He, Bang Liu, Di Niu
Categories: cs.LG
Comments: ICLR 2024 Poster
\\
  Understanding the decision-making process of Graph Neural Networks (GNNs) is
crucial to their interpretability. Most existing methods for explaining GNNs
typically rely on training auxiliary models, resulting in the explanations
remain black-boxed. This paper introduces Graph Output Attribution (GOAt), a
novel method to attribute graph outputs to input graph features, creating GNN
explanations that are faithful, discriminative, as well as stable across
similar samples. By expanding the GNN as a sum of scalar products involving
node features, edge features and activation patterns, we propose an efficient
analytical method to compute contribution of each node or edge feature to each
scalar product and aggregate the contributions from all scalar products in the
expansion form to derive the importance of each node and edge. Through
extensive experiments on synthetic and real-world data, we show that our method
not only outperforms various state-ofthe-art GNN explainers in terms of the
commonly used fidelity metric, but also exhibits stronger discriminability, and
stability by a remarkable margin.
\\ ( https://arxiv.org/abs/2401.14578 ,  1962kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14580
Date: Fri, 26 Jan 2024 00:47:43 GMT   (95kb,D)

Title: Design Your Own Universe: A Physics-Informed Agnostic Method for
  Enhancing Graph Neural Networks
Authors: Dai Shi, Andi Han, Lequan Lin, Yi Guo, Zhiyong Wang, Junbin Gao
Categories: cs.LG
\\
  Physics-informed Graph Neural Networks have achieved remarkable performance
in learning through graph-structured data by mitigating common GNN challenges
such as over-smoothing, over-squashing, and heterophily adaption. Despite these
advancements, the development of a simple yet effective paradigm that
appropriately integrates previous methods for handling all these challenges is
still underway. In this paper, we draw an analogy between the propagation of
GNNs and particle systems in physics, proposing a model-agnostic enhancement
framework. This framework enriches the graph structure by introducing
additional nodes and rewiring connections with both positive and negative
weights, guided by node labeling information. We theoretically verify that GNNs
enhanced through our approach can effectively circumvent the over-smoothing
issue and exhibit robustness against over-squashing. Moreover, we conduct a
spectral analysis on the rewired graph to demonstrate that the corresponding
GNNs can fit both homophilic and heterophilic graphs. Empirical validations on
benchmarks for homophilic, heterophilic graphs, and long-term graph datasets
show that GNNs enhanced by our method significantly outperform their original
counterparts.
\\ ( https://arxiv.org/abs/2401.14580 ,  95kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14585
Date: Fri, 26 Jan 2024 01:16:59 GMT   (11165kb,D)

Title: Diffusion Stochastic Optimization for Min-Max Problems
Authors: Haoyuan Cai, Sulaiman A. Alghunaim, Ali H. Sayed
Categories: cs.LG math.OC
\\
  The optimistic gradient method is useful in addressing minimax optimization
problems. Motivated by the observation that the conventional stochastic version
suffers from the need for a large batch size on the order of
$\mathcal{O}(\varepsilon^{-2})$ to achieve an $\varepsilon$-stationary
solution, we introduce and analyze a new formulation termed Diffusion
Stochastic Same-Sample Optimistic Gradient (DSS-OG). We prove its convergence
and resolve the large batch issue by establishing a tighter upper bound, under
the more general setting of nonconvex Polyak-Lojasiewicz (PL) risk functions.
We also extend the applicability of the proposed method to the distributed
scenario, where agents communicate with their neighbors via a left-stochastic
protocol. To implement DSS-OG, we can query the stochastic gradient oracles in
parallel with some extra memory overhead, resulting in a complexity comparable
to its conventional counterpart. To demonstrate the efficacy of the proposed
algorithm, we conduct tests by training generative adversarial networks.
\\ ( https://arxiv.org/abs/2401.14585 ,  11165kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14591
Date: Fri, 26 Jan 2024 01:36:48 GMT   (8800kb,D)

Title: Ricci flow-guided autoencoders in learning time-dependent dynamics
Authors: Andrew Gracyk
Categories: cs.LG stat.ML
\\
  We present a manifold-based autoencoder method for learning nonlinear
dynamics in time, notably partial differential equations (PDEs), in which the
manifold latent space evolves according to Ricci flow. This can be accomplished
by simulating Ricci flow in a physics-informed setting, and manifold quantities
can be matched so that Ricci flow is empirically achieved. With our
methodology, the manifold is learned as part of the training procedure, so
ideal geometries may be discerned, while the evolution simultaneously induces a
more accommodating latent representation over static methods. We present our
method on a range of numerical experiments consisting of PDEs that encompass
desirable characteristics such as periodicity and randomness, remarking error
on in-distribution and extrapolation scenarios.
\\ ( https://arxiv.org/abs/2401.14591 ,  8800kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14609
Date: Fri, 26 Jan 2024 02:48:45 GMT   (1071kb,D)

Title: Physically Informed Synchronic-adaptive Learning for Industrial Systems
  Modeling in Heterogeneous Media with Unavailable Time-varying Interface
Authors: Aina Wang, Pan Qin, Xi-Ming Sun
Categories: cs.LG cs.CE
\\
  Partial differential equations (PDEs) are commonly employed to model complex
industrial systems characterized by multivariable dependence. Existing
physics-informed neural networks (PINNs) excel in solving PDEs in a homogeneous
medium. However, their feasibility is diminished when PDE parameters are
unknown due to a lack of physical attributions and time-varying interface is
unavailable arising from heterogeneous media. To this end, we propose a
data-physics-hybrid method, physically informed synchronic-adaptive learning
(PISAL), to solve PDEs for industrial systems modeling in heterogeneous media.
First, Net1, Net2, and NetI, are constructed to approximate the solutions
satisfying PDEs and the interface. Net1 and Net2 are utilized to synchronously
learn each solution satisfying PDEs with diverse parameters, while NetI is
employed to adaptively learn the unavailable time-varying interface. Then, a
criterion combined with NetI is introduced to adaptively distinguish the
attributions of measurements and collocation points. Furthermore, NetI is
integrated into a data-physics-hybrid loss function. Accordingly, a
synchronic-adaptive learning (SAL) strategy is proposed to decompose and
optimize each subdomain. Besides, we theoretically prove the approximation
capability of PISAL. Extensive experimental results verify that the proposed
PISAL can be used for industrial systems modeling in heterogeneous media, which
faces the challenges of lack of physical attributions and unavailable
time-varying interface.
\\ ( https://arxiv.org/abs/2401.14609 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14619
Date: Fri, 26 Jan 2024 03:24:55 GMT   (223kb,D)

Title: Resilient Practical Test-Time Adaptation: Soft Batch Normalization
  Alignment and Entropy-driven Memory Bank
Authors: Xingzhi Zhou, Zhiliang Tian, Ka Chun Cheung, Simon See, Nevin L. Zhang
Categories: cs.LG
\\
  Test-time domain adaptation effectively adjusts the source domain model to
accommodate unseen domain shifts in a target domain during inference. However,
the model performance can be significantly impaired by continuous distribution
changes in the target domain and non-independent and identically distributed
(non-i.i.d.) test samples often encountered in practical scenarios. While
existing memory bank methodologies use memory to store samples and mitigate
non-i.i.d. effects, they do not inherently prevent potential model degradation.
To address this issue, we propose a resilient practical test-time adaptation
(ResiTTA) method focused on parameter resilience and data quality.
Specifically, we develop a resilient batch normalization with estimation on
normalization statistics and soft alignments to mitigate overfitting and model
degradation. We use an entropy-driven memory bank that accounts for timeliness,
the persistence of over-confident samples, and sample uncertainty for
high-quality data in adaptation. Our framework periodically adapts the source
domain model using a teacher-student model through a self-training loss on the
memory samples, incorporating soft alignment losses on batch normalization. We
empirically validate ResiTTA across various benchmark datasets, demonstrating
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2401.14619 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14645
Date: Fri, 26 Jan 2024 04:29:53 GMT   (54kb)

Title: Omnipredictors for Regression and the Approximate Rank of Convex
  Functions
Authors: Parikshit Gopalan, Princewill Okoroafor, Prasad Raghavendra, Abhishek
  Shetty, Mihir Singhal
Categories: cs.LG cs.CC cs.DS
\\
  Consider the supervised learning setting where the goal is to learn to
predict labels $\mathbf y$ given points $\mathbf x$ from a distribution. An
\textit{omnipredictor} for a class $\mathcal L$ of loss functions and a class
$\mathcal C$ of hypotheses is a predictor whose predictions incur less expected
loss than the best hypothesis in $\mathcal C$ for every loss in $\mathcal L$.
Since the work of [GKR+21] that introduced the notion, there has been a large
body of work in the setting of binary labels where $\mathbf y \in \{0, 1\}$,
but much less is known about the regression setting where $\mathbf y \in [0,1]$
can be continuous. Our main conceptual contribution is the notion of
\textit{sufficient statistics} for loss minimization over a family of loss
functions: these are a set of statistics about a distribution such that knowing
them allows one to take actions that minimize the expected loss for any loss in
the family. The notion of sufficient statistics relates directly to the
approximate rank of the family of loss functions.
  Our key technical contribution is a bound of $O(1/\varepsilon^{2/3})$ on the
$\epsilon$-approximate rank of convex, Lipschitz functions on the interval
$[0,1]$, which we show is tight up to a factor of $\mathrm{polylog}
(1/\epsilon)$. This yields improved runtimes for learning omnipredictors for
the class of all convex, Lipschitz loss functions under weak learnability
assumptions about the class $\mathcal C$. We also give efficient omnipredictors
when the loss families have low-degree polynomial approximations, or arise from
generalized linear models (GLMs). This translation from sufficient statistics
to faster omnipredictors is made possible by lifting the technique of loss
outcome indistinguishability introduced by [GKH+23] for Boolean labels to the
regression setting.
\\ ( https://arxiv.org/abs/2401.14645 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14694
Date: Fri, 26 Jan 2024 07:34:53 GMT   (2947kb)

Title: TA-RNN: an Attention-based Time-aware Recurrent Neural Network
  Architecture for Electronic Health Records
Authors: Mohammad Al Olaimat (1, 3), Serdar Bozdag (1, 2 and 3) and the
  Alzheimer's Disease Neuroimaging Initiative ((1) Dept. of Computer Science
  and Engineering, University of North Texas, Denton, USA, (2) Dept. of
  Mathematics, University of North Texas, Denton, USA, (3) BioDiscovery
  Institute, University of North Texas, Denton, USA)
Categories: cs.LG cs.AI
Comments: Data used in preparation of this article were obtained from the
  Alzheimer's Disease Neuroimaging Initiative (ADNI) database
  (adni.loni.usc.edu). As such, the investigators within the ADNI contributed
  to the design and implementation of ADNI and/or provided data but did not
  participate in analysis or writing of this report. A complete listing of ADNI
  investigators can be found at:
  http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/
  ADNI_Acknowledgement_List.pdf
\\
  Motivation: Electronic Health Records (EHR) represent a comprehensive
resource of a patient's medical history. EHR are essential for utilizing
advanced technologies such as deep learning (DL), enabling healthcare providers
to analyze extensive data, extract valuable insights, and make precise and
data-driven clinical decisions. DL methods such as Recurrent Neural Networks
(RNN) have been utilized to analyze EHR to model disease progression and
predict diagnosis. However, these methods do not address some inherent
irregularities in EHR data such as irregular time intervals between clinical
visits. Furthermore, most DL models are not interpretable. In this study, we
propose two interpretable DL architectures based on RNN, namely Time-Aware RNN
(TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patient's clinical
outcome in EHR at next visit and multiple visits ahead, respectively. To
mitigate the impact of irregular time intervals, we propose incorporating time
embedding of the elapsed times between visits. For interpretability, we propose
employing a dual-level attention mechanism that operates between visits and
features within each visit.
  Results: The results of the experiments conducted on Alzheimer's Disease
Neuroimaging Initiative (ADNI) and National Alzheimer's Coordinating Center
(NACC) datasets indicated superior performance of proposed models for
predicting Alzheimer's Disease (AD) compared to state-of-the-art and baseline
approaches based on F2 and sensitivity. Additionally, TA-RNN showed superior
performance on Medical Information Mart for Intensive Care (MIMIC-III) dataset
for mortality prediction. In our ablation study, we observed enhanced
predictive performance by incorporating time embedding and attention
mechanisms. Finally, investigating attention weights helped identify
influential visits and features in predictions.
  Availability: https://github.com/bozdaglab/TA-RNN
\\ ( https://arxiv.org/abs/2401.14694 ,  2947kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14695
Date: Fri, 26 Jan 2024 07:36:11 GMT   (694kb,D)

Title: Continuously Evolving Graph Neural Controlled Differential Equations for
  Traffic Forecasting
Authors: Jiajia Wu, Ling Chen
Categories: cs.LG
Comments: 9 pages, 4 figures
\\
  As a crucial technique for developing a smart city, traffic forecasting has
become a popular research focus in academic and industrial communities for
decades. This task is highly challenging due to complex and dynamic
spatial-temporal dependencies in traffic networks. Existing works ignore
continuous temporal dependencies and spatial dependencies evolving over time.
In this paper, we propose Continuously Evolving Graph Neural Controlled
Differential Equations (CEGNCDE) to capture continuous temporal dependencies
and spatial dependencies over time simultaneously. Specifically, a continuously
evolving graph generator (CEGG) based on NCDE is introduced to generate the
spatial dependencies graph that continuously evolves over time from discrete
historical observations. Then, a graph neural controlled differential equations
(GNCDE) framework is introduced to capture continuous temporal dependencies and
spatial dependencies over time simultaneously. Extensive experiments
demonstrate that CEGNCDE outperforms the SOTA methods by average 2.34% relative
MAE reduction, 0.97% relative RMSE reduction, and 3.17% relative MAPE
reduction.
\\ ( https://arxiv.org/abs/2401.14695 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14696
Date: Fri, 26 Jan 2024 07:36:57 GMT   (8001kb,D)

Title: Asymptotic Midpoint Mixup for Margin Balancing and Moderate Broadening
Authors: Hoyong Kim, Semi Lee, Kangil Kim
Categories: cs.LG cs.AI
\\
  In the feature space, the collapse between features invokes critical problems
in representation learning by remaining the features undistinguished.
Interpolation-based augmentation methods such as mixup have shown their
effectiveness in relieving the collapse problem between different classes,
called inter-class collapse. However, intra-class collapse raised in
coarse-to-fine transfer learning has not been discussed in the augmentation
approach. To address them, we propose a better feature augmentation method,
asymptotic midpoint mixup. The method generates augmented features by
interpolation but gradually moves them toward the midpoint of inter-class
feature pairs. As a result, the method induces two effects: 1) balancing the
margin for all classes and 2) only moderately broadening the margin until it
holds maximal confidence. We empirically analyze the collapse effects by
measuring alignment and uniformity with visualizing representations. Then, we
validate the intra-class collapse effects in coarse-to-fine transfer learning
and the inter-class collapse effects in imbalanced learning on long-tailed
datasets. In both tasks, our method shows better performance than other
augmentation methods.
\\ ( https://arxiv.org/abs/2401.14696 ,  8001kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14702
Date: Fri, 26 Jan 2024 08:17:12 GMT   (3236kb,D)

Title: FairSample: Training Fair and Accurate Graph Convolutional Neural
  Networks Efficiently
Authors: Zicun Cong, Shi Baoxu, Shan Li, Jaewon Yang, Qi He, Jian Pei
Categories: cs.LG cs.AI cs.CY
Comments: Accepted by TKDE 2023
DOI: 10.1109/TKDE.2023.3306378
\\
  Fairness in Graph Convolutional Neural Networks (GCNs) becomes a more and
more important concern as GCNs are adopted in many crucial applications.
Societal biases against sensitive groups may exist in many real world graphs.
GCNs trained on those graphs may be vulnerable to being affected by such
biases. In this paper, we adopt the well-known fairness notion of demographic
parity and tackle the challenge of training fair and accurate GCNs efficiently.
We present an in-depth analysis on how graph structure bias, node attribute
bias, and model parameters may affect the demographic parity of GCNs. Our
insights lead to FairSample, a framework that jointly mitigates the three types
of biases. We employ two intuitive strategies to rectify graph structures.
First, we inject edges across nodes that are in different sensitive groups but
similar in node features. Second, to enhance model fairness and retain model
quality, we develop a learnable neighbor sampling policy using reinforcement
learning. To address the bias in node features and model parameters, FairSample
is complemented by a regularization objective to optimize fairness.
\\ ( https://arxiv.org/abs/2401.14702 ,  3236kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14732
Date: Fri, 26 Jan 2024 09:42:51 GMT   (208kb,D)

Title: Residual Quantization with Implicit Neural Codebooks
Authors: Iris Huijben, Matthijs Douze, Matthew Muckley, Ruud van Sloun, Jakob
  Verbeek
Categories: cs.LG
\\
  Vector quantization is a fundamental operation for data compression and
vector search. To obtain high accuracy, multi-codebook methods increase the
rate by representing each vector using codewords across multiple codebooks.
Residual quantization (RQ) is one such method, which increases accuracy by
iteratively quantizing the error of the previous step. The error distribution
is dependent on previously selected codewords. This dependency is, however, not
accounted for in conventional RQ as it uses a generic codebook per quantization
step. In this paper, we propose QINCo, a neural RQ variant which predicts
specialized codebooks per vector using a neural network that is conditioned on
the approximation of the vector from previous steps. Experiments show that
QINCo outperforms state-of-the-art methods by a large margin on several
datasets and code sizes. For example, QINCo achieves better nearest-neighbor
search accuracy using 12 bytes codes than other methods using 16 bytes on the
BigANN and Deep1B dataset.
\\ ( https://arxiv.org/abs/2401.14732 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14758
Date: Fri, 26 Jan 2024 10:33:38 GMT   (5036kb,D)

Title: Off-Policy Primal-Dual Safe Reinforcement Learning
Authors: Zifan Wu, Bo Tang, Qian Lin, Chao Yu, Shangqin Mao, Qianlong Xie,
  Xingxing Wang, Dong Wang
Categories: cs.LG
Comments: ICLR 2024 Poster
\\
  Primal-dual safe RL methods commonly perform iterations between the primal
update of the policy and the dual update of the Lagrange Multiplier. Such a
training paradigm is highly susceptible to the error in cumulative cost
estimation since this estimation serves as the key bond connecting the primal
and dual update processes. We show that this problem causes significant
underestimation of cost when using off-policy methods, leading to the failure
to satisfy the safety constraint. To address this issue, we propose
\textit{conservative policy optimization}, which learns a policy in a
constraint-satisfying area by considering the uncertainty in cost estimation.
This improves constraint satisfaction but also potentially hinders reward
maximization. We then introduce \textit{local policy convexification} to help
eliminate such suboptimality by gradually reducing the estimation uncertainty.
We provide theoretical interpretations of the joint coupling effect of these
two ingredients and further verify them by extensive experiments. Results on
benchmark tasks show that our method not only achieves an asymptotic
performance comparable to state-of-the-art on-policy methods while using much
fewer samples, but also significantly reduces constraint violation during
training. Our code is available at https://github.com/ZifanWu/CAL.
\\ ( https://arxiv.org/abs/2401.14758 ,  5036kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14840
Date: Fri, 26 Jan 2024 13:12:52 GMT   (189kb,D)

Title: GuardML: Efficient Privacy-Preserving Machine Learning Services Through
  Hybrid Homomorphic Encryption
Authors: Eugene Frimpong, Khoa Nguyen, Mindaugas Budzys, Tanveer Khan, Antonis
  Michalas
Categories: cs.LG cs.CR
Comments: 10 pages, accepted at The 39th ACM/SIGAPP Symposium on Applied
  Computing (SAC) conference
\\
  Machine Learning (ML) has emerged as one of data science's most
transformative and influential domains. However, the widespread adoption of ML
introduces privacy-related concerns owing to the increasing number of malicious
attacks targeting ML models. To address these concerns, Privacy-Preserving
Machine Learning (PPML) methods have been introduced to safeguard the privacy
and security of ML models. One such approach is the use of Homomorphic
Encryption (HE). However, the significant drawbacks and inefficiencies of
traditional HE render it impractical for highly scalable scenarios.
Fortunately, a modern cryptographic scheme, Hybrid Homomorphic Encryption
(HHE), has recently emerged, combining the strengths of symmetric cryptography
and HE to surmount these challenges. Our work seeks to introduce HHE to ML by
designing a PPML scheme tailored for end devices. We leverage HHE as the
fundamental building block to enable secure learning of classification outcomes
over encrypted data, all while preserving the privacy of the input data and ML
model. We demonstrate the real-world applicability of our construction by
developing and evaluating an HHE-based PPML application for classifying heart
disease based on sensitive ECG data. Notably, our evaluations revealed a slight
reduction in accuracy compared to inference on plaintext data. Additionally,
both the analyst and end devices experience minimal communication and
computation costs, underscoring the practical viability of our approach. The
successful integration of HHE into PPML provides a glimpse into a more secure
and privacy-conscious future for machine learning on relatively constrained end
devices.
\\ ( https://arxiv.org/abs/2401.14840 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14846
Date: Fri, 26 Jan 2024 13:27:15 GMT   (512kb,D)

Title: Understanding Domain Generalization: A Noise Robustness Perspective
Authors: Rui Qiao, Bryan Kian Hsiang Low
Categories: cs.LG cs.CV
Comments: Our code is available at https://github.com/qiaoruiyt/NoiseRobustDG
\\
  Despite the rapid development of machine learning algorithms for domain
generalization (DG), there is no clear empirical evidence that the existing DG
algorithms outperform the classic empirical risk minimization (ERM) across
standard benchmarks. To better understand this phenomenon, we investigate
whether there are benefits of DG algorithms over ERM through the lens of label
noise. Specifically, our finite-sample analysis reveals that label noise
exacerbates the effect of spurious correlations for ERM, undermining
generalization. Conversely, we illustrate that DG algorithms exhibit implicit
label-noise robustness during finite-sample training even when spurious
correlation is present. Such desirable property helps mitigate spurious
correlations and improve generalization in synthetic experiments. However,
additional comprehensive experiments on real-world benchmark datasets indicate
that label-noise robustness does not necessarily translate to better
performance compared to ERM. We conjecture that the failure mode of ERM arising
from spurious correlations may be less pronounced in practice.
\\ ( https://arxiv.org/abs/2401.14846 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14847
Date: Fri, 26 Jan 2024 13:27:35 GMT   (858kb,D)

Title: Extracting Process-Aware Decision Models from Object-Centric Process
  Data
Authors: Alexandre Goossens, Johannes De Smedt, Jan Vanthienen
Categories: cs.LG
\\
  Organizations execute decisions within business processes on a daily basis
whilst having to take into account multiple stakeholders who might require
multiple point of views of the same process. Moreover, the complexity of the
information systems running these business processes is generally high as they
are linked to databases storing all the relevant data and aspects of the
processes. Given the presence of multiple objects within an information system
which support the processes in their enactment, decisions are naturally
influenced by both these perspectives, logged in object-centric process logs.
However, the discovery of such decisions from object-centric process logs is
not straightforward as it requires to correctly link the involved objects
whilst considering the sequential constraints that business processes impose as
well as correctly discovering what a decision actually does. This paper
proposes the first object-centric decision-mining algorithm called Integrated
Object-centric Decision Discovery Algorithm (IODDA). IODDA is able to discover
how a decision is structured as well as how a decision is made. Moreover, IODDA
is able to discover which activities and object types are involved in the
decision-making process. Next, IODDA is demonstrated with the first artificial
knowledge-intensive process logs whose log generators are provided to the
research community.
\\ ( https://arxiv.org/abs/2401.14847 ,  858kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14876
Date: Fri, 26 Jan 2024 14:02:29 GMT   (3649kb,D)

Title: Cross-Space Adaptive Filter: Integrating Graph Topology and Node
  Attributes for Alleviating the Over-smoothing Problem
Authors: Chen Huang, Haoyang Li, Yifan Zhang, Wenqiang Lei, Jiancheng Lv
Categories: cs.LG cs.AI
Comments: Accepted to WWW 2024. Our code is available at
  https://github.com/huangzichun/Cross-Space-Adaptive-Filter
\\
  The vanilla Graph Convolutional Network (GCN) uses a low-pass filter to
extract low-frequency signals from graph topology, which may lead to the
over-smoothing problem when GCN goes deep. To this end, various methods have
been proposed to create an adaptive filter by incorporating an extra filter
(e.g., a high-pass filter) extracted from the graph topology. However, these
methods heavily rely on topological information and ignore the node attribute
space, which severely sacrifices the expressive power of the deep GCNs,
especially when dealing with disassortative graphs. In this paper, we propose a
cross-space adaptive filter, called CSF, to produce the adaptive-frequency
information extracted from both the topology and attribute spaces.
Specifically, we first derive a tailored attribute-based high-pass filter that
can be interpreted theoretically as a minimizer for semi-supervised kernel
ridge regression. Then, we cast the topology-based low-pass filter as a
Mercer's kernel within the context of GCNs. This serves as a foundation for
combining it with the attribute-based filter to capture the adaptive-frequency
information. Finally, we derive the cross-space filter via an effective
multiple-kernel learning strategy, which unifies the attribute-based high-pass
filter and the topology-based low-pass filter. This helps to address the
over-smoothing problem while maintaining effectiveness. Extensive experiments
demonstrate that CSF not only successfully alleviates the over-smoothing
problem but also promotes the effectiveness of the node classification task.
\\ ( https://arxiv.org/abs/2401.14876 ,  3649kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14893
Date: Fri, 26 Jan 2024 14:21:45 GMT   (121kb,D)

Title: A structured regression approach for evaluating model performance across
  intersectional subgroups
Authors: Christine Herlihy, Kimberly Truong, Alexandra Chouldechova, Miroslav
  Dudik
Categories: cs.LG cs.CY stat.AP stat.ML
\\
  Disaggregated evaluation is a central task in AI fairness assessment, with
the goal to measure an AI system's performance across different subgroups
defined by combinations of demographic or other sensitive attributes. The
standard approach is to stratify the evaluation data across subgroups and
compute performance metrics separately for each group. However, even for
moderately-sized evaluation datasets, sample sizes quickly get small once
considering intersectional subgroups, which greatly limits the extent to which
intersectional groups are considered in many disaggregated evaluations. In this
work, we introduce a structured regression approach to disaggregated evaluation
that we demonstrate can yield reliable system performance estimates even for
very small subgroups. We also provide corresponding inference strategies for
constructing confidence intervals and explore how goodness-of-fit testing can
yield insight into the structure of fairness-related harms experienced by
intersectional groups. We evaluate our approach on two publicly available
datasets, and several variants of semi-synthetic data. The results show that
our method is considerably more accurate than the standard approach, especially
for small subgroups, and goodness-of-fit testing helps identify the key factors
that drive differences in performance.
\\ ( https://arxiv.org/abs/2401.14893 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14948
Date: Fri, 26 Jan 2024 15:33:39 GMT   (17304kb,D)

Title: Conserve-Update-Revise to Cure Generalization and Robustness Trade-off
  in Adversarial Training
Authors: Shruthi Gowda, Bahram Zonooz, Elahe Arani
Categories: cs.LG cs.AI cs.CV
Comments: Accepted as a conference paper at ICLR 2024
\\
  Adversarial training improves the robustness of neural networks against
adversarial attacks, albeit at the expense of the trade-off between standard
and robust generalization. To unveil the underlying factors driving this
phenomenon, we examine the layer-wise learning capabilities of neural networks
during the transition from a standard to an adversarial setting. Our empirical
findings demonstrate that selectively updating specific layers while preserving
others can substantially enhance the network's learning capacity. We therefore
propose CURE, a novel training framework that leverages a gradient prominence
criterion to perform selective conservation, updating, and revision of weights.
Importantly, CURE is designed to be dataset- and architecture-agnostic,
ensuring its applicability across various scenarios. It effectively tackles
both memorization and overfitting issues, thus enhancing the trade-off between
robustness and generalization and additionally, this training approach also
aids in mitigating "robust overfitting". Furthermore, our study provides
valuable insights into the mechanisms of selective adversarial training and
offers a promising avenue for future research.
\\ ( https://arxiv.org/abs/2401.14948 ,  17304kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14953
Date: Fri, 26 Jan 2024 15:37:16 GMT   (1097kb,D)

Title: Learning Universal Predictors
Authors: Jordi Grau-Moya, Tim Genewein, Marcus Hutter, Laurent Orseau,
  Gr\'egoire Del\'etang, Elliot Catt, Anian Ruoss, Li Kevin Wenliang,
  Christopher Mattern, Matthew Aitchison, Joel Veness
Categories: cs.LG cs.AI
Comments: 32 pages, 11 figures
\\
  Meta-learning has emerged as a powerful approach to train neural networks to
learn new tasks quickly from limited data. Broad exposure to different tasks
leads to versatile representations enabling general problem solving. But, what
are the limits of meta-learning? In this work, we explore the potential of
amortizing the most powerful universal predictor, namely Solomonoff Induction
(SI), into neural networks via leveraging meta-learning to its limits. We use
Universal Turing Machines (UTMs) to generate training data used to expose
networks to a broad range of patterns. We provide theoretical analysis of the
UTM data generation processes and meta-training protocols. We conduct
comprehensive experiments with neural architectures (e.g. LSTMs, Transformers)
and algorithmic data generators of varying complexity and universality. Our
results suggest that UTM data is a valuable resource for meta-learning, and
that it can be used to train neural networks capable of learning universal
prediction strategies.
\\ ( https://arxiv.org/abs/2401.14953 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14961
Date: Fri, 26 Jan 2024 15:52:41 GMT   (657kb,D)

Title: End-To-End Set-Based Training for Neural Network Verification
Authors: Lukas Koller, Tobias Ladner, Matthias Althoff
Categories: cs.LG cs.CR cs.LO
\\
  Neural networks are vulnerable to adversarial attacks, i.e., small input
perturbations can result in substantially different outputs of a neural
network. Safety-critical environments require neural networks that are robust
against input perturbations. However, training and formally verifying robust
neural networks is challenging. We address this challenge by employing, for the
first time, a end-to-end set-based training procedure that trains robust neural
networks for formal verification. Our training procedure drastically simplifies
the subsequent formal robustness verification of the trained neural network.
While previous research has predominantly focused on augmenting neural network
training with adversarial attacks, our approach leverages set-based computing
to train neural networks with entire sets of perturbed inputs. Moreover, we
demonstrate that our set-based training procedure effectively trains robust
neural networks, which are easier to verify. In many cases, set-based trained
neural networks outperform neural networks trained with state-of-the-art
adversarial attacks.
\\ ( https://arxiv.org/abs/2401.14961 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14989
Date: Fri, 26 Jan 2024 16:35:48 GMT   (2492kb)

Title: Mapping-to-Parameter Nonlinear Functional Regression with Novel B-spline
  Free Knot Placement Algorithm
Authors: Chengdong Shi, Ching-Hsun Tseng, Wei Zhao, Xiao-Jun Zeng
Categories: cs.LG stat.ML
\\
  We propose a novel approach to nonlinear functional regression, called the
Mapping-to-Parameter function model, which addresses complex and nonlinear
functional regression problems in parameter space by employing any supervised
learning technique. Central to this model is the mapping of function data from
an infinite-dimensional function space to a finite-dimensional parameter space.
This is accomplished by concurrently approximating multiple functions with a
common set of B-spline basis functions by any chosen order, with their knot
distribution determined by the Iterative Local Placement Algorithm, a newly
proposed free knot placement algorithm. In contrast to the conventional
equidistant knot placement strategy that uniformly distributes knot locations
based on a predefined number of knots, our proposed algorithms determine knot
location according to the local complexity of the input or output functions.
The performance of our knot placement algorithms is shown to be robust in both
single-function approximation and multiple-function approximation contexts.
Furthermore, the effectiveness and advantage of the proposed prediction model
in handling both function-on-scalar regression and function-on-function
regression problems are demonstrated through several real data applications, in
comparison with four groups of state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.14989 ,  2492kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14992
Date: Fri, 26 Jan 2024 16:42:49 GMT   (2980kb,D)

Title: Graph-based Active Learning for Entity Cluster Repair
Authors: Victor Christen, Daniel Obraczka, Marvin Hofer, Martin Franke, Erhard
  Rahm
Categories: cs.LG cs.DB
\\
  Cluster repair methods aim to determine errors in clusters and modify them so
that each cluster consists of records representing the same entity. Current
cluster repair methodologies primarily assume duplicate-free data sources,
where each record from one source corresponds to a unique record from another.
However, real-world data often deviates from this assumption due to quality
issues. Recent approaches apply clustering methods in combination with link
categorization methods so they can be applied to data sources with duplicates.
Nevertheless, the results do not show a clear picture since the quality highly
varies depending on the configuration and dataset. In this study, we introduce
a novel approach for cluster repair that utilizes graph metrics derived from
the underlying similarity graphs. These metrics are pivotal in constructing a
classification model to distinguish between correct and incorrect edges. To
address the challenge of limited training data, we integrate an active learning
mechanism tailored to cluster-specific attributes. The evaluation shows that
the method outperforms existing cluster repair methods without distinguishing
between duplicate-free or dirty data sources. Notably, our modified active
learning strategy exhibits enhanced performance when dealing with datasets
containing duplicates, showcasing its effectiveness in such scenarios.
\\ ( https://arxiv.org/abs/2401.14992 ,  2980kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15024
Date: Fri, 26 Jan 2024 17:35:45 GMT   (176kb,D)

Title: SliceGPT: Compress Large Language Models by Deleting Rows and Columns
Authors: Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento,
  Torsten Hoefler, James Hensman
Categories: cs.LG cs.CL
Comments: 22 pages, 8 figures, accepted at ICLR24
\\
  Large language models have become the cornerstone of natural language
processing, but their use comes with substantial costs in terms of compute and
memory resources. Sparsification provides a solution to alleviate these
resource constraints, and recent works have shown that trained models can be
sparsified post-hoc. Existing sparsification techniques face challenges as they
need additional data structures and offer constrained speedup with current
hardware. In this paper we present SliceGPT, a new post-training sparsification
scheme which replaces each weight matrix with a smaller (dense) matrix,
reducing the embedding dimension of the network. Through extensive
experimentation, we show that SliceGPT can remove up to 25% of the model
parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models
while maintaining 99%, 99% and 90% zero-shot task performance of the dense
model respectively. Our sliced models run on fewer GPUs and run faster without
any additional code optimization: on 24GB consumer GPUs we reduce the total
compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB
A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance
in transformer networks, which enables SliceGPT and we hope it will inspire and
enable future avenues to reduce memory and computation demands for pre-trained
models. Code is available at:
https://github.com/microsoft/TransformerCompression
\\ ( https://arxiv.org/abs/2401.15024 ,  176kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15030
Date: Fri, 26 Jan 2024 17:42:59 GMT   (16032kb,D)

Title: On the generalization capacity of neural networks during generic
  multimodal reasoning
Authors: Takuya Ito, Soham Dan, Mattia Rigotti, James Kozloski, Murray Campbell
Categories: cs.LG
Comments: ICLR 2024
\\
  The advent of the Transformer has led to the development of large language
models (LLM), which appear to demonstrate human-like capabilities. To assess
the generality of this class of models and a variety of other base neural
network architectures to multimodal domains, we evaluated and compared their
capacity for multimodal generalization. We introduce a multimodal
question-answer benchmark to evaluate three specific types of
out-of-distribution (OOD) generalization performance: distractor generalization
(generalization in the presence of distractors), systematic compositional
generalization (generalization to new task permutations), and productive
compositional generalization (generalization to more complex tasks structures).
We found that across model architectures (e.g., RNNs, Transformers, Perceivers,
etc.), models with multiple attention layers, or models that leveraged
cross-attention mechanisms between input domains, fared better. Our positive
results demonstrate that for multimodal distractor and systematic
generalization, either cross-modal attention or models with deeper attention
layers are key architectural features required to integrate multimodal inputs.
On the other hand, neither of these architectural features led to productive
generalization, suggesting fundamental limitations of existing architectures
for specific types of multimodal generalization. These results demonstrate the
strengths and limitations of specific architectural components underlying
modern neural models for multimodal reasoning. Finally, we provide Generic COG
(gCOG), a configurable benchmark with several multimodal generalization splits,
for future studies to explore.
\\ ( https://arxiv.org/abs/2401.15030 ,  16032kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15059
Date: Fri, 26 Jan 2024 18:42:01 GMT   (27661kb,D)

Title: Fully Independent Communication in Multi-Agent Reinforcement Learning
Authors: Rafael Pina, Varuna De Silva, Corentin Artaud and Xiaolan Liu
Categories: cs.LG cs.MA
Comments: 11 pages, 8 figures
\\
  Multi-Agent Reinforcement Learning (MARL) comprises a broad area of research
within the field of multi-agent systems. Several recent works have focused
specifically on the study of communication approaches in MARL. While multiple
communication methods have been proposed, these might still be too complex and
not easily transferable to more practical contexts. One of the reasons for that
is due to the use of the famous parameter sharing trick. In this paper, we
investigate how independent learners in MARL that do not share parameters can
communicate. We demonstrate that this setting might incur into some problems,
to which we propose a new learning scheme as a solution. Our results show that,
despite the challenges, independent agents can still learn communication
strategies following our method. Additionally, we use this method to
investigate how communication in MARL is affected by different network
capacities, both for sharing and not sharing parameters. We observe that
communication may not always be needed and that the chosen agent network sizes
need to be considered when used together with communication in order to achieve
efficient learning.
\\ ( https://arxiv.org/abs/2401.15059 ,  27661kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15062
Date: Fri, 26 Jan 2024 18:44:49 GMT   (844kb,D)

Title: Expert with Clustering: Hierarchical Online Preference Learning
  Framework
Authors: Tianyue Zhou, Jung-Hoon Cho, Babak Rahimi Ardabili, Hamed Tabkhi, and
  Cathy Wu
Categories: cs.LG
Comments: This work was submitted to L4DC 2024
\\
  Emerging mobility systems are increasingly capable of recommending options to
mobility users, to guide them towards personalized yet sustainable system
outcomes. Even more so than the typical recommendation system, it is crucial to
minimize regret, because 1) the mobility options directly affect the lives of
the users, and 2) the system sustainability relies on sufficient user
participation. In this study, we consider accelerating user preference learning
by exploiting a low-dimensional latent space that captures the mobility
preferences of users. We introduce a hierarchical contextual bandit framework
named Expert with Clustering (EWC), which integrates clustering techniques and
prediction with expert advice. EWC efficiently utilizes hierarchical user
information and incorporates a novel Loss-guided Distance metric. This metric
is instrumental in generating more representative cluster centroids. In a
recommendation scenario with $N$ users, $T$ rounds per user, and $K$ options,
our algorithm achieves a regret bound of $O(N\sqrt{T\log K} + NT)$. This bound
consists of two parts: the first term is the regret from the Hedge algorithm,
and the second term depends on the average loss from clustering. The algorithm
performs with low regret, especially when a latent hierarchical structure
exists among users. This regret bound underscores the theoretical and
experimental efficacy of EWC, particularly in scenarios that demand rapid
learning and adaptation. Experimental results highlight that EWC can
substantially reduce regret by 27.57% compared to the LinUCB baseline. Our work
offers a data-efficient approach to capturing both individual and collective
behaviors, making it highly applicable to contexts with hierarchical
structures. We expect the algorithm to be applicable to other settings with
layered nuances of user preferences and information.
\\ ( https://arxiv.org/abs/2401.15062 ,  844kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15077
Date: Fri, 26 Jan 2024 18:59:01 GMT   (2194kb,D)

Title: EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty
Authors: Yuhui Li, Fangyun Wei, Chao Zhang, Hongyang Zhang
Categories: cs.LG cs.CL
\\
  Auto-regressive decoding makes the inference of Large Language Models (LLMs)
time-consuming. We propose a simple framework, EAGLE (Extrapolation Algorithm
for Greater Language-model Efficiency), for lossless acceleration. Unlike
traditional speculative sampling methods, EAGLE operates the drafting process
auto-regressively at the more regular (second-top-layer) feature level and
addresses the sampling uncertainty issues in the next-feature prediction
problems by integrating tokens from one time step ahead. The acceleration
provided by EAGLE is lossless: it involves no fine-tuning of the target LLM,
and the generated text maintains the same distribution as that of vanilla
auto-regressive decoding. As of the submission of this paper, EAGLE is the
fastest known framework within the speculative sampling family. On MT-bench,
EAGLE is 3x faster than vanilla decoding, 2x faster than Lookahead, and 1.6x
faster than Medusa. Using gpt-fast, EAGLE attains on average 160 tokens/s with
LLaMA2-Chat 13B on a single RTX 3090 GPU, compared to 24 tokens/s of
Huggingface's implementations.
\\ ( https://arxiv.org/abs/2401.15077 ,  2194kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2311.13544 (*cross-listing*)
Date: Wed, 22 Nov 2023 17:37:42 GMT   (816kb,D)

Title: Piecewise polynomial regression of tame functions via integer
  programming
Authors: Jiri Nemecek, Gilles Bareilles, Johannes Aspman, Jakub Marecek
Categories: math.OC cs.AI cs.LG math.ST stat.TH
\\
  We consider the task of estimating functions belonging to a specific class of
nonsmooth functions, namely so-called tame functions. These functions appear in
a wide range of applications: training deep learning, value functions of
mixed-integer programs, or wave functions of small molecules. We show that tame
functions are approximable by piecewise polynomials on any full-dimensional
cube. We then present the first ever mixed-integer programming formulation of
piecewise polynomial regression. Together, these can be used to estimate tame
functions. We demonstrate promising computational results.
\\ ( https://arxiv.org/abs/2311.13544 ,  816kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14425 (*cross-listing*)
Date: Wed, 24 Jan 2024 08:03:13 GMT   (2093kb,D)

Title: No Longer Trending on Artstation: Prompt Analysis of Generative AI Art
Authors: Jon McCormack, Maria Teresa Llano, Stephen James Krol, Nina Rajcic
Categories: cs.HC cs.AI cs.CV cs.CY cs.NE
Comments: Paper accepted for EvoMUSART 2024, Aberystwyth, Wales, United
  Kingdom, 3-5 April 2024
ACM-class: J.5; I.2
\\
  Image generation using generative AI is rapidly becoming a major new source
of visual media, with billions of AI generated images created using diffusion
models such as Stable Diffusion and Midjourney over the last few years. In this
paper we collect and analyse over 3 million prompts and the images they
generate. Using natural language processing, topic analysis and visualisation
methods we aim to understand collectively how people are using text prompts,
the impact of these systems on artists, and more broadly on the visual cultures
they promote. Our study shows that prompting focuses largely on surface
aesthetics, reinforcing cultural norms, popular conventional representations
and imagery. We also find that many users focus on popular topics (such as
making colouring books, fantasy art, or Christmas cards), suggesting that the
dominant use for the systems analysed is recreational rather than artistic.
\\ ( https://arxiv.org/abs/2401.14425 ,  2093kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14434 (*cross-listing*)
Date: Thu, 25 Jan 2024 09:24:19 GMT   (9322kb,D)

Title: Transforming gradient-based techniques into interpretable methods
Authors: Caroline Mazini Rodrigues (LRDE, LIGM), Nicolas Boutry (LRDE), Laurent
  Najman (LIGM)
Categories: cs.CV cs.AI cs.LG
\\
  The explication of Convolutional Neural Networks (CNN) through xAI techniques
often poses challenges in interpretation. The inherent complexity of input
features, notably pixels extracted from images, engenders complex correlations.
Gradient-based methodologies, exemplified by Integrated Gradients (IG),
effectively demonstrate the significance of these features. Nevertheless, the
conversion of these explanations into images frequently yields considerable
noise. Presently, we introduce GAD (Gradient Artificial Distancing) as a
supportive framework for gradient-based techniques. Its primary objective is to
accentuate influential regions by establishing distinctions between classes.
The essence of GAD is to limit the scope of analysis during visualization and,
consequently reduce image noise. Empirical investigations involving occluded
images have demonstrated that the identified regions through this methodology
indeed play a pivotal role in facilitating class differentiation.
\\ ( https://arxiv.org/abs/2401.14434 ,  9322kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14436 (*cross-listing*)
Date: Thu, 25 Jan 2024 13:31:43 GMT   (2249kb,D)

Title: Trust model of privacy-concerned, emotionally-aware agents in a
  cooperative logistics problem
Authors: J. Carbo, J.M. Molina
Categories: cs.MA cs.AI
DOI: 10.3390/app13158681
\\
  In this paper we propose a trust model to be used into a hypothetical mixed
environment where humans and unmanned vehicles cooperate. We address the
inclusion of emotions inside a trust model in a coherent way to the practical
approaches to the current psychology theories. The most innovative contribution
is how privacy issues play a role in the cooperation decisions of the emotional
trust model. Both, emotions and trust have been cognitively modeled and managed
with the Beliefs, Desires and Intentions (BDI) paradigm into autonomous agents
implemented in GAML (the programming language of GAMA agent platform) that
communicates using the IEEE FIPA standard. The trusting behaviour of these
emotional agents is tested in a cooperative logistics problem where: agents
have to move objects to destinations and some of the objects and places have
privacy issues. The execution of simulations of this logistic problem shows how
emotions and trust contribute to improve the performance of agents in terms of
both, time savings and privacy protection
\\ ( https://arxiv.org/abs/2401.14436 ,  2249kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14444 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:08:00 GMT   (68kb,D)

Title: ICASSP 2024 Speech Signal Improvement Challenge
Authors: Nicolae Catalin Ristea, Ando Saabas, Ross Cutler, Babak Naderi,
  Sebastian Braun, Solomiya Branets
Categories: cs.SD cs.AI cs.CV eess.AS
\\
  The ICASSP 2024 Speech Signal Improvement Grand Challenge is intended to
stimulate research in the area of improving the speech signal quality in
communication systems. This marks our second challenge, building upon the
success from the previous ICASSP 2023 Grand Challenge. We enhance the
competition by introducing a dataset synthesizer, enabling all participating
teams to start at a higher baseline, an objective metric for our extended P.804
tests, transcripts for the 2023 test set, and we add Word Accuracy (WAcc) as a
metric. We evaluate a total of 13 systems in the real-time track and 11 systems
in the non-real-time track using both subjective P.804 and objective Word
Accuracy metrics.
\\ ( https://arxiv.org/abs/2401.14444 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14446 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:58:05 GMT   (386kb,D)

Title: Black-Box Access is Insufficient for Rigorous AI Audits
Authors: Stephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor
  Lynn Curtis, Benjamin Bucknall, Andreas Haupt, Kevin Wei, J\'er\'emy
  Scheurer, Marius Hobbhahn, Lee Sharkey, Satyapriya Krishna, Marvin Von Hagen,
  Silas Alberti, Alan Chan, Qinyi Sun, Michael Gerovitch, David Bau, Max
  Tegmark, David Krueger, Dylan Hadfield-Menell
Categories: cs.CY cs.AI cs.CR
\\
  External audits of AI systems are increasingly recognized as a key mechanism
for AI governance. The effectiveness of an audit, however, depends on the
degree of system access granted to auditors. Recent audits of state-of-the-art
AI systems have primarily relied on black-box access, in which auditors can
only query the system and observe its outputs. However, white-box access to the
system's inner workings (e.g., weights, activations, gradients) allows an
auditor to perform stronger attacks, more thoroughly interpret models, and
conduct fine-tuning. Meanwhile, outside-the-box access to its training and
deployment information (e.g., methodology, code, documentation,
hyperparameters, data, deployment details, findings from internal evaluations)
allows for auditors to scrutinize the development process and design more
targeted evaluations. In this paper, we examine the limitations of black-box
audits and the advantages of white- and outside-the-box audits. We also discuss
technical, physical, and legal safeguards for performing these audits with
minimal security risks. Given that different forms of access can lead to very
different levels of evaluation, we conclude that (1) transparency regarding the
access and methods used by auditors is necessary to properly interpret audit
results, and (2) white- and outside-the-box access allow for substantially more
scrutiny than black-box access alone.
\\ ( https://arxiv.org/abs/2401.14446 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14447 (*cross-listing*)
Date: Thu, 25 Jan 2024 18:58:11 GMT   (1060kb,D)

Title: Wordflow: Social Prompt Engineering for Large Language Models
Authors: Zijie J. Wang, Aishwarya Chakravarthy, David Munechika, Duen Horng
  Chau
Categories: cs.HC cs.AI cs.CL cs.LG
Comments: 8 pages, 7 figures. Wordflow is available at:
  https://poloclub.github.io/wordflow. The code is available at:
  https://github.com/poloclub/wordflow/. For a demo video, see:
  https://youtu.be/3dOcVuofGVo
\\
  Large language models (LLMs) require well-crafted prompts for effective use.
Prompt engineering, the process of designing prompts, is challenging,
particularly for non-experts who are less familiar with AI technologies. While
researchers have proposed techniques and tools to assist LLM users in prompt
design, these works primarily target AI application developers rather than
non-experts. To address this research gap, we propose social prompt
engineering, a novel paradigm that leverages social computing techniques to
facilitate collaborative prompt design. To investigate social prompt
engineering, we introduce Wordflow, an open-source and social text editor that
enables everyday users to easily create, run, share, and discover LLM prompts.
Additionally, by leveraging modern web technologies, Wordflow allows users to
run LLMs locally and privately in their browsers. Two usage scenarios highlight
how social prompt engineering and our tool can enhance laypeople's interaction
with LLMs. Wordflow is publicly accessible at
https://poloclub.github.io/wordflow.
\\ ( https://arxiv.org/abs/2401.14447 ,  1060kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14484 (*cross-listing*)
Date: Thu, 25 Jan 2024 19:38:21 GMT   (1048kb,D)

Title: Design Principles for Generative AI Applications
Authors: Justin D. Weisz, Jessica He, Michael Muller, Gabriela Hoefer, Rachel
  Miles, Werner Geyer
Categories: cs.HC cs.AI
Comments: 34 pages, 4 figures. To be published in CHI 2024
DOI: 10.1145/3613904.3642466
\\
  Generative AI applications present unique design challenges. As generative AI
technologies are increasingly being incorporated into mainstream applications,
there is an urgent need for guidance on how to design user experiences that
foster effective and safe use. We present six principles for the design of
generative AI applications that address unique characteristics of generative AI
UX and offer new interpretations and extensions of known issues in the design
of AI applications. Each principle is coupled with a set of design strategies
for implementing that principle via UX capabilities or through the design
process. The principles and strategies were developed through an iterative
process involving literature review, feedback from design practitioners,
validation against real-world generative AI applications, and incorporation
into the design process of two generative AI applications. We anticipate the
principles to usefully inform the design of generative AI applications by
driving actionable design recommendations.
\\ ( https://arxiv.org/abs/2401.14484 ,  1048kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14489 (*cross-listing*)
Date: Thu, 25 Jan 2024 19:50:31 GMT   (26868kb,D)

Title: The Case for Co-Designing Model Architectures with Hardware
Authors: Quentin Anthony, Jacob Hatef, Deepak Narayanan, Stella Biderman, Stas
  Bekman, Junqi Yin, Aamir Shafi, Hari Subramoni, Dhabaleswar Panda
Categories: cs.DC cs.AI
\\
  While GPUs are responsible for training the vast majority of state-of-the-art
deep learning models, the implications of their architecture are often
overlooked when designing new deep learning (DL) models. As a consequence,
modifying a DL model to be more amenable to the target hardware can
significantly improve the runtime performance of DL training and inference. In
this paper, we provide a set of guidelines for users to maximize the runtime
performance of their transformer models. These guidelines have been created by
carefully considering the impact of various model hyperparameters controlling
model shape on the efficiency of the underlying computation kernels executed on
the GPU. We find the throughput of models with efficient model shapes is up to
39\% higher while preserving accuracy compared to models with a similar number
of parameters but with unoptimized shapes.
\\ ( https://arxiv.org/abs/2401.14489 ,  26868kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14504 (*cross-listing*)
Date: Thu, 25 Jan 2024 20:50:34 GMT   (4148kb,D)

Title: Learning When to See for Long-term Traffic Data Collection on
  Power-constrained Devices
Authors: Ruixuan Zhang, Wenyu Han, Zilin Bian, Kaan Ozbay, Chen Feng
Categories: eess.SY cs.AI cs.LG cs.SY
Comments: Accepted by IEEE 26th International Conference on Intelligent
  Transportation Systems
\\
  Collecting traffic data is crucial for transportation systems and urban
planning, and is often more desirable through easy-to-deploy but
power-constrained devices, due to the unavailability or high cost of power and
network infrastructure. The limited power means an inevitable trade-off between
data collection duration and accuracy/resolution. We introduce a novel
learning-based framework that strategically decides observation timings for
battery-powered devices and reconstructs the full data stream from sparsely
sampled observations, resulting in minimal performance loss and a significantly
prolonged system lifetime. Our framework comprises a predictor, a controller,
and an estimator. The predictor utilizes historical data to forecast future
trends within a fixed time horizon. The controller uses the forecasts to
determine the next optimal timing for data collection. Finally, the estimator
reconstructs the complete data profile from the sampled observations. We
evaluate the performance of the proposed method on PeMS data by an RNN
(Recurrent Neural Network) predictor and estimator, and a DRQN (Deep Recurrent
Q-Network) controller, and compare it against the baseline that uses Kalman
filter and uniform sampling. The results indicate that our method outperforms
the baseline, primarily due to the inclusion of more representative data points
in the profile, resulting in an overall 10\% improvement in estimation
accuracy. Source code will be publicly available.
\\ ( https://arxiv.org/abs/2401.14504 ,  4148kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14523 (*cross-listing*)
Date: Thu, 25 Jan 2024 21:30:06 GMT   (17kb)

Title: Empathy and the Right to Be an Exception: What LLMs Can and Cannot Do
Authors: William Kidder, Jason D'Cruz, and Kush R. Varshney
Categories: cs.CY cs.AI cs.CL
\\
  Advances in the performance of large language models (LLMs) have led some
researchers to propose the emergence of theory of mind (ToM) in artificial
intelligence (AI). LLMs can attribute beliefs, desires, intentions, and
emotions, and they will improve in their accuracy. Rather than employing the
characteristically human method of empathy, they learn to attribute mental
states by recognizing linguistic patterns in a dataset that typically do not
include that individual. We ask whether LLMs' inability to empathize precludes
them from honoring an individual's right to be an exception, that is, from
making assessments of character and predictions of behavior that reflect
appropriate sensitivity to a person's individuality. Can LLMs seriously
consider an individual's claim that their case is different based on internal
mental states like beliefs, desires, and intentions, or are they limited to
judging that case based on its similarities to others? We propose that the
method of empathy has special significance for honoring the right to be an
exception that is distinct from the value of predictive accuracy, at which LLMs
excel. We conclude by considering whether using empathy to consider exceptional
cases has intrinsic or merely practical value and we introduce conceptual and
empirical avenues for advancing this investigation.
\\ ( https://arxiv.org/abs/2401.14523 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14542 (*cross-listing*)
Date: Thu, 25 Jan 2024 22:20:42 GMT   (2250kb,D)

Title: Exploring Musical Roots: Applying Audio Embeddings to Empower Influence
  Attribution for a Generative Music Model
Authors: Julia Barnett, Hugo Flores Garcia, Bryan Pardo
Categories: cs.SD cs.AI eess.AS
Comments: 14 pages + references. Under conference review
\\
  Every artist has a creative process that draws inspiration from previous
artists and their works. Today, "inspiration" has been automated by generative
music models. The black box nature of these models obscures the identity of the
works that influence their creative output. As a result, users may
inadvertently appropriate, misuse, or copy existing artists' works. We
establish a replicable methodology to systematically identify similar pieces of
music audio in a manner that is useful for understanding training data
attribution. A key aspect of our approach is to harness an effective music
audio similarity measure. We compare the effect of applying CLMR and CLAP
embeddings to similarity measurement in a set of 5 million audio clips used to
train VampNet, a recent open source generative music model. We validate this
approach with a human listening study. We also explore the effect that
modifications of an audio example (e.g., pitch shifting, time stretching,
background noise) have on similarity measurements. This work is foundational to
incorporating automated influence attribution into generative modeling, which
promises to let model creators and users move from ignorant appropriation to
informed creation. Audio samples that accompany this paper are available at
https://tinyurl.com/exploring-musical-roots.
\\ ( https://arxiv.org/abs/2401.14542 ,  2250kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14571 (*cross-listing*)
Date: Fri, 26 Jan 2024 00:06:08 GMT   (128kb,D)

Title: Driving Towards Inclusion: Revisiting In-Vehicle Interaction in
  Autonomous Vehicles
Authors: Ashish Bastola, Julian Brinkley, Hao Wang, Abolfazl Razi
Categories: cs.HC cs.AI cs.CY
\\
  This paper presents a comprehensive literature review of the current state of
in-vehicle human-computer interaction (HCI) in the context of self-driving
vehicles, with a specific focus on inclusion and accessibility. This study's
aim is to examine the user-centered design principles for inclusive HCI in
self-driving vehicles, evaluate existing HCI systems, and identify emerging
technologies that have the potential to enhance the passenger experience. The
paper begins by providing an overview of the current state of self-driving
vehicle technology, followed by an examination of the importance of HCI in this
context. Next, the paper reviews the existing literature on inclusive HCI
design principles and evaluates the effectiveness of current HCI systems in
self-driving vehicles. The paper also identifies emerging technologies that
have the potential to enhance the passenger experience, such as voice-activated
interfaces, haptic feedback systems, and augmented reality displays. Finally,
the paper proposes an end-to-end design framework for the development of an
inclusive in-vehicle experience, which takes into consideration the needs of
all passengers, including those with disabilities, or other accessibility
requirements. This literature review highlights the importance of user-centered
design principles in the development of HCI systems for self-driving vehicles
and emphasizes the need for inclusive design to ensure that all passengers can
safely and comfortably use these vehicles. The proposed end-to-end design
framework provides a practical approach to achieving this goal and can serve as
a valuable resource for designers, researchers, and policymakers in this field.
\\ ( https://arxiv.org/abs/2401.14571 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14617 (*cross-listing*)
Date: Fri, 26 Jan 2024 03:20:40 GMT   (3648kb,D)

Title: A Systematic Literature Review on Explainability for Machine/Deep
  Learning-based Software Engineering Research
Authors: Sicong Cao, Xiaobing Sun, Ratnadira Widyasari, David Lo, Xiaoxue Wu,
  Lili Bo, Jiale Zhang, Bin Li, Wei Liu, Di Wu, Yixin Chen
Categories: cs.SE cs.AI
Comments: submitted to ACM Computing Surveys. arXiv admin note: text overlap
  with arXiv:2202.06840 by other authors
\\
  The remarkable achievements of Artificial Intelligence (AI) algorithms,
particularly in Machine Learning (ML) and Deep Learning (DL), have fueled their
extensive deployment across multiple sectors, including Software Engineering
(SE). However, due to their black-box nature, these promising AI-driven SE
models are still far from being deployed in practice. This lack of
explainability poses unwanted risks for their applications in critical tasks,
such as vulnerability detection, where decision-making transparency is of
paramount importance. This paper endeavors to elucidate this interdisciplinary
domain by presenting a systematic literature review of approaches that aim to
improve the explainability of AI models within the context of SE. The review
canvasses work appearing in the most prominent SE & AI conferences and
journals, and spans 63 papers across 21 unique SE tasks. Based on three key
Research Questions (RQs), we aim to (1) summarize the SE tasks where XAI
techniques have shown success to date; (2) classify and analyze different XAI
techniques; and (3) investigate existing evaluation approaches. Based on our
findings, we identified a set of challenges remaining to be addressed in
existing studies, together with a roadmap highlighting potential opportunities
we deemed appropriate and important for future work.
\\ ( https://arxiv.org/abs/2401.14617 ,  3648kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14665 (*cross-listing*)
Date: Fri, 26 Jan 2024 06:13:09 GMT   (1701kb,D)

Title: PepGB: Facilitating peptide drug discovery via graph neural networks
Authors: Yipin Lei, Xu Wang, Meng Fang, Han Li, Xiang Li, Jianyang Zeng
Categories: q-bio.BM cs.AI
\\
  Peptides offer great biomedical potential and serve as promising drug
candidates. Currently, the majority of approved peptide drugs are directly
derived from well-explored natural human peptides. It is quite necessary to
utilize advanced deep learning techniques to identify novel peptide drugs in
the vast, unexplored biochemical space. Despite various in silico methods
having been developed to accelerate peptide early drug discovery, existing
models face challenges of overfitting and lacking generalizability due to the
limited size, imbalanced distribution and inconsistent quality of experimental
data. In this study, we propose PepGB, a deep learning framework to facilitate
peptide early drug discovery by predicting peptide-protein interactions
(PepPIs). Employing graph neural networks, PepGB incorporates a fine-grained
perturbation module and a dual-view objective with contrastive learning-based
peptide pre-trained representation to predict PepPIs. Through rigorous
evaluations, we demonstrated that PepGB greatly outperforms baselines and can
accurately identify PepPIs for novel targets and peptide hits, thereby
contributing to the target identification and hit discovery processes. Next, we
derive an extended version, diPepGB, to tackle the bottleneck of modeling
highly imbalanced data prevalent in lead generation and optimization processes.
Utilizing directed edges to represent relative binding strength between two
peptide nodes, diPepGB achieves superior performance in real-world assays. In
summary, our proposed frameworks can serve as potent tools to facilitate
peptide early drug discovery.
\\ ( https://arxiv.org/abs/2401.14665 ,  1701kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14707 (*cross-listing*)
Date: Fri, 26 Jan 2024 08:38:57 GMT   (2100kb,D)

Title: Mitigating Feature Gap for Adversarial Robustness by Feature
  Disentanglement
Authors: Nuoyan Zhou, Dawei Zhou, Decheng Liu, Xinbo Gao, Nannan Wang
Categories: cs.CV cs.AI cs.LG
Comments: 8 pages, 6 figures
\\
  Deep neural networks are vulnerable to adversarial samples. Adversarial
fine-tuning methods aim to enhance adversarial robustness through fine-tuning
the naturally pre-trained model in an adversarial training manner. However, we
identify that some latent features of adversarial samples are confused by
adversarial perturbation and lead to an unexpectedly increasing gap between
features in the last hidden layer of natural and adversarial samples. To
address this issue, we propose a disentanglement-based approach to explicitly
model and further remove the latent features that cause the feature gap.
Specifically, we introduce a feature disentangler to separate out the latent
features from the features of the adversarial samples, thereby boosting
robustness by eliminating the latent features. Besides, we align features in
the pre-trained model with features of adversarial samples in the fine-tuned
model, to further benefit from the features from natural samples without
confusion. Empirical evaluations on three benchmark datasets demonstrate that
our approach surpasses existing adversarial fine-tuning methods and adversarial
training baselines.
\\ ( https://arxiv.org/abs/2401.14707 ,  2100kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14749 (*cross-listing*)
Date: Fri, 26 Jan 2024 10:14:10 GMT   (3853kb,D)

Title: Topology-Aware Exploration of Energy-Based Models Equilibrium: Toric
  QC-LDPC Codes and Hyperbolic MET QC-LDPC Codes
Authors: Vasiliy Usatyuk, Denis Sapozhnikov, Sergey Egorov
Categories: cs.IT cs.AI cs.CV cs.LG cs.SY eess.SY math.IT
Comments: 16 pages, 29 figures. arXiv admin note: text overlap with
  arXiv:2307.15778
\\
  This paper presents a method for achieving equilibrium in the ISING
Hamiltonian when confronted with unevenly distributed charges on an irregular
grid. Employing (Multi-Edge) QC-LDPC codes and the Boltzmann machine, our
approach involves dimensionally expanding the system, substituting charges with
circulants, and representing distances through circulant shifts. This results
in a systematic mapping of the charge system onto a space, transforming the
irregular grid into a uniform configuration, applicable to Torical and Circular
Hyperboloid Topologies. The paper covers fundamental definitions and notations
related to QC-LDPC Codes, Multi-Edge QC-LDPC codes, and the Boltzmann machine.
It explores the marginalization problem in code on the graph probabilistic
models for evaluating the partition function, encompassing exact and
approximate estimation techniques. Rigorous proof is provided for the
attainability of equilibrium states for the Boltzmann machine under Torical and
Circular Hyperboloid, paving the way for the application of our methodology.
Practical applications of our approach are investigated in Finite Geometry
QC-LDPC Codes, specifically in Material Science. The paper further explores its
effectiveness in the realm of Natural Language Processing Transformer Deep
Neural Networks, examining Generalized Repeat Accumulate Codes,
Spatially-Coupled and Cage-Graph QC-LDPC Codes. The versatile and impactful
nature of our topology-aware hardware-efficient quasi-cycle codes equilibrium
method is showcased across diverse scientific domains without the use of
specific section delineations.
\\ ( https://arxiv.org/abs/2401.14749 ,  3853kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14831 (*cross-listing*)
Date: Fri, 26 Jan 2024 12:59:26 GMT   (1876kb,D)

Title: The Machine Vision Iceberg Explained: Advancing Dynamic Testing by
  Considering Holistic Environmental Circumstances
Authors: Hubert Padusinski, Thilo Braun, Christian Steinhauser, Lennart Ries,
  Eric Sax
Categories: cs.RO cs.AI cs.CV cs.SE eess.IV
Comments: Submitted at IEEE IV 2024
\\
  Are we heading for an iceberg with the current testing of machine vision?
This work delves into the landscape of Machine Vision (MV) testing, which is
heavily required in Highly Automated Driving (HAD) systems. Utilizing the
metaphorical notion of navigating towards an iceberg, we discuss the potential
shortcomings concealed within current testing strategies. We emphasize the
urgent need for a deeper understanding of how to deal with the opaque functions
of MV in development processes. As overlooked considerations can cost lives.
Our main contribution is the hierarchical level model, which we call
Granularity Grades. The model encourages a refined exploration of the
multi-scaled depths of understanding about the circumstances of environments in
which MV is intended to operate. This model aims to provide a holistic overview
of all entities that may impact MV functions, ranging from relations of
individual entities like object attributes to entire environmental scenes. The
application of our model delivers a structured exploration of entities in a
specific domain, their relationships and assigning results of a MV-under-test
to construct an entity-relationship graph. Through clustering patterns of
relations in the graph general MV deficits are arguable. In Summary, our work
contributes to a more nuanced and systematized identification of deficits of a
MV test object in correlation to holistic circumstances in HAD operating
domains.
\\ ( https://arxiv.org/abs/2401.14831 ,  1876kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14856 (*cross-listing*)
Date: Fri, 26 Jan 2024 13:36:12 GMT   (1044kb,D)

Title: Memory-Inspired Temporal Prompt Interaction for Text-Image
  Classification
Authors: Xinyao Yu, Hao Sun, Ziwei Niu, Rui Qin, Zhenjia Bai, Yen-Wei Chen,
  Lanfen Lin
Categories: cs.CV cs.AI
\\
  In recent years, large-scale pre-trained multimodal models (LMM) generally
emerge to integrate the vision and language modalities, achieving considerable
success in various natural language processing and computer vision tasks. The
growing size of LMMs, however, results in a significant computational cost for
fine-tuning these models for downstream tasks. Hence, prompt-based interaction
strategy is studied to align modalities more efficiently. In this contex, we
propose a novel prompt-based multimodal interaction strategy inspired by human
memory strategy, namely Memory-Inspired Temporal Prompt Interaction (MITP). Our
proposed method involves in two stages as in human memory strategy: the
acquiring stage, and the consolidation and activation stage. We utilize
temporal prompts on intermediate layers to imitate the acquiring stage,
leverage similarity-based prompt interaction to imitate memory consolidation,
and employ prompt generation strategy to imitate memory activation. The main
strength of our paper is that we interact the prompt vectors on intermediate
layers to leverage sufficient information exchange between modalities, with
compressed trainable parameters and memory usage. We achieve competitive
results on several datasets with relatively small memory usage and 2.0M of
trainable parameters (about 1% of the pre-trained foundation model).
\\ ( https://arxiv.org/abs/2401.14856 ,  1044kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14915 (*cross-listing*)
Date: Fri, 26 Jan 2024 14:49:29 GMT   (10010kb,D)

Title: Charting the Future of AI in Project-Based Learning: A Co-Design
  Exploration with Students
Authors: Chengbo Zheng, Kangyu Yuan, Bingcan Guo, Reza Hadi Mogavi, Zhenhui
  Peng, Shuai Ma, Xiaojuan Ma
Categories: cs.HC cs.AI
Comments: Conditionally accepted by CHI '24
\\
  The increasing use of Artificial Intelligence (AI) by students in learning
presents new challenges for assessing their learning outcomes in project-based
learning (PBL). This paper introduces a co-design study to explore the
potential of students' AI usage data as a novel material for PBL assessment. We
conducted workshops with 18 college students, encouraging them to speculate an
alternative world where they could freely employ AI in PBL while needing to
report this process to assess their skills and contributions. Our workshops
yielded various scenarios of students' use of AI in PBL and ways of analyzing
these uses grounded by students' vision of education goal transformation. We
also found students with different attitudes toward AI exhibited distinct
preferences in how to analyze and understand the use of AI. Based on these
findings, we discuss future research opportunities on student-AI interactions
and understanding AI-enhanced learning.
\\ ( https://arxiv.org/abs/2401.14915 ,  10010kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14968 (*cross-listing*)
Date: Fri, 26 Jan 2024 16:01:09 GMT   (5165kb)

Title: Atmosphere: Context and situational-aware collaborative IoT architecture
  for edge-fog-cloud computing
Authors: Guadalupe Ortiz, Meftah Zouai, Okba Kazar, Alfonso Garcia-de-Prado,
  Juan Boubeta-Puig
Categories: cs.DC cs.AI cs.SE
Journal-ref: Comput. Stand. Interfaces 79: 103550 (2022)
DOI: 10.1016/j.csi.2021.103550
\\
  The Internet of Things (IoT) has grown significantly in popularity,
accompanied by increased capacity and lower cost of communications, and
overwhelming development of technologies. At the same time, big data and
real-time data analysis have taken on great importance and have been
accompanied by unprecedented interest in sharing data among citizens, public
administrations and other organisms, giving rise to what is known as the
Collaborative Internet of Things. This growth in data and infrastructure must
be accompanied by a software architecture that allows its exploitation.
Although there are various proposals focused on the exploitation of the IoT at
edge, fog and/or cloud levels, it is not easy to find a software solution that
exploits the three tiers together, taking maximum advantage not only of the
analysis of contextual and situational data at each tier, but also of two-way
communications between adjacent ones. In this paper, we propose an architecture
that solves these deficiencies by proposing novel technologies which are
appropriate for managing the resources of each tier: edge, fog and cloud. In
addition, the fact that two-way communications along the three tiers of the
architecture is allowed considerably enriches the contextual and situational
information in each layer, and substantially assists decision making in real
time. The paper illustrates the proposed software architecture through a case
study of respiratory disease surveillance in hospitals. As a result, the
proposed architecture permits efficient communications between the different
tiers responding to the needs of these types of IoT scenarios.
\\ ( https://arxiv.org/abs/2401.14968 ,  5165kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15075 (*cross-listing*)
Date: Fri, 26 Jan 2024 18:57:54 GMT   (38106kb,D)

Title: Annotated Hands for Generative Models
Authors: Yue Yang and Atith N Gandhi and Greg Turk
Categories: cs.CV cs.AI cs.GR
\\
  Generative models such as GANs and diffusion models have demonstrated
impressive image generation capabilities. Despite these successes, these
systems are surprisingly poor at creating images with hands. We propose a novel
training framework for generative models that substantially improves the
ability of such systems to create hand images. Our approach is to augment the
training images with three additional channels that provide annotations to
hands in the image. These annotations provide additional structure that coax
the generative model to produce higher quality hand images. We demonstrate this
approach on two different generative models: a generative adversarial network
and a diffusion model. We demonstrate our method both on a new synthetic
dataset of hand images and also on real photographs that contain hands. We
measure the improved quality of the generated hands through higher confidence
in finger joint identification using an off-the-shelf hand detector.
\\ ( https://arxiv.org/abs/2401.15075 ,  38106kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14664 (*cross-listing*)
Date: Fri, 26 Jan 2024 06:08:47 GMT   (347kb,D)

Title: UNIT-DSR: Dysarthric Speech Reconstruction System Using Speech Unit
  Normalization
Authors: Yuejiao Wang, Xixin Wu, Disong Wang, Lingwei Meng, Helen Meng
Categories: cs.SD cs.CL eess.AS
Comments: Accepted to ICASSP 2024
\\
  Dysarthric speech reconstruction (DSR) systems aim to automatically convert
dysarthric speech into normal-sounding speech. The technology eases
communication with speakers affected by the neuromotor disorder and enhances
their social inclusion. NED-based (Neural Encoder-Decoder) systems have
significantly improved the intelligibility of the reconstructed speech as
compared with GAN-based (Generative Adversarial Network) approaches, but the
approach is still limited by training inefficiency caused by the cascaded
pipeline and auxiliary tasks of the content encoder, which may in turn affect
the quality of reconstruction. Inspired by self-supervised speech
representation learning and discrete speech units, we propose a Unit-DSR
system, which harnesses the powerful domain-adaptation capacity of HuBERT for
training efficiency improvement and utilizes speech units to constrain the
dysarthric content restoration in a discrete linguistic space. Compared with
NED approaches, the Unit-DSR system only consists of a speech unit normalizer
and a Unit HiFi-GAN vocoder, which is considerably simpler without cascaded
sub-modules or auxiliary tasks. Results on the UASpeech corpus indicate that
Unit-DSR outperforms competitive baselines in terms of content restoration,
reaching a 28.2% relative average word error rate reduction when compared to
original dysarthric speech, and shows robustness against speed perturbation and
noise.
\\ ( https://arxiv.org/abs/2401.14664 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14887 (*cross-listing*)
Date: Fri, 26 Jan 2024 14:14:59 GMT   (98kb,D)

Title: The Power of Noise: Redefining Retrieval for RAG Systems
Authors: Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone
  Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio
  Silvestri
Categories: cs.IR cs.CL
\\
  Retrieval-Augmented Generation (RAG) systems represent a significant
advancement over traditional Large Language Models (LLMs). RAG systems enhance
their generation ability by incorporating external data retrieved through an
Information Retrieval (IR) phase, overcoming the limitations of standard LLMs,
which are restricted to their pre-trained knowledge and limited context window.
Most research in this area has predominantly concentrated on the generative
aspect of LLMs within RAG systems. Our study fills this gap by thoroughly and
critically analyzing the influence of IR components on RAG systems. This paper
analyzes which characteristics a retriever should possess for an effective
RAG's prompt formulation, focusing on the type of documents that should be
retrieved. We evaluate various elements, such as the relevance of the documents
to the prompt, their position, and the number included in the context. Our
findings reveal, among other insights, that including irrelevant documents can
unexpectedly enhance performance by more than 30% in accuracy, contradicting
our initial assumption of diminished quality. These findings call for
developing specialized approaches tailored to the specific demands of
integrating retrieval with language generation models and pave the way for
future research. These results underscore the need for developing specialized
strategies to integrate retrieval with language generation models, thereby
laying the groundwork for future research in this field.
\\ ( https://arxiv.org/abs/2401.14887 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14890 (*cross-listing*)
Date: Fri, 26 Jan 2024 14:15:20 GMT   (201kb)

Title: Comparison of parameters of vowel sounds of russian and english
  languages
Authors: V.I. Fedoseev, A.A. Konev, A. Yu. Yakimuk
Categories: cs.SD cs.CL eess.AS
Comments: 7 pages, 1 figures, 3 tables
MSC-class: 68T10
ACM-class: H.2.8
\\
  In multilingual speech recognition systems, a situation can often arise when
the language is not known in advance, but the signal has already been received
and is being processed. For such cases, some generalized model is needed that
will be able to respond to phonetic differences and, depending on them,
correctly recog-nize speech in the desired language. To build such a model, it
is necessary to set the values of phonetic parameters, and then compare similar
sounds, establishing significant differences.
\\ ( https://arxiv.org/abs/2401.14890 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14975 (*cross-listing*)
Date: Fri, 26 Jan 2024 16:07:42 GMT   (3765kb,D)

Title: Embedding-based search in JetBrains IDEs
Authors: Evgeny Abramov and Nikolai Palchikov
Categories: cs.SE cs.CL cs.IR cs.LG
\\
  Most modern Integrated Development Environments (IDEs) and code editors have
a feature to search across available functionality and items in an open
project. In JetBrains IDEs, this feature is called Search Everywhere: it allows
users to search for files, actions, classes, symbols, settings, and anything
from VCS history from a single entry point. However, it works with the
candidates obtained by algorithms that don't account for semantics, e.g.,
synonyms, complex word permutations, part of the speech modifications, and
typos. In this work, we describe the machine learning approach we implemented
to improve the discoverability of search items. We also share the obstacles
encountered during this process and how we overcame them.
\\ ( https://arxiv.org/abs/2401.14975 ,  3765kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15055 (*cross-listing*)
Date: Fri, 26 Jan 2024 18:33:57 GMT   (368kb)

Title: Deep learning-based approach for tomato classification in complex scenes
Authors: Mikael A. Mousse, Bethel C. A. R. K. Atohoun, Cina Motamed
Categories: cs.CV cs.CL
\\
  Tracking ripening tomatoes is time consuming and labor intensive. Artificial
intelligence technologies combined with those of computer vision can help users
optimize the process of monitoring the ripening status of plants. To this end,
we have proposed a tomato ripening monitoring approach based on deep learning
in complex scenes. The objective is to detect mature tomatoes and harvest them
in a timely manner. The proposed approach is declined in two parts. Firstly,
the images of the scene are transmitted to the pre-processing layer. This
process allows the detection of areas of interest (area of the image containing
tomatoes). Then, these images are used as input to the maturity detection
layer. This layer, based on a deep neural network learning algorithm,
classifies the tomato thumbnails provided to it in one of the following five
categories: green, brittle, pink, pale red, mature red. The experiments are
based on images collected from the internet gathered through searches using
tomato state across diverse languages including English, German, French, and
Spanish. The experimental results of the maturity detection layer on a dataset
composed of images of tomatoes taken under the extreme conditions, gave a good
classification rate.
\\ ( https://arxiv.org/abs/2401.15055 ,  368kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14416 (*cross-listing*)
Date: Mon, 22 Jan 2024 09:49:44 GMT   (3897kb,D)

Title: Acoustic characterization of speech rhythm: going beyond metrics with
  recurrent neural networks
Authors: Fran\c{c}ois Deloche, Laurent Bonnasse-Gahot, Judit Gervain
Categories: eess.AS cs.LG cs.SD
Comments: 15 pages, 7 figures
\\
  Languages have long been described according to their perceived rhythmic
attributes. The associated typologies are of interest in psycholinguistics as
they partly predict newborns' abilities to discriminate between languages and
provide insights into how adult listeners process non-native languages. Despite
the relative success of rhythm metrics in supporting the existence of
linguistic rhythmic classes, quantitative studies have yet to capture the full
complexity of temporal regularities associated with speech rhythm. We argue
that deep learning offers a powerful pattern-recognition approach to advance
the characterization of the acoustic bases of speech rhythm. To explore this
hypothesis, we trained a medium-sized recurrent neural network on a language
identification task over a large database of speech recordings in 21 languages.
The network had access to the amplitude envelopes and a variable identifying
the voiced segments, assuming that this signal would poorly convey phonetic
information but preserve prosodic features. The network was able to identify
the language of 10-second recordings in 40% of the cases, and the language was
in the top-3 guesses in two-thirds of the cases. Visualization methods show
that representations built from the network activations are consistent with
speech rhythm typologies, although the resulting maps are more complex than two
separated clusters between stress and syllable-timed languages. We further
analyzed the model by identifying correlations between network activations and
known speech rhythm metrics. The findings illustrate the potential of deep
learning tools to advance our understanding of speech rhythm through the
identification and exploration of linguistically relevant acoustic feature
spaces.
\\ ( https://arxiv.org/abs/2401.14416 ,  3897kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14423 (*cross-listing*)
Date: Wed, 24 Jan 2024 06:20:18 GMT   (9279kb,D)

Title: Prompt Design and Engineering: Introduction and Advanced Methods
Authors: Xavier Amatriain
Categories: cs.SE cs.LG
\\
  Prompt design and engineering has become an important discipline in just the
past few months. In this paper, we provide an introduction to the main concepts
as well as review basic and more advanced approaches to prompt design and
engineering.
\\ ( https://arxiv.org/abs/2401.14423 ,  9279kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14427 (*cross-listing*)
Date: Wed, 24 Jan 2024 09:27:51 GMT   (881kb,D)

Title: Beimingwu: A Learnware Dock System
Authors: Zhi-Hao Tan, Jian-Dong Liu, Xiao-Dong Bi, Peng Tan, Qin-Cheng Zheng,
  Hai-Tian Liu, Yi Xie, Xiao-Chuan Zou, Yang Yu, Zhi-Hua Zhou
Categories: cs.SE cs.CR cs.LG
\\
  The learnware paradigm proposed by Zhou [2016] aims to enable users to reuse
numerous existing well-trained models instead of building machine learning
models from scratch, with the hope of solving new user tasks even beyond
models' original purposes. In this paradigm, developers worldwide can submit
their high-performing models spontaneously to the learnware dock system
(formerly known as learnware market) without revealing their training data.
Once the dock system accepts the model, it assigns a specification and
accommodates the model. This specification allows the model to be adequately
identified and assembled to reuse according to future users' needs, even if
they have no prior knowledge of the model. This paradigm greatly differs from
the current big model direction and it is expected that a learnware dock system
housing millions or more high-performing models could offer excellent
capabilities for both planned tasks where big models are applicable; and
unplanned, specialized, data-sensitive scenarios where big models are not
present or applicable.
  This paper describes Beimingwu, the first open-source learnware dock system
providing foundational support for future research of learnware paradigm.The
system significantly streamlines the model development for new user tasks,
thanks to its integrated architecture and engine design, extensive engineering
implementations and optimizations, and the integration of various algorithms
for learnware identification and reuse. Notably, this is possible even for
users with limited data and minimal expertise in machine learning, without
compromising the raw data's security. Beimingwu supports the entire process of
learnware paradigm. The system lays the foundation for future research in
learnware-related algorithms and systems, and prepares the ground for hosting a
vast array of learnwares and establishing a learnware ecosystem.
\\ ( https://arxiv.org/abs/2401.14427 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14432 (*cross-listing*)
Date: Thu, 25 Jan 2024 02:31:52 GMT   (294kb,D)

Title: A2C: A Modular Multi-stage Collaborative Decision Framework for Human-AI
  Teams
Authors: Shahroz Tariq, Mohan Baruwal Chhetri, Surya Nepal, Cecile Paris
Categories: cs.HC cs.LG
\\
  This paper introduces A2C, a multi-stage collaborative decision framework
designed to enable robust decision-making within human-AI teams. Drawing
inspiration from concepts such as rejection learning and learning to defer, A2C
incorporates AI systems trained to recognise uncertainty in their decisions and
defer to human experts when needed. Moreover, A2C caters to scenarios where
even human experts encounter limitations, such as in incident detection and
response in cyber Security Operations Centres (SOC). In such scenarios, A2C
facilitates collaborative explorations, enabling collective resolution of
complex challenges. With support for three distinct decision-making modes in
human-AI teams: Automated, Augmented, and Collaborative, A2C offers a flexible
platform for developing effective strategies for human-AI collaboration. By
harnessing the strengths of both humans and AI, it significantly improves the
efficiency and effectiveness of complex decision-making in dynamic and evolving
environments. To validate A2C's capabilities, we conducted extensive simulative
experiments using benchmark datasets. The results clearly demonstrate that all
three modes of decision-making can be effectively supported by A2C. Most
notably, collaborative exploration by (simulated) human experts and AI achieves
superior performance compared to AI in isolation, underscoring the framework's
potential to enhance decision-making within human-AI teams.
\\ ( https://arxiv.org/abs/2401.14432 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14442 (*cross-listing*)
Date: Thu, 25 Jan 2024 16:04:17 GMT   (48761kb,D)

Title: Improving Antibody Humanness Prediction using Patent Data
Authors: Talip Ucar, Aubin Ramon, Dino Oglic, Rebecca Croasdale-Wood, Tom
  Diethe, Pietro Sormanni
Categories: q-bio.QM cs.LG stat.ML
Comments: 13 pages, 6 figures, Code: https://github.com/AstraZeneca/SelfPAD
\\
  We investigate the potential of patent data for improving the antibody
humanness prediction using a multi-stage, multi-loss training process.
Humanness serves as a proxy for the immunogenic response to antibody
therapeutics, one of the major causes of attrition in drug discovery and a
challenging obstacle for their use in clinical settings. We pose the initial
learning stage as a weakly-supervised contrastive-learning problem, where each
antibody sequence is associated with possibly multiple identifiers of function
and the objective is to learn an encoder that groups them according to their
patented properties. We then freeze a part of the contrastive encoder and
continue training it on the patent data using the cross-entropy loss to predict
the humanness score of a given antibody sequence. We illustrate the utility of
the patent data and our approach by performing inference on three different
immunogenicity datasets, unseen during training. Our empirical results
demonstrate that the learned model consistently outperforms the alternative
baselines and establishes new state-of-the-art on five out of six inference
tasks, irrespective of the used metric.
\\ ( https://arxiv.org/abs/2401.14442 ,  48761kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14486 (*cross-listing*)
Date: Thu, 25 Jan 2024 19:44:19 GMT   (5756kb,D)

Title: CloudTracks: A Dataset for Localizing Ship Tracks in Satellite Images of
  Clouds
Authors: Muhammad Ahmed Chaudhry, Lyna Kim, Jeremy Irvin, Yuzu Ido, Sonia Chu,
  Jared Thomas Isobe, Andrew Y. Ng, Duncan Watson-Parris
Categories: cs.CV cs.LG
Comments: 11 pages, 5 figures, submitted to Journal of Machine Learning
  Research
\\
  Clouds play a significant role in global temperature regulation through their
effect on planetary albedo. Anthropogenic emissions of aerosols can alter the
albedo of clouds, but the extent of this effect, and its consequent impact on
temperature change, remains uncertain. Human-induced clouds caused by ship
aerosol emissions, commonly referred to as ship tracks, provide visible
manifestations of this effect distinct from adjacent cloud regions and
therefore serve as a useful sandbox to study human-induced clouds. However, the
lack of large-scale ship track data makes it difficult to deduce their general
effects on cloud formation. Towards developing automated approaches to localize
ship tracks at scale, we present CloudTracks, a dataset containing 3,560
satellite images labeled with more than 12,000 ship track instance annotations.
We train semantic segmentation and instance segmentation model baselines on our
dataset and find that our best model substantially outperforms previous
state-of-the-art for ship track localization (61.29 vs. 48.65 IoU). We also
find that the best instance segmentation model is able to identify the number
of ship tracks in each image more accurately than the previous state-of-the-art
(1.64 vs. 4.99 MAE). However, we identify cases where the best model struggles
to accurately localize and count ship tracks, so we believe CloudTracks will
stimulate novel machine learning approaches to better detect elongated and
overlapping features in satellite images. We release our dataset openly at
{zenodo.org/records/10042922}.
\\ ( https://arxiv.org/abs/2401.14486 ,  5756kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14497 (*cross-listing*)
Date: Thu, 25 Jan 2024 20:29:01 GMT   (20639kb,D)

Title: Investigating the Quality of DermaMNIST and Fitzpatrick17k
  Dermatological Image Datasets
Authors: Kumar Abhishek, Aditi Jain, Ghassan Hamarneh
Categories: cs.CV cs.LG
Comments: 36 pages, 8 figures, 3 tables
\\
  The remarkable progress of deep learning in dermatological tasks has brought
us closer to achieving diagnostic accuracies comparable to those of human
experts. However, while large datasets play a crucial role in the development
of reliable deep neural network models, the quality of data therein and their
correct usage are of paramount importance. Several factors can impact data
quality, such as the presence of duplicates, data leakage across train-test
partitions, mislabeled images, and the absence of a well-defined test
partition. In this paper, we conduct meticulous analyses of two popular
dermatological image datasets: DermaMNIST and Fitzpatrick17k, uncovering these
data quality issues, measure the effects of these problems on the benchmark
results, and propose corrections to the datasets. Besides ensuring the
reproducibility of our analysis, by making our analysis pipeline and the
accompanying code publicly available, we aim to encourage similar explorations
and to facilitate the identification and addressing of potential data quality
issues in other large datasets.
\\ ( https://arxiv.org/abs/2401.14497 ,  20639kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14502 (*cross-listing*)
Date: Thu, 25 Jan 2024 20:39:27 GMT   (9486kb,D)

Title: MResT: Multi-Resolution Sensing for Real-Time Control with
  Vision-Language Models
Authors: Saumya Saxena, Mohit Sharma, Oliver Kroemer
Categories: cs.RO cs.CV cs.LG
Comments: CoRL'23, Project website:
  http://tinyurl.com/multi-res-realtime-control
\\
  Leveraging sensing modalities across diverse spatial and temporal resolutions
can improve performance of robotic manipulation tasks. Multi-spatial resolution
sensing provides hierarchical information captured at different spatial scales
and enables both coarse and precise motions. Simultaneously multi-temporal
resolution sensing enables the agent to exhibit high reactivity and real-time
control. In this work, we propose a framework, MResT (Multi-Resolution
Transformer), for learning generalizable language-conditioned multi-task
policies that utilize sensing at different spatial and temporal resolutions
using networks of varying capacities to effectively perform real time control
of precise and reactive tasks. We leverage off-the-shelf pretrained
vision-language models to operate on low-frequency global features along with
small non-pretrained models to adapt to high frequency local feedback. Through
extensive experiments in 3 domains (coarse, precise and dynamic manipulation
tasks), we show that our approach significantly improves (2X on average) over
recent multi-task baselines. Further, our approach generalizes well to visual
and geometric variations in target objects and to varying interaction forces.
\\ ( https://arxiv.org/abs/2401.14502 ,  9486kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14512 (*cross-listing*)
Date: Thu, 25 Jan 2024 21:11:35 GMT   (1801kb,D)

Title: Who Are We Missing? A Principled Approach to Characterizing the
  Underrepresented Population
Authors: Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph
Categories: stat.ME cs.CY cs.LG stat.AP
\\
  Randomized controlled trials (RCTs) serve as the cornerstone for
understanding causal effects, yet extending inferences to target populations
presents challenges due to effect heterogeneity and underrepresentation. Our
paper addresses the critical issue of identifying and characterizing
underrepresented subgroups in RCTs, proposing a novel framework for refining
target populations to improve generalizability. We introduce an
optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to
characterize underrepresented groups. ROOT optimizes the target subpopulation
distribution by minimizing the variance of the target average treatment effect
estimate, ensuring more precise treatment effect estimations. Notably, ROOT
generates interpretable characteristics of the underrepresented population,
aiding researchers in effective communication. Our approach demonstrates
improved precision and interpretability compared to alternatives, as
illustrated with synthetic data experiments. We apply our methodology to extend
inferences from the Starting Treatment with Agonist Replacement Therapies
(START) trial -- investigating the effectiveness of medication for opioid use
disorder -- to the real-world population represented by the Treatment Episode
Dataset: Admissions (TEDS-A). By refining target populations using ROOT, our
framework offers a systematic approach to enhance decision-making accuracy and
inform future trials in diverse populations.
\\ ( https://arxiv.org/abs/2401.14512 ,  1801kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14534 (*cross-listing*)
Date: Thu, 25 Jan 2024 21:59:52 GMT   (109kb,D)

Title: Meta-Learning Linear Quadratic Regulators: A Policy Gradient MAML
  Approach for the Model-free LQR
Authors: Leonardo F. Toso, Donglin Zhan, James Anderson, and Han Wang
Categories: math.OC cs.LG
\\
  We investigate the problem of learning Linear Quadratic Regulators (LQR) in a
multi-task, heterogeneous, and model-free setting. We characterize the
stability and personalization guarantees of a Policy Gradient-based (PG)
Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) approach for the LQR
problem under different task-heterogeneity settings. We show that the MAML-LQR
approach produces a stabilizing controller close to each task-specific optimal
controller up to a task-heterogeneity bias for both model-based and model-free
settings. Moreover, in the model-based setting, we show that this controller is
achieved with a linear convergence rate, which improves upon sub-linear rates
presented in existing MAML-LQR work. In contrast to existing MAML-LQR results,
our theoretical guarantees demonstrate that the learned controller can
efficiently adapt to unseen LQR tasks.
\\ ( https://arxiv.org/abs/2401.14534 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14555 (*cross-listing*)
Date: Thu, 25 Jan 2024 22:50:39 GMT   (2034kb,D)

Title: Revisiting Active Learning in the Era of Vision Foundation Models
Authors: Sanket Rajan Gupte, Josiah Aklilu, Jeffrey J. Nirschl, Serena
  Yeung-Levy
Categories: cs.CV cs.LG
\\
  Foundation vision or vision-language models are trained on large unlabeled or
noisy data and learn robust representations that can achieve impressive zero-
or few-shot performance on diverse tasks. Given these properties, they are a
natural fit for active learning (AL), which aims to maximize labeling
efficiency, but the full potential of foundation models has not been explored
in the context of AL, specifically in the low-budget regime. In this work, we
evaluate how foundation models influence three critical components of effective
AL, namely, 1) initial labeled pool selection, 2) ensuring diverse sampling,
and 3) the trade-off between representative and uncertainty sampling. We
systematically study how the robust representations of foundation models
(DINOv2, OpenCLIP) challenge existing findings in active learning. Our
observations inform the principled construction of a new simple and elegant AL
strategy that balances uncertainty estimated via dropout with sample diversity.
We extensively test our strategy on many challenging image classification
benchmarks, including natural images as well as out-of-domain biomedical images
that are relatively understudied in the AL literature. Source code will be made
available.
\\ ( https://arxiv.org/abs/2401.14555 ,  2034kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14577 (*cross-listing*)
Date: Fri, 26 Jan 2024 00:32:31 GMT   (4536kb,D)

Title: PrivStream: An Algorithm for Streaming Differentially Private Data
Authors: Girish Kumar, Thomas Strohmer, and Roman Vershynin
Categories: cs.DB cs.IT cs.LG math.IT math.ST stat.TH
\\
  Much of the research in differential privacy has focused on offline
applications with the assumption that all data is available at once. When these
algorithms are applied in practice to streams where data is collected over
time, this either violates the privacy guarantees or results in poor utility.
We derive an algorithm for differentially private synthetic streaming data
generation, especially curated towards spatial datasets. Furthermore, we
provide a general framework for online selective counting among a collection of
queries which forms a basis for many tasks such as query answering and
synthetic data generation. The utility of our algorithm is verified on both
real-world and simulated datasets.
\\ ( https://arxiv.org/abs/2401.14577 ,  4536kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14628 (*cross-listing*)
Date: Fri, 26 Jan 2024 03:47:18 GMT   (19666kb,D)

Title: Inferring Data Preconditions from Deep Learning Models for Trustworthy
  Prediction in Deployment
Authors: Shibbir Ahmed, Hongyang Gao, Hridesh Rajan
Categories: cs.SE cs.LG
Comments: Accepted for publication at the 46th International Conference on
  Software Engineering (ICSE 2024)
\\
  Deep learning models are trained with certain assumptions about the data
during the development stage and then used for prediction in the deployment
stage. It is important to reason about the trustworthiness of the model's
predictions with unseen data during deployment. Existing methods for specifying
and verifying traditional software are insufficient for this task, as they
cannot handle the complexity of DNN model architecture and expected outcomes.
In this work, we propose a novel technique that uses rules derived from neural
network computations to infer data preconditions for a DNN model to determine
the trustworthiness of its predictions. Our approach, DeepInfer involves
introducing a novel abstraction for a trained DNN model that enables weakest
precondition reasoning using Dijkstra's Predicate Transformer Semantics. By
deriving rules over the inductive type of neural network abstract
representation, we can overcome the matrix dimensionality issues that arise
from the backward non-linear computation from the output layer to the input
layer. We utilize the weakest precondition computation using rules of each kind
of activation function to compute layer-wise precondition from the given
postcondition on the final output of a deep neural network. We extensively
evaluated DeepInfer on 29 real-world DNN models using four different datasets
collected from five different sources and demonstrated the utility,
effectiveness, and performance improvement over closely related work. DeepInfer
efficiently detects correct and incorrect predictions of high-accuracy models
with high recall (0.98) and high F-1 score (0.84) and has significantly
improved over prior technique, SelfChecker. The average runtime overhead of
DeepInfer is low, 0.22 sec for all unseen datasets. We also compared runtime
overhead using the same hardware settings and found that DeepInfer is 3.27
times faster than SelfChecker.
\\ ( https://arxiv.org/abs/2401.14628 ,  19666kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14657 (*cross-listing*)
Date: Fri, 26 Jan 2024 05:35:50 GMT   (1843kb,D)

Title: Validating Climate Models with Spherical Convolutional Wasserstein
  Distance
Authors: Robert C. Garrett, Trevor Harris, Bo Li, Zhuo Wang
Categories: physics.ao-ph cs.LG stat.AP stat.ML
\\
  The validation of global climate models is crucial to ensure the accuracy and
efficacy of model output. We introduce the spherical convolutional Wasserstein
distance to more comprehensively measure differences between climate models and
reanalysis data. This new similarity measure accounts for spatial variability
using convolutional projections and quantifies local differences in the
distribution of climate variables. We apply this method to evaluate the
historical model outputs of the Coupled Model Intercomparison Project (CMIP)
members by comparing them to observational and reanalysis data products.
Additionally, we investigate the progression from CMIP phase 5 to phase 6 and
find modest improvements in the phase 6 models regarding their ability to
produce realistic climatologies.
\\ ( https://arxiv.org/abs/2401.14657 ,  1843kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14661 (*cross-listing*)
Date: Fri, 26 Jan 2024 05:50:58 GMT   (35971kb,D)

Title: From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection
  with Super Resolution
Authors: Ragib Amin Nihal, Benjamin Yen, Katsutoshi Itoyama, Kazuhiro Nakadai
Categories: cs.CV cs.LG
\\
  The demand for accurate object detection in aerial imagery has surged with
the widespread use of drones and satellite technology. Traditional object
detection models, trained on datasets biased towards large objects, struggle to
perform optimally in aerial scenarios where small, densely clustered objects
are prevalent. To address this challenge, we present an innovative approach
that combines super-resolution and an adapted lightweight YOLOv5 architecture.
We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and
NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5
architecture features Transformer encoder blocks, allowing the model to capture
global context and context information, leading to improved detection results,
especially in high-density, occluded conditions. This lightweight model not
only delivers improved accuracy but also ensures efficient resource
utilization, making it well-suited for real-time applications. Our experimental
results demonstrate the model's superior performance in detecting small and
densely clustered objects, underlining the significance of dataset choice and
architectural adaptation for this specific task. In particular, the method
achieves 52.5% mAP on VisDrone, exceeding top prior works. This approach
promises to significantly advance object detection in aerial imagery,
contributing to more accurate and reliable results in a variety of real-world
applications.
\\ ( https://arxiv.org/abs/2401.14661 ,  35971kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14722 (*cross-listing*)
Date: Fri, 26 Jan 2024 09:11:42 GMT   (110kb,D)

Title: A Nonparametric Bayes Approach to Online Activity Prediction
Authors: Mario Beraha, Lorenzo Masoero, Stefano Favaro, Thomas S. Richardson
Categories: stat.ME cs.LG stat.AP stat.ML
\\
  Accurately predicting the onset of specific activities within defined
timeframes holds significant importance in several applied contexts. In
particular, accurate prediction of the number of future users that will be
exposed to an intervention is an important piece of information for
experimenters running online experiments (A/B tests). In this work, we propose
a novel approach to predict the number of users that will be active in a given
time period, as well as the temporal trajectory needed to attain a desired user
participation threshold. We model user activity using a Bayesian nonparametric
approach which allows us to capture the underlying heterogeneity in user
engagement. We derive closed-form expressions for the number of new users
expected in a given period, and a simple Monte Carlo algorithm targeting the
posterior distribution of the number of days needed to attain a desired number
of users; the latter is important for experimental planning. We illustrate the
performance of our approach via several experiments on synthetic and real world
data, in which we show that our novel method outperforms existing competitors.
\\ ( https://arxiv.org/abs/2401.14722 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14792 (*cross-listing*)
Date: Fri, 26 Jan 2024 11:32:53 GMT   (406kb,D)

Title: Deep Variational Privacy Funnel: General Modeling with Applications in
  Face Recognition
Authors: Behrooz Razeghi, Parsa Rahimi, S\'ebastien Marcel
Categories: cs.CV cs.IT cs.LG math.IT
Comments: IEEE ICASSP 2024
\\
  In this study, we harness the information-theoretic Privacy Funnel (PF) model
to develop a method for privacy-preserving representation learning using an
end-to-end training framework. We rigorously address the trade-off between
obfuscation and utility. Both are quantified through the logarithmic loss, a
measure also recognized as self-information loss. This exploration deepens the
interplay between information-theoretic privacy and representation learning,
offering substantive insights into data protection mechanisms for both
discriminative and generative models. Importantly, we apply our model to
state-of-the-art face recognition systems. The model demonstrates adaptability
across diverse inputs, from raw facial images to both derived or refined
embeddings, and is competent in tasks such as classification, reconstruction,
and generation.
\\ ( https://arxiv.org/abs/2401.14792 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14810 (*cross-listing*)
Date: Fri, 26 Jan 2024 12:16:55 GMT   (17kb)

Title: Cyclic Group Projection for Enumerating Quasi-Cyclic Codes Trapping Sets
Authors: Vasiliy Usatyuk, Yury Kuznetsov, Sergey Egorov
Categories: cs.IT cs.LG math.IT
Comments: 7 pages, 3 tables
\\
  This paper introduces a novel approach to enumerate and assess Trapping sets
in quasi-cyclic codes, those with circulant sizes that are non-prime numbers.
Leveraging the quasi-cyclic properties, the method employs a tabular technique
to streamline the importance sampling step for estimating the pseudo-codeword
weight of Trapping sets. The presented methodology draws on the mathematical
framework established in the provided theorem, which elucidates the behavior of
projection and lifting transformations on pseudo-codewords
\\ ( https://arxiv.org/abs/2401.14810 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14819 (*cross-listing*)
Date: Fri, 26 Jan 2024 12:47:54 GMT   (2281kb,D)

Title: Endowing Protein Language Models with Structural Knowledge
Authors: Dexiong Chen, Philip Hartout, Paolo Pellizzoni, Carlos Oliver, Karsten
  Borgwardt
Categories: q-bio.QM cs.LG q-bio.BM
\\
  Understanding the relationships between protein sequence, structure and
function is a long-standing biological challenge with manifold implications
from drug design to our understanding of evolution. Recently, protein language
models have emerged as the preferred method for this challenge, thanks to their
ability to harness large sequence databases. Yet, their reliance on expansive
sequence data and parameter sets limits their flexibility and practicality in
real-world scenarios. Concurrently, the recent surge in computationally
predicted protein structures unlocks new opportunities in protein
representation learning. While promising, the computational burden carried by
such complex data still hinders widely-adopted practical applications. To
address these limitations, we introduce a novel framework that enhances protein
language models by integrating protein structural data. Drawing from recent
advances in graph transformers, our approach refines the self-attention
mechanisms of pretrained language transformers by integrating structural
information with structure extractor modules. This refined model, termed
Protein Structure Transformer (PST), is further pretrained on a small protein
structure database, using the same masked language modeling objective as
traditional protein language models. Empirical evaluations of PST demonstrate
its superior parameter efficiency relative to protein language models, despite
being pretrained on a dataset comprising only 542K structures. Notably, PST
consistently outperforms the state-of-the-art foundation model for protein
sequences, ESM-2, setting a new benchmark in protein function prediction. Our
findings underscore the potential of integrating structural information into
protein language models, paving the way for more effective and efficient
protein modeling Code and pretrained models are available at
https://github.com/BorgwardtLab/PST.
\\ ( https://arxiv.org/abs/2401.14819 ,  2281kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14845 (*cross-listing*)
Date: Fri, 26 Jan 2024 13:24:45 GMT   (743kb,D)

Title: Adaptive Point Transformer
Authors: Alessandro Baiocchi, Indro Spinelli, Alessandro Nicolosi, Simone
  Scardapane
Categories: cs.CV cs.LG
Comments: 26 pages, 8 figures, submitted to Neural Networs
\\
  The recent surge in 3D data acquisition has spurred the development of
geometric deep learning models for point cloud processing, boosted by the
remarkable success of transformers in natural language processing. While point
cloud transformers (PTs) have achieved impressive results recently, their
quadratic scaling with respect to the point cloud size poses a significant
scalability challenge for real-world applications. To address this issue, we
propose the Adaptive Point Cloud Transformer (AdaPT), a standard PT model
augmented by an adaptive token selection mechanism. AdaPT dynamically reduces
the number of tokens during inference, enabling efficient processing of large
point clouds. Furthermore, we introduce a budget mechanism to flexibly adjust
the computational cost of the model at inference time without the need for
retraining or fine-tuning separate models. Our extensive experimental
evaluation on point cloud classification tasks demonstrates that AdaPT
significantly reduces computational complexity while maintaining competitive
accuracy compared to standard PTs. The code for AdaPT is made publicly
available.
\\ ( https://arxiv.org/abs/2401.14845 ,  743kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14884 (*cross-listing*)
Date: Fri, 26 Jan 2024 14:08:43 GMT   (1335kb,D)

Title: P3LS: Partial Least Squares under Privacy Preservation
Authors: Du Nguyen Duy, Ramin Nikzad-Langerodi
Categories: stat.ML cs.CR cs.LG
Comments: 15 pages, 8 figures
\\
  Modern manufacturing value chains require intelligent orchestration of
processes across company borders in order to maximize profits while fostering
social and environmental sustainability. However, the implementation of
integrated, systems-level approaches for data-informed decision-making along
value chains is currently hampered by privacy concerns associated with
cross-organizational data exchange and integration. We here propose
Privacy-Preserving Partial Least Squares (P3LS) regression, a novel federated
learning technique that enables cross-organizational data integration and
process modeling with privacy guarantees. P3LS involves a singular value
decomposition (SVD) based PLS algorithm and employs removable, random masks
generated by a trusted authority in order to protect the privacy of the data
contributed by each data holder. We demonstrate the capability of P3LS to
vertically integrate process data along a hypothetical value chain consisting
of three parties and to improve the prediction performance on several
process-related key performance indicators. Furthermore, we show the numerical
equivalence of P3LS and PLS model components on simulated data and provide a
thorough privacy analysis of the former. Moreover, we propose a mechanism for
determining the relevance of the contributed data to the problem being
addressed, thus creating a basis for quantifying the contribution of
participants.
\\ ( https://arxiv.org/abs/2401.14884 ,  1335kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14907 (*cross-listing*)
Date: Fri, 26 Jan 2024 14:38:43 GMT   (1947kb,D)

Title: Learning Local Control Barrier Functions for Safety Control of Hybrid
  Systems
Authors: Shuo Yang, Yu Chen, Xiang Yin, Rahul Mangharam
Categories: cs.RO cs.LG cs.SY eess.SY
\\
  Hybrid dynamical systems are ubiquitous as practical robotic applications
often involve both continuous states and discrete switchings. Safety is a
primary concern for hybrid robotic systems. Existing safety-critical control
approaches for hybrid systems are either computationally inefficient,
detrimental to system performance, or limited to small-scale systems. To amend
these drawbacks, in this paper, we propose a learningenabled approach to
construct local Control Barrier Functions (CBFs) to guarantee the safety of a
wide class of nonlinear hybrid dynamical systems. The end result is a safe
neural CBFbased switching controller. Our approach is computationally
efficient, minimally invasive to any reference controller, and applicable to
large-scale systems. We empirically evaluate our framework and demonstrate its
efficacy and flexibility through two robotic examples including a
high-dimensional autonomous racing case, against other CBF-based approaches and
model predictive control.
\\ ( https://arxiv.org/abs/2401.14907 ,  1947kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14973 (*cross-listing*)
Date: Fri, 26 Jan 2024 16:06:01 GMT   (1257kb,D)

Title: Discovering group dynamics in synchronous time series via hierarchical
  recurrent switching-state models
Authors: Michael Wojnowicz, Preetish Rath, Eric Miller, Jeffrey Miller,
  Clifford Hancock, Meghan O'Donovan, Seth Elkin-Frankston, Thaddeus Brunye,
  and Michael C. Hughes
Categories: stat.ML cs.LG
\\
  We seek to model a collection of time series arising from multiple entities
interacting over the same time period. Recent work focused on modeling
individual time series is inadequate for our intended applications, where
collective system-level behavior influences the trajectories of individual
entities. To address such problems, we present a new hierarchical
switching-state model that can be trained in an unsupervised fashion to
simultaneously explain both system-level and individual-level dynamics. We
employ a latent system-level discrete state Markov chain that drives latent
entity-level chains which in turn govern the dynamics of each observed time
series. Feedback from the observations to the chains at both the entity and
system levels improves flexibility via context-dependent state transitions. Our
hierarchical switching recurrent dynamical models can be learned via
closed-form variational coordinate ascent updates to all latent chains that
scale linearly in the number of individual time series. This is asymptotically
no more costly than fitting separate models for each entity. Experiments on
synthetic and real datasets show that our model can produce better forecasts of
future entity behavior than existing methods. Moreover, the availability of
latent state chains at both the entity and system level enables interpretation
of group dynamics.
\\ ( https://arxiv.org/abs/2401.14973 ,  1257kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15018 (*cross-listing*)
Date: Fri, 26 Jan 2024 17:19:59 GMT   (1038kb)

Title: Enhancement of a Text-Independent Speaker Verification System by using
  Feature Combination and Parallel-Structure Classifiers
Authors: Kerlos Atia Abdalmalak and Ascensi\'on Gallardo-Antol'in
Categories: eess.AS cs.LG cs.SD
Journal-ref: Neural Computing and Applications 29 (2018) 637-651
DOI: 10.1007/s00521-016-2470-x
\\
  Speaker Verification (SV) systems involve mainly two individual stages:
feature extraction and classification. In this paper, we explore these two
modules with the aim of improving the performance of a speaker verification
system under noisy conditions. On the one hand, the choice of the most
appropriate acoustic features is a crucial factor for performing robust speaker
verification. The acoustic parameters used in the proposed system are: Mel
Frequency Cepstral Coefficients (MFCC), their first and second derivatives
(Deltas and Delta- Deltas), Bark Frequency Cepstral Coefficients (BFCC),
Perceptual Linear Predictive (PLP), and Relative Spectral Transform -
Perceptual Linear Predictive (RASTA-PLP). In this paper, a complete comparison
of different combinations of the previous features is discussed. On the other
hand, the major weakness of a conventional Support Vector Machine (SVM)
classifier is the use of generic traditional kernel functions to compute the
distances among data points. However, the kernel function of an SVM has great
influence on its performance. In this work, we propose the combination of two
SVM-based classifiers with different kernel functions: Linear kernel and
Gaussian Radial Basis Function (RBF) kernel with a Logistic Regression (LR)
classifier. The combination is carried out by means of a parallel structure
approach, in which different voting rules to take the final decision are
considered. Results show that significant improvement in the performance of the
SV system is achieved by using the combined features with the combined
classifiers either with clean speech or in the presence of noise. Finally, to
enhance the system more in noisy environments, the inclusion of the multiband
noise removal technique as a preprocessing stage is proposed.
\\ ( https://arxiv.org/abs/2401.15018 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15022 (*cross-listing*)
Date: Fri, 26 Jan 2024 17:29:01 GMT   (717kb)

Title: Machine learning-based analysis of glioma tissue sections: a review
Authors: Jan-Philipp Redlich, Friedrich Feuerhake, Joachim Weis, Nadine S.
  Schaadt, Sarah Teuber-Hanselmann, Christoph Buck, Sabine Luttmann, Andrea
  Eberle, Stefan Nikolin, Arno Appenzeller, Andreas Portmann, Andr\'e Homeyer
Categories: eess.IV cs.CV cs.LG
\\
  In recent years, the diagnosis of gliomas has become increasingly complex.
Histological assessment of glioma tissue using modern machine learning
techniques offers new opportunities to support diagnosis and outcome
prediction. To give an overview of the current state of research, this review
examines 70 publicly available research studies on machine learning-based
analysis of stained human glioma tissue sections, covering the diagnostic tasks
of subtyping (16/70), grading (23/70), molecular marker prediction (13/70), and
survival prediction (27/70). All studies were reviewed with regard to
methodological aspects as well as clinical applicability. It was found that the
focus of current research is the assessment of hematoxylin and eosin-stained
tissue sections of adult-type diffuse gliomas. The majority of studies (49/70)
are based on the publicly available glioblastoma and low-grade glioma datasets
from The Cancer Genome Atlas (TCGA) and only a few studies employed other
datasets in isolation (10/70) or in addition to the TCGA datasets (11/70).
Current approaches mostly rely on convolutional neural networks (53/70) for
analyzing tissue at 20x magnification (30/70). A new field of research is the
integration of clinical data, omics data, or magnetic resonance imaging
(27/70). So far, machine learning-based methods have achieved promising
results, but are not yet used in real clinical settings. Future work should
focus on the independent validation of methods on larger, multi-site datasets
with high-quality and up-to-date clinical and molecular pathology annotations
to demonstrate routine applicability.
\\ ( https://arxiv.org/abs/2401.15022 ,  717kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2211.16468
replaced with revised version Fri, 26 Jan 2024 16:03:18 GMT   (457kb,D)

Title: Linear-Time Algorithms for Front-Door Adjustment in Causal Graphs
Authors: Marcel Wien\"obst, Benito van der Zander, Maciej Li\'skiewicz
Categories: cs.AI cs.DS cs.LG stat.ME
Comments: Extended version of paper accepted to the Proceedings of the 38th
  AAAI Conference on Artificial Intelligence (AAAI-2024)
\\ ( https://arxiv.org/abs/2211.16468 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10219
replaced with revised version Thu, 25 Jan 2024 23:30:59 GMT   (15278kb,D)

Title: Exploring Link Prediction over Hyper-Relational Temporal Knowledge
  Graphs Enhanced with Time-Invariant Relational Knowledge
Authors: Zifeng Ding, Jingcheng Wu, Jingpei Wu, Yan Xia, Volker Tresp
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2307.10219 ,  15278kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11211
replaced with revised version Fri, 26 Jan 2024 12:20:34 GMT   (413kb)

Title: Leveraging Generative AI for Clinical Evidence Summarization Needs to
  Ensure Trustworthiness
Authors: Gongbo Zhang, Qiao Jin, Denis Jered McInerney, Yong Chen, Fei Wang,
  Curtis L. Cole, Qian Yang, Yanshan Wang, Bradley A. Malin, Mor Peleg, Byron
  C. Wallace, Zhiyong Lu, Chunhua Weng, Yifan Peng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2311.11211 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08463
replaced with revised version Fri, 26 Jan 2024 12:46:42 GMT   (1631kb,D)

Title: How much can change in a year? Revisiting Evaluation in Multi-Agent
  Reinforcement Learning
Authors: Siddarth Singh, Omayma Mahjoub, Ruan de Kock, Wiem Khlifi, Abidine
  Vall, Kale-ab Tessera and Arnu Pretorius
Categories: cs.AI
Comments: 6 pages, AAAI XAI4DRL workshop 2023; typos corrected, images updated,
  page count updated
MSC-class: I.2.11, I.2.0, A.0
\\ ( https://arxiv.org/abs/2312.08463 ,  1631kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08466
replaced with revised version Fri, 26 Jan 2024 13:07:55 GMT   (10956kb,D)

Title: Efficiently Quantifying Individual Agent Importance in Cooperative MARL
Authors: Omayma Mahjoub, Ruan de Kock, Siddarth Singh, Wiem Khlifi, Abidine
  Vall, Kale-ab Tessera and Arnu Pretorius
Categories: cs.AI
Comments: 8 pages, AAAI XAI4DRL workshop 2023; references updated, figure 8
  style updated, typos
MSC-class: I.2.11, I.2.0, A.0
\\ ( https://arxiv.org/abs/2312.08466 ,  10956kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03568
replaced with revised version Thu, 25 Jan 2024 21:20:27 GMT   (47588kb,D)

Title: Agent AI: Surveying the Horizons of Multimodal Interaction
Authors: Zane Durante, Qiuyuan Huang, Naoki Wake, Ran Gong, Jae Sung Park,
  Bidipta Sarkar, Rohan Taori, Yusuke Noda, Demetri Terzopoulos, Yejin Choi,
  Katsushi Ikeuchi, Hoi Vo, Li Fei-Fei, Jianfeng Gao
Categories: cs.AI cs.HC cs.LG
\\ ( https://arxiv.org/abs/2401.03568 ,  47588kb)
------------------------------------------------------------------------------
\\
arXiv:2302.14220
replaced with revised version Fri, 26 Jan 2024 16:02:53 GMT   (7565kb,D)

Title: Are Character-level Translations Worth the Wait? Comparing ByT5 and mT5
  for Machine Translation
Authors: Lukas Edman, Gabriele Sarti, Antonio Toral, Gertjan van Noord, Arianna
  Bisazza
Categories: cs.CL
Comments: This version of our work is a pre-MIT Press publication version
\\ ( https://arxiv.org/abs/2302.14220 ,  7565kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14827
replaced with revised version Fri, 26 Jan 2024 10:33:08 GMT   (7567kb,D)

Title: ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal,
  Causal, and Discourse Relations
Authors: Chunkit Chan, Jiayang Cheng, Weiqi Wang, Yuxin Jiang, Tianqing Fang,
  Xin Liu, Yangqiu Song
Categories: cs.CL
Comments: Accepted to Findings of EACL2024 "Exploring the Potential of ChatGPT
  on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse
  Relations"
\\ ( https://arxiv.org/abs/2304.14827 ,  7567kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02719
replaced with revised version Fri, 26 Jan 2024 02:56:51 GMT   (34kb)

Title: Multiple output samples per input in a single-output Gaussian process
Authors: Jeremy H. M. Wong, Huayun Zhang, and Nancy F. Chen
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: This paper is presented in the "Symposium for Celebrating 40 Years of
  Bayesian Learning in Speech and Language Processing and Beyond", which is a
  satellite event of the ASRU workshop, on 20 December 2023.
  https://bayesian40.github.io/
\\ ( https://arxiv.org/abs/2306.02719 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07164
replaced with revised version Fri, 26 Jan 2024 07:04:02 GMT   (195kb,D)

Title: Learning to Retrieve In-Context Examples for Large Language Models
Authors: Liang Wang, Nan Yang, Furu Wei
Categories: cs.CL cs.IR
Comments: Accepted by EACL 2024
\\ ( https://arxiv.org/abs/2307.07164 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08153
replaced with revised version Thu, 25 Jan 2024 23:21:10 GMT   (2937kb,D)

Title: Analyzing Dataset Annotation Quality Management in the Wild
Authors: Jan-Christoph Klie, Richard Eckart de Castilho, Iryna Gurevych
Categories: cs.CL
\\ ( https://arxiv.org/abs/2307.08153 ,  2937kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11394
replaced with revised version Thu, 25 Jan 2024 19:48:36 GMT   (170kb,D)

Title: MeetEval: A Toolkit for Computation of Word Error Rates for Meeting
  Transcription Systems
Authors: Thilo von Neumann, Christoph Boeddeker, Marc Delcroix, Reinhold
  Haeb-Umbach
Categories: cs.CL eess.AS
Comments: Presented at the CHiME7 workshop 2023
\\ ( https://arxiv.org/abs/2307.11394 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03795
replaced with revised version Thu, 25 Jan 2024 21:08:14 GMT   (1038kb,D)

Title: Toward Zero-Shot Instruction Following
Authors: Renze Lou, Wenpeng Yin
Categories: cs.CL
Comments: EACL 2024 Student Research Workshop
\\ ( https://arxiv.org/abs/2308.03795 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12060
replaced with revised version Fri, 26 Jan 2024 12:49:04 GMT   (1642kb,D)

Title: FlexKBQA: A Flexible LLM-Powered Framework for Few-Shot Knowledge Base
  Question Answering
Authors: Zhenyu Li, Sunqi Fan, Yu Gu, Xiuxing Li, Zhichao Duan, Bowen Dong,
  Ning Liu, Jianyong Wang
Categories: cs.CL cs.AI
Comments: Accepted as AAAI-24 Oral paper; Knowledge Base Question Answering;
  Large Language Model; Data Generation; Few-Shot & Zero-Shot
\\ ( https://arxiv.org/abs/2308.12060 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02731
replaced with revised version Fri, 26 Jan 2024 04:25:51 GMT   (6949kb,D)

Title: HC3 Plus: A Semantic-Invariant Human ChatGPT Comparison Corpus
Authors: Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma, Songlin Hu
Categories: cs.CL cs.AI
Comments: This paper has been accepted by CIKM2023 workshop
\\ ( https://arxiv.org/abs/2309.02731 ,  6949kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07009
replaced with revised version Fri, 26 Jan 2024 16:45:23 GMT   (191kb,D)

Title: OYXOY: A Modern NLP Test Suite for Modern Greek
Authors: Konstantinos Kogkalidis, Stergios Chatzikyriakidis, Eirini
  Chrysovalantou Giannikouri, Vassiliki Katsouli, Christina Klironomou,
  Christina Koula, Dimitris Papadakis, Thelka Pasparaki, Erofili Psaltaki,
  Efthymia Sakellariou, Hara Soupiona
Categories: cs.CL
Comments: EACL 2023 (Findings)
\\ ( https://arxiv.org/abs/2309.07009 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12537
replaced with revised version Fri, 26 Jan 2024 09:07:59 GMT   (4139kb,D)

Title: Product Attribute Value Extraction using Large Language Models
Authors: Alexander Brinkmann, Roee Shraga, Christian Bizer
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.12537 ,  4139kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18628
replaced with revised version Fri, 26 Jan 2024 10:02:57 GMT   (1703kb,D)

Title: Personalised Distillation: Empowering Open-Sourced LLMs with Adaptive
  Learning for Code Generation
Authors: Hailin Chen, Amrita Saha, Steven Hoi, Shafiq Joty
Categories: cs.CL cs.LG
Comments: Accepted to EMNLP 2023; Codes at:
  https://github.com/SalesforceAIResearch/PersDistill
\\ ( https://arxiv.org/abs/2310.18628 ,  1703kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01339
replaced with revised version Fri, 26 Jan 2024 18:11:43 GMT   (4524kb,D)

Title: ArabIcros: AI-Powered Arabic Crossword Puzzle Generation for Educational
  Applications
Authors: Kamyar Zeinalipour, Mohamed Zaky Saad, Marco Maggini, Marco Gori
Categories: cs.CL cs.AI
Comments: Accepted Paper for ArabicNLP 2023 - The First Arabic Natural Language
  Processing Conference - Co-located with EMNLP 2023 in Singapore
DOI: 10.18653/v1/2023.arabicnlp-1.23
\\ ( https://arxiv.org/abs/2312.01339 ,  4524kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09972
replaced with revised version Fri, 26 Jan 2024 13:12:10 GMT   (8865kb,D)

Title: Better Explain Transformers by Illuminating Important Information
Authors: Linxin Song, Yan Cui, Ao Luo, Freddy Lecue, Irene Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.09972 ,  8865kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10189
replaced with revised version Thu, 25 Jan 2024 22:55:42 GMT   (147kb,D)

Title: Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through
  Text Reconstruction
Authors: Qingyun Wang, Zixuan Zhang, Hongxiang Li, Xuan Liu, Jiawei Han, Huimin
  Zhao, Heng Ji
Categories: cs.CL cs.AI cs.LG
Comments: 16 pages. Accepted by Findings of the Association for Computational
  Linguistics: EACL 2024. Code and resources are available at
  https://github.com/EagleW/Chem-FINESE
\\ ( https://arxiv.org/abs/2401.10189 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11207
replaced with revised version Fri, 26 Jan 2024 05:37:33 GMT   (1734kb)

Title: Unfair TOS: An Automated Approach using Customized BERT
Authors: Bathini Sai Akash, Akshara Kupireddy, Lalita Bhanu Murthy
Categories: cs.CL cs.CY
\\ ( https://arxiv.org/abs/2401.11207 ,  1734kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14011
replaced with revised version Fri, 26 Jan 2024 09:46:03 GMT   (9098kb,D)

Title: CMMU: A Benchmark for Chinese Multi-modal Multi-type Question
  Understanding and Reasoning
Authors: Zheqi He, Xinya Wu, Pengfei Zhou, Richeng Xuan, Guang Liu, Xi Yang,
  Qiannan Zhu, Hua Huang
Categories: cs.CL cs.AI cs.MM
\\ ( https://arxiv.org/abs/2401.14011 ,  9098kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09921
replaced with revised version Fri, 26 Jan 2024 08:06:09 GMT   (34kb)

Title: Finite-time analysis of single-timescale actor-critic
Authors: Xuyang Chen, Lin Zhao
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2210.09921 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08560
replaced with revised version Fri, 26 Jan 2024 16:58:26 GMT   (6780kb,D)

Title: Dual RL: Unification and New Methods for Reinforcement and Imitation
  Learning
Authors: Harshit Sikchi, Qinqing Zheng, Amy Zhang, Scott Niekum
Categories: cs.LG cs.AI cs.RO
Comments: Published as a conference paper (spotlight) at ICLR 2024. 48 pages
\\ ( https://arxiv.org/abs/2302.08560 ,  6780kb)
------------------------------------------------------------------------------
\\
arXiv:2303.00128
replaced with revised version Fri, 26 Jan 2024 18:43:01 GMT   (1450kb,D)

Title: Representation Disentaglement via Regularization by Causal
  Identification
Authors: Juan Castorena
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.00128 ,  1450kb)
------------------------------------------------------------------------------
\\
arXiv:2303.03714
replaced with revised version Fri, 26 Jan 2024 03:13:59 GMT   (12405kb,D)

Title: Generative Modeling with Flow-Guided Density Ratio Learning
Authors: Alvin Heng, Abdul Fatir Ansari, Harold Soh
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2303.03714 ,  12405kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05752
replaced with revised version Fri, 26 Jan 2024 14:56:38 GMT   (102kb,D)

Title: Function Space and Critical Points of Linear Convolutional Networks
Authors: Kathl\'en Kohn, Guido Mont\'ufar, Vahid Shahverdi, Matthew Trager
Categories: cs.LG math.AG
Comments: 35 pages, 1 figure, 2 tables
MSC-class: 68T07, 14B05, 14E99, 14J99, 14N05, 14P10, 90C23
\\ ( https://arxiv.org/abs/2304.05752 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12292
replaced with revised version Fri, 26 Jan 2024 17:34:25 GMT   (3545kb,D)

Title: Optimal Low-Rank Matrix Completion: Semidefinite Relaxations and
  Eigenvector Disjunctions
Authors: Dimitris Bertsimas, Ryan Cory-Wright, Sean Lo, and Jean Pauphilet
Categories: cs.LG math.OC stat.ML
Comments: Updated version with new numerics showcasing relaxation for rank k>1
\\ ( https://arxiv.org/abs/2305.12292 ,  3545kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15260
replaced with revised version Fri, 26 Jan 2024 14:36:49 GMT   (5178kb,D)

Title: Making Offline RL Online: Collaborative World Models for Offline Visual
  Reinforcement Learning
Authors: Qi Wang, Junming Yang, Yunbo Wang, Xin Jin, Wenjun Zeng, Xiaokang Yang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2305.15260 ,  5178kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17191
replaced with revised version Fri, 26 Jan 2024 14:44:42 GMT   (384kb)

Title: MT-SLVR: Multi-Task Self-Supervised Learning for Transformation
  In(Variant) Representations
Authors: Calum Heggan, Tim Hospedales, Sam Budgett, Mehrdad Yaghoobi
Categories: cs.LG cs.SD eess.AS
Comments: Last author version accepted to InterSpeech23. 5 pages
\\ ( https://arxiv.org/abs/2305.17191 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05412
replaced with revised version Fri, 26 Jan 2024 15:03:21 GMT   (5983kb,D)

Title: Decoupled Prioritized Resampling for Offline RL
Authors: Yang Yue, Bingyi Kang, Xiao Ma, Qisen Yang, Gao Huang, Shiji Song,
  Shuicheng Yan
Categories: cs.LG cs.AI
Comments: preprint
\\ ( https://arxiv.org/abs/2306.05412 ,  5983kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05879
replaced with revised version Fri, 26 Jan 2024 05:58:33 GMT   (8298kb,D)

Title: FedWon: Triumphing Multi-domain Federated Learning Without Normalization
Authors: Weiming Zhuang, Lingjuan Lyu
Categories: cs.LG cs.AI cs.CV cs.DC
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2306.05879 ,  8298kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06618
replaced with revised version Fri, 26 Jan 2024 11:29:11 GMT   (115kb,D)

Title: Learning IMM Filter Parameters from Measurements using Gradient Descent
Authors: Andr\'e Brandenburger, Folker Hoffmann and Alexander Charlish
Categories: cs.LG cs.RO cs.SY eess.SY
Comments: Accepted for ISIF 26th International Conference on Information Fusion
  (FUSION)
ACM-class: I.2.6; I.2.9; J.2
DOI: 10.23919/FUSION52260.2023.10224219
\\ ( https://arxiv.org/abs/2307.06618 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12044
replaced with revised version Fri, 26 Jan 2024 08:59:51 GMT   (942kb,D)

Title: A multiobjective continuation method to compute the regularization path
  of deep neural networks
Authors: Augustina C. Amakor, Konstantin Sonntag and Sebastian Peitz
Categories: cs.LG cs.AI math.OC stat.ML
Comments: 7 pages, 6 figures
\\ ( https://arxiv.org/abs/2308.12044 ,  942kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07200
replaced with revised version Fri, 26 Jan 2024 12:40:27 GMT   (10548kb,D)

Title: Latent Representation and Simulation of Markov Processes via Time-Lagged
  Information Bottleneck
Authors: Marco Federici, Patrick Forr\'e, Ryota Tomioka, Bastiaan S. Veeling
Categories: cs.LG cs.AI cs.IT math.IT
Comments: 10 pages, 15 figures, Accepted ICLR 2024
\\ ( https://arxiv.org/abs/2309.07200 ,  10548kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07703
replaced with revised version Fri, 26 Jan 2024 09:55:38 GMT   (34kb)

Title: Causal Entropy and Information Gain for Measuring Causal Control
Authors: Francisco Nunes Ferreira Quialheiro Simoes, Mehdi Dastani, Thijs van
  Ommen
Categories: cs.LG cs.IT math.IT stat.ML
Comments: In Proceedings of the European Conference on Artificial Intelligence
  (ECAI) 2023. Work presented at the third XI-ML workshop of ECAI 2023
\\ ( https://arxiv.org/abs/2309.07703 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13736
replaced with revised version Fri, 26 Jan 2024 13:13:40 GMT   (660kb,D)

Title: Geometry of Linear Neural Networks: Equivariance and Invariance under
  Permutation Groups
Authors: Kathl\'en Kohn, Anna-Laura Sattelberger, Vahid Shahverdi
Categories: cs.LG math.AG
Comments: 40 pages, 8 figures, 1 table; comments welcome!
\\ ( https://arxiv.org/abs/2309.13736 ,  660kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15965
replaced with revised version Fri, 26 Jan 2024 07:38:41 GMT   (3118kb,D)

Title: TraCE: Trajectory Counterfactual Explanation Scores
Authors: Jeffrey N. Clark, Edward A. Small, Nawid Keshtmand, Michelle W.L. Wan,
  Elena Fillola Mayoral, Enrico Werner, Christopher P. Bourdeaux, Raul
  Santos-Rodriguez
Categories: cs.LG cs.CY math.MG
Comments: 10 pages, 4 figures, appendix
\\ ( https://arxiv.org/abs/2309.15965 ,  3118kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01262
replaced with revised version Fri, 26 Jan 2024 10:47:18 GMT   (928kb,D)

Title: Non-Exchangeable Conformal Risk Control
Authors: Ant\'onio Farinhas, Chrysoula Zerva, Dennis Ulmer, Andr\'e F. T.
  Martins
Categories: cs.LG stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.01262 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08876
replaced with revised version Fri, 26 Jan 2024 04:17:09 GMT   (224kb,D)

Title: Gesture Recognition for FMCW Radar on the Edge
Authors: Maximilian Strobel, Stephan Schoenfeldt, Jonas Daugalas
Categories: cs.LG eess.SP
Comments: 4 pages, 5 figures, accepted in 2024 IEEE Topical Conference on
  Wireless Sensors and Sensor Networks (WiSNeT)
\\ ( https://arxiv.org/abs/2310.08876 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09392
replaced with revised version Thu, 25 Jan 2024 22:22:15 GMT   (5432kb,D)

Title: Machine Learning Estimation of Maximum Vertical Velocity from Radar
Authors: Randy J. Chase, Amy McGovern, Cameron Homeyer, Peter Marinescu, Corey
  Potvin
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.09392 ,  5432kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18564
replaced with revised version Fri, 26 Jan 2024 01:23:11 GMT   (6350kb,D)

Title: A General Framework for Robust G-Invariance in G-Equivariant Networks
Authors: Sophia Sanborn, Nina Miolane
Categories: cs.LG cs.AI cs.CV
Journal-ref: Proceedings of the 37th Conference on Neural Information
  Processing Systems (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2310.18564 ,  6350kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08053
replaced with revised version Fri, 26 Jan 2024 09:35:05 GMT   (1295kb,D)

Title: Communication-Constrained Bayesian Active Knowledge Distillation
Authors: Victor Croisfelt, Shashi Raj Pandey, Osvaldo Simeone and Petar
  Popovski
Categories: cs.LG
Comments: 6 pages, 4 figures, conference version, submitted to IEEE ICC 2024
\\ ( https://arxiv.org/abs/2311.08053 ,  1295kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13628
replaced with revised version Fri, 26 Jan 2024 11:33:42 GMT   (2931kb,D)

Title: Where and How to Attack? A Causality-Inspired Recipe for Generating
  Counterfactual Adversarial Examples
Authors: Ruichu Cai, Yuxuan Zhu, Jie Qiao, Zefeng Liang, Furui Liu, Zhifeng Hao
Categories: cs.LG
Comments: Accepted by AAAI-2024
\\ ( https://arxiv.org/abs/2312.13628 ,  2931kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04139
replaced with revised version Fri, 26 Jan 2024 03:49:07 GMT   (803kb)

Title: CCNETS: A Novel Brain-Inspired Approach for Enhanced Pattern Recognition
  in Imbalanced Datasets
Authors: Hanbeot Park (1), Yunjeong Cho (2), Hoon-Hee Kim (3)
Categories: cs.LG
Comments: 31 pages, authors (3) is Corresponding Author
\\ ( https://arxiv.org/abs/2401.04139 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08534
replaced with revised version Fri, 26 Jan 2024 14:48:35 GMT   (6329kb,D)

Title: DiConStruct: Causal Concept-based Explanations through Black-Box
  Distillation
Authors: Ricardo Moreira, Jacopo Bono, M\'ario Cardoso, Pedro Saleiro, M\'ario
  A. T. Figueiredo, Pedro Bizarro
Categories: cs.LG cs.AI cs.HC
Comments: Accepted at Conference on Causal Learning and Reasoning (CLeaR 2024,
  https://www.cclear.cc/2024). To be published at Proceedings of Machine
  Learning Research (PMLR)
\\ ( https://arxiv.org/abs/2401.08534 ,  6329kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11143
replaced with revised version Thu, 25 Jan 2024 22:48:28 GMT   (4160kb,D)

Title: Gaussian Adaptive Attention is All You Need: Robust Contextual
  Representations Across Multiple Modalities
Authors: Georgios Ioannides, Aman Chadha, Aaron Elkins
Categories: cs.LG cs.AI cs.CL cs.CV cs.SD eess.AS eess.SP
\\ ( https://arxiv.org/abs/2401.11143 ,  4160kb)
------------------------------------------------------------------------------
\\
arXiv:2202.03314
replaced with revised version Fri, 26 Jan 2024 18:04:15 GMT   (10043kb,D)

Title: A Robot Web for Distributed Many-Device Localisation
Authors: Riku Murai, Joseph Ortiz, Sajad Saeedi, Paul H.J. Kelly, and Andrew J.
  Davison
Categories: cs.RO cs.AI cs.MA
Comments: Published in IEEE Transactions on Robotics (TRO) 2023
Journal-ref: IEEE Transactions on Robotics, vol. 40, pp. 121-138, 2024
DOI: 10.1109/TRO.2023.3324127
\\ ( https://arxiv.org/abs/2202.03314 ,  10043kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04582
replaced with revised version Fri, 26 Jan 2024 04:54:55 GMT   (21225kb,D)

Title: Towards Holistic Surgical Scene Understanding
Authors: Natalia Valderrama, Paola Ruiz Puentes, Isabela Hern\'andez, Nicol\'as
  Ayobi, Mathilde Verlyk, Jessica Santander, Juan Caicedo, Nicol\'as
  Fern\'andez, Pablo Arbel\'aez
Categories: cs.CV cs.AI
Comments: MICCAI 2022 Oral. Official extension published at arXiv:2401.11174 .
  Data and codes available at https://github.com/BCV-Uniandes/TAPIR
Journal-ref: Medical Image Computing and Computer Assisted Intervention 2022,
DOI: 10.1007/978-3-031-16449-1_42
\\ ( https://arxiv.org/abs/2212.04582 ,  21225kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07250
replaced with revised version Fri, 26 Jan 2024 11:05:01 GMT   (34798kb,D)

Title: Fusing Structure from Motion and Simulation-Augmented Pose Regression
  from Optical Flow for Challenging Indoor Environments
Authors: Felix Ott, Lucas Heublein, David R\"ugamer, Bernd Bischl, Christopher
  Mutschler
Categories: cs.CV cs.AI
MSC-class: 68U01
ACM-class: I.2.9; I.2.10; I.4.1; I.4.10; I.5.4
\\ ( https://arxiv.org/abs/2304.07250 ,  34798kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01747
replaced with revised version Fri, 26 Jan 2024 09:22:49 GMT   (8240kb,D)

Title: Expectation Maximization Pseudo Labels
Authors: Moucheng Xu and Yukun Zhou and Chen Jin and Marius de Groot and Daniel
  C. Alexander and Neil P. Oxtoby and Yipeng Hu and Joseph Jacob
Categories: cs.CV cs.AI cs.LG
Comments: Accepted in Medical Image Analysis
\\ ( https://arxiv.org/abs/2305.01747 ,  8240kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09802
replaced with revised version Thu, 25 Jan 2024 20:04:50 GMT   (2032kb,D)

Title: Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large
  Language Models
Authors: Evan King, Haoxiang Yu, Sangsu Lee, Christine Julien
Categories: cs.HC cs.AI
Comments: To appear in Proceedings of the ACM on Interactive, Mobile, Wearable
  and Ubiquitous Technologies (March 2024)
DOI: 10.1145/3643505
\\ ( https://arxiv.org/abs/2305.09802 ,  2032kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19956
replaced with revised version Thu, 25 Jan 2024 20:52:23 GMT   (4208kb,D)

Title: MicroSegNet: A Deep Learning Approach for Prostate Segmentation on
  Micro-Ultrasound Images
Authors: Hongxu Jiang, Muhammad Imran, Preethika Muralidharan, Anjali Patel,
  Jake Pensa, Muxuan Liang, Tarik Benidir, Joseph R. Grajo, Jason P. Joseph,
  Russell Terry, John Michael DiBianco, Li-Ming Su, Yuyin Zhou, Wayne G.
  Brisbane, and Wei Shao
Categories: cs.CV cs.AI cs.LG eess.IV
Journal-ref: Computerized Medical Imaging and Graphics (2024): 102326
DOI: 10.1016/j.compmedimag.2024.102326
\\ ( https://arxiv.org/abs/2305.19956 ,  4208kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02766
replaced with revised version Fri, 26 Jan 2024 14:24:32 GMT   (3921kb,D)

Title: Networked Communication for Decentralised Agents in Mean-Field Games
Authors: Patrick Benjamin and Alessandro Abate
Categories: cs.MA cs.AI cs.LG cs.SI cs.SY eess.SY
\\ ( https://arxiv.org/abs/2306.02766 ,  3921kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11305
replaced with revised version Fri, 26 Jan 2024 13:37:27 GMT   (48188kb,D)

Title: Progressive Fourier Neural Representation for Sequential Video
  Compilation
Authors: Haeyong Kang, Jaehong Yoon, DaHyun Kim, Sung Ju Hwang, and Chang D Yoo
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2306.11305 ,  48188kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15560 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 14:26:23 GMT   (42971kb,D)

Title: WeatherBench 2: A benchmark for the next generation of data-driven
  global weather models
Authors: Stephan Rasp, Stephan Hoyer, Alexander Merose, Ian Langmore, Peter
  Battaglia, Tyler Russel, Alvaro Sanchez-Gonzalez, Vivian Yang, Rob Carver,
  Shreya Agrawal, Matthew Chantry, Zied Ben Bouallegue, Peter Dueben, Carla
  Bromberg, Jared Sisk, Luke Barrington, Aaron Bell, Fei Sha
Categories: physics.ao-ph cs.AI
\\ ( https://arxiv.org/abs/2308.15560 ,  42971kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12244
replaced with revised version Fri, 26 Jan 2024 16:02:13 GMT   (8527kb,D)

Title: ChaCha: Leveraging Large Language Models to Prompt Children to Share
  Their Emotions about Personal Events
Authors: Woosuk Seo, Chanmo Yang, Young-Ho Kim
Categories: cs.HC cs.AI cs.CL
Comments: 16 pages, 5 figures, 2 tables; Accepted at ACM CHI 2024. More details
  at https://naver-ai.github.io/chacha/
ACM-class: H.5.2; I.2.7
\\ ( https://arxiv.org/abs/2309.12244 ,  8527kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02601
replaced with revised version Fri, 26 Jan 2024 10:21:09 GMT   (20708kb,D)

Title: MagicDrive: Street View Generation with Diverse 3D Geometry Control
Authors: Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo Li, Dit-Yan
  Yeung, Qiang Xu
Categories: cs.CV cs.AI
Comments: Project Page: https://flymin.github.io/magicdrive
\\ ( https://arxiv.org/abs/2310.02601 ,  20708kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02998
replaced with revised version Fri, 26 Jan 2024 18:45:29 GMT   (967kb,D)

Title: ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language
  Models
Authors: Yi-Lin Sung, Jaehong Yoon, Mohit Bansal
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: ICLR 2024 (project page: https://ecoflap.github.io/)
\\ ( https://arxiv.org/abs/2310.02998 ,  967kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08565
replaced with revised version Fri, 26 Jan 2024 02:08:35 GMT   (4769kb,D)

Title: Security Considerations in AI-Robotics: A Survey of Current Methods,
  Challenges, and Opportunities
Authors: Subash Neupane, Shaswata Mitra, Ivan A. Fernandez, Swayamjit Saha,
  Sudip Mittal, Jingdao Chen, Nisha Pillai, Shahram Rahimi
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2310.08565 ,  4769kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08602
replaced with revised version Fri, 26 Jan 2024 04:32:48 GMT   (5927kb,D)

Title: Safe Deep Policy Adaptation
Authors: Wenli Xiao, Tairan He, John Dolan, Guanya Shi
Categories: cs.RO cs.AI cs.LG
Comments: 8 pages, 7 figures
\\ ( https://arxiv.org/abs/2310.08602 ,  5927kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15950
replaced with revised version Thu, 25 Jan 2024 22:06:42 GMT   (867kb,D)

Title: Representation Learning with Large Language Models for Recommendation
Authors: Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang,
  Dawei Yin, Chao Huang
Categories: cs.IR cs.AI
Comments: Published as a WWW'24 full paper
\\ ( https://arxiv.org/abs/2310.15950 ,  867kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19731
replaced with revised version Fri, 26 Jan 2024 18:57:35 GMT   (2487kb,D)

Title: ViR: Towards Efficient Vision Retention Backbones
Authors: Ali Hatamizadeh, Michael Ranzinger, Shiyi Lan, Jose M. Alvarez, Sanja
  Fidler, Jan Kautz
Categories: cs.CV cs.AI cs.LG
Comments: Introduction of Vision Retention Networks (ViR) for Efficient Visual
  Modeling
\\ ( https://arxiv.org/abs/2310.19731 ,  2487kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13691 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 08:41:30 GMT   (604kb)

Title: Next-Generation Earth System Models: Towards Reliable Hybrid Models for
  Weather and Climate Applications
Authors: Tom Beucler, Erwan Koch, Sven Kotlarski, David Leutwyler, Adrien
  Michel, Jonathan Koh
Categories: physics.ao-ph cs.AI physics.comp-ph
Comments: 12 pages, 1 figure, submitted as part of the Swiss Academy of
  Engineering Sciences' 2024 whitepaper on "Artificial Intelligence for Climate
  Change Mitigation"
\\ ( https://arxiv.org/abs/2311.13691 ,  604kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14786
replaced with revised version Thu, 25 Jan 2024 20:55:16 GMT   (28795kb,D)

Title: GPT-4V Takes the Wheel: Promises and Challenges for Pedestrian Behavior
  Prediction
Authors: Jia Huang, Peng Jiang, Alvika Gautam, and Srikanth Saripalli
Categories: cs.CV cs.AI cs.RO
\\ ( https://arxiv.org/abs/2311.14786 ,  28795kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02003
replaced with revised version Fri, 26 Jan 2024 05:14:04 GMT   (2226kb,D)

Title: A Survey on Large Language Model (LLM) Security and Privacy: The Good,
  the Bad, and the Ugly
Authors: Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun and Yue
  Zhang
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2312.02003 ,  2226kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06401
replaced with revised version Fri, 26 Jan 2024 02:36:27 GMT   (826kb,D)

Title: DevEval: Evaluating Code Generation in Practical Software Projects
Authors: Jia Li, Ge Li, Yunfei Zhao, Yongmin Li, Zhi Jin, Hao Zhu, Huanyu Liu,
  Kaibo Liu, Lecheng Wang, Zheng Fang, Lanshen Wang, Jiazheng Ding, Xuanming
  Zhang, Yihong Dong, Yuqi Zhu, Bin Gu, Mengfei Yang
Categories: cs.SE cs.AI cs.CL
Comments: Preprint version. Work in Progress
\\ ( https://arxiv.org/abs/2401.06401 ,  826kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07603
replaced with revised version Fri, 26 Jan 2024 04:45:19 GMT   (21905kb,D)

Title: Multi-task robot data for dual-arm fine manipulation
Authors: Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi
Categories: cs.RO cs.AI
Comments: 10 pages, The dataset is available at
  https://sites.google.com/view/multi-task-fine
\\ ( https://arxiv.org/abs/2401.07603 ,  21905kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08115
replaced with revised version Fri, 26 Jan 2024 11:27:15 GMT   (37884kb,D)

Title: No-Clean-Reference Image Super-Resolution: Application to Electron
  Microscopy
Authors: Mohammad Khateri, Morteza Ghahremani, Alejandra Sierra, and Jussi
  Tohka
Categories: cs.CV cs.AI
Comments: 13 pages, 12 figures, and 2 tables
\\ ( https://arxiv.org/abs/2401.08115 ,  37884kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11174
replaced with revised version Fri, 26 Jan 2024 04:24:07 GMT   (20204kb,D)

Title: Pixel-Wise Recognition for Holistic Surgical Scene Understanding
Authors: Nicol\'as Ayobi and Santiago Rodr\'iguez and Alejandra P\'erez and
  Isabela Hern\'andez and Nicol\'as Aparicio and Eug\'enie Dessevres and
  Sebasti\'an Pe\~na and Jessica Santander and Juan Ignacio Caicedo and
  Nicol\'as Fern\'andez and Pablo Arbel\'aez
Categories: cs.CV cs.AI cs.LG
Comments: Preprint submitted to Medical Image Analysis. Official extension of
  previous MICCAI 2022
  (https://link.springer.com/chapter/10.1007/978-3-031-16449-1_42) and ISBI
  2023 (https://ieeexplore.ieee.org/document/10230819) orals. Data and codes
  are available at https://github.com/BCV-Uniandes/GraSP
\\ ( https://arxiv.org/abs/2401.11174 ,  20204kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13324
replaced with revised version Fri, 26 Jan 2024 09:09:43 GMT   (2521kb)

Title: Information That Matters: Exploring Information Needs of People Affected
  by Algorithmic Decisions
Authors: Timoth\'ee Schmude, Laura Koesten, Torsten M\"oller, Sebastian
  Tschiatschek
Categories: cs.HC cs.AI
Comments: Main text: 21 pages, 3 figures. Supplementary material is provided.
  Manuscript submitted for review to IJHCS
\\ ( https://arxiv.org/abs/2401.13324 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13758 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 03:11:58 GMT   (19kb,D)

Title: Assumptions and Bounds in the Instrumental Variable Model
Authors: Thomas S. Richardson and James M. Robins
Categories: math.ST cs.AI stat.TH
Comments: 27 pages, 1 figure, 1 table. Proofs of Theorems 1 and 2 stated in
  Richardson and Robins (2014) [arXiv:1410.0470]. v2 improves the writing in a
  few places
MSC-class: 62A01 (Primary) 62D20, 62H22 (Secondary)
\\ ( https://arxiv.org/abs/2401.13758 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2204.07236
replaced with revised version Thu, 25 Jan 2024 19:05:33 GMT   (457kb,D)

Title: A* shortest string decoding for non-idempotent semirings
Authors: Kyle Gorman and Cyril Allauzen
Categories: cs.FL cs.CL
Comments: Ten pages, two figures. To appear in the proceedings of the 18th
  Conference of the European Chapter of the Association for Computational
  Linguistics
\\ ( https://arxiv.org/abs/2204.07236 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07610
replaced with revised version Fri, 26 Jan 2024 02:46:11 GMT   (11628kb,D)

Title: Interpretable Online Log Analysis Using Large Language Models with
  Prompt Strategies
Authors: Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yanqing
  Zhao, Yuhang Chen, Hao Yang, Yanfei Jiang, Xun Chen
Categories: cs.SE cs.CL
Comments: Accepted by ICPC 2024
\\ ( https://arxiv.org/abs/2308.07610 ,  11628kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11082
replaced with revised version Fri, 26 Jan 2024 06:18:59 GMT   (10714kb,D)

Title: Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial
  Margin Contrastive Learning
Authors: Chen Jiang, Hong Liu, Xuzheng Yu, Qing Wang, Yuan Cheng, Jia Xu,
  Zhongyi Liu, Qingpei Guo, Wei Chu, Ming Yang, Yuan Qi
Categories: cs.CV cs.CL cs.MM
Comments: Accepted by ACM MM 2023
DOI: 10.1145/3581783.3612006
\\ ( https://arxiv.org/abs/2309.11082 ,  10714kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14196
replaced with revised version Fri, 26 Jan 2024 09:23:11 GMT   (3001kb,D)

Title: DeepSeek-Coder: When the Large Language Model Meets Programming -- The
  Rise of Code Intelligence
Authors: Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang,
  Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng
  Liang
Categories: cs.SE cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.14196 ,  3001kb)
------------------------------------------------------------------------------
\\
arXiv:2103.15010 (*cross-listing*)
replaced with revised version Thu, 25 Jan 2024 19:02:39 GMT   (108kb,D)

Title: On the Stability of Nonlinear Receding Horizon Control: A Geometric
  Perspective
Authors: Tyler Westenbroek, Max Simchowitz, Michael I. Jordan, S. Shankar
  Sastry
Categories: math.OC cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2103.15010 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2105.02487 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 02:28:28 GMT   (757kb,D)

Title: High-dimensional Functional Graphical Model Structure Learning via
  Neighborhood Selection Approach
Authors: Boxin Zhao, Percy S. Zhai, Y. Samuel Wang, Mladen Kolar
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2105.02487 ,  757kb)
------------------------------------------------------------------------------
\\
arXiv:2112.03183
replaced with revised version Fri, 26 Jan 2024 15:18:23 GMT   (325kb,D)

Title: Modification-Fair Cluster Editing
Authors: Vincent Froese, Leon Kellerhals, and Rolf Niedermeier
Categories: cs.DS cs.LG
Comments: AAAI 2022
\\ ( https://arxiv.org/abs/2112.03183 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2206.14674 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 16:33:31 GMT   (3144kb,D)

Title: Signature Methods in Machine Learning
Authors: Terry Lyons and Andrew D. McLeod
Categories: stat.ML cs.LG cs.NA math.CA math.NA math.ST stat.ME stat.TH
MSC-class: 60L10, 93C15, 68Q32, 34F05
\\ ( https://arxiv.org/abs/2206.14674 ,  3144kb)
------------------------------------------------------------------------------
\\
arXiv:2210.16380
replaced with revised version Fri, 26 Jan 2024 16:25:43 GMT   (6076kb,D)

Title: Incorporating Crowdsourced Annotator Distributions into Ensemble
  Modeling to Improve Classification Trustworthiness for Ancient Greek Papyri
Authors: Graham West, Matthew I. Swindall, Ben Keener, Timothy Player, Alex C.
  Williams, James H. Brusuelas, John F. Wallin
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2210.16380 ,  6076kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06787
replaced with revised version Thu, 25 Jan 2024 22:44:34 GMT   (185kb)

Title: A Polynomial Time, Pure Differentially Private Estimator for Binary
  Product Distributions
Authors: Vikrant Singhal
Categories: cs.DS cs.CR cs.LG stat.ML
\\ ( https://arxiv.org/abs/2304.06787 ,  185kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11043 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 15:32:10 GMT   (3146kb,D)

Title: Can Perturbations Help Reduce Investment Risks? Risk-Aware Stock
  Recommendation via Split Variational Adversarial Training
Authors: Jiezhu Cheng, Kaizhu Huang, Zibin Zheng
Categories: q-fin.RM cs.IR cs.LG
Comments: Accepted by TOIS
DOI: 10.1145/3643131
\\ ( https://arxiv.org/abs/2304.11043 ,  3146kb)
------------------------------------------------------------------------------
\\
arXiv:2305.00393
replaced with revised version Fri, 26 Jan 2024 07:24:31 GMT   (26419kb,D)

Title: DynaVol: Unsupervised Learning for Dynamic Scenes through Object-Centric
  Voxelization
Authors: Yanpeng Zhao, Siyu Gao, Yunbo Wang, Xiaokang Yang
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2305.00393 ,  26419kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07331 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 07:58:11 GMT   (697kb,D)

Title: Splitting and Parallelizing of Quantum Convolutional Neural Networks for
  Learning Translationally Symmetric Data
Authors: Koki Chinzei, Quoc Hoan Tran, Kazunori Maruyama, Hirotaka Oshima,
  Shintaro Sato
Categories: quant-ph cs.LG
Comments: 17 pages, 10 figures
\\ ( https://arxiv.org/abs/2306.07331 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01946
replaced with revised version Fri, 26 Jan 2024 18:59:20 GMT   (34234kb,D)

Title: ECG-Image-Kit: A Synthetic Image Generation Toolbox to Facilitate Deep
  Learning-Based Electrocardiogram Digitization
Authors: Kshama Kodthalu Shivashankara, Deepanshi, Afagh Mehri Shervedani, Gari
  D. Clifford, Matthew A. Reyna, Reza Sameni
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2307.01946 ,  34234kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09060 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 07:04:36 GMT   (15491kb,D)

Title: Extreme heatwave sampling and prediction with analog Markov chain and
  comparisons with deep learning
Authors: George Miloshevich, Dario Lucente, Pascal Yiou, Freddy Bouchet
Categories: physics.ao-ph cs.LG physics.data-an
Comments: 30 pages, 13 figures, presented at Climate Informatics 2023, UK
  Cambridge
\\ ( https://arxiv.org/abs/2307.09060 ,  15491kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09357
replaced with revised version Fri, 26 Jan 2024 10:36:43 GMT   (11414kb,D)

Title: Using the IBM Analog In-Memory Hardware Acceleration Kit for Neural
  Network Training and Inference
Authors: Manuel Le Gallo, Corey Lammie, Julian Buechel, Fabio Carta, Omobayode
  Fagbohungbe, Charles Mackin, Hsinyu Tsai, Vijay Narayanan, Abu Sebastian,
  Kaoutar El Maghraoui and Malte J. Rasch
Categories: cs.ET cs.LG
Journal-ref: APL Machine Learning (2023) 1 (4): 041102
DOI: 10.1063/5.0168089
\\ ( https://arxiv.org/abs/2307.09357 ,  11414kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04655
replaced with revised version Fri, 26 Jan 2024 05:40:50 GMT   (9028kb)

Title: Intelligent upper-limb exoskeleton integrated with soft wearable
  bioelectronics and deep-learning for human intention-driven strength
  augmentation based on sensory feedback
Authors: Jinwoo Lee, Kangkyu Kwon, Ira Soltis, Jared Matthews, Yoonjae Lee,
  Hojoong Kim, Lissette Romero, Nathan Zavanelli, Youngjin Kwon, Shinjae Kwon,
  Jimin Lee, Yewon Na, Sung Hoon Lee, Ki Jun Yu, Minoru Shinohara, Frank L.
  Hammond, Woon-Hong Yeo
Categories: cs.RO cs.LG cs.SY eess.SP eess.SY
Comments: 15 pages, 6 figures, 1 table, published in npj flexible electronics
  journals
MSC-class: 68T40 (Primary) 92C55, 68T99 (Secondary)
\\ ( https://arxiv.org/abs/2309.04655 ,  9028kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13223
replaced with revised version Thu, 25 Jan 2024 21:05:17 GMT   (11885kb,D)

Title: Causal Reasoning: Charting a Revolutionary Course for Next-Generation
  AI-Native Wireless Networks
Authors: Christo Kurisummoottil Thomas, Christina Chaccour, Walid Saad,
  Merouane Debbah and Choong Seon Hong
Categories: cs.IT cs.LG math.IT
\\ ( https://arxiv.org/abs/2309.13223 ,  11885kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04402
replaced with revised version Fri, 26 Jan 2024 10:32:10 GMT   (9587kb,D)

Title: Semi-Supervised Active Learning for Semantic Segmentation in Unknown
  Environments Using Informative Path Planning
Authors: Julius R\"uckin, Federico Magistri, Cyrill Stachniss, Marija Popovi\'c
Categories: cs.RO cs.LG
Comments: 8 pages, 9 figures
\\ ( https://arxiv.org/abs/2312.04402 ,  9587kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05398
replaced with revised version Fri, 26 Jan 2024 09:07:46 GMT   (203kb)

Title: Generative Network Layer for Communication Systems with Artificial
  Intelligence
Authors: Mathias Thorsager, Israel Leyva-Mayorga, Beatriz Soret, and Petar
  Popovski
Categories: cs.IT cs.LG math.IT
Comments: Accepted for publication in IEEE Networking Letters
\\ ( https://arxiv.org/abs/2312.05398 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13721
replaced with revised version Fri, 26 Jan 2024 10:59:54 GMT   (40554kb,D)

Title: Uncertainty-Guided Alignment for Unsupervised Domain Adaptation in
  Regression
Authors: Ismail Nejjar, Gaetan Frusque, Florent Forest, Olga Fink
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.13721 ,  40554kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
