Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月30日 17:32
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri 26 Jan 24 19:00:00 GMT  to  Mon 29 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.15081
Date: Sun, 7 Jan 2024 12:36:31 GMT   (292kb,D)

Title: Can generative AI and ChatGPT outperform humans on cognitive-demanding
  problem-solving tasks in science?
Authors: Xiaoming Zhai, Matthew Nyaaba, and Wenchao Ma
Categories: cs.AI
Journal-ref: Science & Education, 2024
\\
  This study aimed to examine an assumption that generative artificial
intelligence (GAI) tools can overcome the cognitive intensity that humans
suffer when solving problems. We compared the performance of ChatGPT and GPT-4
on 2019 NAEP science assessments with students by cognitive demands of the
items. Fifty-four tasks were coded by experts using a two-dimensional cognitive
load framework, including task cognitive complexity and dimensionality. ChatGPT
and GPT-4 responses were scored using the scoring keys of NAEP. The analysis of
the available data was based on the average student ability scores for students
who answered each item correctly and the percentage of students who responded
to individual items. Results showed that both ChatGPT and GPT-4 consistently
outperformed most students who answered the NAEP science assessments. As the
cognitive demand for NAEP tasks increases, statistically higher average student
ability scores are required to correctly address the questions. This pattern
was observed for students in grades 4, 8, and 12, respectively. However,
ChatGPT and GPT-4 were not statistically sensitive to the increase in cognitive
demands of the tasks, except for Grade 4. As the first study focusing on
comparing GAI and K-12 students in problem-solving in science, this finding
implies the need for changes to educational objectives to prepare students with
competence to work with GAI tools in the future. Education ought to emphasize
the cultivation of advanced cognitive skills rather than depending solely on
tasks that demand cognitive intensity. This approach would foster critical
thinking, analytical skills, and the application of knowledge in novel
contexts. Findings also suggest the need for innovative assessment practices by
moving away from cognitive intensity tasks toward creativity and analytical
skills to avoid the negative effects of GAI on testing more efficiently.
\\ ( https://arxiv.org/abs/2401.15081 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15188
Date: Fri, 26 Jan 2024 20:18:25 GMT   (383kb,D)

Title: CAREForMe: Contextual Multi-Armed Bandit Recommendation Framework for
  Mental Health
Authors: Sheng Yu, Narjes Nourzad, Randye J. Semple, Yixue Zhao, Emily Zhou,
  Bhaskar Krishnamachari
Categories: cs.AI
Comments: MOBILESoft 2024
\\
  The COVID-19 pandemic has intensified the urgency for effective and
accessible mental health interventions in people's daily lives. Mobile Health
(mHealth) solutions, such as AI Chatbots and Mindfulness Apps, have gained
traction as they expand beyond traditional clinical settings to support daily
life. However, the effectiveness of current mHealth solutions is impeded by the
lack of context-awareness, personalization, and modularity to foster their
reusability. This paper introduces CAREForMe, a contextual multi-armed bandit
(CMAB) recommendation framework for mental health. Designed with
context-awareness, personalization, and modularity at its core, CAREForMe
harnesses mobile sensing and integrates online learning algorithms with user
clustering capability to deliver timely, personalized recommendations. With its
modular design, CAREForMe serves as both a customizable recommendation
framework to guide future research, and a collaborative platform to facilitate
interdisciplinary contributions in mHealth research. We showcase CAREForMe's
versatility through its implementation across various platforms (e.g., Discord,
Telegram) and its customization to diverse recommendation features.
\\ ( https://arxiv.org/abs/2401.15188 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15196
Date: Fri, 26 Jan 2024 20:45:40 GMT   (156kb,D)

Title: Regularized Q-Learning with Linear Function Approximation
Authors: Jiachen Xi, Alfredo Garcia, Petar Momcilovic
Categories: cs.AI
\\
  Several successful reinforcement learning algorithms make use of
regularization to promote multi-modal policies that exhibit enhanced
exploration and robustness. With functional approximation, the convergence
properties of some of these algorithms (e.g. soft Q-learning) are not well
understood. In this paper, we consider a single-loop algorithm for minimizing
the projected Bellman error with finite time convergence guarantees in the case
of linear function approximation. The algorithm operates on two scales: a
slower scale for updating the target network of the state-action values, and a
faster scale for approximating the Bellman backups in the subspace of the span
of basis vectors. We show that, under certain assumptions, the proposed
algorithm converges to a stationary point in the presence of Markovian noise.
In addition, we provide a performance guarantee for the policies derived from
the proposed algorithm.
\\ ( https://arxiv.org/abs/2401.15196 ,  156kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15356
Date: Sat, 27 Jan 2024 09:13:09 GMT   (3874kb,D)

Title: A Statistical Framework for Measuring AI Reliance
Authors: Ziyang Guo, Yifan Wu, Jason Hartline and Jessica Hullman
Categories: cs.AI cs.HC
\\
  Humans frequently make decisions with the aid of artificially intelligent
(AI) systems. A common pattern is for the AI to recommend an action to the
human who retains control over the final decision. Researchers have identified
ensuring that a human has appropriate reliance on an AI as a critical component
of achieving complementary performance. We argue that the current definition of
appropriate reliance used in such research lacks formal statistical grounding
and can lead to contradictions. We propose a formal definition of reliance,
based on statistical decision theory, which separates the concepts of reliance
as the probability the decision-maker follows the AI's prediction from
challenges a human may face in differentiating the signals and forming accurate
beliefs about the situation. Our definition gives rise to a framework that can
be used to guide the design and interpretation of studies on human-AI
complementarity and reliance. Using recent AI-advised decision making studies
from literature, we demonstrate how our framework can be used to separate the
loss due to mis-reliance from the loss due to not accurately differentiating
the signals. We evaluate these losses by comparing to a baseline and a
benchmark for complementary performance defined by the expected payoff achieved
by a rational agent facing the same decision task as the behavioral agents.
\\ ( https://arxiv.org/abs/2401.15356 ,  3874kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15443
Date: Sat, 27 Jan 2024 15:30:49 GMT   (645kb,D)

Title: DiffuserLite: Towards Real-time Diffusion Planning
Authors: Zibin Dong, Jianye Hao, Yifu Yuan, Fei Ni, Yitian Wang, Pengyi Li and
  Yan Zheng
Categories: cs.AI
\\
  Diffusion planning has been recognized as an effective decision-making
paradigm in various domains. The high-quality conditional generation capability
of long-horizon trajectories makes it a promising research direction. However,
existing diffusion planning methods suffer from low decision-making frequencies
because of the expensive iterative sampling cost. To address this issue, we
introduce DiffuserLite, a fast and lightweight diffusion planning framework.
DiffuserLite employs a planning refinement process (PRP) to generate
coarse-to-fine-grained trajectories, which significantly reduces the modeling
of redundant information and leads to notable increases in decision-making
frequency. Our experimental results demonstrate that DiffuserLite incurs only
$0.88\%$ of the runtime cost compared to previous frameworks, achieves an
average decision-making frequency of $122$Hz, and reaches state-of-the-art
performance on D4RL benchmarks. In addition, our clean DiffuserLite framework
can serve as a flexible plugin to enhance decision frequency in other diffusion
planning algorithms, providing a structural design reference for future works.
More details and visualizations are available at [project
website](https://diffuserlite.github.io/).
\\ ( https://arxiv.org/abs/2401.15443 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15621
Date: Sun, 28 Jan 2024 10:20:15 GMT   (146kb,D)

Title: SNAP: Semantic Stories for Next Activity Prediction
Authors: Alon Oved, Segev Shlomov, Sergey Zeltyn, Nir Mashkif and Avi Yaeli
Categories: cs.AI
\\
  Predicting the next activity in an ongoing process is one of the most common
classification tasks in the business process management (BPM) domain. It allows
businesses to optimize resource allocation, enhance operational efficiency, and
aids in risk mitigation and strategic decision-making. This provides a
competitive edge in the rapidly evolving confluence of BPM and AI. Existing
state-of-the-art AI models for business process prediction do not fully
capitalize on available semantic information within process event logs. As
current advanced AI-BPM systems provide semantically-richer textual data, the
need for novel adequate models grows. To address this gap, we propose the novel
SNAP method that leverages language foundation models by constructing semantic
contextual stories from the process historical event logs and using them for
the next activity prediction. We compared the SNAP algorithm with nine
state-of-the-art models on six benchmark datasets and show that SNAP
significantly outperforms them, especially for datasets with high levels of
semantic content.
\\ ( https://arxiv.org/abs/2401.15621 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15911
Date: Mon, 29 Jan 2024 06:46:15 GMT   (45kb)

Title: Distribution-consistency Structural Causal Models
Authors: Heyang Gong, Chaochao Lu, Yu Zhang
Categories: cs.AI math.ST stat.TH
\\
  In the field of causal modeling, potential outcomes (PO) and structural
causal models (SCMs) stand as the predominant frameworks. However, these
frameworks face notable challenges in practically modeling counterfactuals,
formalized as parameters of the joint distribution of potential outcomes.
Counterfactual reasoning holds paramount importance in contemporary
decision-making processes, especially in scenarios that demand personalized
incentives based on the joint values of $(Y(0), Y(1))$. This paper begins with
an investigation of the PO and SCM frameworks for modeling counterfactuals.
Through the analysis, we identify an inherent model capacity limitation, termed
as the ``degenerative counterfactual problem'', emerging from the consistency
rule that is the cornerstone of both frameworks. To address this limitation, we
introduce a novel \textit{distribution-consistency} assumption, and in
alignment with it, we propose the Distribution-consistency Structural Causal
Models (DiscoSCMs) offering enhanced capabilities to model counterfactuals. To
concretely reveal the enhanced model capacity, we introduce a new identifiable
causal parameter, \textit{the probability of consistency}, which holds
practical significance within DiscoSCM alone, showcased with a personalized
incentive example. Furthermore, we provide a comprehensive set of theoretical
results about the ``Ladder of Causation'' within the DiscoSCM framework. We
hope it opens new avenues for future research of counterfactual modeling,
ultimately enhancing our understanding of causality and its real-world
applications.
\\ ( https://arxiv.org/abs/2401.15911 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16045
Date: Mon, 29 Jan 2024 10:54:28 GMT   (1259kb,D)

Title: Type-based Neural Link Prediction Adapter for Complex Query Answering
Authors: Lingning Song and Yi Zu and Shan Lu and Jieyue He
Categories: cs.AI
Comments: 11 pages, 3 figures
\\
  Answering complex logical queries on incomplete knowledge graphs (KGs) is a
fundamental and challenging task in multi-hop reasoning. Recent work defines
this task as an end-to-end optimization problem, which significantly reduces
the training cost and enhances the generalization of the model by a pretrained
link predictors for query answering. However, most existing proposals ignore
the critical semantic knowledge inherently available in KGs, such as type
information, which could help answer complex logical queries. To this end, we
propose TypE-based Neural Link Prediction Adapter (TENLPA), a novel model that
constructs type-based entity-relation graphs to discover the latent
relationships between entities and relations by leveraging type information in
KGs. Meanwhile, in order to effectively combine type information with complex
logical queries, an adaptive learning mechanism is introduced, which is trained
by back-propagating during the complex query answering process to achieve
adaptive adjustment of neural link predictors. Experiments on 3 standard
datasets show that TENLPA model achieves state-of-the-art performance on
complex query answering with good generalization and robustness.
\\ ( https://arxiv.org/abs/2401.16045 ,  1259kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16119
Date: Mon, 29 Jan 2024 12:45:27 GMT   (6158kb,D)

Title: Triple Disentangled Representation Learning for Multimodal Affective
  Analysis
Authors: Ying Zhou, Xuefeng Liang, Han Chen, Yin Zhao
Categories: cs.AI
Comments: 14 pages, 6 figures
\\
  Multimodal learning has exhibited a significant advantage in affective
analysis tasks owing to the comprehensive information of various modalities,
particularly the complementary information. Thus, many emerging studies focus
on disentangling the modality-invariant and modality-specific representations
from input data and then fusing them for prediction. However, our study shows
that modality-specific representations may contain information that is
irrelevant or conflicting with the tasks, which downgrades the effectiveness of
learned multimodal representations. We revisit the disentanglement issue, and
propose a novel triple disentanglement approach, TriDiRA, which disentangles
the modality-invariant, effective modality-specific and ineffective
modality-specific representations from input data. By fusing only the
modality-invariant and effective modality-specific representations, TriDiRA can
significantly alleviate the impact of irrelevant and conflicting information
across modalities during model training. Extensive experiments conducted on
four benchmark datasets demonstrate the effectiveness and generalization of our
triple disentanglement, which outperforms SOTA methods.
\\ ( https://arxiv.org/abs/2401.16119 ,  6158kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16124
Date: Mon, 29 Jan 2024 12:49:09 GMT   (206kb,D)

Title: On the generalization of learned constraints for ASP solving in temporal
  domains
Authors: Javier Romero, Torsten Schaub, Klaus Strauch
Categories: cs.AI
Comments: 28 pages, 3 figures
\\
  The representation of a dynamic problem in ASP usually boils down to using
copies of variables and constraints, one for each time stamp, no matter whether
it is directly encoded or via an action or temporal language. The
multiplication of variables and constraints is commonly done during grounding
and the solver is completely ignorant about the temporal relationship among the
different instances. On the other hand, a key factor in the performance of
today's ASP solvers is conflict-driven constraint learning. Our question is now
whether a constraint learned for particular time steps can be generalized and
reused at other time stamps, and ultimately whether this enhances the overall
solver performance on temporal problems. Knowing full well the domain of time,
we study conditions under which learned dynamic constraints can be generalized.
We propose a simple translation of the original logic program such that, for
the translated programs, the learned constraints can be generalized to other
time points. Additionally, we identify a property of temporal problems that
allows us to generalize all learned constraints to all time steps. It turns out
that this property is satisfied by many planning problems. Finally, we
empirically evaluate the impact of adding the generalized constraints to an ASP
solver
\\ ( https://arxiv.org/abs/2401.16124 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16270
Date: Mon, 29 Jan 2024 16:18:54 GMT   (808kb,D)

Title: Capturing Knowledge Graphs and Rules with Octagon Embeddings
Authors: Victor Charpenay, Steven Schockaert
Categories: cs.AI
\\
  Region based knowledge graph embeddings represent relations as geometric
regions. This has the advantage that the rules which are captured by the model
are made explicit, making it straightforward to incorporate prior knowledge and
to inspect learned models. Unfortunately, existing approaches are severely
restricted in their ability to model relational composition, and hence also
their ability to model rules, thus failing to deliver on the main promise of
region based models. With the aim of addressing these limitations, we
investigate regions which are composed of axis-aligned octagons. Such octagons
are particularly easy to work with, as intersections and compositions can be
straightforwardly computed, while they are still sufficiently expressive to
model arbitrary knowledge graphs. Among others, we also show that our octagon
embeddings can properly capture a non-trivial class of rule bases. Finally, we
show that our model achieves competitive experimental results.
\\ ( https://arxiv.org/abs/2401.16270 ,  808kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16287
Date: Mon, 29 Jan 2024 16:48:34 GMT   (3829kb,D)

Title: GAPS: Geometry-Aware Problem Solver
Authors: Jiaxin Zhang, Yinghui Jiang, Yashar Moshfeghi
Categories: cs.AI cs.CL
\\
  Geometry problem solving presents a formidable challenge within the NLP
community. Existing approaches often rely on models designed for solving math
word problems, neglecting the unique characteristics of geometry math problems.
Additionally, the current research predominantly focuses on geometry
calculation problems, while overlooking other essential aspects like proving.
In this study, we address these limitations by proposing the Geometry-Aware
Problem Solver (GAPS) model. GAPS is specifically designed to generate solution
programs for geometry math problems of various types with the help of its
unique problem-type classifier. To achieve this, GAPS treats the solution
program as a composition of operators and operands, segregating their
generation processes. Furthermore, we introduce the geometry elements
enhancement method, which enhances the ability of GAPS to recognize geometry
elements accurately. By leveraging these improvements, GAPS showcases
remarkable performance in resolving geometry math problems. Our experiments
conducted on the UniGeo dataset demonstrate the superiority of GAPS over the
state-of-the-art model, Geoformer. Specifically, GAPS achieves an accuracy
improvement of more than 5.3% for calculation tasks and an impressive 41.1% for
proving tasks. Notably, GAPS achieves an impressive accuracy of 97.5% on
proving problems, representing a significant advancement in solving geometry
proving tasks.
\\ ( https://arxiv.org/abs/2401.16287 ,  3829kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16398
Date: Mon, 29 Jan 2024 18:38:29 GMT   (1918kb,D)

Title: Zero-shot Imitation Policy via Search in Demonstration Dataset
Authors: Federco Malato, Florian Leopold, Andrew Melnik, Ville Hautamaki
Categories: cs.AI
\\
  Behavioral cloning uses a dataset of demonstrations to learn a policy. To
overcome computationally expensive training procedures and address the policy
adaptation problem, we propose to use latent spaces of pre-trained foundation
models to index a demonstration dataset, instantly access similar relevant
experiences, and copy behavior from these situations. Actions from a selected
similar situation can be performed by the agent until representations of the
agent's current situation and the selected experience diverge in the latent
space. Thus, we formulate our control problem as a dynamic search problem over
a dataset of experts' demonstrations. We test our approach on BASALT
MineRL-dataset in the latent representation of a Video Pre-Training model. We
compare our model to state-of-the-art, Imitation Learning-based Minecraft
agents. Our approach can effectively recover meaningful demonstrations and show
human-like behavior of an agent in the Minecraft environment in a wide variety
of scenarios. Experimental results reveal that performance of our search-based
approach clearly wins in terms of accuracy and perceptual evaluation over
learning-based models.
\\ ( https://arxiv.org/abs/2401.16398 ,  1918kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16412
Date: Mon, 29 Jan 2024 18:49:50 GMT   (152kb,D)

Title: Learning to Manipulate under Limited Information
Authors: Wesley H. Holliday and Alexander Kristoffersen and Eric Pacuit
Categories: cs.AI cs.GT cs.LG cs.MA econ.TH
Comments: 9 pages, 3 figures. Code available at https://github.com/epacuit/ltm
MSC-class: 91B12, 91B14, 91B10, 68T07
ACM-class: I.2.6; I.2.11
\\
  By classic results in social choice theory, any reasonable preferential
voting method sometimes gives individuals an incentive to report an insincere
preference. The extent to which different voting methods are more or less
resistant to such strategic manipulation has become a key consideration for
comparing voting methods. Here we measure resistance to manipulation by whether
neural networks of varying sizes can learn to profitably manipulate a given
voting method in expectation, given different types of limited information
about how other voters will vote. We trained nearly 40,000 neural networks of
26 sizes to manipulate against 8 different voting methods, under 6 types of
limited information, in committee-sized elections with 5-21 voters and 3-6
candidates. We find that some voting methods, such as Borda, are highly
manipulable by networks with limited information, while others, such as Instant
Runoff, are not, despite being quite profitably manipulated by an ideal
manipulator with full information.
\\ ( https://arxiv.org/abs/2401.16412 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15170
Date: Fri, 26 Jan 2024 19:25:43 GMT   (32kb)

Title: Scalable Qualitative Coding with LLMs: Chain-of-Thought Reasoning
  Matches Human Performance in Some Hermeneutic Tasks
Authors: Zackary Okun Dunivin
Categories: cs.CL cs.AI
\\
  Qualitative coding, or content analysis, extracts meaning from text to
discern quantitative patterns across a corpus of texts. Recently, advances in
the interpretive abilities of large language models (LLMs) offer potential for
automating the coding process (applying category labels to texts), thereby
enabling human researchers to concentrate on more creative research aspects,
while delegating these interpretive tasks to AI. Our case study comprises a set
of socio-historical codes on dense, paragraph-long passages representative of a
humanistic study. We show that GPT-4 is capable of human-equivalent
interpretations, whereas GPT-3.5 is not. Compared to our human-derived gold
standard, GPT-4 delivers excellent intercoder reliability (Cohen's $\kappa \geq
0.79$) for 3 of 9 codes, and substantial reliability ($\kappa \geq 0.6$) for 8
of 9 codes. In contrast, GPT-3.5 greatly underperforms for all codes
($mean(\kappa) = 0.34$; $max(\kappa) = 0.55$). Importantly, we find that coding
fidelity improves considerably when the LLM is prompted to give rationale
justifying its coding decisions (chain-of-thought reasoning). We present these
and other findings along with a set of best practices for adapting traditional
codebooks for LLMs. Our results indicate that for certain codebooks,
state-of-the-art LLMs are already adept at large-scale content analysis.
Furthermore, they suggest the next generation of models will likely render AI
coding a viable option for a majority of codebooks.
\\ ( https://arxiv.org/abs/2401.15170 ,  32kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15222
Date: Fri, 26 Jan 2024 22:19:31 GMT   (973kb,D)

Title: Transfer Learning for the Prediction of Entity Modifiers in Clinical
  Text: Application to Opioid Use Disorder Case Detection
Authors: Abdullateef I. Almudaifer, Tobias O`Leary, Whitney Covington, JaMor
  Hairston, Zachary Deitch, Ankit Anand, Caleb M. Carroll, Estera Crisan,
  William Bradford, Lauren Walter, Eaton Ellen, Sue S. Feldman and John D.
  Osborne
Categories: cs.CL cs.AI cs.LG
Comments: 18 pages, 2 figures, 6 tables. To be submitted to the Journal of
  Biomedical Semantics
\\
  Background: The semantics of entities extracted from a clinical text can be
dramatically altered by modifiers, including entity negation, uncertainty,
conditionality, severity, and subject. Existing models for determining
modifiers of clinical entities involve regular expression or features weights
that are trained independently for each modifier.
  Methods: We develop and evaluate a multi-task transformer architecture design
where modifiers are learned and predicted jointly using the publicly available
SemEval 2015 Task 14 corpus and a new Opioid Use Disorder (OUD) data set that
contains modifiers shared with SemEval as well as novel modifiers specific for
OUD. We evaluate the effectiveness of our multi-task learning approach versus
previously published systems and assess the feasibility of transfer learning
for clinical entity modifiers when only a portion of clinical modifiers are
shared.
  Results: Our approach achieved state-of-the-art results on the ShARe corpus
from SemEval 2015 Task 14, showing an increase of 1.1% on weighted accuracy,
1.7% on unweighted accuracy, and 10% on micro F1 scores.
  Conclusions: We show that learned weights from our shared model can be
effectively transferred to a new partially matched data set, validating the use
of transfer learning for clinical text modifiers
\\ ( https://arxiv.org/abs/2401.15222 ,  973kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15241
Date: Fri, 26 Jan 2024 23:17:31 GMT   (4706kb,D)

Title: Unlearning Reveals the Influential Training Data of Language Models
Authors: Masaru Isonuma and Ivan Titov
Categories: cs.CL cs.AI
Comments: 12 pages, under review
\\
  In order to enhance the performance of language models while mitigating the
risks of generating harmful content, it is crucial to identify which training
dataset affects the model's outputs. Ideally, we can measure the influence of
each dataset by removing it from training; however, it is prohibitively
expensive to retrain a model multiple times. This paper presents UnTrac, which
estimates the influence of a training dataset by unlearning it from the trained
model. UnTrac is extremely simple; each training dataset is unlearned by
gradient ascent, and we evaluate how much the model's predictions change after
unlearning. We empirically examine if our methods can assess the influence of
pretraining datasets on generating toxic, biased, and untruthful content.
Experimental results demonstrate that our method estimates their influence much
more accurately than existing methods while requiring neither excessive memory
space nor multiple model checkpoints.
\\ ( https://arxiv.org/abs/2401.15241 ,  4706kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15269
Date: Sat, 27 Jan 2024 02:29:42 GMT   (487kb,D)

Title: Improving Medical Reasoning through Retrieval and Self-Reflection with
  Retrieval-Augmented Large Language Models
Authors: Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang
Categories: cs.CL cs.AI cs.IR
\\
  Recent proprietary large language models (LLMs), such as GPT-4, have achieved
a milestone in tackling diverse challenges in the biomedical domain, ranging
from multiple-choice questions to long-form generations. To address challenges
that still cannot be handled with the encoded knowledge of LLMs, various
retrieval-augmented generation (RAG) methods have been developed by searching
documents from the knowledge corpus and appending them unconditionally or
selectively to the input of LLMs for generation. However, when applying
existing methods to different domain-specific problems, poor generalization
becomes apparent, leading to fetching incorrect documents or making inaccurate
judgments. In this paper, we introduce Self-BioRAG, a framework reliable for
biomedical text that specializes in generating explanations, retrieving
domain-specific documents, and self-reflecting generated responses. We utilize
84k filtered biomedical instruction sets to train Self-BioRAG that can assess
its generated explanations with customized reflective tokens. Our work proves
that domain-specific components, such as a retriever, domain-related document
corpus, and instruction sets are necessary for adhering to domain-related
instructions. Using three major medical question-answering benchmark datasets,
experimental results of Self-BioRAG demonstrate significant performance gains
by achieving a 7.2% absolute improvement on average over the state-of-the-art
open-foundation model with a parameter size of 7B or less. Overall, we analyze
that Self-BioRAG finds the clues in the question, retrieves relevant documents
if needed, and understands how to answer with information from retrieved
documents and encoded knowledge as a medical expert does. We release our data
and code for training our framework components and model weights (7B and 13B)
to enhance capabilities in biomedical and clinical domains.
\\ ( https://arxiv.org/abs/2401.15269 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15312
Date: Sat, 27 Jan 2024 06:06:16 GMT   (512kb,D)

Title: How We Refute Claims: Automatic Fact-Checking through Flaw
  Identification and Explanation
Authors: Wei-Yu Kao and An-Zi Yen
Categories: cs.CL
\\
  Automated fact-checking is a crucial task in the governance of internet
content. Although various studies utilize advanced models to tackle this issue,
a significant gap persists in addressing complex real-world rumors and
deceptive claims. To address this challenge, this paper explores the novel task
of flaw-oriented fact-checking, including aspect generation and flaw
identification. We also introduce RefuteClaim, a new framework designed
specifically for this task. Given the absence of an existing dataset, we
present FlawCheck, a dataset created by extracting and transforming insights
from expert reviews into relevant aspects and identified flaws. The
experimental results underscore the efficacy of RefuteClaim, particularly in
classifying and elucidating false claims.
\\ ( https://arxiv.org/abs/2401.15312 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15316
Date: Sat, 27 Jan 2024 06:29:07 GMT   (246kb,D)

Title: UNSEE: Unsupervised Non-contrastive Sentence Embeddings
Authors: \"Omer Veysel \c{C}a\u{g}atan
Categories: cs.CL
Comments: Accepted to Main Proceedings of EACL 2024
\\
  We present UNSEE: Unsupervised Non-Contrastive Sentence Embeddings, a novel
approach that outperforms SimCSE in the Massive Text Embedding benchmark. Our
exploration begins by addressing the challenge of representation collapse, a
phenomenon observed when contrastive objectives in SimCSE are replaced with
non-contrastive objectives. To counter this issue, we propose a straightforward
solution known as the target network, effectively mitigating representation
collapse. The introduction of the target network allows us to leverage
non-contrastive objectives, maintaining training stability while achieving
performance improvements comparable to contrastive objectives. Our method has
achieved peak performance in non-contrastive sentence embeddings through
meticulous fine-tuning and optimization. This comprehensive effort has yielded
superior sentence representation models, showcasing the effectiveness of our
approach.
\\ ( https://arxiv.org/abs/2401.15316 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15328
Date: Sat, 27 Jan 2024 07:08:37 GMT   (154kb,D)

Title: Equipping Language Models with Tool Use Capability for Tabular Data
  Analysis in Finance
Authors: Adrian Theuma and Ehsan Shareghi
Categories: cs.CL
Comments: Accepted to EACL2024; code, model and dataset are available at <a
  href="https://raven-lm.github.io">https://raven-lm.github.io</a>
\\
  Large language models (LLMs) have exhibited an array of reasoning
capabilities but face challenges like error propagation and hallucination,
particularly in specialised areas like finance, where data is heterogeneous,
and precision is paramount. We explore the potential of language model
augmentation with external tools to mitigate these limitations and offload
certain reasoning steps to external tools that are more suited for the task,
instead of solely depending on the LLM's inherent abilities. More concretely,
using financial domain question-answering datasets, we apply supervised
fine-tuning on a LLaMA-2 13B Chat model to act both as a 'task router' and
'task solver'. The 'task router' dynamically directs a question to either be
answered internally by the LLM or externally via the right tool from the tool
set. Our tool-equipped SFT model, Raven, demonstrates an improvement of 35.2%
and 5.06% over the base model and SFT-only baselines, respectively, and is
highly competitive with strong GPT-3.5 results. To the best of our knowledge,
our work is the first that investigates tool augmentation of language models
for the finance domain.
\\ ( https://arxiv.org/abs/2401.15328 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15347
Date: Sat, 27 Jan 2024 08:38:56 GMT   (1084kb,D)

Title: A Comprehensive Survey of Compression Algorithms for Language Models
Authors: Seungcheol Park, Jaehyeon Choi, Sojin Lee, and U Kang
Categories: cs.CL cs.AI
MSC-class: 68T50
ACM-class: I.2.7
\\
  How can we compress language models without sacrificing accuracy? The number
of compression algorithms for language models is rapidly growing to benefit
from remarkable advances of recent language models without side effects due to
the gigantic size of language models, such as increased carbon emissions and
expensive maintenance fees. While numerous compression algorithms have shown
remarkable progress in compressing language models, it ironically becomes
challenging to capture emerging trends and identify the fundamental concepts
underlying them due to the excessive number of algorithms. In this paper, we
survey and summarize diverse compression algorithms including pruning,
quantization, knowledge distillation, low-rank approximation, parameter
sharing, and efficient architecture design. We not only summarize the overall
trend of diverse compression algorithms but also select representative
algorithms and provide in-depth analyses of them. We discuss the value of each
category of compression algorithms, and the desired properties of low-cost
compression algorithms which have a significant impact due to the emergence of
large language models. Finally, we introduce promising future research topics
based on our survey results.
\\ ( https://arxiv.org/abs/2401.15347 ,  1084kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15351
Date: Sat, 27 Jan 2024 08:52:19 GMT   (300kb,D)

Title: A Survey on Neural Topic Models: Methods, Applications, and Challenges
Authors: Xiaobao Wu, Thong Nguyen, Anh Tuan Luu
Categories: cs.CL cs.AI cs.IR
Comments: Accepted to Artifcial Intelligence Review. See
  https://doi.org/10.1007/s10462-023-10661-7 and a paper list at
  https://github.com/BobXWu/Paper-Neural-Topic-Models
DOI: 10.1007/s10462-023-10661-7
\\
  Topic models have been prevalent for decades to discover latent topics and
infer topic proportions of documents in an unsupervised fashion. They have been
widely used in various applications like text analysis and context
recommendation. Recently, the rise of neural networks has facilitated the
emergence of a new research field -- Neural Topic Models (NTMs). Different from
conventional topic models, NTMs directly optimize parameters without requiring
model-specific derivations. This endows NTMs with better scalability and
flexibility, resulting in significant research attention and plentiful new
methods and applications. In this paper, we present a comprehensive survey on
neural topic models concerning methods, applications, and challenges.
Specifically, we systematically organize current NTM methods according to their
network structures and introduce the NTMs for various scenarios like short
texts and cross-lingual documents. We also discuss a wide range of popular
applications built on NTMs. Finally, we highlight the challenges confronted by
NTMs to inspire future research.
\\ ( https://arxiv.org/abs/2401.15351 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15360
Date: Sat, 27 Jan 2024 09:27:47 GMT   (7817kb,D)

Title: Importance-Aware Data Augmentation for Document-Level Neural Machine
  Translation
Authors: Minghao Wu, Yufei Wang, George Foster, Lizhen Qu, Gholamreza Haffari
Categories: cs.CL
Comments: 13 pages, 4 figures, 7 tables, accepted by EACL2024 main conference
\\
  Document-level neural machine translation (DocNMT) aims to generate
translations that are both coherent and cohesive, in contrast to its
sentence-level counterpart. However, due to its longer input length and limited
availability of training data, DocNMT often faces the challenge of data
sparsity. To overcome this issue, we propose a novel Importance-Aware Data
Augmentation (IADA) algorithm for DocNMT that augments the training data based
on token importance information estimated by the norm of hidden states and
training gradients. We conduct comprehensive experiments on three widely-used
DocNMT benchmarks. Our empirical results show that our proposed IADA
outperforms strong DocNMT baselines as well as several data augmentation
approaches, with statistical significance on both sentence-level and
document-level BLEU.
\\ ( https://arxiv.org/abs/2401.15360 ,  7817kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15371
Date: Sat, 27 Jan 2024 10:28:27 GMT   (1633kb,D)

Title: LegalDuet: Learning Effective Representations for Legal Judgment
  Prediction through a Dual-View Legal Clue Reasoning
Authors: Pengjie Liu, Zhenghao Liu, Xiaoyuan Yi, Liner Yang, Shuo Wang, Yu Gu,
  Ge Yu, Xing Xie, Shuang-hua Yang
Categories: cs.CL
\\
  Most existing Legal Judgment Prediction (LJP) models focus on discovering the
legal triggers in the criminal fact description. However, in real-world
scenarios, a professional judge not only needs to assimilate the law case
experience that thrives on past sentenced legal judgments but also depends on
the professional legal grounded reasoning that learned from professional legal
knowledge. In this paper, we propose a LegalDuet model, which pretrains
language models to learn a tailored embedding space for making legal judgments.
It proposes a dual-view legal clue reasoning mechanism, which derives from two
reasoning chains of judges: 1) Law Case Reasoning, which makes legal judgments
according to the judgment experiences learned from analogy/confusing legal
cases; 2) Legal Ground Reasoning, which lies in matching the legal clues
between criminal cases and legal decisions. Our experiments show that LegalDuet
achieves state-of-the-art performance on the CAIL2018 dataset and outperforms
baselines with about 4% improvements on average. Our dual-view reasoning based
pretraining can capture critical legal clues to learn a tailored embedding
space to distinguish criminal cases. It reduces LegalDuet's uncertainty during
prediction and brings pretraining advances to the confusing/low frequent
charges. All codes are available at https://github.com/NEUIR/LegalDuet.
\\ ( https://arxiv.org/abs/2401.15371 ,  1633kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15378
Date: Sat, 27 Jan 2024 10:50:11 GMT   (267kb)

Title: A RAG-based Question Answering System Proposal for Understanding Islam:
  MufassirQAS LLM
Authors: Ahmet Yusuf Alan, Enis Karaarslan, Omer Aydin
Categories: cs.CL cs.AI
\\
  There exist challenges in learning and understanding religions as the
presence of complexity and depth of religious doctrines and teachings. Chatbots
as question-answering systems can help in solving these challenges. LLM
chatbots use NLP techniques to establish connections between topics and
accurately respond to complex questions. These capabilities make it perfect to
be used in enlightenment on religion as a question answering chatbot. However,
LLMs also have a tendency to generate false information, known as
hallucination. The responses of the chatbots can include content that insults
personal religious beliefs, interfaith conflicts, and controversial or
sensitive topics. It needs to avoid such cases without promoting hate speech or
offending certain groups of people or their beliefs. This study uses a vector
database-based Retrieval Augmented Generation (RAG) approach to enhance the
accuracy and transparency of LLMs. Our question-answering system is called as
"MufassirQAS". We created a vector database with several open-access books that
include Turkish context. These are Turkish translations, and interpretations on
Islam. We worked on creating system prompts with care, ensuring they provide
instructions that prevent harmful, offensive, or disrespectful responses. We
also tested the MufassirQAS and ChatGPT with sensitive questions. We got better
performance with our system. Study and enhancements are still in progress.
Results and future works are given.
\\ ( https://arxiv.org/abs/2401.15378 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15385
Date: Sat, 27 Jan 2024 11:07:19 GMT   (481kb,D)

Title: Towards Event Extraction from Speech with Contextual Clues
Authors: Jingqi Kang, Tongtong Wu, Jinming Zhao, Guitao Wang, Guilin Qi,
  Yuan-Fang Li, Gholamreza Haffari
Categories: cs.CL cs.MM
Comments: Under Review
\\
  While text-based event extraction has been an active research area and has
seen successful application in many domains, extracting semantic events from
speech directly is an under-explored problem. In this paper, we introduce the
Speech Event Extraction (SpeechEE) task and construct three synthetic training
sets and one human-spoken test set. Compared to event extraction from text,
SpeechEE poses greater challenges mainly due to complex speech signals that are
continuous and have no word boundaries. Additionally, unlike perceptible sound
events, semantic events are more subtle and require a deeper understanding. To
tackle these challenges, we introduce a sequence-to-structure generation
paradigm that can produce events from speech signals in an end-to-end manner,
together with a conditioned generation method that utilizes speech recognition
transcripts as the contextual clue. We further propose to represent events with
a flat format to make outputs more natural language-like. Our experimental
results show that our method brings significant improvements on all datasets,
achieving a maximum F1 gain of 10.7%. The code and datasets are released on
https://github.com/jodie-kang/SpeechEE.
\\ ( https://arxiv.org/abs/2401.15385 ,  481kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15391
Date: Sat, 27 Jan 2024 11:41:48 GMT   (12300kb,D)

Title: MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop
  Queries
Authors: Yixuan Tang and Yi Yang
Categories: cs.CL
Comments: Link: https://github.com/yixuantt/MultiHop-RAG/
\\
  Retrieval-augmented generation (RAG) augments large language models (LLM) by
retrieving relevant knowledge, showing promising potential in mitigating LLM
hallucinations and enhancing response quality, thereby facilitating the great
adoption of LLMs in practice. However, we find that existing RAG systems are
inadequate in answering multi-hop queries, which require retrieving and
reasoning over multiple pieces of supporting evidence. Furthermore, to our
knowledge, no existing RAG benchmarking dataset focuses on multi-hop queries.
In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a
knowledge base, a large collection of multi-hop queries, their ground-truth
answers, and the associated supporting evidence. We detail the procedure of
building the dataset, utilizing an English news article dataset as the
underlying RAG knowledge base. We demonstrate the benchmarking utility of
MultiHop-RAG in two experiments. The first experiment compares different
embedding models for retrieving evidence for multi-hop queries. In the second
experiment, we examine the capabilities of various state-of-the-art LLMs,
including GPT-4, PaLM, and Llama2-70B, in reasoning and answering multi-hop
queries given the evidence. Both experiments reveal that existing RAG methods
perform unsatisfactorily in retrieving and answering multi-hop queries. We hope
MultiHop-RAG will be a valuable resource for the community in developing
effective RAG systems, thereby facilitating greater adoption of LLMs in
practice. The MultiHop-RAG and implemented RAG system is publicly available at
https://github.com/yixuantt/MultiHop-RAG/.
\\ ( https://arxiv.org/abs/2401.15391 ,  12300kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15393
Date: Sat, 27 Jan 2024 11:51:11 GMT   (64kb)

Title: Semantics of Multiword Expressions in Transformer-Based Models: A Survey
Authors: Filip Mileti\'c, Sabine Schulte im Walde
Categories: cs.CL
Comments: Accepted to TACL 2024. This is a pre-MIT Press publication version
\\
  Multiword expressions (MWEs) are composed of multiple words and exhibit
variable degrees of compositionality. As such, their meanings are notoriously
difficult to model, and it is unclear to what extent this issue affects
transformer architectures. Addressing this gap, we provide the first in-depth
survey of MWE processing with transformer models. We overall find that they
capture MWE semantics inconsistently, as shown by reliance on surface patterns
and memorized information. MWE meaning is also strongly localized,
predominantly in early layers of the architecture. Representations benefit from
specific linguistic properties, such as lower semantic idiosyncrasy and
ambiguity of target expressions. Our findings overall question the ability of
transformer models to robustly capture fine-grained semantics. Furthermore, we
highlight the need for more directly comparable evaluation setups.
\\ ( https://arxiv.org/abs/2401.15393 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15400
Date: Sat, 27 Jan 2024 12:33:07 GMT   (9316kb,D)

Title: Indexing Portuguese NLP Resources with PT-Pump-Up
Authors: R\'uben Almeida, Ricardo Campos, Al\'ipio Jorge, S\'ergio Nunes
Categories: cs.CL cs.IR
Comments: Demo Track, 3 pages
MSC-class: 68P20
ACM-class: I.7.1
Journal-ref: PROPOR 2024
\\
  The recent advances in natural language processing (NLP) are linked to
training processes that require vast amounts of corpora. Access to this data is
commonly not a trivial process due to resource dispersion and the need to
maintain these infrastructures online and up-to-date. New developments in NLP
are often compromised due to the scarcity of data or lack of a shared
repository that works as an entry point to the community. This is especially
true in low and mid-resource languages, such as Portuguese, which lack data and
proper resource management infrastructures. In this work, we propose
PT-Pump-Up, a set of tools that aim to reduce resource dispersion and improve
the accessibility to Portuguese NLP resources. Our proposal is divided into
four software components: a) a web platform to list the available resources; b)
a client-side Python package to simplify the loading of Portuguese NLP
resources; c) an administrative Python package to manage the platform and d) a
public GitHub repository to foster future collaboration and contributions. All
four components are accessible using: https://linktr.ee/pt_pump_up
\\ ( https://arxiv.org/abs/2401.15400 ,  9316kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15439
Date: Sat, 27 Jan 2024 15:20:43 GMT   (93kb,D)

Title: Pre-training and Diagnosing Knowledge Base Completion Models
Authors: Vid Kocijan, Myeongjun Erik Jang, Thomas Lukasiewicz
Categories: cs.CL
Comments: Accepted to AIJ, reference to follow. arXiv admin note: substantial
  text overlap with arXiv:2108.13073
\\
  In this work, we introduce and analyze an approach to knowledge transfer from
one collection of facts to another without the need for entity or relation
matching. The method works for both canonicalized knowledge bases and
uncanonicalized or open knowledge bases, i.e., knowledge bases where more than
one copy of a real-world entity or relation may exist. The main contribution is
a method that can make use of large-scale pre-training on facts, which were
collected from unstructured text, to improve predictions on structured data
from a specific domain. The introduced method is most impactful on small
datasets such as ReVerb20k, where a 6% absolute increase of mean reciprocal
rank and 65% relative decrease of mean rank over the previously best method was
achieved, despite not relying on large pre-trained models like Bert. To
understand the obtained pre-trained models better, we then introduce a novel
dataset for the analysis of pre-trained models for Open Knowledge Base
Completion, called Doge (Diagnostics of Open knowledge Graph Embeddings). It
consists of 6 subsets and is designed to measure multiple properties of a
pre-trained model: robustness against synonyms, ability to perform deductive
reasoning, presence of gender stereotypes, consistency with reverse relations,
and coverage of different areas of general knowledge. Using the introduced
dataset, we show that the existing OKBC models lack consistency in the presence
of synonyms and inverse relations and are unable to perform deductive
reasoning. Moreover, their predictions often align with gender stereotypes,
which persist even when presented with counterevidence. We additionally
investigate the role of pre-trained word embeddings and demonstrate that
avoiding biased word embeddings is not a sufficient measure to prevent biased
behavior of OKBC models.
\\ ( https://arxiv.org/abs/2401.15439 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15449
Date: Sat, 27 Jan 2024 16:19:30 GMT   (3212kb,D)

Title: Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for
  Hallucination Mitigation
Authors: Yuxin Liang, Zhuoyang Song, Hao Wang, Jiaxing Zhang
Categories: cs.CL
\\
  We evaluate the ability of Large Language Models (LLMs) to discern and
express their internal knowledge state, a key factor in countering factual
hallucination and ensuring reliable application of LLMs. We observe a robust
self-awareness of internal knowledge state in LLMs, evidenced by over 85%
accuracy in knowledge probing. However, LLMs often fail to express their
internal knowledge during generation, leading to factual hallucinations. We
develop an automated hallucination annotation tool, Dreamcatcher, which merges
knowledge probing and consistency checking methods to rank factual preference
data. Using knowledge preference as reward, We propose a Reinforcement Learning
from Knowledge Feedback (RLKF) training framework, leveraging reinforcement
learning to enhance the factuality and honesty of LLMs. Our experiments across
multiple models show that RLKF training effectively enhances the ability of
models to utilize their internal knowledge state, boosting performance in a
variety of knowledge-based and honesty-related tasks.
\\ ( https://arxiv.org/abs/2401.15449 ,  3212kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15463
Date: Sat, 27 Jan 2024 17:06:53 GMT   (7339kb,D)

Title: DataFrame QA: A Universal LLM Framework on DataFrame Question Answering
  Without Data Exposure
Authors: Junyi Ye, Mengnan Du, Guiling Wang
Categories: cs.CL cs.AI
\\
  This paper introduces DataFrame question answering (QA), a novel task that
utilizes large language models (LLMs) to generate Pandas queries for
information retrieval and data analysis on dataframes, emphasizing safe and
non-revealing data handling. Our method, which solely relies on dataframe
column names, not only ensures data privacy but also significantly reduces the
context window in the prompt, streamlining information processing and
addressing major challenges in LLM-based data analysis. We propose DataFrame QA
as a comprehensive framework that includes safe Pandas query generation and
code execution. Various LLMs, notably GPT-4, are evaluated using the pass@1
metric on the renowned WikiSQL and our newly developed 'UCI-DataFrameQA',
tailored for complex data analysis queries. Our findings indicate that GPT-4
achieves pass@1 rates of 86% on WikiSQL and 97% on UCI-DataFrameQA,
underscoring its capability in securely retrieving and aggregating dataframe
values and conducting sophisticated data analyses. This approach, deployable in
a zero-shot manner without prior training or adjustments, proves to be highly
adaptable and secure for diverse applications.
\\ ( https://arxiv.org/abs/2401.15463 ,  7339kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15471
Date: Sat, 27 Jan 2024 17:51:05 GMT   (528kb,D)

Title: ConvoSense: Overcoming Monotonous Commonsense Inferences for
  Conversational AI
Authors: Sarah E. Finch and Jinho D. Choi
Categories: cs.CL
Comments: accepted to TACL 2024; final author's version of paper; pre-MIT Press
  publication version
\\
  Mastering commonsense understanding and reasoning is a pivotal skill
essential for conducting engaging conversations. While there have been several
attempts to create datasets that facilitate commonsense inferences in dialogue
contexts, existing datasets tend to lack in-depth details, restate information
already present in the conversation, and often fail to capture the multifaceted
nature of commonsense reasoning. In response to these limitations, we compile a
new synthetic dataset for commonsense reasoning in dialogue contexts using GPT,
ConvoSense, that boasts greater contextual novelty, offers a higher volume of
inferences per example, and substantially enriches the detail conveyed by the
inferences. Our dataset contains over 500,000 inferences across 12,000
dialogues with 10 popular inference types, which empowers the training of
generative commonsense models for dialogue that are superior in producing
plausible inferences with high novelty when compared to models trained on the
previous datasets. To the best of our knowledge, ConvoSense is the first of its
kind to provide such a multitude of novel inferences at such a large scale.
\\ ( https://arxiv.org/abs/2401.15471 ,  528kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15476
Date: Sat, 27 Jan 2024 18:34:29 GMT   (8322kb,D)

Title: To Burst or Not to Burst: Generating and Quantifying Improbable Text
Authors: Kuleen Sasse, Samuel Barham, Efsun Sarioglu Kayi, Edward W. Staley
Categories: cs.CL
Comments: Originally published at the Generation, Evaluation & Metrics (GEM)
  Workshop at EMNLP 2023. We are awaiting the release of the proceedings which
  we will reference here
\\
  While large language models (LLMs) are extremely capable at text generation,
their outputs are still distinguishable from human-authored text. We explore
this separation across many metrics over text, many sampling techniques, many
types of text data, and across two popular LLMs, LLaMA and Vicuna. Along the
way, we introduce a new metric, recoverability, to highlight differences
between human and machine text; and we propose a new sampling technique, burst
sampling, designed to close this gap. We find that LLaMA and Vicuna have
distinct distributions under many of the metrics, and that this influences our
results: Recoverability separates real from fake text better than any other
metric when using LLaMA. When using Vicuna, burst sampling produces text which
is distributionally closer to real text compared to other sampling techniques.
\\ ( https://arxiv.org/abs/2401.15476 ,  8322kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15496
Date: Sat, 27 Jan 2024 20:20:39 GMT   (2002kb,D)

Title: Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue
  Summarization
Authors: Jianfei Xiao, Yancan Chen, Yimin Ou, Hanyi Yu, Yiyong Xiao
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) like Llama, Baichuan and Bloom models show
remarkable ability with instruction fine-tuning in many natural language tasks.
Nevertheless, for the dialogue summarization task, which aims to generate
summaries for different roles in dialogue, most of the state-of-the-art methods
conduct on small models (e.g Bart and Bert). Existing methods try to add task
specified optimization on small models like adding global-local centrality
score to models. In this paper, we propose an instruction fine-tuning model:
Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different
instructions for different roles, the model can learn from the dialogue
interactions and output the expected summaries. Furthermore, we applied NEFTune
technique to add suitable noise during training to improve the results. The
experiments demonstrate that the proposed model achieves the new
state-of-the-art results on two public dialogue summarization datasets: CSDS
and SAMSUM. We release our model and related codes to facilitate future studies
on dialogue summarization task.
\\ ( https://arxiv.org/abs/2401.15496 ,  2002kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15498
Date: Sat, 27 Jan 2024 20:26:03 GMT   (8177kb,D)

Title: Do We Need Language-Specific Fact-Checking Models? The Case of Chinese
Authors: Caiqi Zhang, Zhijiang Guo, Andreas Vlachos
Categories: cs.CL
\\
  This paper investigates the potential benefits of language-specific
fact-checking models, focusing on the case of Chinese. We demonstrate the
limitations of methods such as translating Chinese claims and evidence into
English or directly using multilingual large language models (e.g. GPT4),
highlighting the need for language-specific systems. We further develop a
state-of-the-art Chinese fact-checking system that, in contrast to previous
approaches which treat evidence selection as a pairwise sentence classification
task, considers the context of sentences. We also create an adversarial dataset
to identify biases in our model, and while they are present as in English
language datasets and models, they are often specific to the Chinese culture.
Our study emphasizes the importance of language-specific fact-checking models
to effectively combat misinformation.
\\ ( https://arxiv.org/abs/2401.15498 ,  8177kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15499
Date: Sat, 27 Jan 2024 20:31:10 GMT   (146kb,D)

Title: Semantic Properties of cosine based bias scores for word embeddings
Authors: Sarah Schr\"oder, Alexander Schulz, Fabian Hinder and Barbara Hammer
Categories: cs.CL
Comments: 11 pages, 4 figures
\\
  Plenty of works have brought social biases in language models to attention
and proposed methods to detect such biases. As a result, the literature
contains a great deal of different bias tests and scores, each introduced with
the premise to uncover yet more biases that other scores fail to detect. What
severely lacks in the literature, however, are comparative studies that analyse
such bias scores and help researchers to understand the benefits or limitations
of the existing methods. In this work, we aim to close this gap for cosine
based bias scores. By building on a geometric definition of bias, we propose
requirements for bias scores to be considered meaningful for quantifying
biases. Furthermore, we formally analyze cosine based scores from the
literature with regard to these requirements. We underline these findings with
experiments to show that the bias scores' limitations have an impact in the
application case.
\\ ( https://arxiv.org/abs/2401.15499 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15509
Date: Sat, 27 Jan 2024 21:35:29 GMT   (2520kb,D)

Title: Style-News: Incorporating Stylized News Generation and Adversarial
  Verification for Neural Fake News Detection
Authors: Wei-Yao Wang, Yu-Chieh Chang, Wen-Chih Peng
Categories: cs.CL cs.AI cs.SI
Comments: EACL 2024 Main Track
\\
  With the improvements in generative models, the issues of producing
hallucinations in various domains (e.g., law, writing) have been brought to
people's attention due to concerns about misinformation. In this paper, we
focus on neural fake news, which refers to content generated by neural networks
aiming to mimic the style of real news to deceive people. To prevent harmful
disinformation spreading fallaciously from malicious social media (e.g.,
content farms), we propose a novel verification framework, Style-News, using
publisher metadata to imply a publisher's template with the corresponding text
types, political stance, and credibility. Based on threat modeling aspects, a
style-aware neural news generator is introduced as an adversary for generating
news content conditioning for a specific publisher, and style and source
discriminators are trained to defend against this attack by identifying which
publisher the style corresponds with, and discriminating whether the source of
the given news is human-written or machine-generated. To evaluate the quality
of the generated content, we integrate various dimensional metrics (language
fluency, content preservation, and style adherence) and demonstrate that
Style-News significantly outperforms the previous approaches by a margin of
0.35 for fluency, 15.24 for content, and 0.38 for style at most. Moreover, our
discriminative model outperforms state-of-the-art baselines in terms of
publisher prediction (up to 4.64%) and neural fake news detection (+6.94%
$\sim$ 31.72%).
\\ ( https://arxiv.org/abs/2401.15509 ,  2520kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15532
Date: Sun, 28 Jan 2024 00:41:21 GMT   (59kb,D)

Title: Byte Pair Encoding Is All You Need For Automatic Bengali Speech
  Recognition
Authors: Ahnaf Mozib Samin
Categories: cs.CL cs.SD eess.AS
Comments: Under-review
\\
  Byte pair encoding (BPE) emerges as an effective tokenization method for
tackling the out-of-vocabulary (OOV) challenge in various natural language and
speech processing tasks. Recent research highlights the dependency of BPE
subword tokenization's efficacy on the morphological nature of the language,
particularly in languages rich in inflectional morphology, where fewer BPE
merges suffice for generating highly productive tokens. Motivated by this, our
study empirically identifies the optimal number of BPE tokens for Bengali, a
language known for its morphological complexity, thus enhancing
out-of-distribution automatic speech recognition (ASR) performance.
Experimental evaluation reveals that an excessively high number of BPE tokens
can lead to overfitting, while approximately 500-1000 tokens result in superior
OOV performance. Furthermore, we conduct a comparative analysis of BPE with
character-based and unigram-based tokenization methods. By introducing BPE
tokenization to Bengali ASR, we achieve a substantial reduction in the word
error rate (WER) from 66.44% in our character-based baseline system to 63.80%
on the LB-ASRTD eval set and from 46.34% to 42.80% on the SHRUTI eval set, both
of which include out-of-distribution data.
\\ ( https://arxiv.org/abs/2401.15532 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15535
Date: Sun, 28 Jan 2024 01:07:21 GMT   (280kb,D)

Title: Quantifying Stereotypes in Language
Authors: Yang Liu
Categories: cs.CL
Comments: 15 pages, 9 figures
\\
  A stereotype is a generalized perception of a specific group of humans. It is
often potentially encoded in human language, which is more common in texts on
social issues. Previous works simply define a sentence as stereotypical and
anti-stereotypical. However, the stereotype of a sentence may require
fine-grained quantification. In this paper, to fill this gap, we quantify
stereotypes in language by annotating a dataset. We use the pre-trained
language models (PLMs) to learn this dataset to predict stereotypes of
sentences. Then, we discuss stereotypes about common social issues such as hate
speech, sexism, sentiments, and disadvantaged and advantaged groups. We
demonstrate the connections and differences between stereotypes and common
social issues, and all four studies validate the general findings of the
current studies. In addition, our work suggests that fine-grained stereotype
scores are a highly relevant and competitive dimension for research on social
issues.
\\ ( https://arxiv.org/abs/2401.15535 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15555
Date: Sun, 28 Jan 2024 03:37:11 GMT   (635kb,D)

Title: Augment before You Try: Knowledge-Enhanced Table Question Answering via
  Table Expansion
Authors: Yujian Liu, Jiabao Ji, Tong Yu, Ryan Rossi, Sungchul Kim, Handong
  Zhao, Ritwik Sinha, Yang Zhang, Shiyu Chang
Categories: cs.CL
\\
  Table question answering is a popular task that assesses a model's ability to
understand and interact with structured data. However, the given table often
does not contain sufficient information for answering the question,
necessitating the integration of external knowledge. Existing methods either
convert both the table and external knowledge into text, which neglects the
structured nature of the table; or they embed queries for external sources in
the interaction with the table, which complicates the process. In this paper,
we propose a simple yet effective method to integrate external information in a
given table. Our method first constructs an augmenting table containing the
missing information and then generates a SQL query over the two tables to
answer the question. Experiments show that our method outperforms strong
baselines on three table QA benchmarks. Our code is publicly available at
https://github.com/UCSB-NLP-Chang/Augment_tableQA.
\\ ( https://arxiv.org/abs/2401.15555 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15569
Date: Sun, 28 Jan 2024 05:12:09 GMT   (1242kb)

Title: Efficient Tuning and Inference for Large Language Models on Textual
  Graphs
Authors: Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang
Categories: cs.CL
\\
  Rich textual and topological information of textual graphs need to be modeled
in real-world applications such as webpages, e-commerce, and academic articles.
Practitioners have been long following the path of adopting a shallow text
encoder and a subsequent graph neural network (GNN) to solve this problem. In
light of recent advancements in large language models (LLMs), it is apparent
that integrating LLMs for enhanced textual encoding can substantially improve
the performance of textual graphs. Nevertheless, the efficiency of these
methods poses a significant challenge. In this paper, we propose ENGINE, a
parameter- and memory-efficient fine-tuning method for textual graphs with an
LLM encoder. The key insight is to combine the LLMs and GNNs through a tunable
side structure, which significantly reduces the training complexity without
impairing the joint model's capacity. Extensive experiments on textual graphs
demonstrate our method's effectiveness by achieving the best model performance,
meanwhile having the lowest training cost compared to previous methods.
Moreover, we introduce two variants with caching and dynamic early exit to
further enhance training and inference speed. Specifically, caching accelerates
ENGINE's training by 12x, and dynamic early exit achieves up to 5x faster
inference with a negligible performance drop (at maximum 1.17% relevant drop
across 7 datasets).
\\ ( https://arxiv.org/abs/2401.15569 ,  1242kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15579
Date: Sun, 28 Jan 2024 06:27:17 GMT   (115kb,D)

Title: MunTTS: A Text-to-Speech System for Mundari
Authors: Varun Gumma, Rishav Hada, Aditya Yadavalli, Pamir Gogoi, Ishani
  Mondal, Vivek Seshadri, Kalika Bali
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to ComputEL-7
\\
  We present MunTTS, an end-to-end text-to-speech (TTS) system specifically for
Mundari, a low-resource Indian language of the Austo-Asiatic family. Our work
addresses the gap in linguistic technology for underrepresented languages by
collecting and processing data to build a speech synthesis system. We begin our
study by gathering a substantial dataset of Mundari text and speech and train
end-to-end speech models. We also delve into the methods used for training our
models, ensuring they are efficient and effective despite the data constraints.
We evaluate our system with native speakers and objective metrics,
demonstrating its potential as a tool for preserving and promoting the Mundari
language in the digital age.
\\ ( https://arxiv.org/abs/2401.15579 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15585
Date: Sun, 28 Jan 2024 06:50:10 GMT   (404kb,D)

Title: Evaluating Gender Bias in Large Language Models via Chain-of-Thought
  Prompting
Authors: Masahiro Kaneko, Danushka Bollegala, Naoaki Okazaki, Timothy Baldwin
Categories: cs.CL
\\
  There exist both scalable tasks, like reading comprehension and
fact-checking, where model performance improves with model size, and unscalable
tasks, like arithmetic reasoning and symbolic reasoning, where model
performance does not necessarily improve with model size. Large language models
(LLMs) equipped with Chain-of-Thought (CoT) prompting are able to make accurate
incremental predictions even on unscalable tasks. Unfortunately, despite their
exceptional reasoning abilities, LLMs tend to internalize and reproduce
discriminatory societal biases. Whether CoT can provide discriminatory or
egalitarian rationalizations for the implicit information in unscalable tasks
remains an open question.
  In this study, we examine the impact of LLMs' step-by-step predictions on
gender bias in unscalable tasks. For this purpose, we construct a benchmark for
an unscalable task where the LLM is given a list of words comprising feminine,
masculine, and gendered occupational words, and is required to count the number
of feminine and masculine words. In our CoT prompts, we require the LLM to
explicitly indicate whether each word in the word list is a feminine or
masculine before making the final predictions. With counting and handling the
meaning of words, this benchmark has characteristics of both arithmetic
reasoning and symbolic reasoning. Experimental results in English show that
without step-by-step prediction, most LLMs make socially biased predictions,
despite the task being as simple as counting words. Interestingly, CoT
prompting reduces this unconscious social bias in LLMs and encourages fair
predictions.
\\ ( https://arxiv.org/abs/2401.15585 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15626
Date: Sun, 28 Jan 2024 11:02:23 GMT   (1626kb,D)

Title: TA&AT: Enhancing Task-Oriented Dialog with Turn-Level Auxiliary Tasks
  and Action-Tree Based Scheduled Sampling
Authors: Longxiang Liu, Xiuxing Li, Yang Feng
Categories: cs.CL cs.AI
Comments: Accepted by AAAI 2024
\\
  Task-oriented dialog systems have witnessed substantial progress due to
conversational pre-training techniques. Yet, two significant challenges
persist. First, most systems primarily utilize the latest turn's state label
for the generator. This practice overlooks the comprehensive value of state
labels in boosting the model's understanding for future generations. Second, an
overreliance on generated policy often leads to error accumulation, resulting
in suboptimal responses when adhering to incorrect actions. To combat these
challenges, we propose turn-level multi-task objectives for the encoder. With
the guidance of essential information from labeled intermediate states, we
establish a more robust representation for both understanding and generation.
For the decoder, we introduce an action tree-based scheduled sampling
technique. Specifically, we model the hierarchical policy as trees and utilize
the similarity between trees to sample negative policy based on scheduled
sampling, hoping the model to generate invariant responses under perturbations.
This method simulates potential pitfalls by sampling similar negative policy,
bridging the gap between task-oriented dialog training and inference. Among
methods without continual pre-training, our approach achieved state-of-the-art
(SOTA) performance on the MultiWOZ dataset series and was also competitive with
pre-trained SOTA methods.
\\ ( https://arxiv.org/abs/2401.15626 ,  1626kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15656
Date: Sun, 28 Jan 2024 13:21:44 GMT   (205kb,D)

Title: LLsM: Generative Linguistic Steganography with Large Language Model
Authors: Yihao Wang and Ruiqi Song and Ru Zhang and Jianyi Liu and Lingxiao Li
Categories: cs.CL
Comments: 13 pages
\\
  Linguistic Steganography (LS) tasks aim to generate steganographic texts
(stego) based on secret information. Only authorized recipients can perceive
the existence of secret information in the texts and accurately extract it,
thereby preserving privacy. However, the controllability of the stego generated
by existing schemes is poor, and the generated stego is difficult to contain
specific discourse characteristics such as style, genre, and theme. As a
result, the stego are often easily detectable, compromising covert
communication. To address these problems, this paper proposes a novel scheme
named LLsM, a generative LS based on a Large Language Model (LLM). We
fine-tuned the LLM LLaMA2 with a large-scale constructed dataset encompassing
rich discourse characteristics, which enables the fine-tuned LLM to generate
texts with specific discourse in a controllable manner. Then the discourse
characteristics are used as guiding information and inputted into the
fine-tuned LLM in the form of Prompt together with secret information. The
candidate pool, derived from sampling and truncation, undergoes range encoding
to ensure the stego imitate natural text distribution. Experiments demonstrate
that LLsM performs superior to prevalent baselines regarding text quality,
statistical analysis, discourse matching, and anti-steganalysis. In particular,
LLsM's MAUVE surpasses that of some baselines by 70%-80%, and its
anti-steganalysis performance is 30%-40% higher. Notably, we also present the
long stego generated by LLsM, showing its potential superiority in long LS
tasks.
\\ ( https://arxiv.org/abs/2401.15656 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15670
Date: Sun, 28 Jan 2024 14:32:15 GMT   (1080kb,D)

Title: YODA: Teacher-Student Progressive Learning for Language Models
Authors: Jianqiao Lu, Wanjun Zhong, Yufei Wang, Zhijiang Guo, Qi Zhu, Wenyong
  Huang, Yanlin Wang, Fei Mi, Baojun Wang, Yasheng Wang, Lifeng Shang, Xin
  Jiang, Qun Liu
Categories: cs.CL cs.AI cs.LG
Comments: 14 pages, 4 figures, 3 tables
\\
  Although large language models (LLMs) have demonstrated adeptness in a range
of tasks, they still lag behind human learning efficiency. This disparity is
often linked to the inherent human capacity to learn from basic examples,
gradually generalize and handle more complex problems, and refine their skills
with continuous feedback. Inspired by this, this paper introduces YODA, a novel
teacher-student progressive learning framework that emulates the
teacher-student education process to improve the efficacy of model fine-tuning.
The framework operates on an interactive \textit{basic-generalized-harder}
loop. The teacher agent provides tailored feedback on the student's answers,
and systematically organizes the education process. This process unfolds by
teaching the student basic examples, reinforcing understanding through
generalized questions, and then enhancing learning by posing questions with
progressively enhanced complexity. With the teacher's guidance, the student
learns to iteratively refine its answer with feedback, and forms a robust and
comprehensive understanding of the posed questions. The systematic procedural
data, which reflects the progressive learning process of humans, is then
utilized for model training. Taking math reasoning as a testbed, experiments
show that training LLaMA2 with data from YODA improves SFT with significant
performance gain (+17.01\% on GSM8K and +9.98\% on MATH). In addition, we find
that training with curriculum learning further improves learning robustness.
\\ ( https://arxiv.org/abs/2401.15670 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15717
Date: Sun, 28 Jan 2024 17:51:47 GMT   (6618kb,D)

Title: Check News in One Click: NLP-Empowered Pro-Kremlin Propaganda Detection
Authors: Veronika Solopova, Viktoriia Herman, Christoph Benzm\"uller, Tim
  Landgraf
Categories: cs.CL
Comments: Accepted to EACL 2024 System Demonstrations
\\
  Many European citizens become targets of the Kremlin propaganda campaigns,
aiming to minimise public support for Ukraine, foster a climate of mistrust and
disunity, and shape elections (Meister, 2022). To address this challenge, we
developed ''Check News in 1 Click'', the first NLP-empowered pro-Kremlin
propaganda detection application available in 7 languages, which provides the
lay user with feedback on their news, and explains manipulative linguistic
features and keywords. We conducted a user study, analysed user entries and
models' behaviour paired with questionnaire answers, and investigated the
advantages and disadvantages of the proposed interpretative solution.
\\ ( https://arxiv.org/abs/2401.15717 ,  6618kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15724
Date: Sun, 28 Jan 2024 18:26:31 GMT   (1408kb,D)

Title: RE-GAINS & EnCHANT: Intelligent Tool Manipulation Systems For Enhanced
  Query Responses
Authors: Sahil Girhepuje, Siva Sankar Sajeev, Purvam Jain, Arya Sikder, Adithya
  Rama Varma, Ryan George, Akshay Govind Srinivasan, Mahendra Kurup, Ashmit
  Sinha, Sudip Mondal
Categories: cs.CL
\\
  Despite the remarkable success of LLMs, they still suffer from tool
invocation and tool chaining due to inadequate input queries and/or tool
argument descriptions. We propose two novel frameworks, RE-GAINS and EnCHANT,
enabling LLMs to tackle tool manipulation for solving complex user queries by
making API calls. EnCHANT is an open-source solution that makes use of an LLM
format enforcer, an LLM(OpenChat 3.5) and a retriever(ToolBench's API
Retriever). RE-GAINS is based on OpenAI models and embeddings using a special
prompt based on the RAP paper. Both solutions cost less than $0.01 per query
with minimal latency, therefore showcasing the usefulness of the frameworks.
\\ ( https://arxiv.org/abs/2401.15724 ,  1408kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15770
Date: Sun, 28 Jan 2024 21:18:05 GMT   (7549kb,D)

Title: PILOT: Legal Case Outcome Prediction with Case Law
Authors: Lang Cao, Zifeng Wang, Cao Xiao, Jimeng Sun
Categories: cs.CL
\\
  Machine learning shows promise in predicting the outcome of legal cases, but
most research has concentrated on civil law cases rather than case law systems.
We identified two unique challenges in making legal case outcome predictions
with case law. First, it is crucial to identify relevant precedent cases that
serve as fundamental evidence for judges during decision-making. Second, it is
necessary to consider the evolution of legal principles over time, as early
cases may adhere to different legal contexts.
  In this paper, we proposed a new model named PILOT (PredictIng Legal case
OuTcome) for case outcome prediction. It comprises two modules for relevant
case retrieval and temporal pattern handling, respectively. To benchmark the
performance of existing legal case outcome prediction models, we curated a
dataset from a large-scale case law database. We demonstrate the importance of
accurately identifying precedent cases and mitigating the temporal shift when
making predictions for case law, as our method shows a significant improvement
over the prior methods that focus on civil law case outcome predictions.
\\ ( https://arxiv.org/abs/2401.15770 ,  7549kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15777
Date: Sun, 28 Jan 2024 21:58:04 GMT   (40kb,D)

Title: cantnlp@LT-EDI-2024: Automatic Detection of Anti-LGBTQ+ Hate Speech in
  Under-resourced Languages
Authors: Sidney G.-J. Wong and Matthew Durward
Categories: cs.CL
Comments: Accepted EACL 2024 Workshop LTEDI
\\
  This paper describes our homophobia/transphobia in social media comments
detection system developed as part of the shared task at LT-EDI-2024. We took a
transformer-based approach to develop our multiclass classification model for
ten language conditions (English, Spanish, Gujarati, Hindi, Kannada, Malayalam,
Marathi, Tamil, Tulu, and Telugu). We introduced synthetic and organic
instances of script-switched language data during domain adaptation to mirror
the linguistic realities of social media language as seen in the labelled
training data. Our system ranked second for Gujarati and Telugu with varying
levels of performance for other language conditions. The results suggest
incorporating elements of paralinguistic behaviour such as script-switching may
improve the performance of language detection systems especially in the cases
of under-resourced languages conditions.
\\ ( https://arxiv.org/abs/2401.15777 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15780
Date: Sun, 28 Jan 2024 22:11:25 GMT   (245kb)

Title: Fine-Tuned Large Language Models for Symptom Recognition from Spanish
  Clinical Text
Authors: Mai A. Shaaban, Abbas Akkasi, Adnan Khan, Majid Komeili, Mohammad
  Yaqub
Categories: cs.CL cs.LG
DOI: 10.5281/zenodo.10104138
\\
  The accurate recognition of symptoms in clinical reports is significantly
important in the fields of healthcare and biomedical natural language
processing. These entities serve as essential building blocks for clinical
information extraction, enabling retrieval of critical medical insights from
vast amounts of textual data. Furthermore, the ability to identify and
categorize these entities is fundamental for developing advanced clinical
decision support systems, aiding healthcare professionals in diagnosis and
treatment planning. In this study, we participated in SympTEMIST, a shared task
on the detection of symptoms, signs and findings in Spanish medical documents.
We combine a set of large language models fine-tuned with the data released by
the organizers.
\\ ( https://arxiv.org/abs/2401.15780 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15798
Date: Sun, 28 Jan 2024 23:00:40 GMT   (3416kb,D)

Title: UnMASKed: Quantifying Gender Biases in Masked Language Models through
  Linguistically Informed Job Market Prompts
Authors: I\~nigo Parra
Categories: cs.CL
Comments: EACL 2024 SRW
\\
  Language models (LMs) have become pivotal in the realm of technological
advancements. While their capabilities are vast and transformative, they often
include societal biases encoded in the human-produced datasets used for their
training. This research delves into the inherent biases present in masked
language models (MLMs), with a specific focus on gender biases. This study
evaluated six prominent models: BERT, RoBERTa, DistilBERT, BERT-multilingual,
XLM-RoBERTa, and DistilBERT-multilingual. The methodology employed a novel
dataset, bifurcated into two subsets: one containing prompts that encouraged
models to generate subject pronouns in English, and the other requiring models
to return the probabilities of verbs, adverbs, and adjectives linked to the
prompts' gender pronouns. The analysis reveals stereotypical gender alignment
of all models, with multilingual variants showing comparatively reduced biases.
\\ ( https://arxiv.org/abs/2401.15798 ,  3416kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15840
Date: Mon, 29 Jan 2024 02:28:39 GMT   (546kb,D)

Title: Emergent Explainability: Adding a causal chain to neural network
  inference
Authors: Adam Perrett
Categories: cs.CL
\\
  This position paper presents a theoretical framework for enhancing
explainable artificial intelligence (xAI) through emergent communication
(EmCom), focusing on creating a causal understanding of AI model outputs. We
explore the novel integration of EmCom into AI systems, offering a paradigm
shift from conventional associative relationships between inputs and outputs to
a more nuanced, causal interpretation. The framework aims to revolutionize how
AI processes are understood, making them more transparent and interpretable.
While the initial application of this model is demonstrated on synthetic data,
the implications of this research extend beyond these simple applications. This
general approach has the potential to redefine interactions with AI across
multiple domains, fostering trust and informed decision-making in healthcare
and in various sectors where AI's decision-making processes are critical. The
paper discusses the theoretical underpinnings of this approach, its potential
broad applications, and its alignment with the growing need for responsible and
transparent AI systems in an increasingly digital world.
\\ ( https://arxiv.org/abs/2401.15840 ,  546kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15854
Date: Mon, 29 Jan 2024 03:05:35 GMT   (409kb,D)

Title: LSTM-based Deep Neural Network With A Focus on Sentence Representation
  for Sequential Sentence Classification in Medical Scientific Abstracts
Authors: Phat Lam, Lam Pham, Tin Nguyen, Hieu Tang, Seidl Michael, Alexander
  Schindler
Categories: cs.CL
Comments: 5 pages, 2 figures
\\
  The Sequential Sentence Classification task within the domain of medical
abstracts, termed as SSC, involves the categorization of sentences into
pre-defined headings based on their roles in conveying critical information in
the abstract. In the SSC task, sentences are often sequentially related to each
other. For this reason, the role of sentence embedding is crucial for capturing
both the semantic information between words in the sentence and the contextual
relationship of sentences within the abstract to provide a comprehensive
representation for better classification. In this paper, we present a
hierarchical deep learning model for the SSC task. First, we propose a
LSTM-based network with multiple feature branches to create well-presented
sentence embeddings at the sentence level. To perform the sequence of
sentences, a convolutional-recurrent neural network (C-RNN) at the abstract
level and a multi-layer perception network (MLP) at the segment level are
developed that further enhance the model performance. Additionally, an ablation
study is also conducted to evaluate the contribution of individual component in
the entire network to the model performance at different levels. Our proposed
system is very competitive to the state-of-the-art systems and further improve
F1 scores of the baseline by 1.0%, 2.8%, and 2.6% on the benchmark datasets
PudMed 200K RCT, PudMed 20K RCT and NICTA-PIBOSO, respectively.
\\ ( https://arxiv.org/abs/2401.15854 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15861
Date: Mon, 29 Jan 2024 03:25:11 GMT   (63kb,D)

Title: DrBERT: Unveiling the Potential of Masked Language Modeling Decoder in
  BERT pretraining
Authors: Wen Liang, Youzhi Liang
Categories: cs.CL cs.AI
\\
  BERT (Bidirectional Encoder Representations from Transformers) has
revolutionized the field of natural language processing through its exceptional
performance on numerous tasks. Yet, the majority of researchers have mainly
concentrated on enhancements related to the model structure, such as relative
position embedding and more efficient attention mechanisms. Others have delved
into pretraining tricks associated with Masked Language Modeling, including
whole word masking. DeBERTa introduced an enhanced decoder adapted for BERT's
encoder model for pretraining, proving to be highly effective. We argue that
the design and research around enhanced masked language modeling decoders have
been underappreciated. In this paper, we propose several designs of enhanced
decoders and introduce DrBERT (Decoder-refined BERT), a novel method for
modeling training. Typically, a pretrained BERT model is fine-tuned for
specific Natural Language Understanding (NLU) tasks. In our approach, we
utilize the original BERT model as the encoder, making only changes to the
decoder without altering the encoder. This approach does not necessitate
extensive modifications to the model's architecture and can be seamlessly
integrated into existing fine-tuning pipelines and services, offering an
efficient and effective enhancement strategy. Compared to other methods, while
we also incur a moderate training cost for the decoder during the pretraining
process, our approach does not introduce additional training costs during the
fine-tuning phase. We test multiple enhanced decoder structures after
pretraining and evaluate their performance on the GLUE benchmark. Our results
demonstrate that DrBERT, having only undergone subtle refinements to the model
structure during pretraining, significantly enhances model performance without
escalating the inference time and serving budget.
\\ ( https://arxiv.org/abs/2401.15861 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15884
Date: Mon, 29 Jan 2024 04:36:39 GMT   (315kb,D)

Title: Corrective Retrieval Augmented Generation
Authors: Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, Zhen-Hua Ling
Categories: cs.CL
\\
  Large language models (LLMs) inevitably exhibit hallucinations since the
accuracy of generated texts cannot be secured solely by the parametric
knowledge they encapsulate. Although retrieval-augmented generation (RAG) is a
practicable complement to LLMs, it relies heavily on the relevance of retrieved
documents, raising concerns about how the model behaves if retrieval goes
wrong. To this end, we propose the Corrective Retrieval Augmented Generation
(CRAG) to improve the robustness of generation. Specifically, a lightweight
retrieval evaluator is designed to assess the overall quality of retrieved
documents for a query, returning a confidence degree based on which different
knowledge retrieval actions can be triggered. Since retrieval from static and
limited corpora can only return sub-optimal documents, large-scale web searches
are utilized as an extension for augmenting the retrieval results. Besides, a
decompose-then-recompose algorithm is designed for retrieved documents to
selectively focus on key information and filter out irrelevant information in
them. CRAG is plug-and-play and can be seamlessly coupled with various
RAG-based approaches. Experiments on four datasets covering short- and
long-form generation tasks show that CRAG can significantly improve the
performance of RAG-based approaches.
\\ ( https://arxiv.org/abs/2401.15884 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15927
Date: Mon, 29 Jan 2024 07:34:37 GMT   (1674kb,D)

Title: E-EVAL: A Comprehensive Chinese K-12 Education Evaluation Benchmark for
  Large Language Models
Authors: Jinchang Hou, Chang Ao, Haihong Wu, Xiangtao Kong, Zhigang Zheng,
  Daijia Tang, Chengming Li, Xiping Hu, Ruifeng Xu, Shiwen Ni, Min Yang
Categories: cs.CL
\\
  With the accelerating development of Large Language Models (LLMs), many LLMs
are beginning to be used in the Chinese K-12 education domain. The integration
of LLMs and education is getting closer and closer, however, there is currently
no benchmark for evaluating LLMs that focuses on the Chinese K-12 education
domain. Therefore, there is an urgent need for a comprehensive natural language
processing benchmark to accurately assess the capabilities of various LLMs in
the Chinese K-12 education domain. To address this, we introduce the E-EVAL,
the first comprehensive evaluation benchmark specifically designed for the
Chinese K-12 education field. The E-EVAL consists of 4,351 multiple-choice
questions at the primary, middle, and high school levels across a wide range of
subjects, including Chinese, English, Politics, History, Ethics, Physics,
Chemistry, Mathematics, and Geography. We conducted a comprehensive evaluation
of E-EVAL on advanced LLMs, including both English-dominant and
Chinese-dominant models. Findings show that Chinese-dominant models perform
well compared to English-dominant models, with many scoring even above the GPT
4.0. However, almost all models perform poorly in complex subjects such as
mathematics. We also found that most Chinese-dominant LLMs did not achieve
higher scores at the primary school level compared to the middle school level.
We observe that the mastery of higher-order knowledge by the model does not
necessarily imply the mastery of lower-order knowledge as well. Additionally,
the experimental results indicate that the Chain of Thought (CoT) technique is
effective only for the challenging science subjects, while Few-shot prompting
is more beneficial for liberal arts subjects. With E-EVAL, we aim to analyze
the strengths and limitations of LLMs in educational applications, and to
contribute to the progress and development of Chinese K-12 education and LLMs.
\\ ( https://arxiv.org/abs/2401.15927 ,  1674kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15966
Date: Mon, 29 Jan 2024 08:53:41 GMT   (736kb)

Title: Response Generation for Cognitive Behavioral Therapy with Large Language
  Models: Comparative Study with Socratic Questioning
Authors: Kenta Izumi, Hiroki Tanaka, Kazuhiro Shidara, Hiroyoshi Adachi,
  Daisuke Kanayama, Takashi Kudo, and Satoshi Nakamura
Categories: cs.CL cs.AI
Comments: Accepted by IWSDS2024
\\
  Dialogue systems controlled by predefined or rule-based scenarios derived
from counseling techniques, such as cognitive behavioral therapy (CBT), play an
important role in mental health apps. Despite the need for responsible
responses, it is conceivable that using the newly emerging LLMs to generate
contextually relevant utterances will enhance these apps. In this study, we
construct dialogue modules based on a CBT scenario focused on conventional
Socratic questioning using two kinds of LLMs: a Transformer-based dialogue
model further trained with a social media empathetic counseling dataset,
provided by Osaka Prefecture (OsakaED), and GPT-4, a state-of-the art LLM
created by OpenAI. By comparing systems that use LLM-generated responses with
those that do not, we investigate the impact of generated responses on
subjective evaluations such as mood change, cognitive change, and dialogue
quality (e.g., empathy). As a result, no notable improvements are observed when
using the OsakaED model. When using GPT-4, the amount of mood change, empathy,
and other dialogue qualities improve significantly. Results suggest that GPT-4
possesses a high counseling ability. However, they also indicate that even when
using a dialogue model trained with a human counseling dataset, it does not
necessarily yield better outcomes compared to scenario-based dialogues. While
presenting LLM-generated responses, including GPT-4, and having them interact
directly with users in real-life mental health care services may raise ethical
issues, it is still possible for human professionals to produce example
responses or response templates using LLMs in advance in systems that use
rules, scenarios, or example responses.
\\ ( https://arxiv.org/abs/2401.15966 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16012
Date: Mon, 29 Jan 2024 10:00:54 GMT   (1794kb,D)

Title: Finding Challenging Metaphors that Confuse Pretrained Language Models
Authors: Yucheng Li, Frank Guerin, Chenghua Lin
Categories: cs.CL
\\
  Metaphors are considered to pose challenges for a wide spectrum of NLP tasks.
This gives rise to the area of computational metaphor processing. However, it
remains unclear what types of metaphors challenge current state-of-the-art
models. In this paper, we test various NLP models on the VUA metaphor dataset
and quantify to what extent metaphors affect models' performance on various
downstream tasks. Analysis reveals that VUA includes a large number of
metaphors that pose little difficulty to downstream tasks. We would like to
shift the attention of researchers away from these metaphors to instead focus
on challenging metaphors. To identify hard metaphors, we propose an automatic
pipeline that identifies metaphors that challenge a particular model. Our
analysis demonstrates that our detected hard metaphors contrast significantly
with VUA and reduce the accuracy of machine translation by 16\%, QA performance
by 4\%, NLI by 7\%, and metaphor identification recall by over 14\% for various
popular NLP systems.
\\ ( https://arxiv.org/abs/2401.16012 ,  1794kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16055
Date: Mon, 29 Jan 2024 11:04:01 GMT   (52kb,D)

Title: Stolen Subwords: Importance of Vocabularies for Machine Translation
  Model Stealing
Authors: Vil\'em Zouhar
Categories: cs.CL
\\
  In learning-based functionality stealing, the attacker is trying to build a
local model based on the victim's outputs. The attacker has to make choices
regarding the local model's architecture, optimization method and, specifically
for NLP models, subword vocabulary, such as BPE. On the machine translation
task, we explore (1) whether the choice of the vocabulary plays a role in model
stealing scenarios and (2) if it is possible to extract the victim's
vocabulary. We find that the vocabulary itself does not have a large effect on
the local model's performance. Given gray-box model access, it is possible to
collect the victim's vocabulary by collecting the outputs (detokenized subwords
on the output). The results of the minimum effect of vocabulary choice are
important more broadly for black-box knowledge distillation.
\\ ( https://arxiv.org/abs/2401.16055 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16078
Date: Mon, 29 Jan 2024 11:39:46 GMT   (4203kb,D)

Title: Understanding the effects of word-level linguistic annotations in
  under-resourced neural machine translation
Authors: V\'ictor M. S\'anchez-Cartagena, Juan Antonio P\'erez-Ortiz, Felipe
  S\'anchez-Mart\'inez
Categories: cs.CL
Journal-ref: Understanding the effects of word-level linguistic annotations in
  under-resourced neural machine translation (S\'anchez-Cartagena et al.,
  COLING 2020)
DOI: 10.18653/v1/2020.coling-main.349
\\
  This paper studies the effects of word-level linguistic annotations in
under-resourced neural machine translation, for which there is incomplete
evidence in the literature. The study covers eight language pairs, different
training corpus sizes, two architectures, and three types of annotation: dummy
tags (with no linguistic information at all), part-of-speech tags, and
morpho-syntactic description tags, which consist of part of speech and
morphological features. These linguistic annotations are interleaved in the
input or output streams as a single tag placed before each word. In order to
measure the performance under each scenario, we use automatic evaluation
metrics and perform automatic error classification. Our experiments show that,
in general, source-language annotations are helpful and morpho-syntactic
descriptions outperform part of speech for some language pairs. On the
contrary, when words are annotated in the target language, part-of-speech tags
systematically outperform morpho-syntactic description tags in terms of
automatic evaluation metrics, even though the use of morpho-syntactic
description tags improves the grammaticality of the output. We provide a
detailed analysis of the reasons behind this result.
\\ ( https://arxiv.org/abs/2401.16078 ,  4203kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16086
Date: Mon, 29 Jan 2024 11:52:45 GMT   (17810kb,D)

Title: Non-Fluent Synthetic Target-Language Data Improve Neural Machine
  Translation
Authors: V\'ictor M. S\'anchez-Cartagena, Miquel Espl\`a-Gomis, Juan Antonio
  P\'erez-Ortiz, Felipe S\'anchez-Mart\'inez
Categories: cs.CL
Comments: arXiv admin note: text overlap with arXiv:2109.03645
Journal-ref: IEEE Transactions on Pattern Analysis and Machine Intelligence (
  Volume: 46, Issue: 2, February 2024)
DOI: 10.1109/TPAMI.2023.3333949
\\
  When the amount of parallel sentences available to train a neural machine
translation is scarce, a common practice is to generate new synthetic training
samples from them. A number of approaches have been proposed to produce
synthetic parallel sentences that are similar to those in the parallel data
available. These approaches work under the assumption that non-fluent
target-side synthetic training samples can be harmful and may deteriorate
translation performance. Even so, in this paper we demonstrate that synthetic
training samples with non-fluent target sentences can improve translation
performance if they are used in a multilingual machine translation framework as
if they were sentences in another language. We conducted experiments on ten
low-resource and four high-resource translation tasks and found out that this
simple approach consistently improves translation performance as compared to
state-of-the-art methods for generating synthetic training samples similar to
those found in corpora. Furthermore, this improvement is independent of the
size of the original training corpus, the resulting systems are much more
robust against domain shift and produce less hallucinations.
\\ ( https://arxiv.org/abs/2401.16086 ,  17810kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16092
Date: Mon, 29 Jan 2024 12:02:28 GMT   (19141kb,D)

Title: Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and
  Prompt Engineering May Not Help You
Authors: Felix Friedrich, Katharina H\"ammerl, Patrick Schramowski, Jindrich
  Libovicky, Kristian Kersting, Alexander Fraser
Categories: cs.CL cs.CY cs.LG
\\
  Text-to-image generation models have recently achieved astonishing results in
image quality, flexibility, and text alignment and are consequently employed in
a fast-growing number of applications. Through improvements in multilingual
abilities, a larger community now has access to this kind of technology. Yet,
as we will show, multilingual models suffer similarly from (gender) biases as
monolingual models. Furthermore, the natural expectation is that these models
will provide similar results across languages, but this is not the case and
there are important differences between languages. Thus, we propose a novel
benchmark MAGBIG intending to foster research in multilingual models without
gender bias. We investigate whether multilingual T2I models magnify gender bias
with MAGBIG. To this end, we use multilingual prompts requesting portrait
images of persons of a certain occupation or trait (using adjectives). Our
results show not only that models deviate from the normative assumption that
each gender should be equally likely to be generated, but that there are also
big differences across languages. Furthermore, we investigate prompt
engineering strategies, i.e. the use of indirect, neutral formulations, as a
possible remedy for these biases. Unfortunately, they help only to a limited
extent and result in worse text-to-image alignment. Consequently, this work
calls for more research into diverse representations across languages in image
generators.
\\ ( https://arxiv.org/abs/2401.16092 ,  19141kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16107
Date: Mon, 29 Jan 2024 12:25:30 GMT   (291kb,D)

Title: Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation
  for Automatic Diagnosis
Authors: Haochun Wang, Sendong Zhao, Zewen Qiang, Nuwa Xi, Bing Qin, Ting Liu
Categories: cs.CL cs.AI
\\
  Automatic diagnosis is a significant application of AI in healthcare, where
diagnoses are generated based on the symptom description of patients. Previous
works have approached this task directly by modeling the relationship between
the normalized symptoms and all possible diseases. However, in the clinical
diagnostic process, patients are initially consulted by a general practitioner
and, if necessary, referred to specialists in specific domains for a more
comprehensive evaluation. The final diagnosis often emerges from a
collaborative consultation among medical specialist groups. Recently, large
language models have shown impressive capabilities in natural language
understanding. In this study, we adopt tuning-free LLM-based agents as medical
practitioners and propose the Agent-derived Multi-Specialist Consultation
(AMSC) framework to model the diagnosis process in the real world by adaptively
fusing probability distributions of agents over potential diseases.
Experimental results demonstrate the superiority of our approach compared with
baselines. Notably, our approach requires significantly less parameter updating
and training time, enhancing efficiency and practical utility. Furthermore, we
delve into a novel perspective on the role of implicit symptoms within the
context of automatic diagnosis.
\\ ( https://arxiv.org/abs/2401.16107 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16158
Date: Mon, 29 Jan 2024 13:46:37 GMT   (21858kb,D)

Title: Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual
  Perception
Authors: Junyang Wang and Haiyang Xu and Jiabo Ye and Ming Yan and Weizhou Shen
  and Ji Zhang and Fei Huang and Jitao Sang
Categories: cs.CL cs.CV
Comments: 13 pages, 14 figures
\\
  Mobile device agent based on Multimodal Large Language Models (MLLM) is
becoming a popular application. In this paper, we introduce Mobile-Agent, an
autonomous multi-modal mobile device agent. Mobile-Agent first leverages visual
perception tools to accurately identify and locate both the visual and textual
elements within the app's front-end interface. Based on the perceived vision
context, it then autonomously plans and decomposes the complex operation task,
and navigates the mobile Apps through operations step by step. Different from
previous solutions that rely on XML files of Apps or mobile system metadata,
Mobile-Agent allows for greater adaptability across diverse mobile operating
environments in a vision-centric way, thereby eliminating the necessity for
system-specific customizations. To assess the performance of Mobile-Agent, we
introduced Mobile-Eval, a benchmark for evaluating mobile device operations.
Based on Mobile-Eval, we conducted a comprehensive evaluation of Mobile-Agent.
The experimental results indicate that Mobile-Agent achieved remarkable
accuracy and completion rates. Even with challenging instructions, such as
multi-app operations, Mobile-Agent can still complete the requirements. Code
and model will be open-sourced at https://github.com/X-PLUG/MobileAgent.
\\ ( https://arxiv.org/abs/2401.16158 ,  21858kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16182
Date: Mon, 29 Jan 2024 14:23:51 GMT   (25054kb,D)

Title: LLaMandement: Large Language Models for Summarization of French
  Legislative Proposals
Authors: Joseph Gesnouin, Yannis Tannier, Christophe Gomes Da Silva, Hatim
  Tapory, Camille Brier, Hugo Simon, Raphael Rozenberg, Hermann Woehrel, Mehdi
  El Yakaabi, Thomas Binder, Guillaume Marie, Emilie Caron, Mathile Nogueira,
  Thomas Fontas, Laure Puydebois, Marie Theophile, Stephane Morandi, Mael
  Petit, David Creissac, Pauline Ennouchy, Elise Valetoux, Celine Visade,
  Severine Balloux, Emmanuel Cortes, Pierre-Etienne Devineau, Ulrich Tan,
  Esther Mac Namara, Su Yang
Categories: cs.CL cs.AI
Comments: 21 pages, 9 figures
\\
  This report introduces LLaMandement, a state-of-the-art Large Language Model,
fine-tuned by the French government and designed to enhance the efficiency and
efficacy of processing parliamentary sessions (including the production of
bench memoranda and documents required for interministerial meetings) by
generating neutral summaries of legislative proposals. Addressing the
administrative challenges of manually processing a growing volume of
legislative amendments, LLaMandement stands as a significant legal
technological milestone, providing a solution that exceeds the scalability of
traditional human efforts while matching the robustness of a specialized legal
drafter. We release all our fine-tuned models and training data to the
community.
\\ ( https://arxiv.org/abs/2401.16182 ,  25054kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16184
Date: Mon, 29 Jan 2024 14:29:48 GMT   (4343kb,D)

Title: On the Semantics of LM Latent Space: A Vocabulary-defined Approach
Authors: Jian Gu, Chunyang Chen, Aldeida Aleti
Categories: cs.CL cs.LG
Comments: under peer review
\\
  In the realm of deep learning, understanding the latent space of language
models (LMs) like transformers is crucial for refining their performance and
interpretability. However, existing analyses often fall short in providing
absolute and model-centric insights into LM semantics, and neglect essential
aspects of LM adaption. In response, we introduce a pioneering method called
vocabulary-defined semantics, which establishes a fixed reference frame within
the LM latent space, ensuring absolute semantic analysis grounded in LM
vocabulary. Our approach transcends prior relative analyses, leveraging LM
vocabulary for model-centric insights. Furthermore, we propose a novel
technique to compute logits, emphasizing differentiability and local isotropy,
and introduce a neural clustering module for semantically calibrating data
representations during LM adaptation. Through extensive experiments across
diverse text understanding datasets, our approach surpasses state-of-the-art
methods of retrieval-augmented generation and parameters-efficient finetuning,
showcasing its efficacy and broad applicability. Our findings not only shed
light on LM mechanics but also offer practical solutions for enhancing LM
performance and interpretability.
\\ ( https://arxiv.org/abs/2401.16184 ,  4343kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16209
Date: Mon, 29 Jan 2024 15:02:24 GMT   (11261kb,D)

Title: MultiMUC: Multilingual Template Filling on MUC-4
Authors: William Gantt, Shabnam Behzad, Hannah YoungEun An, Yunmo Chen, Aaron
  Steven White, Benjamin Van Durme, Mahsa Yarmohammadi
Categories: cs.CL cs.AI
Comments: EACL 2024
\\
  We introduce MultiMUC, the first multilingual parallel corpus for template
filling, comprising translations of the classic MUC-4 template filling
benchmark into five languages: Arabic, Chinese, Farsi, Korean, and Russian. We
obtain automatic translations from a strong multilingual machine translation
system and manually project the original English annotations into each target
language. For all languages, we also provide human translations for sentences
in the dev and test splits that contain annotated template arguments. Finally,
we present baselines on MultiMUC both with state-of-the-art template filling
models and with ChatGPT.
\\ ( https://arxiv.org/abs/2401.16209 ,  11261kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16240
Date: Mon, 29 Jan 2024 15:42:57 GMT   (8282kb,D)

Title: Clinically meaningful timeline summarisation in social media for mental
  health monitoring
Authors: Jiayu Song, Jenny Chim, Adam Tsakalidis, Julia Ive, Dana Atzil-Slonim,
  Maria Liakata
Categories: cs.CL cs.AI
\\
  We introduce the new task of clinically meaningful summarisation of social
media user timelines, appropriate for mental health monitoring. We develop a
novel approach for unsupervised abstractive summarisation that produces a
two-layer summary consisting of both high-level information, covering aspects
useful to clinical experts, as well as accompanying time sensitive evidence
from a user's social media timeline. A key methodological novelty comes from
the timeline summarisation component based on a version of hierarchical
variational autoencoder (VAE) adapted to represent long texts and guided by
LLM-annotated key phrases. The resulting timeline summary is input into a LLM
(LLaMA-2) to produce the final summary containing both the high level
information, obtained through instruction prompting, as well as corresponding
evidence from the user's timeline. We assess the summaries generated by our
novel architecture via automatic evaluation against expert written summaries
and via human evaluation with clinical experts, showing that timeline
summarisation by TH-VAE results in logically coherent summaries rich in
clinical utility and superior to LLM-only approaches in capturing changes over
time.
\\ ( https://arxiv.org/abs/2401.16240 ,  8282kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16247
Date: Mon, 29 Jan 2024 15:49:40 GMT   (75kb,D)

Title: Towards Red Teaming in Multimodal and Multilingual Translation
Authors: Christophe Ropers, David Dale, Prangthip Hansanti, Gabriel Mejia
  Gonzalez, Ivan Evtimov, Corinne Wong, Christophe Touret, Kristina Pereyra,
  Seohyun Sonia Kim, Cristian Canton Ferrer, Pierre Andrews and Marta R.
  Costa-juss\`a
Categories: cs.CL cs.CY
Comments: arXiv admin note: substantial text overlap with arXiv:2312.05187
ACM-class: I.2.7
\\
  Assessing performance in Natural Language Processing is becoming increasingly
complex. One particular challenge is the potential for evaluation datasets to
overlap with training data, either directly or indirectly, which can lead to
skewed results and overestimation of model performance. As a consequence, human
evaluation is gaining increasing interest as a means to assess the performance
and reliability of models. One such method is the red teaming approach, which
aims to generate edge cases where a model will produce critical errors. While
this methodology is becoming standard practice for generative AI, its
application to the realm of conditional AI remains largely unexplored. This
paper presents the first study on human-based red teaming for Machine
Translation (MT), marking a significant step towards understanding and
improving the performance of translation models. We delve into both human-based
red teaming and a study on automation, reporting lessons learned and providing
recommendations for both translation models and red teaming drills. This
pioneering work opens up new avenues for research and development in the field
of MT.
\\ ( https://arxiv.org/abs/2401.16247 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16265
Date: Mon, 29 Jan 2024 16:12:31 GMT   (420kb,D)

Title: CO2: Efficient Distributed Training with Full Communication-Computation
  Overlap
Authors: Weigao Sun, Zhen Qin, Weixuan Sun, Shidi Li, Dong Li, Xuyang Shen, Yu
  Qiao, Yiran Zhong
Categories: cs.CL cs.DC
Comments: ICLR 2024 Spotlight. Yiran Zhong is the corresponding author. Code is
  available at: https://github.com/OpenNLPLab/CO2
\\
  The fundamental success of large language models hinges upon the efficacious
implementation of large-scale distributed training techniques. Nevertheless,
building a vast, high-performance cluster featuring high-speed communication
interconnectivity is prohibitively costly, and accessible only to prominent
entities. In this work, we aim to lower this barrier and democratize
large-scale training with limited bandwidth clusters. We propose a new approach
called CO2 that introduces local-updating and asynchronous communication to the
distributed data-parallel training, thereby facilitating the full overlap of
COmunication with COmputation. CO2 is able to attain a high scalability even on
extensive multi-node clusters constrained by very limited communication
bandwidth. We further propose the staleness gap penalty and outer momentum
clipping techniques together with CO2 to bolster its convergence and training
stability. Besides, CO2 exhibits seamless integration with well-established
ZeRO-series optimizers which mitigate memory consumption of model states with
large model training. We also provide a mathematical proof of convergence,
accompanied by the establishment of a stringent upper bound. Furthermore, we
validate our findings through an extensive set of practical experiments
encompassing a wide range of tasks in the fields of computer vision and natural
language processing. These experiments serve to demonstrate the capabilities of
CO2 in terms of convergence, generalization, and scalability when deployed
across configurations comprising up to 128 A100 GPUs. The outcomes emphasize
the outstanding capacity of CO2 to hugely improve scalability, no matter on
clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.
\\ ( https://arxiv.org/abs/2401.16265 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16282
Date: Mon, 29 Jan 2024 16:39:39 GMT   (1001kb,D)

Title: MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim
  Verification
Authors: Xia Zeng, Arkaitz Zubiaga
Categories: cs.CL cs.AI cs.LG
Comments: accepted by EACL Findings 2024
\\
  Claim verification is an essential step in the automated fact-checking
pipeline which assesses the veracity of a claim against a piece of evidence. In
this work, we explore the potential of few-shot claim verification, where only
very limited data is available for supervision. We propose MAPLE (Micro
Analysis of Pairwise Language Evolution), a pioneering approach that explores
the alignment between a claim and its evidence with a small seq2seq model and a
novel semantic measure. Its innovative utilization of micro language evolution
path leverages unlabelled pairwise data to facilitate claim verification while
imposing low demand on data annotations and computing resources. MAPLE
demonstrates significant performance improvements over SOTA baselines SEED, PET
and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and
SciFact. Data and code are available here: https://github.com/XiaZeng0223/MAPLE
\\ ( https://arxiv.org/abs/2401.16282 ,  1001kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16285
Date: Mon, 29 Jan 2024 16:42:34 GMT   (817kb,D)

Title: Capturing Pertinent Symbolic Features for Enhanced Content-Based
  Misinformation Detection
Authors: Flavio Merenda and Jos\'e Manuel G\'omez-P\'erez
Categories: cs.CL cs.AI
Comments: Accepted at K-CAP'23: The 12th Knowledge Capture Conference
DOI: 10.1145/3587259.3627566
\\
  Preventing the spread of misinformation is challenging. The detection of
misleading content presents a significant hurdle due to its extreme linguistic
and domain variability. Content-based models have managed to identify deceptive
language by learning representations from textual data such as social media
posts and web articles. However, aggregating representative samples of this
heterogeneous phenomenon and implementing effective real-world applications is
still elusive. Based on analytical work on the language of misinformation, this
paper analyzes the linguistic attributes that characterize this phenomenon and
how representative of such features some of the most popular misinformation
datasets are. We demonstrate that the appropriate use of pertinent symbolic
knowledge in combination with neural language models is helpful in detecting
misleading content. Our results achieve state-of-the-art performance in
misinformation datasets across the board, showing that our approach offers a
valid and robust alternative to multi-task transfer learning without requiring
any additional training data. Furthermore, our results show evidence that
structured knowledge can provide the extra boost required to address a complex
and unpredictable real-world problem like misinformation detection, not only in
terms of accuracy but also time efficiency and resource utilization.
\\ ( https://arxiv.org/abs/2401.16285 ,  817kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16293
Date: Mon, 29 Jan 2024 16:50:56 GMT   (1038kb,D)

Title: Textual Entailment for Effective Triple Validation in Object Prediction
Authors: Andr\'es Garc\'ia-Silva, Cristian Berr\'io, Jos\'e Manuel
  G\'omez-P\'erez
Categories: cs.CL cs.AI cs.DL
Comments: Accepted to ISWC'23 - The International Semantic Web Conference
DOI: 10.1007/978-3-031-47240-4_5
\\
  Knowledge base population seeks to expand knowledge graphs with facts that
are typically extracted from a text corpus. Recently, language models
pretrained on large corpora have been shown to contain factual knowledge that
can be retrieved using cloze-style strategies. Such approach enables zero-shot
recall of facts, showing competitive results in object prediction compared to
supervised baselines. However, prompt-based fact retrieval can be brittle and
heavily depend on the prompts and context used, which may produce results that
are unintended or hallucinatory.We propose to use textual entailment to
validate facts extracted from language models through cloze statements. Our
results show that triple validation based on textual entailment improves
language model predictions in different training regimes. Furthermore, we show
that entailment-based triple validation is also effective to validate candidate
facts extracted from other sources including existing knowledge graphs and text
passages where named entities are recognized.
\\ ( https://arxiv.org/abs/2401.16293 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16313
Date: Mon, 29 Jan 2024 17:17:42 GMT   (8259kb,D)

Title: Machine Translation Meta Evaluation through Translation Accuracy
  Challenge Sets
Authors: Nikita Moghe, Arnisa Fazla, Chantal Amrhein, Tom Kocmi, Mark Steedman,
  Alexandra Birch, Rico Sennrich, Liane Guillou
Categories: cs.CL
Comments: arXiv admin note: substantial text overlap with arXiv:2210.15615
\\
  Recent machine translation (MT) metrics calibrate their effectiveness by
correlating with human judgement but without any insights about their behaviour
across different error types. Challenge sets are used to probe specific
dimensions of metric behaviour but there are very few such datasets and they
either focus on a limited number of phenomena or a limited number of language
pairs. We introduce ACES, a contrastive challenge set spanning 146 language
pairs, aimed at discovering whether metrics can identify 68 translation
accuracy errors. These phenomena range from simple alterations at the
word/character level to more complex errors based on discourse and real-world
knowledge. We conduct a large-scale study by benchmarking ACES on 50 metrics
submitted to the WMT 2022 and 2023 metrics shared tasks. We benchmark metric
performance, assess their incremental performance over successive campaigns,
and measure their sensitivity to a range of linguistic phenomena. We also
investigate claims that Large Language Models (LLMs) are effective as MT
evaluators by evaluating on ACES. Our results demonstrate that different metric
families struggle with different phenomena and that LLM-based methods fail to
demonstrate reliable performance. Our analyses indicate that most metrics
ignore the source sentence, tend to prefer surface-level overlap and end up
incorporating properties of base models which are not always beneficial. We
expand ACES to include error span annotations, denoted as SPAN-ACES and we use
this dataset to evaluate span-based error metrics showing these metrics also
need considerable improvement. Finally, we provide a set of recommendations for
building better MT metrics, including focusing on error labels instead of
scores, ensembling, designing strategies to explicitly focus on the source
sentence, focusing on semantic content and choosing the right base model for
representations.
\\ ( https://arxiv.org/abs/2401.16313 ,  8259kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16332
Date: Mon, 29 Jan 2024 17:38:14 GMT   (1135kb,D)

Title: Tradeoffs Between Alignment and Helpfulness in Language Models
Authors: Yotam Wolf, Noam Wies, Dorin Shteyman, Binyamin Rothberg, Yoav Levine,
  and Amnon Shashua
Categories: cs.CL cs.AI
\\
  Language model alignment has become an important component of AI safety,
allowing safe interactions between humans and language models, by enhancing
desired behaviors and inhibiting undesired ones. It is often done by tuning the
model or inserting preset aligning prompts. Recently, representation
engineering, a method which alters the model's behavior via changing its
representations post-training, was shown to be effective in aligning LLMs (Zou
et al., 2023a). Representation engineering yields gains in alignment oriented
tasks such as resistance to adversarial attacks and reduction of social biases,
but was also shown to cause a decrease in the ability of the model to perform
basic tasks. In this paper we study the tradeoff between the increase in
alignment and decrease in helpfulness of the model. We propose a theoretical
framework which provides bounds for these two quantities, and demonstrate their
relevance empirically. Interestingly, we find that while the helpfulness
generally decreases, it does so quadratically with the norm of the
representation engineering vector, while the alignment increases linearly with
it, indicating a regime in which it is efficient to use representation
engineering. We validate our findings empirically, and chart the boundaries to
the usefulness of representation engineering for alignment.
\\ ( https://arxiv.org/abs/2401.16332 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16348
Date: Mon, 29 Jan 2024 17:54:04 GMT   (5420kb,D)

Title: Beyond Automated Evaluation Metrics: Evaluating Topic Models On
  Practical Social Science Content Analysis Tasks
Authors: Zongxia Li, Andrew Mao, Daniel Stephens, Pranav Goel, Emily Walpole,
  Alden Dima, Juan Fung, Jordan Boyd-Graber
Categories: cs.CL cs.CY cs.HC
Comments: 19 pages, 5 tables, 6 figures, Accepted to EACL Main Conference 2024
\\
  Topic models are a popular tool for understanding text collections, but their
evaluation has been a point of contention. Automated evaluation metrics such as
coherence are often used, however, their validity has been questioned for
neural topic models (NTMs) and can overlook the benefits of a model in real
world applications. To this end, we conduct the first evaluation of neural,
supervised and classical topic models in an interactive task based setting. We
combine topic models with a classifier and test their ability to help humans
conduct content analysis and document annotation. From simulated, real user and
expert pilot studies, the Contextual Neural Topic Model does the best on
cluster evaluation metrics and human evaluations; however, LDA is competitive
with two other NTMs under our simulated experiment and user study results,
contrary to what coherence scores suggest. We show that current automated
metrics do not provide a complete picture of topic modeling capabilities, but
the right choice of NTMs can be better than classical models on practical
tasks.
\\ ( https://arxiv.org/abs/2401.16348 ,  5420kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16349
Date: Mon, 29 Jan 2024 17:55:18 GMT   (269kb,D)

Title: ConFit: Improving Resume-Job Matching using Data Augmentation and
  Contrastive Learning
Authors: Xiao Yu, Jinzhong Zhang, Zhou Yu
Categories: cs.CL cs.CY
Comments: working progress
\\
  A reliable resume-job matching system helps a company find suitable
candidates from a pool of resumes, and helps a job seeker find relevant jobs
from a list of job posts. However, since job seekers apply only to a few jobs,
interaction records in resume-job datasets are sparse. Different from many
prior work that use complex modeling techniques, we tackle this sparsity
problem using data augmentations and a simple contrastive learning approach.
ConFit first creates an augmented resume-job dataset by paraphrasing specific
sections in a resume or a job post. Then, ConFit uses contrastive learning to
further increase training samples from $B$ pairs per batch to $O(B^2)$ per
batch. We evaluate ConFit on two real-world datasets and find it outperforms
prior methods (including BM25 and OpenAI text-ada-002) by up to 19% and 31%
absolute in nDCG@10 for ranking jobs and ranking resumes, respectively.
\\ ( https://arxiv.org/abs/2401.16349 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16380
Date: Mon, 29 Jan 2024 18:19:08 GMT   (523kb,D)

Title: Rephrasing the Web: A Recipe for Compute and Data-Efficient Language
  Modeling
Authors: Pratyush Maini, Skyler Seto, He Bai, David Grangier, Yizhe Zhang,
  Navdeep Jaitly
Categories: cs.CL
\\
  Large language models are trained on massive scrapes of the web, which are
often unstructured, noisy, and poorly phrased. Current scaling laws show that
learning from such data requires an abundance of both compute and data, which
grows with the size of the model being trained. This is infeasible both because
of the large compute costs and duration associated with pre-training, and the
impending scarcity of high-quality data on the web. In this work, we propose
Web Rephrase Augmented Pre-training ($\textbf{WRAP}$) that uses an
off-the-shelf instruction-tuned model prompted to paraphrase documents on the
web in specific styles such as "like Wikipedia" or in "question-answer format"
to jointly pre-train LLMs on real and synthetic rephrases. First, we show that
using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training
by $\sim3x$. At the same pre-training compute budget, it improves perplexity by
more than 10% on average across different subsets of the Pile, and improves
zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we
investigate the impact of the re-phrasing style on the performance of the
model, offering insights into how the composition of the training data can
impact the performance of LLMs in OOD settings. Our gains are attributed to the
fact that re-phrased synthetic data has higher utility than just real data
because it (i) incorporates style diversity that closely reflects downstream
evaluation style, and (ii) has higher 'quality' than web-scraped data.
\\ ( https://arxiv.org/abs/2401.16380 ,  523kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16403
Date: Mon, 29 Jan 2024 18:41:39 GMT   (7914kb,D)

Title: ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media
  Text
Authors: Thanh-Nhi Nguyen, Thanh-Phong Le, Kiet Van Nguyen
Categories: cs.CL
Comments: Accepted at the EACL 2024 Main Conference
\\
  Lexical normalization, a fundamental task in Natural Language Processing
(NLP), involves the transformation of words into their canonical forms. This
process has been proven to benefit various downstream NLP tasks greatly. In
this work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the
first-ever corpus developed for the Vietnamese lexical normalization task. The
corpus comprises over 10,000 pairs of sentences meticulously annotated by human
annotators, sourced from public comments on Vietnam's most popular social media
platforms. Various methods were used to evaluate our corpus, and the
best-performing system achieved a result of 57.74% using the Error Reduction
Rate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline.
For extrinsic evaluation, employing the model trained on ViLexNorm demonstrates
the positive impact of the Vietnamese lexical normalization task on other NLP
tasks. Our corpus is publicly available exclusively for research purposes.
\\ ( https://arxiv.org/abs/2401.16403 ,  7914kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16405
Date: Mon, 29 Jan 2024 18:43:49 GMT   (183kb,D)

Title: Scaling Sparse Fine-Tuning to Large Language Models
Authors: Alan Ansell and Ivan Vuli\'c and Hannah Sterz and Anna Korhonen and
  Edoardo M. Ponti
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with
instructions or human feedback) due to their sheer number of parameters. A
family of parameter-efficient sparse fine-tuning (SFT) methods have proven
promising in terms of performance but their memory requirements increase
proportionally to the size of the LLMs. In this work, we scale sparse
fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. At any given
time, for a desired density level, we maintain an array of parameter indices
and the deltas of these parameters relative to their pretrained values. We
iterate among: (a) updating the active deltas, (b) pruning indices (based on
the change of magnitude of their deltas) and (c) regrowth of indices. For
regrowth, we explore two criteria based on either the accumulated gradients of
a few candidate parameters or their approximate momenta estimated using the
efficient SM3 optimizer. We experiment with instruction-tuning of LLMs on
standard dataset mixtures, finding that SFT is often superior to popular
parameter-efficient fine-tuning methods like LoRA (low-rank adaptation) in
terms of performance and comparable in terms of run time. We additionally show
that SFT is compatible with both quantization and efficient optimizers, to
facilitate scaling to ever-larger model sizes. We release the code for SFT at
https://github.com/AlanAnsell/peft and for the instruction-tuning experiments
at https://github.com/ducdauge/sft-llm.
\\ ( https://arxiv.org/abs/2401.16405 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15089
Date: Mon, 22 Jan 2024 15:14:22 GMT   (769kb,D)

Title: Accelerating Material Property Prediction using Generically Complete
  Isometry Invariants
Authors: Jonathan Balasingham, Viktor Zamaraev, Vitaliy Kurlin
Categories: cs.LG cs.CG physics.comp-ph
\\
  Material or crystal property prediction using machine learning has grown
popular in recent years as it provides a computationally efficient replacement
to classical simulation methods. A crucial first step for any of these
algorithms is the representation used for a periodic crystal. While similar
objects like molecules and proteins have a finite number of atoms and their
representation can be built based upon a finite point cloud interpretation,
periodic crystals are unbounded in size, making their representation more
challenging. In the present work, we adapt the Pointwise Distance Distribution
(PDD), a continuous and generically complete isometry invariant for periodic
point sets, as a representation for our learning algorithm. While the PDD is
effective in distinguishing periodic point sets up to isometry, there is no
consideration for the composition of the underlying material. We develop a
transformer model with a modified self-attention mechanism that can utilize the
PDD and incorporate compositional information via a spatial encoding method.
This model is tested on the crystals of the Materials Project and Jarvis-DFT
databases and shown to produce accuracy on par with state-of-the-art methods
while being several times faster in both training and prediction time.
\\ ( https://arxiv.org/abs/2401.15089 ,  769kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15098
Date: Thu, 25 Jan 2024 03:06:51 GMT   (619kb,D)

Title: Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement
  Learning
Authors: Chaofan Pan, Xin Yang, Hao Wang, Wei Wei, Tianrui Li
Categories: cs.LG cs.AI
\\
  Continual reinforcement learning (CRL) empowers RL agents with the ability to
learn from a sequence of tasks, preserving previous knowledge and leveraging it
to facilitate future learning. However, existing methods often focus on
transferring low-level knowledge across similar tasks, which neglects the
hierarchical structure of human cognitive control, resulting in insufficient
knowledge transfer across diverse tasks. To enhance high-level knowledge
transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge
transfer for Continual reinforcement learning), which is structured in two
layers: 1) the high-level policy formulation which utilizes the powerful
reasoning ability of the Large Language Model (LLM) to set goals and 2) the
low-level policy learning through RL which is oriented by high-level goals.
Moreover, the knowledge base (policy library) is constructed to store policies
that can be retrieved for hierarchical knowledge transfer. Experiments
conducted in MiniGrid have demonstrated the effectiveness of Hi-Core in
handling diverse CRL tasks, outperforming popular baselines.
\\ ( https://arxiv.org/abs/2401.15098 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15103
Date: Thu, 25 Jan 2024 11:53:35 GMT   (630kb,D)

Title: PruneSymNet: A Symbolic Neural Network and Pruning Algorithm for
  Symbolic Regression
Authors: Min Wu, Weijun Li, Lina Yu, Wenqiang Li, Jingyi Liu, Yanjie Li, Meilan
  Hao
Categories: cs.LG cs.AI
\\
  Symbolic regression aims to derive interpretable symbolic expressions from
data in order to better understand and interpret data. %which plays an
important role in knowledge discovery and interpretable machine learning.
  In this study, a symbolic network called PruneSymNet is proposed for symbolic
regression. This is a novel neural network whose activation function consists
of common elementary functions and operators. The whole network is
differentiable and can be trained by gradient descent method. Each subnetwork
in the network corresponds to an expression, and our goal is to extract such
subnetworks to get the desired symbolic expression.
  Therefore, a greedy pruning algorithm is proposed to prune the network into a
subnetwork while ensuring the accuracy of data fitting. The proposed greedy
pruning algorithm preserves the edge with the least loss in each pruning, but
greedy algorithm often can not get the optimal solution. In order to alleviate
this problem, we combine beam search during pruning to obtain multiple
candidate expressions each time, and finally select the expression with the
smallest loss as the final result. It was tested on the public data set and
compared with the current popular algorithms. The results showed that the
proposed algorithm had better accuracy.
\\ ( https://arxiv.org/abs/2401.15103 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15108
Date: Thu, 25 Jan 2024 16:51:52 GMT   (1540kb,D)

Title: Multi-agent Deep Reinforcement Learning for Dynamic Pricing by
  Fast-charging Electric Vehicle Hubs in ccompetition
Authors: Diwas Paudel, Tapas K. Das
Categories: cs.LG cs.AI cs.SY econ.GN eess.SY q-fin.EC
\\
  Fast-charging hubs for electric vehicles will soon become part of the newly
built infrastructure for transportation electrification across the world. These
hubs are expected to host many DC fast-charging stations and will admit EVs
only for charging. Like the gasoline refueling stations, fast-charging hubs in
a neighborhood will dynamically vary their prices to compete for the same pool
of EV owners. These hubs will interact with the electric power network by
making purchase commitments for a significant part of their power needs in the
day-ahead (DA) electricity market and meeting the difference from the real-time
(RT) market. Hubs may have supplemental battery storage systems (BSS), which
they will use for arbitrage. In this paper, we develop a two-step data-driven
dynamic pricing methodology for hubs in price competition. We first obtain the
DA commitment by solving a stochastic DA commitment model. Thereafter we obtain
the hub pricing strategies by modeling the game as a competitive Markov
decision process (CMDP) and solving it using a multi-agent deep reinforcement
learning (MADRL) approach. We develop a numerical case study for a pricing game
between two charging hubs. We solve the case study with our methodology by
using combinations of two different DRL algorithms, DQN and SAC, and two
different neural networks (NN) architectures, a feed-forward (FF) neural
network, and a multi-head attention (MHA) neural network. We construct a
measure of collusion (index) using the hub profits. A value of zero for this
index indicates no collusion (perfect competition) and a value of one indicates
full collusion (monopolistic behavior). Our results show that the collusion
index varies approximately between 0.14 and 0.45 depending on the combinations
of the algorithms and the architectures chosen by the hubs.
\\ ( https://arxiv.org/abs/2401.15108 ,  1540kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15119
Date: Fri, 26 Jan 2024 02:58:59 GMT   (2080kb,D)

Title: Interpreting Time Series Transformer Models and Sensitivity Analysis of
  Population Age Groups to COVID-19 Infections
Authors: Md Khairul Islam, Tyler Valentine, Timothy Joowon Sue, Ayush
  Karmacharya, Luke Neil Benham, Zhengguang Wang, Kingsley Kim, Judy Fox
Categories: cs.LG cs.AI q-bio.PE
\\
  Interpreting deep learning time series models is crucial in understanding the
model's behavior and learning patterns from raw data for real-time
decision-making. However, the complexity inherent in transformer-based time
series models poses challenges in explaining the impact of individual features
on predictions. In this study, we leverage recent local interpretation methods
to interpret state-of-the-art time series models. To use real-world datasets,
we collected three years of daily case data for 3,142 US counties. Firstly, we
compare six transformer-based models and choose the best prediction model for
COVID-19 infection. Using 13 input features from the last two weeks, we can
predict the cases for the next two weeks. Secondly, we present an innovative
way to evaluate the prediction sensitivity to 8 population age groups over
highly dynamic multivariate infection data. Thirdly, we compare our proposed
perturbation-based interpretation method with related work, including a total
of eight local interpretation methods. Finally, we apply our framework to
traffic and electricity datasets, demonstrating that our approach is generic
and can be applied to other time-series domains.
\\ ( https://arxiv.org/abs/2401.15119 ,  2080kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15121
Date: Fri, 26 Jan 2024 05:59:40 GMT   (33kb)

Title: Expressive Power of ReLU and Step Networks under Floating-Point
  Operations
Authors: Yeachan Park, Geonho Hwang, Wonyeol Lee, Sejun Park
Categories: cs.LG cs.AI
\\
  The study of the expressive power of neural networks has investigated the
fundamental limits of neural networks. Most existing results assume real-valued
inputs and parameters as well as exact operations during the evaluation of
neural networks. However, neural networks are typically executed on computers
that can only represent a tiny subset of the reals and apply inexact
operations. In this work, we analyze the expressive power of neural networks
under a more realistic setup: when we use floating-point numbers and
operations. Our first set of results assumes floating-point operations where
the significand of a float is represented by finite bits but its exponent can
take any integer value. Under this setup, we show that neural networks using a
binary threshold unit or ReLU can memorize any finite input/output pairs and
can approximate any continuous function within a small error. We also show
similar results on memorization and universal approximation when floating-point
operations use finite bits for both significand and exponent; these results are
applicable to many popular floating-point formats such as those defined in the
IEEE 754 standard (e.g., 32-bit single-precision format) and bfloat16.
\\ ( https://arxiv.org/abs/2401.15121 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15122
Date: Fri, 26 Jan 2024 09:35:17 GMT   (11870kb,D)

Title: A Multi-Grained Symmetric Differential Equation Model for Learning
  Protein-Ligand Binding Dynamics
Authors: Shengchao Liu, Weitao Du, Yanjing Li, Zhuoxinran Li, Vignesh
  Bhethanabotla, Nakul Rampal, Omar Yaghi, Christian Borgs, Anima Anandkumar,
  Hongyu Guo, Jennifer Chayes
Categories: cs.LG cs.AI q-bio.BM q-bio.QM stat.ML
\\
  In drug discovery, molecular dynamics (MD) simulation for protein-ligand
binding provides a powerful tool for predicting binding affinities, estimating
transport properties, and exploring pocket sites. There has been a long history
of improving the efficiency of MD simulations through better numerical methods
and, more recently, by augmenting them with machine learning (ML) methods. Yet,
challenges remain, such as accurate modeling of extended-timescale simulations.
To address this issue, we propose NeuralMD, the first ML surrogate that can
facilitate numerical MD and provide accurate simulations of protein-ligand
binding dynamics. We propose a principled approach that incorporates a novel
physics-informed multi-grained group symmetric framework. Specifically, we
propose (1) a BindingNet model that satisfies group symmetry using vector
frames and captures the multi-level protein-ligand interactions, and (2) an
augmented neural differential equation solver that learns the trajectory under
Newtonian mechanics. For the experiment, we design ten single-trajectory and
three multi-trajectory binding simulation tasks. We show the efficiency and
effectiveness of NeuralMD, with a 2000$\times$ speedup over standard numerical
MD simulation and outperforming all other ML approaches by up to 80\% under the
stability metric. We further qualitatively show that NeuralMD reaches more
stable binding predictions compared to other machine learning methods.
\\ ( https://arxiv.org/abs/2401.15122 ,  11870kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15123
Date: Fri, 26 Jan 2024 09:51:07 GMT   (2547kb,D)

Title: Large Language Model Guided Knowledge Distillation for Time Series
  Anomaly Detection
Authors: Chen Liu, Shibo He, Qihang Zhou, Shizhong Li, Wenchao Meng
Categories: cs.LG cs.AI
Comments: 12 pages, 5 figures
\\
  Self-supervised methods have gained prominence in time series anomaly
detection due to the scarcity of available annotations. Nevertheless, they
typically demand extensive training data to acquire a generalizable
representation map, which conflicts with scenarios of a few available samples,
thereby limiting their performance. To overcome the limitation, we propose
\textbf{AnomalyLLM}, a knowledge distillation-based time series anomaly
detection approach where the student network is trained to mimic the features
of the large language model (LLM)-based teacher network that is pretrained on
large-scale datasets. During the testing phase, anomalies are detected when the
discrepancy between the features of the teacher and student networks is large.
To circumvent the student network from learning the teacher network's feature
of anomalous samples, we devise two key strategies. 1) Prototypical signals are
incorporated into the student network to consolidate the normal feature
extraction. 2) We use synthetic anomalies to enlarge the representation gap
between the two networks. AnomalyLLM demonstrates state-of-the-art performance
on 15 datasets, improving accuracy by at least 14.5\% in the UCR dataset.
\\ ( https://arxiv.org/abs/2401.15123 ,  2547kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15199
Date: Fri, 26 Jan 2024 20:51:55 GMT   (1269kb,D)

Title: SCANIA Component X Dataset: A Real-World Multivariate Time Series
  Dataset for Predictive Maintenance
Authors: Zahra Kharazian, Tony Lindgren, Sindri Magn\'usson, Olof Steinert,
  Oskar Andersson Reyna
Categories: cs.LG cs.AI
Comments: 10 pages, 8 figures
\\
  This paper presents a description of a real-world, multivariate time series
dataset collected from an anonymized engine component (called Component X) of a
fleet of trucks from SCANIA, Sweden. This dataset includes diverse variables
capturing detailed operational data, repair records, and specifications of
trucks while maintaining confidentiality by anonymization. It is well-suited
for a range of machine learning applications, such as classification,
regression, survival analysis, and anomaly detection, particularly when applied
to predictive maintenance scenarios. The large population size and variety of
features in the format of histograms and numerical counters, along with the
inclusion of temporal information, make this real-world dataset unique in the
field. The objective of releasing this dataset is to give a broad range of
researchers the possibility of working with real-world data from an
internationally well-known company and introduce a standard benchmark to the
predictive maintenance field, fostering reproducible research.
\\ ( https://arxiv.org/abs/2401.15199 ,  1269kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15203
Date: Fri, 26 Jan 2024 21:02:36 GMT   (881kb,D)

Title: FedGT: Federated Node Classification with Scalable Graph Transformer
Authors: Zaixi Zhang, Qingyong Hu, Yang Yu, Weibo Gao, Qi Liu
Categories: cs.LG
Comments: ICLR 24 submission
\\
  Graphs are widely used to model relational data. As graphs are getting larger
and larger in real-world scenarios, there is a trend to store and compute
subgraphs in multiple local systems. For example, recently proposed
\emph{subgraph federated learning} methods train Graph Neural Networks (GNNs)
distributively on local subgraphs and aggregate GNN parameters with a central
server. However, existing methods have the following limitations: (1) The links
between local subgraphs are missing in subgraph federated learning. This could
severely damage the performance of GNNs that follow message-passing paradigms
to update node/edge features. (2) Most existing methods overlook the subgraph
heterogeneity issue, brought by subgraphs being from different parts of the
whole graph. To address the aforementioned challenges, we propose a scalable
\textbf{Fed}erated \textbf{G}raph \textbf{T}ransformer (\textbf{FedGT}) in the
paper. Firstly, we design a hybrid attention scheme to reduce the complexity of
the Graph Transformer to linear while ensuring a global receptive field with
theoretical bounds. Specifically, each node attends to the sampled local
neighbors and a set of curated global nodes to learn both local and global
information and be robust to missing links. The global nodes are dynamically
updated during training with an online clustering algorithm to capture the data
distribution of the corresponding local subgraph. Secondly, FedGT computes
clients' similarity based on the aligned global nodes with optimal transport.
The similarity is then used to perform weighted averaging for personalized
aggregation, which well addresses the data heterogeneity problem. Moreover,
local differential privacy is applied to further protect the privacy of
clients. Finally, extensive experimental results on 6 datasets and 2 subgraph
settings demonstrate the superiority of FedGT.
\\ ( https://arxiv.org/abs/2401.15203 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15207
Date: Fri, 26 Jan 2024 21:14:32 GMT   (1017kb,D)

Title: HiFT: A Hierarchical Full Parameter Fine-Tuning Strategy
Authors: Yongkang Liu, Yiqun Zhang, Qian Li, Shi Feng, Daling Wang, Yifei Zhang
  and Hinrich Sch\"utze
Categories: cs.LG
Comments: under progress
\\
  Full-parameter fine-tuning has become the go-to choice for adapting language
models (LMs) to downstream tasks due to its excellent performance. As LMs grow
in size, fine-tuning the full parameters of LMs requires a prohibitively large
amount of GPU memory. Existing approaches utilize zeroth-order optimizer to
conserve GPU memory, which can potentially compromise the performance of LMs as
non-zero order optimizers tend to converge more readily on most downstream
tasks. In this paper, we propose a novel optimizer-independent end-to-end
hierarchical fine-tuning strategy, HiFT, which only updates a subset of
parameters at each training step. HiFT can significantly reduce the amount of
gradients and optimizer state parameters residing in GPU memory at the same
time, thereby reducing GPU memory usage. Our results demonstrate that: (1) HiFT
achieves comparable performance to parameter-efficient fine-tuning and standard
full parameter fine-tuning. (2) HiFT supports various optimizers including
AdamW, AdaGrad, SGD, etc. (3) HiFT can save more than 60\% GPU memory compared
with standard full-parameter fine-tuning for 7B model. (4) HiFT enables
full-parameter fine-tuning of a 7B model on single 48G A6000 with a precision
of 32 using the AdamW optimizer, without using any memory saving techniques.
\\ ( https://arxiv.org/abs/2401.15207 ,  1017kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15238
Date: Fri, 26 Jan 2024 23:12:41 GMT   (8262kb,D)

Title: Deep Learning with Tabular Data: A Self-supervised Approach
Authors: Tirth Kiranbhai Vyas
Categories: cs.LG cs.AI
\\
  We have described a novel approach for training tabular data using the
TabTransformer model with self-supervised learning. Traditional machine
learning models for tabular data, such as GBDT are being widely used though our
paper examines the effectiveness of the TabTransformer which is a Transformer
based model optimised specifically for tabular data. The TabTransformer
captures intricate relationships and dependencies among features in tabular
data by leveraging the self-attention mechanism of Transformers. We have used a
self-supervised learning approach in this study, where the TabTransformer
learns from unlabelled data by creating surrogate supervised tasks, eliminating
the need for the labelled data. The aim is to find the most effective
TabTransformer model representation of categorical and numerical features. To
address the challenges faced during the construction of various input settings
into the Transformers. Furthermore, a comparative analysis is also been
conducted to examine performance of the TabTransformer model against baseline
models such as MLP and supervised TabTransformer.
  The research has presented with a novel approach by creating various variants
of TabTransformer model namely, Binned-TT, Vanilla-MLP-TT, MLP- based-TT which
has helped to increase the effective capturing of the underlying relationship
between various features of the tabular dataset by constructing optimal inputs.
And further we have employed a self-supervised learning approach in the form of
a masking-based unsupervised setting for tabular data. The findings shed light
on the best way to represent categorical and numerical features, emphasizing
the TabTransormer performance when compared to established machine learning
models and other self-supervised learning methods.
\\ ( https://arxiv.org/abs/2401.15238 ,  8262kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15240
Date: Fri, 26 Jan 2024 23:13:47 GMT   (47kb)

Title: Near-Optimal Policy Optimization for Correlated Equilibrium in
  General-Sum Markov Games
Authors: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng
Categories: cs.LG cs.GT math.OC
Comments: AISTATS 2024 Oral
\\
  We study policy optimization algorithms for computing correlated equilibria
in multi-player general-sum Markov Games. Previous results achieve
$O(T^{-1/2})$ convergence rate to a correlated equilibrium and an accelerated
$O(T^{-3/4})$ convergence rate to the weaker notion of coarse correlated
equilibrium. In this paper, we improve both results significantly by providing
an uncoupled policy optimization algorithm that attains a near-optimal
$\tilde{O}(T^{-1})$ convergence rate for computing a correlated equilibrium.
Our algorithm is constructed by combining two main elements (i) smooth value
updates and (ii) the optimistic-follow-the-regularized-leader algorithm with
the log barrier regularizer.
\\ ( https://arxiv.org/abs/2401.15240 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15246
Date: Fri, 26 Jan 2024 23:41:28 GMT   (56kb,D)

Title: Training Differentially Private Ad Prediction Models with Semi-Sensitive
  Features
Authors: Lynn Chua, Qiliang Cui, Badih Ghazi, Charlie Harrison, Pritish Kamath,
  Walid Krichene, Ravi Kumar, Pasin Manurangsi, Krishna Giri Narra, Amer Sinha,
  Avinash Varadarajan, Chiyuan Zhang
Categories: cs.LG
Comments: 7 pages, 4 figures
\\
  Motivated by problems arising in digital advertising, we introduce the task
of training differentially private (DP) machine learning models with
semi-sensitive features. In this setting, a subset of the features is known to
the attacker (and thus need not be protected) while the remaining features as
well as the label are unknown to the attacker and should be protected by the DP
guarantee. This task interpolates between training the model with full DP
(where the label and all features should be protected) or with label DP (where
all the features are considered known, and only the label should be protected).
We present a new algorithm for training DP models with semi-sensitive features.
Through an empirical evaluation on real ads datasets, we demonstrate that our
algorithm surpasses in utility the baselines of (i) DP stochastic gradient
descent (DP-SGD) run on all features (known and unknown), and (ii) a label DP
algorithm run only on the known features (while discarding the unknown ones).
\\ ( https://arxiv.org/abs/2401.15246 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15248
Date: Fri, 26 Jan 2024 23:52:20 GMT   (3964kb,D)

Title: Better Representations via Adversarial Training in Pre-Training: A
  Theoretical Perspective
Authors: Yue Xing, Xiaofeng Lin, Qifan Song, Yi Xu, Belinda Zeng, Guang Cheng
Categories: cs.LG stat.ML
Comments: To appear in AISTATS2024
\\
  Pre-training is known to generate universal representations for downstream
tasks in large-scale deep learning such as large language models. Existing
literature, e.g., \cite{kim2020adversarial}, empirically observe that the
downstream tasks can inherit the adversarial robustness of the pre-trained
model. We provide theoretical justifications for this robustness inheritance
phenomenon. Our theoretical results reveal that feature purification plays an
important role in connecting the adversarial robustness of the pre-trained
model and the downstream tasks in two-layer neural networks. Specifically, we
show that (i) with adversarial training, each hidden node tends to pick only
one (or a few) feature; (ii) without adversarial training, the hidden nodes can
be vulnerable to attacks. This observation is valid for both supervised
pre-training and contrastive learning. With purified nodes, it turns out that
clean training is enough to achieve adversarial robustness in downstream tasks.
\\ ( https://arxiv.org/abs/2401.15248 ,  3964kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15268
Date: Sat, 27 Jan 2024 02:21:31 GMT   (524kb,D)

Title: Towards Stable Preferences for Stakeholder-aligned Machine Learning
Authors: Haleema Sheraz, Stefan C. Kremer, Joshua August Skorburg, Graham
  Taylor, Walter Sinnott-Armstrong, Kyle Boerstler
Categories: cs.LG cs.AI
Comments: 11 pages, 1 figure, 1 table, 2 appendices, NeurIPS 2023
\\
  In response to the pressing challenge of kidney allocation, characterized by
growing demands for organs, this research sets out to develop a data-driven
solution to this problem, which also incorporates stakeholder values. The
primary objective of this study is to create a method for learning both
individual and group-level preferences pertaining to kidney allocations.
Drawing upon data from the 'Pairwise Kidney Patient Online Survey.' Leveraging
two distinct datasets and evaluating across three levels - Individual, Group
and Stability - we employ machine learning classifiers assessed through several
metrics. The Individual level model predicts individual participant
preferences, the Group level model aggregates preferences across participants,
and the Stability level model, an extension of the Group level, evaluates the
stability of these preferences over time. By incorporating stakeholder
preferences into the kidney allocation process, we aspire to advance the
ethical dimensions of organ transplantation, contributing to more transparent
and equitable practices while promoting the integration of moral values into
algorithmic decision-making.
\\ ( https://arxiv.org/abs/2401.15268 ,  524kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15270
Date: Sat, 27 Jan 2024 02:36:30 GMT   (1741kb,D)

Title: SimFair: Physics-Guided Fairness-Aware Learning with Simulation Models
Authors: Zhihao Wang, Yiqun Xie, Zhili Li, Xiaowei Jia, Zhe Jiang, Aolin Jia,
  Shuo Xu
Categories: cs.LG cs.AI
Comments: AAAI 2024
\\
  Fairness-awareness has emerged as an essential building block for the
responsible use of artificial intelligence in real applications. In many cases,
inequity in performance is due to the change in distribution over different
regions. While techniques have been developed to improve the transferability of
fairness, a solution to the problem is not always feasible with no samples from
the new regions, which is a bottleneck for pure data-driven attempts.
Fortunately, physics-based mechanistic models have been studied for many
problems with major social impacts. We propose SimFair, a physics-guided
fairness-aware learning framework, which bridges the data limitation by
integrating physical-rule-based simulation and inverse modeling into the
training design. Using temperature prediction as an example, we demonstrate the
effectiveness of the proposed SimFair in fairness preservation.
\\ ( https://arxiv.org/abs/2401.15270 ,  1741kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15273
Date: Sat, 27 Jan 2024 02:43:45 GMT   (37338kb,D)

Title: Finite-Time Analysis of On-Policy Heterogeneous Federated Reinforcement
  Learning
Authors: Chenyu Zhang, Han Wang, Aritra Mitra, James Anderson
Categories: cs.LG cs.SY eess.SY math.OC
Comments: Published as a conference paper at ICLR 2024
\\
  Federated reinforcement learning (FRL) has emerged as a promising paradigm
for reducing the sample complexity of reinforcement learning tasks by
exploiting information from different agents. However, when each agent
interacts with a potentially different environment, little to nothing is known
theoretically about the non-asymptotic performance of FRL algorithms. The lack
of such results can be attributed to various technical challenges and their
intricate interplay: Markovian sampling, linear function approximation,
multiple local updates to save communication, heterogeneity in the reward
functions and transition kernels of the agents' MDPs, and continuous
state-action spaces. Moreover, in the on-policy setting, the behavior policies
vary with time, further complicating the analysis. In response, we introduce
FedSARSA, a novel federated on-policy reinforcement learning scheme, equipped
with linear function approximation, to address these challenges and provide a
comprehensive finite-time error analysis. Notably, we establish that FedSARSA
converges to a policy that is near-optimal for all agents, with the extent of
near-optimality proportional to the level of heterogeneity. Furthermore, we
prove that FedSARSA leverages agent collaboration to enable linear speedups as
the number of agents increases, which holds for both fixed and adaptive
step-size configurations.
\\ ( https://arxiv.org/abs/2401.15273 ,  37338kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15290
Date: Sat, 27 Jan 2024 04:09:41 GMT   (335kb,D)

Title: Benchmarking with MIMIC-IV, an irregular, spare clinical time series
  dataset
Authors: Hung Bui, Harikrishna Warrier, Yogesh Gupta
Categories: cs.LG
Comments: 7 pages, 1 figure, 3 tables
\\
  Electronic health record (EHR) is more and more popular, and it comes with
applying machine learning solutions to resolve various problems in the domain.
This growing research area also raises the need for EHRs accessibility. Medical
Information Mart for Intensive Care (MIMIC) dataset is a popular, public, and
free EHR dataset in a raw format that has been used in numerous studies.
However, despite of its popularity, it is lacking benchmarking work, especially
with recent state of the art works in the field of deep learning with
time-series tabular data. The aim of this work is to fill this lack by
providing a benchmark for latest version of MIMIC dataset, MIMIC-IV. We also
give a detailed literature survey about studies that has been already done for
MIIMIC-III.
\\ ( https://arxiv.org/abs/2401.15290 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15292
Date: Sat, 27 Jan 2024 04:24:19 GMT   (687kb,D)

Title: Adaptive Block sparse regularization under arbitrary linear transform
Authors: Takanobu Furuhashi, Hidekata Hontani, Tatsuya Yokota
Categories: cs.LG
Comments: 5 pages, 3 figures
\\
  We propose a convex signal reconstruction method for block sparsity under
arbitrary linear transform with unknown block structure. The proposed method is
a generalization of the existing method LOP-$\ell_2$/$\ell_1$ and can
reconstruct signals with block sparsity under non-invertible transforms, unlike
LOP-$\ell_2$/$\ell_1$. Our work broadens the scope of block sparse
regularization, enabling more versatile and powerful applications across
various signal processing domains. We derive an iterative algorithm for solving
proposed method and provide conditions for its convergence to the optimal
solution. Numerical experiments demonstrate the effectiveness of the proposed
method.
\\ ( https://arxiv.org/abs/2401.15292 ,  687kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15295
Date: Sat, 27 Jan 2024 04:49:37 GMT   (1842kb,D)

Title: Multi-Trigger Backdoor Attacks: More Triggers, More Threats
Authors: Yige Li, Xingjun Ma, Jiabo He, Hanxun Huang, Yu-Gang Jiang
Categories: cs.LG
\\
  Backdoor attacks have emerged as a primary threat to (pre-)training and
deployment of deep neural networks (DNNs). While backdoor attacks have been
extensively studied in a body of works, most of them were focused on
single-trigger attacks that poison a dataset using a single type of trigger.
Arguably, real-world backdoor attacks can be much more complex, e.g., the
existence of multiple adversaries for the same dataset if it is of high value.
In this work, we investigate the practical threat of backdoor attacks under the
setting of \textbf{multi-trigger attacks} where multiple adversaries leverage
different types of triggers to poison the same dataset. By proposing and
investigating three types of multi-trigger attacks, including parallel,
sequential, and hybrid attacks, we provide a set of important understandings of
the coexisting, overwriting, and cross-activating effects between different
triggers on the same dataset. Moreover, we show that single-trigger attacks
tend to cause overly optimistic views of the security of current defense
techniques, as all examined defense methods struggle to defend against
multi-trigger attacks. Finally, we create a multi-trigger backdoor poisoning
dataset to help future evaluation of backdoor attacks and defenses. Although
our work is purely empirical, we hope it can help steer backdoor research
toward more realistic settings.
\\ ( https://arxiv.org/abs/2401.15295 ,  1842kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15299
Date: Sat, 27 Jan 2024 05:14:17 GMT   (779kb,D)

Title: SupplyGraph: A Benchmark Dataset for Supply Chain Planning using Graph
  Neural Networks
Authors: Azmine Toushik Wasi and MD Shafikul Islam and Adipto Raihan Akib
Categories: cs.LG cs.AI cs.SY eess.SY stat.AP
Comments: 9 pages, 8 figures; Accepted to 4th workshop on Graphs and more
  Complex structures for Learning and Reasoning, colocated with AAAI 2024
ACM-class: I.2.1; I.2.8; E.0; J.2; H.3.7
\\
  Graph Neural Networks (GNNs) have gained traction across different domains
such as transportation, bio-informatics, language processing, and computer
vision. However, there is a noticeable absence of research on applying GNNs to
supply chain networks. Supply chain networks are inherently graph-like in
structure, making them prime candidates for applying GNN methodologies. This
opens up a world of possibilities for optimizing, predicting, and solving even
the most complex supply chain problems. A major setback in this approach lies
in the absence of real-world benchmark datasets to facilitate the research and
resolution of supply chain problems using GNNs. To address the issue, we
present a real-world benchmark dataset for temporal tasks, obtained from one of
the leading FMCG companies in Bangladesh, focusing on supply chain planning for
production purposes. The dataset includes temporal data as node features to
enable sales predictions, production planning, and the identification of
factory issues. By utilizing this dataset, researchers can employ GNNs to
address numerous supply chain problems, thereby advancing the field of supply
chain analytics and planning. Source: https://github.com/CIOL-SUST/SupplyGraph
\\ ( https://arxiv.org/abs/2401.15299 ,  779kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15304
Date: Sat, 27 Jan 2024 05:47:12 GMT   (35kb,D)

Title: Adaptive Least Mean Squares Graph Neural Networks and Online Graph
  Signal Estimation
Authors: Yi Yan, Changran Peng, Ercan Engin Kuruoglu
Categories: cs.LG
\\
  The online prediction of multivariate signals, existing simultaneously in
space and time, from noisy partial observations is a fundamental task in
numerous applications. We propose an efficient Neural Network architecture for
the online estimation of time-varying graph signals named the Adaptive Least
Mean Squares Graph Neural Networks (LMS-GNN). LMS-GNN aims to capture the time
variation and bridge the cross-space-time interactions under the condition that
signals are corrupted by noise and missing values. The LMS-GNN is a combination
of adaptive graph filters and Graph Neural Networks (GNN). At each time step,
the forward propagation of LMS-GNN is similar to adaptive graph filters where
the output is based on the error between the observation and the prediction
similar to GNN. The filter coefficients are updated via backpropagation as in
GNN. Experimenting on real-world temperature data reveals that our LMS-GNN
achieves more accurate online predictions compared to graph-based methods like
adaptive graph filters and graph convolutional neural networks.
\\ ( https://arxiv.org/abs/2401.15304 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15330
Date: Sat, 27 Jan 2024 07:26:10 GMT   (11073kb,D)

Title: Optimal Sparse Survival Trees
Authors: Rui Zhang, Rui Xin, Margo Seltzer, Cynthia Rudin
Categories: cs.LG
Comments: AISTATS2024 preprint. arXiv admin note: text overlap with
  arXiv:2211.14980
\\
  Interpretability is crucial for doctors, hospitals, pharmaceutical companies
and biotechnology corporations to analyze and make decisions for high stakes
problems that involve human health. Tree-based methods have been widely adopted
for \textit{survival analysis} due to their appealing interpretablility and
their ability to capture complex relationships. However, most existing methods
to produce survival trees rely on heuristic (or greedy) algorithms, which risk
producing sub-optimal models. We present a dynamic-programming-with-bounds
approach that finds provably-optimal sparse survival tree models, frequently in
only a few seconds.
\\ ( https://arxiv.org/abs/2401.15330 ,  11073kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15337
Date: Sat, 27 Jan 2024 07:59:54 GMT   (5804kb,D)

Title: Deep Learning with Information Fusion and Model Interpretation for
  Health Monitoring of Fetus based on Long-term Prenatal Electronic Fetal Heart
  Rate Monitoring Data
Authors: Zenghui Lin, Xintong Liu, Nan Wang, Ruichen Li, Qingao Liu, Jingying
  Ma, Liwei Wang, Yan Wang, Shenda Hong
Categories: cs.LG cs.AI
\\
  Long-term fetal heart rate (FHR) monitoring during the antepartum period,
increasingly popularized by electronic FHR monitoring, represents a growing
approach in FHR monitoring. This kind of continuous monitoring, in contrast to
the short-term one, collects an extended period of fetal heart data. This
offers a more comprehensive understanding of fetus's conditions. However, the
interpretation of long-term antenatal fetal heart monitoring is still in its
early stages, lacking corresponding clinical standards. Furthermore, the
substantial amount of data generated by continuous monitoring imposes a
significant burden on clinical work when analyzed manually. To address above
challenges, this study develops an automatic analysis system named LARA
(Long-term Antepartum Risk Analysis system) for continuous FHR monitoring,
combining deep learning and information fusion methods. LARA's core is a
well-established convolutional neural network (CNN) model. It processes
long-term FHR data as input and generates a Risk Distribution Map (RDM) and
Risk Index (RI) as the analysis results. We evaluate LARA on inner test
dataset, the performance metrics are as follows: AUC 0.872, accuracy 0.816,
specificity 0.811, sensitivity 0.806, precision 0.271, and F1 score 0.415. In
our study, we observe that long-term FHR monitoring data with higher RI is more
likely to result in adverse outcomes (p=0.0021). In conclusion, this study
introduces LARA, the first automated analysis system for long-term FHR
monitoring, initiating the further explorations into its clinical value in the
future.
\\ ( https://arxiv.org/abs/2401.15337 ,  5804kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15377
Date: Sat, 27 Jan 2024 10:49:33 GMT   (498kb)

Title: Validation of artificial neural networks to model the acoustic behaviour
  of induction motors
Authors: F.J. Jimenez-Romero, D. Guijo-Rubio, F.R. Lara-Raya, A. Ruiz-Gonzalez,
  C. Hervas-Martinez
Categories: cs.LG cs.SD eess.AS
\\
  In the last decade, the sound quality of electric induction motors is a hot
topic in the research field. Specially, due to its high number of applications,
the population is exposed to physical and psychological discomfort caused by
the noise emission. Therefore, it is necessary to minimise its psychological
impact on the population. In this way, the main goal of this work is to
evaluate the use of multitask artificial neural networks as a modelling
technique for simultaneously predicting psychoacoustic parameters of induction
motors. Several inputs are used, such as, the electrical magnitudes of the
motor power signal and the number of poles, instead of separating the noise of
the electric motor from the environmental noise. Two different kind of
artificial neural networks are proposed to evaluate the acoustic quality of
induction motors, by using the equivalent sound pressure, the loudness, the
roughness and the sharpness as outputs. Concretely, two different topologies
have been considered: simple models and more complex models. The former are
more interpretable, while the later lead to higher accuracy at the cost of
hiding the cause-effect relationship. Focusing on the simple interpretable
models, product unit neural networks achieved the best results: for MSE and for
SEP. The main benefit of this product unit model is its simplicity, since only
10 inputs variables are used, outlining the effective transfer mechanism of
multitask artificial neural networks to extract common features of multiple
tasks. Finally, a deep analysis of the acoustic quality of induction motors in
done using the best product unit neural networks.
\\ ( https://arxiv.org/abs/2401.15377 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15417
Date: Sat, 27 Jan 2024 14:12:42 GMT   (965kb,D)

Title: Fault Diagnosis on Induction Motor using Machine Learning and Signal
  Processing
Authors: Muhammad Samiullah, Hasan Ali, Shehryar Zahoor and Anas Ali
Categories: cs.LG cs.SY eess.SY
Comments: 6 pages, 17 figures, 2 tables
\\
  The detection and identification of induction motor faults using machine
learning and signal processing is a valuable approach to avoiding plant
disturbances and shutdowns in the context of Industry 4.0. In this work, we
present a study on the detection and identification of induction motor faults
using machine learning and signal processing with MATLAB Simulink. We developed
a model of a three-phase induction motor in MATLAB Simulink to generate healthy
and faulty motor data. The data collected included stator currents, rotor
currents, input power, slip, rotor speed, and efficiency. We generated four
faults in the induction motor: open circuit fault, short circuit fault,
overload, and broken rotor bars. We collected a total of 150,000 data points
with a 60-40% ratio of healthy to faulty motor data. We applied Fast Fourier
Transform (FFT) to detect and identify healthy and unhealthy conditions and
added a distinctive feature in our data. The generated dataset was trained
different machine learning models. On comparing the accuracy of the models on
the test set, we concluded that the Decision Tree algorithm performed the best
with an accuracy of about 92%. Our study contributes to the literature by
providing a valuable approach to fault detection and classification with
machine learning models for industrial applications.
\\ ( https://arxiv.org/abs/2401.15417 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15422
Date: Sat, 27 Jan 2024 14:19:33 GMT   (93kb)

Title: A Survey on Data Augmentation in Large Model Era
Authors: Yue Zhou, Chenlu Guo, Xu Wang, Yi Chang, and Yuan Wu
Categories: cs.LG
Comments: 30 pages; https://github.com/MLGroup-JLU/LLM-data-aug-survey
\\
  Large models, encompassing large language and diffusion models, have shown
exceptional promise in approximating human-level intelligence, garnering
significant interest from both academic and industrial spheres. However, the
training of these large models necessitates vast quantities of high-quality
data, and with continuous updates to these models, the existing reservoir of
high-quality data may soon be depleted. This challenge has catalyzed a surge in
research focused on data augmentation methods. Leveraging large models, these
data augmentation techniques have outperformed traditional approaches. This
paper offers an exhaustive review of large model-driven data augmentation
methods, adopting a comprehensive perspective. We begin by establishing a
classification of relevant studies into three main categories: image
augmentation, text augmentation, and paired data augmentation. Following this,
we delve into various data post-processing techniques pertinent to large
model-based data augmentation. Our discussion then expands to encompass the
array of applications for these data augmentation methods within natural
language processing, computer vision, and audio signal processing. We proceed
to evaluate the successes and limitations of large model-based data
augmentation across different scenarios. Concluding our review, we highlight
prospective challenges and avenues for future exploration in the field of data
augmentation. Our objective is to furnish researchers with critical insights,
ultimately contributing to the advancement of more sophisticated large models.
We consistently maintain the related open-source materials at:
https://github.com/MLGroup-JLU/LLM-data-aug-survey.
\\ ( https://arxiv.org/abs/2401.15422 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15444
Date: Sat, 27 Jan 2024 15:35:05 GMT   (6344kb,D)

Title: Towards Causal Classification: A Comprehensive Study on Graph Neural
  Networks
Authors: Simi Job, Xiaohui Tao, Taotao Cai, Lin Li, Haoran Xie, Jianming Yong
Categories: cs.LG
\\
  The exploration of Graph Neural Networks (GNNs) for processing
graph-structured data has expanded, particularly their potential for causal
analysis due to their universal approximation capabilities. Anticipated to
significantly enhance common graph-based tasks such as classification and
prediction, the development of a causally enhanced GNN framework is yet to be
thoroughly investigated. Addressing this shortfall, our study delves into nine
benchmark graph classification models, testing their strength and versatility
across seven datasets spanning three varied domains to discern the impact of
causality on the predictive prowess of GNNs. This research offers a detailed
assessment of these models, shedding light on their efficiency, and flexibility
in different data environments, and highlighting areas needing advancement. Our
findings are instrumental in furthering the understanding and practical
application of GNNs in diverse datacentric fields
\\ ( https://arxiv.org/abs/2401.15444 ,  6344kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15447
Date: Sat, 27 Jan 2024 15:52:58 GMT   (5353kb,D)

Title: Continuous Treatment Effect Estimation Using Gradient Interpolation and
  Kernel Smoothing
Authors: Lokesh Nagalapatti, Akshay Iyer, Abir De, Sunita Sarawagi
Categories: cs.LG stat.ML
Comments: Accepted at AAAI 24
\\
  We address the Individualized continuous treatment effect (ICTE) estimation
problem where we predict the effect of any continuous-valued treatment on an
individual using observational data. The main challenge in this estimation task
is the potential confounding of treatment assignment with an individual's
covariates in the training data, whereas during inference ICTE requires
prediction on independently sampled treatments. In contrast to prior work that
relied on regularizers or unstable GAN training, we advocate the direct
approach of augmenting training individuals with independently sampled
treatments and inferred counterfactual outcomes. We infer counterfactual
outcomes using a two-pronged strategy: a Gradient Interpolation for
close-to-observed treatments, and a Gaussian Process based Kernel Smoothing
which allows us to downweigh high variance inferences. We evaluate our method
on five benchmarks and show that our method outperforms six state-of-the-art
methods on the counterfactual estimation error. We analyze the superior
performance of our method by showing that (1) our inferred counterfactual
responses are more accurate, and (2) adding them to the training data reduces
the distributional distance between the confounded training distribution and
test distribution where treatment is independent of covariates. Our proposed
method is model-agnostic and we show that it improves ICTE accuracy of several
existing models.
\\ ( https://arxiv.org/abs/2401.15447 ,  5353kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15469
Date: Sat, 27 Jan 2024 17:43:08 GMT   (13019kb,D)

Title: Wind speed super-resolution and validation: from ERA5 to CERRA via
  diffusion models
Authors: Fabio Merizzi, Andrea Asperti, Stefano Colamonaco
Categories: cs.LG cs.AI
\\
  The Copernicus Regional Reanalysis for Europe, CERRA, is a high-resolution
regional reanalysis dataset for the European domain. In recent years it has
shown significant utility across various climate-related tasks, ranging from
forecasting and climate change research to renewable energy prediction,
resource management, air quality risk assessment, and the forecasting of rare
events, among others. Unfortunately, the availability of CERRA is lagging two
years behind the current date, due to constraints in acquiring the requisite
external data and the intensive computational demands inherent in its
generation. As a solution, this paper introduces a novel method using diffusion
models to approximate CERRA downscaling in a data-driven manner, without
additional informations. By leveraging the lower resolution ERA5 dataset, which
provides boundary conditions for CERRA, we approach this as a super-resolution
task. Focusing on wind speed around Italy, our model, trained on existing CERRA
data, shows promising results, closely mirroring original CERRA data.
Validation with in-situ observations further confirms the model's accuracy in
approximating ground measurements.
\\ ( https://arxiv.org/abs/2401.15469 ,  13019kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15480
Date: Sat, 27 Jan 2024 19:05:21 GMT   (707kb,D)

Title: Social Interpretable Reinforcement Learning
Authors: Leonardo Lucio Custode, Giovanni Iacca
Categories: cs.LG cs.AI cs.MA
Comments: 9 pages, 6 figures, submitted to IJCAI 2024
ACM-class: I.2.6; I.2.8
\\
  Reinforcement Learning (RL) bears the promise of being an enabling technology
for many applications. However, since most of the literature in the field is
currently focused on opaque models, the use of RL in high-stakes scenarios,
where interpretability is crucial, is still limited. Recently, some approaches
to interpretable RL, e.g., based on Decision Trees, have been proposed, but one
of the main limitations of these techniques is their training cost. To overcome
this limitation, we propose a new population-based method, called Social
Interpretable RL (SIRL), inspired by social learning principles, to improve
learning efficiency. Our method mimics a social learning process, where each
agent in a group learns to solve a given task based both on its own individual
experience as well as the experience acquired together with its peers. Our
approach is divided into two phases. In the \emph{collaborative phase}, all the
agents in the population interact with a shared instance of the environment,
where each agent observes the state and independently proposes an action. Then,
voting is performed to choose the action that will actually be performed in the
environment. In the \emph{individual phase}, each agent refines its individual
performance by interacting with its own instance of the environment. This
mechanism makes the agents experience a larger number of episodes while
simultaneously reducing the computational cost of the process. Our results on
six well-known benchmarks show that SIRL reaches state-of-the-art performance
w.r.t. the alternative interpretable methods from the literature.
\\ ( https://arxiv.org/abs/2401.15480 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15482
Date: Sat, 27 Jan 2024 19:07:49 GMT   (21914kb,D)

Title: Unsupervised Solution Operator Learning for Mean-Field Games via
  Sampling-Invariant Parametrizations
Authors: Han Huang, Rongjie Lai
Categories: cs.LG math.OC
\\
  Recent advances in deep learning has witnessed many innovative frameworks
that solve high dimensional mean-field games (MFG) accurately and efficiently.
These methods, however, are restricted to solving single-instance MFG and
demands extensive computational time per instance, limiting practicality. To
overcome this, we develop a novel framework to learn the MFG solution operator.
Our model takes a MFG instances as input and output their solutions with one
forward pass. To ensure the proposed parametrization is well-suited for
operator learning, we introduce and prove the notion of sampling invariance for
our model, establishing its convergence to a continuous operator in the
sampling limit. Our method features two key advantages. First, it is
discretization-free, making it particularly suitable for learning operators of
high-dimensional MFGs. Secondly, it can be trained without the need for access
to supervised labels, significantly reducing the computational overhead
associated with creating training datasets in existing operator learning
methods. We test our framework on synthetic and realistic datasets with varying
complexity and dimensionality to substantiate its robustness.
\\ ( https://arxiv.org/abs/2401.15482 ,  21914kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15500
Date: Sat, 27 Jan 2024 20:41:55 GMT   (99kb)

Title: Data-Driven Estimation of the False Positive Rate of the Bayes Binary
  Classifier via Soft Labels
Authors: Minoh Jeong, Martina Cardone, Alex Dytso
Categories: cs.LG cs.IT math.IT stat.ML
\\
  Classification is a fundamental task in many applications on which
data-driven methods have shown outstanding performances. However, it is
challenging to determine whether such methods have achieved the optimal
performance. This is mainly because the best achievable performance is
typically unknown and hence, effectively estimating it is of prime importance.
In this paper, we consider binary classification problems and we propose an
estimator for the false positive rate (FPR) of the Bayes classifier, that is,
the optimal classifier with respect to accuracy, from a given dataset. Our
method utilizes soft labels, or real-valued labels, which are gaining
significant traction thanks to their properties. We thoroughly examine various
theoretical properties of our estimator, including its consistency,
unbiasedness, rate of convergence, and variance. To enhance the versatility of
our estimator beyond soft labels, we also consider noisy labels, which
encompass binary labels. For noisy labels, we develop effective FPR estimators
by leveraging a denoising technique and the Nadaraya-Watson estimator. Due to
the symmetry of the problem, our results can be readily applied to estimate the
false negative rate of the Bayes classifier.
\\ ( https://arxiv.org/abs/2401.15500 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15520
Date: Sat, 27 Jan 2024 22:45:02 GMT   (44kb)

Title: Oracle-Efficient Hybrid Online Learning with Unknown Distribution
Authors: Changlong Wu, Jin Sima, Wojciech Szpankowski
Categories: cs.LG stat.ML
\\
  We study the problem of oracle-efficient hybrid online learning when the
features are generated by an unknown i.i.d. process and the labels are
generated adversarially. Assuming access to an (offline) ERM oracle, we show
that there exists a computationally efficient online predictor that achieves a
regret upper bounded by $\tilde{O}(T^{\frac{3}{4}})$ for a finite-VC class, and
upper bounded by $\tilde{O}(T^{\frac{p+1}{p+2}})$ for a class with $\alpha$
fat-shattering dimension $\alpha^{-p}$. This provides the first known
oracle-efficient sublinear regret bounds for hybrid online learning with an
unknown feature generation process. In particular, it confirms a conjecture of
Lazaric and Munos (JCSS 2012). We then extend our result to the scenario of
shifting distributions with $K$ changes, yielding a regret of order
$\tilde{O}(T^{\frac{4}{5}}K^{\frac{1}{5}})$. Finally, we establish a regret of
$\tilde{O}((K^{\frac{2}{3}}(\log|\mathcal{H}|)^{\frac{1}{3}}+K)\cdot
T^{\frac{4}{5}})$ for the contextual $K$-armed bandits with a finite policy set
$\mathcal{H}$, i.i.d. generated contexts from an unknown distribution, and
adversarially generated costs.
\\ ( https://arxiv.org/abs/2401.15520 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15530
Date: Sun, 28 Jan 2024 00:36:44 GMT   (65kb)

Title: An Information-Theoretic Analysis of In-Context Learning
Authors: Hong Jun Jeon, Jason D. Lee, Qi Lei, Benjamin Van Roy
Categories: cs.LG cs.IT math.IT
\\
  Previous theoretical results pertaining to meta-learning on sequences build
on contrived assumptions and are somewhat convoluted. We introduce new
information-theoretic tools that lead to an elegant and very general
decomposition of error into three components: irreducible error, meta-learning
error, and intra-task error. These tools unify analyses across many
meta-learning challenges. To illustrate, we apply them to establish new results
about in-context learning with transformers. Our theoretical results
characterizes how error decays in both the number of training sequences and
sequence lengths. Our results are very general; for example, they avoid
contrived mixing time assumptions made by all prior results that establish
decay of error with sequence length.
\\ ( https://arxiv.org/abs/2401.15530 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15543
Date: Sun, 28 Jan 2024 02:09:26 GMT   (1211kb)

Title: Anomaly Detection of Particle Orbit in Accelerator using LSTM Deep
  Learning Technology
Authors: Zhiyuan Chen, Wei Lu, Radhika Bhong, Yimin Hu, Brian Freeman, Adam
  Carpenter
Categories: cs.LG physics.acc-ph
Comments: 6 pages
ACM-class: I.5.4
\\
  A stable, reliable, and controllable orbit lock system is crucial to an
electron (or ion) accelerator because the beam orbit and beam energy
instability strongly affect the quality of the beam delivered to experimental
halls. Currently, when the orbit lock system fails operators must manually
intervene. This paper develops a Machine Learning based fault detection
methodology to identify orbit lock anomalies and notify accelerator operations
staff of the off-normal behavior. Our method is unsupervised, so it does not
require labeled data. It uses Long-Short Memory Networks (LSTM) Auto Encoder to
capture normal patterns and predict future values of monitoring sensors in the
orbit lock system. Anomalies are detected when the prediction error exceeds a
threshold. We conducted experiments using monitoring data from Jefferson Lab's
Continuous Electron Beam Accelerator Facility (CEBAF). The results are
promising: the percentage of real anomalies identified by our solution is
68.6%-89.3% using monitoring data of a single component in the orbit lock
control system. The accuracy can be as high as 82%.
\\ ( https://arxiv.org/abs/2401.15543 ,  1211kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15544
Date: Sun, 28 Jan 2024 02:26:58 GMT   (598kb)

Title: Analog and Multi-modal Manufacturing Datasets Acquired on the Future
  Factories Platform
Authors: Ramy Harik, Fadi El Kalach, Jad Samaha, Devon Clark, Drew Sander,
  Philip Samaha, Liam Burns, Ibrahim Yousif, Victor Gadow, Theodros Tarekegne,
  Nitol Saha
Categories: cs.LG
Comments: 11 pages, datasets for Future Factories
\\
  Two industry-grade datasets are presented in this paper that were collected
at the Future Factories Lab at the University of South Carolina on December
11th and 12th of 2023. These datasets are generated by a manufacturing assembly
line that utilizes industrial standards with respect to actuators, control
mechanisms, and transducers. The two datasets were both generated
simultaneously by operating the assembly line for 30 consecutive hours (with
minor filtering) and collecting data from sensors equipped throughout the
system. During operation, defects were also introduced into the assembly
operation by manually removing parts needed for the final assembly. The
datasets generated include a time series analog dataset and the other is a time
series multi-modal dataset which includes images of the system alongside the
analog data. These datasets were generated with the objective of providing
tools to further the research towards enhancing intelligence in manufacturing.
Real manufacturing datasets can be scarce let alone datasets with anomalies or
defects. As such these datasets hope to address this gap and provide
researchers with a foundation to build and train Artificial Intelligence models
applicable for the manufacturing industry. Finally, these datasets are the
first iteration of published data from the future Factories lab and can be
further adjusted to fit more researchers needs moving forward.
\\ ( https://arxiv.org/abs/2401.15544 ,  598kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15584
Date: Sun, 28 Jan 2024 06:43:13 GMT   (4255kb,D)

Title: DGNN: Decoupled Graph Neural Networks with Structural Consistency
  between Attribute and Graph Embedding Representations
Authors: Jinlu Wang, Jipeng Guo, Yanfeng Sun, Junbin Gao, Shaofan Wang, Yachao
  Yang, Baocai Yin
Categories: cs.LG
\\
  Graph neural networks (GNNs) demonstrate a robust capability for
representation learning on graphs with complex structures, showcasing superior
performance in various applications. The majority of existing GNNs employ a
graph convolution operation by using both attribute and structure information
through coupled learning. In essence, GNNs, from an optimization perspective,
seek to learn a consensus and compromise embedding representation that balances
attribute and graph information, selectively exploring and retaining valid
information. To obtain a more comprehensive embedding representation of nodes,
a novel GNNs framework, dubbed Decoupled Graph Neural Networks (DGNN), is
introduced. DGNN explores distinctive embedding representations from the
attribute and graph spaces by decoupled terms. Considering that semantic graph,
constructed from attribute feature space, consists of different node connection
information and provides enhancement for the topological graph, both
topological and semantic graphs are combined for the embedding representation
learning. Further, structural consistency among attribute embedding and graph
embeddings is promoted to effectively remove redundant information and
establish soft connection. This involves promoting factor sharing for adjacency
reconstruction matrices, facilitating the exploration of a consensus and
high-level correlation. Finally, a more powerful and complete representation is
achieved through the concatenation of these embeddings. Experimental results
conducted on several graph benchmark datasets verify its superiority in node
classification task.
\\ ( https://arxiv.org/abs/2401.15584 ,  4255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15587
Date: Sun, 28 Jan 2024 07:05:30 GMT   (497kb)

Title: Hyperedge Interaction-aware Hypergraph Neural Network
Authors: Xiaobing Pei, Rongping Ye, Haoran Yang, Ruiqi Wang
Categories: cs.LG
\\
  Hypergraphs provide an effective modeling approach for modeling high-order
relationships in many real-world datasets. To capture such complex
relationships, several hypergraph neural networks have been proposed for
learning hypergraph structure, which propagate information from nodes to
hyperedges and then from hyperedges back to nodes. However, most existing
methods focus on information propagation between hyperedges and nodes,
neglecting the interactions among hyperedges themselves. In this paper, we
propose HeIHNN, a hyperedge interaction-aware hypergraph neural network, which
captures the interactions among hyperedges during the convolution process and
introduce a novel mechanism to enhance information flow between hyperedges and
nodes. Specifically, HeIHNN integrates the interactions between hyperedges into
the hypergraph convolution by constructing a three-stage information
propagation process. After propagating information from nodes to hyperedges, we
introduce a hyperedge-level convolution to update the hyperedge embeddings.
Finally, the embeddings that capture rich information from the interaction
among hyperedges will be utilized to update the node embeddings. Additionally,
we introduce a hyperedge outlier removal mechanism in the information
propagation stages between nodes and hyperedges, which dynamically adjusts the
hypergraph structure using the learned embeddings, effectively removing
outliers. Extensive experiments conducted on real-world datasets show the
competitive performance of HeIHNN compared with state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.15587 ,  497kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15603
Date: Sun, 28 Jan 2024 08:12:00 GMT   (1962kb,D)

Title: Improving Expressive Power of Spectral Graph Neural Networks with
  Eigenvalue Correction
Authors: Kangkang Lu, Yanhua Yu, Hao Fei, Xuan Li, Zixuan Yang, Zirui Guo,
  Meiyu Liang, Mengran Yin and Tat-Seng Chua
Categories: cs.LG
Comments: Accepted by AAAI-24
\\
  In recent years, spectral graph neural networks, characterized by polynomial
filters, have garnered increasing attention and have achieved remarkable
performance in tasks such as node classification. These models typically assume
that eigenvalues for the normalized Laplacian matrix are distinct from each
other, thus expecting a polynomial filter to have a high fitting ability.
However, this paper empirically observes that normalized Laplacian matrices
frequently possess repeated eigenvalues. Moreover, we theoretically establish
that the number of distinguishable eigenvalues plays a pivotal role in
determining the expressive power of spectral graph neural networks. In light of
this observation, we propose an eigenvalue correction strategy that can free
polynomial filters from the constraints of repeated eigenvalue inputs.
Concretely, the proposed eigenvalue correction strategy enhances the uniform
distribution of eigenvalues, thus mitigating repeated eigenvalues, and
improving the fitting capacity and expressive power of polynomial filters.
Extensive experimental results on both synthetic and real-world datasets
demonstrate the superiority of our method.
\\ ( https://arxiv.org/abs/2401.15603 ,  1962kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15604
Date: Sun, 28 Jan 2024 08:13:56 GMT   (64kb)

Title: Neural Network-Based Score Estimation in Diffusion Models: Optimization
  and Generalization
Authors: Yinbin Han, Meisam Razaviyayn, Renyuan Xu
Categories: cs.LG stat.ML
Comments: 38 pages
\\
  Diffusion models have emerged as a powerful tool rivaling GANs in generating
high-quality samples with improved fidelity, flexibility, and robustness. A key
component of these models is to learn the score function through score
matching. Despite empirical success on various tasks, it remains unclear
whether gradient-based algorithms can learn the score function with a provable
accuracy. As a first step toward answering this question, this paper
establishes a mathematical framework for analyzing score estimation using
neural networks trained by gradient descent. Our analysis covers both the
optimization and the generalization aspects of the learning procedure. In
particular, we propose a parametric form to formulate the denoising
score-matching problem as a regression with noisy labels. Compared to the
standard supervised learning setup, the score-matching problem introduces
distinct challenges, including unbounded input, vector-valued output, and an
additional time variable, preventing existing techniques from being applied
directly. In this paper, we show that with a properly designed neural network
architecture, the score function can be accurately approximated by a
reproducing kernel Hilbert space induced by neural tangent kernels.
Furthermore, by applying an early-stopping rule for gradient descent and
leveraging certain coupling arguments between neural network training and
kernel regression, we establish the first generalization error (sample
complexity) bounds for learning the score function despite the presence of
noise in the observations. Our analysis is grounded in a novel parametric form
of the neural network and an innovative connection between score matching and
regression analysis, facilitating the application of advanced statistical and
optimization techniques.
\\ ( https://arxiv.org/abs/2401.15604 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15610
Date: Sun, 28 Jan 2024 09:38:14 GMT   (113kb,D)

Title: Prevalidated ridge regression is a highly-efficient drop-in replacement
  for logistic regression for high-dimensional data
Authors: Angus Dempster, Geoffrey I. Webb, Daniel F. Schmidt
Categories: cs.LG stat.ML
Comments: 13 pages, 11 figures
\\
  Logistic regression is a ubiquitous method for probabilistic classification.
However, the effectiveness of logistic regression depends upon careful and
relatively computationally expensive tuning, especially for the regularisation
hyperparameter, and especially in the context of high-dimensional data. We
present a prevalidated ridge regression model that closely matches logistic
regression in terms of classification error and log-loss, particularly for
high-dimensional data, while being significantly more computationally efficient
and having effectively no hyperparameters beyond regularisation. We scale the
coefficients of the model so as to minimise log-loss for a set of prevalidated
predictions derived from the estimated leave-one-out cross-validation error.
This exploits quantities already computed in the course of fitting the ridge
regression model in order to find the scaling parameter with nominal additional
computational expense.
\\ ( https://arxiv.org/abs/2401.15610 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15615
Date: Sun, 28 Jan 2024 10:03:37 GMT   (17kb)

Title: Addressing Noise and Efficiency Issues in Graph-Based Machine Learning
  Models From the Perspective of Adversarial Attack
Authors: Yongyu Wang
Categories: cs.LG cs.CV
\\
  Given that no existing graph construction method can generate a perfect graph
for a given dataset, graph-based algorithms are invariably affected by the
plethora of redundant and erroneous edges present within the constructed
graphs. In this paper, we propose treating these noisy edges as adversarial
attack and use a spectral adversarial robustness evaluation method to diminish
the impact of noisy edges on the performance of graph algorithms. Our method
identifies those points that are less vulnerable to noisy edges and leverages
only these robust points to perform graph-based algorithms. Our experiments
with spectral clustering, one of the most representative and widely utilized
graph algorithms, reveal that our methodology not only substantially elevates
the precision of the algorithm but also greatly accelerates its computational
efficiency by leveraging only a select number of robust data points.
\\ ( https://arxiv.org/abs/2401.15615 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15617
Date: Sun, 28 Jan 2024 10:09:05 GMT   (3266kb,D)

Title: Diffusion-based graph generative methods
Authors: Hongyang Chen, Can Xu, Lingyu Zheng, Qiang Zhang, Xuemin Lin
Categories: cs.LG cs.AI
\\
  Being the most cutting-edge generative methods, diffusion methods have shown
great advances in wide generation tasks. Among them, graph generation attracts
significant research attention for its broad application in real life. In our
survey, we systematically and comprehensively review on diffusion-based graph
generative methods. We first make a review on three mainstream paradigms of
diffusion methods, which are denoising diffusion probabilistic models,
score-based genrative models, and stochastic differential equations. Then we
further categorize and introduce the latest applications of diffusion models on
graphs. In the end, we point out some limitations of current studies and future
directions of future explorations. The summary of existing methods metioned in
this survey is in
https://github.com/zhejiangzhuque/Diffusion-based-Graph-Generative-Methods.
\\ ( https://arxiv.org/abs/2401.15617 ,  3266kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15672
Date: Sun, 28 Jan 2024 14:39:43 GMT   (549kb)

Title: Evaluating Echo State Network for Parkinson's Disease Prediction using
  Voice Features
Authors: Seyedeh Zahra Seyedi Hosseininian, Ahmadreza Tajari, Mohsen
  Ghalehnoie, Alireza Alfi
Categories: cs.LG cs.SD eess.AS
\\
  Parkinson's disease (PD) is a debilitating neurological disorder that
necessitates precise and early diagnosis for effective patient care. This study
aims to develop a diagnostic model capable of achieving both high accuracy and
minimizing false negatives, a critical factor in clinical practice. Given the
limited training data, a feature selection strategy utilizing ANOVA is employed
to identify the most informative features. Subsequently, various machine
learning methods, including Echo State Networks (ESN), Random Forest, k-nearest
Neighbors, Support Vector Classifier, Extreme Gradient Boosting, and Decision
Tree, are employed and thoroughly evaluated. The statistical analyses of the
results highlight ESN's exceptional performance, showcasing not only superior
accuracy but also the lowest false negative rate among all methods.
Consistently, statistical data indicates that the ESN method consistently
maintains a false negative rate of less than 8% in 83% of cases. ESN's capacity
to strike a delicate balance between diagnostic precision and minimizing
misclassifications positions it as an exemplary choice for PD diagnosis,
especially in scenarios characterized by limited data. This research marks a
significant step towards more efficient and reliable PD diagnosis, with
potential implications for enhanced patient outcomes and healthcare dynamics.
\\ ( https://arxiv.org/abs/2401.15672 ,  549kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15691
Date: Sun, 28 Jan 2024 16:30:13 GMT   (1332kb,D)

Title: One for all: A novel Dual-space Co-training baseline for Large-scale
  Multi-View Clustering
Authors: Zisen Kong, Zhiqiang Fu, Dongxia Chang, Yiming Wang, Yao Zhao
Categories: cs.LG
\\
  In this paper, we propose a novel multi-view clustering model, named
Dual-space Co-training Large-scale Multi-view Clustering (DSCMC). The main
objective of our approach is to enhance the clustering performance by
leveraging co-training in two distinct spaces. In the original space, we learn
a projection matrix to obtain latent consistent anchor graphs from different
views. This process involves capturing the inherent relationships and
structures between data points within each view. Concurrently, we employ a
feature transformation matrix to map samples from various views to a shared
latent space. This transformation facilitates the alignment of information from
multiple views, enabling a comprehensive understanding of the underlying data
distribution. We jointly optimize the construction of the latent consistent
anchor graph and the feature transformation to generate a discriminative anchor
graph. This anchor graph effectively captures the essential characteristics of
the multi-view data and serves as a reliable basis for subsequent clustering
analysis. Moreover, the element-wise method is proposed to avoid the impact of
diverse information between different views. Our algorithm has an approximate
linear computational complexity, which guarantees its successful application on
large-scale datasets. Through experimental validation, we demonstrate that our
method significantly reduces computational complexity while yielding superior
clustering performance compared to existing approaches.
\\ ( https://arxiv.org/abs/2401.15691 ,  1332kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15713
Date: Sun, 28 Jan 2024 17:34:42 GMT   (811kb,D)

Title: Contrastive Learning and Mixture of Experts Enables Precise Vector
  Embeddings
Authors: Rohan Kapur, Logan Hallee, Arjun Patel, Bohdan Khomtchouk
Categories: cs.LG cs.AI cs.CL
\\
  The advancement of transformer neural networks has significantly elevated the
capabilities of sentence similarity models, particularly in creating effective
vector representations of natural language inputs. However, these models face
notable challenges in domain-specific contexts, especially in highly
specialized scientific sub-fields. Traditional methods often struggle in this
regime, either overgeneralizing similarities within a niche or being overly
sensitive to minor differences, resulting in inaccurate text classification and
subpar vector representation. In an era where retrieval augmentation and search
are increasingly crucial, precise and concise numerical representations are
essential. In this paper, we target this issue by assembling niche datasets
using co-citations as a similarity metric, focusing on biomedical domains. We
employ two key strategies for fine-tuning state-of-the-art models: 1.
Domain-specific Fine-Tuning, which tailors pretrained models to a single
domain, and 2. Universal Applicability with Mixture of Experts (MoE), adapting
pretrained models with enforced routing for multiple domains simultaneously.
Our training approach emphasizes the use of abstracts for faster training,
incorporating Multiple Negative Rankings loss for efficient contrastive
learning. Notably, our MoE variants, equipped with $N$ experts, achieve the
efficacy of $N$ individual models, heralding a new era of versatile,
One-Size-Fits-All transformer networks for various tasks. This methodology
marks significant advancements in scientific text classification metrics and
holds promise for enhancing vector database search and compilation.
\\ ( https://arxiv.org/abs/2401.15713 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15773
Date: Sun, 28 Jan 2024 21:23:13 GMT   (10475kb)

Title: Evaluation of k-means time series clustering based on z-normalization
  and NP-Free
Authors: Ming-Chang Lee, Jia-Chun Lin, and Volker Stolz
Categories: cs.LG cs.AI
Comments: 12 pages, 6 figures, 8 tables, 13th International Conference on
  Pattern Recognition Applications and Methods (ICPRAM 2024)
\\
  Despite the widespread use of k-means time series clustering in various
domains, there exists a gap in the literature regarding its comprehensive
evaluation with different time series normalization approaches. This paper
seeks to fill this gap by conducting a thorough performance evaluation of
k-means time series clustering on real-world open-source time series datasets.
The evaluation focuses on two distinct normalization techniques:
z-normalization and NP-Free. The former is one of the most commonly used
normalization approach for time series. The latter is a real-time time series
representation approach, which can serve as a time series normalization
approach. The primary objective of this paper is to assess the impact of these
two normalization techniques on k-means time series clustering in terms of its
clustering quality. The experiments employ the silhouette score, a
well-established metric for evaluating the quality of clusters in a dataset. By
systematically investigating the performance of k-means time series clustering
with these two normalization techniques, this paper addresses the current gap
in k-means time series clustering evaluation and contributes valuable insights
to the development of time series clustering.
\\ ( https://arxiv.org/abs/2401.15773 ,  10475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15814
Date: Mon, 29 Jan 2024 00:29:39 GMT   (768kb,D)

Title: OntoMedRec: Logically-Pretrained Model-Agnostic Ontology Encoders for
  Medication Recommendation
Authors: Weicong Tan, Weiqing Wang, Xin Zhou, Wray Buntine, Gordon Bingham
Categories: cs.LG
\\
  Most existing medication recommendation models learn representations for
medical concepts based on electronic health records (EHRs) and make
recommendations with learnt representations. However, most medications appear
in the dataset for limited times, resulting in insufficient learning of their
representations. Medical ontologies are the hierarchical classification systems
for medical terms where similar terms are in the same class on a certain level.
In this paper, we propose OntoMedRec, the logically-pretrained and
model-agnostic medical Ontology Encoders for Medication Recommendation that
addresses data sparsity problem with medical ontologies. We conduct
comprehensive experiments on benchmark datasets to evaluate the effectiveness
of OntoMedRec, and the result shows the integration of OntoMedRec improves the
performance of various models in both the entire EHR datasets and the
admissions with few-shot medications. We provide the GitHub repository for the
source code on https://anonymous.4open.science/r/OntoMedRec-D123
\\ ( https://arxiv.org/abs/2401.15814 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15846
Date: Mon, 29 Jan 2024 02:42:22 GMT   (1084kb,D)

Title: Meta-Learning for Neural Network-based Temporal Point Processes
Authors: Yoshiaki Takimoto, Yusuke Tanaka, Tomoharu Iwata, Maya Okawa, Hideaki
  Kim, Hiroyuki Toda, Takeshi Kurashima
Categories: cs.LG stat.ML
\\
  Human activities generate various event sequences such as taxi trip records,
bike-sharing pick-ups, crime occurrence, and infectious disease transmission.
The point process is widely used in many applications to predict such events
related to human activities. However, point processes present two problems in
predicting events related to human activities. First, recent high-performance
point process models require the input of sufficient numbers of events
collected over a long period (i.e., long sequences) for training, which are
often unavailable in realistic situations. Second, the long-term predictions
required in real-world applications are difficult. To tackle these problems, we
propose a novel meta-learning approach for periodicity-aware prediction of
future events given short sequences. The proposed method first embeds short
sequences into hidden representations (i.e., task representations) via
recurrent neural networks for creating predictions from short sequences. It
then models the intensity of the point process by monotonic neural networks
(MNNs), with the input being the task representations. We transfer the prior
knowledge learned from related tasks and can improve event prediction given
short sequences of target tasks. We design the MNNs to explicitly take temporal
periodic patterns into account, contributing to improved long-term prediction
performance. Experiments on multiple real-world datasets demonstrate that the
proposed method has higher prediction performance than existing alternatives.
\\ ( https://arxiv.org/abs/2401.15846 ,  1084kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15856
Date: Mon, 29 Jan 2024 03:07:04 GMT   (3954kb,D)

Title: Look Around! Unexpected gains from training on environments in the
  vicinity of the target
Authors: Serena Bono, Spandan Madan, Ishaan Grover, Mao Yasueda, Cynthia
  Breazeal, Hanspeter Pfister, Gabriel Kreiman
Categories: cs.LG cs.AI
\\
  Solutions to Markov Decision Processes (MDP) are often very sensitive to
state transition probabilities. As the estimation of these probabilities is
often inaccurate in practice, it is important to understand when and how
Reinforcement Learning (RL) agents generalize when transition probabilities
change. Here we present a new methodology to evaluate such generalization of RL
agents under small shifts in the transition probabilities. Specifically, we
evaluate agents in new environments (MDPs) in the vicinity of the training MDP
created by adding quantifiable, parametric noise into the transition function
of the training MDP. We refer to this process as Noise Injection, and the
resulting environments as $\delta$-environments. This process allows us to
create controlled variations of the same environment with the level of the
noise serving as a metric of distance between environments. Conventional wisdom
suggests that training and testing on the same MDP should yield the best
results. However, we report several cases of the opposite -- when targeting a
specific environment, training the agent in an alternative noise setting can
yield superior outcomes. We showcase this phenomenon across $60$ different
variations of ATARI games, including PacMan, Pong, and Breakout.
\\ ( https://arxiv.org/abs/2401.15856 ,  3954kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15866
Date: Mon, 29 Jan 2024 03:42:37 GMT   (2290kb,D)

Title: Stochastic Amortization: A Unified Approach to Accelerate Feature and
  Data Attribution
Authors: Ian Covert, Chanwoo Kim, Su-In Lee, James Zou, Tatsunori Hashimoto
Categories: cs.LG
\\
  Many tasks in explainable machine learning, such as data valuation and
feature attribution, perform expensive computation for each data point and can
be intractable for large datasets. These methods require efficient
approximations, and learning a network that directly predicts the desired
output, which is commonly known as amortization, is a promising solution.
However, training such models with exact labels is often intractable; we
therefore explore training with noisy labels and find that this is inexpensive
and surprisingly effective. Through theoretical analysis of the label noise and
experiments with various models and datasets, we show that this approach
significantly accelerates several feature attribution and data valuation
methods, often yielding an order of magnitude speedup over existing approaches.
\\ ( https://arxiv.org/abs/2401.15866 ,  2290kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15872
Date: Mon, 29 Jan 2024 04:11:56 GMT   (10147kb,D)

Title: A Deep Q-Network Based on Radial Basis Functions for Multi-Echelon
  Inventory Management
Authors: Liqiang Cheng, Jun Luo, Weiwei Fan, Yidong Zhang, Yuan Li
Categories: cs.LG
\\
  This paper addresses a multi-echelon inventory management problem with a
complex network topology where deriving optimal ordering decisions is
difficult. Deep reinforcement learning (DRL) has recently shown potential in
solving such problems, while designing the neural networks in DRL remains a
challenge. In order to address this, a DRL model is developed whose Q-network
is based on radial basis functions. The approach can be more easily constructed
compared to classic DRL models based on neural networks, thus alleviating the
computational burden of hyperparameter tuning. Through a series of simulation
experiments, the superior performance of this approach is demonstrated compared
to the simple base-stock policy, producing a better policy in the multi-echelon
system and competitive performance in the serial system where the base-stock
policy is optimal. In addition, the approach outperforms current DRL
approaches.
\\ ( https://arxiv.org/abs/2401.15872 ,  10147kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15879
Date: Mon, 29 Jan 2024 04:21:47 GMT   (4708kb,D)

Title: lil'HDoC: An Algorithm for Good Arm Identification under Small Threshold
  Gap
Authors: Tzu-Hsien Tsai, Yun-Da Tsai, Shou-De Lin
Categories: cs.LG stat.ML
\\
  Good arm identification (GAI) is a pure-exploration bandit problem in which a
single learner outputs an arm as soon as it is identified as a good arm. A good
arm is defined as an arm with an expected reward greater than or equal to a
given threshold. This paper focuses on the GAI problem under a small threshold
gap, which refers to the distance between the expected rewards of arms and the
given threshold. We propose a new algorithm called lil'HDoC to significantly
improve the total sample complexity of the HDoC algorithm. We demonstrate that
the sample complexity of the first $\lambda$ output arm in lil'HDoC is bounded
by the original HDoC algorithm, except for one negligible term, when the
distance between the expected reward and threshold is small. Extensive
experiments confirm that our algorithm outperforms the state-of-the-art
algorithms in both synthetic and real-world datasets.
\\ ( https://arxiv.org/abs/2401.15879 ,  4708kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15894
Date: Mon, 29 Jan 2024 05:26:17 GMT   (892kb,D)

Title: A Gated MLP Architecture for Learning Topological Dependencies in
  Spatio-Temporal Graphs
Authors: Yun Young Choi, Minho Lee, Sun Woo Park, Seunghwan Lee, Joohwan Ko
Categories: cs.LG cs.AI
\\
  Graph Neural Networks (GNNs) and Transformer have been increasingly adopted
to learn the complex vector representations of spatio-temporal graphs,
capturing intricate spatio-temporal dependencies crucial for applications such
as traffic datasets. Although many existing methods utilize multi-head
attention mechanisms and message-passing neural networks (MPNNs) to capture
both spatial and temporal relations, these approaches encode temporal and
spatial relations independently, and reflect the graph's topological
characteristics in a limited manner. In this work, we introduce the Cycle to
Mixer (Cy2Mixer), a novel spatio-temporal GNN based on topological non-trivial
invariants of spatio-temporal graphs with gated multi-layer perceptrons (gMLP).
The Cy2Mixer is composed of three blocks based on MLPs: A message-passing block
for encapsulating spatial information, a cycle message-passing block for
enriching topological information through cyclic subgraphs, and a temporal
block for capturing temporal properties. We bolster the effectiveness of
Cy2Mixer with mathematical evidence emphasizing that our cycle message-passing
block is capable of offering differentiated information to the deep learning
model compared to the message-passing block. Furthermore, empirical evaluations
substantiate the efficacy of the Cy2Mixer, demonstrating state-of-the-art
performances across various traffic benchmark datasets.
\\ ( https://arxiv.org/abs/2401.15894 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15903
Date: Mon, 29 Jan 2024 06:10:54 GMT   (5115kb,D)

Title: Toward the Identifiability of Comparative Deep Generative Models
Authors: Romain Lopez, Jan-Christian Huetter, Ehsan Hajiramezanali, Jonathan
  Pritchard and Aviv Regev
Categories: cs.LG q-bio.GN stat.ME
Comments: 45 pages, 3 figures
Journal-ref: Causal Learning and Reasoning 2024
\\
  Deep Generative Models (DGMs) are versatile tools for learning data
representations while adequately incorporating domain knowledge such as the
specification of conditional probability distributions. Recently proposed DGMs
tackle the important task of comparing data sets from different sources. One
such example is the setting of contrastive analysis that focuses on describing
patterns that are enriched in a target data set compared to a background data
set. The practical deployment of those models often assumes that DGMs naturally
infer interpretable and modular latent representations, which is known to be an
issue in practice. Consequently, existing methods often rely on ad-hoc
regularization schemes, although without any theoretical grounding. Here, we
propose a theory of identifiability for comparative DGMs by extending recent
advances in the field of non-linear independent component analysis. We show
that, while these models lack identifiability across a general class of mixing
functions, they surprisingly become identifiable when the mixing function is
piece-wise affine (e.g., parameterized by a ReLU neural network). We also
investigate the impact of model misspecification, and empirically show that
previously proposed regularization techniques for fitting comparative DGMs help
with identifiability when the number of latent variables is not known in
advance. Finally, we introduce a novel methodology for fitting comparative DGMs
that improves the treatment of multiple data sources via multi-objective
optimization and that helps adjust the hyperparameter for the regularization in
an interpretable manner, using constrained optimization. We empirically
validate our theory and new methodology using simulated data as well as a
recent data set of genetic perturbations in cells profiled via single-cell RNA
sequencing.
\\ ( https://arxiv.org/abs/2401.15903 ,  5115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15917
Date: Mon, 29 Jan 2024 07:04:48 GMT   (548kb,D)

Title: Blockchain-enabled Trustworthy Federated Unlearning
Authors: Yijing Lin, Zhipeng Gao, Hongyang Du, Jinke Ren, Zhiqiang Xie, Dusit
  Niyato
Categories: cs.LG cs.CR
\\
  Federated unlearning is a promising paradigm for protecting the data
ownership of distributed clients. It allows central servers to remove
historical data effects within the machine learning model as well as address
the "right to be forgotten" issue in federated learning. However, existing
works require central servers to retain the historical model parameters from
distributed clients, such that allows the central server to utilize these
parameters for further training even, after the clients exit the training
process. To address this issue, this paper proposes a new blockchain-enabled
trustworthy federated unlearning framework. We first design a proof of
federated unlearning protocol, which utilizes the Chameleon hash function to
verify data removal and eliminate the data contributions stored in other
clients' models. Then, an adaptive contribution-based retraining mechanism is
developed to reduce the computational overhead and significantly improve the
training efficiency. Extensive experiments demonstrate that the proposed
framework can achieve a better data removal effect than the state-of-the-art
frameworks, marking a significant stride towards trustworthy federated
unlearning.
\\ ( https://arxiv.org/abs/2401.15917 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15935
Date: Mon, 29 Jan 2024 07:50:28 GMT   (6132kb,D)

Title: Self-Supervised Learning in Event Sequences: A Comparative Study and
  Hybrid Approach of Generative Modeling and Contrastive Learning
Authors: Viktor Moskvoretskii, Dmitry Osin, Egor Shvetsov, Igor Udovichenko,
  Maxim Zhelnin, Andrey Dukhovny, Anna Zhimerikina, Albert Efimov, Evgeny
  Burnaev
Categories: cs.LG cs.AI
Comments: 11 pages, 9 figures
\\
  This study investigates self-supervised learning techniques to obtain
representations of Event Sequences. It is a key modality in various
applications, including but not limited to banking, e-commerce, and healthcare.
  We perform a comprehensive study of generative and contrastive approaches in
self-supervised learning, applying them both independently. We find that there
is no single supreme method. Consequently, we explore the potential benefits of
combining these approaches. To achieve this goal, we introduce a novel method
that aligns generative and contrastive embeddings as distinct modalities,
drawing inspiration from contemporary multimodal research.
  Generative and contrastive approaches are often treated as mutually
exclusive, leaving a gap for their combined exploration. Our results
demonstrate that this aligned model performs at least on par with, and mostly
surpasses, existing methods and is more universal across a variety of tasks.
Furthermore, we demonstrate that self-supervised methods consistently
outperform the supervised approach on our datasets.
\\ ( https://arxiv.org/abs/2401.15935 ,  6132kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15948
Date: Mon, 29 Jan 2024 08:13:51 GMT   (2413kb,D)

Title: AdvNF: Reducing Mode Collapse in Conditional Normalising Flows using
  Adversarial Learning
Authors: Vikas Kanaujia and Mathias S. Scheurer and Vipul Arora
Categories: cs.LG cond-mat.stat-mech physics.comp-ph
Comments: 26 pages, submitted to Scipost Physics
\\
  Deep generative models complement Markov-chain-Monte-Carlo methods for
efficiently sampling from high-dimensional distributions. Among these methods,
explicit generators, such as Normalising Flows (NFs), in combination with the
Metropolis Hastings algorithm have been extensively applied to get unbiased
samples from target distributions. We systematically study central problems in
conditional NFs, such as high variance, mode collapse and data efficiency. We
propose adversarial training for NFs to ameliorate these problems. Experiments
are conducted with low-dimensional synthetic datasets and XY spin models in two
spatial dimensions.
\\ ( https://arxiv.org/abs/2401.15948 ,  2413kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15949
Date: Mon, 29 Jan 2024 08:18:21 GMT   (903kb)

Title: TFDMNet: A Novel Network Structure Combines the Time Domain and
  Frequency Domain Features
Authors: Hengyue Pan, Yixin Chen, Zhiliang Tian, Peng Qiao, Linbo Qiao,
  Dongsheng Li
Categories: cs.LG
Comments: This paper is the updated edition of our paper Learning Convolutional
  Neural Networks in the Frequency Domain (arXiv:2204.06718). Comparing with
  the previous edition, we design a mixture model to get the balance between
  the computation complexity and memory usage
\\
  Convolutional neural network (CNN) has achieved impressive success in
computer vision during the past few decades. The image convolution operation
helps CNNs to get good performance on image-related tasks. However, it also has
high computation complexity and hard to be parallelized. This paper proposes a
novel Element-wise Multiplication Layer (EML) to replace convolution layers,
which can be trained in the frequency domain. Theoretical analyses show that
EMLs lower the computation complexity and easier to be parallelized. Moreover,
we introduce a Weight Fixation mechanism to alleviate the problem of
over-fitting, and analyze the working behavior of Batch Normalization and
Dropout in the frequency domain. To get the balance between the computation
complexity and memory usage, we propose a new network structure, namely
Time-Frequency Domain Mixture Network (TFDMNet), which combines the advantages
of both convolution layers and EMLs. Experimental results imply that TFDMNet
achieves good performance on MNIST, CIFAR-10 and ImageNet databases with less
number of operations comparing with corresponding CNNs.
\\ ( https://arxiv.org/abs/2401.15949 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15952
Date: Mon, 29 Jan 2024 08:27:31 GMT   (1596kb,D)

Title: A Class-aware Optimal Transport Approach with Higher-Order Moment
  Matching for Unsupervised Domain Adaptation
Authors: Tuan Nguyen, Van Nguyen, Trung Le, He Zhao, Quan Hung Tran, Dinh Phung
Categories: cs.LG cs.AI cs.CV
Comments: 18 pages
\\
  Unsupervised domain adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. In this paper, we
introduce a novel approach called class-aware optimal transport (OT), which
measures the OT distance between a distribution over the source
class-conditional distributions and a mixture of source and target data
distribution. Our class-aware OT leverages a cost function that determines the
matching extent between a given data example and a source class-conditional
distribution. By optimizing this cost function, we find the optimal matching
between target examples and source class-conditional distributions, effectively
addressing the data and label shifts that occur between the two domains. To
handle the class-aware OT efficiently, we propose an amortization solution that
employs deep neural networks to formulate the transportation probabilities and
the cost function. Additionally, we propose minimizing class-aware Higher-order
Moment Matching (HMM) to align the corresponding class regions on the source
and target domains. The class-aware HMM component offers an economical
computational approach for accurately evaluating the HMM distance between the
two distributions. Extensive experiments on benchmark datasets demonstrate that
our proposed method significantly outperforms existing state-of-the-art
baselines.
\\ ( https://arxiv.org/abs/2401.15952 ,  1596kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15957
Date: Mon, 29 Jan 2024 08:41:45 GMT   (986kb,D)

Title: Scalable Federated Unlearning via Isolated and Coded Sharding
Authors: Yijing Lin, Zhipeng Gao, Hongyang Du, Dusit Niyato, Gui Gui, Shuguang
  Cui, Jinke Ren
Categories: cs.LG cs.AI cs.CR
\\
  Federated unlearning has emerged as a promising paradigm to erase the
client-level data effect without affecting the performance of collaborative
learning models. However, the federated unlearning process often introduces
extensive storage overhead and consumes substantial computational resources,
thus hindering its implementation in practice. To address this issue, this
paper proposes a scalable federated unlearning framework based on isolated
sharding and coded computing. We first divide distributed clients into multiple
isolated shards across stages to reduce the number of clients being affected.
Then, to reduce the storage overhead of the central server, we develop a coded
computing mechanism by compressing the model parameters across different
shards. In addition, we provide the theoretical analysis of time efficiency and
storage effectiveness for the isolated and coded sharding. Finally, extensive
experiments on two typical learning tasks, i.e., classification and generation,
demonstrate that our proposed framework can achieve better performance than
three state-of-the-art frameworks in terms of accuracy, retraining time,
storage overhead, and F1 scores for resisting membership inference attacks.
\\ ( https://arxiv.org/abs/2401.15957 ,  986kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15964
Date: Mon, 29 Jan 2024 08:49:53 GMT   (3333kb,D)

Title: Spatio-Temporal Attention Graph Neural Network for Remaining Useful Life
  Prediction
Authors: Zhixin Huang and Yujiang He and Bernhard Sick
Categories: cs.LG
Comments: This article has been accepted in the International Conference
  Computational Science & Computational Intelligence (CSCI'23)
\\
  Remaining useful life prediction plays a crucial role in the health
management of industrial systems. Given the increasing complexity of systems,
data-driven predictive models have attracted significant research interest.
Upon reviewing the existing literature, it appears that many studies either do
not fully integrate both spatial and temporal features or employ only a single
attention mechanism. Furthermore, there seems to be inconsistency in the choice
of data normalization methods, particularly concerning operating conditions,
which might influence predictive performance. To bridge these observations,
this study presents the Spatio-Temporal Attention Graph Neural Network. Our
model combines graph neural networks and temporal convolutional neural networks
for spatial and temporal feature extraction, respectively. The cascade of these
extractors, combined with multi-head attention mechanisms for both
spatio-temporal dimensions, aims to improve predictive precision and refine
model explainability. Comprehensive experiments were conducted on the C-MAPSS
dataset to evaluate the impact of unified versus clustering normalization. The
findings suggest that our model performs state-of-the-art results using only
the unified normalization. Additionally, when dealing with datasets with
multiple operating conditions, cluster normalization enhances the performance
of our proposed model by up to 27%.
\\ ( https://arxiv.org/abs/2401.15964 ,  3333kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15973
Date: Mon, 29 Jan 2024 09:04:45 GMT   (1183kb,D)

Title: Sample Weight Estimation Using Meta-Updates for Online Continual
  Learning
Authors: Hamed Hemati, Damian Borth
Categories: cs.LG
\\
  The loss function plays an important role in optimizing the performance of a
learning system. A crucial aspect of the loss function is the assignment of
sample weights within a mini-batch during loss computation. In the context of
continual learning (CL), most existing strategies uniformly treat samples when
calculating the loss value, thereby assigning equal weights to each sample.
While this approach can be effective in certain standard benchmarks, its
optimal effectiveness, particularly in more complex scenarios, remains
underexplored. This is particularly pertinent in training "in the wild," such
as with self-training, where labeling is automated using a reference model.
This paper introduces the Online Meta-learning for Sample Importance (OMSI)
strategy that approximates sample weights for a mini-batch in an online CL
stream using an inner- and meta-update mechanism. This is done by first
estimating sample weight parameters for each sample in the mini-batch, then,
updating the model with the adapted sample weights. We evaluate OMSI in two
distinct experimental settings. First, we show that OMSI enhances both learning
and retained accuracy in a controlled noisy-labeled data stream. Then, we test
the strategy in three standard benchmarks and compare it with other popular
replay-based strategies. This research aims to foster the ongoing exploration
in the area of self-adaptive CL.
\\ ( https://arxiv.org/abs/2401.15973 ,  1183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15989
Date: Mon, 29 Jan 2024 09:19:49 GMT   (6926kb,D)

Title: Deep Embedding Clustering Driven by Sample Stability
Authors: Zhanwen Cheng, Feijiang Li, Jieting Wang, and Yuhua Qian
Categories: cs.LG
Comments: 8 pages,5 figures,submitted to a conference
\\
  Deep clustering methods improve the performance of clustering tasks by
jointly optimizing deep representation learning and clustering. While numerous
deep clustering algorithms have been proposed, most of them rely on
artificially constructed pseudo targets for performing clustering. This
construction process requires some prior knowledge, and it is challenging to
determine a suitable pseudo target for clustering. To address this issue, we
propose a deep embedding clustering algorithm driven by sample stability
(DECS), which eliminates the requirement of pseudo targets. Specifically, we
start by constructing the initial feature space with an autoencoder and then
learn the cluster-oriented embedding feature constrained by sample stability.
The sample stability aims to explore the deterministic relationship between
samples and all cluster centroids, pulling samples to their respective clusters
and keeping them away from other clusters with high determinacy. We analyzed
the convergence of the loss using Lipschitz continuity in theory, which
verifies the validity of the model. The experimental results on five datasets
illustrate that the proposed method achieves superior performance compared to
state-of-the-art clustering approaches.
\\ ( https://arxiv.org/abs/2401.15989 ,  6926kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16011
Date: Mon, 29 Jan 2024 10:00:53 GMT   (5279kb,D)

Title: GPS: Graph Contrastive Learning via Multi-scale Augmented Views from
  Adversarial Pooling
Authors: Wei Ju, Yiyang Gu, Zhengyang Mao, Ziyue Qiao, Yifang Qin, Xiao Luo,
  Hui Xiong, and Ming Zhang
Categories: cs.LG cs.AI cs.SI
Comments: Accepted by SCIENCE CHINA Information Sciences (SCIS 2024)
\\
  Self-supervised graph representation learning has recently shown considerable
promise in a range of fields, including bioinformatics and social networks. A
large number of graph contrastive learning approaches have shown promising
performance for representation learning on graphs, which train models by
maximizing agreement between original graphs and their augmented views (i.e.,
positive views). Unfortunately, these methods usually involve pre-defined
augmentation strategies based on the knowledge of human experts. Moreover,
these strategies may fail to generate challenging positive views to provide
sufficient supervision signals. In this paper, we present a novel approach
named Graph Pooling ContraSt (GPS) to address these issues. Motivated by the
fact that graph pooling can adaptively coarsen the graph with the removal of
redundancy, we rethink graph pooling and leverage it to automatically generate
multi-scale positive views with varying emphasis on providing challenging
positives and preserving semantics, i.e., strongly-augmented view and
weakly-augmented view. Then, we incorporate both views into a joint contrastive
learning framework with similarity learning and consistency learning, where our
pooling module is adversarially trained with respect to the encoder for
adversarial robustness. Experiments on twelve datasets on both graph
classification and transfer learning tasks verify the superiority of the
proposed method over its counterparts.
\\ ( https://arxiv.org/abs/2401.16011 ,  5279kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16024
Date: Mon, 29 Jan 2024 10:17:18 GMT   (660kb,D)

Title: Probabilistic Abduction for Visual Abstract Reasoning via Learning Rules
  in Vector-symbolic Architectures
Authors: Michael Hersche, Francesco di Stefano, Thomas Hofmann, Abu Sebastian,
  Abbas Rahimi
Categories: cs.LG cs.AI
Comments: Accepted in NeurIPS 2023 Workshop on MATH-AI
\\
  Abstract reasoning is a cornerstone of human intelligence, and replicating it
with artificial intelligence (AI) presents an ongoing challenge. This study
focuses on efficiently solving Raven's progressive matrices (RPM), a visual
test for assessing abstract reasoning abilities, by using distributed
computation and operators provided by vector-symbolic architectures (VSA).
Instead of hard-coding the rule formulations associated with RPMs, our approach
can learn the VSA rule formulations (hence the name Learn-VRF) with just one
pass through the training data. Yet, our approach, with compact parameters,
remains transparent and interpretable. Learn-VRF yields accurate predictions on
I-RAVEN's in-distribution data, and exhibits strong out-of-distribution
capabilities concerning unseen attribute-rule pairs, significantly
outperforming pure connectionist baselines including large language models. Our
code is available at
https://github.com/IBM/learn-vector-symbolic-architectures-rule-formulations.
\\ ( https://arxiv.org/abs/2401.16024 ,  660kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16025
Date: Mon, 29 Jan 2024 10:17:54 GMT   (7344kb,D)

Title: Simple Policy Optimization
Authors: Zhengpeng Xie
Categories: cs.LG
\\
  PPO (Proximal Policy Optimization) algorithm has demonstrated excellent
performance in many fields, and it is considered as a simple version of TRPO
(Trust Region Policy Optimization) algorithm. However, the ratio clipping
operation in PPO may not always effectively enforce the trust region
constraints, this can be a potential factor affecting the stability of the
algorithm. In this paper, we propose SPO (Simple Policy Optimization)
algorithm, which introduces a novel clipping method for KL divergence between
the old and current policies. SPO can effectively enforce the trust region
constraints in almost all environments, while still maintaining the simplicity
of a first-order algorithm. Comparative experiments in Atari 2600 environments
show that SPO sometimes provides stronger performance than PPO. Code is
available at https://github.com/MyRepositories-hub/Simple-Policy-Optimization.
\\ ( https://arxiv.org/abs/2401.16025 ,  7344kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16088
Date: Mon, 29 Jan 2024 11:55:45 GMT   (8737kb,D)

Title: Fairness in Algorithmic Recourse Through the Lens of Substantive
  Equality of Opportunity
Authors: Andrew Bell, Joao Fonseca, Carlo Abrate, Francesco Bonchi, and Julia
  Stoyanovich
Categories: cs.LG
\\
  Algorithmic recourse -- providing recommendations to those affected
negatively by the outcome of an algorithmic system on how they can take action
and change that outcome -- has gained attention as a means of giving persons
agency in their interactions with artificial intelligence (AI) systems. Recent
work has shown that even if an AI decision-making classifier is ``fair''
(according to some reasonable criteria), recourse itself may be unfair due to
differences in the initial circumstances of individuals, compounding
disparities for marginalized populations and requiring them to exert more
effort than others. There is a need to define more methods and metrics for
evaluating fairness in recourse that span a range of normative views of the
world, and specifically those that take into account time. Time is a critical
element in recourse because the longer it takes an individual to act, the more
the setting may change due to model or data drift.
  This paper seeks to close this research gap by proposing two notions of
fairness in recourse that are in normative alignment with substantive equality
of opportunity, and that consider time. The first considers the (often
repeated) effort individuals exert per successful recourse event, and the
second considers time per successful recourse event. Building upon an
agent-based framework for simulating recourse, this paper demonstrates how much
effort is needed to overcome disparities in initial circumstances. We then
proposes an intervention to improve the fairness of recourse by rewarding
effort, and compare it to existing strategies.
\\ ( https://arxiv.org/abs/2401.16088 ,  8737kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16094
Date: Mon, 29 Jan 2024 12:04:14 GMT   (11803kb,D)

Title: Federated unsupervised random forest for privacy-preserving patient
  stratification
Authors: Bastian Pfeifer, Christel Sirocchi, Marcus D. Bloice, Markus
  Kreuzthaler, Martin Urschler
Categories: cs.LG cs.AI cs.CR q-bio.QM
\\
  In the realm of precision medicine, effective patient stratification and
disease subtyping demand innovative methodologies tailored for multi-omics
data. Clustering techniques applied to multi-omics data have become
instrumental in identifying distinct subgroups of patients, enabling a
finer-grained understanding of disease variability. This work establishes a
powerful framework for advancing precision medicine through unsupervised
random-forest-based clustering and federated computing. We introduce a novel
multi-omics clustering approach utilizing unsupervised random-forests. The
unsupervised nature of the random forest enables the determination of
cluster-specific feature importance, unraveling key molecular contributors to
distinct patient groups. Moreover, our methodology is designed for federated
execution, a crucial aspect in the medical domain where privacy concerns are
paramount. We have validated our approach on machine learning benchmark data
sets as well as on cancer data from The Cancer Genome Atlas (TCGA). Our method
is competitive with the state-of-the-art in terms of disease subtyping, but at
the same time substantially improves the cluster interpretability. Experiments
indicate that local clustering performance can be improved through federated
computing.
\\ ( https://arxiv.org/abs/2401.16094 ,  11803kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16102
Date: Mon, 29 Jan 2024 12:20:17 GMT   (4268kb,D)

Title: Flexible Parallel Neural Network Architecture Model for Early Prediction
  of Lithium Battery Life
Authors: Lidang Jiang, Zhuoxiang Li, Changyan Hu, Qingsong Huang, Ge He
Categories: cs.LG cs.AI cs.CE
\\
  The early prediction of battery life (EPBL) is vital for enhancing the
efficiency and extending the lifespan of lithium batteries. Traditional models
with fixed architectures often encounter underfitting or overfitting issues due
to the diverse data distributions in different EPBL tasks. An interpretable
deep learning model of flexible parallel neural network (FPNN) is proposed,
which includes an InceptionBlock, a 3D convolutional neural network (CNN), a 2D
CNN, and a dual-stream network. The proposed model effectively extracts
electrochemical features from video-like formatted data using the 3D CNN and
achieves advanced multi-scale feature abstraction through the InceptionBlock.
The FPNN can adaptively adjust the number of InceptionBlocks to flexibly handle
tasks of varying complexity in EPBL. The test on the MIT dataset shows that the
FPNN model achieves outstanding predictive accuracy in EPBL tasks, with MAPEs
of 2.47%, 1.29%, 1.08%, and 0.88% when the input cyclic data volumes are 10,
20, 30, and 40, respectively. The interpretability of the FPNN is mainly
reflected in its flexible unit structure and parameter selection: its diverse
branching structure enables the model to capture features at different scales,
thus allowing the machine to learn informative features. The approach presented
herein provides an accurate, adaptable, and comprehensible solution for early
life prediction of lithium batteries, opening new possibilities in the field of
battery health monitoring.
\\ ( https://arxiv.org/abs/2401.16102 ,  4268kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16133
Date: Mon, 29 Jan 2024 12:58:44 GMT   (1160kb)

Title: BooleanOCT: Optimal Classification Trees based on multivariate Boolean
  Rules
Authors: Jiancheng Tu, Wenqi Fan and Zhibin Wu
Categories: cs.LG
\\
  The global optimization of classification trees has demonstrated considerable
promise, notably in enhancing accuracy, optimizing size, and thereby improving
human comprehensibility. While existing optimal classification trees
substantially enhance accuracy over greedy-based tree models like CART, they
still fall short when compared to the more complex black-box models, such as
random forests. To bridge this gap, we introduce a new mixed-integer
programming (MIP) formulation, grounded in multivariate Boolean rules, to
derive the optimal classification tree. Our methodology integrates both linear
metrics, including accuracy, balanced accuracy, and cost-sensitive cost, as
well as nonlinear metrics such as the F1-score. The approach is implemented in
an open-source Python package named BooleanOCT. We comprehensively benchmark
these methods on the 36 datasets from the UCI machine learning repository. The
proposed models demonstrate practical solvability on real-world datasets,
effectively handling sizes in the tens of thousands. Aiming to maximize
accuracy, this model achieves an average absolute improvement of 3.1\% and
1.5\% over random forests in small-scale and medium-sized datasets,
respectively. Experiments targeting various objectives, including balanced
accuracy, cost-sensitive cost, and F1-score, demonstrate the framework's wide
applicability and its superiority over contemporary state-of-the-art optimal
classification tree methods in small to medium-scale datasets.
\\ ( https://arxiv.org/abs/2401.16133 ,  1160kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16137
Date: Mon, 29 Jan 2024 13:13:32 GMT   (1120kb,D)

Title: X-PEFT: eXtremely Parameter-Efficient Fine-Tuning for Extreme
  Multi-Profile Scenarios
Authors: Namju Kwak and Taesup Kim
Categories: cs.LG cs.AI cs.CL
\\
  Parameter-efficient fine-tuning (PEFT) techniques, such as adapter tuning,
aim to fine-tune a pre-trained language model (PLM) using a minimal number of
parameters for a specific task or profile. Although adapter tuning provides
increased parameter efficiency compared to full-model fine-tuning, it
introduces a small set of additional parameters attached to a PLM for each
profile. This can become problematic in practical applications with multiple
profiles, particularly when a significant increase in the number of profiles
linearly boosts the total number of additional parameters. To mitigate this
issue, we introduce X-PEFT, a novel PEFT method that leverages a multitude of
given adapters by fine-tuning an extremely small set of compact tensors for a
new profile, which serve as binary masks to adaptively select the given
adapters. To efficiently validate our proposed method, we implement it using a
large number of trained or untrained (random) adapters. We evaluate the
performance of X-PEFT through LaMP and GLUE tasks and demonstrate that it
either matches or surpasses the effectiveness of conventional adapter tuning,
despite reducing the memory requirements per profile by a factor of 10,000
compared to it.
\\ ( https://arxiv.org/abs/2401.16137 ,  1120kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16164
Date: Mon, 29 Jan 2024 13:50:56 GMT   (2267kb,D)

Title: Constrained Bi-Level Optimization: Proximal Lagrangian Value function
  Approach and Hessian-free Algorithm
Authors: Wei Yao, Chengming Yu, Shangzhi Zeng, and Jin Zhang
Categories: cs.LG math.OC
\\
  This paper presents a new approach and algorithm for solving a class of
constrained Bi-Level Optimization (BLO) problems in which the lower-level
problem involves constraints coupling both upper-level and lower-level
variables. Such problems have recently gained significant attention due to
their broad applicability in machine learning. However, conventional
gradient-based methods unavoidably rely on computationally intensive
calculations related to the Hessian matrix. To address this challenge, we begin
by devising a smooth proximal Lagrangian value function to handle the
constrained lower-level problem. Utilizing this construct, we introduce a
single-level reformulation for constrained BLOs that transforms the original
BLO problem into an equivalent optimization problem with smooth constraints.
Enabled by this reformulation, we develop a Hessian-free gradient-based
algorithm-termed proximal Lagrangian Value function-based Hessian-free Bi-level
Algorithm (LV-HBA)-that is straightforward to implement in a single loop
manner. Consequently, LV-HBA is especially well-suited for machine learning
applications. Furthermore, we offer non-asymptotic convergence analysis for
LV-HBA, eliminating the need for traditional strong convexity assumptions for
the lower-level problem while also being capable of accommodating non-singleton
scenarios. Empirical results substantiate the algorithm's superior practical
performance.
\\ ( https://arxiv.org/abs/2401.16164 ,  2267kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16176
Date: Mon, 29 Jan 2024 14:18:09 GMT   (603kb,D)

Title: A Survey on Structure-Preserving Graph Transformers
Authors: Van Thuy Hoang and O-Joun Lee
Categories: cs.LG cs.AI
Comments: 12
\\
  The transformer architecture has shown remarkable success in various domains,
such as natural language processing and computer vision. When it comes to graph
learning, transformers are required not only to capture the interactions
between pairs of nodes but also to preserve graph structures connoting the
underlying relations and proximity between them, showing the expressive power
to capture different graph structures. Accordingly, various
structure-preserving graph transformers have been proposed and widely used for
various tasks, such as graph-level tasks in bioinformatics and
chemoinformatics. However, strategies related to graph structure preservation
have not been well organized and systematized in the literature. In this paper,
we provide a comprehensive overview of structure-preserving graph transformers
and generalize these methods from the perspective of their design objective.
First, we divide strategies into four main groups: node feature modulation,
context node sampling, graph rewriting, and transformer architecture
improvements. We then further divide the strategies according to the coverage
and goals of graph structure preservation. Furthermore, we also discuss
challenges and future directions for graph transformer models to preserve the
graph structure and understand the nature of graphs.
\\ ( https://arxiv.org/abs/2401.16176 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16193
Date: Mon, 29 Jan 2024 14:47:26 GMT   (1009kb,D)

Title: Contributing Dimension Structure of Deep Feature for Coreset Selection
Authors: Zhijing Wan, Zhixiang Wang, Yuran Wang, Zheng Wang, Hongyuan Zhu,
  Shin'ichi Satoh
Categories: cs.LG cs.DB
Comments: 13 pages,10 figures, to be published in AAAI2024
\\
  Coreset selection seeks to choose a subset of crucial training samples for
efficient learning. It has gained traction in deep learning, particularly with
the surge in training dataset sizes. Sample selection hinges on two main
aspects: a sample's representation in enhancing performance and the role of
sample diversity in averting overfitting. Existing methods typically measure
both the representation and diversity of data based on similarity metrics, such
as L2-norm. They have capably tackled representation via distribution matching
guided by the similarities of features, gradients, or other information between
data. However, the results of effectively diverse sample selection are mired in
sub-optimality. This is because the similarity metrics usually simply aggregate
dimension similarities without acknowledging disparities among the dimensions
that significantly contribute to the final similarity. As a result, they fall
short of adequately capturing diversity. To address this, we propose a
feature-based diversity constraint, compelling the chosen subset to exhibit
maximum diversity. Our key lies in the introduction of a novel Contributing
Dimension Structure (CDS) metric. Different from similarity metrics that
measure the overall similarity of high-dimensional features, our CDS metric
considers not only the reduction of redundancy in feature dimensions, but also
the difference between dimensions that contribute significantly to the final
similarity. We reveal that existing methods tend to favor samples with similar
CDS, leading to a reduced variety of CDS types within the coreset and
subsequently hindering model performance. In response, we enhance the
performance of five classical selection methods by integrating the CDS
constraint. Our experiments on three datasets demonstrate the general
effectiveness of the proposed method in boosting existing methods.
\\ ( https://arxiv.org/abs/2401.16193 ,  1009kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16197
Date: Mon, 29 Jan 2024 14:53:14 GMT   (13833kb,D)

Title: Geospatial Disparities: A Case Study on Real Estate Prices in Paris
Authors: Agathe Fernandes Machado, Fran\c{c}ois Hu, Philipp Ratz, Ewen Gallic,
  Arthur Charpentier
Categories: cs.LG cs.CY
\\
  Driven by an increasing prevalence of trackers, ever more IoT sensors, and
the declining cost of computing power, geospatial information has come to play
a pivotal role in contemporary predictive models. While enhancing prognostic
performance, geospatial data also has the potential to perpetuate many
historical socio-economic patterns, raising concerns about a resurgence of
biases and exclusionary practices, with their disproportionate impacts on
society. Addressing this, our paper emphasizes the crucial need to identify and
rectify such biases and calibration errors in predictive models, particularly
as algorithms become more intricate and less interpretable. The increasing
granularity of geospatial information further introduces ethical concerns, as
choosing different geographical scales may exacerbate disparities akin to
redlining and exclusionary zoning. To address these issues, we propose a
toolkit for identifying and mitigating biases arising from geospatial data.
Extending classical fairness definitions, we incorporate an ordinal regression
case with spatial attributes, deviating from the binary classification focus.
This extension allows us to gauge disparities stemming from data aggregation
levels and advocates for a less interfering correction approach. Illustrating
our methodology using a Parisian real estate dataset, we showcase practical
applications and scrutinize the implications of choosing geographical
aggregation levels for fairness and calibration measures.
\\ ( https://arxiv.org/abs/2401.16197 ,  13833kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16215
Date: Mon, 29 Jan 2024 15:09:40 GMT   (117kb,D)

Title: Learning big logical rules by joining small rules
Authors: C\'eline Hocquette and Andreas Niskanen and Rolf Morel and Matti
  J\"arvisalo and Andrew Cropper
Categories: cs.LG cs.AI cs.LO
\\
  A major challenge in inductive logic programming is learning big rules. To
address this challenge, we introduce an approach where we join small rules to
learn big rules. We implement our approach in a constraint-driven system and
use constraint solvers to efficiently join rules. Our experiments on many
domains, including game playing and drug design, show that our approach can (i)
learn rules with more than 100 literals, and (ii) drastically outperform
existing approaches in terms of predictive accuracies.
\\ ( https://arxiv.org/abs/2401.16215 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16235
Date: Mon, 29 Jan 2024 15:34:49 GMT   (692kb)

Title: Player Pressure Map - A Novel Representation of Pressure in Soccer for
  Evaluating Player Performance in Different Game Contexts
Authors: Chaoyi Gu, Jiaming Na, Yisheng Pei, Varuna De Silva
Categories: cs.LG stat.AP
\\
  In soccer, contextual player performance metrics are invaluable to coaches.
For example, the ability to perform under pressure during matches distinguishes
the elite from the average. Appropriate pressure metric enables teams to assess
players' performance accurately under pressure and design targeted training
scenarios to address their weaknesses. The primary objective of this paper is
to leverage both tracking and event data and game footage to capture the
pressure experienced by the possession team in a soccer game scene. We propose
a player pressure map to represent a given game scene, which lowers the
dimension of raw data and still contains rich contextual information. Not only
does it serve as an effective tool for visualizing and evaluating the pressure
on the team and each individual, but it can also be utilized as a backbone for
accessing players' performance. Overall, our model provides coaches and
analysts with a deeper understanding of players' performance under pressure so
that they make data-oriented tactical decisions.
\\ ( https://arxiv.org/abs/2401.16235 ,  692kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16236
Date: Mon, 29 Jan 2024 15:35:05 GMT   (1459kb,D)

Title: Effective Communication with Dynamic Feature Compression
Authors: Pietro Talli, Francesco Pase, Federico Chiariotti, Andrea Zanella, and
  Michele Zorzi
Categories: cs.LG cs.IT cs.MA math.IT math.OC
Comments: Submitted to the IEEE Transactions on Communications (under review).
  arXiv admin note: substantial text overlap with arXiv:2301.05901
\\
  The remote wireless control of industrial systems is one of the major use
cases for 5G and beyond systems: in these cases, the massive amounts of sensory
information that need to be shared over the wireless medium may overload even
high-capacity connections. Consequently, solving the effective communication
problem by optimizing the transmission strategy to discard irrelevant
information can provide a significant advantage, but is often a very complex
task. In this work, we consider a prototypal system in which an observer must
communicate its sensory data to a robot controlling a task (e.g., a mobile
robot in a factory). We then model it as a remote Partially Observable Markov
Decision Process (POMDP), considering the effect of adopting semantic and
effective communication-oriented solutions on the overall system performance.
We split the communication problem by considering an ensemble Vector Quantized
Variational Autoencoder (VQ-VAE) encoding, and train a Deep Reinforcement
Learning (DRL) agent to dynamically adapt the quantization level, considering
both the current state of the environment and the memory of past messages. We
tested the proposed approach on the well-known CartPole reference control
problem, obtaining a significant performance increase over traditional
approaches.
\\ ( https://arxiv.org/abs/2401.16236 ,  1459kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16244
Date: Fri, 26 Jan 2024 09:33:18 GMT   (43599kb)

Title: Employing Iterative Feature Selection in Fuzzy Rule-Based Binary
  Classification
Authors: Haoning Li, Cong Wang, and Qinghua Huang
Categories: cs.LG
\\
  The feature selection in a traditional binary classification algorithm is
always used in the stage of dataset preprocessing, which makes the obtained
features not necessarily the best ones for the classification algorithm, thus
affecting the classification performance. For a traditional rule-based binary
classification algorithm, classification rules are usually deterministic, which
results in the fuzzy information contained in the rules being ignored. To do
so, this paper employs iterative feature selection in fuzzy rule-based binary
classification. The proposed algorithm combines feature selection based on
fuzzy correlation family with rule mining based on biclustering. It first
conducts biclustering on the dataset after feature selection. Then it conducts
feature selection again for the biclusters according to the feedback of
biclusters evaluation. In this way, an iterative feature selection framework is
build. During the iteration process, it stops until the obtained bicluster
meets the requirements. In addition, the rule membership function is introduced
to extract vectorized fuzzy rules from the bicluster and construct weak
classifiers. The weak classifiers with good classification performance are
selected by Adaptive Boosting and the strong classifier is constructed by
"weighted average". Finally, we perform the proposed algorithm on different
datasets and compare it with other peers. Experimental results show that it
achieves good classification performance and outperforms its peers.
\\ ( https://arxiv.org/abs/2401.16244 ,  43599kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16258
Date: Mon, 29 Jan 2024 16:08:18 GMT   (1899kb)

Title: MosquIoT: A System Based on IoT and Machine Learning for the Monitoring
  of Aedes aegypti (Diptera: Culicidae)
Authors: Javier Aira, Teresa Olivares Montes, Francisco M. Delicado, Dar\`io
  Vezzani
Categories: cs.LG cs.AI cs.CV cs.CY
DOI: 10.1109/TIM.2023.3265119
\\
  Millions of people around the world are infected with mosquito-borne diseases
each year. One of the most dangerous species is Aedes aegypti, the main vector
of viruses such as dengue, yellow fever, chikungunya, and Zika, among others.
Mosquito prevention and eradication campaigns are essential to avoid major
public health consequences. In this respect, entomological surveillance is an
important tool. At present, this traditional monitoring tool is executed
manually and requires digital transformation to help authorities make better
decisions, improve their planning efforts, speed up execution, and better
manage available resources. Therefore, new technological tools based on proven
techniques need to be designed and developed. However, such tools should also
be cost-effective, autonomous, reliable, and easy to implement, and should be
enabled by connectivity and multi-platform software applications. This paper
presents the design, development, and testing of an innovative system named
MosquIoT. It is based on traditional ovitraps with embedded Internet of Things
(IoT) and Tiny Machine Learning (TinyML) technologies, which enable the
detection and quantification of Ae. aegypti eggs. This innovative and promising
solution may help dynamically understand the behavior of Ae. aegypti
populations in cities, shifting from the current reactive entomological
monitoring model to a proactive and predictive digital one.
\\ ( https://arxiv.org/abs/2401.16258 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16291
Date: Mon, 29 Jan 2024 16:50:32 GMT   (969kb,D)

Title: MachineLearnAthon: An Action-Oriented Machine Learning Didactic Concept
Authors: Michal Tk\'a\v{c}, Jakub Sieber, Lara Kuhlmann, Matthias Brueggenolte,
  Alexandru Rinciog, Michael Henke, Artur M. Schweidtmann, Qinghe Gao,
  Maximilian F. Theisen, Radwa El Shawi
Categories: cs.LG cs.CY
\\
  Machine Learning (ML) techniques are encountered nowadays across disciplines,
from social sciences, through natural sciences to engineering. The broad
application of ML and the accelerated pace of its evolution lead to an
increasing need for dedicated teaching concepts aimed at making the application
of this technology more reliable and responsible. However, teaching ML is a
daunting task. Aside from the methodological complexity of ML algorithms, both
with respect to theory and implementation, the interdisciplinary and empirical
nature of the field need to be taken into consideration. This paper introduces
the MachineLearnAthon format, an innovative didactic concept designed to be
inclusive for students of different disciplines with heterogeneous levels of
mathematics, programming and domain expertise. At the heart of the concept lie
ML challenges, which make use of industrial data sets to solve real-world
problems. These cover the entire ML pipeline, promoting data literacy and
practical skills, from data preparation, through deployment, to evaluation.
\\ ( https://arxiv.org/abs/2401.16291 ,  969kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16294
Date: Mon, 29 Jan 2024 16:53:04 GMT   (1626kb,D)

Title: Dual feature-based and example-based explanation methods
Authors: Andrei V. Konstantinov, Boris V. Kozlov, Stanislav R. Kirpichenko, and
  Lev V. Utkin
Categories: cs.LG cs.AI stat.ML
\\
  A new approach to the local and global explanation is proposed. It is based
on selecting a convex hull constructed for the finite number of points around
an explained instance. The convex hull allows us to consider a dual
representation of instances in the form of convex combinations of extreme
points of a produced polytope. Instead of perturbing new instances in the
Euclidean feature space, vectors of convex combination coefficients are
uniformly generated from the unit simplex, and they form a new dual dataset. A
dual linear surrogate model is trained on the dual dataset. The explanation
feature importance values are computed by means of simple matrix calculations.
The approach can be regarded as a modification of the well-known model LIME.
The dual representation inherently allows us to get the example-based
explanation. The neural additive model is also considered as a tool for
implementing the example-based explanation approach. Many numerical experiments
with real datasets are performed for studying the approach. The code of
proposed algorithms is available.
\\ ( https://arxiv.org/abs/2401.16294 ,  1626kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16299
Date: Mon, 29 Jan 2024 17:00:28 GMT   (8283kb,D)

Title: Enhancing Molecular Property Prediction with Auxiliary Learning and
  Task-Specific Adaptation
Authors: Vishal Dey and Xia Ning
Categories: cs.LG cs.AI
\\
  Pretrained Graph Neural Networks have been widely adopted for various
molecular property prediction tasks. Despite their ability to encode structural
and relational features of molecules, traditional fine-tuning of such
pretrained GNNs on the target task can lead to poor generalization. To address
this, we explore the adaptation of pretrained GNNs to the target task by
jointly training them with multiple auxiliary tasks. This could enable the GNNs
to learn both general and task-specific features, which may benefit the target
task. However, a major challenge is to determine the relatedness of auxiliary
tasks with the target task. To address this, we investigate multiple strategies
to measure the relevance of auxiliary tasks and integrate such tasks by
adaptively combining task gradients or by learning task weights via bi-level
optimization. Additionally, we propose a novel gradient surgery-based approach,
Rotation of Conflicting Gradients ($\mathtt{RCGrad}$), that learns to align
conflicting auxiliary task gradients through rotation. Our experiments with
state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed
methods, with improvements of up to 7.7% over fine-tuning. This suggests that
incorporating auxiliary tasks along with target task fine-tuning can be an
effective way to improve the generalizability of pretrained GNNs for molecular
property prediction.
\\ ( https://arxiv.org/abs/2401.16299 ,  8283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16318
Date: Mon, 29 Jan 2024 17:21:41 GMT   (13690kb,D)

Title: Defining and Extracting generalizable interaction primitives from DNNs
Authors: Lu Chen, Siyu Lou, Benhao Huang, Quanshi Zhang
Categories: cs.LG cs.AI cs.CV
\\
  Faithfully summarizing the knowledge encoded by a deep neural network (DNN)
into a few symbolic primitive patterns without losing much information
represents a core challenge in explainable AI. To this end, Ren et al. (2023c)
have derived a series of theorems to prove that the inference score of a DNN
can be explained as a small set of interactions between input variables.
However, the lack of generalization power makes it still hard to consider such
interactions as faithful primitive patterns encoded by the DNN. Therefore,
given different DNNs trained for the same task, we develop a new method to
extract interactions that are shared by these DNNs. Experiments show that the
extracted interactions can better reflect common knowledge shared by different
DNNs.
\\ ( https://arxiv.org/abs/2401.16318 ,  13690kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16327
Date: Mon, 29 Jan 2024 17:32:22 GMT   (5737kb,D)

Title: PICL: Physics Informed Contrastive Learning for Partial Differential
  Equations
Authors: Cooper Lorsung and Amir Barati Farimani
Categories: cs.LG cs.NA math.NA physics.comp-ph
Comments: 16 pages, 4 figures
\\
  Neural operators have recently grown in popularity as Partial Differential
Equation (PDEs) surrogate models. Learning solution functionals, rather than
functions, has proven to be a powerful approach to calculate fast, accurate
solutions to complex PDEs. While much work has been done evaluating neural
operator performance on a wide variety of surrogate modeling tasks, these works
normally evaluate performance on a single equation at a time. In this work, we
develop a novel contrastive pretraining framework utilizing Generalized
Contrastive Loss that improves neural operator generalization across multiple
governing equations simultaneously. Governing equation coefficients are used to
measure ground-truth similarity between systems. A combination of
physics-informed system evolution and latent-space model output are anchored to
input data and used in our distance function. We find that physics-informed
contrastive pretraining improves both accuracy and generalization for the
Fourier Neural Operator in fixed-future task, with comparable performance on
the autoregressive rollout, and superresolution tasks for the 1D Heat,
Burgers', and linear advection equations.
\\ ( https://arxiv.org/abs/2401.16327 ,  5737kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16335
Date: Mon, 29 Jan 2024 17:43:42 GMT   (495kb,D)

Title: Iterative Data Smoothing: Mitigating Reward Overfitting and
  Overoptimization in RLHF
Authors: Banghua Zhu, Michael I. Jordan and Jiantao Jiao
Categories: cs.LG cs.AI cs.CL stat.ML
\\
  Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that
aligns language models closely with human-centric values. The initial phase of
RLHF involves learning human values using a reward model from ranking data. It
is observed that the performance of the reward model degrades after one epoch
of training, and optimizing too much against the learned reward model
eventually hinders the true objective. This paper delves into these issues,
leveraging the theoretical insights to design improved reward learning
algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during
each training epoch, we not only update the model with the data, but also
update the date using the model, replacing hard labels with soft labels. Our
empirical findings highlight the superior performance of this approach over the
traditional methods.
\\ ( https://arxiv.org/abs/2401.16335 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16350
Date: Mon, 29 Jan 2024 17:56:15 GMT   (205kb,D)

Title: FedFair^3: Unlocking Threefold Fairness in Federated Learning
Authors: Simin Javaherian, Sanjeev Panta, Shelby Williams, Md Sirajul Islam, Li
  Chen
Categories: cs.LG cs.AI cs.CY cs.DC
\\
  Federated Learning (FL) is an emerging paradigm in machine learning without
exposing clients' raw data. In practical scenarios with numerous clients,
encouraging fair and efficient client participation in federated learning is of
utmost importance, which is also challenging given the heterogeneity in data
distribution and device properties. Existing works have proposed different
client-selection methods that consider fairness; however, they fail to select
clients with high utilities while simultaneously achieving fair accuracy
levels. In this paper, we propose a fair client-selection approach that unlocks
threefold fairness in federated learning. In addition to having a fair
client-selection strategy, we enforce an equitable number of rounds for client
participation and ensure a fair accuracy distribution over the clients. The
experimental results demonstrate that FedFair^3, in comparison to the
state-of-the-art baselines, achieves 18.15% less accuracy variance on the IID
data and 54.78% on the non-IID data, without decreasing the global accuracy.
Furthermore, it shows 24.36% less wall-clock training time on average.
\\ ( https://arxiv.org/abs/2401.16350 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16367
Date: Mon, 29 Jan 2024 18:07:56 GMT   (234kb,D)

Title: TQCompressor: improving tensor decomposition methods in neural networks
  via permutations
Authors: V. Abronin, A. Naumov, D. Mazur, D. Bystrov, K. Tsarova, Ar. Melnikov,
  I. Oseledets, S. Dolgov, R. Brasher, M. Perelshtein
Categories: cs.LG cs.AI cs.CL
\\
  We introduce TQCompressor, a novel method for neural network model
compression with improved tensor decompositions. We explore the challenges
posed by the computational and storage demands of pre-trained language models
in NLP tasks and propose a permutation-based enhancement to Kronecker
decomposition. This enhancement makes it possible to reduce loss in model
expressivity which is usually associated with factorization. We demonstrate
this method applied to the GPT-2$_{small}$. The result of the compression is
TQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. in
the GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available. We further
enhance the performance of the TQCompressedGPT-2 through a training strategy
involving multi-step knowledge distillation, using only a 3.1% of the
OpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative
evaluations, marking an advancement in the efficient and effective deployment
of models in resource-constrained environments.
\\ ( https://arxiv.org/abs/2401.16367 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16373
Date: Mon, 29 Jan 2024 18:12:32 GMT   (114kb,D)

Title: Bayesian optimization as a flexible and efficient design framework for
  sustainable process systems
Authors: Joel A. Paulson and Calvin Tsay
Categories: cs.LG math.OC
Comments: 16 pages, 1 figure, 1 table
\\
  Bayesian optimization (BO) is a powerful technology for optimizing noisy
expensive-to-evaluate black-box functions, with a broad range of real-world
applications in science, engineering, economics, manufacturing, and beyond. In
this paper, we provide an overview of recent developments, challenges, and
opportunities in BO for design of next-generation process systems. After
describing several motivating applications, we discuss how advanced BO methods
have been developed to more efficiently tackle important problems in these
applications. We conclude the paper with a summary of challenges and
opportunities related to improving the quality of the probabilistic model, the
choice of internal optimization procedure used to select the next sample point,
and the exploitation of problem structure to improve sample efficiency.
\\ ( https://arxiv.org/abs/2401.16373 ,  114kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16383
Date: Mon, 29 Jan 2024 18:24:16 GMT   (103kb)

Title: Learning logic programs by finding minimal unsatisfiable subprograms
Authors: Andrew Cropper and C\'eline Hocquette
Categories: cs.LG cs.LO
\\
  The goal of inductive logic programming (ILP) is to search for a logic
program that generalises training examples and background knowledge. We
introduce an ILP approach that identifies minimal unsatisfiable subprograms
(MUSPs). We show that finding MUSPs allows us to efficiently and soundly prune
the search space. Our experiments on multiple domains, including program
synthesis and game playing, show that our approach can reduce learning times by
99%.
\\ ( https://arxiv.org/abs/2401.16383 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16386
Date: Mon, 29 Jan 2024 18:27:52 GMT   (613kb,D)

Title: Continual Learning with Pre-Trained Models: A Survey
Authors: Da-Wei Zhou, Hai-Long Sun, Jingyi Ning, Han-Jia Ye, De-Chuan Zhan
Categories: cs.LG cs.CV
Comments: Code is available at: https://github.com/sun-hailong/LAMDA-PILOT
\\
  Nowadays, real-world applications often face streaming data, which requires
the learning system to absorb new knowledge as data evolves. Continual Learning
(CL) aims to achieve this goal and meanwhile overcome the catastrophic
forgetting of former knowledge when learning new ones. Typical CL methods build
the model from scratch to grow with incoming data. However, the advent of the
pre-trained model (PTM) era has sparked immense research interest, particularly
in leveraging PTMs' robust representational capabilities. This paper presents a
comprehensive survey of the latest advancements in PTM-based CL. We categorize
existing methodologies into three distinct groups, providing a comparative
analysis of their similarities, differences, and respective advantages and
disadvantages. Additionally, we offer an empirical study contrasting various
state-of-the-art methods to highlight concerns regarding fairness in
comparisons. The source code to reproduce these evaluations is available at:
https://github.com/sun-hailong/LAMDA-PILOT
\\ ( https://arxiv.org/abs/2401.16386 ,  613kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16419
Date: Mon, 29 Jan 2024 18:57:45 GMT   (268kb,D)

Title: Semi-parametric Expert Bayesian Network Learning with Gaussian Processes
  and Horseshoe Priors
Authors: Yidou Weng, Finale Doshi-Velez
Categories: cs.LG stat.ML
Comments: 8 pages, 4 figures, AAAI-2024 workshops
\\
  This paper proposes a model learning Semi-parametric rela- tionships in an
Expert Bayesian Network (SEBN) with linear parameter and structure constraints.
We use Gaussian Pro- cesses and a Horseshoe prior to introduce minimal nonlin-
ear components. To prioritize modifying the expert graph over adding new edges,
we optimize differential Horseshoe scales. In real-world datasets with unknown
truth, we gen- erate diverse graphs to accommodate user input, addressing
identifiability issues and enhancing interpretability. Evalua- tion on
synthetic and UCI Liver Disorders datasets, using metrics like structural
Hamming Distance and test likelihood, demonstrates our models outperform
state-of-the-art semi- parametric Bayesian Network model.
\\ ( https://arxiv.org/abs/2401.16419 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16421
Date: Mon, 29 Jan 2024 18:59:07 GMT   (578kb,D)

Title: Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length
  Extrapolation
Authors: Zhenyu He, Guhao Feng, Shengjie Luo, Kai Yang, Di He, Jingjing Xu, Zhi
  Zhang, Hongxia Yang, Liwei Wang
Categories: cs.LG cs.AI cs.CL stat.ML
Comments: 17 pages, 7 figures, 8 tables; Working in Progress
\\
  In this work, we leverage the intrinsic segmentation of language sequences
and design a new positional encoding method called Bilevel Positional Encoding
(BiPE). For each position, our BiPE blends an intra-segment encoding and an
inter-segment encoding. The intra-segment encoding identifies the locations
within a segment and helps the model capture the semantic information therein
via absolute positional encoding. The inter-segment encoding specifies the
segment index, models the relationships between segments, and aims to improve
extrapolation capabilities via relative positional encoding. Theoretical
analysis shows this disentanglement of positional information makes learning
more effective. The empirical results also show that our BiPE has superior
length extrapolation capabilities across a wide range of tasks in diverse text
modalities.
\\ ( https://arxiv.org/abs/2401.16421 ,  578kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16422
Date: Mon, 29 Jan 2024 18:59:22 GMT   (2501kb,D)

Title: Strategic Usage in a Multi-Learner Setting
Authors: Eliot Shekhtman and Sarah Dean
Categories: cs.LG cs.GT
Comments: 17 pages, 6 figures
MSC-class: 91A10
\\
  Real-world systems often involve some pool of users choosing between a set of
services. With the increase in popularity of online learning algorithms, these
services can now self-optimize, leveraging data collected on users to maximize
some reward such as service quality. On the flipside, users may strategically
choose which services to use in order to pursue their own reward functions, in
the process wielding power over which services can see and use their data.
Extensive prior research has been conducted on the effects of strategic users
in single-service settings, with strategic behavior manifesting in the
manipulation of observable features to achieve a desired classification;
however, this can often be costly or unattainable for users and fails to
capture the full behavior of multi-service dynamic systems. As such, we analyze
a setting in which strategic users choose among several available services in
order to pursue positive classifications, while services seek to minimize loss
functions on their observations. We focus our analysis on realizable settings,
and show that naive retraining can still lead to oscillation even if all users
are observed at different times; however, if this retraining uses memory of
past observations, convergent behavior can be guaranteed for certain loss
function classes. We provide results obtained from synthetic and real-world
data to empirically validate our theoretical findings.
\\ ( https://arxiv.org/abs/2401.16422 ,  2501kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.14279 (*cross-listing*)
Date: Thu, 25 Jan 2024 16:10:33 GMT   (1742kb,D)

Title: ZS4C: Zero-Shot Synthesis of Compilable Code for Incomplete Code
  Snippets using ChatGPT
Authors: Azmain Kabir, Shaowei Wang, Yuan Tian, Tse-Hsun (Peter) Chen, Muhammad
  Asaduzzaman, Wenbin Zhang
Categories: cs.SE cs.AI
\\
  Technical question and answering (Q&A) sites such as Stack Overflow have
become an important source for software developers to seek knowledge. However,
code snippets on Q&A sites are usually uncompilable and semantically incomplete
for compilation due to unresolved types and missing dependent libraries, which
raises the obstacle for users to reuse or analyze Q&A code snippets. Prior
approaches either are not designed for synthesizing compilable code or suffer
from a low compilation success rate. To address this problem, we propose ZS4C,
a lightweight approach to perform zero-shot synthesis of compilable code from
incomplete code snippets using Large Language Model (LLM). ZS4C operates in two
stages. In the first stage, ZS4C utilizes an LLM, i.e., ChatGPT, to identify
missing import statements for a given code snippet, leveraging our designed
task-specific prompt template. In the second stage, ZS4C fixes compilation
errors caused by incorrect import statements and syntax errors through
collaborative work between ChatGPT and a compiler. We thoroughly evaluated ZS4C
on a widely used benchmark called StatType-SO against the SOTA approach SnR.
Compared with SnR, ZS4C improves the compilation rate from 63% to 87.6%, with a
39.3% improvement. On average, ZS4C can infer more accurate import statements
than SnR, with an improvement of 6.6% in the F1.
\\ ( https://arxiv.org/abs/2401.14279 ,  1742kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15105 (*cross-listing*)
Date: Thu, 25 Jan 2024 13:14:17 GMT   (1851kb,D)

Title: Diffusion Enhancement for Cloud Removal in Ultra-Resolution Remote
  Sensing Imagery
Authors: Jialu Sui, Yiyang Ma, Wenhan Yang, Xiaokang Zhang, Man-On Pun and
  Jiaying Liu
Categories: eess.IV cs.AI cs.LG
\\
  The presence of cloud layers severely compromises the quality and
effectiveness of optical remote sensing (RS) images. However, existing
deep-learning (DL)-based Cloud Removal (CR) techniques encounter difficulties
in accurately reconstructing the original visual authenticity and detailed
semantic content of the images. To tackle this challenge, this work proposes to
encompass enhancements at the data and methodology fronts. On the data side, an
ultra-resolution benchmark named CUHK Cloud Removal (CUHK-CR) of 0.5m spatial
resolution is established. This benchmark incorporates rich detailed textures
and diverse cloud coverage, serving as a robust foundation for designing and
assessing CR models. From the methodology perspective, a novel diffusion-based
framework for CR called Diffusion Enhancement (DE) is proposed to perform
progressive texture detail recovery, which mitigates the training difficulty
with improved inference accuracy. Additionally, a Weight Allocation (WA)
network is developed to dynamically adjust the weights for feature fusion,
thereby further improving performance, particularly in the context of
ultra-resolution image generation. Furthermore, a coarse-to-fine training
strategy is applied to effectively expedite training convergence while reducing
the computational complexity required to handle ultra-resolution images.
Extensive experiments on the newly established CUHK-CR and existing datasets
such as RICE confirm that the proposed DE framework outperforms existing
DL-based methods in terms of both perceptual quality and signal fidelity.
\\ ( https://arxiv.org/abs/2401.15105 ,  1851kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15106 (*cross-listing*)
Date: Thu, 25 Jan 2024 16:21:37 GMT   (637kb)

Title: Decision Theoretic Foundations for Experiments Evaluating Human
  Decisions
Authors: Jessica Hullman, Alex Kale, Jason Hartline
Categories: cs.HC cs.AI
\\
  Decision-making with information displays is a key focus of research in areas
like explainable AI, human-AI teaming, and data visualization. However, what
constitutes a decision problem, and what is required for an experiment to be
capable of concluding that human decisions are flawed in some way, remain open
to speculation. We present a widely applicable definition of a decision problem
synthesized from statistical decision theory and information economics. We
argue that to attribute loss in human performance to forms of bias, an
experiment must provide participants with the information that a rational agent
would need to identify the normative decision. We evaluate the extent to which
recent evaluations of decision-making from the literature on AI-assisted
decisions achieve this criteria. We find that only 6 (17\%) of 35 studies that
claim to identify biased behavior present participants with sufficient
information to characterize their behavior as deviating from good
decision-making. We motivate the value of studying well-defined decision
problems by describing a characterization of performance losses they allow us
to conceive. In contrast, the ambiguities of a poorly communicated decision
problem preclude normative interpretation. We conclude with recommendations for
practice.
\\ ( https://arxiv.org/abs/2401.15106 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15109 (*cross-listing*)
Date: Thu, 25 Jan 2024 19:43:35 GMT   (857kb)

Title: Towards Collective Superintelligence: Amplifying Group IQ using
  Conversational Swarms
Authors: Louis Rosenberg, Gregg Willcox, Hans Schumann, and Ganesh Mani
Categories: cs.HC cs.AI
\\
  Swarm Intelligence (SI) is a natural phenomenon that enables biological
groups to amplify their combined intellect by forming real-time systems.
Artificial Swarm Intelligence (or Swarm AI) is a technology that enables
networked human groups to amplify their combined intelligence by forming
similar systems. In the past, swarm-based methods were constrained to narrowly
defined tasks like probabilistic forecasting and multiple-choice decision
making. A new technology called Conversational Swarm Intelligence (CSI) was
developed in 2023 that amplifies the decision-making accuracy of networked
human groups through natural conversational deliberations. The current study
evaluated the ability of real-time groups using a CSI platform to take a common
IQ test known as Raven's Advanced Progressive Matrices (RAPM). First, a
baseline group of participants took the Raven's IQ test by traditional survey.
This group averaged 45.6% correct. Then, groups of approximately 35 individuals
answered IQ test questions together using a CSI platform called Thinkscape.
These groups averaged 80.5% correct. This places the CSI groups in the 97th
percentile of IQ test-takers and corresponds to an effective IQ increase of 28
points (p<0.001). This is an encouraging result and suggests that CSI is a
powerful method for enabling conversational collective intelligence in large,
networked groups. In addition, because CSI is scalable across groups of
potentially any size, this technology may provide a viable pathway to building
a Collective Superintelligence.
\\ ( https://arxiv.org/abs/2401.15109 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15118 (*cross-listing*)
Date: Fri, 26 Jan 2024 02:39:40 GMT   (11024kb)

Title: GeoDecoder: Empowering Multimodal Map Understanding
Authors: Feng Qi, Mian Dai, Zixian Zheng, Chao Wang
Categories: cs.CV cs.AI
\\
  This paper presents GeoDecoder, a dedicated multimodal model designed for
processing geospatial information in maps. Built on the BeitGPT architecture,
GeoDecoder incorporates specialized expert modules for image and text
processing. On the image side, GeoDecoder utilizes GaoDe Amap as the underlying
base map, which inherently encompasses essential details about road and
building shapes, relative positions, and other attributes. Through the
utilization of rendering techniques, the model seamlessly integrates external
data and features such as symbol markers, drive trajectories, heatmaps, and
user-defined markers, eliminating the need for extra feature engineering. The
text module of GeoDecoder accepts various context texts and question prompts,
generating text outputs in the style of GPT. Furthermore, the GPT-based model
allows for the training and execution of multiple tasks within the same model
in an end-to-end manner. To enhance map cognition and enable GeoDecoder to
acquire knowledge about the distribution of geographic entities in Beijing, we
devised eight fundamental geospatial tasks and conducted pretraining of the
model using large-scale text-image samples. Subsequently, rapid fine-tuning was
performed on three downstream tasks, resulting in significant performance
improvements. The GeoDecoder model demonstrates a comprehensive understanding
of map elements and their associated operations, enabling efficient and
high-quality application of diverse geospatial tasks in different business
scenarios.
\\ ( https://arxiv.org/abs/2401.15118 ,  11024kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15120 (*cross-listing*)
Date: Fri, 26 Jan 2024 03:44:58 GMT   (2953kb,D)

Title: Context-driven self-supervised visual learning: Harnessing the
  environment as a data source
Authors: Lizhen Zhu and James Z. Wang and Wonseuk Lee and Brad Wyble
Categories: cs.CV cs.AI
\\
  Visual learning often occurs in a specific context, where an agent acquires
skills through exploration and tracking of its location in a consistent
environment. The historical spatial context of the agent provides a similarity
signal for self-supervised contrastive learning. We present a unique approach,
termed Environmental Spatial Similarity (ESS), that complements existing
contrastive learning methods. Using images from simulated, photorealistic
environments as an experimental setting, we demonstrate that ESS outperforms
traditional instance discrimination approaches. Moreover, sampling additional
data from the same environment substantially improves accuracy and provides new
augmentations. ESS allows remarkable proficiency in room classification and
spatial prediction tasks, especially in unfamiliar environments. This learning
paradigm has the potential to enable rapid visual learning in agents operating
in new environments with unique visual characteristics. Potentially
transformative applications span from robotics to space exploration. Our proof
of concept demonstrates improved efficiency over methods that rely on
extensive, disconnected datasets.
\\ ( https://arxiv.org/abs/2401.15120 ,  2953kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15124 (*cross-listing*)
Date: Fri, 26 Jan 2024 10:44:44 GMT   (1630kb)

Title: Sensor-Based Data Acquisition via Ubiquitous Device to Detect Muscle
  Strength Training Activities
Authors: E. Wianto, H. Toba, M. Malinda and Chien-Hsu Chen
Categories: cs.HC cs.AI
Comments: 9 pages, 4 figures, AHFE International Conference on Human Factors in
  Design, Engineering, and Computing
ACM-class: H.1.2
DOI: 10.54941/ahfe1004213
\\
  Maintaining a high quality of life through physical activities (PA) to
prevent health decline is crucial. However, the relationship between
individuals health status, PA preferences, and motion factors is complex. PA
discussions consistently show a positive correlation with healthy aging
experiences, but no explicit relation to specific types of musculoskeletal
exercises. Taking advantage of the increasingly widespread existence of
smartphones, especially in Indonesia, this research utilizes embedded sensors
for Human Activity Recognition (HAR). Based on 25 participants data, performing
nine types of selected motion, this study has successfully identified important
sensor attributes that play important roles in the right and left hands for
muscle strength motions as the basis for developing machine learning models
with the LSTM algorithm.
\\ ( https://arxiv.org/abs/2401.15124 ,  1630kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15132 (*cross-listing*)
Date: Fri, 26 Jan 2024 16:09:39 GMT   (1406kb,D)

Title: On the Emergence of Symmetrical Reality
Authors: Zhenliang Zhang, Zeyu Zhang, Ziyuan Jiao, Yao Su, Hangxin Liu, Wei
  Wang, Song-Chun Zhu
Categories: cs.HC cs.AI
Comments: IEEE VR 2024
\\
  Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.
\\ ( https://arxiv.org/abs/2401.15132 ,  1406kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15210 (*cross-listing*)
Date: Fri, 26 Jan 2024 21:16:37 GMT   (903kb)

Title: Roq: Robust Query Optimization Based on a Risk-aware Learned Cost Model
Authors: Amin Kamali, Verena Kantere, Calisto Zuzarte, and Vincent Corvinelli
Categories: cs.DB cs.AI
Comments: 13 pages, 9 figures, submitted to SIGMOD 2024
\\
  Query optimizers in relational database management systems (RDBMSs) search
for execution plans expected to be optimal for a given queries. They use
parameter estimates, often inaccurate, and make assumptions that may not hold
in practice. Consequently, they may select execution plans that are suboptimal
at runtime, when these estimates and assumptions are not valid, which may
result in poor query performance. Therefore, query optimizers do not
sufficiently support robust query optimization. Recent years have seen a surge
of interest in using machine learning (ML) to improve efficiency of data
systems and reduce their maintenance overheads, with promising results obtained
in the area of query optimization in particular. In this paper, inspired by
these advancements, and based on several years of experience of IBM Db2 in this
journey, we propose Robust Optimization of Queries, (Roq), a holistic framework
that enables robust query optimization based on a risk-aware learning approach.
Roq includes a novel formalization of the notion of robustness in the context
of query optimization and a principled approach for its quantification and
measurement based on approximate probabilistic ML. It also includes novel
strategies and algorithms for query plan evaluation and selection. Roq also
includes a novel learned cost model that is designed to predict query execution
cost and the associated risks and performs query optimization accordingly. We
demonstrate experimentally that Roq provides significant improvements to robust
query optimization compared to the state-of-the-art.
\\ ( https://arxiv.org/abs/2401.15210 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15245 (*cross-listing*)
Date: Fri, 26 Jan 2024 23:31:53 GMT   (8652kb,D)

Title: GenPluSSS: A Genetic Algorithm Based Plugin for Measured Subsurface
  Scattering Representation
Authors: Bar{\i}\c{s} Y{\i}ld{\i}r{\i}m, Murat Kurt
Categories: cs.GR cs.AI cs.LG cs.NE
\\
  This paper presents a plugin that adds a representation of homogeneous and
heterogeneous, optically thick, translucent materials on the Blender 3D
modeling tool. The working principle of this plugin is based on a combination
of Genetic Algorithm (GA) and Singular Value Decomposition (SVD)-based
subsurface scattering method (GenSSS). The proposed plugin has been implemented
using Mitsuba renderer, which is an open source rendering software. The
proposed plugin has been validated on measured subsurface scattering data. It's
shown that the proposed plugin visualizes homogeneous and heterogeneous
subsurface scattering effects, accurately, compactly and computationally
efficiently.
\\ ( https://arxiv.org/abs/2401.15245 ,  8652kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15284 (*cross-listing*)
Date: Sat, 27 Jan 2024 03:53:25 GMT   (122kb)

Title: Building ethical guidelines for generative AI in scientific research
Authors: Zhicheng Lin
Categories: cs.CY cs.AI
Comments: 9 pages, 2 tables
\\
  Generative artificial intelligence tools like large language models are
rapidly transforming academic research and real world applications. However,
discussions on ethical guidelines for generative AI in science remain
fragmented, underscoring the urgent need for consensus based standards. This
paper offers an initial framework by developing analyses and mitigation
strategies across five key themes: understanding model limitations regarding
truthfulness and bias; respecting privacy, confidentiality, and copyright;
avoiding plagiarism and policy violations when incorporating model output;
ensuring applications provide overall benefit; and using AI transparently and
reproducibly. Common scenarios are outlined to demonstrate potential ethical
violations. We argue that global consensus coupled with professional training
and reasonable enforcement are critical to promoting the benefits of AI while
safeguarding research integrity.
\\ ( https://arxiv.org/abs/2401.15284 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15293 (*cross-listing*)
Date: Sat, 27 Jan 2024 04:24:49 GMT   (1383kb,D)

Title: SkipViT: Speeding Up Vision Transformers with a Token-Level Skip
  Connection
Authors: Foozhan Ataiefard, Walid Ahmed, Habib Hajimolahoseini, Saina Asani,
  Farnoosh Javadi, Mohammad Hassanpour, Omar Mohamed Awad, Austin Wen, Kangling
  Liu, Yang Liu
Categories: cs.CV cs.AI cs.LG
\\
  Vision transformers are known to be more computationally and data-intensive
than CNN models. These transformer models such as ViT, require all the input
image tokens to learn the relationship among them. However, many of these
tokens are not informative and may contain irrelevant information such as
unrelated background or unimportant scenery. These tokens are overlooked by the
multi-head self-attention (MHSA), resulting in many redundant and unnecessary
computations in MHSA and the feed-forward network (FFN). In this work, we
propose a method to optimize the amount of unnecessary interactions between
unimportant tokens by separating and sending them through a different low-cost
computational path. Our method does not add any parameters to the ViT model and
aims to find the best trade-off between training throughput and achieving a 0%
loss in the Top-1 accuracy of the final model. Our experimental results on
training ViT-small from scratch show that SkipViT is capable of effectively
dropping 55% of the tokens while gaining more than 13% training throughput and
maintaining classification accuracy at the level of the baseline model on
Huawei Ascend910A.
\\ ( https://arxiv.org/abs/2401.15293 ,  1383kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15296 (*cross-listing*)
Date: Sat, 27 Jan 2024 04:52:24 GMT   (89kb,D)

Title: A Survey on 3D Skeleton Based Person Re-Identification: Approaches,
  Designs, Challenges, and Future Directions
Authors: Haocong Rao, Chunyan Miao
Categories: cs.CV cs.AI
Comments: A up-to-date resource (papers, codes, data, etc.) of this survey is
  provided at https://github.com/Kali-Hac/3D-skeleton-based-person-re-ID-survey
\\
  Person re-identification via 3D skeletons is an important emerging research
area that triggers great interest in the pattern recognition community. With
distinctive advantages for many application scenarios, a great diversity of 3D
skeleton based person re-identification (SRID) methods have been proposed in
recent years, effectively addressing prominent problems in skeleton modeling
and feature learning. Despite recent advances, to the best of our knowledge,
little effort has been made to comprehensively summarize these studies and
their challenges. In this paper, we attempt to fill this gap by providing a
systematic survey on current SRID approaches, model designs, challenges, and
future directions. Specifically, we first formulate the SRID problem, and
propose a taxonomy of SRID research with a summary of benchmark datasets,
commonly-used model architectures, and an analytical review of different
methods' characteristics. Then, we elaborate on the design principles of SRID
models from multiple aspects to offer key insights for model improvement.
Finally, we identify critical challenges confronting current studies and
discuss several promising directions for future research of SRID.
\\ ( https://arxiv.org/abs/2401.15296 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15318 (*cross-listing*)
Date: Sat, 27 Jan 2024 06:45:22 GMT   (47237kb,D)

Title: Gaussian Splashing: Dynamic Fluid Synthesis with Gaussian Splatting
Authors: Yutao Feng, Xiang Feng, Yintong Shang, Ying Jiang, Chang Yu, Zeshun
  Zong, Tianjia Shao, Hongzhi Wu, Kun Zhou, Chenfanfu Jiang, Yin Yang
Categories: cs.GR cs.AI cs.CV cs.LG
\\
  We demonstrate the feasibility of integrating physics-based animations of
solids and fluids with 3D Gaussian Splatting (3DGS) to create novel effects in
virtual scenes reconstructed using 3DGS. Leveraging the coherence of the
Gaussian splatting and position-based dynamics (PBD) in the underlying
representation, we manage rendering, view synthesis, and the dynamics of solids
and fluids in a cohesive manner. Similar to Gaussian shader, we enhance each
Gaussian kernel with an added normal, aligning the kernel's orientation with
the surface normal to refine the PBD simulation. This approach effectively
eliminates spiky noises that arise from rotational deformation in solids. It
also allows us to integrate physically based rendering to augment the dynamic
surface reflections on fluids. Consequently, our framework is capable of
realistically reproducing surface highlights on dynamic fluids and facilitating
interactions between scene objects and fluids from new views. For more
information, please visit our project page at
\url{https://amysteriouscat.github.io/GaussianSplashing/}.
\\ ( https://arxiv.org/abs/2401.15318 ,  47237kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15323 (*cross-listing*)
Date: Sat, 27 Jan 2024 06:56:51 GMT   (799kb,D)

Title: Music Auto-Tagging with Robust Music Representation Learned via Domain
  Adversarial Training
Authors: Haesun Joung, Kyogu Lee
Categories: cs.SD cs.AI cs.IR eess.AS
Comments: 5 pages, 3 figures, accepted to ICASSP 2024
\\
  Music auto-tagging is crucial for enhancing music discovery and
recommendation. Existing models in Music Information Retrieval (MIR) struggle
with real-world noise such as environmental and speech sounds in multimedia
content. This study proposes a method inspired by speech-related tasks to
enhance music auto-tagging performance in noisy settings. The approach
integrates Domain Adversarial Training (DAT) into the music domain, enabling
robust music representations that withstand noise. Unlike previous research,
this approach involves an additional pretraining phase for the domain
classifier, to avoid performance degradation in the subsequent phase. Adding
various synthesized noisy music data improves the model's generalization across
different noise levels. The proposed architecture demonstrates enhanced
performance in music auto-tagging by effectively utilizing unlabeled noisy
music data. Additional experiments with supplementary unlabeled data further
improves the model's performance, underscoring its robust generalization
capabilities and broad applicability.
\\ ( https://arxiv.org/abs/2401.15323 ,  799kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15335 (*cross-listing*)
Date: Sat, 27 Jan 2024 07:57:20 GMT   (3115kb,D)

Title: L-AutoDA: Leveraging Large Language Models for Automated Decision-based
  Adversarial Attacks
Authors: Ping Guo, Fei Liu, Xi Lin, Qingchuan Zhao, Qingfu Zhang
Categories: cs.CR cs.AI cs.CV cs.LG
Comments: Under Review of IJCNN 2024
\\
  In the rapidly evolving field of machine learning, adversarial attacks
present a significant challenge to model robustness and security.
Decision-based attacks, which only require feedback on the decision of a model
rather than detailed probabilities or scores, are particularly insidious and
difficult to defend against. This work introduces L-AutoDA (Large Language
Model-based Automated Decision-based Adversarial Attacks), a novel approach
leveraging the generative capabilities of Large Language Models (LLMs) to
automate the design of these attacks. By iteratively interacting with LLMs in
an evolutionary framework, L-AutoDA automatically designs competitive attack
algorithms efficiently without much human effort. We demonstrate the efficacy
of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline
methods in both success rate and computational efficiency. Our findings
underscore the potential of language models as tools for adversarial attack
generation and highlight new avenues for the development of robust AI systems.
\\ ( https://arxiv.org/abs/2401.15335 ,  3115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15390 (*cross-listing*)
Date: Sat, 27 Jan 2024 11:40:38 GMT   (15790kb)

Title: A microservice architecture for real-time IoT data processing: A
  reusable Web of things approach for smart ports
Authors: Guadalupe Ortiz, Juan Boubeta-Puig, Javier Criado, David Corral-Plaza,
  Alfonso Garcia-de-Prado, Inmaculada Medina-Bulo, Luis Iribarne
Categories: cs.SE cs.AI
Journal-ref: G.Ortiz,J.Boubeta-Puig,J.Criado,D.Corral-Plaza,A.Garcia de
  Prado,I.Medina-Bulo,L.Iribarne.A microservice architecture for real-time IoT
  data processing:A reusable Web of things approach for smart
  port.Comput.Stand.Interfaces 81:103604(2022)
DOI: 10.1016/j.csi.2021.103604
\\
  Major advances in telecommunications and the Internet of Things have given
rise to numerous smart city scenarios in which smart services are provided.
What was once a dream for the future has now become reality. However, the need
to provide these smart services quickly, efficiently, in an interoperable
manner and in real time is a cutting-edge technological challenge. Although
some software architectures offer solutions in this area, these are often
limited in terms of reusability and maintenance by independent modules,
involving the need for system downtime when maintaining or evolving, as well as
by a lack of standards in terms of the interoperability of their interface. In
this paper, we propose a fully reusable microservice architecture, standardized
through the use of the Web of things paradigm, and with high efficiency in
real-time data processing, supported by complex event processing techniques. To
illustrate the proposal, we present a fully reusable implementation of the
microservices necessary for the deployment of the architecture in the field of
air quality monitoring and alerting in smart ports. The performance evaluation
of this architecture shows excellent results.
\\ ( https://arxiv.org/abs/2401.15390 ,  15790kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15487 (*cross-listing*)
Date: Sat, 27 Jan 2024 19:34:13 GMT   (23kb)

Title: Artificial Intelligence: Arguments for Catastrophic Risk
Authors: Adam Bales, William D'Alessandro, Cameron Domenico Kirk-Giannini
Categories: cs.CY cs.AI
Comments: 12 pages
\\
  Recent progress in artificial intelligence (AI) has drawn attention to the
technology's transformative potential, including what some see as its prospects
for causing large-scale harm. We review two influential arguments purporting to
show how AI could pose catastrophic risks. The first argument -- the Problem of
Power-Seeking -- claims that, under certain assumptions, advanced AI systems
are likely to engage in dangerous power-seeking behavior in pursuit of their
goals. We review reasons for thinking that AI systems might seek power, that
they might obtain it, that this could lead to catastrophe, and that we might
build and deploy such systems anyway. The second argument claims that the
development of human-level AI will unlock rapid further progress, culminating
in AI systems far more capable than any human -- this is the Singularity
Hypothesis. Power-seeking behavior on the part of such systems might be
particularly dangerous. We discuss a variety of objections to both arguments
and conclude by assessing the state of the debate.
\\ ( https://arxiv.org/abs/2401.15487 ,  23kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15489 (*cross-listing*)
Date: Sat, 27 Jan 2024 19:44:15 GMT   (20902kb,D)

Title: Distilling Privileged Multimodal Information for Expression Recognition
  using Optimal Transport
Authors: Muhammad Haseeb Aslam, Muhammad Osama Zeeshan, Soufiane Belharbi,
  Marco Pedersoli, Alessandro Koerich, Simon Bacon and Eric Granger
Categories: cs.CV cs.AI
\\
  Multimodal affect recognition models have reached remarkable performance in
the lab environment due to their ability to model complementary and redundant
semantic information. However, these models struggle in the wild, mainly
because of the unavailability or quality of modalities used for training. In
practice, only a subset of the training-time modalities may be available at
test time. Learning with privileged information (PI) enables deep learning
models (DL) to exploit data from additional modalities only available during
training. State-of-the-art knowledge distillation (KD) methods have been
proposed to distill multiple teacher models (each trained on a modality) to a
common student model. These privileged KD methods typically utilize
point-to-point matching and have no explicit mechanism to capture the
structural information in the teacher representation space formed by
introducing the privileged modality. We argue that encoding this same structure
in the student space may lead to enhanced student performance. This paper
introduces a new structural KD mechanism based on optimal transport (OT), where
entropy-regularized OT distills the structural dark knowledge. Privileged KD
with OT (PKDOT) method captures the local structures in the multimodal teacher
representation by calculating a cosine similarity matrix and selects the top-k
anchors to allow for sparse OT solutions, resulting in a more stable
distillation process. Experiments were performed on two different problems:
pain estimation on the Biovid dataset (ordinal classification) and
arousal-valance prediction on the Affwild2 dataset (regression). Results show
that the proposed method can outperform state-of-the-art privileged KD methods
on these problems. The diversity of different modalities and fusion
architectures indicates that the proposed PKDOT method is modality and
model-agnostic.
\\ ( https://arxiv.org/abs/2401.15489 ,  20902kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15545 (*cross-listing*)
Date: Sun, 28 Jan 2024 02:27:38 GMT   (613kb,D)

Title: PPM: Automated Generation of Diverse Programming Problems for
  Benchmarking Code Generation Models
Authors: Simin Chen, Xiaoning Feng, Xiaohong Han, Cong Liu, Wei Yang
Categories: cs.SE cs.AI cs.CL cs.PL
Comments: This paper has been accepted to The ACM International Conference on
  the Foundations of Software Engineering FSE 2024
\\
  In recent times, a plethora of Large Code Generation Models (LCGMs) have been
proposed, showcasing significant potential in assisting developers with complex
programming tasks. Benchmarking LCGMs necessitates the creation of a set of
diverse programming problems, and each problem comprises the prompt (including
the task description), canonical solution, and test inputs. The existing
methods for constructing such a problem set can be categorized into two main
types: manual methods and perturbation-based methods. However, manual methods
demand high effort and lack scalability, while also risking data integrity due
to LCGMs' potentially contaminated data collection, and perturbation-based
approaches mainly generate semantically homogeneous problems with the same
canonical solutions and introduce typos that can be easily auto-corrected by
IDE, making them ineffective and unrealistic. In this work, we propose the idea
of programming problem merging (PPM) and provide two implementation of this
idea, we utilize our tool on two widely-used datasets and compare it against
nine baseline methods using eight code generation models. The results
demonstrate the effectiveness of our tool in generating more challenging,
diverse, and natural programming problems, comparing to the baselines.
\\ ( https://arxiv.org/abs/2401.15545 ,  613kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15564 (*cross-listing*)
Date: Sun, 28 Jan 2024 04:14:35 GMT   (784kb)

Title: Design of UAV flight state recognition and trajectory prediction system
  based on trajectory feature construction
Authors: Xingyu Zhou and Zhuoyong Shi
Categories: eess.SY cs.AI cs.SY
\\
  With the impact of artificial intelligence on the traditional UAV industry,
autonomous UAV flight has become a current hot research field. Based on the
demand for research on critical technologies for autonomous flying UAVs, this
paper addresses the field of flight state recognition and trajectory prediction
of UAVs. This paper proposes a method to improve the accuracy of UAV trajectory
prediction based on UAV flight state recognition and verifies it using two
prediction models. Firstly, UAV flight data acquisition and data preprocessing
are carried out; secondly, UAV flight trajectory features are extracted based
on data fusion and a UAV flight state recognition model based on PCA-DAGSVM
model is established; finally, two UAV flight trajectory prediction models are
established and the trajectory prediction errors of the two prediction models
are compared and analyzed after flight state recognition. The results show
that: 1) the UAV flight state recognition model based on PCA-DAGSVM has good
recognition effect. 2) compared with the traditional UAV trajectory prediction
model, the prediction model based on flight state recognition can effectively
reduce the prediction error.
\\ ( https://arxiv.org/abs/2401.15564 ,  784kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15568 (*cross-listing*)
Date: Sun, 28 Jan 2024 04:59:51 GMT   (34099kb,D)

Title: Intriguing Equivalence Structures of the Embedding Space of Vision
  Transformers
Authors: Shaeke Salman and Md Montasir Bin Shams and Xiuwen Liu
Categories: cs.CV cs.AI cs.LG
Comments: 8 pages, 9 figures
\\
  Pre-trained large foundation models play a central role in the recent surge
of artificial intelligence, resulting in fine-tuned models with remarkable
abilities when measured on benchmark datasets, standard exams, and
applications. Due to their inherent complexity, these models are not well
understood. While small adversarial inputs to such models are well known, the
structures of the representation space are not well characterized despite their
fundamental importance. In this paper, using the vision transformers as an
example due to the continuous nature of their input space, we show via analyses
and systematic experiments that the representation space consists of large
piecewise linear subspaces where there exist very different inputs sharing the
same representations, and at the same time, local normal spaces where there are
visually indistinguishable inputs having very different representations. The
empirical results are further verified using the local directional estimations
of the Lipschitz constants of the underlying models. Consequently, the
resulting representations change the results of downstream models, and such
models are subject to overgeneralization and with limited semantically
meaningful generalization capability.
\\ ( https://arxiv.org/abs/2401.15568 ,  34099kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15620 (*cross-listing*)
Date: Sun, 28 Jan 2024 10:17:36 GMT   (1628kb,D)

Title: Data-Driven Strategies for Coping with Incomplete DVL Measurements
Authors: Nadav Cohen and Itzik Klein
Categories: cs.RO cs.AI cs.SY eess.SP eess.SY
\\
  Autonomous underwater vehicles are specialized platforms engineered for deep
underwater operations. Critical to their functionality is autonomous
navigation, typically relying on an inertial navigation system and a Doppler
velocity log. In real-world scenarios, incomplete Doppler velocity log
measurements occur, resulting in positioning errors and mission aborts. To cope
with such situations, a model and learning approaches were derived. This paper
presents a comparative analysis of two cutting-edge deep learning
methodologies, namely LiBeamsNet and MissBeamNet, alongside a model-based
average estimator. These approaches are evaluated for their efficacy in
regressing missing Doppler velocity log beams when two beams are unavailable.
In our study, we used data recorded by a DVL mounted on an autonomous
underwater vehicle operated in the Mediterranean Sea. We found that both deep
learning architectures outperformed model-based approaches by over 16% in
velocity prediction accuracy.
\\ ( https://arxiv.org/abs/2401.15620 ,  1628kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15625 (*cross-listing*)
Date: Sun, 28 Jan 2024 10:46:17 GMT   (3150kb,D)

Title: Generative AI-enabled Blockchain Networks: Fundamentals, Applications,
  and Case Study
Authors: Cong T. Nguyen, Yinqiu Liu, Hongyang Du, Dinh Thai Hoang, Dusit
  Niyato, Diep N. Nguyen, Shiwen Mao
Categories: cs.CR cs.AI
\\
  Generative Artificial Intelligence (GAI) has recently emerged as a promising
solution to address critical challenges of blockchain technology, including
scalability, security, privacy, and interoperability. In this paper, we first
introduce GAI techniques, outline their applications, and discuss existing
solutions for integrating GAI into blockchains. Then, we discuss emerging
solutions that demonstrate the effectiveness of GAI in addressing various
challenges of blockchain, such as detecting unknown blockchain attacks and
smart contract vulnerabilities, designing key secret sharing schemes, and
enhancing privacy. Moreover, we present a case study to demonstrate that GAI,
specifically the generative diffusion model, can be employed to optimize
blockchain network performance metrics. Experimental results clearly show that,
compared to a baseline traditional AI approach, the proposed generative
diffusion model approach can converge faster, achieve higher rewards, and
significantly improve the throughput and latency of the blockchain network.
Additionally, we highlight future research directions for GAI in blockchain
applications, including personalized GAI-enabled blockchains, GAI-blockchain
synergy, and privacy and security considerations within blockchain ecosystems.
\\ ( https://arxiv.org/abs/2401.15625 ,  3150kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15647 (*cross-listing*)
Date: Sun, 28 Jan 2024 12:51:01 GMT   (9973kb,D)

Title: UP-CrackNet: Unsupervised Pixel-Wise Road Crack Detection via
  Adversarial Image Restoration
Authors: Nachuan Ma, Rui Fan, Lihua Xie
Categories: cs.CV cs.AI eess.IV
\\
  Over the past decade, automated methods have been developed to detect cracks
more efficiently, accurately, and objectively, with the ultimate goal of
replacing conventional manual visual inspection techniques. Among these
methods, semantic segmentation algorithms have demonstrated promising results
in pixel-wise crack detection tasks. However, training such data-driven
algorithms requires a large amount of human-annotated datasets with pixel-level
annotations, which is a highly labor-intensive and time-consuming process.
Moreover, supervised learning-based methods often struggle with poor
generalization ability in unseen datasets. Therefore, we propose an
unsupervised pixel-wise road crack detection network, known as UP-CrackNet. Our
approach first generates multi-scale square masks and randomly selects them to
corrupt undamaged road images by removing certain regions. Subsequently, a
generative adversarial network is trained to restore the corrupted regions by
leveraging the semantic context learned from surrounding uncorrupted regions.
During the testing phase, an error map is generated by calculating the
difference between the input and restored images, which allows for pixel-wise
crack detection. Our comprehensive experimental results demonstrate that
UP-CrackNet outperforms other general-purpose unsupervised anomaly detection
algorithms, and exhibits comparable performance and superior generalizability
when compared with state-of-the-art supervised crack segmentation algorithms.
Our source code is publicly available at mias.group/UP-CrackNet.
\\ ( https://arxiv.org/abs/2401.15647 ,  9973kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15675 (*cross-listing*)
Date: Sun, 28 Jan 2024 14:45:52 GMT   (1157kb)

Title: Detection of a facemask in real-time using deep learning methods:
  Prevention of Covid 19
Authors: Gautam Siddharth Kashyap, Jatin Sohlot, Ayesha Siddiqui, Ramsha
  Siddiqui, Karan Malik, Samar Wazir, and Alexander E. I. Brownlee
Categories: cs.CV cs.AI
Comments: Research Advances in Network Technologies (Volume 2) (CRC Press
  Taylor and Francis), 2023 (Accepted)
\\
  A health crisis is raging all over the world with the rapid transmission of
the novel-coronavirus disease (Covid-19). Out of the guidelines issued by the
World Health Organisation (WHO) to protect us against Covid-19, wearing a
facemask is the most effective. Many countries have necessitated the wearing of
face masks, but monitoring a large number of people to ensure that they are
wearing masks in a crowded place is a challenging task in itself. The
novel-coronavirus disease (Covid-19) has already affected our day-to-day life
as well as world trade movements. By the end of April 2021, the world has
recorded 144,358,956 confirmed cases of novel-coronavirus disease (Covid-19)
including 3,066,113 deaths according to the world health organization (WHO).
These increasing numbers motivate automated techniques for the detection of a
facemask in real-time scenarios for the prevention of Covid-19. We propose a
technique using deep learning that works for single and multiple people in a
frame recorded via webcam in still or in motion. We have also experimented with
our approach in night light. The accuracy of our model is good compared to the
other approaches in the literature; ranging from 74% for multiple people in a
nightlight to 99% for a single person in daylight.
\\ ( https://arxiv.org/abs/2401.15675 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15721 (*cross-listing*)
Date: Sun, 28 Jan 2024 18:09:02 GMT   (1774kb,D)

Title: A Study of Acquisition Functions for Medical Imaging Deep Active
  Learning
Authors: Bonaventure F. P. Dossou
Categories: cs.CV cs.AI cs.HC cs.LG
Comments: Best Poster Award at Deep Learning Indaba 2023 Conference
\\
  The Deep Learning revolution has enabled groundbreaking achievements in
recent years. From breast cancer detection to protein folding, deep learning
algorithms have been at the core of very important advancements. However, these
modern advancements are becoming more and more data-hungry, especially on
labeled data whose availability is scarce: this is even more prevalent in the
medical context. In this work, we show how active learning could be very
effective in data scarcity situations, where obtaining labeled data (or
annotation budget is very limited). We compare several selection criteria
(BALD, MeanSTD, and MaxEntropy) on the ISIC 2016 dataset. We also explored the
effect of acquired pool size on the model's performance. Our results suggest
that uncertainty is useful to the Melanoma detection task, and confirms the
hypotheses of the author of the paper of interest, that \textit{bald} performs
on average better than other acquisition functions. Our extended analyses
however revealed that all acquisition functions perform badly on the positive
(cancerous) samples, suggesting exploitation of class unbalance, which could be
crucial in real-world settings. We finish by suggesting future work directions
that would be useful to improve this current work. The code of our
implementation is open-sourced at
\url{https://github.com/bonaventuredossou/ece526_course_project}
\\ ( https://arxiv.org/abs/2401.15721 ,  1774kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15741 (*cross-listing*)
Date: Sun, 28 Jan 2024 19:58:19 GMT   (1054kb)

Title: SERNet-Former: Semantic Segmentation by Efficient Residual Network with
  Attention-Boosting Gates and Attention-Fusion Networks
Authors: Serdar Erisen
Categories: cs.CV cs.AI
\\
  Improving the efficiency of state-of-the-art methods in semantic segmentation
requires overcoming the increasing computational cost as well as issues such as
fusing semantic information from global and local contexts. Based on the recent
success and problems that convolutional neural networks (CNNs) encounter in
semantic segmentation, this research proposes an encoder-decoder architecture
with a unique efficient residual network. Attention-boosting gates (AbGs) and
attention-boosting modules (AbMs) are deployed by aiming to fuse the
feature-based semantic information with the global context of the efficient
residual network in the encoder. Respectively, the decoder network is developed
with the additional attention-fusion networks (AfNs) inspired by AbM. AfNs are
designed to improve the efficiency in the one-to-one conversion of the semantic
information by deploying additional convolution layers in the decoder part. Our
network is tested on the challenging CamVid and Cityscapes datasets, and the
proposed methods reveal significant improvements on the existing baselines,
such as ResNet-50. To the best of our knowledge, the developed network,
SERNet-Former, achieves state-of-the-art results (84.62 % mean IoU) on CamVid
dataset and challenging results (87.35 % mean IoU) on Cityscapes validation
dataset.
\\ ( https://arxiv.org/abs/2401.15741 ,  1054kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15753 (*cross-listing*)
Date: Sun, 28 Jan 2024 20:30:14 GMT   (14346kb,D)

Title: An objective comparison of methods for augmented reality in laparoscopic
  liver resection by preoperative-to-intraoperative image fusion
Authors: Sharib Ali, Yamid Espinel, Yueming Jin, Peng Liu, Bianca G\"uttner,
  Xukun Zhang, Lihua Zhang, Tom Dowrick, Matthew J. Clarkson, Shiting Xiao,
  Yifan Wu, Yijun Yang, Lei Zhu, Dai Sun, Lan Li, Micha Pfeiffer, Shahid Farid,
  Lena Maier-Hein, Emmanuel Buc, Adrien Bartoli
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: 24 pages
\\
  Augmented reality for laparoscopic liver resection is a visualisation mode
that allows a surgeon to localise tumours and vessels embedded within the liver
by projecting them on top of a laparoscopic image. Preoperative 3D models
extracted from CT or MRI data are registered to the intraoperative laparoscopic
images during this process. In terms of 3D-2D fusion, most of the algorithms
make use of anatomical landmarks to guide registration. These landmarks include
the liver's inferior ridge, the falciform ligament, and the occluding contours.
They are usually marked by hand in both the laparoscopic image and the 3D
model, which is time-consuming and may contain errors if done by a
non-experienced user. Therefore, there is a need to automate this process so
that augmented reality can be used effectively in the operating room. We
present the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge
(P2ILF), held during the Medical Imaging and Computer Assisted Interventions
(MICCAI 2022) conference, which investigates the possibilities of detecting
these landmarks automatically and using them in registration. The challenge was
divided into two tasks: 1) A 2D and 3D landmark detection task and 2) a 3D-2D
registration task. The teams were provided with training data consisting of 167
laparoscopic images and 9 preoperative 3D models from 9 patients, with the
corresponding 2D and 3D landmark annotations. A total of 6 teams from 4
countries participated, whose proposed methods were evaluated on 16 images and
two preoperative 3D models from two patients. All the teams proposed deep
learning-based methods for the 2D and 3D landmark segmentation tasks and
differentiable rendering-based methods for the registration task. Based on the
experimental outcomes, we propose three key hypotheses that determine current
limitations and future directions for research in this domain.
\\ ( https://arxiv.org/abs/2401.15753 ,  14346kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15766 (*cross-listing*)
Date: Sun, 28 Jan 2024 21:01:45 GMT   (263kb)

Title: EEG for fatigue monitoring
Authors: Ildar Rakhmatulin
Categories: cs.HC cs.AI eess.SP
\\
  Physiological fatigue, a state of reduced cognitive and physical performance
resulting from prolonged mental or physical exertion, poses significant
challenges in various domains, including healthcare, aviation, transportation,
and industrial sectors. As the understanding of fatigue's impact on human
performance grows, there is a growing interest in developing effective fatigue
monitoring techniques. Among these techniques, electroencephalography (EEG) has
emerged as a promising tool for objectively assessing physiological fatigue due
to its non-invasiveness, high temporal resolution, and sensitivity to neural
activity. This paper aims to provide a comprehensive analysis of the current
state of the use of EEG for monitoring physiological fatigue.
\\ ( https://arxiv.org/abs/2401.15766 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15801 (*cross-listing*)
Date: Sun, 28 Jan 2024 23:18:10 GMT   (47kb)

Title: On the Statistical Properties of Generative Adversarial Models for Low
  Intrinsic Data Dimension
Authors: Saptarshi Chakraborty and Peter L. Bartlett
Categories: stat.ML cs.AI cs.LG math.ST stat.TH
\\
  Despite the remarkable empirical successes of Generative Adversarial Networks
(GANs), the theoretical guarantees for their statistical accuracy remain rather
pessimistic. In particular, the data distributions on which GANs are applied,
such as natural images, are often hypothesized to have an intrinsic
low-dimensional structure in a typically high-dimensional feature space, but
this is often not reflected in the derived rates in the state-of-the-art
analyses. In this paper, we attempt to bridge the gap between the theory and
practice of GANs and their bidirectional variant, Bi-directional GANs (BiGANs),
by deriving statistical guarantees on the estimated densities in terms of the
intrinsic dimension of the data and the latent space. We analytically show that
if one has access to $n$ samples from the unknown target distribution and the
network architectures are properly chosen, the expected Wasserstein-1 distance
of the estimates from the target scales as $O\left( n^{-1/d_\mu } \right)$ for
GANs and $O\left( n^{-1/(d_\mu+\ell)} \right)$ for BiGANs, where $d_\mu$ and
$\ell$ are the upper Wasserstein-1 dimension of the data-distribution and
latent-space dimension, respectively. The theoretical analyses not only suggest
that these methods successfully avoid the curse of dimensionality, in the sense
that the exponent of $n$ in the error rates does not depend on the data
dimension but also serve to bridge the gap between the theoretical analyses of
GANs and the known sharp rates from optimal transport literature. Additionally,
we demonstrate that GANs can effectively achieve the minimax optimal rate even
for non-smooth underlying distributions, with the use of larger generator
networks.
\\ ( https://arxiv.org/abs/2401.15801 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15803 (*cross-listing*)
Date: Sun, 28 Jan 2024 23:26:15 GMT   (20319kb,D)

Title: GarchingSim: An Autonomous Driving Simulator with Photorealistic Scenes
  and Minimalist Workflow
Authors: Liguo Zhou, Yinglei Song, Yichao Gao, Zhou Yu, Michael Sodamin,
  Hongshen Liu, Liang Ma, Lian Liu, Hao Liu, Yang Liu, Haichuan Li, Guang Chen,
  Alois Knoll
Categories: cs.RO cs.AI cs.CV cs.SY eess.SY
\\
  Conducting real road testing for autonomous driving algorithms can be
expensive and sometimes impractical, particularly for small startups and
research institutes. Thus, simulation becomes an important method for
evaluating these algorithms. However, the availability of free and open-source
simulators is limited, and the installation and configuration process can be
daunting for beginners and interdisciplinary researchers. We introduce an
autonomous driving simulator with photorealistic scenes, meanwhile keeping a
user-friendly workflow. The simulator is able to communicate with external
algorithms through ROS2 or Socket.IO, making it compatible with existing
software stacks. Furthermore, we implement a highly accurate vehicle dynamics
model within the simulator to enhance the realism of the vehicle's physical
effects. The simulator is able to serve various functions, including generating
synthetic data and driving with machine learning-based algorithms. Moreover, we
prioritize simplicity in the deployment process, ensuring that beginners find
it approachable and user-friendly.
\\ ( https://arxiv.org/abs/2401.15803 ,  20319kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15810 (*cross-listing*)
Date: Mon, 29 Jan 2024 00:15:50 GMT   (1917kb,D)

Title: Green Runner: A tool for efficient deep learning component selection
Authors: Jai Kannan
Categories: cs.SE cs.AI
\\
  For software that relies on machine-learned functionality, model selection is
key to finding the right model for the task with desired performance
characteristics. Evaluating a model requires developers to i) select from many
models (e.g. the Hugging face model repository), ii) select evaluation metrics
and training strategy, and iii) tailor trade-offs based on the problem domain.
However, current evaluation approaches are either ad-hoc resulting in
sub-optimal model selection or brute force leading to wasted compute. In this
work, we present \toolname, a novel tool to automatically select and evaluate
models based on the application scenario provided in natural language. We
leverage the reasoning capabilities of large language models to propose a
training strategy and extract desired trade-offs from a problem description.
\toolname~features a resource-efficient experimentation engine that integrates
constraints and trade-offs based on the problem into the model selection
process. Our preliminary evaluation demonstrates that \toolname{} is both
efficient and accurate compared to ad-hoc evaluations and brute force. This
work presents an important step toward energy-efficient tools to help reduce
the environmental impact caused by the growing demand for software with
machine-learned functionality.
\\ ( https://arxiv.org/abs/2401.15810 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15820 (*cross-listing*)
Date: Mon, 29 Jan 2024 01:00:17 GMT   (758kb,D)

Title: Knowledge-Aware Neuron Interpretation for Scene Classification
Authors: Yong Guan, Freddy Lecue, Jiaoyan Chen, Ru Li, Jeff Z. Pan
Categories: cs.CV cs.AI
Comments: Accepted to AAAI2024
\\
  Although neural models have achieved remarkable performance, they still
encounter doubts due to the intransparency. To this end, model prediction
explanation is attracting more and more attentions. However, current methods
rarely incorporate external knowledge and still suffer from three limitations:
(1) Neglecting concept completeness. Merely selecting concepts may not
sufficient for prediction. (2) Lacking concept fusion. Failure to merge
semantically-equivalent concepts. (3) Difficult in manipulating model behavior.
Lack of verification for explanation on original model. To address these
issues, we propose a novel knowledge-aware neuron interpretation framework to
explain model predictions for image scene classification. Specifically, for
concept completeness, we present core concepts of a scene based on knowledge
graph, ConceptNet, to gauge the completeness of concepts. Our method,
incorporating complete concepts, effectively provides better prediction
explanations compared to baselines. Furthermore, for concept fusion, we
introduce a knowledge graph-based method known as Concept Filtering, which
produces over 23% point gain on neuron behaviors for neuron interpretation. At
last, we propose Model Manipulation, which aims to study whether the core
concepts based on ConceptNet could be employed to manipulate model behavior.
The results show that core concepts can effectively improve the performance of
original model by over 26%.
\\ ( https://arxiv.org/abs/2401.15820 ,  758kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15834 (*cross-listing*)
Date: Mon, 29 Jan 2024 01:52:49 GMT   (5490kb,D)

Title: Few and Fewer: Learning Better from Few Examples Using Fewer Base
  Classes
Authors: Raphael Lafargue, Yassir Bendou, Bastien Pasdeloup, Jean-Philippe
  Diguet, Ian Reid, Vincent Gripon and Jack Valmadre
Categories: cs.CV cs.AI
Comments: 9.5 pages + bibliography and supplementary material
MSC-class: 68T
ACM-class: I.2; I.4; I.5
\\
  When training data is scarce, it is common to make use of a feature extractor
that has been pre-trained on a large base dataset, either by fine-tuning its
parameters on the ``target'' dataset or by directly adopting its representation
as features for a simple classifier. Fine-tuning is ineffective for few-shot
learning, since the target dataset contains only a handful of examples.
However, directly adopting the features without fine-tuning relies on the base
and target distributions being similar enough that these features achieve
separability and generalization. This paper investigates whether better
features for the target dataset can be obtained by training on fewer base
classes, seeking to identify a more useful base dataset for a given task.We
consider cross-domain few-shot image classification in eight different domains
from Meta-Dataset and entertain multiple real-world settings (domain-informed,
task-informed and uninformed) where progressively less detail is known about
the target task. To our knowledge, this is the first demonstration that
fine-tuning on a subset of carefully selected base classes can significantly
improve few-shot learning. Our contributions are simple and intuitive methods
that can be implemented in any few-shot solution. We also give insights into
the conditions in which these solutions are likely to provide a boost in
accuracy. We release the code to reproduce all experiments from this paper on
GitHub. https://github.com/RafLaf/Few-and-Fewer.git
\\ ( https://arxiv.org/abs/2401.15834 ,  5490kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15842 (*cross-listing*)
Date: Mon, 29 Jan 2024 02:32:25 GMT   (12519kb)

Title: LCVO: An Efficient Pretraining-Free Framework for Visual Question
  Answering Grounding
Authors: Yuhan Chen, Lumei Su, Lihua Chen, Zhiwei Lin
Categories: cs.CV cs.AI
Comments: 21 pages,9 figures
\\
  In this paper, the LCVO modular method is proposed for the Visual Question
Answering (VQA) Grounding task in the vision-language multimodal domain. This
approach relies on a frozen large language model (LLM) as intermediate mediator
between the off-the-shelf VQA model and the off-the-shelf Open-Vocabulary
Object Detection (OVD) model, where the LLM transforms and conveys textual
information between the two modules based on a designed prompt. LCVO establish
an integrated plug-and-play framework without the need for any pre-training
process. This framework can be deployed for VQA Grounding tasks under low
computational resources. The modularized model within the framework allows
application with various state-of-the-art pre-trained models, exhibiting
significant potential to be advance with the times. Experimental
implementations were conducted under constrained computational and memory
resources, evaluating the proposed method's performance on benchmark datasets
including GQA, CLEVR, and VizWiz-VQA-Grounding. Comparative analyses with
baseline methods demonstrate the robust competitiveness of LCVO.
\\ ( https://arxiv.org/abs/2401.15842 ,  12519kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15847 (*cross-listing*)
Date: Mon, 29 Jan 2024 02:43:40 GMT   (9357kb,D)

Title: Muffin or Chihuahua? Challenging Large Vision-Language Models with
  Multipanel VQA
Authors: Yue Fan, Jing Gu, Kaiwen Zhou, Qianqi Yan, Shan Jiang, Ching-Chen Kuo,
  Xinze Guan, Xin Eric Wang
Categories: cs.CV cs.AI cs.CL
\\
  Multipanel images, commonly seen as web screenshots, posters, etc., pervade
our daily lives. These images, characterized by their composition of multiple
subfigures in distinct layouts, effectively convey information to people.
Toward building advanced multimodal AI applications, such as agents that
understand complex scenes and navigate through webpages, the skill of
multipanel visual reasoning is essential, and a comprehensive evaluation of
models in this regard is important. Therefore, our paper introduces Multipanel
Visual Question Answering (MultipanelVQA), a novel benchmark that specifically
challenges models in comprehending multipanel images. The benchmark comprises
6,600 questions and answers related to multipanel images. While these questions
are straightforward for average humans, achieving nearly perfect correctness,
they pose significant challenges to the state-of-the-art Large Vision Language
Models (LVLMs) we tested. In our study, we utilized synthetically curated
multipanel images specifically designed to isolate and evaluate the impact of
diverse factors on model performance, revealing the sensitivity of LVLMs to
various interferences in multipanel images, such as adjacent subfigures and
layout complexity. As a result, MultipanelVQA highlights the need and direction
for improving LVLMs' ability to understand complex visual-language contexts.
Code and data are released at https://sites.google.com/view/multipanelvqa/home.
\\ ( https://arxiv.org/abs/2401.15847 ,  9357kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15859 (*cross-listing*)
Date: Mon, 29 Jan 2024 03:20:19 GMT   (42870kb,D)

Title: Diffusion Facial Forgery Detection
Authors: Harry Cheng and Yangyang Guo and Tianyi Wang and Liqiang Nie and Mohan
  Kankanhalli
Categories: cs.CV cs.AI
Comments: The dataset will be released at
  \url{https://github.com/xaCheng1996/DiFF}
\\
  Detecting diffusion-generated images has recently grown into an emerging
research area. Existing diffusion-based datasets predominantly focus on general
image generation. However, facial forgeries, which pose a more severe social
risk, have remained less explored thus far. To address this gap, this paper
introduces DiFF, a comprehensive dataset dedicated to face-focused
diffusion-generated images. DiFF comprises over 500,000 images that are
synthesized using thirteen distinct generation methods under four conditions.
In particular, this dataset leverages 30,000 carefully collected textual and
visual prompts, ensuring the synthesis of images with both high fidelity and
semantic consistency. We conduct extensive experiments on the DiFF dataset via
a human test and several representative forgery detection methods. The results
demonstrate that the binary detection accuracy of both human observers and
automated detectors often falls below 30%, shedding light on the challenges in
detecting diffusion-generated facial forgeries. Furthermore, we propose an edge
graph regularization approach to effectively enhance the generalization
capability of existing detectors.
\\ ( https://arxiv.org/abs/2401.15859 ,  42870kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15863 (*cross-listing*)
Date: Mon, 29 Jan 2024 03:29:39 GMT   (13430kb,D)

Title: Importance-Aware Adaptive Dataset Distillation
Authors: Guang Li, Ren Togo, Takahiro Ogawa, Miki Haseyama
Categories: cs.CV cs.AI cs.LG
Comments: Published as a journal paper in Elsevier Neural Networks
\\
  Herein, we propose a novel dataset distillation method for constructing small
informative datasets that preserve the information of the large original
datasets. The development of deep learning models is enabled by the
availability of large-scale datasets. Despite unprecedented success,
large-scale datasets considerably increase the storage and transmission costs,
resulting in a cumbersome model training process. Moreover, using raw data for
training raises privacy and copyright concerns. To address these issues, a new
task named dataset distillation has been introduced, aiming to synthesize a
compact dataset that retains the essential information from the large original
dataset. State-of-the-art (SOTA) dataset distillation methods have been
proposed by matching gradients or network parameters obtained during training
on real and synthetic datasets. The contribution of different network
parameters to the distillation process varies, and uniformly treating them
leads to degraded distillation performance. Based on this observation, we
propose an importance-aware adaptive dataset distillation (IADD) method that
can improve distillation performance by automatically assigning importance
weights to different network parameters during distillation, thereby
synthesizing more robust distilled datasets. IADD demonstrates superior
performance over other SOTA dataset distillation methods based on parameter
matching on multiple benchmark datasets and outperforms them in terms of
cross-architecture generalization. In addition, the analysis of self-adaptive
weights demonstrates the effectiveness of IADD. Furthermore, the effectiveness
of IADD is validated in a real-world medical application such as COVID-19
detection.
\\ ( https://arxiv.org/abs/2401.15863 ,  13430kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15889 (*cross-listing*)
Date: Mon, 29 Jan 2024 04:59:30 GMT   (2187kb,D)

Title: Sliced Wasserstein with Random-Path Projecting Directions
Authors: Khai Nguyen and Shujian Zhang and Tam Le and Nhat Ho
Categories: stat.ML cs.AI cs.CV cs.LG
Comments: 27 pages, 3 figures, 1 table
\\
  Slicing distribution selection has been used as an effective technique to
improve the performance of parameter estimators based on minimizing sliced
Wasserstein distance in applications. Previous works either utilize expensive
optimization to select the slicing distribution or use slicing distributions
that require expensive sampling methods. In this work, we propose an
optimization-free slicing distribution that provides a fast sampling for the
Monte Carlo estimation of expectation. In particular, we introduce the
random-path projecting direction (RPD) which is constructed by leveraging the
normalized difference between two random vectors following the two input
measures. From the RPD, we derive the random-path slicing distribution (RPSD)
and two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced
Wasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced
Wasserstein (IWRPSW). We then discuss the topological, statistical, and
computational properties of RPSW and IWRPSW. Finally, we showcase the favorable
performance of RPSW and IWRPSW in gradient flow and the training of denoising
diffusion generative models on images.
\\ ( https://arxiv.org/abs/2401.15889 ,  2187kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15896 (*cross-listing*)
Date: Mon, 29 Jan 2024 05:43:33 GMT   (8831kb,D)

Title: $\boldsymbol{M^2}$-Encoder: Advancing Bilingual Image-Text Understanding
  by Large-scale Efficient Pretraining
Authors: Qingpei Guo, Furong Xu, Hanxiao Zhang, Wang Ren, Ziping Ma, Lin Ju,
  Jian Wang, Jingdong Chen, Ming Yang
Categories: cs.CV cs.AI
\\
  Vision-language foundation models like CLIP have revolutionized the field of
artificial intelligence. Nevertheless, VLM models supporting multi-language,
e.g., in both Chinese and English, have lagged due to the relative scarcity of
large-scale pretraining datasets. Toward this end, we introduce a comprehensive
bilingual (Chinese-English) dataset BM-6B with over 6 billion image-text pairs,
aimed at enhancing multimodal foundation models to well understand images in
both languages. To handle such a scale of dataset, we propose a novel grouped
aggregation approach for image-text contrastive loss computation, which reduces
the communication overhead and GPU memory demands significantly, facilitating a
60% increase in training speed. We pretrain a series of bilingual image-text
foundation models with an enhanced fine-grained understanding ability on BM-6B,
the resulting models, dubbed as $M^2$-Encoders (pronounced "M-Square"), set new
benchmarks in both languages for multimodal retrieval and classification tasks.
Notably, Our largest $M^2$-Encoder-10B model has achieved top-1 accuracies of
88.5% on ImageNet and 80.7% on ImageNet-CN under a zero-shot classification
setting, surpassing previously reported SoTA methods by 2.2% and 21.1%,
respectively. The $M^2$-Encoder series represents one of the most comprehensive
bilingual image-text foundation models to date, so we are making it available
to the research community for further exploration and development.
\\ ( https://arxiv.org/abs/2401.15896 ,  8831kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15914 (*cross-listing*)
Date: Mon, 29 Jan 2024 06:57:48 GMT   (5340kb,D)

Title: Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD
  Generalization
Authors: Yuhang Zang, Hanlin Goh, Josh Susskind, Chen Huang
Categories: cs.CV cs.AI
Comments: ICLR 2024
\\
  Existing vision-language models exhibit strong generalization on a variety of
visual domains and tasks. However, such models mainly perform zero-shot
recognition in a closed-set manner, and thus struggle to handle open-domain
visual concepts by design. There are recent finetuning methods, such as prompt
learning, that not only study the discrimination between in-distribution (ID)
and out-of-distribution (OOD) samples, but also show some improvements in both
ID and OOD accuracies. In this paper, we first demonstrate that vision-language
models, after long enough finetuning but without proper regularization, tend to
overfit the known classes in the given dataset, with degraded performance on
unknown classes. Then we propose a novel approach OGEN to address this pitfall,
with the main focus on improving the OOD GENeralization of finetuned models.
Specifically, a class-conditional feature generator is introduced to synthesize
OOD features using just the class name of any unknown class. Such synthesized
features will provide useful knowledge about unknowns and help regularize the
decision boundary between ID and OOD data when optimized jointly. Equally
important is our adaptive self-distillation mechanism to regularize our feature
generation model during joint optimization, i.e., adaptively transferring
knowledge between model states to further prevent overfitting. Experiments
validate that our method yields convincing gains in OOD generalization
performance in different settings.
\\ ( https://arxiv.org/abs/2401.15914 ,  5340kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15931 (*cross-listing*)
Date: Mon, 29 Jan 2024 07:41:44 GMT   (6400kb,D)

Title: EmoDM: A Diffusion Model for Evolutionary Multi-objective Optimization
Authors: Xueming Yan and Yaochu Jin
Categories: cs.NE cs.AI
\\
  Evolutionary algorithms have been successful in solving multi-objective
optimization problems (MOPs). However, as a class of population-based search
methodology, evolutionary algorithms require a large number of evaluations of
the objective functions, preventing them from being applied to a wide range of
expensive MOPs. To tackle the above challenge, this work proposes for the first
time a diffusion model that can learn to perform evolutionary multi-objective
search, called EmoDM. This is achieved by treating the reversed convergence
process of evolutionary search as the forward diffusion and learn the noise
distributions from previously solved evolutionary optimization tasks. The
pre-trained EmoDM can then generate a set of non-dominated solutions for a new
MOP by means of its reverse diffusion without further evolutionary search,
thereby significantly reducing the required function evaluations. To enhance
the scalability of EmoDM, a mutual entropy-based attention mechanism is
introduced to capture the decision variables that are most important for the
objectives. Experimental results demonstrate the competitiveness of EmoDM in
terms of both the search performance and computational efficiency compared with
state-of-the-art evolutionary algorithms in solving MOPs having up to 5000
decision variables. The pre-trained EmoDM is shown to generalize well to unseen
problems, revealing its strong potential as a general and efficient MOP solver.
\\ ( https://arxiv.org/abs/2401.15931 ,  6400kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15944 (*cross-listing*)
Date: Mon, 29 Jan 2024 08:10:00 GMT   (569kb,D)

Title: Bridging the Domain Gap: A Simple Domain Matching Method for
  Reference-based Image Super-Resolution in Remote Sensing
Authors: Jeongho Min, Yejun Lee, Dongyoung Kim, Jaejun Yoo
Categories: cs.CV cs.AI
Comments: Accepted to IEEE GRSL 2023
Report-no: Article Sequence Number: 8000105, Print ISSN: 1545-598X
Journal-ref: Volume: 21, Year: 2023, Page: 1-5
DOI: 10.1109/LGRS.2023.3336680
\\
  Recently, reference-based image super-resolution (RefSR) has shown excellent
performance in image super-resolution (SR) tasks. The main idea of RefSR is to
utilize additional information from the reference (Ref) image to recover the
high-frequency components in low-resolution (LR) images. By transferring
relevant textures through feature matching, RefSR models outperform existing
single image super-resolution (SISR) models. However, their performance
significantly declines when a domain gap between Ref and LR images exists,
which often occurs in real-world scenarios, such as satellite imaging. In this
letter, we introduce a Domain Matching (DM) module that can be seamlessly
integrated with existing RefSR models to enhance their performance in a
plug-and-play manner. To the best of our knowledge, we are the first to explore
Domain Matching-based RefSR in remote sensing image processing. Our analysis
reveals that their domain gaps often occur in different satellites, and our
model effectively addresses these challenges, whereas existing models struggle.
Our experiments demonstrate that the proposed DM module improves SR performance
both qualitatively and quantitatively for remote sensing super-resolution
tasks.
\\ ( https://arxiv.org/abs/2401.15944 ,  569kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15963 (*cross-listing*)
Date: Mon, 29 Jan 2024 08:47:31 GMT   (1953kb,D)

Title: NoFunEval: Funny How Code LMs Falter on Requirements Beyond Functional
  Correctness
Authors: Manav Singhal, Tushar Aggarwal, Abhijeet Awasthi, Nagarajan Natarajan,
  Aditya Kanade
Categories: cs.SE cs.AI cs.CL cs.LG
Comments: Preprint
\\
  Existing evaluation benchmarks of language models of code (code LMs) focus
almost exclusively on whether the LMs can generate functionally-correct code.
In real-world software engineering, developers think beyond functional
correctness. They have requirements on "how" a functionality should be
implemented to meet overall system design objectives like efficiency, security,
and maintainability. They would also trust the code LMs more if the LMs
demonstrate robust understanding of requirements and code semantics.
  We propose a new benchmark NoFunEval to evaluate code LMs on non-functional
requirements and simple classification instances for both functional and
non-functional requirements. We propose a prompting method, Coding Concepts
(CoCo), as a way for a developer to communicate the domain knowledge to the
LMs. We conduct an extensive evaluation of twenty-two code LMs. Our finding is
that they generally falter when tested on our benchmark, hinting at fundamental
blindspots in their training setups. Surprisingly, even the classification
accuracy on functional-correctness instances derived from the popular HumanEval
benchmark is low, calling in question the depth of their comprehension and the
source of their success in generating functionally-correct code in the first
place. We will release our benchmark and evaluation scripts publicly at
https://aka.ms/NoFunEval.
\\ ( https://arxiv.org/abs/2401.15963 ,  1953kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15969 (*cross-listing*)
Date: Mon, 29 Jan 2024 08:58:07 GMT   (429kb,D)

Title: Routers in Vision Mixture of Experts: An Empirical Study
Authors: Tianlin Liu, Mathieu Blondel, Carlos Riquelme, Joan Puigcerver
Categories: cs.CV cs.AI cs.LG
\\
  Mixture-of-Experts (MoE) models are a promising way to scale up model
capacity without significantly increasing computational cost. A key component
of MoEs is the router, which decides which subset of parameters (experts)
process which feature embeddings (tokens). In this paper, we present a
comprehensive study of routers in MoEs for computer vision tasks. We introduce
a unified MoE formulation that subsumes different MoEs with two parametric
routing tensors. This formulation covers both sparse MoE, which uses a binary
or hard assignment between experts and tokens, and soft MoE, which uses a soft
assignment between experts and weighted combinations of tokens. Routers for
sparse MoEs can be further grouped into two variants: Token Choice, which
matches experts to each token, and Expert Choice, which matches tokens to each
expert. We conduct head-to-head experiments with 6 different routers, including
existing routers from prior work and new ones we introduce. We show that (i)
many routers originally developed for language modeling can be adapted to
perform strongly in vision tasks, (ii) in sparse MoE, Expert Choice routers
generally outperform Token Choice routers, and (iii) soft MoEs generally
outperform sparse MoEs with a fixed compute budget. These results provide new
insights regarding the crucial role of routers in vision MoE models.
\\ ( https://arxiv.org/abs/2401.15969 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15970 (*cross-listing*)
Date: Mon, 29 Jan 2024 08:59:05 GMT   (9645kb,D)

Title: HEQuant: Marrying Homomorphic Encryption and Quantization for
  Communication-Efficient Private Inference
Authors: Tianshi Xu, Meng Li, Runsheng Wang
Categories: cs.CR cs.AI
\\
  Secure two-party computation with homomorphic encryption (HE) protects data
privacy with a formal security guarantee but suffers from high communication
overhead. While previous works, e.g., Cheetah, Iron, etc, have proposed
efficient HE-based protocols for different neural network (NN) operations, they
still assume high precision, e.g., fixed point 37 bit, for the NN operations
and ignore NNs' native robustness against quantization error. In this paper, we
propose HEQuant, which features low-precision-quantization-aware optimization
for the HE-based protocols. We observe the benefit of a naive combination of
quantization and HE quickly saturates as bit precision goes down. Hence, to
further improve communication efficiency, we propose a series of optimizations,
including an intra-coefficient packing algorithm and a quantization-aware
tiling algorithm, to simultaneously reduce the number and precision of the
transferred data. Compared with prior-art HE-based protocols, e.g., CrypTFlow2,
Cheetah, Iron, etc, HEQuant achieves $3.5\sim 23.4\times$ communication
reduction and $3.0\sim 9.3\times$ latency reduction. Meanwhile, when compared
with prior-art network optimization frameworks, e.g., SENet, SNL, etc, HEQuant
also achieves $3.1\sim 3.6\times$ communication reduction.
\\ ( https://arxiv.org/abs/2401.15970 ,  9645kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16013 (*cross-listing*)
Date: Mon, 29 Jan 2024 10:01:10 GMT   (15161kb,D)

Title: SERL: A Software Suite for Sample-Efficient Robotic Reinforcement
  Learning
Authors: Jianlan Luo, Zheyuan Hu, Charles Xu, You Liang Tan, Jacob Berg, Archit
  Sharma, Stefan Schaal, Chelsea Finn, Abhishek Gupta, Sergey Levine
Categories: cs.RO cs.AI
\\
  In recent years, significant progress has been made in the field of robotic
reinforcement learning (RL), enabling methods that handle complex image
observations, train in the real world, and incorporate auxiliary data, such as
demonstrations and prior experience. However, despite these advances, robotic
RL remains hard to use. It is acknowledged among practitioners that the
particular implementation details of these algorithms are often just as
important (if not more so) for performance as the choice of algorithm. We posit
that a significant challenge to widespread adoption of robotic RL, as well as
further development of robotic RL methods, is the comparative inaccessibility
of such methods. To address this challenge, we developed a carefully
implemented library containing a sample efficient off-policy deep RL method,
together with methods for computing rewards and resetting the environment, a
high-quality controller for a widely-adopted robot, and a number of challenging
example tasks. We provide this library as a resource for the community,
describe its design choices, and present experimental results. Perhaps
surprisingly, we find that our implementation can achieve very efficient
learning, acquiring policies for PCB board assembly, cable routing, and object
relocation between 25 to 50 minutes of training per policy on average,
improving over state-of-the-art results reported for similar tasks in the
literature. These policies achieve perfect or near-perfect success rates,
extreme robustness even under perturbations, and exhibit emergent recovery and
correction behaviors. We hope that these promising results and our high-quality
open-source implementation will provide a tool for the robotics community to
facilitate further developments in robotic RL. Our code, documentation, and
videos can be found at https://serl-robot.github.io/
\\ ( https://arxiv.org/abs/2401.16013 ,  15161kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16123 (*cross-listing*)
Date: Mon, 29 Jan 2024 12:48:56 GMT   (14915kb,D)

Title: Looking for a better fit? An Incremental Learning Multimodal Object
  Referencing Framework adapting to Individual Drivers
Authors: Amr Gomaa and Guillermo Reyes and Michael Feld and Antonio Kr\"uger
Categories: cs.HC cs.AI cs.CV cs.LG
Comments: Accepted for publication in the Proceedings of the 29th International
  Conference on Intelligent User Interfaces (IUI'24), March 18--21, 2024, in
  Greenville, SC, USA
\\
  The rapid advancement of the automotive industry towards automated and
semi-automated vehicles has rendered traditional methods of vehicle
interaction, such as touch-based and voice command systems, inadequate for a
widening range of non-driving related tasks, such as referencing objects
outside of the vehicle. Consequently, research has shifted toward gestural
input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of
interaction during driving. However, due to the dynamic nature of driving and
individual variation, there are significant differences in drivers' gestural
input performance. While, in theory, this inherent variability could be
moderated by substantial data-driven machine learning models, prevalent
methodologies lean towards constrained, single-instance trained models for
object referencing. These models show a limited capacity to continuously adapt
to the divergent behaviors of individual drivers and the variety of driving
scenarios. To address this, we propose \textit{IcRegress}, a novel
regression-based incremental learning approach that adapts to changing behavior
and the unique characteristics of drivers engaged in the dual task of driving
and referencing objects. We suggest a more personalized and adaptable solution
for multimodal gestural interfaces, employing continuous lifelong learning to
enhance driver experience, safety, and convenience. Our approach was evaluated
using an outside-the-vehicle object referencing use case, highlighting the
superiority of the incremental learning models adapted over a single trained
model across various driver traits such as handedness, driving experience, and
numerous driving conditions. Finally, to facilitate reproducibility, ease
deployment, and promote further research, we offer our approach as an
open-source framework at \url{https://github.com/amrgomaaelhady/IcRegress}.
\\ ( https://arxiv.org/abs/2401.16123 ,  14915kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16136 (*cross-listing*)
Date: Mon, 29 Jan 2024 13:07:08 GMT   (428kb,D)

Title: Neural Network Training on Encrypted Data with TFHE
Authors: Luis Montero, Jordan Frery, Celia Kherfallah, Roman Bredehoft, Andrei
  Stoian
Categories: cs.CR cs.AI
\\
  We present an approach to outsourcing of training neural networks while
preserving data confidentiality from malicious parties. We use fully
homomorphic encryption to build a unified training approach that works on
encrypted data and learns quantized neural network models. The data can be
horizontally or vertically split between multiple parties, enabling
collaboration on confidential data. We train logistic regression and
multi-layer perceptrons on several datasets.
\\ ( https://arxiv.org/abs/2401.16136 ,  428kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16144 (*cross-listing*)
Date: Mon, 29 Jan 2024 13:23:34 GMT   (11845kb,D)

Title: Divide and Conquer: Rethinking the Training Paradigm of Neural Radiance
  Fields
Authors: Rongkai Ma, Leo Lebrat, Rodrigo Santa Cruz, Gil Avraham, Yan Zuo,
  Clinton Fookes, Olivier Salvado
Categories: cs.CV cs.AI
\\
  Neural radiance fields (NeRFs) have exhibited potential in synthesizing
high-fidelity views of 3D scenes but the standard training paradigm of NeRF
presupposes an equal importance for each image in the training set. This
assumption poses a significant challenge for rendering specific views
presenting intricate geometries, thereby resulting in suboptimal performance.
In this paper, we take a closer look at the implications of the current
training paradigm and redesign this for more superior rendering quality by
NeRFs. Dividing input views into multiple groups based on their visual
similarities and training individual models on each of these groups enables
each model to specialize on specific regions without sacrificing speed or
efficiency. Subsequently, the knowledge of these specialized models is
aggregated into a single entity via a teacher-student distillation paradigm,
enabling spatial efficiency for online render-ing. Empirically, we evaluate our
novel training framework on two publicly available datasets, namely NeRF
synthetic and Tanks&Temples. Our evaluation demonstrates that our DaC training
pipeline enhances the rendering quality of a state-of-the-art baseline model
while exhibiting convergence to a superior minimum.
\\ ( https://arxiv.org/abs/2401.16144 ,  11845kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16157 (*cross-listing*)
Date: Mon, 29 Jan 2024 13:42:01 GMT   (30485kb,D)

Title: Spatial-Aware Latent Initialization for Controllable Image Generation
Authors: Wenqiang Sun, Teng Li, Zehong Lin, Jun Zhang
Categories: cs.CV cs.AI
\\
  Recently, text-to-image diffusion models have demonstrated impressive ability
to generate high-quality images conditioned on the textual input. However,
these models struggle to accurately adhere to textual instructions regarding
spatial layout information. While previous research has primarily focused on
aligning cross-attention maps with layout conditions, they overlook the impact
of the initialization noise on the layout guidance. To achieve better layout
control, we propose leveraging a spatial-aware initialization noise during the
denoising process. Specifically, we find that the inverted reference image with
finite inversion steps contains valuable spatial awareness regarding the
object's position, resulting in similar layouts in the generated images. Based
on this observation, we develop an open-vocabulary framework to customize a
spatial-aware initialization noise for each layout condition. Without modifying
other modules except the initialization noise, our approach can be seamlessly
integrated as a plug-and-play module within other training-free layout guidance
frameworks. We evaluate our approach quantitatively and qualitatively on the
available Stable Diffusion model and COCO dataset. Equipped with the
spatial-aware latent initialization, our method significantly improves the
effectiveness of layout guidance while preserving high-quality content.
\\ ( https://arxiv.org/abs/2401.16157 ,  30485kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16185 (*cross-listing*)
Date: Mon, 29 Jan 2024 14:32:27 GMT   (398kb,D)

Title: LLM4Vuln: A Unified Evaluation Framework for Decoupling and Enhancing
  LLMs' Vulnerability Reasoning
Authors: Yuqiang Sun and Daoyuan Wu and Yue Xue and Han Liu and Wei Ma and
  Lyuye Zhang and Miaolei Shi and Yang Liu
Categories: cs.CR cs.AI cs.SE
Comments: This is a technical report by Nanyang Technological University
\\
  Large language models (LLMs) have demonstrated significant poten- tial for
many downstream tasks, including those requiring human- level intelligence,
such as vulnerability detection. However, recent attempts to use LLMs for
vulnerability detection are still prelim- inary, as they lack an in-depth
understanding of a subject LLM's vulnerability reasoning capability - whether
it originates from the model itself or from external assistance, such as
invoking tool sup- port and retrieving vulnerability knowledge. In this paper,
we aim to decouple LLMs' vulnerability reason- ing capability from their other
capabilities, including the ability to actively seek additional information
(e.g., via function calling in SOTA models), adopt relevant vulnerability
knowledge (e.g., via vector-based matching and retrieval), and follow
instructions to out- put structured results. To this end, we propose a unified
evaluation framework named LLM4Vuln, which separates LLMs' vulnerability
reasoning from their other capabilities and evaluates how LLMs' vulnerability
reasoning could be enhanced when combined with the enhancement of other
capabilities. To demonstrate the effectiveness of LLM4Vuln, we have designed
controlled experiments using 75 ground-truth smart contract vulnerabilities,
which were extensively audited as high-risk on Code4rena from August to
November 2023, and tested them in 4,950 different scenarios across three
represen- tative LLMs (GPT-4, Mixtral, and Code Llama). Our results not only
reveal ten findings regarding the varying effects of knowledge en- hancement,
context supplementation, prompt schemes, and models but also enable us to
identify 9 zero-day vulnerabilities in two pilot bug bounty programs with over
1,000 USD being awarded.
\\ ( https://arxiv.org/abs/2401.16185 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16186 (*cross-listing*)
Date: Mon, 29 Jan 2024 14:32:32 GMT   (1603kb,D)

Title: An Empirical Study on Usage and Perceptions of LLMs in a Software
  Engineering Project
Authors: Sanka Rasnayaka, Guanlin Wang, Ridwan Shariffdeen, Ganesh Neelakanta
  Iyer
Categories: cs.SE cs.AI
Comments: 8 pages, 6 figures, accepted for publication at the LLM4Code workshop
  @ ICSE 2024
ACM-class: D.2.3
\\
  Large Language Models (LLMs) represent a leap in artificial intelligence,
excelling in tasks using human language(s). Although the main focus of
general-purpose LLMs is not code generation, they have shown promising results
in the domain. However, the usefulness of LLMs in an academic software
engineering project has not been fully explored yet. In this study, we explore
the usefulness of LLMs for 214 students working in teams consisting of up to
six members. Notably, in the academic course through which this study is
conducted, students were encouraged to integrate LLMs into their development
tool-chain, in contrast to most other academic courses that explicitly prohibit
the use of LLMs.
  In this paper, we analyze the AI-generated code, prompts used for code
generation, and the human intervention levels to integrate the code into the
code base. We also conduct a perception study to gain insights into the
perceived usefulness, influencing factors, and future outlook of LLM from a
computer science student's perspective. Our findings suggest that LLMs can play
a crucial role in the early stages of software development, especially in
generating foundational code structures, and helping with syntax and error
debugging. These insights provide us with a framework on how to effectively
utilize LLMs as a tool to enhance the productivity of software engineering
students, and highlight the necessity of shifting the educational focus toward
preparing students for successful human-AI collaboration.
\\ ( https://arxiv.org/abs/2401.16186 ,  1603kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16190 (*cross-listing*)
Date: Mon, 29 Jan 2024 14:42:06 GMT   (2486kb)

Title: AI prediction of cardiovascular events using opportunistic epicardial
  adipose tissue assessments from CT calcium score
Authors: Tao Hu, Joshua Freeze, Prerna Singh, Justin Kim, Yingnan Song, Hao Wu,
  Juhwan Lee, Sadeer Al-Kindi, Sanjay Rajagopalan, David L. Wilson, Ammar Hoori
Categories: q-bio.QM cs.AI
Comments: 7 pages, 1 central illustration, 6 figures, 5 tables
\\
  Background: Recent studies have used basic epicardial adipose tissue (EAT)
assessments (e.g., volume and mean HU) to predict risk of
atherosclerosis-related, major adverse cardiovascular events (MACE).
Objectives: Create novel, hand-crafted EAT features, 'fat-omics', to capture
the pathophysiology of EAT and improve MACE prediction. Methods: We segmented
EAT using a previously-validated deep learning method with optional manual
correction. We extracted 148 radiomic features (morphological, spatial, and
intensity) and used Cox elastic-net for feature reduction and prediction of
MACE. Results: Traditional fat features gave marginal prediction
(EAT-volume/EAT-mean-HU/ BMI gave C-index 0.53/0.55/0.57, respectively).
Significant improvement was obtained with 15 fat-omics features (C-index=0.69,
test set). High-risk features included
volume-of-voxels-having-elevated-HU-[-50, -30-HU] and HU-negative-skewness,
both of which assess high HU, which as been implicated in fat inflammation.
Other high-risk features include kurtosis-of-EAT-thickness, reflecting the
heterogeneity of thicknesses, and EAT-volume-in-the-top-25%-of-the-heart,
emphasizing adipose near the proximal coronary arteries. Kaplan-Meyer plots of
Cox-identified, high- and low-risk patients were well separated with the median
of the fat-omics risk, while high-risk group having HR 2.4 times that of the
low-risk group (P<0.001). Conclusion: Preliminary findings indicate an
opportunity to use more finely tuned, explainable assessments on EAT for
improved cardiovascular risk prediction.
\\ ( https://arxiv.org/abs/2401.16190 ,  2486kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16198 (*cross-listing*)
Date: Mon, 29 Jan 2024 14:53:22 GMT   (475kb,D)

Title: Contracting with a Learning Agent
Authors: Guru Guruganesh, Yoav Kolumbus, Jon Schneider, Inbal Talgam-Cohen,
  Emmanouil-Vasileios Vlatakis-Gkaragkounis, Joshua R. Wang, S. Matthew
  Weinberg
Categories: cs.GT cs.AI cs.LG econ.TH
\\
  Many real-life contractual relations differ completely from the clean, static
model at the heart of principal-agent theory. Typically, they involve repeated
strategic interactions of the principal and agent, taking place under
uncertainty and over time. While appealing in theory, players seldom use
complex dynamic strategies in practice, often preferring to circumvent
complexity and approach uncertainty through learning. We initiate the study of
repeated contracts with a learning agent, focusing on agents who achieve
no-regret outcomes.
  Optimizing against a no-regret agent is a known open problem in general
games; we achieve an optimal solution to this problem for a canonical contract
setting, in which the agent's choice among multiple actions leads to
success/failure. The solution has a surprisingly simple structure: for some
$\alpha > 0$, initially offer the agent a linear contract with scalar $\alpha$,
then switch to offering a linear contract with scalar $0$. This switch causes
the agent to ``free-fall'' through their action space and during this time
provides the principal with non-zero reward at zero cost. Despite apparent
exploitation of the agent, this dynamic contract can leave \emph{both} players
better off compared to the best static contract. Our results generalize beyond
success/failure, to arbitrary non-linear contracts which the principal rescales
dynamically.
  Finally, we quantify the dependence of our results on knowledge of the time
horizon, and are the first to address this consideration in the study of
strategizing against learning agents.
\\ ( https://arxiv.org/abs/2401.16198 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16251 (*cross-listing*)
Date: Mon, 29 Jan 2024 16:01:46 GMT   (2892kb,D)

Title: Cross-silo Federated Learning with Record-level Personalized
  Differential Privacy
Authors: Junxu Liu, Jian Lou, Li Xiong, Jinfei Liu, Xiaofeng Meng
Categories: cs.CR cs.AI cs.LG
Comments: 12 pages, 7 figures, under review
\\
  Federated learning enhanced by differential privacy has emerged as a popular
approach to better safeguard the privacy of client-side data by protecting
clients' contributions during the training process. Existing solutions
typically assume a uniform privacy budget for all records and provide
one-size-fits-all solutions that may not be adequate to meet each record's
privacy requirement. In this paper, we explore the uncharted territory of
cross-silo FL with record-level personalized differential privacy. We devise a
novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme
with both client-level sampling and non-uniform record-level sampling to
accommodate varying privacy requirements. A critical and non-trivial problem is
to select the ideal per-record sampling probability q given the personalized
privacy budget {\epsilon}. We introduce a versatile solution named
Simulation-CurveFitting, allowing us to uncover a significant insight into the
nonlinear correlation between q and {\epsilon} and derive an elegant
mathematical model to tackle the problem. Our evaluation demonstrates that our
solution can provide significant performance gains over the baselines that do
not consider personalized privacy preservation.
\\ ( https://arxiv.org/abs/2401.16251 ,  2892kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16298 (*cross-listing*)
Date: Mon, 29 Jan 2024 16:59:39 GMT   (923kb,D)

Title: Breaking the Barrier: Selective Uncertainty-based Active Learning for
  Medical Image Segmentation
Authors: Siteng Ma, Haochang Wu, Aonghus Lawlor, Ruihai Dong
Categories: cs.CV cs.AI
\\
  Active learning (AL) has found wide applications in medical image
segmentation, aiming to alleviate the annotation workload and enhance
performance. Conventional uncertainty-based AL methods, such as entropy and
Bayesian, often rely on an aggregate of all pixel-level metrics. However, in
imbalanced settings, these methods tend to neglect the significance of target
regions, eg., lesions, and tumors. Moreover, uncertainty-based selection
introduces redundancy. These factors lead to unsatisfactory performance, and in
many cases, even underperform random sampling. To solve this problem, we
introduce a novel approach called the Selective Uncertainty-based AL, avoiding
the conventional practice of summing up the metrics of all pixels. Through a
filtering process, our strategy prioritizes pixels within target areas and
those near decision boundaries. This resolves the aforementioned disregard for
target areas and redundancy. Our method showed substantial improvements across
five different uncertainty-based methods and two distinct datasets, utilizing
fewer labeled data to reach the supervised baseline and consistently achieving
the highest overall performance. Our code is available at
https://github.com/HelenMa9998/Selective\_Uncertainty\_AL.
\\ ( https://arxiv.org/abs/2401.16298 ,  923kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16310 (*cross-listing*)
Date: Mon, 29 Jan 2024 17:13:44 GMT   (153kb,D)

Title: Security Code Review by LLMs: A Deep Dive into Responses
Authors: Jiaxin Yu, Peng Liang, Yujia Fu, Amjed Tahir, Mojtaba Shahin, Chong
  Wang, Yangxiao Cai
Categories: cs.SE cs.AI
\\
  Security code review aims to combine automated tools and manual efforts to
detect security defects during development. The rapid development of Large
Language Models (LLMs) has shown promising potential in software development,
as well as opening up new possibilities in automated security code review. To
explore the challenges of applying LLMs in practical code review for security
defect detection, this study compared the detection performance of three
state-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on
549 code files that contain security defects from real-world code reviews.
Through analyzing 82 responses generated by the best-performing LLM-prompt
combination based on 100 randomly selected code files, we extracted and
categorized quality problems present in these responses into 5 themes and 16
categories. Our results indicate that the responses produced by LLMs often
suffer from verbosity, vagueness, and incompleteness, highlighting the
necessity to enhance their conciseness, understandability, and compliance to
security defect detection. This work reveals the deficiencies of LLM-generated
responses in security code review and paves the way for future optimization of
LLMs towards this task.
\\ ( https://arxiv.org/abs/2401.16310 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16352 (*cross-listing*)
Date: Mon, 29 Jan 2024 17:56:42 GMT   (1595kb,D)

Title: Adversarial Training on Purification (AToP): Advancing Both Robustness
  and Generalization
Authors: Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao
Categories: cs.CV cs.AI
\\
  The deep neural networks are known to be vulnerable to well-designed
adversarial attacks. The most successful defense technique based on adversarial
training (AT) can achieve optimal robustness against particular attacks but
cannot generalize well to unseen attacks. Another effective defense technique
based on adversarial purification (AP) can enhance generalization but cannot
achieve optimal robustness. Meanwhile, both methods share one common limitation
on the degraded standard accuracy. To mitigate these issues, we propose a novel
framework called Adversarial Training on Purification (AToP), which comprises
two components: perturbation destruction by random transforms (RT) and purifier
model fine-tuned (FT) by adversarial loss. RT is essential to avoid
overlearning to known attacks resulting in the robustness generalization to
unseen attacks and FT is essential for the improvement of robustness. To
evaluate our method in an efficient and scalable way, we conduct extensive
experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our
method achieves state-of-the-art results and exhibits generalization ability
against unseen attacks.
\\ ( https://arxiv.org/abs/2401.16352 ,  1595kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16402 (*cross-listing*)
Date: Mon, 29 Jan 2024 18:41:21 GMT   (753kb,D)

Title: A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect
Authors: Yunkang Cao, Xiaohao Xu, Jiangning Zhang, Yuqi Cheng, Xiaonan Huang,
  Guansong Pang, Weiming Shen
Categories: cs.CV cs.AI
Comments: Work in progress. Yunkang Cao, Xiaohao Xu, and Jiangning Zhang
  contribute equally to this work
\\
  Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the
concept of normality in visual data, widely applied across diverse domains,
e.g., industrial defect inspection, and medical lesion detection. This survey
comprehensively examines recent advancements in VAD by identifying three
primary challenges: 1) scarcity of training data, 2) diversity of visual
modalities, and 3) complexity of hierarchical anomalies. Starting with a brief
overview of the VAD background and its generic concept definitions, we
progressively categorize, emphasize, and discuss the latest VAD progress from
the perspective of sample number, data modality, and anomaly hierarchy. Through
an in-depth analysis of the VAD field, we finally summarize future developments
for VAD and conclude the key findings and contributions of this survey.
\\ ( https://arxiv.org/abs/2401.16402 ,  753kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15479 (*cross-listing*)
Date: Sat, 27 Jan 2024 19:04:30 GMT   (3565kb,D)

Title: Navigating the Post-API Dilemma Search Engine Results Pages Present a
  Biased View of Social Media Data
Authors: Amrit Poudel, Tim Weninger
Categories: cs.IR cs.CL cs.SI
\\
  Recent decisions to discontinue access to social media APIs are having
detrimental effects on Internet research and the field of computational social
science as a whole. This lack of access to data has been dubbed the Post-API
era of Internet research. Fortunately, popular search engines have the means to
crawl, capture, and surface social media data on their Search Engine Results
Pages (SERP) if provided the proper search query, and may provide a solution to
this dilemma. In the present work we ask: does SERP provide a complete and
unbiased sample of social media data? Is SERP a viable alternative to direct
API-access? To answer these questions, we perform a comparative analysis
between (Google) SERP results and nonsampled data from Reddit and Twitter/X. We
find that SERP results are highly biased in favor of popular posts; against
political, pornographic, and vulgar posts; are more positive in their
sentiment; and have large topical gaps. Overall, we conclude that SERP is not a
viable alternative to social media API access.
\\ ( https://arxiv.org/abs/2401.15479 ,  3565kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15560 (*cross-listing*)
Date: Sun, 28 Jan 2024 03:54:41 GMT   (827kb)

Title: An Analysis of Letter Dynamics in the English Alphabet
Authors: Neil Zhao, Diana Zheng
Categories: cs.IT cs.CL math.IT
Comments: 22 pages, 6 figures, 5 tables
MSC-class: 94A15
\\
  The frequency with which the letters of the English alphabet appear in
writings has been applied to the field of cryptography, the development of
keyboard mechanics, and the study of linguistics. We expanded on the
statistical analysis of the English alphabet by examining the average frequency
which each letter appears in different categories of writings. We evaluated
news articles, novels, plays, scientific publications and calculated the
frequency of each letter of the alphabet, the information density of each
letter, and the overall letter distribution. Furthermore, we developed a metric
known as distance, d that can be used to algorithmically recognize different
categories of writings. The results of our study can be applied to information
transmission, large data curation, and linguistics.
\\ ( https://arxiv.org/abs/2401.15560 ,  827kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15635 (*cross-listing*)
Date: Sun, 28 Jan 2024 11:51:09 GMT   (308kb,D)

Title: RecDCL: Dual Contrastive Learning for Recommendation
Authors: Dan Zhang and Yangliao Geng and Wenwen Gong and Zhongang Qi and Zhiyu
  Chen and Xing Tang and Ying Shan and Yuxiao Dong and Jie Tang
Categories: cs.IR cs.CL
Comments: Accepted to WWW 2024
Journal-ref: Proceedings of TheWebConf 2024 (WWW '24), May 13--17, 2024,
  Singapore
\\
  Self-supervised recommendation (SSR) has achieved great success in mining the
potential interacted behaviors for collaborative filtering in recent years. As
a major branch, Contrastive Learning (CL) based SSR conquers data sparsity in
Web platforms by contrasting the embedding between raw data and augmented data.
However, existing CL-based SSR methods mostly focus on contrasting in a
batch-wise way, failing to exploit potential regularity in the feature-wise
dimension, leading to redundant solutions during the representation learning
process of users (items) from Websites. Furthermore, the joint benefits of
utilizing both Batch-wise CL (BCL) and Feature-wise CL (FCL) for
recommendations remain underexplored. To address these issues, we investigate
the relationship of objectives between BCL and FCL. Our study suggests a
cooperative benefit of employing both methods, as evidenced from theoretical
and experimental perspectives. Based on these insights, we propose a dual CL
method for recommendation, referred to as RecDCL. RecDCL first eliminates
redundant solutions on user-item positive pairs in a feature-wise manner. It
then optimizes the uniform distributions within users and items using a
polynomial kernel from an FCL perspective. Finally, it generates contrastive
embedding on output vectors in a batch-wise objective. We conduct experiments
on four widely-used benchmarks and an industrial dataset. The results
consistently demonstrate that the proposed RecDCL outperforms the
state-of-the-art GNNs-based and SSL-based models (with up to a 5.65\%
improvement in terms of Recall@20), thereby confirming the effectiveness of the
joint-wise objective. All source codes used in this paper are publicly
available at \url{https://github.com/THUDM/RecDCL}}.
\\ ( https://arxiv.org/abs/2401.15635 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15641 (*cross-listing*)
Date: Sun, 28 Jan 2024 12:33:14 GMT   (978kb,D)

Title: PRE: A Peer Review Based Large Language Model Evaluator
Authors: Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, Yiqun Liu
Categories: cs.IR cs.CL
Comments: 11 pages
\\
  The impressive performance of large language models (LLMs) has attracted
considerable attention from the academic and industrial communities. Besides
how to construct and train LLMs, how to effectively evaluate and compare the
capacity of LLMs has also been well recognized as an important yet difficult
problem. Existing paradigms rely on either human annotators or model-based
evaluators to evaluate the performance of LLMs on different tasks. However,
these paradigms often suffer from high cost, low generalizability, and
inherited biases in practice, which make them incapable of supporting the
sustainable development of LLMs in long term. In order to address these issues,
inspired by the peer review systems widely used in academic publication
process, we propose a novel framework that can automatically evaluate LLMs
through a peer-review process. Specifically, for the evaluation of a specific
task, we first construct a small qualification exam to select "reviewers" from
a couple of powerful LLMs. Then, to actually evaluate the "submissions" written
by different candidate LLMs, i.e., the evaluatees, we use the reviewer LLMs to
rate or compare the submissions. The final ranking of evaluatee LLMs is
generated based on the results provided by all reviewers. We conducted
extensive experiments on text summarization tasks with eleven LLMs including
GPT-4. The results demonstrate the existence of biasness when evaluating using
a single LLM. Also, our PRE model outperforms all the baselines, illustrating
the effectiveness of the peer review mechanism.
\\ ( https://arxiv.org/abs/2401.15641 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16167 (*cross-listing*)
Date: Mon, 29 Jan 2024 13:54:48 GMT   (98kb,D)

Title: "You tell me": A Dataset of GPT-4-Based Behaviour Change Support
  Conversations
Authors: Selina Meyer and David Elsweiler
Categories: cs.HC cs.CL
Comments: Preprint as accepted at the 2024 ACM SIGIR Conference on Human
  Information Interaction and Retrieval (CHIIR '24)
\\
  Conversational agents are increasingly used to address emotional needs on top
of information needs. One use case of increasing interest are counselling-style
mental health and behaviour change interventions, with large language model
(LLM)-based approaches becoming more popular. Research in this context so far
has been largely system-focused, foregoing the aspect of user behaviour and the
impact this can have on LLM-generated texts. To address this issue, we share a
dataset containing text-based user interactions related to behaviour change
with two GPT-4-based conversational agents collected in a preregistered user
study. This dataset includes conversation data, user language analysis,
perception measures, and user feedback for LLM-generated turns, and can offer
valuable insights to inform the design of such systems based on real
interactions.
\\ ( https://arxiv.org/abs/2401.16167 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16212 (*cross-listing*)
Date: Wed, 24 Jan 2024 03:53:28 GMT   (695kb,D)

Title: Better Call GPT, Comparing Large Language Models Against Lawyers
Authors: Lauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson,
  Rivindu Perera (Onit AI Centre of Excellence)
Categories: cs.CY cs.CL
Comments: 16 pages
\\
  This paper presents a groundbreaking comparison between Large Language Models
and traditional legal contract reviewers, Junior Lawyers and Legal Process
Outsourcers. We dissect whether LLMs can outperform humans in accuracy, speed,
and cost efficiency during contract review. Our empirical analysis benchmarks
LLMs against a ground truth set by Senior Lawyers, uncovering that advanced
models match or exceed human accuracy in determining legal issues. In speed,
LLMs complete reviews in mere seconds, eclipsing the hours required by their
human counterparts. Cost wise, LLMs operate at a fraction of the price,
offering a staggering 99.97 percent reduction in cost over traditional methods.
These results are not just statistics, they signal a seismic shift in legal
practice. LLMs stand poised to disrupt the legal industry, enhancing
accessibility and efficiency of legal services. Our research asserts that the
era of LLM dominance in legal contract review is upon us, challenging the
status quo and calling for a reimagined future of legal workflows.
\\ ( https://arxiv.org/abs/2401.16212 ,  695kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16420 (*cross-listing*)
Date: Mon, 29 Jan 2024 18:59:02 GMT   (6351kb,D)

Title: InternLM-XComposer2: Mastering Free-form Text-Image Composition and
  Comprehension in Vision-Language Large Model
Authors: Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke
  Ouyang, Xilin Wei, Songyang Zhang, Haodong Duan, Maosong Cao, Wenwei Zhang,
  Yining Li, Hang Yan, Yang Gao, Xinyue Zhang, Wei Li, Jingwen Li, Kai Chen,
  Conghui He, Xingcheng Zhang, Yu Qiao, Dahua Lin, Jiaqi Wang
Categories: cs.CV cs.CL
Comments: Code and models are available at
  https://github.com/InternLM/InternLM-XComposer
\\
  We introduce InternLM-XComposer2, a cutting-edge vision-language model
excelling in free-form text-image composition and comprehension. This model
goes beyond conventional vision-language understanding, adeptly crafting
interleaved text-image content from diverse inputs like outlines, detailed
textual specifications, and reference images, enabling highly customizable
content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach
that applies additional LoRA parameters exclusively to image tokens to preserve
the integrity of pre-trained language knowledge, striking a balance between
precise vision understanding and text composition with literary talent.
Experimental results demonstrate the superiority of InternLM-XComposer2 based
on InternLM2-7B in producing high-quality long-text multi-modal content and its
exceptional vision-language understanding performance across various
benchmarks, where it not only significantly outperforms existing multimodal
models but also matches or even surpasses GPT-4V and Gemini Pro in certain
assessments. This highlights its remarkable proficiency in the realm of
multimodal understanding. The InternLM-XComposer2 model series with 7B
parameters are publicly available at
https://github.com/InternLM/InternLM-XComposer.
\\ ( https://arxiv.org/abs/2401.16420 ,  6351kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15088 (*cross-listing*)
Date: Mon, 22 Jan 2024 08:06:04 GMT   (771kb)

Title: Design & Implementation of Automatic Machine Condition Monitoring and
  Maintenance System in Limited Resource Situations
Authors: Abu Hanif Md. Ripon, Muhammad Ahsan Ullah, Arindam Kumar Paul, Md.
  Mortaza Morshed
Categories: eess.SY cs.IT cs.LG cs.SY math.IT
Comments: Under Peer Review, Journal: Heliyon, Section: Engineering, Page: 26
\\
  In the era of the fourth industrial revolution, it is essential to automate
fault detection and diagnosis of machineries so that a warning system can be
developed that will help to take an appropriate action before any catastrophic
damage. Some machines health monitoring systems are used globally but they are
expensive and need trained personnel to operate and analyse. Predictive
maintenance and occupational health and safety culture are not available due to
inadequate infrastructure, lack of skilled manpower, financial crisis, and
others in developing countries. Starting from developing a cost-effective DAS
for collecting fault data in this study, the effect of limited data and
resources has been investigated while automating the process. To solve this
problem, A feature engineering and data reduction method has been developed
combining the concepts from wavelets, differential calculus, and signal
processing. Finally, for automating the whole process, all the necessary
theoretical and practical considerations to develop a predictive model have
been proposed. The DAS successfully collected the required data from the
machine that is 89% accurate compared to the professional manual monitoring
system. SVM and NN were proposed for the prediction purpose because of their
high predicting accuracy greater than 95% during training and 100% during
testing the new samples. In this study, the combination of the simple algorithm
with a rule-based system instead of a data-intensive system turned out to be
hybridization by validating with collected data. The outcome of this research
can be instantly applied to small and medium-sized industries for finding other
issues and developing accordingly. As one of the foundational studies in
automatic FDD, the findings and procedure of this study can lead others to
extend, generalize, or add other dimensions to FDD automation.
\\ ( https://arxiv.org/abs/2401.15088 ,  771kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15092 (*cross-listing*)
Date: Mon, 22 Jan 2024 20:15:12 GMT   (8kb)

Title: A note on the capacity of the binary perceptron
Authors: Dylan J. Altschuler and Konstantin Tikhomirov
Categories: math.PR cs.DM cs.LG stat.ML
\\
  Determining the capacity $\alpha_c$ of the Binary Perceptron is a
long-standing problem. Krauth and Mezard (1989) conjectured an explicit value
of $\alpha_c$, approximately equal to .833, and a rigorous lower bound matching
this prediction was recently established by Ding and Sun (2019). Regarding the
upper bound, Kim and Roche (1998) and Talagrand (1999) independently showed
that $\alpha_c$ < .996, while Krauth and Mezard outlined an argument which can
be used to show that $\alpha_c$ < .847. The purpose of this expository note is
to record a complete proof of the bound $\alpha_c$ < .847. The proof is a
conditional first moment method combined with known results on the spherical
perceptron
\\ ( https://arxiv.org/abs/2401.15092 ,  8kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15107 (*cross-listing*)
Date: Thu, 25 Jan 2024 16:26:44 GMT   (1123kb,D)

Title: Optimal Potential Shaping on SE(3) via Neural ODEs on Lie Groups
Authors: Yannik P. Wotte, Federico Califano, Stefano Stramigioli
Categories: math.OC cs.LG cs.RO
Comments: Submitted to the International Journal of Robotics Research (IJRR).
  20 pages, 10 figures
\\
  This work presents a novel approach for the optimization of dynamic systems
on finite-dimensional Lie groups. We rephrase dynamic systems as so-called
neural ordinary differential equations (neural ODEs), and formulate the
optimization problem on Lie groups. A gradient descent optimization algorithm
is presented to tackle the optimization numerically. Our algorithm is scalable,
and applicable to any finite dimensional Lie group, including matrix Lie
groups. By representing the system at the Lie algebra level, we reduce the
computational cost of the gradient computation. In an extensive example,
optimal potential energy shaping for control of a rigid body is treated. The
optimal control problem is phrased as an optimization of a neural ODE on the
Lie group SE(3), and the controller is iteratively optimized. The final
controller is validated on a state-regulation task.
\\ ( https://arxiv.org/abs/2401.15107 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15111 (*cross-listing*)
Date: Thu, 25 Jan 2024 20:03:57 GMT   (1389kb,D)

Title: Improving Fairness of Automated Chest X-ray Diagnosis by Contrastive
  Learning
Authors: Mingquan Lin, Tianhao Li, Zhaoyi Sun, Gregory Holste, Ying Ding, Fei
  Wang, George Shih, Yifan Peng
Categories: eess.IV cs.CV cs.LG
Comments: 23 pages, 5 figures
MSC-class: arms.org
\\
  Purpose: Limited studies exploring concrete methods or approaches to tackle
and enhance model fairness in the radiology domain. Our proposed AI model
utilizes supervised contrastive learning to minimize bias in CXR diagnosis.
  Materials and Methods: In this retrospective study, we evaluated our proposed
method on two datasets: the Medical Imaging and Data Resource Center (MIDRC)
dataset with 77,887 CXR images from 27,796 patients collected as of April 20,
2023 for COVID-19 diagnosis, and the NIH Chest X-ray (NIH-CXR) dataset with
112,120 CXR images from 30,805 patients collected between 1992 and 2015. In the
NIH-CXR dataset, thoracic abnormalities include atelectasis, cardiomegaly,
effusion, infiltration, mass, nodule, pneumonia, pneumothorax, consolidation,
edema, emphysema, fibrosis, pleural thickening, or hernia. Our proposed method
utilizes supervised contrastive learning with carefully selected positive and
negative samples to generate fair image embeddings, which are fine-tuned for
subsequent tasks to reduce bias in chest X-ray (CXR) diagnosis. We evaluated
the methods using the marginal AUC difference ($\delta$ mAUC).
  Results: The proposed model showed a significant decrease in bias across all
subgroups when compared to the baseline models, as evidenced by a paired T-test
(p<0.0001). The $\delta$ mAUC obtained by our method were 0.0116 (95\% CI,
0.0110-0.0123), 0.2102 (95% CI, 0.2087-0.2118), and 0.1000 (95\% CI,
0.0988-0.1011) for sex, race, and age on MIDRC, and 0.0090 (95\% CI,
0.0082-0.0097) for sex and 0.0512 (95% CI, 0.0512-0.0532) for age on NIH-CXR,
respectively.
  Conclusion: Employing supervised contrastive learning can mitigate bias in
CXR diagnosis, addressing concerns of fairness and reliability in deep
learning-based diagnostic methods.
\\ ( https://arxiv.org/abs/2401.15111 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15113 (*cross-listing*)
Date: Thu, 25 Jan 2024 20:41:17 GMT   (30278kb,D)

Title: Towards Global Glacier Mapping with Deep Learning and Open Earth
  Observation Data
Authors: Konstantin A. Maslov and Claudio Persello and Thomas Schellenberger
  and Alfred Stein
Categories: cs.CV cs.LG
\\
  Accurate global glacier mapping is critical for understanding climate change
impacts. It is challenged by glacier diversity, difficult-to-classify debris
and big data processing. Here we propose Glacier-VisionTransformer-U-Net
(GlaViTU), a convolutional-transformer deep learning model, and five strategies
for multitemporal global-scale glacier mapping using open satellite imagery.
Assessing the spatial, temporal and cross-sensor generalisation shows that our
best strategy achieves intersection over union >0.85 on previously unobserved
images in most cases, which drops to >0.75 for debris-rich areas such as
High-Mountain Asia and increases to >0.90 for regions dominated by clean ice.
Additionally, adding synthetic aperture radar data, namely, backscatter and
interferometric coherence, increases the accuracy in all regions where
available. The calibrated confidence for glacier extents is reported making the
predictions more reliable and interpretable. We also release a benchmark
dataset that covers 9% of glaciers worldwide. Our results support efforts
towards automated multitemporal and global glacier mapping.
\\ ( https://arxiv.org/abs/2401.15113 ,  30278kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15116 (*cross-listing*)
Date: Thu, 25 Jan 2024 22:38:03 GMT   (875kb,D)

Title: Efficient Online Crowdsourcing with Complex Annotations
Authors: Reshef Meir, Viet-An Nguyen, Xu Chen, Jagdish Ramakrishnan, Udi
  Weinsberg
Categories: cs.HC cs.LG
Comments: full version of a paper accepted to AAAI'24
\\
  Crowdsourcing platforms use various truth discovery algorithms to aggregate
annotations from multiple labelers. In an online setting, however, the main
challenge is to decide whether to ask for more annotations for each item to
efficiently trade off cost (i.e., the number of annotations) for quality of the
aggregated annotations. In this paper, we propose a novel approach for general
complex annotation (such as bounding boxes and taxonomy paths), that works in
an online crowdsourcing setting. We prove that the expected average similarity
of a labeler is linear in their accuracy \emph{conditional on the reported
label}. This enables us to infer reported label accuracy in a broad range of
scenarios. We conduct extensive evaluations on real-world crowdsourcing data
from Meta and show the effectiveness of our proposed online algorithms in
improving the cost-quality trade-off.
\\ ( https://arxiv.org/abs/2401.15116 ,  875kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15127 (*cross-listing*)
Date: Fri, 26 Jan 2024 13:15:24 GMT   (683kb,D)

Title: Evaluation of LLM Chatbots for OSINT-based Cyberthreat Awareness
Authors: Samaneh Shafee, Alysson Bessani, Pedro M. Ferreira
Categories: cs.CR cs.LG
\\
  Knowledge sharing about emerging threats is crucial in the rapidly advancing
field of cybersecurity and forms the foundation of Cyber Threat Intelligence.
In this context, Large Language Models are becoming increasingly significant in
the field of cybersecurity, presenting a wide range of opportunities. This
study explores the capability of chatbots such as ChatGPT, GPT4all,
Dolly,Stanford Alpaca, Alpaca-LoRA, and Falcon to identify
cybersecurity-related text within Open Source Intelligence. We assess the
capabilities of existing chatbot models for Natural Language Processing tasks.
We consider binary classification and Named Entity Recognition as tasks. This
study analyzes well-established data collected from Twitter, derived from
previous research efforts. Regarding cybersecurity binary classification,
Chatbot GPT-4 as a commercial model achieved an acceptable F1-score of 0.94,
and the open-source GPT4all model achieved an F1-score of 0.90. However,
concerning cybersecurity entity recognition, chatbot models have limitations
and are less effective. This study demonstrates the capability of these
chatbots only for specific tasks, such as cybersecurity binary classification,
while highlighting the need for further refinement in other tasks, such as
Named Entity Recognition tasks.
\\ ( https://arxiv.org/abs/2401.15127 ,  683kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15139 (*cross-listing*)
Date: Fri, 26 Jan 2024 18:29:30 GMT   (224kb,D)

Title: FDR-Controlled Portfolio Optimization for Sparse Financial Index
  Tracking
Authors: Jasin Machkour, Daniel P. Palomar, Michael Muma
Categories: q-fin.PM cs.LG stat.ME stat.ML
\\
  In high-dimensional data analysis, such as financial index tracking or
biomedical applications, it is crucial to select the few relevant variables
while maintaining control over the false discovery rate (FDR). In these
applications, strong dependencies often exist among the variables (e.g., stock
returns), which can undermine the FDR control property of existing methods like
the model-X knockoff method or the T-Rex selector. To address this issue, we
have expanded the T-Rex framework to accommodate overlapping groups of highly
correlated variables. This is achieved by integrating a nearest neighbors
penalization mechanism into the framework, which provably controls the FDR at
the user-defined target level. A real-world example of sparse index tracking
demonstrates the proposed method's ability to accurately track the S&P 500
index over the past 20 years based on a small number of stocks. An open-source
implementation is provided within the R package TRexSelector on CRAN.
\\ ( https://arxiv.org/abs/2401.15139 ,  224kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15164 (*cross-listing*)
Date: Fri, 26 Jan 2024 19:17:05 GMT   (3156kb,D)

Title: AMuSE: Adaptive Multimodal Analysis for Speaker Emotion Recognition in
  Group Conversations
Authors: Naresh Kumar Devulapally, Sidharth Anand, Sreyasee Das Bhattacharjee,
  Junsong Yuan, Yu-Ping Chang
Categories: cs.SD cs.CV cs.LG cs.MM eess.AS
\\
  Analyzing individual emotions during group conversation is crucial in
developing intelligent agents capable of natural human-machine interaction.
While reliable emotion recognition techniques depend on different modalities
(text, audio, video), the inherent heterogeneity between these modalities and
the dynamic cross-modal interactions influenced by an individual's unique
behavioral patterns make the task of emotion recognition very challenging. This
difficulty is compounded in group settings, where the emotion and its temporal
evolution are not only influenced by the individual but also by external
contexts like audience reaction and context of the ongoing conversation. To
meet this challenge, we propose a Multimodal Attention Network that captures
cross-modal interactions at various levels of spatial abstraction by jointly
learning its interactive bunch of mode-specific Peripheral and Central
networks. The proposed MAN injects cross-modal attention via its Peripheral
key-value pairs within each layer of a mode-specific Central query network. The
resulting cross-attended mode-specific descriptors are then combined using an
Adaptive Fusion technique that enables the model to integrate the
discriminative and complementary mode-specific data patterns within an
instance-specific multimodal descriptor. Given a dialogue represented by a
sequence of utterances, the proposed AMuSE model condenses both spatial and
temporal features into two dense descriptors: speaker-level and
utterance-level. This helps not only in delivering better classification
performance (3-5% improvement in Weighted-F1 and 5-7% improvement in Accuracy)
in large-scale public datasets but also helps the users in understanding the
reasoning behind each emotion prediction made by the model via its Multimodal
Explainability Visualization module.
\\ ( https://arxiv.org/abs/2401.15164 ,  3156kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15223 (*cross-listing*)
Date: Fri, 26 Jan 2024 22:21:39 GMT   (2826kb,D)

Title: Biological Valuation Map of Flanders: A Sentinel-2 Imagery Analysis
Authors: Mingshi Li, Dusan Grujicic, Steven De Saeger, Stien Heremans, Ben
  Somers, Matthew B. Blaschko
Categories: cs.CV cs.LG
\\
  In recent years, machine learning has become crucial in remote sensing
analysis, particularly in the domain of Land-use/Land-cover (LULC). The synergy
of machine learning and satellite imagery analysis has demonstrated significant
productivity in this field, as evidenced by several studies. A notable
challenge within this area is the semantic segmentation mapping of land usage
over extensive territories, where the accessibility of accurate land-use data
and the reliability of ground truth land-use labels pose significant
difficulties. For example, providing a detailed and accurate pixel-wise labeled
dataset of the Flanders region, a first-level administrative division of
Belgium, can be particularly insightful. Yet there is a notable lack of
regulated, formalized datasets and workflows for such studies in many regions
globally. This paper introduces a comprehensive approach to addressing these
gaps. We present a densely labeled ground truth map of Flanders paired with
Sentinel-2 satellite imagery. Our methodology includes a formalized dataset
division and sampling method, utilizing the topographic map layout
'Kaartbladversnijdingen,' and a detailed semantic segmentation model training
pipeline. Preliminary benchmarking results are also provided to demonstrate the
efficacy of our approach.
\\ ( https://arxiv.org/abs/2401.15223 ,  2826kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15235 (*cross-listing*)
Date: Fri, 26 Jan 2024 22:59:51 GMT   (29502kb,D)

Title: CascadedGaze: Efficiency in Global Context Extraction for Image
  Restoration
Authors: Amirhosein Ghasemabadi, Mohammad Salameh, Muhammad Kamran Janjua,
  Chunhua Zhou, Fengyu Sun, Di Niu
Categories: eess.IV cs.CV cs.LG
Comments: 16 pages. ArXiV preprint
\\
  Image restoration tasks traditionally rely on convolutional neural networks.
However, given the local nature of the convolutional operator, they struggle to
capture global information. The promise of attention mechanisms in Transformers
is to circumvent this problem, but it comes at the cost of intensive
computational overhead. Many recent studies in image restoration have focused
on solving the challenge of balancing performance and computational cost via
Transformer variants. In this paper, we present CascadedGaze Network (CGNet),
an encoder-decoder architecture that employs Global Context Extractor (GCE), a
novel and efficient way to capture global information for image restoration.
The GCE module leverages small kernels across convolutional layers to learn
global dependencies, without requiring self-attention. Extensive experimental
results show that our approach outperforms a range of state-of-the-art methods
on denoising benchmark datasets including both real image denoising and
synthetic image denoising, as well as on image deblurring task, while being
more computationally efficient.
\\ ( https://arxiv.org/abs/2401.15235 ,  29502kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15236 (*cross-listing*)
Date: Fri, 26 Jan 2024 23:04:26 GMT   (9348kb,D)

Title: Adaptive Deep Learning for Efficient Visual Pose Estimation aboard
  Ultra-low-power Nano-drones
Authors: Beatrice Alessandra Motetti, Luca Crupi, Mustafa Omer Mohammed Elamin
  Elshaigi, Matteo Risso, Daniele Jahier Pagliari, Daniele Palossi, Alessio
  Burrello
Categories: cs.CV cs.LG
Comments: Accepted for publication in the 2024 Design, Automation and Test in
  Europe (DATE) conference
\\
  Sub-10cm diameter nano-drones are gaining momentum thanks to their
applicability in scenarios prevented to bigger flying drones, such as in narrow
environments and close to humans. However, their tiny form factor also brings
their major drawback: ultra-constrained memory and processors for the onboard
execution of their perception pipelines. Therefore, lightweight deep
learning-based approaches are becoming increasingly popular, stressing how
computational efficiency and energy-saving are paramount as they can make the
difference between a fully working closed-loop system and a failing one. In
this work, to maximize the exploitation of the ultra-limited resources aboard
nano-drones, we present a novel adaptive deep learning-based mechanism for the
efficient execution of a vision-based human pose estimation task. We leverage
two State-of-the-Art (SoA) convolutional neural networks (CNNs) with different
regression performance vs. computational costs trade-offs. By combining these
CNNs with three novel adaptation strategies based on the output's temporal
consistency and on auxiliary tasks to swap the CNN being executed proactively,
we present six different systems. On a real-world dataset and the actual
nano-drone hardware, our best-performing system, compared to executing only the
bigger and most accurate SoA model, shows 28% latency reduction while keeping
the same mean absolute error (MAE), 3% MAE reduction while being iso-latency,
and the absolute peak performance, i.e., 6% better than SoA model.
\\ ( https://arxiv.org/abs/2401.15236 ,  9348kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15239 (*cross-listing*)
Date: Fri, 26 Jan 2024 23:12:53 GMT   (7853kb,D)

Title: MEA-Defender: A Robust Watermark against Model Extraction Attack
Authors: Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang,
  Ruigang Liang, Shenchen Zhu, Pan Li, and Yingjun Zhang
Categories: cs.CR cs.LG
Comments: To Appear in IEEE Symposium on Security and Privacy 2024 (IEEE S&P
  2024), MAY 20-23, 2024, SAN FRANCISCO, CA, USA
\\
  Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been
trained using deep learning algorithms. To protect the Intellectual Property
(IP) of the original owners over such DNN models, backdoor-based watermarks
have been extensively studied. However, most of such watermarks fail upon model
extraction attack, which utilizes input samples to query the target model and
obtains the corresponding outputs, thus training a substitute model using such
input-output pairs. In this paper, we propose a novel watermark to protect IP
of DNN models against model extraction, named MEA-Defender. In particular, we
obtain the watermark by combining two samples from two source classes in the
input domain and design a watermark loss function that makes the output domain
of the watermark within that of the main task samples. Since both the input
domain and the output domain of our watermark are indispensable parts of those
of the main task samples, the watermark will be extracted into the stolen model
along with the main task during model extraction. We conduct extensive
experiments on four model extraction attacks, using five datasets and six
models trained based on supervised learning and self-supervised learning
algorithms. The experimental results demonstrate that MEA-Defender is highly
robust against different model extraction attacks, and various watermark
removal/detection approaches.
\\ ( https://arxiv.org/abs/2401.15239 ,  7853kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15254 (*cross-listing*)
Date: Sat, 27 Jan 2024 00:15:48 GMT   (267kb,D)

Title: Finite Sample Confidence Regions for Linear Regression Parameters Using
  Arbitrary Predictors
Authors: Charles Guille-Escuret and Eugene Ndiaye
Categories: stat.ML cs.LG math.ST stat.TH
\\
  We explore a novel methodology for constructing confidence regions for
parameters of linear models, using predictions from any arbitrary predictor.
Our framework requires minimal assumptions on the noise and can be extended to
functions deviating from strict linearity up to some adjustable threshold,
thereby accommodating a comprehensive and pragmatically relevant set of
functions. The derived confidence regions can be cast as constraints within a
Mixed Integer Linear Programming framework, enabling optimisation of linear
objectives. This representation enables robust optimization and the extraction
of confidence intervals for specific parameter coordinates. Unlike previous
methods, the confidence region can be empty, which can be used for hypothesis
testing. Finally, we validate the empirical applicability of our method on
synthetic data.
\\ ( https://arxiv.org/abs/2401.15254 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15262 (*cross-listing*)
Date: Sat, 27 Jan 2024 01:16:33 GMT   (83kb,D)

Title: Asymptotic Behavior of Adversarial Training Estimator under
  $\ell_\infty$-Perturbation
Authors: Yiling Xie and Xiaoming Huo
Categories: math.ST cs.LG stat.ME stat.ML stat.TH
\\
  Adversarial training has been proposed to hedge against adversarial attacks
in machine learning and statistical models. This paper focuses on adversarial
training under $\ell_\infty$-perturbation, which has recently attracted much
research attention. The asymptotic behavior of the adversarial training
estimator is investigated in the generalized linear model. The results imply
that the limiting distribution of the adversarial training estimator under
$\ell_\infty$-perturbation could put a positive probability mass at $0$ when
the true parameter is $0$, providing a theoretical guarantee of the associated
sparsity-recovery ability. Alternatively, a two-step procedure is proposed --
adaptive adversarial training, which could further improve the performance of
adversarial training under $\ell_\infty$-perturbation. Specifically, the
proposed procedure could achieve asymptotic unbiasedness and variable-selection
consistency. Numerical experiments are conducted to show the sparsity-recovery
ability of adversarial training under $\ell_\infty$-perturbation and to compare
the empirical performance between classic adversarial training and adaptive
adversarial training.
\\ ( https://arxiv.org/abs/2401.15262 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15285 (*cross-listing*)
Date: Sat, 27 Jan 2024 03:55:28 GMT   (652kb)

Title: Ransomware threat mitigation through network traffic analysis and
  machine learning techniques
Authors: Ali Mehrban, Shirin Karimi Geransayeh
Categories: cs.CR cs.LG
\\
  In recent years, there has been a noticeable increase in cyberattacks using
ransomware. Attackers use this malicious software to break into networks and
harm computer systems. This has caused significant and lasting damage to
various organizations, including government, private companies, and regular
users. These attacks often lead to the loss or exposure of sensitive
information, disruptions in normal operations, and persistent vulnerabilities.
This paper focuses on a method for recognizing and identifying ransomware in
computer networks. The approach relies on using machine learning algorithms and
analyzing the patterns of network traffic. By collecting and studying this
traffic, and then applying machine learning models, we can accurately identify
and detect ransomware. The results of implementing this method show that
machine learning algorithms can effectively pinpoint ransomware based on
network traffic, achieving high levels of precision and accuracy.
\\ ( https://arxiv.org/abs/2401.15285 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15294 (*cross-listing*)
Date: Sat, 27 Jan 2024 04:42:50 GMT   (76kb)

Title: Integral Operator Approaches for Scattered Data Fitting on Spheres
Authors: Shao-Bo Lin
Categories: math.NA cs.LG cs.NA
\\
  This paper focuses on scattered data fitting problems on spheres. We study
the approximation performance of a class of weighted spectral filter
algorithms, including Tikhonov regularization, Landaweber iteration, spectral
cut-off, and iterated Tikhonov, in fitting noisy data with possibly unbounded
random noise. For the analysis, we develop an integral operator approach that
can be regarded as an extension of the widely used sampling inequality approach
and norming set method in the community of scattered data fitting. After
providing an equivalence between the operator differences and quadrature rules,
we succeed in deriving optimal Sobolev-type error estimates of weighted
spectral filter algorithms. Our derived error estimates do not suffer from the
saturation phenomenon for Tikhonov regularization in the literature,
native-space-barrier for existing error analysis and adapts to different
embedding spaces. We also propose a divide-and-conquer scheme to equip weighted
spectral filter algorithms to reduce their computational burden and present the
optimal approximation error bounds.
\\ ( https://arxiv.org/abs/2401.15294 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15305 (*cross-listing*)
Date: Sat, 27 Jan 2024 05:53:16 GMT   (392kb,D)

Title: A Practical Probabilistic Benchmark for AI Weather Models
Authors: Noah D. Brenowitz and Yair Cohen and Jaideep Pathak and Ankur Mahesh
  and Boris Bonev and Thorsten Kurth and Dale R. Durran and Peter Harrington
  and Michael S. Pritchard
Categories: physics.ao-ph cs.LG
Comments: 15 pages, 5 figures
\\
  Since the weather is chaotic, forecasts aim to predict the distribution of
future states rather than make a single prediction. Recently, multiple data
driven weather models have emerged claiming breakthroughs in skill. However,
these have mostly been benchmarked using deterministic skill scores, and little
is known about their probabilistic skill. Unfortunately, it is hard to fairly
compare AI weather models in a probabilistic sense, since variations in choice
of ensemble initialization, definition of state, and noise injection
methodology become confounding. Moreover, even obtaining ensemble forecast
baselines is a substantial engineering challenge given the data volumes
involved. We sidestep both problems by applying a decades-old idea -- lagged
ensembles -- whereby an ensemble can be constructed from a moderately-sized
library of deterministic forecasts. This allows the first parameter-free
intercomparison of leading AI weather models' probabilistic skill against an
operational baseline. The results reveal that two leading AI weather models,
i.e. GraphCast and Pangu, are tied on the probabilistic CRPS metric even though
the former outperforms the latter in deterministic scoring. We also reveal how
multiple time-step loss functions, which many data-driven weather models have
employed, are counter-productive: they improve deterministic metrics at the
cost of increased dissipation, deteriorating probabilistic skill. This is
confirmed through ablations applied to a spherical Fourier Neural Operator
(SFNO) approach to AI weather forecasting. Separate SFNO ablations modulating
effective resolution reveal it has a useful effect on ensemble dispersion
relevant to achieving good ensemble calibration. We hope these and forthcoming
insights from lagged ensembles can help guide the development of AI weather
forecasts and have thus shared the diagnostic code.
\\ ( https://arxiv.org/abs/2401.15305 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15321 (*cross-listing*)
Date: Sat, 27 Jan 2024 06:50:32 GMT   (1299kb)

Title: Localization of Dummy Data Injection Attacks in Power Systems
  Considering Incomplete Topological Information: A Spatio-Temporal Graph
  Wavelet Convolutional Neural Network Approach
Authors: Zhaoyang Qu, Yunchang Dong, Yang Li, Siqi Song, Tao Jiang, Min Li,
  Qiming Wang, Lei Wang, Xiaoyong Bo, Jiye Zang, Qi Xu
Categories: eess.SY cs.CR cs.LG cs.SY
Comments: Accepted by Applied Energy
\\
  The emergence of novel the dummy data injection attack (DDIA) poses a severe
threat to the secure and stable operation of power systems. These attacks are
particularly perilous due to the minimal Euclidean spatial separation between
the injected malicious data and legitimate data, rendering their precise
detection challenging using conventional distance-based methods. Furthermore,
existing research predominantly focuses on various machine learning techniques,
often analyzing the temporal data sequences post-attack or relying solely on
Euclidean spatial characteristics. Unfortunately, this approach tends to
overlook the inherent topological correlations within the non-Euclidean spatial
attributes of power grid data, consequently leading to diminished accuracy in
attack localization. To address this issue, this study takes a comprehensive
approach. Initially, it examines the underlying principles of these new DDIAs
on power systems. Here, an intricate mathematical model of the DDIA is
designed, accounting for incomplete topological knowledge and alternating
current (AC) state estimation from an attacker's perspective. Subsequently, by
integrating a priori knowledge of grid topology and considering the temporal
correlations within measurement data and the topology-dependent attributes of
the power grid, this study introduces temporal and spatial attention matrices.
These matrices adaptively capture the spatio-temporal correlations within the
attacks. Leveraging gated stacked causal convolution and graph wavelet sparse
convolution, the study jointly extracts spatio-temporal DDIA features. Finally,
the research proposes a DDIA localization method based on spatio-temporal graph
neural networks. The accuracy and effectiveness of the DDIA model are
rigorously demonstrated through comprehensive analytical cases.
\\ ( https://arxiv.org/abs/2401.15321 ,  1299kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15434 (*cross-listing*)
Date: Sat, 27 Jan 2024 15:05:25 GMT   (285kb)

Title: Decentralized Gossip Mutual Learning (GML) for brain tumor segmentation
  on multi-parametric MRI
Authors: Jingyun Chen, Yading Yuan
Categories: eess.IV cs.CV cs.LG
Comments: 3 pages, 1 figure, accepted to IEEE EMBS 2023. arXiv admin note: text
  overlap with arXiv:2401.06180
\\
  Federated Learning (FL) enables collaborative model training among medical
centers without sharing private data. However, traditional FL risks on server
failures and suboptimal performance on local data due to the nature of
centralized model aggregation. To address these issues, we present Gossip
Mutual Learning (GML), a decentralized framework that uses Gossip Protocol for
direct peer-to-peer communication. In addition, GML encourages each site to
optimize its local model through mutual learning to account for data variations
among different sites. For the task of tumor segmentation using 146 cases from
four clinical sites in BraTS 2021 dataset, we demonstrated GML outperformed
local models and achieved similar performance as FedAvg with only 25%
communication overhead.
\\ ( https://arxiv.org/abs/2401.15434 ,  285kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15455 (*cross-listing*)
Date: Sat, 27 Jan 2024 16:29:53 GMT   (2938kb,D)

Title: New Foggy Object Detecting Model
Authors: Rahul Banavathu, Modem Veda Sree, Bollina Kavya Sri, Suddhasil De
Categories: cs.CV cs.LG
\\
  Object detection in reduced visibility has become a prominent research area.
The existing techniques are not accurate enough in recognizing objects under
such circumstances. This paper introduces a new foggy object detection method
through a two-staged architecture of region identification from input images
and detecting objects in such regions. The paper confirms notable improvements
of the proposed method's accuracy and detection time over existing techniques.
\\ ( https://arxiv.org/abs/2401.15455 ,  2938kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15478 (*cross-listing*)
Date: Sat, 27 Jan 2024 18:46:19 GMT   (3730kb,D)

Title: Product Manifold Representations for Learning on Biological Pathways
Authors: Daniel McNeela, Frederic Sala, Anthony Gitter
Categories: q-bio.QM cs.LG q-bio.MN
Comments: 28 pages, 19 figures
\\
  Machine learning models that embed graphs in non-Euclidean spaces have shown
substantial benefits in a variety of contexts, but their application has not
been studied extensively in the biological domain, particularly with respect to
biological pathway graphs. Such graphs exhibit a variety of complex network
structures, presenting challenges to existing embedding approaches. Learning
high-quality embeddings for biological pathway graphs is important for
researchers looking to understand the underpinnings of disease and train
high-quality predictive models on these networks. In this work, we investigate
the effects of embedding pathway graphs in non-Euclidean mixed-curvature spaces
and compare against traditional Euclidean graph representation learning models.
We then train a supervised model using the learned node embeddings to predict
missing protein-protein interactions in pathway graphs. We find large
reductions in distortion and boosts on in-distribution edge prediction
performance as a result of using mixed-curvature embeddings and their
corresponding graph neural network models. However, we find that
mixed-curvature representations underperform existing baselines on
out-of-distribution edge prediction performance suggesting that these
representations may overfit to the training graph topology. We provide our
mixed-curvature product GCN code at
https://github.com/mcneela/Mixed-Curvature-GCN and our pathway analysis code at
https://github.com/mcneela/Mixed-Curvature-Pathways.
\\ ( https://arxiv.org/abs/2401.15478 ,  3730kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15502 (*cross-listing*)
Date: Sat, 27 Jan 2024 21:07:11 GMT   (273kb,D)

Title: Differentially Private Bayesian Tests
Authors: Abhisek Chakraborty, Saptati Datta
Categories: stat.ML cs.LG
\\
  Differential privacy has emerged as an significant cornerstone in the realm
of scientific hypothesis testing utilizing confidential data. In reporting
scientific discoveries, Bayesian tests are widely adopted since they
effectively circumnavigate the key criticisms of P-values, namely, lack of
interpretability and inability to quantify evidence in support of the competing
hypotheses. We present a novel differentially private Bayesian hypotheses
testing framework that arise naturally under a principled data generative
mechanism, inherently maintaining the interpretability of the resulting
inferences. Furthermore, by focusing on differentially private Bayes factors
based on widely used test statistics, we circumvent the need to model the
complete data generative mechanism and ensure substantial computational
benefits. We also provide a set of sufficient conditions to establish results
on Bayes factor consistency under the proposed framework. The utility of the
devised technology is showcased via several numerical experiments.
\\ ( https://arxiv.org/abs/2401.15502 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15508 (*cross-listing*)
Date: Sat, 27 Jan 2024 21:32:04 GMT   (9808kb,D)

Title: Proto-MPC: An Encoder-Prototype-Decoder Approach for Quadrotor Control
  in Challenging Winds
Authors: Yuliang Gu, Sheng Cheng and Naira Hovakimyan
Categories: cs.RO cs.LG cs.SY eess.SY
\\
  Quadrotors are increasingly used in the evolving field of aerial robotics for
their agility and mechanical simplicity. However, inherent uncertainties, such
as aerodynamic effects coupled with quadrotors' operation in dynamically
changing environments, pose significant challenges for traditional, nominal
model-based control designs. We propose a multi-task meta-learning method
called Encoder-Prototype-Decoder (EPD), which has the advantage of effectively
balancing shared and distinctive representations across diverse training tasks.
Subsequently, we integrate the EPD model into a model predictive control
problem (Proto-MPC) to enhance the quadrotor's ability to adapt and operate
across a spectrum of dynamically changing tasks with an efficient online
implementation. We validate the proposed method in simulations, which
demonstrates Proto-MPC's robust performance in trajectory tracking of a
quadrotor being subject to static and spatially varying side winds.
\\ ( https://arxiv.org/abs/2401.15508 ,  9808kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15541 (*cross-listing*)
Date: Sun, 28 Jan 2024 02:01:26 GMT   (25321kb,D)

Title: Stitching Satellites to the Edge: Pervasive and Efficient Federated LEO
  Satellite Learning
Authors: Mohamed Elmahallawy and Tie Luo
Categories: cs.DC cs.LG
\\
  In the ambitious realm of space AI, the integration of federated learning
(FL) with low Earth orbit (LEO) satellite constellations holds immense promise.
However, many challenges persist in terms of feasibility, learning efficiency,
and convergence. These hurdles stem from the bottleneck in communication,
characterized by sporadic and irregular connectivity between LEO satellites and
ground stations, coupled with the limited computation capability of satellite
edge computing (SEC). This paper proposes a novel FL-SEC framework that
empowers LEO satellites to execute large-scale machine learning (ML) tasks
onboard efficiently. Its key components include i) personalized learning via
divide-and-conquer, which identifies and eliminates redundant satellite images
and converts complex multi-class classification problems to simple binary
classification, enabling rapid and energy-efficient training of lightweight ML
models suitable for IoT/edge devices on satellites; ii) orbital model
retraining, which generates an aggregated "orbital model" per orbit and
retrains it before sending to the ground station, significantly reducing the
required communication rounds. We conducted experiments using Jetson Nano, an
edge device closely mimicking the limited compute on LEO satellites, and a real
satellite dataset. The results underscore the effectiveness of our approach,
highlighting SEC's ability to run lightweight ML models on real and
high-resolution satellite imagery. Our approach dramatically reduces FL
convergence time by nearly 30 times, and satellite energy consumption down to
as low as 1.38 watts, all while maintaining an exceptional accuracy of up to
96%.
\\ ( https://arxiv.org/abs/2401.15541 ,  25321kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15563 (*cross-listing*)
Date: Sun, 28 Jan 2024 04:07:59 GMT   (26918kb,D)

Title: BrepGen: A B-rep Generative Diffusion Model with Structured Latent
  Geometry
Authors: Xiang Xu, Joseph G. Lambourne, Pradeep Kumar Jayaraman, Zhengqing
  Wang, Karl D.D. Willis, Yasutaka Furukawa
Categories: cs.CV cs.LG
\\
  This paper presents BrepGen, a diffusion-based generative approach that
directly outputs a Boundary representation (B-rep) Computer-Aided Design (CAD)
model. BrepGen represents a B-rep model as a novel structured latent geometry
in a hierarchical tree. With the root node representing a whole CAD solid, each
element of a B-rep model (i.e., a face, an edge, or a vertex) progressively
turns into a child-node from top to bottom. B-rep geometry information goes
into the nodes as the global bounding box of each primitive along with a latent
code describing the local geometric shape. The B-rep topology information is
implicitly represented by node duplication. When two faces share an edge, the
edge curve will appear twice in the tree, and a T-junction vertex with three
incident edges appears six times in the tree with identical node features.
Starting from the root and progressing to the leaf, BrepGen employs
Transformer-based diffusion models to sequentially denoise node features while
duplicated nodes are detected and merged, recovering the B-Rep topology
information. Extensive experiments show that BrepGen sets a new milestone in
CAD B-rep generation, surpassing existing methods on various benchmarks.
Results on our newly collected furniture dataset further showcase its
exceptional capability in generating complicated geometry. While previous
methods were limited to generating simple prismatic shapes, BrepGen
incorporates free-form and doubly-curved surfaces for the first time.
Additional applications of BrepGen include CAD autocomplete and design
interpolation. The code, pretrained models, and dataset will be released.
\\ ( https://arxiv.org/abs/2401.15563 ,  26918kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15566 (*cross-listing*)
Date: Sun, 28 Jan 2024 04:18:44 GMT   (2870kb,D)

Title: On the Robustness of Cross-Concentrated Sampling for Matrix Completion
Authors: HanQin Cai and Longxiu Huang and Chandra Kundu and Bowen Su
Categories: stat.ML cs.IT cs.LG math.IT math.OC
Comments: 58th Annual Conference of Information Sciences and Systems
\\
  Matrix completion is one of the crucial tools in modern data science
research. Recently, a novel sampling model for matrix completion coined
cross-concentrated sampling (CCS) has caught much attention. However, the
robustness of the CCS model against sparse outliers remains unclear in the
existing studies. In this paper, we aim to answer this question by exploring a
novel Robust CCS Completion problem. A highly efficient non-convex iterative
algorithm, dubbed Robust CUR Completion (RCURC), is proposed. The empirical
performance of the proposed algorithm, in terms of both efficiency and
robustness, is verified in synthetic and real datasets.
\\ ( https://arxiv.org/abs/2401.15566 ,  2870kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15623 (*cross-listing*)
Date: Sun, 28 Jan 2024 10:29:23 GMT   (1009kb,D)

Title: GT-PCA: Effective and Interpretable Dimensionality Reduction with
  General Transform-Invariant Principal Component Analysis
Authors: Florian Heinrichs
Categories: stat.ML cs.LG stat.ME
Comments: 15 pages, 11 figures, 8 tables
MSC-class: 62H25 (Primary) 62M10, 62R10, 68T07, 68T10, 62M45 (Secondary)
ACM-class: G.3; I.2.6; I.5.1
\\
  Data analysis often requires methods that are invariant with respect to
specific transformations, such as rotations in case of images or shifts in case
of images and time series. While principal component analysis (PCA) is a
widely-used dimension reduction technique, it lacks robustness with respect to
these transformations. Modern alternatives, such as autoencoders, can be
invariant with respect to specific transformations but are generally not
interpretable. We introduce General Transform-Invariant Principal Component
Analysis (GT-PCA) as an effective and interpretable alternative to PCA and
autoencoders. We propose a neural network that efficiently estimates the
components and show that GT-PCA significantly outperforms alternative methods
in experiments based on synthetic and real data.
\\ ( https://arxiv.org/abs/2401.15623 ,  1009kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15632 (*cross-listing*)
Date: Sun, 28 Jan 2024 11:49:57 GMT   (32318kb,D)

Title: Deep Learning for Gamma-Ray Bursts: A data driven event framework for
  X/Gamma-Ray analysis in space telescopes
Authors: Riccardo Crupi
Categories: astro-ph.HE cs.LG
Comments: PhD thesis
\\
  This thesis comprises the first three chapters dedicated to providing an
overview of Gamma Ray-Bursts (GRBs), their properties, the instrumentation used
to detect them, and Artificial Intelligence (AI) applications in the context of
GRBs, including a literature review and future prospects. Considering both the
current and the next generation of high X-ray monitors, such as Fermi-GBM and
HERMES Pathfinder (an in-orbit demonstration of six 3U nano-satellites), the
research question revolves around the detection of long and faint high-energy
transients, potentially GRBs, that might have been missed by previous detection
algorithms. To address this, two chapters introduce a new data-driven
framework, DeepGRB.
  In Chapter 4, a Neural Network (NN) is described for background count rate
estimation for X/gamma-ray detectors, providing a performance evaluation in
different periods, including both solar maxima, solar minima periods, and one
containing an ultra-long GRB. The application of eXplainable Artificial
Intelligence (XAI) is performed for global and local feature importance
analysis to better understand the behavior of the NN.
  Chapter 5 employs FOCuS-Poisson for anomaly detection in count rate
observations and estimation from the NN. DeepGRB demonstrates its capability to
process Fermi-GBM data, confirming cataloged events and identifying new ones,
providing further analysis with estimates for localization, duration, and
classification. The chapter concludes with an automated classification method
using Machine Learning techniques that incorporates XAI for eventual bias
identification.
\\ ( https://arxiv.org/abs/2401.15632 ,  32318kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15645 (*cross-listing*)
Date: Sun, 28 Jan 2024 12:47:39 GMT   (1723kb,D)

Title: Ensemble-Based Annealed Importance Sampling
Authors: Haoxuan Chen, Lexing Ying
Categories: stat.CO cs.LG cs.NA math.NA physics.comp-ph stat.ML
Comments: 33 pages, 13 figures
MSC-class: 65C05, 65C40, 65C60, 62P35
\\
  Sampling from a multimodal distribution is a fundamental and challenging
problem in computational science and statistics. Among various approaches
proposed for this task, one popular method is Annealed Importance Sampling
(AIS). In this paper, we propose an ensemble-based version of AIS by combining
it with population-based Monte Carlo methods to improve its efficiency. By
keeping track of an ensemble instead of a single particle along some
continuation path between the starting distribution and the target
distribution, we take advantage of the interaction within the ensemble to
encourage the exploration of undiscovered modes. Specifically, our main idea is
to utilize either the snooker algorithm or the genetic algorithm used in
Evolutionary Monte Carlo. We discuss how the proposed algorithm can be
implemented and derive a partial differential equation governing the evolution
of the ensemble under the continuous time and mean-field limit. We also test
the efficiency of the proposed algorithm on various continuous and discrete
distributions.
\\ ( https://arxiv.org/abs/2401.15645 ,  1723kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15695 (*cross-listing*)
Date: Sun, 28 Jan 2024 16:44:17 GMT   (42379kb,D)

Title: HappyRouting: Learning Emotion-Aware Route Trajectories for Scalable
  In-The-Wild Navigation
Authors: David Bethge, Daniel Bulanda, Adam Kozlowski, Thomas Kosch, Albrecht
  Schmidt, Tobias Grosse-Puppendahl
Categories: cs.HC cs.LG
Comments: 17 pages
\\
  Routes represent an integral part of triggering emotions in drivers.
Navigation systems allow users to choose a navigation strategy, such as the
fastest or shortest route. However, they do not consider the driver's emotional
well-being. We present HappyRouting, a novel navigation-based empathic car
interface guiding drivers through real-world traffic while evoking positive
emotions. We propose design considerations, derive a technical architecture,
and implement a routing optimization framework. Our contribution is a machine
learning-based generated emotion map layer, predicting emotions along routes
based on static and dynamic contextual data. We evaluated HappyRouting in a
real-world driving study (N=13), finding that happy routes increase
subjectively perceived valence by 11% (p=.007). Although happy routes take 1.25
times longer on average, participants perceived the happy route as shorter,
presenting an emotion-enhanced alternative to today's fastest routing
mechanisms. We discuss how emotion-based routing can be integrated into
navigation apps, promoting emotional well-being for mobility use.
\\ ( https://arxiv.org/abs/2401.15695 ,  42379kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15719 (*cross-listing*)
Date: Sun, 28 Jan 2024 17:57:39 GMT   (22kb)

Title: Rates of Convergence in the Central Limit Theorem for Markov Chains,
  with an Application to TD Learning
Authors: R. Srikant
Categories: math.PR cs.LG cs.SY eess.SY math.OC
\\
  We prove a non-asymptotic central limit theorem for vector-valued martingale
differences using Stein's method, and use Poisson's equation to extend the
result to functions of Markov Chains. We then show that these results can be
applied to establish a non-asymptotic central limit theorem for Temporal
Difference (TD) learning with averaging.
\\ ( https://arxiv.org/abs/2401.15719 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15739 (*cross-listing*)
Date: Sun, 28 Jan 2024 19:47:17 GMT   (2094kb)

Title: SegmentAnyTree: A sensor and platform agnostic deep learning model for
  tree segmentation using laser scanning data
Authors: Maciej Wielgosz, Stefano Puliti, Binbin Xiang, Konrad Schindler,
  Rasmus Astrup
Categories: cs.CV cs.LG
\\
  This research advances individual tree crown (ITC) segmentation in lidar
data, using a deep learning model applicable to various laser scanning types:
airborne (ULS), terrestrial (TLS), and mobile (MLS). It addresses the challenge
of transferability across different data characteristics in 3D forest scene
analysis. The study evaluates the model's performance based on platform (ULS,
MLS) and data density, testing five scenarios with varying input data,
including sparse versions, to gauge adaptability and canopy layer efficacy. The
model, based on PointGroup architecture, is a 3D CNN with separate heads for
semantic and instance segmentation, validated on diverse point cloud datasets.
Results show point cloud sparsification enhances performance, aiding sparse
data handling and improving detection in dense forests. The model performs well
with >50 points per sq. m densities but less so at 10 points per sq. m due to
higher omission rates. It outperforms existing methods (e.g., Point2Tree,
TLS2trees) in detection, omission, commission rates, and F1 score, setting new
benchmarks on LAUTx, Wytham Woods, and TreeLearn datasets. In conclusion, this
study shows the feasibility of a sensor-agnostic model for diverse lidar data,
surpassing sensor-specific approaches and setting new standards in tree
segmentation, particularly in complex forests. This contributes to future
ecological modeling and forest management advancements.
\\ ( https://arxiv.org/abs/2401.15739 ,  2094kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15742 (*cross-listing*)
Date: Sun, 28 Jan 2024 20:01:44 GMT   (3239kb,D)

Title: Efficient Data-Driven MPC for Demand Response of Commercial Buildings
Authors: Marie-Christine Par\'e, Vasken Dermardiros, Antoine Lesage-Landry
Categories: eess.SY cs.LG cs.SY
\\
  Model predictive control (MPC) has been shown to significantly improve the
energy efficiency of buildings while maintaining thermal comfort. Data-driven
approaches based on neural networks have been proposed to facilitate system
modelling. However, such approaches are generally nonconvex and result in
computationally intractable optimization problems. In this work, we design a
readily implementable energy management method for small commercial buildings.
We then leverage our approach to formulate a real-time demand bidding strategy.
We propose a data-driven and mixed-integer convex MPC which is solved via
derivative-free optimization given a limited computational time of 5 minutes to
respect operational constraints. We consider rooftop unit heating, ventilation,
and air conditioning systems with discrete controls to accurately model the
operation of most commercial buildings. Our approach uses an input convex
recurrent neural network to model the thermal dynamics. We apply our approach
in several demand response (DR) settings, including a demand bidding, a
time-of-use, and a critical peak rebate program. Controller performance is
evaluated on a state-of-the-art building simulation. The proposed approach
improves thermal comfort while reducing energy consumption and cost through DR
participation, when compared to other data-driven approaches or a set-point
controller.
\\ ( https://arxiv.org/abs/2401.15742 ,  3239kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15743 (*cross-listing*)
Date: Sun, 28 Jan 2024 20:02:13 GMT   (1197kb,D)

Title: Real-time EEG-based Emotion Recognition Model using Principal Component
  Analysis and Tree-based Models for Neurohumanities
Authors: Miguel A. Blanco-Rios, Milton O. Candela-Leal, Cecilia Orozco-Romo,
  Paulina Remis-Serna, Carol S. Velez-Saboya, Jorge De-J. Lozoya-Santos, Manuel
  Cebral-Loureda, Mauricio A. Ramirez-Moreno
Categories: eess.SP cs.HC cs.LG q-bio.NC
Comments: 20 pages, 7 figures. Submitted to Frontiers in Human Neuroscience
\\
  Within the field of Humanities, there is a recognized need for educational
innovation, as there are currently no reported tools available that enable
individuals to interact with their environment to create an enhanced learning
experience in the humanities (e.g., immersive spaces). This project proposes a
solution to address this gap by integrating technology and promoting the
development of teaching methodologies in the humanities, specifically by
incorporating emotional monitoring during the learning process of humanistic
context inside an immersive space. In order to achieve this goal, a real-time
emotion detection EEG-based system was developed to interpret and classify
specific emotions. These emotions aligned with the early proposal by Descartes
(Passions), including admiration, love, hate, desire, joy, and sadness. This
system aims to integrate emotional data into the Neurohumanities Lab
interactive platform, creating a comprehensive and immersive learning
environment. This work developed a ML, real-time emotion detection model that
provided Valence, Arousal, and Dominance (VAD) estimations every 5 seconds.
Using PCA, PSD, RF, and Extra-Trees, the best 8 channels and their respective
best band powers were extracted; furthermore, multiple models were evaluated
using shift-based data division and cross-validations. After assessing their
performance, Extra-Trees achieved a general accuracy of 96%, higher than the
reported in the literature (88% accuracy). The proposed model provided
real-time predictions of VAD variables and was adapted to classify Descartes'
six main passions. However, with the VAD values obtained, more than 15 emotions
can be classified (reported in the VAD emotion mapping) and extend the range of
this application.
\\ ( https://arxiv.org/abs/2401.15743 ,  1197kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15767 (*cross-listing*)
Date: Sun, 28 Jan 2024 21:08:45 GMT   (666kb,D)

Title: A Centralized Reinforcement Learning Framework for Adaptive Clustering
  with Low Control Overhead in IoT Networks
Authors: F. Fernando Jurado-Lasso, J. F. Jurado, and Xenofon Fafoutis
Categories: cs.NI cs.LG
Comments: 13 pages, 13 figures, 3 tables, journal
\\
  Wireless Sensor Networks (WSNs) play a pivotal role in enabling Internet of
Things (IoT) devices with sensing and actuation capabilities. Operating in
remote and resource-constrained environments, these IoT devices face challenges
related to energy consumption, crucial for network longevity. Clustering
protocols have emerged as an effective solution to alleviate energy burdens on
IoT devices. This paper introduces Low-Energy Adaptive Clustering Hierarchy
with Reinforcement Learning-based Controller (LEACH-RLC), a novel clustering
protocol that employs a Mixed Integer Linear Programming (MILP) for strategic
selection of cluster heads (CHs) and node-to-cluster assignments. Additionally,
it integrates a Reinforcement Learning (RL) agent to minimize control overhead
by learning optimal timings for generating new clusters. Addressing key
research questions, LEACH-RLC seeks to balance control overhead reduction
without compromising overall network performance. Through extensive
simulations, this paper investigates the frequency and opportune moments for
generating new clustering solutions. Results demonstrate the superior
performance of LEACH-RLC over conventional LEACH and LEACH-C, showcasing
enhanced network lifetime, reduced average energy consumption, and minimized
control overhead. The proposed protocol contributes to advancing the efficiency
and adaptability of WSNs, addressing critical challenges in IoT deployments.
\\ ( https://arxiv.org/abs/2401.15767 ,  666kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15771 (*cross-listing*)
Date: Sun, 28 Jan 2024 21:19:15 GMT   (103kb,D)

Title: Bayesian Nonparametrics meets Data-Driven Robust Optimization
Authors: Nicola Bariletto (1), Nhat Ho (1) ((1) The University of Texas at
  Austin)
Categories: stat.ML cs.LG
\\
  Training machine learning and statistical models often involves optimizing a
data-driven risk criterion. The risk is usually computed with respect to the
empirical data distribution, but this may result in poor and unstable
out-of-sample performance due to distributional uncertainty. In the spirit of
distributionally robust optimization, we propose a novel robust criterion by
combining insights from Bayesian nonparametric (i.e., Dirichlet Process) theory
and recent decision-theoretic models of smooth ambiguity-averse preferences.
First, we highlight novel connections with standard regularized empirical risk
minimization techniques, among which Ridge and LASSO regressions. Then, we
theoretically demonstrate the existence of favorable finite-sample and
asymptotic statistical guarantees on the performance of the robust optimization
procedure. For practical implementation, we propose and study tractable
approximations of the criterion based on well-known Dirichlet Process
representations. We also show that the smoothness of the criterion naturally
leads to standard gradient-based numerical optimization. Finally, we provide
insights into the workings of our method by applying it to high-dimensional
sparse linear regression and robust location parameter estimation tasks.
\\ ( https://arxiv.org/abs/2401.15771 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15785 (*cross-listing*)
Date: Sun, 28 Jan 2024 22:30:50 GMT   (5536kb,D)

Title: Real-time object detection and robotic manipulation for agriculture
  using a YOLO-based learning approach
Authors: Hongyu Zhao, Zezhi Tang, Zhenhong Li, Yi Dong, Yuancheng Si, Mingyang
  Lu, George Panoutsos
Categories: cs.CV cs.LG
Comments: 7 pages, 9 figures
\\
  The optimisation of crop harvesting processes for commonly cultivated crops
is of great importance in the aim of agricultural industrialisation. Nowadays,
the utilisation of machine vision has enabled the automated identification of
crops, leading to the enhancement of harvesting efficiency, but challenges
still exist. This study presents a new framework that combines two separate
architectures of convolutional neural networks (CNNs) in order to
simultaneously accomplish the tasks of crop detection and harvesting (robotic
manipulation) inside a simulated environment. Crop images in the simulated
environment are subjected to random rotations, cropping, brightness, and
contrast adjustments to create augmented images for dataset generation. The you
only look once algorithmic framework is employed with traditional rectangular
bounding boxes for crop localization. The proposed method subsequently utilises
the acquired image data via a visual geometry group model in order to reveal
the grasping positions for the robotic manipulation.
\\ ( https://arxiv.org/abs/2401.15785 ,  5536kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15791 (*cross-listing*)
Date: Sun, 28 Jan 2024 22:43:33 GMT   (85kb,D)

Title: Improving Kernel-Based Nonasymptotic Simultaneous Confidence Bands
Authors: Bal\'azs Csan\'ad Cs\'aji and B\'alint Horv\'ath
Categories: stat.ML cs.LG math.ST stat.TH
Journal-ref: 22nd IFAC World Congress, Yokohama, Japan, 2023, 10357-10362
DOI: 10.1016/j.ifacol.2023.10.1047
\\
  The paper studies the problem of constructing nonparametric simultaneous
confidence bands with nonasymptotic and distribition-free guarantees. The
target function is assumed to be band-limited and the approach is based on the
theory of Paley-Wiener reproducing kernel Hilbert spaces. The starting point of
the paper is a recently developed algorithm to which we propose three types of
improvements. First, we relax the assumptions on the noises by replacing the
symmetricity assumption with a weaker distributional invariance principle.
Then, we propose a more efficient way to estimate the norm of the target
function, and finally we enhance the construction of the confidence bands by
tightening the constraints of the underlying convex optimization problems. The
refinements are also illustrated through numerical experiments.
\\ ( https://arxiv.org/abs/2401.15791 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15792 (*cross-listing*)
Date: Sun, 28 Jan 2024 22:44:41 GMT   (98kb,D)

Title: Sample Complexity of the Sign-Perturbed Sums Identification Method:
  Scalar Case
Authors: Szabolcs Szentp\'eteri and Bal\'azs Csan\'ad Cs\'aji
Categories: stat.ML cs.LG cs.SY eess.SY math.ST stat.TH
Journal-ref: 22nd IFAC World Congress, Yokohama, Japan, 2023, 10363-10370
DOI: 10.1016/j.ifacol.2023.10.1048
\\
  Sign-Perturbed Sum (SPS) is a powerful finite-sample system identification
algorithm which can construct confidence regions for the true data generating
system with exact coverage probabilities, for any finite sample size. SPS was
developed in a series of papers and it has a wide range of applications, from
general linear systems, even in a closed-loop setup, to nonlinear and
nonparametric approaches. Although several theoretical properties of SPS were
proven in the literature, the sample complexity of the method was not analysed
so far. This paper aims to fill this gap and provides the first results on the
sample complexity of SPS. Here, we focus on scalar linear regression problems,
that is we study the behaviour of SPS confidence intervals. We provide high
probability upper bounds, under three different sets of assumptions, showing
that the sizes of SPS confidence intervals shrink at a geometric rate around
the true parameter, if the observation noises are subgaussian. We also show
that similar bounds hold for the previously proposed outer approximation of the
confidence region. Finally, we present simulation experiments comparing the
theoretical and the empirical convergence rates.
\\ ( https://arxiv.org/abs/2401.15792 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15800 (*cross-listing*)
Date: Sun, 28 Jan 2024 23:14:51 GMT   (180kb,D)

Title: Provably Stable Feature Rankings with SHAP and LIME
Authors: Jeremy Goldwasser and Giles Hooker
Categories: stat.ML cs.LG
\\
  Feature attributions are ubiquitous tools for understanding the predictions
of machine learning models. However, popular methods for scoring input
variables such as SHAP and LIME suffer from high instability due to random
sampling. Leveraging ideas from multiple hypothesis testing, we devise
attribution methods that correctly rank the most important features with high
probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley
values have the proper ordering with probability exceeding $1-\alpha$.
Empirical results demonstrate its validity and impressive computational
efficiency. We also build on previous work to yield similar results for LIME,
ensuring the most important features are selected in the right order.
\\ ( https://arxiv.org/abs/2401.15800 ,  180kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15817 (*cross-listing*)
Date: Mon, 29 Jan 2024 00:52:01 GMT   (1168kb)

Title: Transparency Attacks: How Imperceptible Image Layers Can Fool AI
  Perception
Authors: Forrest McKee, David Noever
Categories: cs.CV cs.LG
\\
  This paper investigates a novel algorithmic vulnerability when imperceptible
image layers confound multiple vision models into arbitrary label assignments
and captions. We explore image preprocessing methods to introduce stealth
transparency, which triggers AI misinterpretation of what the human eye
perceives. The research compiles a broad attack surface to investigate the
consequences ranging from traditional watermarking, steganography, and
background-foreground miscues. We demonstrate dataset poisoning using the
attack to mislabel a collection of grayscale landscapes and logos using either
a single attack layer or randomly selected poisoning classes. For example, a
military tank to the human eye is a mislabeled bridge to object classifiers
based on convolutional networks (YOLO, etc.) and vision transformers (ViT,
GPT-Vision, etc.). A notable attack limitation stems from its dependency on the
background (hidden) layer in grayscale as a rough match to the transparent
foreground image that the human eye perceives. This dependency limits the
practical success rate without manual tuning and exposes the hidden layers when
placed on the opposite display theme (e.g., light background, light transparent
foreground visible, works best against a light theme image viewer or browser).
The stealth transparency confounds established vision systems, including
evading facial recognition and surveillance, digital watermarking, content
filtering, dataset curating, automotive and drone autonomy, forensic evidence
tampering, and retail product misclassifying. This method stands in contrast to
traditional adversarial attacks that typically focus on modifying pixel values
in ways that are either slightly perceptible or entirely imperceptible for both
humans and machines.
\\ ( https://arxiv.org/abs/2401.15817 ,  1168kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15838 (*cross-listing*)
Date: Mon, 29 Jan 2024 02:08:40 GMT   (1199kb,D)

Title: Distributed Markov Chain Monte Carlo Sampling based on the Alternating
  Direction Method of Multipliers
Authors: Alexandros E. Tzikas, Licio Romao, Mert Pilanci, Alessandro Abate, and
  Mykel J. Kochenderfer
Categories: stat.ML cs.LG cs.MA math.OC stat.CO
\\
  Many machine learning applications require operating on a spatially
distributed dataset. Despite technological advances, privacy considerations and
communication constraints may prevent gathering the entire dataset in a central
unit. In this paper, we propose a distributed sampling scheme based on the
alternating direction method of multipliers, which is commonly used in the
optimization literature due to its fast convergence. In contrast to distributed
optimization, distributed sampling allows for uncertainty quantification in
Bayesian inference tasks. We provide both theoretical guarantees of our
algorithm's convergence and experimental evidence of its superiority to the
state-of-the-art. For our theoretical results, we use convex optimization tools
to establish a fundamental inequality on the generated local sample iterates.
This inequality enables us to show convergence of the distribution associated
with these iterates to the underlying target distribution in Wasserstein
distance. In simulation, we deploy our algorithm on linear and logistic
regression tasks and illustrate its fast convergence compared to existing
gradient-based methods.
\\ ( https://arxiv.org/abs/2401.15838 ,  1199kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15853 (*cross-listing*)
Date: Mon, 29 Jan 2024 03:04:43 GMT   (4969kb,D)

Title: Attentive Convolutional Deep Reinforcement Learning for Optimizing
  Solar-Storage Systems in Real-Time Electricity Markets
Authors: Jinhao Li, Changlong Wang, Hao Wang
Categories: eess.SY cs.LG cs.SY math.OC
Journal-ref: IEEE Transactions on Industrial Informatics, 2024
DOI: 10.1109/TII.2024.3352229
\\
  This paper studies the synergy of solar-battery energy storage system (BESS)
and develops a viable strategy for the BESS to unlock its economic potential by
serving as a backup to reduce solar curtailments while also participating in
the electricity market. We model the real-time bidding of the solar-battery
system as two Markov decision processes for the solar farm and the BESS,
respectively. We develop a novel deep reinforcement learning (DRL) algorithm to
solve the problem by leveraging attention mechanism (AC) and multi-grained
feature convolution to process DRL input for better bidding decisions.
Simulation results demonstrate that our AC-DRL outperforms two
optimization-based and one DRL-based benchmarks by generating 23%, 20%, and 11%
higher revenue, as well as improving curtailment responses. The excess solar
generation can effectively charge the BESS to bid in the market, significantly
reducing solar curtailments by 76% and creating synergy for the solar-battery
system to be more viable.
\\ ( https://arxiv.org/abs/2401.15853 ,  4969kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15874 (*cross-listing*)
Date: Mon, 29 Jan 2024 04:14:02 GMT   (887kb,D)

Title: Rethinking Personalized Federated Learning with Clustering-based Dynamic
  Graph Propagation
Authors: Jiaqi Wang, Yuzhong Chen, Yuhang Wu, Mahashweta Das, Hao Yang,
  Fenglong Ma
Categories: cs.DC cs.LG
Comments: This paper has been accepted by PAKDD 2024 as an oral presentation
\\
  Most existing personalized federated learning approaches are based on
intricate designs, which often require complex implementation and tuning. In
order to address this limitation, we propose a simple yet effective
personalized federated learning framework. Specifically, during each
communication round, we group clients into multiple clusters based on their
model training status and data distribution on the server side. We then
consider each cluster center as a node equipped with model parameters and
construct a graph that connects these nodes using weighted edges. Additionally,
we update the model parameters at each node by propagating information across
the entire graph. Subsequently, we design a precise personalized model
distribution strategy to allow clients to obtain the most suitable model from
the server side. We conduct experiments on three image benchmark datasets and
create synthetic structured datasets with three types of typologies.
Experimental results demonstrate the effectiveness of the proposed work.
\\ ( https://arxiv.org/abs/2401.15874 ,  887kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15883 (*cross-listing*)
Date: Mon, 29 Jan 2024 04:35:48 GMT   (46502kb,D)

Title: TransTroj: Transferable Backdoor Attacks to Pre-trained Models via
  Embedding Indistinguishability
Authors: Hao Wang, Tao Xiang, Shangwei Guo, Jialing He, Hangcheng Liu, Tianwei
  Zhang
Categories: cs.CR cs.CV cs.LG
Comments: 13 pages, 16 figures, 5 tables
\\
  Pre-trained models (PTMs) are extensively utilized in various downstream
tasks. Adopting untrusted PTMs may suffer from backdoor attacks, where the
adversary can compromise the downstream models by injecting backdoors into the
PTM. However, existing backdoor attacks to PTMs can only achieve partially
task-agnostic and the embedded backdoors are easily erased during the
fine-tuning process. In this paper, we propose a novel transferable backdoor
attack, TransTroj, to simultaneously meet functionality-preserving, durable,
and task-agnostic. In particular, we first formalize transferable backdoor
attacks as the indistinguishability problem between poisoned and clean samples
in the embedding space. We decompose the embedding indistinguishability into
pre- and post-indistinguishability, representing the similarity of the poisoned
and reference embeddings before and after the attack. Then, we propose a
two-stage optimization that separately optimizes triggers and victim PTMs to
achieve embedding indistinguishability. We evaluate TransTroj on four PTMs and
six downstream tasks. Experimental results show that TransTroj significantly
outperforms SOTA task-agnostic backdoor attacks (18%$\sim$99%, 68% on average)
and exhibits superior performance under various system settings. The code is
available at https://github.com/haowang-cqu/TransTroj .
\\ ( https://arxiv.org/abs/2401.15883 ,  46502kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15890 (*cross-listing*)
Date: Mon, 29 Jan 2024 05:05:03 GMT   (2140kb,D)

Title: Probabilistic Guarantees of Stochastic Recursive Gradient in Non-Convex
  Finite Sum Problems
Authors: Yanjie Zhong, Jiaqi Li, Soumendra Lahiri
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 41 pages, 3 figures, accepted to PAKDD 2024
\\
  This paper develops a new dimension-free Azuma-Hoeffding type bound on
summation norm of a martingale difference sequence with random individual
bounds. With this novel result, we provide high-probability bounds for the
gradient norm estimator in the proposed algorithm Prob-SARAH, which is a
modified version of the StochAstic Recursive grAdient algoritHm (SARAH), a
state-of-art variance reduced algorithm that achieves optimal computational
complexity in expectation for the finite sum problem. The in-probability
complexity by Prob-SARAH matches the best in-expectation result up to
logarithmic factors. Empirical experiments demonstrate the superior
probabilistic performance of Prob-SARAH on real datasets compared to other
popular algorithms.
\\ ( https://arxiv.org/abs/2401.15890 ,  2140kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15897 (*cross-listing*)
Date: Mon, 29 Jan 2024 05:46:14 GMT   (576kb)

Title: Red-Teaming for Generative AI: Silver Bullet or Security Theater?
Authors: Michael Feffer, Anusha Sinha, Zachary C. Lipton, Hoda Heidari
Categories: cs.CY cs.HC cs.LG
\\
  In response to rising concerns surrounding the safety, security, and
trustworthiness of Generative AI (GenAI) models, practitioners and regulators
alike have pointed to AI red-teaming as a key component of their strategies for
identifying and mitigating these risks. However, despite AI red-teaming's
central role in policy discussions and corporate messaging, significant
questions remain about what precisely it means, what role it can play in
regulation, and how precisely it relates to conventional red-teaming practices
as originally conceived in the field of cybersecurity. In this work, we
identify recent cases of red-teaming activities in the AI industry and conduct
an extensive survey of the relevant research literature to characterize the
scope, structure, and criteria for AI red-teaming practices. Our analysis
reveals that prior methods and practices of AI red-teaming diverge along
several axes, including the purpose of the activity (which is often vague), the
artifact under evaluation, the setting in which the activity is conducted
(e.g., actors, resources, and methods), and the resulting decisions it informs
(e.g., reporting, disclosure, and mitigation). In light of our findings, we
argue that while red-teaming may be a valuable big-tent idea for characterizing
a broad set of activities and attitudes aimed at improving the behavior of
GenAI models, gestures towards red-teaming as a panacea for every possible risk
verge on security theater. To move toward a more robust toolbox of evaluations
for generative AI, we synthesize our recommendations into a question bank meant
to guide and scaffold future AI red-teaming practices.
\\ ( https://arxiv.org/abs/2401.15897 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15913 (*cross-listing*)
Date: Mon, 29 Jan 2024 06:48:16 GMT   (14167kb,D)

Title: Vision-Informed Flow Image Super-Resolution with Quaternion Spatial
  Modeling and Dynamic Flow Convolution
Authors: Qinglong Cao, Zhengqin Xu, Chao Ma, Xiaokang Yang, Yuntian Chen
Categories: eess.IV cs.CV cs.LG physics.flu-dyn stat.AP
\\
  Flow image super-resolution (FISR) aims at recovering high-resolution
turbulent velocity fields from low-resolution flow images. Existing FISR
methods mainly process the flow images in natural image patterns, while the
critical and distinct flow visual properties are rarely considered. This
negligence would cause the significant domain gap between flow and natural
images to severely hamper the accurate perception of flow turbulence, thereby
undermining super-resolution performance. To tackle this dilemma, we
comprehensively consider the flow visual properties, including the unique flow
imaging principle and morphological information, and propose the first flow
visual property-informed FISR algorithm. Particularly, different from natural
images that are constructed by independent RGB channels in the light field,
flow images build on the orthogonal UVW velocities in the flow field. To
empower the FISR network with an awareness of the flow imaging principle, we
propose quaternion spatial modeling to model this orthogonal spatial
relationship for improved FISR. Moreover, due to viscosity and surface tension
characteristics, fluids often exhibit a droplet-like morphology in flow images.
Inspired by this morphological property, we design the dynamic flow convolution
to effectively mine the morphological information to enhance FISR. Extensive
experiments on the newly acquired flow image datasets demonstrate the
state-of-the-art performance of our method. Code and data will be made
available.
\\ ( https://arxiv.org/abs/2401.15913 ,  14167kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15990 (*cross-listing*)
Date: Mon, 29 Jan 2024 09:20:08 GMT   (1828kb,D)

Title: Gland segmentation via dual encoders and boundary-enhanced attention
Authors: Huadeng Wang, Jiejiang Yu, Bingbing Li, Xipeng Pan, Zhenbing Liu,
  Rushi Lan, Xiaonan Luo
Categories: eess.IV cs.CV cs.LG
Comments: accepted for IEEE ICASSP 2024
\\
  Accurate and automated gland segmentation on pathological images can assist
pathologists in diagnosing the malignancy of colorectal adenocarcinoma.
However, due to various gland shapes, severe deformation of malignant glands,
and overlapping adhesions between glands. Gland segmentation has always been
very challenging. To address these problems, we propose a DEA model. This model
consists of two branches: the backbone encoding and decoding network and the
local semantic extraction network. The backbone encoding and decoding network
extracts advanced Semantic features, uses the proposed feature decoder to
restore feature space information, and then enhances the boundary features of
the gland through boundary enhancement attention. The local semantic extraction
network uses the pre-trained DeepLabv3+ as a Local semantic-guided encoder to
realize the extraction of edge features. Experimental results on two public
datasets, GlaS and CRAG, confirm that the performance of our method is better
than other gland segmentation methods.
\\ ( https://arxiv.org/abs/2401.15990 ,  1828kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16016 (*cross-listing*)
Date: Mon, 29 Jan 2024 10:05:37 GMT   (739kb,D)

Title: Combined track finding with GNN & CKF
Authors: Lukas Heinrich, Benjamin Huth, Andreas Salzburger, Tilo Wettig
Categories: hep-ex cs.LG
Comments: 6 pages, 6 figures, to be published in the Connecting The Dots 2023
  conference proceedings
\\
  The application of Graph Neural Networks (GNN) in track reconstruction is a
promising approach to cope with the challenges arising at the High-Luminosity
upgrade of the Large Hadron Collider (HL-LHC). GNNs show good track-finding
performance in high-multiplicity scenarios and are naturally parallelizable on
heterogeneous compute architectures.
  Typical high-energy-physics detectors have high resolution in the innermost
layers to support vertex reconstruction but lower resolution in the outer
parts. GNNs mainly rely on 3D space-point information, which can cause reduced
track-finding performance in the outer regions.
  In this contribution, we present a novel combination of GNN-based track
finding with the classical Combinatorial Kalman Filter (CKF) algorithm to
circumvent this issue: The GNN resolves the track candidates in the inner pixel
region, where 3D space points can represent measurements very well. These
candidates are then picked up by the CKF in the outer regions, where the CKF
performs well even for 1D measurements.
  Using the ACTS infrastructure, we present a proof of concept based on truth
tracking in the pixels as well as a dedicated GNN pipeline trained on
$t\bar{t}$ events with pile-up 200 in the OpenDataDetector.
\\ ( https://arxiv.org/abs/2401.16016 ,  739kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16039 (*cross-listing*)
Date: Mon, 29 Jan 2024 10:47:37 GMT   (14816kb,D)

Title: Data-Driven Filter Design in FBP: Transforming CT Reconstruction with
  Trainable Fourier Series
Authors: Yipeng Sun, Linda-Sophie Schneider, Fuxin Fan, Mareike Thies, Mingxuan
  Gu, Siyuan Mei, Yuzhong Zhou, Siming Bayer, Andreas Maier
Categories: eess.IV cs.CV cs.LG
Comments: Preprint
\\
  In this study, we introduce a Fourier series-based trainable filter for
computed tomography (CT) reconstruction within the filtered backprojection
(FBP) framework. This method overcomes the limitation in noise reduction,
inherent in conventional FBP methods, by optimizing Fourier series coefficients
to construct the filter. This method enables robust performance across
different resolution scales and maintains computational efficiency with minimal
increment for the trainable parameters compared to other deep learning
frameworks. Additionally, we propose Gaussian edge-enhanced (GEE) loss function
that prioritizes the $L_1$ norm of high-frequency magnitudes, effectively
countering the blurring problems prevalent in mean squared error (MSE)
approaches. The model's foundation in the FBP algorithm ensures excellent
interpretability, as it relies on a data-driven filter with all other
parameters derived through rigorous mathematical procedures. Designed as a
plug-and-play solution, our Fourier series-based filter can be easily
integrated into existing CT reconstruction models, making it a versatile tool
for a wide range of practical applications. Our research presents a robust and
scalable method that expands the utility of FBP in both medical and scientific
imaging.
\\ ( https://arxiv.org/abs/2401.16039 ,  14816kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16304 (*cross-listing*)
Date: Mon, 29 Jan 2024 17:04:32 GMT   (9984kb,D)

Title: Regressing Transformers for Data-efficient Visual Place Recognition
Authors: Mar\'ia Leyva-Vallina, Nicola Strisciuglio and Nicolai Petkov
Categories: cs.CV cs.LG
Comments: Accepted for publication in ICRA 2024
\\
  Visual place recognition is a critical task in computer vision, especially
for localization and navigation systems. Existing methods often rely on
contrastive learning: image descriptors are trained to have small distance for
similar images and larger distance for dissimilar ones in a latent space.
However, this approach struggles to ensure accurate distance-based image
similarity representation, particularly when training with binary pairwise
labels, and complex re-ranking strategies are required. This work introduces a
fresh perspective by framing place recognition as a regression problem, using
camera field-of-view overlap as similarity ground truth for learning. By
optimizing image descriptors to align directly with graded similarity labels,
this approach enhances ranking capabilities without expensive re-ranking,
offering data-efficient training and strong generalization across several
benchmark datasets.
\\ ( https://arxiv.org/abs/2401.16304 ,  9984kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16347 (*cross-listing*)
Date: Mon, 29 Jan 2024 17:53:25 GMT   (1221kb)

Title: Cross-Modal Coordination Across a Diverse Set of Input Modalities
Authors: Jorge S\'anchez and Rodrigo Laguna
Categories: cs.CV cs.LG cs.MM
\\
  Cross-modal retrieval is the task of retrieving samples of a given modality
by using queries of a different one. Due to the wide range of practical
applications, the problem has been mainly focused on the vision and language
case, e.g. text to image retrieval, where models like CLIP have proven
effective in solving such tasks. The dominant approach to learning such
coordinated representations consists of projecting them onto a common space
where matching views stay close and those from non-matching pairs are pushed
away from each other. Although this cross-modal coordination has been applied
also to other pairwise combinations, extending it to an arbitrary number of
diverse modalities is a problem that has not been fully explored in the
literature. In this paper, we propose two different approaches to the problem.
The first is based on an extension of the CLIP contrastive objective to an
arbitrary number of input modalities, while the second departs from the
contrastive formulation and tackles the coordination problem by regressing the
cross-modal similarities towards a target that reflects two simple and
intuitive constraints of the cross-modal retrieval task. We run experiments on
two different datasets, over different combinations of input modalities and
show that the approach is not only simple and effective but also allows for
tackling the retrieval problem in novel ways. Besides capturing a more diverse
set of pair-wise interactions, we show that we can use the learned
representations to improve retrieval performance by combining the embeddings
from two or more such modalities.
\\ ( https://arxiv.org/abs/2401.16347 ,  1221kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16356 (*cross-listing*)
Date: Mon, 29 Jan 2024 17:59:26 GMT   (3781kb,D)

Title: cDVGAN: One Flexible Model for Multi-class Gravitational Wave Signal and
  Glitch Generation
Authors: Tom Dooney, Lyana Curier, Daniel Tan, Melissa Lopez, Chris Van Den
  Broeck, Stefano Bromuri
Categories: physics.ins-det cs.LG gr-qc
Comments: 18 pages, 16 figures, 6 tables
\\
  Simulating realistic time-domain observations of gravitational waves (GWs)
and GW detector glitches can help in advancing GW data analysis. Simulated data
can be used in downstream tasks by augmenting datasets for signal searches,
balancing data sets for machine learning, and validating detection schemes. In
this work, we present Conditional Derivative GAN (cDVGAN), a novel conditional
model in the Generative Adversarial Network framework for simulating multiple
classes of time-domain observations that represent gravitational waves (GWs)
and detector glitches. cDVGAN can also generate generalized hybrid samples that
span the variation between classes through interpolation in the conditioned
class vector. cDVGAN introduces an additional player into the typical 2-player
adversarial game of GANs, where an auxiliary discriminator analyzes the
first-order derivative time-series. Our results show that this provides
synthetic data that better captures the features of the original data. cDVGAN
conditions on three classes, two denoised from LIGO blip and tomte glitch
events from its 3rd observing run (O3), and the third representing binary black
hole (BBH) mergers. Our proposed cDVGAN outperforms 4 different baseline GAN
models in replicating the features of the three classes. Specifically, our
experiments show that training convolutional neural networks (CNNs) with our
cDVGAN-generated data improves the detection of samples embedded in detector
noise beyond the synthetic data from other state-of-the-art GAN models. Our
best synthetic dataset yields as much as a 4.2% increase in
area-under-the-curve (AUC) performance compared to synthetic datasets from
baseline GANs. Moreover, training the CNN with hybrid samples from our cDVGAN
outperforms CNNs trained only on the standard classes, when identifying real
samples embedded in LIGO detector background (4% AUC improvement for cDVGAN).
\\ ( https://arxiv.org/abs/2401.16356 ,  3781kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16407 (*cross-listing*)
Date: Mon, 29 Jan 2024 18:46:53 GMT   (8927kb,D)

Title: Is K-fold cross validation the best model selection method for Machine
  Learning?
Authors: Juan M Gorriz, F Segovia, J Ramirez, A Ortiz and J. Suckling
Categories: stat.ML cs.LG eess.IV eess.SP
Comments: 36 pages, 24 figures
\\
  As a technique that can compactly represent complex patterns, machine
learning has significant potential for predictive inference. K-fold
cross-validation (CV) is the most common approach to ascertaining the
likelihood that a machine learning outcome is generated by chance and
frequently outperforms conventional hypothesis testing. This improvement uses
measures directly obtained from machine learning classifications, such as
accuracy, that do not have a parametric description. To approach a frequentist
analysis within machine learning pipelines, a permutation test or simple
statistics from data partitions (i.e. folds) can be added to estimate
confidence intervals. Unfortunately, neither parametric nor non-parametric
tests solve the inherent problems around partitioning small sample-size
datasets and learning from heterogeneous data sources. The fact that machine
learning strongly depends on the learning parameters and the distribution of
data across folds recapitulates familiar difficulties around excess false
positives and replication. The origins of this problem are demonstrated by
simulating common experimental circumstances, including small sample sizes, low
numbers of predictors, and heterogeneous data sources. A novel statistical test
based on K-fold CV and the Upper Bound of the actual error (K-fold CUBV) is
composed, where uncertain predictions of machine learning with CV are bounded
by the \emph{worst case} through the evaluation of concentration inequalities.
Probably Approximately Correct-Bayesian upper bounds for linear classifiers in
combination with K-fold CV is used to estimate the empirical error. The
performance with neuroimaging datasets suggests this is a robust criterion for
detecting effects, validating accuracy values obtained from machine learning
whilst avoiding excess false positives.
\\ ( https://arxiv.org/abs/2401.16407 ,  8927kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16410 (*cross-listing*)
Date: Mon, 29 Jan 2024 18:47:36 GMT   (2056kb,D)

Title: ReTaSA: A Nonparametric Functional Estimation Approach for Addressing
  Continuous Target Shift
Authors: Hwanwoo Kim, Xin Zhang, Jiwei Zhao, Qinglong Tian
Categories: stat.ML cs.LG
Comments: Accepted by ICLR 2024
\\
  The presence of distribution shifts poses a significant challenge for
deploying modern machine learning models in real-world applications. This work
focuses on the target shift problem in a regression setting (Zhang et al.,
2013; Nguyen et al., 2016). More specifically, the target variable y (also
known as the response variable), which is continuous, has different marginal
distributions in the training source and testing domain, while the conditional
distribution of features x given y remains the same. While most literature
focuses on classification tasks with finite target space, the regression
problem has an infinite dimensional target space, which makes many of the
existing methods inapplicable. In this work, we show that the continuous target
shift problem can be addressed by estimating the importance weight function
from an ill-posed integral equation. We propose a nonparametric regularized
approach named ReTaSA to solve the ill-posed integral equation and provide
theoretical justification for the estimated importance weight function. The
effectiveness of the proposed method has been demonstrated with extensive
numerical studies on synthetic and real-world datasets.
\\ ( https://arxiv.org/abs/2401.16410 ,  2056kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16418 (*cross-listing*)
Date: Mon, 29 Jan 2024 18:56:21 GMT   (10kb)

Title: Boolean Logic as an Error feedback mechanism
Authors: Louis Leconte
Categories: stat.ML cs.LG
\\
  The notion of Boolean logic backpropagation was introduced to build neural
networks with weights and activations being Boolean numbers. Most of
computations can be done with Boolean logic instead of real arithmetic, both
during training and inference phases. But the underlying discrete optimization
problem is NP-hard, and the Boolean logic has no guarantee. In this work we
propose the first convergence analysis, under standard non-convex assumptions.
\\ ( https://arxiv.org/abs/2401.16418 ,  10kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16423 (*cross-listing*)
Date: Mon, 29 Jan 2024 18:59:55 GMT   (488kb,D)

Title: Synchformer: Efficient Synchronization from Sparse Cues
Authors: Vladimir Iashin, Weidi Xie, Esa Rahtu, Andrew Zisserman
Categories: cs.CV cs.LG cs.MM cs.SD eess.AS
Comments: Extended version of the ICASSP 24 paper. Project page:
  https://www.robots.ox.ac.uk/~vgg/research/synchformer/ Code:
  https://github.com/v-iashin/Synchformer
\\
  Our objective is audio-visual synchronization with a focus on 'in-the-wild'
videos, such as those on YouTube, where synchronization cues can be sparse. Our
contributions include a novel audio-visual synchronization model, and training
that decouples feature extraction from synchronization modelling through
multi-modal segment-level contrastive pre-training. This approach achieves
state-of-the-art performance in both dense and sparse settings. We also extend
synchronization model training to AudioSet a million-scale 'in-the-wild'
dataset, investigate evidence attribution techniques for interpretability, and
explore a new capability for synchronization models: audio-visual
synchronizability.
\\ ( https://arxiv.org/abs/2401.16423 ,  488kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2009.07497
replaced with revised version Sun, 28 Jan 2024 12:17:24 GMT   (63kb)

Title: One head is better than two: a polynomial restriction for propositional
  definite Horn forgetting
Authors: Paolo Liberatore
Categories: cs.AI
\\ ( https://arxiv.org/abs/2009.07497 ,  63kb)
------------------------------------------------------------------------------
\\
arXiv:2208.10469
replaced with revised version Mon, 29 Jan 2024 16:37:55 GMT   (6978kb,D)

Title: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL
Authors: Andreas A. Haupt, Phillip J.K. Christoffersen, Mehul Damani, Dylan
  Hadfield-Menell
Categories: cs.AI cs.GT cs.MA econ.TH
\\ ( https://arxiv.org/abs/2208.10469 ,  6978kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06582
replaced with revised version Sat, 27 Jan 2024 19:29:15 GMT   (897kb)

Title: A Convex Hull Cheapest Insertion Heuristic for the Non-Euclidean TSP
Authors: Mithun Goutham, Meghna Menon, Sarah Garrow and Stephanie Stockar
Categories: cs.AI cs.SY eess.SY
Comments: Manuscript submitted 27 January 2024 to the Operations Research
  Letters
\\ ( https://arxiv.org/abs/2302.06582 ,  897kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10096
replaced with revised version Sun, 28 Jan 2024 00:56:10 GMT   (22kb)

Title: Similarity
Authors: Christian Anti\'c
Categories: cs.AI cs.LO
\\ ( https://arxiv.org/abs/2302.10096 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09452
replaced with revised version Sat, 27 Jan 2024 04:59:00 GMT   (2179kb)

Title: A sequential transit network design algorithm with optimal learning
  under correlated beliefs
Authors: Gyugeun Yoon, Joseph Y. J. Chow
Categories: cs.AI cs.CY
\\ ( https://arxiv.org/abs/2305.09452 ,  2179kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08962
replaced with revised version Fri, 26 Jan 2024 20:47:11 GMT   (3249kb,D)

Title: REX: Rapid Exploration and eXploitation for AI Agents
Authors: Rithesh Murthy, Shelby Heinecke, Juan Carlos Niebles, Zhiwei Liu, Le
  Xue, Weiran Yao, Yihao Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit, Ran Xu,
  Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2307.08962 ,  3249kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07745
replaced with revised version Sat, 27 Jan 2024 12:43:46 GMT   (2785kb,D)

Title: Simplifying Complex Observation Models in Continuous POMDP Planning with
  Probabilistic Guarantees and Practice
Authors: Idan Lev-Yehudi, Moran Barenboim, Vadim Indelman
Categories: cs.AI cs.RO
\\ ( https://arxiv.org/abs/2311.07745 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10572
replaced with revised version Sat, 27 Jan 2024 22:57:51 GMT   (312kb,D)

Title: Improved Anonymous Multi-Agent Path Finding Algorithm
Authors: Zain Alabedeen Ali and Konstantin Yakovlev
Categories: cs.AI cs.MA
Comments: Accepted at AAAI24
\\ ( https://arxiv.org/abs/2312.10572 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01836
replaced with revised version Mon, 29 Jan 2024 02:24:09 GMT   (3191kb,D)

Title: Neural Optimal Control: Concurrent System Identification and Control
  Learning with Neural ODE
Authors: Cheng Chi
Categories: cs.AI
Comments: 9 pages, code open sourced in format of Google Colab notebooks;
  Resubmitted for adding missed references in the first submission
\\ ( https://arxiv.org/abs/2401.01836 ,  3191kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06256
replaced with revised version Sat, 27 Jan 2024 19:13:03 GMT   (999kb)

Title: A Universal Knowledge Model and Cognitive Architecture for Prototyping
  AGI
Authors: Artem Sukhobokov, Evgeny Belousov, Danila Gromozdov, Anna Zenger and
  Ilya Popov
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.06256 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13112
replaced with revised version Sat, 27 Jan 2024 19:40:00 GMT   (820kb,D)

Title: DISCOUNT: Distributional Counterfactual Explanation With Optimal
  Transport
Authors: Lei You, Lele Cao, Mattias Nilsson
Categories: cs.AI stat.ML
Comments: Under review in ICML 2024
\\ ( https://arxiv.org/abs/2401.13112 ,  820kb)
------------------------------------------------------------------------------
\\
arXiv:2203.08565
replaced with revised version Sun, 28 Jan 2024 22:57:45 GMT   (8215kb,D)

Title: Geographic Adaptation of Pretrained Language Models
Authors: Valentin Hofmann, Goran Glava\v{s}, Nikola Ljube\v{s}i\'c, Janet B.
  Pierrehumbert, Hinrich Sch\"utze
Categories: cs.CL
Comments: TACL 2024 (pre-MIT Press publication version)
\\ ( https://arxiv.org/abs/2203.08565 ,  8215kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14570
replaced with revised version Mon, 29 Jan 2024 03:58:35 GMT   (155kb,D)

Title: MiniDisc: Minimal Distillation Schedule for Language Model Compression
Authors: Chen Zhang, Yang Yang, Qifan Wang, Jiahao Liu, Jingang Wang, Wei Wu,
  Dawei Song
Categories: cs.CL cs.LG
Comments: Accepted to EACL 2024. Code is available at
  https://github.com/GeneZC/MiniDisc
\\ ( https://arxiv.org/abs/2205.14570 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2207.01079
replaced with revised version Sun, 28 Jan 2024 21:14:26 GMT   (6213kb,D)

Title: DiSCoMaT: Distantly Supervised Composition Extraction from Tables in
  Materials Science Articles
Authors: Tanishq Gupta, Mohd Zaki, Devanshi Khatsuriya, Kausik Hira, N. M.
  Anoop Krishnan, Mausam
Categories: cs.CL cond-mat.mtrl-sci cs.IR
Comments: Accepted long paper at ACL 2023
  (https://2023.aclweb.org/program/accepted_main_conference/)
\\ ( https://arxiv.org/abs/2207.01079 ,  6213kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07661
replaced with revised version Sat, 27 Jan 2024 08:07:34 GMT   (1568kb,D)

Title: On the Relation between Sensitivity and Accuracy in In-context Learning
Authors: Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, He He
Categories: cs.CL cs.AI cs.LG
Comments: EMNLP 2023 camera-ready
\\ ( https://arxiv.org/abs/2209.07661 ,  1568kb)
------------------------------------------------------------------------------
\\
arXiv:2210.12273
replaced with revised version Mon, 29 Jan 2024 13:03:25 GMT   (398kb)

Title: Graphemic Normalization of the Perso-Arabic Script
Authors: Raiomond Doctor and Alexander Gutkin and Cibu Johny and Brian Roark
  and Richard Sproat
Categories: cs.CL
Comments: Pre-print to appear in the Proceedings of Grapholinguistics in the
  21st Century (G21C), 2022. Telecom Paris, Palaiseau, France, June 8-10, 2022.
  41 pages, 38 tables, 3 figures
ACM-class: I.2.7; I.7.2; I.7.1
\\ ( https://arxiv.org/abs/2210.12273 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09949
replaced with revised version Sat, 27 Jan 2024 03:40:26 GMT   (1064kb,D)

Title: Compressing Transformer-based self-supervised models for speech
  processing
Authors: Tzu-Quan Lin, Tsung-Huan Yang, Chun-Yao Chang, Kuang-Ming Chen,
  Tzu-hsun Feng, Hung-yi Lee, Hao Tang
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: Submitted to IEEE Transactions on Audio, Speech and Language
  Processing (TASLP)
\\ ( https://arxiv.org/abs/2211.09949 ,  1064kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12132
replaced with revised version Mon, 29 Jan 2024 10:41:51 GMT   (797kb,D)

Title: AutoPEFT: Automatic Configuration Search for Parameter-Efficient
  Fine-Tuning
Authors: Han Zhou, Xingchen Wan, Ivan Vuli\'c, Anna Korhonen
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to TACL; pre-MIT Press publication version
\\ ( https://arxiv.org/abs/2301.12132 ,  797kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02759
replaced with revised version Mon, 29 Jan 2024 16:59:09 GMT   (638kb)

Title: Detecting Reddit Users with Depression Using a Hybrid Neural Network
  SBERT-CNN
Authors: Ziyi Chen, Ren Yang, Sunyang Fu, Nansu Zong, Hongfang Liu, Ming Huang
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2302.02759 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08957
replaced with revised version Mon, 29 Jan 2024 12:39:57 GMT   (25382kb,D)

Title: Like a Good Nearest Neighbor: Practical Content Moderation and Text
  Classification
Authors: Luke Bates and Iryna Gurevych
Categories: cs.CL
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2302.08957 ,  25382kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01295
replaced with revised version Sat, 27 Jan 2024 03:47:55 GMT   (8422kb,D)

Title: Efficiently Aligned Cross-Lingual Transfer Learning for Conversational
  Tasks using Prompt-Tuning
Authors: Lifu Tu, Jin Qu, Semih Yavuz, Shafiq Joty, Wenhao Liu, Caiming Xiong,
  Yingbo Zhou
Categories: cs.CL cs.AI
Comments: Accepted to the Finding of the ACL: EACL 2024
\\ ( https://arxiv.org/abs/2304.01295 ,  8422kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14402
replaced with revised version Mon, 29 Jan 2024 02:58:23 GMT   (11383kb,D)

Title: LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale
  Instructions
Authors: Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, Alham
  Fikri Aji
Categories: cs.CL
Comments: 21 pages, 8 figures, 17 tables, accepted by EACL2024 main conference
\\ ( https://arxiv.org/abs/2304.14402 ,  11383kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05644
replaced with revised version Mon, 29 Jan 2024 17:13:04 GMT   (1188kb,D)

Title: Towards Building the Federated GPT: Federated Instruction Tuning
Authors: Jianyi Zhang, Saeed Vahidian, Martin Kuo, Chunyuan Li, Ruiyi Zhang,
  Tong Yu, Yufan Zhou, Guoyin Wang, Yiran Chen
Categories: cs.CL cs.DC cs.SY eess.SY
Comments: Project page: https://github.com/JayZhang42/FederatedGPT-Shepherd
\\ ( https://arxiv.org/abs/2305.05644 ,  1188kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07984
replaced with revised version Sat, 27 Jan 2024 09:16:14 GMT   (6987kb,D)

Title: SCENE: Self-Labeled Counterfactuals for Extrapolating to Negative
  Examples
Authors: Deqing Fu, Ameya Godbole, Robin Jia
Categories: cs.CL
Comments: EMNLP 2023
\\ ( https://arxiv.org/abs/2305.07984 ,  6987kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10163
replaced with revised version Mon, 29 Jan 2024 03:25:59 GMT   (1085kb,D)

Title: Large Language Models Leverage External Knowledge to Extend Clinical
  Insight Beyond Language Boundaries
Authors: Jiageng Wu, Xian Wu, Zhaopeng Qiu, Minghui Li, Yingying Zhang, Yefeng
  Zheng, and Jie Yang
Categories: cs.CL cs.AI cs.CY
\\ ( https://arxiv.org/abs/2305.10163 ,  1085kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11789
replaced with revised version Sun, 28 Jan 2024 08:50:06 GMT   (426kb,D)

Title: Solving NLP Problems through Human-System Collaboration: A
  Discussion-based Approach
Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.11789 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12924
replaced with revised version Sat, 27 Jan 2024 13:12:43 GMT   (8557kb,D)

Title: EnCore: Fine-Grained Entity Typing by Pre-Training Entity Encoders on
  Coreference Chains
Authors: Frank Mtumbuka and Steven Schockaert
Categories: cs.CL
Comments: To appear at EACL 2024
\\ ( https://arxiv.org/abs/2305.12924 ,  8557kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13521
replaced with revised version Sat, 27 Jan 2024 21:06:23 GMT   (11016kb,D)

Title: CEO: Corpus-based Open-Domain Event Ontology Induction
Authors: Nan Xu, Hongming Zhang, Jianshu Chen
Categories: cs.CL
Comments: EACL 2024 (Findings)
\\ ( https://arxiv.org/abs/2305.13521 ,  11016kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13684
replaced with revised version Mon, 29 Jan 2024 09:03:43 GMT   (254kb,D)

Title: mPLM-Sim: Better Cross-Lingual Similarity and Transfer in Multilingual
  Pretrained Language Models
Authors: Peiqin Lin, Chengzhi Hu, Zheyu Zhang, Andr\'e F. T. Martins, Hinrich
  Sch\"utze
Categories: cs.CL
Comments: EACL 2024 Findings
\\ ( https://arxiv.org/abs/2305.13684 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16128
replaced with revised version Sat, 27 Jan 2024 16:43:52 GMT   (1263kb,D)

Title: Give Me More Details: Improving Fact-Checking with Latent Retrieval
Authors: Xuming Hu, Junzhe Chen, Zhijiang Guo, Philip S. Yu
Categories: cs.CL
Comments: Fixed minor issues, 11 pages
\\ ( https://arxiv.org/abs/2305.16128 ,  1263kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00168
replaced with revised version Sun, 28 Jan 2024 13:06:38 GMT   (1230kb,D)

Title: Measuring the Robustness of NLP Models to Domain Shifts
Authors: Nitay Calderon, Naveh Porat, Eyal Ben-David, Alexander Chapanin, Zorik
  Gekhman, Nadav Oved, Vitaly Shalumov, Roi Reichart
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.00168 ,  1230kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12896
replaced with revised version Sun, 28 Jan 2024 17:49:35 GMT   (806kb,D)

Title: Corrections of Zipf's and Heaps' Laws Derived from Hapax Rate Models
Authors: {\L}ukasz D\k{e}bowski
Categories: cs.CL stat.AP
Comments: 61 pages, 29 figures, 3 tables
MSC-class: 62P99
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2307.12896 ,  806kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14385
replaced with revised version Sun, 28 Jan 2024 16:54:03 GMT   (4073kb,D)

Title: Mental-LLM: Leveraging Large Language Models for Mental Health
  Prediction via Online Text Data
Authors: Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James
  Hendler, Marzyeh Ghassemi, Anind K. Dey, Dakuo Wang
Categories: cs.CL
Comments: Published at Proceedings of the ACM on Interactive, Mobile, Wearable
  and Ubiquitous Technologies (IMWUT) 2024
MSC-class: 68U35
ACM-class: H.5.2; I.2.m
DOI: 10.1145/3643540
\\ ( https://arxiv.org/abs/2307.14385 ,  4073kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08628
replaced with revised version Mon, 29 Jan 2024 18:00:02 GMT   (11764kb,D)

Title: Learning the meanings of function words from grounded language using a
  visual question answering model
Authors: Eva Portelance and Michael C. Frank and Dan Jurafsky
Categories: cs.CL
ACM-class: I.2.7; I.2.6; I.2.10
\\ ( https://arxiv.org/abs/2308.08628 ,  11764kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10335
replaced with revised version Sat, 27 Jan 2024 05:49:55 GMT   (4322kb,D)

Title: Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability
  of Large Language Model Code Generation
Authors: Li Zhong, Zilong Wang
Categories: cs.CL cs.AI cs.SE
\\ ( https://arxiv.org/abs/2308.10335 ,  4322kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04174
replaced with revised version Mon, 29 Jan 2024 12:31:57 GMT   (370kb,D)

Title: Manifold-based Verbalizer Space Re-embedding for Tuning-free
  Prompt-based Classification
Authors: Haochun Wang, Sendong Zhao, Chi Liu, Nuwa Xi, Muzhen Cai, Bing Qin,
  Ting Liu
Categories: cs.CL cs.AI
Comments: Accepted by AAAI 2024, 11 pages, 3 figures
\\ ( https://arxiv.org/abs/2309.04174 ,  370kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07098
replaced with revised version Mon, 29 Jan 2024 09:08:39 GMT   (73kb,D)

Title: Mitigating Hallucinations and Off-target Machine Translation with
  Source-Contrastive and Language-Contrastive Decoding
Authors: Rico Sennrich and Jannis Vamvas and Alireza Mohammadshahi
Categories: cs.CL
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2309.07098 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08609
replaced with revised version Sat, 27 Jan 2024 09:08:08 GMT   (2812kb,D)

Title: Media of Langue: The dictionary that visualizes Inter-Lingual Semantic
  Network/Space
Authors: Goki Muramoto, Atsuki Sato, Takayoshi Koyama
Categories: cs.CL cs.HC
\\ ( https://arxiv.org/abs/2309.08609 ,  2812kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13340
replaced with revised version Mon, 29 Jan 2024 05:59:12 GMT   (1851kb,D)

Title: Towards LLM-guided Causal Explainability for Black-box Text Classifiers
Authors: Amrita Bhattacharjee, Raha Moraffah, Joshua Garland, Huan Liu
Categories: cs.CL cs.AI cs.LG
Comments: Camera-ready for AAAI ReLM 2024
\\ ( https://arxiv.org/abs/2309.13340 ,  1851kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00863
replaced with revised version Mon, 29 Jan 2024 02:15:08 GMT   (8695kb,D)

Title: Syllable-level lyrics generation from melody exploiting character-level
  language model
Authors: Zhe Zhang, Karol Lasocki, Yi Yu, Atsuhiro Takasu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.00863 ,  8695kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01801
replaced with revised version Mon, 29 Jan 2024 06:25:00 GMT   (1162kb,D)

Title: Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs
Authors: Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng
  Gao
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.01801 ,  1162kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02446
replaced with revised version Sat, 27 Jan 2024 22:54:52 GMT   (123kb,D)

Title: Low-Resource Languages Jailbreak GPT-4
Authors: Zheng-Xin Yong, Cristina Menghini and Stephen H. Bach
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: NeurIPS Workshop on Socially Responsible Language Modelling Research
  (SoLaR) 2023. Best Paper Award
\\ ( https://arxiv.org/abs/2310.02446 ,  123kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07276
replaced with revised version Mon, 29 Jan 2024 03:34:14 GMT   (651kb,D)

Title: BioT5: Enriching Cross-modal Integration in Biology with Chemical
  Knowledge and Natural Language Associations
Authors: Qizhi Pei, Wei Zhang, Jinhua Zhu, Kehan Wu, Kaiyuan Gao, Lijun Wu,
  Yingce Xia, Rui Yan
Categories: cs.CL cs.AI cs.LG q-bio.BM
Comments: Accepted by Empirical Methods in Natural Language Processing 2023
  (EMNLP 2023)
\\ ( https://arxiv.org/abs/2310.07276 ,  651kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08383
replaced with revised version Sun, 28 Jan 2024 15:05:54 GMT   (4748kb,D)

Title: Reconstructing Materials Tetrahedron: Challenges in Materials
  Information Extraction
Authors: Kausik Hira, Mohd Zaki, Dhruvil Sheth, Mausam, N M Anoop Krishnan
Categories: cs.CL cond-mat.mtrl-sci
\\ ( https://arxiv.org/abs/2310.08383 ,  4748kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14703
replaced with revised version Mon, 29 Jan 2024 09:26:36 GMT   (607kb)

Title: Establishing Vocabulary Tests as a Benchmark for Evaluating Large
  Language Models
Authors: Gonzalo Mart\'inez, Javier Conde, Elena Merino-G\'omez, Beatriz
  Berm\'udez-Margaretto, Jos\'e Alberto Hern\'andez, Pedro Reviriego, Marc
  Brysbaert
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.14703 ,  607kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17589
replaced with revised version Mon, 29 Jan 2024 02:11:01 GMT   (3015kb,D)

Title: An Open Source Data Contamination Report for Large Language Models
Authors: Yucheng Li, Frank Guerin, Chenghua Lin
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.17589 ,  3015kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04507
replaced with revised version Mon, 29 Jan 2024 04:14:40 GMT   (8267kb,D)

Title: Conversation Understanding using Relational Temporal Graph Neural
  Networks with Auxiliary Cross-Modality Interaction
Authors: Cam-Van Thi Nguyen, Anh-Tuan Mai, The-Son Le, Hai-Dang Kieu, Duc-Trong
  Le
Categories: cs.CL cs.MM
Comments: EMNLP 2023
Journal-ref: The 2023 Conference on Empirical Methods in Natural Language
  Processing
DOI: 10.18653/v1/2023.emnlp-main.937
\\ ( https://arxiv.org/abs/2311.04507 ,  8267kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04892
replaced with revised version Sat, 27 Jan 2024 08:49:29 GMT   (1093kb,D)

Title: Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs
Authors: Shashank Gupta, Vaishnavi Shrivastava, Ameet Deshpande, Ashwin Kalyan,
  Peter Clark, Ashish Sabharwal, Tushar Khot
Categories: cs.CL
Comments: Project page: https://allenai.github.io/persona-bias. Paper to appear
  at ICLR 2024. Added results for other LLMs in v2 (similar findings)
\\ ( https://arxiv.org/abs/2311.04892 ,  1093kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07536
replaced with revised version Sat, 27 Jan 2024 14:16:54 GMT   (21000kb,D)

Title: A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual
  Question Answering
Authors: Yunxin Li, Longyue Wang, Baotian Hu, Xinyu Chen, Wanqi Zhong, Chenyang
  Lyu, Wei Wang, Min Zhang
Categories: cs.CL
Comments: 18 pages, 13pages; working in progress
\\ ( https://arxiv.org/abs/2311.07536 ,  21000kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08724
replaced with revised version Sat, 27 Jan 2024 07:15:54 GMT   (319kb)

Title: Knowledge Graph Construction in Power Distribution Networks
Authors: Xiang Li, Che Wang, Bing Li, Hao Chen, Sizhe Li
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.08724 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09335
replaced with revised version Mon, 29 Jan 2024 17:59:30 GMT   (152kb,D)

Title: Investigating Hallucinations in Pruned Large Language Models for
  Abstractive Summarization
Authors: George Chrysostomou, Zhixue Zhao, Miles Williams, Nikolaos Aletras
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.09335 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13708
replaced with revised version Sat, 27 Jan 2024 07:32:54 GMT   (350kb)

Title: Dynamic Fault Analysis in Substations Based on Knowledge Graphs
Authors: Weiwei Li, Xing Liu, Wei Wang, Lu Chen, Sizhe Li, Hui Fan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.13708 ,  350kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03731
replaced with revised version Sun, 28 Jan 2024 15:54:00 GMT   (586kb,D)

Title: MultiGPrompt for Multi-Task Pre-Training and Prompting on Graphs
Authors: Xingtong Yu, Chang Zhou, Yuan Fang, Xinming Zhang
Categories: cs.CL cs.LG
Comments: Accepted by WWW2024
\\ ( https://arxiv.org/abs/2312.03731 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10048
replaced with revised version Sat, 27 Jan 2024 00:09:23 GMT   (520kb)

Title: Knowledge Graph Enhanced Aspect-Level Sentiment Analysis
Authors: Kavita Sharma, Ritu Patel, Sunita Iyer
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.10048 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15816
replaced with revised version Mon, 29 Jan 2024 04:39:02 GMT   (3784kb,D)

Title: TEILP: Time Prediction over Knowledge Graphs via Logical Reasoning
Authors: Siheng Xiong, Yuan Yang, Ali Payani, James C Kerce, Faramarz Fekri
Categories: cs.CL
Comments: AAAI24 (Oral)
\\ ( https://arxiv.org/abs/2312.15816 ,  3784kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02968
replaced with revised version Sat, 27 Jan 2024 20:54:46 GMT   (289kb)

Title: Rule-Guided Joint Embedding Learning over Knowledge Graphs
Authors: Qisong Li, Ji Lin, Sijia Wei, Neng Liu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.02968 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03591
replaced with revised version Sat, 27 Jan 2024 00:01:26 GMT   (516kb)

Title: Text Classification Based on Knowledge Graphs and Improved Attention
  Mechanism
Authors: Siyu Li, Lu Chen, Chenwei Song, Xinyi Liu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.03591 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06772
replaced with revised version Sat, 27 Jan 2024 20:56:20 GMT   (422kb)

Title: Semantic Parsing for Question Answering over Knowledge Graphs
Authors: Sijia Wei, Wenwen Zhang, Qisong Li, Jiang Zhao
Categories: cs.CL
Comments: arXiv admin note: text overlap with arXiv:2401.02968
\\ ( https://arxiv.org/abs/2401.06772 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10768
replaced with revised version Sun, 28 Jan 2024 10:18:51 GMT   (8015kb,D)

Title: Mitigating Hallucinations of Large Language Models via Knowledge
  Consistent Alignment
Authors: Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, Shuming
  Shi
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.10768 ,  8015kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11864
replaced with revised version Mon, 29 Jan 2024 10:53:36 GMT   (8638kb,D)

Title: Improving Small Language Models' Mathematical Reasoning via
  Equation-of-Thought Distillation
Authors: Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.11864 ,  8638kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13165
replaced with revised version Sat, 27 Jan 2024 17:12:32 GMT   (272kb)

Title: Misgendering and Assuming Gender in Machine Translation when Working
  with Low-Resource Languages
Authors: Sourojit Ghosh, Srishti Chatterjee
Categories: cs.CL
Comments: Upcoming Publication, Gendered Technology in Translation and
  Interpreting Centering Rights in the Development of Language Technology,
  Routledge 2024
\\ ( https://arxiv.org/abs/2401.13165 ,  272kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13275
replaced with revised version Sun, 28 Jan 2024 09:07:13 GMT   (313kb,D)

Title: Can AI Assistants Know What They Don't Know?
Authors: Qinyuan Cheng and Tianxiang Sun and Xiangyang Liu and Wenwei Zhang and
  Zhangyue Yin and Shimin Li and Linyang Li and Zhengfu He and Kai Chen and
  Xipeng Qiu
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.13275 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13565
replaced with revised version Mon, 29 Jan 2024 07:22:57 GMT   (118kb,D)

Title: Large Malaysian Language Model Based on Mistral for Enhanced Local
  Language Understanding
Authors: Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.13565 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13919
replaced with revised version Sun, 28 Jan 2024 07:57:21 GMT   (18186kb,D)

Title: WebVoyager: Building an End-to-End Web Agent with Large Multimodal
  Models
Authors: Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming
  Zhang, Zhenzhong Lan, Dong Yu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.13919 ,  18186kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14166
replaced with revised version Mon, 29 Jan 2024 08:51:01 GMT   (580kb,D)

Title: BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on
  Few-shot Inference via Debiased Domain Abstraction
Authors: Jiangmeng Li, Fei Song, Yifan Jin, Wenwen Qiang, Changwen Zheng,
  Fuchun Sun, Hui Xiong
Categories: cs.CL cs.AI
Comments: Accepted by ICLR2024
\\ ( https://arxiv.org/abs/2401.14166 ,  580kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14215
replaced with revised version Mon, 29 Jan 2024 06:06:21 GMT   (5822kb,D)

Title: Commonsense-augmented Memory Construction and Management in Long-term
  Conversations via Context-aware Persona Refinement
Authors: Hana Kim, Kai Tzu-iunn Ong, Seoyeon Kim, Dongha Lee, Jinyoung Yeo
Categories: cs.CL cs.AI
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2401.14215 ,  5822kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14680
replaced with revised version Mon, 29 Jan 2024 07:18:59 GMT   (884kb,D)

Title: MaLLaM -- Malaysia Large Language Model
Authors: Husein Zolkepli, Aisyah Razak, Kamarul Adha, Ariff Nazhan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.14680 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:1902.08412
replaced with revised version Sun, 28 Jan 2024 20:16:00 GMT   (637kb)

Title: Adversarial Attacks on Graph Neural Networks via Meta Learning
Authors: Daniel Z\"ugner, Stephan G\"unnemann
Categories: cs.LG cs.CR stat.ML
Comments: ICLR submission
Journal-ref: International Conference on Learning Representations (ICLR), New
  Orleans, LA, USA, 2019
\\ ( https://arxiv.org/abs/1902.08412 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2109.00783
replaced with revised version Fri, 26 Jan 2024 22:16:07 GMT   (10193kb,D)

Title: Computer Vision Self-supervised Learning Methods on Time Series
Authors: Daesoo Lee, Erlend Aune
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2109.00783 ,  10193kb)
------------------------------------------------------------------------------
\\
arXiv:2204.07767
replaced with revised version Sat, 27 Jan 2024 02:09:14 GMT   (4464kb,D)

Title: Towards cost-effective and resource-aware aggregation at Edge for
  Federated Learning
Authors: Ahmad Faraz Khan, Yuze Li, Xinran Wang, Sabaat Haroon, Haider Ali, Yue
  Cheng, Ali R. Butt, and Ali Anwar
Categories: cs.LG cs.DC
Comments: 10 pages, 12 figures, 4 tables This paper has been accepted at the
  2023 IEEE International Conference on Big Data (BigData)
DOI: 10.1109/BigData59044.2023.10386691
\\ ( https://arxiv.org/abs/2204.07767 ,  4464kb)
------------------------------------------------------------------------------
\\
arXiv:2206.02286
replaced with revised version Mon, 29 Jan 2024 01:38:59 GMT   (1466kb,D)

Title: AugLoss: A Robust Augmentation-based Fine Tuning Methodology
Authors: Kyle Otstot, Andrew Yang, John Kevin Cava, Lalitha Sankar
Categories: cs.LG cs.CV stat.ML
Comments: 10 pages, 6 figures, 6 tables
\\ ( https://arxiv.org/abs/2206.02286 ,  1466kb)
------------------------------------------------------------------------------
\\
arXiv:2206.03183
replaced with revised version Mon, 29 Jan 2024 10:01:15 GMT   (7125kb,D)

Title: Risk Measures and Upper Probabilities: Coherence and Stratification
Authors: Christian Fr\"ohlich and Robert C. Williamson
Categories: cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2206.03183 ,  7125kb)
------------------------------------------------------------------------------
\\
arXiv:2206.11396
replaced with revised version Mon, 29 Jan 2024 09:47:05 GMT   (1081kb,D)

Title: Multi-Horizon Representations with Hierarchical Forward Models for
  Reinforcement Learning
Authors: Trevor McInroe, Lukas Sch\"afer, Stefano V. Albrecht
Categories: cs.LG
Comments: Published in TMLR
\\ ( https://arxiv.org/abs/2206.11396 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2210.12288
replaced with revised version Fri, 26 Jan 2024 22:03:58 GMT   (4019kb,D)

Title: Learning Ultrametric Trees for Optimal Transport Regression
Authors: Samantha Chen, Puoya Tabaghi, Yusu Wang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2210.12288 ,  4019kb)
------------------------------------------------------------------------------
\\
arXiv:2211.06236
replaced with revised version Mon, 29 Jan 2024 14:17:03 GMT   (2031kb,D)

Title: Efficient Deep Reinforcement Learning with Predictive Processing
  Proximal Policy Optimization
Authors: Burcu K\"u\c{c}\"uko\u{g}lu, Walraaf Borkent, Bodo Rueckauer, Nasir
  Ahmad, Umut G\"u\c{c}l\"u and Marcel van Gerven
Categories: cs.LG cs.AI
Comments: 24 pages, 8 figures
\\ ( https://arxiv.org/abs/2211.06236 ,  2031kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11529
replaced with revised version Sat, 27 Jan 2024 12:01:57 GMT   (4912kb,D)

Title: Modular Deep Learning
Authors: Jonas Pfeiffer, Sebastian Ruder, Ivan Vuli\'c, Edoardo Maria Ponti
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.11529 ,  4912kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12094
replaced with revised version Mon, 29 Jan 2024 18:56:08 GMT   (6433kb,D)

Title: Evaluating explainability for machine learning predictions using
  model-agnostic metrics
Authors: Cristian Munoz, Kleyton da Costa, Bernardo Modenesi, Adriano Koshiyama
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2302.12094 ,  6433kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04488
replaced with revised version Mon, 29 Jan 2024 10:20:20 GMT   (991kb,D)

Title: Magnushammer: A Transformer-based Approach to Premise Selection
Authors: Maciej Miku{\l}a, Szymon Antoniak, Szymon Tworkowski, Albert Qiaochu
  Jiang, Jin Peng Zhou, Christian Szegedy, {\L}ukasz Kuci\'nski, Piotr
  Mi{\l}o\'s, Yuhuai Wu
Categories: cs.LG cs.AI cs.LO
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2303.04488 ,  991kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08845
replaced with revised version Sun, 28 Jan 2024 10:13:02 GMT   (4912kb,D)

Title: Feasible Policy Iteration
Authors: Yujie Yang, Zhilong Zheng, Shengbo Eben Li, Jingliang Duan, Jingjing
  Liu, Xianyuan Zhan, Ya-Qin Zhang
Categories: cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2304.08845 ,  4912kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12201
replaced with revised version Mon, 29 Jan 2024 18:15:48 GMT   (3134kb,D)

Title: GraVAC: Adaptive Compression for Communication-Efficient Distributed DL
  Training
Authors: Sahil Tyagi, Martin Swany
Categories: cs.LG
Journal-ref: Tyagi, S., & Swany, M. (2023). GraVAC: Adaptive Compression for
  Communication-Efficient Distributed DL Training. 2023 IEEE 16th International
  Conference on Cloud Computing (CLOUD), 319-329
DOI: 10.1109/CLOUD60044.2023.00045
\\ ( https://arxiv.org/abs/2305.12201 ,  3134kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19659
replaced with revised version Mon, 29 Jan 2024 09:14:53 GMT   (312kb,D)

Title: Improving Expressivity of Graph Neural Networks using Localization
Authors: Anant Kumar, Shrutimoy Das, Shubhajit Roy, Binita Maity, Anirban
  Dasgupta
Categories: cs.LG cs.DS
\\ ( https://arxiv.org/abs/2305.19659 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10882
replaced with revised version Sat, 27 Jan 2024 16:12:06 GMT   (1353kb,D)

Title: AdaStop: adaptive statistical testing for sound comparisons of Deep RL
  agents
Authors: Timoth\'ee Mathieu, Riccardo Della Vecchia, Alena Shilova, Matheus
  Medeiros Centa, Hector Kohler, Odalric-Ambrym Maillard, Philippe Preux
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2306.10882 ,  1353kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14023
replaced with revised version Mon, 29 Jan 2024 10:16:41 GMT   (346kb,D)

Title: Are Transformers with One Layer Self-Attention Using Low-Rank Weight
  Matrices Universal Approximators?
Authors: Tokio Kajitsuka and Issei Sato
Categories: cs.LG
Comments: ICLR 2024
MSC-class: 68T07
ACM-class: I.2.0
\\ ( https://arxiv.org/abs/2307.14023 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16348
replaced with revised version Mon, 29 Jan 2024 15:00:50 GMT   (1638kb,D)

Title: Rating-based Reinforcement Learning
Authors: Devin White, Mingkang Wu, Ellen Novoseller, Vernon J. Lawhern,
  Nicholas Waytowich, Yongcan Cao
Categories: cs.LG cs.AI cs.RO
Comments: This is an extended version of the paper "Rating-based Reinforcement
  Learning" accepted to the 38th Annual AAAI Conference on Artificial
  Intelligence
\\ ( https://arxiv.org/abs/2307.16348 ,  1638kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08334
replaced with revised version Mon, 29 Jan 2024 18:34:39 GMT   (118kb)

Title: Learning logic programs by discovering higher-order abstractions
Authors: C\'eline Hocquette, Sebastijan Duman\v{c}i\'c, Andrew Cropper
Categories: cs.LG cs.AI cs.PL
\\ ( https://arxiv.org/abs/2308.08334 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11787
replaced with revised version Sun, 28 Jan 2024 15:29:51 GMT   (3593kb,D)

Title: HypBO: Accelerating Black-Box Scientific Experiments Using Experts'
  Hypotheses
Authors: Abdoulatif Cisse, Xenophon Evangelopoulos, Sam Carruthers, Vladimir V.
  Gusev, Andrew I. Cooper
Categories: cs.LG cs.HC
\\ ( https://arxiv.org/abs/2308.11787 ,  3593kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13111
replaced with revised version Sun, 28 Jan 2024 12:23:21 GMT   (643kb,D)

Title: Bayesian Low-rank Adaptation for Large Language Models
Authors: Adam X. Yang, Maxime Robeyns, Xi Wang, Laurence Aitchison
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.13111 ,  643kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06054
replaced with revised version Mon, 29 Jan 2024 07:04:04 GMT   (1486kb,D)

Title: Breaking through the learning plateaus of in-context learning in
  Transformer
Authors: Jingwen Fu, Tao Yang, Yuwang Wang, Yan Lu, Nanning Zheng
Categories: cs.LG cs.CL cs.CV
\\ ( https://arxiv.org/abs/2309.06054 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11765
replaced with revised version Sun, 28 Jan 2024 00:24:10 GMT   (325kb,D)

Title: Privacy-Preserving In-Context Learning with Differentially Private
  Few-Shot Generation
Authors: Xinyu Tang, Richard Shin, Huseyin A. Inan, Andre Manoel, Fatemehsadat
  Mireshghallah, Zinan Lin, Sivakanth Gopi, Janardhan Kulkarni, Robert Sim
Categories: cs.LG cs.CR
\\ ( https://arxiv.org/abs/2309.11765 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13500
replaced with revised version Mon, 29 Jan 2024 01:11:48 GMT   (1679kb,D)

Title: Enhancing Student Performance Prediction on Learnersourced Questions
  with SGNN-LLM Synergy
Authors: Lin Ni, Sijie Wang, Zeyu Zhang, Xiaoxuan Li, Xianda Zheng, Paul Denny,
  and Jiamou Liu
Categories: cs.LG cs.AI
MSC-class: 97P80
\\ ( https://arxiv.org/abs/2309.13500 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14053
replaced with revised version Sun, 28 Jan 2024 11:01:35 GMT   (23695kb,D)

Title: Revisiting LARS for Large Batch Training Generalization of Neural
  Networks
Authors: Khoi Do, Duong Nguyen, Hoa Nguyen, Long Tran-Thanh, and Quoc-Viet Pham
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2309.14053 ,  23695kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16742
replaced with revised version Sat, 27 Jan 2024 13:03:27 GMT   (449kb,D)

Title: Supervised Learning Models for Early Detection of Albuminuria Risk in
  Type-2 Diabetes Mellitus Patients
Authors: Arief Purnama Muharram, Dicky Levenus Tahapary, Yeni Dwi Lestari,
  Randy Sarayar and Valerie Josephine Dirjayanto
Categories: cs.LG cs.AI q-bio.QM
Comments: Published in the 2023 10th International Conference on Advanced
  Informatics: Concept, Theory and Application (ICAICTA)
Journal-ref: 2023 10th International Conference on Advanced Informatics:
  Concept, Theory and Application (2023) 1-6
DOI: 10.1109/ICAICTA59291.2023.10390334
\\ ( https://arxiv.org/abs/2309.16742 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17194
replaced with revised version Sat, 27 Jan 2024 09:50:08 GMT   (1086kb,D)

Title: Generalized Activation via Multivariate Projection
Authors: Jiayun Li, Yuxiao Cheng, Yiwen Lu, Zhuofan Xia, Yilin Mo, Gao Huang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.17194 ,  1086kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01728
replaced with revised version Mon, 29 Jan 2024 06:27:53 GMT   (2737kb,D)

Title: Time-LLM: Time Series Forecasting by Reprogramming Large Language Models
Authors: Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y. Zhang, Xiaoming
  Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, Qingsong Wen
Categories: cs.LG cs.AI
Comments: Accepted by the 12th International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2310.01728 ,  2737kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05668
replaced with revised version Mon, 29 Jan 2024 05:59:51 GMT   (1730kb,D)

Title: LARA: A Light and Anti-overfitting Retraining Approach for Unsupervised
  Anomaly Detection
Authors: Feiyi Chen, Zhen Qin, Yingying Zhang, Shuiguang Deng, Yi Xiao,
  Guansong Pang and Qingsong Wen
Categories: cs.LG
Comments: Accepted by ACM Web Conference 2024 (WWW 24)
\\ ( https://arxiv.org/abs/2310.05668 ,  1730kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06639
replaced with revised version Fri, 26 Jan 2024 21:31:05 GMT   (25kb,D)

Title: The Lattice Overparametrization Paradigm for the Machine Learning of
  Lattice Operators
Authors: Diego Marcondes and Junior Barrera
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.06639 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10443
replaced with revised version Mon, 29 Jan 2024 17:14:01 GMT   (7410kb,D)

Title: Taming the Sigmoid Bottleneck: Provably Argmaxable Sparse Multi-Label
  Classification
Authors: Andreas Grivas and Antonio Vergari and Adam Lopez
Categories: cs.LG
Comments: Published at AAAI24
\\ ( https://arxiv.org/abs/2310.10443 ,  7410kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10705
replaced with revised version Fri, 26 Jan 2024 19:56:47 GMT   (2402kb)

Title: Empirical and Experimental Insights into Machine Learning-Based Defect
  Classification in Semiconductor Wafers
Authors: Kamal Taha
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.10705 ,  2402kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10818
replaced with revised version Sun, 28 Jan 2024 20:45:31 GMT   (2701kb,D)

Title: Uncertainty-aware transfer across tasks using hybrid model-based
  successor feature reinforcement learning
Authors: Parvin Malekzadeh, Ming Hou, and Konstantinos N. Plataniotis
Categories: cs.LG eess.SP
Comments: 40 pages
Journal-ref: Neurocomputing 530 (2023): 165-187
DOI: 10.1016/j.neucom.2023.01.076
\\ ( https://arxiv.org/abs/2310.10818 ,  2701kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13833
replaced with revised version Sat, 27 Jan 2024 22:10:39 GMT   (332kb,D)

Title: GraphMaker: Can Diffusion Models Generate Large Attributed Graphs?
Authors: Mufei Li, Eleonora Krea\v{c}i\'c, Vamsi K. Potluru, Pan Li
Categories: cs.LG cs.AI
Comments: Code available at https://github.com/Graph-COM/GraphMaker
\\ ( https://arxiv.org/abs/2310.13833 ,  332kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14526
replaced with revised version Sun, 28 Jan 2024 00:22:52 GMT   (601kb,D)

Title: Towards Zero Shot Learning in Restless Multi-armed Bandits
Authors: Yunfan Zhao, Nikhil Behari, Edward Hughes, Edwin Zhang, Dheeraj
  Nagaraj, Karl Tuyls, Aparna Taneja, Milind Tambe
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.14526 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20025
replaced with revised version Sun, 28 Jan 2024 15:04:34 GMT   (2300kb,D)

Title: GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with
  Learned Models
Authors: Mianchu Wang, Rui Yang, Xi Chen, Hao Sun, Giovanni Montana, Meng Fang
Categories: cs.LG cs.AI
Comments: Spotlight Presentation at Goal-conditioned Reinforcement Learning
  Workshop at NeurIPS, 2023
\\ ( https://arxiv.org/abs/2310.20025 ,  2300kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01927
replaced with revised version Sat, 27 Jan 2024 14:52:52 GMT   (8844kb,D)

Title: GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling
Authors: Tobias Katsch
Categories: cs.LG cs.AI cs.CL cs.DS
Comments: Minor updates: Clarified tensor shapes
\\ ( https://arxiv.org/abs/2311.01927 ,  8844kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08053
replaced with revised version Mon, 29 Jan 2024 09:08:28 GMT   (1295kb,D)

Title: ARQ for Active Learning at the Edge
Authors: Victor Croisfelt, Shashi Raj Pandey, Osvaldo Simeone and Petar
  Popovski
Categories: cs.LG
Comments: 6 pages, 4 figures, conference version, submitted to IEEE ICC 2024
\\ ( https://arxiv.org/abs/2311.08053 ,  1295kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08149
replaced with revised version Mon, 29 Jan 2024 06:35:31 GMT   (12790kb,D)

Title: Modeling Complex Disease Trajectories using Deep Generative Models with
  Semi-Supervised Latent Processes
Authors: C\'ecile Trottet, Manuel Sch\"urch, Ahmed Allam, Imon Barua, Liubov
  Petelytska, Oliver Distler, Anna-Maria Hoffmann-Vold, Michael Krauthammer,
  the EUSTAR collaborators
Categories: cs.LG stat.ML
Comments: Extended Abstract presented at Machine Learning for Health (ML4H)
  symposium 2023, December 10th, 2023, New Orleans, United States, 23 pages
\\ ( https://arxiv.org/abs/2311.08149 ,  12790kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14387
replaced with revised version Mon, 29 Jan 2024 00:53:58 GMT   (3217kb,D)

Title: Achieving Margin Maximization Exponentially Fast via Progressive Norm
  Rescaling
Authors: Mingze Wang, Zeping Min, Lei Wu
Categories: cs.LG math.OC
Comments: 38 pages
\\ ( https://arxiv.org/abs/2311.14387 ,  3217kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15480
replaced with revised version Sun, 28 Jan 2024 19:53:22 GMT   (1011kb)

Title: Automatic Time Signature Determination for New Scores Using Lyrics for
  Latent Rhythmic Structure
Authors: Callie C. Liao, Duoduo Liao, Jesse Guessford
Categories: cs.LG cs.AI cs.CL cs.MM cs.SD
Comments: Accepted by 2023 IEEE International Conference on Big Data (IEEE
  BigData 2023)
\\ ( https://arxiv.org/abs/2311.15480 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16522
replaced with revised version Sat, 27 Jan 2024 07:28:42 GMT   (321kb)

Title: Dynamic Fault Characteristics Evaluation in Power Grid
Authors: Hao Pei, Si Lin, Chuanfu Li, Che Wang, Haoming Chen, Sizhe Li
Categories: cs.LG cs.CL eess.SP
\\ ( https://arxiv.org/abs/2311.16522 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16877
replaced with revised version Sun, 28 Jan 2024 08:15:55 GMT   (1396kb,D)

Title: Imputation using training labels and classification via label imputation
Authors: Thu Nguyen, Tuan L. Vo, P{\aa}l Halvorsen, Michael A. Riegler
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.16877 ,  1396kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03905
replaced with revised version Sat, 27 Jan 2024 00:25:22 GMT   (402kb,D)

Title: A Pseudo-Semantic Loss for Autoregressive Models with Logical
  Constraints
Authors: Kareem Ahmed, Kai-Wei Chang, Guy Van den Broeck
Categories: cs.LG cs.AI cs.CL
Comments: Updated detoxification experiments; moved example toxic generations
  to Github and added link
\\ ( https://arxiv.org/abs/2312.03905 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04027
replaced with revised version Mon, 29 Jan 2024 02:17:38 GMT   (20kb)

Title: The sample complexity of multi-distribution learning
Authors: Binghui Peng
Categories: cs.LG cs.AI cs.DS stat.ML
\\ ( https://arxiv.org/abs/2312.04027 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06695
replaced with revised version Mon, 29 Jan 2024 16:08:12 GMT   (4722kb,D)

Title: Evolving Reservoirs for Meta Reinforcement Learning
Authors: Corentin L\'eger and Gautier Hamon and Eleni Nisioti and Xavier Hinaut
  and Cl\'ement Moulin-Frier
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2312.06695 ,  4722kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11456
replaced with revised version Sun, 28 Jan 2024 22:32:48 GMT   (3099kb,D)

Title: Iterative Preference Learning from Human Feedback: Bridging Theory and
  Practice for RLHF under KL-Constraint
Authors: Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan
  Jiang, Tong Zhang
Categories: cs.LG cs.AI stat.ML
Comments: 37 pages; mathematical foundation and practical algorithms of RLHF
\\ ( https://arxiv.org/abs/2312.11456 ,  3099kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11714
replaced with revised version Sun, 28 Jan 2024 11:45:45 GMT   (22085kb,D)

Title: Time-Transformer: Integrating Local and Global Features for Better Time
  Series Generation
Authors: Yuansan Liu, Sudanthi Wijewickrema, Ang Li, Christofer Bester, Stephen
  O'Leary, James Bailey
Categories: cs.LG cs.AI
Comments: 15 pages, 7 figures and 16 tables. SDM24
\\ ( https://arxiv.org/abs/2312.11714 ,  22085kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11730
replaced with revised version Sun, 28 Jan 2024 01:52:26 GMT   (1489kb,D)

Title: Stronger Graph Transformer with Regularized Attention Scores
Authors: Eugene Ku, Swetha Arunraj
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.11730 ,  1489kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15600
replaced with revised version Mon, 29 Jan 2024 09:14:30 GMT   (5965kb,D)

Title: Context-aware Communication for Multi-agent Reinforcement Learning
Authors: Xinran Li, Jun Zhang
Categories: cs.LG cs.MA
Comments: Accepted by the 23nd International Conference on Autonomous Agents
  and Multiagent Systems (AAMAS 2024)
\\ ( https://arxiv.org/abs/2312.15600 ,  5965kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16554
replaced with revised version Mon, 29 Jan 2024 12:27:56 GMT   (9457kb,D)

Title: A Theoretical Analysis of Efficiency Constrained Utility-Privacy
  Bi-Objective Optimization in Federated Learning
Authors: Hanlin Gu, Xinyuan Zhao, Gongxi Zhu, Yuxing Han, Yan Kang, Lixin Fan,
  Qiang Yang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2312.16554 ,  9457kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01801
replaced with revised version Sun, 28 Jan 2024 16:13:37 GMT   (828kb,D)

Title: A quatum inspired neural network for geometric modeling
Authors: Weitao Du, Shengchao Liu, Xuecang Zhang
Categories: cs.LG cs.AI physics.comp-ph
\\ ( https://arxiv.org/abs/2401.01801 ,  828kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04856
replaced with revised version Sat, 27 Jan 2024 17:42:19 GMT   (1245kb,D)

Title: A Good Score Does not Lead to A Good Generative Model
Authors: Sixu Li, Shi Chen, Qin Li
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2401.04856 ,  1245kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05578
replaced with revised version Sun, 28 Jan 2024 10:36:41 GMT   (1513kb)

Title: Fast Cerebral Blood Flow Analysis via Extreme Learning Machine
Authors: Xi Chen, Zhenya Zang, Xingda Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.05578 ,  1513kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05580
replaced with revised version Sun, 28 Jan 2024 10:37:14 GMT   (1285kb)

Title: Enhancing Blood Flow Assessment in Diffuse Correlation Spectroscopy: A
  Transfer Learning Approach with Noise Robustness Analysis
Authors: Xi Chen, Xingda Li
Categories: cs.LG eess.SP
\\ ( https://arxiv.org/abs/2401.05580 ,  1285kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08552
replaced with revised version Mon, 29 Jan 2024 04:44:46 GMT   (707kb,D)

Title: Explaining Time Series via Contrastive and Locally Sparse Perturbations
Authors: Zichuan Liu, Yingying Zhang, Tianchun Wang, Zefan Wang, Dongsheng Luo,
  Mengnan Du, Min Wu, Yi Wang, Chunlin Chen, Lunting Fan, Qingsong Wen
Categories: cs.LG cs.AI
Comments: Accepted by International Conference on Learning Representations
  (ICLR 2024)
\\ ( https://arxiv.org/abs/2401.08552 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11113
replaced with revised version Sat, 27 Jan 2024 02:05:41 GMT   (8042kb,D)

Title: SleepNet: Attention-Enhanced Robust Sleep Prediction using Dynamic
  Social Networks
Authors: Maryam Khalid, Elizabeth B. Klerman, Andrew W. Mchill, Andrew J. K.
  Phillips, Akane Sano
Categories: cs.LG cs.AI cs.SI eess.SP
Comments: Accepted for publication in Proceedings of the ACM on Interactive,
  Mobile, Wearable and Ubiquitous Technologies (IMWUT), 8 (March 2024)
\\ ( https://arxiv.org/abs/2401.11113 ,  8042kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11648
replaced with revised version Sun, 28 Jan 2024 10:39:36 GMT   (6826kb,D)

Title: Next Visit Diagnosis Prediction via Medical Code-Centric Multimodal
  Contrastive EHR Modelling with Hierarchical Regularisation
Authors: Heejoon Koo
Categories: cs.LG cs.AI cs.IR
Comments: Accepted to EACL 2024 (The 18th Conference of the European Chapter of
  the Association for Computational Linguistics)
\\ ( https://arxiv.org/abs/2401.11648 ,  6826kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11798
replaced with revised version Sun, 28 Jan 2024 06:00:23 GMT   (402kb,D)

Title: Knowledge Distillation on Spatial-Temporal Graph Convolutional Network
  for Traffic Prediction
Authors: Mohammad Izadi, Mehran Safayani, Abdolreza Mirzaei
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.11798 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12007
replaced with revised version Mon, 29 Jan 2024 01:54:36 GMT   (606kb,D)

Title: Tensor-view Topological Graph Neural Network
Authors: Tao Wen, Elynn Chen, Yuzhou Chen
Categories: cs.LG cs.AI
Comments: Accepted at AISTATS 2024
\\ ( https://arxiv.org/abs/2401.12007 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12012
replaced with revised version Mon, 29 Jan 2024 17:55:56 GMT   (10103kb,D)

Title: TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for
  Lazy Clients
Authors: Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci
Categories: cs.LG cs.DC
Comments: Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
Journal-ref: Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)
\\ ( https://arxiv.org/abs/2401.12012 ,  10103kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13034
replaced with revised version Sat, 27 Jan 2024 07:21:37 GMT   (2525kb,D)

Title: Locality Sensitive Sparse Encoding for Learning World Models Online
Authors: Zichen Liu, Chao Du, Wee Sun Lee, Min Lin
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2401.13034 ,  2525kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13098
replaced with revised version Mon, 29 Jan 2024 16:43:03 GMT   (5662kb,D)

Title: Gravity-Informed Deep Learning Framework for Predicting Ship Traffic
  Flow and Invasion Risk of Non-Indigenous Species via Ballast Water Discharge
Authors: Ruixin Song, Gabriel Spadon, Ronald Pelot, Stan Matwin, Amilcar Soares
Categories: cs.LG cs.AI cs.SI stat.AP
Comments: 26 pages, 7 figures, under review
\\ ( https://arxiv.org/abs/2401.13098 ,  5662kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14211
replaced with revised version Mon, 29 Jan 2024 13:12:13 GMT   (478kb,D)

Title: Communication-Efficient Federated Learning through Adaptive Weight
  Clustering and Server-Side Distillation
Authors: Vasileios Tsouvalas, Aaqib Saeed, Tanir Ozcelebi and Nirvana Meratnia
Categories: cs.LG cs.DC
Comments: 9 pages, 2 figures, Accepted on ICASSP 2024
\\ ( https://arxiv.org/abs/2401.14211 ,  478kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14424
replaced with revised version Mon, 29 Jan 2024 09:07:17 GMT   (1612kb,D)

Title: Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo
  Tree Search
Authors: Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Meilan
  Hao, Shu Wei, Yusong Deng
Categories: cs.LG cs.AI
Comments: 24 pages
\\ ( https://arxiv.org/abs/2401.14424 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14521
replaced with revised version Mon, 29 Jan 2024 05:25:41 GMT   (7673kb)

Title: Towards Interpretable Physical-Conceptual Catchment-Scale Hydrological
  Modeling using the Mass-Conserving-Perceptron
Authors: Yuan-Heng Wang, Hoshin V. Gupta
Categories: cs.LG cs.AI
Comments: 50 pages, 7 Figures, 2 Tables, 1 Supplementary Material
\\ ( https://arxiv.org/abs/2401.14521 ,  7673kb)
------------------------------------------------------------------------------
\\
arXiv:1611.06189
replaced with revised version Sat, 27 Jan 2024 15:16:16 GMT   (26kb)

Title: Query Complexity of Tournament Solutions
Authors: Arnab Maiti and Palash Dey
Categories: cs.DS cs.AI cs.DM
Comments: Short version appeared in AAAI. Full version with new results will
  appear in Theoretical Computer Science journal
\\ ( https://arxiv.org/abs/1611.06189 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2203.00999 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 05:34:37 GMT   (731kb,D)

Title: DeepAutoPIN: An automorphism orbits based deep neural network for
  characterizing the organizational diversity of protein interactomes across
  the tree of life
Authors: Vikram Singh and Vikram Singh
Categories: q-bio.MN cs.AI q-bio.BM
Comments: 29 pages, 4 figures, 1 algorithm, 2 supplementary files
\\ ( https://arxiv.org/abs/2203.00999 ,  731kb)
------------------------------------------------------------------------------
\\
arXiv:2209.03563
replaced with revised version Mon, 29 Jan 2024 14:25:13 GMT   (15001kb,D)

Title: SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by
  Self-supervised Learning
Authors: Peizhuo Lv, Pan Li, Shenchen Zhu, Shengzhi Zhang, Kai Chen, Ruigang
  Liang, Chang Yue, Fan Xiang, Yuling Cai, Hualong Ma, Yingjun Zhang, Guozhu
  Meng
Categories: cs.CR cs.AI
Comments: To Appear in the Network and Distributed System Security (NDSS)
  Symposium 2024, 26 February - 1 March 2024, San Diego, CA, USA
\\ ( https://arxiv.org/abs/2209.03563 ,  15001kb)
------------------------------------------------------------------------------
\\
arXiv:2210.14164
replaced with revised version Sat, 27 Jan 2024 19:12:15 GMT   (12915kb,D)

Title: No-Box Attacks on 3D Point Cloud Classification
Authors: Hanieh Naderi, Chinthaka Dinesh, Ivan V. Bajic and Shohreh Kasaei
Categories: cs.CV cs.AI cs.CR cs.LG
Comments: 10 pages, 6 figures
\\ ( https://arxiv.org/abs/2210.14164 ,  12915kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13359
replaced with revised version Sun, 28 Jan 2024 02:20:41 GMT   (7941kb,D)

Title: IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing
Authors: Guoyang Xie, Jinbao Wang, Jiaqi Liu, Jiayi Lyu, Yong Liu, Chengjie
  Wang, Feng Zheng, Yaochu Jin
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2301.13359 ,  7941kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01928
replaced with revised version Sun, 28 Jan 2024 15:33:23 GMT   (721kb,D)

Title: Aligning Robot and Human Representations
Authors: Andreea Bobu, Andi Peng, Pulkit Agrawal, Julie Shah, Anca D. Dragan
Categories: cs.RO cs.AI cs.LG
Comments: 14 pages, 3 figures, 1 table
DOI: 10.1145/3610977.3634987
\\ ( https://arxiv.org/abs/2302.01928 ,  721kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04914 (*cross-listing*)
replaced with revised version Sat, 27 Jan 2024 06:48:32 GMT   (1477kb,D)

Title: Flexible, Model-Agnostic Method for Materials Data Extraction from Text
  Using General Purpose Language Models
Authors: Maciej P. Polak, Shrey Modi, Anna Latosinska, Jinming Zhang, Ching-Wen
  Wang, Shanonan Wang, Ayan Deep Hazra, and Dane Morgan
Categories: cond-mat.mtrl-sci cs.AI cs.CL
Comments: 13 pages, 4 figures
\\ ( https://arxiv.org/abs/2302.04914 ,  1477kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08767
replaced with revised version Sun, 28 Jan 2024 13:58:07 GMT   (236kb,D)

Title: Masked Language Model Based Textual Adversarial Example Detection
Authors: Xiaomei Zhang, Zhaoxi Zhang, Qi Zhong, Xufei Zheng, Yanjun Zhang,
  Shengshan Hu, Leo Yu Zhang
Categories: cs.CR cs.AI
Comments: 13 pages,3 figures
\\ ( https://arxiv.org/abs/2304.08767 ,  236kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12971
replaced with revised version Mon, 29 Jan 2024 13:34:15 GMT   (7673kb,D)

Title: Neural Cellular Automata Can Respond to Signals
Authors: James Stovold
Categories: cs.NE cs.AI cs.DC cs.LG
Comments: Accepted to main track at ALIFE 2023
DOI: 10.1162/isal_a_00567
\\ ( https://arxiv.org/abs/2305.12971 ,  7673kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11886
replaced with revised version Mon, 29 Jan 2024 17:28:20 GMT   (8421kb,D)

Title: SPRINT: Scalable Policy Pre-Training via Language Instruction Relabeling
Authors: Jesse Zhang and Karl Pertsch and Jiahui Zhang and Joseph J. Lim
Categories: cs.RO cs.AI cs.LG
Comments: 29 pages, 18 figures. Published at ICRA 2024
\\ ( https://arxiv.org/abs/2306.11886 ,  8421kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15749
replaced with revised version Sun, 28 Jan 2024 11:23:43 GMT   (8430kb,D)

Title: To Spike or Not To Spike: A Digital Hardware Perspective on Deep
  Learning Acceleration
Authors: Fabrizio Ottati, Chang Gao, Qinyu Chen, Giovanni Brignone, Mario R.
  Casu, Jason K. Eshraghian, Luciano Lavagno
Categories: cs.NE cs.AI cs.AR cs.LG
Comments: Fixed error in bio
\\ ( https://arxiv.org/abs/2306.15749 ,  8430kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16048
replaced with revised version Mon, 29 Jan 2024 10:45:58 GMT   (4564kb,D)

Title: Benchmarking Zero-Shot Recognition with Vision-Language Models:
  Challenges on Granularity and Specificity
Authors: Zhenlin Xu, Yi Zhu, Tiffany Deng, Abhay Mittal, Yanbei Chen, Manchen
  Wang, Paolo Favaro, Joseph Tighe, Davide Modolo
Categories: cs.CV cs.AI
Comments: Additional experiments
\\ ( https://arxiv.org/abs/2306.16048 ,  4564kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00012
replaced with revised version Mon, 29 Jan 2024 16:28:00 GMT   (5744kb,D)

Title: FlakyFix: Using Large Language Models for Predicting Flaky Test Fix
  Categories and Test Code Repair
Authors: Sakina Fatima, Hadi Hemmati, Lionel Briand
Categories: cs.SE cs.AI cs.LG
Comments: 22 pages, 16 Figures
\\ ( https://arxiv.org/abs/2307.00012 ,  5744kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13420
replaced with revised version Sun, 28 Jan 2024 02:06:36 GMT   (1886kb,D)

Title: Reinforcement Learning-assisted Evolutionary Algorithm: A Survey and
  Research Opportunities
Authors: Yanjie Song, Yutong Wu, Yangyang Guo, Ran Yan, P. N. Suganthan, Yue
  Zhang, Witold Pedrycz, Swagatam Das, Rammohan Mallipeddi, Oladayo Solomon
  Ajani. Qiang Feng
Categories: cs.NE cs.AI cs.LG
Comments: 28 pages, 16 figures
Report-no: SWEVO-S-2023-00771
\\ ( https://arxiv.org/abs/2308.13420 ,  1886kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13043
replaced with revised version Sat, 27 Jan 2024 22:29:30 GMT   (7966kb,D)

Title: E(2)-Equivariant Graph Planning for Navigation
Authors: Linfeng Zhao, Hongyu Li, Taskin Padir, Huaizu Jiang, Lawson L.S. Wong
Categories: cs.RO cs.AI
Comments: Accepted by RA-L
DOI: 10.1109/LRA.2024.3360011
\\ ( https://arxiv.org/abs/2309.13043 ,  7966kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15560
replaced with revised version Mon, 29 Jan 2024 17:47:55 GMT   (289kb,D)

Title: Identifiability Matters: Revealing the Hidden Recoverable Condition in
  Unbiased Learning to Rank
Authors: Mouxiang Chen, Chenghao Liu, Zemin Liu, Zhuo Li, Jianling Sun
Categories: cs.IR cs.AI cs.LG
Comments: Improve the experiments and theory
\\ ( https://arxiv.org/abs/2309.15560 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02161
replaced with revised version Sun, 28 Jan 2024 20:56:10 GMT   (8846kb,D)

Title: Selenite: Scaffolding Online Sensemaking with Comprehensive Overviews
  Elicited from Large Language Models
Authors: Michael Xieyang Liu, Tongshuang Wu, Tianying Chen, Franklin Mingzhe
  Li, Aniket Kittur, Brad A. Myers
Categories: cs.HC cs.AI
Comments: Accepted to CHI 2024
DOI: 10.1145/3613904.3642149
\\ ( https://arxiv.org/abs/2310.02161 ,  8846kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05969 (*cross-listing*)
replaced with revised version Sat, 27 Jan 2024 12:51:05 GMT   (2388kb,D)

Title: Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning
  Approach
Authors: Arief Purnama Muharram, Hollyana Puteri Haryono, Abassi Haji Juma, Ira
  Puspasari and Nugraha Priya Utama
Categories: eess.IV cs.AI cs.CV
Comments: Published in the 2023 IEEE International Conference on Data and
  Software Engineering (ICoDSE)
Journal-ref: 2023 IEEE International Conference on Data and Software
  Engineering (2023) 25-30
DOI: 10.1109/ICoDSE59534.2023.10291842
\\ ( https://arxiv.org/abs/2310.05969 ,  2388kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18446
replaced with revised version Sat, 27 Jan 2024 04:06:15 GMT   (2002kb,D)

Title: A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem
Authors: Xiaoyang Xu, Hu Ding
Categories: cs.DS cs.AI cs.CG math.OC
\\ ( https://arxiv.org/abs/2310.18446 ,  2002kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02493
replaced with revised version Mon, 29 Jan 2024 18:23:50 GMT   (3707kb,D)

Title: Flexible Communication for Optimal Distributed Learning over
  Unpredictable Networks
Authors: Sahil Tyagi, Martin Swany
Categories: cs.DC cs.AI cs.CV
Comments: 2023 IEEE International Conference on Big Data (BigData)
Journal-ref: 2023 IEEE International Conference on Big Data (BigData), 925-935
DOI: 10.1109/BigData59044.2023.10386724
\\ ( https://arxiv.org/abs/2312.02493 ,  3707kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03687 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 16:02:46 GMT   (10603kb,D)

Title: MatterGen: a generative model for inorganic materials design
Authors: Claudio Zeni, Robert Pinsler, Daniel Z\"ugner, Andrew Fowler, Matthew
  Horton, Xiang Fu, Sasha Shysheya, Jonathan Crabb\'e, Lixin Sun, Jake Smith,
  Bichlien Nguyen, Hannes Schulz, Sarah Lewis, Chin-Wei Huang, Ziheng Lu, Yichi
  Zhou, Han Yang, Hongxia Hao, Jielan Li, Ryota Tomioka, Tian Xie
Categories: cond-mat.mtrl-sci cs.AI
Comments: 13 pages main text, 35 pages supplementary information
\\ ( https://arxiv.org/abs/2312.03687 ,  10603kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05928
replaced with revised version Mon, 29 Jan 2024 12:23:36 GMT   (19978kb,D)

Title: AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer
Authors: Joonwoo Kwon, Sooyoung Kim, Yuewei Lin, Shinjae Yoo, Jiook Cha
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2312.05928 ,  19978kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08616
replaced with revised version Mon, 29 Jan 2024 11:48:30 GMT   (1569kb,D)

Title: A Generalized Neural Diffusion Framework on Graphs
Authors: Yibo Li, Xiao Wang, Hongrui Liu, Chuan Shi
Categories: cs.SI cs.AI
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2312.08616 ,  1569kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03343
replaced with revised version Mon, 29 Jan 2024 11:07:29 GMT   (1653kb)

Title: Rediscovering Ranganathan: A Prismatic View of His Life through the
  Knowledge Graph Spectrum
Authors: B. Dutta and S. Arzoo
Categories: cs.DL cs.AI
Comments: 22 pages, 16 figures
MSC-class: 68T30 Knowledge representation
ACM-class: I.2.1
\\ ( https://arxiv.org/abs/2401.03343 ,  1653kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10282 (*cross-listing*)
replaced with revised version Sat, 27 Jan 2024 17:44:55 GMT   (9093kb,D)

Title: BioDiffusion: A Versatile Diffusion Model for Biomedical Signal
  Synthesis
Authors: Xiaomin Li, Mykhailo Sakevych, Gentry Atkinson, Vangelis Metsis
Categories: eess.SP cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.10282 ,  9093kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10711
replaced with revised version Sun, 28 Jan 2024 08:17:03 GMT   (1673kb,D)

Title: Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal
  Models for Video Question Answering
Authors: Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.10711 ,  1673kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11156
replaced with revised version Sun, 28 Jan 2024 02:53:19 GMT   (2176kb,D)

Title: Generalizing Speaker Verification for Spoof Awareness in the Embedding
  Space
Authors: Xuechen Liu, Md Sahidullah, Kong Aik Lee, Tomi Kinnunen
Categories: cs.CR cs.AI cs.SD eess.AS
Comments: Published in IEEE/ACM Transactions on Audio, Speech, and Language
  Processing (doi updated)
DOI: 10.1109/TASLP.2024.3358056
\\ ( https://arxiv.org/abs/2401.11156 ,  2176kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11257
replaced with revised version Sun, 28 Jan 2024 15:37:54 GMT   (11973kb,D)

Title: Measuring Policy Distance for Multi-Agent Reinforcement Learning
Authors: Tianyi Hu, Zhiqiang Pu, Xiaolin Ai, Tenghai Qiu, Jianqiang Yi
Categories: cs.MA cs.AI
Comments: 9 pages, 6 figures
\\ ( https://arxiv.org/abs/2401.11257 ,  11973kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11414
replaced with revised version Mon, 29 Jan 2024 02:07:56 GMT   (5773kb,D)

Title: S$^3$M-Net: Joint Learning of Semantic Segmentation and Stereo Matching
  for Autonomous Driving
Authors: Zhiyuan Wu, Yi Feng, Chuang-Wei Liu, Fisher Yu, Qijun Chen, Rui Fan
Categories: cs.CV cs.AI cs.RO
Comments: accepted to IEEE Trans. on Intelligent Vehicles (T-IV)
DOI: 10.1109/TIV.2024.3357056
\\ ( https://arxiv.org/abs/2401.11414 ,  5773kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11723
replaced with revised version Sat, 27 Jan 2024 01:22:25 GMT   (40990kb,D)

Title: Unraveling Attacks in Machine Learning-based IoT Ecosystems: A Survey
  and the Open Libraries Behind Them
Authors: Chao Liu, Boxi Chen, Wei Shao, Chris Zhang, Kelvin Wong, Yi Zhang
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2401.11723 ,  40990kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11792
replaced with revised version Mon, 29 Jan 2024 01:50:12 GMT   (10362kb,D)

Title: Safe and Generalized end-to-end Autonomous Driving System with
  Reinforcement Learning and Demonstrations
Authors: Zuojin Tang, Xiaoyu Chen, YongQiang Li, Jianyu Chen
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.11792 ,  10362kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12665
replaced with revised version Mon, 29 Jan 2024 10:57:38 GMT   (12668kb,D)

Title: ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation
Authors: Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, Tao Chen
Categories: cs.CV cs.AI
Comments: 17 pages,17 figures
\\ ( https://arxiv.org/abs/2401.12665 ,  12668kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13324
replaced with revised version Mon, 29 Jan 2024 08:52:18 GMT   (2521kb)

Title: Information That Matters: Exploring Information Needs of People Affected
  by Algorithmic Decisions
Authors: Timoth\'ee Schmude, Laura Koesten, Torsten M\"oller, Sebastian
  Tschiatschek
Categories: cs.HC cs.AI
Comments: Main text: 21 pages, 3 figures. Supplementary material is provided.
  Manuscript submitted for review to IJHCS
\\ ( https://arxiv.org/abs/2401.13324 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13802
replaced with revised version Sat, 27 Jan 2024 04:43:46 GMT   (12329kb,D)

Title: Investigating the Efficacy of Large Language Models for Code Clone
  Detection
Authors: Mohamad Khajezade, Jie JW Wu, Fatemeh Hendijani Fard, Gema
  Rodr\'iguez-P\'erez, Mohamed Sami Shehata
Categories: cs.SE cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.13802 ,  12329kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14257
replaced with revised version Sat, 27 Jan 2024 07:22:06 GMT   (9661kb,D)

Title: Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation
Authors: Minglin Chen and Weihao Yuan and Yukun Wang and Zhe Sheng and Yisheng
  He and Zilong Dong and Liefeng Bo and Yulan Guo
Categories: cs.CV cs.AI
Comments: 11 pages, 9 figures
\\ ( https://arxiv.org/abs/2401.14257 ,  9661kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14403
replaced with revised version Sun, 28 Jan 2024 18:58:29 GMT   (9833kb,D)

Title: Adaptive Mobile Manipulation for Articulated Objects In the Open World
Authors: Haoyu Xiong, Russell Mendonca, Kenneth Shaw, Deepak Pathak
Categories: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY
Comments: Website at https://open-world-mobilemanip.github.io/
\\ ( https://arxiv.org/abs/2401.14403 ,  9833kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14915
replaced with revised version Mon, 29 Jan 2024 14:04:14 GMT   (10042kb,D)

Title: Charting the Future of AI in Project-Based Learning: A Co-Design
  Exploration with Students
Authors: Chengbo Zheng, Kangyu Yuan, Bingcan Guo, Reza Hadi Mogavi, Zhenhui
  Peng, Shuai Ma, Xiaojuan Ma
Categories: cs.HC cs.AI
Comments: Conditionally accepted by CHI '24
\\ ( https://arxiv.org/abs/2401.14915 ,  10042kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15645
replaced with revised version Sat, 27 Jan 2024 17:42:14 GMT   (7502kb,D)

Title: ConvGQR: Generative Query Reformulation for Conversational Search
Authors: Fengran Mo, Kelong Mao, Yutao Zhu, Yihong Wu, Kaiyu Huang, Jian-Yun
  Nie
Categories: cs.IR cs.CL
Comments: Published at ACL 2023
\\ ( https://arxiv.org/abs/2305.15645 ,  7502kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18727
replaced with revised version Sun, 28 Jan 2024 07:16:55 GMT   (1083kb,D)

Title: Automatic Functional Differentiation in JAX
Authors: Min Lin
Categories: cs.PL cs.CL cs.LG
Comments: The Twelfth International Conference on Learning Representations
  (ICLR 2024)
\\ ( https://arxiv.org/abs/2311.18727 ,  1083kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10244
replaced with revised version Sat, 27 Jan 2024 21:50:34 GMT   (492kb)

Title: Knowledge Graph Driven Recommendation System Algorithm
Authors: Chaoyang Zhang, Yanan Li, Shen Chen, Siwei Fan, Wei Li
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2401.10244 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10893
replaced with revised version Sat, 27 Jan 2024 22:25:09 GMT   (563kb)

Title: Location Sensitive Embedding for Knowledge Graph Reasoning
Authors: Deepak Banerjee, Anjali Ishaan
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2401.10893 ,  563kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14887
replaced with revised version Mon, 29 Jan 2024 18:52:52 GMT   (106kb,D)

Title: The Power of Noise: Redefining Retrieval for RAG Systems
Authors: Florin Cuconasu, Giovanni Trappolini, Federico Siciliano, Simone
  Filice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, Fabrizio
  Silvestri
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2401.14887 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2009.10862 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 03:24:49 GMT   (4455kb,D)

Title: An Intuitive Tutorial to Gaussian Process Regression
Authors: Jie Wang
Categories: stat.ML cs.LG cs.RO
Comments: 8 pages, 12 figures
Journal-ref: Computing in Science & Engineering, 2024
DOI: 10.1109/MCSE.2023.3342149
\\ ( https://arxiv.org/abs/2009.10862 ,  4455kb)
------------------------------------------------------------------------------
\\
arXiv:2010.16271 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 13:36:23 GMT   (856kb,D)

Title: View selection in multi-view stacking: Choosing the meta-learner
Authors: Wouter van Loon, Marjolein Fokkema, Botond Szabo, Mark de Rooij
Categories: stat.ML cs.LG stat.ME
Comments: 47 pages, 17 figures. Minor revisions
MSC-class: 62, 68
\\ ( https://arxiv.org/abs/2010.16271 ,  856kb)
------------------------------------------------------------------------------
\\
arXiv:2103.04565
replaced with revised version Sun, 28 Jan 2024 23:42:12 GMT   (0kb,I)

Title: Improving Transformation-based Defenses against Adversarial Examples
  with First-order Perturbations
Authors: Haimin Zhang, Min Xu
Categories: cs.CV cs.LG
Comments: This paper has technical errors
\\ ( https://arxiv.org/abs/2103.04565 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2105.12833
replaced with revised version Fri, 26 Jan 2024 21:42:30 GMT   (0kb,I)

Title: Simulated Data Generation Through Algorithmic Force Coefficient
  Estimation for AI-Based Robotic Projectile Launch Modeling
Authors: Sajiv Shah, Ayaan Haque, Fei Liu
Categories: cs.RO cs.LG
Comments: not relevant work
\\ ( https://arxiv.org/abs/2105.12833 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2202.03772 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 15:22:51 GMT   (983kb,D)

Title: Particle Transformer for Jet Tagging
Authors: Huilin Qu, Congqiao Li, Sitian Qian
Categories: hep-ph cs.LG hep-ex physics.data-an
Comments: 12 pages, 3 figures. Accepted to the 39th International Conference on
  Machine Learning (ICML), 2022. v3: fixed a typo on the interaction matrix
  dimensionality in Sec. 4
Journal-ref: Proceedings of the 39th International Conference on Machine
  Learning, PMLR 162:18281-18292, 2022
\\ ( https://arxiv.org/abs/2202.03772 ,  983kb)
------------------------------------------------------------------------------
\\
arXiv:2202.12435
replaced with revised version Mon, 29 Jan 2024 12:54:30 GMT   (2957kb,D)

Title: Understanding Adversarial Robustness from Feature Maps of Convolutional
  Layers
Authors: Cong Xu, Wei Zhang, Jun Wang and Min Yang
Categories: cs.CV cs.LG
Comments: 14pages
\\ ( https://arxiv.org/abs/2202.12435 ,  2957kb)
------------------------------------------------------------------------------
\\
arXiv:2203.01327 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 23:11:31 GMT   (3024kb,D)

Title: Hyperspectral Pixel Unmixing with Latent Dirichlet Variational
  Autoencoder
Authors: Kiran Mantripragada and Faisal Z. Qureshi
Categories: eess.IV cs.CV cs.LG
DOI: 10.1109/TGRS.2024.3357589
\\ ( https://arxiv.org/abs/2203.01327 ,  3024kb)
------------------------------------------------------------------------------
\\
arXiv:2206.05581 (*cross-listing*)
replaced with revised version Sat, 27 Jan 2024 16:23:25 GMT   (974kb,D)

Title: Federated Offline Reinforcement Learning
Authors: Doudou Zhou, Yufeng Zhang, Aaron Sonabend-W, Zhaoran Wang, Junwei Lu,
  Tianxi Cai
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2206.05581 ,  974kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00109 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 07:25:36 GMT   (2585kb,D)

Title: A DeepParticle method for learning and generating aggregation patterns
  in multi-dimensional Keller-Segel chemotaxis systems
Authors: Zhongjian Wang, Jack Xin, Zhiwen Zhang
Categories: physics.comp-ph cs.LG
MSC-class: 35K57, 37M25, 49Q22, 65C35, 68T07
\\ ( https://arxiv.org/abs/2209.00109 ,  2585kb)
------------------------------------------------------------------------------
\\
arXiv:2211.07092 (*cross-listing*)
replaced with revised version Fri, 26 Jan 2024 20:23:18 GMT   (812kb)

Title: Offline Estimation of Controlled Markov Chains: Minimaxity and Sample
  Complexity
Authors: Imon Banerjee, Harsha Honnappa, Vinayak Rao
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 71 pages, 23 main
\\ ( https://arxiv.org/abs/2211.07092 ,  812kb)
------------------------------------------------------------------------------
\\
arXiv:2211.15223 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 16:07:27 GMT   (53kb)

Title: Gamma-convergence of a nonlocal perimeter arising in adversarial machine
  learning
Authors: Leon Bungert, Kerrek Stinson
Categories: math.AP cs.LG math.OC
Comments: Fixed typos, added new isotropic-anisotropic decomposition formula
  for limit perimeter
MSC-class: 28A75, 49J45, 60D05, 68R10
\\ ( https://arxiv.org/abs/2211.15223 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2301.01642 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 08:56:23 GMT   (9171kb,D)

Title: CI-GNN: A Granger Causality-Inspired Graph Neural Network for
  Interpretable Brain Network-Based Psychiatric Diagnosis
Authors: Kaizhong Zheng, Shujian Yu, Badong Chen
Categories: stat.ML cs.LG q-bio.NC
Comments: Manuscript ia accepted by Neural Networks, The source code and
  implementation details are freely available at GitHub repository
  (https://github.com/ZKZ-Brain/CI-GNN/). 45 pages, 14 figures
\\ ( https://arxiv.org/abs/2301.01642 ,  9171kb)
------------------------------------------------------------------------------
\\
arXiv:2301.08897
replaced with revised version Mon, 29 Jan 2024 18:10:57 GMT   (3171kb,D)

Title: ScaDLES: Scalable Deep Learning over Streaming data at the Edge
Authors: Sahil Tyagi, Martin Swany
Categories: cs.DC cs.LG cs.NI
Journal-ref: Tyagi, S., & Swany, M. (2022). ScaDLES: Scalable Deep Learning
  over Streaming data at the Edge. 2022 IEEE International Conference on Big
  Data (Big Data), 2113-2122
DOI: 10.1109/BigData55660.2022.10020597
\\ ( https://arxiv.org/abs/2301.08897 ,  3171kb)
------------------------------------------------------------------------------
\\
arXiv:2303.12861 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 18:24:21 GMT   (6666kb,D)

Title: Parallel Diffusion Model-based Sparse-view Cone-beam Breast CT
Authors: Wenjun Xia, Hsin Wu Tseng, Chuang Niu, Wenxiang Cong, Xiaohua Zhang,
  Shaohua Liu, Ruola Ning, Srinivasan Vedantham, Ge Wang
Categories: eess.IV cs.LG eess.SP physics.bio-ph
\\ ( https://arxiv.org/abs/2303.12861 ,  6666kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01611
replaced with revised version Mon, 29 Jan 2024 11:16:46 GMT   (22743kb,D)

Title: AutoColor: Learned Light Power Control for Multi-Color Holograms
Authors: Yicheng Zhan, Koray Kavakl{\i}, Hakan Urey, Qi Sun, Kaan Ak\c{s}it
Categories: cs.CV cs.LG eess.IV
Comments: 6 pages, 2 figures, SPIE VR|AR|MR 2024
\\ ( https://arxiv.org/abs/2305.01611 ,  22743kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03686
replaced with revised version Sat, 27 Jan 2024 18:56:15 GMT   (213kb,D)

Title: Provable Preimage Under-Approximation for Neural Networks (Full Version)
Authors: Xiyue Zhang, Benjie Wang, Marta Kwiatkowska
Categories: cs.SE cs.LG cs.LO
\\ ( https://arxiv.org/abs/2305.03686 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05097 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 22:04:51 GMT   (138kb,D)

Title: Self-Repellent Random Walks on General Graphs -- Achieving Minimal
  Sampling Variance via Nonlinear Markov Chains
Authors: Vishwaraj Doshi, Jie Hu and Do Young Eun
Categories: math.PR cs.LG
Comments: Selected for oral presentation at ICML 2023. Recipient of Outstanding
  Paper award
\\ ( https://arxiv.org/abs/2305.05097 ,  138kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05499
replaced with revised version Mon, 29 Jan 2024 17:18:50 GMT   (13261kb,D)

Title: Effects of Real-Life Traffic Sign Alteration on YOLOv7- an Object
  Recognition Model
Authors: Farhin Farhad Riya, Shahinul Hoque, Md Saif Hassan Onim, Edward
  Michaud, Edmon Begoli and Jinyuan Stella Sun
Categories: cs.CV cs.CR cs.LG
\\ ( https://arxiv.org/abs/2305.05499 ,  13261kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16304
replaced with revised version Mon, 29 Jan 2024 05:03:55 GMT   (6864kb,D)

Title: Candidate Set Re-ranking for Composed Image Retrieval with Dual
  Multi-modal Encoder
Authors: Zheyuan Liu, Weixuan Sun, Damien Teney, Stephen Gould
Categories: cs.CV cs.IR cs.LG
Comments: Accepted at TMLR, 19 pages, 8 figures
\\ ( https://arxiv.org/abs/2305.16304 ,  6864kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01875
replaced with revised version Fri, 26 Jan 2024 21:30:17 GMT   (12876kb,D)

Title: DiffECG: A Versatile Probabilistic Diffusion Model for ECG Signals
  Synthesis
Authors: Nour Neifar, Achraf Ben-Hamadou, Afef Mdhaffar, Mohamed Jmaiel
Categories: cs.CV cs.LG
Comments: submitted to Pattern Recognition Letters
\\ ( https://arxiv.org/abs/2306.01875 ,  12876kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09189
replaced with revised version Mon, 29 Jan 2024 03:20:21 GMT   (5413kb,D)

Title: High-Resolution Convolutional Neural Networks on Homomorphically
  Encrypted Data via Sharding Ciphertexts
Authors: Vivian Maloney, Richard F. Obrecht, Vikram Saraph, Prathibha Rama,
  Kate Tallaksen
Categories: cs.CR cs.LG
Comments: 14 pages, 9 figures
\\ ( https://arxiv.org/abs/2306.09189 ,  5413kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00238 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 23:07:56 GMT   (5016kb,D)

Title: Unified Transfer Learning Models in High-Dimensional Linear Regression
Authors: Shuo Shuo Liu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2307.00238 ,  5016kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07950
replaced with revised version Mon, 29 Jan 2024 18:18:56 GMT   (3308kb,D)

Title: Accelerating Distributed ML Training via Selective Synchronization
Authors: Sahil Tyagi, Martin Swany
Categories: cs.DC cs.CV cs.LG
Journal-ref: Tyagi, S., & Swany, M. (2023). Accelerating Distributed ML
  Training via Selective Synchronization. 2023 IEEE International Conference on
  Cluster Computing (CLUSTER), 1-12
DOI: 10.1109/CLUSTER52292.2023.00008
\\ ( https://arxiv.org/abs/2307.07950 ,  3308kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07491
replaced with revised version Sun, 28 Jan 2024 14:07:01 GMT   (6540kb,D)

Title: Adaptive Tracking of a Single-Rigid-Body Character in Various
  Environments
Authors: Taesoo Kwon, Taehong Gu, Jaewon Ahn, Yoonsang Lee
Categories: cs.RO cs.GR cs.LG
Comments: SIGGRAPH Asia 2023 Conference Papers
Journal-ref: SA '23: SIGGRAPH Asia 2023 Conference Papers, December 2023,
  Article No.: 118, Pages 1-11
DOI: 10.1145/3610548.3618187
\\ ( https://arxiv.org/abs/2308.07491 ,  6540kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06782 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 07:29:28 GMT   (2883kb,D)

Title: Improved particle-flow event reconstruction with scalable neural
  networks for current and future particle detectors
Authors: Joosep Pata, Eric Wulff, Farouk Mokhtar, David Southwick, Mengke
  Zhang, Maria Girone, Javier Duarte
Categories: physics.data-an cs.LG hep-ex physics.ins-det stat.ML
Comments: 20 pages, 11 figures
\\ ( https://arxiv.org/abs/2309.06782 ,  2883kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03480 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 11:48:36 GMT   (710kb,D)

Title: The ICASSP SP Cadenza Challenge: Music Demixing/Remixing for Hearing
  Aids
Authors: Gerardo Roa Dabike, Michael A. Akeroyd, Scott Bannister, Jon Barker,
  Trevor J. Cox, Bruno Fazenda, Jennifer Firth, Simone Graetzer, Alinka
  Greasley, Rebecca R. Vos, William M. Whitmer
Categories: eess.AS cs.LG eess.SP
Comments: 2-page paper for ICASSP 2024 SP Grand Challenge
\\ ( https://arxiv.org/abs/2310.03480 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04676
replaced with revised version Sat, 27 Jan 2024 18:07:37 GMT   (4319kb,D)

Title: Surgical Gym: A high-performance GPU-based platform for reinforcement
  learning with surgical robots
Authors: Samuel Schmidgall, Axel Krieger, Jason Eshraghian
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2310.04676 ,  4319kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07736
replaced with revised version Sat, 27 Jan 2024 17:54:48 GMT   (8365kb,D)

Title: Observatory: Characterizing Embeddings of Relational Tables
Authors: Tianji Cong, Madelon Hulsebos, Zhenjie Sun, Paul Groth, H. V. Jagadish
Categories: cs.DB cs.LG
Comments: Camera ready of VLDB 2024
\\ ( https://arxiv.org/abs/2310.07736 ,  8365kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10483
replaced with revised version Sun, 28 Jan 2024 18:02:27 GMT   (29694kb,D)

Title: Passive Inference Attacks on Split Learning via Adversarial
  Regularization
Authors: Xiaochen Zhu, Xinjian Luo, Yuncheng Wu, Yangfan Jiang, Xiaokui Xiao,
  Beng Chin Ooi
Categories: cs.CR cs.LG
Comments: 19 pages, 20 figures
\\ ( https://arxiv.org/abs/2310.10483 ,  29694kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19063
replaced with revised version Sat, 27 Jan 2024 20:45:13 GMT   (5037kb,D)

Title: Feature Aggregation in Joint Sound Classification and Localization
  Neural Networks
Authors: Brendan Healy, Patrick McNamee, and Zahra Nili Ahmadabadi
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2310.19063 ,  5037kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07550
replaced with revised version Mon, 29 Jan 2024 13:23:29 GMT   (3095kb,D)

Title: Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks
  for Tabular Data
Authors: Bart Pleiter, Behrad Tajalli, Stefanos Koffas, Gorka Abad, Jing Xu,
  Martha Larson, Stjepan Picek
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2311.07550 ,  3095kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07957
replaced with revised version Sun, 28 Jan 2024 02:43:40 GMT   (163kb,D)

Title: Language Models are Better Bug Detector Through Code-Pair Classification
Authors: Kamel Alrashedy, Ahmed Binjahlan
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2311.07957 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09486
replaced with revised version Mon, 29 Jan 2024 04:17:35 GMT   (914kb,D)

Title: Unraveling Batch Normalization for Realistic Test-Time Adaptation
Authors: Zixian Su, Jingwei Guo, Kai Yao, Xi Yang, Qiufeng Wang, Kaizhu Huang
Categories: cs.CV cs.LG
Comments: Accepted by AAAI 2024
\\ ( https://arxiv.org/abs/2312.09486 ,  914kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13250 (*cross-listing*)
replaced with revised version Sat, 27 Jan 2024 23:15:21 GMT   (3185kb,D)

Title: The role of data embedding in equivariant quantum convolutional neural
  networks
Authors: Sreetama Das, Stefano Martina, Filippo Caruso
Categories: quant-ph cs.CV cs.ET cs.LG
Comments: 12 pages, 9 figures. Significant changes compared to previous
  version. New results added
\\ ( https://arxiv.org/abs/2312.13250 ,  3185kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16762 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 09:50:16 GMT   (7603kb,D)

Title: Backstepping Neural Operators for $2\times 2$ Hyperbolic PDEs
Authors: Shanshan Wang, Mamadou Diagne and Miroslav Krsti\'c
Categories: math.OC cs.LG math.AP
\\ ( https://arxiv.org/abs/2312.16762 ,  7603kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03451 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 16:46:16 GMT   (530kb,D)

Title: Optimization Over Trained Neural Networks: Taking a Relaxing Walk
Authors: Jiatai Tong and Junyang Cai and Thiago Serra
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2401.03451 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05363 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 04:28:40 GMT   (621kb,D)

Title: Generalizable Sleep Staging via Multi-Level Domain Alignment
Authors: Jiquan Wang, Sha Zhao, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan
Categories: eess.SP cs.LG
Comments: Accepted by the Thirty-Eighth AAAI Conference on Artificial
  Intelligence (AAAI-24)
\\ ( https://arxiv.org/abs/2401.05363 ,  621kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05382
replaced with revised version Mon, 29 Jan 2024 02:01:02 GMT   (685kb)

Title: Enhanced Genetic Programming Models with Multiple Equations for Accurate
  Semi-Autogenous Grinding Mill Throughput Prediction
Authors: Zahra Ghasemi, Mehdi Nesht, Chris Aldrich, John Karageorgos, Max
  Zanin, Frank Neumann, Lei Chen
Categories: cs.NE cs.LG
\\ ( https://arxiv.org/abs/2401.05382 ,  685kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10800 (*cross-listing*)
replaced with revised version Mon, 29 Jan 2024 17:18:30 GMT   (480kb,D)

Title: Estimation of AMOC transition probabilities using a machine learning
  based rare-event algorithm
Authors: Val\'erian Jacques-Dumas, Ren\'e M. van Westen and Henk A. Dijkstra
Categories: physics.ao-ph cs.LG
Comments: 14 pages, 9 figures
\\ ( https://arxiv.org/abs/2401.10800 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12131
replaced with revised version Mon, 29 Jan 2024 11:47:53 GMT   (1730kb,D)

Title: NeuroSynt: A Neuro-symbolic Portfolio Solver for Reactive Synthesis
Authors: Matthias Cosler, Christopher Hahn, Ayham Omar, Frederik Schmitt
Categories: cs.LO cs.LG
\\ ( https://arxiv.org/abs/2401.12131 ,  1730kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12729
replaced with revised version Mon, 29 Jan 2024 13:18:18 GMT   (1699kb,D)

Title: Enhancing Object Detection Performance for Small Objects through
  Synthetic Data Generation and Proportional Class-Balancing Technique: A
  Comparative Study in Industrial Scenarios
Authors: Jibinraj Antony and Vinit Hegiste and Ali Nazeri and Hooman Tavakoli
  and Snehal Walunj and Christiane Plociennik and Martin Ruskowski
Categories: cs.CV cs.LG
Comments: Accepted and presented in conference ESAIM23 1st European Symposium
  on Artificial Intelligence in Manufacturing
\\ ( https://arxiv.org/abs/2401.12729 ,  1699kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12923 (*cross-listing*)
replaced with revised version Sat, 27 Jan 2024 02:55:21 GMT   (830kb,D)

Title: Deep multitask neural networks for solving some stochastic optimal
  control problems
Authors: Christian Yeo
Categories: stat.ML cs.LG cs.SY eess.SY
Comments: 9 pages
\\ ( https://arxiv.org/abs/2401.12923 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13231
replaced with revised version Mon, 29 Jan 2024 03:41:34 GMT   (2417kb,D)

Title: DittoGym: Learning to Control Soft Shape-Shifting Robots
Authors: Suning Huang and Boyuan Chen and Huazhe Xu and Vincent Sitzmann
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2401.13231 ,  2417kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13429
replaced with revised version Sun, 28 Jan 2024 14:02:16 GMT   (36kb)

Title: Detection of Correlated Random Vectors
Authors: Dor Elimelech and Wasim Huleihel
Categories: cs.IT cs.LG math.IT math.ST stat.TH
Comments: 35 pages
\\ ( https://arxiv.org/abs/2401.13429 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13851
replaced with revised version Mon, 29 Jan 2024 18:51:34 GMT   (3855kb)

Title: Scaling NVIDIA's Multi-speaker Multi-lingual TTS Systems with Zero-Shot
  TTS to Indic Languages
Authors: Akshit Arora, Rohan Badlani, Sungwon Kim, Rafael Valle, Bryan
  Catanzaro
Categories: cs.SD cs.LG eess.AS
Comments: Presentation accepted at ICASSP 2024
\\ ( https://arxiv.org/abs/2401.13851 ,  3855kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13947
replaced with revised version Sat, 27 Jan 2024 05:26:16 GMT   (1262kb,D)

Title: Networked Multiagent Reinforcement Learning for Peer-to-Peer Energy
  Trading
Authors: Chen Feng and Andrew L. Liu
Categories: eess.SY cs.LG cs.MA cs.SY
\\ ( https://arxiv.org/abs/2401.13947 ,  1262kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14340 (*cross-listing*)
replaced with revised version Sun, 28 Jan 2024 17:40:06 GMT   (896kb,D)

Title: Estimation of partially known Gaussian graphical models with score-based
  structural priors
Authors: Mart\'in Sevilla, Antonio Garc\'ia Marques, Santiago Segarra
Categories: stat.ML cs.LG
Comments: 16 pages, 6 figures, AISTATS 2024
\\ ( https://arxiv.org/abs/2401.14340 ,  896kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
