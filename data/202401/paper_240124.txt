Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月24日 17:10
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon 22 Jan 24 19:00:00 GMT  to  Tue 23 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.12247
Date: Sat, 20 Jan 2024 15:17:50 GMT   (289kb)

Title: Exploring consumers response to text-based chatbots in e-commerce: The
  moderating role of task complexity and chatbot disclosure
Authors: Xusen Cheng, Ying Bao, Alex Zarifis, Wankun Gong and Jian Mou
Categories: cs.AI
Comments: Internet Research (2021)
DOI: 10.1108/INTR-08-2020-0460
\\
  Artificial intelligence based chatbots have brought unprecedented business
potential. This study aims to explore consumers trust and response to a
text-based chatbot in ecommerce, involving the moderating effects of task
complexity and chatbot identity disclosure. A survey method with 299 useable
responses was conducted in this research. This study adopted the ordinary least
squares regression to test the hypotheses. First, the consumers perception of
both the empathy and friendliness of the chatbot positively impacts their trust
in it. Second, task complexity negatively moderates the relationship between
friendliness and consumers trust. Third, disclosure of the text based chatbot
negatively moderates the relationship between empathy and consumers trust,
while it positively moderates the relationship between friendliness and
consumers trust. Fourth, consumers trust in the chatbot increases their
reliance on the chatbot and decreases their resistance to the chatbot in future
interactions. Adopting the stimulus organism response framework, this study
provides important insights on consumers perception and response to the
text-based chatbot. The findings of this research also make suggestions that
can increase consumers positive responses to text based chatbots. Extant
studies have investigated the effects of automated bots attributes on consumers
perceptions. However, the boundary conditions of these effects are largely
ignored. This research is one of the first attempts to provide a deep
understanding of consumers responses to a chatbot.
\\ ( https://arxiv.org/abs/2401.12247 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12322
Date: Mon, 22 Jan 2024 19:29:33 GMT   (690kb)

Title: Smart Recommendations for Renting Bikes in Bike Sharing Systems
Authors: Holger Billhardt, Alberto Fern\'andez, Sascha Ossowski
Categories: cs.AI
ACM-class: I.2.1
Journal-ref: Applied Sciences, Volume 11, Issue 20 (2021)
DOI: 10.3390/app11209654
\\
  Vehicle-sharing systems -- such as bike-, car-, or motorcycle-sharing systems
-- have become increasingly popular in big cities in recent years. On the one
hand, they provide a cheaper and environmentally friendlier means of
transportation than private cars, and on the other hand, they satisfy the
individual mobility demands of citizens better than traditional public
transport systems. One of their advantages in this regard is their
availability, e.g., the possibility of taking (or leaving) a vehicle almost
anywhere in a city. This availability obviously depends on different strategic
and operational management decisions and policies, such as the dimension of the
fleet or the (re)distribution of vehicles. Agglutination problems -- where, due
to usage patterns, available vehicles are concentrated in certain areas,
whereas no vehicles are available in others -- are quite common in such
systems, and need to be dealt with. Research has been dedicated to this
problem, specifying different techniques to reduce imbalanced situations. In
this paper, we present and compare strategies for recommending stations to
users who wish to rent or return bikes in station-based bike-sharing systems.
Our first contribution is a novel recommendation strategy based on queuing
theory that recommends stations based on their utility to the user in terms of
lower distance and higher probability of finding a bike or slot. Then, we go
one step further, defining a strategy that recommends stations by combining the
utility of a particular user with the utility of the global system, measured in
terms of the improvement in the distribution of bikes and slots with respect to
the expected future demand, with the aim of implicitly avoiding or alleviating
balancing problems. We present several experiments to evaluate our proposal
with real data from the bike sharing system BiciMAD in Madrid.
\\ ( https://arxiv.org/abs/2401.12322 ,  690kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12324
Date: Mon, 22 Jan 2024 19:35:28 GMT   (869kb)

Title: Streamlining Advanced Taxi Assignment Strategies based on Legal Analysis
Authors: Holger Billhardt, Jos\'e-Antonio Santos, Alberto Fern\'andez, Mar
  Moreno, Sascha Ossowski, Jos\'e A. Rodr\'iguez
Categories: cs.AI
ACM-class: I.2.1
Journal-ref: Neurocomputing, Volume 438 (2022)
DOI: 10.1016/j.neucom.2021.10.085
\\
  In recent years many novel applications have appeared that promote the
provision of services and activities in a collaborative manner. The key idea
behind such systems is to take advantage of idle or underused capacities of
existing resources, in order to provide improved services that assist people in
their daily tasks, with additional functionality, enhanced efficiency, and/or
reduced cost. Particularly in the domain of urban transportation, many
researchers have put forward novel ideas, which are then implemented and
evaluated through prototypes that usually draw upon AI methods and tools.
However, such proposals also bring up multiple non-technical issues that need
to be identified and addressed adequately if such systems are ever meant to be
applied to the real world. While, in practice, legal and ethical aspects
related to such AI-based systems are seldomly considered in the beginning of
the research and development process, we argue that they not only restrict
design decisions, but can also help guiding them. In this manuscript, we set
out from a prototype of a taxi coordination service that mediates between
individual (and autonomous) taxis and potential customers. After representing
key aspects of its operation in a semi-structured manner, we analyse its
viability from the viewpoint of current legal restrictions and constraints, so
as to identify additional non-functional requirements as well as options to
address them. Then, we go one step ahead, and actually modify the existing
prototype to incorporate the previously identified recommendations. Performing
experiments with this improved system helps us identify the most adequate
option among several legally admissible alternatives.
\\ ( https://arxiv.org/abs/2401.12324 ,  869kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12379
Date: Mon, 22 Jan 2024 22:05:42 GMT   (1563kb,D)

Title: Analyzing the Effectiveness of Large Language Models on Text-to-SQL
  Synthesis
Authors: Richard Roberson, Gowtham Kaki, Ashutosh Trivedi
Categories: cs.AI cs.DB cs.PL
\\
  This study investigates various approaches to using Large Language Models
(LLMs) for Text-to-SQL program synthesis, focusing on the outcomes and insights
derived. Employing the popular Text-to-SQL dataset, spider, the goal was to
input a natural language question along with the database schema and output the
correct SQL SELECT query. The initial approach was to fine-tune a local and
open-source model to generate the SELECT query. After QLoRa fine-tuning
WizardLM's WizardCoder-15B model on the spider dataset, the execution accuracy
for generated queries rose to a high of 61%. With the second approach, using
the fine-tuned gpt-3.5-turbo-16k (Few-shot) + gpt-4-turbo (Zero-shot error
correction), the execution accuracy reached a high of 82.1%. Of all the
incorrect queries, most can be categorized into a seven different categories of
what went wrong: selecting the wrong columns or wrong order of columns,
grouping by the wrong column, predicting the wrong values in conditionals,
using different aggregates than the ground truth, extra or too few JOIN
clauses, inconsistencies in the Spider dataset, and lastly completely incorrect
query structure. Most if not all of the queries fall into these categories and
it is insightful to understanding where the faults still lie with LLM program
synthesis and where they can be improved.
\\ ( https://arxiv.org/abs/2401.12379 ,  1563kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12435
Date: Tue, 23 Jan 2024 02:04:15 GMT   (8248kb)

Title: Quantitative Analysis of Molecular Transport in the Extracellular Space
  Using Physics-Informed Neural Network
Authors: Jiayi Xie, Hongfeng Li, Yu Jiang, Jin Cheng, Qingrui Cai, Hanbo Tan,
  Lingyun Zu, Xiaobo Qu, and Hongbin Han
Categories: cs.AI cs.LG math.AP
\\
  The brain extracellular space (ECS), an irregular, extremely tortuous
nanoscale space located between cells or between cells and blood vessels, is
crucial for nerve cell survival. It plays a pivotal role in high-level brain
functions such as memory, emotion, and sensation. However, the specific form of
molecular transport within the ECS remain elusive. To address this challenge,
this paper proposes a novel approach to quantitatively analyze the molecular
transport within the ECS by solving an inverse problem derived from the
advection-diffusion equation (ADE) using a physics-informed neural network
(PINN). PINN provides a streamlined solution to the ADE without the need for
intricate mathematical formulations or grid settings. Additionally, the
optimization of PINN facilitates the automatic computation of the diffusion
coefficient governing long-term molecule transport and the velocity of
molecules driven by advection. Consequently, the proposed method allows for the
quantitative analysis and identification of the specific pattern of molecular
transport within the ECS through the calculation of the Peclet number.
Experimental validation on two datasets of magnetic resonance images (MRIs)
captured at different time points showcases the effectiveness of the proposed
method. Notably, our simulations reveal identical molecular transport patterns
between datasets representing rats with tracer injected into the same brain
region. These findings highlight the potential of PINN as a promising tool for
comprehensively exploring molecular transport within the ECS.
\\ ( https://arxiv.org/abs/2401.12435 ,  8248kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12459
Date: Tue, 23 Jan 2024 03:00:03 GMT   (2913kb,D)

Title: Towards Socially and Morally Aware RL agent: Reward Design With LLM
Authors: Zhaoyue Wang
Categories: cs.AI
\\
  When we design and deploy an Reinforcement Learning (RL) agent, reward
functions motivates agents to achieve an objective. An incorrect or incomplete
specification of the objective can result in behavior that does not align with
human values - failing to adhere with social and moral norms that are ambiguous
and context dependent, and cause undesired outcomes such as negative side
effects and exploration that is unsafe. Previous work have manually defined
reward functions to avoid negative side effects, use human oversight for safe
exploration, or use foundation models as planning tools. This work studies the
ability of leveraging Large Language Models (LLM)' understanding of morality
and social norms on safe exploration augmented RL methods. This work evaluates
language model's result against human feedbacks and demonstrates language
model's capability as direct reward signals.
\\ ( https://arxiv.org/abs/2401.12459 ,  2913kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12467
Date: Tue, 23 Jan 2024 03:30:47 GMT   (3115kb,D)

Title: An open dataset for the evolution of oracle bone characters: EVOBC
Authors: Haisu Guan, Jinpeng Wan, Yuliang Liu, Pengjie Wang, Kaile Zhang,
  Zhebin Kuang, Xinyu Wang, Xiang Bai, Lianwen Jin
Categories: cs.AI
\\
  The earliest extant Chinese characters originate from oracle bone
inscriptions, which are closely related to other East Asian languages. These
inscriptions hold immense value for anthropology and archaeology. However,
deciphering oracle bone script remains a formidable challenge, with only
approximately 1,600 of the over 4,500 extant characters elucidated to date.
Further scholarly investigation is required to comprehensively understand this
ancient writing system. Artificial Intelligence technology is a promising
avenue for deciphering oracle bone characters, particularly concerning their
evolution. However, one of the challenges is the lack of datasets mapping the
evolution of these characters over time. In this study, we systematically
collected ancient characters from authoritative texts and websites spanning six
historical stages: Oracle Bone Characters - OBC (15th century B.C.), Bronze
Inscriptions - BI (13th to 221 B.C.), Seal Script - SS (11th to 8th centuries
B.C.), Spring and Autumn period Characters - SAC (770 to 476 B.C.), Warring
States period Characters - WSC (475 B.C. to 221 B.C.), and Clerical Script - CS
(221 B.C. to 220 A.D.). Subsequently, we constructed an extensive dataset,
namely EVolution Oracle Bone Characters (EVOBC), consisting of 229,170 images
representing 13,714 distinct character categories. We conducted validation and
simulated deciphering on the constructed dataset, and the results demonstrate
its high efficacy in aiding the study of oracle bone script. This openly
accessible dataset aims to digitalize ancient Chinese scripts across multiple
eras, facilitating the decipherment of oracle bone script by examining the
evolution of glyph forms.
\\ ( https://arxiv.org/abs/2401.12467 ,  3115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12497
Date: Tue, 23 Jan 2024 05:43:15 GMT   (3811kb,D)

Title: Building Minimal and Reusable Causal State Abstractions for
  Reinforcement Learning
Authors: Zizhao Wang, Caroline Wang, Xuesu Xiao, Yuke Zhu, Peter Stone
Categories: cs.AI cs.LG cs.RO
Comments: Accepted at AAAI24
ACM-class: I.2.9; I.2.8; I.2.6
\\
  Two desiderata of reinforcement learning (RL) algorithms are the ability to
learn from relatively little experience and the ability to learn policies that
generalize to a range of problem specifications. In factored state spaces, one
approach towards achieving both goals is to learn state abstractions, which
only keep the necessary variables for learning the tasks at hand. This paper
introduces Causal Bisimulation Modeling (CBM), a method that learns the causal
relationships in the dynamics and reward functions for each task to derive a
minimal, task-specific abstraction. CBM leverages and improves implicit
modeling to train a high-fidelity causal dynamics model that can be reused for
all tasks in the same environment. Empirical validation on manipulation
environments and Deepmind Control Suite reveals that CBM's learned implicit
dynamics models identify the underlying causal relationships and state
abstractions more accurately than explicit ones. Furthermore, the derived state
abstractions allow a task learner to achieve near-oracle levels of sample
efficiency and outperform baselines on all tasks.
\\ ( https://arxiv.org/abs/2401.12497 ,  3811kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12550
Date: Tue, 23 Jan 2024 08:19:00 GMT   (459kb,D)

Title: UR4NNV: Neural Network Verification, Under-approximation Reachability
  Works!
Authors: Zhen Liang, Taoran Wu, Ran Zhao, Bai Xue, Ji Wang, Wenjing Yang,
  Shaojun Deng and Wanwei Liu
Categories: cs.AI cs.LG
Comments: 11 pages, 4 figures
MSC-class: 68Q60, 68T07
ACM-class: D.2.4; I.2.0
\\
  Recently, formal verification of deep neural networks (DNNs) has garnered
considerable attention, and over-approximation based methods have become
popular due to their effectiveness and efficiency. However, these strategies
face challenges in addressing the "unknown dilemma" concerning whether the
exact output region or the introduced approximation error violates the property
in question. To address this, this paper introduces the UR4NNV verification
framework, which utilizes under-approximation reachability analysis for DNN
verification for the first time. UR4NNV focuses on DNNs with Rectified Linear
Unit (ReLU) activations and employs a binary tree branch-based
under-approximation algorithm. In each epoch, UR4NNV under-approximates a
sub-polytope of the reachable set and verifies this polytope against the given
property. Through a trial-and-error approach, UR4NNV effectively falsifies DNN
properties while providing confidence levels when reaching verification epoch
bounds and failing falsifying properties. Experimental comparisons with
existing verification methods demonstrate the effectiveness and efficiency of
UR4NNV, significantly reducing the impact of the "unknown dilemma".
\\ ( https://arxiv.org/abs/2401.12550 ,  459kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12557
Date: Tue, 23 Jan 2024 08:27:38 GMT   (7kb)

Title: Balancing the AI Strength of Roles in Self-Play Training with Regret
  Matching+
Authors: Xiaoxi Wang
Categories: cs.AI
\\
  When training artificial intelligence for games encompassing multiple roles,
the development of a generalized model capable of controlling any character
within the game presents a viable option. This strategy not only conserves
computational resources and time during the training phase but also reduces
resource requirements during deployment. training such a generalized model
often encounters challenges related to uneven capabilities when controlling
different roles. A simple method is introduced based on Regret Matching+, which
facilitates a more balanced performance of strength by the model when
controlling various roles.
\\ ( https://arxiv.org/abs/2401.12557 ,  7kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12599
Date: Tue, 23 Jan 2024 09:54:36 GMT   (4309kb,D)

Title: Revolutionizing Retrieval-Augmented Generation with Enhanced PDF
  Structure Recognition
Authors: Demiao Lin (chatdoc.com)
Categories: cs.AI
Comments: 18 pages, 16 figures
\\
  With the rapid development of Large Language Models (LLMs),
Retrieval-Augmented Generation (RAG) has become a predominant method in the
field of professional knowledge-based question answering. Presently, major
foundation model companies have opened up Embedding and Chat API interfaces,
and frameworks like LangChain have already integrated the RAG process. It
appears that the key models and steps in RAG have been resolved, leading to the
question: are professional knowledge QA systems now approaching perfection?
This article discovers that current primary methods depend on the premise of
accessing high-quality text corpora. However, since professional documents are
mainly stored in PDFs, the low accuracy of PDF parsing significantly impacts
the effectiveness of professional knowledge-based QA. We conducted an empirical
RAG experiment across hundreds of questions from the corresponding real-world
professional documents. The results show that, ChatDOC, a RAG system equipped
with a panoptic and pinpoint PDF parser, retrieves more accurate and complete
segments, and thus better answers. Empirical experiments show that ChatDOC is
superior to baseline on nearly 47% of questions, ties for 38% of cases, and
falls short on only 15% of cases. It shows that we may revolutionize RAG with
enhanced PDF structure recognition.
\\ ( https://arxiv.org/abs/2401.12599 ,  4309kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12624
Date: Tue, 23 Jan 2024 10:23:13 GMT   (3055kb,D)

Title: Knowledge Distillation from Language-Oriented to Emergent Communication
  for Multi-Agent Remote Control
Authors: Yongjun Kim, Sejin Seo, Jihong Park, Mehdi Bennis, Seong-Lyun Kim,
  Junil Choi
Categories: cs.AI cs.IT cs.LG cs.NI math.IT
\\
  In this work, we compare emergent communication (EC) built upon multi-agent
deep reinforcement learning (MADRL) and language-oriented semantic
communication (LSC) empowered by a pre-trained large language model (LLM) using
human language. In a multi-agent remote navigation task, with multimodal input
data comprising location and channel maps, it is shown that EC incurs high
training cost and struggles when using multimodal data, whereas LSC yields high
inference computing cost due to the LLM's large size. To address their
respective bottlenecks, we propose a novel framework of language-guided EC
(LEC) by guiding the EC training using LSC via knowledge distillation (KD).
Simulations corroborate that LEC achieves faster travel time while avoiding
areas with poor channel conditions, as well as speeding up the MADRL training
convergence by up to 61.8% compared to EC.
\\ ( https://arxiv.org/abs/2401.12624 ,  3055kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12666
Date: Tue, 23 Jan 2024 11:21:32 GMT   (5685kb,D)

Title: EL-VIT: Probing Vision Transformer with Interactive Visualization
Authors: Hong Zhou, Rui Zhang, Peifeng Lai, Chaoran Guo, Yong Wang, Zhida Sun
  and Junjie Li
Categories: cs.AI
Comments: 10 pages, 7 figures, conference
\\
  Nowadays, Vision Transformer (ViT) is widely utilized in various computer
vision tasks, owing to its unique self-attention mechanism. However, the model
architecture of ViT is complex and often challenging to comprehend, leading to
a steep learning curve. ViT developers and users frequently encounter
difficulties in interpreting its inner workings. Therefore, a visualization
system is needed to assist ViT users in understanding its functionality. This
paper introduces EL-VIT, an interactive visual analytics system designed to
probe the Vision Transformer and facilitate a better understanding of its
operations. The system consists of four layers of visualization views. The
first three layers include model overview, knowledge background graph, and
model detail view. These three layers elucidate the operation process of ViT
from three perspectives: the overall model architecture, detailed explanation,
and mathematical operations, enabling users to understand the underlying
principles and the transition process between layers. The fourth interpretation
view helps ViT users and experts gain a deeper understanding by calculating the
cosine similarity between patches. Our two usage scenarios demonstrate the
effectiveness and usability of EL-VIT in helping ViT users understand the
working mechanism of ViT.
\\ ( https://arxiv.org/abs/2401.12666 ,  5685kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12672
Date: Tue, 23 Jan 2024 11:29:19 GMT   (4385kb,D)

Title: ChatGraph: Chat with Your Graphs
Authors: Yun Peng, Sen Lin, Qian Chen, Lyu Xu, Xiaojun Ren, Yafei Li, Jianliang
  Xu
Categories: cs.AI
\\
  Graph analysis is fundamental in real-world applications. Traditional
approaches rely on SPARQL-like languages or clicking-and-dragging interfaces to
interact with graph data. However, these methods either require users to
possess high programming skills or support only a limited range of graph
analysis functionalities. To address the limitations, we propose a large
language model (LLM)-based framework called ChatGraph. With ChatGraph, users
can interact with graphs through natural language, making it easier to use and
more flexible than traditional approaches. The core of ChatGraph lies in
generating chains of graph analysis APIs based on the understanding of the
texts and graphs inputted in the user prompts. To achieve this, ChatGraph
consists of three main modules: an API retrieval module that searches for
relevant APIs, a graph-aware LLM module that enables the LLM to comprehend
graphs, and an API chain-oriented finetuning module that guides the LLM in
generating API chains.
\\ ( https://arxiv.org/abs/2401.12672 ,  4385kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12700
Date: Tue, 23 Jan 2024 12:07:20 GMT   (1567kb,D)

Title: Securing Recommender System via Cooperative Training
Authors: Qingyang Wang, Chenwang Wu, Defu Lian, Enhong Chen
Categories: cs.AI
Comments: arXiv admin note: text overlap with arXiv:2210.13762
\\
  Recommender systems are often susceptible to well-crafted fake profiles,
leading to biased recommendations. Among existing defense methods,
data-processing-based methods inevitably exclude normal samples, while
model-based methods struggle to enjoy both generalization and robustness. To
this end, we suggest integrating data processing and the robust model to
propose a general framework, Triple Cooperative Defense (TCD), which employs
three cooperative models that mutually enhance data and thereby improve
recommendation robustness. Furthermore, Considering that existing attacks
struggle to balance bi-level optimization and efficiency, we revisit poisoning
attacks in recommender systems and introduce an efficient attack strategy,
Co-training Attack (Co-Attack), which cooperatively optimizes the attack
optimization and model training, considering the bi-level setting while
maintaining attack efficiency. Moreover, we reveal a potential reason for the
insufficient threat of existing attacks is their default assumption of
optimizing attacks in undefended scenarios. This overly optimistic setting
limits the potential of attacks. Consequently, we put forth a Game-based
Co-training Attack (GCoAttack), which frames the proposed CoAttack and TCD as a
game-theoretic process, thoroughly exploring CoAttack's attack potential in the
cooperative training of attack and defense. Extensive experiments on three real
datasets demonstrate TCD's superiority in enhancing model robustness.
Additionally, we verify that the two proposed attack strategies significantly
outperform existing attacks, with game-based GCoAttack posing a greater
poisoning threat than CoAttack.
\\ ( https://arxiv.org/abs/2401.12700 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12731
Date: Tue, 23 Jan 2024 13:04:02 GMT   (460kb,D)

Title: The Distributional Uncertainty of the SHAP score in Explainable Machine
  Learning
Authors: Santiago Cifuentes and Leopoldo Bertossi and Nina Pardal and Sergio
  Abriola and Maria Vanina Martinez and Miguel Romero
Categories: cs.AI cs.LG cs.LO
MSC-class: 68T37, 68T27
\\
  Attribution scores reflect how important the feature values in an input
entity are for the output of a machine learning model. One of the most popular
attribution scores is the SHAP score, which is an instantiation of the general
Shapley value used in coalition game theory. The definition of this score
relies on a probability distribution on the entity population. Since the exact
distribution is generally unknown, it needs to be assigned subjectively or be
estimated from data, which may lead to misleading feature scores. In this
paper, we propose a principled framework for reasoning on SHAP scores under
unknown entity population distributions. In our framework, we consider an
uncertainty region that contains the potential distributions, and the SHAP
score of a feature becomes a function defined over this region. We study the
basic problems of finding maxima and minima of this function, which allows us
to determine tight ranges for the SHAP scores of all features. In particular,
we pinpoint the complexity of these problems, and other related ones, showing
them to be NP-complete. Finally, we present experiments on a real-world
dataset, showing that our framework may contribute to a more robust feature
scoring.
\\ ( https://arxiv.org/abs/2401.12731 ,  460kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12783
Date: Tue, 23 Jan 2024 14:11:29 GMT   (614kb,D)

Title: A Review of Deep Learning Methods for Photoplethysmography Data
Authors: Guangkun Nie, Jiabao Zhu, Gongzheng Tang, Deyun Zhang, Shijia Geng,
  Qinghao Zhao, Shenda Hong
Categories: cs.AI cs.LG eess.SP
\\
  Photoplethysmography (PPG) is a highly promising device due to its advantages
in portability, user-friendly operation, and non-invasive capabilities to
measure a wide range of physiological information. Recent advancements in deep
learning have demonstrated remarkable outcomes by leveraging PPG signals for
tasks related to personal health management and other multifaceted
applications. In this review, we systematically reviewed papers that applied
deep learning models to process PPG data between January 1st of 2017 and July
31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed
from three key perspectives: tasks, models, and data. We finally extracted 193
papers where different deep learning frameworks were used to process PPG
signals. Based on the tasks addressed in these papers, we categorized them into
two major groups: medical-related, and non-medical-related. The medical-related
tasks were further divided into seven subgroups, including blood pressure
analysis, cardiovascular monitoring and diagnosis, sleep health, mental health,
respiratory monitoring and analysis, blood glucose analysis, as well as others.
The non-medical-related tasks were divided into four subgroups, which encompass
signal processing, biometric identification, electrocardiogram reconstruction,
and human activity recognition. In conclusion, significant progress has been
made in the field of using deep learning methods to process PPG data recently.
This allows for a more thorough exploration and utilization of the information
contained in PPG signals. However, challenges remain, such as limited quantity
and quality of publicly available databases, a lack of effective validation in
real-world scenarios, and concerns about the interpretability, scalability, and
complexity of deep learning models. Moreover, there are still emerging research
areas that require further investigation.
\\ ( https://arxiv.org/abs/2401.12783 ,  614kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12846
Date: Tue, 23 Jan 2024 15:29:26 GMT   (2031kb,D)

Title: How well can large language models explain business processes?
Authors: Dirk Fahland, Fabian Fournier, Lior Limonad, Inna Skarbovsky, Ava J.E.
  Swevels
Categories: cs.AI
Comments: 39 pages, 12 figures
MSC-class: 68T01
\\
  Large Language Models (LLMs) are likely to play a prominent role in future
AI-augmented business process management systems (ABPMSs) catering
functionalities across all system lifecycle stages. One such system's
functionality is Situation-Aware eXplainability (SAX), which relates to
generating causally sound and yet human-interpretable explanations that take
into account the process context in which the explained condition occurred. In
this paper, we present the SAX4BPM framework developed to generate SAX
explanations. The SAX4BPM suite consists of a set of services and a central
knowledge repository. The functionality of these services is to elicit the
various knowledge ingredients that underlie SAX explanations. A key innovative
component among these ingredients is the causal process execution view. In this
work, we integrate the framework with an LLM to leverage its power to
synthesize the various input ingredients for the sake of improved SAX
explanations. Since the use of LLMs for SAX is also accompanied by a certain
degree of doubt related to its capacity to adequately fulfill SAX along with
its tendency for hallucination and lack of inherent capacity to reason, we
pursued a methodological evaluation of the quality of the generated
explanations. To this aim, we developed a designated scale and conducted a
rigorous user study. Our findings show that the input presented to the LLMs
aided with the guard-railing of its performance, yielding SAX explanations
having better-perceived fidelity. This improvement is moderated by the
perception of trust and curiosity. More so, this improvement comes at the cost
of the perceived interpretability of the explanation.
\\ ( https://arxiv.org/abs/2401.12846 ,  2031kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12866
Date: Tue, 23 Jan 2024 16:00:45 GMT   (2060kb,D)

Title: Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported
  Coordination of Mobile Crowdsourcing
Authors: Ralf Bruns, Jeremias D\"otterl, J\"urgen Dunkel, Sascha Ossowski
Categories: cs.AI cs.LG cs.MA
Journal-ref: Sensors 2023, 23(2), 614
DOI: 10.3390/s23020614
\\
  Mobile crowdsourcing refers to systems where the completion of tasks
necessarily requires physical movement of crowdworkers in an on-demand
workforce. Evidence suggests that in such systems, tasks often get assigned to
crowdworkers who struggle to complete those tasks successfully, resulting in
high failure rates and low service quality. A promising solution to ensure
higher quality of service is to continuously adapt the assignment and respond
to failure-causing events by transferring tasks to better-suited workers who
use different routes or vehicles. However, implementing task transfers in
mobile crowdsourcing is difficult because workers are autonomous and may reject
transfer requests. Moreover, task outcomes are uncertain and need to be
predicted. In this paper, we propose different mechanisms to achieve outcome
prediction and task coordination in mobile crowdsourcing. First, we analyze
different data stream learning approaches for the prediction of task outcomes.
Second, based on the suggested prediction model, we propose and evaluate two
different approaches for task coordination with different degrees of autonomy:
an opportunistic approach for crowdshipping with collaborative, but
non-autonomous workers, and a market-based model with autonomous workers for
crowdsensing.
\\ ( https://arxiv.org/abs/2401.12866 ,  2060kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12869
Date: Tue, 23 Jan 2024 16:03:17 GMT   (1139kb,D)

Title: TroVE: Inducing Verifiable and Efficient Toolboxes for Solving
  Programmatic Tasks
Authors: Zhiruo Wang, Daniel Fried, Graham Neubig
Categories: cs.AI
\\
  Language models (LMs) can solve tasks such as answering questions about
tables or images by writing programs. However, using primitive functions often
leads to verbose and error-prone programs, and higher-level functions require
expert design. To enable better solutions without human labor, we ask code LMs
to curate reusable high-level functions, and use them to write solutions. We
present TROVE, a training-free method of inducing a verifiable and efficient
toolbox of functions, by generating via using, growing, and periodically
trimming the toolbox. On 11 datasets from math, table question answering, and
image reasoning tasks, TROVE consistently yields simpler solutions with higher
accuracy than baselines using CODELLAMA and previous methods using GPT, while
using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more
accurate human verification than baselines. With the same pipeline, it creates
diverse functions for varied tasks and datasets, providing insights into their
individual characteristics.
\\ ( https://arxiv.org/abs/2401.12869 ,  1139kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12915
Date: Tue, 23 Jan 2024 17:07:18 GMT   (3040kb,D)

Title: Red Teaming Visual Language Models
Authors: Mukai Li and Lei Li and Yuwei Yin and Masood Ahmed and Zhenguang Liu
  and Qi Liu
Categories: cs.AI cs.CL cs.CV
Comments: Working in progress
\\
  VLMs (Vision-Language Models) extend the capabilities of LLMs (Large Language
Models) to accept multimodal inputs. Since it has been verified that LLMs can
be induced to generate harmful or inaccurate content through specific test
cases (termed as Red Teaming), how VLMs perform in similar scenarios,
especially with their combination of textual and visual inputs, remains a
question. To explore this problem, we present a novel red teaming dataset
RTVLM, which encompasses 10 subtasks (e.g., image misleading, multi-modal
jail-breaking, face fairness, etc) under 4 primary aspects (faithfulness,
privacy, safety, fairness). Our RTVLM is the first red-teaming dataset to
benchmark current VLMs in terms of these 4 different aspects. Detailed analysis
shows that 10 prominent open-sourced VLMs struggle with the red teaming in
different degrees and have up to 31% performance gap with GPT-4V. Additionally,
we simply apply red teaming alignment to LLaVA-v1.5 with Supervised Fine-tuning
(SFT) using RTVLM, and this bolsters the models' performance with 10% in RTVLM
test set, 13% in MM-Hal, and without noticeable decline in MM-Bench,
overpassing other LLaVA-based models with regular alignment data. This reveals
that current open-sourced VLMs still lack red teaming alignment. Our code and
datasets will be open-source.
\\ ( https://arxiv.org/abs/2401.12915 ,  3040kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12917
Date: Tue, 23 Jan 2024 17:09:25 GMT   (13774kb,D)

Title: Active Inference as a Model of Agency
Authors: Lancelot Da Costa, Samuel Tenka, Dominic Zhao, Noor Sajid
Categories: cs.AI
Comments: Accepted in RLDM2022 for the workshop 'RL as a model of agency'
\\
  Is there a canonical way to think of agency beyond reward maximisation? In
this paper, we show that any type of behaviour complying with physically sound
assumptions about how macroscopic biological agents interact with the world
canonically integrates exploration and exploitation in the sense of minimising
risk and ambiguity about states of the world. This description, known as active
inference, refines the free energy principle, a popular descriptive framework
for action and perception originating in neuroscience. Active inference
provides a normative Bayesian framework to simulate and model agency that is
widely used in behavioural neuroscience, reinforcement learning (RL) and
robotics. The usefulness of active inference for RL is three-fold. \emph{a})
Active inference provides a principled solution to the exploration-exploitation
dilemma that usefully simulates biological agency. \emph{b}) It provides an
explainable recipe to simulate behaviour, whence behaviour follows as an
explainable mixture of exploration and exploitation under a generative world
model, and all differences in behaviour are explicit in differences in world
model. \emph{c}) This framework is universal in the sense that it is
theoretically possible to rewrite any RL algorithm conforming to the
descriptive assumptions of active inference as an active inference algorithm.
Thus, active inference can be used as a tool to uncover and compare the
commitments and assumptions of more specific models of agency.
\\ ( https://arxiv.org/abs/2401.12917 ,  13774kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12920
Date: Tue, 23 Jan 2024 17:14:01 GMT   (1910kb,D)

Title: Truck Parking Usage Prediction with Decomposed Graph Neural Networks
Authors: Rei Tamaru, Yang Cheng, Steven Parker, Ernie Perry, Bin Ran, Soyoung
  Ahn
Categories: cs.AI
Comments: 10 pages, 5 figures, 3 tables, Manuscript for IEEE Transactions on
  Intelligent Transportation Systems
\\
  Truck parking on freight corridors faces various challenges, such as
insufficient parking spaces and compliance with Hour-of-Service (HOS)
regulations. These constraints often result in unauthorized parking practices,
causing safety concerns. To enhance the safety of freight operations, providing
accurate parking usage prediction proves to be a cost-effective solution.
Despite the existing research demonstrating satisfactory accuracy for
predicting individual truck parking site usage, few approaches have been
proposed for predicting usage with spatial dependencies of multiple truck
parking sites. We present the Regional Temporal Graph Neural Network (RegT-GCN)
as a predictive framework for assessing parking usage across the entire state
to provide better truck parking information and mitigate unauthorized parking.
The framework leverages the topological structures of truck parking site
distributions and historical parking data to predict occupancy rates across a
state. To achieve this, we introduce a Regional Decomposition approach, which
effectively captures the geographical characteristics. We also introduce the
spatial module working efficiently with the temporal module. Evaluation results
demonstrate that the proposed model surpasses other baseline models, improving
the performance by more than $20\%$ compared with the original model. The
proposed model allows truck parking sites' percipience of the topological
structures and provides higher performance.
\\ ( https://arxiv.org/abs/2401.12920 ,  1910kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12246
Date: Sat, 20 Jan 2024 12:29:27 GMT   (525kb,D)

Title: Orion-14B: Open-source Multilingual Large Language Models
Authors: Du Chen, Yi Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan,
  Leichao Xu, Dacheng Zhang, Zhipeng Zhang, Kun Han
Categories: cs.CL cs.LG
Comments: Authors are alphabetically listed by last names, except the
  corresponding author who is listed last
\\
  In this study, we introduce Orion-14B, a collection of multilingual large
language models with 14 billion parameters. We utilize a data scheduling
approach to train a foundational model on a diverse corpus of 2.5 trillion
tokens, sourced from texts in English, Chinese, Japanese, Korean, and other
languages. Additionally, we fine-tuned a series of models tailored for
conversational applications and other specific use cases. Our evaluation
results demonstrate that Orion-14B achieves state-of-the-art performance across
a broad spectrum of tasks. We make the Orion-14B model family and its
associated code publicly accessible https://github.com/OrionStarAI/Orion,
aiming to inspire future research and practical applications in the field.
\\ ( https://arxiv.org/abs/2401.12246 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12292
Date: Mon, 22 Jan 2024 19:00:08 GMT   (14338kb,D)

Title: GRATH: Gradual Self-Truthifying for Large Language Models
Authors: Weixin Chen, Bo Li
Categories: cs.CL cs.AI
\\
  Truthfulness is paramount for large language models (LLMs) as they are
increasingly deployed in real-world applications. However, existing LLMs still
struggle with generating truthful answers and content, as evidenced by their
modest performance on benchmarks like TruthfulQA. To address this issue, we
propose GRAdual self-truTHifying (GRATH), a novel post-processing method to
enhance truthfulness of LLMs. GRATH utilizes out-of-domain question prompts to
generate corresponding answers and adaptively optimizes the model via direct
preference optimization (DPO). Note that during this process, GRATH learns
truthfulness in a self-supervised manner without requiring annotated answers.
In particular, GRATH first generates pairwise truthfulness training data by
prompting the LLM itself, with each pair containing a question and its correct
and incorrect answers. The model is then fine-tuned using DPO to learn from the
difference between answer pairs. Subsequently, GRATH iteratively refines the
truthfulness data and optimizes the model, leading to a gradual improvement in
model truthfulness. Empirically, we evaluate GRATH using different 7B-LLMs and
compare with LLMs with similar or even larger sizes on benchmark datasets. Our
results show that GRATH effectively improves LLMs' truthfulness without
compromising other core capabilities. Notably, GRATH achieves state-of-the-art
performance on TruthfulQA, with MC1 accuracy as 54.71% and MC2 accuracy as
69.10%, which even surpass those on larger-scale models, such as
Llama2-Chat-70B, by 23.62% and 24.18%, respectively.
\\ ( https://arxiv.org/abs/2401.12292 ,  14338kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12295
Date: Mon, 22 Jan 2024 19:00:11 GMT   (1328kb,D)

Title: Cheap Learning: Maximising Performance of Language Models for Social
  Data Science Using Minimal Data
Authors: Leonardo Castro-Gonzalez and Yi-Ling Chung and Hannak Rose Kirk and
  John Francis and Angus R. Williams and Pica Johansson and Jonathan Bright
Categories: cs.CL
Comments: 39 pages, 10 figures, 6 tables
ACM-class: I.2.7; J.4
\\
  The field of machine learning has recently made significant progress in
reducing the requirements for labelled training data when building new models.
These `cheaper' learning techniques hold significant potential for the social
sciences, where development of large labelled training datasets is often a
significant practical impediment to the use of machine learning for analytical
tasks. In this article we review three `cheap' techniques that have developed
in recent years: weak supervision, transfer learning and prompt engineering.
For the latter, we also review the particular case of zero-shot prompting of
large language models. For each technique we provide a guide of how it works
and demonstrate its application across six different realistic social science
applications (two different tasks paired with three different dataset makeups).
We show good performance for all techniques, and in particular we demonstrate
how prompting of large language models can achieve high accuracy at very low
cost. Our results are accompanied by a code repository to make it easy for
others to duplicate our work and use it in their own research. Overall, our
article is intended to stimulate further uptake of these techniques in the
social sciences.
\\ ( https://arxiv.org/abs/2401.12295 ,  1328kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12326
Date: Mon, 22 Jan 2024 19:39:05 GMT   (112kb,D)

Title: Fine-tuning Large Language Models for Multigenerator, Multidomain, and
  Multilingual Machine-Generated Text Detection
Authors: Feng Xiong, Thanet Markchom, Ziwei Zheng, Subin Jung, Varun Ojha,
  Huizhi Liang
Categories: cs.CL cs.AI
\\
  SemEval-2024 Task 8 introduces the challenge of identifying machine-generated
texts from diverse Large Language Models (LLMs) in various languages and
domains. The task comprises three subtasks: binary classification in
monolingual and multilingual (Subtask A), multi-class classification (Subtask
B), and mixed text detection (Subtask C). This paper focuses on Subtask A & B.
Each subtask is supported by three datasets for training, development, and
testing. To tackle this task, two methods: 1) using traditional machine
learning (ML) with natural language preprocessing (NLP) for feature extraction,
and 2) fine-tuning LLMs for text classification. The results show that
transformer models, particularly LoRA-RoBERTa, exceed traditional ML methods in
effectiveness, with majority voting being particularly effective in
multilingual contexts for identifying machine-generated texts.
\\ ( https://arxiv.org/abs/2401.12326 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12343
Date: Mon, 22 Jan 2024 20:17:06 GMT   (752kb,D)

Title: Subgraph Extraction-based Feedback-guided Iterative Scheduling for HLS
Authors: Hanchen Ye, David Z. Pan, Chris Leary, Deming Chen, Xiaoqing Xu
Categories: cs.CL
Comments: DATE'24
\\
  This paper proposes ISDC, a novel feedback-guided iterative system of
difference constraints (SDC) scheduling algorithm for high-level synthesis
(HLS). ISDC leverages subgraph extraction-based low-level feedback from
downstream tools like logic synthesizers to iteratively refine HLS scheduling.
Technical innovations include: (1) An enhanced SDC formulation that effectively
integrates low-level feedback into the linear-programming (LP) problem; (2) A
fanout and window-based subgraph extraction mechanism driving the feedback
cycle; (3) A no-human-in-loop ISDC flow compatible with a wide range of
downstream tools and process design kits (PDKs). Evaluation shows that ISDC
reduces register usage by 28.5% against an industrial-strength open-source HLS
tool.
\\ ( https://arxiv.org/abs/2401.12343 ,  752kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12375
Date: Mon, 22 Jan 2024 21:59:00 GMT   (339kb)

Title: Development of an NLP-driven computer-based test guide for visually
  impaired students
Authors: Tubo Faustinah Nemieboka, Ikechukwu E. Onyenwe, Doris C. Asogwa
Categories: cs.CL cs.AI
Comments: 10 pages, 6 figures
Journal-ref: International Journal of Advanced Research in Computer and
  Communication Engineering (IJARCCE) Vol. 12, Issue 9, September 2023
\\
  In recent years, advancements in Natural Language Processing (NLP) techniques
have revolutionized the field of accessibility and exclusivity of testing,
particularly for visually impaired students (VIS). CBT has shown in years back
its relevance in terms of administering exams electronically, making the test
process easier, providing quicker and more accurate results, and offering
greater flexibility and accessibility for candidates. Yet, its relevance was
not felt by the visually impaired students as they cannot access printed
documents. Hence, in this paper, we present an NLP-driven Computer-Based Test
guide for visually impaired students. It employs a speech technology
pre-trained methods to provide real-time assistance and support to visually
impaired students. The system utilizes NLP technologies to convert the
text-based questions and the associated options in a machine-readable format.
Subsequently, the speech technology pre-trained model processes the converted
text enabling the VIS to comprehend and analyze the content. Furthermore, we
validated that this pre-trained model is not perverse by testing for accuracy
using sample audio datasets labels (A, B, C, D, E, F, G) to compare with the
voice recordings obtained from 20 VIS which is been predicted by the system to
attain values for precision, recall, and F1-scores. These metrics are used to
assess the performance of the pre-trained model and have indicated that it is
proficient enough to give its better performance to the evaluated system. The
methodology adopted for this system is Object Oriented Analysis and Design
Methodology (OOADM) where Objects are discussed and built by modeling
real-world instances.
\\ ( https://arxiv.org/abs/2401.12375 ,  339kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12382
Date: Mon, 22 Jan 2024 22:16:55 GMT   (349kb)

Title: Longitudinal Sentiment Classification of Reddit Posts
Authors: Fabian Nwaoha, Ziyad Gaffar, Ho Joon Chun, Marina Sokolova
Categories: cs.CL cs.LG cs.SI
Comments: 11 pages, 10 figures, 4 tables
ACM-class: I.2.6
\\
  We report results of a longitudinal sentiment classification of Reddit posts
written by students of four major Canadian universities. We work with the texts
of the posts, concentrating on the years 2020-2023. By finely tuning a
sentiment threshold to a range of [-0.075,0.075], we successfully built
classifiers proficient in categorizing post sentiments into positive and
negative categories. Noticeably, our sentiment classification results are
consistent across the four university data sets.
\\ ( https://arxiv.org/abs/2401.12382 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12406
Date: Mon, 22 Jan 2024 23:35:09 GMT   (3002kb,D)

Title: Enhancing In-context Learning via Linear Probe Calibration
Authors: Momin Abbas and Yi Zhou and Parikshit Ram and Nathalie Baracaldo and
  Horst Samulowitz and Theodoros Salonidis and Tianyi Chen
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at AISTATS2024
\\
  In-context learning (ICL) is a new paradigm for natural language processing
that utilizes Generative Pre-trained Transformer (GPT)-like models. This
approach uses prompts that include in-context demonstrations to generate the
corresponding output for a new query input. However, applying ICL in real cases
does not scale with the number of samples, and lacks robustness to different
prompt templates and demonstration permutations. In this paper, we first show
that GPT-like models using ICL result in unreliable predictions based on a new
metric based on Shannon entropy. Then, to solve this problem, we propose a new
technique called the Linear Probe Calibration (LinC), a method that calibrates
the model's output probabilities, resulting in reliable predictions and
improved performance, while requiring only minimal additional samples (as few
as five labeled data samples). LinC significantly enhances the ICL test
performance of GPT models on various benchmark datasets, with an average
improvement of up to 21%, and up to a 50% improvement in some cases, and
significantly boosts the performance of PEFT methods, especially in the low
resource regime. Moreover, LinC achieves lower expected calibration error, and
is highly robust to varying label proportions, prompt templates, and
demonstration permutations. Our code is available at
\url{https://github.com/mominabbass/LinC}.
\\ ( https://arxiv.org/abs/2401.12406 ,  3002kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12413
Date: Mon, 22 Jan 2024 23:55:00 GMT   (1861kb,D)

Title: How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual
  Translation via Tiny Multi-Parallel Data
Authors: Di Wu, Shaomu Tan, Yan Meng, David Stap and Christof Monz
Categories: cs.CL cs.LG
Comments: 15 pages, 5 figures
\\
  Zero-shot translation is an open problem, aiming to translate between
language pairs unseen during training in Multilingual Machine Translation
(MMT). A common, albeit resource-consuming, solution is to mine as many
translation directions as possible to add to the parallel corpus. In this
paper, we show that the zero-shot capability of an English-centric model can be
easily enhanced by fine-tuning with a very small amount of multi-parallel data.
For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English
overall improvements (870 directions) can be achieved by using only 100
multi-parallel samples, meanwhile preserving capability in English-centric
directions. We further study the size effect of fine-tuning data and its
transfer capabilities. Surprisingly, our empirical analysis shows that
comparable overall improvements can be achieved even through fine-tuning in a
small, randomly sampled direction set (10\%). Also, the resulting non-English
performance is quite close to the upper bound (complete translation). Due to
its high efficiency and practicality, we encourage the community 1) to consider
the use of the fine-tuning method as a strong baseline for zero-shot
translation and 2) to construct more comprehensive and high-quality
multi-parallel data to cover real-world demand.
\\ ( https://arxiv.org/abs/2401.12413 ,  1861kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12461
Date: Tue, 23 Jan 2024 03:03:57 GMT   (876kb,D)

Title: Fast Adversarial Training against Textual Adversarial Attacks
Authors: Yichen Yang, Xin Liu, Kun He
Categories: cs.CL
Comments: 4 pages, 4 figures
\\
  Many adversarial defense methods have been proposed to enhance the
adversarial robustness of natural language processing models. However, most of
them introduce additional pre-set linguistic knowledge and assume that the
synonym candidates used by attackers are accessible, which is an ideal
assumption. We delve into adversarial training in the embedding space and
propose a Fast Adversarial Training (FAT) method to improve the model
robustness in the synonym-unaware scenario from the perspective of single-step
perturbation generation and perturbation initialization. Based on the
observation that the adversarial perturbations crafted by single-step and
multi-step gradient ascent are similar, FAT uses single-step gradient ascent to
craft adversarial examples in the embedding space to expedite the training
process. Based on the observation that the perturbations generated on the
identical training sample in successive epochs are similar, FAT fully utilizes
historical information when initializing the perturbation. Extensive
experiments demonstrate that FAT significantly boosts the robustness of BERT
models in the synonym-unaware scenario, and outperforms the defense baselines
under various attacks with character-level and word-level modifications.
\\ ( https://arxiv.org/abs/2401.12461 ,  876kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12472
Date: Tue, 23 Jan 2024 03:47:07 GMT   (1314kb,D)

Title: Contrastive Learning in Distilled Models
Authors: Valerie Lim, Kai Wen Ng, Kenneth Lim
Categories: cs.CL
\\
  Natural Language Processing models like BERT can provide state-of-the-art
word embeddings for downstream NLP tasks. However, these models yet to perform
well on Semantic Textual Similarity, and may be too large to be deployed as
lightweight edge applications. We seek to apply a suitable contrastive learning
method based on the SimCSE paper, to a model architecture adapted from a
knowledge distillation based model, DistilBERT, to address these two issues.
Our final lightweight model DistilFace achieves an average of 72.1 in
Spearman's correlation on STS tasks, a 34.2 percent improvement over BERT base.
\\ ( https://arxiv.org/abs/2401.12472 ,  1314kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12474
Date: Tue, 23 Jan 2024 03:56:22 GMT   (704kb,D)

Title: Large Language Models are Superpositions of All Characters: Attaining
  Arbitrary Role-play via Self-Alignment
Authors: Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou
Categories: cs.CL cs.LG
\\
  Considerable efforts have been invested in augmenting the role-playing
proficiency of open-source large language models (LLMs) by emulating
proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor
role-play capabilities, owing to the extensive knowledge of characters and
potential dialogues ingrained in their vast training corpora. Thus, in this
study, we introduce Ditto, a self-alignment method for role-play. Ditto
capitalizes on character knowledge, encouraging an instruction-following LLM to
simulate role-play dialogues as a variant of reading comprehension. This method
creates a role-play training set comprising 4,000 characters, surpassing the
scale of currently available datasets by tenfold regarding the number of roles.
Subsequently, we fine-tune the LLM using this self-generated dataset to augment
its role-playing capabilities. Upon evaluating our meticulously constructed and
reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in
various parameter scales, consistently maintains a consistent role identity and
provides accurate role-specific knowledge in multi-turn role-play
conversations. Notably, it outperforms all open-source role-play baselines,
showcasing performance levels comparable to advanced proprietary chatbots.
Furthermore, we present the first comprehensive cross-supervision alignment
experiment in the role-play domain, revealing that the intrinsic capabilities
of LLMs confine the knowledge within role-play. Meanwhile, the role-play styles
can be easily acquired with the guidance of smaller models. We open-source
related resources at https://github.com/OFA-Sys/Ditto.
\\ ( https://arxiv.org/abs/2401.12474 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12491
Date: Tue, 23 Jan 2024 05:19:47 GMT   (10811kb,D)

Title: Assessing and Understanding Creativity in Large Language Models
Authors: Yunpu Zhao, Rui Zhang, Wenyi Li, Di Huang, Jiaming Guo, Shaohui Peng,
  Yifan Hao, Yuanbo Wen, Xing Hu, Zidong Du, Qi Guo, Ling Li and Yunji Chen
Categories: cs.CL cs.AI
\\
  In the field of natural language processing, the rapid development of large
language model (LLM) has attracted more and more attention. LLMs have shown a
high level of creativity in various tasks, but the methods for assessing such
creativity are inadequate. The assessment of LLM creativity needs to consider
differences from humans, requiring multi-dimensional measurement while
balancing accuracy and efficiency. This paper aims to establish an efficient
framework for assessing the level of creativity in LLMs. By adapting the
modified Torrance Tests of Creative Thinking, the research evaluates the
creative performance of various LLMs across 7 tasks, emphasizing 4 criteria
including Fluency, Flexibility, Originality, and Elaboration. In this context,
we develop a comprehensive dataset of 700 questions for testing and an
LLM-based evaluation method. In addition, this study presents a novel analysis
of LLMs' responses to diverse prompts and role-play situations. We found that
the creativity of LLMs primarily falls short in originality, while excelling in
elaboration. Besides, the use of prompts and the role-play settings of the
model significantly influence creativity. Additionally, the experimental
results also indicate that collaboration among multiple LLMs can enhance
originality. Notably, our findings reveal a consensus between human evaluations
and LLMs regarding the personality traits that influence creativity. The
findings underscore the significant impact of LLM design on creativity and
bridges artificial intelligence and human creativity, offering insights into
LLMs' creativity and potential applications.
\\ ( https://arxiv.org/abs/2401.12491 ,  10811kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12492
Date: Tue, 23 Jan 2024 05:20:35 GMT   (7780kb,D)

Title: Comparing Human-Centered Language Modeling: Is it Better to Model
  Groups, Individual Traits, or Both?
Authors: Nikita Soni, Niranjan Balasubramanian, H. Andrew Schwartz, and Dirk
  Hovy
Categories: cs.CL cs.AI cs.LG
\\
  Natural language processing has made progress in incorporating human context
into its models, but whether it is more effective to use group-wise attributes
(e.g., over-45-year-olds) or model individuals remains open. Group attributes
are technically easier but coarse: not all 45-year-olds write the same way. In
contrast, modeling individuals captures the complexity of each person's
identity. It allows for a more personalized representation, but we may have to
model an infinite number of users and require data that may be impossible to
get. We compare modeling human context via group attributes, individual users,
and combined approaches. Combining group and individual features significantly
benefits user-level regression tasks like age estimation or personality
assessment from a user's documents. Modeling individual users significantly
improves the performance of single document-level classification tasks like
stance and topic detection. We also find that individual-user modeling does
well even without user's historical data.
\\ ( https://arxiv.org/abs/2401.12492 ,  7780kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12520
Date: Tue, 23 Jan 2024 06:30:05 GMT   (1227kb,D)

Title: Key Information Retrieval to Classify the Unstructured Data Content of
  Preferential Trade Agreements
Authors: Jiahui Zhao, Ziyi Meng, Stepan Gordeev, Zijie Pan, Dongjin Song,
  Sandro Steinbach, Caiwen Ding
Categories: cs.CL cs.IR cs.LG
Comments: AI4TS Workshop@AAAI 2024 accepted publication
\\
  With the rapid proliferation of textual data, predicting long texts has
emerged as a significant challenge in the domain of natural language
processing. Traditional text prediction methods encounter substantial
difficulties when grappling with long texts, primarily due to the presence of
redundant and irrelevant information, which impedes the model's capacity to
capture pivotal insights from the text. To address this issue, we introduce a
novel approach to long-text classification and prediction. Initially, we employ
embedding techniques to condense the long texts, aiming to diminish the
redundancy therein. Subsequently,the Bidirectional Encoder Representations from
Transformers (BERT) embedding method is utilized for text classification
training. Experimental outcomes indicate that our method realizes considerable
performance enhancements in classifying long texts of Preferential Trade
Agreements. Furthermore, the condensation of text through embedding methods not
only augments prediction accuracy but also substantially reduces computational
complexity. Overall, this paper presents a strategy for long-text prediction,
offering a valuable reference for researchers and engineers in the natural
language processing sphere.
\\ ( https://arxiv.org/abs/2401.12520 ,  1227kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12522
Date: Tue, 23 Jan 2024 06:36:49 GMT   (2655kb,D)

Title: BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language
  Models
Authors: Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming
  Lu, Rong Xiao
Categories: cs.CL cs.AI cs.LG
Comments: Source code at https://github.com/linfeng93/BiTA
\\
  Large language models (LLMs) commonly employ autoregressive generation during
inference, leading to high memory bandwidth demand and consequently extended
latency. To mitigate this inefficiency, we present Bi-directional Tuning for
lossless Acceleration (BiTA), an innovative method expediting LLMs via
streamlined semi-autoregressive generation and draft verification. Inspired by
the concept of prompt tuning, we enhance LLMs with a parameter-efficient design
called bi-directional tuning for the capability in semi-autoregressive
generation. Employing efficient tree-based decoding, the models perform draft
candidate generation and verification in parallel, ensuring outputs identical
to their autoregressive counterparts under greedy sampling. BiTA serves as a
lightweight plug-in module, seamlessly boosting the inference efficiency of
existing LLMs without requiring additional assistance models or incurring
significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat
achieves a 2.7$\times$ speedup on the MT-Bench benchmark. Extensive experiments
confirm our method surpasses state-of-the-art acceleration techniques.
\\ ( https://arxiv.org/abs/2401.12522 ,  2655kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12566
Date: Tue, 23 Jan 2024 08:49:23 GMT   (2110kb,D)

Title: Automated Fact-Checking of Climate Change Claims with Large Language
  Models
Authors: Markus Leippold and Saeid Ashraf Vaghefi and Dominik Stammbach and
  Veruska Muccione and Julia Bingler and Jingwei Ni and Chiara Colesanti-Senni
  and Tobias Wekhof and Tobias Schimanski and Glen Gostlow and Tingyu Yu and
  Juerg Luterbacher and Christian Huggel
Categories: cs.CL
\\
  This paper presents Climinator, a novel AI-based tool designed to automate
the fact-checking of climate change claims. Utilizing an array of Large
Language Models (LLMs) informed by authoritative sources like the IPCC reports
and peer-reviewed scientific literature, Climinator employs an innovative
Mediator-Advocate framework. This design allows Climinator to effectively
synthesize varying scientific perspectives, leading to robust, evidence-based
evaluations. Our model demonstrates remarkable accuracy when testing claims
collected from Climate Feedback and Skeptical Science. Notably, when
integrating an advocate with a climate science denial perspective in our
framework, Climinator's iterative debate process reliably converges towards
scientific consensus, underscoring its adeptness at reconciling diverse
viewpoints into science-based, factual conclusions. While our research is
subject to certain limitations and necessitates careful interpretation, our
approach holds significant potential. We hope to stimulate further research and
encourage exploring its applicability in other contexts, including political
fact-checking and legal domains.
\\ ( https://arxiv.org/abs/2401.12566 ,  2110kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12576
Date: Tue, 23 Jan 2024 09:11:07 GMT   (8475kb,D)

Title: LLMCheckup: Conversational Examination of Large Language Models via
  Interpretability Tools
Authors: Qianli Wang, Tatiana Anikina, Nils Feldhus, Josef van Genabith,
  Leonhard Hennig, Sebastian M\"oller
Categories: cs.CL cs.AI cs.LG
\\
  Interpretability tools that offer explanations in the form of a dialogue have
demonstrated their efficacy in enhancing users' understanding, as one-off
explanations may occasionally fall short in providing sufficient information to
the user. Current solutions for dialogue-based explanations, however, require
many dependencies and are not easily transferable to tasks they were not
designed for. With LLMCheckup, we present an easily accessible tool that allows
users to chat with any state-of-the-art large language model (LLM) about its
behavior. We enable LLMs to generate all explanations by themselves and take
care of intent recognition without fine-tuning, by connecting them with a broad
spectrum of Explainable AI (XAI) tools, e.g. feature attributions,
embedding-based similarity, and prompting strategies for counterfactual and
rationale generation. LLM (self-)explanations are presented as an interactive
dialogue that supports follow-up questions and generates suggestions.
LLMCheckup provides tutorials for operations available in the system, catering
to individuals with varying levels of expertise in XAI and supports multiple
input modalities. We introduce a new parsing strategy called multi-prompt
parsing substantially enhancing the parsing accuracy of LLMs. Finally, we
showcase the tasks of fact checking and commonsense question answering.
\\ ( https://arxiv.org/abs/2401.12576 ,  8475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12585
Date: Tue, 23 Jan 2024 09:33:31 GMT   (9048kb,D)

Title: SLANG: New Concept Comprehension of Large Language Models
Authors: Lingrui Mei, Shenghua Liu, Yiwei Wang, Baolong Bi, Xueqi Chen
Categories: cs.CL
\\
  The dynamic nature of language, particularly evident in the realm of slang
and memes on the Internet, poses serious challenges to the adaptability of
large language models (LLMs). Traditionally anchored to static datasets, these
models often struggle to keep up with the rapid linguistic evolution
characteristic of online communities. This research addresses the critical need
to bridge this gap, aiming to enhance LLMs' comprehension of evolving new
concepts on the internet, without the high cost and impracticality of continual
retraining. To address this issue, we propose a new benchmark $\textbf{SLANG}$
to assess LLMs' proficiency in comprehending emerging linguistic trends and a
baseline approach $\textbf{FOCUS}$, which uses causal inference to enhance LLMs
to understand new phrases and usage patterns. This approach involves
scrutinizing real-world instances of linguistic shifts, serving as contextual
beacons, to form more precise and contextually relevant connections between
newly emerging expressions and their intended meanings. The empirical analysis
shows that our causal inference-based approach outperforms the traditional
models in terms of precision and relevance in the interpretation of Internet
slang and memes.
\\ ( https://arxiv.org/abs/2401.12585 ,  9048kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12671
Date: Tue, 23 Jan 2024 11:25:34 GMT   (1011kb,D)

Title: Context Matters: Pushing the Boundaries of Open-Ended Answer Generation
  with Graph-Structured Knowledge Context
Authors: Somnath Banerjee, Amruit Sahoo, Sayan Layek, Avik Dutta, Rima Hazra,
  Animesh Mukherjee
Categories: cs.CL
\\
  In the continuously advancing AI landscape, crafting context-rich and
meaningful responses via Large Language Models (LLMs) is essential. Researchers
are becoming more aware of the challenges that LLMs with fewer parameters
encounter when trying to provide suitable answers to open-ended questions. To
address these hurdles, the integration of cutting-edge strategies, augmentation
of rich external domain knowledge to LLMs, offers significant improvements.
This paper introduces a novel framework that combines graph-driven context
retrieval in conjunction to knowledge graphs based enhancement, honing the
proficiency of LLMs, especially in domain specific community question answering
platforms like AskUbuntu, Unix, and ServerFault. We conduct experiments on
various LLMs with different parameter sizes to evaluate their ability to ground
knowledge and determine factual accuracy in answers to open-ended questions.
Our methodology GraphContextGen consistently outperforms dominant text-based
retrieval systems, demonstrating its robustness and adaptability to a larger
number of use cases. This advancement highlights the importance of pairing
context rich data retrieval with LLMs, offering a renewed approach to knowledge
sourcing and generation in AI systems. We also show that, due to rich
contextual data retrieval, the crucial entities, along with the generated
answer, remain factually coherent with the gold answer.
\\ ( https://arxiv.org/abs/2401.12671 ,  1011kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12713
Date: Tue, 23 Jan 2024 12:29:37 GMT   (333kb,D)

Title: Generating Unsupervised Abstractive Explanations for Rumour Verification
Authors: Iman Munire Bilal, Preslav Nakov, Rob Procter, Maria Liakata
Categories: cs.CL
\\
  The task of rumour verification in social media concerns assessing the
veracity of a claim on the basis of conversation threads that result from it.
While previous work has focused on predicting a veracity label, here we
reformulate the task to generate model-centric, free-text explanations of a
rumour's veracity. We follow an unsupervised approach by first utilising
post-hoc explainability methods to score the most important posts within a
thread and then we use these posts to generate informative explanatory
summaries by employing template-guided summarisation. To evaluate the
informativeness of the explanatory summaries, we exploit the few-shot learning
capabilities of a large language model (LLM). Our experiments show that LLMs
can have similar agreement to humans in evaluating summaries. Importantly, we
show that explanatory abstractive summaries are more informative and better
reflect the predicted rumour veracity than just using the highest ranking posts
in the thread.
\\ ( https://arxiv.org/abs/2401.12713 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12720
Date: Tue, 23 Jan 2024 12:41:03 GMT   (749kb,D)

Title: A Comprehensive View of the Biases of Toxicity and Sentiment Analysis
  Methods Towards Utterances with African American English Expressions
Authors: Guilherme H. Resende, Luiz F. Nery, Fabr\'icio Benevenuto, Savvas
  Zannettou, Flavio Figueiredo
Categories: cs.CL cs.SI
Comments: Under peer review
\\
  Language is a dynamic aspect of our culture that changes when expressed in
different technologies/communities. Online social networks have enabled the
diffusion and evolution of different dialects, including African American
English (AAE). However, this increased usage is not without barriers. One
particular barrier is how sentiment (Vader, TextBlob, and Flair) and toxicity
(Google's Perspective and the open-source Detoxify) methods present biases
towards utterances with AAE expressions. Consider Google's Perspective to
understand bias. Here, an utterance such as ``All n*ggers deserve to die
respectfully. The police murder us.'' it reaches a higher toxicity than
``African-Americans deserve to die respectfully. The police murder us.''. This
score difference likely arises because the tool cannot understand the
re-appropriation of the term ``n*gger''. One explanation for this bias is that
AI models are trained on limited datasets, and using such a term in training
data is more likely to appear in a toxic utterance. While this may be
plausible, the tool will make mistakes regardless. Here, we study bias on two
Web-based (YouTube and Twitter) datasets and two spoken English datasets. Our
analysis shows how most models present biases towards AAE in most settings. We
isolate the impact of AAE expression usage via linguistic control features from
the Linguistic Inquiry and Word Count (LIWC) software, grammatical control
features extracted via Part-of-Speech (PoS) tagging from Natural Language
Processing (NLP) models, and the semantic of utterances by comparing sentence
embeddings from recent language models. We present consistent results on how a
heavy usage of AAE expressions may cause the speaker to be considered
substantially more toxic, even when speaking about nearly the same subject. Our
study complements similar analyses focusing on small datasets and/or one method
only.
\\ ( https://arxiv.org/abs/2401.12720 ,  749kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12756
Date: Tue, 23 Jan 2024 13:35:47 GMT   (1560kb,D)

Title: What the Weight?! A Unified Framework for Zero-Shot Knowledge
  Composition
Authors: Carolin Holtermann, Markus Frohmann, Navid Rekabsaz, Anne Lauscher
Categories: cs.CL cs.AI
Comments: Accepted to Findings of the ACL: EACL 2024
\\
  The knowledge encapsulated in a model is the core factor determining its
final performance on downstream tasks. Much research in NLP has focused on
efficient methods for storing and adapting different types of knowledge, e.g.,
in dedicated modularized structures, and on how to effectively combine these,
e.g., by learning additional parameters. However, given the many possible
options, a thorough understanding of the mechanisms involved in these
compositions is missing, and hence it remains unclear which strategies to
utilize. To address this research gap, we propose a novel framework for
zero-shot module composition, which encompasses existing and some novel
variations for selecting, weighting, and combining parameter modules under a
single unified notion. Focusing on the scenario of domain knowledge and adapter
layers, our framework provides a systematic unification of concepts, allowing
us to conduct the first comprehensive benchmarking study of various zero-shot
knowledge composition strategies. In particular, we test two module combination
methods and five selection and weighting strategies for their effectiveness and
efficiency in an extensive experimental setup. Our results highlight the
efficacy of ensembling but also hint at the power of simple though
often-ignored weighting methods. Further in-depth analyses allow us to
understand the role of weighting vs. top-k selection, and show that, to a
certain extent, the performance of adapter composition can even be predicted.
\\ ( https://arxiv.org/abs/2401.12756 ,  1560kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12789
Date: Tue, 23 Jan 2024 14:19:01 GMT   (281kb,D)

Title: Multilingual and Fully Non-Autoregressive ASR with Large Language Model
  Fusion: A Comprehensive Study
Authors: W. Ronny Huang, Cyril Allauzen, Tongzhou Chen, Kilol Gupta, Ke Hu,
  James Qin, Yu Zhang, Yongqiang Wang, Shuo-Yiin Chang, Tara N. Sainath
Categories: cs.CL cs.SD eess.AS
Comments: ICASSP 2024
\\
  In the era of large models, the autoregressive nature of decoding often
results in latency serving as a significant bottleneck. We propose a
non-autoregressive LM-fused ASR system that effectively leverages the
parallelization capabilities of accelerator hardware. Our approach combines the
Universal Speech Model (USM) and the PaLM 2 language model in per-segment
scoring mode, achieving an average relative WER improvement across all
languages of 10.8% on FLEURS and 3.6% on YouTube captioning. Furthermore, our
comprehensive ablation study analyzes key parameters such as LLM size, context
length, vocabulary size, fusion methodology. For instance, we explore the
impact of LLM size ranging from 128M to 340B parameters on ASR performance.
This study provides valuable insights into the factors influencing the
effectiveness of practical large-scale LM-fused speech recognition systems.
\\ ( https://arxiv.org/abs/2401.12789 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12794
Date: Tue, 23 Jan 2024 14:29:17 GMT   (8951kb,D)

Title: Benchmarking LLMs via Uncertainty Quantification
Authors: Fanghua Ye, Mingming Yang, Jianhui Pang, Longyue Wang, Derek F. Wong,
  Emine Yilmaz, Shuming Shi, Zhaopeng Tu
Categories: cs.CL
Comments: 24 pages, preprints
\\
  The proliferation of open-source Large Language Models (LLMs) from various
institutions has highlighted the urgent need for comprehensive evaluation
methods. However, current evaluation platforms, such as the widely recognized
HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty,
which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce
a new benchmarking approach for LLMs that integrates uncertainty
quantification. Our examination involves eight LLMs (LLM series) spanning five
representative natural language processing tasks. Additionally, we introduce an
uncertainty-aware evaluation metric, UAcc, which takes into account both
prediction accuracy and prediction uncertainty. Our findings reveal that: I)
LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs
may display greater uncertainty compared to their smaller counterparts; and
III) Instruction-finetuning tends to increase the uncertainty of LLMs. By
taking uncertainty into account, our new UAcc metric can either amplify or
diminish the relative improvement of one LLM over another and may even change
the relative ranking of two LLMs. These results underscore the significance of
incorporating uncertainty in the evaluation of LLMs.
\\ ( https://arxiv.org/abs/2401.12794 ,  8951kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12863
Date: Tue, 23 Jan 2024 15:56:11 GMT   (544kb,D)

Title: KAM-CoT: Knowledge Augmented Multimodal Chain-of-Thoughts Reasoning
Authors: Debjyoti Mondal, Suraj Modi, Subhadarshi Panda, Rituraj Singh,
  Godawari Sudhakar Rao
Categories: cs.CL cs.AI
Comments: AAAI 2024
\\
  Large Language Models (LLMs) have demonstrated impressive performance in
natural language processing tasks by leveraging chain of thought (CoT) that
enables step-by-step thinking. Extending LLMs with multimodal capabilities is
the recent interest, but incurs computational cost and requires substantial
hardware resources. To address these challenges, we propose KAM-CoT a framework
that integrates CoT reasoning, Knowledge Graphs (KGs), and multiple modalities
for a comprehensive understanding of multimodal tasks. KAM-CoT adopts a
two-stage training process with KG grounding to generate effective rationales
and answers. By incorporating external knowledge from KGs during reasoning, the
model gains a deeper contextual understanding reducing hallucinations and
enhancing the quality of answers. This knowledge-augmented CoT reasoning
empowers the model to handle questions requiring external context, providing
more informed answers. Experimental findings show KAM-CoT outperforms the
state-of-the-art methods. On the ScienceQA dataset, we achieve an average
accuracy of 93.87%, surpassing GPT-3.5 (75.17%) by 18% and GPT-4 (83.99%) by
10%. Remarkably, KAM-CoT achieves these results with only 280M trainable
parameters at a time, demonstrating its cost-efficiency and effectiveness.
\\ ( https://arxiv.org/abs/2401.12863 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12873
Date: Tue, 23 Jan 2024 16:07:43 GMT   (8352kb,D)

Title: Improving Machine Translation with Human Feedback: An Exploration of
  Quality Estimation as a Reward Model
Authors: Zhiwei He, Xing Wang, Wenxiang Jiao, Zhuosheng Zhang, Rui Wang,
  Shuming Shi, Zhaopeng Tu
Categories: cs.CL cs.AI
\\
  Insufficient modeling of human preferences within the reward model is a major
obstacle for leveraging human feedback to improve translation quality.
Fortunately, quality estimation (QE), which predicts the quality of a given
translation without reference, has achieved impressive alignment with human
evaluations in the last two years. In this work, we investigate the potential
of employing the QE model as the reward model (the QE-based reward model) to
predict human preferences for feedback training. We first identify the
overoptimization problem during QE-based feedback training, manifested as an
increase in reward while translation quality declines. We examine the problem
and argue that the vulnerability of the QE model might lead to high rewards for
incorrect translations, resulting in overoptimization and error propagation. To
address the problem, we adopt a simple yet effective method that uses heuristic
rules to detect the incorrect translations and assigns a penalty term to the
QE-based rewards for the detected incorrect translations. Experimental results
show that the proposed QE-based feedback training achieves consistent and
significant improvements across various settings, further verified through
human preference studies. Our subsequent analysis demonstrates the high data
efficiency of the proposed QE-based feedback training: the proposed approach
using a small amount of monolingual data can outperform systems using larger
parallel corpora.
\\ ( https://arxiv.org/abs/2401.12873 ,  8352kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12874
Date: Tue, 23 Jan 2024 16:09:53 GMT   (801kb,D)

Title: From Understanding to Utilization: A Survey on Explainability for Large
  Language Models
Authors: Haoyan Luo, Lucia Specia
Categories: cs.CL cs.AI
\\
  This survey paper delves into the burgeoning field of explainability for
Large Language Models (LLMs), a critical yet challenging aspect of natural
language processing. With LLMs playing a pivotal role in various applications,
their "black-box" nature raises concerns about transparency and ethical use.
This paper emphasizes the necessity for enhanced explainability in LLMs,
addressing both the general public's trust and the technical community's need
for a deeper understanding of these models. We concentrate on pre-trained
Transformer-based LLMs, such as LLaMA, which present unique interpretability
challenges due to their scale and complexity. Our review categorizes existing
explainability methods and discusses their application in improving model
transparency and reliability. We also discuss representative evaluation
methods, highlighting their strengths and limitations. The goal of this survey
is to bridge the gap between theoretical understanding and practical
application, offering insights for future research and development in the field
of LLM explainability.
\\ ( https://arxiv.org/abs/2401.12874 ,  801kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12941
Date: Tue, 23 Jan 2024 17:58:38 GMT   (6883kb,D)

Title: Multicultural Name Recognition For Previously Unseen Names
Authors: Alexandra Loessberg-Zahl
Categories: cs.CL
Comments: 11 pages
\\
  State of the art Named Entity Recognition (NER) models have achieved an
impressive ability to extract common phrases from text that belong to labels
such as location, organization, time, and person. However, typical NER systems
that rely on having seen a specific entity in their training data in order to
label an entity perform poorly on rare or unseen entities ta in order to label
an entity perform poorly on rare or unseen entities (Derczynski et al., 2017).
This paper attempts to improve recognition of person names, a diverse category
that can grow any time someone is born or changes their name. In order for
downstream tasks to not exhibit bias based on cultural background, a model
should perform well on names from a variety of backgrounds. In this paper I
experiment with the training data and input structure of an English Bi-LSTM
name recognition model. I look at names from 103 countries to compare how well
the model performs on names from different cultures, specifically in the
context of a downstream task where extracted names will be matched to
information on file. I find that a model with combined character and word input
outperforms word-only models and may improve on accuracy compared to classical
NER models that are not geared toward identifying unseen entity values.
\\ ( https://arxiv.org/abs/2401.12941 ,  6883kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12947
Date: Tue, 23 Jan 2024 18:07:38 GMT   (32522kb,D)

Title: Transformer-Based Models Are Not Yet Perfect At Learning to Emulate
  Structural Recursion
Authors: Dylan Zhang, Curt Tigges, Zory Zhang, Stella Biderman, Maxim Raginsky,
  Talia Ringer
Categories: cs.CL cs.AI cs.FL cs.LO cs.PL
Comments: arXiv admin note: text overlap with arXiv:2305.14699
\\
  This paper investigates the ability of transformer-based models to learn
structural recursion from examples. Recursion is a universal concept in both
natural and formal languages. Structural recursion is central to the
programming language and formal mathematics tasks where symbolic tools
currently excel beyond neural models, such as inferring semantic relations
between datatypes and emulating program behavior. We introduce a general
framework that nicely connects the abstract concepts of structural recursion in
the programming language domain to concrete sequence modeling problems and
learned models' behavior. The framework includes a representation that captures
the general \textit{syntax} of structural recursion, coupled with two different
frameworks for understanding their \textit{semantics} -- one that is more
natural from a programming languages perspective and one that helps bridge that
perspective with a mechanistic understanding of the underlying transformer
architecture.
  With our framework as a powerful conceptual tool, we identify different
issues under various set-ups. The models trained to emulate recursive
computations cannot fully capture the recursion yet instead fit short-cut
algorithms and thus cannot solve certain edge cases that are under-represented
in the training distribution. In addition, it is difficult for state-of-the-art
large language models (LLMs) to mine recursive rules from in-context
demonstrations. Meanwhile, these LLMs fail in interesting ways when emulating
reduction (step-wise computation) of the recursive function.
\\ ( https://arxiv.org/abs/2401.12947 ,  32522kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12954
Date: Tue, 23 Jan 2024 18:22:19 GMT   (264kb,D)

Title: Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding
Authors: Mirac Suzgun, Adam Tauman Kalai
Categories: cs.CL cs.AI cs.HC
Comments: https://github.com/suzgunmirac/meta-prompting
\\
  We introduce meta-prompting, an effective scaffolding technique designed to
enhance the functionality of language models (LMs). This approach transforms a
single LM into a multi-faceted conductor, adept at managing and integrating
multiple independent LM queries. By employing high-level instructions,
meta-prompting guides the LM to break down complex tasks into smaller, more
manageable subtasks. These subtasks are then handled by distinct "expert"
instances of the same LM, each operating under specific, tailored instructions.
Central to this process is the LM itself, in its role as the conductor, which
ensures seamless communication and effective integration of the outputs from
these expert models. It additionally employs its inherent critical thinking and
robust verification processes to refine and authenticate the end result. This
collaborative prompting approach empowers a single LM to simultaneously act as
a comprehensive orchestrator and a panel of diverse experts, significantly
enhancing its performance across a wide array of tasks. The zero-shot,
task-agnostic nature of meta-prompting greatly simplifies user interaction by
obviating the need for detailed, task-specific instructions. Furthermore, our
research demonstrates the seamless integration of external tools, such as a
Python interpreter, into the meta-prompting framework, thereby broadening its
applicability and utility. Through rigorous experimentation with GPT-4, we
establish the superiority of meta-prompting over conventional scaffolding
methods: When averaged across all tasks, including the Game of 24,
Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented
with a Python interpreter functionality, surpasses standard prompting by 17.1%,
expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.
\\ ( https://arxiv.org/abs/2401.12954 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12970
Date: Tue, 23 Jan 2024 18:57:53 GMT   (589kb,D)

Title: Raidar: geneRative AI Detection viA Rewriting
Authors: Chengzhi Mao, Carl Vondrick, Hao Wang, Junfeng Yang
Categories: cs.CL
Comments: Accepted by ICLR 2024
\\
  We find that large language models (LLMs) are more likely to modify
human-written text than AI-generated text when tasked with rewriting. This
tendency arises because LLMs often perceive AI-generated text as high-quality,
leading to fewer modifications. We introduce a method to detect AI-generated
content by prompting LLMs to rewrite text and calculating the editing distance
of the output. We dubbed our geneRative AI Detection viA Rewriting method
Raidar. Raidar significantly improves the F1 detection scores of existing AI
content detection models -- both academic and commercial -- across various
domains, including News, creative writing, student essays, code, Yelp reviews,
and arXiv papers, with gains of up to 29 points. Operating solely on word
symbols without high-dimensional features, our method is compatible with black
box LLMs, and is inherently robust on new content. Our results illustrate the
unique imprint of machine-generated text through the lens of the machines
themselves.
\\ ( https://arxiv.org/abs/2401.12970 ,  589kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12973
Date: Tue, 23 Jan 2024 18:59:21 GMT   (2538kb,D)

Title: In-Context Language Learning: Arhitectures and Algorithms
Authors: Ekin Aky\"urek, Bailin Wang, Yoon Kim, Jacob Andreas
Categories: cs.CL cs.LG
Comments: 29 pages, 8 figures
\\
  Large-scale neural language models exhibit a remarkable capacity for
in-context learning (ICL): they can infer novel functions from datasets
provided as input. Most of our current understanding of when and how ICL arises
comes from LMs trained on extremely simple learning problems like linear
regression and associative recall. There remains a significant gap between
these model problems and the "real" ICL exhibited by LMs trained on large text
corpora, which involves not just retrieval and function approximation but
free-form generation of language and other structured outputs. In this paper,
we study ICL through the lens of a new family of model problems we term in
context language learning (ICLL). In ICLL, LMs are presented with a set of
strings from a formal language, and must generate additional strings from the
same language. We focus on in-context learning of regular languages generated
by random finite automata. We evaluate a diverse set of neural sequence models
(including several RNNs, Transformers, and state-space model variants) on
regular ICLL tasks, aiming to answer three questions: (1) Which model classes
are empirically capable of ICLL? (2) What algorithmic solutions do successful
models implement to perform ICLL? (3) What architectural changes can improve
ICLL in less performant models? We first show that Transformers significantly
outperform neural sequence models with recurrent or convolutional
representations on ICLL tasks. Next, we provide evidence that their ability to
do so relies on specialized "n-gram heads" (higher-order variants of induction
heads) that compute input-conditional next-token distributions. Finally, we
show that hard-wiring these heads into recurrent and convolutional models
improves performance not just on ICLL, but natural language modeling --
improving the perplexity of 340M-parameter models by up to 1.14 points (6.7%)
on the SlimPajama dataset.
\\ ( https://arxiv.org/abs/2401.12973 ,  2538kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12233
Date: Fri, 19 Jan 2024 11:32:47 GMT   (5012kb,D)

Title: Memorization in Self-Supervised Learning Improves Downstream
  Generalization
Authors: Wenhao Wang, Muhammad Ahmad Kaleem, Adam Dziedzic, Michael Backes,
  Nicolas Papernot, Franziska Boenisch
Categories: cs.LG
Comments: Accepted at ICLR 2024
\\
  Self-supervised learning (SSL) has recently received significant attention
due to its ability to train high-performance encoders purely on unlabeled
data-often scraped from the internet. This data can still be sensitive and
empirical evidence suggests that SSL encoders memorize private information of
their training data and can disclose them at inference time. Since existing
theoretical definitions of memorization from supervised learning rely on
labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a
framework for defining memorization within SSL. Our definition compares the
difference in alignment of representations for data points and their augmented
views returned by both encoders that were trained on these data points and
encoders that were not. Through comprehensive empirical analysis on diverse
encoder architectures and datasets we highlight that even though SSL relies on
large datasets and strong augmentations-both known in supervised learning as
regularization techniques that reduce overfitting-still significant fractions
of training data points experience high memorization. Through our empirical
results, we show that this memorization is essential for encoders to achieve
higher generalization performance on different downstream tasks.
\\ ( https://arxiv.org/abs/2401.12233 ,  5012kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12235
Date: Fri, 19 Jan 2024 13:58:46 GMT   (1518kb)

Title: Stochastic Dynamic Power Dispatch with High Generalization and Few-Shot
  Adaption via Contextual Meta Graph Reinforcement Learning
Authors: Bairong Deng, Tao Yu, Zhenning Pan, Xuehan Zhang, Yufeng Wu, Qiaoyi
  Ding
Categories: cs.LG cs.SY eess.SY
\\
  Reinforcement learning is an emerging approaches to facilitate multi-stage
sequential decision-making problems. This paper studies a real-time multi-stage
stochastic power dispatch considering multivariate uncertainties. Current
researches suffer from low generalization and practicality, that is, the
learned dispatch policy can only handle a specific dispatch scenario, its
performance degrades significantly if actual samples and training samples are
inconsistent. To fill these gaps, a novel contextual meta graph reinforcement
learning (Meta-GRL) for a highly generalized multi-stage optimal dispatch
policy is proposed. Specifically, a more general contextual Markov decision
process (MDP) and scalable graph representation are introduced to achieve a
more generalized multi-stage stochastic power dispatch modeling. An upper
meta-learner is proposed to encode context for different dispatch scenarios and
learn how to achieve dispatch task identification while the lower policy
learner learns context-specified dispatch policy. After sufficient offline
learning, this approach can rapidly adapt to unseen and undefined scenarios
with only a few updations of the hypothesis judgments generated by the
meta-learner. Numerical comparisons with state-of-the-art policies and
traditional reinforcement learning verify the optimality, efficiency,
adaptability, and scalability of the proposed Meta-GRL.
\\ ( https://arxiv.org/abs/2401.12235 ,  1518kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12236
Date: Fri, 19 Jan 2024 15:40:46 GMT   (50kb)

Title: The Surprising Harmfulness of Benign Overfitting for Adversarial
  Robustness
Authors: Yifan Hao, Tong Zhang
Categories: cs.LG cs.CR stat.ML
\\
  Recent empirical and theoretical studies have established the generalization
capabilities of large machine learning models that are trained to
(approximately or exactly) fit noisy data. In this work, we prove a surprising
result that even if the ground truth itself is robust to adversarial examples,
and the benignly overfitted model is benign in terms of the ``standard''
out-of-sample risk objective, this benign overfitting process can be harmful
when out-of-sample data are subject to adversarial manipulation. More
specifically, our main results contain two parts: (i) the min-norm estimator in
overparameterized linear model always leads to adversarial vulnerability in the
``benign overfitting'' setting; (ii) we verify an asymptotic trade-off result
between the standard risk and the ``adversarial'' risk of every ridge
regression estimator, implying that under suitable conditions these two items
cannot both be small at the same time by any single choice of the ridge
regularization parameter. Furthermore, under the lazy training regime, we
demonstrate parallel results on two-layer neural tangent kernel (NTK) model,
which align with empirical observations in deep neural networks. Our finding
provides theoretical insights into the puzzling phenomenon observed in
practice, where the true target function (e.g., human) is robust against
adverasrial attack, while beginly overfitted neural networks lead to models
that are not robust.
\\ ( https://arxiv.org/abs/2401.12236 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12251
Date: Sat, 20 Jan 2024 19:56:42 GMT   (965kb,D)

Title: Diffusion Representation for Asymmetric Kernels
Authors: Alvaro Almeida Gomez, Antonio Silva Neto, Jorge zubelli
Categories: cs.LG eess.IV
Journal-ref: Applied Numerical Mathematics, 2021
\\
  We extend the diffusion-map formalism to data sets that are induced by
asymmetric kernels. Analytical convergence results of the resulting expansion
are proved, and an algorithm is proposed to perform the dimensional reduction.
In this work we study data sets in which its geometry structure is induced by
an asymmetric kernel. We use a priori coordinate system to represent this
geometry and, thus, be able to improve the computational complexity of reducing
the dimensionality of data sets. A coordinate system connected to the tensor
product of Fourier basis is used to represent the underlying geometric
structure obtained by the diffusion-map, thus reducing the dimensionality of
the data set and making use of the speedup provided by the two-dimensional Fast
Fourier Transform algorithm (2-D FFT). We compare our results with those
obtained by other eigenvalue expansions, and verify the efficiency of the
algorithms with synthetic data, as well as with real data from applications
including climate change studies.
\\ ( https://arxiv.org/abs/2401.12251 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12254
Date: Sun, 21 Jan 2024 09:03:30 GMT   (7402kb,D)

Title: Transfer learning-assisted inverse modeling in nanophotonics based on
  mixture density networks
Authors: Liang Cheng and Prashant Singh and Francesco Ferranti
Categories: cs.LG physics.optics
\\
  The simulation of nanophotonic structures relies on electromagnetic solvers,
which play a crucial role in understanding their behavior. However, these
solvers often come with a significant computational cost, making their
application in design tasks, such as optimization, impractical. To address this
challenge, machine learning techniques have been explored for accurate and
efficient modeling and design of photonic devices. Deep neural networks, in
particular, have gained considerable attention in this field. They can be used
to create both forward and inverse models. An inverse modeling approach avoids
the need for coupling a forward model with an optimizer and directly performs
the prediction of the optimal design parameters values.
  In this paper, we propose an inverse modeling method for nanophotonic
structures, based on a mixture density network model enhanced by transfer
learning. Mixture density networks can predict multiple possible solutions at a
time including their respective importance as Gaussian distributions. However,
multiple challenges exist for mixture density network models. An important
challenge is that an upper bound on the number of possible simultaneous
solutions needs to be specified in advance. Also, another challenge is that the
model parameters must be jointly optimized, which can result computationally
expensive. Moreover, optimizing all parameters simultaneously can be
numerically unstable and can lead to degenerate predictions. The proposed
approach allows overcoming these limitations using transfer learning-based
techniques, while preserving a high accuracy in the prediction capability of
the design solutions given an optical response as an input. A dimensionality
reduction step is also explored. Numerical results validate the proposed
method.
\\ ( https://arxiv.org/abs/2401.12254 ,  7402kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12332
Date: Mon, 22 Jan 2024 19:46:30 GMT   (547kb,D)

Title: A Precise Characterization of SGD Stability Using Loss Surface Geometry
Authors: Gregory Dexter, Borja Ocejo, Sathiya Keerthi, Aman Gupta, Ayan
  Acharya, Rajiv Khanna
Categories: cs.LG math.OC
Comments: To appear at ICLR 2024
\\
  Stochastic Gradient Descent (SGD) stands as a cornerstone optimization
algorithm with proven real-world empirical successes but relatively limited
theoretical understanding. Recent research has illuminated a key factor
contributing to its practical efficacy: the implicit regularization it
instigates. Several studies have investigated the linear stability property of
SGD in the vicinity of a stationary point as a predictive proxy for sharpness
and generalization error in overparameterized neural networks (Wu et al., 2022;
Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper
into the relationship between linear stability and sharpness. More
specifically, we meticulously delineate the necessary and sufficient conditions
for linear stability, contingent on hyperparameters of SGD and the sharpness at
the optimum. Towards this end, we introduce a novel coherence measure of the
loss Hessian that encapsulates pertinent geometric properties of the loss
function that are relevant to the linear stability of SGD. It enables us to
provide a simplified sufficient condition for identifying linear instability at
an optimum. Notably, compared to previous works, our analysis relies on
significantly milder assumptions and is applicable for a broader class of loss
functions than known before, encompassing not only mean-squared error but also
cross-entropy loss.
\\ ( https://arxiv.org/abs/2401.12332 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12356
Date: Mon, 22 Jan 2024 20:50:01 GMT   (325kb,D)

Title: Efficient Collaborations through Weight-Driven Coalition Dynamics in
  Federated Learning Systems
Authors: Mohammed El Hanjri, Hamza Reguieg, Adil Attiaoui, Amine Abouaomar,
  Abdellatif Kobbane, Mohamed El Kamili
Categories: cs.LG cs.DC cs.GT
Comments: 6 pages, 4 figures, conference
\\
  In the era of the Internet of Things (IoT), decentralized paradigms for
machine learning are gaining prominence. In this paper, we introduce a
federated learning model that capitalizes on the Euclidean distance between
device model weights to assess their similarity and disparity. This is
foundational for our system, directing the formation of coalitions among
devices based on the closeness of their model weights. Furthermore, the concept
of a barycenter, representing the average of model weights, helps in the
aggregation of updates from multiple devices. We evaluate our approach using
homogeneous and heterogeneous data distribution, comparing it against
traditional federated learning averaging algorithm. Numerical results
demonstrate its potential in offering structured, outperformed and
communication-efficient model for IoT-based machine learning.
\\ ( https://arxiv.org/abs/2401.12356 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12369
Date: Mon, 22 Jan 2024 21:41:26 GMT   (2068kb,D)

Title: SubgroupTE: Advancing Treatment Effect Estimation with Subgroup
  Identification
Authors: Seungyeon Lee, Ruoqi Liu, Wenyu Song, Lang Li, and Ping Zhang
Categories: cs.LG stat.ME
\\
  Precise estimation of treatment effects is crucial for evaluating
intervention effectiveness. While deep learning models have exhibited promising
performance in learning counterfactual representations for treatment effect
estimation (TEE), a major limitation in most of these models is that they treat
the entire population as a homogeneous group, overlooking the diversity of
treatment effects across potential subgroups that have varying treatment
effects. This limitation restricts the ability to precisely estimate treatment
effects and provide subgroup-specific treatment recommendations. In this paper,
we propose a novel treatment effect estimation model, named SubgroupTE, which
incorporates subgroup identification in TEE. SubgroupTE identifies
heterogeneous subgroups with different treatment responses and more precisely
estimates treatment effects by considering subgroup-specific causal effects. In
addition, SubgroupTE iteratively optimizes subgrouping and treatment effect
estimation networks to enhance both estimation and subgroup identification.
Comprehensive experiments on the synthetic and semi-synthetic datasets exhibit
the outstanding performance of SubgroupTE compared with the state-of-the-art
models on treatment effect estimation. Additionally, a real-world study
demonstrates the capabilities of SubgroupTE in enhancing personalized treatment
recommendations for patients with opioid use disorder (OUD) by advancing
treatment effect estimation with subgroup identification.
\\ ( https://arxiv.org/abs/2401.12369 ,  2068kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12416
Date: Tue, 23 Jan 2024 00:27:31 GMT   (826kb,D)

Title: Enhancing Reliability of Neural Networks at the Edge: Inverted
  Normalization with Stochastic Affine Transformations
Authors: Soyed Tuhin Ahmed, Kamal Danouchi, Guillaume Prenat, Lorena Anghel,
  Mehdi B. Tahoori
Categories: cs.LG cs.AR cs.ET
\\
  Bayesian Neural Networks (BayNNs) naturally provide uncertainty in their
predictions, making them a suitable choice in safety-critical applications.
Additionally, their realization using memristor-based in-memory computing (IMC)
architectures enables them for resource-constrained edge applications. In
addition to predictive uncertainty, however, the ability to be inherently
robust to noise in computation is also essential to ensure functional safety.
In particular, memristor-based IMCs are susceptible to various sources of
non-idealities such as manufacturing and runtime variations, drift, and
failure, which can significantly reduce inference accuracy. In this paper, we
propose a method to inherently enhance the robustness and inference accuracy of
BayNNs deployed in IMC architectures. To achieve this, we introduce a novel
normalization layer combined with stochastic affine transformations. Empirical
results in various benchmark datasets show a graceful degradation in inference
accuracy, with an improvement of up to $58.11\%$.
\\ ( https://arxiv.org/abs/2401.12416 ,  826kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12418
Date: Tue, 23 Jan 2024 00:40:20 GMT   (10686kb,D)

Title: Towards Improved Variational Inference for Deep Bayesian Models
Authors: Sebastian W. Ober
Categories: cs.LG stat.ML
Comments: PhD Thesis; University of Cambridge
DOI: 10.17863/CAM.102025
\\
  Deep learning has revolutionized the last decade, being at the forefront of
extraordinary advances in a wide range of tasks including computer vision,
natural language processing, and reinforcement learning, to name but a few.
However, it is well-known that deep models trained via maximum likelihood
estimation tend to be overconfident and give poorly-calibrated predictions.
Bayesian deep learning attempts to address this by placing priors on the model
parameters, which are then combined with a likelihood to perform posterior
inference. Unfortunately, for deep models, the true posterior is intractable,
forcing the user to resort to approximations. In this thesis, we explore the
use of variational inference (VI) as an approximation, as it is unique in
simultaneously approximating the posterior and providing a lower bound to the
marginal likelihood. If tight enough, this lower bound can be used to optimize
hyperparameters and to facilitate model selection. However, this capacity has
rarely been used to its full extent for Bayesian neural networks, likely
because the approximate posteriors typically used in practice can lack the
flexibility to effectively bound the marginal likelihood. We therefore explore
three aspects of Bayesian learning for deep models: 1) we ask whether it is
necessary to perform inference over as many parameters as possible, or whether
it is reasonable to treat many of them as optimizable hyperparameters; 2) we
propose a variational posterior that provides a unified view of inference in
Bayesian neural networks and deep Gaussian processes; 3) we demonstrate how VI
can be improved in certain deep Gaussian process models by analytically
removing symmetries from the posterior, and performing inference on Gram
matrices instead of features. We hope that our contributions will provide a
stepping stone to fully realize the promises of VI in the future.
\\ ( https://arxiv.org/abs/2401.12418 ,  10686kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12436
Date: Tue, 23 Jan 2024 02:08:20 GMT   (2272kb,D)

Title: Wasserstein Differential Privacy
Authors: Chengyi Yang, Jiayin Qi and Aimin Zhou
Categories: cs.LG cs.CR
Comments: Accepted by AAAI 2024
\\
  Differential privacy (DP) has achieved remarkable results in the field of
privacy-preserving machine learning. However, existing DP frameworks do not
satisfy all the conditions for becoming metrics, which prevents them from
deriving better basic private properties and leads to exaggerated values on
privacy budgets. We propose Wasserstein differential privacy (WDP), an
alternative DP framework to measure the risk of privacy leakage, which
satisfies the properties of symmetry and triangle inequality. We show and prove
that WDP has 13 excellent properties, which can be theoretical supports for the
better performance of WDP than other DP frameworks. In addition, we derive a
general privacy accounting method called Wasserstein accountant, which enables
WDP to be applied in stochastic gradient descent (SGD) scenarios containing
sub-sampling. Experiments on basic mechanisms, compositions and deep learning
show that the privacy budgets obtained by Wasserstein accountant are relatively
stable and less influenced by order. Moreover, the overestimation on privacy
budgets can be effectively alleviated. The code is available at
https://github.com/Hifipsysta/WDP.
\\ ( https://arxiv.org/abs/2401.12436 ,  2272kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12470
Date: Tue, 23 Jan 2024 03:43:34 GMT   (293kb,D)

Title: Reinforcement Learning for Graph Coloring: Understanding the Power and
  Limits of Non-Label Invariant Representations
Authors: Chase Cummins and Richard Veras
Categories: cs.LG cs.AI
\\
  Register allocation is one of the most important problems for modern
compilers. With a practically unlimited number of user variables and a small
number of CPU registers, assigning variables to registers without conflicts is
a complex task. This work demonstrates the use of casting the register
allocation problem as a graph coloring problem. Using technologies such as
PyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy
Optimization model can learn to solve the graph coloring problem. We will also
show that the labeling of a graph is critical to the performance of the model
by taking the matrix representation of a graph and permuting it. We then test
the model's effectiveness on each of these permutations and show that it is not
effective when given a relabeling of the same graph. Our main contribution lies
in showing the need for label reordering invariant representations of graphs
for machine learning models to achieve consistent performance.
\\ ( https://arxiv.org/abs/2401.12470 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12478
Date: Tue, 23 Jan 2024 04:16:58 GMT   (346kb,D)

Title: Mini-batch Submodular Maximization
Authors: Gregory Schwartzman
Categories: cs.LG cs.AI cs.DS
\\
  We present the first mini-batch algorithm for maximizing a non-negative
monotone decomposable submodular function, $F=\sum_{i=1}^N f^i$, under a set of
constraints. We improve over the sparsifier based approach both in theory and
in practice. We experimentally observe that our algorithm generates solutions
that are far superior to those generated by the sparsifier based approach.
\\ ( https://arxiv.org/abs/2401.12478 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12485
Date: Tue, 23 Jan 2024 04:50:13 GMT   (2535kb,D)

Title: Adiabatic Quantum Support Vector Machines
Authors: Prasanna Date, Dong Jun Woun, Kathleen Hamilton, Eduardo A. Coello
  Perez, Mayanka Chandra Shekhar, Francisco Rios, John Gounley, In-Saeng Suh,
  Travis Humble, Georgia Tourassi
Categories: cs.LG cs.AI quant-ph stat.ML
\\
  Adiabatic quantum computers can solve difficult optimization problems (e.g.,
the quadratic unconstrained binary optimization problem), and they seem well
suited to train machine learning models. In this paper, we describe an
adiabatic quantum approach for training support vector machines. We show that
the time complexity of our quantum approach is an order of magnitude better
than the classical approach. Next, we compare the test accuracy of our quantum
approach against a classical approach that uses the Scikit-learn library in
Python across five benchmark datasets (Iris, Wisconsin Breast Cancer (WBC),
Wine, Digits, and Lambeq). We show that our quantum approach obtains accuracies
on par with the classical approach. Finally, we perform a scalability study in
which we compute the total training times of the quantum approach and the
classical approach with increasing number of features and number of data points
in the training dataset. Our scalability results show that the quantum approach
obtains a 3.5--4.5 times speedup over the classical approach on datasets with
many (millions of) features.
\\ ( https://arxiv.org/abs/2401.12485 ,  2535kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12489
Date: Tue, 23 Jan 2024 05:06:29 GMT   (997kb)

Title: Unsupervised Learning Method for the Wave Equation Based on Finite
  Difference Residual Constraints Loss
Authors: Xin Feng, Yi Jiang, Jia-Xian Qin, Lai-Ping Zhang, Xiao-Gang Deng
Categories: cs.LG cs.AI
Comments: in Chinese language
\\
  The wave equation is an important physical partial differential equation, and
in recent years, deep learning has shown promise in accelerating or replacing
traditional numerical methods for solving it. However, existing deep learning
methods suffer from high data acquisition costs, low training efficiency, and
insufficient generalization capability for boundary conditions. To address
these issues, this paper proposes an unsupervised learning method for the wave
equation based on finite difference residual constraints. We construct a novel
finite difference residual constraint based on structured grids and finite
difference methods, as well as an unsupervised training strategy, enabling
convolutional neural networks to train without data and predict the forward
propagation process of waves. Experimental results show that finite difference
residual constraints have advantages over physics-informed neural networks
(PINNs) type physical information constraints, such as easier fitting, lower
computational costs, and stronger source term generalization capability, making
our method more efficient in training and potent in application.
\\ ( https://arxiv.org/abs/2401.12489 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12508
Date: Tue, 23 Jan 2024 06:01:29 GMT   (53kb)

Title: On the Stochastic (Variance-Reduced) Proximal Gradient Method for
  Regularized Expected Reward Optimization
Authors: Ling Liang and Haizhao Yang
Categories: cs.LG math.OC
Comments: 18 pages
\\
  We consider a regularized expected reward optimization problem in the
non-oblivious setting that covers many existing problems in reinforcement
learning (RL). In order to solve such an optimization problem, we apply and
analyze the classical stochastic proximal gradient method. In particular, the
method has shown to admit an $O(\epsilon^{-4})$ sample complexity to an
$\epsilon$-stationary point, under standard conditions. Since the variance of
the classical stochastic gradient estimator is typically large which slows down
the convergence, we also apply an efficient stochastic variance-reduce proximal
gradient method with an importance sampling based ProbAbilistic Gradient
Estimator (PAGE). To the best of our knowledge, the application of this method
represents a novel approach in addressing the general regularized reward
optimization problem. Our analysis shows that the sample complexity can be
improved from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$ under additional
conditions. Our results on the stochastic (variance-reduced) proximal gradient
method match the sample complexity of their most competitive counterparts under
similar settings in the RL literature.
\\ ( https://arxiv.org/abs/2401.12508 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12517
Date: Tue, 23 Jan 2024 06:21:34 GMT   (22454kb,D)

Title: DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing
  High-Quality Implicit Neural Representations
Authors: Dogyun Park, Sihyeon Kim, Sojin Lee, Hyunwoo J. Kim
Categories: cs.LG stat.ML
\\
  Recent studies have introduced a new class of generative models for
synthesizing implicit neural representations (INRs) that capture arbitrary
continuous signals in various domains. These models opened the door for
domain-agnostic generative models, but they often fail to achieve high-quality
generation. We observed that the existing methods generate the weights of
neural networks to parameterize INRs and evaluate the network with fixed
positional embeddings (PEs). Arguably, this architecture limits the expressive
power of generative models and results in low-quality INR generation. To
address this limitation, we propose Domain-agnostic Latent Diffusion Model for
INRs (DDMI) that generates adaptive positional embeddings instead of neural
networks' weights. Specifically, we develop a Discrete-to-continuous space
Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and
the continuous signal functions in the shared latent space. Additionally, we
introduce a novel conditioning mechanism for evaluating INRs with the
hierarchically decomposed PEs to further enhance expressive power. Extensive
experiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance
Fields, and videos, with seven benchmark datasets, demonstrate the versatility
of DDMI and its superior performance compared to the existing INR generative
models.
\\ ( https://arxiv.org/abs/2401.12517 ,  22454kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12532
Date: Tue, 23 Jan 2024 07:15:47 GMT   (8870kb,D)

Title: DAFA: Distance-Aware Fair Adversarial Training
Authors: Hyungyu Lee, Saehyung Lee, Hyemi Jang, Junsung Park, Ho Bae, Sungroh
  Yoon
Categories: cs.LG cs.AI
Comments: Accepted to ICLR 2024
\\
  The disparity in accuracy between classes in standard training is amplified
during adversarial training, a phenomenon termed the robust fairness problem.
Existing methodologies aimed to enhance robust fairness by sacrificing the
model's performance on easier classes in order to improve its performance on
harder ones. However, we observe that under adversarial attacks, the majority
of the model's predictions for samples from the worst class are biased towards
classes similar to the worst class, rather than towards the easy classes.
Through theoretical and empirical analysis, we demonstrate that robust fairness
deteriorates as the distance between classes decreases. Motivated by these
insights, we introduce the Distance-Aware Fair Adversarial training (DAFA)
methodology, which addresses robust fairness by taking into account the
similarities between classes. Specifically, our method assigns distinct loss
weights and adversarial margins to each class and adjusts them to encourage a
trade-off in robustness among similar classes. Experimental results across
various datasets demonstrate that our method not only maintains average robust
accuracy but also significantly improves the worst robust accuracy, indicating
a marked improvement in robust fairness compared to existing methods.
\\ ( https://arxiv.org/abs/2401.12532 ,  8870kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12533
Date: Tue, 23 Jan 2024 07:16:32 GMT   (7386kb,D)

Title: Efficient Constrained $k$-Center Clustering with Background Knowledge
Authors: Longkun Guo, Chaoqi Jia, Kewen Liao, Zhigang Lu and Minhui Xue
Categories: cs.LG cs.AI
\\
  Center-based clustering has attracted significant research interest from both
theory and practice. In many practical applications, input data often contain
background knowledge that can be used to improve clustering results. In this
work, we build on widely adopted $k$-center clustering and model its input
background knowledge as must-link (ML) and cannot-link (CL) constraint sets.
However, most clustering problems including $k$-center are inherently
$\mathcal{NP}$-hard, while the more complex constrained variants are known to
suffer severer approximation and computation barriers that significantly limit
their applicability. By employing a suite of techniques including reverse
dominating sets, linear programming (LP) integral polyhedron, and LP duality,
we arrive at the first efficient approximation algorithm for constrained
$k$-center with the best possible ratio of 2. We also construct competitive
baseline algorithms and empirically evaluate our approximation algorithm
against them on a variety of real datasets. The results validate our
theoretical findings and demonstrate the great advantages of our algorithm in
terms of clustering cost, clustering quality, and running time.
\\ ( https://arxiv.org/abs/2401.12533 ,  7386kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12546
Date: Tue, 23 Jan 2024 08:08:09 GMT   (4773kb,D)

Title: On Building Myopic MPC Policies using Supervised Learning
Authors: Christopher A. Orrico, Bokan Yang, Dinesh Krishnamoorthy
Categories: cs.LG cs.SY eess.SY math.OC
\\
  The application of supervised learning techniques in combination with model
predictive control (MPC) has recently generated significant interest,
particularly in the area of approximate explicit MPC, where function
approximators like deep neural networks are used to learn the MPC policy via
optimal state-action pairs generated offline. While the aim of approximate
explicit MPC is to closely replicate the MPC policy, substituting online
optimization with a trained neural network, the performance guarantees that
come with solving the online optimization problem are typically lost. This
paper considers an alternative strategy, where supervised learning is used to
learn the optimal value function offline instead of learning the optimal
policy. This can then be used as the cost-to-go function in a myopic MPC with a
very short prediction horizon, such that the online computation burden reduces
significantly without affecting the controller performance. This approach
differs from existing work on value function approximations in the sense that
it learns the cost-to-go function by using offline-collected state-value pairs,
rather than closed-loop performance data. The cost of generating the
state-value pairs used for training is addressed using a sensitivity-based data
augmentation scheme.
\\ ( https://arxiv.org/abs/2401.12546 ,  4773kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12564
Date: Tue, 23 Jan 2024 08:47:28 GMT   (6611kb,D)

Title: Graph Contrastive Invariant Learning from the Causal Perspective
Authors: Yanhu Mo, Xiao Wang, Shaohua Fan, Chuan Shi
Categories: cs.LG cs.SI
\\
  Graph contrastive learning (GCL), learning the node representation by
contrasting two augmented graphs in a self-supervised way, has attracted
considerable attention. GCL is usually believed to learn the invariant
representation. However, does this understanding always hold in practice? In
this paper, we first study GCL from the perspective of causality. By analyzing
GCL with the structural causal model (SCM), we discover that traditional GCL
may not well learn the invariant representations due to the non-causal
information contained in the graph. How can we fix it and encourage the current
GCL to learn better invariant representations? The SCM offers two requirements
and motives us to propose a novel GCL method. Particularly, we introduce the
spectral graph augmentation to simulate the intervention upon non-causal
factors. Then we design the invariance objective and independence objective to
better capture the causal factors. Specifically, (i) the invariance objective
encourages the encoder to capture the invariant information contained in causal
variables, and (ii) the independence objective aims to reduce the influence of
confounders on the causal variables. Experimental results demonstrate the
effectiveness of our approach on node classification tasks.
\\ ( https://arxiv.org/abs/2401.12564 ,  6611kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12588
Date: Tue, 23 Jan 2024 09:43:30 GMT   (24352kb,D)

Title: Interpreting Equivariant Representations
Authors: Andreas Abildtrup Hansen, Anna Calissano, Aasa Feragen
Categories: cs.LG stat.ML
\\
  Latent representations are used extensively for downstream tasks, such as
visualization, interpolation or feature extraction of deep learning models.
Invariant and equivariant neural networks are powerful and well-established
models for enforcing inductive biases. In this paper, we demonstrate that the
inductive bias imposed on the by an equivariant model must also be taken into
account when using latent representations. We show how not accounting for the
inductive biases leads to decreased performance on downstream tasks, and vice
versa, how accounting for inductive biases can be done effectively by using an
invariant projection of the latent representations. We propose principles for
how to choose such a projection, and show the impact of using these principles
in two common examples: First, we study a permutation equivariant variational
auto-encoder trained for molecule graph generation; here we show that invariant
projections can be designed that incur no loss of information in the resulting
invariant representation. Next, we study a rotation-equivariant representation
used for image classification. Here, we illustrate how random invariant
projections can be used to obtain an invariant representation with a high
degree of retained information. In both cases, the analysis of invariant latent
representations proves superior to their equivariant counterparts. Finally, we
illustrate that the phenomena documented here for equivariant neural networks
have counterparts in standard neural networks where invariance is encouraged
via augmentation. Thus, while these ambiguities may be known by experienced
developers of equivariant models, we make both the knowledge as well as
effective tools to handle the ambiguities available to the broader community.
\\ ( https://arxiv.org/abs/2401.12588 ,  24352kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12610
Date: Tue, 23 Jan 2024 10:09:14 GMT   (831kb,D)

Title: The twin peaks of learning neural networks
Authors: Elizaveta Demyanenko, Christoph Feinauer, Enrico M. Malatesta, Luca
  Saglietti
Categories: cs.LG cond-mat.dis-nn math.PR math.ST stat.TH
Comments: 36 pages, 30 figures
\\
  Recent works demonstrated the existence of a double-descent phenomenon for
the generalization error of neural networks, where highly overparameterized
models escape overfitting and achieve good test performance, at odds with the
standard bias-variance trade-off described by statistical learning theory. In
the present work, we explore a link between this phenomenon and the increase of
complexity and sensitivity of the function represented by neural networks. In
particular, we study the Boolean mean dimension (BMD), a metric developed in
the context of Boolean function analysis. Focusing on a simple teacher-student
setting for the random feature model, we derive a theoretical analysis based on
the replica method that yields an interpretable expression for the BMD, in the
high dimensional regime where the number of data points, the number of
features, and the input size grow to infinity. We find that, as the degree of
overparameterization of the network is increased, the BMD reaches an evident
peak at the interpolation threshold, in correspondence with the generalization
error peak, and then slowly approaches a low asymptotic value. The same
phenomenology is then traced in numerical experiments with different model
classes and training setups. Moreover, we find empirically that adversarially
initialized models tend to show higher BMD values, and that models that are
more robust to adversarial attacks exhibit a lower BMD.
\\ ( https://arxiv.org/abs/2401.12610 ,  831kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12611
Date: Tue, 23 Jan 2024 10:10:01 GMT   (191kb,D)

Title: Prompt Smells: An Omen for Undesirable Generative AI Outputs
Authors: Krishna Ronanki, Beatriz Cabrero-Daniel, Christian Berger
Categories: cs.LG cs.SE
Comments: Accepted at CAIN 2024: Poster Track
\\
  Recent Generative Artificial Intelligence (GenAI) trends focus on various
applications, including creating stories, illustrations, poems, articles,
computer code, music compositions, and videos. Extrinsic hallucinations are a
critical limitation of such GenAI, which can lead to significant challenges in
achieving and maintaining the trustworthiness of GenAI. In this paper, we
propose two new concepts that we believe will aid the research community in
addressing limitations associated with the application of GenAI models. First,
we propose a definition for the "desirability" of GenAI outputs and three
factors which are observed to influence it. Second, drawing inspiration from
Martin Fowler's code smells, we propose the concept of "prompt smells" and the
adverse effects they are observed to have on the desirability of GenAI outputs.
We expect our work will contribute to the ongoing conversation about the
desirability of GenAI outputs and help advance the field in a meaningful way.
\\ ( https://arxiv.org/abs/2401.12611 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12617
Date: Tue, 23 Jan 2024 10:16:44 GMT   (2266kb,D)

Title: The Joint Effect of Task Similarity and Overparameterization on
  Catastrophic Forgetting -- An Analytical Model
Authors: Itay Evron, Daniel Goldfarb, Nir Weinberger, Daniel Soudry, Paul Hand
Categories: cs.LG
Comments: Accepted to the Twelfth International Conference on Learning
  Representations (ICLR 2024)
\\
  In continual learning, catastrophic forgetting is affected by multiple
aspects of the tasks. Previous works have analyzed separately how forgetting is
affected by either task similarity or overparameterization. In contrast, our
paper examines how task similarity and overparameterization jointly affect
forgetting in an analyzable model. Specifically, we focus on two-task continual
linear regression, where the second task is a random orthogonal transformation
of an arbitrary first task (an abstraction of random permutation tasks). We
derive an exact analytical expression for the expected forgetting - and uncover
a nuanced pattern. In highly overparameterized models, intermediate task
similarity causes the most forgetting. However, near the interpolation
threshold, forgetting decreases monotonically with the expected task
similarity. We validate our findings with linear regression on synthetic data,
and with neural networks on established permutation task benchmarks.
\\ ( https://arxiv.org/abs/2401.12617 ,  2266kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12631
Date: Tue, 23 Jan 2024 10:27:42 GMT   (277kb,D)

Title: A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments
Authors: Zhengxuan Wu and Atticus Geiger and Jing Huang and Aryaman Arora and
  Thomas Icard and Christopher Potts and Noah D. Goodman
Categories: cs.LG cs.AI cs.CL
Comments: 20 pages, 14 figures
\\
  We respond to the recent paper by Makelov et al. (2023), which reviews
subspace interchange intervention methods like distributed alignment search
(DAS; Geiger et al. 2023) and claims that these methods potentially cause
"interpretability illusions". We first review Makelov et al. (2023)'s technical
notion of what an "interpretability illusion" is, and then we show that even
intuitive and desirable explanations can qualify as illusions in this sense. As
a result, their method of discovering "illusions" can reject explanations they
consider "non-illusory". We then argue that the illusions Makelov et al. (2023)
see in practice are artifacts of their training and evaluation paradigms. We
close by emphasizing that, though we disagree with their core characterization,
Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the
field of interpretability forward.
\\ ( https://arxiv.org/abs/2401.12631 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12644
Date: Tue, 23 Jan 2024 10:54:13 GMT   (124kb,D)

Title: Binary Feature Mask Optimization for Feature Selection
Authors: Mehmet E. Lorasdagi, Mehmet Y. Turali, Ali T. Koc, Suleyman S. Kozat
Categories: cs.LG
\\
  We investigate feature selection problem for generic machine learning (ML)
models. We introduce a novel framework that selects features considering the
predictions of the model. Our framework innovates by using a novel feature
masking approach to eliminate the features during the selection process,
instead of completely removing them from the dataset. This allows us to use the
same ML model during feature selection, unlike other feature selection methods
where we need to train the ML model again as the dataset has different
dimensions on each iteration. We obtain the mask operator using the predictions
of the ML model, which offers a comprehensive view on the subsets of the
features essential for the predictive performance of the model. A variety of
approaches exist in the feature selection literature. However, no study has
introduced a training-free framework for a generic ML model to select features
while considering the importance of the feature subsets as a whole, instead of
focusing on the individual features. We demonstrate significant performance
improvements on the real-life datasets under different settings using LightGBM
and Multi-Layer Perceptron as our ML models. Additionally, we openly share the
implementation code for our methods to encourage the research and the
contributions in this area.
\\ ( https://arxiv.org/abs/2401.12644 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12648
Date: Tue, 23 Jan 2024 10:56:01 GMT   (1991kb,D)

Title: Consistency Enhancement-Based Deep Multiview Clustering via Contrastive
  Learning
Authors: Hao Yang, Hua Mao, Wai Lok Woo, Jie Chen and Xi Peng
Categories: cs.LG cs.CV
\\
  Multiview clustering (MVC) segregates data samples into meaningful clusters
by synthesizing information across multiple views. Moreover, deep
learning-based methods have demonstrated their strong feature learning
capabilities in MVC scenarios. However, effectively generalizing feature
representations while maintaining consistency is still an intractable problem.
In addition, most existing deep clustering methods based on contrastive
learning overlook the consistency of the clustering representations during the
clustering process. In this paper, we show how the above problems can be
overcome and propose a consistent enhancement-based deep MVC method via
contrastive learning (CCEC). Specifically, semantic connection blocks are
incorporated into a feature representation to preserve the consistent
information among multiple views. Furthermore, the representation process for
clustering is enhanced through spectral clustering, and the consistency across
multiple views is improved. Experiments conducted on five datasets demonstrate
the effectiveness and superiority of our method in comparison with the
state-of-the-art (SOTA) methods. The code for this method can be accessed at
https://anonymous.4open.science/r/CCEC-E84E/.
\\ ( https://arxiv.org/abs/2401.12648 ,  1991kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12681
Date: Tue, 23 Jan 2024 11:46:31 GMT   (14414kb,D)

Title: Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical
  Learning
Authors: Zhishuai Li, Yunhao Nie, Ziyue Li, Lei Bai, Yisheng Lv, Rui Zhao
Categories: cs.LG cs.AI
Comments: Accepted in AISTATS 2024
\\
  Kriging aims at estimating the attributes of unsampled geo-locations from
observations in the spatial vicinity or physical connections, which helps
mitigate skewed monitoring caused by under-deployed sensors. Existing works
assume that neighbors' information offers the basis for estimating the
attributes of the unobserved target while ignoring non-neighbors. However,
non-neighbors could also offer constructive information, and neighbors could
also be misleading. To this end, we propose ``Contrastive-Prototypical''
self-supervised learning for Kriging (KCP) to refine valuable information from
neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we
conduct the Kriging task from a new perspective of representation: we aim to
first learn robust and general representations and then recover attributes from
representations. A neighboring contrastive module is designed that coarsely
learns the representations by narrowing the representation distance between the
target and its neighbors while pushing away the non-neighbors. In parallel, a
prototypical module is introduced to identify similar representations via
exchanged prediction, thus refining the misleading neighbors and recycling the
useful non-neighbors from the neighboring contrast component. As a result, not
all the neighbors and some of the non-neighbors will be used to infer the
target. To encourage the two modules above to learn general and robust
representations, we design an adaptive augmentation module that incorporates
data-driven attribute augmentation and centrality-based topology augmentation
over the spatiotemporal Kriging graph data. Extensive experiments on real-world
datasets demonstrate the superior performance of KCP compared to its peers with
6% improvements and exceptional transferability and robustness. The code is
available at https://github.com/bonaldli/KCP
\\ ( https://arxiv.org/abs/2401.12681 ,  14414kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12683
Date: Tue, 23 Jan 2024 11:46:52 GMT   (1611kb,D)

Title: LLpowershap: Logistic Loss-based Automated Shapley Values Feature
  Selection Method
Authors: Iqbal Madakkatel and Elina Hypp\"onen
Categories: cs.LG
\\
  Shapley values have been used extensively in machine learning, not only to
explain black box machine learning models, but among other tasks, also to
conduct model debugging, sensitivity and fairness analyses and to select
important features for robust modelling and for further follow-up analyses.
Shapley values satisfy certain axioms that promote fairness in distributing
contributions of features toward prediction or reducing error, after accounting
for non-linear relationships and interactions when complex machine learning
models are employed. Recently, a number of feature selection methods utilising
Shapley values have been introduced. Here, we present a novel feature selection
method, LLpowershap, which makes use of loss-based Shapley values to identify
informative features with minimal noise among the selected sets of features.
Our simulation results show that LLpowershap not only identifies higher number
of informative features but outputs fewer noise features compared to other
state-of-the-art feature selection methods. Benchmarking results on four
real-world datasets demonstrate higher or at par predictive performance of
LLpowershap compared to other Shapley based wrapper methods, or filter methods.
\\ ( https://arxiv.org/abs/2401.12683 ,  1611kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12689
Date: Tue, 23 Jan 2024 11:54:09 GMT   (7738kb,D)

Title: Energy-based Automated Model Evaluation
Authors: Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao
Categories: cs.LG cs.AI cs.CL cs.CV
\\
  The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE's validity, together with its
superiority compared with prior approaches. We also prove MDE's versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels.
\\ ( https://arxiv.org/abs/2401.12689 ,  7738kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12708
Date: Tue, 23 Jan 2024 12:15:47 GMT   (9806kb,D)

Title: Deep Neural Network Benchmarks for Selective Classification
Authors: Andrea Pugnana and Lorenzo Perini and Jesse Davis and Salvatore
  Ruggieri
Categories: cs.LG cs.AI stat.ML
\\
  With the increasing deployment of machine learning models in many
socially-sensitive tasks, there is a growing demand for reliable and
trustworthy predictions. One way to accomplish these requirements is to allow a
model to abstain from making a prediction when there is a high risk of making
an error. This requires adding a selection mechanism to the model, which
selects those examples for which the model will provide a prediction. The
selective classification framework aims to design a mechanism that balances the
fraction of rejected predictions (i.e., the proportion of examples for which
the model does not make a prediction) versus the improvement in predictive
performance on the selected predictions. Multiple selective classification
frameworks exist, most of which rely on deep neural network architectures.
However, the empirical evaluation of the existing approaches is still limited
to partial comparisons among methods and settings, providing practitioners with
little insight into their relative merits. We fill this gap by benchmarking 18
baselines on a diverse set of 44 datasets that includes both image and tabular
data. Moreover, there is a mix of binary and multiclass tasks. We evaluate
these approaches using several criteria, including selective error rate,
empirical coverage, distribution of rejected instance's classes, and
performance on out-of-distribution instances. The results indicate that there
is not a single clear winner among the surveyed baselines, and the best method
depends on the users' objectives.
\\ ( https://arxiv.org/abs/2401.12708 ,  9806kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12711
Date: Tue, 23 Jan 2024 12:20:17 GMT   (5738kb,D)

Title: When Redundancy Matters: Machine Teaching of Representations
Authors: C\`esar Ferri, Dario Garigliotti, Brigt Arve Toppe H{\aa}vardstun,
  Jos\`e Hern\'andez-Orallo, Jan Arne Telle
Categories: cs.LG
Comments: 16 pages, 3 figures, 3 tables
MSC-class: 68T05
ACM-class: I.2.6
\\
  In traditional machine teaching, a teacher wants to teach a concept to a
learner, by means of a finite set of examples, the witness set. But concepts
can have many equivalent representations. This redundancy strongly affects the
search space, to the extent that teacher and learner may not be able to easily
determine the equiv- alence class of each representation. In this common
situation, instead of teaching concepts, we explore the idea of teaching
representations. We work with several teaching schemas that exploit
representation and witness size (Eager, Greedy and Optimal) and analyze the
gains in teaching effectiveness for some representational languages (DNF
expressions and Turing-complete P3 programs). Our theoreti- cal and
experimental results indicate that there are various types of redundancy,
handled better by the Greedy schema introduced here than by the Eager schema,
although both can be arbitrarily far away from the Optimal. For P3 programs we
found that witness sets are usually smaller than the programs they identify,
which is an illuminating justification of why machine teaching from examples
makes sense at all.
\\ ( https://arxiv.org/abs/2401.12711 ,  5738kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12722
Date: Tue, 23 Jan 2024 12:48:27 GMT   (2715kb,D)

Title: Falcon: Fair Active Learning using Multi-armed Bandits
Authors: Ki Hyun Tae, Hantian Zhang, Jaeyoung Park, Kexin Rong, Steven Euijong
  Whang
Categories: cs.LG
Comments: 18 pages, 12 figures, 14 tables
\\
  Biased data can lead to unfair machine learning models, highlighting the
importance of embedding fairness at the beginning of data analysis,
particularly during dataset curation and labeling. In response, we propose
Falcon, a scalable fair active learning framework. Falcon adopts a data-centric
approach that improves machine learning model fairness via strategic sample
selection. Given a user-specified group fairness measure, Falcon identifies
samples from "target groups" (e.g., (attribute=female, label=positive)) that
are the most informative for improving fairness. However, a challenge arises
since these target groups are defined using ground truth labels that are not
available during sample selection. To handle this, we propose a novel
trial-and-error method, where we postpone using a sample if the predicted label
is different from the expected one and falls outside the target group. We also
observe the trade-off that selecting more informative samples results in higher
likelihood of postponing due to undesired label prediction, and the optimal
balance varies per dataset. We capture the trade-off between informativeness
and postpone rate as policies and propose to automatically select the best
policy using adversarial multi-armed bandit methods, given their computational
efficiency and theoretical guarantees. Experiments show that Falcon
significantly outperforms existing fair active learning approaches in terms of
fairness and accuracy and is more efficient. In particular, only Falcon
supports a proper trade-off between accuracy and fairness where its maximum
fairness score is 1.8-4.5x higher than the second-best results.
\\ ( https://arxiv.org/abs/2401.12722 ,  2715kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12745
Date: Tue, 23 Jan 2024 13:23:59 GMT   (505kb,D)

Title: On the Utility of Probing Trajectories for Algorithm-Selection
Authors: Quentin Renau and Emma Hart
Categories: cs.LG cs.NE
Comments: To appear in the proceedings of the 27th International Conference,
  EvoApplications 2024
\\
  Machine-learning approaches to algorithm-selection typically take data
describing an instance as input. Input data can take the form of features
derived from the instance description or fitness landscape, or can be a direct
representation of the instance itself, i.e. an image or textual description.
Regardless of the choice of input, there is an implicit assumption that
instances that are similar will elicit similar performance from algorithm, and
that a model is capable of learning this relationship. We argue that viewing
algorithm-selection purely from an instance perspective can be misleading as it
fails to account for how an algorithm `views' similarity between instances. We
propose a novel `algorithm-centric' method for describing instances that can be
used to train models for algorithm-selection: specifically, we use short
probing trajectories calculated by applying a solver to an instance for a very
short period of time. The approach is demonstrated to be promising, providing
comparable or better results to computationally expensive landscape-based
feature-based approaches. Furthermore, projecting the trajectories into a
2-dimensional space illustrates that functions that are similar from an
algorithm-perspective do not necessarily correspond to the accepted
categorisation of these functions from a human perspective.
\\ ( https://arxiv.org/abs/2401.12745 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12780
Date: Tue, 23 Jan 2024 14:06:08 GMT   (2603kb,D)

Title: DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for
  Alleviating Over-squashing
Authors: Li Sun, Zhenhao Huang, Hua Wu, Junda Ye, Hao Peng, Zhengtao Yu, Philip
  S. Yu
Categories: cs.LG
Comments: Accepted by IEEE ICDM 2023, Full paper, 10 pages
\\
  Graph Neural Networks (GNNs) have shown great power for learning and mining
on graphs, and Graph Structure Learning (GSL) plays an important role in
boosting GNNs with a refined graph. In the literature, most GSL solutions
either primarily focus on structure refinement with task-specific supervision
(i.e., node classification), or overlook the inherent weakness of GNNs
themselves (e.g., over-squashing), resulting in suboptimal performance despite
sophisticated designs. In light of these limitations, we propose to study
self-supervised graph structure-feature co-refinement for effectively
alleviating the issue of over-squashing in typical GNNs. In this paper, we take
a fundamentally different perspective of the Ricci curvature in Riemannian
geometry, in which we encounter the challenges of modeling, utilizing and
computing Ricci curvature. To tackle these challenges, we present a
self-supervised Riemannian model, DeepRicci. Specifically, we introduce a
latent Riemannian space of heterogeneous curvatures to model various Ricci
curvatures, and propose a gyrovector feature mapping to utilize Ricci curvature
for typical GNNs. Thereafter, we refine node features by geometric contrastive
learning among different geometric views, and simultaneously refine graph
structure by backward Ricci flow based on a novel formulation of differentiable
Ricci curvature. Finally, extensive experiments on public datasets show the
superiority of DeepRicci, and the connection between backward Ricci flow and
over-squashing. Codes of our work are given in https://github.com/RiemanGraph/.
\\ ( https://arxiv.org/abs/2401.12780 ,  2603kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12790
Date: Tue, 23 Jan 2024 14:25:43 GMT   (4620kb,D)

Title: MORPH: Towards Automated Concept Drift Adaptation for Malware Detection
Authors: Md Tanvirul Alam, Romy Fieblinger, Ashim Mahara, and Nidhi Rastogi
Categories: cs.LG
\\
  Concept drift is a significant challenge for malware detection, as the
performance of trained machine learning models degrades over time, rendering
them impractical. While prior research in malware concept drift adaptation has
primarily focused on active learning, which involves selecting representative
samples to update the model, self-training has emerged as a promising approach
to mitigate concept drift. Self-training involves retraining the model using
pseudo labels to adapt to shifting data distributions. In this research, we
propose MORPH -- an effective pseudo-label-based concept drift adaptation
method specifically designed for neural networks. Through extensive
experimental analysis of Android and Windows malware datasets, we demonstrate
the efficacy of our approach in mitigating the impact of concept drift. Our
method offers the advantage of reducing annotation efforts when combined with
active learning. Furthermore, our method significantly improves over existing
works in automated concept drift adaptation for malware detection.
\\ ( https://arxiv.org/abs/2401.12790 ,  4620kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12806
Date: Tue, 23 Jan 2024 14:37:51 GMT   (6184kb,D)

Title: Binary structured physics-informed neural networks for solving equations
  with rapidly changing solutions
Authors: Yanzhi Liu and Ruifan Wu and Ying Jiang
Categories: cs.LG cs.AI
\\
  Physics-informed neural networks (PINNs), rooted in deep learning, have
emerged as a promising approach for solving partial differential equations
(PDEs). By embedding the physical information described by PDEs into
feedforward neural networks, PINNs are trained as surrogate models to
approximate solutions without the need for label data. Nevertheless, even
though PINNs have shown remarkable performance, they can face difficulties,
especially when dealing with equations featuring rapidly changing solutions.
These difficulties encompass slow convergence, susceptibility to becoming
trapped in local minima, and reduced solution accuracy. To address these
issues, we propose a binary structured physics-informed neural network (BsPINN)
framework, which employs binary structured neural network (BsNN) as the neural
network component. By leveraging a binary structure that reduces inter-neuron
connections compared to fully connected neural networks, BsPINNs excel in
capturing the local features of solutions more effectively and efficiently.
These features are particularly crucial for learning the rapidly changing in
the nature of solutions. In a series of numerical experiments solving Burgers
equation, Euler equation, Helmholtz equation, and high-dimension Poisson
equation, BsPINNs exhibit superior convergence speed and heightened accuracy
compared to PINNs. From these experiments, we discover that BsPINNs resolve the
issues caused by increased hidden layers in PINNs resulting in over-smoothing,
and prevent the decline in accuracy due to non-smoothness of PDEs solutions.
\\ ( https://arxiv.org/abs/2401.12806 ,  6184kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12819
Date: Tue, 23 Jan 2024 14:53:20 GMT   (4878kb,D)

Title: Dynamic Layer Tying for Parameter-Efficient Transformers
Authors: Tamir David Hay, Lior Wolf
Categories: cs.LG cs.AI
\\
  In the pursuit of reducing the number of trainable parameters in deep
transformer networks, we employ Reinforcement Learning to dynamically select
layers during training and tie them together. Every few iterations, the RL
agent is asked whether to train each layer $i$ independently or to copy the
weights of a previous layer $j<i$. This facilitates weight sharing, reduces the
number of trainable parameters, and also serves as an effective regularization
technique. Experimental evaluations validate that our model modestly
outperforms the baseline transformer model with regard to perplexity and
drastically reduces the number of trainable parameters. In particular, the
memory consumption during training is up to one order of magnitude less than
the conventional training method.
\\ ( https://arxiv.org/abs/2401.12819 ,  4878kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12824
Date: Tue, 23 Jan 2024 14:59:46 GMT   (22410kb,D)

Title: MAPPING: Debiasing Graph Neural Networks for Fair Node Classification
  with Limited Sensitive Information Leakage
Authors: Ying Song and Balaji Palanisamy
Categories: cs.LG stat.ML
Comments: Finished May last year. Remember to submit all papers to arXiv early
  without compromising the principles of conferences
\\
  Despite remarkable success in diverse web-based applications, Graph Neural
Networks(GNNs) inherit and further exacerbate historical discrimination and
social stereotypes, which critically hinder their deployments in high-stake
domains such as online clinical diagnosis, financial crediting, etc. However,
current fairness research that primarily craft on i.i.d data, cannot be
trivially replicated to non-i.i.d. graph structures with topological dependence
among samples. Existing fair graph learning typically favors pairwise
constraints to achieve fairness but fails to cast off dimensional limitations
and generalize them into multiple sensitive attributes; besides, most studies
focus on in-processing techniques to enforce and calibrate fairness,
constructing a model-agnostic debiasing GNN framework at the pre-processing
stage to prevent downstream misuses and improve training reliability is still
largely under-explored. Furthermore, previous work on GNNs tend to enhance
either fairness or privacy individually but few probe into their interplays. In
this paper, we propose a novel model-agnostic debiasing framework named MAPPING
(\underline{M}asking \underline{A}nd \underline{P}runing and
Message-\underline{P}assing train\underline{ING}) for fair node classification,
in which we adopt the distance covariance($dCov$)-based fairness constraints to
simultaneously reduce feature and topology biases in arbitrary dimensions, and
combine them with adversarial debiasing to confine the risks of attribute
inference attacks. Experiments on real-world datasets with different GNN
variants demonstrate the effectiveness and flexibility of MAPPING. Our results
show that MAPPING can achieve better trade-offs between utility and fairness,
and mitigate privacy risks of sensitive information leakage.
\\ ( https://arxiv.org/abs/2401.12824 ,  22410kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12830
Date: Tue, 23 Jan 2024 15:07:49 GMT   (174kb,D)

Title: Enhancing Next Destination Prediction: A Novel LSTM Approach Using
  Real-World Airline Data
Authors: Salih Salihoglu, Gulser Koksal, Orhan Abar
Categories: cs.LG cs.AI
\\
  In the modern transportation industry, accurate prediction of travelers' next
destinations brings multiple benefits to companies, such as customer
satisfaction and targeted marketing. This study focuses on developing a precise
model that captures the sequential patterns and dependencies in travel data,
enabling accurate predictions of individual travelers' future destinations. To
achieve this, a novel model architecture with a sliding window approach based
on Long Short-Term Memory (LSTM) is proposed for destination prediction in the
transportation industry. The experimental results highlight satisfactory
performance and high scores achieved by the proposed model across different
data sizes and performance metrics. This research contributes to advancing
destination prediction methods, empowering companies to deliver personalized
recommendations and optimize customer experiences in the dynamic travel
landscape.
\\ ( https://arxiv.org/abs/2401.12830 ,  174kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12842
Date: Tue, 23 Jan 2024 15:23:13 GMT   (1716kb,D)

Title: Iterated Relevance Matrix Analysis (IRMA) for the identification of
  class-discriminative subspaces
Authors: Sofie L\"ovdal and Michael Biehl
Categories: cs.LG
Comments: 17 pages, 5 figures, 1 table. Submitted to Neurocomputing. Extension
  of 2023 ESANN conference contribution
\\
  We introduce and investigate the iterated application of Generalized Matrix
Learning Vector Quantizaton for the analysis of feature relevances in
classification problems, as well as for the construction of
class-discriminative subspaces. The suggested Iterated Relevance Matrix
Analysis (IRMA) identifies a linear subspace representing the classification
specific information of the considered data sets using Generalized Matrix
Learning Vector Quantization (GMLVQ). By iteratively determining a new
discriminative subspace while projecting out all previously identified ones, a
combined subspace carrying all class-specific information can be found. This
facilitates a detailed analysis of feature relevances, and enables improved
low-dimensional representations and visualizations of labeled data sets.
Additionally, the IRMA-based class-discriminative subspace can be used for
dimensionality reduction and the training of robust classifiers with
potentially improved performance.
\\ ( https://arxiv.org/abs/2401.12842 ,  1716kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12849
Date: Tue, 23 Jan 2024 15:33:30 GMT   (869kb,D)

Title: Learning safety critics via a non-contractive binary bellman operator
Authors: Agustin Castellano, Hancheng Min, Juan Andr\'es Bazerque, Enrique
  Mallada
Categories: cs.LG cs.SY eess.SY
\\
  The inability to naturally enforce safety in Reinforcement Learning (RL),
with limited failures, is a core challenge impeding its use in real-world
applications. One notion of safety of vast practical relevance is the ability
to avoid (unsafe) regions of the state space. Though such a safety goal can be
captured by an action-value-like function, a.k.a. safety critics, the
associated operator lacks the desired contraction and uniqueness properties
that the classical Bellman operator enjoys. In this work, we overcome the
non-contractiveness of safety critic operators by leveraging that safety is a
binary property. To that end, we study the properties of the binary safety
critic associated with a deterministic dynamical system that seeks to avoid
reaching an unsafe region. We formulate the corresponding binary Bellman
equation (B2E) for safety and study its properties. While the resulting
operator is still non-contractive, we fully characterize its fixed points
representing--except for a spurious solution--maximal persistently safe regions
of the state space that can always avoid failure. We provide an algorithm that,
by design, leverages axiomatic knowledge of safe data to avoid spurious fixed
points.
\\ ( https://arxiv.org/abs/2401.12849 ,  869kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12882
Date: Tue, 23 Jan 2024 16:22:50 GMT   (1002kb)

Title: Model-Free $\delta$-Policy Iteration Based on Damped Newton Method for
  Nonlinear Continuous-Time H$\infty$ Tracking Control
Authors: Qi Wang
Categories: cs.LG
Comments: 10 pages, 8 figures
\\
  This paper presents a {\delta}-PI algorithm which is based on damped Newton
method for the H{\infty} tracking control problem of unknown continuous-time
nonlinear system. A discounted performance function and an augmented system are
used to get the tracking Hamilton-Jacobi-Isaac (HJI) equation. Tracking HJI
equation is a nonlinear partial differential equation, traditional
reinforcement learning methods for solving the tracking HJI equation are mostly
based on the Newton method, which usually only satisfies local convergence and
needs a good initial guess. Based upon the damped Newton iteration operator
equation, a generalized tracking Bellman equation is derived firstly. The
{\delta}-PI algorithm can seek the optimal solution of the tracking HJI
equation by iteratively solving the generalized tracking Bellman equation.
On-policy learning and off-policy learning {\delta}-PI reinforcement learning
methods are provided, respectively. Off-policy version {\delta}-PI algorithm is
a model-free algorithm which can be performed without making use of a priori
knowledge of the system dynamics. NN-based implementation scheme for the
off-policy {\delta}-PI algorithms is shown. The suitability of the model-free
{\delta}-PI algorithm is illustrated with a nonlinear system simulation.
\\ ( https://arxiv.org/abs/2401.12882 ,  1002kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12926
Date: Tue, 23 Jan 2024 17:22:00 GMT   (1200kb,D)

Title: DsDm: Model-Aware Dataset Selection with Datamodels
Authors: Logan Engstrom, Axel Feldmann, Aleksander Madry
Categories: cs.LG stat.ML
\\
  When selecting data for training large-scale models, standard practice is to
filter for examples that match human notions of data quality. Such filtering
yields qualitatively clean datapoints that intuitively should improve model
behavior. However, in practice the opposite can often happen: we find that
selecting according to similarity with "high quality" data sources may not
increase (and can even hurt) performance compared to randomly selecting data.
  To develop better methods for selecting data, we start by framing dataset
selection as an optimization problem that we can directly solve for: given
target tasks, a learning algorithm, and candidate data, select the subset that
maximizes model performance. This framework thus avoids handpicked notions of
data quality, and instead models explicitly how the learning process uses train
datapoints to predict on the target tasks. Our resulting method greatly
improves language model (LM) performance on both pre-specified tasks and
previously unseen tasks. Specifically, choosing target tasks representative of
standard LM problems and evaluating on diverse held-out benchmarks, our
selected datasets provide a 2x compute multiplier over baseline methods.
\\ ( https://arxiv.org/abs/2401.12926 ,  1200kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12930
Date: Tue, 23 Jan 2024 17:33:41 GMT   (419kb,D)

Title: pyAKI - An Open Source Solution to Automated KDIGO classification
Authors: Christian Porschen, Jan Ernsting, Paul Brauckmann, Raphael Weiss, Till
  W\"urdemann, Hendrik Booke, Wida Amini, Ludwig Maidowski, Benjamin Risse, Tim
  Hahn, Thilo von Groote
Categories: cs.LG cs.SE
\\
  Acute Kidney Injury (AKI) is a frequent complication in critically ill
patients, affecting up to 50% of patients in the intensive care units. The lack
of standardized and open-source tools for applying the Kidney Disease Improving
Global Outcomes (KDIGO) criteria to time series data has a negative impact on
workload and study quality. This project introduces pyAKI, an open-source
pipeline addressing this gap by providing a comprehensive solution for
consistent KDIGO criteria implementation.
  The pyAKI pipeline was developed and validated using a subset of the Medical
Information Mart for Intensive Care (MIMIC)-IV database, a commonly used
database in critical care research. We defined a standardized data model in
order to ensure reproducibility. Validation against expert annotations
demonstrated pyAKI's robust performance in implementing KDIGO criteria.
Comparative analysis revealed its ability to surpass the quality of human
labels.
  This work introduces pyAKI as an open-source solution for implementing the
KDIGO criteria for AKI diagnosis using time series data with high accuracy and
performance.
\\ ( https://arxiv.org/abs/2401.12930 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12950
Date: Tue, 23 Jan 2024 18:15:58 GMT   (7615kb,D)

Title: Bayesian Semi-structured Subspace Inference
Authors: Daniel Dold, David R\"ugamer, Beate Sick, Oliver D\"urr
Categories: cs.LG stat.ML
Comments: Accepted at AISTATS 2024
\\
  Semi-structured regression models enable the joint modeling of interpretable
structured and complex unstructured feature effects. The structured model part
is inspired by statistical models and can be used to infer the input-output
relationship for features of particular importance. The complex unstructured
part defines an arbitrary deep neural network and thereby provides enough
flexibility to achieve competitive prediction performance. While these models
can also account for aleatoric uncertainty, there is still a lack of work on
accounting for epistemic uncertainty. In this paper, we address this problem by
presenting a Bayesian approximation for semi-structured regression models using
subspace inference. To this end, we extend subspace inference for joint
posterior sampling from a full parameter space for structured effects and a
subspace for unstructured effects. Apart from this hybrid sampling scheme, our
method allows for tunable complexity of the subspace and can capture multiple
minima in the loss landscape. Numerical experiments validate our approach's
efficacy in recovering structured effect parameter posteriors in
semi-structured models and approaching the full-space posterior distribution of
MCMC for increasing subspace dimension. Further, our approach exhibits
competitive predictive performance across simulated and real-world datasets.
\\ ( https://arxiv.org/abs/2401.12950 ,  7615kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.12223 (*cross-listing*)
Date: Fri, 22 Dec 2023 00:41:21 GMT   (458kb)

Title: The Global Impact of AI-Artificial Intelligence: Recent Advances and
  Future Directions, A Review
Authors: Chandregowda Pachegowda
Categories: cs.CR cs.AI
Comments: 4 pages
\\
  Artificial intelligence (AI) is an emerging technology that has the potential
to transform many aspects of society, including the economy, healthcare, and
transportation. This article synthesizes recent research literature on the
global impact of AI, exploring its potential benefits and risks. The article
highlights the implications of AI, including its impact on economic, ethical,
social, security & privacy, and job displacement aspects. It discusses the
ethical concerns surrounding AI development, including issues of bias,
security, and privacy violations. To ensure the responsible development and
deployment of AI, collaboration between government, industry, and academia is
essential. The article concludes by emphasizing the importance of public
engagement and education to promote awareness and understanding of AI's impact
on society at large.
\\ ( https://arxiv.org/abs/2401.12223 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12224 (*cross-listing*)
Date: Thu, 28 Dec 2023 15:09:14 GMT   (249kb,D)

Title: LLM4EDA: Emerging Progress in Large Language Models for Electronic
  Design Automation
Authors: Ruizhe Zhong, Xingbo Du, Shixiong Kai, Zhentao Tang, Siyuan Xu,
  Hui-Ling Zhen, Jianye Hao, Qiang Xu, Mingxuan Yuan, Junchi Yan
Categories: cs.AR cs.AI
Comments: 15 pages, 4 figures
\\
  Driven by Moore's Law, the complexity and scale of modern chip design are
increasing rapidly. Electronic Design Automation (EDA) has been widely applied
to address the challenges encountered in the full chip design process. However,
the evolution of very large-scale integrated circuits has made chip design
time-consuming and resource-intensive, requiring substantial prior expert
knowledge. Additionally, intermediate human control activities are crucial for
seeking optimal solutions. In system design stage, circuits are usually
represented with Hardware Description Language (HDL) as a textual format.
Recently, Large Language Models (LLMs) have demonstrated their capability in
context understanding, logic reasoning and answer generation. Since circuit can
be represented with HDL in a textual format, it is reasonable to question
whether LLMs can be leveraged in the EDA field to achieve fully automated chip
design and generate circuits with improved power, performance, and area (PPA).
In this paper, we present a systematic study on the application of LLMs in the
EDA field, categorizing it into the following cases: 1) assistant chatbot, 2)
HDL and script generation, and 3) HDL verification and analysis. Additionally,
we highlight the future research direction, focusing on applying LLMs in logic
synthesis, physical design, multi-modal feature extraction and alignment of
circuits. We collect relevant papers up-to-date in this field via the following
link: https://github.com/Thinklab-SJTU/Awesome-LLM4EDA.
\\ ( https://arxiv.org/abs/2401.12224 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12244 (*cross-listing*)
Date: Sat, 20 Jan 2024 08:10:43 GMT   (43496kb,D)

Title: Large-scale Reinforcement Learning for Diffusion Models
Authors: Yinan Zhang, Eric Tzeng, Yilun Du, Dmitry Kislyuk
Categories: cs.CV cs.AI cs.LG
\\
  Text-to-image diffusion models are a class of deep generative models that
have demonstrated an impressive capacity for high-quality image generation.
However, these models are susceptible to implicit biases that arise from
web-scale text-image training pairs and may inaccurately model aspects of
images we care about. This can result in suboptimal samples, model bias, and
images that do not align with human ethics and preferences. In this paper, we
present an effective scalable algorithm to improve diffusion models using
Reinforcement Learning (RL) across a diverse set of reward functions, such as
human preference, compositionality, and fairness over millions of images. We
illustrate how our approach substantially outperforms existing methods for
aligning diffusion models with human preferences. We further illustrate how
this substantially improves pretrained Stable Diffusion (SD) models, generating
samples that are preferred by humans 80.3% of the time over those from the base
SD model while simultaneously improving both the composition and diversity of
generated samples.
\\ ( https://arxiv.org/abs/2401.12244 ,  43496kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12255 (*cross-listing*)
Date: Sun, 21 Jan 2024 09:51:45 GMT   (3109kb,D)

Title: Instructional Fingerprinting of Large Language Models
Authors: Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang Wei Koh, Chaowei Xiao,
  Muhao Chen
Categories: cs.CR cs.AI
Comments: 30 pages
\\
  The exorbitant cost of training Large language models (LLMs) from scratch
makes it essential to fingerprint the models to protect intellectual property
via ownership authentication and to ensure downstream users and developers
comply with their license terms (e.g. restricting commercial use). In this
study, we present a pilot study on LLM fingerprinting as a form of very
lightweight instruction tuning. Model publisher specifies a confidential
private key and implants it as an instruction backdoor that causes the LLM to
generate specific text when the key is present. Results on 11 popularly-used
LLMs showed that this approach is lightweight and does not affect the normal
behavior of the model. It also prevents publisher overclaim, maintains
robustness against fingerprint guessing and parameter-efficient training, and
supports multi-stage fingerprinting akin to MIT License. Code is available in
https://cnut1648.github.io/Model-Fingerprint/.
\\ ( https://arxiv.org/abs/2401.12255 ,  3109kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12258 (*cross-listing*)
Date: Sun, 21 Jan 2024 16:59:45 GMT   (1401kb,D)

Title: Emergent Dominance Hierarchies in Reinforcement Learning Agents
Authors: Ram Rachum, Yonatan Nakar, Bill Tomlinson, Nitay Alon, Reuth Mirsky
Categories: cs.MA cs.AI cs.GT cs.LG
\\
  Modern Reinforcement Learning (RL) algorithms are able to outperform humans
in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings
present additional challenges, and successful cooperation in mixed-motive
groups of agents depends on a delicate balancing act between individual and
group objectives. Social conventions and norms, often inspired by human
institutions, are used as tools for striking this balance.
  In this paper, we examine a fundamental, well-studied social convention that
underlies cooperation in both animal and human societies: Dominance
hierarchies.
  We adapt the ethological theory of dominance hierarchies to artificial
agents, borrowing the established terminology and definitions with as few
amendments as possible. We demonstrate that populations of RL agents, operating
without explicit programming or intrinsic rewards, can invent, learn, enforce,
and transmit a dominance hierarchy to new populations. The dominance
hierarchies that emerge have a similar structure to those studied in chickens,
mice, fish, and other species.
\\ ( https://arxiv.org/abs/2401.12258 ,  1401kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12259 (*cross-listing*)
Date: Sun, 21 Jan 2024 17:43:08 GMT   (2415kb)

Title: Agreement Technologies for Coordination in Smart Cities
Authors: Holger Billhardt, Alberto Fern\'andez, Marin Lujak, Sascha Ossowski
Categories: cs.MA cs.AI
ACM-class: I.2.1
Journal-ref: Applied Sciences, Volume 8, Issue 5 (2018)
DOI: 10.3390/app8050816
\\
  Many challenges in today's society can be tackled by distributed open
systems. This is particularly true for domains that are commonly perceived
under the umbrella of smart cities, such as intelligent transportation, smart
energy grids, or participative governance. When designing computer applications
for these domains, it is necessary to account for the fact that the elements of
such systems, often called software agents, are usually made by different
designers and act on behalf of particular stakeholders. Furthermore, it is
unknown at design time when such agents will enter or leave the system, and
what interests new agents will represent. To instil coordination in such
systems is particularly demanding, as usually only part of them can be directly
controlled at runtime. Agreement technologies refer to a sandbox of tools and
mechanisms for the development of such open multiagent systems, which are based
on the notion of agreement. In this paper, we argue that agreement technologies
are a suitable means for achieving coordination in smart city domains, and back
our claim through examples of several real-world applications.
\\ ( https://arxiv.org/abs/2401.12259 ,  2415kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12261 (*cross-listing*)
Date: Mon, 22 Jan 2024 00:37:01 GMT   (16083kb,D)

Title: Analyzing the Quality Attributes of AI Vision Models in Open
  Repositories Under Adversarial Attacks
Authors: Zerui Wang, Yan Liu
Categories: cs.CR cs.AI
Comments: 10 pages
\\
  As AI models rapidly evolve, they are frequently released to open
repositories, such as HuggingFace. It is essential to perform quality assurance
validation on these models before integrating them into the production
development lifecycle. In addition to evaluating efficiency in terms of
balanced accuracy and computing costs, adversarial attacks are potential
threats to the robustness and explainability of AI models. Meanwhile, XAI
applies algorithms that approximate inputs to outputs post-hoc to identify the
contributing features. Adversarial perturbations may also degrade the utility
of XAI explanations that require further investigation. In this paper, we
present an integrated process designed for downstream evaluation tasks,
including validating AI model accuracy, evaluating robustness with benchmark
perturbations, comparing explanation utility, and assessing overhead. We
demonstrate an evaluation scenario involving six computer vision models, which
include CNN-based, Transformer-based, and hybrid architectures, three types of
perturbations, and five XAI methods, resulting in ninety unique combinations.
The process reveals the explanation utility among the XAI methods in terms of
the identified key areas responding to the adversarial perturbation. The
process produces aggregated results that illustrate multiple attributes of each
AI model.
\\ ( https://arxiv.org/abs/2401.12261 ,  16083kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12273 (*cross-listing*)
Date: Mon, 22 Jan 2024 17:11:37 GMT   (238kb)

Title: The Ethics of Interaction: Mitigating Security Threats in LLMs
Authors: Ashutosh Kumar, Sagarika Singh, Shiv Vignesh Murty, Swathy Ragupathy
Categories: cs.CR cs.AI cs.CL
\\
  This paper comprehensively explores the ethical challenges arising from
security threats to Language Learning Models (LLMs). These intricate digital
repositories are increasingly integrated into our daily lives, making them
prime targets for attacks that can compromise their training data and the
confidentiality of their data sources. The paper delves into the nuanced
ethical repercussions of such security threats on society and individual
privacy. We scrutinize five major threats: prompt injection, jailbreaking,
Personal Identifiable Information (PII) exposure, sexually explicit content,
and hate based content, going beyond mere identification to assess their
critical ethical consequences and the urgency they create for robust defensive
strategies. The escalating reliance on LLMs underscores the crucial need for
ensuring these systems operate within the bounds of ethical norms, particularly
as their misuse can lead to significant societal and individual harm. We
propose conceptualizing and developing an evaluative tool tailored for LLMs,
which would serve a dual purpose, guiding developers and designers in
preemptive fortification of backend systems and scrutinizing the ethical
dimensions of LLM chatbot responses during the testing phase. By comparing LLM
responses with those expected from humans in a moral context, we aim to discern
the degree to which AI behaviors align with the ethical values held by a
broader society. Ultimately, this paper not only underscores the ethical
troubles presented by LLMs, it also highlights a path toward cultivating trust
in these systems.
\\ ( https://arxiv.org/abs/2401.12273 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12275 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:58:22 GMT   (13255kb,D)

Title: Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation
Authors: Jiachen Li and Chuanbo Hua and Hengbo Ma and Jinkyoo Park and Victoria
  Dax and Mykel J. Kochenderfer
Categories: cs.RO cs.AI cs.CV cs.LG cs.MA
Comments: 19 pages, 8 figures, 6 tables
\\
  Social robot navigation can be helpful in various contexts of daily life but
requires safe human-robot interactions and efficient trajectory planning. While
modeling pairwise relations has been widely studied in multi-agent interacting
systems, the ability to capture larger-scale group-wise activities is limited.
In this paper, we propose a systematic relational reasoning approach with
explicit inference of the underlying dynamically evolving relational
structures, and we demonstrate its effectiveness for multi-agent trajectory
prediction and social robot navigation. In addition to the edges between pairs
of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect
multiple nodes to enable group-wise reasoning in an unsupervised manner. Our
approach infers dynamically evolving relation graphs and hypergraphs to capture
the evolution of relations, which the trajectory predictor employs to generate
future states. Meanwhile, we propose to regularize the sharpness and sparsity
of the learned relations and the smoothness of the relation evolution, which
proves to enhance training stability and model performance. The proposed
approach is validated on synthetic crowd simulations and real-world benchmark
datasets. Experiments demonstrate that the approach infers reasonable relations
and achieves state-of-the-art prediction performance. In addition, we present a
deep reinforcement learning (DRL) framework for social robot navigation, which
incorporates relational reasoning and trajectory prediction systematically. In
a group-based crowd simulation, our method outperforms the strongest baseline
by a significant margin in terms of safety, efficiency, and social compliance
in dense, interactive scenarios.
\\ ( https://arxiv.org/abs/2401.12275 ,  13255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12329 (*cross-listing*)
Date: Mon, 22 Jan 2024 19:43:54 GMT   (482kb)

Title: Towards a prioritised use of transportation infrastructures: the case of
  vehicle-specific dynamic access restrictions to city centres
Authors: Holger Billhardt, Alberto Fern\'andez, Pasqual Mart\'i, Javier Prieto
  Tejedor, Sascha Ossowski
Categories: physics.soc-ph cs.AI
ACM-class: I.2.1
Journal-ref: Electronics, Volume 11, Issue 4 (2022)
DOI: 10.3390/electronics11040576
\\
  One of the main problems that local authorities of large cities have to face
is the regulation of urban mobility. They need to provide the means to allow
for the efficient movement of people and distribution of goods. However, the
provisioning of transportation services needs to take into account general
global objectives, like reducing emissions and having more healthy living
environments, which may not always be aligned with individual interests. Urban
mobility is usually provided through a transport infrastructure that includes
all the elements that support mobility. On many occasions, the capacity of the
elements of this infrastructure is lower than the actual demand and thus
different transportation activities compete for their use. In this paper, we
argue that scarce transport infrastructure elements should be assigned
dynamically and in a prioritised manner to transport activities that have a
higher utility from the point of view of society; for example, activities that
produce less pollution and provide more value to society. In this paper, we
define a general model for prioritizing the use of a particular type of
transportation infrastructure element called time-unlimited elements, whose
usage time is unknown a priori, and illustrate its dynamics through two use
cases: vehicle-specific dynamic access restriction in city centres (i) based on
the usage levels of available parking spaces and (ii) to assure sustained
admissible air quality levels in the city centre. We carry out several
experiments using the SUMO traffic simulation tool to evaluate our proposal.
\\ ( https://arxiv.org/abs/2401.12329 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12340 (*cross-listing*)
Date: Mon, 22 Jan 2024 20:08:57 GMT   (8883kb,D)

Title: Contrastive Learning and Cycle Consistency-based Transductive Transfer
  Learning for Target Annotation
Authors: Shoaib Meraj Sami, Md Mahedi Hasan, Nasser M. Nasrabadi, Raghuveer Rao
Categories: cs.CV cs.AI cs.LG eess.IV stat.ML
Comments: This Paper is Accepted in IEEE TRANSACTIONS ON AEROSPACE AND
  ELECTRONIC SYSTEMS. This Arxiv version is an older version than the reviewed
  version
DOI: 10.1109/TAES.2023.3337768
\\
  Annotating automatic target recognition (ATR) is a highly challenging task,
primarily due to the unavailability of labeled data in the target domain.
Hence, it is essential to construct an optimal target domain classifier by
utilizing the labeled information of the source domain images. The transductive
transfer learning (TTL) method that incorporates a CycleGAN-based unpaired
domain translation network has been previously proposed in the literature for
effective ATR annotation. Although this method demonstrates great potential for
ATR, it severely suffers from lower annotation performance, higher Fr\'echet
Inception Distance (FID) score, and the presence of visual artifacts in the
synthetic images. To address these issues, we propose a hybrid contrastive
learning base unpaired domain translation (H-CUT) network that achieves a
significantly lower FID score. It incorporates both attention and entropy to
emphasize the domain-specific region, a noisy feature mixup module to generate
high variational synthetic negative patches, and a modulated noise contrastive
estimation (MoNCE) loss to reweight all negative patches using optimal
transport for better performance. Our proposed contrastive learning and
cycle-consistency-based TTL (C3TTL) framework consists of two H-CUT networks
and two classifiers. It simultaneously optimizes cycle-consistency, MoNCE, and
identity losses. In C3TTL, two H-CUT networks have been employed through a
bijection mapping to feed the reconstructed source domain images into a
pretrained classifier to guide the optimal target domain classifier. Extensive
experimental analysis conducted on three ATR datasets demonstrates that the
proposed C3TTL method is effective in annotating civilian and military
vehicles, as well as ship targets.
\\ ( https://arxiv.org/abs/2401.12340 ,  8883kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12344 (*cross-listing*)
Date: Mon, 22 Jan 2024 20:17:14 GMT   (41777kb,D)

Title: OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for
  Generalized and Robust Retinal Disease Detection
Authors: Fatema-E Jannat, Sina Gholami, Minhaj Nur Alam, Hamed Tabkhi
Categories: cs.CV cs.AI cs.LG
Comments: 12 pages, 7 figures, 6 tables
\\
  Despite the revolutionary impact of AI and the development of locally trained
algorithms, achieving widespread generalized learning from multi-modal data in
medical AI remains a significant challenge. This gap hinders the practical
deployment of scalable medical AI solutions. Addressing this challenge, our
research contributes a self-supervised robust machine learning framework,
OCT-SelfNet, for detecting eye diseases using optical coherence tomography
(OCT) images. In this work, various data sets from various institutions are
combined enabling a more comprehensive range of representation. Our method
addresses the issue using a two-phase training approach that combines
self-supervised pretraining and supervised fine-tuning with a mask autoencoder
based on the SwinV2 backbone by providing a solution for real-world clinical
deployment. Extensive experiments on three datasets with different encoder
backbones, low data settings, unseen data settings, and the effect of
augmentation show that our method outperforms the baseline model, Resnet-50 by
consistently attaining AUC-ROC performance surpassing 77% across all tests,
whereas the baseline model exceeds 54%. Moreover, in terms of the AUC-PR
metric, our proposed method exceeded 42%, showcasing a substantial increase of
at least 10% in performance compared to the baseline, which exceeded only 33%.
This contributes to our understanding of our approach's potential and
emphasizes its usefulness in clinical settings.
\\ ( https://arxiv.org/abs/2401.12344 ,  41777kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12392 (*cross-listing*)
Date: Mon, 22 Jan 2024 22:47:02 GMT   (21243kb,D)

Title: Evaluating Roadside Perception for Autonomous Vehicles: Insights from
  Field Testing
Authors: Rusheng Zhang, Depu Meng, Shengyin Shen, Tinghan Wang, Tai Karir,
  Michael Maile, Henry X. Liu
Categories: cs.RO cs.AI
Comments: 6 figures, 8 tables, 14 pages
\\
  Roadside perception systems are increasingly crucial in enhancing traffic
safety and facilitating cooperative driving for autonomous vehicles. Despite
rapid technological advancements, a major challenge persists for this newly
arising field: the absence of standardized evaluation methods and benchmarks
for these systems. This limitation hampers the ability to effectively assess
and compare the performance of different systems, thus constraining progress in
this vital field. This paper introduces a comprehensive evaluation methodology
specifically designed to assess the performance of roadside perception systems.
Our methodology encompasses measurement techniques, metric selection, and
experimental trial design, all grounded in real-world field testing to ensure
the practical applicability of our approach.
  We applied our methodology in Mcity\footnote{\url{https://mcity.umich.edu/}},
a controlled testing environment, to evaluate various off-the-shelf perception
systems. This approach allowed for an in-depth comparative analysis of their
performance in realistic scenarios, offering key insights into their respective
strengths and limitations. The findings of this study are poised to inform the
development of industry-standard benchmarks and evaluation methods, thereby
enhancing the effectiveness of roadside perception system development and
deployment for autonomous vehicles. We anticipate that this paper will
stimulate essential discourse on standardizing evaluation methods for roadside
perception systems, thus pushing the frontiers of this technology. Furthermore,
our results offer both academia and industry a comprehensive understanding of
the capabilities of contemporary infrastructure-based perception systems.
\\ ( https://arxiv.org/abs/2401.12392 ,  21243kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12393 (*cross-listing*)
Date: Mon, 22 Jan 2024 22:50:59 GMT   (4507kb,D)

Title: A Learning-based Declarative Privacy-Preserving Framework for Federated
  Data Management
Authors: Hong Guan, Summer Gautier, Deepti Gupta, Rajan Hari Ambrish, Yancheng
  Wang, Harsha Lakamsani, Dhanush Giriyan, Saajan Maslanka, Chaowei Xiao,
  Yingzhen Yang, Jia Zou
Categories: cs.DB cs.AI
\\
  It is challenging to balance the privacy and accuracy for federated query
processing over multiple private data silos. In this work, we will demonstrate
an end-to-end workflow for automating an emerging privacy-preserving technique
that uses a deep learning model trained using the Differentially-Private
Stochastic Gradient Descent (DP-SGD) algorithm to replace portions of actual
data to answer a query. Our proposed novel declarative privacy-preserving
workflow allows users to specify "what private information to protect" rather
than "how to protect". Under the hood, the system automatically chooses
query-model transformation plans as well as hyper-parameters. At the same time,
the proposed workflow also allows human experts to review and tune the selected
privacy-preserving mechanism for audit/compliance, and optimization purposes.
\\ ( https://arxiv.org/abs/2401.12393 ,  4507kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12421 (*cross-listing*)
Date: Tue, 23 Jan 2024 01:10:25 GMT   (3844kb,D)

Title: AdaEmbed: Semi-supervised Domain Adaptation in the Embedding Space
Authors: Ali Mottaghi, Mohammad Abdullah Jamal, Serena Yeung, Omid Mohareri
Categories: cs.CV cs.AI
\\
  Semi-supervised domain adaptation (SSDA) presents a critical hurdle in
computer vision, especially given the frequent scarcity of labeled data in
real-world settings. This scarcity often causes foundation models, trained on
extensive datasets, to underperform when applied to new domains. AdaEmbed, our
newly proposed methodology for SSDA, offers a promising solution to these
challenges. Leveraging the potential of unlabeled data, AdaEmbed facilitates
the transfer of knowledge from a labeled source domain to an unlabeled target
domain by learning a shared embedding space. By generating accurate and uniform
pseudo-labels based on the established embedding space, the model overcomes the
limitations of conventional SSDA, thus enhancing performance significantly. Our
method's effectiveness is validated through extensive experiments on benchmark
datasets such as DomainNet, Office-Home, and VisDA-C, where AdaEmbed
consistently outperforms all the baselines, setting a new state of the art for
SSDA. With its straightforward implementation and high data efficiency,
AdaEmbed stands out as a robust and pragmatic solution for real-world
scenarios, where labeled data is scarce. To foster further research and
application in this area, we are sharing the codebase of our unified framework
for semi-supervised domain adaptation.
\\ ( https://arxiv.org/abs/2401.12421 ,  3844kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12451 (*cross-listing*)
Date: Tue, 23 Jan 2024 02:30:16 GMT   (505kb,D)

Title: Methods and strategies for improving the novel view synthesis quality of
  neural radiation field
Authors: Shun Fang, Ming Cui, Xing Feng, Yanna Lv
Categories: cs.CV cs.AI
\\
  Neural Radiation Field (NeRF) technology can learn a 3D implicit model of a
scene from 2D images and synthesize realistic novel view images. This
technology has received widespread attention from the industry and has good
application prospects. In response to the problem that the rendering quality of
NeRF images needs to be improved, many researchers have proposed various
methods to improve the rendering quality in the past three years. The latest
relevant papers are classified and reviewed, the technical principles behind
quality improvement are analyzed, and the future evolution direction of quality
improvement methods is discussed. This study can help researchers quickly
understand the current state and evolutionary context of technology in this
field, which is helpful in inspiring the development of more efficient
algorithms and promoting the application of NeRF technology in related fields.
\\ ( https://arxiv.org/abs/2401.12451 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12455 (*cross-listing*)
Date: Tue, 23 Jan 2024 02:52:36 GMT   (4631kb,D)

Title: Multi-agent deep reinforcement learning with centralized training and
  decentralized execution for transportation infrastructure management
Authors: M. Saifullah, K.G. Papakonstantinou, C.P. Andriotis, S.M. Stoffels
Categories: cs.MA cs.AI cs.LG cs.SY eess.SY
\\
  We present a multi-agent Deep Reinforcement Learning (DRL) framework for
managing large transportation infrastructure systems over their life-cycle.
Life-cycle management of such engineering systems is a computationally
intensive task, requiring appropriate sequential inspection and maintenance
decisions able to reduce long-term risks and costs, while dealing with
different uncertainties and constraints that lie in high-dimensional spaces. To
date, static age- or condition-based maintenance methods and risk-based or
periodic inspection plans have mostly addressed this class of optimization
problems. However, optimality, scalability, and uncertainty limitations are
often manifested under such approaches. The optimization problem in this work
is cast in the framework of constrained Partially Observable Markov Decision
Processes (POMDPs), which provides a comprehensive mathematical basis for
stochastic sequential decision settings with observation uncertainties, risk
considerations, and limited resources. To address significantly large state and
action spaces, a Deep Decentralized Multi-agent Actor-Critic (DDMAC) DRL method
with Centralized Training and Decentralized Execution (CTDE), termed as
DDMAC-CTDE is developed. The performance strengths of the DDMAC-CTDE method are
demonstrated in a generally representative and realistic example application of
an existing transportation network in Virginia, USA. The network includes
several bridge and pavement components with nonstationary degradation,
agency-imposed constraints, and traffic delay and risk considerations. Compared
to traditional management policies for transportation networks, the proposed
DDMAC-CTDE method vastly outperforms its counterparts. Overall, the proposed
algorithmic framework provides near optimal solutions for transportation
infrastructure management under real-world constraints and complexities.
\\ ( https://arxiv.org/abs/2401.12455 ,  4631kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12456 (*cross-listing*)
Date: Tue, 23 Jan 2024 02:53:06 GMT   (26kb)

Title: Exploration and Improvement of Nerf-based 3D Scene Editing Techniques
Authors: Shun Fang, Ming Cui, Xing Feng, Yanan Zhang
Categories: cs.CV cs.AI cs.GR
\\
  NeRF's high-quality scene synthesis capability was quickly accepted by
scholars in the years after it was proposed, and significant progress has been
made in 3D scene representation and synthesis. However, the high computational
cost limits intuitive and efficient editing of scenes, making NeRF's
development in the scene editing field facing many challenges. This paper
reviews the preliminary explorations of scholars on NeRF in the scene or object
editing field in recent years, mainly changing the shape and texture of scenes
or objects in new synthesized scenes; through the combination of residual
models such as GaN and Transformer with NeRF, the generalization ability of
NeRF scene editing has been further expanded, including realizing real-time new
perspective editing feedback, multimodal editing of text synthesized 3D scenes,
4D synthesis performance, and in-depth exploration in light and shadow editing,
initially achieving optimization of indirect touch editing and detail
representation in complex scenes. Currently, most NeRF editing methods focus on
the touch points and materials of indirect points, but when dealing with more
complex or larger 3D scenes, it is difficult to balance accuracy, breadth,
efficiency, and quality. Overcoming these challenges may become the direction
of future NeRF 3D scene editing technology.
\\ ( https://arxiv.org/abs/2401.12456 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12513 (*cross-listing*)
Date: Tue, 23 Jan 2024 06:08:00 GMT   (1626kb,D)

Title: Detecting and recognizing characters in Greek papyri with YOLOv8, DeiT
  and SimCLR
Authors: Robert Turnbull and Evelyn Mannix
Categories: cs.CV cs.AI
MSC-class: 68T10
\\
  The capacity to isolate and recognize individual characters from facsimile
images of papyrus manuscripts yields rich opportunities for digital analysis.
For this reason the `ICDAR 2023 Competition on Detection and Recognition of
Greek Letters on Papyri' was held as part of the 17th International Conference
on Document Analysis and Recognition. This paper discusses our submission to
the competition. We used an ensemble of YOLOv8 models to detect and classify
individual characters and employed two different approaches for refining the
character predictions, including a transformer based DeiT approach and a
ResNet-50 model trained on a large corpus of unlabelled data using SimCLR, a
self-supervised learning method. Our submission won the recognition challenge
with a mAP of 42.2%, and was runner-up in the detection challenge with a mean
average precision (mAP) of 51.4%. At the more relaxed intersection over union
threshold of 0.5, we achieved the highest mean average precision and mean
average recall results for both detection and classification. We ran our
prediction pipeline on more than 4,500 images from the Oxyrhynchus Papyri to
illustrate the utility of our approach, and we release the results publicly in
multiple formats.
\\ ( https://arxiv.org/abs/2401.12513 ,  1626kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12554 (*cross-listing*)
Date: Tue, 23 Jan 2024 08:25:12 GMT   (628kb,D)

Title: Can Large Language Models Write Parallel Code?
Authors: Daniel Nichols, Joshua H. Davis, Zhaojun Xie, Arjun Rajaram, Abhinav
  Bhatele
Categories: cs.DC cs.AI
\\
  Large Language Models are becoming an increasingly popular tool for software
development. Their ability to model and generate source code has been
demonstrated in a variety of contexts, including code completion,
summarization, translation, and lookup. However, they often struggle to
generate code for more complex tasks. In this paper, we explore the ability of
state-of-the-art language models to generate parallel code. We propose a
benchmark, PCGBench, consisting of a set of 420 tasks for evaluating the
ability of language models to generate parallel code, and we evaluate the
performance of several state-of-the-art open- and closed-source language models
on these tasks. We introduce novel metrics for comparing parallel code
generation performance and use them to explore how well each LLM performs on
various parallel programming models and computational problem types.
\\ ( https://arxiv.org/abs/2401.12554 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12570 (*cross-listing*)
Date: Tue, 23 Jan 2024 08:59:21 GMT   (3657kb,D)

Title: DiffMoog: a Differentiable Modular Synthesizer for Sound Matching
Authors: Noy Uzrad, Oren Barkan, Almog Elharar, Shlomi Shvartzman, Moshe
  Laufer, Lior Wolf, Noam Koenigstein
Categories: eess.AS cs.AI cs.SD
Comments: 5 pages, 7 figures, 1 table, Our code is released at
  https://github.com/aisynth/diffmoog
\\
  This paper presents DiffMoog - a differentiable modular synthesizer with a
comprehensive set of modules typically found in commercial instruments. Being
differentiable, it allows integration into neural networks, enabling automated
sound matching, to replicate a given audio input. Notably, DiffMoog facilitates
modulation capabilities (FM/AM), low-frequency oscillators (LFOs), filters,
envelope shapers, and the ability for users to create custom signal chains. We
introduce an open-source platform that comprises DiffMoog and an end-to-end
sound matching framework. This framework utilizes a novel signal-chain loss and
an encoder network that self-programs its outputs to predict DiffMoogs
parameters based on the user-defined modular architecture. Moreover, we provide
insights and lessons learned towards sound matching using differentiable
synthesis. Combining robust sound capabilities with a holistic platform,
DiffMoog stands as a premier asset for expediting research in audio synthesis
and machine learning.
\\ ( https://arxiv.org/abs/2401.12570 ,  3657kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12593 (*cross-listing*)
Date: Tue, 23 Jan 2024 09:48:08 GMT   (401kb,D)

Title: MOReGIn: Multi-Objective Recommendation at the Global and Individual
  Levels
Authors: Elizabeth G\'omez, David Contreras, Ludovico Boratto, Maria Salam\'o
Categories: cs.IR cs.AI
\\
  Multi-Objective Recommender Systems (MORSs) emerged as a paradigm to
guarantee multiple (often conflicting) goals. Besides accuracy, a MORS can
operate at the global level, where additional beyond-accuracy goals are met for
the system as a whole, or at the individual level, meaning that the
recommendations are tailored to the needs of each user. The state-of-the-art
MORSs either operate at the global or individual level, without assuming the
co-existence of the two perspectives. In this study, we show that when global
and individual objectives co-exist, MORSs are not able to meet both types of
goals. To overcome this issue, we present an approach that regulates the
recommendation lists so as to guarantee both global and individual
perspectives, while preserving its effectiveness. Specifically, as individual
perspective, we tackle genre calibration and, as global perspective, provider
fairness. We validate our approach on two real-world datasets, publicly
released with this paper.
\\ ( https://arxiv.org/abs/2401.12593 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12632 (*cross-listing*)
Date: Tue, 23 Jan 2024 10:28:33 GMT   (1177kb,D)

Title: Modeling Resilience of Collaborative AI Systems
Authors: Diaeddin Rimawi, Antonio Liotta, Marco Todescato, Barbara Russo
Categories: cs.SE cs.AI cs.RO
Comments: This paper is accepted at the 3rd International Conference on AI
  Engineering - Software Engineering for AI (CAIN 2024), Lisbon, Portugal
\\
  A Collaborative Artificial Intelligence System (CAIS) performs actions in
collaboration with the human to achieve a common goal. CAISs can use a trained
AI model to control human-system interaction, or they can use human interaction
to dynamically learn from humans in an online fashion. In online learning with
human feedback, the AI model evolves by monitoring human interaction through
the system sensors in the learning state, and actuates the autonomous
components of the CAIS based on the learning in the operational state.
Therefore, any disruptive event affecting these sensors may affect the AI
model's ability to make accurate decisions and degrade the CAIS performance.
Consequently, it is of paramount importance for CAIS managers to be able to
automatically track the system performance to understand the resilience of the
CAIS upon such disruptive events. In this paper, we provide a new framework to
model CAIS performance when the system experiences a disruptive event. With our
framework, we introduce a model of performance evolution of CAIS. The model is
equipped with a set of measures that aim to support CAIS managers in the
decision process to achieve the required resilience of the system. We tested
our framework on a real-world case study of a robot collaborating online with
the human, when the system is experiencing a disruptive event. The case study
shows that our framework can be adopted in CAIS and integrated into the online
execution of the CAIS activities.
\\ ( https://arxiv.org/abs/2401.12632 ,  1177kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12646 (*cross-listing*)
Date: Tue, 23 Jan 2024 10:55:54 GMT   (3607kb,D)

Title: Emergent Cooperation under Uncertain Incentive Alignment
Authors: Nicole Orzan, Erman Acar, Davide Grossi, Roxana R\u{a}dulescu
Categories: cs.MA cs.AI cs.GT
\\
  Understanding the emergence of cooperation in systems of computational agents
is crucial for the development of effective cooperative AI. Interaction among
individuals in real-world settings are often sparse and occur within a broad
spectrum of incentives, which often are only partially known. In this work, we
explore how cooperation can arise among reinforcement learning agents in
scenarios characterised by infrequent encounters, and where agents face
uncertainty about the alignment of their incentives with those of others. To do
so, we train the agents under a wide spectrum of environments ranging from
fully competitive, to fully cooperative, to mixed-motives. Under this type of
uncertainty we study the effects of mechanisms, such as reputation and
intrinsic rewards, that have been proposed in the literature to foster
cooperation in mixed-motives environments. Our findings show that uncertainty
substantially lowers the agents' ability to engage in cooperative behaviour,
when that would be the best course of action. In this scenario, the use of
effective reputation mechanisms and intrinsic rewards boosts the agents'
capability to act nearly-optimally in cooperative environments, while greatly
enhancing cooperation in mixed-motive environments as well.
\\ ( https://arxiv.org/abs/2401.12646 ,  3607kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12662 (*cross-listing*)
Date: Tue, 23 Jan 2024 11:14:59 GMT   (306kb,D)

Title: Integrating Human Expertise in Continuous Spaces: A Novel Interactive
  Bayesian Optimization Framework with Preference Expected Improvement
Authors: Nikolaus Feith, Elmar Rueckert
Categories: cs.RO cs.AI cs.HC cs.LG
\\
  Interactive Machine Learning (IML) seeks to integrate human expertise into
machine learning processes. However, most existing algorithms cannot be applied
to Realworld Scenarios because their state spaces and/or action spaces are
limited to discrete values. Furthermore, the interaction of all existing
methods is restricted to deciding between multiple proposals. We therefore
propose a novel framework based on Bayesian Optimization (BO). Interactive
Bayesian Optimization (IBO) enables collaboration between machine learning
algorithms and humans. This framework captures user preferences and provides an
interface for users to shape the strategy by hand. Additionally, we've
incorporated a new acquisition function, Preference Expected Improvement (PEI),
to refine the system's efficiency using a probabilistic model of the user
preferences. Our approach is geared towards ensuring that machines can benefit
from human expertise, aiming for a more aligned and effective learning process.
In the course of this work, we applied our method to simulations and in a real
world task using a Franka Panda robot to show human-robot collaboration.
\\ ( https://arxiv.org/abs/2401.12662 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12665 (*cross-listing*)
Date: Tue, 23 Jan 2024 11:20:03 GMT   (12668kb,D)

Title: ClipSAM: CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation
Authors: Shengze Li, Jianjian Cao, Peng Ye, Yuhan Ding, Chongjun Tu, Tao Chen
Categories: cs.CV cs.AI
Comments: 7 pages,6 figures
\\
  Recently, foundational models such as CLIP and SAM have shown promising
performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However,
either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible
key drawbacks: 1) CLIP primarily focuses on global feature alignment across
different inputs, leading to imprecise segmentation of local anomalous parts;
2) SAM tends to generate numerous redundant masks without proper prompt
constraints, resulting in complex post-processing requirements. In this work,
we innovatively propose a CLIP and SAM collaboration framework called ClipSAM
for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding
capability for anomaly localization and rough segmentation, which is further
used as the prompt constraints for SAM to refine the anomaly segmentation
results. In details, we introduce a crucial Unified Multi-scale Cross-modal
Interaction (UMCI) module for interacting language with visual features at
multiple scales of CLIP to reason anomaly positions. Then, we design a novel
Multi-level Mask Refinement (MMR) module, which utilizes the positional
information as multi-level prompts for SAM to acquire hierarchical levels of
masks and merges them. Extensive experiments validate the effectiveness of our
approach, achieving the optimal segmentation performance on the MVTec-AD and
VisA datasets.
\\ ( https://arxiv.org/abs/2401.12665 ,  12668kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12686 (*cross-listing*)
Date: Tue, 23 Jan 2024 11:52:00 GMT   (1998kb,D)

Title: Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach
Authors: Christian Fabian, Kai Cui, Heinz Koeppl
Categories: cs.MA cs.AI cs.GT cs.LG
Comments: accepted at ICLR 2024
\\
  Learning the behavior of large agent populations is an important task for
numerous research areas. Although the field of multi-agent reinforcement
learning (MARL) has made significant progress towards solving these systems,
solutions for many agents often remain computationally infeasible and lack
theoretical guarantees. Mean Field Games (MFGs) address both of these issues
and can be extended to Graphon MFGs (GMFGs) to include network structures
between agents. Despite their merits, the real world applicability of GMFGs is
limited by the fact that graphons only capture dense graphs. Since most
empirically observed networks show some degree of sparsity, such as power law
graphs, the GMFG framework is insufficient for capturing these network
topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which
builds on the graph theoretical concept of graphexes. Graphexes are the
limiting objects to sparse graph sequences that also have other desirable
features such as the small world property. Learning equilibria in these games
is challenging due to the rich and sparse structure of the underlying graphs.
To tackle these challenges, we design a new learning algorithm tailored to the
GXMFG setup. This hybrid graphex learning approach leverages that the system
mainly consists of a highly connected core and a sparse periphery. After
defining the system and providing a theoretical analysis, we state our learning
approach and demonstrate its learning capabilities on both synthetic graphs and
real-world networks. This comparison shows that our GXMFG learning algorithm
successfully extends MFGs to a highly relevant class of hard, realistic
learning problems that are not accurately addressed by current MARL and MFG
methods.
\\ ( https://arxiv.org/abs/2401.12686 ,  1998kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12714 (*cross-listing*)
Date: Tue, 23 Jan 2024 12:29:42 GMT   (747kb,D)

Title: Evaluation of large language models for assessing code maintainability
Authors: Marc Dillmann, Julien Siebert, Adam Trendowicz
Categories: cs.SE cs.AI
Comments: 14 pages, 4 figures, 8 tables
MSC-class: 68
ACM-class: D.2.7
\\
  Increased availability of open-source software repositories and recent
advances in code analysis using large language models (LLMs) has triggered a
wave of new work to automate software engineering tasks that were previously
very difficult to automate. In this paper, we investigate a recent line of work
that hypothesises that comparing the probability of code generated by LLMs with
the probability the current code would have had can indicate potential quality
problems. We investigate the association between the cross-entropy of code
generated by ten different models (based on GPT2 and Llama2) and the following
quality aspects: readability, understandability, complexity, modularisation,
and overall maintainability assessed by experts and available in an benchmark
dataset. Our results show that, controlling for the number of logical lines of
codes (LLOC), cross-entropy computed by LLMs is indeed a predictor of
maintainability on a class level (the higher the cross-entropy the lower the
maintainability). However, this relation is reversed when one does not control
for LLOC (e.g., comparing small classes with longer ones). Furthermore, while
the complexity of LLMs affects the range of cross-entropy (smaller models tend
to have a wider range of cross-entropy), this plays a significant role in
predicting maintainability aspects. Our study limits itself on ten different
pretrained models (based on GPT2 and Llama2) and on maintainability aspects
collected by Schnappinger et al. When controlling for logical lines of code
(LLOC), cross-entropy is a predictor of maintainability. However, while related
work has shown the potential usefulness of cross-entropy at the level of tokens
or short sequences, at the class level this criterion alone may prove
insufficient to predict maintainability and further research is needed to make
best use of this information in practice.
\\ ( https://arxiv.org/abs/2401.12714 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12771 (*cross-listing*)
Date: Tue, 23 Jan 2024 13:57:50 GMT   (1218kb)

Title: Deep Learning-based Intraoperative MRI Reconstruction
Authors: Jon Andr\'e Ottesen, Tryggve Storas, Svein Are Sirirud Vatnehol,
  Grethe L{\o}vland, Einar O. Vik-Mo, Till Schellhorn, Karoline Skogen,
  Christopher Larsson, Atle Bj{\o}rnerud, Inge Rasmus Groote-Eindbaas, Matthan
  W.A. Caan
Categories: eess.IV cs.AI physics.med-ph
\\
  Purpose: To evaluate the quality of deep learning reconstruction for
prospectively accelerated intraoperative magnetic resonance imaging (iMRI)
during resective brain tumor surgery.
  Materials and Methods: Accelerated iMRI was performed during brain surgery
using dual surface coils positioned around the area of resection. A deep
learning (DL) model was trained on the fastMRI neuro dataset to mimic the data
from the iMRI protocol. Evaluation was performed on imaging material from 40
patients imaged between 01.11.2021 - 01.06.2023 that underwent iMRI during
tumor resection surgery. A comparative analysis was conducted between the
conventional compressed sense (CS) method and the trained DL reconstruction
method. Blinded evaluation of multiple image quality metrics was performed by
two working neuro-radiologists and a working neurosurgeon on a 1 to 5 Likert
scale (1=non diagnostic, 2=poor, 3=acceptable, 4=good, 5=excellent), and the
favored reconstruction variant.
  Results: The DL reconstruction was strongly favored or favored over the CS
reconstruction for 33/40, 39/40, and 8/40 of cases for reader 1, 2, and 3,
respectively. Two of three readers consistently assigned higher ratings for the
DL reconstructions, and the DL reconstructions had a higher score than their
respective CS counterparts for 72%, 72%, and 14% of the cases for reader 1, 2,
and 3, respectively. Still, the DL reconstructions exhibited shortcomings such
as a striping artifact and reduced signal.
  Conclusion: DL shows promise to allow for high-quality reconstructions of
intraoperative MRI with equal to or improved perceived spatial resolution,
signal-to-noise ratio, diagnostic confidence, diagnostic conspicuity, and
spatial resolution compared to compressed sense.
\\ ( https://arxiv.org/abs/2401.12771 ,  1218kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12803 (*cross-listing*)
Date: Fri, 12 Jan 2024 10:44:23 GMT   (18246kb,D)

Title: Enhancements for 5G NR PRACH Reception: An AI/ML Approach
Authors: Rohit Singh, Anil Kumar Yerrapragada, Jeeva Keshav S, Radha Krishna
  Ganti
Categories: cs.IT cs.AI cs.LG eess.SP math.IT
\\
  Random Access is an important step in enabling the initial attachment of a
User Equipment (UE) to a Base Station (gNB). The UE identifies itself by
embedding a Preamble Index (RAPID) in the phase rotation of a known base
sequence, which it transmits on the Physical Random Access Channel (PRACH). The
signal on the PRACH also enables the estimation of propagation delay, often
known as Timing Advance (TA), which is induced by virtue of the UE's position.
Traditional receivers estimate the RAPID and TA using correlation-based
techniques. This paper presents an alternative receiver approach that uses
AI/ML models, wherein two neural networks are proposed, one for the RAPID and
one for the TA. Different from other works, these two models can run in
parallel as opposed to sequentially. Experiments with both simulated data and
over-the-air hardware captures highlight the improved performance of the
proposed AI/ML-based techniques compared to conventional correlation methods.
\\ ( https://arxiv.org/abs/2401.12803 ,  18246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12822 (*cross-listing*)
Date: Tue, 23 Jan 2024 14:55:46 GMT   (6001kb,D)

Title: Deep Learning Based Simulators for the Phosphorus Removal Process
  Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms
Authors: Esmaeel Mohammadi, Mikkel Stokholm-Bjerregaard, Aviaja Anna Hansen,
  Per Halkj{\ae}r Nielsen, Daniel Ortiz-Arroyo, Petar Durdevic
Categories: eess.SY cs.AI cs.LG cs.SY
Comments: Journal Paper
\\
  Phosphorus removal is vital in wastewater treatment to reduce reliance on
limited resources. Deep reinforcement learning (DRL) is a machine learning
technique that can optimize complex and nonlinear systems, including the
processes in wastewater treatment plants, by learning control policies through
trial and error. However, applying DRL to chemical and biological processes is
challenging due to the need for accurate simulators. This study trained six
models to identify the phosphorus removal process and used them to create a
simulator for the DRL environment. Although the models achieved high accuracy
(>97%), uncertainty and incorrect prediction behavior limited their performance
as simulators over longer horizons. Compounding errors in the models'
predictions were identified as one of the causes of this problem. This approach
for improving process control involves creating simulation environments for DRL
algorithms, using data from supervisory control and data acquisition (SCADA)
systems with a sufficient historical horizon without complex system modeling or
parameter estimation.
\\ ( https://arxiv.org/abs/2401.12822 ,  6001kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12835 (*cross-listing*)
Date: Tue, 23 Jan 2024 15:18:20 GMT   (19783kb,D)

Title: SGTR+: End-to-end Scene Graph Generation with Transformer
Authors: Rongjie Li, Songyang Zhang, Xuming He
Categories: cs.CV cs.AI
Comments: Accepted by TPAMI: https://ieeexplore.ieee.org/document/10315230
\\
  Scene Graph Generation (SGG) remains a challenging visual understanding task
due to its compositional property. Most previous works adopt a bottom-up,
two-stage or point-based, one-stage approach, which often suffers from high
time complexity or suboptimal designs. In this work, we propose a novel SGG
method to address the aforementioned issues, formulating the task as a
bipartite graph construction problem. To address the issues above, we create a
transformer-based end-to-end framework to generate the entity and entity-aware
predicate proposal set, and infer directed edges to form relation triplets.
Moreover, we design a graph assembling module to infer the connectivity of the
bipartite scene graph based on our entity-aware structure, enabling us to
generate the scene graph in an end-to-end manner. Based on bipartite graph
assembling paradigm, we further propose a new technical design to address the
efficacy of entity-aware modeling and optimization stability of graph
assembling. Equipped with the enhanced entity-aware design, our method achieves
optimal performance and time-complexity. Extensive experimental results show
that our design is able to achieve the state-of-the-art or comparable
performance on three challenging benchmarks, surpassing most of the existing
approaches and enjoying higher efficiency in inference. Code is available:
https://github.com/Scarecrow0/SGTR
\\ ( https://arxiv.org/abs/2401.12835 ,  19783kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12850 (*cross-listing*)
Date: Tue, 23 Jan 2024 15:35:44 GMT   (8619kb,D)

Title: Overlap-aware End-to-End Supervised Hierarchical Graph Clustering for
  Speaker Diarization
Authors: Prachi Singh, Sriram Ganapathy
Categories: eess.AS cs.AI cs.SD
Comments: 10 pages
\\
  Speaker diarization, the task of segmenting an audio recording based on
speaker identity, constitutes an important speech pre-processing step for
several downstream applications. The conventional approach to diarization
involves multiple steps of embedding extraction and clustering, which are often
optimized in an isolated fashion. While end-to-end diarization systems attempt
to learn a single model for the task, they are often cumbersome to train and
require large supervised datasets. In this paper, we propose an end-to-end
supervised hierarchical clustering algorithm based on graph neural networks
(GNN), called End-to-end Supervised HierARchical Clustering (E-SHARC). The
E-SHARC approach uses front-end mel-filterbank features as input and jointly
learns an embedding extractor and the GNN clustering module, performing
representation learning, metric learning, and clustering with end-to-end
optimization. Further, with additional inputs from an external overlap
detector, the E-SHARC approach is capable of predicting the speakers in the
overlapping speech regions. The experimental evaluation on several benchmark
datasets like AMI, VoxConverse and DISPLACE, illustrates that the proposed
E-SHARC framework improves significantly over the state-of-art diarization
systems.
\\ ( https://arxiv.org/abs/2401.12850 ,  8619kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12851 (*cross-listing*)
Date: Tue, 23 Jan 2024 15:35:50 GMT   (19978kb,D)

Title: Classification of grapevine varieties using UAV hyperspectral imaging
Authors: Alfonso L\'opez, Carlos Javier Ogayar, Francisco Ram\'on Feito,
  Joaquim Jo\~ao Sousa
Categories: cs.CV cs.AI cs.LG
\\
  The classification of different grapevine varieties is a relevant phenotyping
task in Precision Viticulture since it enables estimating the growth of
vineyard rows dedicated to different varieties, among other applications
concerning the wine industry. This task can be performed with destructive
methods that require time-consuming tasks, including data collection and
analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a
more efficient and less prohibitive approach to collecting hyperspectral data,
despite acquiring noisier data. Therefore, the first task is the processing of
these data to correct and downsample large amounts of data. In addition, the
hyperspectral signatures of grape varieties are very similar. In this work, a
Convolutional Neural Network (CNN) is proposed for classifying seventeen
varieties of red and white grape variants. Rather than classifying single
samples, these are processed together with their neighbourhood. Hence, the
extraction of spatial and spectral features is addressed with 1) a spatial
attention layer and 2) Inception blocks. The pipeline goes from processing to
dataset elaboration, finishing with the training phase. The fitted model is
evaluated in terms of response time, accuracy and data separability, and
compared with other state-of-the-art CNNs for classifying hyperspectral data.
Our network was proven to be much more lightweight with a reduced number of
input bands, a lower number of trainable weights and therefore, reduced
training time. Despite this, the evaluated metrics showed much better results
for our network (~99% overall accuracy), in comparison with previous works
barely achieving 81% OA.
\\ ( https://arxiv.org/abs/2401.12851 ,  19978kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12862 (*cross-listing*)
Date: Tue, 23 Jan 2024 15:52:57 GMT   (12033kb,D)

Title: FedRSU: Federated Learning for Scene Flow Estimation on Roadside Units
Authors: Shaoheng Fang, Rui Ye, Wenhao Wang, Zuhong Liu, Yuxiao Wang, Yafei
  Wang, Siheng Chen, Yanfeng Wang
Categories: cs.CV cs.AI
\\
  Roadside unit (RSU) can significantly improve the safety and robustness of
autonomous vehicles through Vehicle-to-Everything (V2X) communication.
Currently, the usage of a single RSU mainly focuses on real-time inference and
V2X collaboration, while neglecting the potential value of the high-quality
data collected by RSU sensors. Integrating the vast amounts of data from
numerous RSUs can provide a rich source of data for model training. However,
the absence of ground truth annotations and the difficulty of transmitting
enormous volumes of data are two inevitable barriers to fully exploiting this
hidden value. In this paper, we introduce FedRSU, an innovative federated
learning framework for self-supervised scene flow estimation. In FedRSU, we
present a recurrent self-supervision training paradigm, where for each RSU, the
scene flow prediction of points at every timestamp can be supervised by its
subsequent future multi-modality observation. Another key component of FedRSU
is federated learning, where multiple devices collaboratively train an ML model
while keeping the training data local and private. With the power of the
recurrent self-supervised learning paradigm, FL is able to leverage innumerable
underutilized data from RSU. To verify the FedRSU framework, we construct a
large-scale multi-modality dataset RSU-SF. The dataset consists of 17 RSU
clients, covering various scenarios, modalities, and sensor settings. Based on
RSU-SF, we show that FedRSU can greatly improve model performance in ITS and
provide a comprehensive benchmark under diverse FL scenarios. To the best of
our knowledge, we provide the first real-world LiDAR-camera multi-modal dataset
and benchmark for the FL community.
\\ ( https://arxiv.org/abs/2401.12862 ,  12033kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12914 (*cross-listing*)
Date: Tue, 23 Jan 2024 17:06:13 GMT   (5193kb,D)

Title: Emergent Communication Protocol Learning for Task Offloading in
  Industrial Internet of Things
Authors: Salwa Mostafa, Mateus P. Mota, Alvaro Valcarce, and Mehdi Bennis
Categories: cs.IT cs.AI cs.MA math.IT
Journal-ref: GLOBECOM 2023
\\
  In this paper, we leverage a multi-agent reinforcement learning (MARL)
framework to jointly learn a computation offloading decision and multichannel
access policy with corresponding signaling. Specifically, the base station and
industrial Internet of Things mobile devices are reinforcement learning agents
that need to cooperate to execute their computation tasks within a deadline
constraint. We adopt an emergent communication protocol learning framework to
solve this problem. The numerical results illustrate the effectiveness of
emergent communication in improving the channel access success rate and the
number of successfully computed tasks compared to contention-based,
contention-free, and no-communication approaches. Moreover, the proposed task
offloading policy outperforms remote and local computation baselines.
\\ ( https://arxiv.org/abs/2401.12914 ,  5193kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12963 (*cross-listing*)
Date: Tue, 23 Jan 2024 18:45:54 GMT   (25601kb,D)

Title: AutoRT: Embodied Foundation Models for Large Scale Orchestration of
  Robotic Agents
Authors: Michael Ahn, Debidatta Dwibedi, Chelsea Finn, Montse Gonzalez Arenas,
  Keerthana Gopalakrishnan, Karol Hausman, Brian Ichter, Alex Irpan, Nikhil
  Joshi, Ryan Julian, Sean Kirmani, Isabel Leal, Edward Lee, Sergey Levine, Yao
  Lu, Isabel Leal, Sharath Maddineni, Kanishka Rao, Dorsa Sadigh, Pannag
  Sanketi, Pierre Sermanet, Quan Vuong, Stefan Welker, Fei Xia, Ted Xiao, Peng
  Xu, Steve Xu, Zhuo Xu
Categories: cs.RO cs.AI cs.CL cs.CV cs.LG
Comments: 26 pages, 9 figures
\\
  Foundation models that incorporate language, vision, and more recently
actions have revolutionized the ability to harness internet scale data to
reason about useful tasks. However, one of the key challenges of training
embodied foundation models is the lack of data grounded in the physical world.
In this paper, we propose AutoRT, a system that leverages existing foundation
models to scale up the deployment of operational robots in completely unseen
scenarios with minimal human supervision. AutoRT leverages vision-language
models (VLMs) for scene understanding and grounding, and further uses large
language models (LLMs) for proposing diverse and novel instructions to be
performed by a fleet of robots. Guiding data collection by tapping into the
knowledge of foundation models enables AutoRT to effectively reason about
autonomy tradeoffs and safety while significantly scaling up data collection
for robot learning. We demonstrate AutoRT proposing instructions to over 20
robots across multiple buildings and collecting 77k real robot episodes via
both teleoperation and autonomous robot policies. We experimentally show that
such "in-the-wild" data collected by AutoRT is significantly more diverse, and
that AutoRT's use of LLMs allows for instruction following data collection
robots that can align to human preferences.
\\ ( https://arxiv.org/abs/2401.12963 ,  25601kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12972 (*cross-listing*)
Date: Tue, 23 Jan 2024 18:58:35 GMT   (2651kb,D)

Title: On the Efficacy of Text-Based Input Modalities for Action Anticipation
Authors: Apoorva Beedu, Karan Samel, Irfan Essa
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  Although the task of anticipating future actions is highly uncertain,
information from additional modalities help to narrow down plausible action
choices. Each modality provides different environmental context for the model
to learn from. While previous multi-modal methods leverage information from
modalities such as video and audio, we primarily explore how text inputs for
actions and objects can also enable more accurate action anticipation.
Therefore, we propose a Multi-modal Anticipative Transformer (MAT), an
attention-based video transformer architecture that jointly learns from
multi-modal features and text captions. We train our model in two-stages, where
the model first learns to predict actions in the video clip by aligning with
captions, and during the second stage, we fine-tune the model to predict future
actions. Compared to existing methods, MAT has the advantage of learning
additional environmental context from two kinds of text inputs: action
descriptions during the pre-training stage, and the text inputs for detected
objects and actions during modality feature fusion. Through extensive
experiments, we evaluate the effectiveness of the pre-training stage, and show
that our model outperforms previous methods on all datasets. In addition, we
examine the impact of object and action information obtained via text and
perform extensive ablations. We evaluate the performance on on three datasets:
EpicKitchens-100, EpicKitchens-55 and EGTEA GAZE+; and show that text
descriptions do indeed aid in more effective action anticipation.
\\ ( https://arxiv.org/abs/2401.12972 ,  2651kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12975 (*cross-listing*)
Date: Tue, 23 Jan 2024 18:59:43 GMT   (10892kb,D)

Title: HAZARD Challenge: Embodied Decision Making in Dynamically Changing
  Environments
Authors: Qinhong Zhou, Sunli Chen, Yisong Wang, Haozhe Xu, Weihua Du, Hongxin
  Zhang, Yilun Du, Joshua B. Tenenbaum, Chuang Gan
Categories: cs.CV cs.AI cs.CL
Comments: ICLR 2024. The first two authors contributed equally to this work
\\
  Recent advances in high-fidelity virtual environments serve as one of the
major driving forces for building intelligent embodied agents to perceive,
reason and interact with the physical world. Typically, these environments
remain unchanged unless agents interact with them. However, in real-world
scenarios, agents might also face dynamically changing environments
characterized by unexpected events and need to rapidly take action accordingly.
To remedy this gap, we propose a new simulated embodied benchmark, called
HAZARD, specifically designed to assess the decision-making abilities of
embodied agents in dynamic situations. HAZARD consists of three unexpected
disaster scenarios, including fire, flood, and wind, and specifically supports
the utilization of large language models (LLMs) to assist common sense
reasoning and decision-making. This benchmark enables us to evaluate autonomous
agents' decision-making capabilities across various pipelines, including
reinforcement learning (RL), rule-based, and search-based methods in
dynamically changing environments. As a first step toward addressing this
challenge using large language models, we further develop an LLM-based agent
and perform an in-depth analysis of its promise and challenge of solving these
challenging tasks. HAZARD is available at https://vis-www.cs.umass.edu/hazard/.
\\ ( https://arxiv.org/abs/2401.12975 ,  10892kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12425 (*cross-listing*)
Date: Tue, 23 Jan 2024 01:25:00 GMT   (15234kb,D)

Title: The Neglected Tails of Vision-Language Models
Authors: Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva
  Ramanan, James Caverlee, Shu Kong
Categories: cs.CV cs.CL cs.LG
Comments: Project Page:
  https://shubhamprshr27.github.io/neglected-tails-of-vlms/
\\
  Vision-language models (VLMs) excel in zero-shot recognition but exhibit
drastically imbalanced performance across visual concepts. For example, CLIP,
despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields
$<$10% on ten concepts (e.g., gyromitra and night snake), presumably, because
these concepts are under-represented in VLMs' imbalanced pretraining data. Yet,
assessing this imbalance is challenging as it is non-trivial to calculate the
frequency of specific concepts within VLMs' large-scale pretraining data. Our
work makes the first attempt to measure the concept frequency by analyzing
pretraining texts. We use off-the-shelf language models to help count relevant
texts that contain synonyms of the given concepts and resolve linguistic
ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit
long-tailed concept distributions, which strongly correlate with per-class
accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and
text-to-image generators, also struggle with the rare concepts identified by
our method. To mitigate VLMs' imbalanced performance in zero-shot recognition,
we propose REtrieval-Augmented Learning REAL. First, instead of prompting VLMs
using the original class names, REAL uses their most frequent synonyms found in
VLMs' pretraining texts. This already outperforms human-engineered and
LLM-generated prompts over nine benchmark datasets, likely because VLMs have
seen more images associated with the frequently used synonyms. Second, REAL
uses all the concept synonyms to retrieve a small, class-balanced set of
pretraining data to train a robust classifier. REAL surpasses the recent
retrieval-augmented solution REACT, using 400x less storage and 10,000x less
training time!
\\ ( https://arxiv.org/abs/2401.12425 ,  15234kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12428 (*cross-listing*)
Date: Tue, 23 Jan 2024 01:33:09 GMT   (2735kb,D)

Title: CIM-MLC: A Multi-level Compilation Stack for Computing-In-Memory
  Accelerators
Authors: Songyun Qu, Shixin Zhao, Bing Li, Yintao He, Xuyi Cai, Lei Zhang, Ying
  Wang
Categories: cs.AR cs.CL
Comments: 16 pages, 22 figures
ACM-class: D.3.4
DOI: 10.1145/3620665.3640359
\\
  In recent years, various computing-in-memory (CIM) processors have been
presented, showing superior performance over traditional architectures. To
unleash the potential of various CIM architectures, such as device precision,
crossbar size, and crossbar number, it is necessary to develop compilation
tools that are fully aware of the CIM architectural details and implementation
diversity. However, due to the lack of architectural support in current popular
open-source compiling stacks, existing CIM designs either manually deploy
networks or build their own compilers, which is time-consuming and
labor-intensive. Although some works expose the specific CIM device programming
interfaces to compilers, they are often bound to a fixed CIM architecture,
lacking the flexibility to support the CIM architectures with different
computing granularity. On the other hand, existing compilation works usually
consider the scheduling of limited operation types (such as crossbar-bound
matrix-vector multiplication). Unlike conventional processors, CIM accelerators
are featured by their diverse architecture, circuit, and device, which cannot
be simply abstracted by a single level if we seek to fully explore the
advantages brought by CIM. Therefore, we propose CIM-MLC, a universal
multi-level compilation framework for general CIM architectures. We first
establish a general hardware abstraction for CIM architectures and computing
modes to represent various CIM accelerators. Based on the proposed abstraction,
CIM-MLC can compile tasks onto a wide range of CIM accelerators having
different devices, architectures, and programming interfaces. More importantly,
compared with existing compilation work, CIM-MLC can explore the mapping and
scheduling strategies across multiple architectural tiers, which form a
tractable yet effective design space, to achieve better scheduling and
instruction generation results.
\\ ( https://arxiv.org/abs/2401.12428 ,  2735kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12540 (*cross-listing*)
Date: Tue, 23 Jan 2024 07:48:58 GMT   (7626kb,D)

Title: DREditor: An Time-efficient Approach for Building a Domain-specific
  Dense Retrieval Model
Authors: Chen Huang, Duanyu Feng, Wenqiang Lei, Jiancheng Lv
Categories: cs.IR cs.CL
Comments: 15 pages, 6 figures, Codes are available at
  https://github.com/huangzichun/DREditor
\\
  Deploying dense retrieval models efficiently is becoming increasingly
important across various industries. This is especially true for enterprise
search services, where customizing search engines to meet the time demands of
different enterprises in different domains is crucial. Motivated by this, we
develop a time-efficient approach called DREditor to edit the matching rule of
an off-the-shelf dense retrieval model to suit a specific domain. This is
achieved by directly calibrating the output embeddings of the model using an
efficient and effective linear mapping. This mapping is powered by an edit
operator that is obtained by solving a specially constructed least squares
problem. Compared to implicit rule modification via long-time finetuning, our
experimental results show that DREditor provides significant advantages on
different domain-specific datasets, dataset sources, retrieval models, and
computing devices. It consistently enhances time efficiency by 100-300 times
while maintaining comparable or even superior retrieval performance. In a
broader context, we take the first step to introduce a novel embedding
calibration approach for the retrieval task, filling the technical blank in the
current field of embedding calibration. This approach also paves the way for
building domain-specific dense retrieval models efficiently and inexpensively.
\\ ( https://arxiv.org/abs/2401.12540 ,  7626kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12798 (*cross-listing*)
Date: Tue, 23 Jan 2024 14:31:12 GMT   (496kb,D)

Title: Gradient Flow of Energy: A General and Efficient Approach for Entity
  Alignment Decoding
Authors: Yuanyi Wang, Haifeng Sun, Jingyu Wang, Qi Qi, Shaoling Sun and Jianxin
  Liao
Categories: cs.IR cs.CL
\\
  Entity alignment (EA), a pivotal process in integrating multi-source
Knowledge Graphs (KGs), seeks to identify equivalent entity pairs across these
graphs. Most existing approaches regard EA as a graph representation learning
task, concentrating on enhancing graph encoders. However, the decoding process
in EA - essential for effective operation and alignment accuracy - has received
limited attention and remains tailored to specific datasets and model
architectures, necessitating both entity and additional explicit relation
embeddings. This specificity limits its applicability, particularly in
GNN-based models. To address this gap, we introduce a novel, generalized, and
efficient decoding approach for EA, relying solely on entity embeddings. Our
method optimizes the decoding process by minimizing Dirichlet energy, leading
to the gradient flow within the graph, to promote graph homophily. The
discretization of the gradient flow produces a fast and scalable approach,
termed Triple Feature Propagation (TFP). TFP innovatively channels gradient
flow through three views: entity-to-entity, entity-to-relation, and
relation-to-entity. This generalized gradient flow enables TFP to harness the
multi-view structural information of KGs. Rigorous experimentation on diverse
real-world datasets demonstrates that our approach significantly enhances
various EA methods. Notably, the approach achieves these advancements with less
than 6 seconds of additional computational time, establishing a new benchmark
in efficiency and adaptability for future EA methods.
\\ ( https://arxiv.org/abs/2401.12798 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2103.12890 (*cross-listing*)
Date: Tue, 23 Mar 2021 23:26:09 GMT   (4380kb,D)

Title: Dual Online Stein Variational Inference for Control and Dynamics
Authors: Lucas Barcelos, Alexander Lambert, Rafael Oliveira, Paulo Borges,
  Byron Boots and Fabio Ramos
Categories: cs.RO cs.LG
Comments: Corresponding author: lucas.barcelos@sydney.edu.au
\\
  Model predictive control (MPC) schemes have a proven track record for
delivering aggressive and robust performance in many challenging control tasks,
coping with nonlinear system dynamics, constraints, and observational noise.
Despite their success, these methods often rely on simple control
distributions, which can limit their performance in highly uncertain and
complex environments. MPC frameworks must be able to accommodate changing
distributions over system parameters, based on the most recent measurements. In
this paper, we devise an implicit variational inference algorithm able to
estimate distributions over model parameters and control inputs on-the-fly. The
method incorporates Stein Variational gradient descent to approximate the
target distributions as a collection of particles, and performs updates based
on a Bayesian formulation. This enables the approximation of complex
multi-modal posterior distributions, typically occurring in challenging and
realistic robot navigation tasks. We demonstrate our approach on both simulated
and real-world experiments requiring real-time execution in the face of
dynamically changing environments.
\\ ( https://arxiv.org/abs/2103.12890 ,  4380kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12196 (*cross-listing*)
Date: Mon, 22 Jan 2024 18:36:29 GMT   (1129kb,D)

Title: Learning Dynamics from Multicellular Graphs with Deep Neural Networks
Authors: Haiqian Yang, Florian Meyer, Shaoxun Huang, Liu Yang, Cristiana Lungu,
  Monilola A. Olayioye, Markus J. Buehler, Ming Guo
Categories: physics.bio-ph cond-mat.soft cs.LG
\\
  The inference of multicellular self-assembly is the central quest of
understanding morphogenesis, including embryos, organoids, tumors, and many
others. However, it has been tremendously difficult to identify structural
features that can indicate multicellular dynamics. Here we propose to harness
the predictive power of graph-based deep neural networks (GNN) to discover
important graph features that can predict dynamics. To demonstrate, we apply a
physically informed GNN (piGNN) to predict the motility of multicellular
collectives from a snapshot of their positions both in experiments and
simulations. We demonstrate that piGNN is capable of navigating through complex
graph features of multicellular living systems, which otherwise can not be
achieved by classical mechanistic models. With increasing amounts of
multicellular data, we propose that collaborative efforts can be made to create
a multicellular data bank (MDB) from which it is possible to construct a large
multicellular graph model (LMGM) for general-purposed predictions of
multicellular organization.
\\ ( https://arxiv.org/abs/2401.12196 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12225 (*cross-listing*)
Date: Fri, 5 Jan 2024 08:40:06 GMT   (1617kb,D)

Title: Multimodal Data Curation via Object Detection and Filter Ensembles
Authors: Tzu-Heng Huang, Changho Shin, Sui Jiet Tay, Dyah Adila, Frederic Sala
Categories: cs.CV cs.LG
Comments: Appeared in the Workshop of Towards the Next Generation of Computer
  Vision Datasets (TNGCV) on ICCV 2023
\\
  We propose an approach for curating multimodal data that we used for our
entry in the 2023 DataComp competition filtering track. Our technique combines
object detection and weak supervision-based ensembling. In the first of two
steps in our approach, we employ an out-of-the-box zero-shot object detection
model to extract granular information and produce a variety of filter designs.
In the second step, we employ weak supervision to ensemble filtering rules.
This approach results in a 4% performance improvement when compared to the
best-performing baseline, producing the top-ranking position in the small scale
track at the time of writing. Furthermore, in the medium scale track, we
achieve a noteworthy 4.2% improvement over the baseline by simply ensembling
existing baselines with weak supervision.
\\ ( https://arxiv.org/abs/2401.12225 ,  1617kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12230 (*cross-listing*)
Date: Wed, 17 Jan 2024 20:34:11 GMT   (891kb,D)

Title: Computing in the Era of Large Generative Models: From Cloud-Native to
  AI-Native
Authors: Yao Lu, Song Bian, Lequn Chen, Yongjun He, Yulong Hui, Matthew Lentz,
  Beibin Li, Fei Liu, Jialin Li, Qi Liu, Rui Liu, Xiaoxuan Liu, Lin Ma, Kexin
  Rong, Jianguo Wang, Yingjun Wu, Yongji Wu, Huanchen Zhang, Minjia Zhang,
  Qizhen Zhang, Tianyi Zhou, Danyang Zhuo
Categories: cs.DC cs.LG
\\
  In this paper, we investigate the intersection of large generative AI models
and cloud-native computing architectures. Recent large models such as ChatGPT,
while revolutionary in their capabilities, face challenges like escalating
costs and demand for high-end GPUs. Drawing analogies between
large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we
describe an AI-native computing paradigm that harnesses the power of both
cloud-native technologies (e.g., multi-tenancy and serverless computing) and
advanced machine learning runtime (e.g., batched LoRA inference). These joint
efforts aim to optimize costs-of-goods-sold (COGS) and improve resource
accessibility. The journey of merging these two domains is just at the
beginning and we hope to stimulate future research and development in this
area.
\\ ( https://arxiv.org/abs/2401.12230 ,  891kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12231 (*cross-listing*)
Date: Thu, 18 Jan 2024 09:59:00 GMT   (1547kb,D)

Title: Disentangled Condensation for Large-scale Graphs
Authors: Zhenbang Xiao, Shunyu Liu, Yu Wang, Tongya Zheng, Mingli Song
Categories: cs.SI cs.LG
Comments: Under Review
\\
  Graph condensation has emerged as an intriguing technique to provide Graph
Neural Networks for large-scale graphs with a more compact yet informative
small graph to save the expensive costs of large-scale graph learning. Despite
the promising results achieved, previous graph condensation methods often
employ an entangled condensation strategy that involves condensing nodes and
edges simultaneously, leading to substantial GPU memory demands. This entangled
strategy has considerably impeded the scalability of graph condensation,
impairing its capability to condense extremely large-scale graphs and produce
condensed graphs with high fidelity. Therefore, this paper presents
Disentangled Condensation for large-scale graphs, abbreviated as DisCo, to
provide scalable graph condensation for graphs of varying sizes. At the heart
of DisCo are two complementary components, namely node and edge condensation
modules, that realize the condensation of nodes and edges in a disentangled
manner. In the node condensation module, we focus on synthesizing condensed
nodes that exhibit a similar node feature distribution to original nodes using
a pre-trained node classification model while incorporating class centroid
alignment and anchor attachment regularizers. After node condensation, in the
edge condensation module, we preserve the topology structure by transferring
the link prediction model of the original graph to the condensed nodes,
generating the corresponding condensed edges. Based on the disentangled
strategy, the proposed DisCo can successfully scale up to the ogbn-papers100M
graph with over 100 million nodes and 1 billion edges with flexible reduction
rates. Extensive experiments on five common datasets further demonstrate that
the proposed DisCo yields results superior to state-of-the-art counterparts by
a significant margin. The source code is available at
https://github.com/BangHonor/DisCo.
\\ ( https://arxiv.org/abs/2401.12231 ,  1547kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12232 (*cross-listing*)
Date: Thu, 18 Jan 2024 23:00:34 GMT   (5115kb)

Title: Machine Learning Modeling Of SiRNA Structure-Potency Relationship With
  Applications Against Sars-Cov-2 Spike Gene
Authors: Damilola Oshunyinka
Categories: q-bio.BM cs.LG
Comments: Master's thesis
\\
  The pharmaceutical Research and development (R&D) process is lengthy and
costly, taking nearly a decade to bring a new drug to the market. However,
advancements in biotechnology, computational methods, and machine learning
algorithms have the potential to revolutionize drug discovery, speeding up the
process and improving patient outcomes. The COVID-19 pandemic has further
accelerated and deepened the recognition of the potential of these techniques,
especially in the areas of drug repurposing and efficacy predictions.
Meanwhile, non-small molecule therapeutic modalities such as cell therapies,
monoclonal antibodies, and RNA interference (RNAi) technology have gained
importance due to their ability to target specific disease pathways and/or
patient populations. In the field of RNAi, many experiments have been carried
out to design and select highly efficient siRNAs. However, the established
patterns for efficient siRNAs are sometimes contradictory and unable to
consistently determine the most potent siRNA molecules against a target mRNA.
Thus, this paper focuses on developing machine learning models based on the
cheminformatics representation of the nucleotide composition (i.e. AUTGC) of
siRNA to predict their potency and aid the selection of the most efficient
siRNAs for further development. The PLS (Partial Least Square) and SVR (Support
Vector Regression) machine learning models built in this work outperformed
previously published models. These models can help in predicting siRNA potency
and aid in selecting the best siRNA molecules for experimental validation and
further clinical development. The study has demonstrated the potential of
AI/machine learning models to help expedite siRNA-based drug discovery
including the discovery of potent siRNAs against SARS-CoV-2.
\\ ( https://arxiv.org/abs/2401.12232 ,  5115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12234 (*cross-listing*)
Date: Fri, 19 Jan 2024 13:51:24 GMT   (791kb,D)

Title: A Lightweight FPGA-based IDS-ECU Architecture for Automotive CAN
Authors: Shashwat Khandelwal, Shreejith Shanker
Categories: cs.AR cs.CR cs.LG cs.SY eess.SY
Comments: 9 pages, 3 figures, 11 tables
Journal-ref: 2022 International Conference on Field-Programmable Technology
  (ICFPT)
DOI: 10.1109/ICFPT56656.2022.9974508
\\
  Recent years have seen an exponential rise in complex software-driven
functionality in vehicles, leading to a rising number of electronic control
units (ECUs), network capabilities, and interfaces. These expanded capabilities
also bring-in new planes of vulnerabilities making intrusion detection and
management a critical capability; however, this can often result in more ECUs
and network elements due to the high computational overheads. In this paper, we
present a consolidated ECU architecture incorporating an Intrusion Detection
System (IDS) for Automotive Controller Area Network (CAN) along with
traditional ECU functionality on an off-the-shelf hybrid FPGA device, with
near-zero overhead for the ECU functionality. We propose two quantised
multi-layer perceptrons (QMLP's) as isolated IDSs for detecting a range of
attack vectors including Denial-of-Service, Fuzzing and Spoofing, which are
accelerated using off-the-shelf deep-learning processing unit (DPU) IP block
from Xilinx, operating fully transparently to the software on the ECU. The
proposed models achieve the state-of-the-art classification accuracy for all
the attacks, while we observed a 15x reduction in power consumption when
compared against the GPU-based implementation of the same models quantised
using Nvidia libraries. We also achieved a 2.3x speed up in per-message
processing latency (at 0.24 ms from the arrival of a CAN message) to meet the
strict end-to-end latency on critical CAN nodes and a 2.6x reduction in power
consumption for inference when compared to the state-of-the-art IDS models on
embedded IDS and loosely coupled IDS accelerators (GPUs) discussed in the
literature.
\\ ( https://arxiv.org/abs/2401.12234 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12237 (*cross-listing*)
Date: Fri, 19 Jan 2024 17:07:05 GMT   (958kb,D)

Title: A distribution-guided Mapper algorithm
Authors: Yuyang Tao and Shufei Ge
Categories: math.AT cs.LG q-bio.QM
\\
  Motivation: The Mapper algorithm is an essential tool to explore shape of
data in topology data analysis. With a dataset as an input, the Mapper
algorithm outputs a graph representing the topological features of the whole
dataset. This graph is often regarded as an approximation of a reeb graph of
data. The classic Mapper algorithm uses fixed interval lengths and overlapping
ratios, which might fail to reveal subtle features of data, especially when the
underlying structure is complex.
  Results: In this work, we introduce a distribution guided Mapper algorithm
named D-Mapper, that utilizes the property of the probability model and data
intrinsic characteristics to generate density guided covers and provides
enhanced topological features. Our proposed algorithm is a probabilistic
model-based approach, which could serve as an alternative to non-prababilistic
ones. Moreover, we introduce a metric accounting for both the quality of
overlap clustering and extended persistence homology to measure the performance
of Mapper type algorithm. Our numerical experiments indicate that the D-Mapper
outperforms the classical Mapper algorithm in various scenarios. We also apply
the D-Mapper to a SARS-COV-2 coronavirus RNA sequences dataset to explore the
topological structure of different virus variants. The results indicate that
the D-Mapper algorithm can reveal both vertical and horizontal evolution
processes of the viruses.
  Availability: Our package is available at
https://github.com/ShufeiGe/D-Mapper.
\\ ( https://arxiv.org/abs/2401.12237 ,  958kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12238 (*cross-listing*)
Date: Fri, 19 Jan 2024 19:01:13 GMT   (419kb,D)

Title: Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound
  Event Localization and Detection in Realistic Rooms
Authors: Iran R. Roman, Christopher Ick, Sivan Ding, Adrian S. Roman, Brian
  McFee, Juan P. Bello
Categories: eess.AS cs.LG cs.SD
Comments: 5 pages, 4 figures, 1 table, to be presented at ICASSP 2024 in Seoul,
  South Korea
\\
  Sound event localization and detection (SELD) is an important task in machine
listening. Major advancements rely on simulated data with sound events in
specific rooms and strong spatio-temporal labels. SELD data is simulated by
convolving spatialy-localized room impulse responses (RIRs) with sound
waveforms to place sound events in a soundscape. However, RIRs require manual
collection in specific rooms. We present SpatialScaper, a library for SELD data
simulation and augmentation. Compared to existing tools, SpatialScaper emulates
virtual rooms via parameters such as size and wall absorption. This allows for
parameterized placement (including movement) of foreground and background sound
sources. SpatialScaper also includes data augmentation pipelines that can be
applied to existing SELD data. As a case study, we use SpatialScaper to add
rooms to the DCASE SELD data. Training a model with our data led to progressive
performance improves as a direct function of acoustic diversity. These results
show that SpatialScaper is valuable to train robust SELD models.
\\ ( https://arxiv.org/abs/2401.12238 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12240 (*cross-listing*)
Date: Fri, 19 Jan 2024 21:19:48 GMT   (608kb,D)

Title: Quantised Neural Network Accelerators for Low-Power IDS in Automotive
  Networks
Authors: Shashwat Khandelwal, Anneliese Walsh, Shanker Shreejith
Categories: cs.CR cs.AR cs.LG cs.SY eess.SY
Comments: 2 pages, 1 figure, 2 tables. arXiv admin note: text overlap with
  arXiv:2401.11030
Journal-ref: 2023 Design, Automation & Test in Europe Conference & Exhibition
  (DATE)
DOI: 10.23919/DATE56975.2023.10137016
\\
  In this paper, we explore low-power custom quantised Multi-Layer Perceptrons
(MLPs) as an Intrusion Detection System (IDS) for automotive controller area
network (CAN). We utilise the FINN framework from AMD/Xilinx to quantise, train
and generate hardware IP of our MLP to detect denial of service (DoS) and
fuzzying attacks on CAN network, using ZCU104 (XCZU7EV) FPGA as our target ECU
architecture with integrated IDS capabilities. Our approach achieves
significant improvements in latency (0.12 ms per-message processing latency)
and inference energy consumption (0.25 mJ per inference) while achieving
similar classification performance as state-of-the-art approaches in the
literature.
\\ ( https://arxiv.org/abs/2401.12240 ,  608kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12242 (*cross-listing*)
Date: Sat, 20 Jan 2024 04:53:35 GMT   (8993kb,D)

Title: BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models
Authors: Zhen Xiang, Fengqing Jiang, Zidi Xiong, Bhaskar Ramasubramanian, Radha
  Poovendran, Bo Li
Categories: cs.CR cs.LG
Comments: Accepted to ICLR2024
\\
  Large language models (LLMs) are shown to benefit from chain-of-thought (COT)
prompting, particularly when tackling tasks that require systematic reasoning
processes. On the other hand, COT prompting also poses new vulnerabilities in
the form of backdoor attacks, wherein the model will output unintended
malicious content under specific backdoor-triggered conditions during
inference. Traditional methods for launching backdoor attacks involve either
contaminating the training dataset with backdoored instances or directly
manipulating the model parameters during deployment. However, these approaches
are not practical for commercial LLMs that typically operate via API access. In
this paper, we propose BadChain, the first backdoor attack against LLMs
employing COT prompting, which does not require access to the training dataset
or model parameters and imposes low computational overhead. BadChain leverages
the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning
step into the sequence of reasoning steps of the model output, thereby altering
the final response when a backdoor trigger exists in the query prompt.
Empirically, we show the effectiveness of BadChain for two COT strategies
across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark
tasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover,
we show that LLMs endowed with stronger reasoning capabilities exhibit higher
susceptibility to BadChain, exemplified by a high average attack success rate
of 97.0% across the six benchmark tasks on GPT-4. Finally, we propose two
defenses based on shuffling and demonstrate their overall ineffectiveness
against BadChain. Therefore, BadChain remains a severe threat to LLMs,
underscoring the urgency for the development of robust and effective future
defenses.
\\ ( https://arxiv.org/abs/2401.12242 ,  8993kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12243 (*cross-listing*)
Date: Sat, 20 Jan 2024 07:12:57 GMT   (5200kb,D)

Title: Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming
  for Policy Optimization in Mixed Discrete-Continuous MDPs
Authors: Michael Gimelfarb, Ayal Taitler, Scott Sanner
Categories: math.OC cs.LG cs.RO cs.SC cs.SY eess.SY
\\
  We propose Constraint-Generation Policy Optimization (CGPO) for optimizing
policy parameters within compact and interpretable policy classes for mixed
discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able
to provide bounded policy error guarantees over an infinite range of initial
states for many DC-MDPs with expressive nonlinear dynamics, but it can also
provably derive optimal policies in cases where it terminates with zero error.
Furthermore, CGPO can generate worst-case state trajectories to diagnose policy
deficiencies and provide counterfactual explanations of optimal actions. To
achieve such results, CGPO proposes a bi-level mixed-integer nonlinear
optimization framework for optimizing policies within defined expressivity
classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint
generation methodology that adversarially generates worst-case state
trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can
obtain solutions with bounded optimality gap guarantees. We handle stochastic
transitions through explicit marginalization (where applicable) or
chance-constraints, providing high-probability policy performance guarantees.
We also present a road-map for understanding the computational complexities
associated with different expressivity classes of policy, reward, and
transition dynamics. We experimentally demonstrate the applicability of CGPO in
diverse domains, including inventory control, management of a system of water
reservoirs, and physics control. In summary, we provide a solution for deriving
structured, compact, and explainable policies with bounded performance
guarantees, enabling worst-case scenario generation and counterfactual policy
diagnostics.
\\ ( https://arxiv.org/abs/2401.12243 ,  5200kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12253 (*cross-listing*)
Date: Sat, 20 Jan 2024 21:23:09 GMT   (381kb,D)

Title: Accelerating Sinkhorn Algorithm with Sparse Newton Iterations
Authors: Xun Tang, Michael Shavlovsky, Holakou Rahmanian, Elisa Tardini, Kiran
  Koshy Thekumparampil, Tesi Xiao, Lexing Ying
Categories: math.OC cs.LG stat.ML
Comments: In ICLR 2024
\\
  Computing the optimal transport distance between statistical distributions is
a fundamental task in machine learning. One remarkable recent advancement is
entropic regularization and the Sinkhorn algorithm, which utilizes only matrix
scaling and guarantees an approximated solution with near-linear runtime.
Despite the success of the Sinkhorn algorithm, its runtime may still be slow
due to the potentially large number of iterations needed for convergence. To
achieve possibly super-exponential convergence, we present
Sinkhorn-Newton-Sparse (SNS), an extension to the Sinkhorn algorithm, by
introducing early stopping for the matrix scaling steps and a second stage
featuring a Newton-type subroutine. Adopting the variational viewpoint that the
Sinkhorn algorithm maximizes a concave Lyapunov potential, we offer the insight
that the Hessian matrix of the potential function is approximately sparse.
Sparsification of the Hessian results in a fast $O(n^2)$ per-iteration
complexity, the same as the Sinkhorn algorithm. In terms of total iteration
count, we observe that the SNS algorithm converges orders of magnitude faster
across a wide range of practical cases, including optimal transportation
between empirical distributions and calculating the Wasserstein $W_1, W_2$
distance of discretized densities. The empirical performance is corroborated by
a rigorous bound on the approximate sparsity of the Hessian matrix.
\\ ( https://arxiv.org/abs/2401.12253 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12262 (*cross-listing*)
Date: Mon, 22 Jan 2024 05:49:41 GMT   (1935kb,D)

Title: Machine learning-based network intrusion detection for big and
  imbalanced data using oversampling, stacking feature embedding and feature
  extraction
Authors: Md. Alamin Talukder, Md. Manowarul Islam, Md Ashraf Uddin, Khondokar
  Fida Hasan, Selina Sharmin, Salem A. Alyami and Mohammad Ali Moni
Categories: cs.CR cs.LG
Comments: Accepted in Journal of Big Data (Q1, IF: 8.1, SCIE) on Jan 19, 2024
\\
  Cybersecurity has emerged as a critical global concern. Intrusion Detection
Systems (IDS) play a critical role in protecting interconnected networks by
detecting malicious actors and activities. Machine Learning (ML)-based behavior
analysis within the IDS has considerable potential for detecting dynamic cyber
threats, identifying abnormalities, and identifying malicious conduct within
the network. However, as the number of data grows, dimension reduction becomes
an increasingly difficult task when training ML models. Addressing this, our
paper introduces a novel ML-based network intrusion detection model that uses
Random Oversampling (RO) to address data imbalance and Stacking Feature
Embedding based on clustering results, as well as Principal Component Analysis
(PCA) for dimension reduction and is specifically designed for large and
imbalanced datasets. This model's performance is carefully evaluated using
three cutting-edge benchmark datasets: UNSW-NB15, CIC-IDS-2017, and
CIC-IDS-2018. On the UNSW-NB15 dataset, our trials show that the RF and ET
models achieve accuracy rates of 99.59% and 99.95%, respectively. Furthermore,
using the CIC-IDS2017 dataset, DT, RF, and ET models reach 99.99% accuracy,
while DT and RF models obtain 99.94% accuracy on CIC-IDS2018. These performance
results continuously outperform the state-of-art, indicating significant
progress in the field of network intrusion detection. This achievement
demonstrates the efficacy of the suggested methodology, which can be used
practically to accurately monitor and identify network traffic intrusions,
thereby blocking possible threats.
\\ ( https://arxiv.org/abs/2401.12262 ,  1935kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12272 (*cross-listing*)
Date: Mon, 22 Jan 2024 16:24:04 GMT   (116kb,D)

Title: Transfer Learning for Nonparametric Regression: Non-asymptotic Minimax
  Analysis and Adaptive Procedure
Authors: T. Tony Cai and Hongming Pu
Categories: stat.ML cs.LG
\\
  Transfer learning for nonparametric regression is considered. We first study
the non-asymptotic minimax risk for this problem and develop a novel estimator
called the confidence thresholding estimator, which is shown to achieve the
minimax optimal risk up to a logarithmic factor. Our results demonstrate two
unique phenomena in transfer learning: auto-smoothing and super-acceleration,
which differentiate it from nonparametric regression in a traditional setting.
We then propose a data-driven algorithm that adaptively achieves the minimax
risk up to a logarithmic factor across a wide range of parameter spaces.
Simulation studies are conducted to evaluate the numerical performance of the
adaptive transfer learning algorithm, and a real-world example is provided to
demonstrate the benefits of the proposed method.
\\ ( https://arxiv.org/abs/2401.12272 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12350 (*cross-listing*)
Date: Mon, 22 Jan 2024 20:32:31 GMT   (1474kb,D)

Title: Scaling Up Quantization-Aware Neural Architecture Search for Efficient
  Deep Learning on the Edge
Authors: Yao Lu, Hiram Rayo Torres Rodriguez, Sebastian Vogel, Nick van de
  Waterlaat, Pavol Jancura
Categories: cs.CV cs.LG
Comments: Accepted at Workshop on Compilers, Deployment, and Tooling for Edge
  AI (CODAI '23 ), September 21, 2023, Hamburg, Germany
DOI: 10.1145/3615338.3618122
\\
  Neural Architecture Search (NAS) has become the de-facto approach for
designing accurate and efficient networks for edge devices. Since models are
typically quantized for edge deployment, recent work has investigated
quantization-aware NAS (QA-NAS) to search for highly accurate and efficient
quantized models. However, existing QA-NAS approaches, particularly few-bit
mixed-precision (FB-MP) methods, do not scale to larger tasks. Consequently,
QA-NAS has mostly been limited to low-scale tasks and tiny networks. In this
work, we present an approach to enable QA-NAS (INT8 and FB-MP) on large-scale
tasks by leveraging the block-wise formulation introduced by block-wise NAS. We
demonstrate strong results for the semantic segmentation task on the Cityscapes
dataset, finding FB-MP models 33% smaller and INT8 models 17.6% faster than
DeepLabV3 (INT8) without compromising task performance.
\\ ( https://arxiv.org/abs/2401.12350 ,  1474kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12362 (*cross-listing*)
Date: Mon, 22 Jan 2024 21:11:22 GMT   (881kb,D)

Title: VC dimension of Graph Neural Networks with Pfaffian activation functions
Authors: Giuseppe Alessio D'Inverno, Monica Bianchini, Franco Scarselli
Categories: stat.ML cs.LG
Comments: 37 pages, 9 figures
\\
  Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool
to learn tasks across a wide range of graph domains in a data-driven fashion;
based on a message passing mechanism, GNNs have gained increasing popularity
due to their intuitive formulation, closely linked with the Weisfeiler-Lehman
(WL) test for graph isomorphism, to which they have proven equivalent. From a
theoretical point of view, GNNs have been shown to be universal approximators,
and their generalization capability (namely, bounds on the Vapnik Chervonekis
(VC) dimension) has recently been investigated for GNNs with piecewise
polynomial activation functions. The aim of our work is to extend this analysis
on the VC dimension of GNNs to other commonly used activation functions, such
as sigmoid and hyperbolic tangent, using the framework of Pfaffian function
theory. Bounds are provided with respect to architecture parameters (depth,
number of neurons, input size) as well as with respect to the number of colors
resulting from the 1-WL test applied on the graph domain. The theoretical
analysis is supported by a preliminary experimental study.
\\ ( https://arxiv.org/abs/2401.12362 ,  881kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12424 (*cross-listing*)
Date: Tue, 23 Jan 2024 01:20:15 GMT   (461kb,D)

Title: DALex: Lexicase-like Selection via Diverse Aggregation
Authors: Andrew Ni, Li Ding, Lee Spector
Categories: cs.NE cs.LG
Comments: 15 pages, 4 figures. Submitted to EuroGP'24
\\
  Lexicase selection has been shown to provide advantages over other selection
algorithms in several areas of evolutionary computation and machine learning.
In its standard form, lexicase selection filters a population or other
collection based on randomly ordered training cases that are considered one at
a time. This iterated filtering process can be time-consuming, particularly in
settings with large numbers of training cases. In this paper, we propose a new
method that is nearly equivalent to lexicase selection in terms of the
individuals that it selects, but which does so significantly more quickly. The
new method, called DALex (for Diversely Aggregated Lexicase), selects the best
individual with respect to a weighted sum of training case errors, where the
weights are randomly sampled. This allows us to formulate the core computation
required for selection as matrix multiplication instead of recursive loops of
comparisons, which in turn allows us to take advantage of optimized and
parallel algorithms designed for matrix multiplication for speedup.
Furthermore, we show that we can interpolate between the behavior of lexicase
selection and its "relaxed" variants, such as epsilon or batch lexicase
selection, by adjusting a single hyperparameter, named "particularity
pressure," which represents the importance granted to each individual training
case. Results on program synthesis, deep learning, symbolic regression, and
learning classifier systems demonstrate that DALex achieves significant
speedups over lexicase selection and its relaxed variants while maintaining
almost identical problem-solving performance. Under a fixed computational
budget, these savings free up resources that can be directed towards increasing
population size or the number of generations, enabling the potential for
solving more difficult problems.
\\ ( https://arxiv.org/abs/2401.12424 ,  461kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12438 (*cross-listing*)
Date: Tue, 23 Jan 2024 02:14:05 GMT   (1894kb,D)

Title: Secure Federated Learning Approaches to Diagnosing COVID-19
Authors: Rittika Adhikari, Christopher Settles
Categories: eess.IV cs.CV cs.DC cs.LG
\\
  The recent pandemic has underscored the importance of accurately diagnosing
COVID-19 in hospital settings. A major challenge in this regard is
differentiating COVID-19 from other respiratory illnesses based on chest
X-rays, compounded by the restrictions of HIPAA compliance which limit the
comparison of patient X-rays. This paper introduces a HIPAA-compliant model to
aid in the diagnosis of COVID-19, utilizing federated learning. Federated
learning is a distributed machine learning approach that allows for algorithm
training across multiple decentralized devices using local data samples,
without the need for data sharing. Our model advances previous efforts in chest
X-ray diagnostic models. We examined leading models from established
competitions in this domain and developed our own models tailored to be
effective with specific hospital data. Considering the model's operation in a
federated learning context, we explored the potential impact of biased data
updates on the model's performance. To enhance hospital understanding of the
model's decision-making process and to verify that the model is not focusing on
irrelevant features, we employed a visualization technique that highlights key
features in chest X-rays indicative of a positive COVID-19 diagnosis.
\\ ( https://arxiv.org/abs/2401.12438 ,  1894kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12440 (*cross-listing*)
Date: Tue, 23 Jan 2024 02:19:31 GMT   (20kb)

Title: Post-Training Embedding Alignment for Decoupling Enrollment and Runtime
  Speaker Recognition Models
Authors: Chenyang Gao, Brecht Desplanques, Chelsea J.-T. Ju, Aman Chadha,
  Andreas Stolcke
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to ICASSP 2024
\\
  Automated speaker identification (SID) is a crucial step for the
personalization of a wide range of speech-enabled services. Typical SID systems
use a symmetric enrollment-verification framework with a single model to derive
embeddings both offline for voice profiles extracted from enrollment
utterances, and online from runtime utterances. Due to the distinct
circumstances of enrollment and runtime, such as different computation and
latency constraints, several applications would benefit from an asymmetric
enrollment-verification framework that uses different models for enrollment and
runtime embedding generation. To support this asymmetric SID where each of the
two models can be updated independently, we propose using a lightweight neural
network to map the embeddings from the two independent models to a shared
speaker embedding space. Our results show that this approach significantly
outperforms cosine scoring in a shared speaker logit space for models that were
trained with a contrastive loss on large datasets with many speaker identities.
This proposed Neural Embedding Speaker Space Alignment (NESSA) combined with an
asymmetric update of only one of the models delivers at least 60% of the
performance gain achieved by updating both models in the standard symmetric SID
approach.
\\ ( https://arxiv.org/abs/2401.12440 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12476 (*cross-listing*)
Date: Tue, 23 Jan 2024 04:05:26 GMT   (19507kb,D)

Title: Bayesian identification of nonseparable Hamiltonians with multiplicative
  noise using deep learning and reduced-order modeling
Authors: Nicholas Galioto, Harsh Sharma, Boris Kramer, Alex Arkady Gorodetsky
Categories: stat.ML cs.LG math.DS physics.data-an stat.CO
\\
  This paper presents a structure-preserving Bayesian approach for learning
nonseparable Hamiltonian systems using stochastic dynamic models allowing for
statistically-dependent, vector-valued additive and multiplicative measurement
noise. The approach is comprised of three main facets. First, we derive a
Gaussian filter for a statistically-dependent, vector-valued, additive and
multiplicative noise model that is needed to evaluate the likelihood within the
Bayesian posterior. Second, we develop a novel algorithm for cost-effective
application of Bayesian system identification to high-dimensional systems.
Third, we demonstrate how structure-preserving methods can be incorporated into
the proposed framework, using nonseparable Hamiltonians as an illustrative
system class. We compare the Bayesian method to a state-of-the-art machine
learning method on a canonical nonseparable Hamiltonian model and a chaotic
double pendulum model with small, noisy training datasets. The results show
that using the Bayesian posterior as a training objective can yield upwards of
724 times improvement in Hamiltonian mean squared error using training data
with up to 10% multiplicative noise compared to a standard training objective.
Lastly, we demonstrate the utility of the novel algorithm for parameter
estimation of a 64-dimensional model of the spatially-discretized nonlinear
Schr\"odinger equation with data corrupted by up to 20% multiplicative noise.
\\ ( https://arxiv.org/abs/2401.12476 ,  19507kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12496 (*cross-listing*)
Date: Tue, 23 Jan 2024 05:37:32 GMT   (14934kb,D)

Title: DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity
Authors: Kang-Won Lee, Yuzhe Qin, Xiaolong Wang and Soo-Chul Lim
Categories: cs.RO cs.LG
Comments: Project page: https://lee-kangwon.github.io/dextouch/
\\
  The sense of touch is an essential ability for skillfully performing a
variety of tasks, providing the capacity to search and manipulate objects
without relying on visual information. Extensive research has been conducted
over time to apply these human tactile abilities to robots. In this paper, we
introduce a multi-finger robot system designed to search for and manipulate
objects using the sense of touch without relying on visual information.
Randomly located target objects are searched using tactile sensors, and the
objects are manipulated for tasks that mimic daily-life. The objective of the
study is to endow robots with human-like tactile capabilities. To achieve this,
binary tactile sensors are implemented on one side of the robot hand to
minimize the Sim2Real gap. Training the policy through reinforcement learning
in simulation and transferring the trained policy to the real environment, we
demonstrate that object search and manipulation using tactile sensors is
possible even in an environment without vision information. In addition, an
ablation study was conducted to analyze the effect of tactile information on
manipulative tasks. Our project page is available at
https://lee-kangwon.github.io/dextouch/
\\ ( https://arxiv.org/abs/2401.12496 ,  14934kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12509 (*cross-listing*)
Date: Tue, 23 Jan 2024 06:02:03 GMT   (1611kb)

Title: Digital cloning of online social networks for language-sensitive
  agent-based modeling of misinformation spread
Authors: Prateek Puri, Gabriel Hassler, Anton Shenk, Sai Katragadda
Categories: cs.SI cs.LG
\\
  We develop a simulation framework for studying misinformation spread within
online social networks that blends agent-based modeling and natural language
processing techniques. While many other agent-based simulations exist in this
space, their ability to provide actionable insights in in part limited by their
lack of fidelity and generalizability to existing networks. To partially
address these concerns, we create a 'digital clone' of a known misinformation
sharing network by downloading social media histories for over ten thousand of
its users. We parse these histories to both extract the structure of the
network and model the nuanced ways in which information is shared and spread
among its members. Unlike many other agent-based methods in this space,
information sharing between users in our framework is sensitive to topic of
discussion, user preferences, and online community dynamics. To evaluate the
fidelity of our method, we seed our cloned network with a set of posts recorded
in the base network and compare propagation dynamics between the two, observing
reasonable agreement across the twin networks over a variety of metrics.
Lastly, we explore how the cloned network may serve as a flexible, low-cost
testbed for misinformation countermeasure evaluation and red teaming analysis.
We hope the tools explored here augment existing efforts in the space and
unlock new opportunities for misinformation countermeasure evaluation, a field
that may become increasingly important to consider with the anticipated rise of
misinformation campaigns fueled by generative artificial intelligence.
\\ ( https://arxiv.org/abs/2401.12509 ,  1611kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12609 (*cross-listing*)
Date: Tue, 23 Jan 2024 10:07:41 GMT   (3667kb,D)

Title: Fast Semi-supervised Unmixing using Non-convex Optimization
Authors: Behnood Rasti, Alexandre Zouaoui, Julien Mairal, Jocelyn Chanussot
Categories: cs.CV cs.LG eess.IV
\\
  In this paper, we introduce a novel linear model tailored for
semisupervised/library-based unmixing. Our model incorporates considerations
for library mismatch while enabling the enforcement of the abundance sum-to-one
constraint (ASC). Unlike conventional sparse unmixing methods, this model
involves nonconvex optimization, presenting significant computational
challenges. We demonstrate the efficacy of Alternating Methods of Multipliers
(ADMM) in cyclically solving these intricate problems. We propose two
semisupervised unmixing approaches, each relying on distinct priors applied to
the new model in addition to the ASC: sparsity prior and convexity constraint.
Our experimental results validate that enforcing the convexity constraint
outperforms the sparsity prior for the endmember library. These results are
corroborated across three simulated datasets (accounting for spectral
variability and varying pixel purity levels) and the Cuprite dataset.
Additionally, our comparison with conventional sparse unmixing methods
showcases considerable advantages of our proposed model, which entails
nonconvex optimization. Notably, our implementations of the proposed
algorithms-fast semisupervised unmixing (FaSUn) and sparse unmixing using
soft-shrinkage (SUnS)-prove considerably more efficient than traditional sparse
unmixing methods. SUnS and FaSUn were implemented using PyTorch and provided in
a dedicated Python package called Fast Semisupervised Unmixing (FUnmix), which
is open-source and available at https://github.com/BehnoodRasti/FUnmix
\\ ( https://arxiv.org/abs/2401.12609 ,  3667kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12627 (*cross-listing*)
Date: Tue, 23 Jan 2024 10:26:15 GMT   (183kb)

Title: Blind Channel Estimation and Joint Symbol Detection with Data-Driven
  Factor Graphs
Authors: Luca Schmid, Tomer Raviv, Nir Shlezinger, Laurent Schmalen
Categories: cs.IT cs.LG eess.SP math.IT
Comments: Submitted to IEEE for peer review
\\
  We investigate the application of the factor graph framework for blind joint
channel estimation and symbol detection on time-variant linear inter-symbol
interference channels. In particular, we consider the expectation maximization
(EM) algorithm for maximum likelihood estimation, which typically suffers from
high complexity as it requires the computation of the symbol-wise posterior
distributions in every iteration. We address this issue by efficiently
approximating the posteriors using the belief propagation (BP) algorithm on a
suitable factor graph. By interweaving the iterations of BP and EM, the
detection complexity can be further reduced to a single BP iteration per EM
step. In addition, we propose a data-driven version of our algorithm that
introduces momentum in the BP updates and learns a suitable EM parameter update
schedule, thereby significantly improving the performance-complexity tradeoff
with a few offline training samples. Our numerical experiments demonstrate the
excellent performance of the proposed blind detector and show that it even
outperforms coherent BP detection in high signal-to-noise scenarios.
\\ ( https://arxiv.org/abs/2401.12627 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12630 (*cross-listing*)
Date: Tue, 23 Jan 2024 10:27:38 GMT   (588kb,D)

Title: Full-Stack Optimization for CAM-Only DNN Inference
Authors: Jo\~ao Paulo C. de Lima, Asif Ali Khan, Luigi Carro and Jeronimo
  Castrillon
Categories: cs.AR cs.ET cs.LG
Comments: To be presented at DATE24
\\
  The accuracy of neural networks has greatly improved across various domains
over the past years. Their ever-increasing complexity, however, leads to
prohibitively high energy demands and latency in von Neumann systems. Several
computing-in-memory (CIM) systems have recently been proposed to overcome this,
but trade-offs involving accuracy, hardware reliability, and scalability for
large models remain a challenge. Additionally, for some CIM designs, the
activation movement still requires considerable time and energy. This paper
explores the combination of algorithmic optimizations for ternary weight neural
networks and associative processors (APs) implemented using racetrack memory
(RTM). We propose a novel compilation flow to optimize convolutions on APs by
reducing their arithmetic intensity. By leveraging the benefits of RTM-based
APs, this approach substantially reduces data transfers within the memory while
addressing accuracy, energy efficiency, and reliability concerns. Concretely,
our solution improves the energy efficiency of ResNet-18 inference on ImageNet
by 7.5x compared to crossbar in-memory accelerators while retaining software
accuracy.
\\ ( https://arxiv.org/abs/2401.12630 ,  588kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12645 (*cross-listing*)
Date: Tue, 23 Jan 2024 10:55:29 GMT   (458kb)

Title: On the Robustness of Deep Learning-aided Symbol Detectors to Varying
  Conditions and Imperfect Channel Knowledge
Authors: Chin-Hung Chen, Boris Karanov, Wim van Houtum, Wu Yan, Alex Young,
  Alex Alvarado
Categories: cs.IT cs.LG eess.SP math.IT
Comments: Accepted paper at IEEE Wireless Communications and Networking
  Conference (WCNC) 2024
\\
  Recently, a data-driven Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm tailored to
channels with intersymbol interference has been introduced. This so-called
BCJRNet algorithm utilizes neural networks to calculate channel likelihoods.
BCJRNet has demonstrated resilience against inaccurate channel tap estimations
when applied to a time-invariant channel with ideal exponential decay profiles.
However, its generalization capabilities for practically-relevant time-varying
channels, where the receiver can only access incorrect channel parameters,
remain largely unexplored. The primary contribution of this paper is to expand
upon the results from existing literature to encompass a variety of imperfect
channel knowledge cases that appear in real-world transmissions. Our findings
demonstrate that BCJRNet significantly outperforms the conventional BCJR
algorithm for stationary transmission scenarios when learning from noisy
channel data and with imperfect channel decay profiles. However, this advantage
is shown to diminish when the operating channel is also rapidly time-varying.
Our results also show the importance of memory assumptions for conventional
BCJR and BCJRNet. An underestimation of the memory largely degrades the
performance of both BCJR and BCJRNet, especially in a slow-decaying channel. To
mimic a situation closer to a practical scenario, we also combined channel tap
uncertainty with imperfect channel memory knowledge. Somewhat surprisingly, our
results revealed improved performance when employing the conventional BCJR with
an underestimated memory assumption. BCJRNet, on the other hand, showed a
consistent performance improvement as the level of accurate memory knowledge
increased.
\\ ( https://arxiv.org/abs/2401.12645 ,  458kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12667 (*cross-listing*)
Date: Tue, 23 Jan 2024 11:22:03 GMT   (469kb)

Title: Feature Selection via Robust Weighted Score for High Dimensional Binary
  Class-Imbalanced Gene Expression Data
Authors: Zardad Khan, Amjad Ali, Saeed Aldahmani
Categories: stat.ML cs.LG
Comments: 25 pages
MSC-class: 14J60
\\
  In this paper, a robust weighted score for unbalanced data (ROWSU) is
proposed for selecting the most discriminative feature for high dimensional
gene expression binary classification with class-imbalance problem. The method
addresses one of the most challenging problems of highly skewed class
distributions in gene expression datasets that adversely affect the performance
of classification algorithms. First, the training dataset is balanced by
synthetically generating data points from minority class observations. Second,
a minimum subset of genes is selected using a greedy search approach. Third, a
novel weighted robust score, where the weights are computed by support vectors,
is introduced to obtain a refined set of genes. The highest-scoring genes based
on this approach are combined with the minimum subset of genes selected by the
greedy search approach to form the final set of genes. The novel method ensures
the selection of the most discriminative genes, even in the presence of skewed
class distribution, thus improving the performance of the classifiers. The
performance of the proposed ROWSU method is evaluated on $6$ gene expression
datasets. Classification accuracy and sensitivity are used as performance
metrics to compare the proposed ROWSU algorithm with several other
state-of-the-art methods. Boxplots and stability plots are also constructed for
a better understanding of the results. The results show that the proposed
method outperforms the existing feature selection procedures based on
classification performance from k nearest neighbours (kNN) and random forest
(RF) classifiers.
\\ ( https://arxiv.org/abs/2401.12667 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12687 (*cross-listing*)
Date: Tue, 23 Jan 2024 11:52:25 GMT   (117kb,D)

Title: DVL Calibration using Data-driven Methods
Authors: Zeev Yampolsky and Itzik Klein
Categories: cs.RO cs.LG
Comments: 5 pages , 3 figures , 5 tables
\\
  Autonomous underwater vehicles (AUVs) are used in a wide range of underwater
applications, ranging from seafloor mapping to industrial operations. While
underwater, the AUV navigation solution commonly relies on the fusion between
inertial sensors and Doppler velocity logs (DVL). To achieve accurate DVL
measurements a calibration procedure should be conducted before the mission
begins. Model-based calibration approaches include filtering approaches
utilizing global navigation satellite system signals. In this paper, we propose
an end-to-end deep-learning framework for the calibration procedure. Using
stimulative data, we show that our proposed approach outperforms model-based
approaches by 35% in accuracy and 80% in the required calibration time.
\\ ( https://arxiv.org/abs/2401.12687 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12717 (*cross-listing*)
Date: Tue, 23 Jan 2024 12:39:15 GMT   (633kb)

Title: Gas trap prediction from 3D seismic and well test data using machine
  learning
Authors: Dmitry Ivlev
Categories: physics.geo-ph cs.LG
Comments: 11 pages, 3 figures
\\
  The aim of this work is to create and apply a methodological approach for
predicting gas traps from 3D seismic data and gas well testing. The paper
formalizes the approach to creating a training dataset by selecting volumes
with established gas saturation and filtration properties within the seismic
wavefield. The training dataset thus created is used in a process stack of
sequential application of data processing methods and ensemble machine learning
algorithms. As a result, a cube of calibrated probabilities of belonging of the
study space to gas reservoirs was obtained. The high efficiency of this
approach is shown on a delayed test sample of three wells (blind wells). The
final value of the gas reservoir prediction quality metric f1 score was
0.893846.
\\ ( https://arxiv.org/abs/2401.12717 ,  633kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12729 (*cross-listing*)
Date: Tue, 23 Jan 2024 13:02:11 GMT   (1703kb,D)

Title: Enhancing Object Detection Performance for Small Objects through
  Synthetic Data Generation and Proportional Class-Balancing Technique: A
  Comparative Study in Industrial Scenarios
Authors: Jibinraj Antony and Vinit Hegiste and Ali Nazeri and Hooman Tavakoli
  and Snehal Walunj and Christiane Plociennik and Martin Ruskowski
Categories: cs.CV cs.LG
Comments: Accepted and presented in conference ESAIM23 1st European Symposium
  on Artificial Intelligence in Manufacturing
\\
  Object Detection (OD) has proven to be a significant computer vision method
in extracting localized class information and has multiple applications in the
industry. Although many of the state-of-the-art (SOTA) OD models perform well
on medium and large sized objects, they seem to under perform on small objects.
In most of the industrial use cases, it is difficult to collect and annotate
data for small objects, as it is time-consuming and prone to human errors.
Additionally, those datasets are likely to be unbalanced and often result in an
inefficient model convergence. To tackle this challenge, this study presents a
novel approach that injects additional data points to improve the performance
of the OD models. Using synthetic data generation, the difficulties in data
collection and annotations for small object data points can be minimized and to
create a dataset with balanced distribution. This paper discusses the effects
of a simple proportional class-balancing technique, to enable better anchor
matching of the OD models. A comparison was carried out on the performances of
the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and
synthetic datasets within an industrial use case.
\\ ( https://arxiv.org/abs/2401.12729 ,  1703kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12733 (*cross-listing*)
Date: Tue, 23 Jan 2024 13:11:05 GMT   (1032kb,D)

Title: TNANet: A Temporal-Noise-Aware Neural Network for Suicidal Ideation
  Prediction with Noisy Physiological Data
Authors: Niqi Liu, Fang Liu, Wenqi Ji, Xinxin Du, Xu Liu, Guozhen Zhao, Wenting
  Mu, Yong-Jin Liu
Categories: cs.CY cs.LG
\\
  The robust generalization of deep learning models in the presence of inherent
noise remains a significant challenge, especially when labels are subjective
and noise is indiscernible in natural settings. This problem is particularly
pronounced in many practical applications. In this paper, we address a special
and important scenario of monitoring suicidal ideation, where time-series data,
such as photoplethysmography (PPG), is susceptible to such noise. Current
methods predominantly focus on image and text data or address artificially
introduced noise, neglecting the complexities of natural noise in time-series
analysis. To tackle this, we introduce a novel neural network model tailored
for analyzing noisy physiological time-series data, named TNANet, which merges
advanced encoding techniques with confidence learning, enhancing prediction
accuracy. Another contribution of our work is the collection of a specialized
dataset of PPG signals derived from real-world environments for suicidal
ideation prediction. Employing this dataset, our TNANet achieves the prediction
accuracy of 63.33% in a binary classification task, outperforming
state-of-the-art models. Furthermore, comprehensive evaluations were conducted
on three other well-known public datasets with artificially introduced noise to
rigorously test the TNANet's capabilities. These tests consistently
demonstrated TNANet's superior performance by achieving an accuracy improvement
of more than 10% compared to baseline methods.
\\ ( https://arxiv.org/abs/2401.12733 ,  1032kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12764 (*cross-listing*)
Date: Tue, 23 Jan 2024 13:44:15 GMT   (36kb)

Title: Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving
  $\mathcal{O}(1/k)$ Finite-Sample Complexity
Authors: Thinh T. Doan
Categories: math.OC cs.LG
\\
  This paper proposes to develop a new variant of the two-time-scale stochastic
approximation to find the roots of two coupled nonlinear operators, assuming
only noisy samples of these operators can be observed. Our key idea is to
leverage the classic Ruppert-Polyak averaging technique to dynamically estimate
the operators through their samples. The estimated values of these averaging
steps will then be used in the two-time-scale stochastic approximation updates
to find the desired solution. Our main theoretical result is to show that under
the strongly monotone condition of the underlying nonlinear operators the
mean-squared errors of the iterates generated by the proposed method converge
to zero at an optimal rate $\mathcal{O}(1/k)$, where $k$ is the number of
iterations. Our result significantly improves the existing result of
two-time-scale stochastic approximation, where the best known finite-time
convergence rate is $\mathcal{O}(1/k^{2/3})$.
\\ ( https://arxiv.org/abs/2401.12764 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12800 (*cross-listing*)
Date: Mon, 8 Jan 2024 18:38:51 GMT   (833kb)

Title: Deep Learning in Physical Layer: Review on Data Driven End-to-End
  Communication Systems and their Enabling Semantic Applications
Authors: Nazmul Islam and Seokjoo Shin
Categories: cs.NI cs.LG
\\
  Deep Learning (DL) has enabled a paradigm shift in wireless communication
system with data driven end-to-end (E2E) learning and optimization of the
Physical Layer (PHY). By leveraging the representation learning of DL, E2E
systems exhibit enhanced adaptability and performance in complex wireless
environments, fulfilling the demands of 5G and beyond network systems and
applications. The evolution of data-driven techniques in the PHY has enabled
advanced semantic applications across various modalities including text, image,
audio, video, and multi-modal transmissions. These applications transcend from
traditional bit-level communication to semantic-level intelligent communication
systems, which are capable of understanding and adapting to the context and
intent of the data transmission. Although PHY as a DL architecture for
data-driven E2E communication is a key factor in enabling semantic
communication systems (SemCom), and various studies in recent years have
surveyed them separately, their combination has not been thoroughly reviewed.
Additionally, these are emerging fields that are still in their infancy, with
several techniques having been developed and evolved in recent years.
Therefore, this article provides a holistic review of data-driven PHY for E2E
communication system, and their enabling semantic applications across different
modalities. Furthermore, it identifies critical challenges and prospective
research directions, providing a pivotal reference for future development of DL
in PHY and SemCom.
\\ ( https://arxiv.org/abs/2401.12800 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12801 (*cross-listing*)
Date: Thu, 11 Jan 2024 00:45:33 GMT   (2906kb,D)

Title: Deep Learning-based Target-To-User Association in Integrated Sensing and
  Communication Systems
Authors: Lorenzo Cazzella, Marouan Mizmizi, Dario Tagliaferri, Damiano Badini,
  Matteo Matteucci, Umberto Spagnolini
Categories: cs.NI cs.LG eess.SP
\\
  In Integrated Sensing and Communication (ISAC) systems, matching the radar
targets with communication user equipments (UEs) is functional to several
communication tasks, such as proactive handover and beam prediction. In this
paper, we consider a radar-assisted communication system where a base station
(BS) is equipped with a multiple-input-multiple-output (MIMO) radar that has a
double aim: (i) associate vehicular radar targets to vehicular equipments (VEs)
in the communication beamspace and (ii) predict the beamforming vector for each
VE from radar data. The proposed target-to-user (T2U) association consists of
two stages. First, vehicular radar targets are detected from range-angle
images, and, for each, a beamforming vector is estimated. Then, the inferred
per-target beamforming vectors are matched with the ones utilized at the BS for
communication to perform target-to-user (T2U) association. Joint multi-target
detection and beam inference is obtained by modifying the you only look once
(YOLO) model, which is trained over simulated range-angle radar images.
Simulation results over different urban vehicular mobility scenarios show that
the proposed T2U method provides a probability of correct association that
increases with the size of the BS antenna array, highlighting the respective
increase of the separability of the VEs in the beamspace. Moreover, we show
that the modified YOLO architecture can effectively perform both beam
prediction and radar target detection, with similar performance in mean average
precision on the latter over different antenna array sizes.
\\ ( https://arxiv.org/abs/2401.12801 ,  2906kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12820 (*cross-listing*)
Date: Tue, 23 Jan 2024 14:53:32 GMT   (11706kb,D)

Title: DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained
  Self-supervised Vision Transformer
Authors: Sonal Kumar, Arijit Sur and Rashmi Dutta Baruah
Categories: cs.CV cs.LG
Comments: The manuscript contains 13 pages, 9 figures and 7 tables
ACM-class: I.4; I.5
\\
  Successive proposals of several self-supervised training schemes continue to
emerge, taking one step closer to developing a universal foundation model. In
this process, the unsupervised downstream tasks are recognized as one of the
evaluation methods to validate the quality of visual features learned with a
self-supervised training scheme. However, unsupervised dense semantic
segmentation has not been explored as a downstream task, which can utilize and
evaluate the quality of semantic information introduced in patch-level feature
representations during self-supervised training of a vision transformer.
Therefore, this paper proposes a novel data-driven approach for unsupervised
semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates
semantically consistent and dense pseudo annotate segmentation masks for the
unlabeled image dataset without using any visual-prior or synchronized data. We
compare these pseudo-annotated segmentation masks with ground truth masks for
evaluating recent self-supervised training schemes to learn shared semantic
properties at the patch level and discriminative semantic properties at the
segment level. Finally, we evaluate existing state-of-the-art self-supervised
training schemes with our proposed downstream task, i.e., DatUS^2. Also, the
best version of DatUS^2 outperforms the existing state-of-the-art method for
the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47%
Pixel accuracy on the SUIM dataset. It also achieves a competitive level of
accuracy for a large-scale and complex dataset, i.e., the COCO dataset.
\\ ( https://arxiv.org/abs/2401.12820 ,  11706kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12843 (*cross-listing*)
Date: Tue, 23 Jan 2024 15:25:21 GMT   (936kb,D)

Title: An embedding-based distance for temporal graphs
Authors: Lorenzo Dall'Amico, Alain Barrat, Ciro Cattuto
Categories: cs.SI cs.LG
\\
  We define a distance between temporal graphs based on graph embeddings built
using time-respecting random walks. We study both the case of matched graphs,
when there exists a known relation between the nodes, and the unmatched case,
when such a relation is unavailable and the graphs may be of different sizes.
We illustrate the interest of our distance definition, using both real and
synthetic temporal network data, by showing its ability to discriminate between
graphs with different structural and temporal properties. Leveraging
state-of-the-art machine learning techniques, we propose an efficient
implementation of distance computation that is viable for large-scale temporal
graphs.
\\ ( https://arxiv.org/abs/2401.12843 ,  936kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12923 (*cross-listing*)
Date: Tue, 23 Jan 2024 17:20:48 GMT   (828kb,D)

Title: Deep multitask neural networks for solving some stochastic optimal
  control problems
Authors: Christian Yeo
Categories: stat.ML cs.LG cs.SY eess.SY
Comments: 8 pages
\\
  Most existing neural network-based approaches for solving stochastic optimal
control problems using the associated backward dynamic programming principle
rely on the ability to simulate the underlying state variables. However, in
some problems, this simulation is infeasible, leading to the discretization of
state variable space and the need to train one neural network for each data
point. This approach becomes computationally inefficient when dealing with
large state variable spaces. In this paper, we consider a class of this type of
stochastic optimal control problems and introduce an effective solution
employing multitask neural networks. To train our multitask neural network, we
introduce a novel scheme that dynamically balances the learning across tasks.
Through numerical experiments on real-world derivatives pricing problems, we
prove that our method outperforms state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2401.12923 ,  828kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12924 (*cross-listing*)
Date: Tue, 23 Jan 2024 17:20:52 GMT   (1391kb,D)

Title: Performance Analysis of Support Vector Machine (SVM) on Challenging
  Datasets for Forest Fire Detection
Authors: Ankan Kar, Nirjhar Nath, Utpalraj Kemprai, Aman
Categories: stat.ML cs.LG stat.ME
Comments: 19 pages, 8 figures, accepted in IJCNS of SCIRP (not yet published)
\\
  This article delves into the analysis of performance and utilization of
Support Vector Machines (SVMs) for the critical task of forest fire detection
using image datasets. With the increasing threat of forest fires to ecosystems
and human settlements, the need for rapid and accurate detection systems is of
utmost importance. SVMs, renowned for their strong classification capabilities,
exhibit proficiency in recognizing patterns associated with fire within images.
By training on labeled data, SVMs acquire the ability to identify distinctive
attributes associated with fire, such as flames, smoke, or alterations in the
visual characteristics of the forest area. The document thoroughly examines the
use of SVMs, covering crucial elements like data preprocessing, feature
extraction, and model training. It rigorously evaluates parameters such as
accuracy, efficiency, and practical applicability. The knowledge gained from
this study aids in the development of efficient forest fire detection systems,
enabling prompt responses and improving disaster management. Moreover, the
correlation between SVM accuracy and the difficulties presented by
high-dimensional datasets is carefully investigated, demonstrated through a
revealing case study. The relationship between accuracy scores and the
different resolutions used for resizing the training datasets has also been
discussed in this article. These comprehensive studies result in a definitive
overview of the difficulties faced and the potential sectors requiring further
improvement and focus.
\\ ( https://arxiv.org/abs/2401.12924 ,  1391kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12934 (*cross-listing*)
Date: Tue, 23 Jan 2024 17:42:17 GMT   (772kb,D)

Title: Reward-Relevance-Filtered Linear Offline Reinforcement Learning
Authors: Angela Zhou
Categories: stat.ML cs.LG math.OC
Comments: conference version accepted at AISTATS 2024
\\
  This paper studies offline reinforcement learning with linear function
approximation in a setting with decision-theoretic, but not estimation
sparsity. The structural restrictions of the data-generating process presume
that the transitions factor into a sparse component that affects the reward and
could affect additional exogenous dynamics that do not affect the reward.
Although the minimally sufficient adjustment set for estimation of full-state
transition properties depends on the whole state, the optimal policy and
therefore state-action value function depends only on the sparse component: we
call this causal/decision-theoretic sparsity. We develop a method for
reward-filtering the estimation of the state-action value function to the
sparse component by a modification of thresholded lasso in least-squares policy
evaluation. We provide theoretical guarantees for our reward-filtered linear
fitted-Q-iteration, with sample complexity depending only on the size of the
sparse component.
\\ ( https://arxiv.org/abs/2401.12934 ,  772kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12961 (*cross-listing*)
Date: Tue, 23 Jan 2024 18:45:27 GMT   (2918kb,D)

Title: Chatterbox: Robust Transport for LLM Token Streaming under Unstable
  Network
Authors: Hanchen Li, Yuhan Liu, Yihua Cheng, Siddhant Ray, Kuntai Du, Junchen
  Jiang
Categories: cs.NI cs.LG
\\
  To render each generated token in real time, the LLM server generates
response tokens one by one and streams each generated token (or group of a few
tokens) through the network to the user right after it is generated, which we
refer to as LLM token streaming. However, under unstable network conditions,
the LLM token streaming experience could suffer greatly from stalls since one
packet loss could block the rendering of tokens contained in subsequent packets
even if they arrive on time. With a real-world measurement study, we show that
current applications including ChatGPT, Claude, and Bard all suffer from
increased stall under unstable network.
  For this emerging token streaming problem in LLM Chatbots, we propose a novel
transport layer scheme, called Chatterbox, which puts new generated tokens as
well as currently unacknowledged tokens in the next outgoing packet. This
ensures that each packet contains some new tokens and can be independently
rendered when received, thus avoiding aforementioned stalls caused by missing
packets. Through simulation under various network conditions, we show
Chatterbox reduces stall ratio (proportion of token rendering wait time) by
71.0% compared to the token streaming method commonly used by real chatbot
applications and by 31.6% compared to a custom packet duplication scheme. By
tailoring Chatterbox to fit the token-by-token generation of LLM, we enable the
Chatbots to respond like an eloquent speaker for users to better enjoy
pervasive AI.
\\ ( https://arxiv.org/abs/2401.12961 ,  2918kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2102.04107
replaced with revised version Tue, 23 Jan 2024 12:55:17 GMT   (79kb)

Title: An extended Knowledge Compilation Map for Conditional Preference
  Statements-based and Generalized Additive Utilities-based Languages
Authors: H\'el\`ene Fargier (IRIT-ADRIA), Stefan Mengel (CRIL), J\'er\^ome
  Mengin (IRIT-ADRIA)
Categories: cs.AI cs.CC
Report-no: IRIT/RR--2023--03--FR
\\ ( https://arxiv.org/abs/2102.04107 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09970
replaced with revised version Tue, 23 Jan 2024 11:36:51 GMT   (364kb,D)

Title: Learning policies for resource allocation in business processes
Authors: J. Middelhuis, R. Lo Bianco, E. Scherzer, Z. A. Bukhsh, I. J. B. F.
  Adan, R. M. Dijkman
Categories: cs.AI
\\ ( https://arxiv.org/abs/2304.09970 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10487
replaced with revised version Tue, 23 Jan 2024 08:18:27 GMT   (219kb,D)

Title: Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees
Authors: Lue Tao, Yu-Xuan Huang, Wang-Zhou Dai, Yuan Jiang
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2308.10487 ,  219kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09552
replaced with revised version Tue, 23 Jan 2024 02:59:44 GMT   (1550kb,D)

Title: A Multitask Training Approach to Enhance Whisper with Contextual Biasing
  and Open-Vocabulary Keyword Spotting
Authors: Yuang Li, Yinglu Li, Min Zhang, Chang Su, Mengxin Ren, Xiaosong Qiao,
  Xiaofeng Zhao, Mengyao Piao, Jiawei Yu, Xinglin Lv, Miaomiao Ma, Yanqing
  Zhao, Hao Yang
Categories: cs.AI cs.CL
Comments: 5 pages, 2 figures
\\ ( https://arxiv.org/abs/2309.09552 ,  1550kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08535
replaced with revised version Tue, 23 Jan 2024 18:35:40 GMT   (249kb,D)

Title: Formally Specifying the High-Level Behavior of LLM-Based Agents
Authors: Maxwell Crouse, Ibrahim Abdelaziz, Ramon Astudillo, Kinjal Basu, Soham
  Dan, Sadhana Kumaravel, Achille Fokoue, Pavan Kapanipathi, Salim Roukos, Luis
  Lastras
Categories: cs.AI cs.CL
Comments: Preprint under review
\\ ( https://arxiv.org/abs/2310.08535 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02099
replaced with revised version Mon, 22 Jan 2024 20:15:26 GMT   (2117kb,D)

Title: A Safe Preference Learning Approach for Personalization with
  Applications to Autonomous Vehicles
Authors: Ruya Karagulle and Nikos Arechiga and Andrew Best and Jonathan
  DeCastro and Necmiye Ozay
Categories: cs.AI cs.SY eess.SY
Comments: 9 pages, 3 figures, 2 tables. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2311.02099 ,  2117kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13884
replaced with revised version Tue, 23 Jan 2024 14:11:04 GMT   (2334kb,D)

Title: Controlling Large Language Model-based Agents for Large-Scale
  Decision-Making: An Actor-Critic Approach
Authors: Bin Zhang, Hangyu Mao, Jingqing Ruan, Ying Wen, Yang Li, Shao Zhang,
  Zhiwei Xu, Dapeng Li, Ziyue Li, Rui Zhao, Lijuan Li, Guoliang Fan
Categories: cs.AI
Comments: 13 pages, 11 figures
\\ ( https://arxiv.org/abs/2311.13884 ,  2334kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08517
replaced with revised version Tue, 23 Jan 2024 13:29:20 GMT   (3770kb,D)

Title: Supporting Student Decisions on Learning Recommendations: An LLM-Based
  Chatbot with Knowledge Graph Contextualization for Conversational
  Explainability and Mentoring
Authors: Hasan Abu-Rasheed, Mohamad Hussam Abdulsalam, Christian Weber, Madjid
  Fathi
Categories: cs.AI cs.CL cs.HC
\\ ( https://arxiv.org/abs/2401.08517 ,  3770kb)
------------------------------------------------------------------------------
\\
arXiv:2212.12192
replaced with revised version Tue, 23 Jan 2024 09:05:41 GMT   (650kb,D)

Title: Learning to Generate Questions by Enhancing Text Generation with
  Sentence Selection
Authors: Pham Quoc-Hung, Minh-Tien Nguyen, Manh Tran-Tien, Hung Le, and
  Xuan-Hieu Phan
Categories: cs.CL
Comments: This paper describes an on-going work
\\ ( https://arxiv.org/abs/2212.12192 ,  650kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02317
replaced with revised version Tue, 23 Jan 2024 02:29:35 GMT   (12870kb,D)

Title: Visual Chain of Thought: Bridging Logical Gaps with Multimodal
  Infillings
Authors: Daniel Rose, Vaishnavi Himakunthala, Andy Ouyang, Ryan He, Alex Mei,
  Yujie Lu, Michael Saxon, Chinmay Sonar, Diba Mirza, William Yang Wang
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2305.02317 ,  12870kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09781
replaced with revised version Tue, 23 Jan 2024 05:02:03 GMT   (9118kb,D)

Title: SpecInfer: Accelerating Generative Large Language Model Serving with
  Tree-based Speculative Inference and Verification
Authors: Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang,
  Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi,
  Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao Jia
Categories: cs.CL cs.DC cs.LG
\\ ( https://arxiv.org/abs/2305.09781 ,  9118kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14259
replaced with revised version Mon, 22 Jan 2024 20:47:51 GMT   (1495kb,D)

Title: Learning to Generate Novel Scientific Directions with Contextualized
  Literature-based Discovery
Authors: Qingyun Wang, Doug Downey, Heng Ji, Tom Hope
Categories: cs.CL cs.AI cs.LG
Comments: 25 pages. Code and resource is available at
  https://github.com/EagleW/CLBD
\\ ( https://arxiv.org/abs/2305.14259 ,  1495kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02272
replaced with revised version Tue, 23 Jan 2024 16:28:49 GMT   (197kb,D)

Title: OWQ: Lessons learned from activation outliers for weight quantization in
  large language models
Authors: Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.02272 ,  197kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12890
replaced with revised version Tue, 23 Jan 2024 13:42:03 GMT   (1383kb,D)

Title: Large Language Models Vote: Prompting for Rare Disease Identification
Authors: David Oniani, Jordan Hilsman, Hang Dong, Fengyi Gao, Shiven Verma,
  Yanshan Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2308.12890 ,  1383kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16692
replaced with revised version Tue, 23 Jan 2024 01:56:57 GMT   (1926kb,D)

Title: SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language
  Models
Authors: Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, Xipeng Qiu
Categories: cs.CL cs.SD eess.AS
Comments: Accepted by ICLR 2024. Project page is at
  https://0nutation.github.io/SpeechTokenizer.github.io/
\\ ( https://arxiv.org/abs/2308.16692 ,  1926kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00367
replaced with revised version Tue, 23 Jan 2024 15:20:33 GMT   (1330kb,D)

Title: AutomaTikZ: Text-Guided Synthesis of Scientific Vector Graphics with
  TikZ
Authors: Jonas Belouadi, Anne Lauscher, Steffen Eger
Categories: cs.CL cs.CV
Comments: Accepted at ICLR 2024 (poster); Project Page:
  https://github.com/potamides/AutomaTikZ
\\ ( https://arxiv.org/abs/2310.00367 ,  1330kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03025
replaced with revised version Tue, 23 Jan 2024 07:49:13 GMT   (404kb,D)

Title: Retrieval meets Long Context Large Language Models
Authors: Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu,
  Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, Bryan Catanzaro
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: Published at ICLR 2024
\\ ( https://arxiv.org/abs/2310.03025 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04691
replaced with revised version Tue, 23 Jan 2024 03:25:22 GMT   (878kb,D)

Title: EMO: Earth Mover Distance Optimization for Auto-Regressive Language
  Modeling
Authors: Siyu Ren, Zhiyong Wu, Kenny Q. Zhu
Categories: cs.CL
Comments: To appear at ICLR 2024
\\ ( https://arxiv.org/abs/2310.04691 ,  878kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17715
replaced with revised version Tue, 23 Jan 2024 18:19:18 GMT   (16866kb,D)

Title: Outlier Dimensions Encode Task-Specific Knowledge
Authors: William Rudman, Catherine Chen, and Carsten Eickhoff
Categories: cs.CL cs.AI
Comments: Camera-ready version for EMNLP 2023
\\ ( https://arxiv.org/abs/2310.17715 ,  16866kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12373
replaced with revised version Tue, 23 Jan 2024 07:12:01 GMT   (265kb,D)

Title: Beyond Turing: A Comparative Analysis of Approaches for Detecting
  Machine-Generated Text
Authors: Muhammad Farid Adilazuarda
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.12373 ,  265kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01185
replaced with revised version Tue, 23 Jan 2024 09:16:00 GMT   (764kb,D)

Title: A ripple in time: a discontinuity in American history
Authors: Alexander Kolpakov, Igor Rivin
Categories: cs.CL cs.AI cs.LG cs.SI
Comments: 7 pages, 8 figures; GitHub repository
  https://github.com/sashakolpakov/ripple_in_time
ACM-class: I.2.7; I.5.4; H.3.1; H.3.3
\\ ( https://arxiv.org/abs/2312.01185 ,  764kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07913
replaced with revised version Tue, 23 Jan 2024 13:26:56 GMT   (2653kb,D)

Title: A Survey of Text Watermarking in the Era of Large Language Models
Authors: Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Xi Zhang,
  Lijie Wen, Irwin King, Hui Xiong and Philip S. Yu
Categories: cs.CL
Comments: 35 pages, 7 figures
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2312.07913 ,  2653kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13010
replaced with revised version Tue, 23 Jan 2024 02:12:35 GMT   (2341kb,D)

Title: AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and
  Optimisation
Authors: Dong Huang, Qingwen Bu, Jie M.Zhang, Michael Luck, and Heming Cui
Categories: cs.CL
Comments: 21 pages, 12 figures
\\ ( https://arxiv.org/abs/2312.13010 ,  2341kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02994
replaced with revised version Tue, 23 Jan 2024 04:43:56 GMT   (8621kb,D)

Title: Blending Is All You Need: Cheaper, Better Alternative to
  Trillion-Parameters LLM
Authors: Xiaoding Lu, Zongyi Liu, Adian Liusie, Vyas Raina, Vineet Mudupalli,
  Yuwen Zhang, William Beauchamp
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.02994 ,  8621kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04867
replaced with revised version Tue, 23 Jan 2024 06:48:45 GMT   (919kb,D)

Title: An Analysis of User Behaviors for Objectively Evaluating Spoken Dialogue
  Systems
Authors: Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara, Gabriel Skantze
Categories: cs.CL cs.AI cs.HC
Comments: This paper has been accepted for presentation at International
  Workshop on Spoken Dialogue Systems Technology 2024 (IWSDS 2024) and
  represents the author's version of the work
\\ ( https://arxiv.org/abs/2401.04867 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08919
replaced with revised version Mon, 22 Jan 2024 19:07:07 GMT   (309kb,D)

Title: Partial Diacritization: A Context-Contrastive Inference Approach
Authors: Muhammad ElNokrashy, Badr AlKhamissi
Categories: cs.CL cs.LG
Comments: 13 equations, 5 tables, 5 figures
\\ ( https://arxiv.org/abs/2401.08919 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10225
replaced with revised version Tue, 23 Jan 2024 05:04:32 GMT   (620kb,D)

Title: ChatQA: Building GPT-4 Level Conversational QA Models
Authors: Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad
  Shoeybi, Bryan Catanzaro
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: We added ChatQA-22B results
\\ ( https://arxiv.org/abs/2401.10225 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11033
replaced with revised version Tue, 23 Jan 2024 03:30:11 GMT   (8223kb,D)

Title: FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for
  Large Language Models' Training?
Authors: Shaina Raza, Shardul Ghuge, Chen Ding, Deval Pandya
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.11033 ,  8223kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11624
replaced with revised version Tue, 23 Jan 2024 03:35:40 GMT   (178kb,D)

Title: In-context Learning with Retrieved Demonstrations for Language Models: A
  Survey
Authors: Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, Mehran Kazemi
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.11624 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11969
replaced with revised version Tue, 23 Jan 2024 09:35:02 GMT   (930kb,D)

Title: Claim Detection for Automated Fact-checking: A Survey on Monolingual,
  Multilingual and Cross-Lingual Research
Authors: Rrubaa Panchendrarajan and Arkaitz Zubiaga
Categories: cs.CL
Comments: Typo corrected
\\ ( https://arxiv.org/abs/2401.11969 ,  930kb)
------------------------------------------------------------------------------
\\
arXiv:2106.01135
replaced with revised version Tue, 23 Jan 2024 16:52:16 GMT   (88kb)

Title: MNL-Bandit with Knapsacks: a near-optimal algorithm
Authors: Abdellah Aznag, Vineet Goyal and Noemie Perivier
Categories: cs.LG cs.DS
\\ ( https://arxiv.org/abs/2106.01135 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2110.07869
replaced with revised version Tue, 23 Jan 2024 12:50:42 GMT   (13977kb,D)

Title: DPGNN: Dual-Perception Graph Neural Network for Representation Learning
Authors: Li Zhou, Wenyu Chen, Dingyi Zeng, Shaohuan Cheng, Wanlong Liu, Malu
  Zhang, Hong Qu
Categories: cs.LG
Comments: Published in Knowledge-Based Systems
Journal-ref: Knowledge-Based Systems 268 (2023): 110377
DOI: 10.1016/j.knosys.2023.110377
\\ ( https://arxiv.org/abs/2110.07869 ,  13977kb)
------------------------------------------------------------------------------
\\
arXiv:2112.11628
replaced with revised version Tue, 23 Jan 2024 03:03:54 GMT   (3189kb,D)

Title: SkipNode: On Alleviating Performance Degradation for Deep Graph
  Convolutional Networks
Authors: Weigang Lu, Yibing Zhan, Binbin Lin, Ziyu Guan, Liu Liu, Baosheng Yu,
  Wei Zhao, Yaming Yang, and Dacheng Tao
Categories: cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2112.11628 ,  3189kb)
------------------------------------------------------------------------------
\\
arXiv:2203.08082
replaced with revised version Mon, 22 Jan 2024 20:43:29 GMT   (1118kb,D)

Title: Regenerative Particle Thompson Sampling
Authors: Zeyu Zhou, Bruce Hajek, Nakjung Choi, Anwar Walid
Categories: cs.LG stat.CO
Comments: Mainbody 14 pages, appendix 32 pages, 16 figures
Journal-ref: "Particle Thompson Sampling with Static Particles" and "Improving
  Particle Thompson Sampling through Regenerative Particles," 2023 57th Annual
  Conference on Information Sciences and Systems (CISS), Baltimore, MD, USA,
  2023
DOI: 10.1109/CISS56502.2023.10089653 10.1109/CISS56502.2023.10089647
\\ ( https://arxiv.org/abs/2203.08082 ,  1118kb)
------------------------------------------------------------------------------
\\
arXiv:2203.13883
replaced with revised version Tue, 23 Jan 2024 03:54:48 GMT   (423kb,D)

Title: Multi-modal Misinformation Detection: Approaches, Challenges and
  Opportunities
Authors: Sara Abdali
Categories: cs.LG cs.AI cs.CV cs.CY cs.MM cs.SI
\\ ( https://arxiv.org/abs/2203.13883 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2205.05173
replaced with revised version Tue, 23 Jan 2024 06:25:31 GMT   (9035kb,D)

Title: Self-Supervised Anomaly Detection in Computer Vision and Beyond: A
  Survey and Outlook
Authors: Hadi Hojjati, Thi Kieu Khanh Ho, Narges Armanfard
Categories: cs.LG
Comments: 18 pages, 4 figures, 5 tables
Journal-ref: Neural Networks, Volume 172, April 2024, 106106
DOI: 10.1016/j.neunet.2024.106106
\\ ( https://arxiv.org/abs/2205.05173 ,  9035kb)
------------------------------------------------------------------------------
\\
arXiv:2205.13743
replaced with revised version Tue, 23 Jan 2024 17:53:23 GMT   (3612kb,D)

Title: Personalized Algorithmic Recourse with Preference Elicitation
Authors: Giovanni De Toni, Paolo Viappiani, Stefano Teso, Bruno Lepri, Andrea
  Passerini
Categories: cs.LG cs.AI
Comments: Published in Transactions in Machine Learning Research (TMLR),
  January 2024. See https://openreview.net/forum?id=8sg2I9zXgO for the official
  submission
\\ ( https://arxiv.org/abs/2205.13743 ,  3612kb)
------------------------------------------------------------------------------
\\
arXiv:2206.02059
replaced with revised version Tue, 23 Jan 2024 08:06:35 GMT   (2119kb,D)

Title: Empowering GNNs via Edge-Aware Weisfeiler-Leman Algorithm
Authors: Meng Liu, Haiyang Yu, Shuiwang Ji
Categories: cs.LG cs.AI
Comments: Accepted to Transactions on Machine Learning Research (TMLR)
\\ ( https://arxiv.org/abs/2206.02059 ,  2119kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07805
replaced with revised version Tue, 23 Jan 2024 17:14:20 GMT   (8438kb,D)

Title: A Comprehensive Benchmark for COVID-19 Predictive Modeling Using
  Electronic Health Records in Intensive Care
Authors: Junyi Gao, Yinghao Zhu, Wenqing Wang, Yasha Wang, Wen Tang, Ewen M.
  Harrison, Liantao Ma
Categories: cs.LG
Comments: Junyi Gao, Yinghao Zhu and Wenqing Wang contributed equally
\\ ( https://arxiv.org/abs/2209.07805 ,  8438kb)
------------------------------------------------------------------------------
\\
arXiv:2211.01758
replaced with revised version Tue, 23 Jan 2024 12:29:23 GMT   (999kb,D)

Title: Optimal Algorithms for Stochastic Complementary Composite Minimization
Authors: Alexandre d'Aspremont, Crist\'obal Guzm\'an, Cl\'ement Lezane
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2211.01758 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2212.03281
replaced with revised version Tue, 23 Jan 2024 09:24:04 GMT   (2651kb,D)

Title: Copula Conformal Prediction for Multi-step Time Series Forecasting
Authors: Sophia Sun, Rose Yu
Categories: cs.LG stat.AP
\\ ( https://arxiv.org/abs/2212.03281 ,  2651kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04631
replaced with revised version Mon, 22 Jan 2024 23:32:50 GMT   (43348kb,D)

Title: The Normalized Cross Density Functional: A Framework to Quantify
  Statistical Dependence for Random Processes
Authors: Bo Hu and Jose C. Principe
Categories: cs.LG cs.AI cs.IT math.IT
\\ ( https://arxiv.org/abs/2212.04631 ,  43348kb)
------------------------------------------------------------------------------
\\
arXiv:2212.12970
replaced with revised version Tue, 23 Jan 2024 07:57:55 GMT   (0kb,I)

Title: Refined Edge Usage of Graph Neural Networks for Edge Prediction
Authors: Jiarui Jin, Yangkun Wang, Weinan Zhang, Quan Gan, Xiang Song, Yong Yu,
  Zheng Zhang, David Wipf
Categories: cs.LG cs.AI cs.IR
Comments: Need major revisions
\\ ( https://arxiv.org/abs/2212.12970 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2212.13069
replaced with revised version Tue, 23 Jan 2024 05:44:44 GMT   (3817kb,D)

Title: Homophily modulates double descent generalization in graph convolution
  networks
Authors: Cheng Shi, Liming Pan, Hong Hu and Ivan Dokmani\'c
Categories: cs.LG cond-mat.dis-nn stat.ML
\\ ( https://arxiv.org/abs/2212.13069 ,  3817kb)
------------------------------------------------------------------------------
\\
arXiv:2301.02424
replaced with revised version Tue, 23 Jan 2024 00:48:54 GMT   (3683kb,D)

Title: Conformal Loss-Controlling Prediction
Authors: Di Wang, Ping Wang, Zhong Ji, Xiaojun Yang, Hongyue Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.02424 ,  3683kb)
------------------------------------------------------------------------------
\\
arXiv:2301.04378
replaced with revised version Tue, 23 Jan 2024 01:56:09 GMT   (2934kb,D)

Title: Loss-Controlling Calibration for Predictive Models
Authors: Di Wang, Junzhi Shi, Pingping Wang, Shuo Zhuang, Hongyue Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2301.04378 ,  2934kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02444
replaced with revised version Tue, 23 Jan 2024 10:39:54 GMT   (516kb,D)

Title: Calibrating Transformers via Sparse Gaussian Processes
Authors: Wenlong Chen, Yingzhen Li
Categories: cs.LG stat.ML
Comments: Published at The Eleventh International Conference on Learning
  Representations (ICLR 2023). This latest Arxiv version includes a
  clarification of how ECE/MCE are computed (at page 10)
\\ ( https://arxiv.org/abs/2303.02444 ,  516kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07846
replaced with revised version Tue, 23 Jan 2024 16:14:46 GMT   (515kb,D)

Title: Sample-efficient Adversarial Imitation Learning
Authors: Dahuin Jung, Hyungyu Lee, Sungroh Yoon
Categories: cs.LG cs.AI
Comments: Published at JMLR (Journal of Machine Learning Research), A
  preliminary version of this manuscript was presented at Deep RL Workshop,
  NeurIPS 2022
\\ ( https://arxiv.org/abs/2303.07846 ,  515kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18417
replaced with revised version Tue, 23 Jan 2024 10:50:06 GMT   (16516kb,D)

Title: Determinantal Point Process Attention Over Grid Cell Code Supports Out
  of Distribution Generalization
Authors: Shanka Subhra Mondal, Steven Frankland, Taylor Webb, and Jonathan D.
  Cohen
Categories: cs.LG q-bio.NC
Comments: 29 pages (including Appendix), 21 figures
\\ ( https://arxiv.org/abs/2305.18417 ,  16516kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02869
replaced with revised version Tue, 23 Jan 2024 13:37:15 GMT   (5234kb,D)

Title: Data-Driven Online Model Selection With Regret Guarantees
Authors: Aldo Pacchiano, Christoph Dann, Claudio Gentile
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2306.02869 ,  5234kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03401
replaced with revised version Tue, 23 Jan 2024 05:16:12 GMT   (2489kb,D)

Title: A Lightweight Method for Tackling Unknown Participation Statistics in
  Federated Averaging
Authors: Shiqiang Wang, Mingyue Ji
Categories: cs.LG cs.DC cs.IT math.IT math.OC stat.ML
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2306.03401 ,  2489kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14624
replaced with revised version Tue, 23 Jan 2024 09:43:23 GMT   (79kb)

Title: Insights From Insurance for Fair Machine Learning
Authors: Christian Fr\"ohlich and Robert C. Williamson
Categories: cs.LG cs.CY
\\ ( https://arxiv.org/abs/2306.14624 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00384
replaced with revised version Mon, 22 Jan 2024 22:12:02 GMT   (5158kb,D)

Title: CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular
  Data Synthesis
Authors: Abdallah Alshantti, Damiano Varagnolo, Adil Rasheed, Aria Rahmati and
  Frank Westad
Categories: cs.LG cs.AI
DOI: 10.1109/ACCESS.2024.3356913
\\ ( https://arxiv.org/abs/2307.00384 ,  5158kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02764
replaced with revised version Tue, 23 Jan 2024 16:01:02 GMT   (453kb,D)

Title: When Does Confidence-Based Cascade Deferral Suffice?
Authors: Wittawat Jitkrittum, Neha Gupta, Aditya Krishna Menon, Harikrishna
  Narasimhan, Ankit Singh Rawat, Sanjiv Kumar
Categories: cs.LG stat.ML
Comments: NeurIPS 2023
\\ ( https://arxiv.org/abs/2307.02764 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11477
replaced with revised version Mon, 22 Jan 2024 21:06:55 GMT   (3287kb)

Title: An improved column-generation-based matheuristic for learning
  classification trees
Authors: Krunal Kishor Patel, Guy Desaulniers, Andrea Lodi
Categories: cs.LG cs.AI math.OC
Comments: Submitted to Computers and Operations Research journal
\\ ( https://arxiv.org/abs/2308.11477 ,  3287kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11624
replaced with revised version Tue, 23 Jan 2024 14:05:13 GMT   (3076kb)

Title: Revolutionizing TCAD Simulations with Universal Device Encoding and
  Graph Attention Networks
Authors: Guangxi Fan, Leilai Shao, Kain Lu Low
Categories: cs.LG cs.AI cs.AR
Comments: 32 pages, 13 figures and 4 tables
\\ ( https://arxiv.org/abs/2308.11624 ,  3076kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10016
replaced with revised version Tue, 23 Jan 2024 06:16:38 GMT   (168kb)

Title: Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction
Authors: Shaika Chowdhury, Sivaraman Rajaganapathy, Lichao Sun, James Cerhan,
  Nansu Zong
Categories: cs.LG cs.AI
Comments: AMIA Informatics Summit 2024
\\ ( https://arxiv.org/abs/2309.10016 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10140
replaced with revised version Tue, 23 Jan 2024 18:08:34 GMT   (5834kb,D)

Title: A Geometric Framework for Neural Feature Learning
Authors: Xiangxiang Xu, Lizhong Zheng
Categories: cs.LG stat.ML
Comments: 76 pages, 24 figures
\\ ( https://arxiv.org/abs/2309.10140 ,  5834kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15318
replaced with revised version Tue, 23 Jan 2024 18:27:30 GMT   (1129kb,D)

Title: HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained
  Heterogeneous Graph Neural Networks
Authors: Yihong Ma, Ning Yan, Jiayu Li, Masood Mortazavi and Nitesh V. Chawla
Categories: cs.LG cs.AI
Comments: Accepted to WWW 2024 as research paper
\\ ( https://arxiv.org/abs/2310.15318 ,  1129kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18304
replaced with revised version Tue, 23 Jan 2024 04:01:25 GMT   (107kb,D)

Title: A Stability Principle for Learning under Non-Stationarity
Authors: Chengpiao Huang, Kaizheng Wang
Categories: cs.LG cs.AI math.OC stat.ML
Comments: 48 pages, 1 figure
MSC-class: 68T05, 90C15
\\ ( https://arxiv.org/abs/2310.18304 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18382
replaced with revised version Tue, 23 Jan 2024 15:27:21 GMT   (4325kb,D)

Title: From Generative AI to Generative Internet of Things: Fundamentals,
  Framework, and Outlooks
Authors: Jinbo Wen, Jiangtian Nie, Jiawen Kang, Dusit Niyato, Hongyang Du, Yang
  Zhang, Mohsen Guizani
Categories: cs.LG cs.GT cs.NI
\\ ( https://arxiv.org/abs/2310.18382 ,  4325kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10309
replaced with revised version Tue, 23 Jan 2024 06:03:10 GMT   (8742kb,D)

Title: Imagination-Augmented Hierarchical Reinforcement Learning for Safe and
  Interactive Autonomous Driving in Urban Environments
Authors: Sang-Hyun Lee, Yoonjae Jung, Seung-Woo Seo
Categories: cs.LG cs.RO
Comments: 15 pages, 9 figures; corrected typos, added references, revised
  experiments (results unchanged)
\\ ( https://arxiv.org/abs/2311.10309 ,  8742kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16536
replaced with revised version Tue, 23 Jan 2024 07:01:19 GMT   (9106kb,D)

Title: Personalized Predictions of Glioblastoma Infiltration: Mathematical
  Models, Physics-Informed Neural Networks and Multimodal Scans
Authors: Ray Zirui Zhang, Ivan Ezhov, Michal Balcerak, Andy Zhu, Benedikt
  Wiestler, Bjoern Menze, John Lowengrub
Categories: cs.LG eess.IV q-bio.QM
MSC-class: 92-08, 92C50, 35Q92
ACM-class: J.3; J.2; I.2.6
\\ ( https://arxiv.org/abs/2311.16536 ,  9106kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12937
replaced with revised version Tue, 23 Jan 2024 04:10:56 GMT   (217kb,D)

Title: Robust Loss Functions for Training Decision Trees with Noisy Labels
Authors: Jonathan Wilton, Nan Ye
Categories: cs.LG stat.ML
Comments: Accepted at AAAI Conference on Artificial Intelligence 2024
\\ ( https://arxiv.org/abs/2312.12937 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08119
replaced with revised version Tue, 23 Jan 2024 06:14:04 GMT   (1563kb,D)

Title: SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic
  Spatio-Temporal Traffic Forecasting
Authors: Lequan Lin, Dai Shi, Andi Han, Junbin Gao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.08119 ,  1563kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09943
replaced with revised version Tue, 23 Jan 2024 14:41:41 GMT   (1220kb,D)

Title: Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance
  Sparse Information Aggregation
Authors: Ruizhe Zhang, Xinke Jiang, Yuchen Fang, Jiayuan Luo, Yongxin Xu,
  Yichen Zhu, Xu Chu, Junfeng Zhao and Yasha Wang
Categories: cs.LG cs.SI
Comments: v1
\\ ( https://arxiv.org/abs/2401.09943 ,  1220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10134
replaced with revised version Tue, 23 Jan 2024 07:42:40 GMT   (273kb,D)

Title: Spatial-Temporal Large Language Model for Traffic Prediction
Authors: Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li,
  Rui Zhao
Categories: cs.LG cs.CL
Comments: Revise
\\ ( https://arxiv.org/abs/2401.10134 ,  273kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10754
replaced with revised version Tue, 23 Jan 2024 12:53:33 GMT   (11096kb,D)

Title: Data Augmentation for Traffic Classification
Authors: Chao Wang, Alessandro Finamore, Pietro Michiardi, Massimo Gallo, Dario
  Rossi
Categories: cs.LG cs.NI
Comments: to appear at Passive and Active Measurements (PAM), 2024
\\ ( https://arxiv.org/abs/2401.10754 ,  11096kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11202
replaced with revised version Tue, 23 Jan 2024 15:11:46 GMT   (1079kb,D)

Title: PartIR: Composing SPMD Partitioning Strategies for Machine Learning
Authors: Sami Alabed, Bart Chrzaszcz, Juliana Franco, Dominik Grewe, Dougal
  Maclaurin, James Molloy, Tom Natan, Tamara Norman, Xiaoyue Pan, Adam Paszke,
  Norman A. Rink, Michael Schaarschmidt, Timur Sitdikov, Agnieszka Swietlik,
  Dimitrios Vytiniotis, Joel Wee
Categories: cs.LG cs.DC cs.PL
\\ ( https://arxiv.org/abs/2401.11202 ,  1079kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11447
replaced with revised version Tue, 23 Jan 2024 07:44:49 GMT   (807kb,D)

Title: Sequential Model for Predicting Patient Adherence in Subcutaneous
  Immunotherapy for Allergic Rhinitis
Authors: Yin Li, Yu Xiong, Wenxin Fan, Kai Wang, Qingqing Yu, Liping Si,
  Patrick van der Smagt, Jun Tang, and Nutan Chen
Categories: cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2401.11447 ,  807kb)
------------------------------------------------------------------------------
\\
arXiv:2110.11334
replaced with revised version Tue, 23 Jan 2024 07:36:33 GMT   (3406kb,D)

Title: Generalized Out-of-Distribution Detection: A Survey
Authors: Jingkang Yang, Kaiyang Zhou, Yixuan Li, Ziwei Liu
Categories: cs.CV cs.AI cs.LG
Comments: Feel free to comment on our Overleaf manuscript:
  https://www.overleaf.com/9899719915wmccvdtwpkct#c25192
\\ ( https://arxiv.org/abs/2110.11334 ,  3406kb)
------------------------------------------------------------------------------
\\
arXiv:2208.04883
replaced with revised version Mon, 22 Jan 2024 21:40:00 GMT   (24347kb,D)

Title: Neural-Rendezvous: Provably Robust Guidance and Control to Encounter
  Interstellar Objects
Authors: Hiroyasu Tsukamoto, Soon-Jo Chung, Benjamin Donitz, Michel Ingham,
  Declan Mages, Yashwanth Kumar Nakka
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY math.OC
Comments: Minor Revision, AIAA Journal of Guidance, Control, and Dynamics
\\ ( https://arxiv.org/abs/2208.04883 ,  24347kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06470
replaced with revised version Tue, 23 Jan 2024 05:03:53 GMT   (13895kb,D)

Title: Qualitative Failures of Image Generation Models and Their Application in
  Detecting Deepfakes
Authors: Ali Borji
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2304.06470 ,  13895kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14391
replaced with revised version Tue, 23 Jan 2024 15:52:28 GMT   (7002kb,D)

Title: Energy-based Models are Zero-Shot Planners for Compositional Scene
  Rearrangement
Authors: Nikolaos Gkanatsios, Ayush Jain, Zhou Xian, Yunchu Zhang, Christopher
  Atkeson, Katerina Fragkiadaki
Categories: cs.RO cs.AI cs.CL cs.CV cs.LG
Comments: First two authors contributed equally | RSS 2023
\\ ( https://arxiv.org/abs/2304.14391 ,  7002kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13208
replaced with revised version Tue, 23 Jan 2024 08:18:27 GMT   (1637kb,D)

Title: Iterative Adversarial Attack on Image-guided Story Ending Generation
Authors: Youze Wang, Wenbo Hu, Richang Hong
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2305.13208 ,  1637kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09549 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 21:53:01 GMT   (623kb,D)

Title: QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules
Authors: Haiyang Yu, Meng Liu, Youzhi Luo, Alex Strasser, Xiaofeng Qian,
  Xiaoning Qian, Shuiwang Ji
Categories: physics.chem-ph cs.AI cs.LG
Comments: Accepted by NeurIPS 2023, Track on Datasets and Benchmarks
\\ ( https://arxiv.org/abs/2306.09549 ,  623kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03212
replaced with revised version Tue, 23 Jan 2024 13:15:31 GMT   (24365kb)

Title: Region-Wise Attentive Multi-View Representation Learning for Urban
  Region Embeddings
Authors: Weiliang Chan and Qianqian Ren
Categories: cs.CV cs.AI cs.CY cs.LG
Journal-ref: CIKM '23: The 32nd ACM International Conference on Information and
  Knowledge Management Birmingham United Kingdom October 21 - 25, 2023
DOI: 10.1145/3583780.3615194
\\ ( https://arxiv.org/abs/2307.03212 ,  24365kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14190 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 14:51:41 GMT   (5453kb,D)

Title: Score-Based Generative Models for PET Image Reconstruction
Authors: Imraj RD Singh, Alexander Denker, Riccardo Barbano, \v{Z}eljko Kereta,
  Bangti Jin, Kris Thielemans, Peter Maass, Simon Arridge
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:001
MSC-class: 15A29, 45Q05
ACM-class: I.4.9; J.2; I.2.1
Journal-ref: Machine.Learning.for.Biomedical.Imaging. 2 (2024)
DOI: 10.59275/j.melba.2024-5d51
\\ ( https://arxiv.org/abs/2308.14190 ,  5453kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06801
replaced with revised version Tue, 23 Jan 2024 15:11:08 GMT   (55kb)

Title: Defensive Alliances in Signed Networks
Authors: Emmanuel Arrighi, Zhidan Feng, Henning Fernau, Kevin Mann, Xingqin Qi,
  Petra Wolf
Categories: cs.CC cs.AI cs.SI
\\ ( https://arxiv.org/abs/2309.06801 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16606
replaced with revised version Tue, 23 Jan 2024 10:19:51 GMT   (6317kb,D)

Title: "AI enhances our performance, I have no doubt this one will do the
  same": The Placebo effect is robust to negative descriptions of AI
Authors: Agnes M. Kloft, Robin Welsch, Thomas Kosch, Steeven Villa
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2309.16606 ,  6317kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00737
replaced with revised version Mon, 22 Jan 2024 22:12:05 GMT   (3028kb,D)

Title: GenAI Against Humanity: Nefarious Applications of Generative Artificial
  Intelligence and Large Language Models
Authors: Emilio Ferrara
Categories: cs.CY cs.AI cs.CL cs.HC
Comments: Accepted in: Journal of Computational Social Science
\\ ( https://arxiv.org/abs/2310.00737 ,  3028kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05207
replaced with revised version Mon, 22 Jan 2024 19:06:15 GMT   (1718kb,D)

Title: Boosting Facial Action Unit Detection Through Jointly Learning Facial
  Landmark Detection and Domain Separation and Reconstruction
Authors: Ziqiao Shang, Li Yu
Categories: cs.CV cs.AI cs.LG
Comments: 5 pages, 1 figure, published to ICASSP 2024
\\ ( https://arxiv.org/abs/2310.05207 ,  1718kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08276
replaced with revised version Tue, 23 Jan 2024 09:42:41 GMT   (10587kb,D)

Title: Direction-Oriented Visual-semantic Embedding Model for Remote Sensing
  Image-text Retrieval
Authors: Qing Ma, Jiancheng Pan, Cong Bai
Categories: cs.CV cs.AI
Comments: 14 pages, 11 figures
\\ ( https://arxiv.org/abs/2310.08276 ,  10587kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13712
replaced with revised version Tue, 23 Jan 2024 07:13:22 GMT   (3353kb,D)

Title: Impact of Guidance and Interaction Strategies for LLM Use on Learner
  Performance and Perception
Authors: Harsh Kumar, Ilya Musabirov, Mohi Reza, Jiakai Shi, Xinyuan Wang,
  Joseph Jay Williams, Anastasia Kuzminykh, Michael Liut
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2310.13712 ,  3353kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16716
replaced with revised version Tue, 23 Jan 2024 14:05:58 GMT   (1364kb,D)

Title: GraphPro: Graph Pre-training and Prompt Learning for Recommendation
Authors: Yuhao Yang, Lianghao Xia, Da Luo, Kangyi Lin, Chao Huang
Categories: cs.IR cs.AI
Comments: Accepted by WWW'2024, full paper
\\ ( https://arxiv.org/abs/2311.16716 ,  1364kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02246
replaced with revised version Tue, 23 Jan 2024 11:26:42 GMT   (41607kb,D)

Title: Conditional Variational Diffusion Models
Authors: Gabriel della Maggiora, Luis Alberto Croquevielle, Nikita Deshpande,
  Harry Horsley, Thomas Heinis, Artur Yakimovich
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: Denoising Diffusion Probabilistic Models, Inverse Problems,
  Generative Models, Super Resolution, Phase Quantification, Variational
  Methods
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2312.02246 ,  41607kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12433
replaced with revised version Tue, 23 Jan 2024 18:59:39 GMT   (10782kb,D)

Title: Tracking Any Object Amodally
Authors: Cheng-Yen Hsieh, Tarasha Khurana, Achal Dave, Deva Ramanan
Categories: cs.CV cs.AI cs.LG
Comments: Project Page: https://tao-amodal.github.io
\\ ( https://arxiv.org/abs/2312.12433 ,  10782kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14794
replaced with revised version Tue, 23 Jan 2024 13:07:38 GMT   (250kb,D)

Title: An Empirical Study on Compliance with Ranking Transparency in the
  Software Documentation of EU Online Platforms
Authors: Francesco Sovrano, Micha\"el Lognoul, Alberto Bacchelli
Categories: cs.SE cs.AI
Comments: Accepted for publication at the 46th International Conference on
  Software Engineering (ICSE 2024), Software Engineering in Society (SEIS)
  track
DOI: 10.1145/3639475.3640112
\\ ( https://arxiv.org/abs/2312.14794 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01651
replaced with revised version Tue, 23 Jan 2024 15:31:17 GMT   (7558kb,D)

Title: AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated
  by AI
Authors: Fanda Fan, Chunjie Luo, Wanling Gao, Jianfeng Zhan
Categories: cs.CV cs.AI
Comments: Accepted to BenchCouncil Transactions on Benchmarks, Standards and
  Evaluations (TBench)
\\ ( https://arxiv.org/abs/2401.01651 ,  7558kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06827
replaced with revised version Tue, 23 Jan 2024 08:54:15 GMT   (425kb,D)

Title: APLe: Token-Wise Adaptive for Multi-Modal Prompt Learning
Authors: Guiming Cao, Kaize Shi, Hong Fu, Huaiwen Zhang and Guandong Xu
Categories: cs.CV cs.AI cs.CL
Comments: 7 pages,3 figures
\\ ( https://arxiv.org/abs/2401.06827 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07709
replaced with revised version Tue, 23 Jan 2024 11:22:03 GMT   (4419kb,D)

Title: Towards Efficient Diffusion-Based Image Editing with Instant Attention
  Masks
Authors: Siyu Zou, Jiji Tang, Yiyi Zhou, Jing He, Chaoyi Zhao, Rongsheng Zhang,
  Zhipeng Hu, Xiaoshuai Sun
Categories: cs.CV cs.AI
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2401.07709 ,  4419kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09479
replaced with revised version Tue, 23 Jan 2024 07:04:18 GMT   (2581kb,D)

Title: Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep
  Learning
Authors: Rahul Vishwakarma, Amin Rezaei
Categories: cs.CR cs.AI cs.LG
Comments: 2024 Design, Automation and Test in Europe Conference | The European
  Event for Electronic System Design & Test (accepted)
Journal-ref: 2024 Design, Automation and Test in Europe Conference | The
  European Event for Electronic System Design & Test
\\ ( https://arxiv.org/abs/2401.09479 ,  2581kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11748
replaced with revised version Tue, 23 Jan 2024 02:13:03 GMT   (7707kb,D)

Title: GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient
  Inversion Attacks?
Authors: Yu sun, Gaojian Xiong, Xianxun Yao, Kailang Ma, Jian Cui
Categories: cs.CR cs.AI cs.LG
Comments: Accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2401.11748 ,  7707kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11792
replaced with revised version Tue, 23 Jan 2024 02:53:58 GMT   (10362kb,D)

Title: Safe and Generalized end-to-end Autonomous Driving System with
  Reinforcement Learning and Demonstrations
Authors: Zuojin Tang, Xiaoyu Chen, YongQiang Li, Jianyu Chen
Categories: cs.RO cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.11792 ,  10362kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11851
replaced with revised version Tue, 23 Jan 2024 04:17:07 GMT   (355kb,D)

Title: BETA: Binarized Energy-Efficient Transformer Accelerator at the Edge
Authors: Yuhao Ji, Chao Fang, Zhongfeng Wang
Categories: cs.AR cs.AI
Comments: This paper is accepted by 2024 IEEE International Symposium on
  Circuits and Systems (ISCAS 2024)
\\ ( https://arxiv.org/abs/2401.11851 ,  355kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07213
replaced with revised version Tue, 23 Jan 2024 04:59:29 GMT   (3318kb,D)

Title: Human-centered NLP Fact-checking: Co-Designing with Fact-checkers using
  Matchmaking for AI
Authors: Houjiang Liu, Anubrata Das, Alexander Boltz, Didi Zhou, Daisy Pinaroc,
  Matthew Lease, Min Kyung Lee
Categories: cs.HC cs.CL cs.CY
\\ ( https://arxiv.org/abs/2308.07213 ,  3318kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08007 (*cross-listing*)
replaced with revised version Mon, 22 Jan 2024 23:05:55 GMT   (129kb)

Title: DiariST: Streaming Speech Translation with Speaker Diarization
Authors: Mu Yang, Naoyuki Kanda, Xiaofei Wang, Junkun Chen, Peidong Wang, Jian
  Xue, Jinyu Li, Takuya Yoshioka
Categories: eess.AS cs.CL cs.SD
Comments: Accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2309.08007 ,  129kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10543 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 14:46:23 GMT   (18749kb,D)

Title: Multilingual acoustic word embeddings for zero-resource languages
Authors: Christiaan Jacobs
Categories: eess.AS cs.CL cs.SD
Comments: PhD thesis
\\ ( https://arxiv.org/abs/2401.10543 ,  18749kb)
------------------------------------------------------------------------------
\\
arXiv:2204.13209
replaced with revised version Tue, 23 Jan 2024 17:31:48 GMT   (2722kb,D)

Title: Robust stabilization of polytopic systems via fast and reliable neural
  network-based approximations
Authors: Filippo Fabiani, Paul J. Goulart
Categories: eess.SY cs.LG cs.SY math.OC
\\ ( https://arxiv.org/abs/2204.13209 ,  2722kb)
------------------------------------------------------------------------------
\\
arXiv:2205.05587 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 17:26:09 GMT   (1563kb,D)

Title: Choice of training label matters: how to best use deep learning for
  quantitative MRI parameter estimation
Authors: Sean C. Epstein, Timothy J. P. Bray, Margaret Hall-Craggs and Hui
  Zhang
Categories: physics.med-ph cs.LG eess.IV
Comments: Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2024:002
Journal-ref: Machine.Learning.for.Biomedical.Imaging. 2 (2024)
DOI: 10.59275/j.melba.2024-geb5
\\ ( https://arxiv.org/abs/2205.05587 ,  1563kb)
------------------------------------------------------------------------------
\\
arXiv:2206.00775 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 18:31:01 GMT   (11429kb,D)

Title: Adaptive Local Neighborhood-based Neural Networks for MR Image
  Reconstruction from Undersampled Data
Authors: Shijun Liang, Anish Lahiri and Saiprasad Ravishankar
Categories: eess.IV cs.LG
\\ ( https://arxiv.org/abs/2206.00775 ,  11429kb)
------------------------------------------------------------------------------
\\
arXiv:2206.11492 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 05:52:26 GMT   (1201kb,D)

Title: Gradual Domain Adaptation via Normalizing Flows
Authors: Shogo Sagawa, Hideitsu Hino
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2206.11492 ,  1201kb)
------------------------------------------------------------------------------
\\
arXiv:2207.02829 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 14:00:54 GMT   (9881kb,D)

Title: Online Bilevel Optimization: Regret Analysis of Online Alternating
  Gradient Methods
Authors: Davoud Ataee Tarzanagh, Parvin Nazari, Bojian Hou, Li Shen, Laura
  Balzano
Categories: math.OC cs.DS cs.LG
Comments: Accepted for publication at AISTATS 2024. v5: experiments are
  expanded
\\ ( https://arxiv.org/abs/2207.02829 ,  9881kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03053
replaced with revised version Mon, 22 Jan 2024 20:56:16 GMT   (7027kb,D)

Title: ZipIt! Merging Models from Different Tasks without Training
Authors: George Stoica, Daniel Bolya, Jakob Bjorner, Pratik Ramesh, Taylor
  Hearn, Judy Hoffman
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2305.03053 ,  7027kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19004 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 09:59:27 GMT   (90kb)

Title: Policy Gradient Algorithms for Robust MDPs with Non-Rectangular
  Uncertainty Sets
Authors: Mengmeng Li, Daniel Kuhn, Tobias Sutter
Categories: math.OC cs.LG
Comments: comments are welcome
MSC-class: 90C17, 90C26
\\ ( https://arxiv.org/abs/2305.19004 ,  90kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05739 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 09:05:19 GMT   (1917kb,D)

Title: Leaping through tree space: continuous phylogenetic inference for rooted
  and unrooted trees
Authors: Matthew J Penn, Neil Scheidwasser, Joseph Penn, Christl A Donnelly,
  David A Duch\^ene, and Samir Bhatt
Categories: q-bio.PE cs.LG
Comments: 26 pages, 3 figures, 2 tables, 20 supplementary pages, 3
  supplementary figures
Journal-ref: Genome Biol. Evol. 15 (2023) evad213
DOI: 10.1093/gbe/evad213
\\ ( https://arxiv.org/abs/2306.05739 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06075
replaced with revised version Tue, 23 Jan 2024 09:06:46 GMT   (2032kb)

Title: DeepSeaNet: Improving Underwater Object Detection using EfficientDet
Authors: Sanyam Jain
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.06075 ,  2032kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17396
replaced with revised version Tue, 23 Jan 2024 05:03:55 GMT   (1098kb,D)

Title: Koopman operator learning using invertible neural networks
Authors: Yuhuang Meng, Jianguo Huang, Yue Qiu
Categories: math.NA cs.LG cs.NA
MSC-class: 65Mxx (Primary), 68Txx (Secondary)
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2306.17396 ,  1098kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03131 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 17:29:54 GMT   (2363kb,D)

Title: Reservoir-Computing Model for Mapping and Forecasting Neuronal
  Interactions from Electrophysiological Data
Authors: Ilya Auslender, Giorgio Letti, Yasaman Heydari, Clara Zaccaria,
  Lorenzo Pavesi
Categories: q-bio.QM cs.LG physics.bio-ph
Comments: Pre-submission draft
\\ ( https://arxiv.org/abs/2311.03131 ,  2363kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03311 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 16:34:53 GMT   (217kb,D)

Title: On the Nystrom Approximation for Preconditioning in Kernel Machines
Authors: Amirhesam Abedsoltan, Parthe Pandit, Luis Rademacher, Mikhail Belkin
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2312.03311 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09126
replaced with revised version Tue, 23 Jan 2024 09:37:39 GMT   (97kb,D)

Title: Towards Trustworthy AI Software Development Assistance
Authors: Daniel Maninger, Krishna Narasimhan, Mira Mezini
Categories: cs.SE cs.LG
Comments: 6 pages, 1 figure; to be published in New Ideas and Emerging Results
  (ICSE-NIER'24), April 14-20, 2024, Lisbon, Portugal; updated version to
  reflect the information provided by ACM
DOI: 10.1145/3639476.3639770
\\ ( https://arxiv.org/abs/2312.09126 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11486
replaced with revised version Mon, 22 Jan 2024 19:57:27 GMT   (101kb,D)

Title: Preference and Concurrence Aware Bayesian Graph Neural Networks for
  Recommender Systems
Authors: Hongjian Gu, Yaochen Hu, Yingxue Zhang
Categories: cs.IR cs.LG
\\ ( https://arxiv.org/abs/2312.11486 ,  101kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15282 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 13:32:03 GMT   (5331kb,D)

Title: Causal Forecasting for Pricing
Authors: Douglas Schultz, Johannes Stephan, Julian Sieber, Trudie Yeh, Manuel
  Kunz, Patrick Doupe, Tim Januschowski
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2312.15282 ,  5331kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16613
replaced with revised version Tue, 23 Jan 2024 10:04:49 GMT   (647kb,D)

Title: Self-supervised Pretraining for Robust Personalized Voice Activity
  Detection in Adverse Conditions
Authors: Holger Severin Bovbjerg (1), Jesper Jensen (1, 2), Jan {\O}stergaard
  (1), Zheng-Hua Tan (1, 3) ((1) Aalborg University, (2) Oticon, (3) Pioneer
  Centre for AI, Denmark)
Categories: cs.SD cs.LG eess.AS
Comments: To be published at ICASSP2024, 14th of April 2024, Seoul, South
  Korea. Copyright (c) 2023 IEEE. 5 pages, 2, figures, 5 tables
MSC-class: 68T10
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2312.16613 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00334
replaced with revised version Tue, 23 Jan 2024 05:38:56 GMT   (3226kb,D)

Title: Explainability-Driven Leaf Disease Classification Using Adversarial
  Training and Knowledge Distillation
Authors: Sebastian-Vasile Echim, Iulian-Marius T\u{a}iatu, Dumitru-Clementin
  Cercel, Florin Pop
Categories: cs.CV cs.LG
Comments: 10 pages, 8 figures, Accepted by ICAART 2024
\\ ( https://arxiv.org/abs/2401.00334 ,  3226kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01520
replaced with revised version Tue, 23 Jan 2024 02:59:04 GMT   (6921kb,D)

Title: S$^{2}$-DMs:Skip-Step Diffusion Models
Authors: Yixuan Wang and Shuangyin Li
Categories: cs.CV cs.LG eess.IV
Comments: 12 pages
\\ ( https://arxiv.org/abs/2401.01520 ,  6921kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04079 (*cross-listing*)
replaced with revised version Tue, 23 Jan 2024 18:59:52 GMT   (561kb,D)

Title: RudolfV: A Foundation Model by Pathologists for Pathologists
Authors: Jonas Dippel, Barbara Feulner, Tobias Winterhoff, Simon Schallenberg,
  Gabriel Dernbach, Andreas Kunft, Stephan Tietz, Philipp Jurmeister, David
  Horst, Lukas Ruff, Klaus-Robert M\"uller, Frederick Klauschen, Maximilian
  Alber
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.04079 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11488
replaced with revised version Tue, 23 Jan 2024 17:49:42 GMT   (1135kb,D)

Title: HARDCORE: H-field and power loss estimation for arbitrary waveforms with
  residual, dilated convolutional neural networks in ferrite cores
Authors: Wilhelm Kirchg\"assner, Nikolas F\"orster, Till Piepenbrock, Oliver
  Schweins, Oliver Wallscheid
Categories: eess.SY cs.LG cs.SY physics.app-ph
Comments: Competition submission version, slightly change author order
\\ ( https://arxiv.org/abs/2401.11488 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11687
replaced with revised version Tue, 23 Jan 2024 02:08:09 GMT   (464kb,D)

Title: TIM: An Efficient Temporal Interaction Module for Spiking Transformer
Authors: Sicheng Shen, Dongcheng Zhao, Guobin Shen and Yi Zeng
Categories: cs.NE cs.CV cs.LG
Comments: 9pages,6figures
\\ ( https://arxiv.org/abs/2401.11687 ,  464kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
