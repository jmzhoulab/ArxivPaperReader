Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月22日 17:26
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu 18 Jan 24 19:00:00 GMT  to  Fri 19 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.10420
Date: Thu, 18 Jan 2024 23:19:47 GMT   (327kb,D)

Title: Generalized Nested Rollout Policy Adaptation with Limited Repetitions
Authors: Tristan Cazenave
Categories: cs.AI
\\
  Generalized Nested Rollout Policy Adaptation (GNRPA) is a Monte Carlo search
algorithm for optimizing a sequence of choices. We propose to improve on GNRPA
by avoiding too deterministic policies that find again and again the same
sequence of choices. We do so by limiting the number of repetitions of the best
sequence found at a given level. Experiments show that it improves the
algorithm for three different combinatorial problems: Inverse RNA Folding, the
Traveling Salesman Problem with Time Windows and the Weak Schur problem.
\\ ( https://arxiv.org/abs/2401.10420 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10428
Date: Fri, 19 Jan 2024 00:13:44 GMT   (653kb)

Title: Understanding Learning through the Lens of Dynamical Invariants
Authors: Alex Ushveridze
Categories: cs.AI cs.IT math.IT
Comments: 19 pages
\\
  This paper proposes a novel perspective on learning, positing it as the
pursuit of dynamical invariants -- data combinations that remain constant or
exhibit minimal change over time as a system evolves. This concept is
underpinned by both informational and physical principles, rooted in the
inherent properties of these invariants. Firstly, their stability makes them
ideal for memorization and integration into associative networks, forming the
basis of our knowledge structures. Secondly, the predictability of these stable
invariants makes them valuable sources of usable energy, quantifiable as kTln2
per bit of accurately predicted information. This energy can be harnessed to
explore new transformations, rendering learning systems energetically
autonomous and increasingly effective. Such systems are driven to continuously
seek new data invariants as energy sources. The paper further explores several
meta-architectures of autonomous, self-propelled learning agents that utilize
predictable information patterns as a source of usable energy.
\\ ( https://arxiv.org/abs/2401.10428 ,  653kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10431
Date: Fri, 19 Jan 2024 00:22:31 GMT   (187kb,D)

Title: Learning a Prior for Monte Carlo Search by Replaying Solutions to
  Combinatorial Problems
Authors: Tristan Cazenave
Categories: cs.AI
\\
  Monte Carlo Search gives excellent results in multiple difficult
combinatorial problems. Using a prior to perform non uniform playouts during
the search improves a lot the results compared to uniform playouts. Handmade
heuristics tailored to the combinatorial problem are often used as priors. We
propose a method to automatically compute a prior. It uses statistics on solved
problems. It is a simple and general method that incurs no computational cost
at playout time and that brings large performance gains. The method is applied
to three difficult combinatorial problems: Latin Square Completion, Kakuro, and
Inverse RNA Folding.
\\ ( https://arxiv.org/abs/2401.10431 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10444
Date: Fri, 19 Jan 2024 01:14:45 GMT   (424kb)

Title: Can A Cognitive Architecture Fundamentally Enhance LLMs? Or Vice Versa?
Authors: Ron Sun
Categories: cs.AI cs.CY
\\
  The paper discusses what is needed to address the limitations of current
LLM-centered AI systems. The paper argues that incorporating insights from
human cognition and psychology, as embodied by a computational cognitive
architecture, can help develop systems that are more capable, more reliable,
and more human-like. It emphasizes the importance of the dual-process
architecture and the hybrid neuro-symbolic approach in addressing the
limitations of current LLMs. In the opposite direction, the paper also
highlights the need for an overhaul of computational cognitive architectures to
better reflect advances in AI and computing technology. Overall, the paper
advocates for a multidisciplinary, mutually beneficial approach towards
developing better models both for AI and for understanding the human mind.
\\ ( https://arxiv.org/abs/2401.10444 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10467
Date: Fri, 19 Jan 2024 03:39:43 GMT   (5013kb,D)

Title: Learning Backdoors for Mixed Integer Programs with Contrastive Learning
Authors: Junyang Cai, Taoan Huang, Bistra Dilkina
Categories: cs.AI cs.LG math.OC
\\
  Many real-world problems can be efficiently modeled as Mixed Integer Programs
(MIPs) and solved with the Branch-and-Bound method. Prior work has shown the
existence of MIP backdoors, small sets of variables such that prioritizing
branching on them when possible leads to faster running times. However, finding
high-quality backdoors that improve running times remains an open question.
Previous work learns to estimate the relative solver speed of randomly sampled
backdoors through ranking and then decide whether to use it. In this paper, we
utilize the Monte-Carlo tree search method to collect backdoors for training,
rather than relying on random sampling, and adapt a contrastive learning
framework to train a Graph Attention Network model to predict backdoors. Our
method, evaluated on four common MIP problem domains, demonstrates performance
improvements over both Gurobi and previous models.
\\ ( https://arxiv.org/abs/2401.10467 ,  5013kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10568
Date: Fri, 19 Jan 2024 09:14:11 GMT   (10015kb,D)

Title: CivRealm: A Learning and Reasoning Odyssey in Civilization for
  Decision-Making Agents
Authors: Siyuan Qi, Shuo Chen, Yexin Li, Xiangyu Kong, Junqi Wang, Bangcheng
  Yang, Pring Wong, Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Wei
  Wang, Yaodong Yang, Song-Chun Zhu
Categories: cs.AI
\\
  The generalization of decision-making agents encompasses two fundamental
elements: learning from past experiences and reasoning in novel contexts.
However, the predominant emphasis in most interactive environments is on
learning, often at the expense of complexity in reasoning. In this paper, we
introduce CivRealm, an environment inspired by the Civilization game.
Civilization's profound alignment with human history and society necessitates
sophisticated learning, while its ever-changing situations demand strong
reasoning to generalize. Particularly, CivRealm sets up an
imperfect-information general-sum game with a changing number of players; it
presents a plethora of complex features, challenging the agent to deal with
open-ended stochastic environments that require diplomacy and negotiation
skills. Within CivRealm, we provide interfaces for two typical agent types:
tensor-based agents that focus on learning, and language-based agents that
emphasize reasoning. To catalyze further research, we present initial results
for both paradigms. The canonical RL-based agents exhibit reasonable
performance in mini-games, whereas both RL- and LLM-based agents struggle to
make substantial progress in the full game. Overall, CivRealm stands as a
unique learning and reasoning challenge for decision-making agents. The code is
available at https://github.com/bigai-ai/civrealm.
\\ ( https://arxiv.org/abs/2401.10568 ,  10015kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10589
Date: Fri, 19 Jan 2024 09:59:02 GMT   (379kb,D)

Title: Rethinking the Soft Conflict Pseudo Boolean Constraint on MaxSAT Local
  Search Solvers
Authors: Jiongzhi Zheng and Zhuo Chen and Chu-Min Li and Kun He
Categories: cs.AI
\\
  MaxSAT is an optimization version of the famous NP-complete Satisfiability
problem (SAT). Algorithms for MaxSAT mainly include complete solvers and local
search incomplete solvers. In many complete solvers, once a better solution is
found, a Soft conflict Pseudo Boolean (SPB) constraint will be generated to
enforce the algorithm to find better solutions. In many local search
algorithms, clause weighting is a key technique for effectively guiding the
search directions. In this paper, we propose to transfer the SPB constraint
into the clause weighting system of the local search method, leading the
algorithm to better solutions. We further propose an adaptive clause weighting
strategy that breaks the tradition of using constant values to adjust clause
weights. Based on the above methods, we propose a new local search algorithm
called SPB-MaxSAT that provides new perspectives for clause weighting on MaxSAT
local search solvers. Extensive experiments demonstrate the excellent
performance of the proposed methods.
\\ ( https://arxiv.org/abs/2401.10589 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10744
Date: Fri, 19 Jan 2024 15:09:39 GMT   (3580kb,D)

Title: FinLLMs: A Framework for Financial Reasoning Dataset Generation with
  Large Language Models
Authors: Ziqiang Yuan, Kaiyuan Wang, Shoutai Zhu, Ye Yuan, Jingya Zhou, Yanlin
  Zhu, Wenqi Wei
Categories: cs.AI
Comments: Under submission of IEEE Transactions
\\
  Large Language models (LLMs) usually rely on extensive training datasets. In
the financial domain, creating numerical reasoning datasets that include a mix
of tables and long text often involves substantial manual annotation expenses.
To address the limited data resources and reduce the annotation cost, we
introduce FinLLMs, a method for generating financial question-answering data
based on common financial formulas using Large Language Models. First, we
compile a list of common financial formulas and construct a graph based on the
variables these formulas employ. We then augment the formula set by combining
those that share identical variables as new elements. Specifically, we explore
formulas obtained by manual annotation and merge those formulas with shared
variables by traversing the constructed graph. Finally, utilizing GPT-3.5, we
generate financial question-answering data that encompasses both tabular
information and long textual content, building on the collected formula set.
Our experiments demonstrate that synthetic data generated by FinLLMs
effectively enhances the performance of several large-scale numerical reasoning
models in the financial domain, outperforming two established benchmark
financial question-answering datasets.
\\ ( https://arxiv.org/abs/2401.10744 ,  3580kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10751
Date: Fri, 19 Jan 2024 15:20:57 GMT   (1305kb,D)

Title: EFO: the Emotion Frame Ontology
Authors: Stefano De Giorgis and Aldo Gangemi
Categories: cs.AI cs.CY cs.SC
\\
  Emotions are a subject of intense debate in various disciplines. Despite the
proliferation of theories and definitions, there is still no consensus on what
emotions are, and how to model the different concepts involved when we talk
about - or categorize - them. In this paper, we propose an OWL frame-based
ontology of emotions: the Emotion Frames Ontology (EFO). EFO treats emotions as
semantic frames, with a set of semantic roles that capture the different
aspects of emotional experience. EFO follows pattern-based ontology design, and
is aligned to the DOLCE foundational ontology. EFO is used to model multiple
emotion theories, which can be cross-linked as modules in an Emotion Ontology
Network. In this paper, we exemplify it by modeling Ekman's Basic Emotions (BE)
Theory as an EFO-BE module, and demonstrate how to perform automated inferences
on the representation of emotion situations. EFO-BE has been evaluated by
lexicalizing the BE emotion frames from within the Framester knowledge graph,
and implementing a graph-based emotion detector from text. In addition, an EFO
integration of multimodal datasets, including emotional speech and emotional
face expressions, has been performed to enable further inquiry into crossmodal
emotion semantics.
\\ ( https://arxiv.org/abs/2401.10751 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10781
Date: Fri, 19 Jan 2024 16:01:38 GMT   (44kb,D)

Title: Metric Dynamic Equilibrium Logic
Authors: Arvid Becker, Pedro Cabalar, Mart\'in Di\'eguez, Luis Fari\~nas,
  Torsten Schaub, Anna Schuhmann
Categories: cs.AI cs.LO
Comments: arXiv admin note: text overlap with arXiv:2304.14778
\\
  In temporal extensions of Answer Set Programming (ASP) based on linear-time,
the behavior of dynamic systems is captured by sequences of states. While this
representation reflects their relative order, it abstracts away the specific
times associated with each state. In many applications, however, timing
constraints are important like, for instance, when planning and scheduling go
hand in hand. We address this by developing a metric extension of linear-time
Dynamic Equilibrium Logic, in which dynamic operators are constrained by
intervals over integers. The resulting Metric Dynamic Equilibrium Logic
provides the foundation of an ASP-based approach for specifying qualitative and
quantitative dynamic constraints. As such, it constitutes the most general
among a whole spectrum of temporal extensions of Equilibrium Logic. In detail,
we show that it encompasses Temporal, Dynamic, Metric, and regular Equilibrium
Logic, as well as its classic counterparts once the law of the excluded middle
is added.
\\ ( https://arxiv.org/abs/2401.10781 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10819
Date: Fri, 19 Jan 2024 17:09:32 GMT   (28473kb,D)

Title: Optimisation in Neurosymbolic Learning Systems
Authors: Emile van Krieken
Categories: cs.AI cs.LG
Comments: PhD dissertation
DOI: 10.5463/thesis.466
\\
  Neurosymbolic AI aims to integrate deep learning with symbolic AI. This
integration has many promises, such as decreasing the amount of data required
to train a neural network, improving the explainability and interpretability of
answers given by models and verifying the correctness of trained systems. We
study neurosymbolic learning, where we have both data and background knowledge
expressed using symbolic languages. How do we connect the symbolic and neural
components to communicate this knowledge? One option is fuzzy reasoning, which
studies degrees of truth. For example, being tall is not a binary concept.
Instead, probabilistic reasoning studies the probability that something is true
or will happen. Our first research question studies how different forms of
fuzzy reasoning combine with learning. We find surprising results like a
connection to the Raven paradox stating we confirm "ravens are black" when we
observe a green apple. In this study, we did not use the background knowledge
when we deployed our models after training. In our second research question, we
studied how to use background knowledge in deployed models. We developed a new
neural network layer based on fuzzy reasoning. Probabilistic reasoning is a
natural fit for neural networks, which we usually train to be probabilistic.
However, they are expensive to compute and do not scale well to large tasks. In
our third research question, we study how to connect probabilistic reasoning
with neural networks by sampling to estimate averages, while in the final
research question, we study scaling probabilistic neurosymbolic learning to
much larger problems than before. Our insight is to train a neural network with
synthetic data to predict the result of probabilistic reasoning.
\\ ( https://arxiv.org/abs/2401.10819 ,  28473kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10286
Date: Tue, 16 Jan 2024 02:11:35 GMT   (168kb,D)

Title: Top in Chinese Data Processing: English Code Models
Authors: Linghan Zheng, Hui Liu, Xiaojun Lin, Jiayuan Dong, Yue Sheng, Gang
  Shi, Zhiwei Liu, Hongwei Chen
Categories: cs.CL cs.AI
\\
  While the alignment between tasks and training corpora is a fundamental
consensus in the application of language models, our series of experiments and
the metrics we designed reveal that code-based Large Language Models (LLMs)
significantly outperform models trained on data that is closely matched to the
tasks in non-coding Chinese tasks. Moreover, in tasks high sensitivity to
Chinese hallucinations, models exhibiting fewer linguistic features of the
Chinese language achieve better performance. Our experimental results can be
easily replicated in Chinese data processing tasks, such as preparing data for
Retrieval-Augmented Generation (RAG), by simply replacing the base model with a
code-based model. Additionally, our research offers a distinct perspective for
discussion on the philosophical "Chinese Room" thought experiment.
\\ ( https://arxiv.org/abs/2401.10286 ,  168kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10352
Date: Thu, 18 Jan 2024 19:42:04 GMT   (10857kb,D)

Title: Bridging Cultural Nuances in Dialogue Agents through Cultural Value
  Surveys
Authors: Yong Cao, Min Chen, Daniel Hershcovich
Categories: cs.CL
Comments: 16pages, 7 figures, EACL 2024 main
\\
  The cultural landscape of interactions with dialogue agents is a compelling
yet relatively unexplored territory. It's clear that various sociocultural
aspects -- from communication styles and beliefs to shared metaphors and
knowledge -- profoundly impact these interactions. To delve deeper into this
dynamic, we introduce cuDialog, a first-of-its-kind benchmark for dialogue
generation with a cultural lens. We also develop baseline models capable of
extracting cultural attributes from dialogue exchanges, with the goal of
enhancing the predictive accuracy and quality of dialogue agents. To
effectively co-learn cultural understanding and multi-turn dialogue
predictions, we propose to incorporate cultural dimensions with dialogue
encoding features. Our experimental findings highlight that incorporating
cultural value surveys boosts alignment with references and cultural markers,
demonstrating its considerable influence on personalization and dialogue
quality. To facilitate further exploration in this exciting domain, we publish
our benchmark publicly accessible at https://github.com/yongcaoplus/cuDialog.
\\ ( https://arxiv.org/abs/2401.10352 ,  10857kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10353
Date: Thu, 18 Jan 2024 19:46:04 GMT   (198kb,D)

Title: Inconsistent dialogue responses and how to recover from them
Authors: Mian Zhang, Lifeng Jin, Linfeng Song, Haitao Mi and Dong Yu
Categories: cs.CL
Comments: Accepted in EACL 2024. Code and dataset available at
  https://github.com/mianzhang/CIDER
\\
  One critical issue for chat systems is to stay consistent about preferences,
opinions, beliefs and facts of itself, which has been shown a difficult
problem. In this work, we study methods to assess and bolster utterance
consistency of chat systems. A dataset is first developed for studying the
inconsistencies, where inconsistent dialogue responses, explanations of the
inconsistencies, and recovery utterances are authored by annotators. This
covers the life span of inconsistencies, namely introduction, understanding,
and resolution. Building on this, we introduce a set of tasks centered on
dialogue consistency, specifically focused on its detection and resolution. Our
experimental findings indicate that our dataset significantly helps the
progress in identifying and resolving conversational inconsistencies, and
current popular large language models like ChatGPT which are good at resolving
inconsistencies however still struggle with detection.
\\ ( https://arxiv.org/abs/2401.10353 ,  198kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10407
Date: Thu, 18 Jan 2024 22:32:31 GMT   (445kb,D)

Title: Learning High-Quality and General-Purpose Phrase Representations
Authors: Lihu Chen and Ga\"el Varoquaux and Fabian M. Suchanek
Categories: cs.CL
Comments: Findings of EACL 2024
\\
  Phrase representations play an important role in data science and natural
language processing, benefiting various tasks like Entity Alignment, Record
Linkage, Fuzzy Joins, and Paraphrase Classification. The current
state-of-the-art method involves fine-tuning pre-trained language models for
phrasal embeddings using contrastive learning. However, we have identified
areas for improvement. First, these pre-trained models tend to be unnecessarily
complex and require to be pre-trained on a corpus with context sentences.
Second, leveraging the phrase type and morphology gives phrase representations
that are both more precise and more flexible. We propose an improved framework
to learn phrase representations in a context-free fashion. The framework
employs phrase type classification as an auxiliary task and incorporates
character-level information more effectively into the phrase representation.
Furthermore, we design three granularities of data augmentation to increase the
diversity of training samples. Our experiments across a wide range of tasks
show that our approach generates superior phrase embeddings compared to
previous methods while requiring a smaller model size. The code is available at
\faGithub~ \url{https://github.com/tigerchen52/PEARL} \end{abstract}
\\ ( https://arxiv.org/abs/2401.10407 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10415
Date: Thu, 18 Jan 2024 23:00:54 GMT   (99kb,D)

Title: Can Large Language Model Summarizers Adapt to Diverse Scientific
  Communication Goals?
Authors: Marcio Fonseca, Shay B. Cohen
Categories: cs.CL cs.AI
\\
  In this work, we investigate the controllability of large language models
(LLMs) on scientific summarization tasks. We identify key stylistic and content
coverage factors that characterize different types of summaries such as paper
reviews, abstracts, and lay summaries. By controlling stylistic features, we
find that non-fine-tuned LLMs outperform humans in the MuP review generation
task, both in terms of similarity to reference summaries and human preferences.
Also, we show that we can improve the controllability of LLMs with
keyword-based classifier-free guidance (CFG) while achieving lexical overlap
comparable to strong fine-tuned baselines on arXiv and PubMed. However, our
results also indicate that LLMs cannot consistently generate long summaries
with more than 8 sentences. Furthermore, these models exhibit limited capacity
to produce highly abstractive lay summaries. Although LLMs demonstrate strong
generic summarization competency, sophisticated content control without costly
fine-tuning remains an open problem for domain-specific applications.
\\ ( https://arxiv.org/abs/2401.10415 ,  99kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10440
Date: Fri, 19 Jan 2024 01:07:50 GMT   (2258kb,D)

Title: Breaking the Curse of Multilinguality with Cross-lingual Expert Language
  Models
Authors: Terra Blevins, Tomasz Limisiewicz, Suchin Gururangan, Margaret Li,
  Hila Gonen, Noah A. Smith, Luke Zettlemoyer
Categories: cs.CL
\\
  Despite their popularity in non-English NLP, multilingual language models
often underperform monolingual ones due to inter-language competition for model
parameters. We propose Cross-lingual Expert Language Models (X-ELM), which
mitigate this competition by independently training language models on subsets
of the multilingual corpus. This process specializes X-ELMs to different
languages while remaining effective as a multilingual ensemble. Our experiments
show that when given the same compute budget, X-ELM outperforms jointly trained
multilingual models across all considered languages and that these gains
transfer to downstream tasks. X-ELM provides additional benefits over
performance improvements: new experts can be iteratively added, adapting X-ELM
to new languages without catastrophic forgetting. Furthermore, training is
asynchronous, reducing the hardware requirements for multilingual training and
democratizing multilingual modeling.
\\ ( https://arxiv.org/abs/2401.10440 ,  2258kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10446
Date: Fri, 19 Jan 2024 01:29:27 GMT   (3222kb,D)

Title: Large Language Models are Efficient Learners of Noise-Robust Speech
  Recognition
Authors: Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang,
  Pin-Yu Chen, EnSiong Chng
Categories: cs.CL cs.AI cs.LG cs.SD eess.AS
Comments: Accepted to ICLR 2024, Spotlight top 5%, 24 pages. This work will be
  open sourced at: https://github.com/YUCHEN005/RobustGER under MIT license
\\
  Recent advances in large language models (LLMs) have promoted generative
error correction (GER) for automatic speech recognition (ASR), which leverages
the rich linguistic knowledge and powerful reasoning ability of LLMs to improve
recognition results. The latest work proposes a GER benchmark with HyPoradise
dataset to learn the mapping from ASR N-best hypotheses to ground-truth
transcription by efficient LLM finetuning, which shows great effectiveness but
lacks specificity on noise-robust ASR. In this work, we extend the benchmark to
noisy conditions and investigate if we can teach LLMs to perform denoising for
GER just like what robust ASR do}, where one solution is introducing noise
information as a conditioner into LLM. However, directly incorporating noise
embeddings from audio encoder could harm the LLM tuning due to cross-modality
gap. To this end, we propose to extract a language-space noise embedding from
the N-best list to represent the noise conditions of source speech, which can
promote the denoising process in GER. Furthermore, in order to enhance its
representation ability of audio noise, we design a knowledge distillation (KD)
approach via mutual information estimation to distill the real noise
information in audio embeddings to our language embedding. Experiments on
various latest LLMs demonstrate our approach achieves a new breakthrough with
up to 53.9% correction improvement in terms of word error rate while with
limited training data. Analysis shows that our language-space noise embedding
can well represent the noise conditions of source speech, under which
off-the-shelf LLMs show strong ability of language-space denoising.
\\ ( https://arxiv.org/abs/2401.10446 ,  3222kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10447
Date: Fri, 19 Jan 2024 01:30:16 GMT   (567kb,D)

Title: Investigating Training Strategies and Model Robustness of Low-Rank
  Adaptation for Language Modeling in Speech Recognition
Authors: Yu Yu, Chao-Han Huck Yang, Tuan Dinh, Sungho Ryu, Jari Kolehmainen,
  Roger Ren, Denis Filimonov, Prashanth G. Shivakumar, Ankur Gandhe, Ariya
  Rastow, Jia Xu, Ivan Bulyko, Andreas Stolcke
Categories: cs.CL cs.AI cs.LG cs.NE cs.SD eess.AS
\\
  The use of low-rank adaptation (LoRA) with frozen pretrained language models
(PLMs) has become increasing popular as a mainstream, resource-efficient
modeling approach for memory-constrained hardware. In this study, we first
explore how to enhance model performance by introducing various LoRA training
strategies, achieving relative word error rate reductions of 3.50\% on the
public Librispeech dataset and of 3.67\% on an internal dataset in the
messaging domain. To further characterize the stability of LoRA-based
second-pass speech recognition models, we examine robustness against input
perturbations. These perturbations are rooted in homophone replacements and a
novel metric called N-best Perturbation-based Rescoring Robustness (NPRR), both
designed to measure the relative degradation in the performance of rescoring
models. Our experimental results indicate that while advanced variants of LoRA,
such as dynamic rank-allocated LoRA, lead to performance degradation in
$1$-best perturbation, they alleviate the degradation in $N$-best perturbation.
This finding is in comparison to fully-tuned models and vanilla LoRA tuning
baselines, suggesting that a comprehensive selection is needed when using
LoRA-based adaptation for compute-cost savings and robust language modeling.
\\ ( https://arxiv.org/abs/2401.10447 ,  567kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10463
Date: Fri, 19 Jan 2024 03:24:36 GMT   (8291kb,D)

Title: Critical Data Size of Language Models from a Grokking Perspective
Authors: Xuekai Zhu, Yao Fu, Bowen Zhou, Zhouhan Lin
Categories: cs.CL cs.AI cs.LG
\\
  We explore the critical data size in language models, a threshold that marks
a fundamental shift from quick memorization to slow generalization. We
formalize the phase transition under the grokking configuration into the Data
Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus
regimes in language models training dynamics. We develop a grokking
configuration to reproduce grokking on simplistic language models stably by
rescaling initialization and weight decay. We show that generalization occurs
only when language models reach a critical size. We analyze grokking across
sample-wise and model-wise, verifying the proposed data efficiency hypothesis.
Our experiments reveal smoother phase transitions occurring at the critical
dataset size for language datasets. As the model size increases, this critical
point also becomes larger, indicating that larger models require more data. Our
results deepen the understanding of language model training, offering a novel
perspective on the role of data in the learning mechanism of language models.
\\ ( https://arxiv.org/abs/2401.10463 ,  8291kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10465
Date: Fri, 19 Jan 2024 03:37:27 GMT   (439kb,D)

Title: Data-driven grapheme-to-phoneme representations for a lexicon-free
  text-to-speech
Authors: Abhinav Garg, Jiyeon Kim, Sushil Khyalia, Chanwoo Kim, Dhananjaya
  Gowda
Categories: cs.CL cs.SD eess.AS
Comments: Accepted at ICASSP 2024
\\
  Grapheme-to-Phoneme (G2P) is an essential first step in any modern,
high-quality Text-to-Speech (TTS) system. Most of the current G2P systems rely
on carefully hand-crafted lexicons developed by experts. This poses a two-fold
problem. Firstly, the lexicons are generated using a fixed phoneme set,
usually, ARPABET or IPA, which might not be the most optimal way to represent
phonemes for all languages. Secondly, the man-hours required to produce such an
expert lexicon are very high. In this paper, we eliminate both of these issues
by using recent advances in self-supervised learning to obtain data-driven
phoneme representations instead of fixed representations. We compare our
lexicon-free approach against strong baselines that utilize a well-crafted
lexicon. Furthermore, we show that our data-driven lexicon-free method performs
as good or even marginally better than the conventional rule-based or
lexicon-based neural G2Ps in terms of Mean Opinion Score (MOS) while using no
prior language lexicon or phoneme set, i.e. no linguistic expertise.
\\ ( https://arxiv.org/abs/2401.10465 ,  439kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10471
Date: Fri, 19 Jan 2024 03:48:27 GMT   (1341kb,D)

Title: DeepEdit: Knowledge Editing as Decoding with Constraints
Authors: Yiwei Wang, Muhao Chen, Nanyun Peng, Kai-Wei Chang
Categories: cs.CL cs.AI
\\
  We develop a new perspective of knowledge editing for large language models
(LLMs) as decoding with constraints. We propose DeepEdit (Depth-first Search
based Progressive Decoding for Knowledge Editing), a neuro-symbolic method that
improves knowledge editing with better coherence of reasoning, relevance to the
question, and awareness of updated knowledge. DeepEdit can be flexibly applied
to all black-box LLMs: it does not require any access to the model parameters,
representations, or output vocabulary distributions. DeepEdit progressively
produces the high-quality reasoning steps towards effective knowledge editing.
It utilizes a depth-first search to revise the LLMs' output, which improves the
output's informativeness to the input question and awareness of the updated
knowledge. Qualitatively, DeepEdit effectively controls LLMs to produce more
succinct reasoning in accord with knowledge editing. Quantitatively, DeepEdit
yields significant gains on MQuaKE, a challenging multi-hop question-answering
dataset with knowledge editing. We release the source code at
https://github.com/wangywUST/DeepEdit.
\\ ( https://arxiv.org/abs/2401.10471 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10472
Date: Fri, 19 Jan 2024 03:49:28 GMT   (2114kb,D)

Title: Name Tagging Under Domain Shift via Metric Learning for Life Sciences
Authors: Hongyi Liu, Qingyun Wang, Payam Karisani, Heng Ji
Categories: cs.CL
Comments: 19 pages
\\
  Name tagging is a key component of Information Extraction (IE), particularly
in scientific domains such as biomedicine and chemistry, where large language
models (LLMs), e.g., ChatGPT, fall short. We investigate the applicability of
transfer learning for enhancing a name tagging model trained in the biomedical
domain (the source domain) to be used in the chemical domain (the target
domain). A common practice for training such a model in a few-shot learning
setting is to pretrain the model on the labeled source data, and then, to
finetune it on a hand-full of labeled target examples. In our experiments we
observed that such a model is prone to mis-labeling the source entities, which
can often appear in the text, as the target entities. To alleviate this
problem, we propose a model to transfer the knowledge from the source domain to
the target domain, however, at the same time, to project the source entities
and target entities into separate regions of the feature space. This diminishes
the risk of mis-labeling the source entities as the target entities. Our model
consists of two stages: 1) entity grouping in the source domain, which
incorporates knowledge from annotated events to establish relations between
entities, and 2) entity discrimination in the target domain, which relies on
pseudo labeling and contrastive learning to enhance discrimination between the
entities in the two domains. We carry out our extensive experiments across
three source and three target datasets, and demonstrate that our method
outperforms the baselines, in some scenarios by 5\% absolute value.
\\ ( https://arxiv.org/abs/2401.10472 ,  2114kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10480
Date: Fri, 19 Jan 2024 04:03:59 GMT   (266kb,D)

Title: Escape Sky-high Cost: Early-stopping Self-Consistency for Multi-step
  Reasoning
Authors: Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin Wang, Bin
  Sun, Heda Wang, Kan Li
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\
  Self-consistency (SC) has been a widely used decoding strategy for
chain-of-thought reasoning. Despite bringing significant performance
improvements across a variety of multi-step reasoning tasks, it is a high-cost
method that requires multiple sampling with the preset size. In this paper, we
propose a simple and scalable sampling process, \textbf{E}arly-Stopping
\textbf{S}elf-\textbf{C}onsistency (ESC), to greatly reduce the cost of SC
without sacrificing performance. On this basis, one control scheme for ESC is
further derivated to dynamically choose the performance-cost balance for
different tasks and models. To demonstrate ESC's effectiveness, we conducted
extensive experiments on three popular categories of reasoning tasks:
arithmetic, commonsense and symbolic reasoning over language models with
varying scales. The empirical results show that ESC reduces the average number
of sampling of chain-of-thought reasoning by a significant margin on six
benchmarks, including MATH (-33.8%), GSM8K (-80.1%), StrategyQA (-76.8%),
CommonsenseQA (-78.5%), Coin Flip (-84.2%) and Last Letters (-67.4%), while
attaining comparable performances.
\\ ( https://arxiv.org/abs/2401.10480 ,  266kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10491
Date: Fri, 19 Jan 2024 05:02:46 GMT   (306kb,D)

Title: Knowledge Fusion of Large Language Models
Authors: Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi
Categories: cs.CL
Comments: Accepted to ICLR 2024
\\
  While training large language models (LLMs) from scratch can generate models
with distinct functionalities and strengths, it comes at significant costs and
may result in redundant capabilities. Alternatively, a cost-effective and
compelling approach is to merge existing pre-trained LLMs into a more potent
model. However, due to the varying architectures of these LLMs, directly
blending their weights is impractical. In this paper, we introduce the notion
of knowledge fusion for LLMs, aimed at combining the capabilities of existing
LLMs and transferring them into a single LLM. By leveraging the generative
distributions of source LLMs, we externalize their collective knowledge and
unique strengths, thereby potentially elevating the capabilities of the target
model beyond those of any individual source LLM. We validate our approach using
three popular LLMs with different architectures--Llama-2, MPT, and
OpenLLaMA--across various benchmarks and tasks. Our findings confirm that the
fusion of LLMs can improve the performance of the target model across a range
of capabilities such as reasoning, commonsense, and code generation. Our code,
model weights, and data are public at
\url{https://github.com/fanqiwan/FuseLLM}.
\\ ( https://arxiv.org/abs/2401.10491 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10506
Date: Fri, 19 Jan 2024 05:48:07 GMT   (1087kb,D)

Title: FinSQL: Model-Agnostic LLMs-based Text-to-SQL Framework for Financial
  Analysis
Authors: Chao Zhang, Yuren Mao, Yijiang Fan, Yu Mi, Yunjun Gao, Lu Chen,
  Dongfang Lou, Jinshu Lin
Categories: cs.CL cs.AI cs.DB
Comments: 13 pages, 13 figures
\\
  Text-to-SQL, which provides zero-code interface for operating relational
databases, has gained much attention in financial analysis; because, financial
professionals may not well-skilled in SQL programming. However, until now,
there is no practical Text-to-SQL benchmark dataset for financial analysis, and
existing Text-to-SQL methods have not considered the unique characteristics of
databases in financial applications, such as commonly existing wide tables. To
address these issues, we collect a practical Text-to-SQL benchmark dataset and
propose a model-agnostic Large Language Model (LLMs)-based Text-to-SQL
framework for financial analysis. The benchmark dataset, BULL, is collected
from the practical financial analysis business of Hundsun Technologies Inc.,
including databases for fund, stock, and macro economy. Besides, the proposed
LLMs-based Text-to-SQL framework, FinSQL, provides a systematic treatment for
financial Text-to-SQL from the perspectives of prompt construction,
parameter-efficient fine-tuning and output calibration. Extensive experimental
results on BULL demonstrate that FinSQL achieves the state-of-the-art
Text-to-SQL performance at a small cost; furthermore, FinSQL can bring up to
36.64% performance improvement in scenarios requiring few-shot cross-database
model transfer.
\\ ( https://arxiv.org/abs/2401.10506 ,  1087kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10521
Date: Fri, 19 Jan 2024 06:54:39 GMT   (8788kb,D)

Title: Cross-lingual Editing in Multilingual Language Models
Authors: Himanshu Beniwal, Kowsik Nandagopan D, Mayank Singh
Categories: cs.CL cs.AI
Comments: Accepted at EACL 2024
\\
  The training of large language models (LLMs) necessitates substantial data
and computational resources, and updating outdated LLMs entails significant
efforts and resources. While numerous model editing techniques (METs) have
emerged to efficiently update model outputs without retraining, their
effectiveness in multilingual LLMs, where knowledge is stored in diverse
languages, remains an underexplored research area. This research paper
introduces the cross-lingual model editing (\textbf{XME}) paradigm, wherein a
fact is edited in one language, and the subsequent update propagation is
observed across other languages. To investigate the XME paradigm, we conducted
experiments using BLOOM, mBERT, and XLM-RoBERTa using the two writing scripts:
\textit{Latin} (English, French, and Spanish) and \textit{Indic} (Hindi,
Gujarati, and Bengali). The results reveal notable performance limitations of
state-of-the-art METs under the XME setting, mainly when the languages involved
belong to two distinct script families. These findings highlight the need for
further research and development of XME techniques to address these challenges.
For more comprehensive information, the dataset used in this research and the
associated code are publicly available at the following
URL\url{https://github.com/lingo-iitgn/XME}.
\\ ( https://arxiv.org/abs/2401.10521 ,  8788kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10535
Date: Fri, 19 Jan 2024 07:21:45 GMT   (729kb,D)

Title: The "Colonial Impulse" of Natural Language Processing: An Audit of
  Bengali Sentiment Analysis Tools and Their Identity-based Biases
Authors: Dipto Das and Shion Guha and Jed Brubaker and Bryan Semaan
Categories: cs.CL cs.CY cs.HC cs.LG
\\
  While colonization has sociohistorically impacted people's identities across
various dimensions, those colonial values and biases continue to be perpetuated
by sociotechnical systems. One category of sociotechnical systems--sentiment
analysis tools--can also perpetuate colonial values and bias, yet less
attention has been paid to how such tools may be complicit in perpetuating
coloniality, although they are often used to guide various practices (e.g.,
content moderation). In this paper, we explore potential bias in sentiment
analysis tools in the context of Bengali communities that have experienced and
continue to experience the impacts of colonialism. Drawing on identity
categories most impacted by colonialism amongst local Bengali communities, we
focused our analytic attention on gender, religion, and nationality. We
conducted an algorithmic audit of all sentiment analysis tools for Bengali,
available on the Python package index (PyPI) and GitHub. Despite similar
semantic content and structure, our analyses showed that in addition to
inconsistencies in output from different tools, Bengali sentiment analysis
tools exhibit bias between different identity categories and respond
differently to different ways of identity expression. Connecting our findings
with colonially shaped sociocultural structures of Bengali communities, we
discuss the implications of downstream bias of sentiment analysis tools.
\\ ( https://arxiv.org/abs/2401.10535 ,  729kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10536
Date: Fri, 19 Jan 2024 07:30:57 GMT   (348kb,D)

Title: Speech Swin-Transformer: Exploring a Hierarchical Transformer with
  Shifted Windows for Speech Emotion Recognition
Authors: Yong Wang, Cheng Lu, Hailun Lian, Yan Zhao, Bj\"orn Schuller, Yuan
  Zong, Wenming Zheng
Categories: cs.CL
Comments: Accepted by ICASSP 2024
\\
  Swin-Transformer has demonstrated remarkable success in computer vision by
leveraging its hierarchical feature representation based on Transformer. In
speech signals, emotional information is distributed across different scales of
speech features, e.\,g., word, phrase, and utterance. Drawing above
inspiration, this paper presents a hierarchical speech Transformer with shifted
windows to aggregate multi-scale emotion features for speech emotion
recognition (SER), called Speech Swin-Transformer. Specifically, we first
divide the speech spectrogram into segment-level patches in the time domain,
composed of multiple frame patches. These segment-level patches are then
encoded using a stack of Swin blocks, in which a local window Transformer is
utilized to explore local inter-frame emotional information across frame
patches of each segment patch. After that, we also design a shifted window
Transformer to compensate for patch correlations near the boundaries of segment
patches. Finally, we employ a patch merging operation to aggregate
segment-level emotional features for hierarchical speech representation by
expanding the receptive field of Transformer from frame-level to segment-level.
Experimental results demonstrate that our proposed Speech Swin-Transformer
outperforms the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.10536 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10567
Date: Fri, 19 Jan 2024 09:13:28 GMT   (111kb)

Title: Self-training from Self-memory in Data-to-text Generation
Authors: Hoang-Thang Ta
Categories: cs.CL
Comments: 14 pages
\\
  This paper introduces a novel training model, self-training from self-memory
(STSM) in data-to-text generation (DTG), allowing the model to self-train on
subsets, including self-memory as outputs inferred directly from the trained
models and/or the new data. The quality of self-memory is validated by two
models, data-to-text (D2T) and text-to-data (T2D), by two pre-defined
conditions: (1) the appearance of all source values in the outputs of the D2T
model and (2) the ability to convert back to source data in the outputs in the
T2D model. We utilize a greedy algorithm to generate shorter D2T outputs if
they contain all source values. Subsequently, we use the T2D model to confirm
that these outputs can capture input relationships by demonstrating their
capacity to convert text back into data. With 30% of the dataset, we can train
the D2T model with a competitive performance compared to full training in the
same setup. We experiment with our model on two datasets, E2E NLG and DART.
STSM offers the D2T model a generalization capability from its subset memory
while reducing training data volume. Ultimately, we anticipate that this paper
will contribute to continual learning solutions that adapt to new training
data, incorporating it as a form of self-memory in DTG tasks. The curated
dataset is publicly available at: https://github.com/hoangthangta/STSM.
\\ ( https://arxiv.org/abs/2401.10567 ,  111kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10580
Date: Fri, 19 Jan 2024 09:46:08 GMT   (547kb,D)

Title: PHOENIX: Open-Source Language Adaption for Direct Preference
  Optimization
Authors: Matthias Uhlig, Sigurd Schacht, Sudarshan Kamath Barkur
Categories: cs.CL
\\
  Large language models have gained immense importance in recent years and have
demonstrated outstanding results in solving various tasks. However, despite
these achievements, many questions remain unanswered in the context of large
language models. Besides the optimal use of the models for inference and the
alignment of the results to the desired specifications, the transfer of models
to other languages is still an underdeveloped area of research. The recent
publication of models such as Llama-2 and Zephyr has provided new insights into
architectural improvements and the use of human feedback. However, insights
into adapting these techniques to other languages remain scarce. In this paper,
we build on latest improvements and apply the Direct Preference
Optimization(DPO) approach to the German language. The model is available at
https://huggingface.co/DRXD1000/Phoenix.
\\ ( https://arxiv.org/abs/2401.10580 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10647
Date: Fri, 19 Jan 2024 11:48:09 GMT   (483kb,D)

Title: Sowing the Wind, Reaping the Whirlwind: The Impact of Editing Language
  Models
Authors: Rima Hazra, Sayan Layek, Somnath Banerjee, Soujanya Poria
Categories: cs.CL
\\
  In the rapidly advancing field of artificial intelligence, the concept of
Red-Teaming or Jailbreaking large language models (LLMs) has emerged as a
crucial area of study. This approach is especially significant in terms of
assessing and enhancing the safety and robustness of these models. This paper
investigates the intricate consequences of such modifications through model
editing, uncovering a complex relationship between enhancing model accuracy and
preserving its ethical integrity. Our in-depth analysis reveals a striking
paradox: while injecting accurate information is crucial for model reliability,
it can paradoxically destabilize the model's foundational framework, resulting
in unpredictable and potentially unsafe behaviors. Additionally, we propose a
benchmark dataset NicheHazardQA to investigate this unsafe behavior both within
the same and cross topical domain. This aspect of our research sheds light on
how the edits, impact the model's safety metrics and guardrails. Our findings
show that model editing serves as a cost-effective tool for topical red-teaming
by methodically applying targeted edits and evaluating the resultant model
behavior
\\ ( https://arxiv.org/abs/2401.10647 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10653
Date: Fri, 19 Jan 2024 11:59:13 GMT   (2909kb,D)

Title: Attentive Fusion: A Transformer-based Approach to Multimodal Hate Speech
  Detection
Authors: Atanu Mandal, Gargi Roy, Amit Barman, Indranil Dutta, Sudip Kumar
  Naskar
Categories: cs.CL cs.LG cs.SD eess.AS eess.SP
Comments: Accepted in 20th International Conference on Natural Language
  Processing (ICON)
\\
  With the recent surge and exponential growth of social media usage,
scrutinizing social media content for the presence of any hateful content is of
utmost importance. Researchers have been diligently working since the past
decade on distinguishing between content that promotes hatred and content that
does not. Traditionally, the main focus has been on analyzing textual content.
However, recent research attempts have also commenced into the identification
of audio-based content. Nevertheless, studies have shown that relying solely on
audio or text-based content may be ineffective, as recent upsurge indicates
that individuals often employ sarcasm in their speech and writing. To overcome
these challenges, we present an approach to identify whether a speech promotes
hate or not utilizing both audio and textual representations. Our methodology
is based on the Transformer framework that incorporates both audio and text
sampling, accompanied by our very own layer called "Attentive Fusion". The
results of our study surpassed previous state-of-the-art techniques, achieving
an impressive macro F1 score of 0.927 on the Test Set.
\\ ( https://arxiv.org/abs/2401.10653 ,  2909kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10660
Date: Fri, 19 Jan 2024 12:26:57 GMT   (4095kb,D)

Title: A Simple Framework to Accelerate Multilingual Language Model for
  Monolingual Text Generation
Authors: Jimin Hong and Gibbeum Lee and Jaewoong Cho
Categories: cs.CL cs.AI
\\
  Recent advancements in large language models have facilitated the execution
of complex language tasks, not only in English but also in non-English
languages. However, the tokenizers of most language models, such as Llama,
trained on English-centric corpora, tend to excessively fragment tokens in
non-English languages. This issue is especially pronounced in non-roman
alphabetic languages, which are often divided at a character or even Unicode
level, leading to slower text generation. To address this, our study introduces
a novel framework designed to expedite text generation in these languages. This
framework predicts larger linguistic units than those of conventional
multilingual tokenizers and is specifically tailored to the target language,
thereby reducing the number of decoding steps required. Our empirical results
demonstrate that the proposed framework increases the generation speed by a
factor of 1.9 compared to standard decoding while maintaining the performance
of a pre-trained multilingual model on monolingual tasks.
\\ ( https://arxiv.org/abs/2401.10660 ,  4095kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10695
Date: Fri, 19 Jan 2024 14:00:19 GMT   (8542kb,D)

Title: LangBridge: Multilingual Reasoning Without Multilingual Supervision
Authors: Dongkeun Yoon, Joel Jang, Sungdong Kim, Seungone Kim, Sheikh Shafayat,
  Minjoon Seo
Categories: cs.CL
Comments: Work in progress
\\
  We introduce LangBridge, a zero-shot approach to adapt language models for
multilingual reasoning tasks without multilingual supervision. LangBridge
operates by bridging two models, each specialized in different aspects: (1) one
specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one
specialized in reasoning (e.g., Orca 2). LangBridge connects the two models by
introducing minimal trainable parameters between them. Despite utilizing only
English data for training, LangBridge considerably enhances the performance of
language models on low-resource languages across mathematical reasoning,
coding, and logical reasoning. Our analysis suggests that the efficacy of
LangBridge stems from the language-agnostic characteristics of multilingual
representations. We publicly release our code and models.
\\ ( https://arxiv.org/abs/2401.10695 ,  8542kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10716
Date: Fri, 19 Jan 2024 14:27:44 GMT   (183kb,D)

Title: Structured Code Representations Enable Data-Efficient Adaptation of Code
  Language Models
Authors: Mayank Agarwal, Yikang Shen, Bailin Wang, Yoon Kim, Jie Chen
Categories: cs.CL
\\
  Current language models tailored for code tasks often adopt the
pre-training-then-fine-tuning paradigm from natural language processing,
modeling source code as plain text. This approach, however, overlooks the
unambiguous structures inherent in programming languages. In this work, we
explore data-efficient adaptation of pre-trained code models by further
pre-training and fine-tuning them with program structures. Specifically, we
represent programs as parse trees -- also known as concrete syntax trees (CSTs)
-- and adapt pre-trained models on serialized CSTs. Although the models that we
adapt have been pre-trained only on the surface form of programs, we find that
a small amount of continual pre-training and fine-tuning on CSTs without
changing the model architecture yields improvements over the baseline approach
across various code tasks. The improvements are found to be particularly
significant when there are limited training examples, demonstrating the
effectiveness of integrating program structures with plain-text representation
even when working with backbone models that have not been pre-trained with
structures.
\\ ( https://arxiv.org/abs/2401.10716 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10768
Date: Fri, 19 Jan 2024 15:39:49 GMT   (8021kb,D)

Title: Mitigating Hallucinations of Large Language Models via Knowledge
  Consistent Alignment
Authors: Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, Shuming
  Shi
Categories: cs.CL
Comments: Work in progress
\\
  While Large Language Models (LLMs) have proven to be exceptional on a variety
of tasks after alignment, they may still produce responses that contradict the
context or world knowledge confidently, a phenomenon known as
``hallucination''. In this paper, we demonstrate that reducing the
inconsistency between the external knowledge encapsulated in the training data
and the intrinsic knowledge inherited in the pretraining corpus could mitigate
hallucination in alignment. Specifically, we introduce a novel knowledge
consistent alignment (KCA) approach, which involves automatically formulating
examinations based on external knowledge for accessing the comprehension of
LLMs. For data encompassing knowledge inconsistency, KCA implements several
simple yet efficient strategies for processing. We illustrate the superior
performance of the proposed KCA approach in mitigating hallucinations across
six benchmarks using LLMs of different backbones and scales. Furthermore, we
confirm the correlation between knowledge inconsistency and hallucination,
signifying the effectiveness of reducing knowledge inconsistency in alleviating
hallucinations. Our code, model weights, and data are public at
\url{https://github.com/fanqiwan/KCA}.
\\ ( https://arxiv.org/abs/2401.10768 ,  8021kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10825
Date: Fri, 19 Jan 2024 17:21:05 GMT   (2497kb,D)

Title: A survey on recent advances in named entity recognition
Authors: Imed Keraghel and Stanislas Morbieu and Mohamed Nadif
Categories: cs.CL cs.LG
Comments: 30 pages
MSC-class: 68T50, 68Q32
\\
  Named Entity Recognition seeks to extract substrings within a text that name
real-world objects and to determine their type (for example, whether they refer
to persons or organizations). In this survey, we first present an overview of
recent popular approaches, but we also look at graph- and transformer- based
methods including Large Language Models (LLMs) that have not had much coverage
in other surveys. Second, we focus on methods designed for datasets with scarce
annotations. Third, we evaluate the performance of the main NER implementations
on a variety of datasets with differing characteristics (as regards their
domain, their size, and their number of classes). We thus provide a deep
comparison of algorithms that are never considered together. Our experiments
shed some light on how the characteristics of datasets affect the behavior of
the methods that we compare.
\\ ( https://arxiv.org/abs/2401.10825 ,  2497kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10841
Date: Fri, 19 Jan 2024 17:40:50 GMT   (1336kb,D)

Title: Using LLMs to discover emerging coded antisemitic hate-speech emergence
  in extremist social media
Authors: Dhanush Kikkisetti, Raza Ul Mustafa, Wendy Melillo, Roberto Corizzo,
  Zois Boukouvalas, Jeff Gill and Nathalie Japkowicz
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: 9 pages, 4 figures, 2 algorithms, 3 tables
\\
  Online hate speech proliferation has created a difficult problem for social
media platforms. A particular challenge relates to the use of coded language by
groups interested in both creating a sense of belonging for its users and
evading detection. Coded language evolves quickly and its use varies over time.
This paper proposes a methodology for detecting emerging coded hate-laden
terminology. The methodology is tested in the context of online antisemitic
discourse. The approach considers posts scraped from social media platforms,
often used by extremist users. The posts are scraped using seed expressions
related to previously known discourse of hatred towards Jews. The method begins
by identifying the expressions most representative of each post and calculating
their frequency in the whole corpus. It filters out grammatically incoherent
expressions as well as previously encountered ones so as to focus on emergent
well-formed terminology. This is followed by an assessment of semantic
similarity to known antisemitic terminology using a fine-tuned large language
model, and subsequent filtering out of the expressions that are too distant
from known expressions of hatred. Emergent antisemitic expressions containing
terms clearly relating to Jewish topics are then removed to return only coded
expressions of hatred.
\\ ( https://arxiv.org/abs/2401.10841 ,  1336kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10850
Date: Fri, 19 Jan 2024 17:51:11 GMT   (131kb,D)

Title: Advancements in eHealth Data Analytics through Natural Language
  Processing and Deep Learning
Authors: Elena-Simona Apostol and Ciprian-Octavian Truic\u{a}
Categories: cs.CL cs.AI
\\
  The healthcare environment is commonly referred to as "information-rich" but
also "knowledge poor". Healthcare systems collect huge amounts of data from
various sources: lab reports, medical letters, logs of medical tools or
programs, medical prescriptions, etc. These massive sets of data can provide
great knowledge and information that can improve the medical services, and
overall the healthcare domain, such as disease prediction by analyzing the
patient's symptoms or disease prevention, by facilitating the discovery of
behavioral factors for diseases. Unfortunately, only a relatively small volume
of the textual eHealth data is processed and interpreted, an important factor
being the difficulty in efficiently performing Big Data operations. In the
medical field, detecting domain-specific multi-word terms is a crucial task as
they can define an entire concept with a few words. A term can be defined as a
linguistic structure or a concept, and it is composed of one or more words with
a specific meaning to a domain. All the terms of a domain create its
terminology. This chapter offers a critical study of the current, most
performant solutions for analyzing unstructured (image and textual) eHealth
data. This study also provides a comparison of the current Natural Language
Processing and Deep Learning techniques in the eHealth context. Finally, we
examine and discuss some of the current issues, and we define a set of research
directions in this area.
\\ ( https://arxiv.org/abs/2401.10850 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10882
Date: Fri, 19 Jan 2024 18:49:36 GMT   (1311kb,D)

Title: Reinforcement learning for question answering in programming domain
  using public community scoring as a human feedback
Authors: Alexey Gorbatovski and Sergey Kovalchuk
Categories: cs.CL cs.AI cs.HC
\\
  In this study, we investigate the enhancement of the GPT Neo 125M performance
in Community Question Answering (CQA) with a focus on programming, through the
integration of Reinforcement Learning from Human Feedback (RLHF) and the
utilization of scores from Stack Overflow. Two distinct reward model training
strategies are employed for fine-tuning with Proximal Policy Optimization
(PPO). Notably, the improvements in performance achieved through this method
are comparable to those of GPT Neo 2.7B parameter variant. Additionally, an
auxiliary scoring mechanism is introduced, which demonstrates the limitations
of conventional linguistic metrics in evaluating responses in the programming
domain. Through accurate analysis, this paper looks at the divergence between
traditional linguistic metrics and our human-preferences-based reward model,
underscoring the imperative for domain-specific evaluation methods. By
elucidating the complexities involved in applying RLHF to programming CQA and
accentuating the significance of context-aware evaluation, this study
contributes to the ongoing efforts in refining Large Language Models through
focused human feedback.
\\ ( https://arxiv.org/abs/2401.10882 ,  1311kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10266
Date: Wed, 3 Jan 2024 21:35:03 GMT   (907kb,D)

Title: Intelligent Condition Monitoring of Industrial Plants: An Overview of
  Methodologies and Uncertainty Management Strategies
Authors: Maryam Ahang, Todd Charter, Oluwaseyi Ogunfowora, Maziyar Khadivi,
  Mostafa Abbasi, Homayoun Najjaran
Categories: cs.LG cs.AI cs.SY eess.SP eess.SY
\\
  Condition monitoring plays a significant role in the safety and reliability
of modern industrial systems. Artificial intelligence (AI) approaches are
gaining attention from academia and industry as a growing subject in industrial
applications and as a powerful way of identifying faults. This paper provides
an overview of intelligent condition monitoring and fault detection and
diagnosis methods for industrial plants with a focus on the open-source
benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and
state-of-the-art deep learning (DL) and machine learning (ML) algorithms for
industrial plant condition monitoring, fault detection, and diagnosis are
summarized and the advantages and disadvantages of each algorithm are studied.
Challenges like imbalanced data, unlabelled samples and how deep learning
models can handle them are also covered. Finally, a comparison of the
accuracies and specifications of different algorithms utilizing the Tennessee
Eastman Process (TEP) is conducted. This research will be beneficial for both
researchers who are new to the field and experts, as it covers the literature
on condition monitoring and state-of-the-art methods alongside the challenges
and possible solutions to them.
\\ ( https://arxiv.org/abs/2401.10266 ,  907kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10287
Date: Tue, 16 Jan 2024 08:51:58 GMT   (36kb,D)

Title: Open-Source Fermionic Neural Networks with Ionic Charge Initialization
Authors: Shai Pranesh, Shang Zhu, Venkat Viswanathan, Bharath Ramsundar
Categories: cs.LG physics.chem-ph
Comments: Accepted at 3rd Annual AAAI Workshop on AI to Accelerate Science and
  Engineering (AI2ASE)
\\
  Finding accurate solutions to the electronic Schr\"odinger equation plays an
important role in discovering important molecular and material energies and
characteristics. Consequently, solving systems with large numbers of electrons
has become increasingly important. Variational Monte Carlo (VMC) methods,
especially those approximated through deep neural networks, are promising in
this regard. In this paper, we aim to integrate one such model called the
FermiNet, a post-Hartree-Fock (HF) Deep Neural Network (DNN) model, into a
standard and widely used open source library, DeepChem. We also propose novel
initialization techniques to overcome the difficulties associated with the
assignment of excess or lack of electrons for ions.
\\ ( https://arxiv.org/abs/2401.10287 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10288
Date: Wed, 17 Jan 2024 03:57:36 GMT   (6472kb,D)

Title: CLAN: A Contrastive Learning based Novelty Detection Framework for Human
  Activity Recognition
Authors: Hyunju Kim and Dongman Lee
Categories: cs.LG eess.SP
\\
  In ambient assisted living, human activity recognition from time series
sensor data mainly focuses on predefined activities, often overlooking new
activity patterns. We propose CLAN, a two-tower contrastive learning-based
novelty detection framework with diverse types of negative pairs for human
activity recognition. It is tailored to challenges with human activity
characteristics, including the significance of temporal and frequency features,
complex activity dynamics, shared features across activities, and sensor
modality variations. The framework aims to construct invariant representations
of known activity robust to the challenges. To generate suitable negative
pairs, it selects data augmentation methods according to the temporal and
frequency characteristics of each dataset. It derives the key representations
against meaningless dynamics by contrastive and classification losses-based
representation learning and score function-based novelty detection that
accommodate dynamic numbers of the different types of augmented samples. The
proposed two-tower model extracts the representations in terms of time and
frequency, mutually enhancing expressiveness for distinguishing between new and
known activities, even when they share common features. Experiments on four
real-world human activity datasets show that CLAN surpasses the best
performance of existing novelty detection methods, improving by 8.3%, 13.7%,
and 53.3% in AUROC, balanced accuracy, and FPR@TPR0.95 metrics respectively.
\\ ( https://arxiv.org/abs/2401.10288 ,  6472kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10290
Date: Wed, 17 Jan 2024 05:17:40 GMT   (554kb,D)

Title: Early Prediction of Geomagnetic Storms by Machine Learning Algorithms
Authors: Iris Yan
Categories: cs.LG
Comments: 14 pages, 7 figures
\\
  Geomagnetic storms (GS) occur when solar winds disrupt Earth's magnetosphere.
GS can cause severe damages to satellites, power grids, and communication
infrastructures. Estimate of direct economic impacts of a large scale GS
exceeds $40 billion a day in the US. Early prediction is critical in preventing
and minimizing the hazards. However, current methods either predict several
hours ahead but fail to identify all types of GS, or make predictions within
short time, e.g., one hour ahead of the occurrence. This work aims to predict
all types of geomagnetic storms reliably and as early as possible using big
data and machine learning algorithms. By fusing big data collected from
multiple ground stations in the world on different aspects of solar
measurements and using Random Forests regression with feature selection and
downsampling on minor geomagnetic storm instances (which carry majority of the
data), we are able to achieve an accuracy of 82.55% on data collected in 2021
when making early predictions three hours in advance. Given that important
predictive features such as historic Kp indices are measured every 3 hours and
their importance decay quickly with the amount of time in advance, an early
prediction of 3 hours ahead of time is believed to be close to the practical
limit.
\\ ( https://arxiv.org/abs/2401.10290 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10299
Date: Thu, 18 Jan 2024 06:26:44 GMT   (1541kb)

Title: An attempt to generate new bridge types from latent space of generative
  flow
Authors: Hongjun Zhang
Categories: cs.LG cs.AI cs.CV
Comments: 10 pages, 11 figures
\\
  Through examples of coordinate and probability transformation between
different distributions, the basic principle of normalizing flow is introduced
in a simple and concise manner. From the perspective of the distribution of
random variable function, the essence of probability transformation is
explained, and the scaling factor Jacobian determinant of probability
transformation is introduced. Treating the dataset as a sample from the
population, obtaining normalizing flow is essentially through sampling surveys
to statistically infer the numerical features of the population, and then the
loss function is established by using the maximum likelihood estimation method.
This article introduces how normalizing flow cleverly solves the two major
application challenges of high-dimensional matrix determinant calculation and
neural network reversible transformation. Using symmetric structured image
dataset of three-span beam bridge, arch bridge, cable-stayed bridge and
suspension bridge, constructing and training normalizing flow based on the Glow
API in the TensorFlow Probability library. The model can smoothly transform the
complex distribution of the bridge dataset into a standard normal distribution,
and from the obtained latent space sampling, it can generate new bridge types
that are different from the training dataset.
\\ ( https://arxiv.org/abs/2401.10299 ,  1541kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10304
Date: Thu, 18 Jan 2024 12:11:27 GMT   (2225kb,D)

Title: On the Readiness of Scientific Data for a Fair and Transparent Use in
  Machine Learning
Authors: Joan Giner-Miguelez, Abel G\'omez, Jordi Cabot
Categories: cs.LG cs.AI cs.DL
\\
  To ensure the fairness and trustworthiness of machine learning (ML) systems,
recent legislative initiatives and relevant research in the ML community have
pointed out the need to document the data used to train ML models. Besides,
data-sharing practices in many scientific domains have evolved in recent years
for reproducibility purposes. In this sense, the adoption of these practices by
academic institutions has encouraged researchers to publish their data and
technical documentation in peer-reviewed publications such as data papers. In
this study, we analyze how this scientific data documentation meets the needs
of the ML community and regulatory bodies for its use in ML technologies. We
examine a sample of 4041 data papers of different domains, assessing their
completeness and coverage of the requested dimensions, and trends in recent
years, putting special emphasis on the most and least documented dimensions. As
a result, we propose a set of recommendation guidelines for data creators and
scientific data publishers to increase their data's preparedness for its
transparent and fairer use in ML technologies.
\\ ( https://arxiv.org/abs/2401.10304 ,  2225kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10310
Date: Thu, 18 Jan 2024 15:32:38 GMT   (104kb,D)

Title: Mathematical Algorithm Design for Deep Learning under Societal and
  Judicial Constraints: The Algorithmic Transparency Requirement
Authors: Holger Boche, Adalbert Fono, Gitta Kutyniok
Categories: cs.LG cs.AI cs.CC
\\
  Deep learning still has drawbacks in terms of trustworthiness, which
describes a comprehensible, fair, safe, and reliable method. To mitigate the
potential risk of AI, clear obligations associated to trustworthiness have been
proposed via regulatory guidelines, e.g., in the European AI Act. Therefore, a
central question is to what extent trustworthy deep learning can be realized.
Establishing the described properties constituting trustworthiness requires
that the factors influencing an algorithmic computation can be retraced, i.e.,
the algorithmic implementation is transparent. Motivated by the observation
that the current evolution of deep learning models necessitates a change in
computing technology, we derive a mathematical framework which enables us to
analyze whether a transparent implementation in a computing model is feasible.
We exemplarily apply our trustworthiness framework to analyze deep learning
approaches for inverse problems in digital and analog computing models
represented by Turing and Blum-Shub-Smale Machines, respectively. Based on
previous results, we find that Blum-Shub-Smale Machines have the potential to
establish trustworthy solvers for inverse problems under fairly general
conditions, whereas Turing machines cannot guarantee trustworthiness to the
same degree.
\\ ( https://arxiv.org/abs/2401.10310 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10337
Date: Thu, 18 Jan 2024 19:02:00 GMT   (928kb,D)

Title: Noise Contrastive Estimation-based Matching Framework for Low-resource
  Security Attack Pattern Recognition
Authors: Tu Nguyen, Nedim Srndic, Alexander Neth
Categories: cs.LG cs.AI cs.CL cs.CR
Comments: accepted at EACL 2024, in ARR October 2023
\\
  Tactics, Techniques and Procedures (TTPs) represent sophisticated attack
patterns in the cybersecurity domain, described encyclopedically in textual
knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP
mapping, is an important and challenging task. Conventional learning approaches
often target the problem in the classical multi-class or multilabel
classification setting. This setting hinders the learning ability of the model
due to a large number of classes (i.e., TTPs), the inevitable skewness of the
label distribution and the complex hierarchical structure of the label space.
We formulate the problem in a different learning paradigm, where the assignment
of a text to a TTP label is decided by the direct semantic similarity between
the two, thus reducing the complexity of competing solely over the large
labeling space. To that end, we propose a neural matching architecture with an
effective sampling-based learn-to-compare mechanism, facilitating the learning
process of the matching model despite constrained resources.
\\ ( https://arxiv.org/abs/2401.10337 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10338
Date: Thu, 18 Jan 2024 19:02:41 GMT   (1656kb)

Title: MELODY: Robust Semi-Supervised Hybrid Model for Entity-Level Online
  Anomaly Detection with Multivariate Time Series
Authors: Jingchao Ni, Gauthier Guinet, Peihong Jiang, Laurent Callot, Andrey
  Kan
Categories: cs.LG
\\
  In large IT systems, software deployment is a crucial process in online
services as their code is regularly updated. However, a faulty code change may
degrade the target service's performance and cause cascading outages in
downstream services. Thus, software deployments should be comprehensively
monitored, and their anomalies should be detected timely. In this paper, we
study the problem of anomaly detection for deployments. We begin by identifying
the challenges unique to this anomaly detection problem, which is at
entity-level (e.g., deployments), relative to the more typical problem of
anomaly detection in multivariate time series (MTS). The unique challenges
include the heterogeneity of deployments, the low latency tolerance, the
ambiguous anomaly definition, and the limited supervision. To address them, we
propose a novel framework, semi-supervised hybrid Model for Entity-Level Online
Detection of anomalY (MELODY). MELODY first transforms the MTS of different
entities to the same feature space by an online feature extractor, then uses a
newly proposed semi-supervised deep one-class model for detecting anomalous
entities. We evaluated MELODY on real data of cloud services with 1.2M+ time
series. The relative F1 score improvement of MELODY over the state-of-the-art
methods ranges from 7.6% to 56.5%. The user evaluation suggests MELODY is
suitable for monitoring deployments in large online systems.
\\ ( https://arxiv.org/abs/2401.10338 ,  1656kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10361
Date: Thu, 18 Jan 2024 20:05:34 GMT   (10050kb,D)

Title: Hierarchical Federated Learning in Multi-hop Cluster-Based VANETs
Authors: M. Saeid HaghighiFard and Sinem Coleri
Categories: cs.LG cs.AI cs.DC cs.NI cs.SY eess.SY
\\
  The usage of federated learning (FL) in Vehicular Ad hoc Networks (VANET) has
garnered significant interest in research due to the advantages of reducing
transmission overhead and protecting user privacy by communicating local
dataset gradients instead of raw data. However, implementing FL in VANETs faces
challenges, including limited communication resources, high vehicle mobility,
and the statistical diversity of data distributions. In order to tackle these
issues, this paper introduces a novel framework for hierarchical federated
learning (HFL) over multi-hop clustering-based VANET. The proposed method
utilizes a weighted combination of the average relative speed and cosine
similarity of FL model parameters as a clustering metric to consider both data
diversity and high vehicle mobility. This metric ensures convergence with
minimum changes in cluster heads while tackling the complexities associated
with non-independent and identically distributed (non-IID) data scenarios.
Additionally, the framework includes a novel mechanism to manage seamless
transitions of cluster heads (CHs), followed by transferring the most recent FL
model parameter to the designated CH. Furthermore, the proposed approach
considers the option of merging CHs, aiming to reduce their count and,
consequently, mitigate associated overhead. Through extensive simulations, the
proposed hierarchical federated learning over clustered VANET has been
demonstrated to improve accuracy and convergence time significantly while
maintaining an acceptable level of packet overhead compared to previously
proposed clustering algorithms and non-clustered VANET.
\\ ( https://arxiv.org/abs/2401.10361 ,  10050kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10364
Date: Thu, 18 Jan 2024 20:14:10 GMT   (1234kb)

Title: Using LLM such as ChatGPT for Designing and Implementing a RISC
  Processor: Execution,Challenges and Limitations
Authors: Shadeeb Hossain, Aayush Gohil, Yizhou Wang
Categories: cs.LG cs.AR cs.SE
\\
  This paper discusses the feasibility of using Large Language Models LLM for
code generation with a particular application in designing an RISC. The paper
also reviews the associated steps such as parsing, tokenization, encoding,
attention mechanism, sampling the tokens and iterations during code generation.
The generated code for the RISC components is verified through testbenches and
hardware implementation on a FPGA board. Four metric parameters Correct output
on the first iteration, Number of errors embedded in the code, Number of trials
required to achieve the code and Failure to generate the code after three
iterations, are used to compare the efficiency of using LLM in programming. In
all the cases, the generated code had significant errors and human intervention
was always required to fix the bugs. LLM can therefore be used to complement a
programmer code design.
\\ ( https://arxiv.org/abs/2401.10364 ,  1234kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10371
Date: Thu, 18 Jan 2024 20:35:47 GMT   (737kb,D)

Title: Langevin Unlearning: A New Perspective of Noisy Gradient Descent for
  Machine Unlearning
Authors: Eli Chien, Haoyu Wang, Ziang Chen, Pan Li
Categories: cs.LG
\\
  Machine unlearning has raised significant interest with the adoption of laws
ensuring the ``right to be forgotten''. Researchers have provided a
probabilistic notion of approximate unlearning under a similar definition of
Differential Privacy (DP), where privacy is defined as statistical
indistinguishability to retraining from scratch. We propose Langevin
unlearning, an unlearning framework based on noisy gradient descent with
privacy guarantees for approximate unlearning problems. Langevin unlearning
unifies the DP learning process and the privacy-certified unlearning process
with many algorithmic benefits. These include approximate certified unlearning
for non-convex problems, complexity saving compared to retraining, sequential
and batch unlearning for multiple unlearning requests. We verify the
practicality of Langevin unlearning by studying its privacy-utility-complexity
trade-off via experiments on benchmark datasets, and also demonstrate its
superiority against gradient-decent-plus-output-perturbation based approximate
unlearning.
\\ ( https://arxiv.org/abs/2401.10371 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10383
Date: Thu, 18 Jan 2024 21:36:17 GMT   (1053kb,D)

Title: Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret Analysis
Authors: Phevos Paschalidis, Runyu Zhang, and Na Li
Categories: cs.LG cs.MA stat.ML
\\
  In this paper, we formulate the multi-agent graph bandit problem as a
multi-agent extension of the graph bandit problem introduced by Zhang,
Johansson, and Li [CISS 57, 1-6 (2023)]. In our formulation, $N$ cooperative
agents travel on a connected graph $G$ with $K$ nodes. Upon arrival at each
node, agents observe a random reward drawn from a node-dependent probability
distribution. The reward of the system is modeled as a weighted sum of the
rewards the agents observe, where the weights capture the decreasing marginal
reward associated with multiple agents sampling the same node at the same time.
We propose an Upper Confidence Bound (UCB)-based learning algorithm,
Multi-G-UCB, and prove that its expected regret over $T$ steps is bounded by
$O(N\log(T)[\sqrt{KT} + DK])$, where $D$ is the diameter of graph $G$. Lastly,
we numerically test our algorithm by comparing it to alternative methods.
\\ ( https://arxiv.org/abs/2401.10383 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10386
Date: Thu, 18 Jan 2024 21:49:04 GMT   (415kb,D)

Title: Noninvasive Acute Compartment Syndrome Diagnosis Using Random Forest
  Machine Learning
Authors: Zaina Abu Hweij, Florence Liang, Sophie Zhang
Categories: cs.LG eess.SP physics.med-ph
\\
  Acute compartment syndrome (ACS) is an orthopedic emergency, caused by
elevated pressure within a muscle compartment, that leads to permanent tissue
damage and eventually death. Diagnosis of ACS relies heavily on
patient-reported symptoms, a method that is clinically unreliable and often
supplemented with invasive intracompartmental pressure measurements. This study
proposes a continuous, objective, noninvasive diagnostic for ACS. The device
detects ACS through a random forest machine learning model that uses pressure
readings from force-sensitive resistors (FSRs) placed on the skin. The final
diagnosis is exported real-time to a web application via Bluetooth. To validate
the diagnostic, a data set containing FSR measurements and the corresponding
simulated intracompartmental pressure was created. The diagnostic achieved an
accuracy, on par to the invasive gold standard, of 97%. The device excelled in
key performance metrics including precision, sensitivity, and F1 score.
Manufactured for 73 USD, our device may be an economic alternative to
needle-based diagnostics. These results demonstrate the potential of
noninvasive ACS diagnostics to meet clinical standards and enhance patient
care.
\\ ( https://arxiv.org/abs/2401.10386 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10393
Date: Thu, 18 Jan 2024 22:06:38 GMT   (1601kb,D)

Title: Catastrophic Interference is Mitigated in Naturalistic Power-Law
  Learning Environments
Authors: Atith Gandhi, Raj Sanjay Shah, Vijay Marupudi, Sashank Varma
Categories: cs.LG cs.AI
\\
  Neural networks often suffer from catastrophic interference (CI): performance
on previously learned tasks drops off significantly when learning a new task.
This contrasts strongly with humans, who can sequentially learn new tasks
without appreciably forgetting previous tasks. Prior work has explored various
techniques for mitigating CI such as regularization, rehearsal, generative
replay, and distillation methods. The current work takes a different approach,
one guided by cognitive science research showing that in naturalistic
environments, the probability of encountering a task decreases as a power-law
of the time since it was last performed. We argue that a realistic evaluation
of techniques for the mitigation of CI should be performed in simulated
naturalistic learning environments. Thus, we evaluate the extent of mitigation
of CI when training simple rehearsal-based methods in power-law environments
similar to the ones humans face. Our work explores this novel rehearsal-based
approach for a domain-incremental task: learning permutations in the MNIST
task. We compare our rehearsal environment with other baselines to show its
efficacy in promoting continual learning. Additionally, we investigate whether
this environment shows forward facilitation, i.e., faster learning of later
tasks. Next, we explore the robustness of our learning environment to the
number of tasks, model size, and amount of data rehearsed after each task.
Notably, our results show that the performance is comparable or superior to
that of models trained using popular regularization methods and also to
rehearsals in non-power-law environments. The benefits of this training
paradigm include simplicity and the lack of a need for extra neural circuitry.
In addition, because our method is orthogonal to other methods, future research
can combine training in power-law environments with other continual learning
mechanisms.
\\ ( https://arxiv.org/abs/2401.10393 ,  1601kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10394
Date: Thu, 18 Jan 2024 22:07:48 GMT   (1653kb,D)

Title: Distribution Consistency based Self-Training for Graph Neural Networks
  with Sparse Labels
Authors: Fali Wang, Tianxiang Zhao, Suhang Wang
Categories: cs.LG cs.AI
Comments: Accepted by WSDM 2024
ACM-class: F.2.2; I.2.7
DOI: 10.1145/3616855.3635793
\\
  Few-shot node classification poses a significant challenge for Graph Neural
Networks (GNNs) due to insufficient supervision and potential distribution
shifts between labeled and unlabeled nodes. Self-training has emerged as a
widely popular framework to leverage the abundance of unlabeled data, which
expands the training set by assigning pseudo-labels to selected unlabeled
nodes. Efforts have been made to develop various selection strategies based on
confidence, information gain, etc. However, none of these methods takes into
account the distribution shift between the training and testing node sets. The
pseudo-labeling step may amplify this shift and even introduce new ones,
hindering the effectiveness of self-training. Therefore, in this work, we
explore the potential of explicitly bridging the distribution shift between the
expanded training set and test set during self-training. To this end, we
propose a novel Distribution-Consistent Graph Self-Training (DC-GST) framework
to identify pseudo-labeled nodes that are both informative and capable of
redeeming the distribution discrepancy and formulate it as a differentiable
optimization task. A distribution-shift-aware edge predictor is further adopted
to augment the graph and increase the model's generalizability in assigning
pseudo labels. We evaluate our proposed method on four publicly available
benchmark datasets and extensive experiments demonstrate that our framework
consistently outperforms state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2401.10394 ,  1653kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10405
Date: Thu, 18 Jan 2024 22:26:31 GMT   (2928kb,D)

Title: Differentially Private and Adversarially Robust Machine Learning: An
  Empirical Evaluation
Authors: Janvi Thakkar, Giulio Zizzo, Sergio Maffeis
Categories: cs.LG
Comments: Accepted at PPAI-24: The 5th AAAI Workshop on Privacy-Preserving
  Artificial Intelligence
\\
  Malicious adversaries can attack machine learning models to infer sensitive
information or damage the system by launching a series of evasion attacks.
Although various work addresses privacy and security concerns, they focus on
individual defenses, but in practice, models may undergo simultaneous attacks.
This study explores the combination of adversarial training and differentially
private training to defend against simultaneous attacks. While
differentially-private adversarial training, as presented in DP-Adv,
outperforms the other state-of-the-art methods in performance, it lacks formal
privacy guarantees and empirical validation. Thus, in this work, we benchmark
the performance of this technique using a membership inference attack and
empirically show that the resulting approach is as private as non-robust
private models. This work also highlights the need to explore privacy
guarantees in dynamic training paradigms.
\\ ( https://arxiv.org/abs/2401.10405 ,  2928kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10432
Date: Fri, 19 Jan 2024 00:27:34 GMT   (143kb,D)

Title: A2Q+: Improving Accumulator-Aware Weight Quantization
Authors: Ian Colbert, Alessandro Pappalardo, Jakoba Petri-Koenig, Yaman
  Umuroglu
Categories: cs.LG cs.AR cs.PF
\\
  Quantization techniques commonly reduce the inference costs of neural
networks by restricting the precision of weights and activations. Recent
studies show that also reducing the precision of the accumulator can further
improve hardware efficiency at the risk of numerical overflow, which introduces
arithmetic errors that can degrade model accuracy. To avoid numerical overflow
while maintaining accuracy, recent work proposed accumulator-aware quantization
(A2Q), a quantization-aware training method that constrains model weights
during training to safely use a target accumulator bit width during inference.
Although this shows promise, we demonstrate that A2Q relies on an overly
restrictive constraint and a sub-optimal weight initialization strategy that
each introduce superfluous quantization error. To address these shortcomings,
we introduce: (1) an improved bound that alleviates accumulator constraints
without compromising overflow avoidance; and (2) a new strategy for
initializing quantized weights from pre-trained floating-point checkpoints. We
combine these contributions with weight normalization to introduce A2Q+. We
support our analysis with experiments that show A2Q+ significantly improves the
trade-off between accumulator bit width and model accuracy and characterize new
trade-offs that arise as a consequence of accumulator constraints.
\\ ( https://arxiv.org/abs/2401.10432 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10458
Date: Fri, 19 Jan 2024 02:16:30 GMT   (661kb,D)

Title: Contrastive Unlearning: A Contrastive Approach to Machine Unlearning
Authors: Hong kyu Lee, Qiuchen Zhang, Carl Yang, Jian Lou, Li Xiong
Categories: cs.LG cs.CR
\\
  Machine unlearning aims to eliminate the influence of a subset of training
samples (i.e., unlearning samples) from a trained model. Effectively and
efficiently removing the unlearning samples without negatively impacting the
overall model performance is still challenging. In this paper, we propose a
contrastive unlearning framework, leveraging the concept of representation
learning for more effective unlearning. It removes the influence of unlearning
samples by contrasting their embeddings against the remaining samples so that
they are pushed away from their original classes and pulled toward other
classes. By directly optimizing the representation space, it effectively
removes the influence of unlearning samples while maintaining the
representations learned from the remaining samples. Experiments on a variety of
datasets and models on both class unlearning and sample unlearning showed that
contrastive unlearning achieves the best unlearning effects and efficiency with
the lowest performance loss compared with the state-of-the-art algorithms.
\\ ( https://arxiv.org/abs/2401.10458 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10474
Date: Fri, 19 Jan 2024 03:50:19 GMT   (1802kb,D)

Title: LDReg: Local Dimensionality Regularized Self-Supervised Learning
Authors: Hanxun Huang, Ricardo J. G. B. Campello, Sarah Monazam Erfani, Xingjun
  Ma, Michael E. Houle, James Bailey
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: ICLR 2024
\\
  Representations learned via self-supervised learning (SSL) can be susceptible
to dimensional collapse, where the learned representation subspace is of
extremely low dimensionality and thus fails to represent the full data
distribution and modalities. Dimensional collapse also known as the
"underfilling" phenomenon is one of the major causes of degraded performance on
downstream tasks. Previous work has investigated the dimensional collapse
problem of SSL at a global level. In this paper, we demonstrate that
representations can span over high dimensional space globally, but collapse
locally. To address this, we propose a method called $\textit{local
dimensionality regularization (LDReg)}$. Our formulation is based on the
derivation of the Fisher-Rao metric to compare and optimize local distance
distributions at an asymptotically small radius for each data point. By
increasing the local intrinsic dimensionality, we demonstrate through a range
of experiments that LDReg improves the representation quality of SSL. The
results also show that LDReg can regularize dimensionality at both local and
global levels.
\\ ( https://arxiv.org/abs/2401.10474 ,  1802kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10478
Date: Fri, 19 Jan 2024 04:02:49 GMT   (115kb,D)

Title: Budgeted Online Model Selection and Fine-Tuning via Federated Learning
Authors: Pouya M. Ghari, Yanning Shen
Categories: cs.LG
Comments: Accepted by Transactions on Machine Learning Research (TMLR)
\\
  Online model selection involves selecting a model from a set of candidate
models 'on the fly' to perform prediction on a stream of data. The choice of
candidate models henceforth has a crucial impact on the performance. Although
employing a larger set of candidate models naturally leads to more flexibility
in model selection, this may be infeasible in cases where prediction tasks are
performed on edge devices with limited memory. Faced with this challenge, the
present paper proposes an online federated model selection framework where a
group of learners (clients) interacts with a server with sufficient memory such
that the server stores all candidate models. However, each client only chooses
to store a subset of models that can be fit into its memory and performs its
own prediction task using one of the stored models. Furthermore, employing the
proposed algorithm, clients and the server collaborate to fine-tune models to
adapt them to a non-stationary environment. Theoretical analysis proves that
the proposed algorithm enjoys sub-linear regret with respect to the best model
in hindsight. Experiments on real datasets demonstrate the effectiveness of the
proposed algorithm.
\\ ( https://arxiv.org/abs/2401.10478 ,  115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10490
Date: Fri, 19 Jan 2024 05:01:43 GMT   (13272kb,D)

Title: Generalization Error Guaranteed Auto-Encoder-Based Nonlinear Model
  Reduction for Operator Learning
Authors: Hao Liu, Biraj Dahal, Rongjie Lai, Wenjing Liao
Categories: cs.LG
\\
  Many physical processes in science and engineering are naturally represented
by operators between infinite-dimensional function spaces. The problem of
operator learning, in this context, seeks to extract these physical processes
from empirical data, which is challenging due to the infinite or high
dimensionality of data. An integral component in addressing this challenge is
model reduction, which reduces both the data dimensionality and problem size.
In this paper, we utilize low-dimensional nonlinear structures in model
reduction by investigating Auto-Encoder-based Neural Network (AENet). AENet
first learns the latent variables of the input data and then learns the
transformation from these latent variables to corresponding output data. Our
numerical experiments validate the ability of AENet to accurately learn the
solution operator of nonlinear partial differential equations. Furthermore, we
establish a mathematical and statistical estimation theory that analyzes the
generalization error of AENet. Our theoretical framework shows that the sample
complexity of training AENet is intricately tied to the intrinsic dimension of
the modeled process, while also demonstrating the remarkable resilience of
AENet to noise.
\\ ( https://arxiv.org/abs/2401.10490 ,  13272kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10495
Date: Fri, 19 Jan 2024 05:18:28 GMT   (17kb)

Title: Causal Layering via Conditional Entropy
Authors: Itai Feigenbaum, Devansh Arpit, Huan Wang, Shelby Heinecke, Juan
  Carlos Niebles, Weiran Yao, Caiming Xiong, and Silvio Savarese
Categories: cs.LG cs.AI stat.ME
\\
  Causal discovery aims to recover information about an unobserved causal graph
from the observable data it generates. Layerings are orderings of the variables
which place causes before effects. In this paper, we provide ways to recover
layerings of a graph by accessing the data via a conditional entropy oracle,
when distributions are discrete. Our algorithms work by repeatedly removing
sources or sinks from the graph. Under appropriate assumptions and
conditioning, we can separate the sources or sinks from the remainder of the
nodes by comparing their conditional entropy to the unconditional entropy of
their noise. Our algorithms are provably correct and run in worst-case
quadratic time. The main assumptions are faithfulness and injective noise, and
either known noise entropies or weakly monotonically increasing noise entropies
along directed paths. In addition, we require one of either a very mild
extension of faithfulness, or strictly monotonically increasing noise
entropies, or expanding noise injectivity to include an additional single
argument in the structural functions.
\\ ( https://arxiv.org/abs/2401.10495 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10516
Date: Fri, 19 Jan 2024 06:14:36 GMT   (1256kb,D)

Title: Episodic Reinforcement Learning with Expanded State-reward Space
Authors: Dayang Liang, Yaru Zhang and Yunlong Liu
Categories: cs.LG cs.AI
Comments: Accepted at AAMAS'24
\\
  Empowered by deep neural networks, deep reinforcement learning (DRL) has
demonstrated tremendous empirical successes in various domains, including
games, health care, and autonomous driving. Despite these advancements, DRL is
still identified as data-inefficient as effective policies demand vast numbers
of environmental samples. Recently, episodic control (EC)-based model-free DRL
methods enable sample efficiency by recalling past experiences from episodic
memory. However, existing EC-based methods suffer from the limitation of
potential misalignment between the state and reward spaces for neglecting the
utilization of (past) retrieval states with extensive information, which
probably causes inaccurate value estimation and degraded policy performance. To
tackle this issue, we introduce an efficient EC-based DRL framework with
expanded state-reward space, where the expanded states used as the input and
the expanded rewards used in the training both contain historical and current
information. To be specific, we reuse the historical states retrieved by EC as
part of the input states and integrate the retrieved MC-returns into the
immediate reward in each interactive transition. As a result, our method is
able to simultaneously achieve the full utilization of retrieval information
and the better evaluation of state values by a Temporal Difference (TD) loss.
Empirical results on challenging Box2d and Mujoco tasks demonstrate the
superiority of our method over a recent sibling method and common baselines.
Further, we also verify our method's effectiveness in alleviating Q-value
overestimation by additional experiments of Q-value comparison.
\\ ( https://arxiv.org/abs/2401.10516 ,  1256kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10518
Date: Fri, 19 Jan 2024 06:26:05 GMT   (6641kb,D)

Title: Spatial-temporal Forecasting for Regions without Observations
Authors: Xinyu Su and Jianzhong Qi and Egemen Tanin and Yanchuan Chang and
  Majid Sarvi
Categories: cs.LG
Comments: Accepted by EDBT2024
\\
  Spatial-temporal forecasting plays an important role in many real-world
applications, such as traffic forecasting, air pollutant forecasting,
crowd-flow forecasting, and so on. State-of-the-art spatial-temporal
forecasting models take data-driven approaches and rely heavily on data
availability. Such models suffer from accuracy issues when data is incomplete,
which is common in reality due to the heavy costs of deploying and maintaining
sensors for data collection. A few recent studies attempted to address the
issue of incomplete data. They typically assume some data availability in a
region of interest either for a short period or at a few locations. In this
paper, we further study spatial-temporal forecasting for a region of interest
without any historical observations, to address scenarios such as unbalanced
region development, progressive deployment of sensors or lack of open data. We
propose a model named STSM for the task. The model takes a contrastive
learning-based approach to learn spatial-temporal patterns from adjacent
regions that have recorded data. Our key insight is to learn from the locations
that resemble those in the region of interest, and we propose a selective
masking strategy to enable the learning. As a result, our model outperforms
adapted state-of-the-art models, reducing errors consistently over both traffic
and air pollutant forecasting tasks. The source code is available at
https://github.com/suzy0223/STSM.
\\ ( https://arxiv.org/abs/2401.10518 ,  6641kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10541
Date: Fri, 19 Jan 2024 07:44:32 GMT   (208kb,D)

Title: I-SplitEE: Image classification in Split Computing DNNs with Early Exits
Authors: Divya Jyoti Bajpai, Aastha Jaiswal, Manjesh Kumar Hanawal
Categories: cs.LG cs.CV cs.DC
Comments: To appear in proceedings of IEEE International Conference on
  Communications 2024
\\
  The recent advances in Deep Neural Networks (DNNs) stem from their
exceptional performance across various domains. However, their inherent large
size hinders deploying these networks on resource-constrained devices like
edge, mobile, and IoT platforms. Strategies have emerged, from partial cloud
computation offloading (split computing) to integrating early exits within DNN
layers. Our work presents an innovative unified approach merging early exits
and split computing. We determine the 'splitting layer', the optimal depth in
the DNN for edge device computations, and whether to infer on edge device or be
offloaded to the cloud for inference considering accuracy, computational
efficiency, and communication costs. Also, Image classification faces diverse
environmental distortions, influenced by factors like time of day, lighting,
and weather. To adapt to these distortions, we introduce I-SplitEE, an online
unsupervised algorithm ideal for scenarios lacking ground truths and with
sequential data. Experimental validation using Caltech-256 and Cifar-10
datasets subjected to varied distortions showcases I-SplitEE's ability to
reduce costs by a minimum of 55% with marginal performance degradation of at
most 5%.
\\ ( https://arxiv.org/abs/2401.10541 ,  208kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10547
Date: Fri, 19 Jan 2024 08:13:10 GMT   (13431kb,D)

Title: PhoGAD: Graph-based Anomaly Behavior Detection with Persistent Homology
  Optimization
Authors: Ziqi Yuan, Haoyi Zhou, Tianyu Chen, Jianxin Li
Categories: cs.LG cs.SI
Comments: Accepted by WSDM 2024
\\
  A multitude of toxic online behaviors, ranging from network attacks to
anonymous traffic and spam, have severely disrupted the smooth operation of
networks. Due to the inherent sender-receiver nature of network behaviors,
graph-based frameworks are commonly used for detecting anomalous behaviors.
However, in real-world scenarios, the boundary between normal and anomalous
behaviors tends to be ambiguous. The local heterophily of graphs interferes
with the detection, and existing methods based on nodes or edges introduce
unwanted noise into representation results, thereby impacting the effectiveness
of detection. To address these issues, we propose PhoGAD, a graph-based anomaly
detection framework. PhoGAD leverages persistent homology optimization to
clarify behavioral boundaries. Building upon this, the weights of adjacent
edges are designed to mitigate the effects of local heterophily. Subsequently,
to tackle the noise problem, we conduct a formal analysis and propose a
disentangled representation-based explicit embedding method, ultimately
achieving anomaly behavior detection. Experiments on intrusion, traffic, and
spam datasets verify that PhoGAD has surpassed the performance of
state-of-the-art (SOTA) frameworks in detection efficacy. Notably, PhoGAD
demonstrates robust detection even with diminished anomaly proportions,
highlighting its applicability to real-world scenarios. The analysis of
persistent homology demonstrates its effectiveness in capturing the topological
structure formed by normal edge features. Additionally, ablation experiments
validate the effectiveness of the innovative mechanisms integrated within
PhoGAD.
\\ ( https://arxiv.org/abs/2401.10547 ,  13431kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10549
Date: Fri, 19 Jan 2024 08:26:44 GMT   (1926kb,D)

Title: Unified View Imputation and Feature Selection Learning for Incomplete
  Multi-view Data
Authors: Yanyong Huang, Zongxin Shen, Tianrui Li, Fengmao Lv
Categories: cs.LG
\\
  Although multi-view unsupervised feature selection (MUFS) is an effective
technology for reducing dimensionality in machine learning, existing methods
cannot directly deal with incomplete multi-view data where some samples are
missing in certain views. These methods should first apply predetermined values
to impute missing data, then perform feature selection on the complete dataset.
Separating imputation and feature selection processes fails to capitalize on
the potential synergy where local structural information gleaned from feature
selection could guide the imputation, thereby improving the feature selection
performance in turn. Additionally, previous methods only focus on leveraging
samples' local structure information, while ignoring the intrinsic locality of
the feature space. To tackle these problems, a novel MUFS method, called
UNified view Imputation and Feature selectIon lEaRning (UNIFIER), is proposed.
UNIFIER explores the local structure of multi-view data by adaptively learning
similarity-induced graphs from both the sample and feature spaces. Then,
UNIFIER dynamically recovers the missing views, guided by the sample and
feature similarity graphs during the feature selection procedure. Furthermore,
the half-quadratic minimization technique is used to automatically weight
different instances, alleviating the impact of outliers and unreliable restored
data. Comprehensive experimental results demonstrate that UNIFIER outperforms
other state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.10549 ,  1926kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10559
Date: Fri, 19 Jan 2024 08:50:54 GMT   (605kb,D)

Title: OrchMoE: Efficient Multi-Adapter Learning with Task-Skill Synergy
Authors: Haowen Wang, Tao Sun, Kaixiang Ji, Jian Wang, Cong Fan, Jinjie Gu
Categories: cs.LG cs.AI cs.CL
Comments: 9 pages, 3 figures
\\
  We advance the field of Parameter-Efficient Fine-Tuning (PEFT) with our novel
multi-adapter method, OrchMoE, which capitalizes on modular skill architecture
for enhanced forward transfer in neural networks. Unlike prior models that
depend on explicit task identification inputs, OrchMoE automatically discerns
task categories, streamlining the learning process. This is achieved through an
integrated mechanism comprising an Automatic Task Classification module and a
Task-Skill Allocation module, which collectively deduce task-specific
classifications and tailor skill allocation matrices. Our extensive evaluations
on the 'Super Natural Instructions' dataset, featuring 1,600 diverse
instructional tasks, indicate that OrchMoE substantially outperforms comparable
multi-adapter baselines in terms of both performance and sample utilization
efficiency, all while operating within the same parameter constraints. These
findings suggest that OrchMoE offers a significant leap forward in multi-task
learning efficiency.
\\ ( https://arxiv.org/abs/2401.10559 ,  605kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10566
Date: Fri, 19 Jan 2024 09:10:58 GMT   (16326kb,D)

Title: Robust Multi-Modal Density Estimation
Authors: Anna M\'esz\'aros, Julian F. Schumann, Javier Alonso-Mora, Arkady
  Zgonnikov, Jens Kober
Categories: cs.LG stat.ML
\\
  Development of multi-modal, probabilistic prediction models has lead to a
need for comprehensive evaluation metrics. While several metrics can
characterize the accuracy of machine-learned models (e.g., negative
log-likelihood, Jensen-Shannon divergence), these metrics typically operate on
probability densities. Applying them to purely sample-based prediction models
thus requires that the underlying density function is estimated. However,
common methods such as kernel density estimation (KDE) have been demonstrated
to lack robustness, while more complex methods have not been evaluated in
multi-modal estimation problems. In this paper, we present ROME (RObust
Multi-modal density Estimator), a non-parametric approach for density
estimation which addresses the challenge of estimating multi-modal, non-normal,
and highly correlated distributions. ROME utilizes clustering to segment a
multi-modal set of samples into multiple uni-modal ones and then combines
simple KDE estimates obtained for individual clusters in a single multi-modal
estimate. We compared our approach to state-of-the-art methods for density
estimation as well as ablations of ROME, showing that it not only outperforms
established methods but is also more robust to a variety of distributions. Our
results demonstrate that ROME can overcome the issues of over-fitting and
over-smoothing exhibited by other estimators, promising a more robust
evaluation of probabilistic machine learning models.
\\ ( https://arxiv.org/abs/2401.10566 ,  16326kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10590
Date: Fri, 19 Jan 2024 10:02:20 GMT   (277kb,D)

Title: Adversarially Robust Signed Graph Contrastive Learning from Balance
  Augmentation
Authors: Jialong Zhou, Xing Ai, Yuni Lai, Kai Zhou
Categories: cs.LG cs.CR
\\
  Signed graphs consist of edges and signs, which can be separated into
structural information and balance-related information, respectively. Existing
signed graph neural networks (SGNNs) typically rely on balance-related
information to generate embeddings. Nevertheless, the emergence of recent
adversarial attacks has had a detrimental impact on the balance-related
information. Similar to how structure learning can restore unsigned graphs,
balance learning can be applied to signed graphs by improving the balance
degree of the poisoned graph. However, this approach encounters the challenge
"Irreversibility of Balance-related Information" - while the balance degree
improves, the restored edges may not be the ones originally affected by
attacks, resulting in poor defense effectiveness. To address this challenge, we
propose a robust SGNN framework called Balance Augmented-Signed Graph
Contrastive Learning (BA-SGCL), which combines Graph Contrastive Learning
principles with balance augmentation techniques. Experimental results
demonstrate that BA-SGCL not only enhances robustness against existing
adversarial attacks but also achieves superior performance on link sign
prediction task across various datasets.
\\ ( https://arxiv.org/abs/2401.10590 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10620
Date: Fri, 19 Jan 2024 10:52:57 GMT   (1726kb,D)

Title: Polytopic Autoencoders with Smooth Clustering for Reduced-order
  Modelling of Flows
Authors: Jan Heiland, Yongho Kim
Categories: cs.LG cs.CV math.DS
Comments: 28 pages, 18 figures
\\
  With the advancement of neural networks, there has been a notable increase,
both in terms of quantity and variety, in research publications concerning the
application of autoencoders to reduced-order models. We propose a polytopic
autoencoder architecture that includes a lightweight nonlinear encoder, a
convex combination decoder, and a smooth clustering network. Supported by
several proofs, the model architecture ensures that all reconstructed states
lie within a polytope, accompanied by a metric indicating the quality of the
constructed polytopes, referred to as polytope error. Additionally, it offers a
minimal number of convex coordinates for polytopic linear-parameter varying
systems while achieving acceptable reconstruction errors compared to proper
orthogonal decomposition (POD). To validate our proposed model, we conduct
simulations involving two flow scenarios with the incompressible Navier-Stokes
equation. Numerical results demonstrate the guaranteed properties of the model,
low reconstruction errors compared to POD, and the improvement in error using a
clustering network.
\\ ( https://arxiv.org/abs/2401.10620 ,  1726kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10632
Date: Fri, 19 Jan 2024 11:20:31 GMT   (5303kb,D)

Title: Interventional Fairness on Partially Known Causal Graphs: A Constrained
  Optimization Approach
Authors: Aoqi Zuo, Yiqing Li, Susan Wei, Mingming Gong
Categories: cs.LG
Comments: Accepted to ICLR24
\\
  Fair machine learning aims to prevent discrimination against individuals or
sub-populations based on sensitive attributes such as gender and race. In
recent years, causal inference methods have been increasingly used in fair
machine learning to measure unfairness by causal effects. However, current
methods assume that the true causal graph is given, which is often not true in
real-world applications. To address this limitation, this paper proposes a
framework for achieving causal fairness based on the notion of interventions
when the true causal graph is partially known. The proposed approach involves
modeling fair prediction using a Partially Directed Acyclic Graph (PDAG),
specifically, a class of causal DAGs that can be learned from observational
data combined with domain knowledge. The PDAG is used to measure causal
fairness, and a constrained optimization problem is formulated to balance
between fairness and accuracy. Results on both simulated and real-world
datasets demonstrate the effectiveness of this method.
\\ ( https://arxiv.org/abs/2401.10632 ,  5303kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10648
Date: Fri, 19 Jan 2024 11:48:52 GMT   (5261kb,D)

Title: Area Modeling using Stay Information for Large-Scale Users and Analysis
  for Influence of COVID-19
Authors: Kazuyuki Shoji, Shunsuke Aoki, Takuro Yonezawa, Nobuo Kawaguchi
Categories: cs.LG
Comments: This paper is an English translation of the paper published in the
  Transactions of the Information Processing Society of Japan
  (http://doi.org/10.20729/00213190)
\\
  Understanding how people use area in a city can be a valuable information in
a wide range of fields, from marketing to urban planning. Area usage is subject
to change over time due to various events including seasonal shifts and
pandemics. Before the spread of smartphones, this data had been collected
through questionnaire survey. However, this is not a sustainable approach in
terms of time to results and cost. There are many existing studies on area
modeling, which characterize an area with some kind of information, using Point
of Interest (POI) or inter-area movement data. However, since POI is data that
is statically tied to space, and inter-area movement data ignores the behavior
of people within an area, existing methods are not sufficient in terms of
capturing area usage changes. In this paper, we propose a novel area modeling
method named Area2Vec, inspired by Word2Vec, which models areas based on
people's location data. This method is based on the discovery that it is
possible to characterize an area based on its usage by using people's stay
information in the area. And it is a novel method that can reflect the
dynamically changing people's behavior in an area in the modeling results. We
validated Area2vec by performing a functional classification of areas in a
district of Japan. The results show that Area2Vec can be usable in general area
analysis. We also investigated area usage changes due to COVID-19 in two
districts in Japan. We could find that COVID-19 made people refrain from
unnecessary going out, such as visiting entertainment areas.
\\ ( https://arxiv.org/abs/2401.10648 ,  5261kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10657
Date: Fri, 19 Jan 2024 12:04:31 GMT   (7134kb,D)

Title: FIMBA: Evaluating the Robustness of AI in Genomics via Feature
  Importance Adversarial Attacks
Authors: Heorhii Skovorodnikov, Hoda Alkhzaimi
Categories: cs.LG cs.CR q-bio.GN
Comments: 15 pages, core code available at:
  https://github.com/HeorhiiS/fimba-attack
\\
  With the steady rise of the use of AI in bio-technical applications and the
widespread adoption of genomics sequencing, an increasing amount of AI-based
algorithms and tools is entering the research and production stage affecting
critical decision-making streams like drug discovery and clinical outcomes.
This paper demonstrates the vulnerability of AI models often utilized
downstream tasks on recognized public genomics datasets. We undermine model
robustness by deploying an attack that focuses on input transformation while
mimicking the real data and confusing the model decision-making, ultimately
yielding a pronounced deterioration in model performance. Further, we enhance
our approach by generating poisoned data using a variational autoencoder-based
model. Our empirical findings unequivocally demonstrate a decline in model
performance, underscored by diminished accuracy and an upswing in false
positives and false negatives. Furthermore, we analyze the resulting
adversarial samples via spectral analysis yielding conclusions for
countermeasures against such attacks.
\\ ( https://arxiv.org/abs/2401.10657 ,  7134kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10685
Date: Fri, 19 Jan 2024 13:32:55 GMT   (2032kb,D)

Title: Towards End-to-End GPS Localization with Neural Pseudorange Correction
Authors: Xu Weng, KV Ling, Haochen Liu, Kun Cao
Categories: cs.LG cs.AI eess.SP
\\
  Pseudorange errors are the root cause of localization inaccuracy in GPS.
Previous data-driven methods regress and eliminate pseudorange errors using
handcrafted intermediate labels. Unlike them, we propose an end-to-end GPS
localization framework, E2E-PrNet, to train a neural network for pseudorange
correction (PrNet) directly using the final task loss calculated with the
ground truth of GPS receiver states. The gradients of the loss with respect to
learnable parameters are backpropagated through a differentiable nonlinear
least squares optimizer to PrNet. The feasibility is verified with GPS data
collected by Android phones, showing that E2E-PrNet outperforms the
state-of-the-art end-to-end GPS localization methods.
\\ ( https://arxiv.org/abs/2401.10685 ,  2032kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10686
Date: Fri, 19 Jan 2024 13:33:23 GMT   (520kb,D)

Title: Manipulating Sparse Double Descent
Authors: Ya Shi Zhang
Categories: cs.LG
\\
  This paper investigates the double descent phenomenon in two-layer neural
networks, focusing on the role of L1 regularization and representation
dimensions. It explores an alternative double descent phenomenon, named sparse
double descent. The study emphasizes the complex relationship between model
complexity, sparsity, and generalization, and suggests further research into
more diverse models and datasets. The findings contribute to a deeper
understanding of neural network training and optimization.
\\ ( https://arxiv.org/abs/2401.10686 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10690
Date: Fri, 19 Jan 2024 13:41:08 GMT   (255kb,D)

Title: Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and
  unfairness in dyadic regression models
Authors: Jorge Paz-Ruza, Amparo Alonso-Betanzos, Bertha Guijarro-Berdi\~nas,
  Brais Cancela, Carlos Eiras-Franco
Categories: cs.LG cs.AI cs.IR
\\
  Dyadic regression models, which predict real-valued outcomes for pairs of
entities, are fundamental in many domains (e.g. predicting the rating of a user
to a product in Recommender Systems) and promising and under exploration in
many others (e.g. approximating the adequate dosage of a drug for a patient in
personalized pharmacology). In this work, we demonstrate that non-uniformity in
the observed value distributions of individual entities leads to severely
biased predictions in state-of-the-art models, skewing predictions towards the
average of observed past values for the entity and providing worse-than-random
predictive power in eccentric yet equally important cases. We show that the
usage of global error metrics like Root Mean Squared Error (RMSE) and Mean
Absolute Error (MAE) is insufficient to capture this phenomenon, which we name
eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as
a new complementary metric that can quantify it in all studied models and
datasets. We also prove the adequateness of EAUC by using naive de-biasing
corrections to demonstrate that a lower model bias correlates with a lower EAUC
and vice-versa. This work contributes a bias-aware evaluation of dyadic
regression models to avoid potential unfairness and risks in critical
real-world applications of such systems.
\\ ( https://arxiv.org/abs/2401.10690 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10700
Date: Fri, 19 Jan 2024 14:05:09 GMT   (7697kb,D)

Title: Safe Offline Reinforcement Learning with Feasibility-Guided Diffusion
  Model
Authors: Yinan Zheng, Jianxiong Li, Dongjie Yu, Yujie Yang, Shengbo Eben Li,
  Xianyuan Zhan, Jingjing Liu
Categories: cs.LG cs.AI cs.RO
Comments: ICLR 2024, 30pages, 11 figures
\\
  Safe offline RL is a promising way to bypass risky online interactions
towards safe policy learning. Most existing methods only enforce soft
constraints, i.e., constraining safety violations in expectation below
thresholds predetermined. This can lead to potentially unsafe outcomes, thus
unacceptable in safety-critical scenarios. An alternative is to enforce the
hard constraint of zero violation. However, this can be challenging in offline
setting, as it needs to strike the right balance among three highly intricate
and correlated aspects: safety constraint satisfaction, reward maximization,
and behavior regularization imposed by offline datasets. Interestingly, we
discover that via reachability analysis of safe-control theory, the hard safety
constraint can be equivalently translated to identifying the largest feasible
region given the offline dataset. This seamlessly converts the original trilogy
problem to a feasibility-dependent objective, i.e., maximizing reward value
within the feasible region while minimizing safety risks in the infeasible
region. Inspired by these, we propose FISOR (FeasIbility-guided Safe Offline
RL), which allows safety constraint adherence, reward maximization, and offline
policy learning to be realized via three decoupled processes, while offering
strong safety performance and stability. In FISOR, the optimal policy for the
translated optimization problem can be derived in a special form of weighted
behavior cloning. Thus, we propose a novel energy-guided diffusion model that
does not require training a complicated time-dependent classifier to extract
the policy, greatly simplifying the training. We compare FISOR against
baselines on DSRL benchmark for safe offline RL. Evaluation results show that
FISOR is the only method that can guarantee safety satisfaction in all tasks,
while achieving top returns in most tasks.
\\ ( https://arxiv.org/abs/2401.10700 ,  7697kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10710
Date: Fri, 19 Jan 2024 14:18:32 GMT   (4025kb,D)

Title: Classification with neural networks with quadratic decision functions
Authors: Leon Frischauf, Otmar Scherzer, Cong Shi
Categories: cs.LG cs.NA math.NA
MSC-class: 49N45, 41A30, 65XX, 68TXX
\\
  Neural network with quadratic decision functions have been introduced as
alternatives to standard neural networks with affine linear one. They are
advantageous when the objects to be identified are of compact basic geometries
like circles, ellipsis etc. In this paper we investigate the use of such ansatz
functions for classification. In particular we test and compare the algorithm
on the MNIST dataset for classification of handwritten digits and for
classification of subspecies. We also show, that the implementation can be
based on the neural network structure in the software Tensorflow and Keras,
respectively.
\\ ( https://arxiv.org/abs/2401.10710 ,  4025kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10754
Date: Fri, 19 Jan 2024 15:25:09 GMT   (11078kb,D)

Title: Data Augmentation for Traffic Classification
Authors: Chao Wang, Alessandro Finamore, Pietro Michiardi, Massimo Gallo, Dario
  Rossi
Categories: cs.LG cs.NI
Comments: to appear at Passive and Active Measurements (PAM), 2024
\\
  Data Augmentation (DA) -- enriching training data by adding synthetic samples
-- is a technique widely adopted in Computer Vision (CV) and Natural Language
Processing (NLP) tasks to improve models performance. Yet, DA has struggled to
gain traction in networking contexts, particularly in Traffic Classification
(TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation
functions applied to 3 TC datasets using packet time series as input
representation and considering a variety of training conditions. Our results
show that (i) DA can reap benefits previously unexplored with (ii)
augmentations acting on time series sequence order and masking being a better
suit for TC and (iii) simple latent space analysis can provide hints about why
augmentations have positive or negative effects.
\\ ( https://arxiv.org/abs/2401.10754 ,  11078kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10765
Date: Fri, 19 Jan 2024 15:37:11 GMT   (11685kb,D)

Title: Starlit: Privacy-Preserving Federated Learning to Enhance Financial
  Fraud Detection
Authors: Aydin Abadi, Bradley Doyle, Francesco Gini, Kieron Guinamard, Sasi
  Kumar Murakonda, Jack Liddell, Paul Mellor, Steven J. Murdoch, Mohammad
  Naseri, Hector Page, George Theodorakopoulos, Suzanne Weller
Categories: cs.LG cs.CR
\\
  Federated Learning (FL) is a data-minimization approach enabling
collaborative model training across diverse clients with local data, avoiding
direct data exchange. However, state-of-the-art FL solutions to identify
fraudulent financial transactions exhibit a subset of the following
limitations. They (1) lack a formal security definition and proof, (2) assume
prior freezing of suspicious customers' accounts by financial institutions
(limiting the solutions' adoption), (3) scale poorly, involving either $O(n^2)$
computationally expensive modular exponentiation (where $n$ is the total number
of financial institutions) or highly inefficient fully homomorphic encryption,
(4) assume the parties have already completed the identity alignment phase,
hence excluding it from the implementation, performance evaluation, and
security analysis, and (5) struggle to resist clients' dropouts. This work
introduces Starlit, a novel scalable privacy-preserving FL mechanism that
overcomes these limitations. It has various applications, such as enhancing
financial fraud detection, mitigating terrorism, and enhancing digital health.
We implemented Starlit and conducted a thorough performance analysis using
synthetic data from a key player in global financial transactions. The
evaluation indicates Starlit's scalability, efficiency, and accuracy.
\\ ( https://arxiv.org/abs/2401.10765 ,  11685kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10774
Date: Fri, 19 Jan 2024 15:48:40 GMT   (1632kb,D)

Title: Medusa: Simple LLM Inference Acceleration Framework with Multiple
  Decoding Heads
Authors: Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee,
  Deming Chen, Tri Dao
Categories: cs.LG cs.CL
Comments: The code for this implementation is available at
  https://github.com/FasterDecoding/Medusa
\\
  The inference process in Large Language Models (LLMs) is often limited due to
the absence of parallelism in the auto-regressive decoding process, resulting
in most operations being restricted by the memory bandwidth of accelerators.
While methods such as speculative decoding have been suggested to address this
issue, their implementation is impeded by the challenges associated with
acquiring and maintaining a separate draft model. In this paper, we present
Medusa, an efficient method that augments LLM inference by adding extra
decoding heads to predict multiple subsequent tokens in parallel. Using a
tree-based attention mechanism, Medusa constructs multiple candidate
continuations and verifies them simultaneously in each decoding step. By
leveraging parallel processing, Medusa introduces only minimal overhead in
terms of single-step latency while substantially reducing the number of
decoding steps required.
  We present two levels of fine-tuning procedures for Medusa to meet the needs
of different use cases: Medusa-1: Medusa is directly fine-tuned on top of a
frozen backbone LLM, enabling lossless inference acceleration. Medusa-2: Medusa
is fine-tuned together with the backbone LLM, enabling better prediction
accuracy of Medusa heads and higher speedup but needing a special training
recipe that preserves the backbone model's capabilities.
  Moreover, we propose several extensions that improve or expand the utility of
Medusa, including a self-distillation to handle situations where no training
data is available and a typical acceptance scheme to boost the acceptance rate
while maintaining generation quality. We evaluate Medusa on models of various
sizes and training procedures. Our experiments demonstrate that Medusa-1 can
achieve over 2.2x speedup without compromising generation quality, while
Medusa-2 further improves the speedup to 2.3-3.6x.
\\ ( https://arxiv.org/abs/2401.10774 ,  1632kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10791
Date: Fri, 19 Jan 2024 16:23:53 GMT   (4429kb,D)

Title: Early alignment in two-layer networks training is a two-edged sword
Authors: Etienne Boursier, Nicolas Flammarion
Categories: cs.LG
\\
  Training neural networks with first order optimisation methods is at the core
of the empirical success of deep learning. The scale of initialisation is a
crucial factor, as small initialisations are generally associated to a feature
learning regime, for which gradient descent is implicitly biased towards simple
solutions. This work provides a general and quantitative description of the
early alignment phase, originally introduced by Maennel et al. (2018) . For
small initialisation and one hidden ReLU layer networks, the early stage of the
training dynamics leads to an alignment of the neurons towards key directions.
This alignment induces a sparse representation of the network, which is
directly related to the implicit bias of gradient flow at convergence. This
sparsity inducing alignment however comes at the expense of difficulties in
minimising the training objective: we also provide a simple data example for
which overparameterised networks fail to converge towards global minima and
only converge to a spurious stationary point instead.
\\ ( https://arxiv.org/abs/2401.10791 ,  4429kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10794
Date: Fri, 19 Jan 2024 16:26:35 GMT   (4212kb,D)

Title: Deep Reinforcement Learning Empowered Activity-Aware Dynamic Health
  Monitoring Systems
Authors: Ziqiaing Ye, Yulan Gao, Yue Xiao, Zehui Xiong and Dusit Niyato
Categories: cs.LG cs.CY
\\
  In smart healthcare, health monitoring utilizes diverse tools and
technologies to analyze patients' real-time biosignal data, enabling immediate
actions and interventions. Existing monitoring approaches were designed on the
premise that medical devices track several health metrics concurrently,
tailored to their designated functional scope. This means that they report all
relevant health values within that scope, which can result in excess resource
use and the gathering of extraneous data due to monitoring irrelevant health
metrics. In this context, we propose Dynamic Activity-Aware Health Monitoring
strategy (DActAHM) for striking a balance between optimal monitoring
performance and cost efficiency, a novel framework based on Deep Reinforcement
Learning (DRL) and SlowFast Model to ensure precise monitoring based on users'
activities. Specifically, with the SlowFast Model, DActAHM efficiently
identifies individual activities and captures these results for enhanced
processing. Subsequently, DActAHM refines health metric monitoring in response
to the identified activity by incorporating a DRL framework. Extensive
experiments comparing DActAHM against three state-of-the-art approaches
demonstrate it achieves 27.3% higher gain than the best-performing baseline
that fixes monitoring actions over timeline.
\\ ( https://arxiv.org/abs/2401.10794 ,  4212kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10799
Date: Fri, 19 Jan 2024 16:34:37 GMT   (3620kb,D)

Title: Novel Representation Learning Technique using Graphs for Performance
  Analytics
Authors: Tarek Ramadan, Ankur Lahiry, Tanzima Z. Islam
Categories: cs.LG
Comments: This paper has been accepted at 22nd International Conference on
  Machine Learning and Applications (ICMLA2023)
\\
  The performance analytics domain in High Performance Computing (HPC) uses
tabular data to solve regression problems, such as predicting the execution
time. Existing Machine Learning (ML) techniques leverage the correlations among
features given tabular datasets, not leveraging the relationships between
samples directly. Moreover, since high-quality embeddings from raw features
improve the fidelity of the downstream predictive models, existing methods rely
on extensive feature engineering and pre-processing steps, costing time and
manual effort. To fill these two gaps, we propose a novel idea of transforming
tabular performance data into graphs to leverage the advancement of Graph
Neural Network-based (GNN) techniques in capturing complex relationships
between features and samples. In contrast to other ML application domains, such
as social networks, the graph is not given; instead, we need to build it. To
address this gap, we propose graph-building methods where nodes represent
samples, and the edges are automatically inferred iteratively based on the
similarity between the features in the samples. We evaluate the effectiveness
of the generated embeddings from GNNs based on how well they make even a simple
feed-forward neural network perform for regression tasks compared to other
state-of-the-art representation learning techniques. Our evaluation
demonstrates that even with up to 25% random missing values for each dataset,
our method outperforms commonly used graph and Deep Neural Network (DNN)-based
approaches and achieves up to 61.67% & 78.56% improvement in MSE loss over the
DNN baseline respectively for HPC dataset and Machine Learning Datasets.
\\ ( https://arxiv.org/abs/2401.10799 ,  3620kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10809
Date: Fri, 19 Jan 2024 16:52:53 GMT   (1174kb,D)

Title: Neglected Hessian component explains mysteries in Sharpness
  regularization
Authors: Yann N. Dauphin, Atish Agarwala, Hossein Mobahi
Categories: cs.LG
\\
  Recent work has shown that methods like SAM which either explicitly or
implicitly penalize second order information can improve generalization in deep
learning. Seemingly similar methods like weight noise and gradient penalties
often fail to provide such benefits. We show that these differences can be
explained by the structure of the Hessian of the loss. First, we show that a
common decomposition of the Hessian can be quantitatively interpreted as
separating the feature exploitation from feature exploration. The feature
exploration, which can be described by the Nonlinear Modeling Error matrix
(NME), is commonly neglected in the literature since it vanishes at
interpolation. Our work shows that the NME is in fact important as it can
explain why gradient penalties are sensitive to the choice of activation
function. Using this insight we design interventions to improve performance. We
also provide evidence that challenges the long held equivalence of weight noise
and gradient penalties. This equivalence relies on the assumption that the NME
can be ignored, which we find does not hold for modern networks since they
involve significant feature learning. We find that regularizing feature
exploitation but not feature exploration yields performance similar to gradient
penalties.
\\ ( https://arxiv.org/abs/2401.10809 ,  1174kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10862
Date: Fri, 19 Jan 2024 18:05:34 GMT   (1301kb,D)

Title: Pruning for Protection: Increasing Jailbreak Resistance in Aligned LLMs
  Without Fine-Tuning
Authors: Adib Hasan, Ileana Rugina and Alex Wang
Categories: cs.LG cs.AI cs.CL cs.CR
\\
  Large Language Models (LLMs) are vulnerable to `Jailbreaking' prompts, a type
of attack that can coax these models into generating harmful and illegal
content. In this paper, we show that pruning up to 20% of LLM parameters
markedly increases their resistance to such attacks without additional training
and without sacrificing their performance in standard benchmarks. Intriguingly,
we discovered that the enhanced safety observed post-pruning correlates to the
initial safety training level of the model, hinting that the effect of pruning
could be more general and may hold for other LLM behaviors beyond safety.
Additionally, we introduce a curated dataset of 225 harmful tasks across five
categories, inserted into ten different Jailbreaking prompts, showing that
pruning aids LLMs in concentrating attention on task-relevant tokens in
jailbreaking prompts. Lastly, our experiments reveal that the prominent chat
models, such as LLaMA-2 Chat, Vicuna, and Mistral Instruct exhibit high
susceptibility to jailbreaking attacks, with some categories achieving nearly
70-100% success rate. These insights underline the potential of pruning as a
generalizable approach for improving LLM safety, reliability, and potentially
other desired behaviors.
\\ ( https://arxiv.org/abs/2401.10862 ,  1301kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.10241 (*cross-listing*)
Date: Thu, 30 Nov 2023 10:40:34 GMT   (636kb,D)

Title: Zero Bubble Pipeline Parallelism
Authors: Penghui Qi, Xinyi Wan, Guangxing Huang and Min Lin
Categories: cs.DC cs.AI cs.LG
\\
  Pipeline parallelism is one of the key components for large-scale distributed
training, yet its efficiency suffers from pipeline bubbles which were deemed
inevitable. In this work, we introduce a scheduling strategy that, to our
knowledge, is the first to successfully achieve zero pipeline bubbles under
synchronous training semantics. The key idea behind this improvement is to
split the backward computation into two parts, one that computes gradient for
the input and another that computes for the parameters. Based on this idea, we
handcraft novel pipeline schedules that significantly outperform the baseline
methods. We further develop an algorithm that automatically finds an optimal
schedule based on specific model configuration and memory limit. Additionally,
to truly achieve zero bubble, we introduce a novel technique to bypass
synchronizations during the optimizer step. Experimental evaluations show that
our method outperforms the 1F1B schedule up to 23% in throughput under a
similar memory limit. This number can be further pushed to 31% when the memory
constraint is relaxed. We believe our results mark a major step forward in
harnessing the true potential of pipeline parallelism. We open sourced our
implementation based on the popular Megatron-LM repository on
https://github.com/sail-sg/zero-bubble-pipeline-parallelism.
\\ ( https://arxiv.org/abs/2401.10241 ,  636kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10262 (*cross-listing*)
Date: Mon, 1 Jan 2024 03:32:28 GMT   (4568kb,D)

Title: Null Space Properties of Neural Networks with Applications to Image
  Steganography
Authors: Xiang Li, Kevin M. Short
Categories: cs.CV cs.AI cs.CR cs.LG
\\
  This paper explores the null space properties of neural networks. We extend
the null space definition from linear to nonlinear maps and discuss the
presence of a null space in neural networks. The null space of a given neural
network can tell us the part of the input data that makes no contribution to
the final prediction so that we can use it to trick the neural network. This
reveals an inherent weakness in neural networks that can be exploited. One
application described here leads to a method of image steganography. Through
experiments on image datasets such as MNIST, we show that we can use null space
components to force the neural network to choose a selected hidden image class,
even though the overall image can be made to look like a completely different
image. We conclude by showing comparisons between what a human viewer would
see, and the part of the image that the neural network is actually using to
make predictions and, hence, show that what the neural network ``sees'' is
completely different than what we would expect.
\\ ( https://arxiv.org/abs/2401.10262 ,  4568kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10264 (*cross-listing*)
Date: Wed, 3 Jan 2024 12:20:28 GMT   (1215kb)

Title: Harnessing Transparent Learning Analytics for Individualized Support
  through Auto-detection of Engagement in Face-to-Face Collaborative Learning
Authors: Qi Zhou, Wannapon Suraworachet, Mutlu Cukurova
Categories: cs.CY cs.AI
Comments: 12 pages, 5 figures
DOI: 10.1145/3636555.3636894
\\
  Using learning analytics to investigate and support collaborative learning
has been explored for many years. Recently, automated approaches with various
artificial intelligence approaches have provided promising results for
modelling and predicting student engagement and performance in collaborative
learning tasks. However, due to the lack of transparency and interpretability
caused by the use of "black box" approaches in learning analytics design and
implementation, guidance for teaching and learning practice may become a
challenge. On the one hand, the black box created by machine learning
algorithms and models prevents users from obtaining educationally meaningful
learning and teaching suggestions. On the other hand, focusing on group and
cohort level analysis only can make it difficult to provide specific support
for individual students working in collaborative groups. This paper proposes a
transparent approach to automatically detect student's individual engagement in
the process of collaboration. The results show that the proposed approach can
reflect student's individual engagement and can be used as an indicator to
distinguish students with different collaborative learning challenges
(cognitive, behavioural and emotional) and learning outcomes. The potential of
the proposed collaboration analytics approach for scaffolding collaborative
learning practice in face-to-face contexts is discussed and future research
suggestions are provided.
\\ ( https://arxiv.org/abs/2401.10264 ,  1215kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10267 (*cross-listing*)
Date: Thu, 4 Jan 2024 01:12:33 GMT   (26962kb,D)

Title: HyperSense: Accelerating Hyper-Dimensional Computing for Intelligent
  Sensor Data Processing
Authors: Sanggeon Yun, Hanning Chen, Ryozo Masukawa, Hamza Errahmouni Barkam,
  Andrew Ding, Wenjun Huang, Arghavan Rezvani, Shaahin Angizi, Mohsen Imani
Categories: cs.AR cs.AI
\\
  Introducing HyperSense, our co-designed hardware and software system
efficiently controls Analog-to-Digital Converter (ADC) modules' data generation
rate based on object presence predictions in sensor data. Addressing challenges
posed by escalating sensor quantities and data rates, HyperSense reduces
redundant digital data using energy-efficient low-precision ADC, diminishing
machine learning system costs. Leveraging neurally-inspired HyperDimensional
Computing (HDC), HyperSense analyzes real-time raw low-precision sensor data,
offering advantages in handling noise, memory-centricity, and real-time
learning.
  Our proposed HyperSense model combines high-performance software for object
detection with real-time hardware prediction, introducing the novel concept of
Intelligent Sensor Control. Comprehensive software and hardware evaluations
demonstrate our solution's superior performance, evidenced by the highest Area
Under the Curve (AUC) and sharpest Receiver Operating Characteristic (ROC)
curve among lightweight models. Hardware-wise, our FPGA-based domain-specific
accelerator tailored for HyperSense achieves a 5.6x speedup compared to YOLOv4
on NVIDIA Jetson Orin while showing up to 92.1% energy saving compared to the
conventional system. These results underscore HyperSense's effectiveness and
efficiency, positioning it as a promising solution for intelligent sensing and
real-time data processing across diverse applications.
\\ ( https://arxiv.org/abs/2401.10267 ,  26962kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10268 (*cross-listing*)
Date: Thu, 4 Jan 2024 03:08:13 GMT   (2312kb)

Title: The complementary contributions of academia and industry to AI research
Authors: Lizhen Liang (Syracuse University), Han Zhuang (Northeastern
  University), James Zou (Stanford University), Daniel E. Acuna (University of
  Colorado at Boulder)
Categories: cs.CY cs.AI cs.SI
Comments: 28 pages, 7 figures
\\
  Artificial intelligence (AI) has seen tremendous development in industry and
academia. However, striking recent advances by industry have stunned the world,
inviting a fresh perspective on the role of academic research in this field.
Here, we characterize the impact and type of AI produced by both environments
over the last 25 years and establish several patterns. We find that articles
published by teams consisting exclusively of industry researchers tend to get
greater attention, with a higher chance of being highly cited and
citation-disruptive, and several times more likely to produce state-of-the-art
models. In contrast, we find that exclusively academic teams publish the bulk
of AI research and tend to produce higher novelty work, with single papers
having several times higher likelihood of being unconventional and atypical.
The respective impact-novelty advantages of industry and academia are robust to
controls for subfield, team size, seniority, and prestige. We find that
academic-industry collaborations struggle to replicate the novelty of academic
teams and tend to look similar to industry teams. Together, our findings
identify the unique and nearly irreplaceable contributions that both academia
and industry make toward the healthy progress of AI.
\\ ( https://arxiv.org/abs/2401.10268 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10271 (*cross-listing*)
Date: Thu, 4 Jan 2024 18:44:12 GMT   (976kb,D)

Title: Querying Triadic Concepts through Partial or Complete Matching of
  Triples
Authors: Pedro Henrique B. Ruas, Rokia Missaoui and Mohamed Hamza Ibrahim
Categories: cs.DB cs.AI
\\
  In this paper, we introduce a new method for querying triadic concepts
through partial or complete matching of triples using an inverted index, to
retrieve already computed triadic concepts that contain a set of terms in their
extent, intent, and/or modus. As opposed to the approximation approach
described in Ananias, this method (i) does not need to keep the initial triadic
context or its three dyadic counterparts, (ii) avoids the application of
derivation operators on the triple components through context exploration, and
(iii) eliminates the requirement for a factorization phase to get triadic
concepts as the answer to one-dimensional queries. Additionally, our solution
introduces a novel metric for ranking the retrieved triadic concepts based on
their similarity to a given query. Lastly, an empirical study is primarily done
to illustrate the effectiveness and scalability of our approach against the
approximation one. Our solution not only showcases superior efficiency, but
also highlights a better scalability, making it suitable for big data
scenarios.
\\ ( https://arxiv.org/abs/2401.10271 ,  976kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10272 (*cross-listing*)
Date: Fri, 5 Jan 2024 01:21:37 GMT   (2031kb,D)

Title: Multi-Source Collaborative Gradient Discrepancy Minimization for
  Federated Domain Generalization
Authors: Yikang Wei and Yahong Han
Categories: cs.CV cs.AI
Comments: Accepted by AAAI 2024
\\
  Federated Domain Generalization aims to learn a domain-invariant model from
multiple decentralized source domains for deployment on unseen target domain.
Due to privacy concerns, the data from different source domains are kept
isolated, which poses challenges in bridging the domain gap. To address this
issue, we propose a Multi-source Collaborative Gradient Discrepancy
Minimization (MCGDM) method for federated domain generalization. Specifically,
we propose intra-domain gradient matching between the original images and
augmented images to avoid overfitting the domain-specific information within
isolated domains. Additionally, we propose inter-domain gradient matching with
the collaboration of other domains, which can further reduce the domain shift
across decentralized domains. Combining intra-domain and inter-domain gradient
matching, our method enables the learned model to generalize well on unseen
domains. Furthermore, our method can be extended to the federated domain
adaptation task by fine-tuning the target model on the pseudo-labeled target
domain. The extensive experiments on federated domain generalization and
adaptation indicate that our method outperforms the state-of-the-art methods
significantly.
\\ ( https://arxiv.org/abs/2401.10272 ,  2031kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10273 (*cross-listing*)
Date: Fri, 5 Jan 2024 04:01:09 GMT   (1014kb)

Title: Revolutionizing Pharma: Unveiling the AI and LLM Trends in the
  Pharmaceutical Industry
Authors: Yu Han, Jingwen Tao
Categories: cs.CY cs.AI
\\
  This document offers a critical overview of the emerging trends and
significant advancements in artificial intelligence (AI) within the
pharmaceutical industry. Detailing its application across key operational
areas, including research and development, animal testing, clinical trials,
hospital clinical stages, production, regulatory affairs, quality control and
other supporting areas, the paper categorically examines AI's role in each
sector. Special emphasis is placed on cutting-edge AI technologies like machine
learning algorithms and their contributions to various aspects of
pharmaceutical operations. Through this comprehensive analysis, the paper
highlights the transformative potential of AI in reshaping the pharmaceutical
industry's future.
\\ ( https://arxiv.org/abs/2401.10273 ,  1014kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10274 (*cross-listing*)
Date: Tue, 9 Jan 2024 15:26:44 GMT   (14431kb)

Title: Knowledge-Assisted Dual-Stage Evolutionary Optimization of Large-Scale
  Crude Oil Scheduling
Authors: Wanting Zhang, Wei Du, Guo Yu, Renchu He, Wenli Du, Yaochu Jin
Categories: cs.NE cs.AI
\\
  With the scaling up of crude oil scheduling in modern refineries, large-scale
crude oil scheduling problems (LSCOSPs) emerge with thousands of binary
variables and non-linear constraints, which are challenging to be optimized by
traditional optimization methods. To solve LSCOSPs, we take the practical crude
oil scheduling from a marine-access refinery as an example and start with
modeling LSCOSPs from crude unloading, transportation, crude distillation unit
processing, and inventory management of intermediate products. On the basis of
the proposed model, a dual-stage evolutionary algorithm driven by heuristic
rules (denoted by DSEA/HR) is developed, where the dual-stage search mechanism
consists of global search and local refinement. In the global search stage, we
devise several heuristic rules based on the empirical operating knowledge to
generate a well-performing initial population and accelerate convergence in the
mixed variables space. In the local refinement stage, a repair strategy is
proposed to move the infeasible solutions towards feasible regions by further
optimizing the local continuous variables. During the whole evolutionary
process, the proposed dual-stage framework plays a crucial role in balancing
exploration and exploitation. Experimental results have shown that DSEA/HR
outperforms the state-of-the-art and widely-used mathematical programming
methods and metaheuristic algorithms on LSCOSP instances within a reasonable
time.
\\ ( https://arxiv.org/abs/2401.10274 ,  14431kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10278 (*cross-listing*)
Date: Thu, 11 Jan 2024 17:36:24 GMT   (2166kb,D)

Title: EEGFormer: Towards Transferable and Interpretable Large-Scale EEG
  Foundation Model
Authors: Yuqi Chen, Kan Ren, Kaitao Song, Yansen Wang, Yifan Wang, Dongsheng
  Li, Lili Qiu
Categories: eess.SP cs.AI cs.LG cs.MM q-bio.NC
Comments: A preprint version of an ongoing work
\\
  Self-supervised learning has emerged as a highly effective approach in the
fields of natural language processing and computer vision. It is also
applicable to brain signals such as electroencephalography (EEG) data, given
the abundance of available unlabeled data that exist in a wide spectrum of
real-world medical applications ranging from seizure detection to wave
analysis. The existing works leveraging self-supervised learning on EEG
modeling mainly focus on pretraining upon each individual dataset corresponding
to a single downstream task, which cannot leverage the power of abundant data,
and they may derive sub-optimal solutions with a lack of generalization.
Moreover, these methods rely on end-to-end model learning which is not easy for
humans to understand. In this paper, we present a novel EEG foundation model,
namely EEGFormer, pretrained on large-scale compound EEG data. The pretrained
model cannot only learn universal representations on EEG signals with adaptable
performance on various downstream tasks but also provide interpretable outcomes
of the useful patterns within the data. To validate the effectiveness of our
model, we extensively evaluate it on various downstream tasks and assess the
performance under different transfer settings. Furthermore, we demonstrate how
the learned model exhibits transferable anomaly detection performance and
provides valuable interpretability of the acquired patterns via self-supervised
learning.
\\ ( https://arxiv.org/abs/2401.10278 ,  2166kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10279 (*cross-listing*)
Date: Fri, 12 Jan 2024 12:43:33 GMT   (727kb)

Title: A systematic review of geospatial location embedding approaches in large
  language models: A path to spatial AI systems
Authors: Sean Tucker
Categories: cs.IR cs.AI cs.CL
Comments: 20 pages, 11 figures, 3 appendices
\\
  Geospatial Location Embedding (GLE) helps a Large Language Model (LLM)
assimilate and analyze spatial data. GLE emergence in Geospatial Artificial
Intelligence (GeoAI) is precipitated by the need for deeper geospatial
awareness in our complex contemporary spaces and the success of LLMs in
extracting deep meaning in Generative AI. We searched Google Scholar, Science
Direct, and arXiv for papers on geospatial location embedding and LLM and
reviewed articles focused on gaining deeper spatial "knowing" through LLMs. We
screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE
themes - Entity Location Embedding (ELE), Document Location Embedding (DLE),
Sequence Location Embedding (SLE), and Token Location Embedding (TLE).
Synthesis is tabular and narrative, including a dialogic conversation between
"Space" and "LLM." Though GLEs aid spatial understanding by superimposing
spatial data, they emphasize the need to advance in the intricacies of spatial
modalities and generalized reasoning. GLEs signal the need for a Spatial
Foundation/Language Model (SLM) that embeds spatial knowing within the model
architecture. The SLM framework advances Spatial Artificial Intelligence
Systems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to
physical space. The resulting spatially imbued Language Model is unique. It
simultaneously represents actual space and an AI-capable space, paving the way
for AI native geo storage, analysis, and multi-modality as the basis for
Spatial Artificial Intelligence Systems (SPAIS).
\\ ( https://arxiv.org/abs/2401.10279 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10280 (*cross-listing*)
Date: Fri, 12 Jan 2024 13:34:34 GMT   (584kb,D)

Title: GANs for EVT Based Model Parameter Estimation in Real-time
  Ultra-Reliable Communication
Authors: Parmida Valiahdi and Sinem Coleri
Categories: eess.SP cs.AI cs.SY eess.SY
\\
  The Ultra-Reliable Low-Latency Communications (URLLC) paradigm in
sixth-generation (6G) systems heavily relies on precise channel modeling,
especially when dealing with rare and extreme events within wireless
communication channels. This paper explores a novel methodology integrating
Extreme Value Theory (EVT) and Generative Adversarial Networks (GANs) to
achieve the precise channel modeling in real-time. The proposed approach
harnesses EVT by employing the Generalized Pareto Distribution (GPD) to model
the distribution of extreme events. Subsequently, Generative Adversarial
Networks (GANs) are employed to estimate the parameters of the GPD. In contrast
to conventional GAN configurations that focus on estimating the overall
distribution, the proposed approach involves the incorporation of an additional
block within the GAN structure. This specific augmentation is designed with the
explicit purpose of directly estimating the parameters of the Generalized
Pareto Distribution (GPD). Through extensive simulations across different
sample sizes, the proposed GAN based approach consistently demonstrates
superior adaptability, surpassing Maximum Likelihood Estimation (MLE),
particularly in scenarios with limited sample sizes.
\\ ( https://arxiv.org/abs/2401.10280 ,  584kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10282 (*cross-listing*)
Date: Fri, 12 Jan 2024 23:52:44 GMT   (9092kb,D)

Title: BioDiffusion: A Versatile Diffusion Model for Biomedical Signal
  Synthesis
Authors: Xiaomin Li, Mykhailo Sakevych, Gentry Atkinson, Vangelis Metsis
Categories: eess.SP cs.AI cs.LG
\\
  Machine learning tasks involving biomedical signals frequently grapple with
issues such as limited data availability, imbalanced datasets, labeling
complexities, and the interference of measurement noise. These challenges often
hinder the optimal training of machine learning algorithms. Addressing these
concerns, we introduce BioDiffusion, a diffusion-based probabilistic model
optimized for the synthesis of multivariate biomedical signals. BioDiffusion
demonstrates excellence in producing high-fidelity, non-stationary,
multivariate signals for a range of tasks including unconditional,
label-conditional, and signal-conditional generation. Leveraging these
synthesized signals offers a notable solution to the aforementioned challenges.
Our research encompasses both qualitative and quantitative assessments of the
synthesized data quality, underscoring its capacity to bolster accuracy in
machine learning tasks tied to biomedical signals. Furthermore, when juxtaposed
with current leading time-series generative models, empirical evidence suggests
that BioDiffusion outperforms them in biomedical signal generation quality.
\\ ( https://arxiv.org/abs/2401.10282 ,  9092kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10284 (*cross-listing*)
Date: Sun, 14 Jan 2024 17:52:08 GMT   (998kb,D)

Title: MorpheusNet: Resource efficient sleep stage classifier for embedded
  on-line systems
Authors: Ali Kavoosi, Morgan P. Mitchell, Raveen Kariyawasam, John E. Fleming,
  Penny Lewis, Heidi Johansen-Berg, Hayriye Cagnan, Timothy Denison
Categories: eess.SP cs.AI cs.LG
Comments: This paper was presented at the 2023 IEEE conference on Systems, Man,
  and Cybernetics (SMC)
\\
  Sleep Stage Classification (SSC) is a labor-intensive task, requiring experts
to examine hours of electrophysiological recordings for manual classification.
This is a limiting factor when it comes to leveraging sleep stages for
therapeutic purposes. With increasing affordability and expansion of wearable
devices, automating SSC may enable deployment of sleep-based therapies at
scale. Deep Learning has gained increasing attention as a potential method to
automate this process. Previous research has shown accuracy comparable to
manual expert scores. However, previous approaches require sizable amount of
memory and computational resources. This constrains the ability to classify in
real time and deploy models on the edge. To address this gap, we aim to provide
a model capable of predicting sleep stages in real-time, without requiring
access to external computational sources (e.g., mobile phone, cloud). The
algorithm is power efficient to enable use on embedded battery powered systems.
Our compact sleep stage classifier can be deployed on most off-the-shelf
microcontrollers (MCU) with constrained hardware settings. This is due to the
memory footprint of our approach requiring significantly fewer operations. The
model was tested on three publicly available data bases and achieved
performance comparable to the state of the art, whilst reducing model
complexity by orders of magnitude (up to 280 times smaller compared to state of
the art). We further optimized the model with quantization of parameters to 8
bits with only an average drop of 0.95% in accuracy. When implemented in
firmware, the quantized model achieves a latency of 1.6 seconds on an Arm
CortexM4 processor, allowing its use for on-line SSC-based therapies.
\\ ( https://arxiv.org/abs/2401.10284 ,  998kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10289 (*cross-listing*)
Date: Wed, 17 Jan 2024 04:42:49 GMT   (853kb)

Title: Design and development of opto-neural processors for simulation of
  neural networks trained in image detection for potential implementation in
  hybrid robotics
Authors: Sanjana Shetty
Categories: cs.ET cs.AI cs.LG cs.NE
\\
  Neural networks have been employed for a wide range of processing
applications like image processing, motor control, object detection and many
others. Living neural networks offer advantages of lower power consumption,
faster processing, and biological realism. Optogenetics offers high spatial and
temporal control over biological neurons and presents potential in training
live neural networks. This work proposes a simulated living neural network
trained indirectly by backpropagating STDP based algorithms using precision
activation by optogenetics achieving accuracy comparable to traditional neural
network training algorithms.
\\ ( https://arxiv.org/abs/2401.10289 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10300 (*cross-listing*)
Date: Thu, 18 Jan 2024 08:55:05 GMT   (3780kb,D)

Title: A Hierarchical Framework with Spatio-Temporal Consistency Learning for
  Emergence Detection in Complex Adaptive Systems
Authors: Siyuan Chen, Xin Du, Jiahai Wang
Categories: cs.MA cs.AI cs.LG
Comments: 18 pages, under review
\\
  Emergence, a global property of complex adaptive systems (CASs) constituted
by interactive agents, is prevalent in real-world dynamic systems, e.g.,
network-level traffic congestions. Detecting its formation and evaporation
helps to monitor the state of a system, allowing to issue a warning signal for
harmful emergent phenomena. Since there is no centralized controller of CAS,
detecting emergence based on each agent's local observation is desirable but
challenging. Existing works are unable to capture emergence-related spatial
patterns, and fail to model the nonlinear relationships among agents. This
paper proposes a hierarchical framework with spatio-temporal consistency
learning to solve these two problems by learning the system representation and
agent representations, respectively. Especially, spatio-temporal encoders are
tailored to capture agents' nonlinear relationships and the system's complex
evolution. Representations of the agents and the system are learned by
preserving the intrinsic spatio-temporal consistency in a self-supervised
manner. Our method achieves more accurate detection than traditional methods
and deep learning methods on three datasets with well-known yet hard-to-detect
emergent behaviors. Notably, our hierarchical framework is generic, which can
employ other deep learning methods for agent-level and system-level detection.
\\ ( https://arxiv.org/abs/2401.10300 ,  3780kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10314 (*cross-listing*)
Date: Thu, 18 Jan 2024 18:52:06 GMT   (1492kb,D)

Title: LangProp: A code optimization framework using Language Models applied to
  driving
Authors: Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd
  Russell, Jamie Shotton, Jo\~ao F. Henriques, Anthony Hu
Categories: cs.SE cs.AI cs.LG cs.RO
\\
  LangProp is a framework for iteratively optimizing code generated by large
language models (LLMs) in a supervised/reinforcement learning setting. While
LLMs can generate sensible solutions zero-shot, the solutions are often
sub-optimal. Especially for code generation tasks, it is likely that the
initial code will fail on certain edge cases. LangProp automatically evaluates
the code performance on a dataset of input-output pairs, as well as catches any
exceptions, and feeds the results back to the LLM in the training loop, so that
the LLM can iteratively improve the code it generates. By adopting a metric-
and data-driven training paradigm for this code optimization procedure, one
could easily adapt findings from traditional machine learning techniques such
as imitation learning, DAgger, and reinforcement learning. We demonstrate the
first proof of concept of automated code optimization for autonomous driving in
CARLA, showing that LangProp can generate interpretable and transparent driving
policies that can be verified and improved in a metric- and data-driven way.
Our code will be open-sourced and is available at
https://github.com/shuishida/LangProp.
\\ ( https://arxiv.org/abs/2401.10314 ,  1492kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10316 (*cross-listing*)
Date: Thu, 18 Jan 2024 18:59:55 GMT   (393kb,D)

Title: Improving One-class Recommendation with Multi-tasking on Various
  Preference Intensities
Authors: Chu-Jen Shao, Hao-Ming Fu, Pu-Jen Cheng
Categories: cs.IR cs.AI cs.LG
Comments: RecSys 2020 (ACM Conference on Recommender Systems 2020)
Journal-ref: RecSys 2020: Proceedings of the 14th ACM Conference on Recommender
  Systems, Pages 498 to 502
DOI: 10.1145/3383313.3412224
\\
  In the one-class recommendation problem, it's required to make
recommendations basing on users' implicit feedback, which is inferred from
their action and inaction. Existing works obtain representations of users and
items by encoding positive and negative interactions observed from training
data. However, these efforts assume that all positive signals from implicit
feedback reflect a fixed preference intensity, which is not realistic.
Consequently, representations learned with these methods usually fail to
capture informative entity features that reflect various preference
intensities.
  In this paper, we propose a multi-tasking framework taking various preference
intensities of each signal from implicit feedback into consideration.
Representations of entities are required to satisfy the objective of each
subtask simultaneously, making them more robust and generalizable. Furthermore,
we incorporate attentive graph convolutional layers to explore high-order
relationships in the user-item bipartite graph and dynamically capture the
latent tendencies of users toward the items they interact with. Experimental
results show that our method performs better than state-of-the-art methods by a
large margin on three large-scale real-world benchmark datasets.
\\ ( https://arxiv.org/abs/2401.10316 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10334 (*cross-listing*)
Date: Thu, 28 Dec 2023 10:46:56 GMT   (5330kb,D)

Title: DrugAssist: A Large Language Model for Molecule Optimization
Authors: Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue
  Wang, Wei Liu, Xiangxiang Zeng
Categories: q-bio.QM cs.AI cs.CL cs.LG
Comments: Geyan Ye and Xibao Cai are equal contributors; Longyue Wang is
  corresponding author
\\
  Recently, the impressive performance of large language models (LLMs) on a
wide range of tasks has attracted an increasing number of attempts to apply
LLMs in drug discovery. However, molecule optimization, a critical task in the
drug discovery pipeline, is currently an area that has seen little involvement
from LLMs. Most of existing approaches focus solely on capturing the underlying
patterns in chemical structures provided by the data, without taking advantage
of expert feedback. These non-interactive approaches overlook the fact that the
drug discovery process is actually one that requires the integration of expert
experience and iterative refinement. To address this gap, we propose
DrugAssist, an interactive molecule optimization model which performs
optimization through human-machine dialogue by leveraging LLM's strong
interactivity and generalizability. DrugAssist has achieved leading results in
both single and multiple property optimization, simultaneously showcasing
immense potential in transferability and iterative optimization. In addition,
we publicly release a large instruction-based dataset called
MolOpt-Instructions for fine-tuning language models on molecule optimization
tasks. We have made our code and data publicly available at
https://github.com/blazerye/DrugAssist, which we hope to pave the way for
future research in LLMs' application for drug discovery.
\\ ( https://arxiv.org/abs/2401.10334 ,  5330kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10341 (*cross-listing*)
Date: Thu, 18 Jan 2024 19:09:47 GMT   (5772kb,D)

Title: ELRT: Efficient Low-Rank Training for Compact Convolutional Neural
  Networks
Authors: Yang Sui, Miao Yin, Yu Gong, Jinqi Xiao, Huy Phan, Bo Yuan
Categories: cs.CV cs.AI
\\
  Low-rank compression, a popular model compression technique that produces
compact convolutional neural networks (CNNs) with low rankness, has been
well-studied in the literature. On the other hand, low-rank training, as an
alternative way to train low-rank CNNs from scratch, has been exploited little
yet. Unlike low-rank compression, low-rank training does not need pre-trained
full-rank models, and the entire training phase is always performed on the
low-rank structure, bringing attractive benefits for practical applications.
However, the existing low-rank training solutions still face several
challenges, such as a considerable accuracy drop and/or still needing to update
full-size models during the training. In this paper, we perform a systematic
investigation on low-rank CNN training. By identifying the proper low-rank
format and performance-improving strategy, we propose ELRT, an efficient
low-rank training solution for high-accuracy, high-compactness, low-rank CNN
models. Our extensive evaluation results for training various CNNs on different
datasets demonstrate the effectiveness of ELRT.
\\ ( https://arxiv.org/abs/2401.10341 ,  5772kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10348 (*cross-listing*)
Date: Thu, 18 Jan 2024 19:28:26 GMT   (1562kb,D)

Title: Exploring General Intelligence via Gated Graph Transformer in Functional
  Connectivity Studies
Authors: Gang Qu, Anton Orlichenko, Junqi Wang, Gemeng Zhang, Li Xiao, Aiying
  Zhang, Zhengming Ding, Yu-Ping Wang
Categories: q-bio.NC cs.AI
\\
  Functional connectivity (FC) as derived from fMRI has emerged as a pivotal
tool in elucidating the intricacies of various psychiatric disorders and
delineating the neural pathways that underpin cognitive and behavioral dynamics
inherent to the human brain. While Graph Neural Networks (GNNs) offer a
structured approach to represent neuroimaging data, they are limited by their
need for a predefined graph structure to depict associations between brain
regions, a detail not solely provided by FCs. To bridge this gap, we introduce
the Gated Graph Transformer (GGT) framework, designed to predict cognitive
metrics based on FCs. Empirical validation on the Philadelphia
Neurodevelopmental Cohort (PNC) underscores the superior predictive prowess of
our model, further accentuating its potential in identifying pivotal neural
connectivities that correlate with human cognitive processes.
\\ ( https://arxiv.org/abs/2401.10348 ,  1562kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10359 (*cross-listing*)
Date: Thu, 18 Jan 2024 19:56:27 GMT   (1426kb,D)

Title: Keeping Deep Learning Models in Check: A History-Based Approach to
  Mitigate Overfitting
Authors: Hao Li, Gopi Krishnan Rajbahadur, Dayi Lin, Cor-Paul Bezemer, and Zhen
  Ming (Jack) Jiang
Categories: cs.SE cs.AI
\\
  In software engineering, deep learning models are increasingly deployed for
critical tasks such as bug detection and code review. However, overfitting
remains a challenge that affects the quality, reliability, and trustworthiness
of software systems that utilize deep learning models. Overfitting can be (1)
prevented (e.g., using dropout or early stopping) or (2) detected in a trained
model (e.g., using correlation-based approaches). Both overfitting detection
and prevention approaches that are currently used have constraints (e.g.,
requiring modification of the model structure, and high computing resources).
In this paper, we propose a simple, yet powerful approach that can both detect
and prevent overfitting based on the training history (i.e., validation
losses). Our approach first trains a time series classifier on training
histories of overfit models. This classifier is then used to detect if a
trained model is overfit. In addition, our trained classifier can be used to
prevent overfitting by identifying the optimal point to stop a model's
training. We evaluate our approach on its ability to identify and prevent
overfitting in real-world samples. We compare our approach against
correlation-based detection approaches and the most commonly used prevention
approach (i.e., early stopping). Our approach achieves an F1 score of 0.91
which is at least 5% higher than the current best-performing non-intrusive
overfitting detection approach. Furthermore, our approach can stop training to
avoid overfitting at least 32% of the times earlier than early stopping and has
the same or a better rate of returning the best model.
\\ ( https://arxiv.org/abs/2401.10359 ,  1426kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10372 (*cross-listing*)
Date: Thu, 18 Jan 2024 20:38:27 GMT   (949kb,D)

Title: MutaBot: A Mutation Testing Approach for Chatbots
Authors: Michael Ferdinando Urrico, Diego Clerissi, Leonardo Mariani
Categories: cs.SE cs.AI
Comments: 5 pages, 2 figures, 2024 IEEE/ACM 46th International Conference on
  Software Engineering: Companion Proceedings (ICSE-Companion '24)
DOI: 10.1145/3639478.3640032
\\
  Mutation testing is a technique aimed at assessing the effectiveness of test
suites by seeding artificial faults into programs. Although available for many
platforms and languages, no mutation testing tool is currently available for
conversational chatbots, which represent an increasingly popular solution to
design systems that can interact with users through a natural language
interface. Note that since conversations must be explicitly engineered by the
developers of conversational chatbots, these systems are exposed to specific
types of faults not supported by existing mutation testing tools.
  In this paper, we present MutaBot, a mutation testing tool for conversational
chatbots. MutaBot addresses mutations at multiple levels, including
conversational flows, intents, and contexts. We designed the tool to
potentially target multiple platforms, while we implemented initial support for
Google Dialogflow chatbots. We assessed the tool with three Dialogflow chatbots
and test cases generated with Botium, revealing weaknesses in the test suites.
\\ ( https://arxiv.org/abs/2401.10372 ,  949kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10379 (*cross-listing*)
Date: Thu, 18 Jan 2024 21:04:25 GMT   (3010kb)

Title: Agricultural Object Detection with You Look Only Once (YOLO) Algorithm:
  A Bibliometric and Systematic Literature Review
Authors: Chetan M Badgujar, Alwin Poulose and Hao Gan
Categories: cs.CV cs.AI cs.RO
\\
  Vision is a major component in several digital technologies and tools used in
agriculture. The object detector, You Look Only Once (YOLO), has gained
popularity in agriculture in a relatively short span due to its
state-of-the-art performance. YOLO offers real-time detection with good
accuracy and is implemented in various agricultural tasks, including
monitoring, surveillance, sensing, automation, and robotics. The research and
application of YOLO in agriculture are accelerating rapidly but are fragmented
and multidisciplinary. Moreover, the performance characteristics (i.e.,
accuracy, speed, computation) of the object detector influence the rate of
technology implementation and adoption in agriculture. Thus, the study aims to
collect extensive literature to document and critically evaluate the advances
and application of YOLO for agricultural object recognition. First, we
conducted a bibliometric review of 257 articles to understand the scholarly
landscape of YOLO in agricultural domain. Secondly, we conducted a systematic
review of 30 articles to identify current knowledge, gaps, and modifications in
YOLO for specific agricultural tasks. The study critically assesses and
summarizes the information on YOLO's end-to-end learning approach, including
data acquisition, processing, network modification, integration, and
deployment. We also discussed task-specific YOLO algorithm modification and
integration to meet the agricultural object or environment-specific challenges.
In general, YOLO-integrated digital tools and technologies show the potential
for real-time, automated monitoring, surveillance, and object handling to
reduce labor, production cost, and environmental impact while maximizing
resource efficiency. The study provides detailed documentation and
significantly advances the existing knowledge on applying YOLO in agriculture,
which can greatly benefit the scientific community.
\\ ( https://arxiv.org/abs/2401.10379 ,  3010kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10484 (*cross-listing*)
Date: Fri, 19 Jan 2024 04:17:50 GMT   (1664kb,D)

Title: Enhancing Scalability in Recommender Systems through Lottery Ticket
  Hypothesis and Knowledge Distillation-based Neural Network Pruning
Authors: Rajaram R, Manoj Bharadhwaj, Vasan VS and Nargis Pervin
Categories: cs.IR cs.AI cs.AR
Comments: Accepted in WITS 2023 as a workshop paper
\\
  This study introduces an innovative approach aimed at the efficient pruning
of neural networks, with a particular focus on their deployment on edge
devices. Our method involves the integration of the Lottery Ticket Hypothesis
(LTH) with the Knowledge Distillation (KD) framework, resulting in the
formulation of three distinct pruning models. These models have been developed
to address scalability issue in recommender systems, whereby the complexities
of deep learning models have hindered their practical deployment. With
judicious application of the pruning techniques, we effectively curtail the
power consumption and model dimensions without compromising on accuracy.
Empirical evaluation has been performed using two real world datasets from
diverse domains against two baselines. Gratifyingly, our approaches yielded a
GPU computation-power reduction of up to 66.67%. Notably, our study contributes
to the field of recommendation system by pioneering the application of LTH and
KD.
\\ ( https://arxiv.org/abs/2401.10484 ,  1664kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10510 (*cross-listing*)
Date: Fri, 19 Jan 2024 05:58:30 GMT   (980kb,D)

Title: A match made in consistency heaven: when large language models meet
  evolutionary algorithms
Authors: Wang Chao, Jiaxuan Zhao, Licheng Jiao, Lingling Li, Fang Liu, Shuyuan
  Yang
Categories: cs.NE cs.AI cs.CL cs.LG
Comments: A perspective article under review
\\
  Pre-trained large language models (LLMs) have powerful capabilities for
generating creative natural text. Evolutionary algorithms (EAs) can discover
diverse solutions to complex real-world problems. Motivated by the common
collective and directionality of text sequence generation and evolution, this
paper illustrates the strong consistency of LLMs and EAs, which includes
multiple one-to-one key characteristics: token embedding and genotype-phenotype
mapping, position encoding and fitness shaping, position embedding and
selection, attention and crossover, feed-forward neural network and mutation,
model training and parameter update, and multi-task learning and
multi-objective optimization. Based on this consistency perspective, existing
coupling studies are analyzed, including evolutionary fine-tuning and
LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap
for future research in coupling LLMs and EAs, while highlighting key challenges
along the way. The consistency not only reveals the evolution mechanism behind
LLMs but also facilitates the development of evolved artificial agents that
approach or surpass biological organisms.
\\ ( https://arxiv.org/abs/2401.10510 ,  980kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10529 (*cross-listing*)
Date: Fri, 19 Jan 2024 07:10:13 GMT   (44857kb,D)

Title: Mementos: A Comprehensive Benchmark for Multimodal Large Language Model
  Reasoning over Image Sequences
Authors: Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong
  He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, Furong
  Huang
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: 27 pages, 23 figures
\\
  Multimodal Large Language Models (MLLMs) have demonstrated proficiency in
handling a variety of visual-language tasks. However, current MLLM benchmarks
are predominantly designed to evaluate reasoning based on static information
about a single image, and the ability of modern MLLMs to extrapolate from image
sequences, which is essential for understanding our ever-changing world, has
been less investigated. To address this challenge, this paper introduces
Mementos, a new benchmark designed to assess MLLMs' sequential image reasoning
abilities. Mementos features 4,761 diverse image sequences with varying
lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning
performance. Through a careful evaluation of nine recent MLLMs on Mementos,
including GPT-4V and Gemini, we find that they struggle to accurately describe
dynamic information about given image sequences, often leading to
hallucinations/misrepresentations of objects and their corresponding behaviors.
Our quantitative analysis and case studies identify three key factors impacting
MLLMs' sequential image reasoning: the correlation between object and
behavioral hallucinations, the influence of cooccurring behaviors, and the
compounding impact of behavioral hallucinations. Our dataset is available at
https://github.com/umd-huang-lab/Mementos.
\\ ( https://arxiv.org/abs/2401.10529 ,  44857kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10544 (*cross-listing*)
Date: Fri, 19 Jan 2024 08:07:59 GMT   (122kb,D)

Title: AAT: Adapting Audio Transformer for Various Acoustics Recognition Tasks
Authors: Yun Liang, Hai Lin, Shaojian Qiu, Yihang Zhang
Categories: cs.SD cs.AI eess.AS
Comments: Preprint version for ICASSP 2024, Korea
\\
  Recently, Transformers have been introduced into the field of acoustics
recognition. They are pre-trained on large-scale datasets using methods such as
supervised learning and semi-supervised learning, demonstrating robust
generality--It fine-tunes easily to downstream tasks and shows more robust
performance. However, the predominant fine-tuning method currently used is
still full fine-tuning, which involves updating all parameters during training.
This not only incurs significant memory usage and time costs but also
compromises the model's generality. Other fine-tuning methods either struggle
to address this issue or fail to achieve matching performance. Therefore, we
conducted a comprehensive analysis of existing fine-tuning methods and proposed
an efficient fine-tuning approach based on Adapter tuning, namely AAT. The core
idea is to freeze the audio Transformer model and insert extra learnable
Adapters, efficiently acquiring downstream task knowledge without compromising
the model's original generality. Extensive experiments have shown that our
method achieves performance comparable to or even superior to full fine-tuning
while optimizing only 7.118% of the parameters. It also demonstrates
superiority over other fine-tuning methods.
\\ ( https://arxiv.org/abs/2401.10544 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10586 (*cross-listing*)
Date: Fri, 19 Jan 2024 09:54:23 GMT   (19283kb,D)

Title: PuriDefense: Randomized Local Implicit Adversarial Purification for
  Defending Black-box Query-based Attacks
Authors: Ping Guo, Zhiyuan Yang, Xi Lin, Qingchuan Zhao, Qingfu Zhang
Categories: cs.CR cs.AI cs.LG
\\
  Black-box query-based attacks constitute significant threats to Machine
Learning as a Service (MLaaS) systems since they can generate adversarial
examples without accessing the target model's architecture and parameters.
Traditional defense mechanisms, such as adversarial training, gradient masking,
and input transformations, either impose substantial computational costs or
compromise the test accuracy of non-adversarial inputs. To address these
challenges, we propose an efficient defense mechanism, PuriDefense, that
employs random patch-wise purifications with an ensemble of lightweight
purification models at a low level of inference cost. These models leverage the
local implicit function and rebuild the natural image manifold. Our theoretical
analysis suggests that this approach slows down the convergence of query-based
attacks by incorporating randomness into purifications. Extensive experiments
on CIFAR-10 and ImageNet validate the effectiveness of our proposed
purifier-based defense mechanism, demonstrating significant improvements in
robustness against query-based attacks.
\\ ( https://arxiv.org/abs/2401.10586 ,  19283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10603 (*cross-listing*)
Date: Fri, 19 Jan 2024 10:21:27 GMT   (1950kb,D)

Title: ZnTrack -- Data as Code
Authors: Fabian Zills, Moritz Sch\"afer, Samuel Tovey, Johannes K\"astner and
  Christian Holm
Categories: cs.SE cs.AI cs.LG
Comments: 22 pages, 10 figures, 2MB PDF
\\
  The past decade has seen tremendous breakthroughs in computation and there is
no indication that this will slow any time soon. Machine learning, large-scale
computing resources, and increased industry focus have resulted in rising
investments in computer-driven solutions for data management, simulations, and
model generation. However, with this growth in computation has come an even
larger expansion of data and with it, complexity in data storage, sharing, and
tracking. In this work, we introduce ZnTrack, a Python-driven data versioning
tool. ZnTrack builds upon established version control systems to provide a
user-friendly and easy-to-use interface for tracking parameters in experiments,
designing workflows, and storing and sharing data. From this ability to reduce
large datasets to a simple Python script emerges the concept of Data as Code, a
core component of the work presented here and an undoubtedly important concept
as the age of computation continues to evolve. ZnTrack offers an open-source,
FAIR data compatible Python package to enable users to harness these concepts
of the future.
\\ ( https://arxiv.org/abs/2401.10603 ,  1950kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10640 (*cross-listing*)
Date: Fri, 19 Jan 2024 11:35:52 GMT   (3881kb,D)

Title: A comprehensive study on fidelity metrics for XAI
Authors: Miquel Mir\'o-Nicolau, Antoni Jaume-i-Cap\'o, Gabriel Moy\`a-Alcover
Categories: cs.CV cs.AI
\\
  The use of eXplainable Artificial Intelligence (XAI) systems has introduced a
set of challenges that need resolution. Herein, we focus on how to correctly
select an XAI method, an open questions within the field. The inherent
difficulty of this task is due to the lack of a ground truth. Several authors
have proposed metrics to approximate the fidelity of different XAI methods.
These metrics lack verification and have concerning disagreements. In this
study, we proposed a novel methodology to verify fidelity metrics, using a
well-known transparent model, namely a decision tree. This model allowed us to
obtain explanations with perfect fidelity. Our proposal constitutes the first
objective benchmark for these metrics, facilitating a comparison of existing
proposals, and surpassing existing methods. We applied our benchmark to assess
the existing fidelity metrics in two different experiments, each using public
datasets comprising 52,000 images. The images from these datasets had a size a
128 by 128 pixels and were synthetic data that simplified the training process.
All metric values, indicated a lack of fidelity, with the best one showing a 30
\% deviation from the expected values for perfect explanation. Our
experimentation led us to conclude that the current fidelity metrics are not
reliable enough to be used in real scenarios. From this finding, we deemed it
necessary to development new metrics, to avoid the detected problems, and we
recommend the usage of our proposal as a benchmark within the scientific
community to address these limitations.
\\ ( https://arxiv.org/abs/2401.10640 ,  3881kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10641 (*cross-listing*)
Date: Fri, 19 Jan 2024 11:37:30 GMT   (243kb,D)

Title: An Effective Index for Truss-based Community Search on Large Directed
  Graphs
Authors: Wei Ai, CanHao Xie, Tao Meng, Yinghao Wu, KeQin Li
Categories: cs.SI cs.AI
Comments: 8 pages, 8figures
\\
  Community search is a derivative of community detection that enables online
and personalized discovery of communities and has found extensive applications
in massive real-world networks. Recently, there needs to be more focus on the
community search issue within directed graphs, even though substantial research
has been carried out on undirected graphs. The recently proposed D-truss model
has achieved good results in the quality of retrieved communities. However,
existing D-truss-based work cannot perform efficient community searches on
large graphs because it consumes too many computing resources to retrieve the
maximal D-truss. To overcome this issue, we introduce an innovative merge
relation known as D-truss-connected to capture the inherent density and
cohesiveness of edges within D-truss. This relation allows us to partition all
the edges in the original graph into a series of D-truss-connected classes.
Then, we construct a concise and compact index, ConDTruss, based on
D-truss-connected. Using ConDTruss, the efficiency of maximum D-truss retrieval
will be greatly improved, making it a theoretically optimal approach.
Experimental evaluations conducted on large directed graph certificate the
effectiveness of our proposed method.
\\ ( https://arxiv.org/abs/2401.10641 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10642 (*cross-listing*)
Date: Fri, 19 Jan 2024 11:44:09 GMT   (344kb,D)

Title: Fast Butterfly-Core Community Search For Large Labeled Graphs
Authors: JiaYi Du, Yinghao Wu, Wei Ai, Tao Meng, CanHao Xie, KeQin Li
Categories: cs.SI cs.AI
Comments: 8 pages, 8 figures
\\
  Community Search (CS) aims to identify densely interconnected subgraphs
corresponding to query vertices within a graph. However, existing heterogeneous
graph-based community search methods need help identifying cross-group
communities and suffer from efficiency issues, making them unsuitable for large
graphs. This paper presents a fast community search model based on the
Butterfly-Core Community (BCC) structure for heterogeneous graphs. The Random
Walk with Restart (RWR) algorithm and butterfly degree comprehensively evaluate
the importance of vertices within communities, allowing leader vertices to be
rapidly updated to maintain cross-group cohesion. Moreover, we devised a more
efficient method for updating vertex distances, which minimizes vertex visits
and enhances operational efficiency. Extensive experiments on several
real-world temporal graphs demonstrate the effectiveness and efficiency of this
solution.
\\ ( https://arxiv.org/abs/2401.10642 ,  344kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10643 (*cross-listing*)
Date: Fri, 19 Jan 2024 11:45:10 GMT   (919kb)

Title: A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification:
  Models, Data Sets and Challenges
Authors: Ali Amiri, Aydin Kaya and Ali Seydi Keceli
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  Vehicle re-identification (ReID) endeavors to associate vehicle images
collected from a distributed network of cameras spanning diverse traffic
environments. This task assumes paramount importance within the spectrum of
vehicle-centric technologies, playing a pivotal role in deploying Intelligent
Transportation Systems (ITS) and advancing smart city initiatives. Rapid
advancements in deep learning have significantly propelled the evolution of
vehicle ReID technologies in recent years. Consequently, undertaking a
comprehensive survey of methodologies centered on deep learning for vehicle
re-identification has become imperative and inescapable. This paper extensively
explores deep learning techniques applied to vehicle ReID. It outlines the
categorization of these methods, encompassing supervised and unsupervised
approaches, delves into existing research within these categories, introduces
datasets and evaluation criteria, and delineates forthcoming challenges and
potential research directions. This comprehensive assessment examines the
landscape of deep learning in vehicle ReID and establishes a foundation and
starting point for future works. It aims to serve as a complete reference by
highlighting challenges and emerging trends, fostering advancements and
applications in vehicle ReID utilizing deep learning models.
\\ ( https://arxiv.org/abs/2401.10643 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10711 (*cross-listing*)
Date: Fri, 19 Jan 2024 14:21:46 GMT   (1671kb,D)

Title: Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal
  Models for Video Question Answering
Authors: Haibo Wang, Chenghang Lai, Yixuan Sun, Weifeng Ge
Categories: cs.CV cs.AI cs.CL
\\
  Video Question Answering (VideoQA) aims to answer natural language questions
based on the information observed in videos. Despite the recent success of
Large Multimodal Models (LMMs) in image-language understanding and reasoning,
they deal with VideoQA insufficiently by simply taking uniformly sampled frames
as visual inputs, which ignores question-relevant visual clues. Moreover, there
are no human annotations for question-critical timestamps in existing VideoQA
datasets. In light of this, we propose a novel weakly supervised framework to
enforce the LMMs to reason out the answers with question-critical moments as
visual inputs. Specifically, we fuse the question and answer pairs as event
descriptions to find multiple keyframes as target moments, which will be
pseudo-labels. With these pseudo-labels as additionally weak supervision, we
devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG
learns multiple Gaussian functions to characterize the temporal structure of
the video, and sample question-critical frames as positive moments to be the
visual inputs of LMMs. Extensive experiments on several VideoQA benchmarks
verify the effectiveness of our framework, and we achieve substantial
improvements compared to previous state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.10711 ,  1671kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10712 (*cross-listing*)
Date: Fri, 19 Jan 2024 14:22:29 GMT   (4713kb,D)

Title: Q&A Prompts: Discovering Rich Visual Clues through Mining
  Question-Answer Prompts for VQA requiring Diverse World Knowledge
Authors: Haibi Wang, Weifeng Ge
Categories: cs.CV cs.AI cs.CL
\\
  With the breakthrough of multi-modal large language models, answering complex
visual questions that demand advanced reasoning abilities and world knowledge
has become a much more important testbed for developing AI models than ever.
However, equipping AI models with robust cross-modality reasoning ability
remains challenging since the cognition scheme of humans has not been
understood systematically. In this paper, we believe that if we can collect
visual clues in the given image as much as possible, we will recognize the
image more accurately, understand the question better, recall relevant
knowledge more easily, and finally reason out the answer. We discover these
rich visual clues by mining question-answer pairs in images and sending them
into multi-modal large language models as prompts. We call the proposed method
Q&A Prompts. Specifically, we first use the image-answer pairs and the
corresponding questions in the training set as inputs and outputs to train a
visual question generation model. Then, we use an image tagging model to
identify various instances and send packaged image-tag pairs into the visual
question generation model to generate relevant questions with the extracted
image tags as answers. Finally, we encode these generated question-answer pairs
as prompts with a visual-aware prompting module and send them into pre-trained
multi-modal large language models to reason out the final answers. Experimental
results show that, compared with state-of-the-art methods, our Q&A Prompts
achieves substantial improvements on the challenging visual question answering
datasets requiring reasoning over diverse world knowledge, such as OK-VQA and
A-OKVQA.
\\ ( https://arxiv.org/abs/2401.10712 ,  4713kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10725 (*cross-listing*)
Date: Fri, 19 Jan 2024 14:42:08 GMT   (8kb)

Title: Proceedings 14th International Conference on Automated Deduction in
  Geometry
Authors: Pedro Quaresma (University of Coimbra, Portugal), Zolt\'an Kov\'acs
  (The Private University College of Education of the Diocese of Linz, Austria)
Categories: cs.LO cs.AI cs.CG cs.MS
Journal-ref: EPTCS 398, 2024
DOI: 10.4204/EPTCS.398
\\
  ADG is a forum to exchange ideas and views, to present research results and
progress, and to demonstrate software tools at the intersection between
geometry and automated deduction. The conference is held every two years. The
previous editions of ADG were held in Hagenberg in 2021 (online, postponed from
2020 due to COVID-19), Nanning in 2018, Strasbourg in 2016, Coimbra in 2014,
Edinburgh in 2012, Munich in 2010, Shanghai in 2008, Pontevedra in 2006,
Gainesville in 2004, Hagenberg in 2002, Zurich in 2000, Beijing in 1998, and
Toulouse in 1996.
  The 14th edition, ADG 2023, was held in Belgrade, Serbia, in September 20-22,
2023. This edition of ADG had an additional special focus topic, Deduction in
Education.
  Invited Speakers: Julien Narboux, University of Strasbourg, France
"Formalisation, arithmetization and automatisation of geometry"; Filip Mari\'c,
University of Belgrade, Serbia, "Automatization, formalization and
visualization of hyperbolic geometry"; Zlatan Magajna, University of Ljubljana,
Slovenia, "Workshop OK Geometry"
\\ ( https://arxiv.org/abs/2401.10725 ,  8kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10733 (*cross-listing*)
Date: Fri, 19 Jan 2024 14:50:22 GMT   (450kb,D)

Title: Dynamic Q&A of Clinical Documents with Large Language Models
Authors: Ran Elgedawy, Sudarshan Srinivasan, Ioana Danciu
Categories: cs.IR cs.AI
Comments: 8 pages, 4 figures
\\
  Electronic health records (EHRs) house crucial patient data in clinical
notes. As these notes grow in volume and complexity, manual extraction becomes
challenging. This work introduces a natural language interface using large
language models (LLMs) for dynamic question-answering on clinical notes. Our
chatbot, powered by Langchain and transformer-based LLMs, allows users to query
in natural language, receiving relevant answers from clinical notes.
Experiments, utilizing various embedding models and advanced LLMs, show Wizard
Vicuna's superior accuracy, albeit with high compute demands. Model
optimization, including weight quantization, improves latency by approximately
48 times. Promising results indicate potential, yet challenges such as model
hallucinations and limited diverse medical case evaluations remain. Addressing
these gaps is crucial for unlocking the value in clinical notes and advancing
AI-driven clinical decision-making.
\\ ( https://arxiv.org/abs/2401.10733 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10745 (*cross-listing*)
Date: Tue, 19 Dec 2023 06:28:43 GMT   (20kb)

Title: Ethical Artificial Intelligence Principles and Guidelines for the
  Governance and Utilization of Highly Advanced Large Language Models
Authors: Soaad Hossain, Syed Ishtiaque Ahmed
Categories: cs.CY cs.AI cs.CE cs.LG
Comments: 4 pages, accepted to workshop on Responsible Language Models (ReLM)
  at Association of the Advancement of Artificial Intelligence Conference (AAAI
  2024)
MSC-class: 68Txx
ACM-class: I.2; K.4.1; K.5.2; K.6.5; K.4.2
\\
  Given the success of ChatGPT, LaMDA and other large language models (LLMs),
there has been an increase in development and usage of LLMs within the
technology sector and other sectors. While the level in which LLMs has not
reached a level where it has surpassed human intelligence, there will be a time
when it will. Such LLMs can be referred to as advanced LLMs. Currently, there
are limited usage of ethical artificial intelligence (AI) principles and
guidelines addressing advanced LLMs due to the fact that we have not reached
that point yet. However, this is a problem as once we do reach that point, we
will not be adequately prepared to deal with the aftermath of it in an ethical
and optimal way, which will lead to undesired and unexpected consequences. This
paper addresses this issue by discussing what ethical AI principles and
guidelines can be used to address highly advanced LLMs.
\\ ( https://arxiv.org/abs/2401.10745 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10746 (*cross-listing*)
Date: Fri, 19 Jan 2024 15:13:30 GMT   (411kb,D)

Title: A Systematic Evaluation of Euclidean Alignment with Deep Learning for
  EEG Decoding
Authors: Bruna Junqueira, Bruno Aristimunha, Sylvain Chevallier, Raphael Y. de
  Camargo
Categories: eess.SP cs.AI cs.LG
Comments: 14 pages and 10 figures
ACM-class: I.5.1; I.6.3; I.2.6
\\
  Electroencephalography (EEG) signals are frequently used for various
Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have
shown promising results, they are hindered by the substantial data
requirements. By leveraging data from multiple subjects, transfer learning
enables more effective training of DL models. A technique that is gaining
popularity is Euclidean Alignment (EA) due to its ease of use, low
computational complexity, and compatibility with Deep Learning models. However,
few studies evaluate its impact on the training performance of shared and
individual DL models. In this work, we systematically evaluate the effect of EA
combined with DL for decoding BCI signals. We used EA to train shared models
with data from multiple subjects and evaluated its transferability to new
subjects. Our experimental results show that it improves decoding in the target
subject by 4.33% and decreases convergence time by more than 70%. We also
trained individual models for each subject to use as a majority-voting ensemble
classifier. In this scenario, using EA improved the 3-model ensemble accuracy
by 3.7%. However, when compared to the shared model with EA, the ensemble
accuracy was 3.62% lower.
\\ ( https://arxiv.org/abs/2401.10746 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10747 (*cross-listing*)
Date: Thu, 28 Dec 2023 06:47:18 GMT   (1053kb,D)

Title: Multimodal Sentiment Analysis with Missing Modality: A
  Knowledge-Transfer Approach
Authors: Weide Liu, Huijing Zhan, Hao Chen, Fengmao Lv
Categories: cs.SD cs.AI cs.CL cs.LG eess.AS
Comments: 5 pages
\\
  Multimodal sentiment analysis aims to identify the emotions expressed by
individuals through visual, language, and acoustic cues. However, most of the
existing research efforts assume that all modalities are available during both
training and testing, making their algorithms susceptible to the missing
modality scenario. In this paper, we propose a novel knowledge-transfer network
to translate between different modalities to reconstruct the missing audio
modalities. Moreover, we develop a cross-modality attention mechanism to retain
the maximal information of the reconstructed and observed modalities for
sentiment prediction. Extensive experiments on three publicly available
datasets demonstrate significant improvements over baselines and achieve
comparable results to the previous methods with complete multi-modality
supervision.
\\ ( https://arxiv.org/abs/2401.10747 ,  1053kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10753 (*cross-listing*)
Date: Fri, 19 Jan 2024 15:22:28 GMT   (4048kb,D)

Title: BoolGebra: Attributed Graph-learning for Boolean Algebraic Manipulation
Authors: Yingjie Li, Anthony Agnesina, Yanqing Zhang, Haoxing Ren, Cunxi Yu
Categories: cs.AR cs.AI cs.LG
Comments: DATE 2024 extended version. arXiv admin note: text overlap with
  arXiv:2310.07846
\\
  Boolean algebraic manipulation is at the core of logic synthesis in
Electronic Design Automation (EDA) design flow. Existing methods struggle to
fully exploit optimization opportunities, and often suffer from an explosive
search space and limited scalability efficiency. This work presents BoolGebra,
a novel attributed graph-learning approach for Boolean algebraic manipulation
that aims to improve fundamental logic synthesis. BoolGebra incorporates Graph
Neural Networks (GNNs) and takes initial feature embeddings from both
structural and functional information as inputs. A fully connected neural
network is employed as the predictor for direct optimization result
predictions, significantly reducing the search space and efficiently locating
the optimization space. The experiments involve training the BoolGebra model
w.r.t design-specific and cross-design inferences using the trained model,
where BoolGebra demonstrates generalizability for cross-design inference and
its potential to scale from small, simple training datasets to large, complex
inference datasets. Finally, BoolGebra is integrated with existing synthesis
tool ABC to perform end-to-end logic minimization evaluation w.r.t SOTA
baselines.
\\ ( https://arxiv.org/abs/2401.10753 ,  4048kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10759 (*cross-listing*)
Date: Fri, 19 Jan 2024 15:32:46 GMT   (41197kb,D)

Title: Interactions with Prompt Problems: A New Way to Teach Programming with
  Large Language Models
Authors: James Prather, Paul Denny, Juho Leinonen, David H. Smith IV, Brent N.
  Reeves, Stephen MacNeil, Brett A. Becker, Andrew Luxton-Reilly, Thezyrie
  Amarouche, Bailey Kimmel
Categories: cs.HC cs.AI
Comments: accepted for CHI 2024
\\
  Large Language Models (LLMs) have upended decades of pedagogy in computing
education. Students previously learned to code through \textit{writing} many
small problems with less emphasis on code reading and comprehension. Recent
research has shown that free code generation tools powered by LLMs can solve
introductory programming problems presented in natural language with ease. In
this paper, we propose a new way to teach programming with Prompt Problems.
Students receive a problem visually, indicating how input should be transformed
to output, and must translate that to a prompt for an LLM to decipher. The
problem is considered correct when the code that is generated by the student
prompt can pass all test cases. In this paper we present the design of this
tool, discuss student interactions with it as they learn, and provide insights
into this new class of programming problems as well as the design tools that
integrate LLMs.
\\ ( https://arxiv.org/abs/2401.10759 ,  41197kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10805 (*cross-listing*)
Date: Fri, 19 Jan 2024 16:48:49 GMT   (3827kb,D)

Title: Learning to Visually Connect Actions and their Effects
Authors: Eric Peh, Paritosh Parmar, Basura Fernando
Categories: cs.CV cs.AI cs.LG cs.RO
\\
  In this work, we introduce the novel concept of visually Connecting Actions
and Their Effects (CATE) in video understanding. CATE can have applications in
areas like task planning and learning from demonstration. We propose different
CATE-based task formulations, such as action selection and action
specification, where video understanding models connect actions and effects at
semantic and fine-grained levels. We observe that different formulations
produce representations capturing intuitive action properties. We also design
various baseline models for action selection and action specification. Despite
the intuitive nature of the task, we observe that models struggle, and humans
outperform them by a large margin. The study aims to establish a foundation for
future efforts, showcasing the flexibility and versatility of connecting
actions and effects in video understanding, with the hope of inspiring advanced
formulations and models.
\\ ( https://arxiv.org/abs/2401.10805 ,  3827kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10816 (*cross-listing*)
Date: Fri, 19 Jan 2024 17:03:37 GMT   (1590kb,D)

Title: Co-Pilot for Health: Personalized Algorithmic AI Nudging to Improve
  Health Outcomes
Authors: Jodi Chiam, Aloysius Lim, Cheryl Nott, Nicholas Mark, Ankur Teredesai,
  Sunil Shinde
Categories: cs.HC cs.AI cs.LG
Comments: 19 pages, 2 figures
\\
  The ability to shape health behaviors of large populations automatically,
across wearable types and disease conditions at scale has tremendous potential
to improve global health outcomes. We designed and implemented an AI driven
platform for digital algorithmic nudging, enabled by a Graph-Neural Network
(GNN) based Recommendation System, and granular health behavior data from
wearable fitness devices. Here we describe the efficacy results of this
platform with its capabilities of personalized and contextual nudging to
$n=84,764$ individuals over a 12-week period in Singapore. We statistically
validated that participants in the target group who received such AI optimized
daily nudges increased daily physical activity like step count by 6.17% ($p =
3.09\times10^{-4}$) and weekly minutes of Moderate to Vigorous Physical
Activity (MVPA) by 7.61% ($p = 1.16\times10^{-2}$), compared to matched
participants in control group who did not receive any nudges. Further, such
nudges were very well received, with a 13.1% of nudges sent being opened (open
rate), and 11.7% of the opened nudges rated useful compared to 1.9% rated as
not useful thereby demonstrating significant improvement in population level
engagement metrics.
\\ ( https://arxiv.org/abs/2401.10816 ,  1590kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10831 (*cross-listing*)
Date: Fri, 19 Jan 2024 17:27:21 GMT   (33120kb,D)

Title: Understanding Video Transformers via Universal Concept Discovery
Authors: Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos
  G. Derpanis, Pavel Tokmakov
Categories: cs.CV cs.AI cs.LG cs.RO
\\
  This paper studies the problem of concept-based interpretability of
transformer representations for videos. Concretely, we seek to explain the
decision-making process of video transformers based on high-level,
spatiotemporal concepts that are automatically discovered. Prior research on
concept-based interpretability has concentrated solely on image-level tasks.
Comparatively, video models deal with the added temporal dimension, increasing
complexity and posing challenges in identifying dynamic concepts over time. In
this work, we systematically address these challenges by introducing the first
Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose
an efficient approach for unsupervised identification of units of video
transformer representations - concepts, and ranking their importance to the
output of a model. The resulting concepts are highly interpretable, revealing
spatio-temporal reasoning mechanisms and object-centric representations in
unstructured video models. Performing this analysis jointly over a diverse set
of supervised and self-supervised representations, we discover that some of
these mechanism are universal in video transformers. Finally, we demonstrate
that VTCDcan be used to improve model performance for fine-grained tasks.
\\ ( https://arxiv.org/abs/2401.10831 ,  33120kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10839 (*cross-listing*)
Date: Fri, 29 Dec 2023 12:03:42 GMT   (544kb,D)

Title: Holonic Learning: A Flexible Agent-based Distributed Machine Learning
  Framework
Authors: Ahmad Esmaeili, Zahra Ghorrati, Eric T. Matson
Categories: cs.DC cs.AI cs.LG cs.MA
\\
  Ever-increasing ubiquity of data and computational resources in the last
decade have propelled a notable transition in the machine learning paradigm
towards more distributed approaches. Such a transition seeks to not only tackle
the scalability and resource distribution challenges but also to address
pressing privacy and security concerns. To contribute to the ongoing discourse,
this paper introduces Holonic Learning (HoL), a collaborative and
privacy-focused learning framework designed for training deep learning models.
By leveraging holonic concepts, the HoL framework establishes a structured
self-similar hierarchy in the learning process, enabling more nuanced control
over collaborations through the individual model aggregation approach of each
holon, along with their intra-holon commitment and communication patterns. HoL,
in its general form, provides extensive design and flexibility potentials. For
empirical analysis and to demonstrate its effectiveness, this paper implements
HoloAvg, a special variant of HoL that employs weighted averaging for model
aggregation across all holons. The convergence of the proposed method is
validated through experiments on both IID and Non-IID settings of the standard
MNISt dataset. Furthermore, the performance behaviors of HoL are investigated
under various holarchical designs and data distribution scenarios. The
presented results affirm HoL's prowess in delivering competitive performance
particularly, in the context of the Non-IID data distribution.
\\ ( https://arxiv.org/abs/2401.10839 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10840 (*cross-listing*)
Date: Sat, 30 Dec 2023 09:40:10 GMT   (2614kb,D)

Title: Symbolic Cognitive Diagnosis via Hybrid Optimization for Intelligent
  Education Systems
Authors: Junhao Shen and Hong Qian and Wei Zhang and Aimin Zhou
Categories: cs.CY cs.AI cs.LG
Journal-ref: Published in AAAI 2024
\\
  Cognitive diagnosis assessment is a fundamental and crucial task for student
learning. It models the student-exercise interaction, and discovers the
students' proficiency levels on each knowledge attribute. In real-world
intelligent education systems, generalization and interpretability of cognitive
diagnosis methods are of equal importance. However, most existing methods can
hardly make the best of both worlds due to the complicated student-exercise
interaction. To this end, this paper proposes a symbolic cognitive
diagnosis~(SCD) framework to simultaneously enhance generalization and
interpretability. The SCD framework incorporates the symbolic tree to
explicably represent the complicated student-exercise interaction function, and
utilizes gradient-based optimization methods to effectively learn the student
and exercise parameters. Meanwhile, the accompanying challenge is that we need
to tunnel the discrete symbolic representation and continuous parameter
optimization. To address this challenge, we propose to hybridly optimize the
representation and parameters in an alternating manner. To fulfill SCD, it
alternately learns the symbolic tree by derivative-free genetic programming and
learns the student and exercise parameters via gradient-based Adam. The
extensive experimental results on various real-world datasets show the
superiority of SCD on both generalization and interpretability. The ablation
study verifies the efficacy of each ingredient in SCD, and the case study
explicitly showcases how the interpretable ability of SCD works.
\\ ( https://arxiv.org/abs/2401.10840 ,  2614kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10848 (*cross-listing*)
Date: Fri, 19 Jan 2024 17:48:05 GMT   (31070kb,D)

Title: Source-Free and Image-Only Unsupervised Domain Adaptation for Category
  Level Object Pose Estimation
Authors: Prakhar Kaushik, Aayush Mishra, Adam Kortylewski, Alan Yuille
Categories: cs.CV cs.AI
Comments: 36 pages, 9 figures, 50 tables; ICLR 2024 (Poster)
\\
  We consider the problem of source-free unsupervised category-level pose
estimation from only RGB images to a target domain without any access to source
domain data or 3D annotations during adaptation. Collecting and annotating
real-world 3D data and corresponding images is laborious, expensive, yet
unavoidable process, since even 3D pose domain adaptation methods require 3D
data in the target domain. We introduce 3DUDA, a method capable of adapting to
a nuisance-ridden target domain without 3D or depth data. Our key insight stems
from the observation that specific object subparts remain stable across
out-of-domain (OOD) scenarios, enabling strategic utilization of these
invariant subcomponents for effective model updates. We represent object
categories as simple cuboid meshes, and harness a generative model of neural
feature activations modeled at each mesh vertex learnt using differential
rendering. We focus on individual locally robust mesh vertex features and
iteratively update them based on their proximity to corresponding features in
the target domain even when the global pose is not correct. Our model is then
trained in an EM fashion, alternating between updating the vertex features and
the feature extractor. We show that our method simulates fine-tuning on a
global pseudo-labeled dataset under mild assumptions, which converges to the
target domain asymptotically. Through extensive empirical validation, including
a complex extreme UDA setup which combines real nuisances, synthetic noise, and
occlusion, we demonstrate the potency of our simple approach in addressing the
domain shift challenge and significantly improving pose estimation accuracy.
\\ ( https://arxiv.org/abs/2401.10848 ,  31070kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10886 (*cross-listing*)
Date: Fri, 19 Jan 2024 18:57:46 GMT   (7472kb,D)

Title: SCENES: Subpixel Correspondence Estimation With Epipolar Supervision
Authors: Dominik A. Kloepfer, Jo\~ao F. Henriques, Dylan Campbell
Categories: cs.CV cs.AI cs.LG cs.RO
\\
  Extracting point correspondences from two or more views of a scene is a
fundamental computer vision problem with particular importance for relative
camera pose estimation and structure-from-motion. Existing local feature
matching approaches, trained with correspondence supervision on large-scale
datasets, obtain highly-accurate matches on the test sets. However, they do not
generalise well to new datasets with different characteristics to those they
were trained on, unlike classic feature extractors. Instead, they require
finetuning, which assumes that ground-truth correspondences or ground-truth
camera poses and 3D structure are available. We relax this assumption by
removing the requirement of 3D structure, e.g., depth maps or point clouds, and
only require camera pose information, which can be obtained from odometry. We
do so by replacing correspondence losses with epipolar losses, which encourage
putative matches to lie on the associated epipolar line. While weaker than
correspondence supervision, we observe that this cue is sufficient for
finetuning existing models on new data. We then further relax the assumption of
known camera poses by using pose estimates in a novel bootstrapping approach.
We evaluate on highly challenging datasets, including an indoor drone dataset
and an outdoor smartphone camera dataset, and obtain state-of-the-art results
without strong supervision.
\\ ( https://arxiv.org/abs/2401.10886 ,  7472kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10889 (*cross-listing*)
Date: Fri, 19 Jan 2024 18:59:11 GMT   (5760kb,D)

Title: Synthesizing Moving People with 3D Control
Authors: Boyi Li, Jathushan Rajasegaran, Yossi Gandelsman, Alexei A. Efros,
  Jitendra Malik
Categories: cs.CV cs.AI
\\
  In this paper, we present a diffusion model-based framework for animating
people from a single image for a given target 3D motion sequence. Our approach
has two core components: a) learning priors about invisible parts of the human
body and clothing, and b) rendering novel body poses with proper clothing and
texture. For the first part, we learn an in-filling diffusion model to
hallucinate unseen parts of a person given a single image. We train this model
on texture map space, which makes it more sample-efficient since it is
invariant to pose and viewpoint. Second, we develop a diffusion-based rendering
pipeline, which is controlled by 3D human poses. This produces realistic
renderings of novel poses of the person, including clothing, hair, and
plausible in-filling of unseen regions. This disentangled approach allows our
method to generate a sequence of images that are faithful to the target motion
in the 3D pose and, to the input image in terms of visual similarity. In
addition to that, the 3D control allows various synthetic camera trajectories
to render a person. Our experiments show that our method is resilient in
generating prolonged motions and varied challenging and complex poses compared
to prior methods. Please check our website for more details:
https://boyiliee.github.io/3DHM.github.io/.
\\ ( https://arxiv.org/abs/2401.10889 ,  5760kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10244 (*cross-listing*)
Date: Fri, 1 Dec 2023 21:50:43 GMT   (544kb)

Title: Knowledge graph driven recommendation model of graph neural network
Authors: Chaoyang Zhang, Yanan Li, Shen Chen, Siwei Fan, Wei Li
Categories: cs.IR cs.CL
\\
  A new graph neural network-based recommendation model called KGLN, which
leverages Knowledge Graph (KG) information, was developed to enhance the
accuracy and effectiveness of personalized recommendations. This model begins
by using a single-layer neural network to merge individual node features in the
graph. It then adjusts the aggregation weights of neighboring entities by
incorporating influence factors. The model evolves from a single layer to
multiple layers through iteration, enabling entities to access extensive
multi-order associated entity information. The final step involves integrating
features of entities and users to produce a recommendation score. The model's
performance was evaluated by comparing its effects on various aggregation
methods and influence factors. In tests using the MovieLen-1M and Book-Crossing
datasets, KGLN showed an AUC (Area Under the ROC curve) improvement of 0.3% to
5.9% and 1.1% to 8.2%, respectively, over established benchmark methods like
LibFM, DeepFM, Wide&Deep, and RippleNet.
\\ ( https://arxiv.org/abs/2401.10244 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10449 (*cross-listing*)
Date: Fri, 19 Jan 2024 01:36:07 GMT   (538kb,D)

Title: Contextualized Automatic Speech Recognition with Attention-Based Bias
  Phrase Boosted Beam Search
Authors: Yui Sudo, Muhammad Shakeel, Yosuke Fukumoto, Yifan Peng, Shinji
  Watanabe
Categories: eess.AS cs.CL cs.SD
Comments: accepted by ICASSP20224
\\
  End-to-end (E2E) automatic speech recognition (ASR) methods exhibit
remarkable performance. However, since the performance of such methods is
intrinsically linked to the context present in the training data, E2E-ASR
methods do not perform as desired for unseen user contexts (e.g., technical
terms, personal names, and playlists). Thus, E2E-ASR methods must be easily
contextualized by the user or developer. This paper proposes an attention-based
contextual biasing method that can be customized using an editable phrase list
(referred to as a bias list). The proposed method can be trained effectively by
combining a bias phrase index loss and special tokens to detect the bias
phrases in the input speech data. In addition, to improve the contextualization
performance during inference further, we propose a bias phrase boosted (BPB)
beam search algorithm based on the bias phrase index probability. Experimental
results demonstrate that the proposed method consistently improves the word
error rate and the character error rate of the target phrases in the bias list
on both the Librispeech-960 (English) and our in-house (Japanese) dataset,
respectively.
\\ ( https://arxiv.org/abs/2401.10449 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10487 (*cross-listing*)
Date: Fri, 19 Jan 2024 04:24:07 GMT   (154kb,D)

Title: Generative Dense Retrieval: Memory Can Be a Burden
Authors: Peiwen Yuan, Xinglin Wang, Shaoxiong Feng, Boyuan Pan, Yiwei Li, Heda
  Wang, Xupeng Miao, Kan Li
Categories: cs.IR cs.CL
Comments: EACL 2024 main
Journal-ref: EACL 2024 main
\\
  Generative Retrieval (GR), autoregressively decoding relevant document
identifiers given a query, has been shown to perform well under the setting of
small-scale corpora. By memorizing the document corpus with model parameters,
GR implicitly achieves deep interaction between query and document. However,
such a memorizing mechanism faces three drawbacks: (1) Poor memory accuracy for
fine-grained features of documents; (2) Memory confusion gets worse as the
corpus size increases; (3) Huge memory update costs for new documents. To
alleviate these problems, we propose the Generative Dense Retrieval (GDR)
paradigm. Specifically, GDR first uses the limited memory volume to achieve
inter-cluster matching from query to relevant document clusters.
Memorizing-free matching mechanism from Dense Retrieval (DR) is then introduced
to conduct fine-grained intra-cluster matching from clusters to relevant
documents. The coarse-to-fine process maximizes the advantages of GR's deep
interaction and DR's scalability. Besides, we design a cluster identifier
constructing strategy to facilitate corpus memory and a cluster-adaptive
negative sampling strategy to enhance the intra-cluster mapping ability.
Empirical results show that GDR obtains an average of 3.0 R@100 improvement on
NQ dataset under multiple settings and has better scalability.
\\ ( https://arxiv.org/abs/2401.10487 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10543 (*cross-listing*)
Date: Fri, 19 Jan 2024 08:02:37 GMT   (18749kb,D)

Title: Multilingual acoustic word embeddings for zero-resource languages
Authors: Christiaan Jacobs and Herman Kamper
Categories: eess.AS cs.CL cs.SD
\\
  This research addresses the challenge of developing speech applications for
zero-resource languages that lack labelled data. It specifically uses acoustic
word embedding (AWE) -- fixed-dimensional representations of variable-duration
speech segments -- employing multilingual transfer, where labelled data from
several well-resourced languages are used for pertaining. The study introduces
a new neural network that outperforms existing AWE models on zero-resource
languages. It explores the impact of the choice of well-resourced languages.
AWEs are applied to a keyword-spotting system for hate speech detection in
Swahili radio broadcasts, demonstrating robustness in real-world scenarios.
Additionally, novel semantic AWE models improve semantic query-by-example
search.
\\ ( https://arxiv.org/abs/2401.10543 ,  18749kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10238 (*cross-listing*)
Date: Mon, 27 Nov 2023 16:25:28 GMT   (7315kb,D)

Title: Interplay between Cryptocurrency Transactions and Online Financial
  Forums
Authors: Ana Fern\'andez Vilas and Rebeca P. D\'iaz Redondo and Daniel Couto
  Cancela and Alejandro Torrado Pazos
Categories: q-fin.GN cs.CY cs.LG
Journal-ref: Mathematics 2021, 9(4), 411;
DOI: 10.3390/math9040411
\\
  Cryptocurrencies are a type of digital money meant to provide security and
anonymity while using cryptography techniques. Although cryptocurrencies
represent a breakthrough and provide some important benefits, their usage poses
some risks that are a result of the lack of supervising institutions and
transparency. Because disinformation and volatility is discouraging for
personal investors, cryptocurrencies emerged hand-in-hand with the
proliferation of online users' communities and forums as places to share
information that can alleviate users' mistrust. This research focuses on the
study of the interplay between these cryptocurrency forums and fluctuations in
cryptocurrency values. In particular, the most popular cryptocurrency Bitcoin
(BTC) and a related active discussion community, Bitcointalk, are analyzed.
This study shows that the activity of Bitcointalk forum keeps a direct
relationship with the trend in the values of BTC, therefore analysis of this
interaction would be a perfect base to support personal investments in a
non-regulated market and, to confirm whether cryptocurrency forums show
evidences to detect abnormal behaviors in BTC values as well as to predict or
estimate these values. The experiment highlights that forum data can explain
specific events in the financial field. It also underlines the relevance of
quotes (regular mechanism to response a post) at periods: (1) when there is a
high concentration of posts around certain topics; (2) when peaks in the BTC
price are observed; and, (3) when the BTC price gradually shifts downwards and
users intend to sell.
\\ ( https://arxiv.org/abs/2401.10238 ,  7315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10247 (*cross-listing*)
Date: Thu, 7 Dec 2023 04:20:16 GMT   (3167kb,D)

Title: Resolution Chromatography of Diffusion Models
Authors: Juno Hwang and Yong-Hyun Park and Junghyo Jo
Categories: cs.CV cs.LG
Comments: 24 pages, 9 figures
\\
  Diffusion models generate high-resolution images through iterative stochastic
processes. In particular, the denoising method is one of the most popular
approaches that predicts the noise in samples and denoises it at each time
step. It has been commonly observed that the resolution of generated samples
changes over time, starting off blurry and coarse, and becoming sharper and
finer. In this paper, we introduce "resolution chromatography" that indicates
the signal generation rate of each resolution, which is very helpful concept to
mathematically explain this coarse-to-fine behavior in generation process, to
understand the role of noise schedule, and to design time-dependent modulation.
Using resolution chromatography, we determine which resolution level becomes
dominant at a specific time step, and experimentally verify our theory with
text-to-image diffusion models. We also propose some direct applications
utilizing the concept: upscaling pre-trained models to higher resolutions and
time-dependent prompt composing. Our theory not only enables a better
understanding of numerous pre-existing techniques for manipulating image
generation, but also suggests the potential for designing better noise
schedules.
\\ ( https://arxiv.org/abs/2401.10247 ,  3167kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10253 (*cross-listing*)
Date: Sat, 23 Dec 2023 04:25:12 GMT   (1417kb,D)

Title: Hybrid-Task Meta-Learning: A Graph Neural Network Approach for Scalable
  and Transferable Bandwidth Allocation
Authors: Xin Hao, Changyang She, Phee Lep Yeoh, Yuhong Liu, Branka Vucetic, and
  Yonghui Li
Categories: cs.NI cs.LG
\\
  In this paper, we develop a deep learning-based bandwidth allocation policy
that is: 1) scalable with the number of users and 2) transferable to different
communication scenarios, such as non-stationary wireless channels, different
quality-of-service (QoS) requirements, and dynamically available resources. To
support scalability, the bandwidth allocation policy is represented by a graph
neural network (GNN), with which the number of training parameters does not
change with the number of users. To enable the generalization of the GNN, we
develop a hybrid-task meta-learning (HML) algorithm that trains the initial
parameters of the GNN with different communication scenarios during
meta-training. Next, during meta-testing, a few samples are used to fine-tune
the GNN with unseen communication scenarios. Simulation results demonstrate
that our HML approach can improve the initial performance by $8.79\%$, and
sampling efficiency by $73\%$, compared with existing benchmarks. After
fine-tuning, our near-optimal GNN-based policy can achieve close to the same
reward with much lower inference complexity compared to the optimal policy
obtained using iterative optimization.
\\ ( https://arxiv.org/abs/2401.10253 ,  1417kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10254 (*cross-listing*)
Date: Sat, 23 Dec 2023 04:32:07 GMT   (43kb)

Title: Beyond the Frame: Single and mutilple video summarization method with
  user-defined length
Authors: Vahid Ahmadi Kalkhorani, Qingquan Zhang, Guanqun Song, Ting Zhu
Categories: cs.CV cs.LG
\\
  Video smmarization is a crucial method to reduce the time of videos which
reduces the spent time to watch/review a long video. This apporach has became
more important as the amount of publisehed video is increasing everyday. A
single or multiple videos can be summarized into a relatively short video using
various of techniques from multimodal audio-visual techniques, to natural
language processing approaches. Audiovisual techniques may be used to recognize
significant visual events and pick the most important parts, while NLP
techniques can be used to evaluate the audio transcript and extract the main
sentences (timestamps) and corresponding video frames from the original video.
Another approach is to use the best of both domain. Meaning that we can use
audio-visual cues as well as video transcript to extract and summarize the
video. In this paper, we combine a variety of NLP techniques (extractive and
contect-based summarizers) with video processing techniques to convert a long
video into a single relatively short video. We design this toll in a way that
user can specify the relative length of the summarized video. We have also
explored ways of summarizing and concatenating multiple videos into a single
short video which will help having most important concepts from the same
subject in a single short video. Out approach shows that video summarizing is a
difficult but significant work, with substantial potential for further research
and development, and it is possible thanks to the development of NLP models.
\\ ( https://arxiv.org/abs/2401.10254 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10255 (*cross-listing*)
Date: Sun, 24 Dec 2023 20:40:54 GMT   (41kb,D)

Title: Nowcasting Madagascar's real GDP using machine learning algorithms
Authors: Franck Ramaharo and Gerzhino Rasolofomanana
Categories: econ.GN cs.LG q-fin.EC
Comments: 13 pages, 6 figures, 5 tables
MSC-class: 91-10, 62P20
\\
  We investigate the predictive power of different machine learning algorithms
to nowcast Madagascar's gross domestic product (GDP). We trained popular
regression models, including linear regularized regression (Ridge, Lasso,
Elastic-net), dimensionality reduction model (principal component regression),
k-nearest neighbors algorithm (k-NN regression), support vector regression
(linear SVR), and tree-based ensemble models (Random forest and XGBoost
regressions), on 10 Malagasy quarterly macroeconomic leading indicators over
the period 2007Q1--2022Q4, and we used simple econometric models as a
benchmark. We measured the nowcast accuracy of each model by calculating the
root mean square error (RMSE), mean absolute error (MAE), and mean absolute
percentage error (MAPE). Our findings reveal that the Ensemble Model, formed by
aggregating individual predictions, consistently outperforms traditional
econometric models. We conclude that machine learning models can deliver more
accurate and timely nowcasts of Malagasy economic performance and provide
policymakers with additional guidance for data-driven decision making.
\\ ( https://arxiv.org/abs/2401.10255 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10257 (*cross-listing*)
Date: Tue, 26 Dec 2023 02:04:53 GMT   (2008kb,D)

Title: Curriculum Design Helps Spiking Neural Networks to Classify Time Series
Authors: Chenxi Sun, Hongyan Li, Moxian Song, Derun Can, Shenda Hong
Categories: cs.NE cs.LG
Comments: 11 pages, 3 figures
\\
  Spiking Neural Networks (SNNs) have a greater potential for modeling time
series data than Artificial Neural Networks (ANNs), due to their inherent
neuron dynamics and low energy consumption. However, it is difficult to
demonstrate their superiority in classification accuracy, because current
efforts mainly focus on designing better network structures. In this work,
enlighten by brain-inspired science, we find that, not only the structure but
also the learning process should be human-like. To achieve this, we investigate
the power of Curriculum Learning (CL) on SNNs by designing a novel method named
CSNN with two theoretically guaranteed mechanisms: The active-to-dormant
training order makes the curriculum similar to that of human learning and
suitable for spiking neurons; The value-based regional encoding makes the
neuron activity to mimic the brain memory when learning sequential data.
Experiments on multiple time series sources including simulated, sensor,
motion, and healthcare demonstrate that CL has a more positive effect on SNNs
than ANNs with about twice the accuracy change, and CSNN can increase about 3%
SNNs' accuracy by improving network sparsity, neuron firing status, anti-noise
ability, and convergence speed.
\\ ( https://arxiv.org/abs/2401.10257 ,  2008kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10265 (*cross-listing*)
Date: Wed, 3 Jan 2024 15:02:40 GMT   (110kb,D)

Title: The Best Time for an Update: Risk-Sensitive Minimization of Age-Based
  Metrics
Authors: Wanja de Sombre, Andrea Ortiz, Frank Aurzada, Anja Klein
Categories: cs.IT cs.LG cs.NI math.IT
\\
  Popular methods to quantify transmitted data quality are the Age of
Information (AoI), the Query Age of Information (QAoI), and the Age of
Incorrect Information (AoII). We consider these metrics in a point-to-point
wireless communication system, where the transmitter monitors a process and
sends status updates to a receiver. The challenge is to decide on the best time
for an update, balancing the transmission energy and the age-based metric at
the receiver. Due to the inherent risk of high age-based metric values causing
complications such as unstable system states, we introduce the new concept of
risky states to denote states with high age-based metric. We use this new
notion of risky states to quantify and minimize this risk of experiencing high
age-based metrics by directly deriving the frequency of risky states as a novel
risk-metric. Building on this foundation, we introduce two risk-sensitive
strategies for AoI, QAoI and AoII. The first strategy uses system knowledge,
i.e., channel quality and packet arrival probability, to find an optimal
strategy that transmits when the age-based metric exceeds a tunable threshold.
A lower threshold leads to higher risk-sensitivity. The second strategy uses an
enhanced Q-learning approach and balances the age-based metric, the
transmission energy and the frequency of risky states without requiring
knowledge about the system. Numerical results affirm our risk-sensitive
strategies' high effectiveness.
\\ ( https://arxiv.org/abs/2401.10265 ,  110kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10270 (*cross-listing*)
Date: Thu, 4 Jan 2024 08:11:03 GMT   (599kb)

Title: Migrating Birds Optimization-Based Feature Selection for Text
  Classification
Authors: Cem Kaya, Zeynep Hilal Kilimci, Mitat Uysal, Murat Kaya
Categories: cs.NE cs.LG
\\
  This research introduces a novel approach, MBO-NB, that leverages Migrating
Birds Optimization (MBO) coupled with Naive Bayes as an internal classifier to
address feature selection challenges in text classification having large number
of features. Focusing on computational efficiency, we preprocess raw data using
the Information Gain algorithm, strategically reducing the feature count from
an average of 62221 to 2089. Our experiments demonstrate MBO-NB's superior
effectiveness in feature reduction compared to other existing techniques,
emphasizing an increased classification accuracy. The successful integration of
Naive Bayes within MBO presents a well-rounded solution. In individual
comparisons with Particle Swarm Optimization (PSO), MBO-NB consistently
outperforms by an average of 6.9% across four setups. This research offers
valuable insights into enhancing feature selection methods, providing a
scalable and effective solution for text classification
\\ ( https://arxiv.org/abs/2401.10270 ,  599kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10283 (*cross-listing*)
Date: Sun, 14 Jan 2024 17:45:52 GMT   (2429kb,D)

Title: Window Stacking Meta-Models for Clinical EEG Classification
Authors: Yixuan Zhu, Rohan Kandasamy, Luke J. W. Canham, David Western
Categories: eess.SP cs.LG
Comments: 17 pages, 10 figures
\\
  Windowing is a common technique in EEG machine learning classification and
other time series tasks. However, a challenge arises when employing this
technique: computational expense inhibits learning global relationships across
an entire recording or set of recordings. Furthermore, the labels inherited by
windows from their parent recordings may not accurately reflect the content of
that window in isolation. To resolve these issues, we introduce a multi-stage
model architecture, incorporating meta-learning principles tailored to
time-windowed data aggregation. We further tested two distinct strategies to
alleviate these issues: lengthening the window and utilizing overlapping to
augment data. Our methods, when tested on the Temple University Hospital
Abnormal EEG Corpus (TUAB), dramatically boosted the benchmark accuracy from
89.8 percent to 99.0 percent. This breakthrough performance surpasses prior
performance projections for this dataset and paves the way for clinical
applications of machine learning solutions to EEG interpretation challenges. On
a broader and more varied dataset from the Temple University Hospital EEG
Corpus (TUEG), we attained an accuracy of 86.7%, nearing the assumed
performance ceiling set by variable inter-rater agreement on such datasets.
\\ ( https://arxiv.org/abs/2401.10283 ,  2429kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10285 (*cross-listing*)
Date: Mon, 15 Jan 2024 18:57:25 GMT   (652kb)

Title: Analyzing Brain Activity During Learning Tasks with EEG and Machine
  Learning
Authors: Ryan Cho, Mobasshira Zaman, Kyu Taek Cho, Jaejin Hwang
Categories: eess.SP cs.LG q-bio.NC
Comments: 20 pages, 7 figures
\\
  This study aimed to analyze brain activity during various STEM activities,
exploring the feasibility of classifying between different tasks. EEG brain
data from twenty subjects engaged in five cognitive tasks were collected and
segmented into 4-second clips. Power spectral densities of brain frequency
waves were then analyzed. Testing different k-intervals with XGBoost, Random
Forest, and Bagging Classifier revealed that Random Forest performed best,
achieving a testing accuracy of 91.07% at an interval size of two. When
utilizing all four EEG channels, cognitive flexibility was most recognizable.
Task-specific classification accuracy showed the right frontal lobe excelled in
mathematical processing and planning, the left frontal lobe in cognitive
flexibility and mental flexibility, and the left temporoparietal lobe in
connections. Notably, numerous connections between frontal and temporoparietal
lobes were observed during STEM activities. This study contributes to a deeper
understanding of implementing machine learning in analyzing brain activity and
sheds light on the brain's mechanisms.
\\ ( https://arxiv.org/abs/2401.10285 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10293 (*cross-listing*)
Date: Wed, 17 Jan 2024 19:00:00 GMT   (1340kb,D)

Title: Symmetry breaking in geometric quantum machine learning in the presence
  of noise
Authors: Cenk T\"uys\"uz, Su Yeon Chang, Maria Demidik, Karl Jansen, Sofia
  Vallecorsa, Michele Grossi
Categories: quant-ph cs.LG
Comments: 12 pages, 10 figures. supplementary material 7 pages, 6 figures
\\
  Geometric quantum machine learning based on equivariant quantum neural
networks (EQNN) recently appeared as a promising direction in quantum machine
learning. Despite the encouraging progress, the studies are still limited to
theory, and the role of hardware noise in EQNN training has never been
explored. This work studies the behavior of EQNN models in the presence of
noise. We show that certain EQNN models can preserve equivariance under Pauli
channels, while this is not possible under the amplitude damping channel. We
claim that the symmetry breaking grows linearly in the number of layers and
noise strength. We support our claims with numerical data from simulations as
well as hardware up to 64 qubits. Furthermore, we provide strategies to enhance
the symmetry protection of EQNN models in the presence of noise.
\\ ( https://arxiv.org/abs/2401.10293 ,  1340kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10294 (*cross-listing*)
Date: Wed, 17 Jan 2024 23:07:59 GMT   (233kb,D)

Title: Tight Group-Level DP Guarantees for DP-SGD with Sampling via Mixture of
  Gaussians Mechanisms
Authors: Arun Ganesh
Categories: cs.CR cs.LG
\\
  We give a procedure for computing group-level $(\epsilon, \delta)$-DP
guarantees for DP-SGD, when using Poisson sampling or fixed batch size
sampling. Up to discretization errors in the implementation, the DP guarantees
computed by this procedure are tight (assuming we release every intermediate
iterate).
\\ ( https://arxiv.org/abs/2401.10294 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10297 (*cross-listing*)
Date: Thu, 18 Jan 2024 04:44:34 GMT   (280kb,D)

Title: Learning Non-myopic Power Allocation in Constrained Scenarios
Authors: Arindam Chowdhury, Santiago Paternain, Gunjan Verma, Ananthram Swami,
  and Santiago Segarra
Categories: eess.SP cs.LG cs.NI
Comments: ASILOMAR 2023
\\
  We propose a learning-based framework for efficient power allocation in ad
hoc interference networks under episodic constraints. The problem of optimal
power allocation -- for maximizing a given network utility metric -- under
instantaneous constraints has recently gained significant popularity. Several
learnable algorithms have been proposed to obtain fast, effective, and
near-optimal performance. However, a more realistic scenario arises when the
utility metric has to be optimized for an entire episode under time-coupled
constraints. In this case, the instantaneous power needs to be regulated so
that the given utility can be optimized over an entire sequence of wireless
network realizations while satisfying the constraint at all times. Solving each
instance independently will be myopic as the long-term constraint cannot
modulate such a solution. Instead, we frame this as a constrained and
sequential decision-making problem, and employ an actor-critic algorithm to
obtain the constraint-aware power allocation at each step. We present
experimental analyses to illustrate the effectiveness of our method in terms of
superior episodic network-utility performance and its efficiency in terms of
time and computational complexity.
\\ ( https://arxiv.org/abs/2401.10297 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10298 (*cross-listing*)
Date: Thu, 18 Jan 2024 05:02:36 GMT   (2879kb,D)

Title: Machine learning approach to detect dynamical states from recurrence
  measures
Authors: Dheeraja Thakur, Athul Mohan, G. Ambika, Chandrakala Meena
Categories: physics.data-an cs.LG
\\
  We integrate machine learning approaches with nonlinear time series analysis,
specifically utilizing recurrence measures to classify various dynamical states
emerging from time series. We implement three machine learning algorithms
Logistic Regression, Random Forest, and Support Vector Machine for this study.
The input features are derived from the recurrence quantification of nonlinear
time series and characteristic measures of the corresponding recurrence
networks. For training and testing we generate synthetic data from standard
nonlinear dynamical systems and evaluate the efficiency and performance of the
machine learning algorithms in classifying time series into periodic, chaotic,
hyper-chaotic, or noisy categories. Additionally, we explore the significance
of input features in the classification scheme and find that the features
quantifying the density of recurrence points are the most relevant.
Furthermore, we illustrate how the trained algorithms can successfully predict
the dynamical states of two variable stars, SX Her and AC Her from the data of
their light curves.
\\ ( https://arxiv.org/abs/2401.10298 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10305 (*cross-listing*)
Date: Thu, 18 Jan 2024 13:18:51 GMT   (207kb,D)

Title: Personality Trait Inference Via Mobile Phone Sensors: A Machine Learning
  Approach
Authors: Wun Yung Shaney Sze, Maryglen Pearl Herrero, Roger Garriga
Categories: eess.SP cs.LG
\\
  This study provides evidence that personality can be reliably predicted from
activity data collected through mobile phone sensors. Employing a set of well
informed indicators calculable from accelerometer records and movement
patterns, we were able to predict users' personality up to a 0.78 F1 score on a
two class problem. Given the fast growing number of data collected from mobile
phones, our novel personality indicators open the door to exciting avenues for
future research in social sciences. Our results reveal distinct behavioral
patterns that proved to be differentially predictive of big five personality
traits. They potentially enable cost effective, questionnaire free
investigation of personality related questions at an unprecedented scale.
Overall, this paper shows how a combination of rich behavioral data obtained
with smartphone sensing and the use of machine learning techniques can help to
advance personality research and can inform both practitioners and researchers
about the different behavioral patterns of personality. These findings have
practical implications for organizations harnessing mobile sensor data for
personality assessment, guiding the refinement of more precise and efficient
prediction models in the future.
\\ ( https://arxiv.org/abs/2401.10305 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10306 (*cross-listing*)
Date: Thu, 18 Jan 2024 13:51:48 GMT   (3948kb,D)

Title: Physics-constrained convolutional neural networks for inverse problems
  in spatiotemporal partial differential equations
Authors: Daniel Kelshaw, Luca Magri
Categories: physics.flu-dyn cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2306.04600,
  arXiv:2306.10990
\\
  We propose a physics-constrained convolutional neural network (PC-CNN) to
solve two types of inverse problems in partial differential equations (PDEs),
which are nonlinear and vary both in space and time. In the first inverse
problem, we are given data that is offset by spatially varying systematic error
(i.e., the bias, also known the epistemic uncertainty). The task is to uncover
from the biased data the true state, which is the solution of the PDE. In the
second inverse problem, we are given sparse information on the solution of a
PDE. The task is to reconstruct the solution in space with high-resolution.
First, we present the PC-CNN, which constrains the PDE with a simple
time-windowing scheme to handle sequential data. Second, we analyse the
performance of the PC-CNN for uncovering solutions from biased data. We analyse
both linear and nonlinear convection-diffusion equations, and the Navier-Stokes
equations, which govern the spatiotemporally chaotic dynamics of turbulent
flows. We find that the PC-CNN correctly recovers the true solution for a
variety of biases, which are parameterised as non-convex functions. Third, we
analyse the performance of the PC-CNN for reconstructing solutions from biased
data for the turbulent flow. We reconstruct the spatiotemporal chaotic solution
on a high-resolution grid from only 2\% of the information contained in it. For
both tasks, we further analyse the Navier-Stokes solutions. We find that the
inferred solutions have a physical spectral energy content, whereas traditional
methods, such as interpolation, do not. This work opens opportunities for
solving inverse problems with partial differential equations.
\\ ( https://arxiv.org/abs/2401.10306 ,  3948kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10313 (*cross-listing*)
Date: Thu, 18 Jan 2024 18:47:29 GMT   (7461kb,D)

Title: Hacking Predictors Means Hacking Cars: Using Sensitivity Analysis to
  Identify Trajectory Prediction Vulnerabilities for Autonomous Driving
  Security
Authors: Marsalis Gibson, David Babazadeh, Claire Tomlin, Shankar Sastry
Categories: cs.CR cs.LG cs.RO cs.SY eess.SY
Comments: 10 pages, 6 figures, 1 tables
\\
  Adversarial attacks on learning-based trajectory predictors have already been
demonstrated. However, there are still open questions about the effects of
perturbations on trajectory predictor inputs other than state histories, and
how these attacks impact downstream planning and control. In this paper, we
conduct a sensitivity analysis on two trajectory prediction models,
Trajectron++ and AgentFormer. We observe that between all inputs, almost all of
the perturbation sensitivities for Trajectron++ lie only within the most recent
state history time point, while perturbation sensitivities for AgentFormer are
spread across state histories over time. We additionally demonstrate that,
despite dominant sensitivity on state history perturbations, an undetectable
image map perturbation made with the Fast Gradient Sign Method can induce large
prediction error increases in both models. Even though image maps may
contribute slightly to the prediction output of both models, this result
reveals that rather than being robust to adversarial image perturbations,
trajectory predictors are susceptible to image attacks. Using an
optimization-based planner and example perturbations crafted from sensitivity
results, we show how this vulnerability can cause a vehicle to come to a sudden
stop from moderate driving speeds.
\\ ( https://arxiv.org/abs/2401.10313 ,  7461kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10354 (*cross-listing*)
Date: Thu, 18 Jan 2024 19:46:24 GMT   (3913kb,D)

Title: Towards providing reliable job completion time predictions using PCS
Authors: Abdullah Bin Faisal and Noah Martin and Hafiz Mohsin Bashir and
  Swaminathan Lamelas and Fahad R. Dogar
Categories: cs.DC cs.LG
\\
  In this paper we build a case for providing job completion time predictions
to cloud users, similar to the delivery date of a package or arrival time of a
booked ride. Our analysis reveals that providing predictability can come at the
expense of performance and fairness. Existing cloud scheduling systems optimize
for extreme points in the trade-off space, making them either extremely
unpredictable or impractical.
  To address this challenge, we present PCS, a new scheduling framework that
aims to provide predictability while balancing other traditional objectives.
The key idea behind PCS is to use Weighted-Fair-Queueing (WFQ) and find a
suitable configuration of different WFQ parameters (e.g., class weights) that
meets specific goals for predictability. It uses a simulation-aided search
strategy, to efficiently discover WFQ configurations that lie on the Pareto
front of the trade-off space between these objectives. We implement and
evaluate PCS in the context of DNN job scheduling on GPUs. Our evaluation, on a
small scale GPU testbed and larger-scale simulations, shows that PCS can
provide accurate completion time estimates while marginally compromising on
performance and fairness.
\\ ( https://arxiv.org/abs/2401.10354 ,  3913kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10355 (*cross-listing*)
Date: Thu, 18 Jan 2024 19:48:53 GMT   (4553kb,D)

Title: Intelligent Optimization and Machine Learning Algorithms for Structural
  Anomaly Detection using Seismic Signals
Authors: Maximilian Trapp and Can Bogoclu and Tamara Nestorovi\'c and Dirk Roos
Categories: eess.SP cs.LG physics.app-ph
Journal-ref: Mechanical Systems and Signal Processing, Volume 133, 2019, 106250
DOI: 10.1016/j.ymssp.2019.106250
\\
  The lack of anomaly detection methods during mechanized tunnelling can cause
financial loss and deficits in drilling time. On-site excavation requires hard
obstacles to be recognized prior to drilling in order to avoid damaging the
tunnel boring machine and to adjust the propagation velocity. The efficiency of
the structural anomaly detection can be increased with intelligent optimization
techniques and machine learning. In this research, the anomaly in a simple
structure is detected by comparing the experimental measurements of the
structural vibrations with numerical simulations using parameter estimation
methods.
\\ ( https://arxiv.org/abs/2401.10355 ,  4553kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10360 (*cross-listing*)
Date: Thu, 18 Jan 2024 19:58:59 GMT   (54kb)

Title: Excuse me, sir? Your language model is leaking (information)
Authors: Or Zamir
Categories: cs.CR cs.LG
\\
  We introduce a cryptographic method to hide an arbitrary secret payload in
the response of a Large Language Model (LLM). A secret key is required to
extract the payload from the model's response, and without the key it is
provably impossible to distinguish between the responses of the original LLM
and the LLM that hides a payload. In particular, the quality of generated text
is not affected by the payload. Our approach extends a recent result of Christ,
Gunn and Zamir (2023) who introduced an undetectable watermarking scheme for
LLMs.
\\ ( https://arxiv.org/abs/2401.10360 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10370 (*cross-listing*)
Date: Thu, 18 Jan 2024 20:35:32 GMT   (2083kb,D)

Title: Deep Generative Modeling for Financial Time Series with Application in
  VaR: A Comparative Review
Authors: Lars Ericson, Xuejun Zhu, Xusi Han, Rao Fu, Shuang Li, Steve Guo, Ping
  Hu
Categories: q-fin.CP cs.LG q-fin.RM q-fin.ST
\\
  In the financial services industry, forecasting the risk factor distribution
conditional on the history and the current market environment is the key to
market risk modeling in general and value at risk (VaR) model in particular. As
one of the most widely adopted VaR models in commercial banks, Historical
simulation (HS) uses the empirical distribution of daily returns in a
historical window as the forecast distribution of risk factor returns in the
next day. The objectives for financial time series generation are to generate
synthetic data paths with good variety, and similar distribution and dynamics
to the original historical data. In this paper, we apply multiple existing deep
generative methods (e.g., CGAN, CWGAN, Diffusion, and Signature WGAN) for
conditional time series generation, and propose and test two new methods for
conditional multi-step time series generation, namely Encoder-Decoder CGAN and
Conditional TimeVAE. Furthermore, we introduce a comprehensive framework with a
set of KPIs to measure the quality of the generated time series for financial
modeling. The KPIs cover distribution distance, autocorrelation and
backtesting. All models (HS, parametric and neural networks) are tested on both
historical USD yield curve data and additional data simulated from GARCH and
CIR processes. The study shows that top performing models are HS, GARCH and
CWGAN models. Future research directions in this area are also discussed.
\\ ( https://arxiv.org/abs/2401.10370 ,  2083kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10373 (*cross-listing*)
Date: Thu, 18 Jan 2024 20:43:43 GMT   (19131kb,D)

Title: Harmonized Spatial and Spectral Learning for Robust and Generalized
  Medical Image Segmentation
Authors: Vandan Gorade, Sparsh Mittal, Debesh Jha, Rekha Singhal, Ulas Bagci
Categories: eess.IV cs.CV cs.LG
\\
  Deep learning has demonstrated remarkable achievements in medical image
segmentation. However, prevailing deep learning models struggle with poor
generalization due to (i) intra-class variations, where the same class appears
differently in different samples, and (ii) inter-class independence, resulting
in difficulties capturing intricate relationships between distinct objects,
leading to higher false negative cases. This paper presents a novel approach
that synergies spatial and spectral representations to enhance
domain-generalized medical image segmentation. We introduce the innovative
Spectral Correlation Coefficient objective to improve the model's capacity to
capture middle-order features and contextual long-range dependencies. This
objective complements traditional spatial objectives by incorporating valuable
spectral information. Extensive experiments reveal that optimizing this
objective with existing architectures like UNet and TransUNet significantly
enhances generalization, interpretability, and noise robustness, producing more
confident predictions. For instance, in cardiac segmentation, we observe a 0.81
pp and 1.63 pp (pp = percentage point) improvement in DSC over UNet and
TransUNet, respectively. Our interpretability study demonstrates that, in most
tasks, objectives optimized with UNet outperform even TransUNet by introducing
global contextual information alongside local details. These findings
underscore the versatility and effectiveness of our proposed method across
diverse imaging modalities and medical domains.
\\ ( https://arxiv.org/abs/2401.10373 ,  19131kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10375 (*cross-listing*)
Date: Thu, 18 Jan 2024 20:56:42 GMT   (1666kb,D)

Title: Vulnerabilities of Foundation Model Integrated Federated Learning Under
  Adversarial Threats
Authors: Chen Wu, Xi Li, Jiaqi Wang
Categories: cs.CR cs.DC cs.LG
Comments: Chen Wu and Xi Li are equal contribution. The corresponding author is
  Jiaqi Wang
\\
  Federated Learning (FL) addresses critical issues in machine learning related
to data privacy and security, yet suffering from data insufficiency and
imbalance under certain circumstances. The emergence of foundation models (FMs)
offers potential solutions to the limitations of existing FL frameworks, e.g.,
by generating synthetic data for model initialization. However, due to the
inherent safety concerns of FMs, integrating FMs into FL could introduce new
risks, which remains largely unexplored. To address this gap, we conduct the
first investigation on the vulnerability of FM integrated FL (FM-FL) under
adversarial threats. Based on a unified framework of FM-FL, we introduce a
novel attack strategy that exploits safety issues of FM to compromise FL client
models. Through extensive experiments with well-known models and benchmark
datasets in both image and text domains, we reveal the high susceptibility of
the FM-FL to this new threat under various FL configurations. Furthermore, we
find that existing FL defense strategies offer limited protection against this
novel attack approach. This research highlights the critical need for enhanced
security measures in FL in the era of FMs.
\\ ( https://arxiv.org/abs/2401.10375 ,  1666kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10385 (*cross-listing*)
Date: Thu, 18 Jan 2024 21:45:09 GMT   (1507kb,D)

Title: Approximation of Solution Operators for High-dimensional PDEs
Authors: Nathan Gaby and Xiaojing Ye
Categories: math.NA cs.LG cs.NA math.OC
Comments: 14 pages, 4 page appendix, 4 figures
MSC-class: 65M99, 49M05
\\
  We propose a finite-dimensional control-based method to approximate solution
operators for evolutional partial differential equations (PDEs), particularly
in high-dimensions. By employing a general reduced-order model, such as a deep
neural network, we connect the evolution of the model parameters with
trajectories in a corresponding function space. Using the computational
technique of neural ordinary differential equation, we learn the control over
the parameter space such that from any initial starting point, the controlled
trajectories closely approximate the solutions to the PDE. Approximation
accuracy is justified for a general class of second-order nonlinear PDEs.
Numerical results are presented for several high-dimensional PDEs, including
real-world applications to solving Hamilton-Jacobi-Bellman equations. These are
demonstrated to show the accuracy and efficiency of the proposed method.
\\ ( https://arxiv.org/abs/2401.10385 ,  1507kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10396 (*cross-listing*)
Date: Thu, 18 Jan 2024 22:10:21 GMT   (1575kb,D)

Title: Deep Dict: Deep Learning-based Lossy Time Series Compressor for IoT Data
Authors: Jinxin Liu, Petar Djukic, Michel Kulhandjian, Burak Kantarci
Categories: eess.SP cs.IT cs.LG math.IT
Comments: 6 pages, 13 figures, IEEE International Conference on Communications
  (ICC) 2024
\\
  We propose Deep Dict, a deep learning-based lossy time series compressor
designed to achieve a high compression ratio while maintaining decompression
error within a predefined range. Deep Dict incorporates two essential
components: the Bernoulli transformer autoencoder (BTAE) and a distortion
constraint. BTAE extracts Bernoulli representations from time series data,
reducing the size of the representations compared to conventional autoencoders.
The distortion constraint limits the prediction error of BTAE to the desired
range. Moreover, in order to address the limitations of common regression
losses such as L1/L2, we introduce a novel loss function called quantized
entropy loss (QEL). QEL takes into account the specific characteristics of the
problem, enhancing robustness to outliers and alleviating optimization
challenges. Our evaluation of Deep Dict across ten diverse time series datasets
from various domains reveals that Deep Dict outperforms state-of-the-art lossy
compressors in terms of compression ratio by a significant margin by up to
53.66%.
\\ ( https://arxiv.org/abs/2401.10396 ,  1575kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10419 (*cross-listing*)
Date: Thu, 18 Jan 2024 23:10:08 GMT   (1917kb)

Title: M3BUNet: Mobile Mean Max UNet for Pancreas Segmentation on CT-Scans
Authors: Juwita juwita, Ghulam Mubashar Hassan, Naveed Akhtar, Amitava Datta
Categories: eess.IV cs.CV cs.LG
\\
  Segmenting organs in CT scan images is a necessary process for multiple
downstream medical image analysis tasks. Currently, manual CT scan segmentation
by radiologists is prevalent, especially for organs like the pancreas, which
requires a high level of domain expertise for reliable segmentation due to
factors like small organ size, occlusion, and varying shapes. When resorting to
automated pancreas segmentation, these factors translate to limited reliable
labeled data to train effective segmentation models. Consequently, the
performance of contemporary pancreas segmentation models is still not within
acceptable ranges. To improve that, we propose M3BUNet, a fusion of MobileNet
and U-Net neural networks, equipped with a novel Mean-Max (MM) attention that
operates in two stages to gradually segment pancreas CT images from coarse to
fine with mask guidance for object detection. This approach empowers the
network to surpass segmentation performance achieved by similar network
architectures and achieve results that are on par with complex state-of-the-art
methods, all while maintaining a low parameter count. Additionally, we
introduce external contour segmentation as a preprocessing step for the coarse
stage to assist in the segmentation process through image standardization. For
the fine segmentation stage, we found that applying a wavelet decomposition
filter to create multi-input images enhances pancreas segmentation performance.
We extensively evaluate our approach on the widely known NIH pancreas dataset
and MSD pancreas dataset. Our approach demonstrates a considerable performance
improvement, achieving an average Dice Similarity Coefficient (DSC) value of up
to 89.53% and an Intersection Over Union (IOU) score of up to 81.16 for the NIH
pancreas dataset, and 88.60% DSC and 79.90% IOU for the MSD Pancreas dataset.
\\ ( https://arxiv.org/abs/2401.10419 ,  1917kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10442 (*cross-listing*)
Date: Fri, 19 Jan 2024 01:11:44 GMT   (31182kb,D)

Title: Path Choice Matters for Clear Attribution in Path Methods
Authors: Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
Categories: cs.CV cs.LG
Comments: ICLR 2024 accepted
\\
  Rigorousness and clarity are both essential for interpretations of DNNs to
engender human trust. Path methods are commonly employed to generate rigorous
attributions that satisfy three axioms. However, the meaning of attributions
remains ambiguous due to distinct path choices. To address the ambiguity, we
introduce \textbf{Concentration Principle}, which centrally allocates high
attributions to indispensable features, thereby endowing aesthetic and
sparsity. We then present \textbf{SAMP}, a model-agnostic interpreter, which
efficiently searches the near-optimal path from a pre-defined set of
manipulation paths. Moreover, we propose the infinitesimal constraint (IC) and
momentum strategy (MS) to improve the rigorousness and optimality.
Visualizations show that SAMP can precisely reveal DNNs by pinpointing salient
image pixels. We also perform quantitative experiments and observe that our
method significantly outperforms the counterparts. Code:
https://github.com/zbr17/SAMP.
\\ ( https://arxiv.org/abs/2401.10442 ,  31182kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10451 (*cross-listing*)
Date: Fri, 19 Jan 2024 01:40:58 GMT   (2179kb,D)

Title: Learning-assisted Stochastic Capacity Expansion Planning: A Bayesian
  Optimization Approach
Authors: Aron Brenner, Rahman Khorramfar, Dharik Mallapragada, Saurabh Amin
Categories: eess.SY cs.LG cs.SY
\\
  Solving large-scale capacity expansion problems (CEPs) is central to
cost-effective decarbonization of regional-scale energy systems. To ensure the
intended outcomes of CEPs, modeling uncertainty due to weather-dependent
variable renewable energy (VRE) supply and energy demand becomes crucially
important. However, the resulting stochastic optimization models are often less
computationally tractable than their deterministic counterparts. Here, we
propose a learning-assisted approximate solution method to tractably solve
two-stage stochastic CEPs. Our method identifies low-cost planning decisions by
constructing and solving a sequence of tractable temporally aggregated
surrogate problems. We adopt a Bayesian optimization approach to searching the
space of time series aggregation hyperparameters and compute approximate
solutions that minimize costs on a validation set of supply-demand projections.
Importantly, we evaluate solved planning outcomes on a held-out set of test
projections. We apply our approach to generation and transmission expansion
planning for a joint power-gas system spanning New England. We show that our
approach yields an estimated cost savings of up to 3.8% in comparison to
benchmark time series aggregation approaches.
\\ ( https://arxiv.org/abs/2401.10451 ,  2179kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10460 (*cross-listing*)
Date: Fri, 19 Jan 2024 02:51:00 GMT   (3613kb,D)

Title: Ultra-lightweight Neural Differential DSP Vocoder For High Quality
  Speech Synthesis
Authors: Prabhav Agrawal, Thilo Koehler, Zhiping Xiu, Prashant Serai, Qing He
Categories: cs.SD cs.LG eess.AS
Comments: Accepted for ICASSP 2024
\\
  Neural vocoders model the raw audio waveform and synthesize high-quality
audio, but even the highly efficient ones, like MB-MelGAN and LPCNet, fail to
run real-time on a low-end device like a smartglass. A pure digital signal
processing (DSP) based vocoder can be implemented via lightweight fast Fourier
transforms (FFT), and therefore, is a magnitude faster than any neural vocoder.
A DSP vocoder often gets a lower audio quality due to consuming over-smoothed
acoustic model predictions of approximate representations for the vocal tract.
In this paper, we propose an ultra-lightweight differential DSP (DDSP) vocoder
that uses a jointly optimized acoustic model with a DSP vocoder, and learns
without an extracted spectral feature for the vocal tract. The model achieves
audio quality comparable to neural vocoders with a high average MOS of 4.36
while being efficient as a DSP vocoder. Our C++ implementation, without any
hardware-specific optimization, is at 15 MFLOPS, surpasses MB-MelGAN by 340
times in terms of FLOPS, and achieves a vocoder-only RTF of 0.003 and overall
RTF of 0.044 while running single-threaded on a 2GHz Intel Xeon CPU.
\\ ( https://arxiv.org/abs/2401.10460 ,  3613kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10522 (*cross-listing*)
Date: Fri, 19 Jan 2024 06:56:09 GMT   (710kb)

Title: FARe: Fault-Aware GNN Training on ReRAM-based PIM Accelerators
Authors: Pratyush Dhingra, Chukwufumnanya Ogbogu, Biresh Kumar Joardar,
  Janardhan Rao Doppa, Ananth Kalyanaraman, Partha Pratim Pande
Categories: cs.AR cs.LG
Comments: This paper has been accepted to the conference DATE (Design,
  Automation and Test in Europe) - 2024
ACM-class: B.8.1
\\
  Resistive random-access memory (ReRAM)-based processing-in-memory (PIM)
architecture is an attractive solution for training Graph Neural Networks
(GNNs) on edge platforms. However, the immature fabrication process and limited
write endurance of ReRAMs make them prone to hardware faults, thereby limiting
their widespread adoption for GNN training. Further, the existing
fault-tolerant solutions prove inadequate for effectively training GNNs in the
presence of faults. In this paper, we propose a fault-aware framework referred
to as FARe that mitigates the effect of faults during GNN training. FARe
outperforms existing approaches in terms of both accuracy and timing overhead.
Experimental results demonstrate that FARe framework can restore GNN test
accuracy by 47.6% on faulty ReRAM hardware with a ~1% timing overhead compared
to the fault-free counterpart.
\\ ( https://arxiv.org/abs/2401.10522 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10637 (*cross-listing*)
Date: Fri, 19 Jan 2024 11:35:07 GMT   (11644kb,D)

Title: Towards Universal Unsupervised Anomaly Detection in Medical Imaging
Authors: Cosmin I. Bercea and Benedikt Wiestler and Daniel Rueckert and Julia
  A. Schnabel
Categories: eess.IV cs.CV cs.LG
\\
  The increasing complexity of medical imaging data underscores the need for
advanced anomaly detection methods to automatically identify diverse
pathologies. Current methods face challenges in capturing the broad spectrum of
anomalies, often limiting their use to specific lesion types in brain scans. To
address this challenge, we introduce a novel unsupervised approach, termed
\textit{Reversed Auto-Encoders (RA)}, designed to create realistic
pseudo-healthy reconstructions that enable the detection of a wider range of
pathologies. We evaluate the proposed method across various imaging modalities,
including magnetic resonance imaging (MRI) of the brain, pediatric wrist X-ray,
and chest X-ray, and demonstrate superior performance in detecting anomalies
compared to existing state-of-the-art methods. Our unsupervised anomaly
detection approach may enhance diagnostic accuracy in medical imaging by
identifying a broader range of unknown pathologies. Our code is publicly
available at: \url{https://github.com/ci-ber/RA}.
\\ ( https://arxiv.org/abs/2401.10637 ,  11644kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10646 (*cross-listing*)
Date: Fri, 19 Jan 2024 11:47:49 GMT   (2286kb,D)

Title: Empowering HWNs with Efficient Data Labeling: A Clustered Federated
  Semi-Supervised Learning Approach
Authors: Moqbel Hamood and Abdullatif Albaseer and Mohamed Abdallah and Ala
  Al-Fuqaha
Categories: cs.NI cs.LG
Comments: Accepted for IEEE Wireless Communications and Networking Conference
  (WCNC) 2024
\\
  Clustered Federated Multitask Learning (CFL) has gained considerable
attention as an effective strategy for overcoming statistical challenges,
particularly when dealing with non independent and identically distributed (non
IID) data across multiple users. However, much of the existing research on CFL
operates under the unrealistic premise that devices have access to accurate
ground truth labels. This assumption becomes especially problematic in
hierarchical wireless networks (HWNs), where edge networks contain a large
amount of unlabeled data, resulting in slower convergence rates and increased
processing times, particularly when dealing with two layers of model
aggregation. To address these issues, we introduce a novel framework, Clustered
Federated Semi-Supervised Learning (CFSL), designed for more realistic HWN
scenarios. Our approach leverages a best-performing specialized model
algorithm, wherein each device is assigned a specialized model that is highly
adept at generating accurate pseudo-labels for unlabeled data, even when the
data stems from diverse environments. We validate the efficacy of CFSL through
extensive experiments, comparing it with existing methods highlighted in recent
literature. Our numerical results demonstrate that CFSL significantly improves
upon key metrics such as testing accuracy, labeling accuracy, and labeling
latency under varying proportions of labeled and unlabeled data while also
accommodating the non-IID nature of the data and the unique characteristics of
wireless edge networks.
\\ ( https://arxiv.org/abs/2401.10646 ,  2286kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10652 (*cross-listing*)
Date: Fri, 19 Jan 2024 11:58:13 GMT   (1991kb,D)

Title: AutoChunk: Automated Activation Chunk for Memory-Efficient Long Sequence
  Inference
Authors: Xuanlei Zhao, Shenggan Cheng, Guangyang Lu, Jiarui Fang, Haotian Zhou,
  Bin Jia, Ziming Liu, Yang You
Categories: cs.PF cs.DC cs.LG
Comments: ICLR 2024
\\
  Large deep learning models have achieved impressive performance across a
range of applications. However, their large memory requirements, including
parameter memory and activation memory, have become a significant challenge for
their practical serving. While existing methods mainly address parameter
memory, the importance of activation memory has been overlooked. Especially for
long input sequences, activation memory is expected to experience a significant
exponential growth as the length of sequences increases. In this approach, we
propose AutoChunk, an automatic and adaptive compiler system that efficiently
reduces activation memory for long sequence inference by chunk strategies. The
proposed system generates chunk plans by optimizing through multiple stages. In
each stage, the chunk search pass explores all possible chunk candidates and
the chunk selection pass identifies the optimal one. At runtime, AutoChunk
employs code generation to automatically apply chunk strategies. The
experiments demonstrate that AutoChunk can reduce over 80\% of activation
memory while maintaining speed loss within 10%, extend max sequence length by
3.2x to 11.7x, and outperform state-of-the-art methods by a large margin.
\\ ( https://arxiv.org/abs/2401.10652 ,  1991kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10674 (*cross-listing*)
Date: Fri, 19 Jan 2024 13:13:38 GMT   (347kb,D)

Title: Deep Learning-based Embedded Intrusion Detection System for Automotive
  CAN
Authors: Shashwat Khandelwal, Eashan Wadhwa, Shreejith Shanker
Categories: cs.CR cs.LG
Comments: 5 pages, 1 figure, 8 tables
Journal-ref: IEEE 33rd International Conference on Application-specific
  Systems, Architectures and Processors (ASAP), Gothenburg, Sweden, 2022, pp.
  88-92
DOI: 10.1109/ASAP54787.2022.00023
\\
  Rising complexity of in-vehicle electronics is enabling new capabilities like
autonomous driving and active safety. However, rising automation also increases
risk of security threats which is compounded by lack of in-built security
measures in legacy networks like CAN, allowing attackers to observe, tamper and
modify information shared over such broadcast networks. Various intrusion
detection approaches have been proposed to detect and tackle such threats, with
machine learning models proving highly effective. However, deploying machine
learning models will require high processing power through high-end processors
or GPUs to perform them close to line rate. In this paper, we propose a hybrid
FPGA-based ECU approach that can transparently integrate IDS functionality
through a dedicated off-the-shelf hardware accelerator that implements a
deep-CNN intrusion detection model. Our results show that the proposed approach
provides an average accuracy of over 99% across multiple attack datasets with
0.64% false detection rates while consuming 94% less energy and achieving 51.8%
reduction in per-message processing latency when compared to IDS
implementations on GPUs.
\\ ( https://arxiv.org/abs/2401.10674 ,  347kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10689 (*cross-listing*)
Date: Fri, 19 Jan 2024 13:39:05 GMT   (477kb,D)

Title: A Lightweight Multi-Attack CAN Intrusion Detection System on Hybrid
  FPGAs
Authors: Shashwat Khandelwal, Shreejith Shanker
Categories: cs.CR cs.LG cs.SY eess.SY
Comments: 5 pages, 2 figures, 6 tables
Journal-ref: 32nd International Conference on Field-Programmable Logic and
  Applications (FPL) FPL 2022, 425-429
DOI: 10.1109/FPL57034.2022.00070
\\
  Rising connectivity in vehicles is enabling new capabilities like connected
autonomous driving and advanced driver assistance systems (ADAS) for improving
the safety and reliability of next-generation vehicles. This increased access
to in-vehicle functions compromises critical capabilities that use legacy
invehicle networks like Controller Area Network (CAN), which has no inherent
security or authentication mechanism. Intrusion detection and mitigation
approaches, particularly using machine learning models, have shown promising
results in detecting multiple attack vectors in CAN through their ability to
generalise to new vectors. However, most deployments require dedicated
computing units like GPUs to perform line-rate detection, consuming much higher
power. In this paper, we present a lightweight multi-attack quantised machine
learning model that is deployed using Xilinx's Deep Learning Processing Unit IP
on a Zynq Ultrascale+ (XCZU3EG) FPGA, which is trained and validated using the
public CAN Intrusion Detection dataset. The quantised model detects denial of
service and fuzzing attacks with an accuracy of above 99 % and a false positive
rate of 0.07%, which are comparable to the state-of-the-art techniques in the
literature. The Intrusion Detection System (IDS) execution consumes just 2.0 W
with software tasks running on the ECU and achieves a 25 % reduction in
per-message processing latency over the state-of-the-art implementations. This
deployment allows the ECU function to coexist with the IDS with minimal changes
to the tasks, making it ideal for real-time IDS in in-vehicle systems.
\\ ( https://arxiv.org/abs/2401.10689 ,  477kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10721 (*cross-listing*)
Date: Fri, 19 Jan 2024 14:32:50 GMT   (10689kb,D)

Title: Generative Model for Constructing Reaction Path from Initial to Final
  States
Authors: Akihide Hayashi, So Takamoto, Ju Li, Daisuke Okanohara
Categories: physics.comp-ph cs.LG physics.chem-ph
\\
  Mapping out reaction pathways and their corresponding activation barriers is
a significant aspect of molecular simulation. Given their inherent complexity
and nonlinearity, even generating a initial guess of these paths remains a
challenging problem. Presented in this paper is an innovative approach that
utilizes neural networks to generate initial guess for these reaction pathways.
The proposed method is initiated by inputting the coordinates of the initial
state, followed by progressive alterations to its structure. This iterative
process culminates in the generation of the approximate representation of the
reaction path and the coordinates of the final state. The application of this
method extends to complex reaction pathways illustrated by organic reactions.
Training was executed on the Transition1x dataset, an organic reaction pathway
dataset. The results revealed generation of reactions that bore substantial
similarities with the corresponding test data. The method's flexibility allows
for reactions to be generated either to conform to predetermined conditions or
in a randomized manner.
\\ ( https://arxiv.org/abs/2401.10721 ,  10689kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10724 (*cross-listing*)
Date: Fri, 19 Jan 2024 14:36:01 GMT   (1475kb,D)

Title: Real-Time Zero-Day Intrusion Detection System for Automotive Controller
  Area Network on FPGAs
Authors: Shashwat Khandelwal, Shreejith Shanker
Categories: cs.CR cs.LG cs.SY eess.SY
Comments: 8 pages, 6 figures, 7 tables
Journal-ref: 2023 IEEE 34th International Conference on Application-specific
  Systems, Architectures and Processors (ASAP)
DOI: 10.1109/ASAP57973.2023.00033
\\
  Increasing automation in vehicles enabled by increased connectivity to the
outside world has exposed vulnerabilities in previously siloed automotive
networks like controller area networks (CAN). Attributes of CAN such as
broadcast-based communication among electronic control units (ECUs) that
lowered deployment costs are now being exploited to carry out active injection
attacks like denial of service (DoS), fuzzing, and spoofing attacks. Research
literature has proposed multiple supervised machine learning models deployed as
Intrusion detection systems (IDSs) to detect such malicious activity; however,
these are largely limited to identifying previously known attack vectors. With
the ever-increasing complexity of active injection attacks, detecting zero-day
(novel) attacks in these networks in real-time (to prevent propagation) becomes
a problem of particular interest. This paper presents an
unsupervised-learning-based convolutional autoencoder architecture for
detecting zero-day attacks, which is trained only on benign (attack-free) CAN
messages. We quantise the model using Vitis-AI tools from AMD/Xilinx targeting
a resource-constrained Zynq Ultrascale platform as our IDS-ECU system for
integration. The proposed model successfully achieves equal or higher
classification accuracy (> 99.5%) on unseen DoS, fuzzing, and spoofing attacks
from a publicly available attack dataset when compared to the state-of-the-art
unsupervised learning-based IDSs. Additionally, by cleverly overlapping IDS
operation on a window of CAN messages with the reception, the model is able to
meet line-rate detection (0.43 ms per window) of high-speed CAN, which when
coupled with the low energy consumption per inference, makes this architecture
ideally suited for detecting zero-day attacks on critical CAN networks.
\\ ( https://arxiv.org/abs/2401.10724 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10726 (*cross-listing*)
Date: Fri, 19 Jan 2024 14:43:04 GMT   (473kb,D)

Title: Empowering Aggregators with Practical Data-Driven Tools: Harnessing
  Aggregated and Disaggregated Flexibility for Demand Response
Authors: Costas Mylonas, Donata Boric, Leila Luttenberger Maric, Alexandros
  Tsitsanis, Eleftheria Petrianou, Magda Foti
Categories: eess.SY cs.LG cs.SY
\\
  This study explores the crucial interplay between aggregators and building
occupants in activating flexibility through Demand Response (DR) programs, with
a keen focus on achieving robust decarbonization and fortifying the resilience
of the energy system amidst the uncertainties presented by Renewable Energy
Sources (RES). Firstly, it introduces a methodology of optimizing aggregated
flexibility provision strategies in environments with limited data, utilizing
Discrete Fourier Transformation (DFT) and clustering techniques to identify
building occupant's activity patterns. Secondly, the study assesses the
disaggregated flexibility provision of Heating Ventilation and Air Conditioning
(HVAC) systems during DR events, employing machine learning and optimization
techniques for precise, device-level analysis. The first approach offers a
non-intrusive pathway for aggregators to provide flexibility services in
environments of a single smart meter for the whole building's consumption,
while the second approach carefully considers building occupants' thermal
comfort profiles, while maximizing flexibility in case of existence of
dedicated smart meters to the HVAC systems. Through the application of
data-driven techniques and encompassing case studies from both industrial and
residential buildings, this paper not only unveils pivotal opportunities for
aggregators in the balancing and emerging flexibility markets but also
successfully develops end-to-end practical tools for aggregators. Furthermore,
the efficacy of this tool is validated through detailed case studies,
substantiating its operational capability and contributing to the evolution of
a resilient and efficient energy system.
\\ ( https://arxiv.org/abs/2401.10726 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10748 (*cross-listing*)
Date: Thu, 28 Dec 2023 18:30:13 GMT   (2582kb,D)

Title: Fast gradient-free activation maximization for neurons in spiking neural
  networks
Authors: Nikita Pospelov, Andrei Chertkov, Maxim Beketov, Ivan Oseledets,
  Konstantin Anokhin
Categories: cs.NE cs.LG
\\
  Neural networks (NNs), both living and artificial, work due to being complex
systems of neurons, each having its own specialization. Revealing these
specializations is important for understanding NNs inner working mechanisms.
The only way to do this for a living system, the neural response of which to a
stimulus is not a known (let alone differentiable) function is to build a
feedback loop of exposing it to stimuli, the properties of which can be
iteratively varied aiming in the direction of maximal response. To test such a
loop on a living network, one should first learn how to run it quickly and
efficiently, reaching most effective stimuli (ones that maximize certain
neurons activation) in least possible number of iterations. We present a
framework with an effective design of such a loop, successfully testing it on
an artificial spiking neural network (SNN, a model that mimics the behaviour of
NNs in living brains). Our optimization method used for activation maximization
(AM) was based on low-rank tensor decomposition (Tensor Train, TT) of the
activation function's discretization over its domain the latent parameter space
of stimuli (CIFAR10-size color images, generated by either VQ-VAE or SN-GAN
from their latent description vectors, fed to the SNN). To our knowledge, the
present work is the first attempt to perform effective AM for SNNs. The source
code of our framework, MANGO (for Maximization of neural Activation via
Non-Gradient Optimization) is available on GitHub.
\\ ( https://arxiv.org/abs/2401.10748 ,  2582kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10749 (*cross-listing*)
Date: Fri, 29 Dec 2023 07:30:58 GMT   (4407kb,D)

Title: ReliCD: A Reliable Cognitive Diagnosis Framework with Confidence
  Awareness
Authors: Yunfei Zhang, Chuan Qin, Dazhong Shen, Haiping Ma, Le Zhang, Xingyi
  Zhang, Hengshu Zhu
Categories: cs.CY cs.LG
\\
  During the past few decades, cognitive diagnostics modeling has attracted
increasing attention in computational education communities, which is capable
of quantifying the learning status and knowledge mastery levels of students.
Indeed, the recent advances in neural networks have greatly enhanced the
performance of traditional cognitive diagnosis models through learning the deep
representations of students and exercises. Nevertheless, existing approaches
often suffer from the issue of overconfidence in predicting students' mastery
levels, which is primarily caused by the unavoidable noise and sparsity in
realistic student-exercise interaction data, severely hindering the educational
application of diagnostic feedback. To address this, in this paper, we propose
a novel Reliable Cognitive Diagnosis(ReliCD) framework, which can quantify the
confidence of the diagnosis feedback and is flexible for different cognitive
diagnostic functions. Specifically, we first propose a Bayesian method to
explicitly estimate the state uncertainty of different knowledge concepts for
students, which enables the confidence quantification of diagnostic feedback.
In particular, to account for potential differences, we suggest modeling
individual prior distributions for the latent variables of different ability
concepts using a pre-trained model. Additionally, we introduce a logical
hypothesis for ranking confidence levels. Along this line, we design a novel
calibration loss to optimize the confidence parameters by modeling the process
of student performance prediction. Finally, extensive experiments on four
real-world datasets clearly demonstrate the effectiveness of our ReliCD
framework.
\\ ( https://arxiv.org/abs/2401.10749 ,  4407kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10790 (*cross-listing*)
Date: Fri, 19 Jan 2024 16:21:55 GMT   (381kb)

Title: Measuring the Impact of Scene Level Objects on Object Detection: Towards
  Quantitative Explanations of Detection Decisions
Authors: Lynn Vonder Haar, Timothy Elvira, Luke Newcomb, Omar Ochoa
Categories: cs.CV cs.LG
Comments: 9 pages, 4 figures, 1 table
\\
  Although accuracy and other common metrics can provide a useful window into
the performance of an object detection model, they lack a deeper view of the
model's decision process. Regardless of the quality of the training data and
process, the features that an object detection model learns cannot be
guaranteed. A model may learn a relationship between certain background
context, i.e., scene level objects, and the presence of the labeled classes.
Furthermore, standard performance verification and metrics would not identify
this phenomenon. This paper presents a new black box explainability method for
additional verification of object detection models by finding the impact of
scene level objects on the identification of the objects within the image. By
comparing the accuracies of a model on test data with and without certain scene
level objects, the contributions of these objects to the model's performance
becomes clearer. The experiment presented here will assess the impact of
buildings and people in image context on the detection of emergency road
vehicles by a fine-tuned YOLOv8 model. A large increase in accuracy in the
presence of a scene level object will indicate the model's reliance on that
object to make its detections. The results of this research lead to providing a
quantitative explanation of the object detection model's decision process,
enabling a deeper understanding of the model's performance.
\\ ( https://arxiv.org/abs/2401.10790 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10800 (*cross-listing*)
Date: Fri, 19 Jan 2024 16:36:27 GMT   (484kb,D)

Title: Estimation of AMOC transition probabilities using a machine learning
  based rare-event algorithm
Authors: Val\'erian Jacques-Dumas, Ren\'e M. van Westen and Henk A. Dijkstra
Categories: physics.ao-ph cs.LG
Comments: 16 pages, 9 figures
\\
  The Atlantic Meridional Overturning Circulation (AMOC) is an important
component of the global climate, known to be a tipping element, as it could
collapse under global warming. The main objective of this study is to compute
the probability that the AMOC collapses within a specified time window, using a
rare-event algorithm called Trajectory-Adaptive Multilevel Splitting (TAMS).
However, the efficiency and accuracy of TAMS depend on the choice of the score
function. Although the definition of the optimal score function, called
``committor function" is known, it is impossible in general to compute it a
priori. Here, we combine TAMS with a Next-Generation Reservoir Computing
technique that estimates the committor function from the data generated by the
rare-event algorithm. We test this technique in a stochastic box model of the
AMOC for which two types of transition exist, the so-called F(ast)-transitions
and S(low)-transitions. Results for the F-transtions compare favorably with
those in the literature where a physically-informed score function was used. We
show that coupling a rare-event algorithm with machine learning allows for a
correct estimation of transition probabilities, transition times, and even
transition paths for a wide range of model parameters. We then extend these
results to the more difficult problem of S-transitions in the same model. In
both cases of F- and S-transitions, we also show how the Next-Generation
Reservoir Computing technique can be interpreted to retrieve an analytical
estimate of the committor function.
\\ ( https://arxiv.org/abs/2401.10800 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10811 (*cross-listing*)
Date: Fri, 19 Jan 2024 16:56:11 GMT   (2275kb,D)

Title: Simulation Based Bayesian Optimization
Authors: Roi Naveiro, Becky Tang
Categories: stat.ML cs.LG
\\
  Bayesian Optimization (BO) is a powerful method for optimizing black-box
functions by combining prior knowledge with ongoing function evaluations. BO
constructs a probabilistic surrogate model of the objective function given the
covariates, which is in turn used to inform the selection of future evaluation
points through an acquisition function. For smooth continuous search spaces,
Gaussian Processes (GPs) are commonly used as the surrogate model as they offer
analytical access to posterior predictive distributions, thus facilitating the
computation and optimization of acquisition functions. However, in complex
scenarios involving optimizations over categorical or mixed covariate spaces,
GPs may not be ideal.
  This paper introduces Simulation Based Bayesian Optimization (SBBO) as a
novel approach to optimizing acquisition functions that only requires
\emph{sampling-based} access to posterior predictive distributions. SBBO allows
the use of surrogate probabilistic models tailored for combinatorial spaces
with discrete variables. Any Bayesian model in which posterior inference is
carried out through Markov chain Monte Carlo can be selected as the surrogate
model in SBBO. In applications involving combinatorial optimization, we
demonstrate empirically the effectiveness of SBBO method using various choices
of surrogate models.
\\ ( https://arxiv.org/abs/2401.10811 ,  2275kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10843 (*cross-listing*)
Date: Fri, 5 Jan 2024 09:54:44 GMT   (12729kb,D)

Title: Training a General Spiking Neural Network with Improved Efficiency and
  Minimum Latency
Authors: Yunpeng Yao, Man Wu, Zheng Chen, Renyuan Zhang
Categories: cs.NE cs.LG
Comments: Accepted by ACML 2023
\\
  Spiking Neural Networks (SNNs) that operate in an event-driven manner and
employ binary spike representation have recently emerged as promising
candidates for energy-efficient computing. However, a cost bottleneck arises in
obtaining high-performance SNNs: training a SNN model requires a large number
of time steps in addition to the usual learning iterations, hence this limits
their energy efficiency. This paper proposes a general training framework that
enhances feature learning and activation efficiency within a limited time step,
providing a new solution for more energy-efficient SNNs. Our framework allows
SNN neurons to learn robust spike feature from different receptive fields and
update neuron states by utilizing both current stimuli and recurrence
information transmitted from other neurons. This setting continuously
complements information within a single time step. Additionally, we propose a
projection function to merge these two stimuli to smoothly optimize neuron
weights (spike firing threshold and activation). We evaluate the proposal for
both convolution and recurrent models. Our experimental results indicate
state-of-the-art visual classification tasks, including CIFAR10, CIFAR100, and
TinyImageNet.Our framework achieves 72.41% and 72.31% top-1 accuracy with only
1 time step on CIFAR100 for CNNs and RNNs, respectively. Our method reduces 10x
and 3x joule energy than a standard ANN and SNN, respectively, on CIFAR10,
without additional time steps.
\\ ( https://arxiv.org/abs/2401.10843 ,  12729kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10844 (*cross-listing*)
Date: Sat, 6 Jan 2024 06:54:58 GMT   (673kb)

Title: Neural Population Decoding and Imbalanced Multi-Omic Datasets For Cancer
  Subtype Diagnosis
Authors: Charles Theodore Kent, Leila Bagheriye and Johan Kwisthout
Categories: cs.NE cs.LG q-bio.GN q-bio.QM stat.AP
Comments: This paper has been accepted in BIOINFORMATICS 2024 (BIOSTEC 2024)
\\
  Recent strides in the field of neural computation has seen the adoption of
Winner Take All (WTA) circuits to facilitate the unification of hierarchical
Bayesian inference and spiking neural networks as a neurobiologically plausible
model of information processing. Current research commonly validates the
performance of these networks via classification tasks, particularly of the
MNIST dataset. However, researchers have not yet reached consensus about how
best to translate the stochastic responses from these networks into discrete
decisions, a process known as population decoding. Despite being an often
underexamined part of SNNs, in this work we show that population decoding has a
significanct impact on the classification performance of WTA networks. For this
purpose, we apply a WTA network to the problem of cancer subtype diagnosis from
multi omic data, using datasets from The Cancer Genome Atlas (TCGA). In doing
so we utilise a novel implementation of gene similarity networks, a feature
encoding technique based on Kohoens self organising map algorithm. We further
show that the impact of selecting certain population decoding methods is
amplified when facing imbalanced datasets.
\\ ( https://arxiv.org/abs/2401.10844 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10859 (*cross-listing*)
Date: Fri, 19 Jan 2024 18:03:21 GMT   (9265kb,D)

Title: Ensembler: Combating model inversion attacks using model ensemble during
  collaborative inference
Authors: Dancheng Liu, Jinjun Xiong
Categories: cs.CR cs.LG
Comments: in submission
\\
  Deep learning models have exhibited remarkable performance across various
domains. Nevertheless, the burgeoning model sizes compel edge devices to
offload a significant portion of the inference process to the cloud. While this
practice offers numerous advantages, it also raises critical concerns regarding
user data privacy. In scenarios where the cloud server's trustworthiness is in
question, the need for a practical and adaptable method to safeguard data
privacy becomes imperative. In this paper, we introduce Ensembler, an
extensible framework designed to substantially increase the difficulty of
conducting model inversion attacks for adversarial parties. Ensembler leverages
model ensembling on the adversarial server, running in parallel with existing
approaches that introduce perturbations to sensitive data during colloborative
inference. Our experiments demonstrate that when combined with even basic
Gaussian noise, Ensembler can effectively shield images from reconstruction
attacks, achieving recognition levels that fall below human performance in some
strict settings, significantly outperforming baseline methods lacking the
Ensembler framework.
\\ ( https://arxiv.org/abs/2401.10859 ,  9265kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10874 (*cross-listing*)
Date: Fri, 19 Jan 2024 18:33:52 GMT   (772kb,D)

Title: Applications of flow models to the generation of correlated lattice QCD
  ensembles
Authors: Ryan Abbott, Aleksandar Botev, Denis Boyda, Daniel C. Hackett, Gurtej
  Kanwar, S\'ebastien Racani\`ere, Danilo J. Rezende, Fernando Romero-L\'opez,
  Phiala E. Shanahan and Julian M. Urban
Categories: hep-lat cs.LG
Comments: 11 pages, 2 tables, 5 figures
Report-no: MIT-CTP/5658, FERMILAB-PUB-24-0014-T
\\
  Machine-learned normalizing flows can be used in the context of lattice
quantum field theory to generate statistically correlated ensembles of lattice
gauge fields at different action parameters. This work demonstrates how these
correlations can be exploited for variance reduction in the computation of
observables. Three different proof-of-concept applications are demonstrated
using a novel residual flow architecture: continuum limits of gauge theories,
the mass dependence of QCD observables, and hadronic matrix elements based on
the Feynman-Hellmann approach. In all three cases, it is shown that statistical
uncertainties are significantly reduced when machine-learned flows are
incorporated as compared with the same calculations performed with uncorrelated
ensembles or direct reweighting.
\\ ( https://arxiv.org/abs/2401.10874 ,  772kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2211.13350
replaced with revised version Fri, 19 Jan 2024 17:33:36 GMT   (12958kb,D)

Title: Choreographer: Learning and Adapting Skills in Imagination
Authors: Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Alexandre Lacoste, Sai
  Rajeswar
Categories: cs.AI cs.LG
Comments: Accepted at ICLR 2023 (notable top 25%)
\\ ( https://arxiv.org/abs/2211.13350 ,  12958kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07863
replaced with revised version Fri, 19 Jan 2024 06:59:26 GMT   (847kb,D)

Title: Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer
  Control
Authors: Longtao Zheng, Rundong Wang, Xinrun Wang, Bo An
Categories: cs.AI
Comments: ICLR 2024. Project page: https://ltzheng.github.io/Synapse
\\ ( https://arxiv.org/abs/2306.07863 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10444
replaced with revised version Fri, 19 Jan 2024 13:19:13 GMT   (620kb,D)

Title: Exploring Iterative Enhancement for Improving Learnersourced
  Multiple-Choice Question Explanations with Large Language Models
Authors: Qiming Bao, Juho Leinonen, Alex Yuxuan Peng, Wanjun Zhong, Ga\"el
  Gendron, Timothy Pistotti, Alice Huang, Paul Denny, Michael Witbrock and
  Jiamou Liu
Categories: cs.AI cs.CL
Comments: Preprint. Under review
\\ ( https://arxiv.org/abs/2309.10444 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00772
replaced with revised version Fri, 19 Jan 2024 17:14:25 GMT   (25252kb,D)

Title: SAGE: Smart home Agent with Grounded Execution
Authors: Dmitriy Rivkin, Francois Hogan, Amal Feriani, Abhisek Konar, Adam
  Sigal, Steve Liu, Greg Dudek
Categories: cs.AI cs.HC cs.RO
\\ ( https://arxiv.org/abs/2311.00772 ,  25252kb)
------------------------------------------------------------------------------
\\
arXiv:2212.09726
replaced with revised version Thu, 18 Jan 2024 19:27:04 GMT   (6675kb,D)

Title: Improving Faithfulness of Abstractive Summarization by Controlling
  Confounding Effect of Irrelevant Sentences
Authors: Asish Ghoshal, Arash Einolghozati, Ankit Arun, Haoran Li, Lili Yu,
  Vera Gor, Yashar Mehdad, Scott Wen-tau Yih, Asli Celikyilmaz
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2212.09726 ,  6675kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00168
replaced with revised version Fri, 19 Jan 2024 13:05:04 GMT   (1164kb,D)

Title: Measuring the Robustness of NLP Models to Domain Shifts
Authors: Nitay Calderon, Naveh Porat, Eyal Ben-David, Alexander Chapanin, Zorik
  Gekhman, Nadav Oved, Vitaly Shalumov, Roi Reichart
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.00168 ,  1164kb)
------------------------------------------------------------------------------
\\
arXiv:2306.16143
replaced with revised version Fri, 19 Jan 2024 15:05:14 GMT   (11493kb,D)

Title: Generative User-Experience Research for Developing Domain-specific
  Natural Language Processing Applications
Authors: Anastasia Zhukova, Lukas von Sperl, Christian E. Matt, Bela Gipp
Categories: cs.CL cs.HC
\\ ( https://arxiv.org/abs/2306.16143 ,  11493kb)
------------------------------------------------------------------------------
\\
arXiv:2307.14995
replaced with revised version Fri, 19 Jan 2024 07:47:01 GMT   (420kb,D)

Title: TransNormerLLM: A Faster and Better Large Language Model with Improved
  TransNormer
Authors: Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han,
  Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, Yiran Zhong
Categories: cs.CL
Comments: Technical Report. Yiran Zhong is the corresponding author. Zhen Qin,
  Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen contribute equally to this
  paper. Code is released at: https://github.com/OpenNLPLab/TransnormerLLM
\\ ( https://arxiv.org/abs/2307.14995 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03279
replaced with revised version Fri, 19 Jan 2024 02:26:38 GMT   (7813kb,D)

Title: UniversalNER: Targeted Distillation from Large Language Models for Open
  Named Entity Recognition
Authors: Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung Poon
Categories: cs.CL
Comments: Accepted at ICLR 2024. Project page: https://universal-ner.github.io/
\\ ( https://arxiv.org/abs/2308.03279 ,  7813kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07107
replaced with revised version Fri, 19 Jan 2024 16:01:28 GMT   (4406kb,D)

Title: Large Language Models for Information Retrieval: A Survey
Authors: Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu,
  Chenlong Deng, Haonan Chen, Zhicheng Dou, and Ji-Rong Wen
Categories: cs.CL cs.IR
Comments: updated to version 2
\\ ( https://arxiv.org/abs/2308.07107 ,  4406kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08565
replaced with revised version Fri, 19 Jan 2024 16:48:59 GMT   (8263kb,D)

Title: How Transferable are Attribute Controllers on Pretrained Multilingual
  Translation Models?
Authors: Danni Liu, Jan Niehues
Categories: cs.CL cs.AI
Comments: EACL 2024
\\ ( https://arxiv.org/abs/2309.08565 ,  8263kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14393
replaced with revised version Fri, 19 Jan 2024 17:33:44 GMT   (694kb,D)

Title: LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language
  Models
Authors: Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Osi, Prateek Sharma, Fan
  Chen, Lei Jiang
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: 15 pages, 8 figures
Journal-ref: published in ICLR2024
\\ ( https://arxiv.org/abs/2309.14393 ,  694kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04965
replaced with revised version Thu, 18 Jan 2024 21:17:04 GMT   (6736kb,D)

Title: MULTISCRIPT: Multimodal Script Learning for Supporting Open Domain
  Everyday Tasks
Authors: Jingyuan Qi, Minqian Liu, Ying Shen, Zhiyang Xu, Lifu Huang
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by AAAI 2024. 11 pages, 9 figures, 4 tables
\\ ( https://arxiv.org/abs/2310.04965 ,  6736kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05492
replaced with revised version Fri, 19 Jan 2024 06:06:46 GMT   (917kb,D)

Title: How Abilities in Large Language Models are Affected by Supervised
  Fine-tuning Data Composition
Authors: Guanting Dong, Hongyi Yuan, Keming Lu, Chengpeng Li, Mingfeng Xue,
  Dayiheng Liu, Wei Wang, Zheng Yuan, Chang Zhou, Jingren Zhou
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.05492 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13274
replaced with revised version Fri, 19 Jan 2024 10:06:50 GMT   (2751kb,D)

Title: Enhancing Summarization Performance through Transformer-Based Prompt
  Engineering in Automated Medical Reporting
Authors: Daphne van Zandvoort, Laura Wiersema, Tom Huibers, Sandra van Dulmen,
  Sjaak Brinkkemper
Categories: cs.CL
Comments: 12 pages, 4 figures, to be presented at HEALTHINF 2024, author
  contributions: research conducted and written by Daphne van Zandvoort and
  Laura Wiersema, research suggested and used software created by Tom Huibers,
  data provided and feedback provided by Sandra van Dulmen, supervision and
  feedback provided by Sjaak Brinkkemper
\\ ( https://arxiv.org/abs/2311.13274 ,  2751kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01185
replaced with revised version Fri, 19 Jan 2024 12:34:07 GMT   (764kb,D)

Title: A ripple in time: a discontinuity in American history
Authors: Alexander Kolpakov, Igor Rivin
Categories: cs.CL cs.AI cs.LG cs.SI
Comments: 7 pages, 8 figures; GitHub repository
  https://github.com/sashakolpakov/ripple_in_time
ACM-class: I.2.7; I.5.4; H.3.1; H.3.3
\\ ( https://arxiv.org/abs/2312.01185 ,  764kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15880
replaced with revised version Fri, 19 Jan 2024 06:42:16 GMT   (963kb,D)

Title: KnowledgeNavigator: Leveraging Large Language Models for Enhanced
  Reasoning over Knowledge Graph
Authors: Tiezheng Guo and Qingwen Yang and Chen Wang and Yanyi Liu and Pan Li
  and Jiawei Tang and Dapeng Li and Yingyou Wen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.15880 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00368
replaced with revised version Fri, 19 Jan 2024 05:16:20 GMT   (133kb,D)

Title: Improving Text Embeddings with Large Language Models
Authors: Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder,
  Furu Wei
Categories: cs.CL cs.IR
Comments: 20 pages, 15 tables
\\ ( https://arxiv.org/abs/2401.00368 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04398
replaced with revised version Fri, 19 Jan 2024 01:05:05 GMT   (1041kb,D)

Title: Chain-of-Table: Evolving Tables in the Reasoning Chain for Table
  Understanding
Authors: Zilong Wang, Hao Zhang, Chun-Liang Li, Julian Martin Eisenschlos,
  Vincent Perot, Zifeng Wang, Lesly Miculicich, Yasuhisa Fujii, Jingbo Shang,
  Chen-Yu Lee, Tomas Pfister
Categories: cs.CL
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2401.04398 ,  1041kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05273
replaced with revised version Fri, 19 Jan 2024 16:57:30 GMT   (1580kb,D)

Title: INACIA: Integrating Large Language Models in Brazilian Audit Courts:
  Opportunities and Challenges
Authors: Jayr Pereira, Andre Assumpcao, Julio Trecenti, Luiz Airosa, Caio
  Lente, Jhonatan Cl\'eto, Guilherme Dobins, Rodrigo Nogueira, Luis Mitchell,
  Roberto Lotufo
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.05273 ,  1580kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08326
replaced with revised version Fri, 19 Jan 2024 08:48:37 GMT   (8728kb,D)

Title: RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large
  Language Models in Tool Learning
Authors: Junjie Ye, Yilong Wu, Songyang Gao, Caishuang Huang, Sixian Li, Guanyu
  Li, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.08326 ,  8728kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09343
replaced with revised version Fri, 19 Jan 2024 13:33:22 GMT   (70kb,D)

Title: Efficient slot labelling
Authors: Vladimir Vlasov
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.09343 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09566
replaced with revised version Fri, 19 Jan 2024 08:57:19 GMT   (92kb,D)

Title: Aligning Large Language Models with Counterfactual DPO
Authors: Bradley Butcher
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.09566 ,  92kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09972
replaced with revised version Fri, 19 Jan 2024 04:29:42 GMT   (8865kb,D)

Title: Better Explain Transformers by Illuminating Important Information
Authors: Linxin Song, Yan Cui, Ao Luo, Freddy Lecue, Irene Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.09972 ,  8865kb)
------------------------------------------------------------------------------
\\
arXiv:2109.06826
replaced with revised version Thu, 18 Jan 2024 19:12:12 GMT   (2564kb,D)

Title: Few-shot Quality-Diversity Optimization
Authors: Achkan Salehi, Alexandre Coninx, Stephane Doncieux
Categories: cs.LG cs.AI cs.NE
Comments: Accepted for publication in the IEEE Robotics and Automation Letters
  (RA-L) journal
Journal-ref: IEEE Robotics and Automation Letters 7.2 (2022): 4424-4431
DOI: 10.1109/LRA.2022.3148438
\\ ( https://arxiv.org/abs/2109.06826 ,  2564kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14102
replaced with revised version Fri, 19 Jan 2024 15:30:04 GMT   (2989kb)

Title: Group-level Brain Decoding with Deep Learning
Authors: Richard Csaky, Mats Van Es, Oiwi Parker Jones, Mark Woolrich
Categories: cs.LG eess.SP q-bio.NC
Comments: Published in Human Brain Mapping
DOI: 10.1002/hbm.26500
\\ ( https://arxiv.org/abs/2205.14102 ,  2989kb)
------------------------------------------------------------------------------
\\
arXiv:2206.01409
replaced with revised version Fri, 19 Jan 2024 00:23:28 GMT   (2179kb,D)

Title: Hybrid Parameter Search and Dynamic Model Selection for Mixed-Variable
  Bayesian Optimization
Authors: Hengrui Luo, Younghyun Cho, James W. Demmel, Xiaoye S. Li, Yang Liu
Categories: cs.LG math.ST stat.ML stat.TH
Comments: 33 pages, 8 Figures
MSC-class: 60G15, 62F15, 65C05
DOI: 10.1080/10618600.2024.2308216
\\ ( https://arxiv.org/abs/2206.01409 ,  2179kb)
------------------------------------------------------------------------------
\\
arXiv:2208.07626
replaced with revised version Fri, 19 Jan 2024 16:52:27 GMT   (44kb)

Title: Algorithmic Assistance with Recommendation-Dependent Preferences
Authors: Bryce McLaughlin and Jann Spiess
Categories: cs.LG cs.HC econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2208.07626 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2212.01521
replaced with revised version Fri, 19 Jan 2024 03:21:28 GMT   (6653kb,D)

Title: Distribution Fitting for Combating Mode Collapse in Generative
  Adversarial Networks
Authors: Yanxiang Gong, Zhiwei Xie, Guozhen Duan, Zheng Ma, Mei Xie
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2212.01521 ,  6653kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02506
replaced with revised version Thu, 18 Jan 2024 22:09:40 GMT   (12820kb,D)

Title: Prismer: A Vision-Language Model with Multi-Task Experts
Authors: Shikun Liu, Linxi Fan, Edward Johns, Zhiding Yu, Chaowei Xiao, Anima
  Anandkumar
Categories: cs.LG cs.AI cs.CV
Comments: Published at TMLR 2024. Project Page:
  https://shikun.io/projects/prismer Code: https://github.com/NVlabs/prismer
\\ ( https://arxiv.org/abs/2303.02506 ,  12820kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17046
replaced with revised version Fri, 19 Jan 2024 15:33:12 GMT   (2312kb,D)

Title: Have it your way: Individualized Privacy Assignment for DP-SGD
Authors: Franziska Boenisch, Christopher M\"uhl, Adam Dziedzic, Roy Rinberg,
  Nicolas Papernot
Categories: cs.LG cs.AI cs.CR
Comments: Published at NeurIPS'2024
\\ ( https://arxiv.org/abs/2303.17046 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11171
replaced with revised version Fri, 19 Jan 2024 03:23:21 GMT   (30149kb,D)

Title: Granular-ball computing: an efficient, robust, and interpretable
  adaptive multi-granularity representation and computation method
Authors: Shuyin Xia, Guoyin Wang, Xinbo Gao, Xiaoyu Lian
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2304.11171 ,  30149kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00119
replaced with revised version Fri, 19 Jan 2024 18:30:27 GMT   (1793kb,D)

Title: Optimal Sets and Solution Paths of ReLU Networks
Authors: Aaron Mishkin, Mert Pilanci
Categories: cs.LG
Comments: Minor updates and corrections to clarify the role of merge/split
  symmetries in formation of ReLU optimal set and add missing sufficient
  conditions for all minimal models to have the same cardinality
Journal-ref: Proceedings of the 40th International Conference on Machine
  Learning, PMLR 202:24888-24924, 2023
\\ ( https://arxiv.org/abs/2306.00119 ,  1793kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17248
replaced with revised version Fri, 19 Jan 2024 15:01:52 GMT   (34298kb,D)

Title: TemperatureGAN: Generative Modeling of Regional Atmospheric Temperatures
Authors: Emmanuel Balogun, Ram Rajagopal, and Arun Majumdar
Categories: cs.LG physics.ao-ph stat.ML
\\ ( https://arxiv.org/abs/2306.17248 ,  34298kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07988
replaced with revised version Fri, 19 Jan 2024 00:28:45 GMT   (657kb,D)

Title: Folding Attention: Memory and Power Optimization for On-Device
  Transformer-based Streaming Speech Recognition
Authors: Yang Li, Liangzhen Lai, Yuan Shangguan, Forrest N. Iandola, Zhaoheng
  Ni, Ernie Chang, Yangyang Shi, Vikas Chandra
Categories: cs.LG cs.AR cs.SD eess.AS
\\ ( https://arxiv.org/abs/2309.07988 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03320
replaced with revised version Fri, 19 Jan 2024 02:47:51 GMT   (1695kb,D)

Title: BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs
Authors: Zifeng Wang, Zichen Wang, Balasubramaniam Srinivasan, Vassilis N.
  Ioannidis, Huzefa Rangwala, Rishita Anubhai
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.03320 ,  1695kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12955
replaced with revised version Fri, 19 Jan 2024 17:12:23 GMT   (2784kb,D)

Title: Towards Robust Offline Reinforcement Learning under Diverse Data
  Corruption
Authors: Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han,
  Tong Zhang
Categories: cs.LG cs.AI
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.12955 ,  2784kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13384
replaced with revised version Fri, 19 Jan 2024 15:19:54 GMT   (341kb,D)

Title: Salted Inference: Enhancing Privacy while Maintaining Efficiency of
  Split Inference in Mobile Computing
Authors: Mohammad Malekzadeh and Fahim Kawsar
Categories: cs.LG cs.DC
Comments: To be appeared in the 25th International Workshop on Mobile Computing
  Systems and Applications (HotMobile 2024)
\\ ( https://arxiv.org/abs/2310.13384 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03976
replaced with revised version Fri, 19 Jan 2024 14:34:47 GMT   (33350kb,D)

Title: A Foundation Graph Model
Authors: Alex O. Davies, Riku W. Green, Nirav S. Ajmeri, Telmo M. Silva Filho
Categories: cs.LG cs.AI
Comments: Presented at the NeurIPS 2023 New Frontiers in Graph Learning
  workshop
\\ ( https://arxiv.org/abs/2311.03976 ,  33350kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07202
replaced with revised version Fri, 19 Jan 2024 05:54:53 GMT   (140kb,D)

Title: Input Convex LSTM: A Convex Approach for Fast Lyapunov-Based Model
  Predictive Control
Authors: Zihao Wang, Zhe Wu
Categories: cs.LG cs.CE cs.SY eess.SY
Comments: Submitted to 6th Annual Learning for Dynamics & Control Conference
  (L4DC 2024)
\\ ( https://arxiv.org/abs/2311.07202 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12399
replaced with revised version Fri, 19 Jan 2024 09:49:46 GMT   (440kb,D)

Title: A Survey of Graph Meets Large Language Model: Progress and Future
  Directions
Authors: Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng,
  Jeffrey Xu Yu
Categories: cs.LG cs.CL cs.SI
Comments: Work in progress; 13 pages, 5 figures
\\ ( https://arxiv.org/abs/2311.12399 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05225
replaced with revised version Fri, 19 Jan 2024 03:34:11 GMT   (1418kb,D)

Title: Neural Spectral Methods: Self-supervised learning in the spectral domain
Authors: Yiheng Du, Nithin Chalapathi, Aditi Krishnapriyan
Categories: cs.LG
Comments: Accepted to International Conference on Learning Representations
  (ICLR) 2024
\\ ( https://arxiv.org/abs/2312.05225 ,  1418kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09234
replaced with revised version Fri, 19 Jan 2024 14:57:06 GMT   (14459kb,D)

Title: Let's do the time-warp-attend: Learning topological invariants of
  dynamical systems
Authors: Noa Moriel, Matthew Ricci, Mor Nitzan
Categories: cs.LG math.DS stat.ML
\\ ( https://arxiv.org/abs/2312.09234 ,  14459kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10401
replaced with revised version Fri, 19 Jan 2024 01:25:39 GMT   (3265kb,D)

Title: Rethinking Dimensional Rationale in Graph Contrastive Learning from
  Causal Perspective
Authors: Qirui Ji, Jiangmeng Li, Jie Hu, Rui Wang, Changwen Zheng, Fanjiang Xu
Categories: cs.LG cs.AI
Comments: Accepted by AAAI2024
\\ ( https://arxiv.org/abs/2312.10401 ,  3265kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13110
replaced with revised version Thu, 18 Jan 2024 22:28:06 GMT   (718kb)

Title: Pre-training of Molecular GNNs via Conditional Boltzmann Generator
Authors: Daiki Koge, Naoaki Ono, Shigehiko Kanaya
Categories: cs.LG physics.chem-ph q-bio.BM
Comments: 4 pages
\\ ( https://arxiv.org/abs/2312.13110 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13486
replaced with revised version Thu, 18 Jan 2024 20:42:48 GMT   (105kb,D)

Title: Meta-Learning with Versatile Loss Geometries for Fast Adaptation Using
  Mirror Descent
Authors: Yilang Zhang, Bingcong Li, Georgios B. Giannakis
Categories: cs.LG
Comments: Accepted by 2024 IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP-24)
\\ ( https://arxiv.org/abs/2312.13486 ,  105kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04336
replaced with revised version Fri, 19 Jan 2024 01:30:04 GMT   (2272kb,D)

Title: Deep Efficient Private Neighbor Generation for Subgraph Federated
  Learning
Authors: Ke Zhang, Lichao Sun, Bolin Ding, Siu Ming Yiu, Carl Yang
Categories: cs.LG cs.AI cs.CR
Comments: Accepted to SDM 2024
\\ ( https://arxiv.org/abs/2401.04336 ,  2272kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07494
replaced with revised version Fri, 19 Jan 2024 06:16:59 GMT   (11427kb,D)

Title: Input Convex Lipschitz RNN: A Fast and Robust Approach for Engineering
  Tasks
Authors: Zihao Wang, P S Pravin, Zhe Wu
Categories: cs.LG cs.CE cs.SY eess.SY
\\ ( https://arxiv.org/abs/2401.07494 ,  11427kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08897
replaced with revised version Fri, 19 Jan 2024 02:39:59 GMT   (22240kb,D)

Title: CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in
  Variational AutoEncoder
Authors: Hee-Jun Jung, Jaehyoung Jeong and Kangil Kim
Categories: cs.LG cs.AI
Comments: 21 pages, 14 figures
\\ ( https://arxiv.org/abs/2401.08897 ,  22240kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09796
replaced with revised version Fri, 19 Jan 2024 15:09:45 GMT   (2094kb,D)

Title: A Fast, Performant, Secure Distributed Training Framework For Large
  Language Model
Authors: Wei Huang, Yinggui Wang, Anda Cheng, Aihui Zhou, Chaofan Yu, Lei Wang
Categories: cs.LG cs.CR
Comments: Accepted by ICASSP 2024 (Federated LLM)
\\ ( https://arxiv.org/abs/2401.09796 ,  2094kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10191
replaced with revised version Fri, 19 Jan 2024 10:01:36 GMT   (6944kb,D)

Title: Divide and not forget: Ensemble of selectively trained experts in
  Continual Learning
Authors: Grzegorz Rype\'s\'c, Sebastian Cygert, Valeriya Khan, Tomasz
  Trzci\'nski, Bartosz Zieli\'nski, Bart{\l}omiej Twardowski
Categories: cs.LG cs.CV
Comments: Accepted for ICLR 2024 (main track), code is available at:
  https://github.com/grypesc/SEED
\\ ( https://arxiv.org/abs/2401.10191 ,  6944kb)
------------------------------------------------------------------------------
\\
arXiv:2205.05359 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 01:30:56 GMT   (2544kb)

Title: Exploring Local Explanations of Nonlinear Models Using Animated Linear
  Projections
Authors: Nicholas Spyrison, Dianne Cook, Przemyslaw Biecek
Categories: stat.ML cs.AI cs.LG
\\ ( https://arxiv.org/abs/2205.05359 ,  2544kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10766
replaced with revised version Fri, 19 Jan 2024 01:51:45 GMT   (1598kb,D)

Title: On the Adversarial Robustness of Camera-based 3D Object Detection
Authors: Shaoyuan Xie, Zichao Li, Zeyu Wang, Cihang Xie
Categories: cs.CV cs.AI
Comments: Transactions on Machine Learning Research, 2024. ISSN 2835-8856
\\ ( https://arxiv.org/abs/2301.10766 ,  1598kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13359
replaced with revised version Fri, 19 Jan 2024 13:25:03 GMT   (7941kb,D)

Title: IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing
Authors: Guoyang Xie, Jinbao Wang, Jiaqi Liu, Jiayi Lyu, Yong Liu, Chengjie
  Wang, Feng Zheng, Yaochu Jin
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2301.13359 ,  7941kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12190
replaced with revised version Fri, 19 Jan 2024 16:30:14 GMT   (3468kb,D)

Title: MCWDST: a Minimum-Cost Weighted Directed Spanning Tree Algorithm for
  Real-Time Fake News Mitigation in Social Media
Authors: Ciprian-Octavian Truic\u{a} and Elena-Simona Apostol and
  Radu-C\u{a}t\u{a}lin Nicolescu and Panagiotis Karras
Categories: cs.SI cs.AI cs.CL cs.NE
Journal-ref: IEEE Access, 11:125861-125873, 2023
DOI: 10.1109/ACCESS.2023.3331220
\\ ( https://arxiv.org/abs/2302.12190 ,  3468kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00746
replaced with revised version Fri, 19 Jan 2024 00:42:13 GMT   (24011kb,D)

Title: OTS: A One-shot Learning Approach for Text Spotting in Historical
  Manuscripts
Authors: Wenbo Hu, Hongjian Zhan, Cong Liu, Bing Yin, Yue Lu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2304.00746 ,  24011kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13189 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 01:23:50 GMT   (33956kb,D)

Title: Onboard Science Instrument Autonomy for the Detection of Microscopy
  Biosignatures on the Ocean Worlds Life Surveyor
Authors: Mark Wronkiewicz, Jake Lee, Lukas Mandrake, Jack Lightholder, Gary
  Doran, Steffen Mauceri, Taewoo Kim, Nathan Oborny, Thomas Schibler, Jay
  Nadeau, James K. Wallace, Eshaan Moorjani, Chris Lindensmith
Categories: astro-ph.IM astro-ph.EP cs.AI
Comments: 56 pages, 18 figures, accepted by The Planetary Science Journal on
  2023-10-09
\\ ( https://arxiv.org/abs/2304.13189 ,  33956kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08251
replaced with revised version Fri, 19 Jan 2024 18:35:54 GMT   (14038kb,D)

Title: GBSD: Generative Bokeh with Stage Diffusion
Authors: Jieren Deng, Xin Zhou, Hao Tian, Zhihong Pan, and Derek Aguiar
Categories: cs.CV cs.AI
Comments: Short Version is accepted by International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP) 2024
\\ ( https://arxiv.org/abs/2306.08251 ,  14038kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08033
replaced with revised version Fri, 19 Jan 2024 15:58:34 GMT   (2161kb,D)

Title: Domain Adaptation for Deep Unit Test Case Generation
Authors: Jiho Shin, Sepehr Hashtroudi, Hadi Hemmati, Song Wang
Categories: cs.SE cs.AI
Comments: 10 pages + reference
\\ ( https://arxiv.org/abs/2308.08033 ,  2161kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09868
replaced with revised version Fri, 19 Jan 2024 01:36:57 GMT   (5687kb,D)

Title: INTERVENOR: Prompt the Coding Ability of Large Language Models with the
  Interactive Chain of Repairing
Authors: Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan
  Liu and Ge Yu
Categories: cs.SE cs.AI
Comments: 26 pages, 19 figures, 8 tables
\\ ( https://arxiv.org/abs/2311.09868 ,  5687kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15497
replaced with revised version Fri, 19 Jan 2024 02:45:44 GMT   (5761kb,D)

Title: Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning
  and Optimization Functions for Enhanced Precision
Authors: Gabriel De Araujo, Shanlin Sun, Xiaohui Xie
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.15497 ,  5761kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03263
replaced with revised version Fri, 19 Jan 2024 17:33:25 GMT   (19210kb,D)

Title: Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying
  Partially Observable Environment
Authors: Gokul Puthumanaillam, Xiangyu Liu, Negar Mehr and Melkior Ornik
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: Page 3, fixed typo
\\ ( https://arxiv.org/abs/2312.03263 ,  19210kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16451
replaced with revised version Fri, 19 Jan 2024 07:27:18 GMT   (3911kb,D)

Title: Domain Generalization with Vital Phase Augmentation
Authors: Ingyun Lee, Wooju Lee, Hyun Myung
Categories: cs.CV cs.AI
Comments: Accepted by AAAI-24
\\ ( https://arxiv.org/abs/2312.16451 ,  3911kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00110
replaced with revised version Fri, 19 Jan 2024 00:35:35 GMT   (15284kb,D)

Title: Diffusion Model with Perceptual Loss
Authors: Shanchuan Lin, Xiao Yang
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.00110 ,  15284kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02542
replaced with revised version Fri, 19 Jan 2024 01:50:57 GMT   (1677kb,D)

Title: A Community Detection and Graph Neural Network Based Link Prediction
  Approach for Scientific Literature
Authors: Chunjiang Liu, Yikun Han, Haiyun Xu, Shihan Yang, Kaidi Wang, Yongye
  Su
Categories: cs.SI cs.AI
\\ ( https://arxiv.org/abs/2401.02542 ,  1677kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09691
replaced with revised version Fri, 19 Jan 2024 12:43:36 GMT   (1365kb,D)

Title: Imitation Learning Inputting Image Feature to Each Layer of Neural
  Network
Authors: Koki Yamane, Sho Sakaino, Toshiaki Tsuji
Categories: cs.RO cs.AI cs.LG
Comments: 6 pages, 4 figures, Accepted at AMC2024
\\ ( https://arxiv.org/abs/2401.09691 ,  1365kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07106
replaced with revised version Thu, 18 Jan 2024 20:19:54 GMT   (283kb,D)

Title: Directed Regular and Context-Free Languages
Authors: Moses Ganardi, Irmak Saglam, Georg Zetzsche
Categories: cs.FL cs.CL
\\ ( https://arxiv.org/abs/2401.07106 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2111.10891 (*cross-listing*)
replaced with revised version Thu, 18 Jan 2024 22:43:56 GMT   (3911kb,D)

Title: Active Restoration of Lost Audio Signals Using Machine Learning and
  Latent Information
Authors: Zohra Adila Cheddad, Abbas Cheddad
Categories: eess.AS cs.LG cs.SD
Comments: 18 Pages, 2 Tables, 8 Figures
Journal-ref: Lecture Notes in Networks and Systems, vol 822, 2024, Springer,
  Cham
DOI: 10.1007/978-3-031-47721-8_1
\\ ( https://arxiv.org/abs/2111.10891 ,  3911kb)
------------------------------------------------------------------------------
\\
arXiv:2201.05158 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 16:26:46 GMT   (3866kb,D)

Title: Towards Quantum Graph Neural Networks: An Ego-Graph Learning Approach
Authors: Xing Ai, Zhihong Zhang, Luzhe Sun, Junchi Yan, Edwin Hancock
Categories: quant-ph cs.LG
\\ ( https://arxiv.org/abs/2201.05158 ,  3866kb)
------------------------------------------------------------------------------
\\
arXiv:2203.16031
replaced with revised version Thu, 18 Jan 2024 19:59:42 GMT   (44588kb,D)

Title: How Deep is Your Art: An Experimental Study on the Limits of Artistic
  Understanding in a Single-Task, Single-Modality Neural Network
Authors: Mahan Agha Zahedi, Niloofar Gholamrezaei, Alex Doboli
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2203.16031 ,  44588kb)
------------------------------------------------------------------------------
\\
arXiv:2210.02672
replaced with revised version Fri, 19 Jan 2024 00:57:05 GMT   (4792kb,D)

Title: A Novel Maximum-Entropy-Driven Technique for Low-Rank Orthogonal
  Nonnegative Matrix Factorization with $\ell_0$-Norm sparsity Constraint
Authors: Salar Basiri and Srinivasa Salapaka
Categories: cs.DS cs.IT cs.LG math.IT math.PR
\\ ( https://arxiv.org/abs/2210.02672 ,  4792kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00219 (*cross-listing*)
replaced with revised version Thu, 18 Jan 2024 21:07:17 GMT   (12756kb,D)

Title: Are you using test log-likelihood correctly?
Authors: Sameer K. Deshpande and Soumya Ghosh and Tin D. Nguyen and Tamara
  Broderick
Categories: stat.ML cs.LG stat.OT
Comments: Presented at the ICBINB Workshop at NeurIPS 2022. This version
  accepted at TMLR, available at https://openreview.net/forum?id=n2YifD4Dxo
\\ ( https://arxiv.org/abs/2212.00219 ,  12756kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06120 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 04:13:33 GMT   (2395kb,D)

Title: Knowledge from Large-Scale Protein Contact Prediction Models Can Be
  Transferred to the Data-Scarce RNA Contact Prediction Task
Authors: Yiren Jian and Chongyang Gao and Chen Zeng and Yunjie Zhao and Soroush
  Vosoughi
Categories: q-bio.QM cs.LG
Comments: The code is available at
  https://github.com/yiren-jian/CoT-RNA-Transfer
\\ ( https://arxiv.org/abs/2302.06120 ,  2395kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13854 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 02:19:29 GMT   (796kb,D)

Title: A Deep Neural Network Based Reverse Radio Spectrogram Search Algorithm
Authors: Peter Xiangyuan Ma, Steve Croft, Chris Lintott, Andrew P. V. Siemion
Categories: eess.SP astro-ph.IM cs.LG cs.SD eess.AS
Comments: 8 pages, 8 figures
Journal-ref: RAS Techniques and Instruments 2023
DOI: 10.1093/rasti/rzad056
\\ ( https://arxiv.org/abs/2302.13854 ,  796kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02901 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 14:53:51 GMT   (1867kb,D)

Title: $\alpha$-divergence Improves the Entropy Production Estimation via
  Machine Learning
Authors: Euijoon Kwon, Yongjoo Baek
Categories: cond-mat.stat-mech cs.LG stat.ML
Comments: 11 pages, 9 figures
\\ ( https://arxiv.org/abs/2303.02901 ,  1867kb)
------------------------------------------------------------------------------
\\
arXiv:2303.03183
replaced with revised version Fri, 19 Jan 2024 02:31:58 GMT   (1754kb)

Title: Utilizing synthetic training data for the supervised classification of
  rat ultrasonic vocalizations
Authors: K. Jack Scott, Lucinda J. Speers, David K. Bilkey
Categories: cs.SD cs.LG eess.AS
Comments: 25 pages, 5 main figures, 2 tables
Journal-ref: J Acoust Soc Am 1 January 2024 155 (1)
DOI: 10.1121/10.0024340
\\ ( https://arxiv.org/abs/2303.03183 ,  1754kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03077 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 15:16:37 GMT   (3276kb,D)

Title: Explaining dark matter halo density profiles with neural networks
Authors: Luisa Lucie-Smith, Hiranya V. Peiris and Andrew Pontzen
Categories: astro-ph.CO cs.LG
Comments: 7 pages, 5 figures. Minor changes to match version accepted for
  publication in PRL
Journal-ref: Phys. Rev. Lett. 132, 031001 (2024)
DOI: 10.1103/PhysRevLett.132.031001
\\ ( https://arxiv.org/abs/2305.03077 ,  3276kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14402
replaced with revised version Fri, 19 Jan 2024 00:16:49 GMT   (955kb,D)

Title: Enhancing Speech Emotion Recognition Through Differentiable Architecture
  Search
Authors: Thejan Rajapakshe, Rajib Rana, Sara Khalifa, Berrak Sisman, Bj\"orn
  Schuller
Categories: cs.SD cs.LG eess.AS
Comments: 5 pages, 4 figures
\\ ( https://arxiv.org/abs/2305.14402 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10822 (*cross-listing*)
replaced with revised version Thu, 18 Jan 2024 20:13:41 GMT   (4111kb,D)

Title: Interpreting Deep Neural Networks with the Package innsight
Authors: Niklas Koenen, Marvin N. Wright
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2306.10822 ,  4111kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12871
replaced with revised version Thu, 18 Jan 2024 21:20:08 GMT   (4288kb,D)

Title: IPA: Inference Pipeline Adaptation to Achieve High Accuracy and
  Cost-Efficiency
Authors: Saeid Ghafouri, Kamran Razavi, Mehran Salmani, Alireza Sanaee, Tania
  Lorido-Botran, Lin Wang, Joseph Doyle, Pooyan Jamshidi
Categories: cs.DC cs.LG cs.PF
\\ ( https://arxiv.org/abs/2308.12871 ,  4288kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04452 (*cross-listing*)
replaced with revised version Thu, 18 Jan 2024 19:02:54 GMT   (3552kb,D)

Title: Postprocessing of Ensemble Weather Forecasts Using Permutation-invariant
  Neural Networks
Authors: Kevin H\"ohlein, Benedikt Schulz, R\"udiger Westermann and Sebastian
  Lerch
Categories: stat.ML cs.LG physics.ao-ph
Comments: in press
Journal-ref: Artificial Intelligence for the Earth Systems, 2023
DOI: 10.1175/AIES-D-23-0070.1
\\ ( https://arxiv.org/abs/2309.04452 ,  3552kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11623
replaced with revised version Thu, 18 Jan 2024 22:37:48 GMT   (114kb)

Title: Leveraging Negative Signals with Self-Attention for Sequential Music
  Recommendation
Authors: Pavan Seshadri, Peter Knees
Categories: cs.IR cs.LG
Comments: Accepted to the 1st Workshop on Music Recommender Systems, co-located
  with the 17th ACM Conference on Recommender Systems (MuRS @ RecSys 2023)
DOI: 10.5281/zenodo.8372449
\\ ( https://arxiv.org/abs/2309.11623 ,  114kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01202 (*cross-listing*)
replaced with revised version Thu, 18 Jan 2024 22:48:17 GMT   (1653kb,D)

Title: Unified Uncertainty Calibration
Authors: Kamalika Chaudhuri and David Lopez-Paz
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.01202 ,  1653kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03298 (*cross-listing*)
replaced with revised version Thu, 18 Jan 2024 19:45:02 GMT   (4462kb)

Title: A Latent Variable Approach for Non-Hierarchical Multi-Fidelity Adaptive
  Sampling
Authors: Yi-Ping Chen, Liwei Wang, Yigitcan Comlek, Wei Chen
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.03298 ,  4462kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11809
replaced with revised version Fri, 19 Jan 2024 10:10:27 GMT   (150kb,D)

Title: LogLead -- Fast and Integrated Log Loader, Enhancer, and Anomaly
  Detector
Authors: Mika M\"antyl\"a, Yuqing Wang, Jesse Nyyss\"ol\"a
Categories: cs.SE cs.LG
Comments: 2024 IEEE International Conference on Software Analysis, Evolution
  and Reengineering (SANER)
\\ ( https://arxiv.org/abs/2311.11809 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18426 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 02:56:41 GMT   (82kb,D)

Title: Convergence Analysis of Fractional Gradient Descent
Authors: Ashwani Aggarwal
Categories: math.OC cs.LG cs.NA math.NA
Comments: 24 pages, 4 figures. Added additional results for smooth and convex
  functions
ACM-class: G.1.6
\\ ( https://arxiv.org/abs/2311.18426 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00067 (*cross-listing*)
replaced with revised version Thu, 18 Jan 2024 20:58:50 GMT   (0kb,I)

Title: Predicting breast cancer with AI for individual risk-adjusted MRI
  screening and early detection
Authors: Lukas Hirsch, Yu Huang, Hernan A. Makse, Danny F. Martinez, Mary
  Hughes, Sarah Eskreis-Winkler, Katja Pinker, Elizabeth Morris, Lucas C.
  Parra, Elizabeth J. Sutton
Categories: physics.med-ph cs.CV cs.LG eess.IV
Comments: Major revisions and rewriting in progress
\\ ( https://arxiv.org/abs/2312.00067 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08010
replaced with revised version Fri, 19 Jan 2024 12:19:48 GMT   (30813kb,D)

Title: EZ-CLIP: Efficient Zeroshot Video Action Recognition
Authors: Shahzad Ahmad, Sukalpa Chanda, Yogesh S Rawat
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.08010 ,  30813kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15591
replaced with revised version Fri, 19 Jan 2024 14:08:23 GMT   (425kb,D)

Title: Privacy-Preserving Neural Graph Databases
Authors: Qi Hu, Haoran Li, Jiaxin Bai, Yangqiu Song
Categories: cs.DB cs.CR cs.LG
\\ ( https://arxiv.org/abs/2312.15591 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07961 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 15:55:16 GMT   (5117kb,D)

Title: Solution of the Probabilistic Lambert Problem: Connections with Optimal
  Mass Transport, Schr\"odinger Bridge and Reaction-Diffusion PDEs
Authors: Alexis M.H. Teter, Iman Nodozi, Abhishek Halder
Categories: math.OC cs.LG cs.SY eess.SY math-ph math.MP stat.ML
\\ ( https://arxiv.org/abs/2401.07961 ,  5117kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08169 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 07:48:24 GMT   (1339kb,D)

Title: Statistical Test for Attention Map in Vision Transformer
Authors: Tomohiro Shiraishi, Daiki Miwa, Teruyuki Katsuoka, Vo Nguyen Le Duy,
  Kouichi Taji, Ichiro Takeuchi
Categories: stat.ML cs.LG
Comments: 42pages, 17figures
\\ ( https://arxiv.org/abs/2401.08169 ,  1339kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08216
replaced with revised version Fri, 19 Jan 2024 05:31:07 GMT   (3960kb,D)

Title: Towards Efficient and Certified Recovery from Poisoning Attacks in
  Federated Learning
Authors: Yu Jiang, Jiyuan Shen, Ziyao Liu, Chee Wei Tan, Kwok-Yan Lam
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2401.08216 ,  3960kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09902 (*cross-listing*)
replaced with revised version Fri, 19 Jan 2024 14:04:22 GMT   (4257kb,D)

Title: Interplay between depth and width for interpolation in neural ODEs
Authors: Antonio \'Alvarez-L\'opez, Arselane Hadj Slimane, Enrique Zuazua
Categories: math.OC cs.LG
Comments: 16 pages, 10 figures, double column
MSC-class: 34H05, 68T07, 93B05 (Primary) 35Q49 (Secondary)
\\ ( https://arxiv.org/abs/2401.09902 ,  4257kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
