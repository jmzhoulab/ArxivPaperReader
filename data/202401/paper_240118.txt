Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80011 26
send mail ONLY to cs <no-reply@arxiv.org>	2024年1月18日 17:31
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue 16 Jan 24 19:00:00 GMT  to  Wed 17 Jan 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2401.08660
Date: Wed, 27 Dec 2023 02:56:41 GMT   (6992kb,D)

Title: Gemini Pro Defeated by GPT-4V: Evidence from Education
Authors: Gyeong-Geon Lee, Ehsan Latif, Lehong Shi, and Xiaoming Zhai
Categories: cs.AI cs.CL
\\
  This study compared the classification performance of Gemini Pro and GPT-4V
in educational settings. Employing visual question answering (VQA) techniques,
the study examined both models' abilities to read text-based rubrics and then
automatically score student-drawn models in science education. We employed both
quantitative and qualitative analyses using a dataset derived from
student-drawn scientific models and employing NERIF (Notation-Enhanced Rubrics
for Image Feedback) prompting methods. The findings reveal that GPT-4V
significantly outperforms Gemini Pro in terms of scoring accuracy and Quadratic
Weighted Kappa. The qualitative analysis reveals that the differences may be
due to the models' ability to process fine-grained texts in images and overall
image classification performance. Even adapting the NERIF approach by further
de-sizing the input images, Gemini Pro seems not able to perform as well as
GPT-4V. The findings suggest GPT-4V's superior capability in handling complex
multimodal educational tasks. The study concludes that while both models
represent advancements in AI, GPT-4V's higher performance makes it a more
suitable tool for educational applications involving multimodal data
interpretation.
\\ ( https://arxiv.org/abs/2401.08660 ,  6992kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08663
Date: Wed, 27 Dec 2023 14:26:34 GMT   (10713kb,D)

Title: An Integrated Imitation and Reinforcement Learning Methodology for
  Robust Agile Aircraft Control with Limited Pilot Demonstration Data
Authors: Gulay Goktas Sever, Umut Demir, Abdullah Sadik Satir, Mustafa Cagatay
  Sahin, Nazim Kemal Ure
Categories: cs.AI cs.LG cs.RO cs.SY eess.SY
Comments: Preprint submitted to Aerospace Science and Technology
\\
  In this paper, we present a methodology for constructing data-driven maneuver
generation models for agile aircraft that can generalize across a wide range of
trim conditions and aircraft model parameters. Maneuver generation models play
a crucial role in the testing and evaluation of aircraft prototypes, providing
insights into the maneuverability and agility of the aircraft. However,
constructing the models typically requires extensive amounts of real pilot
data, which can be time-consuming and costly to obtain. Moreover, models built
with limited data often struggle to generalize beyond the specific flight
conditions covered in the original dataset. To address these challenges, we
propose a hybrid architecture that leverages a simulation model, referred to as
the source model. This open-source agile aircraft simulator shares similar
dynamics with the target aircraft and allows us to generate unlimited data for
building a proxy maneuver generation model. We then fine-tune this model to the
target aircraft using a limited amount of real pilot data. Our approach
combines techniques from imitation learning, transfer learning, and
reinforcement learning to achieve this objective. To validate our methodology,
we utilize real agile pilot data provided by Turkish Aerospace Industries
(TAI). By employing the F-16 as the source model, we demonstrate that it is
possible to construct a maneuver generation model that generalizes across
various trim conditions and aircraft parameters without requiring any
additional real pilot data. Our results showcase the effectiveness of our
approach in developing robust and adaptable models for agile aircraft.
\\ ( https://arxiv.org/abs/2401.08663 ,  10713kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08664
Date: Wed, 27 Dec 2023 14:37:32 GMT   (268kb,D)

Title: Adapting Large Language Models for Education: Foundational Capabilities,
  Potentials, and Challenges
Authors: Qingyao Li, Lingyue Fu, Weiming Zhang, Xianyu Chen, Jingwei Yu, Wei
  Xia, Weinan Zhang, Ruiming Tang, Yong Yu
Categories: cs.AI cs.CL
Comments: 13 pages, 3 figures, 1 table
\\
  Online education platforms, leveraging the internet to distribute education
resources, seek to provide convenient education but often fall short in
real-time communication with students. They often struggle to offer
personalized education resources due to the challenge of addressing the diverse
obstacles students encounter throughout their learning journey. Recently, the
emergence of large language models (LLMs), such as ChatGPT, offers the
possibility for resolving this issue by comprehending individual requests.
Although LLMs have been successful in various fields, creating an LLM-based
education system is still challenging for the wide range of educational skills
required. This paper reviews the recently emerged LLM researches related to
educational capabilities, including mathematics, writing, programming,
reasoning, and knowledge-based question answering, with the aim to explore
their potential in constructing the next-generation intelligent education
system. Based on the current development status, we further outline two
approaches for an LLM-based education system: a unified approach and a
mixture-of-expert (MoE) approach. Finally, we explore the challenges and future
directions, providing new research opportunities and perspectives on adapting
LLMs for education.
\\ ( https://arxiv.org/abs/2401.08664 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08695
Date: Sun, 14 Jan 2024 02:10:54 GMT   (18374kb,D)

Title: Enabling Collaborative Clinical Diagnosis of Infectious Keratitis by
  Integrating Expert Knowledge and Interpretable Data-driven Intelligence
Authors: Zhengqing Fang, Shuowen Zhou, Zhouhang Yuan, Yuxuan Si, Mengze Li,
  Jinxu Li, Yesheng Xu, Wenjia Xie, Kun Kuang, Yingming Li, Fei Wu, and Yu-Feng
  Yao
Categories: cs.AI cs.CV cs.HC
Comments: 33 pages
\\
  Although data-driven artificial intelligence (AI) in medical image diagnosis
has shown impressive performance in silico, the lack of interpretability makes
it difficult to incorporate the "black box" into clinicians' workflows. To make
the diagnostic patterns learned from data understandable by clinicians, we
develop an interpretable model, knowledge-guided diagnosis model (KGDM), that
provides a visualized reasoning process containing AI-based biomarkers and
retrieved cases that with the same diagnostic patterns. It embraces clinicians'
prompts into the interpreted reasoning through human-AI interaction, leading to
potentially enhanced safety and more accurate predictions. This study
investigates the performance, interpretability, and clinical utility of KGDM in
the diagnosis of infectious keratitis (IK), which is the leading cause of
corneal blindness. The classification performance of KGDM is evaluated on a
prospective validation dataset, an external testing dataset, and an publicly
available testing dataset. The diagnostic odds ratios (DOR) of the interpreted
AI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit
consistent diagnostic patterns with clinic experience. Moreover, a human-AI
collaborative diagnosis test is conducted and the participants with
collaboration achieved a performance exceeding that of both humans and AI. By
synergistically integrating interpretability and interaction, this study
facilitates the convergence of clinicians' expertise and data-driven
intelligence. The promotion of inexperienced ophthalmologists with the aid of
AI-based biomarkers, as well as increased AI prediction by intervention from
experienced ones, demonstrate a promising diagnostic paradigm for infectious
keratitis using KGDM, which holds the potential for extension to other diseases
where experienced medical practitioners are limited and the safety of AI is
concerned.
\\ ( https://arxiv.org/abs/2401.08695 ,  18374kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08743
Date: Tue, 16 Jan 2024 18:59:24 GMT   (6170kb,D)

Title: MMToM-QA: Multimodal Theory of Mind Question Answering
Authors: Chuanyang Jin, Yutong Wu, Jing Cao, Jiannan Xiang, Yen-Ling Kuo,
  Zhiting Hu, Tomer Ullman, Antonio Torralba, Joshua B. Tenenbaum, Tianmin Shu
Categories: cs.AI cs.CL cs.CV cs.LG
Comments: 27 pages, 11 figures, 7 tables
\\
  Theory of Mind (ToM), the ability to understand people's minds, is an
essential ingredient for developing machines with human-level social
intelligence. Recent machine learning models, particularly large language
models, seem to show some aspects of ToM understanding. However, existing ToM
benchmarks use unimodal datasets - either video or text. Human ToM, on the
other hand, is more than video or text understanding. People can flexibly
reason about another person's mind based on conceptual representations (e.g.,
goals, beliefs, plans) extracted from any available data, which can include
visual cues, linguistic narratives, or both. To address this, we introduce a
multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA
comprehensively evaluates machine ToM both on multimodal data and on different
kinds of unimodal data about a person's activity in a household environment. To
engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian
Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified
representations from multimodal data and utilizes language models for scalable
Bayesian inverse planning. We conducted a systematic comparison of human
performance, BIP-ALM, and state-of-the-art models, including GPT-4. The
experiments demonstrate that large language models and large multimodal models
still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising
results, by leveraging the power of both model-based mental inference and
language models.
\\ ( https://arxiv.org/abs/2401.08743 ,  6170kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08879
Date: Tue, 16 Jan 2024 23:27:42 GMT   (109kb,D)

Title: Contribution Functions for Quantitative Bipolar Argumentation Graphs: A
  Principle-based Analysis
Authors: Timotheus Kampik, Nico Potyka, Xiang Yin, Kristijonas \v{C}yras,
  Francesca Toni
Categories: cs.AI
\\
  We present a principle-based analysis of contribution functions for
quantitative bipolar argumentation graphs that quantify the contribution of one
argument to another. The introduced principles formalise the intuitions
underlying different contribution functions as well as expectations one would
have regarding the behaviour of contribution functions in general. As none of
the covered contribution functions satisfies all principles, our analysis can
serve as a tool that enables the selection of the most suitable function based
on the requirements of a given use case.
\\ ( https://arxiv.org/abs/2401.08879 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08936
Date: Wed, 17 Jan 2024 03:14:28 GMT   (465kb,D)

Title: DeLF: Designing Learning Environments with Foundation Models
Authors: Aida Afshar, Wenchao Li
Categories: cs.AI cs.LG
Comments: AAAI 2024 Workshop on Synergy of Reinforcement Learning and Large
  Language Models
\\
  Reinforcement learning (RL) offers a capable and intuitive structure for the
fundamental sequential decision-making problem. Despite impressive
breakthroughs, it can still be difficult to employ RL in practice in many
simple applications. In this paper, we try to address this issue by introducing
a method for designing the components of the RL environment for a given,
user-intended application. We provide an initial formalization for the problem
of RL component design, that concentrates on designing a good representation
for observation and action space. We propose a method named DeLF: Designing
Learning Environments with Foundation Models, that employs large language
models to design and codify the user's intended learning scenario. By testing
our method on four different learning environments, we demonstrate that DeLF
can obtain executable environment codes for the corresponding RL problems.
\\ ( https://arxiv.org/abs/2401.08936 ,  465kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08999
Date: Wed, 17 Jan 2024 06:29:34 GMT   (1898kb,D)

Title: Continuous Time Continuous Space Homeostatic Reinforcement Learning
  (CTCS-HRRL) : Towards Biological Self-Autonomous Agent
Authors: Hugo Laurencon, Yesoda Bhargava, Riddhi Zantye, Charbel-Rapha\"el
  S\'egerie, Johann Lussange, Veeky Baths, Boris Gutkin
Categories: cs.AI cs.LG
Comments: This work is a result of the ongoing collaboration between Cognitive
  Neuroscience Lab, BITS Pilani K K Birla Goa Campus and Ecole Normale
  Superieure, Paris France. This work is jointly supervised by Prof. Boris
  Gutkin and Prof. Veeky Baths. arXiv admin note: substantial text overlap with
  arXiv:2109.06580
\\
  Homeostasis is a biological process by which living beings maintain their
internal balance. Previous research suggests that homeostasis is a learned
behaviour. Recently introduced Homeostatic Regulated Reinforcement Learning
(HRRL) framework attempts to explain this learned homeostatic behavior by
linking Drive Reduction Theory and Reinforcement Learning. This linkage has
been proven in the discrete time-space, but not in the continuous time-space.
In this work, we advance the HRRL framework to a continuous time-space
environment and validate the CTCS-HRRL (Continuous Time Continuous Space HRRL)
framework. We achieve this by designing a model that mimics the homeostatic
mechanisms in a real-world biological agent. This model uses the
Hamilton-Jacobian Bellman Equation, and function approximation based on neural
networks and Reinforcement Learning. Through a simulation-based experiment we
demonstrate the efficacy of this model and uncover the evidence linked to the
agent's ability to dynamically choose policies that favor homeostasis in a
continuously changing internal-state milieu. Results of our experiments
demonstrate that agent learns homeostatic behaviour in a CTCS environment,
making CTCS-HRRL a promising framework for modellng animal dynamics and
decision-making.
\\ ( https://arxiv.org/abs/2401.08999 ,  1898kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09042
Date: Wed, 17 Jan 2024 08:22:52 GMT   (2224kb,D)

Title: LLMs for Relational Reasoning: How Far are We?
Authors: Zhiming Li, Yushi Cao, Xiufeng Xu, Junzhe Jiang, Xu Liu, Yon Shin Teo,
  Shang-wei Lin, Yang Liu
Categories: cs.AI cs.CL
Comments: Accepted by The First International Workshop on Large Language Models
  for Code (ICSE 2024)
\\
  Large language models (LLMs) have revolutionized many areas (e.g. natural
language processing, software engineering, etc.) by achieving state-of-the-art
performance on extensive downstream tasks. Aiming to achieve robust and general
artificial intelligence, there has been a surge of interest in investigating
the reasoning ability of the LLMs. Whereas the textual and numerical reasoning
benchmarks adopted by previous works are rather shallow and simple, it is hard
to conclude that the LLMs possess strong reasoning ability by merely achieving
positive results on these benchmarks. Recent efforts have demonstrated that the
LLMs are poor at solving sequential decision-making problems that require
common-sense planning by evaluating their performance on the reinforcement
learning benchmarks. In this work, we conduct an in-depth assessment of several
state-of-the-art LLMs' reasoning ability based on the inductive logic
programming (ILP) benchmark, which is broadly recognized as a representative
and challenging measurement for evaluating logic program induction/synthesis
systems as it requires inducing strict cause-effect logic to achieve robust
deduction on independent and identically distributed (IID) and
out-of-distribution (OOD) test samples. Our evaluations illustrate that
compared with the neural program induction systems which are much smaller in
model size, the state-of-the-art LLMs are much poorer in terms of reasoning
ability by achieving much lower performance and generalization using either
natural language prompting or truth-value matrix prompting.
\\ ( https://arxiv.org/abs/2401.09042 ,  2224kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09070
Date: Wed, 17 Jan 2024 09:08:23 GMT   (1802kb)

Title: Knowledge Pyramid: A Novel Hierarchical Reasoning Structure for
  Generalized Knowledge Augmentation and Inference
Authors: Qinghua Huang, Yongzhen Wang
Categories: cs.AI cs.IR
Comments: 10 pages,8 figures
\\
  Knowledge graph (KG) based reasoning has been regarded as an effective means
for the analysis of semantic networks and is of great usefulness in areas of
information retrieval, recommendation, decision-making, and man-machine
interaction. It is widely used in recommendation, decision-making,
question-answering, search, and other fields. However, previous studies mainly
used low-level knowledge in the KG for reasoning, which may result in
insufficient generalization and poor robustness of reasoning. To this end, this
paper proposes a new inference approach using a novel knowledge augmentation
strategy to improve the generalization capability of KG. This framework
extracts high-level pyramidal knowledge from low-level knowledge and applies it
to reasoning in a multi-level hierarchical KG, called knowledge pyramid in this
paper. We tested some medical data sets using the proposed approach, and the
experimental results show that the proposed knowledge pyramid has improved the
knowledge inference performance with better generalization. Especially, when
there are fewer training samples, the inference accuracy can be significantly
improved.
\\ ( https://arxiv.org/abs/2401.09070 ,  1802kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08688
Date: Sat, 13 Jan 2024 07:13:08 GMT   (479kb,D)

Title: Automated Answer Validation using Text Similarity
Authors: Balaji Ganesan, Arjun Ravikumar, Lakshay Piplani, Rini Bhaumik, Dhivya
  Padmanaban, Shwetha Narasimhamurthy, Chetan Adhikary, Subhash Deshapogu
Categories: cs.CL cs.IR
Comments: 8 pages, 4 figures, International Conference on Natural Language
  Processing (ICON) 2023
\\
  Automated answer validation can help improve learning outcomes by providing
appropriate feedback to learners, and by making question answering systems and
online learning solutions more widely available. There have been some works in
science question answering which show that information retrieval methods
outperform neural methods, especially in the multiple choice version of this
problem. We implement Siamese neural network models and produce a generalised
solution to this problem. We compare our supervised model with other text
similarity based solutions.
\\ ( https://arxiv.org/abs/2401.08688 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08694
Date: Sat, 13 Jan 2024 16:36:58 GMT   (326kb,D)

Title: Combining Confidence Elicitation and Sample-based Methods for
  Uncertainty Quantification in Misinformation Mitigation
Authors: Mauricio Rivera, Jean-Fran\c{c}ois Godbout, Reihaneh Rabbany, Kellin
  Pelrine
Categories: cs.CL cs.AI
Comments: 11 pages, 11 figures
\\
  Large Language Models have emerged as prime candidates to tackle
misinformation mitigation. However, existing approaches struggle with
hallucinations and overconfident predictions. We propose an uncertainty
quantification framework that leverages both direct confidence elicitation and
sampled-based consistency methods to provide better calibration for NLP
misinformation mitigation solutions. We first investigate the calibration of
sample-based consistency methods that exploit distinct features of consistency
across sample sizes and stochastic levels. Next, we evaluate the performance
and distributional shift of a robust numeric verbalization prompt across single
vs. two-step confidence elicitation procedure. We also compare the performance
of the same prompt with different versions of GPT and different numerical
scales. Finally, we combine the sample-based consistency and verbalized methods
to propose a hybrid framework that yields a better uncertainty estimation for
GPT models. Overall, our work proposes novel uncertainty quantification methods
that will improve the reliability of Large Language Models in misinformation
mitigation applications.
\\ ( https://arxiv.org/abs/2401.08694 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08772
Date: Tue, 16 Jan 2024 19:00:10 GMT   (189kb,D)

Title: HuixiangDou: Overcoming Group Chat Scenarios with LLM-based Technical
  Assistance
Authors: Huanjun Kong, Songyang Zhang, Kai Chen
Categories: cs.CL
Comments: Technical report, 11 pages, 3 figures
\\
  In this work, we present HuixiangDou, a technical assistant powered by Large
Language Models (LLM). This system is designed to assist algorithm developers
by providing insightful responses to questions related to open-source algorithm
projects, such as computer vision and deep learning projects from OpenMMLab. We
further explore the integration of this assistant into the group chats of
instant messaging (IM) tools such as WeChat and Lark. Through several iterative
improvements and trials, we have developed a sophisticated technical chat
assistant capable of effectively answering users' technical questions without
causing message flooding. This paper's contributions include: 1) Designing an
algorithm pipeline specifically for group chat scenarios; 2) Verifying the
reliable performance of text2vec in task rejection; 3) Identifying three
critical requirements for LLMs in technical-assistant-like products, namely
scoring ability, In-Context Learning (ICL), and Long Context. We have made the
software and source code available at https://github.com/internlm/huixiangdou
to aid in future research and application. HuixiangDou is applicable to any
group chat within IM tools.
\\ ( https://arxiv.org/abs/2401.08772 ,  189kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08835
Date: Tue, 16 Jan 2024 21:16:12 GMT   (398kb,D)

Title: Improving ASR Contextual Biasing with Guided Attention
Authors: Jiyang Tang, Kwangyoun Kim, Suwon Shon, Felix Wu, Prashant Sridhar,
  Shinji Watanabe
Categories: cs.CL eess.AS
Comments: Accepted at ICASSP 2024
\\
  In this paper, we propose a Guided Attention (GA) auxiliary training loss,
which improves the effectiveness and robustness of automatic speech recognition
(ASR) contextual biasing without introducing additional parameters. A common
challenge in previous literature is that the word error rate (WER) reduction
brought by contextual biasing diminishes as the number of bias phrases
increases. To address this challenge, we employ a GA loss as an additional
training objective besides the Transducer loss. The proposed GA loss aims to
teach the cross attention how to align bias phrases with text tokens or audio
frames. Compared to studies with similar motivations, the proposed loss
operates directly on the cross attention weights and is easier to implement.
Through extensive experiments based on Conformer Transducer with Contextual
Adapter, we demonstrate that the proposed method not only leads to a lower WER
but also retains its effectiveness as the number of bias phrases increases.
Specifically, the GA loss decreases the WER of rare vocabularies by up to 19.2%
on LibriSpeech compared to the contextual biasing baseline, and up to 49.3%
compared to a vanilla Transducer.
\\ ( https://arxiv.org/abs/2401.08835 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08919
Date: Wed, 17 Jan 2024 02:04:59 GMT   (307kb,D)

Title: Partial Diacritization: A Context-Contrastive Inference Approach
Authors: Muhammad ElNokrashy, Badr AlKhamissi
Categories: cs.CL cs.LG
Comments: 13 equations, 5 tables, 5 figures
\\
  Diacritization plays a pivotal role in improving readability and
disambiguating the meaning of Arabic texts. Efforts have so far focused on
marking every eligible character (Full Diacritization). Comparatively
overlooked, Partial Diacritzation (PD) is the selection of a subset of
characters to be marked to aid comprehension where needed. Research has
indicated that excessive diacritic marks can hinder skilled readers--reducing
reading speed and accuracy. We conduct a behavioral experiment and show that
partially marked text is often easier to read than fully marked text, and
sometimes easier than plain text. In this light, we introduce
Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which
integrates seamlessly with existing Arabic diacritization systems. CCPD
processes each word twice, once with context and once without, and diacritizes
only the characters with disparities between the two inferences. Further, we
introduce novel indicators for measuring partial diacritization quality (SR,
PDER, HDER, ERE), essential for establishing this as a machine learning task.
Lastly, we introduce TD2, a Transformer-variant of an established model which
offers a markedly different per formance profile on our proposed indicators
compared to all other known systems.
\\ ( https://arxiv.org/abs/2401.08919 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08967
Date: Wed, 17 Jan 2024 04:43:21 GMT   (3687kb,D)

Title: ReFT: Reasoning with Reinforced Fine-Tuning
Authors: Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin,
  Hang Li
Categories: cs.CL
Comments: 13 pages
\\
  One way to enhance the reasoning capability of Large Language Models (LLMs)
is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT)
annotations. This approach does not show sufficiently strong generalization
ability, however, because the training only relies on the given CoT data. In
math problem-solving, for example, there is usually only one annotated
reasoning path for each question in the training data. Intuitively, it would be
better for the algorithm to learn from multiple annotated reasoning paths given
a question. To address this issue, we propose a simple yet effective approach
called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of
learning LLMs for reasoning, with math problem-solving as an example. ReFT
first warmups the model with SFT, and then employs on-line reinforcement
learning, specifically the PPO algorithm in this paper, to further fine-tune
the model, where an abundance of reasoning paths are automatically sampled
given the question and the rewards are naturally derived from the ground-truth
answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that
ReFT significantly outperforms SFT, and the performance can be potentially
further boosted by combining inference-time strategies such as majority voting
and re-ranking. Note that ReFT obtains the improvement by learning from the
same training questions as SFT, without relying on extra or augmented training
questions. This indicates a superior generalization ability for ReFT.
\\ ( https://arxiv.org/abs/2401.08967 ,  3687kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08992
Date: Wed, 17 Jan 2024 06:01:16 GMT   (80kb,D)

Title: Efficient Adapter Finetuning for Tail Languages in Streaming
  Multilingual ASR
Authors: Junwen Bai, Bo Li, Qiujia Li, Tara N. Sainath, Trevor Strohman
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: Accepted to ICASSP 2024
\\
  The end-to-end ASR model is often desired in the streaming multilingual
scenario since it is easier to deploy and can benefit from pre-trained speech
models such as powerful foundation models. Meanwhile, the heterogeneous nature
and imbalanced data abundance of different languages may cause performance
degradation, leading to asynchronous peak performance for different languages
during training, especially on tail ones. Sometimes even the data itself may
become unavailable as a result of the enhanced privacy protection. Existing
work tend to significantly increase the model size or learn language-specific
decoders to accommodate each language separately. In this study, we explore
simple yet effective Language-Dependent Adapter (LDA) finetuning under a
cascaded Conformer transducer framework enhanced by teacher pseudo-labeling for
tail languages in the streaming multilingual ASR. The adapter only accounts for
0.4% of the full model per language. It is plugged into the frozen foundation
model and is the only trainable module during the finetuning process with noisy
student training. The final model merges the adapter parameters from different
checkpoints for different languages. The model performance is validated on a
challenging multilingual dictation dataset, which includes 39 tail languages
across Latin, Greek, Arabic, etc. Our proposed method brings 12.2% word error
rate reduction on average and up to 37.5% on a single locale. Furthermore, we
show that our parameter-efficient LDA can match the quality of the full model
finetuning, thus greatly alleviating the asynchronous peak performance issue.
\\ ( https://arxiv.org/abs/2401.08992 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09002
Date: Wed, 17 Jan 2024 06:42:44 GMT   (7692kb,D)

Title: AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on
  Large Language Models
Authors: Dong shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong
  Zhang, Yongfeng Zhang
Categories: cs.CL
\\
  In our research, we pioneer a novel approach to evaluate the effectiveness of
jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2,
diverging from traditional robustness-focused binary evaluations. Our study
introduces two distinct evaluation frameworks: a coarse-grained evaluation and
a fine-grained evaluation. Each framework, using a scoring range from 0 to 1,
offers a unique perspective, enabling a more comprehensive and nuanced
evaluation of attack effectiveness and empowering attackers to refine their
attack prompts with greater understanding. Furthermore, we have developed a
comprehensive ground truth dataset specifically tailored for jailbreak tasks.
This dataset not only serves as a crucial benchmark for our current study but
also establishes a foundational resource for future research, enabling
consistent and comparative analyses in this evolving field. Upon meticulous
comparison with traditional evaluation methods, we discovered that our
evaluation aligns with the baseline's trend while offering a more profound and
detailed assessment. We believe that by accurately evaluating the effectiveness
of attack prompts in the Jailbreak task, our work lays a solid foundation for
assessing a wider array of similar or even more complex tasks in the realm of
prompt injection, potentially revolutionizing this field.
\\ ( https://arxiv.org/abs/2401.09002 ,  7692kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09003
Date: Wed, 17 Jan 2024 06:48:16 GMT   (83kb,D)

Title: Augmenting Math Word Problems via Iterative Question Composing
Authors: Haoxiong Liu, Andrew Chi-Chih Yao
Categories: cs.CL cs.AI cs.LG
\\
  Despite recent progress in improving the mathematical reasoning ability of
large language models(LLMs), solving competition-level math problems without
the use of external tools remains challenging for open-source LLMs. In this
work, we introduce the MMIQC dataset, a mixture of processed web data and
synthetic question-response pairs, to equip base models with better
mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by
fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on
MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B)
SOTA. Our experiments also show that a large part of the improvement attributes
to our novel augmentation method IQC(Iterative Question Composing), where we
iteratively ask an LLM to compose new questions from the given seed problems
and do rejection sampling from another LLM. MMIQC has now been released on
https://huggingface.co/datasets/Vivacem/MMIQC.
\\ ( https://arxiv.org/abs/2401.09003 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09023
Date: Wed, 17 Jan 2024 07:36:22 GMT   (386kb,D)

Title: Explain Thyself Bully: Sentiment Aided Cyberbullying Detection with
  Explanation
Authors: Krishanu Maity, Prince Jha, Raghav Jain, Sriparna Saha, Pushpak
  Bhattacharyya
Categories: cs.CL
Comments: ICDAR 2023
\\
  Cyberbullying has become a big issue with the popularity of different social
media networks and online communication apps. While plenty of research is going
on to develop better models for cyberbullying detection in monolingual
language, there is very little research on the code-mixed languages and
explainability aspect of cyberbullying. Recent laws like "right to
explanations" of General Data Protection Regulation, have spurred research in
developing interpretable models rather than focusing on performance. Motivated
by this we develop the first interpretable multi-task model called {\em mExCB}
for automatic cyberbullying detection from code-mixed languages which can
simultaneously solve several tasks, cyberbullying detection,
explanation/rationale identification, target group detection and sentiment
analysis. We have introduced {\em BullyExplain}, the first benchmark dataset
for explainable cyberbullying detection in code-mixed language. Each post in
{\em BullyExplain} dataset is annotated with four labels, i.e., {\em bully
label, sentiment label, target and rationales (explainability)}, i.e., which
phrases are being responsible for annotating the post as a bully. The proposed
multitask framework (mExCB) based on CNN and GRU with word and sub-sentence
(SS) level attention is able to outperform several baselines and state of the
art models when applied on {\em BullyExplain} dataset.
\\ ( https://arxiv.org/abs/2401.09023 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09041
Date: Wed, 17 Jan 2024 08:16:05 GMT   (765kb,D)

Title: Textual Summarisation of Large Sets: Towards a General Approach
Authors: Kittipitch Kuptavanich, Ehud Reiter, Kees Van Deemter, Advaith
  Siddharthan
Categories: cs.CL
\\
  We are developing techniques to generate summary descriptions of sets of
objects. In this paper, we present and evaluate a rule-based NLG technique for
summarising sets of bibliographical references in academic papers. This extends
our previous work on summarising sets of consumer products and shows how our
model generalises across these two very different domains.
\\ ( https://arxiv.org/abs/2401.09041 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09082
Date: Wed, 17 Jan 2024 09:44:03 GMT   (720kb,D)

Title: What makes for a 'good' social actor? Using respect as a lens to
  evaluate interactions with language agents
Authors: Lize Alberts, Geoff Keeling and Amanda McCroskery
Categories: cs.CL cs.AI cs.HC
MSC-class: 68T42
ACM-class: H.5.2; I.2; J.4; J.5
\\
  With the growing popularity of dialogue agents based on large language models
(LLMs), urgent attention has been drawn to finding ways to ensure their
behaviour is ethical and appropriate. These are largely interpreted in terms of
the 'HHH' criteria: making outputs more helpful and honest, and avoiding
harmful (biased, toxic, or inaccurate) statements. Whilst this semantic focus
is useful from the perspective of viewing LLM agents as mere mediums for
information, it fails to account for pragmatic factors that can make the same
utterance seem more or less offensive or tactless in different social
situations. We propose an approach to ethics that is more centred on relational
and situational factors, exploring what it means for a system, as a social
actor, to treat an individual respectfully in a (series of) interaction(s). Our
work anticipates a set of largely unexplored risks at the level of situated
interaction, and offers practical suggestions to help LLM technologies behave
as 'good' social actors and treat people respectfully.
\\ ( https://arxiv.org/abs/2401.09082 ,  720kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09150
Date: Wed, 17 Jan 2024 11:50:53 GMT   (11437kb,D)

Title: Bridging Research and Readers: A Multi-Modal Automated Academic Papers
  Interpretation System
Authors: Feng Jiang, Kuang Wang, Haizhou Li
Categories: cs.CL
\\
  In the contemporary information era, significantly accelerated by the advent
of Large-scale Language Models, the proliferation of scientific literature is
reaching unprecedented levels. Researchers urgently require efficient tools for
reading and summarizing academic papers, uncovering significant scientific
literature, and employing diverse interpretative methodologies. To address this
burgeoning demand, the role of automated scientific literature interpretation
systems has become paramount. However, prevailing models, both commercial and
open-source, confront notable challenges: they often overlook multimodal data,
grapple with summarizing over-length texts, and lack diverse user interfaces.
In response, we introduce an open-source multi-modal automated academic paper
interpretation system (MMAPIS) with three-step process stages, incorporating
LLMs to augment its functionality. Our system first employs the hybrid modality
preprocessing and alignment module to extract plain text, and tables or figures
from documents separately. It then aligns this information based on the section
names they belong to, ensuring that data with identical section names are
categorized under the same section. Following this, we introduce a hierarchical
discourse-aware summarization method. It utilizes the extracted section names
to divide the article into shorter text segments, facilitating specific
summarizations both within and between sections via LLMs with specific prompts.
Finally, we have designed four types of diversified user interfaces, including
paper recommendation, multimodal Q\&A, audio broadcasting, and interpretation
blog, which can be widely applied across various scenarios. Our qualitative and
quantitative evaluations underscore the system's superiority, especially in
scientific summarization, where it outperforms solutions relying solely on
GPT-4.
\\ ( https://arxiv.org/abs/2401.09150 ,  11437kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09168
Date: Wed, 17 Jan 2024 12:21:20 GMT   (283kb,D)

Title: Fine-tuning Strategies for Domain Specific Question Answering under Low
  Annotation Budget Constraints
Authors: Kunpeng Guo, Dennis Diefenbach, Antoine Gourru, Christophe Gravier
Categories: cs.CL
DOI: 10.1109/ICTAI59109.2023.00032
\\
  The progress introduced by pre-trained language models and their fine-tuning
has resulted in significant improvements in most downstream NLP tasks. The
unsupervised training of a language model combined with further target task
fine-tuning has become the standard QA fine-tuning procedure. In this work, we
demonstrate that this strategy is sub-optimal for fine-tuning QA models,
especially under a low QA annotation budget, which is a usual setting in
practice due to the extractive QA labeling cost. We draw our conclusions by
conducting an exhaustive analysis of the performance of the alternatives of the
sequential fine-tuning strategy on different QA datasets. Based on the
experiments performed, we observed that the best strategy to fine-tune the QA
model in low-budget settings is taking a pre-trained language model (PLM) and
then fine-tuning PLM with a dataset composed of the target dataset and SQuAD
dataset. With zero extra annotation effort, the best strategy outperforms the
standard strategy by 2.28% to 6.48%. Our experiments provide one of the first
investigations on how to best fine-tune a QA system under a low budget and are
therefore of the utmost practical interest to the QA practitioners.
\\ ( https://arxiv.org/abs/2401.09168 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09175
Date: Wed, 17 Jan 2024 12:31:45 GMT   (1399kb,D)

Title: QAnswer: Towards Question Answering Search over Websites
Authors: Kunpeng Guo, Clement Defretiere, Dennis Diefenbach, Christophe
  Gravier, Antoine Gourru
Categories: cs.CL
DOI: 10.1145/3487553
\\
  Question Answering (QA) is increasingly used by search engines to provide
results to their end-users, yet very few websites currently use QA technologies
for their search functionality. To illustrate the potential of QA technologies
for the website search practitioner, we demonstrate web searches that combine
QA over knowledge graphs and QA over free text -- each being usually tackled
separately. We also discuss the different benefits and drawbacks of both
approaches for web site searches. We use the case studies made of websites
hosted by the Wikimedia Foundation (namely Wikipedia and Wikidata). Differently
from a search engine (e.g. Google, Bing, etc), the data are indexed integrally,
i.e. we do not index only a subset, and they are indexed exclusively, i.e. we
index only data available on the corresponding website.
\\ ( https://arxiv.org/abs/2401.09175 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09220
Date: Wed, 17 Jan 2024 14:02:36 GMT   (5928kb,D)

Title: UniVIE: A Unified Label Space Approach to Visual Information Extraction
  from Form-like Documents
Authors: Kai Hu, Jiawei Wang, Weihong Lin, Zhuoyao Zhong, Lei Sun, Qiang Huo
Categories: cs.CL
\\
  Existing methods for Visual Information Extraction (VIE) from form-like
documents typically fragment the process into separate subtasks, such as key
information extraction, key-value pair extraction, and choice group extraction.
However, these approaches often overlook the hierarchical structure of form
documents, including hierarchical key-value pairs and hierarchical choice
groups. To address these limitations, we present a new perspective, reframing
VIE as a relation prediction problem and unifying labels of different tasks
into a single label space. This unified approach allows for the definition of
various relation types and effectively tackles hierarchical relationships in
form-like documents. In line with this perspective, we present UniVIE, a
unified model that addresses the VIE problem comprehensively. UniVIE functions
using a coarse-to-fine strategy. It initially generates tree proposals through
a tree proposal network, which are subsequently refined into hierarchical trees
by a relation decoder module. To enhance the relation prediction capabilities
of UniVIE, we incorporate two novel tree constraints into the relation decoder:
a tree attention mask and a tree level embedding. Extensive experimental
evaluations on both our in-house dataset HierForms and a publicly available
dataset SIBR, substantiate that our method achieves state-of-the-art results,
underscoring the effectiveness and potential of our unified approach in
advancing the field of VIE.
\\ ( https://arxiv.org/abs/2401.09220 ,  5928kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09244
Date: Wed, 17 Jan 2024 14:44:27 GMT   (551kb,D)

Title: Cross-lingual Offensive Language Detection: A Systematic Review of
  Datasets, Transfer Approaches and Challenges
Authors: Aiqi Jiang, Arkaitz Zubiaga
Categories: cs.CL
Comments: 35 pages, 7 figures
\\
  The growing prevalence and rapid evolution of offensive language in social
media amplify the complexities of detection, particularly highlighting the
challenges in identifying such content across diverse languages. This survey
presents a systematic and comprehensive exploration of Cross-Lingual Transfer
Learning (CLTL) techniques in offensive language detection in social media. Our
study stands as the first holistic overview to focus exclusively on the
cross-lingual scenario in this domain. We analyse 67 relevant papers and
categorise these studies across various dimensions, including the
characteristics of multilingual datasets used, the cross-lingual resources
employed, and the specific CLTL strategies implemented. According to "what to
transfer", we also summarise three main CLTL transfer approaches: instance,
feature, and parameter transfer. Additionally, we shed light on the current
challenges and future research opportunities in this field. Furthermore, we
have made our survey resources available online, including two comprehensive
tables that provide accessible references to the multilingual datasets and CLTL
methods used in the reviewed literature.
\\ ( https://arxiv.org/abs/2401.09244 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09248
Date: Wed, 17 Jan 2024 14:52:26 GMT   (8451kb,D)

Title: Learning from Emotions, Demographic Information and Implicit User
  Feedback in Task-Oriented Document-Grounded Dialogues
Authors: Dominic Petrak, Thy Thy Tran, Iryna Gurevych
Categories: cs.CL cs.HC
\\
  The success of task-oriented and document-grounded dialogue systems depends
on users accepting and enjoying using them. To achieve this, recently published
work in the field of Human-Computer Interaction suggests that the combination
of considering demographic information, user emotions and learning from the
implicit feedback in their utterances, is particularly important. However,
these findings have not yet been transferred to the field of Natural Language
Processing, where these data are primarily studied separately. Accordingly, no
sufficiently annotated dataset is available. To address this gap, we introduce
FEDI, the first English dialogue dataset for task-oriented document-grounded
dialogues annotated with demographic information, user emotions and implicit
feedback. Our experiments with FLAN-T5, GPT-2 and LLaMA-2 show that these data
have the potential to improve task completion and the factual consistency of
the generated responses and user acceptance.
\\ ( https://arxiv.org/abs/2401.09248 ,  8451kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09333
Date: Wed, 17 Jan 2024 16:57:18 GMT   (7551kb,D)

Title: Machines Do See Color: A Guideline to Classify Different Forms of Racist
  Discourse in Large Corpora
Authors: Diana Davila Gordillo, Joan Timoneda, Sebastian Vallejo Vera
Categories: cs.CL cs.LG
Comments: 37 pages, 5 figures, 4 tables
\\
  Current methods to identify and classify racist language in text rely on
small-n qualitative approaches or large-n approaches focusing exclusively on
overt forms of racist discourse. This article provides a step-by-step
generalizable guideline to identify and classify different forms of racist
discourse in large corpora. In our approach, we start by conceptualizing racism
and its different manifestations. We then contextualize these racist
manifestations to the time and place of interest, which allows researchers to
identify their discursive form. Finally, we apply XLM-RoBERTa (XLM-R), a
cross-lingual model for supervised text classification with a cutting-edge
contextual understanding of text. We show that XLM-R and XLM-R-Racismo, our
pretrained model, outperform other state-of-the-art approaches in classifying
racism in large corpora. We illustrate our approach using a corpus of tweets
relating to the Ecuadorian ind\'igena community between 2018 and 2021.
\\ ( https://arxiv.org/abs/2401.09333 ,  7551kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09334
Date: Wed, 17 Jan 2024 16:57:19 GMT   (147kb,D)

Title: Large Language Models Are Neurosymbolic Reasoners
Authors: Meng Fang, Shilong Deng, Yudi Zhang, Zijing Shi, Ling Chen, Mykola
  Pechenizkiy, Jun Wang
Categories: cs.CL cs.AI
Comments: Accepted by AAAI 2024
\\
  A wide range of real-world applications is characterized by their symbolic
nature, necessitating a strong capability for symbolic reasoning. This paper
investigates the potential application of Large Language Models (LLMs) as
symbolic reasoners. We focus on text-based games, significant benchmarks for
agents with natural language capabilities, particularly in symbolic tasks like
math, map reading, sorting, and applying common sense in text-based worlds. To
facilitate these agents, we propose an LLM agent designed to tackle symbolic
challenges and achieve in-game objectives. We begin by initializing the LLM
agent and informing it of its role. The agent then receives observations and a
set of valid actions from the text-based games, along with a specific symbolic
module. With these inputs, the LLM agent chooses an action and interacts with
the game environments. Our experimental results demonstrate that our method
significantly enhances the capability of LLMs as automated agents for symbolic
reasoning, and our LLM agent is effective in text-based games involving
symbolic tasks, achieving an average performance of 88% across all tasks.
\\ ( https://arxiv.org/abs/2401.09334 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09343
Date: Wed, 17 Jan 2024 17:08:36 GMT   (70kb,D)

Title: Efficient slot labelling
Authors: Vladimir Vlasov
Categories: cs.CL
\\
  Slot labelling is an essential component of any dialogue system, aiming to
find important arguments in every user turn. Common approaches involve large
pre-trained language models (PLMs) like BERT or RoBERTa, but they face
challenges such as high computational requirements and dependence on
pre-training data. In this work, we propose a lightweight method which performs
on par or better than the state-of-the-art PLM-based methods, while having
almost 10x less trainable parameters. This makes it especially applicable for
real-life industry scenarios.
\\ ( https://arxiv.org/abs/2401.09343 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09395
Date: Wed, 17 Jan 2024 18:13:07 GMT   (1504kb,D)

Title: Stuck in the Quicksand of Numeracy, Far from AGI Summit: Evaluating
  LLMs' Mathematical Competency through Ontology-guided Perturbations
Authors: Pengfei Hong, Deepanway Ghosal, Navonil Majumder, Somak Aditya, Rada
  Mihalcea, Soujanya Poria
Categories: cs.CL
\\
  Recent advancements in Large Language Models (LLMs) have showcased striking
results on existing logical reasoning benchmarks, with some models even
surpassing human performance. However, the true depth of their competencies and
robustness, in mathematical reasoning tasks, remains an open question. In
response, we develop (i) an ontology of perturbations of maths questions, (ii)
a semi-automatic method of perturbation, and (iii) a dataset of perturbed maths
questions to probe the limits of LLM capabilities in mathematical reasoning
tasks. These controlled perturbations span across multiple fine dimensions of
the structural and representational aspects of maths questions. Using GPT-4, we
generated the MORE dataset by perturbing randomly selected five seed questions
from GSM8K. This process was guided by our ontology and involved a thorough
automatic and manual filtering process, yielding a set of 216 maths problems.
We conducted comprehensive evaluation of both closed-source and open-source
LLMs on MORE. The results show a significant performance drop across all the
models against the perturbed questions. This strongly suggests that current
LLMs lack robust mathematical skills and deep reasoning abilities. This
research not only identifies multiple gaps in the capabilities of current
models, but also highlights multiple potential directions for future
development. Our dataset will be made publicly available at
https://huggingface.co/datasets/declare-lab/GSM8k_MORE.
\\ ( https://arxiv.org/abs/2401.09395 ,  1504kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09407
Date: Wed, 17 Jan 2024 18:45:13 GMT   (14125kb,D)

Title: Deciphering Textual Authenticity: A Generalized Strategy through the
  Lens of Large Language Semantics for Detecting Human vs. Machine-Generated
  Text
Authors: Mazal Bethany, Brandon Wherry, Emet Bethany, Nishant Vishwamitra,
  Peyman Najafirad
Categories: cs.CL cs.LG
\\
  With the recent proliferation of Large Language Models (LLMs), there has been
an increasing demand for tools to detect machine-generated text. The effective
detection of machine-generated text face two pertinent problems: First, they
are severely limited in generalizing against real-world scenarios, where
machine-generated text is produced by a variety of generators, including but
not limited to GPT-4 and Dolly, and spans diverse domains, ranging from
academic manuscripts to social media posts. Second, existing detection
methodologies treat texts produced by LLMs through a restrictive binary
classification lens, neglecting the nuanced diversity of artifacts generated by
different LLMs. In this work, we undertake a systematic study on the detection
of machine-generated text in real-world scenarios. We first study the
effectiveness of state-of-the-art approaches and find that they are severely
limited against text produced by diverse generators and domains in the real
world. Furthermore, t-SNE visualizations of the embeddings from a pretrained
LLM's encoder show that they cannot reliably distinguish between human and
machine-generated text. Based on our findings, we introduce a novel system,
T5LLMCipher, for detecting machine-generated text using a pretrained T5 encoder
combined with LLM embedding sub-clustering to address the text produced by
diverse generators and domains in the real world. We evaluate our approach
across 9 machine-generated text systems and 9 domains and find that our
approach provides state-of-the-art generalization ability, with an average
increase in F1 score on machine-generated text of 19.6\% on unseen generators
and domains compared to the top performing existing approaches and correctly
attributes the generator of text with an accuracy of 93.6\%.
\\ ( https://arxiv.org/abs/2401.09407 ,  14125kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08619
Date: Tue, 5 Dec 2023 11:30:00 GMT   (2405kb,D)

Title: MATE-Pred: Multimodal Attention-based TCR-Epitope interaction Predictor
Authors: Etienne Goffinet, Raghvendra Mall, Ankita Singh, Rahul Kaushik and
  Filippo Castiglione
Categories: cs.LG cs.AI
Comments: Patent pending: U.S. Provisional Application No. 63/603,952
\\
  An accurate binding affinity prediction between T-cell receptors and epitopes
contributes decisively to develop successful immunotherapy strategies. Some
state-of-the-art computational methods implement deep learning techniques by
integrating evolutionary features to convert the amino acid residues of cell
receptors and epitope sequences into numerical values, while some other methods
employ pre-trained language models to summarize the embedding vectors at the
amino acid residue level to obtain sequence-wise representations.
  Here, we propose a highly reliable novel method, MATE-Pred, that performs
multi-modal attention-based prediction of T-cell receptors and epitopes binding
affinity. The MATE-Pred is compared and benchmarked with other deep learning
models that leverage multi-modal representations of T-cell receptors and
epitopes. In the proposed method, the textual representation of proteins is
embedded with a pre-trained bi-directional encoder model and combined with two
additional modalities: a) a comprehensive set of selected physicochemical
properties; b) predicted contact maps that estimate the 3D distances between
amino acid residues in the sequences.
  The MATE-Pred demonstrates the potential of multi-modal model in achieving
state-of-the-art performance (+8.4\% MCC, +5.5\% AUC compared to baselines) and
efficiently capturing contextual, physicochemical, and structural information
from amino acid residues. The performance of MATE-Pred projects its potential
application in various drug discovery regimes.
\\ ( https://arxiv.org/abs/2401.08619 ,  2405kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08669
Date: Mon, 8 Jan 2024 21:13:07 GMT   (5065kb,D)

Title: Deep Reinforcement Learning for Multi-Truck Vehicle Routing Problems
  with Multi-Leg Demand Routes
Authors: Joshua Levin, Randall Correll, Takanori Ide, Takafumi Suzuki, Takaho
  Saito, Alan Arai
Categories: cs.LG cs.AI
Comments: 13 pages, 4 figures
\\
  Deep reinforcement learning (RL) has been shown to be effective in producing
approximate solutions to some vehicle routing problems (VRPs), especially when
using policies generated by encoder-decoder attention mechanisms. While these
techniques have been quite successful for relatively simple problem instances,
there are still under-researched and highly complex VRP variants for which no
effective RL method has been demonstrated. In this work we focus on one such
VRP variant, which contains multiple trucks and multi-leg routing requirements.
In these problems, demand is required to move along sequences of nodes, instead
of just from a start node to an end node. With the goal of making deep RL a
viable strategy for real-world industrial-scale supply chain logistics, we
develop new extensions to existing encoder-decoder attention models which allow
them to handle multiple trucks and multi-leg routing requirements. Our models
have the advantage that they can be trained for a small number of trucks and
nodes, and then embedded into a large supply chain to yield solutions for
larger numbers of trucks and nodes. We test our approach on a real supply chain
environment arising in the operations of Japanese automotive parts manufacturer
Aisin Corporation, and find that our algorithm outperforms Aisin's previous
best solution.
\\ ( https://arxiv.org/abs/2401.08669 ,  5065kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08672
Date: Tue, 9 Jan 2024 23:32:18 GMT   (29kb)

Title: Concept Alignment
Authors: Sunayana Rane, Polyphony J. Bruna, Ilia Sucholutsky, Christopher
  Kello, Thomas L. Griffiths
Categories: cs.LG cs.AI q-bio.NC
Comments: NeurIPS MP2 Workshop 2023
\\
  Discussion of AI alignment (alignment between humans and AI systems) has
focused on value alignment, broadly referring to creating AI systems that share
human values. We argue that before we can even attempt to align values, it is
imperative that AI systems and humans align the concepts they use to understand
the world. We integrate ideas from philosophy, cognitive science, and deep
learning to explain the need for concept alignment, not just value alignment,
between humans and machines. We summarize existing accounts of how humans and
machines currently learn concepts, and we outline opportunities and challenges
in the path towards shared concepts. Finally, we explain how we can leverage
the tools already being developed in cognitive science and AI research to
accelerate progress towards concept alignment.
\\ ( https://arxiv.org/abs/2401.08672 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08690
Date: Sat, 13 Jan 2024 11:18:18 GMT   (7599kb,D)

Title: Contrastive Learning with Negative Sampling Correction
Authors: Lu Wang, Chao Du, Pu Zhao, Chuan Luo, Zhangchi Zhu, Bo Qiao, Wei
  Zhang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang
Categories: cs.LG
Comments: 9 pages, 3 figures
\\
  As one of the most effective self-supervised representation learning methods,
contrastive learning (CL) relies on multiple negative pairs to contrast against
each positive pair. In the standard practice of contrastive learning, data
augmentation methods are utilized to generate both positive and negative pairs.
While existing works have been focusing on improving the positive sampling, the
negative sampling process is often overlooked. In fact, the generated negative
samples are often polluted by positive samples, which leads to a biased loss
and performance degradation. To correct the negative sampling bias, we propose
a novel contrastive learning method named Positive-Unlabeled Contrastive
Learning (PUCL). PUCL treats the generated negative samples as unlabeled
samples and uses information from positive samples to correct bias in
contrastive loss. We prove that the corrected loss used in PUCL only incurs a
negligible bias compared to the unbiased contrastive loss. PUCL can be applied
to general contrastive learning problems and outperforms state-of-the-art
methods on various image and graph classification tasks. The code of PUCL is in
the supplementary file.
\\ ( https://arxiv.org/abs/2401.08690 ,  7599kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08703
Date: Mon, 15 Jan 2024 03:33:39 GMT   (2493kb,D)

Title: Decoupled Prototype Learning for Reliable Test-Time Adaptation
Authors: Guowei Wang, Changxing Ding, Wentao Tan, Mingkui Tan
Categories: cs.LG
Comments: 12 pages, 5 figures
\\
  Test-time adaptation (TTA) is a task that continually adapts a pre-trained
source model to the target domain during inference. One popular approach
involves fine-tuning model with cross-entropy loss according to estimated
pseudo-labels. However, its performance is significantly affected by noisy
pseudo-labels. This study reveals that minimizing the classification error of
each sample causes the cross-entropy loss's vulnerability to label noise. To
address this issue, we propose a novel Decoupled Prototype Learning (DPL)
method that features prototype-centric loss computation. First, we decouple the
optimization of class prototypes. For each class prototype, we reduce its
distance with positive samples and enlarge its distance with negative samples
in a contrastive manner. This strategy prevents the model from overfitting to
noisy pseudo-labels. Second, we propose a memory-based strategy to enhance
DPL's robustness for the small batch sizes often encountered in TTA. We update
each class's pseudo-feature from a memory in a momentum manner and insert an
additional DPL loss. Finally, we introduce a consistency regularization-based
approach to leverage samples with unconfident pseudo-labels. This approach
transfers feature styles of samples with unconfident pseudo-labels to those
with confident pseudo-labels. Thus, more reliable samples for TTA are created.
The experimental results demonstrate that our methods achieve state-of-the-art
performance on domain generalization benchmarks, and reliably improve the
performance of self-training-based methods on image corruption benchmarks. The
code will be released.
\\ ( https://arxiv.org/abs/2401.08703 ,  2493kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08715
Date: Tue, 16 Jan 2024 00:14:37 GMT   (891kb)

Title: Selecting Subsets of Source Data for Transfer Learning with Applications
  in Metal Additive Manufacturing
Authors: Yifan Tang, M. Rahmani Dehaghani, Pouyan Sajadi, G. Gary Wang
Categories: cs.LG cs.AI
Comments: 26 pages, 9 figures
\\
  Considering data insufficiency in metal additive manufacturing (AM), transfer
learning (TL) has been adopted to extract knowledge from source domains (e.g.,
completed printings) to improve the modeling performance in target domains
(e.g., new printings). Current applications use all accessible source data
directly in TL with no regard to the similarity between source and target data.
This paper proposes a systematic method to find appropriate subsets of source
data based on similarities between the source and target datasets for a given
set of limited target domain data. Such similarity is characterized by the
spatial and model distance metrics. A Pareto frontier-based source data
selection method is developed, where the source data located on the Pareto
frontier defined by two similarity distance metrics are selected iteratively.
The method is integrated into an instance-based TL method (decision tree
regression model) and a model-based TL method (fine-tuned artificial neural
network). Both models are then tested on several regression tasks in metal AM.
Comparison results demonstrate that 1) the source data selection method is
general and supports integration with various TL methods and distance metrics,
2) compared with using all source data, the proposed method can find a small
subset of source data from the same domain with better TL performance in metal
AM regression tasks involving different processes and machines, and 3) when
multiple source domains exist, the source data selection method could find the
subset from one source domain to obtain comparable or better TL performance
than the model constructed using data from all source domains.
\\ ( https://arxiv.org/abs/2401.08715 ,  891kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08718
Date: Tue, 16 Jan 2024 05:21:51 GMT   (514kb,D)

Title: Investigating Fouling Efficiency in Football Using Expected Booking (xB)
  Model
Authors: Adnan Azmat, Su Su Yi
Categories: cs.LG
\\
  This paper introduces the Expected Booking (xB) model, a novel metric
designed to estimate the likelihood of a foul resulting in a yellow card in
football. Through three iterative experiments, employing ensemble methods, the
model demonstrates improved performance with additional features and an
expanded dataset. Analysis of FIFA World Cup 2022 data validates the model's
efficacy in providing insights into team and player fouling tactics, aligning
with actual defensive performance. The xB model addresses a gap in fouling
efficiency examination, emphasizing defensive strategies which often
overlooked. Further enhancements are suggested through the incorporation of
comprehensive data and spatial features.
\\ ( https://arxiv.org/abs/2401.08718 ,  514kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08727
Date: Tue, 16 Jan 2024 14:22:44 GMT   (427kb,D)

Title: MA2GCN: Multi Adjacency relationship Attention Graph Convolutional
  Networks for Traffic Prediction using Trajectory data
Authors: Zhengke Sun, Yuliang Ma
Categories: cs.LG cs.AI
\\
  The problem of traffic congestion not only causes a large amount of economic
losses, but also seriously endangers the urban environment. Predicting traffic
congestion has important practical significance. So far, most studies have been
based on historical data from sensors placed on different roads to predict
future traffic flow and speed, to analyze the traffic congestion conditions of
a certain road segment. However, due to the fixed position of sensors, it is
difficult to mine new information. On the other hand, vehicle trajectory data
is more flexible and can extract traffic information as needed. Therefore, we
proposed a new traffic congestion prediction model - Multi Adjacency
relationship Attention Graph Convolutional Networks(MA2GCN). This model
transformed vehicle trajectory data into graph structured data in grid form,
and proposed a vehicle entry and exit matrix based on the mobility between
different grids. At the same time, in order to improve the performance of the
model, this paper also built a new adaptive adjacency matrix generation method
and adjacency matrix attention module. This model mainly used gated temporal
convolution and graph convolution to extract temporal and spatial information,
respectively. Compared with multiple baselines, our model achieved the best
performance on Shanghai taxi GPS trajectory dataset. The code is available at
https://github.com/zachysun/Taxi Traffic Benchmark.
\\ ( https://arxiv.org/abs/2401.08727 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08732
Date: Tue, 16 Jan 2024 16:01:37 GMT   (33364kb,D)

Title: Bayes Conditional Distribution Estimation for Knowledge Distillation
  Based on Conditional Mutual Information
Authors: Linfeng Ye, Shayan Mohajer Hamidi, Renhao Tan, En-Hui Yang
Categories: cs.LG cs.CV cs.IT math.IT
Comments: 32 pages, 19 figures, Published as a conference paper at ICLR 2024
MSC-class: 68T30
ACM-class: I.2.6
\\
  It is believed that in knowledge distillation (KD), the role of the teacher
is to provide an estimate for the unknown Bayes conditional probability
distribution (BCPD) to be used in the student training process. Conventionally,
this estimate is obtained by training the teacher using maximum log-likelihood
(MLL) method. To improve this estimate for KD, in this paper we introduce the
concept of conditional mutual information (CMI) into the estimation of BCPD and
propose a novel estimator called the maximum CMI (MCMI) method. Specifically,
in MCMI estimation, both the log-likelihood and CMI of the teacher are
simultaneously maximized when the teacher is trained. Through Eigen-CAM, it is
further shown that maximizing the teacher's CMI value allows the teacher to
capture more contextual information in an image cluster. Via conducting a
thorough set of experiments, we show that by employing a teacher trained via
MCMI estimation rather than one trained via MLL estimation in various
state-of-the-art KD frameworks, the student's classification accuracy
consistently increases, with the gain of up to 3.32\%. This suggests that the
teacher's BCPD estimate provided by MCMI method is more accurate than that
provided by MLL method. In addition, we show that such improvements in the
student's accuracy are more drastic in zero-shot and few-shot settings.
Notably, the student's accuracy increases with the gain of up to 5.72\% when
5\% of the training samples are available to the student (few-shot), and
increases from 0\% to as high as 84\% for an omitted class (zero-shot). The
code is available at \url{https://github.com/iclr2024mcmi/ICLRMCMI}.
\\ ( https://arxiv.org/abs/2401.08732 ,  33364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08788
Date: Tue, 16 Jan 2024 19:16:22 GMT   (8093kb,D)

Title: The Impact of Differential Feature Under-reporting on Algorithmic
  Fairness
Authors: Nil-Jana Akpinar, Zachary C. Lipton, Alexandra Chouldechova
Categories: cs.LG cs.CY stat.ML
\\
  Predictive risk models in the public sector are commonly developed using
administrative data that is more complete for subpopulations that more greatly
rely on public services. In the United States, for instance, information on
health care utilization is routinely available to government agencies for
individuals supported by Medicaid and Medicare, but not for the privately
insured. Critiques of public sector algorithms have identified such
differential feature under-reporting as a driver of disparities in algorithmic
decision-making. Yet this form of data bias remains understudied from a
technical viewpoint. While prior work has examined the fairness impacts of
additive feature noise and features that are clearly marked as missing, the
setting of data missingness absent indicators (i.e. differential feature
under-reporting) has been lacking in research attention. In this work, we
present an analytically tractable model of differential feature under-reporting
which we then use to characterize the impact of this kind of data bias on
algorithmic fairness. We demonstrate how standard missing data methods
typically fail to mitigate bias in this setting, and propose a new set of
methods specifically tailored to differential feature under-reporting. Our
results show that, in real world data settings, under-reporting typically leads
to increasing disparities. The proposed solution methods show success in
mitigating increases in unfairness.
\\ ( https://arxiv.org/abs/2401.08788 ,  8093kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08808
Date: Tue, 16 Jan 2024 20:20:10 GMT   (2955kb,D)

Title: Sample Relationship from Learning Dynamics Matters for Generalisation
Authors: Shangmin Guo, Yi Ren, Stefano V.Albrecht, Kenny Smith
Categories: cs.LG
Comments: ICLR-2024
\\
  Although much research has been done on proposing new models or loss
functions to improve the generalisation of artificial neural networks (ANNs),
less attention has been directed to the impact of the training data on
generalisation. In this work, we start from approximating the interaction
between samples, i.e. how learning one sample would modify the model's
prediction on other samples. Through analysing the terms involved in weight
updates in supervised learning, we find that labels influence the interaction
between samples. Therefore, we propose the labelled pseudo Neural Tangent
Kernel (lpNTK) which takes label information into consideration when measuring
the interactions between samples. We first prove that lpNTK asymptotically
converges to the empirical neural tangent kernel in terms of the Frobenius norm
under certain assumptions. Secondly, we illustrate how lpNTK helps to
understand learning phenomena identified in previous work, specifically the
learning difficulty of samples and forgetting events during learning. Moreover,
we also show that using lpNTK to identify and remove poisoning training samples
does not hurt the generalisation performance of ANNs.
\\ ( https://arxiv.org/abs/2401.08808 ,  2955kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08819
Date: Tue, 16 Jan 2024 20:42:15 GMT   (2387kb,D)

Title: Learning from Sparse Offline Datasets via Conservative Density
  Estimation
Authors: Zhepeng Cen, Zuxin Liu, Zitong Wang, Yihang Yao, Henry Lam, Ding Zhao
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\
  Offline reinforcement learning (RL) offers a promising direction for learning
policies from pre-collected datasets without requiring further interactions
with the environment. However, existing methods struggle to handle
out-of-distribution (OOD) extrapolation errors, especially in sparse reward or
scarce data settings. In this paper, we propose a novel training algorithm
called Conservative Density Estimation (CDE), which addresses this challenge by
explicitly imposing constraints on the state-action occupancy stationary
distribution. CDE overcomes the limitations of existing approaches, such as the
stationary distribution correction method, by addressing the support mismatch
issue in marginal importance sampling. Our method achieves state-of-the-art
performance on the D4RL benchmark. Notably, CDE consistently outperforms
baselines in challenging tasks with sparse rewards or insufficient data,
demonstrating the advantages of our approach in addressing the extrapolation
error problem in offline RL.
\\ ( https://arxiv.org/abs/2401.08819 ,  2387kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08825
Date: Tue, 16 Jan 2024 20:57:36 GMT   (14859kb,D)

Title: AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant
  Reviews and Images on Social Media
Authors: Alessandro Gambetti, Qiwei Han
Categories: cs.LG cs.CL cs.CV
\\
  Online reviews in the form of user-generated content (UGC) significantly
impact consumer decision-making. However, the pervasive issue of not only human
fake content but also machine-generated content challenges UGC's reliability.
Recent advances in Large Language Models (LLMs) may pave the way to fabricate
indistinguishable fake generated content at a much lower cost. Leveraging
OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a
multi-modal dataset of 20,144 restaurant review-image pairs divided into
authentic and machine-generated. We explore unimodal and multimodal detection
models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from
readability and photographic theories to score reviews and images,
respectively, demonstrating their utility as hand-crafted features in scalable
and interpretable detection models, with comparable performance. The paper
contributes by open-sourcing the dataset and releasing fake review detectors,
recommending its use in unimodal and multimodal fake review detection tasks,
and evaluating linguistic and visual features in synthetic versus authentic
data.
\\ ( https://arxiv.org/abs/2401.08825 ,  14859kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08830
Date: Tue, 16 Jan 2024 21:07:04 GMT   (5627kb,D)

Title: Stochastic Subnetwork Annealing: A Regularization Technique for Fine
  Tuning Pruned Subnetworks
Authors: Tim Whitaker, Darrell Whitley
Categories: cs.LG
Comments: 9 pages, 2 figures; Rejected at ICLR-2024; Revised and updated with
  new experiments; Submitted to WCCI-2024
\\
  Pruning methods have recently grown in popularity as an effective way to
reduce the size and computational complexity of deep neural networks. Large
numbers of parameters can be removed from trained models with little
discernible loss in accuracy after a small number of continued training epochs.
However, pruning too many parameters at once often causes an initial steep drop
in accuracy which can undermine convergence quality. Iterative pruning
approaches mitigate this by gradually removing a small number of parameters
over multiple epochs. However, this can still lead to subnetworks that overfit
local regions of the loss landscape. We introduce a novel and effective
approach to tuning subnetworks through a regularization technique we call
Stochastic Subnetwork Annealing. Instead of removing parameters in a discrete
manner, we instead represent subnetworks with stochastic masks where each
parameter has a probabilistic chance of being included or excluded on any given
forward pass. We anneal these probabilities over time such that subnetwork
structure slowly evolves as mask values become more deterministic, allowing for
a smoother and more robust optimization of subnetworks at high levels of
sparsity.
\\ ( https://arxiv.org/abs/2401.08830 ,  5627kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08850
Date: Tue, 16 Jan 2024 21:47:23 GMT   (2102kb,D)

Title: REValueD: Regularised Ensemble Value-Decomposition for Factorisable
  Markov Decision Processes
Authors: David Ireland and Giovanni Montana
Categories: cs.LG cs.AI
Comments: To appear in ICLR 2024
\\
  Discrete-action reinforcement learning algorithms often falter in tasks with
high-dimensional discrete action spaces due to the vast number of possible
actions. A recent advancement leverages value-decomposition, a concept from
multi-agent reinforcement learning, to tackle this challenge. This study delves
deep into the effects of this value-decomposition, revealing that whilst it
curtails the over-estimation bias inherent to Q-learning algorithms, it
amplifies target variance. To counteract this, we present an ensemble of
critics to mitigate target variance. Moreover, we introduce a regularisation
loss that helps to mitigate the effects that exploratory actions in one
dimension can have on the value of optimal actions in other dimensions. Our
novel algorithm, REValueD, tested on discretised versions of the DeepMind
Control Suite tasks, showcases superior performance, especially in the
challenging humanoid and dog tasks. We further dissect the factors influencing
REValueD's performance, evaluating the significance of the regularisation loss
and the scalability of REValueD with increasing sub-actions per dimension.
\\ ( https://arxiv.org/abs/2401.08850 ,  2102kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08851
Date: Tue, 16 Jan 2024 21:56:27 GMT   (596kb)

Title: Using i-vectors for subject-independent cross-session EEG transfer
  learning
Authors: Jonathan Lasko, Jeff Ma, Mike Nicoletti, Jonathan Sussman-Fort,
  Sooyoung Jeong, William Hartmann
Categories: cs.LG cs.CL cs.SD eess.AS q-bio.NC
Comments: 11 pages
\\
  Cognitive load classification is the task of automatically determining an
individual's utilization of working memory resources during performance of a
task based on physiologic measures such as electroencephalography (EEG). In
this paper, we follow a cross-disciplinary approach, where tools and
methodologies from speech processing are used to tackle this problem. The
corpus we use was released publicly in 2021 as part of the first passive
brain-computer interface competition on cross-session workload estimation. We
present our approach which used i-vector-based neural network classifiers to
accomplish inter-subject cross-session EEG transfer learning, achieving 18%
relative improvement over equivalent subject-dependent models. We also report
experiments showing how our subject-independent models perform competitively on
held-out subjects and improve with additional subject data, suggesting that
subject-dependent training is not required for effective cognitive load
determination.
\\ ( https://arxiv.org/abs/2401.08851 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08863
Date: Tue, 16 Jan 2024 22:35:14 GMT   (3653kb,D)

Title: Robust Localization of Key Fob Using Channel Impulse Response of Ultra
  Wide Band Sensors for Keyless Entry Systems
Authors: Abhiram Kolli, Filippo Casamassima, Horst Possegger, Horst Bischof
Categories: cs.LG cs.AI cs.CR
\\
  Using neural networks for localization of key fob within and surrounding a
car as a security feature for keyless entry is fast emerging. In this paper we
study: 1) the performance of pre-computed features of neural networks based UWB
(ultra wide band) localization classification forming the baseline of our
experiments. 2) Investigate the inherent robustness of various neural networks;
therefore, we include the study of robustness of the adversarial examples
without any adversarial training in this work. 3) Propose a multi-head
self-supervised neural network architecture which outperforms the baseline
neural networks without any adversarial training. The model's performance
improved by 67% at certain ranges of adversarial magnitude for fast gradient
sign method and 37% each for basic iterative method and projected gradient
descent method.
\\ ( https://arxiv.org/abs/2401.08863 ,  3653kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08867
Date: Tue, 16 Jan 2024 22:44:12 GMT   (429kb,D)

Title: MambaTab: A Simple Yet Effective Approach for Handling Tabular Data
Authors: Md Atik Ahamed and Qiang Cheng
Categories: cs.LG
\\
  Tabular data remains ubiquitous across domains despite growing use of images
and texts for machine learning. While deep learning models like convolutional
neural networks and transformers achieve strong performance on tabular data,
they require extensive data preprocessing, tuning, and resources, limiting
accessibility and scalability. This work develops an innovative approach based
on a structured state-space model (SSM), MambaTab, for tabular data. SSMs have
strong capabilities for efficiently extracting effective representations from
data with long-range dependencies. MambaTab leverages Mamba, an emerging SSM
variant, for end-to-end supervised learning on tables. Compared to
state-of-the-art baselines, MambaTab delivers superior performance while
requiring significantly fewer parameters and minimal preprocessing, as
empirically validated on diverse benchmark datasets. MambaTab's efficiency,
scalability, generalizability, and predictive gains signify it as a
lightweight, "out-of-the-box" solution for diverse tabular data with promise
for enabling wider practical applications.
\\ ( https://arxiv.org/abs/2401.08867 ,  429kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08875
Date: Tue, 16 Jan 2024 23:16:18 GMT   (646kb)

Title: DCRMTA: Unbiased Causal Representation for Multi-touch Attribution
Authors: Jiaming Tang
Categories: cs.LG cs.AI stat.ME
Comments: 9 pages, 7 figures
\\
  Multi-touch attribution (MTA) currently plays a pivotal role in achieving a
fair estimation of the contributions of each advertising touchpoint to-wards
conversion behavior, deeply influencing budget allocation and advertising
recommenda-tion. Traditional multi-touch attribution methods initially build a
conversion prediction model, an-ticipating learning the inherent relationship
be-tween touchpoint sequences and user purchasing behavior through historical
data. Based on this, counterfactual touchpoint sequences are con-structed from
the original sequence subset, and conversions are estimated using the
prediction model, thus calculating advertising contributions. A covert
assumption of these methods is the un-biased nature of conversion prediction
models. However, due to confounding variables factors arising from user
preferences and internet recom-mendation mechanisms such as homogenization of
ad recommendations resulting from past shop-ping records, bias can easily occur
in conversion prediction models trained on observational data. This paper
redefines the causal effect of user fea-tures on conversions and proposes a
novel end-to-end approach, Deep Causal Representation for MTA (DCRMTA). Our
model while eliminating confounding variables, extracts features with causal
relations to conversions from users. Fur-thermore, Extensive experiments on
both synthet-ic and real-world Criteo data demonstrate DCRMTA's superior
performance in converting prediction across varying data distributions, while
also effectively attributing value across dif-ferent advertising channels
\\ ( https://arxiv.org/abs/2401.08875 ,  646kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08886
Date: Tue, 16 Jan 2024 23:45:14 GMT   (2244kb,D)

Title: RiemannONets: Interpretable Neural Operators for Riemann Problems
Authors: Ahmad Peyvan, Vivek Oommen, Ameya D. Jagtap, George Em Karniadakis
Categories: cs.LG physics.flu-dyn
\\
  Developing the proper representations for simulating high-speed flows with
strong shock waves, rarefactions, and contact discontinuities has been a
long-standing question in numerical analysis. Herein, we employ neural
operators to solve Riemann problems encountered in compressible flows for
extreme pressure jumps (up to $10^{10}$ pressure ratio). In particular, we
first consider the DeepONet that we train in a two-stage process, following the
recent work of Lee and Shin, wherein the first stage, a basis is extracted from
the trunk net, which is orthonormalized and subsequently is used in the second
stage in training the branch net. This simple modification of DeepONet has a
profound effect on its accuracy, efficiency, and robustness and leads to very
accurate solutions to Riemann problems compared to the vanilla version. It also
enables us to interpret the results physically as the hierarchical data-driven
produced basis reflects all the flow features that would otherwise be
introduced using ad hoc feature expansion layers. We also compare the results
with another neural operator based on the U-Net for low, intermediate, and very
high-pressure ratios that are very accurate for Riemann problems, especially
for large pressure ratios, due to their multiscale nature but computationally
more expensive. Overall, our study demonstrates that simple neural network
architectures, if properly pre-trained, can achieve very accurate solutions of
Riemann problems for real-time forecasting.
\\ ( https://arxiv.org/abs/2401.08886 ,  2244kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08893
Date: Wed, 17 Jan 2024 00:16:46 GMT   (4515kb,D)

Title: MADA: Meta-Adaptive Optimizers through hyper-gradient Descent
Authors: Kaan Ozkara, Can Karakus, Parameswaran Raman, Mingyi Hong, Shoham
  Sabach, Branislav Kveton, Volkan Cevher
Categories: cs.LG math.OC
\\
  Since Adam was introduced, several novel adaptive optimizers for deep
learning have been proposed. These optimizers typically excel in some tasks but
may not outperform Adam uniformly across all tasks. In this work, we introduce
Meta-Adaptive Optimizers (MADA), a unified optimizer framework that can
generalize several known optimizers and dynamically learn the most suitable one
during training. The key idea in MADA is to parameterize the space of
optimizers and search through it using hyper-gradient descent. Numerical
results suggest that MADA is robust against sub-optimally tuned
hyper-parameters, and outperforms Adam, Lion, and Adan with their default
hyper-parameters, often even with optimized hyper-parameters. We also propose
AVGrad, a variant of AMSGrad where the maximum operator is replaced with
averaging, and observe that it performs better within MADA. Finally, we provide
a convergence analysis to show that interpolation of optimizers (specifically,
AVGrad and Adam) can improve their error bounds (up to constants), hinting at
an advantage for meta-optimizers.
\\ ( https://arxiv.org/abs/2401.08893 ,  4515kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08895
Date: Wed, 17 Jan 2024 00:36:58 GMT   (1034kb,D)

Title: cedar: Composable and Optimized Machine Learning Input Data Pipelines
Authors: Mark Zhao, Emanuel Adamiak, Christos Kozyrakis
Categories: cs.LG cs.DC cs.PF
\\
  The input data pipeline is an essential component of each machine learning
(ML) training job. It is responsible for reading massive amounts of training
data, processing batches of samples using complex of transformations, and
loading them onto training nodes at low latency and high throughput. Performant
input data systems are becoming increasingly critical, driven by skyrocketing
data volumes and training throughput demands. Unfortunately, current input data
systems cannot fully leverage key performance optimizations, resulting in
hugely inefficient infrastructures that require significant resources -- or
worse -- underutilize expensive accelerators.
  To address these demands, we present cedar, a programming model and framework
that allows users to easily build, optimize, and execute input data pipelines.
cedar presents an easy-to-use programming interface, allowing users to define
input data pipelines using composable operators that support arbitrary ML
frameworks and libraries. Meanwhile, cedar transparently applies a complex and
extensible set of optimization techniques (e.g., offloading, caching,
prefetching, fusion, and reordering). It then orchestrates processing across a
customizable set of local and distributed compute resources in order to
maximize processing performance and efficiency, all without user input. On
average across six diverse input data pipelines, cedar achieves a 2.49x, 1.87x,
2.18x, and 2.74x higher performance compared to tf.data, tf.data service, Ray
Data, and PyTorch's DataLoader, respectively.
\\ ( https://arxiv.org/abs/2401.08895 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08897
Date: Wed, 17 Jan 2024 00:46:24 GMT   (22239kb,D)

Title: CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in
  Variational AutoEncoder
Authors: Hee-Jun Jung, Jaehyoung Jeong and Kangil Kim
Categories: cs.LG cs.AI
Comments: 21 pages, 14 figures
\\
  Symmetries of input and latent vectors have provided valuable insights for
disentanglement learning in VAEs.However, only a few works were proposed as an
unsupervised method, and even these works require known factor information in
training data. We propose a novel method, Composite Factor-Aligned Symmetry
Learning (CFASL), which is integrated into VAEs for learning symmetry-based
disentanglement in unsupervised learning without any knowledge of the dataset
factor information.CFASL incorporates three novel features for learning
symmetry-based disentanglement: 1) Injecting inductive bias to align latent
vector dimensions to factor-aligned symmetries within an explicit learnable
symmetry codebook 2) Learning a composite symmetry to express unknown factors
change between two random samples by learning factor-aligned symmetries within
the codebook 3) Inducing group equivariant encoder and decoder in training VAEs
with the two conditions. In addition, we propose an extended evaluation metric
for multi-factor changes in comparison to disentanglement evaluation in VAEs.
In quantitative and in-depth qualitative analysis, CFASL demonstrates a
significant improvement of disentanglement in single-factor change, and
multi-factor change conditions compared to state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.08897 ,  22239kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08898
Date: Wed, 17 Jan 2024 00:47:43 GMT   (16302kb,D)

Title: Bridging State and History Representations: Understanding
  Self-Predictive RL
Authors: Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement
  Gehring, Aditya Mahajan, Pierre-Luc Bacon
Categories: cs.LG cs.AI
Comments: ICLR 2024 (Poster). Code is available at
  https://github.com/twni2016/self-predictive-rl
\\
  Representations are at the core of all deep reinforcement learning (RL)
methods for both Markov decision processes (MDPs) and partially observable
Markov decision processes (POMDPs). Many representation learning methods and
theoretical frameworks have been developed to understand what constitutes an
effective representation. However, the relationships between these methods and
the shared properties among them remain unclear. In this paper, we show that
many of these seemingly distinct methods and frameworks for state and history
abstractions are, in fact, based on a common idea of self-predictive
abstraction. Furthermore, we provide theoretical insights into the widely
adopted objectives and optimization, such as the stop-gradient technique, in
learning self-predictive representations. These findings together yield a
minimalist algorithm to learn self-predictive representations for states and
histories. We validate our theories by applying our algorithm to standard MDPs,
MDPs with distractors, and POMDPs with sparse rewards. These findings culminate
in a set of practical guidelines for RL practitioners.
\\ ( https://arxiv.org/abs/2401.08898 ,  16302kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08909
Date: Wed, 17 Jan 2024 01:33:23 GMT   (166kb,D)

Title: Characterising Gradients for Unsupervised Accuracy Estimation under
  Distribution Shift
Authors: Renchunzi Xie, Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko,
  Jianfeng Zhang, Bo An
Categories: cs.LG
\\
  Estimating test accuracy without access to the ground-truth test labels under
varying test environments is a challenging, yet extremely important problem in
the safe deployment of machine learning algorithms. Existing works rely on the
information from either the outputs or the extracted features of neural
networks to formulate an estimation score correlating with the ground-truth
test accuracy. In this paper, we investigate--both empirically and
theoretically--how the information provided by the gradients can be predictive
of the ground-truth test accuracy even under a distribution shift.
Specifically, we use the norm of classification-layer gradients, backpropagated
from the cross-entropy loss after only one gradient step over test data. Our
key idea is that the model should be adjusted with a higher magnitude of
gradients when it does not generalize to the test dataset with a distribution
shift. We provide theoretical insights highlighting the main ingredients of
such an approach ensuring its empirical success. Extensive experiments
conducted on diverse distribution shifts and model structures demonstrate that
our method significantly outperforms state-of-the-art algorithms.
\\ ( https://arxiv.org/abs/2401.08909 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08940
Date: Wed, 17 Jan 2024 03:26:04 GMT   (1604kb)

Title: CEL: A Continual Learning Model for Disease Outbreak Prediction by
  Leveraging Domain Adaptation via Elastic Weight Consolidation
Authors: Saba Aslam, Abdur Rasool, Hongyan Wu, Xiaoli Li
Categories: cs.LG cs.AI
\\
  Continual learning, the ability of a model to learn over time without
forgetting previous knowledge and, therefore, be adaptive to new data, is
paramount in dynamic fields such as disease outbreak prediction. Deep neural
networks, i.e., LSTM, are prone to error due to catastrophic forgetting. This
study introduces a novel CEL model for continual learning by leveraging domain
adaptation via Elastic Weight Consolidation (EWC). This model aims to mitigate
the catastrophic forgetting phenomenon in a domain incremental setting. The
Fisher Information Matrix (FIM) is constructed with EWC to develop a
regularization term that penalizes changes to important parameters, namely, the
important previous knowledge. CEL's performance is evaluated on three distinct
diseases, Influenza, Mpox, and Measles, with different metrics. The high
R-squared values during evaluation and reevaluation outperform the other
state-of-the-art models in several contexts, indicating that CEL adapts to
incremental data well. CEL's robustness and reliability are underscored by its
minimal 65% forgetting rate and 18% higher memory stability compared to
existing benchmark studies. This study highlights CEL's versatility in disease
outbreak prediction, addressing evolving data with temporal patterns. It offers
a valuable model for proactive disease control with accurate, timely
predictions.
\\ ( https://arxiv.org/abs/2401.08940 ,  1604kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08959
Date: Wed, 17 Jan 2024 04:19:33 GMT   (3192kb,D)

Title: Towards Off-Policy Reinforcement Learning for Ranking Policies with
  Human Feedback
Authors: Teng Xiao, Suhang Wang
Categories: cs.LG cs.AI
DOI: 10.1609/aaai.v36i8.20849
\\
  Probabilistic learning to rank (LTR) has been the dominating approach for
optimizing the ranking metric, but cannot maximize long-term rewards.
Reinforcement learning models have been proposed to maximize user long-term
rewards by formulating the recommendation as a sequential decision-making
problem, but could only achieve inferior accuracy compared to LTR counterparts,
primarily due to the lack of online interactions and the characteristics of
ranking. In this paper, we propose a new off-policy value ranking (VR)
algorithm that can simultaneously maximize user long-term rewards and optimize
the ranking metric offline for improved sample efficiency in a unified
Expectation-Maximization (EM) framework. We theoretically and empirically show
that the EM process guides the leaned policy to enjoy the benefit of
integration of the future reward and ranking metric, and learn without any
online interactions. Extensive offline and online experiments demonstrate the
effectiveness of our methods.
\\ ( https://arxiv.org/abs/2401.08959 ,  3192kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08961
Date: Wed, 17 Jan 2024 04:20:26 GMT   (2327kb,D)

Title: Cascading Reinforcement Learning
Authors: Yihan Du, R. Srikant, Wei Chen
Categories: cs.LG
\\
  Cascading bandits have gained popularity in recent years due to their
applicability to recommendation systems and online advertising. In the
cascading bandit model, at each timestep, an agent recommends an ordered subset
of items (called an item list) from a pool of items, each associated with an
unknown attraction probability. Then, the user examines the list, and clicks
the first attractive item (if any), and after that, the agent receives a
reward. The goal of the agent is to maximize the expected cumulative reward.
However, the prior literature on cascading bandits ignores the influences of
user states (e.g., historical behaviors) on recommendations and the change of
states as the session proceeds. Motivated by this fact, we propose a
generalized cascading RL framework, which considers the impact of user states
and state transition into decisions. In cascading RL, we need to select items
not only with large attraction probabilities but also leading to good successor
states. This imposes a huge computational challenge due to the combinatorial
action space. To tackle this challenge, we delve into the properties of value
functions, and design an oracle BestPerm to efficiently find the optimal item
list. Equipped with BestPerm, we develop two algorithms CascadingVI and
CascadingBPI, which are both computationally-efficient and sample-efficient,
and provide near-optimal regret and sample complexity guarantees. Furthermore,
we present experiments to show the improved computational and sample
efficiencies of our algorithms compared to straightforward adaptations of
existing RL algorithms in practice.
\\ ( https://arxiv.org/abs/2401.08961 ,  2327kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08976
Date: Wed, 17 Jan 2024 05:03:53 GMT   (804kb)

Title: ACT-GAN: Radio map construction based on generative adversarial networks
  with ACT blocks
Authors: Chen Qi, Yang Jingjing, Huang Ming, Zhou Qiang
Categories: cs.LG eess.SP
Comments: 11 pages, 10 figures
\\
  The radio map, serving as a visual representation of electromagnetic spatial
characteristics, plays a pivotal role in assessment of wireless communication
networks and radio monitoring coverage. Addressing the issue of low accuracy
existing in the current radio map construction, this paper presents a novel
radio map construction method based on generative adversarial network (GAN) in
which the Aggregated Contextual-Transformation (AOT) block, Convolutional Block
Attention Module (CBAM), and Transposed Convolution (T-Conv) block are applied
to the generator, and we name it as ACT-GAN. It significantly improves the
reconstruction accuracy and local texture of the radio maps. The performance of
ACT-GAN across three different scenarios is demonstrated. Experiment results
reveal that in the scenario without sparse discrete observations, the proposed
method reduces the root mean square error (RMSE) by 14.6% in comparison to the
state-of-the-art models. In the scenario with sparse discrete observations, the
RMSE is diminished by 13.2%. Furthermore, the predictive results of the
proposed model show a more lucid representation of electromagnetic spatial
field distribution. To verify the universality of this model in radio map
construction tasks, the scenario of unknown radio emission source is
investigated. The results indicate that the proposed model is robust radio map
construction and accurate in predicting the location of the emission source.
\\ ( https://arxiv.org/abs/2401.08976 ,  804kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08977
Date: Wed, 17 Jan 2024 05:04:33 GMT   (2574kb,D)

Title: FedLoGe: Joint Local and Generic Federated Learning under Long-tailed
  Data
Authors: Zikai Xiao, Zihan Chen, Liyinglan Liu, Yang Feng, Jian Wu, Wanlu Liu,
  Joey Tianyi Zhou, Howard Hao Yang, Zuozhu Liu
Categories: cs.LG cs.AI
Comments: Accepted by ICLR 2024
ACM-class: I.2.0
\\
  Federated Long-Tailed Learning (Fed-LT), a paradigm wherein data collected
from decentralized local clients manifests a globally prevalent long-tailed
distribution, has garnered considerable attention in recent times. In the
context of Fed-LT, existing works have predominantly centered on addressing the
data imbalance issue to enhance the efficacy of the generic global model while
neglecting the performance at the local level. In contrast, conventional
Personalized Federated Learning (pFL) techniques are primarily devised to
optimize personalized local models under the presumption of a balanced global
data distribution. This paper introduces an approach termed Federated Local and
Generic Model Training in Fed-LT (FedLoGe), which enhances both local and
generic model performance through the integration of representation learning
and classifier alignment within a neural collapse framework. Our investigation
reveals the feasibility of employing a shared backbone as a foundational
framework for capturing overarching global trends, while concurrently employing
individualized classifiers to encapsulate distinct refinements stemming from
each client's local features. Building upon this discovery, we establish the
Static Sparse Equiangular Tight Frame Classifier (SSE-C), inspired by neural
collapse principles that naturally prune extraneous noisy features and foster
the acquisition of potent data representations. Furthermore, leveraging
insights from imbalance neural collapse's classifier norm patterns, we develop
Global and Local Adaptive Feature Realignment (GLA-FR) via an auxiliary global
classifier and personalized Euclidean norm transfer to align global features
with client preferences. Extensive experimental results on CIFAR-10/100-LT,
ImageNet, and iNaturalist demonstrate the advantage of our method over
state-of-the-art pFL and Fed-LT approaches.
\\ ( https://arxiv.org/abs/2401.08977 ,  2574kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08984
Date: Wed, 17 Jan 2024 05:31:08 GMT   (1100kb,D)

Title: A GAN-based data poisoning framework against anomaly detection in
  vertical federated learning
Authors: Xiaolin Chen, Daoguang Zan, Wei Li, Bei Guan, Yongji Wang
Categories: cs.LG cs.AI cs.CR
Comments: 6 pages, 7 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\
  In vertical federated learning (VFL), commercial entities collaboratively
train a model while preserving data privacy. However, a malicious participant's
poisoning attack may degrade the performance of this collaborative model. The
main challenge in achieving the poisoning attack is the absence of access to
the server-side top model, leaving the malicious participant without a clear
target model. To address this challenge, we introduce an innovative end-to-end
poisoning framework P-GAN. Specifically, the malicious participant initially
employs semi-supervised learning to train a surrogate target model.
Subsequently, this participant employs a GAN-based method to produce
adversarial perturbations to degrade the surrogate target model's performance.
Finally, the generator is obtained and tailored for VFL poisoning. Besides, we
develop an anomaly detection algorithm based on a deep auto-encoder (DAE),
offering a robust defense mechanism to VFL scenarios. Through extensive
experiments, we evaluate the efficacy of P-GAN and DAE, and further analyze the
factors that influence their performance.
\\ ( https://arxiv.org/abs/2401.08984 ,  1100kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08986
Date: Wed, 17 Jan 2024 05:39:03 GMT   (6284kb,D)

Title: Rigid Protein-Protein Docking via Equivariant Elliptic-Paraboloid
  Interface Prediction
Authors: Ziyang Yu, Wenbing Huang, Yang Liu
Categories: cs.LG q-bio.BM
Comments: ICLR 2024
\\
  The study of rigid protein-protein docking plays an essential role in a
variety of tasks such as drug design and protein engineering. Recently, several
learning-based methods have been proposed for the task, exhibiting much faster
docking speed than those computational methods. In this paper, we propose a
novel learning-based method called ElliDock, which predicts an elliptic
paraboloid to represent the protein-protein docking interface. To be specific,
our model estimates elliptic paraboloid interfaces for the two input proteins
respectively, and obtains the roto-translation transformation for docking by
making two interfaces coincide. By its design, ElliDock is independently
equivariant with respect to arbitrary rotations/translations of the proteins,
which is an indispensable property to ensure the generalization of the docking
process. Experimental evaluations show that ElliDock achieves the fastest
inference time among all compared methods and is strongly competitive with
current state-of-the-art learning-based models such as DiffDock-PP and Multimer
particularly for antibody-antigen docking.
\\ ( https://arxiv.org/abs/2401.08986 ,  6284kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08996
Date: Wed, 17 Jan 2024 06:17:42 GMT   (4389kb,D)

Title: MicroNAS: Zero-Shot Neural Architecture Search for MCUs
Authors: Ye Qiao, Haocheng Xu, Yifan Zhang, Sitao Huang
Categories: cs.LG cs.AI
\\
  Neural Architecture Search (NAS) effectively discovers new Convolutional
Neural Network (CNN) architectures, particularly for accuracy optimization.
However, prior approaches often require resource-intensive training on super
networks or extensive architecture evaluations, limiting practical
applications. To address these challenges, we propose MicroNAS, a
hardware-aware zero-shot NAS framework designed for microcontroller units
(MCUs) in edge computing. MicroNAS considers target hardware optimality during
the search, utilizing specialized performance indicators to identify optimal
neural architectures without high computational costs. Compared to previous
works, MicroNAS achieves up to 1104x improvement in search efficiency and
discovers models with over 3.23x faster MCU inference while maintaining similar
accuracy
\\ ( https://arxiv.org/abs/2401.08996 ,  4389kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08998
Date: Wed, 17 Jan 2024 06:22:47 GMT   (34876kb,D)

Title: Attack and Reset for Unlearning: Exploiting Adversarial Noise toward
  Machine Unlearning through Parameter Re-initialization
Authors: Yoonhwa Jung and Ikhyun Cho and Shun-Hsiang Hsu and Julia Hockenmaier
Categories: cs.LG cs.CR cs.CV
\\
  With growing concerns surrounding privacy and regulatory compliance, the
concept of machine unlearning has gained prominence, aiming to selectively
forget or erase specific learned information from a trained model. In response
to this critical need, we introduce a novel approach called Attack-and-Reset
for Unlearning (ARU). This algorithm leverages meticulously crafted adversarial
noise to generate a parameter mask, effectively resetting certain parameters
and rendering them unlearnable. ARU outperforms current state-of-the-art
results on two facial machine-unlearning benchmark datasets, MUFAC and MUCAC.
In particular, we present the steps involved in attacking and masking that
strategically filter and re-initialize network parameters biased towards the
forget set. Our work represents a significant advancement in rendering data
unexploitable to deep learning models through parameter re-initialization,
achieved by harnessing adversarial noise to craft a mask.
\\ ( https://arxiv.org/abs/2401.08998 ,  34876kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09011
Date: Wed, 17 Jan 2024 07:14:04 GMT   (436kb)

Title: Inductive Models for Artificial Intelligence Systems are Insufficient
  without Good Explanations
Authors: Udesh Habaraduwa
Categories: cs.LG cs.AI
\\
  This paper discusses the limitations of machine learning (ML), particularly
deep artificial neural networks (ANNs), which are effective at approximating
complex functions but often lack transparency and explanatory power. It
highlights the `problem of induction' : the philosophical issue that past
observations may not necessarily predict future events, a challenge that ML
models face when encountering new, unseen data. The paper argues for the
importance of not just making predictions but also providing good explanations,
a feature that current models often fail to deliver. It suggests that for AI to
progress, we must seek models that offer insights and explanations, not just
predictions.
\\ ( https://arxiv.org/abs/2401.09011 ,  436kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09018
Date: Wed, 17 Jan 2024 07:30:14 GMT   (33252kb,D)

Title: Residual Alignment: Uncovering the Mechanisms of Residual Networks
Authors: Jianing Li, Vardan Papyan
Categories: cs.LG
Comments: Accepted at NeurIPS 2023 as a Poster paper
\\
  The ResNet architecture has been widely adopted in deep learning due to its
significant boost to performance through the use of simple skip connections,
yet the underlying mechanisms leading to its success remain largely unknown. In
this paper, we conduct a thorough empirical study of the ResNet architecture in
classification tasks by linearizing its constituent residual blocks using
Residual Jacobians and measuring their singular value decompositions. Our
measurements reveal a process called Residual Alignment (RA) characterized by
four properties:
  (RA1) intermediate representations of a given input are equispaced on a line,
embedded in high dimensional space, as observed by Gai and Zhang [2021];
  (RA2) top left and right singular vectors of Residual Jacobians align with
each other and across different depths;
  (RA3) Residual Jacobians are at most rank C for fully-connected ResNets,
where C is the number of classes; and
  (RA4) top singular values of Residual Jacobians scale inversely with depth.
  RA consistently occurs in models that generalize well, in both
fully-connected and convolutional architectures, across various depths and
widths, for varying numbers of classes, on all tested benchmark datasets, but
ceases to occur once the skip connections are removed. It also provably occurs
in a novel mathematical model we propose. This phenomenon reveals a strong
alignment between residual branches of a ResNet (RA2+4), imparting a highly
rigid geometric structure to the intermediate representations as they progress
linearly through the network (RA1) up to the final layer, where they undergo
Neural Collapse.
\\ ( https://arxiv.org/abs/2401.09018 ,  33252kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09031
Date: Wed, 17 Jan 2024 07:58:18 GMT   (11216kb,D)

Title: Data Attribution for Diffusion Models: Timestep-induced Bias in
  Influence Estimation
Authors: Tong Xie, Haoyu Li, Andrew Bai, Cho-Jui Hsieh
Categories: cs.LG
\\
  Data attribution methods trace model behavior back to its training dataset,
offering an effective approach to better understand ``black-box'' neural
networks. While prior research has established quantifiable links between model
output and training data in diverse settings, interpreting diffusion model
outputs in relation to training samples remains underexplored. In particular,
diffusion models operate over a sequence of timesteps instead of instantaneous
input-output relationships in previous contexts, posing a significant challenge
to extend existing frameworks to diffusion models directly. Notably, we present
Diffusion-TracIn that incorporates this temporal dynamics and observe that
samples' loss gradient norms are highly dependent on timestep. This trend leads
to a prominent bias in influence estimation, and is particularly noticeable for
samples trained on large-norm-inducing timesteps, causing them to be generally
influential. To mitigate this effect, we introduce Diffusion-ReTrac as a
re-normalized adaptation that enables the retrieval of training samples more
targeted to the test sample of interest, facilitating a localized measurement
of influence and considerably more intuitive visualization. We demonstrate the
efficacy of our approach through various evaluation metrics and auxiliary
tasks, reducing the amount of generally influential samples to $\frac{1}{3}$ of
its original quantity.
\\ ( https://arxiv.org/abs/2401.09031 ,  11216kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09067
Date: Wed, 17 Jan 2024 09:01:29 GMT   (247kb,D)

Title: Towards Continual Learning Desiderata via HSIC-Bottleneck
  Orthogonalization and Equiangular Embedding
Authors: Depeng Li, Tianqi Wang, Junwei Chen, Qining Ren, Kenji Kawaguchi,
  Zhigang Zeng
Categories: cs.LG cs.AI cs.CV
Comments: Accepted to AAAI 2024
\\
  Deep neural networks are susceptible to catastrophic forgetting when trained
on sequential tasks. Various continual learning (CL) methods often rely on
exemplar buffers or/and network expansion for balancing model stability and
plasticity, which, however, compromises their practical value due to privacy
and memory concerns. Instead, this paper considers a strict yet realistic
setting, where the training data from previous tasks is unavailable and the
model size remains relatively constant during sequential training. To achieve
such desiderata, we propose a conceptually simple yet effective method that
attributes forgetting to layer-wise parameter overwriting and the resulting
decision boundary distortion. This is achieved by the synergy between two key
components: HSIC-Bottleneck Orthogonalization (HBO) implements non-overwritten
parameter updates mediated by Hilbert-Schmidt independence criterion in an
orthogonal space and EquiAngular Embedding (EAE) enhances decision boundary
adaptation between old and new tasks with predefined basis vectors. Extensive
experiments demonstrate that our method achieves competitive accuracy
performance, even with absolute superiority of zero exemplar buffer and 1.02x
the base model.
\\ ( https://arxiv.org/abs/2401.09067 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09068
Date: Wed, 17 Jan 2024 09:01:50 GMT   (761kb,D)

Title: DTMM: Deploying TinyML Models on Extremely Weak IoT Devices with Pruning
Authors: Lixiang Han, Zhen Xiao, Zhenjiang Li
Categories: cs.LG cs.AI
\\
  DTMM is a library designed for efficient deployment and execution of machine
learning models on weak IoT devices such as microcontroller units (MCUs). The
motivation for designing DTMM comes from the emerging field of tiny machine
learning (TinyML), which explores extending the reach of machine learning to
many low-end IoT devices to achieve ubiquitous intelligence. Due to the weak
capability of embedded devices, it is necessary to compress models by pruning
enough weights before deploying. Although pruning has been studied extensively
on many computing platforms, two key issues with pruning methods are
exacerbated on MCUs: models need to be deeply compressed without significantly
compromising accuracy, and they should perform efficiently after pruning.
Current solutions only achieve one of these objectives, but not both. In this
paper, we find that pruned models have great potential for efficient deployment
and execution on MCUs. Therefore, we propose DTMM with pruning unit selection,
pre-execution pruning optimizations, runtime acceleration, and post-execution
low-cost storage to fill the gap for efficient deployment and execution of
pruned models. It can be integrated into commercial ML frameworks for practical
deployment, and a prototype system has been developed. Extensive experiments on
various models show promising gains compared to state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.09068 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09071
Date: Wed, 17 Jan 2024 09:12:31 GMT   (2753kb,D)

Title: Rethinking Spectral Graph Neural Networks with Spatially Adaptive
  Filtering
Authors: Jingwei Guo, Kaizhu Huang, Xinping Yi, Zixian Su, and Rui Zhang
Categories: cs.LG cs.AI
\\
  Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded
in the spectral domain, their practical reliance on polynomial approximation
implies a profound linkage to the spatial domain. As previous studies rarely
examine spectral GNNs from the spatial perspective, their spatial-domain
interpretability remains elusive, e.g., what information is essentially encoded
by spectral GNNs in the spatial domain? In this paper, to answer this question,
we establish a theoretical connection between spectral filtering and spatial
aggregation, unveiling an intrinsic interaction that spectral filtering
implicitly leads the original graph to an adapted new graph, explicitly
computed for spatial aggregation. Both theoretical and empirical investigations
reveal that the adapted new graph not only exhibits non-locality but also
accommodates signed edge weights to reflect label consistency between nodes.
These findings thus highlight the interpretable role of spectral GNNs in the
spatial domain and inspire us to rethink graph spectral filters beyond the
fixed-order polynomials, which neglect global information. Built upon the
theoretical findings, we revisit the state-of-the-art spectral GNNs and propose
a novel Spatially Adaptive Filtering (SAF) framework, which leverages the
adapted new graph by spectral filtering for an auxiliary non-local aggregation.
Notably, our proposed SAF comprehensively models both node similarity and
dissimilarity from a global perspective, therefore alleviating persistent
deficiencies of GNNs related to long-range dependencies and graph heterophily.
Extensive experiments over 13 node classification benchmarks demonstrate the
superiority of our proposed framework to the state-of-the-art models.
\\ ( https://arxiv.org/abs/2401.09071 ,  2753kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09073
Date: Wed, 17 Jan 2024 09:23:25 GMT   (2179kb,D)

Title: Fixed-Budget Differentially Private Best Arm Identification
Authors: Zhirui Chen, P. N. Karthik, Yeow Meng Chee, and Vincent Y. F. Tan
Categories: cs.LG cs.AI cs.IT math.IT math.ST stat.ML stat.TH
Comments: Accepted to ICLR 2024
\\
  We study best arm identification (BAI) in linear bandits in the fixed-budget
regime under differential privacy constraints, when the arm rewards are
supported on the unit interval. Given a finite budget $T$ and a privacy
parameter $\varepsilon>0$, the goal is to minimise the error probability in
finding the arm with the largest mean after $T$ sampling rounds, subject to the
constraint that the policy of the decision maker satisfies a certain {\em
$\varepsilon$-differential privacy} ($\varepsilon$-DP) constraint. We construct
a policy satisfying the $\varepsilon$-DP constraint (called {\sc DP-BAI}) by
proposing the principle of {\em maximum absolute determinants}, and derive an
upper bound on its error probability. Furthermore, we derive a minimax lower
bound on the error probability, and demonstrate that the lower and the upper
bounds decay exponentially in $T$, with exponents in the two bounds matching
order-wise in (a) the sub-optimality gaps of the arms, (b) $\varepsilon$, and
(c) the problem complexity that is expressible as the sum of two terms, one
characterising the complexity of standard fixed-budget BAI (without privacy
constraints), and the other accounting for the $\varepsilon$-DP constraint.
Additionally, we present some auxiliary results that contribute to the
derivation of the lower bound on the error probability. These results, we
posit, may be of independent interest and could prove instrumental in proving
lower bounds on error probabilities in several other bandit problems. Whereas
prior works provide results for BAI in the fixed-budget regime without privacy
constraints or in the fixed-confidence regime with privacy constraints, our
work fills the gap in the literature by providing the results for BAI in the
fixed-budget regime under the $\varepsilon$-DP constraint.
\\ ( https://arxiv.org/abs/2401.09073 ,  2179kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09074
Date: Wed, 17 Jan 2024 09:23:59 GMT   (8355kb,D)

Title: Code Simulation Challenges for Large Language Models
Authors: Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin,
  Anthony Cohn, Nigel Shadbolt, Michael Wooldridge
Categories: cs.LG cs.AI cs.CL cs.PL
Comments: main paper (10 pages) + Appendix (11 pages)
\\
  We investigate the extent to which Large Language Models (LLMs) can simulate
the execution of computer code and algorithms. We begin by looking straight
line programs, and show that current LLMs demonstrate poor performance even
with such simple programs -- performance rapidly degrades with the length of
code. We then investigate the ability of LLMs to simulate programs that contain
critical paths and redundant instructions. We also go beyond straight line
program simulation with sorting algorithms and nested loops, and we show the
computational complexity of a routine directly affects the ability of an LLM to
simulate its execution. We observe that LLMs execute instructions sequentially
and with a low error margin only for short programs or standard procedures.
LLMs' code simulation is in tension with their pattern recognition and
memorisation capabilities: on tasks where memorisation is detrimental, we
propose a novel prompting method to simulate code execution line by line.
Empirically, our new Chain of Simulation (CoSm) method improves on the standard
Chain of Thought prompting approach by avoiding the pitfalls of memorisation.
\\ ( https://arxiv.org/abs/2401.09074 ,  8355kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09093
Date: Wed, 17 Jan 2024 09:56:10 GMT   (382kb,D)

Title: RWKV-TS: Beyond Traditional Recurrent Neural Network for Time Series
  Tasks
Authors: Haowen Hou and F. Richard Yu
Categories: cs.LG
Comments: 13 pages. 2 figures, 14 tables
\\
  Traditional Recurrent Neural Network (RNN) architectures, such as LSTM and
GRU, have historically held prominence in time series tasks. However, they have
recently seen a decline in their dominant position across various time series
tasks. As a result, recent advancements in time series forecasting have seen a
notable shift away from RNNs towards alternative architectures such as
Transformers, MLPs, and CNNs. To go beyond the limitations of traditional RNNs,
we design an efficient RNN-based model for time series tasks, named RWKV-TS,
with three distinctive features: (i) A novel RNN architecture characterized by
$O(L)$ time complexity and memory usage. (ii) An enhanced ability to capture
long-term sequence information compared to traditional RNNs. (iii) High
computational efficiency coupled with the capacity to scale up effectively.
Through extensive experimentation, our proposed RWKV-TS model demonstrates
competitive performance when compared to state-of-the-art Transformer-based or
CNN-based models. Notably, RWKV-TS exhibits not only comparable performance but
also demonstrates reduced latency and memory utilization. The success of
RWKV-TS encourages further exploration and innovation in leveraging RNN-based
approaches within the domain of Time Series. The combination of competitive
performance, low latency, and efficient memory usage positions RWKV-TS as a
promising avenue for future research in time series tasks. Code is available
at:\href{https://github.com/howard-hou/RWKV-TS}{
https://github.com/howard-hou/RWKV-TS}
\\ ( https://arxiv.org/abs/2401.09093 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09125
Date: Wed, 17 Jan 2024 11:01:28 GMT   (504kb,D)

Title: Understanding Heterophily for Graph Neural Networks
Authors: Junfu Wang, Yuanfang Guo, Liang Yang, Yunhong Wang
Categories: cs.LG stat.ML
\\
  Graphs with heterophily have been regarded as challenging scenarios for Graph
Neural Networks (GNNs), where nodes are connected with dissimilar neighbors
through various patterns. In this paper, we present theoretical understandings
of the impacts of different heterophily patterns for GNNs by incorporating the
graph convolution (GC) operations into fully connected networks via the
proposed Heterophilous Stochastic Block Models (HSBM), a general random graph
model that can accommodate diverse heterophily patterns. Firstly, we show that
by applying a GC operation, the separability gains are determined by two
factors, i.e., the Euclidean distance of the neighborhood distributions and
$\sqrt{\mathbb{E}\left[\operatorname{deg}\right]}$, where
$\mathbb{E}\left[\operatorname{deg}\right]$ is the averaged node degree. It
reveals that the impact of heterophily on classification needs to be evaluated
alongside the averaged node degree. Secondly, we show that the topological
noise has a detrimental impact on separability, which is equivalent to
degrading $\mathbb{E}\left[\operatorname{deg}\right]$. Finally, when applying
multiple GC operations, we show that the separability gains are determined by
the normalized distance of the $l$-powered neighborhood distributions. It
indicates that the nodes still possess separability as $l$ goes to infinity in
a wide range of regimes. Extensive experiments on both synthetic and real-world
data verify the effectiveness of our theory.
\\ ( https://arxiv.org/abs/2401.09125 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09135
Date: Wed, 17 Jan 2024 11:17:04 GMT   (6353kb,D)

Title: Asynchronous Local-SGD Training for Language Modeling
Authors: Bo Liu, Rachita Chhaparia, Arthur Douillard, Satyen Kale, Andrei A.
  Rusu, Jiajun Shen, Arthur Szlam, Marc'Aurelio Ranzato
Categories: cs.LG cs.CL
\\
  Local stochastic gradient descent (Local-SGD), also referred to as federated
averaging, is an approach to distributed optimization where each device
performs more than one SGD update per communication. This work presents an
empirical study of {\it asynchronous} Local-SGD for training language models;
that is, each worker updates the global parameters as soon as it has finished
its SGD steps. We conduct a comprehensive investigation by examining how worker
hardware heterogeneity, model size, number of workers, and optimizer could
impact the learning performance. We find that with naive implementations,
asynchronous Local-SGD takes more iterations to converge than its synchronous
counterpart despite updating the (global) model parameters more frequently. We
identify momentum acceleration on the global parameters when worker gradients
are stale as a key challenge. We propose a novel method that utilizes a delayed
Nesterov momentum update and adjusts the workers' local training steps based on
their computation speed. This approach, evaluated with models up to 150M
parameters on the C4 dataset, matches the performance of synchronous Local-SGD
in terms of perplexity per update step, and significantly surpasses it in terms
of wall clock time.
\\ ( https://arxiv.org/abs/2401.09135 ,  6353kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09176
Date: Wed, 17 Jan 2024 12:34:17 GMT   (1821kb)

Title: ADCNet: a unified framework for predicting the activity of antibody-drug
  conjugates
Authors: Liye Chen, Biaoshun Li, Yihao Chen, Mujie Lin, Shipeng Zhang, Chenxin
  Li, Yu Pang and Ling Wang
Categories: cs.LG
\\
  Antibody-drug conjugate (ADC) has revolutionized the field of cancer
treatment in the era of precision medicine due to their ability to precisely
target cancer cells and release highly effective drug. Nevertheless, the
realization of rational design of ADC is very difficult because the
relationship between their structures and activities is difficult to
understand. In the present study, we introduce a unified deep learning
framework called ADCNet to help design potential ADCs. The ADCNet highly
integrates the protein representation learning language model ESM-2 and
small-molecule representation learning language model FG-BERT models to achieve
activity prediction through learning meaningful features from antigen and
antibody protein sequences of ADC, SMILES strings of linker and payload, and
drug-antibody ratio (DAR) value. Based on a carefully designed and manually
tailored ADC data set, extensive evaluation results reveal that ADCNet performs
best on the test set compared to baseline machine learning models across all
evaluation metrics. For example, it achieves an average prediction accuracy of
87.12%, a balanced accuracy of 0.8689, and an area under receiver operating
characteristic curve of 0.9293 on the test set. In addition, cross-validation,
ablation experiments, and external independent testing results further prove
the stability, advancement, and robustness of the ADCNet architecture. For the
convenience of the community, we develop the first online platform
(https://ADCNet.idruglab.cn) for the prediction of ADCs activity based on the
optimal ADCNet model, and the source code is publicly available at
https://github.com/idrugLab/ADCNet.
\\ ( https://arxiv.org/abs/2401.09176 ,  1821kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09180
Date: Wed, 17 Jan 2024 12:43:28 GMT   (949kb,D)

Title: Unsupervised Multiple Domain Translation through Controlled
  Disentanglement in Variational Autoencoder
Authors: Almud\'evar Antonio and Mariotte Th\'eo and Ortega Alfonso and Tahon
  Marie
Categories: cs.LG cs.CV
\\
  Unsupervised Multiple Domain Translation is the task of transforming data
from one domain to other domains without having paired data to train the
systems. Typically, methods based on Generative Adversarial Networks (GANs) are
used to address this task. However, our proposal exclusively relies on a
modified version of a Variational Autoencoder. This modification consists of
the use of two latent variables disentangled in a controlled way by design. One
of this latent variables is imposed to depend exclusively on the domain, while
the other one must depend on the rest of the variability factors of the data.
Additionally, the conditions imposed over the domain latent variable allow for
better control and understanding of the latent space. We empirically
demonstrate that our approach works on different vision datasets improving the
performance of other well known methods. Finally, we prove that, indeed, one of
the latent variables stores all the information related to the domain and the
other one hardly contains any domain information.
\\ ( https://arxiv.org/abs/2401.09180 ,  949kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09181
Date: Wed, 17 Jan 2024 12:44:17 GMT   (4307kb,D)

Title: Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with
  Positive Forward Transfer
Authors: Junhao Zheng, Qianli Ma, Zhen Liu, Binquan Wu, Huawen Feng
Categories: cs.LG
\\
  Multimodal Continual Instruction Tuning (MCIT) enables Multimodal Large
Language Models (MLLMs) to meet continuously emerging requirements without
expensive retraining. MCIT faces two major obstacles: catastrophic forgetting
(where old knowledge is forgotten) and negative forward transfer (where the
performance of future tasks is degraded). Although existing methods have
greatly alleviated catastrophic forgetting, they still suffer from negative
forward transfer. By performing singular value decomposition (SVD) on input
embeddings, we discover a large discrepancy in different input embeddings. The
discrepancy results in the model learning irrelevant information for old and
pre-trained tasks, which leads to catastrophic forgetting and negative forward
transfer. To address these issues, we propose Fwd-Prompt, a prompt-based method
projecting prompt gradient to the residual space to minimize the interference
between tasks and to the pre-trained subspace for reusing pre-trained
knowledge. Our experiments demonstrate that Fwd-Prompt achieves
state-of-the-art performance while updating fewer parameters and requiring no
old samples. Our research sheds light on the potential of continuously adapting
MLLMs to new tasks under the instruction tuning paradigm and encourages future
studies to explore MCIT. The code will soon be publicly available.
\\ ( https://arxiv.org/abs/2401.09181 ,  4307kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09191
Date: Wed, 17 Jan 2024 13:03:47 GMT   (307kb,D)

Title: An Optimal Transport Approach for Computing Adversarial Training Lower
  Bounds in Multiclass Classification
Authors: Nicolas Garcia Trillos, Matt Jacobs, Jakwang Kim, Matthew Werenski
Categories: cs.LG math.OC stat.ML
\\
  Despite the success of deep learning-based algorithms, it is widely known
that neural networks may fail to be robust. A popular paradigm to enforce
robustness is adversarial training (AT), however, this introduces many
computational and theoretical difficulties. Recent works have developed a
connection between AT in the multiclass classification setting and
multimarginal optimal transport (MOT), unlocking a new set of tools to study
this problem. In this paper, we leverage the MOT connection to propose
computationally tractable numerical algorithms for computing universal lower
bounds on the optimal adversarial risk and identifying optimal classifiers. We
propose two main algorithms based on linear programming (LP) and entropic
regularization (Sinkhorn). Our key insight is that one can harmlessly truncate
the higher order interactions between classes, preventing the combinatorial run
times typically encountered in MOT problems. We validate these results with
experiments on MNIST and CIFAR-$10$, which demonstrate the tractability of our
approach.
\\ ( https://arxiv.org/abs/2401.09191 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09192
Date: Wed, 17 Jan 2024 13:04:14 GMT   (1088kb,D)

Title: Preparing Lessons for Progressive Training on Language Models
Authors: Yu Pan, Ye Yuan, Yichun Yin, Jiaxin Shi, Zenglin Xu, Ming Zhang,
  Lifeng Shang, Xin Jiang, Qun Liu
Categories: cs.LG cs.AI
\\
  The rapid progress of Transformers in artificial intelligence has come at the
cost of increased resource consumption and greenhouse gas emissions due to
growing model sizes. Prior work suggests using pretrained small models to
improve training efficiency, but this approach may not be suitable for new
model structures. On the other hand, training from scratch can be slow, and
progressively stacking layers often fails to achieve significant acceleration.
To address these challenges, we propose a novel method called Apollo, which
prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by
\textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of
low layers. Our approach involves low-value-prioritized sampling (LVPS) to
train different depths and weight sharing to facilitate efficient expansion. We
also introduce an interpolation method for stable model depth extension.
Experiments demonstrate that Apollo achieves state-of-the-art acceleration
ratios, even rivaling methods using pretrained models, making it a universal
and efficient solution for training deep models while reducing time, financial,
and environmental costs.
\\ ( https://arxiv.org/abs/2401.09192 ,  1088kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09193
Date: Wed, 17 Jan 2024 13:04:23 GMT   (409kb,D)

Title: GNN-LoFI: a Novel Graph Neural Network through Localized Feature-based
  Histogram Intersection
Authors: Alessandro Bicciato, Luca Cosmo, Giorgia Minello, Luca Rossi, Andrea
  Torsello
Categories: cs.LG
Journal-ref: Pattern Recognition (2024) 110210
DOI: 10.1016/j.patcog.2023.110210
\\
  Graph neural networks are increasingly becoming the framework of choice for
graph-based machine learning. In this paper, we propose a new graph neural
network architecture that substitutes classical message passing with an
analysis of the local distribution of node features. To this end, we extract
the distribution of features in the egonet for each local neighbourhood and
compare them against a set of learned label distributions by taking the
histogram intersection kernel. The similarity information is then propagated to
other nodes in the network, effectively creating a message passing-like
mechanism where the message is determined by the ensemble of the features. We
perform an ablation study to evaluate the network's performance under different
choices of its hyper-parameters. Finally, we test our model on standard graph
classification and regression benchmarks, and we find that it outperforms
widely used alternative approaches, including both graph kernels and graph
neural networks.
\\ ( https://arxiv.org/abs/2401.09193 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09198
Date: Wed, 17 Jan 2024 13:24:04 GMT   (23222kb,D)

Title: Space and Time Continuous Physics Simulation From Partial Observations
Authors: Janny Steeven, Nadri Madiha, Digne Julie, Wolf Christian
Categories: cs.LG
Comments: Project Page: https://continuous-pde.github.io/
\\
  Modern techniques for physical simulations rely on numerical schemes and
mesh-refinement methods to address trade-offs between precision and complexity,
but these handcrafted solutions are tedious and require high computational
power. Data-driven methods based on large-scale machine learning promise high
adaptivity by integrating long-range dependencies more directly and
efficiently. In this work, we focus on fluid dynamics and address the
shortcomings of a large part of the literature, which are based on fixed
support for computations and predictions in the form of regular or irregular
grids. We propose a novel setup to perform predictions in a continuous spatial
and temporal domain while being trained on sparse observations. We formulate
the task as a double observation problem and propose a solution with two
interlinked dynamical systems defined on, respectively, the sparse positions
and the continuous domain, which allows to forecast and interpolate a solution
from the initial condition. Our practical implementation involves recurrent
GNNs and a spatio-temporal attention observer capable of interpolating the
solution at arbitrary locations. Our model not only generalizes to new initial
conditions (as standard auto-regressive models do) but also performs evaluation
at arbitrary space and time locations. We evaluate on three standard datasets
in fluid dynamics and compare to strong baselines, which are outperformed both
in classical settings and in the extended new task requiring continuous
predictions.
\\ ( https://arxiv.org/abs/2401.09198 ,  23222kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09235
Date: Wed, 17 Jan 2024 14:30:46 GMT   (34kb)

Title: A Characterization Theorem for Equivariant Networks with Point-wise
  Activations
Authors: Marco Pacini, Xiaowen Dong, Bruno Lepri and Gabriele Santin
Categories: cs.LG cs.AI
Comments: Accepted at the 12th International Conference on Learning
  Representations (ICLR 2024)
\\
  Equivariant neural networks have shown improved performance, expressiveness
and sample complexity on symmetrical domains. But for some specific symmetries,
representations, and choice of coordinates, the most common point-wise
activations, such as ReLU, are not equivariant, hence they cannot be employed
in the design of equivariant neural networks. The theorem we present in this
paper describes all possible combinations of finite-dimensional
representations, choice of coordinates and point-wise activations to obtain an
exactly equivariant layer, generalizing and strengthening existing
characterizations. Notable cases of practical relevance are discussed as
corollaries. Indeed, we prove that rotation-equivariant networks can only be
invariant, as it happens for any network which is equivariant with respect to
connected compact groups. Then, we discuss implications of our findings when
applied to important instances of exactly equivariant networks. First, we
completely characterize permutation equivariant networks such as Invariant
Graph Networks with point-wise nonlinearities and their geometric counterparts,
highlighting a plethora of models whose expressive power and performance are
still unknown. Second, we show that feature spaces of disentangled steerable
convolutional neural networks are trivial representations.
\\ ( https://arxiv.org/abs/2401.09235 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09237
Date: Wed, 17 Jan 2024 14:34:32 GMT   (2022kb,D)

Title: Classification and Reconstruction Processes in Deep Predictive Coding
  Networks: Antagonists or Allies?
Authors: Jan Rathjens and Laurenz Wiskott
Categories: cs.LG
\\
  Predictive coding-inspired deep networks for visual computing integrate
classification and reconstruction processes in shared intermediate layers.
Although synergy between these processes is commonly assumed, it has yet to be
convincingly demonstrated. In this study, we take a critical look at how
classifying and reconstructing interact in deep learning architectures. Our
approach utilizes a purposefully designed family of model architectures
reminiscent of autoencoders, each equipped with an encoder, a decoder, and a
classification head featuring varying modules and complexities. We meticulously
analyze the extent to which classification- and reconstruction-driven
information can seamlessly coexist within the shared latent layer of the model
architectures. Our findings underscore a significant challenge:
Classification-driven information diminishes reconstruction-driven information
in intermediate layers' shared representations and vice versa. While expanding
the shared representation's dimensions or increasing the network's complexity
can alleviate this trade-off effect, our results challenge prevailing
assumptions in predictive coding and offer guidance for future iterations of
predictive coding concepts in deep networks.
\\ ( https://arxiv.org/abs/2401.09237 ,  2022kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09251
Date: Wed, 17 Jan 2024 14:56:42 GMT   (118kb)

Title: Bridging the Gap Between General and Down-Closed Convex Sets in
  Submodular Maximization
Authors: Loay Mualem, Murad Tukan, Moran Fledman
Categories: cs.LG math.OC
\\
  Optimization of DR-submodular functions has experienced a notable surge in
significance in recent times, marking a pivotal development within the domain
of non-convex optimization. Motivated by real-world scenarios, some recent
works have delved into the maximization of non-monotone DR-submodular functions
over general (not necessarily down-closed) convex set constraints. Up to this
point, these works have all used the minimum $\ell_\infty$ norm of any feasible
solution as a parameter. Unfortunately, a recent hardness result due to Mualem
\& Feldman~\cite{mualem2023resolving} shows that this approach cannot yield a
smooth interpolation between down-closed and non-down-closed constraints. In
this work, we suggest novel offline and online algorithms that provably provide
such an interpolation based on a natural decomposition of the convex body
constraint into two distinct convex bodies: a down-closed convex body and a
general convex body. We also empirically demonstrate the superiority of our
proposed algorithms across three offline and two online applications.
\\ ( https://arxiv.org/abs/2401.09251 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09257
Date: Wed, 17 Jan 2024 15:03:37 GMT   (88kb,D)

Title: A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level
  Optimization
Authors: Feiyang Ye, Baijiong Lin, Xiaofeng Cao, Yu Zhang, Ivor Tsang
Categories: cs.LG
Comments: Technical Report
\\
  In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO)
problem, where the upper-level subproblem is a multi-objective optimization
problem and the lower-level subproblem is for scalar optimization. Existing
gradient-based MOBLO algorithms need to compute the Hessian matrix, causing the
computational inefficient problem. To address this, we propose an efficient
first-order multi-gradient method for MOBLO, called FORUM. Specifically, we
reformulate MOBLO problems as a constrained multi-objective optimization (MOO)
problem via the value-function approach. Then we propose a novel multi-gradient
aggregation method to solve the challenging constrained MOO problem.
Theoretically, we provide the complexity analysis to show the efficiency of the
proposed method and a non-asymptotic convergence result. Empirically, extensive
experiments demonstrate the effectiveness and efficiency of the proposed FORUM
method in different learning problems. In particular, it achieves
state-of-the-art performance on three multi-task learning benchmark datasets.
\\ ( https://arxiv.org/abs/2401.09257 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09261
Date: Wed, 17 Jan 2024 15:12:11 GMT   (327kb,D)

Title: MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series
  Forecasting
Authors: Zongjiang Shang, Ling Chen
Categories: cs.LG
Comments: 9 pages, 5 figures
\\
  Demystifying interactions between temporal patterns of different scales is
fundamental to precise long-range time series forecasting. However, previous
works lack the ability to model high-order interactions. To promote more
comprehensive pattern interaction modeling for long-range time series
forecasting, we propose a Multi-Scale Hypergraph Transformer (MSHyper)
framework. Specifically, a multi-scale hypergraph is introduced to provide
foundations for modeling high-order pattern interactions. Then by treating
hyperedges as nodes, we also build a hyperedge graph to enhance hypergraph
modeling. In addition, a tri-stage message passing mechanism is introduced to
aggregate pattern information and learn the interaction strength between
temporal patterns of different scales. Extensive experiments on five real-world
datasets demonstrate that MSHyper achieves state-of-the-art performance,
reducing prediction errors by an average of 8.73% and 7.15% over the best
baseline in MSE and MAE, respectively.
\\ ( https://arxiv.org/abs/2401.09261 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09267
Date: Wed, 17 Jan 2024 15:15:52 GMT   (1076kb,D)

Title: Risk-Aware Accelerated Wireless Federated Learning with Heterogeneous
  Clients
Authors: Mohamed Ads, Hesham ElSawy and Hossam S. Hassanein
Categories: cs.LG
\\
  Wireless Federated Learning (FL) is an emerging distributed machine learning
paradigm, particularly gaining momentum in domains with confidential and
private data on mobile clients. However, the location-dependent performance, in
terms of transmission rates and susceptibility to transmission errors, poses
major challenges for wireless FL's convergence speed and accuracy. The
challenge is more acute for hostile environments without a metric that
authenticates the data quality and security profile of the clients. In this
context, this paper proposes a novel risk-aware accelerated FL framework that
accounts for the clients heterogeneity in the amount of possessed data,
transmission rates, transmission errors, and trustworthiness. Classifying
clients according to their location-dependent performance and trustworthiness
profiles, we propose a dynamic risk-aware global model aggregation scheme that
allows clients to participate in descending order of their transmission rates
and an ascending trustworthiness constraint. In particular, the transmission
rate is the dominant participation criterion for initial rounds to accelerate
the convergence speed. Our model then progressively relaxes the transmission
rate restriction to explore more training data at cell-edge clients. The
aggregation rounds incorporate a debiasing factor that accounts for
transmission errors. Risk-awareness is enabled by a validation set, where the
base station eliminates non-trustworthy clients at the fine-tuning stage. The
proposed scheme is benchmarked against a conservative scheme (i.e., only
allowing trustworthy devices) and an aggressive scheme (i.e., oblivious to the
trust metric). The numerical results highlight the superiority of the proposed
scheme in terms of accuracy and convergence speed when compared to both
benchmarks.
\\ ( https://arxiv.org/abs/2401.09267 ,  1076kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09278
Date: Wed, 17 Jan 2024 15:32:04 GMT   (6049kb,D)

Title: Adaptive Regret for Bandits Made Possible: Two Queries Suffice
Authors: Zhou Lu, Qiuyi Zhang, Xinyi Chen, Fred Zhang, David Woodruff, Elad
  Hazan
Categories: cs.LG
Comments: ICLR2024
\\
  Fast changing states or volatile environments pose a significant challenge to
online optimization, which needs to perform rapid adaptation under limited
observation. In this paper, we give query and regret optimal bandit algorithms
under the strict notion of strongly adaptive regret, which measures the maximum
regret over any contiguous interval $I$. Due to its worst-case nature, there is
an almost-linear $\Omega(|I|^{1-\epsilon})$ regret lower bound, when only one
query per round is allowed [Daniely el al, ICML 2015]. Surprisingly, with just
two queries per round, we give Strongly Adaptive Bandit Learner (StABL) that
achieves $\tilde{O}(\sqrt{n|I|})$ adaptive regret for multi-armed bandits with
$n$ arms. The bound is tight and cannot be improved in general. Our algorithm
leverages a multiplicative update scheme of varying stepsizes and a carefully
chosen observation distribution to control the variance. Furthermore, we extend
our results and provide optimal algorithms in the bandit convex optimization
setting. Finally, we empirically demonstrate the superior performance of our
algorithms under volatile environments and for downstream tasks, such as
algorithm selection for hyperparameter optimization.
\\ ( https://arxiv.org/abs/2401.09278 ,  6049kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09323
Date: Wed, 17 Jan 2024 16:47:39 GMT   (5476kb,D)

Title: BENO: Boundary-embedded Neural Operators for Elliptic PDEs
Authors: Haixin Wang, Jiaxin Li, Anubhav Dwivedi, Kentaro Hara, Tailin Wu
Categories: cs.LG
Comments: Accepted by ICLR 2024
\\
  Elliptic partial differential equations (PDEs) are a major class of
time-independent PDEs that play a key role in many scientific and engineering
domains such as fluid dynamics, plasma physics, and solid mechanics. Recently,
neural operators have emerged as a promising technique to solve elliptic PDEs
more efficiently by directly mapping the input to solutions. However, existing
networks typically cannot handle complex geometries and inhomogeneous boundary
values present in the real world. Here we introduce Boundary-Embedded Neural
Operators (BENO), a novel neural operator architecture that embeds the complex
geometries and inhomogeneous boundary values into the solving of elliptic PDEs.
Inspired by classical Green's function, BENO consists of two branches of Graph
Neural Networks (GNNs) for interior source term and boundary values,
respectively. Furthermore, a Transformer encoder maps the global boundary
geometry into a latent vector which influences each message passing layer of
the GNNs. We test our model extensively in elliptic PDEs with various boundary
conditions. We show that all existing baseline methods fail to learn the
solution operator. In contrast, our model, endowed with boundary-embedded
architecture, outperforms state-of-the-art neural operators and strong
baselines by an average of 60.96\%. Our source code can be found
https://github.com/AI4Science-WestlakeU/beno.git.
\\ ( https://arxiv.org/abs/2401.09323 ,  5476kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09376
Date: Wed, 17 Jan 2024 17:46:10 GMT   (1265kb,D)

Title: Unlocking Unlabeled Data: Ensemble Learning with the Hui- Walter
  Paradigm for Performance Estimation in Online and Static Settings
Authors: Kevin Slote, Elaine Lee
Categories: cs.LG math.ST stat.ML stat.TH
\\
  In the realm of machine learning and statistical modeling, practitioners
often work under the assumption of accessible, static, labeled data for
evaluation and training. However, this assumption often deviates from reality
where data may be private, encrypted, difficult- to-measure, or unlabeled. In
this paper, we bridge this gap by adapting the Hui-Walter paradigm, a method
traditionally applied in epidemiology and medicine, to the field of machine
learning. This approach enables us to estimate key performance metrics such as
false positive rate, false negative rate, and priors in scenarios where no
ground truth is available. We further extend this paradigm for handling online
data, opening up new possibilities for dynamic data environments. Our
methodology involves partitioning data into latent classes to simulate multiple
data populations (if natural populations are unavailable) and independently
training models to replicate multiple tests. By cross-tabulating binary
outcomes across ensemble categorizers and multiple populations, we are able to
estimate unknown parameters through Gibbs sampling, eliminating the need for
ground-truth or labeled data. This paper showcases the potential of our
methodology to transform machine learning practices by allowing for accurate
model assessment under dynamic and uncertain data conditions.
\\ ( https://arxiv.org/abs/2401.09376 ,  1265kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2401.08579 (*cross-listing*)
Date: Tue, 3 Oct 2023 20:49:56 GMT   (1797kb)

Title: Curve-based Neural Style Transfer
Authors: Yu-hsuan Chen, Levent Burak Kara, Jonathan Cagan
Categories: cs.CV cs.AI cs.GR
\\
  This research presents a new parametric style transfer framework specifically
designed for curve-based design sketches. In this research, traditional
challenges faced by neural style transfer methods in handling binary sketch
transformations are effectively addressed through the utilization of parametric
shape-editing rules, efficient curve-to-pixel conversion techniques, and the
fine-tuning of VGG19 on ImageNet-Sketch, enhancing its role as a feature
pyramid network for precise style extraction. By harmonizing intuitive
curve-based imagery with rule-based editing, this study holds the potential to
significantly enhance design articulation and elevate the practice of style
transfer within the realm of product design.
\\ ( https://arxiv.org/abs/2401.08579 ,  1797kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08581 (*cross-listing*)
Date: Mon, 16 Oct 2023 02:53:29 GMT   (32286kb,D)

Title: Temporal Embeddings: Scalable Self-Supervised Temporal Representation
  Learning from Spatiotemporal Data for Multimodal Computer Vision
Authors: Yi Cao and Swetava Ganguli and Vipul Pandey
Categories: cs.CV cs.AI cs.LG
Comments: Extended abstract accepted for presentation at BayLearn 2023. 3
  pages, 7 figures. Abstract based on IEEE IGARSS 2023 research track paper:
  arXiv:2304.13143
\\
  There exists a correlation between geospatial activity temporal patterns and
type of land use. A novel self-supervised approach is proposed to stratify
landscape based on mobility activity time series. First, the time series signal
is transformed to the frequency domain and then compressed into task-agnostic
temporal embeddings by a contractive autoencoder, which preserves cyclic
temporal patterns observed in time series. The pixel-wise embeddings are
converted to image-like channels that can be used for task-based, multimodal
modeling of downstream geospatial tasks using deep semantic segmentation.
Experiments show that temporal embeddings are semantically meaningful
representations of time series data and are effective across different tasks
such as classifying residential area and commercial areas. Temporal embeddings
transform sequential, spatiotemporal motion trajectory data into semantically
meaningful image-like tensor representations that can be combined (multimodal
fusion) with other data modalities that are or can be transformed into
image-like tensor representations (for e.g., RBG imagery, graph embeddings of
road networks, passively collected imagery like SAR, etc.) to facilitate
multimodal learning in geospatial computer vision. Multimodal computer vision
is critical for training machine learning models for geospatial feature
detection to keep a geospatial mapping service up-to-date in real-time and can
significantly improve user experience and above all, user safety.
\\ ( https://arxiv.org/abs/2401.08581 ,  32286kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08584 (*cross-listing*)
Date: Fri, 3 Nov 2023 12:35:07 GMT   (673kb)

Title: Nahid: AI-based Algorithm for operating fully-automatic surgery
Authors: Sina Saadati
Categories: cs.CV cs.AI cs.LG cs.NE cs.RO eess.IV
Comments: 8 pages, 10 figures, 1 table
\\
  In this paper, for the first time, a method is presented that can provide a
fully automated surgery based on software and computer vision techniques. Then,
the advantages and challenges of computerization of medical surgery are
examined. Finally, the surgery related to isolated ovarian endometriosis
disease has been examined, and based on the presented method, a more detailed
algorithm is presented that is capable of automatically diagnosing and treating
this disease during surgery as proof of our proposed method where a U-net is
trained to detect the endometriosis during surgery.
\\ ( https://arxiv.org/abs/2401.08584 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08585 (*cross-listing*)
Date: Mon, 6 Nov 2023 15:08:22 GMT   (7366kb,D)

Title: From Conceptual Spaces to Quantum Concepts: Formalising and Learning
  Structured Conceptual Models
Authors: Sean Tull, Razin A. Shaikh, Sara Sabrina Zemljic and Stephen Clark
Categories: q-bio.NC cs.AI quant-ph
Comments: This article consolidates our previous reports on concept
  formalisation and learning: arXiv:2302.14822 and arXiv:2203.11216
\\
  In this article we present a new modelling framework for structured concepts
using a category-theoretic generalisation of conceptual spaces, and show how
the conceptual representations can be learned automatically from data, using
two very different instantiations: one classical and one quantum. A
contribution of the work is a thorough category-theoretic formalisation of our
framework. We claim that the use of category theory, and in particular the use
of string diagrams to describe quantum processes, helps elucidate some of the
most important features of our approach. We build upon Gardenfors' classical
framework of conceptual spaces, in which cognition is modelled geometrically
through the use of convex spaces, which in turn factorise in terms of simpler
spaces called domains. We show how concepts from the domains of shape, colour,
size and position can be learned from images of simple shapes, where concepts
are represented as Gaussians in the classical implementation, and quantum
effects in the quantum one. In the classical case we develop a new model which
is inspired by the Beta-VAE model of concepts, but is designed to be more
closely connected with language, so that the names of concepts form part of the
graphical model. In the quantum case, concepts are learned by a hybrid
classical-quantum network trained to perform concept classification, where the
classical image processing is carried out by a convolutional neural network and
the quantum representations are produced by a parameterised quantum circuit.
Finally, we consider the question of whether our quantum models of concepts can
be considered conceptual spaces in the Gardenfors sense.
\\ ( https://arxiv.org/abs/2401.08585 ,  7366kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08587 (*cross-listing*)
Date: Fri, 10 Nov 2023 05:29:25 GMT   (381kb)

Title: Automatic extraction and 3D reconstruction of split wire from point
  cloud data based on improved DPC algorithm
Authors: Jia Cheng
Categories: cs.CV cs.AI
\\
  In order to solve the problem of point cloud data splitting improved by DPC
algorithm, a research on automatic separation and 3D reconstruction of point
cloud data split lines is proposed. First, the relative coordinates of each
point in the cloud point are calculated. Second, it is planned to develop a
relative ensemble-based DPC swarm algorithm for analyzing the number of
separation lines to determine all parts in the cloud content. Finally, fit each
separator using the least squares method. iron. The cloud point of the
resulting split subconductors has a clear demarcation line, and the distance
between adjacent split subconductors is 0.45 m, divided by the four vertices of
the square.
\\ ( https://arxiv.org/abs/2401.08587 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08604 (*cross-listing*)
Date: Wed, 22 Nov 2023 08:29:45 GMT   (6700kb,D)

Title: SAM4UDASS: When SAM Meets Unsupervised Domain Adaptive Semantic
  Segmentation in Intelligent Vehicles
Authors: Weihao Yan, Yeqiang Qian, Xingyuan Chen, Hanyang Zhuang, Chunxiang
  Wang, Ming Yang
Categories: cs.CV cs.AI
Comments: 10 pages,9 figures,9 tables
\\
  Semantic segmentation plays a critical role in enabling intelligent vehicles
to comprehend their surrounding environments. However, deep learning-based
methods usually perform poorly in domain shift scenarios due to the lack of
labeled data for training. Unsupervised domain adaptation (UDA) techniques have
emerged to bridge the gap across different driving scenes and enhance model
performance on unlabeled target environments. Although self-training UDA
methods have achieved state-of-the-art results, the challenge of generating
precise pseudo-labels persists. These pseudo-labels tend to favor majority
classes, consequently sacrificing the performance of rare classes or small
objects like traffic lights and signs. To address this challenge, we introduce
SAM4UDASS, a novel approach that incorporates the Segment Anything Model (SAM)
into self-training UDA methods for refining pseudo-labels. It involves
Semantic-Guided Mask Labeling, which assigns semantic labels to unlabeled SAM
masks using UDA pseudo-labels. Furthermore, we devise fusion strategies aimed
at mitigating semantic granularity inconsistency between SAM masks and the
target domain. SAM4UDASS innovatively integrate SAM with UDA for semantic
segmentation in driving scenes and seamlessly complements existing
self-training UDA methodologies. Extensive experiments on synthetic-to-real and
normal-to-adverse driving datasets demonstrate its effectiveness. It brings
more than 3% mIoU gains on GTA5-to-Cityscapes, SYNTHIA-to-Cityscapes, and
Cityscapes-to-ACDC when using DAFormer and achieves SOTA when using MIC. The
code will be available at https://github.com/ywher/SAM4UDASS.
\\ ( https://arxiv.org/abs/2401.08604 ,  6700kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08623 (*cross-listing*)
Date: Wed, 6 Dec 2023 18:15:08 GMT   (6582kb,D)

Title: Wake-Sleep Consolidated Learning
Authors: Amelia Sorrenti, Giovanni Bellitto, Federica Proietto Salanitri,
  Matteo Pennisi, Simone Palazzo, Concetto Spampinato
Categories: cs.NE cs.AI cs.CV cs.LG
\\
  We propose Wake-Sleep Consolidated Learning (WSCL), a learning strategy
leveraging Complementary Learning System theory and the wake-sleep phases of
the human brain to improve the performance of deep neural networks for visual
classification tasks in continual learning settings. Our method learns
continually via the synchronization between distinct wake and sleep phases.
During the wake phase, the model is exposed to sensory input and adapts its
representations, ensuring stability through a dynamic parameter freezing
mechanism and storing episodic memories in a short-term temporary memory
(similarly to what happens in the hippocampus). During the sleep phase, the
training process is split into NREM and REM stages. In the NREM stage, the
model's synaptic weights are consolidated using replayed samples from the
short-term and long-term memory and the synaptic plasticity mechanism is
activated, strengthening important connections and weakening unimportant ones.
In the REM stage, the model is exposed to previously-unseen realistic visual
sensory experience, and the dreaming process is activated, which enables the
model to explore the potential feature space, thus preparing synapses to future
knowledge. We evaluate the effectiveness of our approach on three benchmark
datasets: CIFAR-10, Tiny-ImageNet and FG-ImageNet. In all cases, our method
outperforms the baselines and prior work, yielding a significant performance
gain on continual visual classification tasks. Furthermore, we demonstrate the
usefulness of all processing stages and the importance of dreaming to enable
positive forward transfer.
\\ ( https://arxiv.org/abs/2401.08623 ,  6582kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08632 (*cross-listing*)
Date: Sun, 10 Dec 2023 19:53:15 GMT   (15250kb,D)

Title: Synergizing Quality-Diversity with Descriptor-Conditioned Reinforcement
  Learning
Authors: Maxence Faldor, F\'elix Chalumeau, Manon Flageat, Antoine Cully
Categories: cs.NE cs.AI cs.LG cs.RO
Comments: arXiv admin note: substantial text overlap with arXiv:2303.03832
\\
  A fundamental trait of intelligence involves finding novel and creative
solutions to address a given challenge or to adapt to unforeseen situations.
Reflecting this, Quality-Diversity optimization is a family of Evolutionary
Algorithms, that generates collections of both diverse and high-performing
solutions. Among these, MAP-Elites is a prominent example, that has been
successfully applied to a variety of domains, including evolutionary robotics.
However, MAP-Elites performs a divergent search with random mutations
originating from Genetic Algorithms, and thus, is limited to evolving
populations of low-dimensional solutions. PGA-MAP-Elites overcomes this
limitation using a gradient-based variation operator inspired by deep
reinforcement learning which enables the evolution of large neural networks.
Although high-performing in many environments, PGA-MAP-Elites fails on several
tasks where the convergent search of the gradient-based variation operator
hinders diversity. In this work, we present three contributions: (1) we enhance
the Policy Gradient variation operator with a descriptor-conditioned critic
that reconciles diversity search with gradient-based methods, (2) we leverage
the actor-critic training to learn a descriptor-conditioned policy at no
additional cost, distilling the knowledge of the population into one single
versatile policy that can execute a diversity of behaviors, (3) we exploit the
descriptor-conditioned actor by injecting it in the population, despite network
architecture differences. Our method, DCG-MAP-Elites, achieves equal or higher
QD score and coverage compared to all baselines on seven challenging continuous
control locomotion tasks.
\\ ( https://arxiv.org/abs/2401.08632 ,  15250kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08636 (*cross-listing*)
Date: Mon, 11 Dec 2023 19:06:06 GMT   (331kb,D)

Title: MLCommons Cloud Masking Benchmark with Early Stopping
Authors: Varshitha Chennamsetti and Gregor von Laszewski and Ruochen Gu and
  Laiba Mehnaz and Juri Papay and Samuel Jackson and Jeyan Thiyagalingam and
  Sergey V. Samsonau and Geoffrey C. Fox
Categories: cs.DC cs.AI
Comments: 11 pages, 9 figures, 3 tables. arXiv admin note: text overlap with
  arXiv:2312.04799
\\
  In this paper, we report on work performed for the MLCommons Science Working
Group on the cloud masking benchmark. MLCommons is a consortium that develops
and maintains several scientific benchmarks that aim to benefit developments in
AI. The benchmarks are conducted on the High Performance Computing (HPC)
Clusters of New York University and University of Virginia, as well as a
commodity desktop. We provide a description of the cloud masking benchmark, as
well as a summary of our submission to MLCommons on the benchmark experiment we
conducted. It includes a modification to the reference implementation of the
cloud masking benchmark enabling early stopping. This benchmark is executed on
the NYU HPC through a custom batch script that runs the various experiments
through the batch queuing system while allowing for variation on the number of
epochs trained. Our submission includes the modified code, a custom batch
script to modify epochs, documentation, and the benchmark results. We report
the highest accuracy (scientific metric) and the average time taken
(performance metric) for training and inference that was achieved on NYU HPC
Greene. We also provide a comparison of the compute capabilities between
different systems by running the benchmark for one epoch. Our submission can be
found in a Globus repository that is accessible to MLCommons Science Working
Group.
\\ ( https://arxiv.org/abs/2401.08636 ,  331kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08655 (*cross-listing*)
Date: Mon, 25 Dec 2023 04:40:32 GMT   (3878kb,D)

Title: SAiD: Speech-driven Blendshape Facial Animation with Diffusion
Authors: Inkyu Park, Jaewoong Cho
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
\\
  Speech-driven 3D facial animation is challenging due to the scarcity of
large-scale visual-audio datasets despite extensive research. Most prior works,
typically focused on learning regression models on a small dataset using the
method of least squares, encounter difficulties generating diverse lip
movements from speech and require substantial effort in refining the generated
outputs. To address these issues, we propose a speech-driven 3D facial
animation with a diffusion model (SAiD), a lightweight Transformer-based U-Net
with a cross-modality alignment bias between audio and visual to enhance lip
synchronization. Moreover, we introduce BlendVOCA, a benchmark dataset of pairs
of speech audio and parameters of a blendshape facial model, to address the
scarcity of public resources. Our experimental results demonstrate that the
proposed approach achieves comparable or superior performance in lip
synchronization to baselines, ensures more diverse lip movements, and
streamlines the animation editing process.
\\ ( https://arxiv.org/abs/2401.08655 ,  3878kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08658 (*cross-listing*)
Date: Tue, 26 Dec 2023 12:00:58 GMT   (3844kb,D)

Title: End-To-End Planning of Autonomous Driving in Industry and Academia:
  2022-2023
Authors: Gongjin Lan an Qi Hao
Categories: cs.RO cs.AI
Comments: 8 pages, 14 figures
\\
  This paper aims to provide a quick review of the methods including the
technologies in detail that are currently reported in industry and academia.
Specifically, this paper reviews the end-to-end planning, including Tesla FSD
V12, Momenta 2023, Horizon Robotics 2023, Motional RoboTaxi 2022, Woven Planet
(Toyota): Urban Driver, and Nvidia. In addition, we review the state-of-the-art
academic studies that investigate end-to-end planning of autonomous driving.
This paper provides readers with a concise structure and fast learning of
state-of-the-art end-to-end planning for 2022-2023. This article provides a
meaningful overview as introductory material for beginners to follow the
state-of-the-art end-to-end planning of autonomous driving in industry and
academia, as well as supplementary material for advanced researchers.
\\ ( https://arxiv.org/abs/2401.08658 ,  3844kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08683 (*cross-listing*)
Date: Fri, 12 Jan 2024 17:41:38 GMT   (766kb,D)

Title: Zero-Shot RTL Code Generation with Attention Sink Augmented Large
  Language Models
Authors: Selim Sandal, Ismail Akturk
Categories: cs.AR cs.AI cs.LG cs.PL cs.SE
\\
  The design and optimization of hardware have traditionally been
resource-intensive, demanding considerable expertise and dependence on
established design automation tools. This paper discusses the possibility of
exploiting large language models to streamline the code generation process in
hardware design. In contrast to earlier studies, this paper aims to use large
language models that accepts high-level design specifications through a single
prompt to generate corresponding Register-Transfer Level (RTL) code. The
ability to use large language models on RTL code generation not only expedites
design iteration cycles but also facilitates the exploration of design spaces
that have computational challenges for conventional techniques. Through our
evaluation, we demonstrate the shortcoming of existing attention mechanisms,
and present the abilities of language models to produce functional, optimized,
and industry-standard compliant RTL code when a novel attention mechanism is
used. These findings underscore the expanding role of large language models in
shaping the future landscape of architectural exploration and automation in
hardware design.
\\ ( https://arxiv.org/abs/2401.08683 ,  766kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08696 (*cross-listing*)
Date: Sun, 14 Jan 2024 07:24:08 GMT   (371kb,D)

Title: Hierarchical Source-to-Post-Route QoR Prediction in High-Level Synthesis
  with GNNs
Authors: Mingzhe Gao, Jieru Zhao, Zhe Lin, Minyi Guo
Categories: cs.AR cs.AI cs.LG
Comments: Accepted for publication at DATE 2024
\\
  High-level synthesis (HLS) notably speeds up the hardware design process by
avoiding RTL programming. However, the turnaround time of HLS increases
significantly when post-route quality of results (QoR) are considered during
optimization. To tackle this issue, we propose a hierarchical post-route QoR
prediction approach for FPGA HLS, which features: (1) a modeling flow that
directly estimates latency and post-route resource usage from C/C++ programs;
(2) a graph construction method that effectively represents the control and
data flow graph of source code and effects of HLS pragmas; and (3) a
hierarchical GNN training and prediction method capable of capturing the impact
of loop hierarchies. Experimental results show that our method presents a
prediction error of less than 10% for different types of QoR metrics, which
gains tremendous improvement compared with the state-of-the-art GNN methods. By
adopting our proposed methodology, the runtime for design space exploration in
HLS is shortened to tens of minutes and the achieved ADRS is reduced to 6.91%
on average.
\\ ( https://arxiv.org/abs/2401.08696 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08699 (*cross-listing*)
Date: Sun, 14 Jan 2024 12:38:49 GMT   (4227kb)

Title: On Image Search in Histopathology
Authors: H.R. Tizhoosh, Liron Pantanowitz
Categories: eess.IV cs.AI cs.CV cs.IR q-bio.QM
\\
  Pathology images of histopathology can be acquired from camera-mounted
microscopes or whole slide scanners. Utilizing similarity calculations to match
patients based on these images holds significant potential in research and
clinical contexts. Recent advancements in search technologies allow for nuanced
quantification of cellular structures across diverse tissue types, facilitating
comparisons and enabling inferences about diagnosis, prognosis, and predictions
for new patients when compared against a curated database of diagnosed and
treated cases. In this paper, we comprehensively review the latest developments
in image search technologies for histopathology, offering a concise overview
tailored for computational pathology researchers seeking effective, fast and
efficient image search methods in their work.
\\ ( https://arxiv.org/abs/2401.08699 ,  4227kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08711 (*cross-listing*)
Date: Mon, 15 Jan 2024 15:15:48 GMT   (409kb)

Title: Assistant, Parrot, or Colonizing Loudspeaker? ChatGPT Metaphors for
  Developing Critical AI Literacies
Authors: Anuj Gupta, Yasser Atef, Anna Mills, Maha Bali
Categories: cs.HC cs.AI cs.CY
Comments: This is a preprint (accepted version) of an article that has been
  accepted for publication at the journal Open Praxis: https://openpraxis.org/
ACM-class: I.2.0; K.3.0; K.3.1; K.4.0; K.4.2; J.4; J.5
\\
  This study explores how discussing metaphors for AI can help build awareness
of the frames that shape our understanding of AI systems, particularly large
language models (LLMs) like ChatGPT. Given the pressing need to teach "critical
AI literacy", discussion of metaphor provides an opportunity for inquiry and
dialogue with space for nuance, playfulness, and critique. Using a
collaborative autoethnographic methodology, we analyzed metaphors from a range
of sources, and reflected on them individually according to seven questions,
then met and discussed our interpretations. We then analyzed how our
reflections contributed to the three kinds of literacies delineated in Selber's
multiliteracies framework: functional, critical, and rhetorical. These allowed
us to analyze questions of ethics, equity, and accessibility in relation to AI.
We explored each metaphor along the dimension of whether or not it was
promoting anthropomorphizing, and to what extent such metaphors imply that AI
is sentient. Our findings highlight the role of metaphor reflection in
fostering a nuanced understanding of AI, suggesting that our collaborative
autoethnographic approach as well as the heuristic model of plotting AI
metaphors on dimensions of anthropomorphism and multiliteracies, might be
useful for educators and researchers in the pursuit of advancing critical AI
literacy.
\\ ( https://arxiv.org/abs/2401.08711 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08714 (*cross-listing*)
Date: Mon, 15 Jan 2024 20:40:46 GMT   (7265kb,D)

Title: Training program on sign language: social inclusion through Virtual
  Reality in ISENSE project
Authors: Alessia Bisio, Enrique Yeguas-Bol\'ivar, Pilar Aparicio-Mart\'inez,
  Mar\'ia Dolores Redel-Mac\'ias, Sara Pinzi, Stefano Rossi and Juri Taborri
Categories: cs.HC cs.AI cs.CV cs.GR
Comments: 6 pages, 4 figures, MetroXRAINE 2023 Conference, ISENSE european
  project
Journal-ref: 2023 IEEE International Conference on Metrology for Extended
  Reality, Artificial Intelligence and Neural Engineering (MetroXRAINE),
  Milano, Italy, 2023, pp. 104-109
\\
  Structured hand gestures that incorporate visual motions and signs are used
in sign language. Sign language is a valuable means of daily communication for
individuals who are deaf or have speech impairments, but it is still rare among
hearing people, and fewer are capable of understand it. Within the academic
context, parents and teachers play a crucial role in supporting deaf students
from childhood by facilitating their learning of sign language. In the last
years, among all the teaching tools useful for learning sign language, the use
of Virtual Reality (VR) has increased, as it has been demonstrated to improve
retention, memory and attention during the learning process. The ISENSE project
has been created to assist students with deafness during their academic life by
proposing different technological tools for teaching sign language to the
hearing community in the academic context. As part of the ISENSE project, this
work aims to develop an application for Spanish and Italian sign language
recognition that exploits the VR environment to quickly and easily create a
comprehensive database of signs and an Artificial Intelligence (AI)-based
software to accurately classify and recognize static and dynamic signs: from
letters to sentences.
\\ ( https://arxiv.org/abs/2401.08714 ,  7265kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08721 (*cross-listing*)
Date: Tue, 16 Jan 2024 08:35:36 GMT   (1113kb)

Title: A Telerehabilitation System for the Selection, Evaluation and Remote
  Management of Therapies
Authors: David Anton, Idoia Berges, Jes\'us Berm\'udez, Alfredo Go\~ni, Arantza
  Illarramendi
Categories: cs.HC cs.AI
Journal-ref: Sensors 18(5): 1459 (2018)
DOI: 10.3390/s18051459
\\
  Telerehabilitation systems that support physical therapy sessions anywhere
can help save healthcare costs while also improving the quality of life of the
users that need rehabilitation. The main contribution of this paper is to
present, as a whole, all the features supported by the innovative Kinect-based
Telerehabilitation System (KiReS). In addition to the functionalities provided
by current systems, it handles two new ones that could be incorporated into
them, in order to give a step forward towards a new generation of
telerehabilitation systems. The knowledge extraction functionality handles
knowledge about the physical therapy record of patients and treatment protocols
described in an ontology, named TRHONT, to select the adequate exercises for
the rehabilitation of patients. The teleimmersion functionality provides a
convenient, effective and user-friendly experience when performing the
telerehabilitation, through a two-way real-time multimedia communication. The
ontology contains about 2300 classes and 100 properties, and the system allows
a reliable transmission of Kinect video depth, audio and skeleton data, being
able to adapt to various network conditions. Moreover, the system has been
tested with patients who suffered from shoulder disorders or total hip
replacement.
\\ ( https://arxiv.org/abs/2401.08721 ,  1113kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08728 (*cross-listing*)
Date: Tue, 16 Jan 2024 15:32:41 GMT   (830kb,D)

Title: AgentMixer: Multi-Agent Correlated Policy Factorization
Authors: Zhiyuan Li, Wenshuai Zhao, Lijun Wu, Joni Pajarinen
Categories: cs.MA cs.AI
\\
  Centralized training with decentralized execution (CTDE) is widely employed
to stabilize partially observable multi-agent reinforcement learning (MARL) by
utilizing a centralized value function during training. However, existing
methods typically assume that agents make decisions based on their local
observations independently, which may not lead to a correlated joint policy
with sufficient coordination. Inspired by the concept of correlated
equilibrium, we propose to introduce a \textit{strategy modification} to
provide a mechanism for agents to correlate their policies. Specifically, we
present a novel framework, AgentMixer, which constructs the joint fully
observable policy as a non-linear combination of individual partially
observable policies. To enable decentralized execution, one can derive
individual policies by imitating the joint policy. Unfortunately, such
imitation learning can lead to \textit{asymmetric learning failure} caused by
the mismatch between joint policy and individual policy information. To
mitigate this issue, we jointly train the joint policy and individual policies
and introduce \textit{Individual-Global-Consistency} to guarantee mode
consistency between the centralized and decentralized policies. We then
theoretically prove that AgentMixer converges to an $\epsilon$-approximate
Correlated Equilibrium. The strong experimental performance on three MARL
benchmarks demonstrates the effectiveness of our method.
\\ ( https://arxiv.org/abs/2401.08728 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08739 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:55:22 GMT   (14611kb,D)

Title: EgoGen: An Egocentric Synthetic Data Generator
Authors: Gen Li, Kaifeng Zhao, Siwei Zhang, Xiaozhong Lyu, Mihai Dusmanu, Yan
  Zhang, Marc Pollefeys, Siyu Tang
Categories: cs.CV cs.AI
Comments: 22 pages, 16 figures. Project page: https://ego-gen.github.io/
\\
  Understanding the world in first-person view is fundamental in Augmented
Reality (AR). This immersive perspective brings dramatic visual changes and
unique challenges compared to third-person views. Synthetic data has empowered
third-person-view vision models, but its application to embodied egocentric
perception tasks remains largely unexplored. A critical challenge lies in
simulating natural human movements and behaviors that effectively steer the
embodied cameras to capture a faithful egocentric representation of the 3D
world. To address this challenge, we introduce EgoGen, a new synthetic data
generator that can produce accurate and rich ground-truth training data for
egocentric perception tasks. At the heart of EgoGen is a novel human motion
synthesis model that directly leverages egocentric visual inputs of a virtual
human to sense the 3D environment. Combined with collision-avoiding motion
primitives and a two-stage reinforcement learning approach, our motion
synthesis model offers a closed-loop solution where the embodied perception and
movement of the virtual human are seamlessly coupled. Compared to previous
works, our model eliminates the need for a pre-defined global path, and is
directly applicable to dynamic environments. Combined with our easy-to-use and
scalable data generation pipeline, we demonstrate EgoGen's efficacy in three
tasks: mapping and localization for head-mounted cameras, egocentric camera
tracking, and human mesh recovery from egocentric views. EgoGen will be fully
open-sourced, offering a practical solution for creating realistic egocentric
training data and aiming to serve as a useful tool for egocentric computer
vision research. Refer to our project page: https://ego-gen.github.io/.
\\ ( https://arxiv.org/abs/2401.08739 ,  14611kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08741 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:55:54 GMT   (23777kb,D)

Title: Fixed Point Diffusion Models
Authors: Xingjian Bai and Luke Melas-Kyriazi
Categories: cs.CV cs.AI cs.LG
Comments: Project page:
  https://lukemelas.github.io/fixed-point-diffusion-models
\\
  We introduce the Fixed Point Diffusion Model (FPDM), a novel approach to
image generation that integrates the concept of fixed point solving into the
framework of diffusion-based generative modeling. Our approach embeds an
implicit fixed point solving layer into the denoising network of a diffusion
model, transforming the diffusion process into a sequence of closely-related
fixed point problems. Combined with a new stochastic training method, this
approach significantly reduces model size, reduces memory usage, and
accelerates training. Moreover, it enables the development of two new
techniques to improve sampling efficiency: reallocating computation across
timesteps and reusing fixed point solutions between timesteps. We conduct
extensive experiments with state-of-the-art models on ImageNet, FFHQ,
CelebA-HQ, and LSUN-Church, demonstrating substantial improvements in
performance and efficiency. Compared to the state-of-the-art DiT model, FPDM
contains 87% fewer parameters, consumes 60% less memory during training, and
improves image generation quality in situations where sampling computation or
time is limited. Our code and pretrained models are available at
https://lukemelas.github.io/fixed-point-diffusion-models.
\\ ( https://arxiv.org/abs/2401.08741 ,  23777kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08815 (*cross-listing*)
Date: Tue, 16 Jan 2024 20:31:46 GMT   (41434kb,D)

Title: Adversarial Supervision Makes Layout-to-Image Diffusion Models Thrive
Authors: Yumeng Li and Margret Keuper and Dan Zhang and Anna Khoreva
Categories: cs.CV cs.AI cs.LG
Comments: Accepted at ICLR 2024. Project page:
  https://yumengli007.github.io/ALDM/ and code:
  https://github.com/boschresearch/ALDM
\\
  Despite the recent advances in large-scale diffusion models, little progress
has been made on the layout-to-image (L2I) synthesis task. Current L2I models
either suffer from poor editability via text or weak alignment between the
generated image and the input layout. This limits their usability in practice.
To mitigate this, we propose to integrate adversarial supervision into the
conventional training pipeline of L2I diffusion models (ALDM). Specifically, we
employ a segmentation-based discriminator which provides explicit feedback to
the diffusion generator on the pixel-level alignment between the denoised image
and the input layout. To encourage consistent adherence to the input layout
over the sampling steps, we further introduce the multistep unrolling strategy.
Instead of looking at a single timestep, we unroll a few steps recursively to
imitate the inference process, and ask the discriminator to assess the
alignment of denoised images with the layout over a certain time window. Our
experiments show that ALDM enables layout faithfulness of the generated images,
while allowing broad editability via text prompts. Moreover, we showcase its
usefulness for practical applications: by synthesizing target distribution
samples via text control, we improve domain generalization of semantic
segmentation models by a large margin (~12 mIoU points).
\\ ( https://arxiv.org/abs/2401.08815 ,  41434kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08887 (*cross-listing*)
Date: Tue, 16 Jan 2024 23:50:26 GMT   (39kb)

Title: NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant
  Meeting Transcription
Authors: Alon Vinnikov, Amir Ivry, Aviv Hurvitz, Igor Abramovski, Sharon Koubi,
  Ilya Gurvich, Shai Pe`er, Xiong Xiao, Benjamin Martinez Elizalde, Naoyuki
  Kanda, Xiaofei Wang, Shalev Shaer, Stav Yagev, Yossi Asher, Sunit
  Sivasankaran, Yifan Gong, Min Tang, Huaming Wang, Eyal Krupka
Categories: cs.SD cs.AI cs.CL eess.AS
Comments: preprint
\\
  We introduce the first Natural Office Talkers in Settings of Far-field Audio
Recordings (``NOTSOFAR-1'') Challenge alongside datasets and baseline system.
The challenge focuses on distant speaker diarization and automatic speech
recognition (DASR) in far-field meeting scenarios, with single-channel and
known-geometry multi-channel tracks, and serves as a launch platform for two
new datasets: First, a benchmarking dataset of 315 meetings, averaging 6
minutes each, capturing a broad spectrum of real-world acoustic conditions and
conversational dynamics. It is recorded across 30 conference rooms, featuring
4-8 attendees and a total of 35 unique speakers. Second, a 1000-hour simulated
training dataset, synthesized with enhanced authenticity for real-world
generalization, incorporating 15,000 real acoustic transfer functions. The
tasks focus on single-device DASR, where multi-channel devices always share the
same known geometry. This is aligned with common setups in actual conference
rooms, and avoids technical complexities associated with multi-device tasks. It
also allows for the development of geometry-specific solutions. The NOTSOFAR-1
Challenge aims to advance research in the field of distant conversational
speech recognition, providing key resources to unlock the potential of
data-driven methods, which we believe are currently constrained by the absence
of comprehensive high-quality training and benchmarking datasets.
\\ ( https://arxiv.org/abs/2401.08887 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08930 (*cross-listing*)
Date: Wed, 17 Jan 2024 02:59:34 GMT   (1800kb,D)

Title: 3D Human Pose Analysis via Diffusion Synthesis
Authors: Haorui Ji, Hongdong Li
Categories: cs.CV cs.AI
\\
  Diffusion models have demonstrated remarkable success in generative modeling.
In this paper, we propose PADS (Pose Analysis by Diffusion Synthesis), a novel
framework designed to address various challenges in 3D human pose analysis
through a unified pipeline. Central to PADS are two distinctive strategies: i)
learning a task-agnostic pose prior using a diffusion synthesis process to
effectively capture the kinematic constraints in human pose data, and ii)
unifying multiple pose analysis tasks like estimation, completion, denoising,
etc, as instances of inverse problems. The learned pose prior will be treated
as a regularization imposing on task-specific constraints, guiding the
optimization process through a series of conditional denoising steps. PADS
represents the first diffusion-based framework for tackling general 3D human
pose analysis within the inverse problem framework. Its performance has been
validated on different benchmarks, signaling the adaptability and robustness of
this pipeline.
\\ ( https://arxiv.org/abs/2401.08930 ,  1800kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08932 (*cross-listing*)
Date: Wed, 17 Jan 2024 03:02:31 GMT   (4300kb,D)

Title: Learning to detect cloud and snow in remote sensing images from noisy
  labels
Authors: Zili Liu, Hao Chen, Wenyuan Li, Keyan Chen, Zipeng Qi, Chenyang Liu,
  Zhengxia Zou, Zhenwei Shi
Categories: cs.CV cs.AI
\\
  Detecting clouds and snow in remote sensing images is an essential
preprocessing task for remote sensing imagery. Previous works draw inspiration
from semantic segmentation models in computer vision, with most research
focusing on improving model architectures to enhance detection performance.
However, unlike natural images, the complexity of scenes and the diversity of
cloud types in remote sensing images result in many inaccurate labels in cloud
and snow detection datasets, introducing unnecessary noises into the training
and testing processes. By constructing a new dataset and proposing a novel
training strategy with the curriculum learning paradigm, we guide the model in
reducing overfitting to noisy labels. Additionally, we design a more
appropriate model performance evaluation method, that alleviates the
performance assessment bias caused by noisy labels. By conducting experiments
on models with UNet and Segformer, we have validated the effectiveness of our
proposed method. This paper is the first to consider the impact of label noise
on the detection of clouds and snow in remote sensing images.
\\ ( https://arxiv.org/abs/2401.08932 ,  4300kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08957 (*cross-listing*)
Date: Wed, 17 Jan 2024 04:15:56 GMT   (10864kb,D)

Title: SWBT: Similarity Weighted Behavior Transformer with the Imperfect
  Demonstration for Robotic Manipulation
Authors: Kun Wu, Ning Liu, Zhen Zhao, Di Qiu, Jinming Li, Zhengping Che,
  Zhiyuan Xu, Qinru Qiu, Jian Tang
Categories: cs.RO cs.AI
Comments: 8 pages, 5 figures
ACM-class: I.2.9
\\
  Imitation learning (IL), aiming to learn optimal control policies from expert
demonstrations, has been an effective method for robot manipulation tasks.
However, previous IL methods either only use expensive expert demonstrations
and omit imperfect demonstrations or rely on interacting with the environment
and learning from online experiences. In the context of robotic manipulation,
we aim to conquer the above two challenges and propose a novel framework named
Similarity Weighted Behavior Transformer (SWBT). SWBT effectively learn from
both expert and imperfect demonstrations without interaction with environments.
We reveal that the easy-to-get imperfect demonstrations, such as forward and
inverse dynamics, significantly enhance the network by learning fruitful
information. To the best of our knowledge, we are the first to attempt to
integrate imperfect demonstrations into the offline imitation learning setting
for robot manipulation tasks. Extensive experiments on the ManiSkill2 benchmark
built on the high-fidelity Sapien simulator and real-world robotic manipulation
tasks demonstrated that the proposed method can extract better features and
improve the success rates for all tasks. Our code will be released upon
acceptance of the paper.
\\ ( https://arxiv.org/abs/2401.08957 ,  10864kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08960 (*cross-listing*)
Date: Wed, 17 Jan 2024 04:20:10 GMT   (4620kb,D)

Title: From User Surveys to Telemetry-Driven Agents: Exploring the Potential of
  Personalized Productivity Solutions
Authors: Subigya Nepal, Javier Hernandez, Talie Massachi, Kael Rowan, Judith
  Amores, Jina Suh, Gonzalo Ramos, Brian Houck, Shamsi T. Iqbal, Mary
  Czerwinski
Categories: cs.HC cs.AI cs.CY
ACM-class: H.5.0; H.5.3; H.5.m; J.0
\\
  We present a comprehensive, user-centric approach to understand preferences
in AI-based productivity agents and develop personalized solutions tailored to
users' needs. Utilizing a two-phase method, we first conducted a survey with
363 participants, exploring various aspects of productivity, communication
style, agent approach, personality traits, personalization, and privacy.
Drawing on the survey insights, we developed a GPT-4 powered personalized
productivity agent that utilizes telemetry data gathered via Viva Insights from
information workers to provide tailored assistance. We compared its performance
with alternative productivity-assistive tools, such as dashboard and narrative,
in a study involving 40 participants. Our findings highlight the importance of
user-centric design, adaptability, and the balance between personalization and
privacy in AI-assisted productivity tools. By building on the insights
distilled from our study, we believe that our work can enable and guide future
research to further enhance productivity solutions, ultimately leading to
optimized efficiency and user experiences for information workers.
\\ ( https://arxiv.org/abs/2401.08960 ,  4620kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08973 (*cross-listing*)
Date: Wed, 17 Jan 2024 04:52:40 GMT   (21354kb,D)

Title: OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed
  Reality
Authors: Aditya Sharma, Luke Yoffe, Tobias H\"ollerer
Categories: cs.CV cs.AI cs.CL
Comments: 2024 IEEE International Conference on Artificial Intelligence and
  eXtended and Virtual Reality (AIXVR)
\\
  One key challenge in Augmented Reality is the placement of virtual content in
natural locations. Most existing automated techniques can only work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce and
evaluate several methods for automatic object placement using recent advances
in open-vocabulary vision-language models. Through a multifaceted evaluation,
we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark
for automatically evaluating the placement of virtual objects in augmented
reality, alleviating the need for costly user studies. Through this, in
addition to human evaluations, we find that OCTO+ places objects in a valid
region over 70% of the time, outperforming other methods on a range of metrics.
\\ ( https://arxiv.org/abs/2401.08973 ,  21354kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09008 (*cross-listing*)
Date: Wed, 17 Jan 2024 07:06:56 GMT   (315kb,D)

Title: Hybrid of DiffStride and Spectral Pooling in Convolutional Neural
  Networks
Authors: Sulthan Rafif, Mochamad Arfan Ravy Wahyu Pratama, Mohammad Faris
  Azhar, Ahmad Mustafidul Ibad, Lailil Muflikhah, Novanto Yudistira
Categories: cs.CV cs.AI
Journal-ref: CSIAM Transactions on Applied Mathematics; R. Riad et al,
  "Learning strides in convolutional neural networks," pp. 1-16, 2022.
  [Online];
DOI: 10.1145/3626641.3626930
\\
  Stride determines the distance between adjacent filter positions as the
filter moves across the input. A fixed stride causes important information
contained in the image can not be captured, so that important information is
not classified. Therefore, in previous research, the DiffStride Method was
applied, namely the Strided Convolution Method with which it can learn its own
stride value. Severe Quantization and a constraining lower bound on preserved
information are arises with Max Pooling Downsampling Method. Spectral Pooling
reduce the constraint lower bound on preserved information by cutting off the
representation in the frequency domain. In this research a CNN Model is
proposed with the Downsampling Learnable Stride Technique performed by
Backpropagation combined with the Spectral Pooling Technique. Diffstride and
Spectral Pooling techniques are expected to maintain most of the information
contained in the image. In this study, we compare the Hybrid Method, which is a
combined implementation of Spectral Pooling and DiffStride against the Baseline
Method, which is the DiffStride implementation on ResNet 18. The accuracy
result of the DiffStride combination with Spectral Pooling improves over
DiffStride which is baseline method by 0.0094. This shows that the Hybrid
Method can maintain most of the information by cutting of the representation in
the frequency domain and determine the stride of the learning result through
Backpropagation.
\\ ( https://arxiv.org/abs/2401.09008 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09019 (*cross-listing*)
Date: Wed, 17 Jan 2024 07:30:52 GMT   (1638kb,D)

Title: Change Detection Between Optical Remote Sensing Imagery and Map Data via
  Segment Anything Model (SAM)
Authors: Hongruixuan Chen and Jian Song and Naoto Yokoya
Categories: eess.IV cs.AI cs.CV cs.MM
\\
  Unsupervised multimodal change detection is pivotal for time-sensitive tasks
and comprehensive multi-temporal Earth monitoring. In this study, we explore
unsupervised multimodal change detection between two key remote sensing data
sources: optical high-resolution imagery and OpenStreetMap (OSM) data.
Specifically, we propose to utilize the vision foundation model Segmentation
Anything Model (SAM), for addressing our task. Leveraging SAM's exceptional
zero-shot transfer capability, high-quality segmentation maps of optical images
can be obtained. Thus, we can directly compare these two heterogeneous data
forms in the so-called segmentation domain. We then introduce two strategies
for guiding SAM's segmentation process: the 'no-prompt' and 'box/mask prompt'
methods. The two strategies are designed to detect land-cover changes in
general scenarios and to identify new land-cover objects within existing
backgrounds, respectively. Experimental results on three datasets indicate that
the proposed approach can achieve more competitive results compared to
representative unsupervised multimodal change detection methods.
\\ ( https://arxiv.org/abs/2401.09019 ,  1638kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09029 (*cross-listing*)
Date: Wed, 17 Jan 2024 07:54:49 GMT   (704kb,D)

Title: Cross-modality Guidance-aided Multi-modal Learning with Dual Attention
  for MRI Brain Tumor Grading
Authors: Dunyuan Xu, Xi Wang, Jinyue Cai and Pheng-Ann Heng
Categories: cs.CV cs.AI
\\
  Brain tumor represents one of the most fatal cancers around the world, and is
very common in children and the elderly. Accurate identification of the type
and grade of tumor in the early stages plays an important role in choosing a
precise treatment plan. The Magnetic Resonance Imaging (MRI) protocols of
different sequences provide clinicians with important contradictory information
to identify tumor regions. However, manual assessment is time-consuming and
error-prone due to big amount of data and the diversity of brain tumor types.
Hence, there is an unmet need for MRI automated brain tumor diagnosis. We
observe that the predictive capability of uni-modality models is limited and
their performance varies widely across modalities, and the commonly used
modality fusion methods would introduce potential noise, which results in
significant performance degradation. To overcome these challenges, we propose a
novel cross-modality guidance-aided multi-modal learning with dual attention
for addressing the task of MRI brain tumor grading. To balance the tradeoff
between model efficiency and efficacy, we employ ResNet Mix Convolution as the
backbone network for feature extraction. Besides, dual attention is applied to
capture the semantic interdependencies in spatial and slice dimensions
respectively. To facilitate information interaction among modalities, we design
a cross-modality guidance-aided module where the primary modality guides the
other secondary modalities during the process of training, which can
effectively leverage the complementary information of different MRI modalities
and meanwhile alleviate the impact of the possible noise.
\\ ( https://arxiv.org/abs/2401.09029 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09034 (*cross-listing*)
Date: Wed, 17 Jan 2024 08:01:18 GMT   (5949kb,D)

Title: UOEP: User-Oriented Exploration Policy for Enhancing Long-Term User
  Experiences in Recommender Systems
Authors: Changshuo Zhang, Sirui Chen, Xiao Zhang, Sunhao Dai, Weijie Yu, Jun Xu
Categories: cs.IR cs.AI
\\
  Reinforcement learning (RL) has gained traction for enhancing user long-term
experiences in recommender systems by effectively exploring users' interests.
However, modern recommender systems exhibit distinct user behavioral patterns
among tens of millions of items, which increases the difficulty of exploration.
For example, user behaviors with different activity levels require varying
intensity of exploration, while previous studies often overlook this aspect and
apply a uniform exploration strategy to all users, which ultimately hurts user
experiences in the long run. To address these challenges, we propose
User-Oriented Exploration Policy (UOEP), a novel approach facilitating
fine-grained exploration among user groups. We first construct a distributional
critic which allows policy optimization under varying quantile levels of
cumulative reward feedbacks from users, representing user groups with varying
activity levels. Guided by this critic, we devise a population of distinct
actors aimed at effective and fine-grained exploration within its respective
user group. To simultaneously enhance diversity and stability during the
exploration process, we further introduce a population-level diversity
regularization term and a supervision module. Experimental results on public
recommendation datasets demonstrate that our approach outperforms all other
baselines in terms of long-term performance, validating its user-oriented
exploration effectiveness. Meanwhile, further analyses reveal our approach's
benefits of improved performance for low-activity users as well as increased
fairness among users.
\\ ( https://arxiv.org/abs/2401.09034 ,  5949kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09075 (*cross-listing*)
Date: Wed, 17 Jan 2024 09:27:13 GMT   (2939kb,D)

Title: GPT in Sheep's Clothing: The Risk of Customized GPTs
Authors: Sagiv Antebi, Noam Azulay, Edan Habler, Ben Ganon, Asaf Shabtai, Yuval
  Elovici
Categories: cs.CR cs.AI
\\
  In November 2023, OpenAI introduced a new service allowing users to create
custom versions of ChatGPT (GPTs) by using specific instructions and knowledge
to guide the model's behavior. We aim to raise awareness of the fact that GPTs
can be used maliciously, posing privacy and security risks to their users.
\\ ( https://arxiv.org/abs/2401.09075 ,  2939kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09239 (*cross-listing*)
Date: Wed, 17 Jan 2024 14:39:55 GMT   (10102kb,D)

Title: DaFoEs: Mixing Datasets towards the generalization of vision-state
  deep-learning Force Estimation in Minimally Invasive Robotic Surgery
Authors: Mikel De Iturrate Reyzabal, Mingcong Chen, Wei Huang, Sebastien
  Ourselin and Hongbin Liu
Categories: cs.CV cs.AI cs.RO
\\
  Precisely determining the contact force during safe interaction in Minimally
Invasive Robotic Surgery (MIRS) is still an open research challenge. Inspired
by post-operative qualitative analysis from surgical videos, the use of
cross-modality data driven deep neural network models has been one of the
newest approaches to predict sensorless force trends. However, these methods
required for large and variable datasets which are not currently available. In
this paper, we present a new vision-haptic dataset (DaFoEs) with variable soft
environments for the training of deep neural models. In order to reduce the
bias from a single dataset, we present a pipeline to generalize different
vision and state data inputs for mixed dataset training, using a previously
validated dataset with different setup. Finally, we present a variable
encoder-decoder architecture to predict the forces done by the laparoscopic
tool using single input or sequence of inputs. For input sequence, we use a
recurrent decoder, named with the prefix R, and a new temporal sampling to
represent the acceleration of the tool. During our training, we demonstrate
that single dataset training tends to overfit to the training data domain, but
has difficulties on translating the results across new domains. However,
dataset mixing presents a good translation with a mean relative estimated force
error of 5% and 12% for the recurrent and non-recurrent models respectively.
Our method, also marginally increase the effectiveness of transformers for
force estimation up to a maximum of ~15%, as the volume of available data is
increase by 150%. In conclusion, we demonstrate that mixing experimental set
ups for vision-state force estimation in MIRS is a possible approach towards
the general solution of the problem.
\\ ( https://arxiv.org/abs/2401.09239 ,  10102kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09240 (*cross-listing*)
Date: Wed, 17 Jan 2024 14:40:09 GMT   (718kb)

Title: A Blockchain-based Model for Securing Data Pipeline in a Heterogeneous
  Information System
Authors: MN Ramahlosi, Y Madani, A Akanbi
Categories: cs.CR cs.AI cs.DC cs.NI
Comments: 13 pages
\\
  In our digital world, access to personal and public data has become an item
of concern, with challenging security and privacy aspects. Modern information
systems are heterogeneous in nature and have an inherent security
vulnerability, which is susceptible to data interception and data modification
due to unsecured communication data pipelines between connected endpoints. This
re-search article presents a blockchain-based model for securing data pipelines
in a heterogeneous information system using an integrated multi-hazard early
warning system (MHEWS) as a case study. The proposed model utilizes the
inherent security features of blockchain technology to address the security and
privacy concerns that arise in data pipelines. The model is designed to ensure
data integrity, confidentiality, and authenticity in a decentralized manner.
The model is evaluated in a hybrid environment using a prototype implementation
and simulation experiments with outcomes that demonstrate advantages over
traditional approaches for a tamper-proof and immutable data pipeline for data
authenticity and integrity using a confidential ledger.
\\ ( https://arxiv.org/abs/2401.09240 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09243 (*cross-listing*)
Date: Wed, 17 Jan 2024 14:43:59 GMT   (1516kb,D)

Title: DiffClone: Enhanced Behaviour Cloning in Robotics with Diffusion-Driven
  Policy Learning
Authors: Sabariswaran Mani, Abhranil Chandra, Sreyas Venkataraman, Adyan Rizvi,
  Yash Sirvi, Soumojit Bhattacharya, Aritra Hazra
Categories: cs.RO cs.AI cs.LG
Comments: NeurIPS 2023 Train Offline Test Online Workshop and Competition
\\
  Robot learning tasks are extremely compute-intensive and hardware-specific.
Thus the avenues of tackling these challenges, using a diverse dataset of
offline demonstrations that can be used to train robot manipulation agents, is
very appealing. The Train-Offline-Test-Online (TOTO) Benchmark provides a
well-curated open-source dataset for offline training comprised mostly of
expert data and also benchmark scores of the common offline-RL and behaviour
cloning agents. In this paper, we introduce DiffClone, an offline algorithm of
enhanced behaviour cloning agent with diffusion-based policy learning, and
measured the efficacy of our method on real online physical robots at test
time. This is also our official submission to the Train-Offline-Test-Online
(TOTO) Benchmark Challenge organized at NeurIPS 2023. We experimented with both
pre-trained visual representation and agent policies. In our experiments, we
find that MOCO finetuned ResNet50 performs the best in comparison to other
finetuned representations. Goal state conditioning and mapping to transitions
resulted in a minute increase in the success rate and mean-reward. As for the
agent policy, we developed DiffClone, a behaviour cloning agent improved using
conditional diffusion.
\\ ( https://arxiv.org/abs/2401.09243 ,  1516kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09252 (*cross-listing*)
Date: Wed, 17 Jan 2024 14:57:27 GMT   (16672kb,D)

Title: 3D Scene Geometry Estimation from 360$^\circ$ Imagery: A Survey
Authors: Thiago Lopes Trugillo da Silveira, Paulo Gamarra Lessa Pinto, Jeffri
  Erwin Murrugarra Llerena, Claudio Rosito Jung
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Published in ACM Computing Surveys
Journal-ref: ACM Comput. Surv. 55, 4, Article 68, 2023
DOI: 10.1145/3519021
\\
  This paper provides a comprehensive survey on pioneer and state-of-the-art 3D
scene geometry estimation methodologies based on single, two, or multiple
images captured under the omnidirectional optics. We first revisit the basic
concepts of the spherical camera model, and review the most common acquisition
technologies and representation formats suitable for omnidirectional (also
called 360$^\circ$, spherical or panoramic) images and videos. We then survey
monocular layout and depth inference approaches, highlighting the recent
advances in learning-based solutions suited for spherical data. The classical
stereo matching is then revised on the spherical domain, where methodologies
for detecting and describing sparse and dense features become crucial. The
stereo matching concepts are then extrapolated for multiple view camera setups,
categorizing them among light fields, multi-view stereo, and structure from
motion (or visual simultaneous localization and mapping). We also compile and
discuss commonly adopted datasets and figures of merit indicated for each
purpose and list recent results for completeness. We conclude this paper by
pointing out current and future trends.
\\ ( https://arxiv.org/abs/2401.09252 ,  16672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09286 (*cross-listing*)
Date: Wed, 17 Jan 2024 15:40:11 GMT   (6458kb,D)

Title: Deployable Reinforcement Learning with Variable Control Rate
Authors: Dong Wang and Giovanni Beltrame
Categories: cs.RO cs.AI
Comments: Paper for AAAI-DAI 2024 workshop
\\
  Deploying controllers trained with Reinforcement Learning (RL) on real robots
can be challenging: RL relies on agents' policies being modeled as Markov
Decision Processes (MDPs), which assume an inherently discrete passage of time.
The use of MDPs results in that nearly all RL-based control systems employ a
fixed-rate control strategy with a period (or time step) typically chosen based
on the developer's experience or specific characteristics of the application
environment. Unfortunately, the system should be controlled at the highest,
worst-case frequency to ensure stability, which can demand significant
computational and energy resources and hinder the deployability of the
controller on onboard hardware. Adhering to the principles of reactive
programming, we surmise that applying control actions only when necessary
enables the use of simpler hardware and helps reduce energy consumption. We
challenge the fixed frequency assumption by proposing a variant of RL with
variable control rate. In this approach, the policy decides the action the
agent should take as well as the duration of the time step associated with that
action. In our new setting, we expand Soft Actor-Critic (SAC) to compute the
optimal policy with a variable control rate, introducing the Soft Elastic
Actor-Critic (SEAC) algorithm. We show the efficacy of SEAC through a
proof-of-concept simulation driving an agent with Newtonian kinematics. Our
experiments show higher average returns, shorter task completion times, and
reduced computational resources when compared to fixed rate policies.
\\ ( https://arxiv.org/abs/2401.09286 ,  6458kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09294 (*cross-listing*)
Date: Wed, 17 Jan 2024 15:54:36 GMT   (1962kb,D)

Title: T-FOLEY: A Controllable Waveform-Domain Diffusion Model for
  Temporal-Event-Guided Foley Sound Synthesis
Authors: Yoonjin Chung, Junwon Lee, Juhan Nam
Categories: cs.SD cs.AI cs.LG eess.AS eess.SP
\\
  Foley sound, audio content inserted synchronously with videos, plays a
critical role in the user experience of multimedia content. Recently, there has
been active research in Foley sound synthesis, leveraging the advancements in
deep generative models. However, such works mainly focus on replicating a
single sound class or a textual sound description, neglecting temporal
information, which is crucial in the practical applications of Foley sound. We
present T-Foley, a Temporal-event-guided waveform generation model for Foley
sound synthesis. T-Foley generates high-quality audio using two conditions: the
sound class and temporal event feature. For temporal conditioning, we devise a
temporal event feature and a novel conditioning technique named Block-FiLM.
T-Foley achieves superior performance in both objective and subjective
evaluation metrics and generates Foley sound well-synchronized with the
temporal events. Additionally, we showcase T-Foley's practical applications,
particularly in scenarios involving vocal mimicry for temporal event control.
We show the demo on our companion website.
\\ ( https://arxiv.org/abs/2401.09294 ,  1962kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09322 (*cross-listing*)
Date: Wed, 17 Jan 2024 16:46:38 GMT   (5586kb,D)

Title: FIT-SLAM -- Fisher Information and Traversability estimation-based
  Active SLAM for exploration in 3D environments
Authors: Suchetan Saravanan, Corentin Chauffaut, Caroline Chanel, Damien Vivet
Categories: cs.RO cs.AI
Comments: 6 pages, 6 figures, IEEE ICARA 2024
\\
  Active visual SLAM finds a wide array of applications in GNSS-Denied
sub-terrain environments and outdoor environments for ground robots. To achieve
robust localization and mapping accuracy, it is imperative to incorporate the
perception considerations in the goal selection and path planning towards the
goal during an exploration mission. Through this work, we propose FIT-SLAM
(Fisher Information and Traversability estimation-based Active SLAM), a new
exploration method tailored for unmanned ground vehicles (UGVs) to explore 3D
environments. This approach is devised with the dual objectives of sustaining
an efficient exploration rate while optimizing SLAM accuracy. Initially, an
estimation of a global traversability map is conducted, which accounts for the
environmental constraints pertaining to traversability. Subsequently, we
propose a goal candidate selection approach along with a path planning method
towards this goal that takes into account the information provided by the
landmarks used by the SLAM backend to achieve robust localization and
successful path execution . The entire algorithm is tested and evaluated first
in a simulated 3D world, followed by a real-world environment and is compared
to pre-existing exploration methods. The results obtained during this
evaluation demonstrate a significant increase in the exploration rate while
effectively minimizing the localization covariance.
\\ ( https://arxiv.org/abs/2401.09322 ,  5586kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09340 (*cross-listing*)
Date: Wed, 17 Jan 2024 17:04:35 GMT   (32021kb,D)

Title: SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene
  Understanding
Authors: Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu
  Liu, Qing Li, Siyuan Huang
Categories: cs.CV cs.AI cs.CL cs.LG cs.RO
Comments: 21 pages
\\
  3D vision-language grounding, which focuses on aligning language with the 3D
physical environment, stands as a cornerstone in the development of embodied
agents. In comparison to recent advancements in the 2D domain, grounding
language in 3D scenes faces several significant challenges: (i) the inherent
complexity of 3D scenes due to the diverse object configurations, their rich
attributes, and intricate relationships; (ii) the scarcity of paired 3D
vision-language data to support grounded learning; and (iii) the absence of a
unified learning framework to distill knowledge from grounded 3D data. In this
work, we aim to address these three major challenges in 3D vision-language by
examining the potential of systematically upscaling 3D vision-language learning
in indoor environments. We introduce the first million-scale 3D vision-language
dataset, SceneVerse, encompassing about 68K 3D indoor scenes and comprising
2.5M vision-language pairs derived from both human annotations and our scalable
scene-graph-based generation approach. We demonstrate that this scaling allows
for a unified pre-training framework, Grounded Pre-training for Scenes (GPS),
for 3D vision-language learning. Through extensive experiments, we showcase the
effectiveness of GPS by achieving state-of-the-art performance on all existing
3D visual grounding benchmarks. The vast potential of SceneVerse and GPS is
unveiled through zero-shot transfer experiments in the challenging 3D
vision-language tasks. Project website: https://scene-verse.github.io .
\\ ( https://arxiv.org/abs/2401.09340 ,  32021kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09352 (*cross-listing*)
Date: Wed, 17 Jan 2024 17:18:21 GMT   (41604kb,D)

Title: Neural Contractive Dynamical Systems
Authors: Hadi Beik-Mohammadi, S{\o}ren Hauberg, Georgios Arvanitidis, Nadia
  Figueroa, Gerhard Neumann, and Leonel Rozo
Categories: cs.RO cs.AI cs.LG
\\
  Stability guarantees are crucial when ensuring a fully autonomous robot does
not take undesirable or potentially harmful actions. Unfortunately, global
stability guarantees are hard to provide in dynamical systems learned from
data, especially when the learned dynamics are governed by neural networks. We
propose a novel methodology to learn neural contractive dynamical systems,
where our neural architecture ensures contraction, and hence, global stability.
To efficiently scale the method to high-dimensional dynamical systems, we
develop a variant of the variational autoencoder that learns dynamics in a
low-dimensional latent representation space while retaining contractive
stability after decoding. We further extend our approach to learning
contractive systems on the Lie group of rotations to account for full-pose
end-effector dynamic motions. The result is the first highly flexible learning
architecture that provides contractive stability guarantees with capability to
perform obstacle avoidance. Empirically, we demonstrate that our approach
encodes the desired dynamics more accurately than the current state-of-the-art,
which provides less strong stability guarantees.
\\ ( https://arxiv.org/abs/2401.09352 ,  41604kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09354 (*cross-listing*)
Date: Fri, 12 Jan 2024 16:10:04 GMT   (255kb)

Title: Transcending Controlled Environments Assessing the Transferability of
  ASRRobust NLU Models to Real-World Applications
Authors: Hania Khan, Aleena Fatima Khalid, Zaryab Hassan
Categories: eess.AS cs.AI cs.SD
\\
  This research investigates the transferability of Automatic Speech
Recognition (ASR)-robust Natural Language Understanding (NLU) models from
controlled experimental conditions to practical, real-world applications.
Focused on smart home automation commands in Urdu, the study assesses model
performance under diverse noise profiles, linguistic variations, and ASR error
scenarios. Leveraging the UrduBERT model, the research employs a systematic
methodology involving real-world data collection, cross-validation, transfer
learning, noise variation studies, and domain adaptation. Evaluation metrics
encompass task-specific accuracy, latency, user satisfaction, and robustness to
ASR errors. The findings contribute insights into the challenges and
adaptability of ASR-robust NLU models in transcending controlled environments.
\\ ( https://arxiv.org/abs/2401.09354 ,  255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09410 (*cross-listing*)
Date: Wed, 17 Jan 2024 18:47:30 GMT   (928kb,D)

Title: Through the Looking-Glass: Transparency Implications and Challenges in
  Enterprise AI Knowledge Systems
Authors: Karina Corti\~nas-Lorenzo, Si\^an Lindley, Ida Larsen-Ledet and
  Bhaskar Mitra
Categories: cs.CY cs.AI cs.HC
\\
  Knowledge can't be disentangled from people. As AI knowledge systems mine
vast volumes of work-related data, the knowledge that's being extracted and
surfaced is intrinsically linked to the people who create and use it. When
these systems get embedded in organizational settings, the information that is
brought to the foreground and the information that's pushed to the periphery
can influence how individuals see each other and how they see themselves at
work. In this paper, we present the looking-glass metaphor and use it to
conceptualize AI knowledge systems as systems that reflect and distort,
expanding our view on transparency requirements, implications and challenges.
We formulate transparency as a key mediator in shaping different ways of
seeing, including seeing into the system, which unveils its capabilities,
limitations and behavior, and seeing through the system, which shapes workers'
perceptions of their own contributions and others within the organization.
Recognizing the sociotechnical nature of these systems, we identify three
transparency dimensions necessary to realize the value of AI knowledge systems,
namely system transparency, procedural transparency and transparency of
outcomes. We discuss key challenges hindering the implementation of these forms
of transparency, bringing to light the wider sociotechnical gap and
highlighting directions for future Computer-supported Cooperative Work (CSCW)
research.
\\ ( https://arxiv.org/abs/2401.09410 ,  928kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09414 (*cross-listing*)
Date: Wed, 17 Jan 2024 18:55:12 GMT   (2672kb,D)

Title: Vlogger: Make Your Dream A Vlog
Authors: Shaobin Zhuang, Kunchang Li, Xinyuan Chen, Yaohui Wang, Ziwei Liu, Yu
  Qiao, Yali Wang
Categories: cs.CV cs.AI cs.LG cs.MM
Comments: 16 pages, 8 figures, 11 tables
\\
  In this work, we present Vlogger, a generic AI system for generating a
minute-level video blog (i.e., vlog) of user descriptions. Different from short
videos with a few seconds, vlog often contains a complex storyline with
diversified scenes, which is challenging for most existing video generation
approaches. To break through this bottleneck, our Vlogger smartly leverages
Large Language Model (LLM) as Director and decomposes a long video generation
task of vlog into four key stages, where we invoke various foundation models to
play the critical roles of vlog professionals, including (1) Script, (2) Actor,
(3) ShowMaker, and (4) Voicer. With such a design of mimicking human beings,
our Vlogger can generate vlogs through explainable cooperation of top-down
planning and bottom-up shooting. Moreover, we introduce a novel video diffusion
model, ShowMaker, which serves as a videographer in our Vlogger for generating
the video snippet of each shooting scene. By incorporating Script and Actor
attentively as textual and visual prompts, it can effectively enhance
spatial-temporal coherence in the snippet. Besides, we design a concise mixed
training paradigm for ShowMaker, boosting its capacity for both T2V generation
and prediction. Finally, the extensive experiments show that our method
achieves state-of-the-art performance on zero-shot T2V generation and
prediction tasks. More importantly, Vlogger can generate over 5-minute vlogs
from open-world descriptions, without loss of video coherence on script and
actor. The code and model is all available at
https://github.com/zhuangshaobin/Vlogger.
\\ ( https://arxiv.org/abs/2401.09414 ,  2672kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08833 (*cross-listing*)
Date: Tue, 16 Jan 2024 21:13:22 GMT   (323kb,D)

Title: Revisiting Self-supervised Learning of Speech Representation from a
  Mutual Information Perspective
Authors: Alexander H. Liu, Sung-Lin Yeh, James Glass
Categories: eess.AS cs.CL cs.SD
Comments: ICASSP 2024
\\
  Existing studies on self-supervised speech representation learning have
focused on developing new training methods and applying pre-trained models for
different applications. However, the quality of these models is often measured
by the performance of different downstream tasks. How well the representations
access the information of interest is less studied. In this work, we take a
closer look into existing self-supervised methods of speech from an
information-theoretic perspective. We aim to develop metrics using mutual
information to help practical problems such as model design and selection. We
use linear probes to estimate the mutual information between the target
information and learned representations, showing another insight into the
accessibility to the target information from speech representations. Further,
we explore the potential of evaluating representations in a self-supervised
fashion, where we estimate the mutual information between different parts of
the data without using any labels. Finally, we show that both supervised and
unsupervised measures echo the performance of the models on layer-wise linear
probing and speech recognition.
\\ ( https://arxiv.org/abs/2401.08833 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08602 (*cross-listing*)
Date: Tue, 21 Nov 2023 13:07:20 GMT   (2907kb,D)

Title: Learning with Chemical versus Electrical Synapses -- Does it Make a
  Difference?
Authors: M\'onika Farsang, Mathias Lechner, David Lung, Ramin Hasani, Daniela
  Rus, Radu Grosu
Categories: cs.NE cs.LG
\\
  Bio-inspired neural networks have the potential to advance our understanding
of neural computation and improve the state-of-the-art of AI systems.
Bio-electrical synapses directly transmit neural signals, by enabling fast
current flow between neurons. In contrast, bio-chemical synapses transmit
neural signals indirectly, through neurotransmitters. Prior work showed that
interpretable dynamics for complex robotic control, can be achieved by using
chemical synapses, within a sparse, bio-inspired architecture, called Neural
Circuit Policies (NCPs). However, a comparison of these two synaptic models,
within the same architecture, remains an unexplored area. In this work we aim
to determine the impact of using chemical synapses compared to electrical
synapses, in both sparse and all-to-all connected networks. We conduct
experiments with autonomous lane-keeping through a photorealistic autonomous
driving simulator to evaluate their performance under diverse conditions and in
the presence of noise. The experiments highlight the substantial influence of
the architectural and synaptic-model choices, respectively. Our results show
that employing chemical synapses yields noticeable improvements compared to
electrical synapses, and that NCPs lead to better results in both synaptic
models.
\\ ( https://arxiv.org/abs/2401.08602 ,  2907kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08603 (*cross-listing*)
Date: Wed, 22 Nov 2023 07:58:14 GMT   (1642kb,D)

Title: Representation Learning in a Decomposed Encoder Design for Bio-inspired
  Hebbian Learning
Authors: Achref Jaziri, Sina Ditzel, Iuliia Pliushch, Visvanathan Ramesh
Categories: cs.NE cs.LG
\\
  Modern data-driven machine learning system designs exploit inductive biases
on architectural structure, invariance and equivariance requirements, task
specific loss functions, and computational optimization tools. Previous works
have illustrated that inductive bias in the early layers of the encoder in the
form of human specified quasi-invariant filters can serve as a powerful
inductive bias to attain better robustness and transparency in learned
classifiers. This paper explores this further in the context of representation
learning with local plasticity rules i.e. bio-inspired Hebbian learning . We
propose a modular framework trained with a bio-inspired variant of contrastive
predictive coding (Hinge CLAPP Loss). Our framework is composed of parallel
encoders each leveraging a different invariant visual descriptor as an
inductive bias. We evaluate the representation learning capacity of our system
in a classification scenario on image data of various difficulties (GTSRB,
STL10, CODEBRIM) as well as video data (UCF101). Our findings indicate that
this form of inductive bias can be beneficial in closing the gap between models
with local plasticity rules and backpropagation models as well as learning more
robust representations in general.
\\ ( https://arxiv.org/abs/2401.08603 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08627 (*cross-listing*)
Date: Fri, 8 Dec 2023 04:27:11 GMT   (1094kb,D)

Title: Predicting and Interpreting Energy Barriers of Metallic Glasses with
  Graph Neural Networks
Authors: Haoyu Li, Shichang Zhang, Longwen Tang, Mathieu Bauchy, Yizhou Sun
Categories: cond-mat.dis-nn cond-mat.mtrl-sci cs.LG
\\
  Metallic Glasses (MGs) are widely used disordered materials. Understanding
the relationship between the local structure and physical properties of MGs is
one of the greatest challenges for both material science and condensed matter
physics. In this work, we utilize Graph Neural Networks (GNNs) to model the
atomic graph structure and study the connection between the structure and the
corresponding local energy barrier, which is believed to govern many critical
physical properties in MGs. One of our key contributions is to propose a novel
Symmetrized GNN (SymGNN) model for predicting the energy barriers, which is
invariant under orthogonal transformations of the structure, e.g., rotations
and reflections. Such invariance is a desired property that standard GNNs like
Graph Convolutional Networks cannot capture. SymGNNs handle the invariance by
aggregating over orthogonal transformations of the graph structure for
representation learning, and an optimal distribution over all 3D orthogonal
transformations $\mathcal{O}_3$ is learned to maximize the benefit of
invariance. We demonstrate in our experiments that SymGNN can significantly
improve the energy barrier prediction over other GNNs and non-graph machine
learning models. With such an accurate model, we also apply graph explanation
algorithms to better reveal the structure-property relationship of MGs. Our GNN
framework allows effective prediction of material physical properties and
bolsters material science research through the use of AI models.
\\ ( https://arxiv.org/abs/2401.08627 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08637 (*cross-listing*)
Date: Mon, 11 Dec 2023 23:30:01 GMT   (5593kb,D)

Title: Collaborative Inference via Dynamic Composition of Tiny AI Accelerators
  on MCUs
Authors: Taesik Gong, Si Young Jang, Utku G\"unay Acer, Fahim Kawsar, Chulhong
  Min
Categories: cs.DC cs.LG
\\
  The advent of tiny AI accelerators opens opportunities for deep neural
network deployment at the extreme edge, offering reduced latency, lower power
cost, and improved privacy in on-device ML inference. Despite these
advancements, challenges persist due to inherent limitations of these
accelerators, such as restricted onboard memory and single-device focus. This
paper introduces Synergy, a system that dynamically composes tiny AI
accelerators for multi-tenant models, effectively addressing tinyML's critical
challenges for the increasing demand for on-device AI. A key feature of Synergy
is its virtual computing space, providing a unified, virtualized view of
resources and enabling efficient task mapping to physical devices. Synergy's
runtime orchestration module ensures optimal inference across dynamic and
heterogeneous accelerators. Our evaluations with 7 baselines and 8 models
demonstrate that Synergy improves throughput by an average of 8.0X compared to
baselines.
\\ ( https://arxiv.org/abs/2401.08637 ,  5593kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08639 (*cross-listing*)
Date: Tue, 12 Dec 2023 07:28:40 GMT   (2578kb,D)

Title: One-Step Diffusion Distillation via Deep Equilibrium Models
Authors: Zhengyang Geng and Ashwini Pokle and J. Zico Kolter
Categories: cs.CV cs.LG
Comments: NeurIPS 2023
\\
  Diffusion models excel at producing high-quality samples but naively require
hundreds of iterations, prompting multiple attempts to distill the generation
process into a faster network. However, many existing approaches suffer from a
variety of challenges: the process for distillation training can be complex,
often requiring multiple training stages, and the resulting models perform
poorly when utilized in single-step generative applications. In this paper, we
introduce a simple yet effective means of distilling diffusion models directly
from initial noise to the resulting image. Of particular importance to our
approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled
architecture: the Generative Equilibrium Transformer (GET). Our method enables
fully offline training with just noise/image pairs from the diffusion model
while achieving superior performance compared to existing one-step methods on
comparable training budgets. We demonstrate that the DEQ architecture is
crucial to this capability, as GET matches a $5\times$ larger ViT in terms of
FID scores while striking a critical balance of computational cost and image
quality. Code, checkpoints, and datasets are available.
\\ ( https://arxiv.org/abs/2401.08639 ,  2578kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08649 (*cross-listing*)
Date: Sun, 24 Dec 2023 08:26:00 GMT   (2790kb,D)

Title: Deep Pulse-Coupled Neural Networks
Authors: Zexiang Yi, Jing Lian, Yunliang Qi, Zhaofei Yu, Huajin Tang, Yide Ma
  and Jizhao Liu
Categories: cs.NE cs.LG
\\
  Spiking Neural Networks (SNNs) capture the information processing mechanism
of the brain by taking advantage of spiking neurons, such as the Leaky
Integrate-and-Fire (LIF) model neuron, which incorporates temporal dynamics and
transmits information via discrete and asynchronous spikes. However, the
simplified biological properties of LIF ignore the neuronal coupling and
dendritic structure of real neurons, which limits the spatio-temporal dynamics
of neurons and thus reduce the expressive power of the resulting SNNs. In this
work, we leverage a more biologically plausible neural model with complex
dynamics, i.e., a pulse-coupled neural network (PCNN), to improve the
expressiveness and recognition performance of SNNs for vision tasks. The PCNN
is a type of cortical model capable of emulating the complex neuronal
activities in the primary visual cortex. We construct deep pulse-coupled neural
networks (DPCNNs) by replacing commonly used LIF neurons in SNNs with PCNN
neurons. The intra-coupling in existing PCNN models limits the coupling between
neurons only within channels. To address this limitation, we propose
inter-channel coupling, which allows neurons in different feature maps to
interact with each other. Experimental results show that inter-channel coupling
can efficiently boost performance with fewer neurons, synapses, and less
training time compared to widening the networks. For instance, compared to the
LIF-based SNN with wide VGG9, DPCNN with VGG9 uses only 50%, 53%, and 73% of
neurons, synapses, and training time, respectively. Furthermore, we propose
receptive field and time dependent batch normalization (RFTD-BN) to speed up
the convergence and performance of DPCNNs.
\\ ( https://arxiv.org/abs/2401.08649 ,  2790kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08661 (*cross-listing*)
Date: Wed, 27 Dec 2023 06:03:34 GMT   (803kb)

Title: Risk-anticipatory autonomous driving strategies considering vehicles'
  weights, based on hierarchical deep reinforcement learning
Authors: Di Chen, Hao Li, Zhicheng Jin and Huizhao Tu
Categories: cs.RO cs.LG
Comments: 12 pages, 7 figures, 6 tables
\\
  Autonomous vehicles (AVs) have the potential to prevent accidents caused by
drivers' error and reduce road traffic risks. Due to the nature of heavy
vehicles, whose collisions cause more serious crashes, the weights of vehicles
need to be considered when making driving strategies aimed at reducing the
potential risks and their consequences in the context of autonomous driving.
This study develops an autonomous driving strategy based on risk anticipation,
considering the weights of surrounding vehicles and using hierarchical deep
reinforcement learning. A risk indicator integrating surrounding vehicles'
weights, based on the risk field theory, is proposed and incorporated into
autonomous driving decisions. A hybrid action space is designed to allow for
left lane changes, right lane changes and car-following, which enables AVs to
act more freely and realistically whenever possible. To solve the above hybrid
decision-making problem, a hierarchical proximal policy optimization (HPPO)
algorithm is developed and an attention mechanism is incorporated, providing
great advantages in maintaining stable performance. An indicator, potential
collision energy in conflicts (PCEC), is newly proposed to evaluate the
performance of the developed AV driving strategy from both the perspectives of
the likelihood and the consequences of potential accidents. An application is
carried out and the simulation results demonstrate that our model provides
driving strategies that reduce both the likelihood and consequences of
potential accidents, at the same time maintaining driving efficiency. The
developed method is especially meaningful for AVs driving on highways, where
heavy vehicles make up a high proportion of the traffic.
\\ ( https://arxiv.org/abs/2401.08661 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08667 (*cross-listing*)
Date: Fri, 5 Jan 2024 16:31:16 GMT   (11022kb,D)

Title: Data-Driven Physics-Informed Neural Networks: A Digital Twin Perspective
Authors: Sunwoong Yang, Hojin Kim, Yoonpyo Hong, Kwanjung Yee, Romit Maulik,
  Namwoo Kang
Categories: physics.flu-dyn cs.CE cs.LG
\\
  This study explores the potential of physics-informed neural networks (PINNs)
for the realization of digital twins (DT) from various perspectives. First,
various adaptive sampling approaches for collocation points are investigated to
verify their effectiveness in the mesh-free framework of PINNs, which allows
automated construction of virtual representation without manual mesh
generation. Then, the overall performance of the data-driven PINNs (DD-PINNs)
framework is examined, which can utilize the acquired datasets in DT scenarios.
Its scalability to more general physics is validated within parametric
Navier-Stokes equations, where PINNs do not need to be retrained as the
Reynolds number varies. In addition, since datasets can be often collected from
different fidelity/sparsity in practice, multi-fidelity DD-PINNs are also
proposed and evaluated. They show remarkable prediction performance even in the
extrapolation tasks, with $42\sim62\%$ improvement over the single-fidelity
approach. Finally, the uncertainty quantification performance of multi-fidelity
DD-PINNs is investigated by the ensemble method to verify their potential in
DT, where an accurate measure of predictive uncertainty is critical. The
DD-PINN frameworks explored in this study are found to be more suitable for DT
scenarios than traditional PINNs from the above perspectives, bringing
engineers one step closer to seamless DT realization.
\\ ( https://arxiv.org/abs/2401.08667 ,  11022kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08671 (*cross-listing*)
Date: Tue, 9 Jan 2024 06:49:40 GMT   (2808kb,D)

Title: DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and
  DeepSpeed-Inference
Authors: Connor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff
  Rasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash
  Bakhtiari, Lev Kurilenko, Yuxiong He
Categories: cs.PF cs.LG
\\
  The deployment and scaling of large language models (LLMs) have become
critical as they permeate various applications, demanding high-throughput and
low-latency serving systems. Existing frameworks struggle to balance these
requirements, especially for workloads with long prompts. This paper introduces
DeepSpeed-FastGen, a system that employs Dynamic SplitFuse, a novel prompt and
generation composition strategy, to deliver up to 2.3x higher effective
throughput, 2x lower latency on average, and up to 3.7x lower (token-level)
tail latency, compared to state-of-the-art systems like vLLM. We leverage a
synergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an
efficient and easy-to-use serving system for LLMs. DeepSpeed-FastGen's advanced
implementation supports a range of models and offers both non-persistent and
persistent deployment options, catering to diverse user scenarios from
interactive sessions to long-running applications. We present a detailed
benchmarking methodology, analyze the performance through latency-throughput
curves, and investigate scalability via load balancing. Our evaluations
demonstrate substantial improvements in throughput and latency across various
models and hardware configurations. We discuss our roadmap for future
enhancements, including broader model support and new hardware backends. The
DeepSpeed-FastGen code is readily available for community engagement and
contribution.
\\ ( https://arxiv.org/abs/2401.08671 ,  2808kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08684 (*cross-listing*)
Date: Fri, 12 Jan 2024 18:58:37 GMT   (1356kb)

Title: A Physics-informed machine learning model for time-dependent wave runup
  prediction
Authors: Saeed Saviz Naeini, Reda Snaiki
Categories: physics.flu-dyn cs.LG physics.ao-ph
\\
  Wave runup is a critical factor affecting coastal flooding, shoreline
changes, and damage to coastal structures. Climate change is also expected to
amplify wave runup's impact on coastal areas. Therefore, fast and accurate wave
runup estimation is essential for effective coastal engineering design and
management. However, predicting the time-dependent wave runup is challenging
due to the intrinsic nonlinearities and non-stationarity of the process, even
with the use of the most advanced machine learning techniques. In this study, a
physics-informed machine learning-based approach is proposed to efficiently and
accurately simulate time-series wave runup. The methodology combines the
computational efficiency of the Surfbeat (XBSB) mode with the accuracy of the
nonhydrostatic (XBNH) mode of the XBeach model. Specifically, a conditional
generative adversarial network (cGAN) is used to map the image representation
of wave runup from XBSB to the corresponding image from XBNH. These images are
generated by first converting wave runup signals into time-frequency scalograms
and then transforming them into image representations. The cGAN model achieves
improved performance in image-to-image mapping tasks by incorporating
physics-based knowledge from XBSB. After training the model, the high-fidelity
XBNH-based scalograms can be predicted, which are then employed to reconstruct
the time-series wave runup using the inverse wavelet transform. The simulation
results underscore the efficiency and robustness of the proposed model in
predicting wave runup, suggesting its potential value for applications in risk
assessment and management.
\\ ( https://arxiv.org/abs/2401.08684 ,  1356kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08689 (*cross-listing*)
Date: Sat, 13 Jan 2024 08:30:13 GMT   (1423kb,D)

Title: NODI: Out-Of-Distribution Detection with Noise from Diffusion
Authors: Jingqiu Zhou, Aojun Zou, Hongshen Li
Categories: cs.CV cs.LG
\\
  Out-of-distribution (OOD) detection is a crucial part of deploying machine
learning models safely. It has been extensively studied with a plethora of
methods developed in the literature. This problem is tackled with an OOD score
computation, however, previous methods compute the OOD scores with limited
usage of the in-distribution dataset. For instance, the OOD scores are computed
with information from a small portion of the in-distribution data. Furthermore,
these methods encode images with a neural image encoder. The robustness of
these methods is rarely checked with respect to image encoders of different
training methods and architectures. In this work, we introduce the diffusion
process into the OOD task. The diffusion model integrates information on the
whole training set into the predicted noise vectors. What's more, we deduce a
closed-form solution for the noise vector (stable point). Then the noise vector
is converted into our OOD score, we test both the deep model predicted noise
vector and the closed-form noise vector on the OOD benchmarks \cite{openood}.
Our method outperforms previous OOD methods across all types of image encoders
(Table. \ref{main}). A $3.5\%$ performance gain is achieved with the MAE-based
image encoder. Moreover, we studied the robustness of OOD methods by applying
different types of image encoders. Some OOD methods failed to generalize well
when switching image encoders from ResNet to Vision Transformers, our method
performs exhibits good robustness with all the image encoders.
\\ ( https://arxiv.org/abs/2401.08689 ,  1423kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08691 (*cross-listing*)
Date: Sat, 13 Jan 2024 14:07:09 GMT   (2435kb)

Title: Towards Responsible AI in Banking: Addressing Bias for Fair
  Decision-Making
Authors: Alessandro Castelnovo
Categories: stat.ML cs.CY cs.LG stat.AP
Comments: Ph.D. Thesis
\\
  In an era characterized by the pervasive integration of artificial
intelligence into decision-making processes across diverse industries, the
demand for trust has never been more pronounced. This thesis embarks on a
comprehensive exploration of bias and fairness, with a particular emphasis on
their ramifications within the banking sector, where AI-driven decisions bear
substantial societal consequences. In this context, the seamless integration of
fairness, explainability, and human oversight is of utmost importance,
culminating in the establishment of what is commonly referred to as
"Responsible AI". This emphasizes the critical nature of addressing biases
within the development of a corporate culture that aligns seamlessly with both
AI regulations and universal human rights standards, particularly in the realm
of automated decision-making systems. Nowadays, embedding ethical principles
into the development, training, and deployment of AI models is crucial for
compliance with forthcoming European regulations and for promoting societal
good. This thesis is structured around three fundamental pillars: understanding
bias, mitigating bias, and accounting for bias. These contributions are
validated through their practical application in real-world scenarios, in
collaboration with Intesa Sanpaolo. This collaborative effort not only
contributes to our understanding of fairness but also provides practical tools
for the responsible implementation of AI-based decision-making systems. In line
with open-source principles, we have released Bias On Demand and FairView as
accessible Python packages, further promoting progress in the field of AI
fairness.
\\ ( https://arxiv.org/abs/2401.08691 ,  2435kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08700 (*cross-listing*)
Date: Sun, 14 Jan 2024 14:05:26 GMT   (18355kb,D)

Title: Computationally Efficient Optimisation of Elbow-Type Draft Tube Using
  Neural Network Surrogates
Authors: Ante Sikirica, Ivana Lu\v{c}in, Marta Alvir, Lado Kranj\v{c}evi\'c and
  Zoran \v{C}arija
Categories: math.OC cs.LG cs.NE
Comments: 41 pages, brief appendix
\\
  This study aims to provide a comprehensive assessment of single-objective and
multi-objective optimisation algorithms for the design of an elbow-type draft
tube, as well as to introduce a computationally efficient optimisation
workflow. The proposed workflow leverages deep neural network surrogates
trained on data obtained from numerical simulations. The use of surrogates
allows for a more flexible and faster evaluation of novel designs. The success
history-based adaptive differential evolution with linear reduction and the
multi-objective evolutionary algorithm based on decomposition were identified
as the best-performing algorithms and used to determine the influence of
different objectives in the single-objective optimisation and their combined
impact on the draft tube design in the multi-objective optimisation. The
results for the single-objective algorithm are consistent with those of the
multi-objective algorithm when the objectives are considered separately.
Multi-objective approach, however, should typically be chosen, especially for
computationally inexpensive surrogates. A multi-criteria decision analysis
method was used to obtain optimal multi-objective results, showing an
improvement of 1.5% and 17% for the pressure recovery factor and drag
coefficient, respectively. The difference between the predictions and the
numerical results is less than 0.5% for the pressure recovery factor and 3% for
the drag coefficient. As the demand for renewable energy continues to increase,
the relevance of data-driven optimisation workflows, as discussed in this
study, will become increasingly important, especially in the context of global
sustainability efforts.
\\ ( https://arxiv.org/abs/2401.08700 ,  18355kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08702 (*cross-listing*)
Date: Sun, 14 Jan 2024 23:19:21 GMT   (19221kb,D)

Title: Do We Really Even Need Data?
Authors: Kentaro Hoffman, Stephen Salerno, Awan Afiaz, Jeffrey T. Leek, Tyler
  H. McCormick
Categories: stat.ME cs.LG
\\
  As artificial intelligence and machine learning tools become more accessible,
and scientists face new obstacles to data collection (e.g. rising costs,
declining survey response rates), researchers increasingly use predictions from
pre-trained algorithms as outcome variables. Though appealing for financial and
logistical reasons, using standard tools for inference can misrepresent the
association between independent variables and the outcome of interest when the
true, unobserved outcome is replaced by a predicted value. In this paper, we
characterize the statistical challenges inherent to this so-called
``post-prediction inference'' problem and elucidate three potential sources of
error: (i) the relationship between predicted outcomes and their true,
unobserved counterparts, (ii) robustness of the machine learning model to
resampling or uncertainty about the training data, and (iii) appropriately
propagating not just bias but also uncertainty from predictions into the
ultimate inference procedure. We also contrast the framework for
post-prediction inference with classical work spanning several related fields,
including survey sampling, missing data, and semi-supervised learning. This
contrast elucidates the role of design in both classical and modern inference
problems.
\\ ( https://arxiv.org/abs/2401.08702 ,  19221kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08712 (*cross-listing*)
Date: Mon, 15 Jan 2024 17:51:14 GMT   (706kb)

Title: Survival Analysis of Young Triple-Negative Breast Cancer Patients
Authors: M. Mehdi Owrang O, Fariba Jafari Horestani, Ginger Schwarz
Categories: q-bio.QM cs.LG stat.AP
Comments: 31 Pages, 11 Figures, 7 Tables, Peer-reviewed article
\\
  Breast cancer prognosis is crucial for effective treatment, with the disease
more common in women over 40 years old but rare under 40 years old, where less
than 5 percent of cases occur in the U.S. Studies indicate a worse prognosis in
younger women, which varies by ethnicity. Breast cancers are classified based
on receptors like estrogen, progesterone, and HER2. Triple-negative breast
cancer (TNBC), lacking these receptors, accounts for about 15 percent of cases
and is more prevalent in younger patients, often resulting in poorer outcomes.
Nevertheless, the impact of age on TNBC prognosis remains unclear. Factors like
age, race, tumor grade, size, and lymph node status are studied for their role
in TNBC's clinical outcomes, but current research is inconclusive about
age-related differences. This study uses SEER data set to examine the influence
of younger age on survivability in TNBC patients, aiming to determine if age is
a significant prognostic factor. Our experimental results on SEER dataset
confirm the existing research reports that TNBC patients have worse prognosis
compared to non-TNBC based on age. Our main goal was to investigate whether
younger age has any significance on the survivability of TNBC patients.
Experimental results do not show that younger age has any significance on the
prognosis and survival rate of the TNBC patients
\\ ( https://arxiv.org/abs/2401.08712 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08723 (*cross-listing*)
Date: Tue, 16 Jan 2024 09:34:10 GMT   (1332kb,D)

Title: HierSFL: Local Differential Privacy-aided Split Federated Learning in
  Mobile Edge Computing
Authors: Minh K. Quan, Dinh C. Nguyen, Van-Dinh Nguyen, Mayuri Wijayasundara,
  Sujeeva Setunge, Pubudu N. Pathirana
Categories: cs.CR cs.CV cs.DC cs.LG
Comments: 6 Pages, 5 figures, IEEE Virtual Conference on Communications 2023
\\
  Federated Learning is a promising approach for learning from user data while
preserving data privacy. However, the high requirements of the model training
process make it difficult for clients with limited memory or bandwidth to
participate. To tackle this problem, Split Federated Learning is utilized,
where clients upload their intermediate model training outcomes to a cloud
server for collaborative server-client model training. This methodology
facilitates resource-constrained clients' participation in model training but
also increases the training time and communication overhead. To overcome these
limitations, we propose a novel algorithm, called Hierarchical Split Federated
Learning (HierSFL), that amalgamates models at the edge and cloud phases,
presenting qualitative directives for determining the best aggregation
timeframes to reduce computation and communication expenses. By implementing
local differential privacy at the client and edge server levels, we enhance
privacy during local model parameter updates. Our experiments using CIFAR-10
and MNIST datasets show that HierSFL outperforms standard FL approaches with
better training accuracy, training time, and communication-computing
trade-offs. HierSFL offers a promising solution to mobile edge computing's
challenges, ultimately leading to faster content delivery and improved mobile
service quality.
\\ ( https://arxiv.org/abs/2401.08723 ,  1332kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08734 (*cross-listing*)
Date: Tue, 16 Jan 2024 17:42:36 GMT   (15006kb,D)

Title: Bag of Tricks to Boost Adversarial Transferability
Authors: Zeliang Zhang, Rongyi Zhu, Wei Yao, Xiaosen Wang, Chenliang Xu
Categories: cs.CV cs.LG
\\
  Deep neural networks are widely known to be vulnerable to adversarial
examples. However, vanilla adversarial examples generated under the white-box
setting often exhibit low transferability across different models. Since
adversarial transferability poses more severe threats to practical
applications, various approaches have been proposed for better transferability,
including gradient-based, input transformation-based, and model-related
attacks, \etc. In this work, we find that several tiny changes in the existing
adversarial attacks can significantly affect the attack performance, \eg, the
number of iterations and step size. Based on careful studies of existing
adversarial attacks, we propose a bag of tricks to enhance adversarial
transferability, including momentum initialization, scheduled step size, dual
example, spectral-based input transformation, and several ensemble strategies.
Extensive experiments on the ImageNet dataset validate the high effectiveness
of our proposed tricks and show that combining them can further boost
adversarial transferability. Our work provides practical insights and
techniques to enhance adversarial transferability, and offers guidance to
improve the attack performance on the real-world application through simple
adjustments.
\\ ( https://arxiv.org/abs/2401.08734 ,  15006kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08735 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:03:07 GMT   (19962kb,D)

Title: A Framework for Scalable Ambient Air Pollution Concentration Estimation
Authors: Liam J Berrisford, Lucy S Neal, Helen J Buttery, Benjamin R Evans,
  Ronaldo Menezes
Categories: stat.AP cs.LG
Comments: Main: 27 pages, 11 figures, 6 tables. Supplementary: 32 pages, 21
  figures, 11 tables
\\
  Ambient air pollution remains a critical issue in the United Kingdom, where
data on air pollution concentrations form the foundation for interventions
aimed at improving air quality. However, the current air pollution monitoring
station network in the UK is characterized by spatial sparsity, heterogeneous
placement, and frequent temporal data gaps, often due to issues such as power
outages. We introduce a scalable data-driven supervised machine learning model
framework designed to address temporal and spatial data gaps by filling missing
measurements. This approach provides a comprehensive dataset for England
throughout 2018 at a 1kmx1km hourly resolution. Leveraging machine learning
techniques and real-world data from the sparsely distributed monitoring
stations, we generate 355,827 synthetic monitoring stations across the study
area, yielding data valued at approximately \pounds70 billion. Validation was
conducted to assess the model's performance in forecasting, estimating missing
locations, and capturing peak concentrations. The resulting dataset is of
particular interest to a diverse range of stakeholders engaged in downstream
assessments supported by outdoor air pollution concentration data for NO2, O3,
PM10, PM2.5, and SO2. This resource empowers stakeholders to conduct studies at
a higher resolution than was previously possible.
\\ ( https://arxiv.org/abs/2401.08735 ,  19962kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08738 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:31:23 GMT   (2463kb)

Title: Machine Learning-Based Analysis of Ebola Virus' Impact on Gene
  Expression in Nonhuman Primates
Authors: Mostafa Rezapour, Muhammad Khalid Khan Niazi, Hao Lu, Aarthi
  Narayanan, Metin Nafi Gurcan
Categories: q-bio.GN cs.LG
Comments: 28 pages, 8 figures, 2 tables
\\
  This study introduces the Supervised Magnitude-Altitude Scoring (SMAS)
methodology, a machine learning-based approach, for analyzing gene expression
data obtained from nonhuman primates (NHPs) infected with Ebola virus (EBOV).
We utilize a comprehensive dataset of NanoString gene expression profiles from
Ebola-infected NHPs, deploying the SMAS system for nuanced host-pathogen
interaction analysis. SMAS effectively combines gene selection based on
statistical significance and expression changes, employing linear classifiers
such as logistic regression to accurately differentiate between RT-qPCR
positive and negative NHP samples. A key finding of our research is the
identification of IFI6 and IFI27 as critical biomarkers, demonstrating
exceptional predictive performance with 100% accuracy and Area Under the Curve
(AUC) metrics in classifying various stages of Ebola infection. Alongside IFI6
and IFI27, genes, including MX1, OAS1, and ISG15, were significantly
upregulated, highlighting their essential roles in the immune response to EBOV.
Our results underscore the efficacy of the SMAS method in revealing complex
genetic interactions and response mechanisms during EBOV infection. This
research provides valuable insights into EBOV pathogenesis and aids in
developing more precise diagnostic tools and therapeutic strategies to address
EBOV infection in particular and viral infection in general.
\\ ( https://arxiv.org/abs/2401.08738 ,  2463kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08740 (*cross-listing*)
Date: Tue, 16 Jan 2024 18:55:25 GMT   (16564kb,D)

Title: SiT: Exploring Flow and Diffusion-based Generative Models with Scalable
  Interpolant Transformers
Authors: Nanye Ma, Mark Goldstein, Michael S. Albergo, Nicholas M. Boffi, Eric
  Vanden-Eijnden, and Saining Xie
Categories: cs.CV cs.LG
Comments: Code available: https://github.com/willisma/SiT
\\
  We present Scalable Interpolant Transformers (SiT), a family of generative
models built on the backbone of Diffusion Transformers (DiT). The interpolant
framework, which allows for connecting two distributions in a more flexible way
than standard diffusion models, makes possible a modular study of various
design choices impacting generative models built on dynamical transport: using
discrete vs. continuous time learning, deciding the objective for the model to
learn, choosing the interpolant connecting the distributions, and deploying a
deterministic or stochastic sampler. By carefully introducing the above
ingredients, SiT surpasses DiT uniformly across model sizes on the conditional
ImageNet 256x256 benchmark using the exact same backbone, number of parameters,
and GFLOPs. By exploring various diffusion coefficients, which can be tuned
separately from learning, SiT achieves an FID-50K score of 2.06.
\\ ( https://arxiv.org/abs/2401.08740 ,  16564kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08763 (*cross-listing*)
Date: Tue, 16 Jan 2024 19:00:03 GMT   (25624kb,D)

Title: The weird and the wonderful in our Solar System: Searching for
  serendipity in the Legacy Survey of Space and Time
Authors: Brian Rogers, Chris J. Lintott, Steve Croft, Megan E. Schwamb, James
  R. A. Davenport
Categories: astro-ph.EP astro-ph.IM cs.LG
Comments: Accepted by AJ
\\
  We present a novel method for anomaly detection in Solar System object data,
in preparation for the Legacy Survey of Space and Time. We train a deep
autoencoder for anomaly detection and use the learned latent space to search
for other interesting objects. We demonstrate the efficacy of the autoencoder
approach by finding interesting examples, such as interstellar objects, and
show that using the autoencoder, further examples of interesting classes can be
found. We also investigate the limits of classic unsupervised approaches to
anomaly detection through the generation of synthetic anomalies and evaluate
the feasibility of using a supervised learning approach. Future work should
consider expanding the feature space to increase the variety of anomalies that
can be uncovered during the survey using an autoencoder.
\\ ( https://arxiv.org/abs/2401.08763 ,  25624kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08777 (*cross-listing*)
Date: Tue, 16 Jan 2024 19:00:20 GMT   (2630kb,D)

Title: Robust Anomaly Detection for Particle Physics Using Multi-Background
  Representation Learning
Authors: Abhijith Gandrakota, Lily Zhang, Aahlad Puli, Kyle Cranmer, Jennifer
  Ngadiuba, Rajesh Ranganath, and Nhan Tran
Categories: hep-ex cs.LG hep-ph physics.data-an
Report-no: FERMILAB-PUB-23-675-CMS-CSAID
\\
  Anomaly, or out-of-distribution, detection is a promising tool for aiding
discoveries of new particles or processes in particle physics. In this work, we
identify and address two overlooked opportunities to improve anomaly detection
for high-energy physics. First, rather than train a generative model on the
single most dominant background process, we build detection algorithms using
representation learning from multiple background types, thus taking advantage
of more information to improve estimation of what is relevant for detection.
Second, we generalize decorrelation to the multi-background setting, thus
directly enforcing a more complete definition of robustness for anomaly
detection. We demonstrate the benefit of the proposed robust multi-background
anomaly detection algorithms on a high-dimensional dataset of particle decays
at the Large Hadron Collider.
\\ ( https://arxiv.org/abs/2401.08777 ,  2630kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08818 (*cross-listing*)
Date: Tue, 16 Jan 2024 20:41:11 GMT   (7045kb,D)

Title: Link Me Baby One More Time: Social Music Discovery on Spotify
Authors: Shazia'Ayn Babul, Desislava Hristova, Antonio Lima, Renaud Lambiotte,
  Mariano Beguerisse-D\'iaz
Categories: cs.SI cs.IR cs.LG physics.soc-ph
\\
  We explore the social and contextual factors that influence the outcome of
person-to-person music recommendations and discovery. Specifically, we use data
from Spotify to investigate how a link sent from one user to another results in
the receiver engaging with the music of the shared artist. We consider several
factors that may influence this process, such as the strength of the
sender-receiver relationship, the user's role in the Spotify social network,
their music social cohesion, and how similar the new artist is to the
receiver's taste. We find that the receiver of a link is more likely to engage
with a new artist when (1) they have similar music taste to the sender and the
shared track is a good fit for their taste, (2) they have a stronger and more
intimate tie with the sender, and (3) the shared artist is popular with the
receiver's connections. Finally, we use these findings to build a Random Forest
classifier to predict whether a shared music track will result in the
receiver's engagement with the shared artist. This model elucidates which type
of social and contextual features are most predictive, although peak
performance is achieved when a diverse set of features are included. These
findings provide new insights into the multifaceted mechanisms underpinning the
interplay between music discovery and social processes.
\\ ( https://arxiv.org/abs/2401.08818 ,  7045kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08821 (*cross-listing*)
Date: Tue, 16 Jan 2024 20:47:19 GMT   (663kb)

Title: Surface-Enhanced Raman Spectroscopy and Transfer Learning Toward
  Accurate Reconstruction of the Surgical Zone
Authors: Ashutosh Raman, Ren A. Odion, Kent K. Yamamoto, Weston Ross, Tuan
  Vo-Dinh, Patrick J. Codd
Categories: eess.IV cs.LG cs.RO
Comments: Accepted to Hamlyn Symposium on Medical Robotics, 2023
\\
  Raman spectroscopy, a photonic modality based on the inelastic backscattering
of coherent light, is a valuable asset to the intraoperative sensing space,
offering non-ionizing potential and highly-specific molecular fingerprint-like
spectroscopic signatures that can be used for diagnosis of pathological tissue
in the dynamic surgical field. Though Raman suffers from weakness in intensity,
Surface-Enhanced Raman Spectroscopy (SERS), which uses metal nanostructures to
amplify Raman signals, can achieve detection sensitivities that rival
traditional photonic modalities. In this study, we outline a robotic Raman
system that can reliably pinpoint the location and boundaries of a tumor
embedded in healthy tissue, modeled here as a tissue-mimicking phantom with
selectively infused Gold Nanostar regions. Further, due to the relative dearth
of collected biological SERS or Raman data, we implement transfer learning to
achieve 100% validation classification accuracy for Gold Nanostars compared to
Control Agarose, thus providing a proof-of-concept for Raman-based deep
learning training pipelines. We reconstruct a surgical field of 30x60mm in 10.2
minutes, and achieve 98.2% accuracy, preserving relative measurements between
features in the phantom. We also achieve an 84.3% Intersection-over-Union
score, which is the extent of overlap between the ground truth and predicted
reconstructions. Lastly, we also demonstrate that the Raman system and
classification algorithm do not discern based on sample color, but instead on
presence of SERS agents. This study provides a crucial step in the translation
of intelligent Raman systems in intraoperative oncological spaces.
\\ ( https://arxiv.org/abs/2401.08821 ,  663kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08847 (*cross-listing*)
Date: Tue, 16 Jan 2024 21:45:08 GMT   (367kb,D)

Title: RIDGE: Reproducibility, Integrity, Dependability, Generalizability, and
  Efficiency Assessment of Medical Image Segmentation Models
Authors: Farhad Maleki, Linda Moy, Reza Forghani, Tapotosh Ghosh, Katie Ovens,
  Steve Langer, Pouria Rouzrokh, Bardia Khosravi, Ali Ganjizadeh, Daniel
  Warren, Roxana Daneshjou, Mana Moassefi, Atlas Haddadi Avval, Susan Sotardi,
  Neil Tenenholtz, Felipe Kitamura, Timothy Kline
Categories: eess.IV cs.CV cs.LG
Comments: 20 pages, 1 Figure, 1 Table
\\
  Deep learning techniques, despite their potential, often suffer from a lack
of reproducibility and generalizability, impeding their clinical adoption.
Image segmentation is one of the critical tasks in medical image analysis, in
which one or several regions/volumes of interest should be annotated. This
paper introduces the RIDGE checklist, a framework for assessing the
Reproducibility, Integrity, Dependability, Generalizability, and Efficiency of
deep learning-based medical image segmentation models. The checklist serves as
a guide for researchers to enhance the quality and transparency of their work,
ensuring that segmentation models are not only scientifically sound but also
clinically relevant.
\\ ( https://arxiv.org/abs/2401.08847 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08859 (*cross-listing*)
Date: Tue, 16 Jan 2024 22:20:36 GMT   (2767kb,D)

Title: Shabari: Delayed Decision-Making for Faster and Efficient Serverless
  Function
Authors: Prasoon Sinha and Kostis Kaffes and Neeraja J. Yadwadkar
Categories: cs.DC cs.LG
Comments: 17 pages, 14 figures
\\
  Serverless computing relieves developers from the burden of resource
management, thus providing ease-of-use to the users and the opportunity to
optimize resource utilization for the providers. However, today's serverless
systems lack performance guarantees for function invocations, thus limiting
support for performance-critical applications: we observed severe performance
variability (up to 6x). Providers lack visibility into user functions and hence
find it challenging to right-size them: we observed heavy resource
underutilization (up to 80%). To understand the causes behind the performance
variability and underutilization, we conducted a measurement study of commonly
deployed serverless functions and learned that the function performance and
resource utilization depend crucially on function semantics and inputs. Our key
insight is to delay making resource allocation decisions until after the
function inputs are available. We introduce Shabari, a resource management
framework for serverless systems that makes decisions as late as possible to
right-size each invocation to meet functions' performance objectives (SLOs) and
improve resource utilization. Shabari uses an online learning agent to
right-size each function invocation based on the features of the function input
and makes cold-start-aware scheduling decisions. For a range of serverless
functions and inputs, Shabari reduces SLO violations by 11-73% while not
wasting any vCPUs and reducing wasted memory by 64-94% in the median case,
compared to state-of-the-art systems, including Aquatope, Parrotfish, and
Cypress.
\\ ( https://arxiv.org/abs/2401.08859 ,  2767kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08861 (*cross-listing*)
Date: Tue, 16 Jan 2024 22:23:27 GMT   (981kb,D)

Title: Semi-Supervised Learning Approach for Efficient Resource Allocation with
  Network Slicing in O-RAN
Authors: Salar Nouri, Mojdeh Karbalaee Motalleb, Vahid Shah-Mansouri, Seyed
  Pooya Shariatpanahi
Categories: cs.NI cs.LG cs.MS cs.NA math.NA
Comments: Submitted to IEEE Transactions on Network and Service Management
\\
  The Open Radio Access Network (O-RAN) technology has emerged as a promising
solution for network operators, providing them with an open and favorable
environment. Ensuring effective coordination of x-applications (xAPPs) is
crucial to enhance flexibility and optimize network performance within the
O-RAN. In this paper, we introduce an innovative approach to the resource
allocation problem, aiming to coordinate multiple independent xAPPs for network
slicing and resource allocation in O-RAN. Our proposed method focuses on
maximizing the weighted throughput among user equipments (UE), as well as
allocating physical resource blocks (PRBs). We prioritize two service types,
namely enhanced Mobile Broadband and Ultra Reliable Low Latency Communication.
To achieve this, we have designed two xAPPs: a power control xAPP for each UE
and a PRB allocation xAPP. The proposed method consists of a two-part training
phase, where the first part uses supervised learning with a Variational
Autoencoder trained to regress the power transmission as well as the user
association and PRB allocation decisions, and the second part uses unsupervised
learning with a contrastive loss approach to improve the generalization and
robustness of the model. We evaluate the performance of our proposed method by
comparing its results to those obtained from an exhaustive search algorithm,
deep Q-network algorithm, and by reporting performance metrics for the
regression task. We also evaluate the proposed model's performance in different
scenarios among the service types. The results show that the proposed method is
a more efficient and effective solution for network slicing problems compared
to state-of-the-art methods.
\\ ( https://arxiv.org/abs/2401.08861 ,  981kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08864 (*cross-listing*)
Date: Tue, 16 Jan 2024 22:36:12 GMT   (1382kb,D)

Title: Binaural Angular Separation Network
Authors: Yang Yang, George Sung, Shao-Fu Shih, Hakan Erdogan, Chehung Lee,
  Matthias Grundmann
Categories: eess.AS cs.LG cs.SD
Comments: Accepted to ICASSP 2024
\\
  We propose a neural network model that can separate target speech sources
from interfering sources at different angular regions using two microphones.
The model is trained with simulated room impulse responses (RIRs) using
omni-directional microphones without needing to collect real RIRs. By relying
on specific angular regions and multiple room simulations, the model utilizes
consistent time difference of arrival (TDOA) cues, or what we call delay
contrast, to separate target and interference sources while remaining robust in
various reverberation environments. We demonstrate the model is not only
generalizable to a commercially available device with a slightly different
microphone geometry, but also outperforms our previous work which uses one
additional microphone on the same device. The model runs in real-time on-device
and is suitable for low-latency streaming applications such as telephony and
video conferencing.
\\ ( https://arxiv.org/abs/2401.08864 ,  1382kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08865 (*cross-listing*)
Date: Tue, 16 Jan 2024 22:36:23 GMT   (1568kb,D)

Title: The Effect of Intrinsic Dataset Properties on Generalization: Unraveling
  Learning Differences Between Natural and Medical Images
Authors: Nicholas Konz, Maciej A. Mazurowski
Categories: cs.CV cs.LG eess.IV stat.ML
Comments: ICLR 2024. Code:
  https://github.com/mazurowski-lab/intrinsic-properties
\\
  This paper investigates discrepancies in how neural networks learn from
different imaging domains, which are commonly overlooked when adopting computer
vision techniques from the domain of natural images to other specialized
domains such as medical images. Recent works have found that the generalization
error of a trained network typically increases with the intrinsic dimension
($d_{data}$) of its training set. Yet, the steepness of this relationship
varies significantly between medical (radiological) and natural imaging
domains, with no existing theoretical explanation. We address this gap in
knowledge by establishing and empirically validating a generalization scaling
law with respect to $d_{data}$, and propose that the substantial scaling
discrepancy between the two considered domains may be at least partially
attributed to the higher intrinsic "label sharpness" ($K_F$) of medical imaging
datasets, a metric which we propose. Next, we demonstrate an additional benefit
of measuring the label sharpness of a training set: it is negatively correlated
with the trained model's adversarial robustness, which notably leads to models
for medical images having a substantially higher vulnerability to adversarial
attack. Finally, we extend our $d_{data}$ formalism to the related metric of
learned representation intrinsic dimension ($d_{repr}$), derive a
generalization scaling law with respect to $d_{repr}$, and show that $d_{data}$
serves as an upper bound for $d_{repr}$. Our theoretical results are supported
by thorough experiments with six models and eleven natural and medical imaging
datasets over a range of training set sizes. Our findings offer insights into
the influence of intrinsic dataset properties on generalization, representation
learning, and robustness in deep neural networks.
\\ ( https://arxiv.org/abs/2401.08865 ,  1568kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08876 (*cross-listing*)
Date: Tue, 16 Jan 2024 23:19:30 GMT   (5145kb,D)

Title: Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image
  Labeling
Authors: Dongping Zhang, Angelos Chatzimparmpas, Negar Kamali, and Jessica
  Hullman
Categories: cs.HC cs.CV cs.LG
Comments: 28 pages, 11 figures, 8 tables
\\
  As deep neural networks are more commonly deployed in high-stakes domains,
their lack of interpretability makes uncertainty quantification challenging. We
investigate the effects of presenting conformal prediction
sets$\unicode{x2013}$a method for generating valid confidence sets in
distribution-free uncertainty quantification$\unicode{x2013}$to express
uncertainty in AI-advised decision-making. Through a large pre-registered
experiment, we compare the utility of conformal prediction sets to displays of
Top-1 and Top-k predictions for AI-advised image labeling. We find that the
utility of prediction sets for accuracy varies with the difficulty of the task:
while they result in accuracy on par with or less than Top-1 and Top-k displays
for easy images, prediction sets excel at assisting humans in labeling
out-of-distribution (OOD) images especially when the set size is small. Our
results empirically pinpoint the practical challenges of conformal prediction
sets and provide implications on how to incorporate them for real-world
decision-making.
\\ ( https://arxiv.org/abs/2401.08876 ,  5145kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08889 (*cross-listing*)
Date: Wed, 17 Jan 2024 00:12:13 GMT   (163kb,D)

Title: On the Effect of Data-Augmentation on Local Embedding Properties in the
  Contrastive Learning of Music Audio Representations
Authors: Matthew C. McCallum, Matthew E. P. Davies, Florian Henkel, Jaehun Kim,
  Samuel E. Sandberg
Categories: cs.SD cs.IR cs.LG cs.MM eess.AS
Comments: Accepted to the International Conference on Acoustics, Speech and
  Signal Processing (ICASSP) 2024
\\
  Audio embeddings are crucial tools in understanding large catalogs of music.
Typically embeddings are evaluated on the basis of the performance they provide
in a wide range of downstream tasks, however few studies have investigated the
local properties of the embedding spaces themselves which are important in
nearest neighbor algorithms, commonly used in music search and recommendation.
In this work we show that when learning audio representations on music datasets
via contrastive learning, musical properties that are typically homogeneous
within a track (e.g., key and tempo) are reflected in the locality of
neighborhoods in the resulting embedding space. By applying appropriate data
augmentation strategies, localisation of such properties can not only be
reduced but the localisation of other attributes is increased. For example,
locality of features such as pitch and tempo that are less relevant to
non-expert listeners, may be mitigated while improving the locality of more
salient features such as genre and mood, achieving state-of-the-art performance
in nearest neighbor retrieval accuracy. Similarly, we show that the optimal
selection of data augmentation strategies for contrastive learning of music
audio embeddings is dependent on the downstream task, highlighting this as an
important embedding design decision.
\\ ( https://arxiv.org/abs/2401.08889 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08891 (*cross-listing*)
Date: Wed, 17 Jan 2024 00:15:16 GMT   (727kb,D)

Title: Tempo estimation as fully self-supervised binary classification
Authors: Florian Henkel, Jaehun Kim, Matthew C. McCallum, Samuel E. Sandberg,
  Matthew E. P. Davies
Categories: cs.SD cs.LG eess.AS
Comments: Accepted to the International Conference on Acoustics, Speech and
  Signal Processing (ICASSP) 2024
\\
  This paper addresses the problem of global tempo estimation in musical audio.
Given that annotating tempo is time-consuming and requires certain musical
expertise, few publicly available data sources exist to train machine learning
models for this task. Towards alleviating this issue, we propose a fully
self-supervised approach that does not rely on any human labeled data. Our
method builds on the fact that generic (music) audio embeddings already encode
a variety of properties, including information about tempo, making them easily
adaptable for downstream tasks. While recent work in self-supervised tempo
estimation aimed to learn a tempo specific representation that was subsequently
used to train a supervised classifier, we reformulate the task into the binary
classification problem of predicting whether a target track has the same or a
different tempo compared to a reference. While the former still requires
labeled training data for the final classification model, our approach uses
arbitrary unlabeled music data in combination with time-stretching for model
training as well as a small set of synthetically created reference samples for
predicting the final tempo. Evaluation of our approach in comparison with the
state-of-the-art reveals highly competitive performance when the constraint of
finding the precise tempo octave is relaxed.
\\ ( https://arxiv.org/abs/2401.08891 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08902 (*cross-listing*)
Date: Wed, 17 Jan 2024 01:06:22 GMT   (246kb,D)

Title: Similar but Faster: Manipulation of Tempo in Music Audio Embeddings for
  Tempo Prediction and Search
Authors: Matthew C. McCallum, Florian Henkel, Jaehun Kim, Samuel E. Sandberg,
  Matthew E. P. Davies
Categories: cs.SD cs.DL cs.IR cs.LG eess.AS
Comments: Accepted to the International Conference on Acoustics, Speech and
  Signal Processing (ICASSP) 2024
\\
  Audio embeddings enable large scale comparisons of the similarity of audio
files for applications such as search and recommendation. Due to the
subjectivity of audio similarity, it can be desirable to design systems that
answer not only whether audio is similar, but similar in what way (e.g., wrt.
tempo, mood or genre). Previous works have proposed disentangled embedding
spaces where subspaces representing specific, yet possibly correlated,
attributes can be weighted to emphasize those attributes in downstream tasks.
However, no research has been conducted into the independence of these
subspaces, nor their manipulation, in order to retrieve tracks that are similar
but different in a specific way. Here, we explore the manipulation of tempo in
embedding spaces as a case-study towards this goal. We propose tempo
translation functions that allow for efficient manipulation of tempo within a
pre-existing embedding space whilst maintaining other properties such as genre.
As this translation is specific to tempo it enables retrieval of tracks that
are similar but have specifically different tempi. We show that such a function
can be used as an efficient data augmentation strategy for both training of
downstream tempo predictors, and improved nearest neighbor retrieval of
properties largely independent of tempo.
\\ ( https://arxiv.org/abs/2401.08902 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08903 (*cross-listing*)
Date: Wed, 17 Jan 2024 01:10:17 GMT   (278kb,D)

Title: PPR: Enhancing Dodging Attacks while Maintaining Impersonation Attacks
  on Face Recognition Systems
Authors: Fengfan Zhou, Heifei Ling
Categories: cs.CV cs.LG
\\
  Adversarial Attacks on Face Recognition (FR) encompass two types:
impersonation attacks and evasion attacks. We observe that achieving a
successful impersonation attack on FR does not necessarily ensure a successful
dodging attack on FR in the black-box setting. Introducing a novel attack
method named Pre-training Pruning Restoration Attack (PPR), we aim to enhance
the performance of dodging attacks whilst avoiding the degradation of
impersonation attacks. Our method employs adversarial example pruning, enabling
a portion of adversarial perturbations to be set to zero, while tending to
maintain the attack performance. By utilizing adversarial example pruning, we
can prune the pre-trained adversarial examples and selectively free up certain
adversarial perturbations. Thereafter, we embed adversarial perturbations in
the pruned area, which enhances the dodging performance of the adversarial face
examples. The effectiveness of our proposed attack method is demonstrated
through our experimental results, showcasing its superior performance.
\\ ( https://arxiv.org/abs/2401.08903 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08908 (*cross-listing*)
Date: Wed, 17 Jan 2024 01:32:45 GMT   (1005kb,D)

Title: Herding LLaMaS: Using LLMs as an OS Module
Authors: Aditya K Kamath and Sujay Yadalam
Categories: cs.OS cs.LG
Comments: ASPLOS 2023, Wild and Crazy Ideas session
\\
  Computer systems are becoming increasingly heterogeneous with the emergence
of new memory technologies and compute devices. GPUs alongside CPUs have become
commonplace and CXL is poised to be a mainstay of cloud systems. The operating
system is responsible for managing these hardware resources, requiring
modification every time a new device is released. Years of research and
development are sunk into tuning the OS for high performance with each new
heterogeneous device. With the recent explosion in memory technologies and
domain-specific accelerators, it would be beneficial to have an OS that could
provide high performance for new devices without significant effort.
  We propose LLaMaS which can adapt to new devices easily. LLaMaS uses Large
Language Models (LLMs) to extract the useful features of new devices from their
textual description and uses these features to make operating system decisions
at runtime. Adding support to LLaMaS for a new device is as simple as
describing the system and new device properties in plaintext.
  LLaMaS reduces the burden on system administrators to enable easy integration
of new devices into production systems.
  Preliminary evaluation using ChatGPT shows that LLMs are capable of
extracting device features from text and make correct OS decisions based on
those features.
\\ ( https://arxiv.org/abs/2401.08908 ,  1005kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08947 (*cross-listing*)
Date: Wed, 17 Jan 2024 03:44:27 GMT   (1294kb)

Title: AntiPhishStack: LSTM-based Stacked Generalization Model for Optimized
  Phishing URLs Detection
Authors: Saba Aslam, Hafsa Aslam, Arslan Manzoor, Chen Hui, Abdur Rasool
Categories: cs.CR cs.LG
\\
  The escalating reliance on revolutionary online web services has introduced
heightened security risks, with persistent challenges posed by phishing despite
extensive security measures. Traditional phishing systems, reliant on machine
learning and manual features, struggle with evolving tactics. Recent advances
in deep learning offer promising avenues for tackling novel phishing challenges
and malicious URLs. This paper introduces a two-phase stack generalized model
named AntiPhishStack, designed to detect phishing sites. The model leverages
the learning of URLs and character-level TF-IDF features symmetrically,
enhancing its ability to combat emerging phishing threats. In Phase I, features
are trained on a base machine learning classifier, employing K-fold
cross-validation for robust mean prediction. Phase II employs a two-layered
stacked-based LSTM network with five adaptive optimizers for dynamic
compilation, ensuring premier prediction on these features. Additionally, the
symmetrical predictions from both phases are optimized and integrated to train
a meta-XGBoost classifier, contributing to a final robust prediction. The
significance of this work lies in advancing phishing detection with
AntiPhishStack, operating without prior phishing-specific feature knowledge.
Experimental validation on two benchmark datasets, comprising benign and
phishing or malicious URLs, demonstrates the model's exceptional performance,
achieving a notable 96.04% accuracy compared to existing studies. This research
adds value to the ongoing discourse on symmetry and asymmetry in information
security and provides a forward-thinking solution for enhancing network
security in the face of evolving cyber threats.
\\ ( https://arxiv.org/abs/2401.08947 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08962 (*cross-listing*)
Date: Wed, 17 Jan 2024 04:21:04 GMT   (1640kb,D)

Title: DOO-RE: A dataset of ambient sensors in a meeting room for activity
  recognition
Authors: Hyunju Kim and Geon Kim and Taehoon Lee and Kisoo Kim and Dongman Lee
Categories: cs.HC cs.LG cs.SD eess.AS
\\
  With the advancement of IoT technology, recognizing user activities with
machine learning methods is a promising way to provide various smart services
to users. High-quality data with privacy protection is essential for deploying
such services in the real world. Data streams from surrounding ambient sensors
are well suited to the requirement. Existing ambient sensor datasets only
support constrained private spaces and those for public spaces have yet to be
explored despite growing interest in research on them. To meet this need, we
build a dataset collected from a meeting room equipped with ambient sensors.
The dataset, DOO-RE, includes data streams from various ambient sensor types
such as Sound and Projector. Each sensor data stream is segmented into activity
units and multiple annotators provide activity labels through a
cross-validation annotation process to improve annotation quality. We finally
obtain 9 types of activities. To our best knowledge, DOO-RE is the first
dataset to support the recognition of both single and group activities in a
real meeting room with reliable annotations.
\\ ( https://arxiv.org/abs/2401.08962 ,  1640kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09050 (*cross-listing*)
Date: Wed, 17 Jan 2024 08:32:07 GMT   (11874kb,D)

Title: Consistent3D: Towards Consistent High-Fidelity Text-to-3D Generation
  with Deterministic Sampling Prior
Authors: Zike Wu, Pan Zhou, Xuanyu Yi, Xiaoding Yuan, Hanwang Zhang
Categories: cs.CV cs.LG
\\
  Score distillation sampling (SDS) and its variants have greatly boosted the
development of text-to-3D generation, but are vulnerable to geometry collapse
and poor textures yet. To solve this issue, we first deeply analyze the SDS and
find that its distillation sampling process indeed corresponds to the
trajectory sampling of a stochastic differential equation (SDE): SDS samples
along an SDE trajectory to yield a less noisy sample which then serves as a
guidance to optimize a 3D model. However, the randomness in SDE sampling often
leads to a diverse and unpredictable sample which is not always less noisy, and
thus is not a consistently correct guidance, explaining the vulnerability of
SDS. Since for any SDE, there always exists an ordinary differential equation
(ODE) whose trajectory sampling can deterministically and consistently converge
to the desired target point as the SDE, we propose a novel and effective
"Consistent3D" method that explores the ODE deterministic sampling prior for
text-to-3D generation. Specifically, at each training iteration, given a
rendered image by a 3D model, we first estimate its desired 3D score function
by a pre-trained 2D diffusion model, and build an ODE for trajectory sampling.
Next, we design a consistency distillation sampling loss which samples along
the ODE trajectory to generate two adjacent samples and uses the less noisy
sample to guide another more noisy one for distilling the deterministic prior
into the 3D model. Experimental results show the efficacy of our Consistent3D
in generating high-fidelity and diverse 3D objects and large-scale scenes, as
shown in Fig. 1. The codes are available at
https://github.com/sail-sg/Consistent3D.
\\ ( https://arxiv.org/abs/2401.09050 ,  11874kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09184 (*cross-listing*)
Date: Wed, 17 Jan 2024 12:50:50 GMT   (127kb,D)

Title: A Two-Scale Complexity Measure for Deep Learning Models
Authors: Massimiliano Datres, Gian Paolo Leonardi, Alessio Figalli, David
  Sutter
Categories: stat.ML cs.LG
\\
  We introduce a novel capacity measure 2sED for statistical models based on
the effective dimension. The new quantity provably bounds the generalization
error under mild assumptions on the model. Furthermore, simulations on standard
data sets and popular model architectures show that 2sED correlates well with
the training error. For Markovian models, we show how to efficiently
approximate 2sED from below through a layerwise iterative approach, which
allows us to tackle deep learning models with a large number of parameters.
Simulation results suggest that the approximation is good for different
prominent models and data sets.
\\ ( https://arxiv.org/abs/2401.09184 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09190 (*cross-listing*)
Date: Wed, 17 Jan 2024 13:00:57 GMT   (5764kb,D)

Title: Exploring the Role of Convolutional Neural Networks (CNN) in Dental
  Radiography Segmentation: A Comprehensive Systematic Literature Review
Authors: Walid Brahmi and Imen Jdey and Fadoua Drira
Categories: cs.CV cs.LG
\\
  In the field of dentistry, there is a growing demand for increased precision
in diagnostic tools, with a specific focus on advanced imaging techniques such
as computed tomography, cone beam computed tomography, magnetic resonance
imaging, ultrasound, and traditional intra-oral periapical X-rays. Deep
learning has emerged as a pivotal tool in this context, enabling the
implementation of automated segmentation techniques crucial for extracting
essential diagnostic data. This integration of cutting-edge technology
addresses the urgent need for effective management of dental conditions, which,
if left undetected, can have a significant impact on human health. The
impressive track record of deep learning across various domains, including
dentistry, underscores its potential to revolutionize early detection and
treatment of oral health issues. Objective: Having demonstrated significant
results in diagnosis and prediction, deep convolutional neural networks (CNNs)
represent an emerging field of multidisciplinary research. The goals of this
study were to provide a concise overview of the state of the art, standardize
the current debate, and establish baselines for future research. Method: In
this study, a systematic literature review is employed as a methodology to
identify and select relevant studies that specifically investigate the deep
learning technique for dental imaging analysis. This study elucidates the
methodological approach, including the systematic collection of data,
statistical analysis, and subsequent dissemination of outcomes. Conclusion:
This work demonstrates how Convolutional Neural Networks (CNNs) can be employed
to analyze images, serving as effective tools for detecting dental pathologies.
Although this research acknowledged some limitations, CNNs utilized for
segmenting and categorizing teeth exhibited their highest level of performance
overall.
\\ ( https://arxiv.org/abs/2401.09190 ,  5764kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09200 (*cross-listing*)
Date: Wed, 17 Jan 2024 13:25:32 GMT   (25935kb,D)

Title: A Real-Time Lyrics Alignment System Using Chroma And Phonetic Features
  For Classical Vocal Performance
Authors: Jiyun Park, Sangeon Yong, Taegyun Kwon, and Juhan Nam
Categories: cs.SD cs.LG eess.AS
Comments: To Appear IEEE ICASSP 2024
\\
  The goal of real-time lyrics alignment is to take live singing audio as input
and to pinpoint the exact position within given lyrics on the fly. The task can
benefit real-world applications such as the automatic subtitling of live
concerts or operas. However, designing a real-time model poses a great
challenge due to the constraints of only using past input and operating within
a minimal latency. Furthermore, due to the lack of datasets for real-time
models for lyrics alignment, previous studies have mostly evaluated with
private in-house datasets, resulting in a lack of standard evaluation methods.
This paper presents a real-time lyrics alignment system for classical vocal
performances with two contributions. First, we improve the lyrics alignment
algorithm by finding an optimal combination of chromagram and phonetic
posteriorgram (PPG) that capture melodic and phonetics features of the singing
voice, respectively. Second, we recast the Schubert Winterreise Dataset (SWD)
which contains multiple performance renditions of the same pieces as an
evaluation set for the real-time lyrics alignment.
\\ ( https://arxiv.org/abs/2401.09200 ,  25935kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09274 (*cross-listing*)
Date: Wed, 17 Jan 2024 15:25:50 GMT   (512kb)

Title: Avoiding strict saddle points of nonconvex regularized problems
Authors: Luwei Bai
Categories: math.OC cs.LG
Comments: 24 pages
\\
  We introduce a strict saddle property for $\ell_p$ regularized functions, and
propose an iterative reweighted $\ell_1$ algorithm to solve the $\ell_p$
regularized problems. The algorithm is guaranteed to converge only to local
minimizers when randomly initialized. The strict saddle property is shown
generic on these sparse optimization problems. Those analyses as well as the
proposed algorithm can be easily extended to general nonconvex regularized
problems.
\\ ( https://arxiv.org/abs/2401.09274 ,  512kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09339 (*cross-listing*)
Date: Wed, 17 Jan 2024 17:01:08 GMT   (612kb,D)

Title: Central Limit Theorem for Two-Timescale Stochastic Approximation with
  Markovian Noise: Theory and Applications
Authors: Jie Hu, Vishwaraj Doshi, Do Young Eun
Categories: stat.ML cs.LG math.OC
\\
  Two-timescale stochastic approximation (TTSA) is among the most general
frameworks for iterative stochastic algorithms. This includes well-known
stochastic optimization methods such as SGD variants and those designed for
bilevel or minimax problems, as well as reinforcement learning like the family
of gradient-based temporal difference (GTD) algorithms. In this paper, we
conduct an in-depth asymptotic analysis of TTSA under controlled Markovian
noise via central limit theorem (CLT), uncovering the coupled dynamics of TTSA
influenced by the underlying Markov chain, which has not been addressed by
previous CLT results of TTSA only with Martingale difference noise. Building
upon our CLT, we expand its application horizon of efficient sampling
strategies from vanilla SGD to a wider TTSA context in distributed learning,
thus broadening the scope of Hu et al. (2022). In addition, we leverage our CLT
result to deduce the statistical properties of GTD algorithms with nonlinear
function approximation using Markovian samples and show their identical
asymptotic performance, a perspective not evident from current finite-time
bounds.
\\ ( https://arxiv.org/abs/2401.09339 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09346 (*cross-listing*)
Date: Wed, 17 Jan 2024 17:11:45 GMT   (9530kb,D)

Title: High Confidence Level Inference is Almost Free using Parallel Stochastic
  Optimization
Authors: Wanrong Zhu, Zhipeng Lou, Ziyang Wei, Wei Biao Wu
Categories: stat.ML cs.LG
\\
  Uncertainty quantification for estimation through stochastic optimization
solutions in an online setting has gained popularity recently. This paper
introduces a novel inference method focused on constructing confidence
intervals with efficient computation and fast convergence to the nominal level.
Specifically, we propose to use a small number of independent multi-runs to
acquire distribution information and construct a t-based confidence interval.
Our method requires minimal additional computation and memory beyond the
standard updating of estimates, making the inference process almost cost-free.
We provide a rigorous theoretical guarantee for the confidence interval,
demonstrating that the coverage is approximately exact with an explicit
convergence rate and allowing for high confidence level inference. In
particular, a new Gaussian approximation result is developed for the online
estimators to characterize the coverage properties of our confidence intervals
in terms of relative errors. Additionally, our method also allows for
leveraging parallel computing to further accelerate calculations using multiple
cores. It is easy to implement and can be integrated with existing stochastic
algorithms without the need for complicated modifications.
\\ ( https://arxiv.org/abs/2401.09346 ,  9530kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09356 (*cross-listing*)
Date: Wed, 17 Jan 2024 17:24:36 GMT   (1137kb,D)

Title: Swing: Short-cutting Rings for Higher Bandwidth Allreduce
Authors: Daniele De Sensi and Tommaso Bonato and David Saam and Torsten Hoefler
Categories: cs.DC cs.LG cs.NI cs.PF
ACM-class: C.2.4; C.2.2
Journal-ref: NSDI 2024
\\
  The allreduce collective operation accounts for a significant fraction of the
runtime of workloads running on distributed systems. One factor determining its
performance is the distance between communicating nodes, especially on networks
like torus, where a higher distance implies multiple messages being forwarded
on the same link, thus reducing the allreduce bandwidth. Torus networks are
widely used on systems optimized for machine learning workloads (e.g., Google
TPUs and Amazon Trainium devices), as well as on some of the Top500
supercomputers. To improve allreduce performance on torus networks we introduce
Swing, a new algorithm that keeps a low distance between communicating nodes by
swinging between torus directions. Our analysis and experimental evaluation
show that Swing outperforms by up to 3x existing allreduce algorithms for
vectors ranging from 32B to 128MiB, on different types of torus and torus-like
topologies, regardless of their shape and size.
\\ ( https://arxiv.org/abs/2401.09356 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09384 (*cross-listing*)
Date: Wed, 17 Jan 2024 17:55:06 GMT   (13311kb,D)

Title: Diverse Part Synthesis for 3D Shape Creation
Authors: Yanran Guan, Oliver van Kaick
Categories: cs.GR cs.CV cs.LG
\\
  Methods that use neural networks for synthesizing 3D shapes in the form of a
part-based representation have been introduced over the last few years. These
methods represent shapes as a graph or hierarchy of parts and enable a variety
of applications such as shape sampling and reconstruction. However, current
methods do not allow easily regenerating individual shape parts according to
user preferences. In this paper, we investigate techniques that allow the user
to generate multiple, diverse suggestions for individual parts. Specifically,
we experiment with multimodal deep generative models that allow sampling
diverse suggestions for shape parts and focus on models which have not been
considered in previous work on shape synthesis. To provide a comparative study
of these techniques, we introduce a method for synthesizing 3D shapes in a
part-based representation and evaluate all the part suggestion techniques
within this synthesis method. In our method, which is inspired by previous
work, shapes are represented as a set of parts in the form of implicit
functions which are then positioned in space to form the final shape. Synthesis
in this representation is enabled by a neural network architecture based on an
implicit decoder and a spatial transformer. We compare the various multimodal
generative models by evaluating their performance in generating part
suggestions. Our contribution is to show with qualitative and quantitative
evaluations which of the new techniques for multimodal part generation perform
the best and that a synthesis method based on the top-performing techniques
allows the user to more finely control the parts that are generated in the 3D
shapes while maintaining high shape fidelity when reconstructing shapes.
\\ ( https://arxiv.org/abs/2401.09384 ,  13311kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09393 (*cross-listing*)
Date: Wed, 17 Jan 2024 18:09:26 GMT   (8417kb,D)

Title: \'Eliv\'agar: Efficient Quantum Circuit Search for Classification
Authors: Sashwat Anagolum, Narges Alavisamani, Poulami Das, Moinuddin Qureshi,
  Eric Kessler, Yunong Shi
Categories: quant-ph cs.AR cs.LG
Comments: 13 pages, 11 figures. To appear in ASPLOS 2024
\\
  Designing performant and noise-robust circuits for Quantum Machine Learning
(QML) is challenging -- the design space scales exponentially with circuit
size, and there are few well-supported guiding principles for QML circuit
design. Although recent Quantum Circuit Search (QCS) methods attempt to search
for performant QML circuits that are also robust to hardware noise, they
directly adopt designs from classical Neural Architecture Search (NAS) that are
misaligned with the unique constraints of quantum hardware, resulting in high
search overheads and severe performance bottlenecks.
  We present \'Eliv\'agar, a novel resource-efficient, noise-guided QCS
framework. \'Eliv\'agar innovates in all three major aspects of QCS -- search
space, search algorithm and candidate evaluation strategy -- to address the
design flaws in current classically-inspired QCS methods. \'Eliv\'agar achieves
hardware-efficiency and avoids an expensive circuit-mapping co-search via
noise- and device topology-aware candidate generation. By introducing two
cheap-to-compute predictors, Clifford noise resilience and Representational
capacity, \'Eliv\'agar decouples the evaluation of noise robustness and
performance, enabling early rejection of low-fidelity circuits and reducing
circuit evaluation costs. Due to its resource-efficiency, \'Eliv\'agar can
further search for data embeddings, significantly improving performance.
  Based on a comprehensive evaluation of \'Eliv\'agar on 12 real quantum
devices and 9 QML applications, \'Eliv\'agar achieves 5.3% higher accuracy and
a 271$\times$ speedup compared to state-of-the-art QCS methods.
\\ ( https://arxiv.org/abs/2401.09393 ,  8417kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09417 (*cross-listing*)
Date: Wed, 17 Jan 2024 18:56:18 GMT   (627kb,D)

Title: Vision Mamba: Efficient Visual Representation Learning with
  Bidirectional State Space Model
Authors: Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu,
  Xinggang Wang
Categories: cs.CV cs.LG
Comments: Work in progress. Code is available at https://github.com/hustvl/Vim
\\
  Recently the state space models (SSMs) with efficient hardware-aware designs,
i.e., Mamba, have shown great potential for long sequence modeling. Building
efficient and generic vision backbones purely upon SSMs is an appealing
direction. However, representing visual data is challenging for SSMs due to the
position-sensitivity of visual data and the requirement of global context for
visual understanding. In this paper, we show that the reliance of visual
representation learning on self-attention is not necessary and propose a new
generic vision backbone with bidirectional Mamba blocks (Vim), which marks the
image sequences with position embeddings and compresses the visual
representation with bidirectional state space models. On ImageNet
classification, COCO object detection, and ADE20k semantic segmentation tasks,
Vim achieves higher performance compared to well-established vision
transformers like DeiT, while also demonstrating significantly improved
computation & memory efficiency. For example, Vim is 2.8$\times$ faster than
DeiT and saves 86.8% GPU memory when performing batch inference to extract
features on images with a resolution of 1248$\times$1248. The results
demonstrate that Vim is capable of overcoming the computation & memory
constraints on performing Transformer-style understanding for high-resolution
images and it has great potential to become the next-generation backbone for
vision foundation models. Code is available at https://github.com/hustvl/Vim.
\\ ( https://arxiv.org/abs/2401.09417 ,  627kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2208.05764
replaced with revised version Wed, 17 Jan 2024 16:04:03 GMT   (56kb)

Title: The dynamics of belief: continuously monitoring and visualising complex
  systems
Authors: Edwin J. Beggs and John V. Tucker
Categories: cs.AI cs.SY eess.SY
Comments: Comments welcome
ACM-class: I.6.5; D.2.2; I.2.0
\\ ( https://arxiv.org/abs/2208.05764 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11563
replaced with revised version Wed, 17 Jan 2024 07:34:14 GMT   (13504kb,D)

Title: Self-supervised network distillation: an effective approach to
  exploration in sparse reward environments
Authors: Matej Pech\'a\v{c}, Michal Chovanec, Igor Farka\v{s}
Categories: cs.AI
\\ ( https://arxiv.org/abs/2302.11563 ,  13504kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04180
replaced with revised version Wed, 17 Jan 2024 08:37:45 GMT   (4018kb,D)

Title: Train a Real-world Local Path Planner in One Hour via Partially
  Decoupled Reinforcement Learning and Vectorized Diversity
Authors: Jinghao Xin, Jinwoo Kim, Zhi Li, and Ning Li
Categories: cs.AI cs.RO
Comments: 15 pages
\\ ( https://arxiv.org/abs/2305.04180 ,  4018kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14243
replaced with revised version Tue, 16 Jan 2024 22:34:04 GMT   (2590kb,D)

Title: Training Transitive and Commutative Multimodal Transformers with LoReTTa
Authors: Manuel Tran, Yashin Dicente Cid, Amal Lahiani, Fabian J. Theis,
  Tingying Peng, Eldad Klaiman
Categories: cs.AI cs.CV
Comments: Accepted at NeurIPS 2023 (poster). Camera-ready version
\\ ( https://arxiv.org/abs/2305.14243 ,  2590kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00229
replaced with revised version Wed, 17 Jan 2024 14:10:15 GMT   (4389kb,D)

Title: Combining Spatial and Temporal Abstraction in Planning for Better
  Generalization
Authors: Mingde Zhao, Safa Alver, Harm van Seijen, Romain Laroche, Doina
  Precup, Yoshua Bengio
Categories: cs.AI cs.LG
Comments: accepted version for ICLR 2024
\\ ( https://arxiv.org/abs/2310.00229 ,  4389kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05960
replaced with revised version Wed, 17 Jan 2024 13:26:09 GMT   (3684kb,D)

Title: Machine Learning Insides OptVerse AI Solver: Design Principles and
  Applications
Authors: Xijun Li, Fangzhou Zhu, Hui-Ling Zhen, Weilin Luo, Meng Lu, Yimin
  Huang, Zhenan Fan, Zirui Zhou, Yufei Kuang, Zhihai Wang, Zijie Geng, Yang Li,
  Haoyang Liu, Zhiwu An, Muming Yang, Jianshu Li, Jie Wang, Junchi Yan, Defeng
  Sun, Tao Zhong, Yong Zhang, Jia Zeng, Mingxuan Yuan, Jianye Hao, Jun Yao, Kun
  Mao
Categories: cs.AI
\\ ( https://arxiv.org/abs/2401.05960 ,  3684kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07421
replaced with revised version Wed, 17 Jan 2024 16:12:38 GMT   (920kb,D)

Title: SummaryMixing: A Linear-Complexity Alternative to Self-Attention for
  Speech Recognition and Understanding
Authors: Titouan Parcollet and Rogier van Dalen and Shucong Zhang and Sourav
  Bhattacharya
Categories: cs.CL cs.SD eess.AS
\\ ( https://arxiv.org/abs/2307.07421 ,  920kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01497
replaced with revised version Tue, 16 Jan 2024 19:00:56 GMT   (379kb)

Title: Large Language Model Displays Emergent Ability to Interpret Novel
  Literary Metaphors
Authors: Nicholas Ichien, Du\v{s}an Stamenkovi\'c, Keith J. Holyoak
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.01497 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12519
replaced with revised version Wed, 17 Jan 2024 13:09:30 GMT   (788kb,D)

Title: Rational Decision-Making Agent with Internalized Utility Judgment
Authors: Yining Ye, Xin Cong, Shizuo Tian, Yujia Qin, Chong Liu, Yankai Lin,
  Zhiyuan Liu, Maosong Sun
Categories: cs.CL
Comments: Received 8,6,6,6 scores on ICLR 2024
\\ ( https://arxiv.org/abs/2308.12519 ,  788kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12697
replaced with revised version Wed, 17 Jan 2024 08:50:59 GMT   (2627kb,D)

Title: Semantic similarity prediction is better than other semantic similarity
  measures
Authors: Steffen Herbold
Categories: cs.CL cs.LG
Comments: Accepted at TMLR: https://openreview.net/forum?id=bfsNmgN5je
\\ ( https://arxiv.org/abs/2309.12697 ,  2627kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13426
replaced with revised version Wed, 17 Jan 2024 16:36:58 GMT   (343kb,D)

Title: A Chat About Boring Problems: Studying GPT-based text normalization
Authors: Yang Zhang, Travis M. Bartley, Mariana Graterol-Fuenmayor, Vitaly
  Lavrukhin, Evelina Bakhturina, Boris Ginsburg
Categories: cs.CL cs.AI
Comments: Accepted to ICASSP 2024
\\ ( https://arxiv.org/abs/2309.13426 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11374
replaced with revised version Wed, 17 Jan 2024 02:44:56 GMT   (2097kb,D)

Title: DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models
  for Emotion Recognition in Conversations
Authors: Yazhou Zhang, Mengyao Wang, Youxi Wu, Prayag Tiwari, Qiuchi Li, Benyou
  Wang, Jing Qin
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.11374 ,  2097kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01070
replaced with revised version Wed, 17 Jan 2024 10:56:08 GMT   (2353kb,D)

Title: Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech
  Models via Language-Specific Experts
Authors: Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, Vassilina
  Nikoulina
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to IEEE ICASSP 2024
\\ ( https://arxiv.org/abs/2311.01070 ,  2353kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05268
replaced with revised version Wed, 17 Jan 2024 10:09:46 GMT   (1084kb)

Title: Modelling prospective memory and resilient situated communications via
  Wizard of Oz
Authors: Yanzhe Li, Frank Broz, Mark Neerincx
Categories: cs.CL
Comments: In WTF Workshop Proceedings (arXiv:2401.04108) held in conjunction
  with the ACM conference on Conversational User Interfaces (CUI), 19 - 21/07
  2023, in Eindhoven, The Netherlands
Report-no: WTFCUI/2023/03
\\ ( https://arxiv.org/abs/2311.05268 ,  1084kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12023
replaced with revised version Wed, 17 Jan 2024 17:01:57 GMT   (417kb,D)

Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient
  Language Model Finetuning
Authors: Han Guo, Philip Greengard, Eric P. Xing, Yoon Kim
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.12023 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13708
replaced with revised version Wed, 17 Jan 2024 05:03:03 GMT   (349kb)

Title: Dynamic Fault Analysis in Substations Based on Knowledge Graphs
Authors: Weiwei Li, Xing Liu, Wei Wang, Lu Chen, Sizhe Li, Hui Fan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.13708 ,  349kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04350
replaced with revised version Wed, 17 Jan 2024 14:41:55 GMT   (579kb,D)

Title: CLadder: Assessing Causal Reasoning in Language Models
Authors: Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Ojasv Kamal,
  Zhiheng Lyu, Kevin Blin, Fernando Gonzalez Adauto, Max Kleiman-Weiner,
  Mrinmaya Sachan, Bernhard Sch\"olkopf
Categories: cs.CL cs.AI cs.LG
Comments: NeurIPS 2023; updated with CLadder dataset v1.5
\\ ( https://arxiv.org/abs/2312.04350 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15316
replaced with revised version Wed, 17 Jan 2024 17:07:37 GMT   (253kb,D)

Title: Paralinguistics-Enhanced Large Language Modeling of Spoken Dialogue
Authors: Guan-Ting Lin, Prashanth Gurunath Shivakumar, Ankur Gandhe, Chao-Han
  Huck Yang, Yile Gu, Shalini Ghosh, Andreas Stolcke, Hung-yi Lee, Ivan Bulyko
Categories: cs.CL eess.AS
Comments: Accepted by ICASSP 2024. Camera-ready version
\\ ( https://arxiv.org/abs/2312.15316 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05268
replaced with revised version Wed, 17 Jan 2024 17:57:24 GMT   (7415kb,D)

Title: AUTOACT: Automatic Agent Learning from Scratch via Self-Planning
Authors: Shuofei Qiao, Ningyu Zhang, Runnan Fang, Yujie Luo, Wangchunshu Zhou,
  Yuchen Eleanor Jiang, Chengfei Lv, Huajun Chen
Categories: cs.CL cs.AI cs.HC cs.LG cs.MA
Comments: Work in progress
\\ ( https://arxiv.org/abs/2401.05268 ,  7415kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06408
replaced with revised version Tue, 16 Jan 2024 19:35:28 GMT   (4379kb,D)

Title: AboutMe: Using Self-Descriptions in Webpages to Document the Effects of
  English Pretraining Data Filters
Authors: Li Lucy, Suchin Gururangan, Luca Soldaini, Emma Strubell, David
  Bamman, Lauren Klein, Jesse Dodge
Categories: cs.CL
Comments: 28 pages, 13 figures
\\ ( https://arxiv.org/abs/2401.06408 ,  4379kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06792
replaced with revised version Wed, 17 Jan 2024 04:40:13 GMT   (575kb,D)

Title: LightHouse: A Survey of AGI Hallucination
Authors: Feng Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.06792 ,  575kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06855
replaced with revised version Wed, 17 Jan 2024 17:23:20 GMT   (1410kb,D)

Title: Fine-grained Hallucination Detection and Editing for Language Models
Authors: Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham
  Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.06855 ,  1410kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07544
replaced with revised version Wed, 17 Jan 2024 05:28:18 GMT   (492kb,D)

Title: See the Unseen: Better Context-Consistent Knowledge-Editing by Noises
Authors: Youcheng Huang, Wenqiang Lei, Zheng Zhang, Jiancheng Lv, Shuicheng Yan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.07544 ,  492kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08350
replaced with revised version Wed, 17 Jan 2024 06:47:29 GMT   (9200kb,D)

Title: Salute the Classic: Revisiting Challenges of Machine Translation in the
  Age of Large Language Models
Authors: Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu, Derek F. Wong,
  Shuming Shi, Zhaopeng Tu
Categories: cs.CL
Comments: 17 pages. Longyue Wang is the Corresponding Author
\\ ( https://arxiv.org/abs/2401.08350 ,  9200kb)
------------------------------------------------------------------------------
\\
arXiv:2206.01864
replaced with revised version Wed, 17 Jan 2024 07:55:12 GMT   (1049kb)

Title: Model-Informed Generative Adversarial Network (MI-GAN) for Learning
  Optimal Power Flow
Authors: Yuxuan Li, Chaoyue Zhao, and Chenang Liu
Categories: cs.LG stat.ML
Journal-ref: IISE Transactions, 1-22, 2023
DOI: 10.1080/24725854.2023.2286507
\\ ( https://arxiv.org/abs/2206.01864 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2210.07996
replaced with revised version Wed, 17 Jan 2024 00:36:29 GMT   (141kb)

Title: Degeneracy is OK: Logarithmic Regret for Network Revenue Management with
  Indiscrete Distributions
Authors: Jiashuo Jiang, Will Ma and Jiawei Zhang
Categories: cs.LG math.PR
\\ ( https://arxiv.org/abs/2210.07996 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13247
replaced with revised version Wed, 17 Jan 2024 10:36:43 GMT   (1928kb,D)

Title: Online Loss Function Learning
Authors: Christian Raymond, Qi Chen, Bing Xue, Mengjie Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2301.13247 ,  1928kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02811
replaced with revised version Wed, 17 Jan 2024 18:14:20 GMT   (2593kb,D)

Title: HomPINNs: homotopy physics-informed neural networks for solving the
  inverse problems of nonlinear differential equations with multiple solutions
Authors: Haoyang Zheng, Yao Huang, Ziyang Huang, Wenrui Hao, Guang Lin
Categories: cs.LG cs.AI cs.NA math.NA
Comments: 20 pages, 15 figures, 7 tables
Journal-ref: Volume 500, 2024
DOI: 10.1016/j.jcp.2023.112751
\\ ( https://arxiv.org/abs/2304.02811 ,  2593kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10045
replaced with revised version Wed, 17 Jan 2024 16:28:51 GMT   (1693kb,D)

Title: ID-MixGCL: Identity Mixup for Graph Contrastive Learning
Authors: Gehang Zhang and Bowen Yu and Jiangxia Cao and Xinghua Zhang and
  Jiawei Sheng and Chuan Zhou and Tingwen Liu
Categories: cs.LG cs.AI
Comments: 10 pages, 7 figures, accepted by IEEE BigData 2023
\\ ( https://arxiv.org/abs/2304.10045 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05118
replaced with revised version Wed, 17 Jan 2024 17:27:10 GMT   (1072kb,D)

Title: Flame: Simplifying Topology Extension in Federated Learning
Authors: Harshit Daga, Jaemin Shin, Dhruv Garg, Ada Gavrilovska, Myungjin Lee
  and Ramana Rao Kompella
Categories: cs.LG cs.DC
\\ ( https://arxiv.org/abs/2305.05118 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16102
replaced with revised version Thu, 11 Jan 2024 23:41:05 GMT   (901kb,D)

Title: Demystifying Oversmoothing in Attention-Based Graph Neural Networks
Authors: Xinyi Wu, Amir Ajorlou, Zihui Wu, Ali Jadbabaie
Categories: cs.LG cs.SI stat.ML
Comments: NeurIPS 2023 spotlight. Camera-ready version. Fixed an error in the
  previous draft
\\ ( https://arxiv.org/abs/2305.16102 ,  901kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06155
replaced with revised version Wed, 17 Jan 2024 17:13:59 GMT   (620kb,D)

Title: Intensity Profile Projection: A Framework for Continuous-Time
  Representation Learning for Dynamic Networks
Authors: Alexander Modell, Ian Gallagher, Emma Ceccherini, Nick Whiteley and
  Patrick Rubin-Delanchy
Categories: cs.LG stat.ME stat.ML
Comments: 38 pages, 10 figures
MSC-class: 62H12 (primary), 62H30 (secondary)
\\ ( https://arxiv.org/abs/2306.06155 ,  620kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09980
replaced with revised version Wed, 17 Jan 2024 15:03:18 GMT   (3880kb,D)

Title: Creating Multi-Level Skill Hierarchies in Reinforcement Learning
Authors: Joshua B. Evans and \"Ozg\"ur \c{S}im\c{s}ek
Categories: cs.LG cs.AI
Comments: 20 pages, 10 figures. Accepted at NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.09980 ,  3880kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13649
replaced with revised version Wed, 17 Jan 2024 03:23:23 GMT   (420kb,D)

Title: On-Policy Distillation of Language Models: Learning from Self-Generated
  Mistakes
Authors: Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela
  Ramos, Matthieu Geist, Olivier Bachem
Categories: cs.LG cs.AI cs.CL
Comments: Accepted at ICLR 2024. First two authors contributed equally
\\ ( https://arxiv.org/abs/2306.13649 ,  420kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14411
replaced with revised version Wed, 17 Jan 2024 14:55:36 GMT   (34001kb,D)

Title: Score-based Source Separation with Applications to Digital Communication
  Signals
Authors: Tejas Jayashankar, Gary C.F. Lee, Alejandro Lancho, Amir Weiss, Yury
  Polyanskiy, Gregory W. Wornell
Categories: cs.LG eess.SP
Comments: 34 pages, 18 figures, for associated project webpage see
  https://alpha-rgs.github.io
\\ ( https://arxiv.org/abs/2306.14411 ,  34001kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02040
replaced with revised version Wed, 17 Jan 2024 07:54:46 GMT   (15259kb,D)

Title: VertiBench: Advancing Feature Distribution Diversity in Vertical
  Federated Learning Benchmarks
Authors: Zhaomin Wu, Junyi Hou, Bingsheng He
Categories: cs.LG cs.AI
Journal-ref: The Twelfth International Conference on Learning Representations
  (ICLR 2024)
\\ ( https://arxiv.org/abs/2307.02040 ,  15259kb)
------------------------------------------------------------------------------
\\
arXiv:2308.06895
replaced with revised version Tue, 16 Jan 2024 19:14:45 GMT   (6606kb,D)

Title: Federated Classification in Hyperbolic Spaces via Secure Aggregation of
  Convex Hulls
Authors: Saurav Prakash, Jin Sima, Chao Pan, Eli Chien, Olgica Milenkovic
Categories: cs.LG cs.CR cs.DC
Comments: Published in the Transactions on Machine Learning Research (TMLR).
  Link: https://openreview.net/forum?id=umggDfMHha
\\ ( https://arxiv.org/abs/2308.06895 ,  6606kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12143
replaced with revised version Wed, 17 Jan 2024 15:25:22 GMT   (307kb,D)

Title: A Probabilistic Fluctuation based Membership Inference Attack for
  Diffusion Models
Authors: Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang
Categories: cs.LG cs.AI cs.CR cs.CV
\\ ( https://arxiv.org/abs/2308.12143 ,  307kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12252
replaced with revised version Tue, 16 Jan 2024 21:58:37 GMT   (14250kb,D)

Title: How Safe Am I Given What I See? Calibrated Prediction of Safety Chances
  for Image-Controlled Autonomy
Authors: Zhenjiang Mao, Carson Sobolewski, Ivan Ruchkin
Categories: cs.LG
\\ ( https://arxiv.org/abs/2308.12252 ,  14250kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08748
replaced with revised version Wed, 17 Jan 2024 14:07:16 GMT   (1029kb,D)

Title: Wasserstein Distributionally Robust Policy Evaluation and Learning for
  Contextual Bandits
Authors: Yi Shen, Pan Xu, Michael M. Zavlanos
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.08748 ,  1029kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16042
replaced with revised version Wed, 17 Jan 2024 04:07:06 GMT   (697kb,D)

Title: Towards Best Practices of Activation Patching in Language Models:
  Metrics and Methods
Authors: Fred Zhang and Neel Nanda
Categories: cs.LG cs.AI cs.CL
Comments: 27 pages. ICLR 2024
\\ ( https://arxiv.org/abs/2309.16042 ,  697kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16746
replaced with revised version Wed, 17 Jan 2024 13:20:12 GMT   (3299kb,D)

Title: Implicit Gaussian process representation of vector fields over arbitrary
  latent manifolds
Authors: Robert L. Peach, Matteo Vinao-Carl, Nir Grossman, Michael David, Emma
  Mallas, David Sharp, Paresh A. Malhotra, Pierre Vandergheynst, Adam Gosztolai
Categories: cs.LG cs.MS physics.data-an q-bio.QM stat.ML
Comments: ICLR 2024 conference paper. Associated code:
  https://github.com/agosztolai/RVGP
\\ ( https://arxiv.org/abs/2309.16746 ,  3299kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03266
replaced with revised version Tue, 16 Jan 2024 20:15:18 GMT   (2746kb,D)

Title: UniPredict: Large Language Models are Universal Tabular Classifiers
Authors: Ruiyu Wang, Zifeng Wang, Jimeng Sun
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.03266 ,  2746kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03838
replaced with revised version Tue, 16 Jan 2024 21:06:25 GMT   (1006kb,D)

Title: Chameleon: Increasing Label-Only Membership Leakage with Adaptive
  Poisoning
Authors: Harsh Chaudhari, Giorgio Severi, Alina Oprea, Jonathan Ullman
Categories: cs.LG
Comments: To appear at International Conference on Learning Representations
  (ICLR) 2024
\\ ( https://arxiv.org/abs/2310.03838 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04741
replaced with revised version Wed, 17 Jan 2024 15:10:26 GMT   (5227kb,D)

Title: Balancing stability and plasticity in continual learning: the
  readout-decomposition of activation change (RDAC) framework
Authors: Daniel Anthes and Sushrut Thorat and Peter K\"onig and Tim C.
  Kietzmann
Categories: cs.LG cs.CV q-bio.NC
Comments: 15 pages, 5 figures, Revision
\\ ( https://arxiv.org/abs/2310.04741 ,  5227kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08056
replaced with revised version Wed, 17 Jan 2024 12:41:45 GMT   (921kb,D)

Title: Learning from Label Proportions: Bootstrapping Supervised Learners via
  Belief Propagation
Authors: Shreyas Havaldar, Navodita Sharma, Shubhi Sareen, Karthikeyan
  Shanmugam, Aravindan Raghuveer
Categories: cs.LG cs.AI
Comments: Accepted at The Twelfth International Conference on Learning
  Representations (ICLR 2024) & Oral Presentation at Regulatable ML @ NeurIPS
  2023
\\ ( https://arxiv.org/abs/2310.08056 ,  921kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13121
replaced with revised version Wed, 17 Jan 2024 04:22:12 GMT   (1066kb,D)

Title: Understanding Addition in Transformers
Authors: Philip Quirke, Fazl Barez
Categories: cs.LG cs.AI
Comments: 9 pages, 8 figures, accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2310.13121 ,  1066kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19802
replaced with revised version Wed, 17 Jan 2024 14:45:45 GMT   (150kb,D)

Title: Stochastic Thermodynamics of Learning Parametric Probabilistic Models
Authors: Shervin Sadat Parsi
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.19802 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01771
replaced with revised version Wed, 17 Jan 2024 08:33:22 GMT   (5694kb,D)

Title: Efficient Generalized Low-Rank Tensor Contextual Bandits
Authors: Qianxin Yi, Yiyang Yang, Shaojie Tang, Jiapeng Liu, Yao Wang
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.01771 ,  5694kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16522
replaced with revised version Wed, 17 Jan 2024 05:04:12 GMT   (320kb)

Title: Dynamic Fault Characteristics Evaluation in Power Grid
Authors: Hao Pei, Si Lin, Chuanfu Li, Che Wang, Haoming Chen, Sizhe Li
Categories: cs.LG cs.CL eess.SP
\\ ( https://arxiv.org/abs/2311.16522 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08846
replaced with revised version Wed, 17 Jan 2024 08:05:07 GMT   (8157kb,D)

Title: TiMix: Text-aware Image Mixing for Effective Vision-Language
  Pre-training
Authors: Chaoya Jiang, Wei ye, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Fei
  Huang, Shikun Zhang
Categories: cs.LG cs.CL cs.CV
Comments: Accepted on AAAI2024
\\ ( https://arxiv.org/abs/2312.08846 ,  8157kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15965
replaced with revised version Wed, 17 Jan 2024 07:08:16 GMT   (8385kb,D)

Title: Efficient Reinforcemen Learning via Decoupling Exploration and
  Utilization
Authors: Jingpu Yang, Qirui Zhao, Helin Wang, Yuxiao Huang, Zirui Song, Miao
  Fang
Categories: cs.LG
Comments: Update V3
\\ ( https://arxiv.org/abs/2312.15965 ,  8385kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00828
replaced with revised version Wed, 17 Jan 2024 14:17:41 GMT   (261kb,D)

Title: Multi-Lattice Sampling of Quantum Field Theories via Neural
  Operator-based Flows
Authors: B\'alint M\'at\'e, Fran\c{c}ois Fleuret
Categories: cs.LG hep-lat stat.ML
\\ ( https://arxiv.org/abs/2401.00828 ,  261kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01506
replaced with revised version Wed, 17 Jan 2024 16:53:09 GMT   (395kb)

Title: AIRI: Predicting Retention Indices and their Uncertainties using
  Artificial Intelligence
Authors: Lewis Y. Geer, Stephen E. Stein, William Gary Mallard, Douglas J.
  Slotta
Categories: cs.LG q-bio.QM
DOI: 10.1021/acs.jcim.3c01758
\\ ( https://arxiv.org/abs/2401.01506 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03006
replaced with revised version Wed, 17 Jan 2024 14:02:12 GMT   (963kb)

Title: The Rise of Diffusion Models in Time-Series Forecasting
Authors: Caspar Meijer and Lydia Y. Chen
Categories: cs.LG cs.AI
Comments: Version 2, 24 pages, 10 figures, 12 tables, For complete LuaTeX
  source:
  https://github.com/Capsar/The-Rise-of-Diffusion-Models-in-Time-Series-Forecasting
  , Written by: Caspar Meijer, Supervised by: Lydia Y. Chen
\\ ( https://arxiv.org/abs/2401.03006 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03955
replaced with revised version Wed, 17 Jan 2024 16:27:24 GMT   (2249kb,D)

Title: Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced
  Zero/Few-Shot Forecasting of Multivariate Time Series
Authors: Vijay Ekambaram, Arindam Jati, Nam H. Nguyen, Pankaj Dayama, Chandra
  Reddy, Wesley M. Gifford, Jayant Kalagnanam
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.03955 ,  2249kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05794
replaced with revised version Wed, 17 Jan 2024 07:32:02 GMT   (19kb)

Title: Bounds on the price of feedback for mistake-bounded online learning
Authors: Jesse Geneson and Linus Tang
Categories: cs.LG cs.DM math.CO
\\ ( https://arxiv.org/abs/2401.05794 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07231
replaced with revised version Wed, 17 Jan 2024 04:06:44 GMT   (220kb,D)

Title: Use of Prior Knowledge to Discover Causal Additive Models with
  Unobserved Variables and its Application to Time Series Data
Authors: Takashi Nicholas Maeda, Shohei Shohei
Categories: cs.LG stat.ME stat.ML
\\ ( https://arxiv.org/abs/2401.07231 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07595
replaced with revised version Wed, 17 Jan 2024 13:08:36 GMT   (1497kb,D)

Title: E3x: $\mathrm{E}(3)$-Equivariant Deep Learning Made Easy
Authors: Oliver T. Unke and Hartmut Maennel
Categories: cs.LG cs.AI physics.chem-ph
\\ ( https://arxiv.org/abs/2401.07595 ,  1497kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07993
replaced with revised version Wed, 17 Jan 2024 16:02:27 GMT   (36535kb,D)

Title: Carrying over algorithm in transformers
Authors: Jorrit Kruthoff
Categories: cs.LG cs.AI
Comments: Comments welcome!
\\ ( https://arxiv.org/abs/2401.07993 ,  36535kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08197
replaced with revised version Wed, 17 Jan 2024 13:42:40 GMT   (1729kb,D)

Title: Matrix Completion with Hypergraphs:Sharp Thresholds and Efficient
  Algorithms
Authors: Zhongtian Ma, Qiaosheng Zhang and Zhen Wang
Categories: cs.LG cs.IT eess.SP math.IT
Comments: Submitted to IEEE for possible publication
\\ ( https://arxiv.org/abs/2401.08197 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08383
replaced with revised version Wed, 17 Jan 2024 03:37:00 GMT   (21639kb,D)

Title: Exploiting Inter-Layer Expert Affinity for Accelerating
  Mixture-of-Experts Model Inference
Authors: Jinghan Yao, Quentin Anthony, Aamir Shafi, Hari Subramoni, Dhabaleswar
  K. (DK) Panda
Categories: cs.LG cs.AI cs.DC
\\ ( https://arxiv.org/abs/2401.08383 ,  21639kb)
------------------------------------------------------------------------------
\\
arXiv:2204.02779 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 09:38:09 GMT   (1407kb,D)

Title: A Dempster-Shafer approach to trustworthy AI with application to fetal
  brain MRI segmentation
Authors: Lucas Fidon, Michael Aertsen, Florian Kofler, Andrea Bink, Anna L.
  David, Thomas Deprest, Doaa Emam, Fr\'ed\'eric Guffens, Andr\'as Jakab,
  Gregor Kasprian, Patric Kienast, Andrew Melbourne, Bjoern Menze, Nada Mufti,
  Ivana Pogledic, Daniela Prayer, Marlene Stuempflen, Esther Van Elslander,
  S\'ebastien Ourselin, Jan Deprest, Tom Vercauteren
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Published in IEEE TPAMI. Minor revision compared to the previous
  version
DOI: 10.1109/TPAMI.2023.3346330
\\ ( https://arxiv.org/abs/2204.02779 ,  1407kb)
------------------------------------------------------------------------------
\\
arXiv:2210.02612
replaced with revised version Wed, 17 Jan 2024 03:15:15 GMT   (446kb)

Title: Lyapunov Function Consistent Adaptive Network Signal Control with Back
  Pressure and Reinforcement Learning
Authors: Chaolun Ma, Bruce Wang, Zihao Li, Ahmadreza Mahmoudzadeh, Yunlong
  Zhang
Categories: eess.SY cs.AI cs.LG cs.SY math.OC
\\ ( https://arxiv.org/abs/2210.02612 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13118
replaced with revised version Wed, 17 Jan 2024 09:11:16 GMT   (810kb,D)

Title: Decision Diagram-Based Branch-and-Bound with Caching for Dominance and
  Suboptimality Detection
Authors: Vianney Copp\'e, Xavier Gillard, Pierre Schaus
Categories: cs.DS cs.AI cs.DM math.OC
Comments: Submitted to INFORMS Journal on Computing
MSC-class: 90C39, 90C27, 90C57
ACM-class: I.2.8; G.2.1
\\ ( https://arxiv.org/abs/2211.13118 ,  810kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12162
replaced with revised version Wed, 17 Jan 2024 12:48:38 GMT   (300kb,D)

Title: A Scalable Neural Network for DSIC Affine Maximizer Auction Design
Authors: Zhijian Duan, Haoran Sun, Yurong Chen, Xiaotie Deng
Categories: cs.GT cs.AI cs.LG cs.MA
Comments: NeurIPS 2023 (spotlight)
\\ ( https://arxiv.org/abs/2305.12162 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16494
replaced with revised version Wed, 17 Jan 2024 15:38:27 GMT   (21191kb,D)

Title: Diffusion-Based Adversarial Sample Generation for Improved Stealthiness
  and Controllability
Authors: Haotian Xue, Alexandre Araujo, Bin Hu, Yongxin Chen
Categories: cs.CV cs.AI
Comments: Accepted as a conference paper in NeurIPS'2023. Code repo:
  https://github.com/xavihart/Diff-PGD
\\ ( https://arxiv.org/abs/2305.16494 ,  21191kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17225 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 14:22:19 GMT   (6997kb,D)

Title: Causal Component Analysis
Authors: Liang Wendong, Armin Keki\'c, Julius von K\"ugelgen, Simon Buchholz,
  Michel Besserve, Luigi Gresele, Bernhard Sch\"olkopf
Categories: stat.ML cs.AI cs.LG
Comments: NeurIPS 2023 final camera-ready version
\\ ( https://arxiv.org/abs/2305.17225 ,  6997kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06755
replaced with revised version Wed, 17 Jan 2024 04:07:11 GMT   (2324kb,D)

Title: CoTran: An LLM-based Code Translator using Reinforcement Learning with
  Feedback from Compiler and Symbolic Execution
Authors: Prithwish Jana, Piyush Jha, Haoyang Ju, Gautham Kishore, Aryan Mahajan
  and Vijay Ganesh
Categories: cs.PL cs.AI cs.SE
ACM-class: I.2.7; I.2.5; D.2
\\ ( https://arxiv.org/abs/2306.06755 ,  2324kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08177
replaced with revised version Tue, 16 Jan 2024 19:56:18 GMT   (2519kb,D)

Title: Using an LLM to Help With Code Understanding
Authors: Daye Nam and Andrew Macvean and Vincent Hellendoorn and Bogdan
  Vasilescu and Brad Myers
Categories: cs.SE cs.AI cs.HC
\\ ( https://arxiv.org/abs/2307.08177 ,  2519kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09487
replaced with revised version Wed, 17 Jan 2024 13:44:07 GMT   (2017kb,D)

Title: DFB: A Data-Free, Low-Budget, and High-Efficacy Clean-Label Backdoor
  Attack
Authors: Binhao Ma, Jiahui Wang, Dejun Wang, Bo Meng
Categories: cs.CR cs.AI
\\ ( https://arxiv.org/abs/2308.09487 ,  2017kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14517
replaced with revised version Wed, 17 Jan 2024 17:41:18 GMT   (4516kb,D)

Title: Watch Your Language: Investigating Content Moderation with Large
  Language Models
Authors: Deepak Kumar, Yousef AbuHashem, Zakir Durumeric
Categories: cs.HC cs.AI cs.CL cs.CR cs.SI
\\ ( https://arxiv.org/abs/2309.14517 ,  4516kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05937
replaced with revised version Wed, 17 Jan 2024 10:52:46 GMT   (857kb)

Title: Genetic Algorithm enhanced by Deep Reinforcement Learning in parent
  selection mechanism and mutation : Minimizing makespan in permutation flow
  shop scheduling problems
Authors: Maissa Irmouli, Nourelhouda Benazzoug, Alaa Dania Adimi, Fatma Zohra
  Rezkellah, Imane Hamzaoui, Thanina Hamitouche, Malika Bessedik, Fatima Si
  Tayeb
Categories: cs.NE cs.AI
\\ ( https://arxiv.org/abs/2311.05937 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11866
replaced with revised version Wed, 17 Jan 2024 17:10:31 GMT   (16722kb,D)

Title: Analyzing Emissions and Energy Efficiency at Unsignalized Real-world
  Intersections Under Mixed Traffic Control
Authors: Michael Villarreal, Dawei Wang, Jia Pan, Weizi Li
Categories: cs.CY cs.AI
Comments: Accepted to 4th IEEE Forum for Innovative Sustainable Transportation
  Systems
\\ ( https://arxiv.org/abs/2311.11866 ,  16722kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04118
replaced with revised version Wed, 17 Jan 2024 09:43:14 GMT   (4858kb,D)

Title: Caregiver Talk Shapes Toddler Vision: A Computational Study of Dyadic
  Play
Authors: Timothy Schauml\"offel, Arthur Aubret, Gemma Roig, Jochen Triesch
Categories: cs.CV cs.AI cs.LG
Comments: Proceedings of the 2023 IEEE International Conference on Development
  and Learning (ICDL)
Journal-ref: "Caregiver Talk Shapes Toddler Vision: A Computational Study of
  Dyadic Play," 2023 IEEE International Conference on Development and Learning
  (ICDL), Macau, China, 2023, pp. 67-72
DOI: 10.1109/ICDL55364.2023.10364409
\\ ( https://arxiv.org/abs/2312.04118 ,  4858kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04864
replaced with revised version Tue, 16 Jan 2024 19:06:59 GMT   (821kb)

Title: Critical Analysis of 5G Networks Traffic Intrusion using PCA, t-SNE and
  UMAP Visualization and Classifying Attacks
Authors: Humera Ghani, Shahram Salekzamankhani, Bal Virdee
Categories: cs.CR cs.AI
Journal-ref: Proceedings of Data Analytics and Management. ICDAM 2023. Lecture
  Notes in Networks and Systems, vol 785. Springer, Singapore
DOI: 10.1007/978-981-99-6544-1_32
\\ ( https://arxiv.org/abs/2312.04864 ,  821kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04960
replaced with revised version Wed, 17 Jan 2024 13:47:32 GMT   (3894kb,D)

Title: MIMIR: Masked Image Modeling for Mutual Information-based Adversarial
  Robustness
Authors: Xiaoyun Xu, Shujian Yu, Jingzheng Wu, Stjepan Picek
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.04960 ,  3894kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04124
replaced with revised version Wed, 17 Jan 2024 06:35:45 GMT   (5950kb,D)

Title: MobileAgent: enhancing mobile control via human-machine interaction and
  SOP integration
Authors: Tinghe Ding
Categories: cs.HC cs.AI
Comments: agent, mobile control, SOP, human-machine interaction
\\ ( https://arxiv.org/abs/2401.04124 ,  5950kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05302
replaced with revised version Wed, 17 Jan 2024 18:45:39 GMT   (5276kb,D)

Title: Theory of Mind abilities of Large Language Models in Human-Robot
  Interaction : An Illusion?
Authors: Mudit Verma, Siddhant Bhambri, Subbarao Kambhampati
Categories: cs.RO cs.AI cs.HC
Comments: Accepted in alt.HRI 2024
DOI: 10.1145/3610978.3640767
\\ ( https://arxiv.org/abs/2401.05302 ,  5276kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06506
replaced with revised version Wed, 17 Jan 2024 07:44:50 GMT   (871kb,D)

Title: Frequency Masking for Universal Deepfake Detection
Authors: Chandler Timm Doloriel, Ngai-Man Cheung
Categories: cs.CV cs.AI
Comments: Accepted to IEEE ICASSP-2024
\\ ( https://arxiv.org/abs/2401.06506 ,  871kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08268 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 13:28:04 GMT   (426kb,D)

Title: An Explainable Proxy Model for Multiabel Audio Segmentation
Authors: Th\'eo Mariotte and Antonio Almud\'evar and Marie Tahon and Alfonso
  Ortega
Categories: eess.AS cs.AI cs.LG cs.SD eess.SP
Comments: Accepted at ICASSP 2024
Report-no: AA001
\\ ( https://arxiv.org/abs/2401.08268 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2303.00915
replaced with revised version Tue, 16 Jan 2024 21:42:24 GMT   (8889kb,D)

Title: BiomedCLIP: a multimodal biomedical foundation model pretrained from
  fifteen million scientific image-text pairs
Authors: Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga,
  Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, Cliff Wong,
  Andrea Tupini, Yu Wang, Matt Mazzola, Swadheen Shukla, Lars Liden, Jianfeng
  Gao, Matthew P. Lungren, Tristan Naumann, Sheng Wang, and Hoifung Poon
Categories: cs.CV cs.CL
Comments: The models are released at https://aka.ms/biomedclip
\\ ( https://arxiv.org/abs/2303.00915 ,  8889kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07895
replaced with revised version Wed, 17 Jan 2024 12:02:33 GMT   (2225kb,D)

Title: On the Hidden Mystery of OCR in Large Multimodal Models
Authors: Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin
  Liu, Lianwen Jin, Xiang Bai
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2305.07895 ,  2225kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09084
replaced with revised version Wed, 17 Jan 2024 10:26:04 GMT   (140kb,D)

Title: Language Modeling on a SpiNNaker 2 Neuromorphic Chip
Authors: Khaleelulla Khan Nazeer, Mark Sch\"one, Rishav Mukherji, Bernhard
  Vogginger, Christian Mayr, David Kappel, Anand Subramoney
Categories: cs.NE cs.CL cs.ET cs.LG
\\ ( https://arxiv.org/abs/2312.09084 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02906
replaced with revised version Wed, 17 Jan 2024 12:58:36 GMT   (2136kb,D)

Title: MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance
Authors: Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong,
  Jipeng Zhang, Tong Zhang
Categories: cs.CR cs.CL cs.CV
\\ ( https://arxiv.org/abs/2401.02906 ,  2136kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06758
replaced with revised version Tue, 16 Jan 2024 23:54:43 GMT   (9936kb,D)

Title: Exploring Contextual Representation and Multi-Modality for End-to-End
  Autonomous Driving
Authors: Shoaib Azam, Farzeen Munir, Ville Kyrki, Moongu Jeon, and Witold
  Pedrycz
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2210.06758 ,  9936kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13506
replaced with revised version Wed, 17 Jan 2024 15:45:54 GMT   (11487kb,D)

Title: Supporting Safety Analysis of Image-processing DNNs through
  Clustering-based Approaches
Authors: Mohammed Oualid Attaoui, Hazem Fahmy, Fabrizio Pastore and Lionel
  Briand
Categories: cs.SE cs.LG
Comments: 16 Tables, 15 Figures
\\ ( https://arxiv.org/abs/2301.13506 ,  11487kb)
------------------------------------------------------------------------------
\\
arXiv:2303.03678 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 08:49:31 GMT   (351kb,D)

Title: A Comparative Study of Deep Learning and Iterative Algorithms for Joint
  Channel Estimation and Signal Detection
Authors: Haocheng Ju, Haimiao Zhang, Lin Li, Xiao Li, Bin Dong
Categories: eess.SP cs.LG
Comments: Code is available at https://github.com/j991222/MIMO_JCESD
\\ ( https://arxiv.org/abs/2303.03678 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18171
replaced with revised version Wed, 17 Jan 2024 16:38:47 GMT   (1785kb,D)

Title: Improved Probabilistic Image-Text Representations
Authors: Sanghyuk Chun
Categories: cs.CV cs.LG
Comments: ICLR 2024; Code: https://github.com/naver-ai/pcmepp. Project page:
  https://naver-ai.github.io/pcmepp/. 26 pages, 2.4 MB
\\ ( https://arxiv.org/abs/2305.18171 ,  1785kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11700 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 04:52:39 GMT   (2292kb,D)

Title: Last-Iterate Convergent Policy Gradient Primal-Dual Methods for
  Constrained MDPs
Authors: Dongsheng Ding and Chen-Yu Wei and Kaiqing Zhang and Alejandro Ribeiro
Categories: math.OC cs.LG cs.SY eess.SY
Comments: 65 pages, 17 figures, and 1 table; NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.11700 ,  2292kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11863 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 01:45:27 GMT   (278kb,D)

Title: KinSPEAK: Improving speech recognition for Kinyarwanda via
  semi-supervised learning methods
Authors: Antoine Nzeyimana
Categories: eess.AS cs.LG cs.SD
Comments: 9 pages, 2 figures, 5 tables
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2308.11863 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04001
replaced with revised version Wed, 17 Jan 2024 01:47:40 GMT   (6217kb,D)

Title: MMSFormer: Multimodal Transformer for Material and Semantic Segmentation
Authors: Md Kaykobad Reza, Ashley Prater-Bennette, M. Salman Asif
Categories: cs.CV cs.LG
Comments: 14 pages, 3 figures, 8 tables
\\ ( https://arxiv.org/abs/2309.04001 ,  6217kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07690 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 13:13:37 GMT   (965kb)

Title: A DenseNet-based method for decoding auditory spatial attention with EEG
Authors: Xiran Xu, Bo Wang, Yujie Yan, Xihong Wu, Jing Chen
Categories: eess.SP cs.LG
Comments: 5 pages, 3 figures, has been accepted by ICASSP 2024
\\ ( https://arxiv.org/abs/2309.07690 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08971
replaced with revised version Wed, 17 Jan 2024 11:35:33 GMT   (374kb,D)

Title: Regularized Contrastive Pre-training for Few-shot Bioacoustic Sound
  Detection
Authors: Ilyass Moummad, Romain Serizel, Nicolas Farrugia
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2309.08971 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05725 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 08:06:39 GMT   (263kb,D)

Title: Post-hoc Bias Scoring Is Optimal For Fair Classification
Authors: Wenlong Chen, Yegor Klochkov, Yang Liu
Categories: stat.ML cs.LG
Comments: Accepted for publication at The Twelfth International Conference on
  Learning Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2310.05725 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06234
replaced with revised version Wed, 17 Jan 2024 00:03:00 GMT   (1397kb,D)

Title: Efficient Adaptation of Large Vision Transformer via Adapter
  Re-Composing
Authors: Wei Dong, Dawei Yan, Zhijun Lin, Peng Wang
Categories: cs.CV cs.LG
Comments: Paper is accepted to NeurIPS 2023
\\ ( https://arxiv.org/abs/2310.06234 ,  1397kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10224 (*cross-listing*)
replaced with revised version Wed, 17 Jan 2024 15:13:37 GMT   (2039kb,D)

Title: Generalizing Medical Image Representations via Quaternion Wavelet
  Networks
Authors: Luigi Sigillo, Eleonora Grassucci, Aurelio Uncini, Danilo Comminiello
Categories: eess.IV cs.CV cs.LG
Comments: This paper is currently under review
\\ ( https://arxiv.org/abs/2310.10224 ,  2039kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01475
replaced with revised version Mon, 15 Jan 2024 21:03:53 GMT   (8524kb,D)

Title: Patch-Based Deep Unsupervised Image Segmentation using Graph Cuts
Authors: Isaac Wasserman and Jeova Farias Sales Rocha Neto
Categories: cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2311.01475 ,  8524kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10359
replaced with revised version Wed, 17 Jan 2024 04:52:24 GMT   (4186kb,D)

Title: FIKIT: Priority-Based Real-time GPU Multi-tasking Scheduling with Kernel
  Identification
Authors: Wenqing Wu
Categories: cs.DC cs.LG
Comments: 20 pages, 20 figures. Delete a duplicated paragraph in the
  introduction section; Add more experiments with 2 additional figures; Update
  the conclusion
\\ ( https://arxiv.org/abs/2311.10359 ,  4186kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18672 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 20:51:48 GMT   (2971kb,D)

Title: A Comparison Between Invariant and Equivariant Classical and Quantum
  Graph Neural Networks
Authors: Roy T. Forestano, Mar\c{c}al Comajoan Cara, Gopal Ramesh Dahale,
  Zhongtian Dong, Sergei Gleyzer, Daniel Justice, Kyoungchul Kong, Tom
  Magorsch, Konstantin T. Matchev, Katia Matcheva, Eyup B. Unlu
Categories: quant-ph cs.LG hep-ph stat.ML
Comments: 14 pages, 7 figures, 3 appendices
\\ ( https://arxiv.org/abs/2311.18672 ,  2971kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10209
replaced with revised version Tue, 16 Jan 2024 19:51:15 GMT   (2382kb,D)

Title: Beyond Empirical Windowing: An Attention-Based Approach for Trust
  Prediction in Autonomous Vehicles
Authors: Minxue Niu, Zhaobo Zheng, Kumar Akash, Teruhisa Misu
Categories: cs.HC cs.LG
\\ ( https://arxiv.org/abs/2312.10209 ,  2382kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15824
replaced with revised version Tue, 16 Jan 2024 20:00:56 GMT   (14kb)

Title: Self-Supervised Learning for Few-Shot Bird Sound Classification
Authors: Ilyass Moummad and Romain Serizel and Nicolas Farrugia
Categories: cs.SD cs.LG eess.AS
\\ ( https://arxiv.org/abs/2312.15824 ,  14kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00313
replaced with revised version Wed, 17 Jan 2024 06:41:47 GMT   (1334kb,D)

Title: Matching of Users and Creators in Two-Sided Markets with Departures
Authors: Daniel Huttenlocher, Hannah Li, Liang Lyu, Asuman Ozdaglar and James
  Siderius
Categories: cs.GT cs.LG cs.SI econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2401.00313 ,  1334kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01783
replaced with revised version Wed, 17 Jan 2024 01:02:06 GMT   (2059kb,D)

Title: Approximating Numerical Fluxes Using Fourier Neural Operators for
  Hyperbolic Conservation Laws
Authors: Taeyoung Kim and Myungjoo Kang
Categories: math.NA cs.LG cs.NA
Comments: 26 pages, 28 figures
\\ ( https://arxiv.org/abs/2401.01783 ,  2059kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03506 (*cross-listing*)
replaced with revised version Tue, 16 Jan 2024 23:12:55 GMT   (175kb,D)

Title: DiarizationLM: Speaker Diarization Post-Processing with Large Language
  Models
Authors: Quan Wang, Yiling Huang, Guanlong Zhao, Evan Clark, Wei Xia, Hank Liao
Categories: eess.AS cs.LG cs.SD
\\ ( https://arxiv.org/abs/2401.03506 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07671
replaced with revised version Wed, 17 Jan 2024 13:49:05 GMT   (1277kb,D)

Title: CLSA-CIM: A Cross-Layer Scheduling Approach for Computing-in-Memory
  Architectures
Authors: Rebecca Pelke, Jose Cubero-Cascante, Nils Bosbach, Felix Staudigl,
  Rainer Leupers, Jan Moritz Joseph
Categories: cs.AR cs.ET cs.LG
\\ ( https://arxiv.org/abs/2401.07671 ,  1277kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07769
replaced with revised version Wed, 17 Jan 2024 03:38:26 GMT   (1766kb,D)

Title: Deep Evolutional Instant Interest Network for CTR Prediction in
  Trigger-Induced Recommendation
Authors: Zhibo Xiao, Luwei Yang, Tao Zhang, Wen Jiang, Wei Ning and Yujiu Yang
Categories: cs.IR cs.LG
Comments: 7 pages, 3 figures, accepted by the 17th ACM International Conference
  on Web Search and Data Mining(WSDM'2024)
\\ ( https://arxiv.org/abs/2401.07769 ,  1766kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
