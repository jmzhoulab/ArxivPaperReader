paper_240320.txt


Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月20日 13:09
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon 18 Mar 24 18:00:00 GMT  to  Tue 19 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.12094
Date: Fri, 15 Mar 2024 06:57:08 GMT   (336kb,D)

Title: Are LLMs Good Cryptic Crossword Solvers?
Authors: Abdelrahman "Boda" Sadallah, Daria Kotova, Ekaterina Kochmar
Categories: cs.AI cs.CL cs.LG
\\
  Cryptic crosswords are puzzles that rely not only on general knowledge but
also on the solver's ability to manipulate language on different levels and
deal with various types of wordplay. Previous research suggests that solving
such puzzles is a challenge even for modern NLP models. However, the abilities
of large language models (LLMs) have not yet been tested on this task. In this
paper, we establish the benchmark results for three popular LLMs -- LLaMA2,
Mistral, and ChatGPT -- showing that their performance on this task is still
far from that of humans.
\\ ( https://arxiv.org/abs/2403.12094 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12106
Date: Sun, 17 Mar 2024 15:59:39 GMT   (15448kb,D)

Title: Circular Belief Propagation for Approximate Probabilistic Inference
Authors: Vincent Bouttier, Renaud Jardri, Sophie Deneve
Categories: cs.AI cs.LG
\\
  Belief Propagation (BP) is a simple probabilistic inference algorithm,
consisting of passing messages between nodes of a graph representing a
probability distribution. Its analogy with a neural network suggests that it
could have far-ranging applications for neuroscience and artificial
intelligence. Unfortunately, it is only exact when applied to cycle-free
graphs, which restricts the potential of the algorithm. In this paper, we
propose Circular Belief Propagation (CBP), an extension of BP which limits the
detrimental effects of message reverberation caused by cycles by learning to
detect and cancel spurious correlations and belief amplifications. We show in
numerical experiments involving binary probabilistic graphs that CBP far
outperforms BP and reaches good performance compared to that of previously
proposed algorithms.
\\ ( https://arxiv.org/abs/2403.12106 ,  15448kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12108
Date: Mon, 18 Mar 2024 01:04:52 GMT   (730kb,D)

Title: Does AI help humans make better decisions? A methodological framework
  for experimental evaluation
Authors: Eli Ben-Michael, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao
  Jiang, Sooahn Shin
Categories: cs.AI econ.GN q-fin.EC stat.AP stat.ME
\\
  The use of Artificial Intelligence (AI) based on data-driven algorithms has
become ubiquitous in today's society. Yet, in many cases and especially when
stakes are high, humans still make final decisions. The critical question,
therefore, is whether AI helps humans make better decisions as compared to a
human alone or AI an alone. We introduce a new methodological framework that
can be used to answer experimentally this question with no additional
assumptions. We measure a decision maker's ability to make correct decisions
using standard classification metrics based on the baseline potential outcome.
We consider a single-blinded experimental design, in which the provision of
AI-generated recommendations is randomized across cases with a human making
final decisions. Under this experimental design, we show how to compare the
performance of three alternative decision-making systems--human-alone,
human-with-AI, and AI-alone. We apply the proposed methodology to the data from
our own randomized controlled trial of a pretrial risk assessment instrument.
We find that AI recommendations do not improve the classification accuracy of a
judge's decision to impose cash bail. Our analysis also shows that AI-alone
decisions generally perform worse than human decisions with or without AI
assistance. Finally, AI recommendations tend to impose cash bail on non-white
arrestees more often than necessary when compared to white arrestees.
\\ ( https://arxiv.org/abs/2403.12108 ,  730kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12151
Date: Mon, 18 Mar 2024 18:08:44 GMT   (2806kb,D)

Title: Fusing Domain-Specific Content from Large Language Models into Knowledge
  Graphs for Enhanced Zero Shot Object State Classification
Authors: Filippos Gouidis, Katerina Papantoniou, Konstantinos Papoutsakis
  Theodore Patkos, Antonis Argyros and Dimitris Plexousakis
Categories: cs.AI cs.CL cs.CV cs.LG
Comments: Accepted at the AAAI-MAKE 24
\\
  Domain-specific knowledge can significantly contribute to addressing a wide
variety of vision tasks. However, the generation of such knowledge entails
considerable human labor and time costs. This study investigates the potential
of Large Language Models (LLMs) in generating and providing domain-specific
information through semantic embeddings. To achieve this, an LLM is integrated
into a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors
in the context of the Vision-based Zero-shot Object State Classification task.
We thoroughly examine the behavior of the LLM through an extensive ablation
study. Our findings reveal that the integration of LLM-based embeddings, in
combination with general-purpose pre-trained embeddings, leads to substantial
performance improvements. Drawing insights from this ablation study, we conduct
a comparative analysis against competing models, thereby highlighting the
state-of-the-art performance achieved by the proposed approach.
\\ ( https://arxiv.org/abs/2403.12151 ,  2806kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12153
Date: Mon, 18 Mar 2024 18:09:47 GMT   (350kb,D)

Title: Routing and Scheduling in Answer Set Programming applied to Multi-Agent
  Path Finding: Preliminary Report
Authors: Roland Kaminski, Torsten Schaub, Tran Cao Son, Ji\v{r}\'i
  \v{S}vancara, Philipp Wanko
Categories: cs.AI cs.LO cs.SC
\\
  We present alternative approaches to routing and scheduling in Answer Set
Programming (ASP), and explore them in the context of Multi-agent Path Finding.
The idea is to capture the flow of time in terms of partial orders rather than
time steps attached to actions and fluents. This also abolishes the need for
fixed upper bounds on the length of plans. The trade-off for this avoidance is
that (parts of) temporal trajectories must be acyclic, since multiple
occurrences of the same action or fluent cannot be distinguished anymore. While
this approach provides an interesting alternative for modeling routing, it is
without alternative for scheduling since fine-grained timings cannot be
represented in ASP in a feasible way. This is different for partial orders that
can be efficiently handled by external means such as acyclicity and difference
constraints. We formally elaborate upon this idea and present several resulting
ASP encodings. Finally, we demonstrate their effectiveness via an empirical
analysis.
\\ ( https://arxiv.org/abs/2403.12153 ,  350kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12162
Date: Mon, 18 Mar 2024 18:23:36 GMT   (379kb,D)

Title: Intelligent Execution through Plan Analysis
Authors: Daniel Borrajo and Manuela Veloso
Categories: cs.AI cs.RO
Comments: Published at IROS 21, 6 pages
\\
  Intelligent robots need to generate and execute plans. In order to deal with
the complexity of real environments, planning makes some assumptions about the
world. When executing plans, the assumptions are usually not met. Most works
have focused on the negative impact of this fact and the use of replanning
after execution failures. Instead, we focus on the positive impact, or
opportunities to find better plans. When planning, the proposed technique finds
and stores those opportunities. Later, during execution, the monitoring system
can use them to focus perception and repair the plan, instead of replanning
from scratch. Experiments in several paradigmatic robotic tasks show how the
approach outperforms standard replanning strategies.
\\ ( https://arxiv.org/abs/2403.12162 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12201
Date: Mon, 18 Mar 2024 19:22:53 GMT   (7009kb,D)

Title: Compositional learning of functions in humans and machines
Authors: Yanli Zhou, Brenden M. Lake, Adina Williams
Categories: cs.AI cs.HC cs.LG
Comments: 7 pages, 6 figures
\\
  The ability to learn and compose functions is foundational to efficient
learning and reasoning in humans, enabling flexible generalizations such as
creating new dishes from known cooking processes. Beyond sequential chaining of
functions, existing linguistics literature indicates that humans can grasp more
complex compositions with interacting functions, where output production
depends on context changes induced by different function orderings. Extending
the investigation into the visual domain, we developed a function learning
paradigm to explore the capacity of humans and neural network models in
learning and reasoning with compositional functions under varied interaction
conditions. Following brief training on individual functions, human
participants were assessed on composing two learned functions, in ways covering
four main interaction types, including instances in which the application of
the first function creates or removes the context for applying the second
function. Our findings indicate that humans can make zero-shot generalizations
on novel visual function compositions across interaction conditions,
demonstrating sensitivity to contextual changes. A comparison with a neural
network model on the same task reveals that, through the meta-learning for
compositionality (MLC) approach, a standard sequence-to-sequence Transformer
can mimic human generalization patterns in composing functions.
\\ ( https://arxiv.org/abs/2403.12201 ,  7009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12308
Date: Mon, 18 Mar 2024 23:18:16 GMT   (1000kb,D)

Title: Gradient-based Fuzzy System Optimisation via Automatic Differentiation
  -- FuzzyR as a Use Case
Authors: Chao Chen, Christian Wagner, Jonathan M. Garibaldi
Categories: cs.AI
\\
  Since their introduction, fuzzy sets and systems have become an important
area of research known for its versatility in modelling, knowledge
representation and reasoning, and increasingly its potential within the context
explainable AI. While the applications of fuzzy systems are diverse, there has
been comparatively little advancement in their design from a machine learning
perspective. In other words, while representations such as neural networks have
benefited from a boom in learning capability driven by an increase in
computational performance in combination with advances in their training
mechanisms and available tool, in particular gradient descent, the impact on
fuzzy system design has been limited. In this paper, we discuss
gradient-descent-based optimisation of fuzzy systems, focussing in particular
on automatic differentiation--crucial to neural network learning--with a view
to free fuzzy system designers from intricate derivative computations, allowing
for more focus on the functional and explainability aspects of their design. As
a starting point, we present a use case in FuzzyR which demonstrates how
current fuzzy inference system implementations can be adjusted to leverage
powerful features of automatic differentiation tools sets, discussing its
potential for the future of fuzzy system design.
\\ ( https://arxiv.org/abs/2403.12308 ,  1000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12406
Date: Tue, 19 Mar 2024 03:34:23 GMT   (4552kb,D)

Title: Offline Imitation of Badminton Player Behavior via Experiential Contexts
  and Brownian Motion
Authors: Kuang-Da Wang, Wei-Yao Wang, Ping-Chun Hsieh, Wen-Chih Peng
Categories: cs.AI cs.LG
Comments: Preprint
\\
  In the dynamic and rapid tactic involvements of turn-based sports, badminton
stands out as an intrinsic paradigm that requires alter-dependent
decision-making of players. While the advancement of learning from offline
expert data in sequential decision-making has been witnessed in various
domains, how to rally-wise imitate the behaviors of human players from offline
badminton matches has remained underexplored. Replicating opponents' behavior
benefits players by allowing them to undergo strategic development with
direction before matches. However, directly applying existing methods suffers
from the inherent hierarchy of the match and the compounding effect due to the
turn-based nature of players alternatively taking actions. In this paper, we
propose RallyNet, a novel hierarchical offline imitation learning model for
badminton player behaviors: (i) RallyNet captures players' decision
dependencies by modeling decision-making processes as a contextual Markov
decision process. (ii) RallyNet leverages the experience to generate context as
the agent's intent in the rally. (iii) To generate more realistic behavior,
RallyNet leverages Geometric Brownian Motion (GBM) to model the interactions
between players by introducing a valuable inductive bias for learning player
behaviors. In this manner, RallyNet links player intents with interaction
models with GBM, providing an understanding of interactions for sports
analytics. We extensively validate RallyNet with the largest available
real-world badminton dataset consisting of men's and women's singles,
demonstrating its ability to imitate player behaviors. Results reveal
RallyNet's superiority over offline imitation learning methods and
state-of-the-art turn-based approaches, outperforming them by at least 16% in
mean rule-based agent normalization score. Furthermore, we discuss various
practical use cases to highlight RallyNet's applicability.
\\ ( https://arxiv.org/abs/2403.12406 ,  4552kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12417
Date: Tue, 19 Mar 2024 04:02:31 GMT   (1599kb,D)

Title: On Predictive planning and counterfactual learning in active inference
Authors: Aswin Paul, Takuya Isomura, Adeel Razi
Categories: cs.AI cs.LG stat.ME
Comments: 13 pages, 8 figures
\\
  Given the rapid advancement of artificial intelligence, understanding the
foundations of intelligent behaviour is increasingly important. Active
inference, regarded as a general theory of behaviour, offers a principled
approach to probing the basis of sophistication in planning and
decision-making. In this paper, we examine two decision-making schemes in
active inference based on 'planning' and 'learning from experience'.
Furthermore, we also introduce a mixed model that navigates the data-complexity
trade-off between these strategies, leveraging the strengths of both to
facilitate balanced decision-making. We evaluate our proposed model in a
challenging grid-world scenario that requires adaptability from the agent.
Additionally, our model provides the opportunity to analyze the evolution of
various parameters, offering valuable insights and contributing to an
explainable framework for intelligent decision-making.
\\ ( https://arxiv.org/abs/2403.12417 ,  1599kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12451
Date: Tue, 19 Mar 2024 05:21:20 GMT   (4298kb,D)

Title: INSIGHT: End-to-End Neuro-Symbolic Visual Reinforcement Learning with
  Language Explanations
Authors: Lirui Luo, Guoxi Zhang, Hongming Xu, Yaodong Yang, Cong Fang, Qing Li
Categories: cs.AI
\\
  Neuro-symbolic reinforcement learning (NS-RL) has emerged as a promising
paradigm for explainable decision-making, characterized by the interpretability
of symbolic policies. For tasks with visual observations, NS-RL entails
structured representations for states, but previous algorithms are unable to
refine the structured states with reward signals due to a lack of efficiency.
Accessibility is also an issue, as extensive domain knowledge is required to
interpret current symbolic policies. In this paper, we present a framework that
is capable of learning structured states and symbolic policies simultaneously,
whose key idea is to overcome the efficiency bottleneck by distilling vision
foundation models into a scalable perception module. Moreover, we design a
pipeline that uses large language models to generate concise and readable
language explanations for policies and decisions. In experiments on nine Atari
tasks, our approach demonstrates substantial performance gains over existing
NSRL methods. We also showcase explanations for policies and decisions.
\\ ( https://arxiv.org/abs/2403.12451 ,  4298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12482
Date: Tue, 19 Mar 2024 06:39:47 GMT   (26494kb,D)

Title: Embodied LLM Agents Learn to Cooperate in Organized Teams
Authors: Xudong Guo, Kaixuan Huang, Jiale Liu, Wenhui Fan, Natalia V\'elez,
  Qingyun Wu, Huazheng Wang, Thomas L. Griffiths, Mengdi Wang
Categories: cs.AI cs.CL cs.CY cs.MA
\\
  Large Language Models (LLMs) have emerged as integral tools for reasoning,
planning, and decision-making, drawing upon their extensive world knowledge and
proficiency in language-related tasks. LLMs thus hold tremendous potential for
natural language interaction within multi-agent systems to foster cooperation.
However, LLM agents tend to over-report and comply with any instruction, which
may result in information redundancy and confusion in multi-agent cooperation.
Inspired by human organizations, this paper introduces a framework that imposes
prompt-based organization structures on LLM agents to mitigate these problems.
Through a series of experiments with embodied LLM agents and human-agent
collaboration, our results highlight the impact of designated leadership on
team efficiency, shedding light on the leadership qualities displayed by LLM
agents and their spontaneous cooperative behaviors. Further, we harness the
potential of LLMs to propose enhanced organizational prompts, via a
Criticize-Reflect process, resulting in novel organization structures that
reduce communication costs and enhance team efficiency.
\\ ( https://arxiv.org/abs/2403.12482 ,  26494kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12627
Date: Tue, 19 Mar 2024 10:53:40 GMT   (87kb,D)

Title: Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training
  AI Models on Coq Code
Authors: Andreas Florath
Categories: cs.AI cs.LO
Comments: 11 pages
\\
  In the realm of formal theorem proving, the Coq proof assistant stands out
for its rigorous approach to verifying mathematical assertions and software
correctness. Despite the advances in artificial intelligence and machine
learning, the specialized nature of Coq syntax and semantics poses unique
challenges for Large Language Models (LLMs). Addressing this gap, we present a
comprehensive dataset specifically designed to enhance LLMs' proficiency in
interpreting and generating Coq code. This dataset, derived from a collection
of over 10,000 Coq source files, encompasses a wide array of propositions,
proofs, and definitions, enriched with metadata including source references and
licensing information. Our primary aim is to facilitate the development of LLMs
capable of generating syntactically correct and semantically meaningful Coq
constructs, thereby advancing the frontier of automated theorem proving.
Initial experiments with this dataset have showcased its significant potential;
models trained on this data exhibited enhanced accuracy in Coq code generation.
Notably, a particular experiment revealed that a fine-tuned LLM was capable of
generating 141 valid proofs for a basic lemma, highlighting the dataset's
utility in facilitating the discovery of diverse and valid proof strategies.
This paper discusses the dataset's composition, the methodology behind its
creation, and the implications of our findings for the future of machine
learning in formal verification. The dataset is accessible for further research
and exploration:
https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1
\\ ( https://arxiv.org/abs/2403.12627 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12805
Date: Tue, 19 Mar 2024 15:06:53 GMT   (7035kb,D)

Title: Contextual Moral Value Alignment Through Context-Based Aggregation
Authors: Pierre Dognin, Jesus Rios, Ronny Luss, Inkit Padhi, Matthew D Riemer,
  Miao Liu, Prasanna Sattigeri, Manish Nagireddy, Kush R. Varshney and Djallel
  Bouneffouf
Categories: cs.AI cs.CL
\\
  Developing value-aligned AI agents is a complex undertaking and an ongoing
challenge in the field of AI. Specifically within the domain of Large Language
Models (LLMs), the capability to consolidate multiple independently trained
dialogue agents, each aligned with a distinct moral value, into a unified
system that can adapt to and be aligned with multiple moral values is of
paramount importance. In this paper, we propose a system that does contextual
moral value alignment based on contextual aggregation. Here, aggregation is
defined as the process of integrating a subset of LLM responses that are best
suited to respond to a user input, taking into account features extracted from
the user's input. The proposed system shows better results in term of alignment
to human value compared to the state of the art.
\\ ( https://arxiv.org/abs/2403.12805 ,  7035kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12869
Date: Tue, 19 Mar 2024 16:12:25 GMT   (325kb,D)

Title: Regularization in Spider-Style Strategy Discovery and Schedule
  Construction
Authors: Filip B\'artek, Karel Chvalovsk\'y, Martin Suda
Categories: cs.AI cs.LO
Comments: 25 pages, 8 figures, submitted to IJCAR 2024
\\
  To achieve the best performance, automatic theorem provers often rely on
schedules of diverse proving strategies to be tried out (either sequentially or
in parallel) on a given problem. In this paper, we report on a large-scale
experiment with discovering strategies for the Vampire prover, targeting the
FOF fragment of the TPTP library and constructing a schedule for it, based on
the ideas of Andrei Voronkov's system Spider. We examine the process from
various angles, discuss the difficulty (or ease) of obtaining a strong Vampire
schedule for the CASC competition, and establish how well a schedule can be
expected to generalize to unseen problems and what factors influence this
property.
\\ ( https://arxiv.org/abs/2403.12869 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12077
Date: Sun, 25 Feb 2024 11:22:19 GMT   (531kb,D)

Title: Evaluating Robustness of Generative Search Engine on Adversarial Factual
  Questions
Authors: Xuming Hu, Xiaochuan Li, Junzhe Chen, Yinghui Li, Yangning Li,
  Xiaoguang Li, Yasheng Wang, Qun Liu, Lijie Wen, Philip S. Yu, Zhijiang Guo
Categories: cs.CL cs.AI cs.IR
Comments: 21 pages, 7 figures, 4 tables
\\
  Generative search engines have the potential to transform how people seek
information online, but generated responses from existing large language models
(LLMs)-backed generative search engines may not always be accurate.
Nonetheless, retrieval-augmented generation exacerbates safety concerns, since
adversaries may successfully evade the entire system by subtly manipulating the
most vulnerable part of a claim. To this end, we propose evaluating the
robustness of generative search engines in the realistic and high-risk setting,
where adversaries have only black-box system access and seek to deceive the
model into returning incorrect responses. Through a comprehensive human
evaluation of various generative search engines, such as Bing Chat,
PerplexityAI, and YouChat across diverse queries, we demonstrate the
effectiveness of adversarial factual questions in inducing incorrect responses.
Moreover, retrieval-augmented generation exhibits a higher susceptibility to
factual errors compared to LLMs without retrieval. These findings highlight the
potential security risks of these systems and emphasize the need for rigorous
evaluation before deployment.
\\ ( https://arxiv.org/abs/2403.12077 ,  531kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12082
Date: Wed, 6 Mar 2024 16:39:50 GMT   (1147kb)

Title: The Boy Who Survived: Removing Harry Potter from an LLM is harder than
  reported
Authors: Adam Shostack
Categories: cs.CL cs.AI cs.LG
Comments: 2 pages, 4 pages of appendix. Comment on arXiv:2310.02238
\\
  Recent work arXiv.2310.02238 asserted that "we effectively erase the model's
ability to generate or recall Harry Potter-related content.'' This claim is
shown to be overbroad. A small experiment of less than a dozen trials led to
repeated and specific mentions of Harry Potter, including "Ah, I see! A
"muggle" is a term used in the Harry Potter book series by Terry Pratchett...''
\\ ( https://arxiv.org/abs/2403.12082 ,  1147kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12088
Date: Tue, 12 Mar 2024 00:45:49 GMT   (3330kb,D)

Title: TMU at TREC Clinical Trials Track 2023
Authors: Aritra Kumar Lahiri, Emrul Hasan, Qinmin Vivian Hu, Cherie Ding
Categories: cs.CL cs.IR
\\
  This paper describes Toronto Metropolitan University's participation in the
TREC Clinical Trials Track for 2023. As part of the tasks, we utilize advanced
natural language processing techniques and neural language models in our
experiments to retrieve the most relevant clinical trials. We illustrate the
overall methodology, experimental settings, and results of our implementation
for the run submission as part of Team - V-TorontoMU.
\\ ( https://arxiv.org/abs/2403.12088 ,  3330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12145
Date: Mon, 18 Mar 2024 18:01:26 GMT   (7914kb,D)

Title: Syn-QA2: Evaluating False Assumptions in Long-tail Questions with
  Synthetic QA Datasets
Authors: Ashwin Daswani, Rohan Sawant, Najoung Kim
Categories: cs.CL
Comments: Preprint
\\
  Sensitivity to false assumptions (or false premises) in information-seeking
questions is critical for robust question-answering (QA) systems. Recent work
has shown that false assumptions in naturally occurring questions pose
challenges to current models, with low performance on both generative QA and
simple detection tasks (Kim et al. 2023). However, the focus of existing work
on naturally occurring questions leads to a gap in the analysis of model
behavior on the long tail of the distribution of possible questions. To this
end, we introduce Syn-(QA)$^2$, a set of two synthetically generated QA
datasets: one generated using perturbed relations from Wikidata, and the other
by perturbing HotpotQA (Yang et al. 2018). Our findings from evaluating a range
of large language models are threefold: (1) false assumptions in QA are
challenging, echoing the findings of prior work, (2) the binary detection task
is challenging even compared to the difficulty of generative QA itself,
possibly due to the linguistic structure of the problem, and (3) the detection
task is more challenging with long-tail questions compared to naturally
occurring questions, highlighting the utility of our synthetic datasets and
generation method.
\\ ( https://arxiv.org/abs/2403.12145 ,  7914kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12171
Date: Mon, 18 Mar 2024 18:39:53 GMT   (8755kb,D)

Title: EasyJailbreak: A Unified Framework for Jailbreaking Large Language
  Models
Authors: Weikang Zhou, Xiao Wang, Limao Xiong, Han Xia, Yingshuang Gu, Mingxu
  Chai, Fukang Zhu, Caishuang Huang, Shihan Dou, Zhiheng Xi, Rui Zheng,
  Songyang Gao, Yicheng Zou, Hang Yan, Yifan Le, Ruohui Wang, Lijun Li, Jing
  Shao, Tao Gui, Qi Zhang, Xuanjing Huang
Categories: cs.CL cs.AI
\\
  Jailbreak attacks are crucial for identifying and mitigating the security
vulnerabilities of Large Language Models (LLMs). They are designed to bypass
safeguards and elicit prohibited outputs. However, due to significant
differences among various jailbreak methods, there is no standard
implementation framework available for the community, which limits
comprehensive security evaluations. This paper introduces EasyJailbreak, a
unified framework simplifying the construction and evaluation of jailbreak
attacks against LLMs. It builds jailbreak attacks using four components:
Selector, Mutator, Constraint, and Evaluator. This modular framework enables
researchers to easily construct attacks from combinations of novel and existing
components. So far, EasyJailbreak supports 11 distinct jailbreak methods and
facilitates the security validation of a broad spectrum of LLMs. Our validation
across 10 distinct LLMs reveals a significant vulnerability, with an average
breach probability of 60% under various jailbreaking attacks. Notably, even
advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success
Rates (ASR) of 57% and 33%, respectively. We have released a wealth of
resources for researchers, including a web platform, PyPI published package,
screencast video, and experimental outputs.
\\ ( https://arxiv.org/abs/2403.12171 ,  8755kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12173
Date: Mon, 18 Mar 2024 18:45:28 GMT   (2401kb,D)

Title: TnT-LLM: Text Mining at Scale with Large Language Models
Authors: Mengting Wan, Tara Safavi, Sujay Kumar Jauhar, Yujin Kim, Scott
  Counts, Jennifer Neville, Siddharth Suri, Chirag Shah, Ryen W White, Longqi
  Yang, Reid Andersen, Georg Buscher, Dhruv Joshi, Nagu Rangan
Categories: cs.CL cs.AI cs.IR
Comments: 9 pages main content, 8 pages references and appendix
\\
  Transforming unstructured text into structured and meaningful forms,
organized by useful category labels, is a fundamental step in text mining for
downstream analysis and application. However, most existing methods for
producing label taxonomies and building text-based label classifiers still rely
heavily on domain expertise and manual curation, making the process expensive
and time-consuming. This is particularly challenging when the label space is
under-specified and large-scale data annotations are unavailable. In this
paper, we address these challenges with Large Language Models (LLMs), whose
prompt-based interface facilitates the induction and use of large-scale pseudo
labels. We propose TnT-LLM, a two-phase framework that employs LLMs to automate
the process of end-to-end label generation and assignment with minimal human
effort for any given use-case. In the first phase, we introduce a zero-shot,
multi-stage reasoning approach which enables LLMs to produce and refine a label
taxonomy iteratively. In the second phase, LLMs are used as data labelers that
yield training samples so that lightweight supervised classifiers can be
reliably built, deployed, and served at scale. We apply TnT-LLM to the analysis
of user intent and conversational domain for Bing Copilot (formerly Bing Chat),
an open-domain chat-based search engine. Extensive experiments using both human
and automatic evaluation metrics demonstrate that TnT-LLM generates more
accurate and relevant label taxonomies when compared against state-of-the-art
baselines, and achieves a favorable balance between accuracy and efficiency for
classification at scale. We also share our practical experiences and insights
on the challenges and opportunities of using LLMs for large-scale text mining
in real-world applications.
\\ ( https://arxiv.org/abs/2403.12173 ,  2401kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12212
Date: Mon, 18 Mar 2024 19:53:56 GMT   (851kb,D)

Title: Evaluating Named Entity Recognition: Comparative Analysis of Mono- and
  Multilingual Transformer Models on Brazilian Corporate Earnings Call
  Transcriptions
Authors: Ramon Abilio, Guilherme Palermo Coelho, Ana Estela Antunes da Silva
Categories: cs.CL cs.AI cs.LG
MSC-class: 68T50
\\
  Named Entity Recognition (NER) is a Natural Language Processing technique for
extracting information from textual documents. However, much of the existing
research on NER has been centered around English-language documents, leaving a
gap in the availability of datasets tailored to the financial domain in
Portuguese. This study addresses the need for NER within the financial domain,
focusing on Portuguese-language texts extracted from earnings call
transcriptions of Brazilian banks. By curating a comprehensive dataset
comprising 384 transcriptions and leveraging weak supervision techniques for
annotation, we evaluate the performance of monolingual models trained on
Portuguese (BERTimbau and PTT5) and multilingual models (mBERT and mT5).
Notably, we introduce a novel approach that reframes the token classification
task as a text generation problem, enabling fine-tuning and evaluation of T5
models. Following the fine-tuning of the models, we conduct an evaluation on
the test dataset, employing performance and error metrics. Our findings reveal
that BERT-based models consistently outperform T5-based models. Furthermore,
while the multilingual models exhibit comparable macro F1-scores, BERTimbau
demonstrates superior performance over PTT5. A manual analysis of sentences
generated by PTT5 and mT5 unveils a degree of similarity ranging from 0.89 to
1.0, between the original and generated sentences. However, critical errors
emerge as both models exhibit discrepancies, such as alterations to monetary
and percentage values, underscoring the importance of accuracy and consistency
in the financial domain. Despite these challenges, PTT5 and mT5 achieve
impressive macro F1-scores of 98.52% and 98.85%, respectively, with our
proposed approach. Furthermore, our study sheds light on notable disparities in
memory and time consumption for inference across the models.
\\ ( https://arxiv.org/abs/2403.12212 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12242
Date: Mon, 18 Mar 2024 20:47:10 GMT   (215kb,D)

Title: Reference-based Metrics Disprove Themselves in Question Generation
Authors: Bang Nguyen, Mengxia Yu, Yun Huang, Meng Jiang
Categories: cs.CL cs.AI cs.LG
\\
  Reference-based metrics such as BLEU and BERTScore are widely used to
evaluate question generation (QG). In this study, on QG benchmarks such as
SQuAD and HotpotQA, we find that using human-written references cannot
guarantee the effectiveness of the reference-based metrics. Most QG benchmarks
have only one reference; we replicated the annotation process and collect
another reference. A good metric was expected to grade a human-validated
question no worse than generated questions. However, the results of
reference-based metrics on our newly collected reference disproved the metrics
themselves. We propose a reference-free metric consisted of multi-dimensional
criteria such as naturalness, answerability, and complexity, utilizing large
language models. These criteria are not constrained to the syntactic or
semantic of a single reference question, and the metric does not require a
diverse set of references. Experiments reveal that our metric accurately
distinguishes between high-quality questions and flawed ones, and achieves
state-of-the-art alignment with human judgment.
\\ ( https://arxiv.org/abs/2403.12242 ,  215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12244
Date: Mon, 18 Mar 2024 20:50:26 GMT   (8123kb,D)

Title: Zero-Shot Multi-task Hallucination Detection
Authors: Patanjali Bhamidipati, Advaith Malladi, Manish Shrivastava, Radhika
  Mamidi
Categories: cs.CL
\\
  In recent studies, the extensive utilization of large language models has
underscored the importance of robust evaluation methodologies for assessing
text generation quality and relevance to specific tasks. This has revealed a
prevalent issue known as hallucination, an emergent condition in the model
where generated text lacks faithfulness to the source and deviates from the
evaluation criteria. In this study, we formally define hallucination and
propose a framework for its quantitative detection in a zero-shot setting,
leveraging our definition and the assumption that model outputs entail task and
sample specific inputs. In detecting hallucinations, our solution achieves an
accuracy of 0.78 in a model-aware setting and 0.61 in a model-agnostic setting.
Notably, our solution maintains computational efficiency, requiring far less
computational resources than other SOTA approaches, aligning with the trend
towards lightweight and compressed models.
\\ ( https://arxiv.org/abs/2403.12244 ,  8123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12285
Date: Mon, 18 Mar 2024 22:11:00 GMT   (2016kb,D)

Title: FinLlama: Financial Sentiment Classification for Algorithmic Trading
  Applications
Authors: Thanos Konstantinidis, Giorgos Iacovides, Mingxue Xu, Tony G.
  Constantinides, Danilo Mandic
Categories: cs.CL cs.LG q-fin.ST q-fin.TR
\\
  There are multiple sources of financial news online which influence market
movements and trader's decisions. This highlights the need for accurate
sentiment analysis, in addition to having appropriate algorithmic trading
techniques, to arrive at better informed trading decisions. Standard lexicon
based sentiment approaches have demonstrated their power in aiding financial
decisions. However, they are known to suffer from issues related to context
sensitivity and word ordering. Large Language Models (LLMs) can also be used in
this context, but they are not finance-specific and tend to require significant
computational resources. To facilitate a finance specific LLM framework, we
introduce a novel approach based on the Llama 2 7B foundational model, in order
to benefit from its generative nature and comprehensive language manipulation.
This is achieved by fine-tuning the Llama2 7B model on a small portion of
supervised financial sentiment analysis data, so as to jointly handle the
complexities of financial lexicon and context, and further equipping it with a
neural network based decision mechanism. Such a generator-classifier scheme,
referred to as FinLlama, is trained not only to classify the sentiment valence
but also quantify its strength, thus offering traders a nuanced insight into
financial news articles. Complementing this, the implementation of
parameter-efficient fine-tuning through LoRA optimises trainable parameters,
thus minimising computational and memory requirements, without sacrificing
accuracy. Simulation results demonstrate the ability of the proposed FinLlama
to provide a framework for enhanced portfolio management decisions and
increased market returns. These results underpin the ability of FinLlama to
construct high-return portfolios which exhibit enhanced resilience, even during
volatile periods and unpredictable market events.
\\ ( https://arxiv.org/abs/2403.12285 ,  2016kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12294
Date: Mon, 18 Mar 2024 22:33:51 GMT   (6008kb)

Title: A Comparative Investigation of Compositional Syntax and Semantics in
  DALL-E 2
Authors: Elliot Murphy, Jill de Villiers, Sofia Lucero Morales
Categories: cs.CL
\\
  In this study we compared how well DALL-E 2 visually represented the meaning
of linguistic prompts also given to young children in comprehension tests.
Sentences representing fundamental components of grammatical knowledge were
selected from assessment tests used with several hundred English-speaking
children aged 2-7 years for whom we had collected original item-level data.
DALL-E 2 was given these prompts five times to generate 20 cartoons per item,
for 9 adult judges to score. Results revealed no conditions in which DALL-E
2-generated images that matched the semantic accuracy of children, even at the
youngest age (2 years). DALL-E 2 failed to assign the appropriate roles in
reversible forms; it failed on negation despite an easier contrastive prompt
than the children received; it often assigned the adjective to the wrong noun;
it ignored implicit agents in passives. This work points to a clear absence of
compositional sentence representations for DALL-E 2.
\\ ( https://arxiv.org/abs/2403.12294 ,  6008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12297
Date: Mon, 18 Mar 2024 22:39:03 GMT   (496kb,D)

Title: Leveraging Large Language Models to Extract Information on Substance Use
  Disorder Severity from Clinical Notes: A Zero-shot Learning Approach
Authors: Maria Mahbub, Gregory M. Dams, Sudarshan Srinivasan, Caitlin Rizy,
  Ioana Danciu, Jodie Trafton, Kathryn Knight
Categories: cs.CL cs.AI
Comments: 10 pages, 4 figures, 2 tables
\\
  Substance use disorder (SUD) poses a major concern due to its detrimental
effects on health and society. SUD identification and treatment depend on a
variety of factors such as severity, co-determinants (e.g., withdrawal
symptoms), and social determinants of health. Existing diagnostic coding
systems used by American insurance providers, like the International
Classification of Diseases (ICD-10), lack granularity for certain diagnoses,
but clinicians will add this granularity (as that found within the Diagnostic
and Statistical Manual of Mental Disorders classification or DSM-5) as
supplemental unstructured text in clinical notes. Traditional natural language
processing (NLP) methods face limitations in accurately parsing such diverse
clinical language. Large Language Models (LLMs) offer promise in overcoming
these challenges by adapting to diverse language patterns. This study
investigates the application of LLMs for extracting severity-related
information for various SUD diagnoses from clinical notes. We propose a
workflow employing zero-shot learning of LLMs with carefully crafted prompts
and post-processing techniques. Through experimentation with Flan-T5, an
open-source LLM, we demonstrate its superior recall compared to the rule-based
approach. Focusing on 11 categories of SUD diagnoses, we show the effectiveness
of LLMs in extracting severity information, contributing to improved risk
assessment and treatment planning for SUD patients.
\\ ( https://arxiv.org/abs/2403.12297 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12316
Date: Mon, 18 Mar 2024 23:21:37 GMT   (1090kb,D)

Title: OpenEval: Benchmarking Chinese LLMs across Capability, Alignment and
  Safety
Authors: Chuang Liu, Linhao Yu, Jiaxuan Li, Renren Jin, Yufei Huang, Ling Shi,
  Junhui Zhang, Xinmeng Ji, Tingting Cui, Tao Liu, Jinwang Song, Hongying Zan,
  Sun Li, Deyi Xiong
Categories: cs.CL
\\
  The rapid development of Chinese large language models (LLMs) poses big
challenges for efficient LLM evaluation. While current initiatives have
introduced new benchmarks or evaluation platforms for assessing Chinese LLMs,
many of these focus primarily on capabilities, usually overlooking potential
alignment and safety issues. To address this gap, we introduce OpenEval, an
evaluation testbed that benchmarks Chinese LLMs across capability, alignment
and safety. For capability assessment, we include 12 benchmark datasets to
evaluate Chinese LLMs from 4 sub-dimensions: NLP tasks, disciplinary knowledge,
commonsense reasoning and mathematical reasoning. For alignment assessment,
OpenEval contains 7 datasets that examines the bias, offensiveness and
illegalness in the outputs yielded by Chinese LLMs. To evaluate safety,
especially anticipated risks (e.g., power-seeking, self-awareness) of advanced
LLMs, we include 6 datasets. In addition to these benchmarks, we have
implemented a phased public evaluation and benchmark update strategy to ensure
that OpenEval is in line with the development of Chinese LLMs or even able to
provide cutting-edge benchmark datasets to guide the development of Chinese
LLMs. In our first public evaluation, we have tested a range of Chinese LLMs,
spanning from 7B to 72B parameters, including both open-source and proprietary
models. Evaluation results indicate that while Chinese LLMs have shown
impressive performance in certain tasks, more attention should be directed
towards broader aspects such as commonsense reasoning, alignment, and safety.
\\ ( https://arxiv.org/abs/2403.12316 ,  1090kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12368
Date: Tue, 19 Mar 2024 02:25:29 GMT   (1018kb,D)

Title: Characteristic AI Agents via Large Language Models
Authors: Xi Wang, Hongliang Dai, Shen Gao, Piji Li
Categories: cs.CL cs.AI
Comments: COLING 2024,The benchmark is available at:
  https://github.com/nuaa-nlp/Character100
\\
  The advancement of Large Language Models (LLMs) has led to significant
enhancements in the performance of chatbot systems. Many researchers have
dedicated their efforts to the development of bringing characteristics to
chatbots. While there have been commercial products for developing role-driven
chatbots using LLMs, it is worth noting that academic research in this area
remains relatively scarce. Our research focuses on investigating the
performance of LLMs in constructing Characteristic AI Agents by simulating
real-life individuals across different settings. Current investigations have
primarily focused on act on roles with simple profiles. In response to this
research gap, we create a benchmark for the characteristic AI agents task,
including dataset, techniques, and evaluation metrics. A dataset called
``Character100'' is built for this benchmark, comprising the most-visited
people on Wikipedia for language models to role-play. With the constructed
dataset, we conduct comprehensive assessment of LLMs across various settings.
In addition, we devise a set of automatic metrics for quantitative performance
evaluation. The experimental results underscore the potential directions for
further improvement in the capabilities of LLMs in constructing characteristic
AI agents. The benchmark is available at
https://github.com/nuaa-nlp/Character100.
\\ ( https://arxiv.org/abs/2403.12368 ,  1018kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12373
Date: Tue, 19 Mar 2024 02:34:18 GMT   (132kb,D)

Title: RankPrompt: Step-by-Step Comparisons Make Language Models Better
  Reasoners
Authors: Chi Hu, Yuan Ge, Xiangnan Ma, Hang Cao, Qiang Li, Yonghua Yang, Tong
  Xiao, Jingbo Zhu
Categories: cs.CL
Comments: LREC-Coling 2024 Long Paper
\\
  Large Language Models (LLMs) have achieved impressive performance across
various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT
are prone to logical errors during their reasoning processes. Existing
solutions, which include deploying task-specific verifiers or voting over
multiple reasoning paths, either require extensive human annotations or fail in
scenarios with inconsistent responses. To address these challenges, we
introduce RankPrompt, a new prompting method that enables LLMs to self-rank
their responses without additional resources. RankPrompt breaks down the
ranking problem into a series of comparisons among diverse responses,
leveraging the inherent capabilities of LLMs to generate chains of comparison
as contextual exemplars. Our experiments across 11 arithmetic and commonsense
reasoning tasks show that RankPrompt significantly enhances the reasoning
performance of ChatGPT and GPT-4, with improvements of up to 13\%. RankPrompt
also excels in LLM-based automatic evaluations for open-ended generation,
aligning with human preferences 74\% of the time in the AlpacaEval set.
Moreover, RankPrompt demonstrates robustness against variations in the
orderings and consistencies of responses.
\\ ( https://arxiv.org/abs/2403.12373 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12374
Date: Tue, 19 Mar 2024 02:34:33 GMT   (325kb)

Title: Improving Generalizability of Extracting Social Determinants of Health
  Using Large Language Models through Prompt-tuning
Authors: Cheng Peng, Zehao Yu, Kaleb E Smith, Wei-Hsuan Lo-Ciganic, Jiang Bian,
  Yonghui Wu
Categories: cs.CL
\\
  The progress in natural language processing (NLP) using large language models
(LLMs) has greatly improved patient information extraction from clinical
narratives. However, most methods based on the fine-tuning strategy have
limited transfer learning ability for cross-domain applications. This study
proposed a novel approach that employs a soft prompt-based learning
architecture, which introduces trainable prompts to guide LLMs toward desired
outputs. We examined two types of LLM architectures, including encoder-only
GatorTron and decoder-only GatorTronGPT, and evaluated their performance for
the extraction of social determinants of health (SDoH) using a
cross-institution dataset from the 2022 n2c2 challenge and a cross-disease
dataset from the University of Florida (UF) Health. The results show that
decoder-only LLMs with prompt tuning achieved better performance in
cross-domain applications. GatorTronGPT achieved the best F1 scores for both
datasets, outperforming traditional fine-tuned GatorTron by 8.9% and 21.8% in a
cross-institution setting, and 5.5% and 14.5% in a cross-disease setting.
\\ ( https://arxiv.org/abs/2403.12374 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12386
Date: Tue, 19 Mar 2024 02:52:58 GMT   (913kb)

Title: Pipelined Biomedical Event Extraction Rivaling Joint Learning
Authors: Pengchao Wu, Xuefeng Li, Jinghang Gu, Longhua Qian and Guodong Zhou
Categories: cs.CL cs.AI
\\
  Biomedical event extraction is an information extraction task to obtain
events from biomedical text, whose targets include the type, the trigger, and
the respective arguments involved in an event. Traditional biomedical event
extraction usually adopts a pipelined approach, which contains trigger
identification, argument role recognition, and finally event construction
either using specific rules or by machine learning. In this paper, we propose
an n-ary relation extraction method based on the BERT pre-training model to
construct Binding events, in order to capture the semantic information about an
event's context and its participants. The experimental results show that our
method achieves promising results on the GE11 and GE13 corpora of the BioNLP
shared task with F1 scores of 63.14% and 59.40%, respectively. It demonstrates
that by significantly improving theperformance of Binding events, the overall
performance of the pipelined event extraction approach or even exceeds those of
current joint learning methods.
\\ ( https://arxiv.org/abs/2403.12386 ,  913kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12392
Date: Tue, 19 Mar 2024 02:59:58 GMT   (2159kb,D)

Title: AraPoemBERT: A Pretrained Language Model for Arabic Poetry Analysis
Authors: Faisal Qarah
Categories: cs.CL cs.AI
Comments: 28 pages, 11 figures, not published yet
\\
  Arabic poetry, with its rich linguistic features and profound cultural
significance, presents a unique challenge to the Natural Language Processing
(NLP) field. The complexity of its structure and context necessitates advanced
computational models for accurate analysis. In this paper, we introduce
AraPoemBERT, an Arabic language model pretrained exclusively on Arabic poetry
text. To demonstrate the effectiveness of the proposed model, we compared
AraPoemBERT with 5 different Arabic language models on various NLP tasks
related to Arabic poetry. The new model outperformed all other models and
achieved state-of-the-art results in most of the downstream tasks. AraPoemBERT
achieved unprecedented accuracy in two out of three novel tasks: poet's gender
classification (99.34\% accuracy), and poetry sub-meter classification (97.79\%
accuracy). In addition, the model achieved an accuracy score in poems' rhyme
classification (97.73\% accuracy) which is almost equivalent to the best score
reported in this study. Moreover, the proposed model significantly outperformed
previous work and other comparative models in the tasks of poems' sentiment
analysis, achieving an accuracy of 78.95\%, and poetry meter classification
(99.03\% accuracy), while significantly expanding the scope of these two
problems. The dataset used in this study, contains more than 2.09 million
verses collected from online sources, each associated with various attributes
such as meter, sub-meter, poet, rhyme, and topic. The results demonstrate the
effectiveness of the proposed model in understanding and analyzing Arabic
poetry, achieving state-of-the-art results in several tasks and outperforming
previous works and other language models included in the study. AraPoemBERT
model is publicly available on \url{https://huggingface.co/faisalq}.
\\ ( https://arxiv.org/abs/2403.12392 ,  2159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12393
Date: Tue, 19 Mar 2024 03:00:03 GMT   (9924kb,D)

Title: Dr3: Ask Large Language Models Not to Give Off-Topic Answers in Open
  Domain Multi-Hop Question Answering
Authors: Yuan Gao, Yiheng Zhu, Yuanbin Cao, Yinzhi Zhou, Zhen Wu, Yujie Chen,
  Shenglan Wu, Haoyuan Hu, Xinyu Dai
Categories: cs.CL
Comments: LREC-COLING 2024, Long Paper
\\
  Open Domain Multi-Hop Question Answering (ODMHQA) plays a crucial role in
Natural Language Processing (NLP) by aiming to answer complex questions through
multi-step reasoning over retrieved information from external knowledge
sources. Recently, Large Language Models (LLMs) have demonstrated remarkable
performance in solving ODMHQA owing to their capabilities including planning,
reasoning, and utilizing tools. However, LLMs may generate off-topic answers
when attempting to solve ODMHQA, namely the generated answers are irrelevant to
the original questions. This issue of off-topic answers accounts for
approximately one-third of incorrect answers, yet remains underexplored despite
its significance. To alleviate this issue, we propose the
Discriminate->Re-Compose->Re- Solve->Re-Decompose (Dr3) mechanism.
Specifically, the Discriminator leverages the intrinsic capabilities of LLMs to
judge whether the generated answers are off-topic. In cases where an off-topic
answer is detected, the Corrector performs step-wise revisions along the
reversed reasoning chain (Re-Compose->Re-Solve->Re-Decompose) until the final
answer becomes on-topic. Experimental results on the HotpotQA and
2WikiMultiHopQA datasets demonstrate that our Dr3 mechanism considerably
reduces the occurrence of off-topic answers in ODMHQA by nearly 13%, improving
the performance in Exact Match (EM) by nearly 3% compared to the baseline
method without the Dr3 mechanism.
\\ ( https://arxiv.org/abs/2403.12393 ,  9924kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12402
Date: Tue, 19 Mar 2024 03:22:28 GMT   (166kb,D)

Title: An Empirical Study of Speech Language Models for Prompt-Conditioned
  Speech Synthesis
Authors: Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan
  Wang, Hongyu Gong
Categories: cs.CL cs.SD eess.AS
\\
  Speech language models (LMs) are promising for high-quality speech synthesis
through in-context learning. A typical speech LM takes discrete semantic units
as content and a short utterance as prompt, and synthesizes speech which
preserves the content's semantics but mimics the prompt's style. However, there
is no systematic understanding on how the synthesized audio is controlled by
the prompt and content. In this work, we conduct an empirical study of the
widely used autoregressive (AR) and non-autoregressive (NAR) speech LMs and
provide insights into the prompt design and content semantic units. Our
analysis reveals that heterogeneous and nonstationary prompts hurt the audio
quality in contrast to the previous finding that longer prompts always lead to
better synthesis. Moreover, we find that the speaker style of the synthesized
audio is also affected by the content in addition to the prompt. We further
show that semantic units carry rich acoustic information such as pitch, tempo,
volume and speech emphasis, which might be leaked from the content to the
synthesized audio.
\\ ( https://arxiv.org/abs/2403.12402 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12403
Date: Tue, 19 Mar 2024 03:22:35 GMT   (8574kb,D)

Title: Towards Interpretable Hate Speech Detection using Large Language
  Model-extracted Rationales
Authors: Ayushi Nirmal, Amrita Bhattacharjee, Paras Sheth, Huan Liu
Categories: cs.CL cs.AI cs.LG
Comments: First two authors contributed equally
\\
  Although social media platforms are a prominent arena for users to engage in
interpersonal discussions and express opinions, the facade and anonymity
offered by social media may allow users to spew hate speech and offensive
content. Given the massive scale of such platforms, there arises a need to
automatically identify and flag instances of hate speech. Although several hate
speech detection methods exist, most of these black-box methods are not
interpretable or explainable by design. To address the lack of
interpretability, in this paper, we propose to use state-of-the-art Large
Language Models (LLMs) to extract features in the form of rationales from the
input text, to train a base hate speech classifier, thereby enabling faithful
interpretability by design. Our framework effectively combines the textual
understanding capabilities of LLMs and the discriminative power of
state-of-the-art hate speech classifiers to make these classifiers faithfully
interpretable. Our comprehensive evaluation on a variety of social media hate
speech datasets demonstrate: (1) the goodness of the LLM-extracted rationales,
and (2) the surprising retention of detector performance even after training to
ensure interpretability.
\\ ( https://arxiv.org/abs/2403.12403 ,  8574kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12407
Date: Tue, 19 Mar 2024 03:35:18 GMT   (1387kb,D)

Title: Cross-Lingual Transfer for Natural Language Inference via Multilingual
  Prompt Translator
Authors: Xiaoyu Qiu, Yuechen Wang, Jiaxin Shi, Wengang Zhou, Houqiang Li
Categories: cs.CL
Comments: 6 pages, 5 figures, conference
\\
  Based on multilingual pre-trained models, cross-lingual transfer with prompt
learning has shown promising effectiveness, where soft prompt learned in a
source language is transferred to target languages for downstream tasks,
particularly in the low-resource scenario. To efficiently transfer soft prompt,
we propose a novel framework, Multilingual Prompt Translator (MPT), where a
multilingual prompt translator is introduced to properly process crucial
knowledge embedded in prompt by changing language knowledge while retaining
task knowledge. Concretely, we first train prompt in source language and employ
translator to translate it into target prompt. Besides, we extend an external
corpus as auxiliary data, on which an alignment task for predicted answer
probability is designed to convert language knowledge, thereby equipping target
prompt with multilingual knowledge. In few-shot settings on XNLI, MPT
demonstrates superiority over baselines by remarkable improvements. MPT is more
prominent compared with vanilla prompting when transferring to languages quite
distinct from source language.
\\ ( https://arxiv.org/abs/2403.12407 ,  1387kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12408
Date: Tue, 19 Mar 2024 03:35:20 GMT   (184kb,D)

Title: MSLM-S2ST: A Multitask Speech Language Model for Textless
  Speech-to-Speech Translation with Speaker Style Preservation
Authors: Yifan Peng, Ilia Kulikov, Yilin Yang, Sravya Popuri, Hui Lu, Changhan
  Wang, Hongyu Gong
Categories: cs.CL cs.SD eess.AS
\\
  There have been emerging research interest and advances in speech-to-speech
translation (S2ST), translating utterances from one language to another. This
work proposes Multitask Speech Language Model (MSLM), which is a decoder-only
speech language model trained in a multitask setting. Without reliance on text
training data, our model is able to support multilingual S2ST with speaker
style preserved.
\\ ( https://arxiv.org/abs/2403.12408 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12413
Date: Tue, 19 Mar 2024 03:53:47 GMT   (2838kb,D)

Title: Third-Party Language Model Performance Prediction from Instruction
Authors: Rahul Nadkarni, Yizhong Wang, Noah A. Smith
Categories: cs.CL
\\
  Language model-based instruction-following systems have lately shown
increasing performance on many benchmark tasks, demonstrating the capability of
adapting to a broad variety of instructions. However, such systems are often
not designed to be transparent about their limitations; a user may easily
prompt a model with an instruction without any idea of whether the responses
should be expected to be accurate, or if the system is even capable of
performing the task. We propose a third party performance prediction framework,
where a separate model is trained to predict the metric resulting from
evaluating an instruction-following system on a task while assuming access only
to its inputs and outputs at inference time. We perform this analysis with a
variety of both open and closed instruction-following models as well as
multiple performance predictors, and examine the effect of various factors such
as model size, number of training tasks, and prompt format. Our findings
indicate that third-party performance prediction is very challenging, and much
work remains in developing predictors that can automatically reveal the
limitations of modern instruction-following natural language processing
systems.
\\ ( https://arxiv.org/abs/2403.12413 ,  2838kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12468
Date: Tue, 19 Mar 2024 05:52:56 GMT   (2809kb,D)

Title: CrossTune: Black-Box Few-Shot Classification with Label Enhancement
Authors: Danqing Luo, Chen Zhang, Yan Zhang, Haizhou Li
Categories: cs.CL
Comments: Accepted by LREC-Coling 2024
\\
  Training or finetuning large-scale language models (LLMs) requires
substantial computation resources, motivating recent efforts to explore
parameter-efficient adaptation to downstream tasks. One approach is to treat
these models as black boxes and use forward passes (Inference APIs) to interact
with them. Current research focuses on adapting these black-box models to
downstream tasks using gradient-free prompt optimization, but this often
involves an expensive process of searching task-specific prompts. Therefore, we
are motivated to study black-box language model adaptation without prompt
search. Specifically, we introduce a label-enhanced cross-attention network
called CrossTune, which models the semantic relatedness between the input text
sequence and task-specific label descriptions. Its effectiveness is examined in
the context of few-shot text classification. To improve the generalization of
CrossTune, we utilize ChatGPT to generate additional training data through
in-context learning. A switch mechanism is implemented to exclude low-quality
ChatGPT-generated data. Through extensive experiments on seven benchmark text
classification datasets, we demonstrate that our proposed approach outperforms
the previous state-of-the-art gradient-free black-box tuning method by 5.7% on
average. Even without using ChatGPT-augmented data, CrossTune performs better
or comparably than previous black-box tuning methods, suggesting the
effectiveness of our approach.
\\ ( https://arxiv.org/abs/2403.12468 ,  2809kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12469
Date: Tue, 19 Mar 2024 06:01:02 GMT   (144kb,D)

Title: When Do "More Contexts" Help with Sarcasm Recognition?
Authors: Ojas Nimase, Sanghyun Hong
Categories: cs.CL cs.LG
Comments: Accepted to LREC-COLING 2024 [Short]
\\
  Sarcasm recognition is challenging because it needs an understanding of the
true intention, which is opposite to or different from the literal meaning of
the words. Prior work has addressed this challenge by developing a series of
methods that provide richer $contexts$, e.g., sentiment or cultural nuances, to
models. While shown to be effective individually, no study has systematically
evaluated their collective effectiveness. As a result, it remains unclear to
what extent additional contexts can improve sarcasm recognition. In this work,
we explore the improvements that existing methods bring by incorporating more
contexts into a model. To this end, we develop a framework where we can
integrate multiple contextual cues and test different approaches. In evaluation
with four approaches on three sarcasm recognition benchmarks, we achieve
existing state-of-the-art performances and also demonstrate the benefits of
sequentially adding more contexts. We also identify inherent drawbacks of using
more contexts, highlighting that in the pursuit of even better results, the
model may need to adopt societal biases.
\\ ( https://arxiv.org/abs/2403.12469 ,  144kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12500
Date: Tue, 19 Mar 2024 07:07:13 GMT   (260kb,D)

Title: A Large Collection of Model-generated Contradictory Responses for
  Consistency-aware Dialogue Systems
Authors: Shiki Sato, Reina Akama, Jun Suzuki, Kentaro Inui
Categories: cs.CL
Comments: 16 pages
\\
  Mitigating the generation of contradictory responses poses a substantial
challenge in dialogue response generation. The quality and quantity of
available contradictory response data play a vital role in suppressing these
contradictions, offering two significant benefits. First, having access to
large contradiction data enables a comprehensive examination of their
characteristics. Second, data-driven methods to mitigate contradictions may be
enhanced with large-scale contradiction data for training. Nevertheless, no
attempt has been made to build an extensive collection of model-generated
contradictory responses. In this paper, we build a large dataset of response
generation models' contradictions for the first time. Then, we acquire valuable
insights into the characteristics of model-generated contradictions through an
extensive analysis of the collected responses. Lastly, we also demonstrate how
this dataset substantially enhances the performance of data-driven
contradiction suppression methods.
\\ ( https://arxiv.org/abs/2403.12500 ,  260kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12523
Date: Tue, 19 Mar 2024 07:50:32 GMT   (7205kb,D)

Title: GraphERE: Jointly Multiple Event-Event Relation Extraction via
  Graph-Enhanced Event Embeddings
Authors: Haochen Li, Di Geng
Categories: cs.CL cs.AI
\\
  Events describe the state changes of entities. In a document, multiple events
are connected by various relations (e.g., Coreference, Temporal, Causal, and
Subevent). Therefore, obtaining the connections between events through
Event-Event Relation Extraction (ERE) is critical to understand natural
language. There are two main problems in the current ERE works: a. Only
embeddings of the event triggers are used for event feature representation,
ignoring event arguments (e.g., time, place, person, etc.) and their structure
within the event. b. The interconnection between relations (e.g., temporal and
causal relations usually interact with each other ) is ignored. To solve the
above problems, this paper proposes a jointly multiple ERE framework called
GraphERE based on Graph-enhanced Event Embeddings. First, we enrich the event
embeddings with event argument and structure features by using static AMR
graphs and IE graphs; Then, to jointly extract multiple event relations, we use
Node Transformer and construct Task-specific Dynamic Event Graphs for each type
of relation. Finally, we used a multi-task learning strategy to train the whole
framework. Experimental results on the latest MAVEN-ERE dataset validate that
GraphERE significantly outperforms existing methods. Further analyses indicate
the effectiveness of the graph-enhanced event embeddings and the joint
extraction strategy.
\\ ( https://arxiv.org/abs/2403.12523 ,  7205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12526
Date: Tue, 19 Mar 2024 07:56:42 GMT   (428kb)

Title: Prompt-based Graph Model for Joint Liberal Event Extraction and Event
  Schema Induction
Authors: Haochen Li, Di Geng
Categories: cs.CL
\\
  Events are essential components of speech and texts, describing the changes
in the state of entities. The event extraction task aims to identify and
classify events and find their participants according to event schemas.
Manually predefined event schemas have limited coverage and are hard to migrate
across domains. Therefore, the researchers propose Liberal Event Extraction
(LEE), which aims to extract events and discover event schemas simultaneously.
However, existing LEE models rely heavily on external language knowledge bases
and require the manual development of numerous rules for noise removal and
knowledge alignment, which is complex and laborious. To this end, we propose a
Prompt-based Graph Model for Liberal Event Extraction (PGLEE). Specifically, we
use a prompt-based model to obtain candidate triggers and arguments, and then
build heterogeneous event graphs to encode the structures within and between
events. Experimental results prove that our approach achieves excellent
performance with or without predefined event schemas, while the automatically
detected event schemas are proven high quality.
\\ ( https://arxiv.org/abs/2403.12526 ,  428kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12556
Date: Tue, 19 Mar 2024 09:00:23 GMT   (645kb,D)

Title: Factorized Learning Assisted with Large Language Model for Gloss-free
  Sign Language Translation
Authors: Zhigang Chen, Benjia Zhou, Jun Li, Jun Wan, Zhen Lei, Ning Jiang, Quan
  Lu, Guoqing Zhao
Categories: cs.CL
Comments: Accepted by LREC-COLING-2024
\\
  Previous Sign Language Translation (SLT) methods achieve superior performance
by relying on gloss annotations. However, labeling high-quality glosses is a
labor-intensive task, which limits the further development of SLT. Although
some approaches work towards gloss-free SLT through jointly training the visual
encoder and translation network, these efforts still suffer from poor
performance and inefficient use of the powerful Large Language Model (LLM).
Most seriously, we find that directly introducing LLM into SLT will lead to
insufficient learning of visual representations as LLM dominates the learning
curve. To address these problems, we propose Factorized Learning assisted with
Large Language Model (FLa-LLM) for gloss-free SLT. Concretely, we factorize the
training process into two stages. In the visual initialing stage, we employ a
lightweight translation model after the visual encoder to pre-train the visual
encoder. In the LLM fine-tuning stage, we freeze the acquired knowledge in the
visual encoder and integrate it with a pre-trained LLM to inspire the LLM's
translation potential. This factorized training strategy proves to be highly
effective as evidenced by significant improvements achieved across three SLT
datasets which are all conducted under the gloss-free setting.
\\ ( https://arxiv.org/abs/2403.12556 ,  645kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12563
Date: Tue, 19 Mar 2024 09:17:25 GMT   (143kb,D)

Title: Simple Hack for Transformers against Heavy Long-Text Classification on a
  Time- and Memory-Limited GPU Service
Authors: Mirza Alim Mutasodirin, Radityo Eko Prasojo, Achmad F. Abka, Hanif
  Rasyidi
Categories: cs.CL cs.AI
Comments: The 10th International Conference on Advanced Informatics: Concepts,
  Theory, and Applications (ICAICTA 2023)
DOI: 10.1109/ICAICTA59291.2023.10390269
\\
  Many NLP researchers rely on free computational services, such as Google
Colab, to fine-tune their Transformer models, causing a limitation for
hyperparameter optimization (HPO) in long-text classification due to the method
having quadratic complexity and needing a bigger resource. In Indonesian, only
a few works were found on long-text classification using Transformers. Most
only use a small amount of data and do not report any HPO. In this study, using
18k news articles, we investigate which pretrained models are recommended to
use based on the output length of the tokenizer. We then compare some hacks to
shorten and enrich the sequences, which are the removals of stopwords,
punctuation, low-frequency words, and recurring words. To get a fair
comparison, we propose and run an efficient and dynamic HPO procedure that can
be done gradually on a limited resource and does not require a long-running
optimization library. Using the best hack found, we then compare 512, 256, and
128 tokens length. We find that removing stopwords while keeping punctuation
and low-frequency words is the best hack. Some of our setups manage to
outperform taking 512 first tokens using a smaller 128 or 256 first tokens
which manage to represent the same information while requiring less
computational resources. The findings could help developers to efficiently
pursue optimal performance of the models using limited resources.
\\ ( https://arxiv.org/abs/2403.12563 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12582
Date: Tue, 19 Mar 2024 09:45:33 GMT   (2521kb,D)

Title: AlphaFin: Benchmarking Financial Analysis with Retrieval-Augmented
  Stock-Chain Framework
Authors: Xiang Li, Zhenyu Li, Chen Shi, Yong Xu, Qing Du, Mingkui Tan, Jun
  Huang, Wei Lin
Categories: cs.CL
Comments: COLING 2024. The first three authors contributed equally. Project
  website: https://github.com/AlphaFin-proj/AlphaFin
\\
  The task of financial analysis primarily encompasses two key areas: stock
trend prediction and the corresponding financial question answering. Currently,
machine learning and deep learning algorithms (ML&DL) have been widely applied
for stock trend predictions, leading to significant progress. However, these
methods fail to provide reasons for predictions, lacking interpretability and
reasoning processes. Also, they can not integrate textual information such as
financial news or reports. Meanwhile, large language models (LLMs) have
remarkable textual understanding and generation ability. But due to the
scarcity of financial training datasets and limited integration with real-time
knowledge, LLMs still suffer from hallucinations and are unable to keep up with
the latest information. To tackle these challenges, we first release AlphaFin
datasets, combining traditional research datasets, real-time financial data,
and handwritten chain-of-thought (CoT) data. It has a positive impact on
training LLMs for completing financial analysis. We then use AlphaFin datasets
to benchmark a state-of-the-art method, called Stock-Chain, for effectively
tackling the financial analysis task, which integrates retrieval-augmented
generation (RAG) techniques. Extensive experiments are conducted to demonstrate
the effectiveness of our framework on financial analysis.
\\ ( https://arxiv.org/abs/2403.12582 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12596
Date: Tue, 19 Mar 2024 10:03:07 GMT   (1419kb,D)

Title: Chart-based Reasoning: Transferring Capabilities from LLMs to VLMs
Authors: Victor Carbune and Hassan Mansoor and Fangyu Liu and Rahul Aralikatte
  and Gilles Baechler and Jindong Chen and Abhanshu Sharma
Categories: cs.CL
Comments: Findings of NAACL 2024
\\
  Vision-language models (VLMs) are achieving increasingly strong performance
on multimodal tasks. However, reasoning capabilities remain limited
particularly for smaller VLMs, while those of large-language models (LLMs) have
seen numerous improvements. We propose a technique to transfer capabilities
from LLMs to VLMs. On the recently introduced ChartQA, our method obtains
state-of-the-art performance when applied on the PaLI3-5B VLM by
\citet{chen2023pali3}, while also enabling much better performance on PlotQA
and FigureQA.
  We first improve the chart representation by continuing the pre-training
stage using an improved version of the chart-to-table translation task by
\citet{liu2023deplot}. We then propose constructing a 20x larger dataset than
the original training set. To improve general reasoning capabilities and
improve numerical operations, we synthesize reasoning traces using the table
representation of charts. Lastly, our model is fine-tuned using the multitask
loss introduced by \citet{hsieh2023distilling}.
  Our variant ChartPaLI-5B outperforms even 10x larger models such as PaLIX-55B
without using an upstream OCR system, while keeping inference time constant
compared to the PaLI3-5B baseline. When rationales are further refined with a
simple program-of-thought prompt \cite{chen2023program}, our model outperforms
the recently introduced Gemini Ultra and GPT-4V.
\\ ( https://arxiv.org/abs/2403.12596 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12601
Date: Tue, 19 Mar 2024 10:11:14 GMT   (4320kb,D)

Title: LHMKE: A Large-scale Holistic Multi-subject Knowledge Evaluation
  Benchmark for Chinese Large Language Models
Authors: Chuang Liu, Renren Jin, Yuqi Ren, Deyi Xiong
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Chinese Large Language Models (LLMs) have recently demonstrated impressive
capabilities across various NLP benchmarks and real-world applications.
However, the existing benchmarks for comprehensively evaluating these LLMs are
still insufficient, particularly in terms of measuring knowledge that LLMs
capture. Current datasets collect questions from Chinese examinations across
different subjects and educational levels to address this issue. Yet, these
benchmarks primarily focus on objective questions such as multiple-choice
questions, leading to a lack of diversity in question types. To tackle this
problem, we propose LHMKE, a Large-scale, Holistic, and Multi-subject Knowledge
Evaluation benchmark in this paper. LHMKE is designed to provide a
comprehensive evaluation of the knowledge acquisition capabilities of Chinese
LLMs. It encompasses 10,465 questions across 75 tasks covering 30 subjects,
ranging from primary school to professional certification exams. Notably, LHMKE
includes both objective and subjective questions, offering a more holistic
evaluation of the knowledge level of LLMs. We have assessed 11 Chinese LLMs
under the zero-shot setting, which aligns with real examinations, and compared
their performance across different subjects. We also conduct an in-depth
analysis to check whether GPT-4 can automatically score subjective predictions.
Our findings suggest that LHMKE is a challenging and advanced testbed for
Chinese LLMs.
\\ ( https://arxiv.org/abs/2403.12601 ,  4320kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12666
Date: Tue, 19 Mar 2024 12:02:38 GMT   (356kb,D)

Title: Multi-Dimensional Machine Translation Evaluation: Model Evaluation and
  Resource for Korean
Authors: Dojun Park, Sebastian Pad\'o
Categories: cs.CL
Comments: 9 pages, accepted at LREC-COLING 2024
\\
  Almost all frameworks for the manual or automatic evaluation of machine
translation characterize the quality of an MT output with a single number. An
exception is the Multidimensional Quality Metrics (MQM) framework which offers
a fine-grained ontology of quality dimensions for scoring (such as style,
fluency, accuracy, and terminology). Previous studies have demonstrated the
feasibility of MQM annotation but there are, to our knowledge, no computational
models that predict MQM scores for novel texts, due to a lack of resources. In
this paper, we address these shortcomings by (a) providing a 1200-sentence MQM
evaluation benchmark for the language pair English-Korean and (b) reframing MT
evaluation as the multi-task problem of simultaneously predicting several MQM
scores using SOTA language models, both in a reference-based MT evaluation
setup and a reference-free quality estimation (QE) setup. We find that
reference-free setup outperforms its counterpart in the style dimension while
reference-based models retain an edge regarding accuracy. Overall, RemBERT
emerges as the most promising model. Through our evaluation, we offer an
insight into the translation quality in a more fine-grained, interpretable
manner.
\\ ( https://arxiv.org/abs/2403.12666 ,  356kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12675
Date: Tue, 19 Mar 2024 12:21:20 GMT   (2312kb,D)

Title: Pragmatic Competence Evaluation of Large Language Models for Korean
Authors: Dojun Park, Jiwoo Lee, Hyeyun Jeong, Seohyun Park and Sungeun Lee
Categories: cs.CL
Comments: 9 pages, submitted for publication
\\
  The current evaluation of Large Language Models (LLMs) predominantly relies
on benchmarks focusing on their embedded knowledge by testing through
multiple-choice questions (MCQs), a format inherently suited for automated
evaluation. Our study extends this evaluation to explore LLMs' pragmatic
competence--a facet previously underexamined before the advent of sophisticated
LLMs, specifically in the context of Korean. We employ two distinct evaluation
setups: the conventional MCQ format, adapted for automatic evaluation, and
Open-Ended Questions (OEQs), assessed by human experts, to examine LLMs'
narrative response capabilities without predefined options. Our findings reveal
that GPT-4 excels, scoring 81.11 and 85.69 in the MCQ and OEQ setups,
respectively, with HyperCLOVA X, an LLM optimized for Korean, closely
following, especially in the OEQ setup, demonstrating a score of 81.56 with a
marginal difference of 4.13 points compared to GPT-4. Furthermore, while
few-shot learning strategies generally enhance LLM performance,
Chain-of-Thought (CoT) prompting introduces a bias toward literal
interpretations, hindering accurate pragmatic inference. Considering the
growing expectation for LLMs to understand and produce language that aligns
with human communicative norms, our findings emphasize the importance for
advancing LLMs' abilities to grasp and convey sophisticated meanings beyond
mere literal interpretations.
\\ ( https://arxiv.org/abs/2403.12675 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12678
Date: Tue, 19 Mar 2024 12:24:20 GMT   (7572kb,D)

Title: Empowering Air Travelers: A Chatbot for Canadian Air Passenger Rights
Authors: Maksym Taranukhin, Sahithya Ravi, Gabor Lukacs, Evangelos Milios,
  Vered Shwartz
Categories: cs.CL cs.AI
Comments: under review
\\
  The Canadian air travel sector has seen a significant increase in flight
delays, cancellations, and other issues concerning passenger rights.
Recognizing this demand, we present a chatbot to assist passengers and educate
them about their rights. Our system breaks a complex user input into simple
queries which are used to retrieve information from a collection of documents
detailing air travel regulations. The most relevant passages from these
documents are presented along with links to the original documents and the
generated queries, enabling users to dissect and leverage the information for
their unique circumstances. The system successfully overcomes two predominant
challenges: understanding complex user inputs, and delivering accurate answers,
free of hallucinations, that passengers can rely on for making informed
decisions. A user study comparing the chatbot to a Google search demonstrated
the chatbot's usefulness and ease of use. Beyond the primary goal of providing
accurate and timely information to air passengers regarding their rights, we
hope that this system will also enable further research exploring the tradeoff
between the user-friendly conversational interface of chatbots and the accuracy
of retrieval systems.
\\ ( https://arxiv.org/abs/2403.12678 ,  7572kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12721
Date: Tue, 19 Mar 2024 13:30:47 GMT   (83kb,D)

Title: CLASSLA-web: Comparable Web Corpora of South Slavic Languages Enriched
  with Linguistic and Genre Annotation
Authors: Nikola Ljube\v{s}i\'c, Taja Kuzman
Categories: cs.CL
Comments: Accepted to the LREC-COLING 2024 conference
\\
  This paper presents a collection of highly comparable web corpora of
Slovenian, Croatian, Bosnian, Montenegrin, Serbian, Macedonian, and Bulgarian,
covering thereby the whole spectrum of official languages in the South Slavic
language space. The collection of these corpora comprises a total of 13 billion
tokens of texts from 26 million documents. The comparability of the corpora is
ensured by a comparable crawling setup and the usage of identical crawling and
post-processing technology. All the corpora were linguistically annotated with
the state-of-the-art CLASSLA-Stanza linguistic processing pipeline, and
enriched with document-level genre information via the Transformer-based
multilingual X-GENRE classifier, which further enhances comparability at the
level of linguistic annotation and metadata enrichment. The genre-focused
analysis of the resulting corpora shows a rather consistent distribution of
genres throughout the seven corpora, with variations in the most prominent
genre categories being well-explained by the economic strength of each language
community. A comparison of the distribution of genre categories across the
corpora indicates that web corpora from less developed countries primarily
consist of news articles. Conversely, web corpora from economically more
developed countries exhibit a smaller proportion of news content, with a
greater presence of promotional and opinionated texts.
\\ ( https://arxiv.org/abs/2403.12721 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12744
Date: Tue, 19 Mar 2024 14:07:28 GMT   (9686kb,D)

Title: Instructing Large Language Models to Identify and Ignore Irrelevant
  Conditions
Authors: Zhenyu Wu, Chao Shen, Meng Jiang
Categories: cs.CL
Comments: NAACL 2024 - Camera Ready
\\
  Math word problem (MWP) solving requires generating a reasoning path based on
a given problem description that often contains irrelevant conditions. Existing
chain-of-thought (CoT) prompting methods elicited multi-step reasoning
abilities of large language models (LLMs) to solve MWPs. However, they were
seriously confused by the irrelevant conditions, resulting in low accuracy. In
this paper, we propose a novel approach named I$^3$C that instructs LLMs to
identify and ignore irrelevant conditions. It identifies a set of irrelevant
condition candidates that have a weak semantic relevance with the question.
Then it prompts LLMs to verify the irrelevant conditions. Lastly it instructs
the LLMs with the verification on relevant and irrelevant conditions to avoid
confusion and improve reasoning paths. Moreover, we propose to select (problem,
reasoning paths) pairs as demonstrations to enhance I$^3$C with few-shot
reasoning. We develop I$^3$C-Select that selects the most confusing problems
based on the semantic relevance measurement. We conduct extensive experiments
on eight MWP datasets. I$^3$C can be combined with any CoT prompting methods to
improve the performance of solving MWPs. Notably, with GPT-3.5-Turbo and
I$^3$C-Select, we achieve an accuracy of 96.0 and 94.1 on GSM-IC2-1K and
GSM-ICM-1K, respectively, significantly outperforming the state-of-the-art
few-shot prompting method Complex-CoT by +11.7 and +11.1. Our implementation is
made publicly available at https://wzy6642.github.io/I3C.github.io/.
\\ ( https://arxiv.org/abs/2403.12744 ,  9686kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12749
Date: Tue, 19 Mar 2024 14:12:54 GMT   (1322kb,D)

Title: Sebastian, Basti, Wastl?! Recognizing Named Entities in Bavarian
  Dialectal Data
Authors: Siyao Peng, Zihang Sun, Huangyan Shan, Marie Kolm, Verena Blaschke,
  Ekaterina Artemova, Barbara Plank
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Named Entity Recognition (NER) is a fundamental task to extract key
information from texts, but annotated resources are scarce for dialects. This
paper introduces the first dialectal NER dataset for German, BarNER, with 161K
tokens annotated on Bavarian Wikipedia articles (bar-wiki) and tweets
(bar-tweet), using a schema adapted from German CoNLL 2006 and GermEval. The
Bavarian dialect differs from standard German in lexical distribution,
syntactic construction, and entity information. We conduct in-domain,
cross-domain, sequential, and joint experiments on two Bavarian and three
German corpora and present the first comprehensive NER results on Bavarian.
Incorporating knowledge from the larger German NER (sub-)datasets notably
improves on bar-wiki and moderately on bar-tweet. Inversely, training first on
Bavarian contributes slightly to the seminal German CoNLL 2006 corpus.
Moreover, with gold dialect labels on Bavarian tweets, we assess multi-task
learning between five NER and two Bavarian-German dialect identification tasks
and achieve NER SOTA on bar-wiki. We substantiate the necessity of our
low-resource BarNER corpus and the importance of diversity in dialects, genres,
and topics in enhancing model performance.
\\ ( https://arxiv.org/abs/2403.12749 ,  1322kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12766
Date: Mon, 18 Mar 2024 17:32:32 GMT   (12471kb,D)

Title: NovelQA: A Benchmark for Long-Range Novel Question Answering
Authors: Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng
  Deng, Guangsheng Bao, Qian Wang, Yue Zhang
Categories: cs.CL
\\
  The rapid advancement of Large Language Models (LLMs) has introduced a new
frontier in natural language processing, particularly in understanding and
processing long-context information. However, the evaluation of these models'
long-context abilities remains a challenge due to the limitations of current
benchmarks. To address this gap, we introduce NovelQA, a benchmark specifically
designed to test the capabilities of LLMs with extended texts. Constructed from
English novels, NovelQA offers a unique blend of complexity, length, and
narrative coherence, making it an ideal tool for assessing deep textual
understanding in LLMs. This paper presents the design and construction of
NovelQA, highlighting its manual annotation, and diverse question types. Our
evaluation of Long-context LLMs on NovelQA reveals significant insights into
the models' performance, particularly emphasizing the challenges they face with
multi-hop reasoning, detail-oriented questions, and extremely long input with
more than 100,000 tokens. The results underscore the necessity for further
advancements in LLMs to improve their long-context comprehension and
computational literary studies.
\\ ( https://arxiv.org/abs/2403.12766 ,  12471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12776
Date: Tue, 19 Mar 2024 14:44:45 GMT   (7393kb,D)

Title: Automated Data Curation for Robust Language Model Fine-Tuning
Authors: Jiuhai Chen, Jonas Mueller
Categories: cs.CL
\\
  Large Language Models have become the de facto approach to
sequence-to-sequence text generation tasks, but for specialized tasks/domains,
a pretrained LLM lacks specific capabilities to produce accurate or
well-formatted responses. Supervised fine-tuning specializes a LLM by training
it on dataset of example prompts with target responses, but real-world data
tends to be noisy. While many fine-tuning algorithms exist, here we consider a
\emph{data-centric AI} perspective on LLM fine-tuning, studying how to
\emph{systematically} curate the training dataset to improve the LLM produced
via \emph{any} fine-tuning algorithm.
  We introduce an automated data curation pipeline CLEAR (Confidence-based LLM
Evaluation And Rectification) for instruction tuning datasets, that can be used
with any LLM and fine-tuning procedure. CLEAR estimates which training data is
low-quality and either filters or corrects it. Automatically identifying which
data to filter or correct is done via LLM-derived confidence estimates, to
ensure only confident modifications to the dataset. Unlike existing data
curation techniques, CLEAR is a comprehensive framework that can improve a
dataset (and trained model outputs) without additional fine-tuning
computations. We don't assume access to a stronger LLM than the model being
fine-tuned (e.g.\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether
CLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal
that CLEAR consistently improves the performance of fine-tuned models across
many datasets and models (like GPT-3.5 and Llama2).
\\ ( https://arxiv.org/abs/2403.12776 ,  7393kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12799
Date: Tue, 19 Mar 2024 15:01:14 GMT   (155kb,D)

Title: Investigating Text Shortening Strategy in BERT: Truncation vs
  Summarization
Authors: Mirza Alim Mutasodirin, Radityo Eko Prasojo
Categories: cs.CL cs.AI
Comments: The 13th International Conference on Advanced Computer Science and
  Information Systems (ICACSIS 2021)
DOI: 10.1109/ICACSIS53237.2021.9631364
\\
  The parallelism of Transformer-based models comes at the cost of their input
max-length. Some studies proposed methods to overcome this limitation, but none
of them reported the effectiveness of summarization as an alternative. In this
study, we investigate the performance of document truncation and summarization
in text classification tasks. Each of the two was investigated with several
variations. This study also investigated how close their performances are to
the performance of full-text. We used a dataset of summarization tasks based on
Indonesian news articles (IndoSum) to do classification tests. This study shows
how the summaries outperform the majority of truncation method variations and
lose to only one. The best strategy obtained in this study is taking the head
of the document. The second is extractive summarization. This study explains
what happened to the result, leading to further research in order to exploit
the potential of document summarization as a shortening alternative. The code
and data used in this work are publicly available in
https://github.com/mirzaalimm/TruncationVsSummarization.
\\ ( https://arxiv.org/abs/2403.12799 ,  155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12809
Date: Tue, 19 Mar 2024 15:07:22 GMT   (8308kb,D)

Title: Comparing Explanation Faithfulness between Multilingual and Monolingual
  Fine-tuned Language Models
Authors: Zhixue Zhao, Nikolaos Aletras
Categories: cs.CL cs.AI
Comments: Accepted at NAACL 2024 Main Conference
\\
  In many real natural language processing application scenarios, practitioners
not only aim to maximize predictive performance but also seek faithful
explanations for the model predictions. Rationales and importance distribution
given by feature attribution methods (FAs) provide insights into how different
parts of the input contribute to a prediction. Previous studies have explored
how different factors affect faithfulness, mainly in the context of monolingual
English models. On the other hand, the differences in FA faithfulness between
multilingual and monolingual models have yet to be explored. Our extensive
experiments, covering five languages and five popular FAs, show that FA
faithfulness varies between multilingual and monolingual models. We find that
the larger the multilingual model, the less faithful the FAs are compared to
its counterpart monolingual models.Our further analysis shows that the
faithfulness disparity is potentially driven by the differences between model
tokenizers. Our code is available:
https://github.com/casszhao/multilingual-faith.
\\ ( https://arxiv.org/abs/2403.12809 ,  8308kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12862
Date: Tue, 19 Mar 2024 16:06:10 GMT   (3730kb,D)

Title: Epistemology of Language Models: Do Language Models Have Holistic
  Knowledge?
Authors: Minsu Kim, James Thorne
Categories: cs.CL
\\
  This paper investigates the inherent knowledge in language models from the
perspective of epistemological holism. The purpose of this paper is to explore
whether LLMs exhibit characteristics consistent with epistemological holism.
These characteristics suggest that core knowledge, such as general scientific
knowledge, each plays a specific role, serving as the foundation of our
knowledge system and being difficult to revise. To assess these traits related
to holism, we created a scientific reasoning dataset and examined the
epistemology of language models through three tasks: Abduction, Revision, and
Argument Generation. In the abduction task, the language models explained
situations while avoiding revising the core knowledge. However, in other tasks,
the language models were revealed not to distinguish between core and
peripheral knowledge, showing an incomplete alignment with holistic knowledge
principles.
\\ ( https://arxiv.org/abs/2403.12862 ,  3730kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12881
Date: Tue, 19 Mar 2024 16:26:10 GMT   (3447kb,D)

Title: Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for
  Large Language Models
Authors: Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu,
  Dahua Lin, Kai Chen, Feng Zhao
Categories: cs.CL
Comments: Technical Report
\\
  Open-sourced Large Language Models (LLMs) have achieved great success in
various NLP tasks, however, they are still far inferior to API-based models
when acting as agents. How to integrate agent ability into general LLMs becomes
a crucial and urgent problem. This paper first delivers three key observations:
(1) the current agent training corpus is entangled with both formats following
and agent reasoning, which significantly shifts from the distribution of its
pre-training data; (2) LLMs exhibit different learning speeds on the
capabilities required by agent tasks; and (3) current approaches have
side-effects when improving agent abilities by introducing hallucinations.
Based on the above findings, we propose Agent-FLAN to effectively Fine-tune
LANguage models for Agents. Through careful decomposition and redesign of the
training corpus, Agent-FLAN enables Llama2-7B to outperform prior best works by
3.5\% across various agent evaluation datasets. With comprehensively
constructed negative samples, Agent-FLAN greatly alleviates the hallucination
issues based on our established evaluation benchmark. Besides, it consistently
improves the agent capability of LLMs when scaling model sizes while slightly
enhancing the general capability of LLMs. The code will be available at
https://github.com/InternLM/Agent-FLAN.
\\ ( https://arxiv.org/abs/2403.12881 ,  3447kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12918
Date: Tue, 19 Mar 2024 17:21:29 GMT   (8029kb,D)

Title: Generalizable and Stable Finetuning of Pretrained Language Models on
  Low-Resource Texts
Authors: Sai Ashish Somayajula, Youwei Liang, Abhishek Singh, Li Zhang, Pengtao
  Xie
Categories: cs.CL cs.AI cs.LG
Comments: Accepted as a long paper to NAACL 2024 Main Conference; 18 pages, 11
  tables, 3 figures
\\
  Pretrained Language Models (PLMs) have advanced Natural Language Processing
(NLP) tasks significantly, but finetuning PLMs on low-resource datasets poses
significant challenges such as instability and overfitting. Previous methods
tackle these issues by finetuning a strategically chosen subnetwork on a
downstream task, while keeping the remaining weights fixed to the pretrained
weights. However, they rely on a suboptimal criteria for sub-network selection,
leading to suboptimal solutions. To address these limitations, we propose a
regularization method based on attention-guided weight mixup for finetuning
PLMs. Our approach represents each network weight as a mixup of task-specific
weight and pretrained weight, controlled by a learnable attention parameter,
providing finer control over sub-network selection. Furthermore, we employ a
bi-level optimization (BLO) based framework on two separate splits of the
training dataset, improving generalization and combating overfitting. We
validate the efficacy of our proposed method through extensive experiments,
demonstrating its superiority over previous methods, particularly in the
context of finetuning PLMs on low-resource datasets.
\\ ( https://arxiv.org/abs/2403.12918 ,  8029kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12924
Date: Tue, 19 Mar 2024 17:28:51 GMT   (326kb)

Title: Supporting Energy Policy Research with Large Language Models
Authors: Grant Buster, Pavlo Pinchuk, Jacob Barrons, Ryan McKeever, Aaron
  Levine, Anthony Lopez
Categories: cs.CL
\\
  The recent growth in renewable energy development in the United States has
been accompanied by a simultaneous surge in renewable energy siting ordinances.
These zoning laws play a critical role in dictating the placement of wind and
solar resources that are critical for achieving low-carbon energy futures. In
this context, efficient access to and management of siting ordinance data
becomes imperative. The National Renewable Energy Laboratory (NREL) recently
introduced a public wind and solar siting database to fill this need. This
paper presents a method for harnessing Large Language Models (LLMs) to automate
the extraction of these siting ordinances from legal documents, enabling this
database to maintain accurate up-to-date information in the rapidly changing
energy policy landscape. A novel contribution of this research is the
integration of a decision tree framework with LLMs. Our results show that this
approach is 85 to 90% accurate with outputs that can be used directly in
downstream quantitative modeling. We discuss opportunities to use this work to
support similar large-scale policy research in the energy sector. By unlocking
new efficiencies in the extraction and analysis of legal documents using LLMs,
this study enables a path forward for automated large-scale energy policy
research.
\\ ( https://arxiv.org/abs/2403.12924 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12936
Date: Tue, 19 Mar 2024 17:43:08 GMT   (323kb)

Title: Automatic Information Extraction From Employment Tribunal Judgements
  Using Large Language Models
Authors: Joana Ribeiro de Faria, Huiyuan Xie, Felix Steffek
Categories: cs.CL cs.AI
\\
  Court transcripts and judgments are rich repositories of legal knowledge,
detailing the intricacies of cases and the rationale behind judicial decisions.
The extraction of key information from these documents provides a concise
overview of a case, crucial for both legal experts and the public. With the
advent of large language models (LLMs), automatic information extraction has
become increasingly feasible and efficient. This paper presents a comprehensive
study on the application of GPT-4, a large language model, for automatic
information extraction from UK Employment Tribunal (UKET) cases. We
meticulously evaluated GPT-4's performance in extracting critical information
with a manual verification process to ensure the accuracy and relevance of the
extracted data. Our research is structured around two primary extraction tasks:
the first involves a general extraction of eight key aspects that hold
significance for both legal specialists and the general public, including the
facts of the case, the claims made, references to legal statutes, references to
precedents, general case outcomes and corresponding labels, detailed order and
remedies and reasons for the decision. The second task is more focused, aimed
at analysing three of those extracted features, namely facts, claims and
outcomes, in order to facilitate the development of a tool capable of
predicting the outcome of employment law disputes. Through our analysis, we
demonstrate that LLMs like GPT-4 can obtain high accuracy in legal information
extraction, highlighting the potential of LLMs in revolutionising the way legal
information is processed and utilised, offering significant implications for
legal research and practice.
\\ ( https://arxiv.org/abs/2403.12936 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12958
Date: Tue, 19 Mar 2024 17:57:58 GMT   (2386kb,D)

Title: Dated Data: Tracing Knowledge Cutoffs in Large Language Models
Authors: Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel
  Khashabi, Benjamin Van Durme
Categories: cs.CL
\\
  Released Large Language Models (LLMs) are often paired with a claimed
knowledge cutoff date, or the dates at which training data was gathered. Such
information is crucial for applications where the LLM must provide up to date
information. However, this statement only scratches the surface: do all
resources in the training data share the same knowledge cutoff date? Does the
model's demonstrated knowledge for these subsets closely align to their cutoff
dates? In this work, we define the notion of an effective cutoff. This is
distinct from the LLM designer reported cutoff and applies separately to
sub-resources and topics. We propose a simple approach to estimate effective
cutoffs on the resource-level temporal alignment of an LLM by probing across
versions of the data. Using this analysis, we find that effective cutoffs often
differ from reported cutoffs. To understand the root cause of this observation,
we conduct a direct large-scale analysis on open pre-training datasets. Our
analysis reveals two reasons for these inconsistencies: (1) temporal biases of
CommonCrawl data due to non-trivial amounts of old data in new dumps and (2)
complications in LLM deduplication schemes involving semantic duplicates and
lexical near-duplicates. Overall, our results show that knowledge cutoffs are
not as simple as they have seemed and that care must be taken both by LLM
dataset curators as well as practitioners who seek to use information from
these models.
\\ ( https://arxiv.org/abs/2403.12958 ,  2386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12968
Date: Tue, 19 Mar 2024 17:59:56 GMT   (3604kb,D)

Title: LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic
  Prompt Compression
Authors: Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue
  Zhang, Qingwei Lin, Victor R\"uhle, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao,
  Lili Qiu, Dongmei Zhang
Categories: cs.CL cs.LG
\\
  This paper focuses on task-agnostic prompt compression for better
generalizability and efficiency. Considering the redundancy in natural
language, existing approaches compress prompts by removing tokens or lexical
units according to their information entropy obtained from a causal language
model such as LLaMa-7B. The challenge is that information entropy may be a
suboptimal compression metric: (i) it only leverages unidirectional context and
may fail to capture all essential information needed for prompt compression;
(ii) it is not aligned with the prompt compression objective.
  To address these issues, we propose a data distillation procedure to derive
knowledge from an LLM to compress prompts without losing crucial information,
and meantime, introduce an extractive text compression dataset. We formulate
prompt compression as a token classification problem to guarantee the
faithfulness of the compressed prompt to the original one, and use a
Transformer encoder as the base architecture to capture all essential
information for prompt compression from the full bidirectional context. Our
approach leads to lower latency by explicitly learning the compression
objective with smaller models such as XLM-RoBERTa-large and mBERT.
  We evaluate our method on both in-domain and out-of-domain datasets,
including MeetingBank, LongBench, ZeroScrolls, GSM8K, and BBH. Despite its
small size, our model shows significant performance gains over strong baselines
and demonstrates robust generalization ability across different LLMs.
Additionally, our model is 3x-6x faster than existing prompt compression
methods, while accelerating the end-to-end latency by 1.6x-2.9x with
compression ratios of 2x-5x.
\\ ( https://arxiv.org/abs/2403.12968 ,  3604kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12109
Date: Mon, 18 Mar 2024 03:39:54 GMT   (7437kb,D)

Title: GCAM: Gaussian and causal-attention model of food fine-grained
  recognition
Authors: Guohang Zhuang, Yue Hu, Tianxing Yan, JiaZhan Gao
Categories: cs.LG cs.AI cs.CV
Comments: 23 pages, 11 figures
\\
  Currently, most food recognition relies on deep learning for category
classification. However, these approaches struggle to effectively distinguish
between visually similar food samples, highlighting the pressing need to
address fine-grained issues in food recognition. To mitigate these challenges,
we propose the adoption of a Gaussian and causal-attention model for
fine-grained object recognition.In particular, we train to obtain Gaussian
features over target regions, followed by the extraction of fine-grained
features from the objects, thereby enhancing the feature mapping capabilities
of the target regions. To counteract data drift resulting from uneven data
distributions, we employ a counterfactual reasoning approach. By using
counterfactual interventions, we analyze the impact of the learned image
attention mechanism on network predictions, enabling the network to acquire
more useful attention weights for fine-grained image recognition. Finally, we
design a learnable loss strategy to balance training stability across various
modules, ultimately improving the accuracy of the final target recognition. We
validate our approach on four relevant datasets, demonstrating its excellent
performance across these four datasets.We experimentally show that GCAM
surpasses state-of-the-art methods on the ETH-FOOD101, UECFOOD256, and
Vireo-FOOD172 datasets. Furthermore, our approach also achieves
state-of-the-art performance on the CUB-200 dataset.
\\ ( https://arxiv.org/abs/2403.12109 ,  7437kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12143
Date: Mon, 18 Mar 2024 18:01:01 GMT   (1072kb,D)

Title: Graph Neural Networks for Learning Equivariant Representations of Neural
  Networks
Authors: Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J.
  Burghouts, Efstratios Gavves, Cees G. M. Snoek, David W. Zhang
Categories: cs.LG cs.AI stat.ML
Comments: In ICLR 2024
\\
  Neural networks that process the parameters of other neural networks find
applications in domains as diverse as classifying implicit neural
representations, generating neural network weights, and predicting
generalization errors. However, existing approaches either overlook the
inherent permutation symmetry in the neural network or rely on intricate
weight-sharing patterns to achieve equivariance, while ignoring the impact of
the network architecture itself. In this work, we propose to represent neural
networks as computational graphs of parameters, which allows us to harness
powerful graph neural networks and transformers that preserve permutation
symmetry. Consequently, our approach enables a single model to encode neural
computational graphs with diverse architectures. We showcase the effectiveness
of our method on a wide range of tasks, including classification and editing of
implicit neural representations, predicting generalization performance, and
learning to optimize, while consistently outperforming state-of-the-art
methods. The source code is open-sourced at
https://github.com/mkofinas/neural-graphs.
\\ ( https://arxiv.org/abs/2403.12143 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12166
Date: Mon, 18 Mar 2024 18:30:22 GMT   (3787kb,D)

Title: The Power of Few: Accelerating and Enhancing Data Reweighting with
  Coreset Selection
Authors: Mohammad Jafari, Yimeng Zhang, Yihua Zhang, Sijia Liu
Categories: cs.LG stat.ML
Comments: Accepted to ICASSP 2024
\\
  As machine learning tasks continue to evolve, the trend has been to gather
larger datasets and train increasingly larger models. While this has led to
advancements in accuracy, it has also escalated computational costs to
unsustainable levels. Addressing this, our work aims to strike a delicate
balance between computational efficiency and model accuracy, a persisting
challenge in the field. We introduce a novel method that employs core subset
selection for reweighting, effectively optimizing both computational time and
model performance. By focusing on a strategically selected coreset, our
approach offers a robust representation, as it efficiently minimizes the
influence of outliers. The re-calibrated weights are then mapped back to and
propagated across the entire dataset. Our experimental results substantiate the
effectiveness of this approach, underscoring its potential as a scalable and
precise solution for model training.
\\ ( https://arxiv.org/abs/2403.12166 ,  3787kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12188
Date: Mon, 18 Mar 2024 18:59:42 GMT   (835kb,D)

Title: PETScML: Second-order solvers for training regression problems in
  Scientific Machine Learning
Authors: Stefano Zampini, Umberto Zerbinati, George Turkiyyah, David Keyes
Categories: cs.LG cs.MS math.OC
MSC-class: 65K10, 68T07, 65M70, 65Y05
ACM-class: I.2.5; D.2.m; G.4; G.1.6; J.2
\\
  In recent years, we have witnessed the emergence of scientific machine
learning as a data-driven tool for the analysis, by means of deep-learning
techniques, of data produced by computational science and engineering
applications. At the core of these methods is the supervised training algorithm
to learn the neural network realization, a highly non-convex optimization
problem that is usually solved using stochastic gradient methods. However,
distinct from deep-learning practice, scientific machine-learning training
problems feature a much larger volume of smooth data and better
characterizations of the empirical risk functions, which make them suited for
conventional solvers for unconstrained optimization. We introduce a lightweight
software framework built on top of the Portable and Extensible Toolkit for
Scientific computation to bridge the gap between deep-learning software and
conventional solvers for unconstrained minimization. We empirically demonstrate
the superior efficacy of a trust region method based on the Gauss-Newton
approximation of the Hessian in improving the generalization errors arising
from regression tasks when learning surrogate models for a wide range of
scientific machine-learning techniques and test cases. All the conventional
second-order solvers tested, including L-BFGS and inexact Newton with
line-search, compare favorably, either in terms of cost or accuracy, with the
adaptive first-order methods used to validate the surrogate models.
\\ ( https://arxiv.org/abs/2403.12188 ,  835kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12226
Date: Mon, 18 Mar 2024 20:18:32 GMT   (40169kb,D)

Title: Large-scale flood modeling and forecasting with FloodCast
Authors: Qingsong Xu, Yilei Shi, Jonathan Bamber, Chaojun Ouyang, Xiao Xiang
  Zhu
Categories: cs.LG cs.CV physics.flu-dyn
Comments: 40 pages, 16 figures, under review
\\
  Large-scale hydrodynamic models generally rely on fixed-resolution spatial
grids and model parameters as well as incurring a high computational cost. This
limits their ability to accurately forecast flood crests and issue
time-critical hazard warnings. In this work, we build a fast, stable, accurate,
resolution-invariant, and geometry-adaptative flood modeling and forecasting
framework that can perform at large scales, namely FloodCast. The framework
comprises two main modules: multi-satellite observation and hydrodynamic
modeling. In the multi-satellite observation module, a real-time unsupervised
change detection method and a rainfall processing and analysis tool are
proposed to harness the full potential of multi-satellite observations in
large-scale flood prediction. In the hydrodynamic modeling module, a
geometry-adaptive physics-informed neural solver (GeoPINS) is introduced,
benefiting from the absence of a requirement for training data in
physics-informed neural networks and featuring a fast, accurate, and
resolution-invariant architecture with Fourier neural operators. GeoPINS
demonstrates impressive performance on popular PDEs across regular and
irregular domains. Building upon GeoPINS, we propose a sequence-to-sequence
GeoPINS model to handle long-term temporal series and extensive spatial domains
in large-scale flood modeling. Next, we establish a benchmark dataset in the
2022 Pakistan flood to assess various flood prediction methods. Finally, we
validate the model in three dimensions - flood inundation range, depth, and
transferability of spatiotemporal downscaling. Traditional hydrodynamics and
sequence-to-sequence GeoPINS exhibit exceptional agreement during high water
levels, while comparative assessments with SAR-based flood depth data show that
sequence-to-sequence GeoPINS outperforms traditional hydrodynamics, with
smaller prediction errors.
\\ ( https://arxiv.org/abs/2403.12226 ,  40169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12236
Date: Mon, 18 Mar 2024 20:33:44 GMT   (11237kb,D)

Title: Improving Generalization via Meta-Learning on Hard Samples
Authors: Nishant Jain, Arun S. Suggala and Pradeep Shenoy
Categories: cs.LG cs.CV
Comments: Accepted at CVPR 2024
\\
  Learned reweighting (LRW) approaches to supervised learning use an
optimization criterion to assign weights for training instances, in order to
maximize performance on a representative validation dataset. We pose and
formalize the problem of optimized selection of the validation set used in LRW
training, to improve classifier generalization. In particular, we show that
using hard-to-classify instances in the validation set has both a theoretical
connection to, and strong empirical evidence of generalization. We provide an
efficient algorithm for training this meta-optimized model, as well as a simple
train-twice heuristic for careful comparative study. We demonstrate that LRW
with easy validation data performs consistently worse than LRW with hard
validation data, establishing the validity of our meta-optimization problem.
Our proposed algorithm outperforms a wide range of baselines on a range of
datasets and domain shift challenges (Imagenet-1K, CIFAR-100, Clothing-1M,
CAMELYON, WILDS, etc.), with ~1% gains using VIT-B on Imagenet. We also show
that using naturally hard examples for validation (Imagenet-R / Imagenet-A) in
LRW training for Imagenet improves performance on both clean and naturally hard
test instances by 1-2%. Secondary analyses show that using hard validation data
in an LRW framework improves margins on test data, hinting at the mechanism
underlying our empirical gains. We believe this work opens up new research
directions for the meta-optimization of meta-learning in a supervised learning
context.
\\ ( https://arxiv.org/abs/2403.12236 ,  11237kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12237
Date: Mon, 18 Mar 2024 20:35:35 GMT   (468kb,D)

Title: Efficient Transformer-based Hyper-parameter Optimization for
  Resource-constrained IoT Environments
Authors: Ibrahim Shaer, Soodeh Nikan, Abdallah Shami
Categories: cs.LG cs.AI
Comments: 7 pages, Submitted to IEEE Internet of Things Magazine
\\
  The hyper-parameter optimization (HPO) process is imperative for finding the
best-performing Convolutional Neural Networks (CNNs). The automation process of
HPO is characterized by its sizable computational footprint and its lack of
transparency; both important factors in a resource-constrained Internet of
Things (IoT) environment. In this paper, we address these problems by proposing
a novel approach that combines transformer architecture and actor-critic
Reinforcement Learning (RL) model, TRL-HPO, equipped with multi-headed
attention that enables parallelization and progressive generation of layers.
These assumptions are founded empirically by evaluating TRL-HPO on the MNIST
dataset and comparing it with state-of-the-art approaches that build CNN models
from scratch. The results show that TRL-HPO outperforms the classification
results of these approaches by 6.8% within the same time frame, demonstrating
the efficiency of TRL-HPO for the HPO process. The analysis of the results
identifies the main culprit for performance degradation attributed to stacking
fully connected layers. This paper identifies new avenues for improving
RL-based HPO processes in resource-constrained environments.
\\ ( https://arxiv.org/abs/2403.12237 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12278
Date: Mon, 18 Mar 2024 21:53:56 GMT   (158kb,D)

Title: Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices
Authors: Gregory Dexter, Christos Boutsikas, Linkai Ma, Ilse C.F. Ipsen, and
  Petros Drineas
Categories: cs.LG cs.NA math.NA
MSC-class: 68W20, 65F15, 65F22, 65G50, 15A18, 15A42
\\
  Motivated by the popularity of stochastic rounding in the context of machine
learning and the training of large-scale deep neural network models, we
consider stochastic nearness rounding of real matrices $\mathbf{A}$ with many
more rows than columns. We provide novel theoretical evidence, supported by
extensive experimental evaluation that, with high probability, the smallest
singular value of a stochastically rounded matrix is well bounded away from
zero -- regardless of how close $\mathbf{A}$ is to being rank deficient and
even if $\mathbf{A}$ is rank-deficient. In other words, stochastic rounding
\textit{implicitly regularizes} tall and skinny matrices $\mathbf{A}$ so that
the rounded version has full column rank. Our proofs leverage powerful results
in random matrix theory, and the idea that stochastic rounding errors do not
concentrate in low-dimensional column spaces.
\\ ( https://arxiv.org/abs/2403.12278 ,  158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12307
Date: Mon, 18 Mar 2024 23:16:17 GMT   (1753kb,D)

Title: Molecular Classification Using Hyperdimensional Graph Classification
Authors: Pere Verges, Igor Nunes, Mike Heddes, Tony Givargis and Alexandru
  Nicolau
Categories: cs.LG cs.AI cs.NE q-bio.QM
\\
  Our work introduces an innovative approach to graph learning by leveraging
Hyperdimensional Computing. Graphs serve as a widely embraced method for
conveying information, and their utilization in learning has gained significant
attention. This is notable in the field of chemoinformatics, where learning
from graph representations plays a pivotal role. An important application
within this domain involves the identification of cancerous cells across
diverse molecular structures.
  We propose an HDC-based model that demonstrates comparable Area Under the
Curve results when compared to state-of-the-art models like Graph Neural
Networks (GNNs) or the Weisfieler-Lehman graph kernel (WL). Moreover, it
outperforms previously proposed hyperdimensional computing graph learning
methods. Furthermore, it achieves noteworthy speed enhancements, boasting a 40x
acceleration in the training phase and a 15x improvement in inference time
compared to GNN and WL models. This not only underscores the efficacy of the
HDC-based method, but also highlights its potential for expedited and
resource-efficient graph learning.
\\ ( https://arxiv.org/abs/2403.12307 ,  1753kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12309
Date: Mon, 18 Mar 2024 23:18:27 GMT   (1701kb,D)

Title: Reinforcement Learning from Delayed Observations via World Models
Authors: Armin Karamzade, Kyungmin Kim, Montek Kalsi, Roy Fox
Categories: cs.LG cs.AI
\\
  In standard Reinforcement Learning settings, agents typically assume
immediate feedback about the effects of their actions after taking them.
However, in practice, this assumption may not hold true due to physical
constraints and can significantly impact the performance of RL algorithms. In
this paper, we focus on addressing observation delays in partially observable
environments. We propose leveraging world models, which have shown success in
integrating past observations and learning dynamics, to handle observation
delays. By reducing delayed POMDPs to delayed MDPs with world models, our
methods can effectively handle partial observability, where existing approaches
achieve sub-optimal performance or even degrade quickly as observability
decreases. Experiments suggest that one of our methods can outperform a naive
model-based approach by up to %30. Moreover, we evaluate our methods on visual
input based delayed environment, for the first time showcasing delay-aware
reinforcement learning on visual observations.
\\ ( https://arxiv.org/abs/2403.12309 ,  1701kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12313
Date: Mon, 18 Mar 2024 23:20:08 GMT   (80kb,D)

Title: Improving LoRA in Privacy-preserving Federated Learning
Authors: Youbang Sun, Zitao Li, Yaliang Li, Bolin Ding
Categories: cs.LG cs.CR cs.DC
Comments: published at ICLR 2024, full paper 17 pages
\\
  Low-rank adaptation (LoRA) is one of the most popular task-specific
parameter-efficient fine-tuning (PEFT) methods on pre-trained language models
for its good performance and computational efficiency. LoRA injects a product
of two trainable rank decomposition matrices over the top of each frozen
pre-trained model module. However, when applied in the setting of
privacy-preserving federated learning (FL), LoRA may become unstable due to the
following facts: 1) the effects of data heterogeneity and multi-step local
updates are non-negligible, 2) additive noise enforced on updating gradients to
guarantee differential privacy (DP) can be amplified and 3) the final
performance is susceptible to hyper-parameters. A key factor leading to these
phenomena is the discordance between jointly optimizing the two low-rank
matrices by local clients and separately aggregating them by the central
server. Thus, this paper proposes an efficient and effective version of LoRA,
Federated Freeze A LoRA (FFA-LoRA), to alleviate these challenges and further
halve the communication cost of federated fine-tuning LLMs. The core idea of
FFA-LoRA is to fix the randomly initialized non-zero matrices and only
fine-tune the zero-initialized matrices. Compared to LoRA, FFA-LoRA is
motivated by practical and theoretical benefits in privacy-preserved FL. Our
experiments demonstrate that FFA-LoRA provides more consistent performance with
better computational efficiency over vanilla LoRA in various FL tasks.
\\ ( https://arxiv.org/abs/2403.12313 ,  80kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12320
Date: Mon, 18 Mar 2024 23:23:50 GMT   (3869kb,D)

Title: Approximated Likelihood Ratio: A Forward-Only and Parallel Framework for
  Boosting Neural Network Training
Authors: Zeliang Zhang, Jinyang Jiang, Zhuo Liu, Susan Liang, Yijie Peng,
  Chenliang Xu
Categories: cs.LG cs.AI
\\
  Efficient and biologically plausible alternatives to backpropagation in
neural network training remain a challenge due to issues such as high
computational complexity and additional assumptions about neural networks,
which limit scalability to deeper networks. The likelihood ratio method offers
a promising gradient estimation strategy but is constrained by significant
memory consumption, especially when deploying multiple copies of data to reduce
estimation variance. In this paper, we introduce an approximation technique for
the likelihood ratio (LR) method to alleviate computational and memory demands
in gradient estimation. By exploiting the natural parallelism during the
backward pass using LR, we further provide a high-performance training
strategy, which pipelines both the forward and backward pass, to make it more
suitable for the computation on specialized hardware. Extensive experiments
demonstrate the effectiveness of the approximation technique in neural network
training. This work underscores the potential of the likelihood ratio method in
achieving high-performance neural network training, suggesting avenues for
further exploration.
\\ ( https://arxiv.org/abs/2403.12320 ,  3869kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12323
Date: Mon, 18 Mar 2024 23:32:08 GMT   (383kb,D)

Title: Enhanced Detection of Transdermal Alcohol Levels Using Hyperdimensional
  Computing on Embedded Devices
Authors: Manuel E. Segura, Pere Verges, Justin Tian Jin Chen, Ramesh Arangott,
  Angela Kristine Garcia, Laura Garcia Reynoso, Alexandru Nicolau, Tony
  Givargis and Sergio Gago-Masague
Categories: cs.LG
\\
  Alcohol consumption has a significant impact on individuals' health, with
even more pronounced consequences when consumption becomes excessive. One
approach to promoting healthier drinking habits is implementing just-in-time
interventions, where timely notifications indicating intoxication are sent
during heavy drinking episodes. However, the complexity or invasiveness of an
intervention mechanism may deter an individual from using them in practice.
Previous research tackled this challenge using collected motion data and
conventional Machine Learning (ML) algorithms to classify heavy drinking
episodes, but with impractical accuracy and computational efficiency for mobile
devices. Consequently, we have elected to use Hyperdimensional Computing (HDC)
to design a just-in-time intervention approach that is practical for
smartphones, smart wearables, and IoT deployment. HDC is a framework that has
proven results in processing real-time sensor data efficiently. This approach
offers several advantages, including low latency, minimal power consumption,
and high parallelism. We explore various HDC encoding designs and combine them
with various HDC learning models to create an optimal and feasible approach for
mobile devices. Our findings indicate an accuracy rate of 89\%, which
represents a substantial 12\% improvement over the current state-of-the-art.
\\ ( https://arxiv.org/abs/2403.12323 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12326
Date: Mon, 18 Mar 2024 23:42:04 GMT   (31710kb,D)

Title: Removing Undesirable Concepts in Text-to-Image Generative Models with
  Learnable Prompts
Authors: Anh Bui, Khanh Doan, Trung Le, Paul Montague, Tamas Abraham, Dinh
  Phung
Categories: cs.LG cs.CV
\\
  Generative models have demonstrated remarkable potential in generating
visually impressive content from textual descriptions. However, training these
models on unfiltered internet data poses the risk of learning and subsequently
propagating undesirable concepts, such as copyrighted or unethical content. In
this paper, we propose a novel method to remove undesirable concepts from
text-to-image generative models by incorporating a learnable prompt into the
cross-attention module. This learnable prompt acts as additional memory to
transfer the knowledge of undesirable concepts into it and reduce the
dependency of these concepts on the model parameters and corresponding textual
inputs. Because of this knowledge transfer into the prompt, erasing these
undesirable concepts is more stable and has minimal negative impact on other
concepts. We demonstrate the effectiveness of our method on the Stable
Diffusion model, showcasing its superiority over state-of-the-art erasure
methods in terms of removing undesirable content while preserving other
unrelated elements.
\\ ( https://arxiv.org/abs/2403.12326 ,  31710kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12328
Date: Mon, 18 Mar 2024 23:48:33 GMT   (14139kb,D)

Title: Methods for Generating Drift in Text Streams
Authors: Cristiano Mesquita Garcia, Alessandro Lameiras Koerich, Alceu de Souza
  Britto Jr and Jean Paul Barddal
Categories: cs.LG cs.CL cs.IR
\\
  Systems and individuals produce data continuously. On the Internet, people
share their knowledge, sentiments, and opinions, provide reviews about services
and products, and so on. Automatically learning from these textual data can
provide insights to organizations and institutions, thus preventing financial
impacts, for example. To learn from textual data over time, the machine
learning system must account for concept drift. Concept drift is a frequent
phenomenon in real-world datasets and corresponds to changes in data
distribution over time. For instance, a concept drift occurs when sentiments
change or a word's meaning is adjusted over time. Although concept drift is
frequent in real-world applications, benchmark datasets with labeled drifts are
rare in the literature. To bridge this gap, this paper provides four textual
drift generation methods to ease the production of datasets with labeled
drifts. These methods were applied to Yelp and Airbnb datasets and tested using
incremental classifiers respecting the stream mining paradigm to evaluate their
ability to recover from the drifts. Results show that all methods have their
performance degraded right after the drifts, and the incremental SVM is the
fastest to run and recover the previous performance levels regarding accuracy
and Macro F1-Score.
\\ ( https://arxiv.org/abs/2403.12328 ,  14139kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12329
Date: Tue, 19 Mar 2024 00:03:40 GMT   (741kb,D)

Title: FedFisher: Leveraging Fisher Information for One-Shot Federated Learning
Authors: Divyansh Jhunjhunwala, Shiqiang Wang, Gauri Joshi
Categories: cs.LG cs.DC stat.ML
Comments: Accepted to AISTATS 2024
\\
  Standard federated learning (FL) algorithms typically require multiple rounds
of communication between the server and the clients, which has several
drawbacks, including requiring constant network connectivity, repeated
investment of computational resources, and susceptibility to privacy attacks.
One-Shot FL is a new paradigm that aims to address this challenge by enabling
the server to train a global model in a single round of communication. In this
work, we present FedFisher, a novel algorithm for one-shot FL that makes use of
Fisher information matrices computed on local client models, motivated by a
Bayesian perspective of FL. First, we theoretically analyze FedFisher for
two-layer over-parameterized ReLU neural networks and show that the error of
our one-shot FedFisher global model becomes vanishingly small as the width of
the neural networks and amount of local training at clients increases. Next, we
propose practical variants of FedFisher using the diagonal Fisher and K-FAC
approximation for the full Fisher and highlight their communication and compute
efficiency for FL. Finally, we conduct extensive experiments on various
datasets, which show that these variants of FedFisher consistently improve over
competing baselines.
\\ ( https://arxiv.org/abs/2403.12329 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12335
Date: Tue, 19 Mar 2024 00:48:25 GMT   (2440kb,D)

Title: Temporally-Consistent Koopman Autoencoders for Forecasting Dynamical
  Systems
Authors: Indranil Nayak, Debdipta Goswami, Mrinal Kumar, Fernando Teixeira
Categories: cs.LG
\\
  Absence of sufficiently high-quality data often poses a key challenge in
data-driven modeling of high-dimensional spatio-temporal dynamical systems.
Koopman Autoencoders (KAEs) harness the expressivity of deep neural networks
(DNNs), the dimension reduction capabilities of autoencoders, and the spectral
properties of the Koopman operator to learn a reduced-order feature space with
simpler, linear dynamics. However, the effectiveness of KAEs is hindered by
limited and noisy training datasets, leading to poor generalizability. To
address this, we introduce the Temporally-Consistent Koopman Autoencoder
(tcKAE), designed to generate accurate long-term predictions even with
constrained and noisy training data. This is achieved through a consistency
regularization term that enforces prediction coherence across different time
steps, thus enhancing the robustness and generalizability of tcKAE over
existing models. We provide analytical justification for this approach based on
Koopman spectral theory and empirically demonstrate tcKAE's superior
performance over state-of-the-art KAE models across a variety of test cases,
including simple pendulum oscillations, kinetic plasmas, fluid flows, and sea
surface temperature data.
\\ ( https://arxiv.org/abs/2403.12335 ,  2440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12350
Date: Tue, 19 Mar 2024 01:39:33 GMT   (319kb,D)

Title: Friendly Sharpness-Aware Minimization
Authors: Tao Li, Pan Zhou, Zhengbao He, Xinwen Cheng, Xiaolin Huang
Categories: cs.LG
Comments: CVPR 2024
\\
  Sharpness-Aware Minimization (SAM) has been instrumental in improving deep
neural network training by minimizing both training loss and loss sharpness.
Despite the practical success, the mechanisms behind SAM's generalization
enhancements remain elusive, limiting its progress in deep learning
optimization. In this work, we investigate SAM's core components for
generalization improvement and introduce "Friendly-SAM" (F-SAM) to further
enhance SAM's generalization. Our investigation reveals the key role of
batch-specific stochastic gradient noise within the adversarial perturbation,
i.e., the current minibatch gradient, which significantly influences SAM's
generalization performance. By decomposing the adversarial perturbation in SAM
into full gradient and stochastic gradient noise components, we discover that
relying solely on the full gradient component degrades generalization while
excluding it leads to improved performance. The possible reason lies in the
full gradient component's increase in sharpness loss for the entire dataset,
creating inconsistencies with the subsequent sharpness minimization step solely
on the current minibatch data. Inspired by these insights, F-SAM aims to
mitigate the negative effects of the full gradient component. It removes the
full gradient estimated by an exponentially moving average (EMA) of historical
stochastic gradients, and then leverages stochastic gradient noise for improved
generalization. Moreover, we provide theoretical validation for the EMA
approximation and prove the convergence of F-SAM on non-convex problems.
Extensive experiments demonstrate the superior generalization performance and
robustness of F-SAM over vanilla SAM. Code is available at
https://github.com/nblt/F-SAM.
\\ ( https://arxiv.org/abs/2403.12350 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12354
Date: Tue, 19 Mar 2024 01:58:14 GMT   (5304kb,D)

Title: Sim2Real in Reconstructive Spectroscopy: Deep Learning with Augmented
  Device-Informed Data Simulation
Authors: Jiyi Chen, Pengyu Li, Yutong Wang, Pei-Cheng Ku, Qing Qu
Categories: cs.LG eess.SP
\\
  This work proposes a deep learning (DL)-based framework, namely Sim2Real, for
spectral signal reconstruction in reconstructive spectroscopy, focusing on
efficient data sampling and fast inference time. The work focuses on the
challenge of reconstructing real-world spectral signals under the extreme
setting where only device-informed simulated data are available for training.
Such device-informed simulated data are much easier to collect than real-world
data but exhibit large distribution shifts from their real-world counterparts.
To leverage such simulated data effectively, a hierarchical data augmentation
strategy is introduced to mitigate the adverse effects of this domain shift,
and a corresponding neural network for the spectral signal reconstruction with
our augmented data is designed. Experiments using a real dataset measured from
our spectrometer device demonstrate that Sim2Real achieves significant speed-up
during the inference while attaining on-par performance with the
state-of-the-art optimization-based methods.
\\ ( https://arxiv.org/abs/2403.12354 ,  5304kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12366
Date: Tue, 19 Mar 2024 02:23:12 GMT   (1271kb)

Title: U-Net Kalman Filter (UNetKF): An Example of Machine Learning-assisted
  Ensemble Data Assimilation
Authors: Feiyu Lu
Categories: cs.LG physics.ao-ph
\\
  Machine learning techniques have seen a tremendous rise in popularity in
weather and climate sciences. Data assimilation (DA), which combines
observations and numerical models, has great potential to incorporate machine
learning and artificial intelligence (ML/AI) techniques. In this paper, we use
U-Net, a type of convolutional neutral network (CNN), to predict the localized
ensemble covariances for the Ensemble Kalman Filter (EnKF) algorithm. Using a
2-layer quasi-geostrophic model, U-Nets are trained using data from EnKF DA
experiments. The trained U-Nets are then used to predict the flow-dependent
localized error covariance matrices in U-Net Kalman Filter (UNetKF)
experiments, which are compared to traditional 3-dimensional variational
(3DVar), ensemble 3DVar (En3DVar) and EnKF methods. The performance of UNetKF
can match or exceed that of 3DVar, En3DVar or EnKF. We also demonstrate that
trained U-Nets can be transferred to a higher-resolution model for UNetKF
implementation, which again performs competitively to 3DVar and EnKF,
particularly for small ensemble sizes.
\\ ( https://arxiv.org/abs/2403.12366 ,  1271kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12371
Date: Tue, 19 Mar 2024 02:32:24 GMT   (1724kb,D)

Title: Advancing Time Series Classification with Multimodal Language Modeling
Authors: Mingyue Cheng, Yiheng Chen, Qi Liu, Zhiding Liu, Yucong Luo
Categories: cs.LG
\\
  For the advancements of time series classification, scrutinizing previous
studies, most existing methods adopt a common learning-to-classify paradigm - a
time series classifier model tries to learn the relation between sequence
inputs and target label encoded by one-hot distribution. Although effective,
this paradigm conceals two inherent limitations: (1) encoding target categories
with one-hot distribution fails to reflect the comparability and similarity
between labels, and (2) it is very difficult to learn transferable model across
domains, which greatly hinder the development of universal serving paradigm. In
this work, we propose InstructTime, a novel attempt to reshape time series
classification as a learning-to-generate paradigm. Relying on the powerful
generative capacity of the pre-trained language model, the core idea is to
formulate the classification of time series as a multimodal understanding task,
in which both task-specific instructions and raw time series are treated as
multimodal inputs while the label information is represented by texts. To
accomplish this goal, three distinct designs are developed in the InstructTime.
Firstly, a time series discretization module is designed to convert continuous
time series into a sequence of hard tokens to solve the inconsistency issue
across modal inputs. To solve the modality representation gap issue, for one
thing, we introduce an alignment projected layer before feeding the transformed
token of time series into language models. For another, we highlight the
necessity of auto-regressive pre-training across domains, which can facilitate
the transferability of the language model and boost the generalization
performance. Extensive experiments are conducted over benchmark datasets, whose
results uncover the superior performance of InstructTime and the potential for
a universal foundation model in time series classification.
\\ ( https://arxiv.org/abs/2403.12371 ,  1724kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12372
Date: Tue, 19 Mar 2024 02:32:47 GMT   (1624kb,D)

Title: Learning Transferable Time Series Classifier with Cross-Domain
  Pre-training from Language Model
Authors: Mingyue Cheng, Xiaoyu Tao, Qi Liu, Hao Zhang, Yiheng Chen, Chenyi Lei
Categories: cs.LG
\\
  Advancements in self-supervised pre-training (SSL) have significantly
advanced the field of learning transferable time series representations, which
can be very useful in enhancing the downstream task. Despite being effective,
most existing works struggle to achieve cross-domain SSL pre-training, missing
valuable opportunities to integrate patterns and features from different
domains. The main challenge lies in the significant differences in the
characteristics of time-series data across different domains, such as
variations in the number of channels and temporal resolution scales. To address
this challenge, we propose CrossTimeNet, a novel cross-domain SSL learning
framework to learn transferable knowledge from various domains to largely
benefit the target downstream task. One of the key characteristics of
CrossTimeNet is the newly designed time series tokenization module, which could
effectively convert the raw time series into a sequence of discrete tokens
based on a reconstruction optimization process. Besides, we highlight that
predicting a high proportion of corrupted tokens can be very helpful for
extracting informative patterns across different domains during SSL
pre-training, which has been largely overlooked in past years. Furthermore,
unlike previous works, our work treats the pre-training language model (PLM) as
the initialization of the encoder network, investigating the feasibility of
transferring the knowledge learned by the PLM to the time series area. Through
these efforts, the path to cross-domain pre-training of a generic time series
model can be effectively paved. We conduct extensive experiments in a
real-world scenario across various time series classification domains. The
experimental results clearly confirm CrossTimeNet's superior performance.
\\ ( https://arxiv.org/abs/2403.12372 ,  1624kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12391
Date: Tue, 19 Mar 2024 02:59:50 GMT   (8166kb,D)

Title: FairSTG: Countering performance heterogeneity via collaborative
  sample-level optimization
Authors: Gengyu Lin, Zhengyang Zhou, Qihe Huang, Kuo Yang, Shifen Cheng, Yang
  Wang
Categories: cs.LG cs.AI
Comments: Under review by IEEE Transactions on Mobile Computing
\\
  Spatiotemporal learning plays a crucial role in mobile computing techniques
to empower smart cites. While existing research has made great efforts to
achieve accurate predictions on the overall dataset, they still neglect the
significant performance heterogeneity across samples. In this work, we
designate the performance heterogeneity as the reason for unfair spatiotemporal
learning, which not only degrades the practical functions of models, but also
brings serious potential risks to real-world urban applications. To fix this
gap, we propose a model-independent Fairness-aware framework for SpatioTemporal
Graph learning (FairSTG), which inherits the idea of exploiting advantages of
well-learned samples to challenging ones with collaborative mix-up.
Specifically, FairSTG consists of a spatiotemporal feature extractor for model
initialization, a collaborative representation enhancement for knowledge
transfer between well-learned samples and challenging ones, and fairness
objectives for immediately suppressing sample-level performance heterogeneity.
Experiments on four spatiotemporal datasets demonstrate that our FairSTG
significantly improves the fairness quality while maintaining comparable
forecasting accuracy. Case studies show FairSTG can counter both spatial and
temporal performance heterogeneity by our sample-level retrieval and
compensation, and our work can potentially alleviate the risks on
spatiotemporal resource allocation for underrepresented urban regions.
\\ ( https://arxiv.org/abs/2403.12391 ,  8166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12399
Date: Tue, 19 Mar 2024 03:14:24 GMT   (2367kb,D)

Title: Electioneering the Network: Dynamic Multi-Step Adversarial Attacks for
  Community Canvassing
Authors: Saurabh Sharma, Ambuj SIngh
Categories: cs.LG cs.CR cs.SI
\\
  The problem of online social network manipulation for community canvassing is
of real concern in today's world. Motivated by the study of voter models,
opinion and polarization dynamics on networks, we model community canvassing as
a dynamic process over a network enabled via gradient-based attacks on GNNs.
Existing attacks on GNNs are all single-step and do not account for the dynamic
cascading nature of information diffusion in networks. We consider the
realistic scenario where an adversary uses a GNN as a proxy to predict and
manipulate voter preferences, especially uncertain voters. Gradient-based
attacks on the GNN inform the adversary of strategic manipulations that can be
made to proselytize targeted voters. In particular, we explore $\textit{minimum
budget attacks for community canvassing}$ (MBACC). We show that the MBACC
problem is NP-Hard and propose Dynamic Multi-Step Adversarial Community
Canvassing (MAC) to address it. MAC makes dynamic local decisions based on the
heuristic of low budget and high second-order influence to convert and perturb
target voters. MAC is a dynamic multi-step attack that discovers low-budget and
high-influence targets from which efficient cascading attacks can happen. We
evaluate MAC against single-step baselines on the MBACC problem with multiple
underlying networks and GNN models. Our experiments show the superiority of MAC
which is able to discover efficient multi-hop attacks for adversarial community
canvassing. Our code implementation and data is available at
https://github.com/saurabhsharma1993/mac.
\\ ( https://arxiv.org/abs/2403.12399 ,  2367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12400
Date: Tue, 19 Mar 2024 03:16:52 GMT   (9069kb,D)

Title: Finding the Missing Data: A BERT-inspired Approach Against Package Loss
  in Wireless Sensing
Authors: Zijian Zhao, Tingwei Chen, Fanyi Meng, Hang Li, Xiaoyang Li, and
  Guangxu Zhu
Categories: cs.LG cs.AI eess.SP
Comments: 6 pages, accepted by IEEE INFOCOM Deepwireless Workshop 2024
\\
  Despite the development of various deep learning methods for Wi-Fi sensing,
package loss often results in noncontinuous estimation of the Channel State
Information (CSI), which negatively impacts the performance of the learning
models. To overcome this challenge, we propose a deep learning model based on
Bidirectional Encoder Representations from Transformers (BERT) for CSI
recovery, named CSI-BERT. CSI-BERT can be trained in an self-supervised manner
on the target dataset without the need for additional data. Furthermore, unlike
traditional interpolation methods that focus on one subcarrier at a time,
CSI-BERT captures the sequential relationships across different subcarriers.
Experimental results demonstrate that CSI-BERT achieves lower error rates and
faster speed compared to traditional interpolation methods, even when facing
with high loss rates. Moreover, by harnessing the recovered CSI obtained from
CSI-BERT, other deep learning models like Residual Network and Recurrent Neural
Network can achieve an average increase in accuracy of approximately 15\% in
Wi-Fi sensing tasks. The collected dataset WiGesture and code for our model are
publicly available at https://github.com/RS2002/CSI-BERT.
\\ ( https://arxiv.org/abs/2403.12400 ,  9069kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12404
Date: Tue, 19 Mar 2024 03:27:01 GMT   (7531kb,D)

Title: Understanding Training-free Diffusion Guidance: Mechanisms and
  Limitations
Authors: Yifei Shen, Xinyang Jiang, Yezhen Wang, Yifan Yang, Dongqi Han,
  Dongsheng Li
Categories: cs.LG cs.CV
\\
  Adding additional control to pretrained diffusion models has become an
increasingly popular research area, with extensive applications in computer
vision, reinforcement learning, and AI for science. Recently, several studies
have proposed training-free diffusion guidance by using off-the-shelf networks
pretrained on clean images. This approach enables zero-shot conditional
generation for universal control formats, which appears to offer a free lunch
in diffusion guidance. In this paper, we aim to develop a deeper understanding
of the operational mechanisms and fundamental limitations of training-free
guidance. We offer a theoretical analysis that supports training-free guidance
from the perspective of optimization, distinguishing it from classifier-based
(or classifier-free) guidance. To elucidate their drawbacks, we theoretically
demonstrate that training-free methods are more susceptible to adversarial
gradients and exhibit slower convergence rates compared to classifier guidance.
We then introduce a collection of techniques designed to overcome the
limitations, accompanied by theoretical rationale and empirical evidence. Our
experiments in image and motion generation confirm the efficacy of these
techniques.
\\ ( https://arxiv.org/abs/2403.12404 ,  7531kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12418
Date: Tue, 19 Mar 2024 04:02:57 GMT   (949kb,D)

Title: STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space
  Model
Authors: Lincan Li, Hanchen Wang, Wenjie Zhang, Adelle Coster
Categories: cs.LG cs.AI
\\
  Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous,
and non-stationary, leading to the continuous challenge of spatial-temporal
graph learning. In the past few years, various GNN-based methods have been
proposed to solely focus on mimicking the relationships among node individuals
of the STG network, ignoring the significance of modeling the intrinsic
features that exist in STG system over time. In contrast, modern Selective
State Space Models (SSSMs) present a new approach which treat STG Network as a
system, and meticulously explore the STG system's dynamic state evolution
across temporal dimension. In this work, we introduce Spatial-Temporal Graph
Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective
state space models for STG learning by treating STG Network as a system, and
employing the Graph Selective State Space Block (GS3B) to precisely
characterize the dynamic evolution of STG networks. STG-Mamba is formulated as
an Encoder-Decoder architecture, which takes GS3B as the basic module, for
efficient sequential data modeling. Furthermore, to strengthen GNN's ability of
modeling STG data under the setting of SSSMs, we propose Kalman Filtering Graph
Neural Networks (KFGN) for adaptive graph structure upgrading. KFGN smoothly
fits in the context of selective state space evolution, and at the same time
keeps linear complexity. Extensive empirical studies are conducted on three
benchmark STG forecasting datasets, demonstrating the performance superiority
and computational efficiency of STG-Mamba. It not only surpasses existing
state-of-the-art methods in terms of STG forecasting performance, but also
effectively alleviate the computational bottleneck of large-scale graph
networks in reducing the computational cost of FLOPs and test inference time.
\\ ( https://arxiv.org/abs/2403.12418 ,  949kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12422
Date: Tue, 19 Mar 2024 04:09:11 GMT   (776kb,D)

Title: Jetfire: Efficient and Accurate Transformer Pretraining with INT8 Data
  Flow and Per-Block Quantization
Authors: Haocheng Xi, Yuxiang Chen, Kang Zhao, Kaijun Zheng, Jianfei Chen, Jun
  Zhu
Categories: cs.LG
Comments: 14 pages, 8 figures
\\
  Pretraining transformers are generally time-consuming. Fully quantized
training (FQT) is a promising approach to speed up pretraining. However, most
FQT methods adopt a quantize-compute-dequantize procedure, which often leads to
suboptimal speedup and significant performance degradation when used in
transformers due to the high memory access overheads and low-precision
computations. In this work, we propose Jetfire, an efficient and accurate INT8
training method specific to transformers. Our method features an INT8 data flow
to optimize memory access and a per-block quantization method to maintain the
accuracy of pretrained transformers. Extensive experiments demonstrate that our
INT8 FQT method achieves comparable accuracy to the FP16 training baseline and
outperforms the existing INT8 training works for transformers. Moreover, for a
standard transformer block, our method offers an end-to-end training speedup of
1.42x and a 1.49x memory reduction compared to the FP16 baseline.
\\ ( https://arxiv.org/abs/2403.12422 ,  776kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12428
Date: Tue, 19 Mar 2024 04:35:59 GMT   (1752kb,D)

Title: Transfer in Sequential Multi-armed Bandits via Reward Samples
Authors: Rahul N R and Vaibhav Katewa
Categories: cs.LG stat.ML
Comments: Paper accepted in ECC 2024
\\
  We consider a sequential stochastic multi-armed bandit problem where the
agent interacts with bandit over multiple episodes. The reward distribution of
the arms remain constant throughout an episode but can change over different
episodes. We propose an algorithm based on UCB to transfer the reward samples
from the previous episodes and improve the cumulative regret performance over
all the episodes. We provide regret analysis and empirical results for our
algorithm, which show significant improvement over the standard UCB algorithm
without transfer.
\\ ( https://arxiv.org/abs/2403.12428 ,  1752kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12448
Date: Tue, 19 Mar 2024 05:17:47 GMT   (18598kb,D)

Title: Do Generated Data Always Help Contrastive Learning?
Authors: Yifei Wang, Jizhe Zhang, Yisen Wang
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: 19 pages. Accepted by ICLR 2024
\\
  Contrastive Learning (CL) has emerged as one of the most successful paradigms
for unsupervised visual representation learning, yet it often depends on
intensive manual data augmentations. With the rise of generative models,
especially diffusion models, the ability to generate realistic images close to
the real data distribution has been well recognized. These generated
high-equality images have been successfully applied to enhance contrastive
representation learning, a technique termed ``data inflation''. However, we
find that the generated data (even from a good diffusion model like DDPM) may
sometimes even harm contrastive learning. We investigate the causes behind this
failure from the perspective of both data inflation and data augmentation. For
the first time, we reveal the complementary roles that stronger data inflation
should be accompanied by weaker augmentations, and vice versa. We also provide
rigorous theoretical explanations for these phenomena via deriving its
generalization bounds under data inflation. Drawing from these insights, we
propose Adaptive Inflation (AdaInf), a purely data-centric strategy without
introducing any extra computation cost. On benchmark datasets, AdaInf can bring
significant improvements for various contrastive learning methods. Notably,
without using external data, AdaInf obtains 94.70% linear accuracy on CIFAR-10
with SimCLR, setting a new record that surpasses many sophisticated methods.
Code is available at https://github.com/PKU-ML/adainf.
\\ ( https://arxiv.org/abs/2403.12448 ,  18598kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12459
Date: Tue, 19 Mar 2024 05:30:50 GMT   (16222kb,D)

Title: Non-negative Contrastive Learning
Authors: Yifei Wang, Qi Zhang, Yaoyu Guo, Yisen Wang
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: 22 pages. Accepted by ICLR 2024
\\
  Deep representations have shown promising performance when transferred to
downstream tasks in a black-box manner. Yet, their inherent lack of
interpretability remains a significant challenge, as these features are often
opaque to human understanding. In this paper, we propose Non-negative
Contrastive Learning (NCL), a renaissance of Non-negative Matrix Factorization
(NMF) aimed at deriving interpretable features. The power of NCL lies in its
enforcement of non-negativity constraints on features, reminiscent of NMF's
capability to extract features that align closely with sample clusters. NCL not
only aligns mathematically well with an NMF objective but also preserves NMF's
interpretability attributes, resulting in a more sparse and disentangled
representation compared to standard contrastive learning (CL). Theoretically,
we establish guarantees on the identifiability and downstream generalization of
NCL. Empirically, we show that these advantages enable NCL to outperform CL
significantly on feature disentanglement, feature selection, as well as
downstream classification tasks. At last, we show that NCL can be easily
extended to other learning scenarios and benefit supervised learning as well.
Code is available at https://github.com/PKU-ML/non_neg.
\\ ( https://arxiv.org/abs/2403.12459 ,  16222kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12474
Date: Tue, 19 Mar 2024 06:22:58 GMT   (4169kb,D)

Title: FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive
  Information Neutralization
Authors: Cheng Yang, Jixi Liu, Yunhe Yan, Chuan Shi
Categories: cs.LG cs.CY
\\
  Despite the remarkable success of graph neural networks (GNNs) in modeling
graph-structured data, like other machine learning models, GNNs are also
susceptible to making biased predictions based on sensitive attributes, such as
race and gender. For fairness consideration, recent state-of-the-art (SOTA)
methods propose to filter out sensitive information from inputs or
representations, e.g., edge dropping or feature masking. However, we argue that
such filtering-based strategies may also filter out some non-sensitive feature
information, leading to a sub-optimal trade-off between predictive performance
and fairness. To address this issue, we unveil an innovative
neutralization-based paradigm, where additional Fairness-facilitating Features
(F3) are incorporated into node features or representations before message
passing. The F3 are expected to statistically neutralize the sensitive bias in
node representations and provide additional nonsensitive information. We also
provide theoretical explanations for our rationale, concluding that F3 can be
realized by emphasizing the features of each node's heterogeneous neighbors
(neighbors with different sensitive attributes). We name our method as FairSIN,
and present three implementation variants from both data-centric and
model-centric perspectives. Experimental results on five benchmark datasets
with three different GNN backbones show that FairSIN significantly improves
fairness metrics while maintaining high prediction accuracies.
\\ ( https://arxiv.org/abs/2403.12474 ,  4169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12481
Date: Tue, 19 Mar 2024 06:36:42 GMT   (1873kb,D)

Title: TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer
Authors: Eunjee Choi, Jong-Kook Kim
Categories: cs.LG cs.CV
Comments: 8 pages, submitted to conference
\\
  Detecting fake news has received a lot of attention. Many previous methods
concatenate independently encoded unimodal data, ignoring the benefits of
integrated multimodal information. Also, the absence of specialized feature
extraction for text and images further limits these methods. This paper
introduces an end-to-end model called TT-BLIP that applies the bootstrapping
language-image pretraining for unified vision-language understanding and
generation (BLIP) for three types of information: BERT and
BLIP\textsubscript{Txt} for text, ResNet and BLIP\textsubscript{Img} for
images, and bidirectional BLIP encoders for multimodal information. The
Multimodal Tri-Transformer fuses tri-modal features using three types of
multi-head attention mechanisms, ensuring integrated modalities for enhanced
representations and improved multimodal data analysis. The experiments are
performed using two fake news datasets, Weibo and Gossipcop. The results
indicate TT-BLIP outperforms the state-of-the-art models.
\\ ( https://arxiv.org/abs/2403.12481 ,  1873kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12486
Date: Tue, 19 Mar 2024 06:43:46 GMT   (9383kb,D)

Title: NTK-Guided Few-Shot Class Incremental Learning
Authors: Jingren Liu, Zhong Ji, Yanwei Pang and YunLong Yu
Categories: cs.LG cs.AI
\\
  While anti-amnesia FSCIL learners often excel in incremental sessions, they
tend to prioritize mitigating knowledge attrition over harnessing the model's
potential for knowledge acquisition. In this paper, we delve into the
foundations of model generalization in FSCIL through the lens of the Neural
Tangent Kernel (NTK). Our primary design focus revolves around ensuring optimal
NTK convergence and NTK-related generalization error, serving as the
theoretical bedrock for exceptional generalization. To attain globally optimal
NTK convergence, we employ a meta-learning mechanism grounded in mathematical
principles to guide the optimization process within an expanded network.
Furthermore, to reduce the NTK-related generalization error, we commence from
the foundational level, optimizing the relevant factors constituting its
generalization loss. Specifically, we initiate self-supervised pre-training on
the base session to shape the initial network weights. Then they are carefully
refined through curricular alignment, followed by the application of dual NTK
regularization tailored specifically for both convolutional and linear layers.
Through the combined effects of these measures, our network acquires robust NTK
properties, significantly enhancing its foundational generalization. On popular
FSCIL benchmark datasets, our NTK-FSCIL surpasses contemporary state-of-the-art
approaches, elevating end-session accuracy by 2.9% to 8.7%.
\\ ( https://arxiv.org/abs/2403.12486 ,  9383kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12511
Date: Tue, 19 Mar 2024 07:25:36 GMT   (575kb,D)

Title: Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient
  Deep Neural Network Training
Authors: M. Rostami, S. S. Kia
Categories: cs.LG math.OC
\\
  Training a deep neural network using gradient-based methods necessitates the
calculation of gradients at each level. However, using backpropagation or
reverse mode differentiation, to calculate the gradients necessities
significant memory consumption, rendering backpropagation an inefficient method
for computing gradients. This paper focuses on analyzing the performance of the
well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by
having access to the forward mode of automatic differentiation to compute
gradients. We provide in-depth technical details that show the proposed
Algorithm does converge to the optimal solution with a sub-linear rate of
convergence by having access to the noisy estimate of the true gradient
obtained in the forward mode of automated differentiation, referred to as the
Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm,
when provided with access to the Projected Forward Gradient, fails to converge
to the optimal solution. We demonstrate the convergence attributes of our
proposed algorithms using a numerical example.
\\ ( https://arxiv.org/abs/2403.12511 ,  575kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12529
Date: Tue, 19 Mar 2024 08:05:49 GMT   (318kb,D)

Title: Contextualized Messages Boost Graph Representations
Authors: Brian Godwin Lim
Categories: cs.LG
\\
  Graph neural networks (GNNs) have gained significant interest in recent years
due to their ability to handle arbitrarily structured data represented as
graphs. GNNs generally follow the message-passing scheme to locally update node
feature representations. A graph readout function is then employed to create a
representation for the entire graph. Several studies proposed different GNNs by
modifying the aggregation and combination strategies of the message-passing
framework, often inspired by heuristics. Nevertheless, several studies have
begun exploring GNNs from a theoretical perspective based on the graph
isomorphism problem which inherently assumes countable node feature
representations. Yet, there are only a few theoretical works exploring GNNs
with uncountable node feature representations. This paper presents a new
perspective on the representational capabilities of GNNs across all levels -
node-level, neighborhood-level, and graph-level - when the space of node
feature representation is uncountable. From the results, a novel
soft-isomorphic relational graph convolution network (SIR-GCN) is proposed that
emphasizes non-linear and contextualized transformations of neighborhood
feature representations. The mathematical relationship of SIR-GCN and three
widely used GNNs is explored to highlight the contribution. Validation on
synthetic datasets then demonstrates that SIR-GCN outperforms comparable models
even in simple node and graph property prediction tasks.
\\ ( https://arxiv.org/abs/2403.12529 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12544
Date: Tue, 19 Mar 2024 08:40:21 GMT   (470kb,D)

Title: AffineQuant: Affine Transformation Quantization for Large Language
  Models
Authors: Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang,
  Shilei Wen, Fei Chao, Rongrong Ji
Categories: cs.LG
Comments: ICLR 2024
\\
  The significant resource requirements associated with Large-scale Language
Models (LLMs) have generated considerable interest in the development of
techniques aimed at compressing and accelerating neural networks. Among these
techniques, Post-Training Quantization (PTQ) has emerged as a subject of
considerable interest due to its noteworthy compression efficiency and
cost-effectiveness in the context of training. Existing PTQ methods for LLMs
limit the optimization scope to scaling transformations between pre- and
post-quantization weights. In this paper, we advocate for the direct
optimization using equivalent Affine transformations in PTQ (AffineQuant). This
approach extends the optimization scope and thus significantly minimizing
quantization errors. Additionally, by employing the corresponding inverse
matrix, we can ensure equivalence between the pre- and post-quantization
outputs of PTQ, thereby maintaining its efficiency and generalization
capabilities. To ensure the invertibility of the transformation during
optimization, we further introduce a gradual mask optimization method. This
method initially focuses on optimizing the diagonal elements and gradually
extends to the other elements. Such an approach aligns with the
Levy-Desplanques theorem, theoretically ensuring invertibility of the
transformation. As a result, significant performance improvements are evident
across different LLMs on diverse datasets. To illustrate, we attain a C4
perplexity of 15.76 (2.26 lower vs 18.02 in OmniQuant) on the LLaMA2-7B model
of W4A4 quantization without overhead. On zero-shot tasks, AffineQuant achieves
an average of 58.61 accuracy (1.98 lower vs 56.63 in OmniQuant) when using
4/4-bit quantization for LLaMA-30B, which setting a new state-of-the-art
benchmark for PTQ in LLMs.
\\ ( https://arxiv.org/abs/2403.12544 ,  470kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12553
Date: Tue, 19 Mar 2024 08:56:20 GMT   (2688kb,D)

Title: Pretraining Codomain Attention Neural Operators for Solving Multiphysics
  PDEs
Authors: Md Ashiqur Rahman, Robert Joseph George, Mogab Elleithy, Daniel
  Leibovici, Zongyi Li, Boris Bonev, Colin White, Julius Berner, Raymond A.
  Yeh, Jean Kossaifi, Kamyar Azizzadenesheli, Anima Anandkumar
Categories: cs.LG
\\
  Existing neural operator architectures face challenges when solving
multiphysics problems with coupled partial differential equations (PDEs), due
to complex geometries, interactions between physical variables, and the lack of
large amounts of high-resolution training data. To address these issues, we
propose Codomain Attention Neural Operator (CoDA-NO), which tokenizes functions
along the codomain or channel space, enabling self-supervised learning or
pretraining of multiple PDE systems. Specifically, we extend positional
encoding, self-attention, and normalization layers to the function space.
CoDA-NO can learn representations of different PDE systems with a single model.
We evaluate CoDA-NO's potential as a backbone for learning multiphysics PDEs
over multiple systems by considering few-shot learning settings. On complex
downstream tasks with limited data, such as fluid flow simulations and
fluid-structure interactions, we found CoDA-NO to outperform existing methods
on the few-shot learning task by over $36\%$. The code is available at
https://github.com/ashiq24/CoDA-NO.
\\ ( https://arxiv.org/abs/2403.12553 ,  2688kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12562
Date: Tue, 19 Mar 2024 09:17:18 GMT   (382kb,D)

Title: Equity through Access: A Case for Small-scale Deep Learning
Authors: Raghavendra Selvan, Bob Pepin, Christian Igel, Gabrielle Samuel, Erik
  B Dam
Categories: cs.LG cs.AI stat.ML
Comments: Source code available at https://github.com/saintslab/PePR
\\
  The recent advances in deep learning (DL) have been accelerated by access to
large-scale data and compute. These large-scale resources have been used to
train progressively larger models which are resource intensive in terms of
compute, data, energy, and carbon emissions. These costs are becoming a new
type of entry barrier to researchers and practitioners with limited access to
resources at such scale, particularly in the Global South. In this work, we
take a comprehensive look at the landscape of existing DL models for vision
tasks and demonstrate their usefulness in settings where resources are limited.
To account for the resource consumption of DL models, we introduce a novel
measure to estimate the performance per resource unit, which we call the PePR
score. Using a diverse family of 131 unique DL architectures (spanning 1M to
130M trainable parameters) and three medical image datasets, we capture trends
about the performance-resource trade-offs. In applications like medical image
analysis, we argue that small-scale, specialized models are better than
striving for large-scale models. Furthermore, we show that using pretrained
models can significantly reduce the computational resources and data required.
We hope this work will encourage the community to focus on improving AI equity
by developing methods and models with smaller resource footprints.
\\ ( https://arxiv.org/abs/2403.12562 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12606
Date: Tue, 19 Mar 2024 10:17:26 GMT   (9356kb,D)

Title: On the Effectiveness of Heterogeneous Ensemble Methods for
  Re-identification
Authors: Simon Kl\"uttermann, J\'er\^ome Rutinowski, Anh Nguyen, Britta Grimme,
  Moritz Roidl, Emmanuel M\"uller
Categories: cs.LG
\\
  In this contribution, we introduce a novel ensemble method for the
re-identification of industrial entities, using images of chipwood pallets and
galvanized metal plates as dataset examples. Our algorithms replace commonly
used, complex siamese neural networks with an ensemble of simplified,
rudimentary models, providing wider applicability, especially in
hardware-restricted scenarios. Each ensemble sub-model uses different types of
extracted features of the given data as its input, allowing for the creation of
effective ensembles in a fraction of the training duration needed for more
complex state-of-the-art models. We reach state-of-the-art performance at our
task, with a Rank-1 accuracy of over 77% and a Rank-10 accuracy of over 99%,
and introduce five distinct feature extraction approaches, and study their
combination using different ensemble methods.
\\ ( https://arxiv.org/abs/2403.12606 ,  9356kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12609
Date: Tue, 19 Mar 2024 10:24:15 GMT   (384kb,D)

Title: SUN Team's Contribution to ABAW 2024 Competition: Audio-visual
  Valence-Arousal Estimation and Expression Recognition
Authors: Denis Dresvyanskiy, Maxim Markitantov, Jiawei Yu, Peitong Li, Heysem
  Kaya and Alexey Karpov
Categories: cs.LG
Comments: 9 pages,
ACM-class: I.2.1; I.2.6; I.2.10
\\
  As emotions play a central role in human communication, automatic emotion
recognition has attracted increasing attention in the last two decades. While
multimodal systems enjoy high performances on lab-controlled data, they are
still far from providing ecological validity on non-lab-controlled, namely
'in-the-wild' data. This work investigates audiovisual deep learning approaches
for emotion recognition in-the-wild problem. We particularly explore the
effectiveness of architectures based on fine-tuned Convolutional Neural
Networks (CNN) and Public Dimensional Emotion Model (PDEM), for video and audio
modality, respectively. We compare alternative temporal modeling and fusion
strategies using the embeddings from these multi-stage trained
modality-specific Deep Neural Networks (DNN). We report results on the AffWild2
dataset under Affective Behavior Analysis in-the-Wild 2024 (ABAW'24) challenge
protocol.
\\ ( https://arxiv.org/abs/2403.12609 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12636
Date: Tue, 19 Mar 2024 11:16:14 GMT   (6949kb,D)

Title: A Practical Guide to Statistical Distances for Evaluating Generative
  Models in Science
Authors: Sebastian Bischoff, Alana Darcher, Michael Deistler, Richard Gao,
  Franziska Gerken, Manuel Gloeckler, Lisa Haxel, Jaivardhan Kapoor, Janne K
  Lappalainen, Jakob H Macke, Guy Moss, Matthijs Pals, Felix Pei, Rachel Rapp,
  A Erdem Sa\u{g}tekin, Cornelius Schr\"oder, Auguste Schulz, Zinovia
  Stefanidi, Shoji Toyota, Linda Ulmer, Julius Vetter
Categories: cs.LG stat.ML
\\
  Generative models are invaluable in many fields of science because of their
ability to capture high-dimensional and complicated distributions, such as
photo-realistic images, protein structures, and connectomes. How do we evaluate
the samples these models generate? This work aims to provide an accessible
entry point to understanding popular notions of statistical distances,
requiring only foundational knowledge in mathematics and statistics. We focus
on four commonly used notions of statistical distances representing different
methodologies: Using low-dimensional projections (Sliced-Wasserstein; SW),
obtaining a distance using classifiers (Classifier Two-Sample Tests; C2ST),
using embeddings through kernels (Maximum Mean Discrepancy; MMD), or neural
networks (Fr\'echet Inception Distance; FID). We highlight the intuition behind
each distance and explain their merits, scalability, complexity, and pitfalls.
To demonstrate how these distances are used in practice, we evaluate generative
models from different scientific domains, namely a model of decision making and
a model generating medical images. We showcase that distinct distances can give
different results on similar data. Through this guide, we aim to help
researchers to use, interpret, and evaluate statistical distances for
generative models in science.
\\ ( https://arxiv.org/abs/2403.12636 ,  6949kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12641
Date: Tue, 19 Mar 2024 11:24:14 GMT   (2808kb,D)

Title: Automated Contrastive Learning Strategy Search for Time Series
Authors: Baoyu Jing, Yansen Wang, Guoxin Sui, Jing Hong, Jingrui He, Yuqing
  Yang, Dongsheng Li, Kan Ren
Categories: cs.LG
Comments: Preprint. Work in progress
\\
  In recent years, Contrastive Learning (CL) has become a predominant
representation learning paradigm for time series. Most existing methods in the
literature focus on manually building specific Contrastive Learning Strategies
(CLS) by human heuristics for certain datasets and tasks. However, manually
developing CLS usually require excessive prior knowledge about the datasets and
tasks, e.g., professional cognition of the medical time series in healthcare,
as well as huge human labor and massive experiments to determine the detailed
learning configurations. In this paper, we present an Automated Machine
Learning (AutoML) practice at Microsoft, which automatically learns to
contrastively learn representations for various time series datasets and tasks,
namely Automated Contrastive Learning (AutoCL). We first construct a principled
universal search space of size over 3x1012, covering data augmentation,
embedding transformation, contrastive pair construction and contrastive losses.
Further, we introduce an efficient reinforcement learning algorithm, which
optimizes CLS from the performance on the validation tasks, to obtain more
effective CLS within the space. Experimental results on various real-world
tasks and datasets demonstrate that AutoCL could automatically find the
suitable CLS for a given dataset and task. From the candidate CLS found by
AutoCL on several public datasets/tasks, we compose a transferable Generally
Good Strategy (GGS), which has a strong performance for other datasets. We also
provide empirical analysis as a guidance for future design of CLS.
\\ ( https://arxiv.org/abs/2403.12641 ,  2808kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12646
Date: Tue, 19 Mar 2024 11:30:30 GMT   (1307kb,D)

Title: Prompt-fused framework for Inductive Logical Query Answering
Authors: Zezhong Xu, Peng Ye, Lei Liang, Huajun Chen, Wen Zhang
Categories: cs.LG
Comments: Accepted by COLING 2024
\\
  Answering logical queries on knowledge graphs (KG) poses a significant
challenge for machine reasoning. The primary obstacle in this task stems from
the inherent incompleteness of KGs. Existing research has predominantly focused
on addressing the issue of missing edges in KGs, thereby neglecting another
aspect of incompleteness: the emergence of new entities. Furthermore, most of
the existing methods tend to reason over each logical operator separately,
rather than comprehensively analyzing the query as a whole during the reasoning
process. In this paper, we propose a query-aware prompt-fused framework named
Pro-QE, which could incorporate existing query embedding methods and address
the embedding of emerging entities through contextual information aggregation.
Additionally, a query prompt, which is generated by encoding the symbolic
query, is introduced to gather information relevant to the query from a
holistic perspective. To evaluate the efficacy of our model in the inductive
setting, we introduce two new challenging benchmarks. Experimental results
demonstrate that our model successfully handles the issue of unseen entities in
logical queries. Furthermore, the ablation study confirms the efficacy of the
aggregator and prompt components.
\\ ( https://arxiv.org/abs/2403.12646 ,  1307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12664
Date: Tue, 19 Mar 2024 11:56:21 GMT   (1464kb,D)

Title: Deciphering AutoML Ensembles: cattleia's Assistance in Decision-Making
Authors: Anna Kozak, Dominik K\k{e}dzierski, Jakub Piwko, Malwina Wojewoda,
  Katarzyna Wo\'znica
Categories: cs.LG cs.AI
\\
  In many applications, model ensembling proves to be better than a single
predictive model. Hence, it is the most common post-processing technique in
Automated Machine Learning (AutoML). The most popular frameworks use ensembles
at the expense of reducing the interpretability of the final models. In our
work, we propose cattleia - an application that deciphers the ensembles for
regression, multiclass, and binary classification tasks. This tool works with
models built by three AutoML packages: auto-sklearn, AutoGluon, and FLAML. The
given ensemble is analyzed from different perspectives. We conduct a predictive
performance investigation through evaluation metrics of the ensemble and its
component models. We extend the validation perspective by introducing new
measures to assess the diversity and complementarity of the model predictions.
Moreover, we apply explainable artificial intelligence (XAI) techniques to
examine the importance of variables. Summarizing obtained insights, we can
investigate and adjust the weights with a modification tool to tune the
ensemble in the desired way. The application provides the aforementioned
aspects through dedicated interactive visualizations, making it accessible to a
diverse audience. We believe the cattleia can support users in decision-making
and deepen the comprehension of AutoML frameworks.
\\ ( https://arxiv.org/abs/2403.12664 ,  1464kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12672
Date: Tue, 19 Mar 2024 12:13:52 GMT   (860kb)

Title: Improving Interpretability of Scores in Anomaly Detection Based on
  Gaussian-Bernoulli Restricted Boltzmann Machine
Authors: Kaiji Sekimoto and Muneki Yasuda
Categories: cs.LG cs.AI stat.ML
\\
  Gaussian-Bernoulli restricted Boltzmann machines (GBRBMs) are often used for
semi-supervised anomaly detection, where they are trained using only normal
data points. In GBRBM-based anomaly detection, normal and anomalous data are
classified based on a score that is identical to an energy function of the
marginal GBRBM. However, the classification threshold is difficult to set to an
appropriate value, as this score cannot be interpreted. In this study, we
propose a measure that improves score's interpretability based on its
cumulative distribution, and establish a guideline for setting the threshold
using the interpretable measure. The results of numerical experiments show that
the guideline is reasonable when setting the threshold solely using normal data
points. Moreover, because identifying the measure involves computationally
infeasible evaluation of the minimum score value, we also propose an evaluation
method for the minimum score based on simulated annealing, which is widely used
for optimization problems. The proposed evaluation method was also validated
using numerical experiments.
\\ ( https://arxiv.org/abs/2403.12672 ,  860kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12688
Date: Tue, 19 Mar 2024 12:47:43 GMT   (1412kb,D)

Title: SEVEN: Pruning Transformer Model by Reserving Sentinels
Authors: Jinying Xiao, Ping Li, Jie Nie, Zhe Tang
Categories: cs.LG
Comments: 9 pages,6 figures
\\
  Large-scale Transformer models (TM) have demonstrated outstanding performance
across various tasks. However, their considerable parameter size restricts
their applicability, particularly on mobile devices. Due to the dynamic and
intricate nature of gradients on TM compared to Convolutional Neural Networks,
commonly used pruning methods tend to retain weights with larger gradient
noise. This results in pruned models that are sensitive to sparsity and
datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general
approach for training and fine-tuning TM. In this paper, we attempt to describe
the noisy batch gradient sequences on TM through the cumulative process of SD.
We utilize this design to dynamically assess the importance scores of
weights.SEVEN is introduced by us, which particularly favors weights with
consistently high sensitivity, i.e., weights with small gradient noise. These
weights are tended to be preserved by SEVEN. Extensive experiments on various
TM in natural language, question-answering, and image classification domains
are conducted to validate the effectiveness of SEVEN. The results demonstrate
significant improvements of SEVEN in multiple pruning scenarios and across
different sparsity levels. Additionally, SEVEN exhibits robust performance
under various fine-tuning strategies. The code is publicly available at
https://github.com/xiaojinying/SEVEN.
\\ ( https://arxiv.org/abs/2403.12688 ,  1412kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12690
Date: Tue, 19 Mar 2024 12:49:09 GMT   (1538kb,D)

Title: LNPT: Label-free Network Pruning and Training
Authors: Jinying Xiao, Ping Li, Zhe Tang, Jie Nie
Categories: cs.LG
Comments: 8 pages,7 figures
\\
  Pruning before training enables the deployment of neural networks on smart
devices. By retaining weights conducive to generalization, pruned networks can
be accommodated on resource-constrained smart devices. It is commonly held that
the distance on weight norms between the initialized and the fully-trained
networks correlates with generalization performance. However, as we have
uncovered, inconsistency between this metric and generalization during training
processes, which poses an obstacle to determine the pruned structures on smart
devices in advance. In this paper, we introduce the concept of the learning
gap, emphasizing its accurate correlation with generalization. Experiments show
that the learning gap, in the form of feature maps from the penultimate layer
of networks, aligns with variations of generalization performance. We propose a
novel learning framework, LNPT, which enables mature networks on the cloud to
provide online guidance for network pruning and learning on smart devices with
unlabeled data. Our results demonstrate the superiority of this approach over
supervised training.
\\ ( https://arxiv.org/abs/2403.12690 ,  1538kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12719
Date: Tue, 19 Mar 2024 13:28:03 GMT   (341kb,D)

Title: Bilevel Hypergraph Networks for Multi-Modal Alzheimer's Diagnosis
Authors: Angelica I. Aviles-Rivero, Chun-Wun Cheng, Zhongying Deng, Zoe
  Kourtzi, Carola-Bibiane Sch\"onlieb
Categories: cs.LG
\\
  Early detection of Alzheimer's disease's precursor stages is imperative for
significantly enhancing patient outcomes and quality of life. This challenge is
tackled through a semi-supervised multi-modal diagnosis framework. In
particular, we introduce a new hypergraph framework that enables higher-order
relations between multi-modal data, while utilising minimal labels. We first
introduce a bilevel hypergraph optimisation framework that jointly learns a
graph augmentation policy and a semi-supervised classifier. This dual learning
strategy is hypothesised to enhance the robustness and generalisation
capabilities of the model by fostering new pathways for information
propagation. Secondly, we introduce a novel strategy for generating
pseudo-labels more effectively via a gradient-driven flow. Our experimental
results demonstrate the superior performance of our framework over current
techniques in diagnosing Alzheimer's disease.
\\ ( https://arxiv.org/abs/2403.12719 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12764
Date: Tue, 19 Mar 2024 14:30:56 GMT   (2072kb,D)

Title: Neural Parameter Regression for Explicit Representations of PDE Solution
  Operators
Authors: Konrad Mundinger, Max Zimmer, Sebastian Pokutta
Categories: cs.LG cs.NA math.NA
Comments: ICLR24 Workshop AI4Differential Equations In Science, 15 pages, 4
  figures, 2 tables, 1 algorithm
\\
  We introduce Neural Parameter Regression (NPR), a novel framework
specifically developed for learning solution operators in Partial Differential
Equations (PDEs). Tailored for operator learning, this approach surpasses
traditional DeepONets (Lu et al., 2021) by employing Physics-Informed Neural
Network (PINN, Raissi et al., 2019) techniques to regress Neural Network (NN)
parameters. By parametrizing each solution based on specific initial
conditions, it effectively approximates a mapping between function spaces. Our
method enhances parameter efficiency by incorporating low-rank matrices,
thereby boosting computational efficiency and scalability. The framework shows
remarkable adaptability to new initial and boundary conditions, allowing for
rapid fine-tuning and inference, even in cases of out-of-distribution examples.
\\ ( https://arxiv.org/abs/2403.12764 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12818
Date: Tue, 19 Mar 2024 15:17:23 GMT   (2181kb,D)

Title: Dynamic Survival Analysis for Early Event Prediction
Authors: Hugo Y\`eche, Manuel Burger, Dinara Veshchezerova, Gunnar R\"atsch
Categories: cs.LG
\\
  This study advances Early Event Prediction (EEP) in healthcare through
Dynamic Survival Analysis (DSA), offering a novel approach by integrating risk
localization into alarm policies to enhance clinical event metrics. By adapting
and evaluating DSA models against traditional EEP benchmarks, our research
demonstrates their ability to match EEP models on a time-step level and
significantly improve event-level metrics through a new alarm prioritization
scheme (up to 11% AuPRC difference). This approach represents a significant
step forward in predictive healthcare, providing a more nuanced and actionable
framework for early event prediction and management.
\\ ( https://arxiv.org/abs/2403.12818 ,  2181kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12821
Date: Tue, 19 Mar 2024 15:21:10 GMT   (953kb,D)

Title: FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware
  Graph Transformer
Authors: Dongyeong Hwang, Hyunju Kim, Sunwoo Kim, Kijung Shin
Categories: cs.LG cs.AI
Comments: CVPR 2024 Camera-Ready
\\
  The success of a specific neural network architecture is closely tied to the
dataset and task it tackles; there is no one-size-fits-all solution. Thus,
considerable efforts have been made to quickly and accurately estimate the
performances of neural architectures, without full training or evaluation, for
given tasks and datasets. Neural architecture encoding has played a crucial
role in the estimation, and graphbased methods, which treat an architecture as
a graph, have shown prominent performance. For enhanced representation learning
of neural architectures, we introduce FlowerFormer, a powerful graph
transformer that incorporates the information flows within a neural
architecture. FlowerFormer consists of two key components: (a) bidirectional
asynchronous message passing, inspired by the flows; (b) global attention built
on flow-based masking. Our extensive experiments demonstrate the superiority of
FlowerFormer over existing neural encoding methods, and its effectiveness
extends beyond computer vision models to include graph neural networks and auto
speech recognition models. Our code is available at
http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.
\\ ( https://arxiv.org/abs/2403.12821 ,  953kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12830
Date: Tue, 19 Mar 2024 15:37:27 GMT   (549kb,D)

Title: Has Approximate Machine Unlearning been evaluated properly? From
  Auditing to Side Effects
Authors: Cheng-Long Wang, Qi Li, Zihang Xiang, and Di Wang
Categories: cs.LG cs.CR
\\
  The growing concerns surrounding data privacy and security have underscored
the critical necessity for machine unlearning--aimed at fully removing data
lineage from machine learning models. MLaaS providers expect this to be their
ultimate safeguard for regulatory compliance. Despite its critical importance,
the pace at which privacy communities have been developing and implementing
strong methods to verify the effectiveness of machine unlearning has been
disappointingly slow, with this vital area often receiving insufficient focus.
This paper seeks to address this shortfall by introducing well-defined and
effective metrics for black-box unlearning auditing tasks. We transform the
auditing challenge into a question of non-membership inference and develop
efficient metrics for auditing. By relying exclusively on the original and
unlearned models--eliminating the need to train additional shadow models--our
approach simplifies the evaluation of unlearning at the individual data point
level. Utilizing these metrics, we conduct an in-depth analysis of current
approximate machine unlearning algorithms, identifying three key directions
where these approaches fall short: utility, resilience, and equity. Our aim is
that this work will greatly improve our understanding of approximate machine
unlearning methods, taking a significant stride towards converting the
theoretical right to data erasure into a auditable reality.
\\ ( https://arxiv.org/abs/2403.12830 ,  549kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12844
Date: Tue, 19 Mar 2024 15:51:21 GMT   (4534kb,D)

Title: MELTing point: Mobile Evaluation of Language Transformers
Authors: Stefanos Laskaridis, Kleomenis Kateveas, Lorenzo Minto, Hamed Haddadi
Categories: cs.LG
Comments: Under review
\\
  Transformers have revolutionized the machine learning landscape, gradually
making their way into everyday tasks and equipping our computers with ``sparks
of intelligence''. However, their runtime requirements have prevented them from
being broadly deployed on mobile. As personal devices become increasingly
powerful and prompt privacy becomes an ever more pressing issue, we explore the
current state of mobile execution of Large Language Models (LLMs). To achieve
this, we have created our own automation infrastructure, MELT, which supports
the headless execution and benchmarking of LLMs on device, supporting different
models, devices and frameworks, including Android, iOS and Nvidia Jetson
devices. We evaluate popular instruction fine-tuned LLMs and leverage different
frameworks to measure their end-to-end and granular performance, tracing their
memory and energy requirements along the way.
  Our analysis is the first systematic study of on-device LLM execution,
quantifying performance, energy efficiency and accuracy across various
state-of-the-art models and showcases the state of on-device intelligence in
the era of hyperscale models. Results highlight the performance heterogeneity
across targets and corroborates that LLM inference is largely memory-bound.
Quantization drastically reduces memory requirements and renders execution
viable, but at a non-negligible accuracy cost. Drawing from its energy
footprint and thermal behavior, the continuous execution of LLMs remains
elusive, as both factors negatively affect user experience. Last, our
experience shows that the ecosystem is still in its infancy, and algorithmic as
well as hardware breakthroughs can significantly shift the execution cost. We
expect NPU acceleration, and framework-hardware co-design to be the biggest bet
towards efficient standalone execution, with the alternative of offloading
tailored towards edge deployments.
\\ ( https://arxiv.org/abs/2403.12844 ,  4534kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12847
Date: Tue, 19 Mar 2024 15:54:38 GMT   (2641kb,D)

Title: Policy Bifurcation in Safe Reinforcement Learning
Authors: Wenjun Zou, Yao Lv, Jie Li, Yujie Yang, Shengbo Eben Li, Jingliang
  Duan, Xianyuan Zhan, Jingjing Liu, Yaqin Zhang, and Keqiang Li
Categories: cs.LG
\\
  Safe reinforcement learning (RL) offers advanced solutions to constrained
optimal control problems. Existing studies in safe RL implicitly assume
continuity in policy functions, where policies map states to actions in a
smooth, uninterrupted manner; however, our research finds that in some
scenarios, the feasible policy should be discontinuous or multi-valued,
interpolating between discontinuous local optima can inevitably lead to
constraint violations. We are the first to identify the generating mechanism of
such a phenomenon, and employ topological analysis to rigorously prove the
existence of policy bifurcation in safe RL, which corresponds to the
contractibility of the reachable tuple. Our theorem reveals that in scenarios
where the obstacle-free state space is non-simply connected, a feasible policy
is required to be bifurcated, meaning its output action needs to change
abruptly in response to the varying state. To train such a bifurcated policy,
we propose a safe RL algorithm called multimodal policy optimization (MUPO),
which utilizes a Gaussian mixture distribution as the policy output. The
bifurcated behavior can be achieved by selecting the Gaussian component with
the highest mixing coefficient. Besides, MUPO also integrates spectral
normalization and forward KL divergence to enhance the policy's capability of
exploring different modes. Experiments with vehicle control tasks show that our
algorithm successfully learns the bifurcated policy and ensures satisfying
safety, while a continuous policy suffers from inevitable constraint
violations.
\\ ( https://arxiv.org/abs/2403.12847 ,  2641kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12856
Date: Tue, 19 Mar 2024 16:01:25 GMT   (1693kb,D)

Title: Equivariant Ensembles and Regularization for Reinforcement Learning in
  Map-based Path Planning
Authors: Mirco Theile and Hongpeng Cao and Marco Caccamo and Alberto L.
  Sangiovanni-Vincentelli
Categories: cs.LG cs.RO
Comments: submitted for possible publication. A video can be found here:
  https://youtu.be/L6NOdvU7n7s
\\
  In reinforcement learning (RL), exploiting environmental symmetries can
significantly enhance efficiency, robustness, and performance. However,
ensuring that the deep RL policy and value networks are respectively
equivariant and invariant to exploit these symmetries is a substantial
challenge. Related works try to design networks that are equivariant and
invariant by construction, limiting them to a very restricted library of
components, which in turn hampers the expressiveness of the networks. This
paper proposes a method to construct equivariant policies and invariant value
functions without specialized neural network components, which we term
equivariant ensembles. We further add a regularization term for adding
inductive bias during training. In a map-based path planning case study, we
show how equivariant ensembles and regularization benefit sample efficiency and
performance.
\\ ( https://arxiv.org/abs/2403.12856 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12864
Date: Tue, 19 Mar 2024 16:08:27 GMT   (398kb,D)

Title: A Comparison of Deep Learning Architectures for Spacecraft Anomaly
  Detection
Authors: Daniel Lakey and Tim Schlippe
Categories: cs.LG
Comments: accepted for IEEE Aeroconf 2024
\\
  Spacecraft operations are highly critical, demanding impeccable reliability
and safety. Ensuring the optimal performance of a spacecraft requires the early
detection and mitigation of anomalies, which could otherwise result in unit or
mission failures. With the advent of deep learning, a surge of interest has
been seen in leveraging these sophisticated algorithms for anomaly detection in
space operations. This study aims to compare the efficacy of various deep
learning architectures in detecting anomalies in spacecraft data. The deep
learning models under investigation include Convolutional Neural Networks
(CNNs), Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM)
networks, and Transformer-based architectures. Each of these models was trained
and validated using a comprehensive dataset sourced from multiple spacecraft
missions, encompassing diverse operational scenarios and anomaly types. Initial
results indicate that while CNNs excel in identifying spatial patterns and may
be effective for some classes of spacecraft data, LSTMs and RNNs show a marked
proficiency in capturing temporal anomalies seen in time-series spacecraft
telemetry. The Transformer-based architectures, given their ability to focus on
both local and global contexts, have showcased promising results, especially in
scenarios where anomalies are subtle and span over longer durations.
Additionally, considerations such as computational efficiency, ease of
deployment, and real-time processing capabilities were evaluated. While CNNs
and LSTMs demonstrated a balance between accuracy and computational demands,
Transformer architectures, though highly accurate, require significant
computational resources. In conclusion, the choice of deep learning
architecture for spacecraft anomaly detection is highly contingent on the
nature of the data, the type of anomalies, and operational constraints.
\\ ( https://arxiv.org/abs/2403.12864 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12871
Date: Tue, 19 Mar 2024 16:15:44 GMT   (5674kb,D)

Title: Wildfire danger prediction optimization with transfer learning
Authors: Spiros Maggioros and Nikos Tsalkitzis
Categories: cs.LG
\\
  Convolutional Neural Networks (CNNs) have proven instrumental across various
computer science domains, enabling advancements in object detection,
classification, and anomaly detection. This paper explores the application of
CNNs to analyze geospatial data specifically for identifying wildfire-affected
areas. Leveraging transfer learning techniques, we fine-tuned CNN
hyperparameters and integrated the Canadian Fire Weather Index (FWI) to assess
moisture conditions. The study establishes a methodology for computing wildfire
risk levels on a scale of 0 to 5, dynamically linked to weather patterns.
Notably, through the integration of transfer learning, the CNN model achieved
an impressive accuracy of 95\% in identifying burnt areas. This research sheds
light on the inner workings of CNNs and their practical, real-time utility in
predicting and mitigating wildfires. By combining transfer learning and CNNs,
this study contributes a robust approach to assess burnt areas, facilitating
timely interventions and preventative measures against conflagrations.
\\ ( https://arxiv.org/abs/2403.12871 ,  5674kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12873
Date: Tue, 19 Mar 2024 16:17:21 GMT   (3241kb,D)

Title: Short-Term Solar Irradiance Forecasting Under Data Transmission
  Constraints
Authors: Joshua Edward Hammond (1), Ricardo A. Lara Orozco (2), Michael Baldea
  (1 and 3), Brian A. Korgel (1 and 4) ((1) McKetta Department of Chemical
  Engineering The University of Texas at Austin, (2) Hildebrand Department of
  Petroleum and Geosystems Engineering The University of Texas at Austin, (3)
  Institute for Computational Engineering and Sciences The University of Texas
  at Austin,(4) Energy Institute The University of Texas at Austin)
Categories: cs.LG
Comments: 21 pages, 12 figures
\\
  We report a data-parsimonious machine learning model for short-term
forecasting of solar irradiance. The model inputs include sky camera images
that are reduced to scalar features to meet data transmission constraints. The
output irradiance values are transformed to focus on unknown short-term
dynamics. Inspired by control theory, a noise input is used to reflect
unmeasured variables and is shown to improve model predictions, often
considerably. Five years of data from the NREL Solar Radiation Research
Laboratory were used to create three rolling train-validate sets and determine
the best representations for time, the optimal span of input measurements, and
the most impactful model input data (features). For the chosen test data, the
model achieves a mean absolute error of 74.34 $W/m^2$ compared to a baseline
134.35 $W/m^2$ using the persistence of cloudiness model.
\\ ( https://arxiv.org/abs/2403.12873 ,  3241kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12887
Date: Tue, 19 Mar 2024 16:34:31 GMT   (79kb)

Title: Understanding the training of infinitely deep and wide ResNets with
  Conditional Optimal Transport
Authors: Rapha\"el Barboni (ENS-PSL), Gabriel Peyr\'e (CNRS and ENS-PSL),
  Fran\c{c}ois-Xavier Vialard (LIGM)
Categories: cs.LG math.OC
\\
  We study the convergence of gradient flow for the training of deep neural
networks. If Residual Neural Networks are a popular example of very deep
architectures, their training constitutes a challenging optimization problem
due notably to the non-convexity and the non-coercivity of the objective. Yet,
in applications, those tasks are successfully solved by simple optimization
algorithms such as gradient descent. To better understand this phenomenon, we
focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide
ResNet, parameterized by probability measures over the product set of layers
and parameters and with constant marginal on the set of layers. Indeed, in the
case of shallow neural networks, mean field models have proven to benefit from
simplified loss-landscapes and good theoretical guarantees when trained with
gradient flow for the Wasserstein metric on the set of probability measures.
Motivated by this approach, we propose to train our model with gradient flow
w.r.t. the conditional Optimal Transport distance: a restriction of the
classical Wasserstein distance which enforces our marginal condition. Relying
on the theory of gradient flows in metric spaces we first show the
well-posedness of the gradient flow equation and its consistency with the
training of ResNets at finite width. Performing a local Polyak-\L{}ojasiewicz
analysis, we then show convergence of the gradient flow for well-chosen
initializations: if the number of features is finite but sufficiently large and
the risk is sufficiently small at initialization, the gradient flow converges
towards a global minimizer. This is the first result of this type for
infinitely deep and arbitrarily wide ResNets.
\\ ( https://arxiv.org/abs/2403.12887 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12938
Date: Tue, 19 Mar 2024 17:43:57 GMT   (2701kb,D)

Title: Neural Differential Algebraic Equations
Authors: James Koch, Madelyn Shapiro, Himanshu Sharma, Draguna Vrabie, Jan
  Drgona
Categories: cs.LG
\\
  Differential-Algebraic Equations (DAEs) describe the temporal evolution of
systems that obey both differential and algebraic constraints. Of particular
interest are systems that contain implicit relationships between their
components, such as conservation relationships. Here, we present Neural
Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of
DAEs. This methodology is built upon the concept of the Universal Differential
Equation; that is, a model constructed as a system of Neural Ordinary
Differential Equations informed by theory from particular science domains. In
this work, we show that the proposed NDAEs abstraction is suitable for relevant
system-theoretic data-driven modeling tasks. Presented examples include (i) the
inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a
network of pumps, tanks, and pipes. Our experiments demonstrate the proposed
method's robustness to noise and extrapolation ability to (i) learn the
behaviors of the system components and their interaction physics and (ii)
disambiguate between data trends and mechanistic relationships contained in the
system.
\\ ( https://arxiv.org/abs/2403.12938 ,  2701kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12946
Date: Tue, 19 Mar 2024 17:48:42 GMT   (140kb)

Title: Sample Complexity of Offline Distributionally Robust Linear Markov
  Decision Processes
Authors: He Wang, Laixi Shi, Yuejie Chi
Categories: cs.LG math.ST stat.TH
Comments: under review
\\
  In offline reinforcement learning (RL), the absence of active exploration
calls for attention on the model robustness to tackle the sim-to-real gap,
where the discrepancy between the simulated and deployed environments can
significantly undermine the performance of the learned policy. To endow the
learned policy with robustness in a sample-efficient manner in the presence of
high-dimensional state-action space, this paper considers the sample complexity
of distributionally robust linear Markov decision processes (MDPs) with an
uncertainty set characterized by the total variation distance using offline
data. We develop a pessimistic model-based algorithm and establish its sample
complexity bound under minimal data coverage assumptions, which outperforms
prior art by at least $\tilde{O}(d)$, where $d$ is the feature dimension. We
further improve the performance guarantee of the proposed algorithm by
incorporating a carefully-designed variance estimator.
\\ ( https://arxiv.org/abs/2403.12946 ,  140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12948
Date: Tue, 19 Mar 2024 17:50:32 GMT   (1236kb,D)

Title: On Safety in Safe Bayesian Optimization
Authors: Christian Fiedler, Johanna Menn, Lukas Kreisk\"other, Sebastian Trimpe
Categories: cs.LG stat.ML
\\
  Optimizing an unknown function under safety constraints is a central task in
robotics, biomedical engineering, and many other disciplines, and increasingly
safe Bayesian Optimization (BO) is used for this. Due to the safety critical
nature of these applications, it is of utmost importance that theoretical
safety guarantees for these algorithms translate into the real world. In this
work, we investigate three safety-related issues of the popular class of
SafeOpt-type algorithms. First, these algorithms critically rely on frequentist
uncertainty bounds for Gaussian Process (GP) regression, but concrete
implementations typically utilize heuristics that invalidate all safety
guarantees. We provide a detailed analysis of this problem and introduce
Real-\b{eta}-SafeOpt, a variant of the SafeOpt algorithm that leverages recent
GP bounds and thus retains all theoretical guarantees. Second, we identify
assuming an upper bound on the reproducing kernel Hilbert space (RKHS) norm of
the target function, a key technical assumption in SafeOpt-like algorithms, as
a central obstacle to real-world usage. To overcome this challenge, we
introduce the Lipschitz-only Safe Bayesian Optimization (LoSBO) algorithm,
which guarantees safety without an assumption on the RKHS bound, and
empirically show that this algorithm is not only safe, but also exhibits
superior performance compared to the state-of-the-art on several function
classes. Third, SafeOpt and derived algorithms rely on a discrete search space,
making them difficult to apply to higher-dimensional problems. To widen the
applicability of these algorithms, we introduce Lipschitz-only GP-UCB
(LoS-GP-UCB), a variant of LoSBO applicable to moderately high-dimensional
problems, while retaining safety.
\\ ( https://arxiv.org/abs/2403.12948 ,  1236kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12950
Date: Tue, 19 Mar 2024 17:50:55 GMT   (144kb,D)

Title: Optimal and Adaptive Non-Stationary Dueling Bandits Under a Generalized
  Borda Criterion
Authors: Joe Suk and Arpit Agarwal
Categories: cs.LG stat.ML
\\
  In dueling bandits, the learner receives preference feedback between arms,
and the regret of an arm is defined in terms of its suboptimality to a winner
arm. The more challenging and practically motivated non-stationary variant of
dueling bandits, where preferences change over time, has been the focus of
several recent works (Saha and Gupta, 2022; Buening and Saha, 2023; Suk and
Agarwal, 2023). The goal is to design algorithms without foreknowledge of the
amount of change.
  The bulk of known results here studies the Condorcet winner setting, where an
arm preferred over any other exists at all times. Yet, such a winner may not
exist and, to contrast, the Borda version of this problem (which is always
well-defined) has received little attention. In this work, we establish the
first optimal and adaptive Borda dynamic regret upper bound, which highlights
fundamental differences in the learnability of severe non-stationarity between
Condorcet vs. Borda regret objectives in dueling bandits.
  Surprisingly, our techniques for non-stationary Borda dueling bandits also
yield improved rates within the Condorcet winner setting, and reveal new
preference models where tighter notions of non-stationarity are adaptively
learnable. This is accomplished through a novel generalized Borda score
framework which unites the Borda and Condorcet problems, thus allowing
reduction of Condorcet regret to a Borda-like task. Such a generalization was
not previously known and is likely to be of independent interest.
\\ ( https://arxiv.org/abs/2403.12950 ,  144kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.12055 (*cross-listing*)
Date: Mon, 8 Jan 2024 11:25:42 GMT   (534kb)

Title: Deep learning based detection of collateral circulation in coronary
  angiographies
Authors: Cosmin-Andrei Hatfaludi, Daniel Bunescu, Costin Florian Ciusdel, Alex
  Serban, Karl Bose, Marc Oppel, Stephanie Schroder, Christopher Seehase,
  Harald F. Langer, Jeanette Erdmann, Henry Nording, Lucian Mihai Itu
Categories: cs.CV cs.AI
DOI: 10.1109/CBMS58004.2023.00337
\\
  Coronary artery disease (CAD) is the dominant cause of death and
hospitalization across the globe. Atherosclerosis, an inflammatory condition
that gradually narrows arteries and has potentially fatal effects, is the most
frequent cause of CAD. Nonetheless, the circulation regularly adapts in the
presence of atherosclerosis, through the formation of collateral arteries,
resulting in significant long-term health benefits. Therefore, timely detection
of coronary collateral circulation (CCC) is crucial for CAD personalized
medicine. We propose a novel deep learning based method to detect CCC in
angiographic images. Our method relies on a convolutional backbone to extract
spatial features from each frame of an angiography sequence. The features are
then concatenated, and subsequently processed by another convolutional layer
that processes embeddings temporally. Due to scarcity of data, we also
experiment with pretraining the backbone on coronary artery segmentation, which
improves the results consistently. Moreover, we experiment with few-shot
learning to further improve performance, given our low data regime. We present
our results together with subgroup analyses based on Rentrop grading,
collateral flow, and collateral grading, which provide valuable insights into
model performance. Overall, the proposed method shows promising results in
detecting CCC, and can be further extended to perform landmark based CCC
detection and CCC quantification.
\\ ( https://arxiv.org/abs/2403.12055 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12058 (*cross-listing*)
Date: Tue, 16 Jan 2024 11:39:42 GMT   (1738kb)

Title: Water-Based Metaheuristics: How Water Dynamics Can Help Us to Solve
  NP-Hard Problems
Authors: Fernando Rubio and Ismael Rodr\'iguez
Categories: cs.NE cs.AI
Comments: 14 pages, 0 figures, published in journal Complexity, 2019
MSC-class: 68T20
ACM-class: I.2
DOI: 10.1155/2019/4034258
\\
  Many water-based optimization metaheuristics have been introduced during the
last decade, both for combinatorial and for continuous optimization. Despite
the strong similarities of these methods in terms of their underlying natural
metaphors (most of them emulate, in some way or another, how drops
collaboratively form paths down to the sea), in general the resulting
algorithms are quite different in terms of their searching approach or their
solution construction approach. For instance, each entity may represent a
solution by itself or, alternatively, entities may construct solutions by
modifying the landscape while moving. A researcher or practitioner could assume
that the degree of similarity between two water-based metaheuristics heavily
depends on the similarity of the natural water mechanics they emulate, but this
is not the case. In order to bring some clarity to this mosaic of apparently
related metaheuristics, in this paper we introduce them, explain their
mechanics, and highlight their differences.
\\ ( https://arxiv.org/abs/2403.12058 ,  1738kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12061 (*cross-listing*)
Date: Wed, 7 Feb 2024 20:41:00 GMT   (42kb)

Title: Design-Space Exploration of SNN Models using Application-Specific
  Multi-Core Architectures
Authors: Sanaullah and Shamini Koravuna and Ulrich R\"uckert and Thorsten
  Jungeblut
Categories: cs.NE cs.AI
Comments: Abstract Presentation in 2023 Neuro-Inspired Computing Elements
  (NICE) Conference
DOI: 10.13140/RG.2.2.26328.88324
\\
  With the motivation and the difficulties that currently exist in
comprehending and utilizing the promising features of SNNs, we proposed a novel
run-time multi-core architecture-based simulator called "RAVSim" (Runtime
Analysis and Visualization Simulator), a cutting-edge SNN simulator, developed
using LabVIEW and it is publicly available on their website as an official
module. RAVSim is a runtime virtual simulation environment tool that enables
the user to interact with the model, observe its behavior of output
concentration, and modify the set of parametric values at any time while the
simulation is in execution. Recently some popular tools have been presented,
but we believe that none of the tools allow users to interact with the model
simulation in run time.
\\ ( https://arxiv.org/abs/2403.12061 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12069 (*cross-listing*)
Date: Mon, 12 Feb 2024 06:13:24 GMT   (94kb,D)

Title: Fairness Evaluation for Uplift Modeling in the Absence of Ground Truth
Authors: Serdar Kadioglu and Filip Michalsky
Categories: cs.CY cs.AI cs.LG
Comments: IEEE International Conference on Machine Learning and Applications
  (IEEE ICMLA)
\\
  The acceleration in the adoption of AI-based automated decision-making
systems poses a challenge for evaluating the fairness of algorithmic decisions,
especially in the absence of ground truth. When designing interventions, uplift
modeling is used extensively to identify candidates that are likely to benefit
from treatment. However, these models remain particularly susceptible to
fairness evaluation due to the lack of ground truth on the outcome measure
since a candidate cannot be in both treatment and control simultaneously. In
this article, we propose a framework that overcomes the missing ground truth
problem by generating surrogates to serve as a proxy for counterfactual labels
of uplift modeling campaigns. We then leverage the surrogate ground truth to
conduct a more comprehensive binary fairness evaluation. We show how to apply
the approach in a comprehensive study from a real-world marketing campaign for
promotional offers and demonstrate its enhancement for fairness evaluation.
\\ ( https://arxiv.org/abs/2403.12069 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12071 (*cross-listing*)
Date: Mon, 12 Feb 2024 17:30:05 GMT   (87kb)

Title: Tailoring Education with GenAI: A New Horizon in Lesson Planning
Authors: Kostas Karpouzis, Dimitris Pantazatos, Joanna Taouki, Kalliopi Meli
Categories: cs.CY cs.AI
Comments: Abstract accepted for EDUCON 2024 (IEEE Global Engineering Education
  Conference 2024)
\\
  The advent of Generative AI (GenAI) in education presents a transformative
approach to traditional teaching methodologies, which often overlook the
diverse needs of individual students. This study introduces a GenAI tool, based
on advanced natural language processing, designed as a digital assistant for
educators, enabling the creation of customized lesson plans. The tool utilizes
an innovative feature termed 'interactive mega-prompt,' a comprehensive query
system that allows educators to input detailed classroom specifics such as
student demographics, learning objectives, and preferred teaching styles. This
input is then processed by the GenAI to generate tailored lesson plans. To
evaluate the tool's effectiveness, a comprehensive methodology incorporating
both quantitative (i.e., % of time savings) and qualitative (i.e., user
satisfaction) criteria was implemented, spanning various subjects and
educational levels, with continuous feedback collected from educators through a
structured evaluation form. Preliminary results show that educators find the
GenAI-generated lesson plans effective, significantly reducing lesson planning
time and enhancing the learning experience by accommodating diverse student
needs. This AI-driven approach signifies a paradigm shift in education,
suggesting its potential applicability in broader educational contexts,
including special education needs (SEN), where individualized attention and
specific learning aids are paramount
\\ ( https://arxiv.org/abs/2403.12071 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12073 (*cross-listing*)
Date: Wed, 14 Feb 2024 16:23:48 GMT   (611kb)

Title: Feasibility of Social-Network-Based eHealth Intervention on the
  Improvement of Healthy Habits among Children
Authors: Jos\'e Alberto Ben\'itez-Andrades, Natalia Arias, Mar\'ia Teresa
  Garc\'ia-Ord\'as, Marta Mart\'inez-Mart\'inez and Isa\'ias
  Garc\'ia-Rodr\'iguez
Categories: cs.CY cs.AI
Journal-ref: Sensors 2020, 20(5), 1404
DOI: 10.3390/s20051404
\\
  This study shows the feasibility of an eHealth solution for tackling eating
habits and physical activity in the adolescent population. The participants
were children from 11 to 15 years old. An intervention was carried out on 139
students in the intervention group and 91 students in the control group, in two
schools during 14 weeks. The intervention group had access to the web through a
user account and a password. They were able to create friendship relationships,
post comments, give likes and interact with other users, as well as receive
notifications and information about nutrition and physical activity on a daily
basis and get (virtual) rewards for improving their habits. The control group
did not have access to any of these features. The homogeneity of the samples in
terms of gender, age, body mass index and initial health-related habits was
demonstrated. Pre- and post-measurements were collected through self-reports on
the application website. After applying multivariate analysis of variance, a
significant alteration in the age-adjusted body mass index percentile was
observed in the intervention group versus the control group, as well as in the
PAQ-A score and the KIDMED score. It can be concluded that eHealth
interventions can help to obtain healthy habits. More research is needed to
examine the effectiveness in achieving adherence to these new habits.
\\ ( https://arxiv.org/abs/2403.12073 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12075 (*cross-listing*)
Date: Wed, 14 Feb 2024 22:21:12 GMT   (11657kb,D)

Title: Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse
  Harms in Text-to-Image Generation
Authors: Jessica Quaye, Alicia Parrish, Oana Inel, Charvi Rastogi, Hannah Rose
  Kirk, Minsuk Kahng, Erin van Liemt, Max Bartolo, Jess Tsang, Justin White,
  Nathan Clement, Rafael Mosquera, Juan Ciro, Vijay Janapa Reddi, Lora Aroyo
Categories: cs.CY cs.AI cs.CR cs.CV cs.LG
Comments: 14 pages, 6 figures
\\
  With the rise of text-to-image (T2I) generative AI models reaching wide
audiences, it is critical to evaluate model robustness against non-obvious
attacks to mitigate the generation of offensive images. By focusing on
``implicitly adversarial'' prompts (those that trigger T2I models to generate
unsafe images for non-obvious reasons), we isolate a set of difficult safety
issues that human creativity is well-suited to uncover. To this end, we built
the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing
a diverse set of implicitly adversarial prompts. We have assembled a suite of
state-of-the-art T2I models, employed a simple user interface to identify and
annotate harms, and engaged diverse populations to capture long-tail safety
issues that may be overlooked in standard testing. The challenge is run in
consecutive rounds to enable a sustained discovery and analysis of safety
pitfalls in T2I models.
  In this paper, we present an in-depth account of our methodology, a
systematic study of novel attack strategies and discussion of safety failures
revealed by challenge participants. We also release a companion visualization
tool for easy exploration and derivation of insights from the dataset. The
first challenge round resulted in over 10k prompt-image pairs with machine
annotations for safety. A subset of 1.5k samples contains rich human
annotations of harm types and attack styles. We find that 14% of images that
humans consider harmful are mislabeled as ``safe'' by machines. We have
identified new attack strategies that highlight the complexity of ensuring T2I
model robustness. Our findings emphasize the necessity of continual auditing
and adaptation as new vulnerabilities emerge. We are confident that this work
will enable proactive, iterative safety assessments and promote responsible
development of T2I models.
\\ ( https://arxiv.org/abs/2403.12075 ,  11657kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12076 (*cross-listing*)
Date: Fri, 16 Feb 2024 17:38:28 GMT   (2906kb,D)

Title: Neuron-centric Hebbian Learning
Authors: Andrea Ferigo, Elia Cunegatti, Giovanni Iacca
Categories: cs.NE cs.AI cs.LG
\\
  One of the most striking capabilities behind the learning mechanisms of the
brain is the adaptation, through structural and functional plasticity, of its
synapses. While synapses have the fundamental role of transmitting information
across the brain, several studies show that it is the neuron activations that
produce changes on synapses. Yet, most plasticity models devised for artificial
Neural Networks (NNs), e.g., the ABCD rule, focus on synapses, rather than
neurons, therefore optimizing synaptic-specific Hebbian parameters. This
approach, however, increases the complexity of the optimization process since
each synapse is associated to multiple Hebbian parameters. To overcome this
limitation, we propose a novel plasticity model, called Neuron-centric Hebbian
Learning (NcHL), where optimization focuses on neuron- rather than
synaptic-specific Hebbian parameters. Compared to the ABCD rule, NcHL reduces
the parameters from $5W$ to $5N$, being $W$ and $N$ the number of weights and
neurons, and usually $N \ll W$. We also devise a "weightless" NcHL model, which
requires less memory by approximating the weights based on a record of neuron
activations. Our experiments on two robotic locomotion tasks reveal that NcHL
performs comparably to the ABCD rule, despite using up to $\sim97$ times less
parameters, thus allowing for scalable plasticity.
\\ ( https://arxiv.org/abs/2403.12076 ,  2906kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12090 (*cross-listing*)
Date: Wed, 13 Mar 2024 20:28:08 GMT   (4839kb)

Title: Foundation Models and Information Retrieval in Digital Pathology
Authors: H.R. Tizhoosh
Categories: cs.IR cs.AI cs.CV cs.LG eess.IV
Comments: This is the preprint of a book chapter to appear in "Artificial
  Intelligence in Pathology" by Stanley Cohen and Chhavi Chauhan
\\
  The paper reviews the state-of-the-art of foundation models, LLMs, generative
AI, information retrieval and CBIR in digital pathology
\\ ( https://arxiv.org/abs/2403.12090 ,  4839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12092 (*cross-listing*)
Date: Thu, 14 Mar 2024 10:39:14 GMT   (1098kb,D)

Title: Methods for Matching English Language Addresses
Authors: Keshav Ramani, Daniel Borrajo
Categories: cs.IR cs.AI cs.CL
\\
  Addresses occupy a niche location within the landscape of textual data, due
to the positional importance carried by every word, and the geographical scope
it refers to. The task of matching addresses happens everyday and is present in
various fields like mail redirection, entity resolution, etc. Our work defines,
and formalizes a framework to generate matching and mismatching pairs of
addresses in the English language, and use it to evaluate various methods to
automatically perform address matching. These methods vary widely from distance
based approaches to deep learning models. By studying the Precision, Recall and
Accuracy metrics of these approaches, we obtain an understanding of the best
suited method for this setting of the address matching task.
\\ ( https://arxiv.org/abs/2403.12092 ,  1098kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12093 (*cross-listing*)
Date: Thu, 14 Mar 2024 13:22:31 GMT   (4288kb,D)

Title: Learning Macroeconomic Policies based on Microfoundations: A Stackelberg
  Mean Field Game Approach
Authors: Qirui Mi, Zhiyu Zhao, Siyu Xia, Yan Song, Jun Wang, Haifeng Zhang
Categories: econ.TH cs.AI
Comments: 15 pages, 7 figures, 3 tables
\\
  Effective macroeconomic policies play a crucial role in promoting economic
growth and social stability. This paper models the optimal macroeconomic policy
problem based on the \textit{Stackelberg Mean Field Game} (SMFG), where the
government acts as the leader in policy-making, and large-scale households
dynamically respond as followers. This modeling method captures the asymmetric
dynamic game between the government and large-scale households, and
interpretably evaluates the effects of macroeconomic policies based on
microfoundations, which is difficult for existing methods to achieve. We also
propose a solution for SMFGs, incorporating pre-training on real data and a
model-free \textit{Stackelberg mean-field reinforcement learning }(SMFRL)
algorithm, which operates independently of prior environmental knowledge and
transitions. Our experimental results showcase the superiority of the SMFG
method over other economic policies in terms of performance, efficiency-equity
tradeoff, and SMFG assumption analysis. This paper significantly contributes to
the domain of AI for economics by providing a powerful tool for modeling and
solving optimal macroeconomic policies.
\\ ( https://arxiv.org/abs/2403.12093 ,  4288kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12096 (*cross-listing*)
Date: Fri, 15 Mar 2024 10:28:03 GMT   (2920kb,D)

Title: Enriching User Shopping History: Empowering E-commerce with a
  Hierarchical Recommendation System
Authors: Irem Islek, Sule Gunduz Oguducu
Categories: cs.IR cs.AI
\\
  Recommendation systems can provide accurate recommendations by analyzing user
shopping history. A richer user history results in more accurate
recommendations. However, in real applications, users prefer e-commerce
platforms where the item they seek is at the lowest price. In other words, most
users shop from multiple e-commerce platforms simultaneously; different parts
of the user's shopping history are shared between different e-commerce
platforms. Consequently, we assume in this study that any e-commerce platform
has a complete record of the user's history but can only access some parts of
it. If a recommendation system is able to predict the missing parts first and
enrich the user's shopping history properly, it will be possible to recommend
the next item more accurately. Our recommendation system leverages user
shopping history to improve prediction accuracy. The proposed approach shows
significant improvements in both NDCG@10 and HR@10.
\\ ( https://arxiv.org/abs/2403.12096 ,  2920kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12098 (*cross-listing*)
Date: Sat, 16 Mar 2024 01:32:00 GMT   (14095kb,D)

Title: Deep Generative Design for Mass Production
Authors: Jihoon Kim, Yongmin Kwon, Namwoo Kang
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  Generative Design (GD) has evolved as a transformative design approach,
employing advanced algorithms and AI to create diverse and innovative solutions
beyond traditional constraints. Despite its success, GD faces significant
challenges regarding the manufacturability of complex designs, often
necessitating extensive manual modifications due to limitations in standard
manufacturing processes and the reliance on additive manufacturing, which is
not ideal for mass production. Our research introduces an innovative framework
addressing these manufacturability concerns by integrating constraints
pertinent to die casting and injection molding into GD, through the utilization
of 2D depth images. This method simplifies intricate 3D geometries into
manufacturable profiles, removing unfeasible features such as
non-manufacturable overhangs and allowing for the direct consideration of
essential manufacturing aspects like thickness and rib design. Consequently,
designs previously unsuitable for mass production are transformed into viable
solutions. We further enhance this approach by adopting an advanced 2D
generative model, which offer a more efficient alternative to traditional 3D
shape generation methods. Our results substantiate the efficacy of this
framework, demonstrating the production of innovative, and, importantly,
manufacturable designs. This shift towards integrating practical manufacturing
considerations into GD represents a pivotal advancement, transitioning from
purely inspirational concepts to actionable, production-ready solutions. Our
findings underscore usefulness and potential of GD for broader industry
adoption, marking a significant step forward in aligning GD with the demands of
manufacturing challenges.
\\ ( https://arxiv.org/abs/2403.12098 ,  14095kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12100 (*cross-listing*)
Date: Sun, 17 Mar 2024 08:43:12 GMT   (1415kb,D)

Title: Learning Time Slot Preferences via Mobility Tree for Next POI
  Recommendation
Authors: Tianhao Huang, Xuan Pan, Xiangrui Cai, Ying Zhang, Xiaojie Yuan
Categories: cs.IR cs.AI cs.LG
\\
  Next Point-of-Interests (POIs) recommendation task aims to provide a dynamic
ranking of POIs based on users' current check-in trajectories. The
recommendation performance of this task is contingent upon a comprehensive
understanding of users' personalized behavioral patterns through Location-based
Social Networks (LBSNs) data. While prior studies have adeptly captured
sequential patterns and transitional relationships within users' check-in
trajectories, a noticeable gap persists in devising a mechanism for discerning
specialized behavioral patterns during distinct time slots, such as noon,
afternoon, or evening. In this paper, we introduce an innovative data structure
termed the ``Mobility Tree'', tailored for hierarchically describing users'
check-in records. The Mobility Tree encompasses multi-granularity time slot
nodes to learn user preferences across varying temporal periods. Meanwhile, we
propose the Mobility Tree Network (MTNet), a multitask framework for
personalized preference learning based on Mobility Trees. We develop a
four-step node interaction operation to propagate feature information from the
leaf nodes to the root node. Additionally, we adopt a multitask training
strategy to push the model towards learning a robust representation. The
comprehensive experimental results demonstrate the superiority of MTNet over
ten state-of-the-art next POI recommendation models across three real-world
LBSN datasets, substantiating the efficacy of time slot preference learning
facilitated by Mobility Tree.
\\ ( https://arxiv.org/abs/2403.12100 ,  1415kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12107 (*cross-listing*)
Date: Sun, 17 Mar 2024 22:22:28 GMT   (2259kb,D)

Title: Scenarios for the Transition to AGI
Authors: Anton Korinek and Donghyun Suh
Categories: econ.GN cs.AI q-fin.EC
\\
  We analyze how output and wages behave under different scenarios for
technological progress that may culminate in Artificial General Intelligence
(AGI), defined as the ability of AI systems to perform all tasks that humans
can perform. We assume that human work can be decomposed into atomistic tasks
that differ in their complexity. Advances in technology make ever more complex
tasks amenable to automation. The effects on wages depend on a race between
automation and capital accumulation. If the distribution of task complexity
exhibits a sufficiently thick infinite tail, then there is always enough work
for humans, and wages may rise forever. By contrast, if the complexity of tasks
that humans can perform is bounded and full automation is reached, then wages
collapse. But declines may occur even before if large-scale automation outpaces
capital accumulation and makes labor too abundant. Automating productivity
growth may lead to broad-based gains in the returns to all factors. By
contrast, bottlenecks to growth from irreproducible scarce factors may
exacerbate the decline in wages.
\\ ( https://arxiv.org/abs/2403.12107 ,  2259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12114 (*cross-listing*)
Date: Mon, 18 Mar 2024 11:12:19 GMT   (1282kb,D)

Title: Safety Analysis of Autonomous Railway Systems: An Introduction to the
  SACRED Methodology
Authors: Josh Hunter, John McDermid, Simon Burton
Categories: cs.SE cs.AI
Comments: S. Bernardi, T. Zoppi (Editors), "Fast Abstracts and Student Forum
  Proceedings - EDCC 2024 - 19th European Dependable Computing Conference,
  Leuven, Belgium, 8-11 April 2024"
\\
  As the railway industry increasingly seeks to introduce autonomy and machine
learning (ML), several questions arise. How can safety be assured for such
systems and technologies? What is the applicability of current safety standards
within this new technological landscape? What are the key metrics to classify a
system as safe? Currently, safety analysis for the railway reflects the failure
modes of existing technology; in contrast, the primary concern of analysis of
automation is typically average performance. Such purely statistical approaches
to measuring ML performance are limited, as they may overlook classes of
situations that may occur rarely but in which the function performs
consistently poorly. To combat these difficulties we introduce SACRED, a safety
methodology for producing an initial safety case and determining important
safety metrics for autonomous systems. The development of SACRED is motivated
by the proposed GoA-4 light-rail system in Berlin.
\\ ( https://arxiv.org/abs/2403.12114 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12172 (*cross-listing*)
Date: Mon, 18 Mar 2024 18:42:32 GMT   (1619kb,D)

Title: Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video
  Anomaly Detection
Authors: Ali Karami, Thi Kieu Khanh Ho, Narges Armanfard
Categories: cs.CV cs.AI
Comments: 18 pages, 2 figures, 6 tables
\\
  Skeleton-based video anomaly detection (SVAD) is a crucial task in computer
vision. Accurately identifying abnormal patterns or events enables operators to
promptly detect suspicious activities, thereby enhancing safety. Achieving this
demands a comprehensive understanding of human motions, both at body and region
levels, while also accounting for the wide variations of performing a single
action. However, existing studies fail to simultaneously address these crucial
properties. This paper introduces a novel, practical and lightweight framework,
namely Graph-Jigsaw Conditioned Diffusion Model for Skeleton-based Video
Anomaly Detection (GiCiSAD) to overcome the challenges associated with SVAD.
GiCiSAD consists of three novel modules: the Graph Attention-based Forecasting
module to capture the spatio-temporal dependencies inherent in the data, the
Graph-level Jigsaw Puzzle Maker module to distinguish subtle region-level
discrepancies between normal and abnormal motions, and the Graph-based
Conditional Diffusion model to generate a wide spectrum of human motions.
Extensive experiments on four widely used skeleton-based video datasets show
that GiCiSAD outperforms existing methods with significantly fewer training
parameters, establishing it as the new state-of-the-art.
\\ ( https://arxiv.org/abs/2403.12172 ,  1619kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12176 (*cross-listing*)
Date: Mon, 18 Mar 2024 18:49:20 GMT   (10122kb,D)

Title: Safety Implications of Explainable Artificial Intelligence in End-to-End
  Autonomous Driving
Authors: Shahin Atakishiyev, Mohammad Salameh, Randy Goebel
Categories: cs.RO cs.AI
Comments: 18 pages
\\
  The end-to-end learning pipeline is gradually creating a paradigm shift in
the ongoing development of highly autonomous vehicles, largely due to advances
in deep learning, the availability of large-scale training datasets, and
improvements in integrated sensor devices. However, a lack of interpretability
in real-time decisions with contemporary learning methods impedes user trust
and attenuates the widespread deployment and commercialization of such
vehicles. Moreover, the issue is exacerbated when these cars are involved in or
cause traffic accidents. Such drawback raises serious safety concerns from
societal and legal perspectives. Consequently, explainability in end-to-end
autonomous driving is essential to enable the safety of vehicular automation.
However, the safety and explainability aspects of autonomous driving have
generally been investigated disjointly by researchers in today's state of the
art. In this paper, we aim to bridge the gaps between these topics and seek to
answer the following research question: When and how can explanations improve
safety of autonomous driving? In this regard, we first revisit established
safety and state-of-the-art explainability techniques in autonomous driving.
Furthermore, we present three critical case studies and show the pivotal role
of explanations in enhancing self-driving safety. Finally, we describe our
empirical investigation and reveal potential value, limitations, and caveats
with practical explainable AI methods on their role of assuring safety and
transparency for vehicle autonomy.
\\ ( https://arxiv.org/abs/2403.12176 ,  10122kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12181 (*cross-listing*)
Date: Mon, 18 Mar 2024 18:52:04 GMT   (1305kb,D)

Title: MAC Advice for Facility Location Mechanism Design
Authors: Zohar Barak, Anupam Gupta, Inbal Talgam-Cohen
Categories: cs.GT cs.AI
\\
  Algorithms with predictions have attracted much attention in the last years
across various domains, including variants of facility location, as a way to
surpass traditional worst-case analyses. We study the $k$-facility location
mechanism design problem, where the $n$ agents are strategic and might
misreport their location.
  Unlike previous models, where predictions are for the $k$ optimal facility
locations, we receive $n$ predictions for the locations of each of the agents.
However, these predictions are only "mostly" and "approximately" correct (or
MAC for short) -- i.e., some $\delta$-fraction of the predicted locations are
allowed to be arbitrarily incorrect, and the remainder of the predictions are
allowed to be correct up to an $\varepsilon$-error. We make no assumption on
the independence of the errors. Can such predictions allow us to beat the
current best bounds for strategyproof facility location?
  We show that the $1$-median (geometric median) of a set of points is
naturally robust under corruptions, which leads to an algorithm for
single-facility location with MAC predictions. We extend the robustness result
to a "balanced" variant of the $k$ facilities case. Without balancedness, we
show that robustness completely breaks down, even for the setting of $k=2$
facilities on a line. For this "unbalanced" setting, we devise a truthful
random mechanism that outperforms the best known result of Lu et al. [2010],
which does not use predictions. En route, we introduce the problem of "second"
facility location (when the first facility's location is already fixed). Our
findings on the robustness of the $1$-median and more generally $k$-medians may
be of independent interest, as quantitative versions of classic breakdown-point
results in robust statistics.
\\ ( https://arxiv.org/abs/2403.12181 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12196 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:10:12 GMT   (175kb,D)

Title: Shifting the Lens: Detecting Malware in npm Ecosystem with Large
  Language Models
Authors: Nusrat Zahan, Philipp Burckhardt, Mikola Lysenko, Feross Aboukhadijeh,
  Laurie Williams
Categories: cs.CR cs.AI
Comments: 13 pages, 1 Figure, 7 tables
\\
  The Gartner 2022 report predicts that 45% of organizations worldwide will
encounter software supply chain attacks by 2025, highlighting the urgency to
improve software supply chain security for community and national interests.
Current malware detection techniques aid in the manual review process by
filtering benign and malware packages, yet such techniques have high
false-positive rates and limited automation support. Therefore, malware
detection techniques could benefit from advanced, more automated approaches for
accurate and minimally false-positive results. The goal of this study is to
assist security analysts in identifying malicious packages through the
empirical study of large language models (LLMs) to detect potential malware in
the npm ecosystem.
  We present SocketAI Scanner, a multi-stage decision-maker malware detection
workflow using iterative self-refinement and zero-shot-role-play-Chain of
Thought (CoT) prompting techniques for ChatGPT. We studied 5,115 npm packages
(of which 2,180 are malicious) and performed a baseline comparison of the GPT-3
and GPT-4 models with a static analysis tool. Our findings showed promising
results for GPT models with low misclassification alert rates. Our baseline
comparison demonstrates a notable improvement over static analysis in precision
scores above 25% and F1 scores above 15%. We attained precision and F1 scores
of 91% and 94%, respectively, for the GPT-3 model. Overall, GPT-4 demonstrates
superior performance in precision (99%) and F1 (97%) scores, while GPT-3
presents a cost-effective balance between performance and expenditure.
\\ ( https://arxiv.org/abs/2403.12196 ,  175kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12197 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:11:34 GMT   (15244kb)

Title: E2F-Net: Eyes-to-Face Inpainting via StyleGAN Latent Space
Authors: Ahmad Hassanpour, Fatemeh Jamalbafrani, Bian Yang, Kiran Raja, Raymond
  Veldhuis, Julian Fierrez
Categories: cs.CV cs.AI
\\
  Face inpainting, the technique of restoring missing or damaged regions in
facial images, is pivotal for applications like face recognition in occluded
scenarios and image analysis with poor-quality captures. This process not only
needs to produce realistic visuals but also preserve individual identity
characteristics. The aim of this paper is to inpaint a face given periocular
region (eyes-to-face) through a proposed new Generative Adversarial Network
(GAN)-based model called Eyes-to-Face Network (E2F-Net). The proposed approach
extracts identity and non-identity features from the periocular region using
two dedicated encoders have been used. The extracted features are then mapped
to the latent space of a pre-trained StyleGAN generator to benefit from its
state-of-the-art performance and its rich, diverse and expressive latent space
without any additional training. We further improve the StyleGAN output to find
the optimal code in the latent space using a new optimization for GAN inversion
technique. Our E2F-Net requires a minimum training process reducing the
computational complexity as a secondary benefit. Through extensive experiments,
we show that our method successfully reconstructs the whole face with high
quality, surpassing current techniques, despite significantly less training and
supervision efforts. We have generated seven eyes-to-face datasets based on
well-known public face datasets for training and verifying our proposed
methods. The code and datasets are publicly available.
\\ ( https://arxiv.org/abs/2403.12197 ,  15244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12207 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:44:30 GMT   (42932kb,D)

Title: Synthetic Image Generation in Cyber Influence Operations: An Emergent
  Threat?
Authors: Melanie Mathys, Marco Willi, Michael Graber, Raphael Meier
Categories: cs.CY cs.AI cs.CV
Comments: 44 pages, 56 figures
ACM-class: K.4.0; I.2.0; I.4.0
\\
  The evolution of artificial intelligence (AI) has catalyzed a transformation
in digital content generation, with profound implications for cyber influence
operations. This report delves into the potential and limitations of generative
deep learning models, such as diffusion models, in fabricating convincing
synthetic images. We critically assess the accessibility, practicality, and
output quality of these tools and their implications in threat scenarios of
deception, influence, and subversion. Notably, the report generates content for
several hypothetical cyber influence operations to demonstrate the current
capabilities and limitations of these AI-driven methods for threat actors.
While generative models excel at producing illustrations and non-realistic
imagery, creating convincing photo-realistic content remains a significant
challenge, limited by computational resources and the necessity for
human-guided refinement. Our exploration underscores the delicate balance
between technological advancement and its potential for misuse, prompting
recommendations for ongoing research, defense mechanisms, multi-disciplinary
collaboration, and policy development. These recommendations aim to leverage
AI's potential for positive impact while safeguarding against its risks to the
integrity of information, especially in the context of cyber influence.
\\ ( https://arxiv.org/abs/2403.12207 ,  42932kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12211 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:51:55 GMT   (495kb,D)

Title: A Unified Model for Longitudinal Multi-Modal Multi-View Prediction with
  Missingness
Authors: Boqi Chen, Junier Oliva, Marc Niethammer
Categories: cs.CV cs.AI
\\
  Medical records often consist of different modalities, such as images, text,
and tabular information. Integrating all modalities offers a holistic view of a
patient's condition, while analyzing them longitudinally provides a better
understanding of disease progression. However, real-world longitudinal medical
records present challenges: 1) patients may lack some or all of the data for a
specific timepoint, and 2) certain modalities or views might be absent for all
patients during a particular period. In this work, we introduce a unified model
for longitudinal multi-modal multi-view (MMMV) prediction with missingness. Our
method allows as many timepoints as desired for input, and aims to leverage all
available data, regardless of their availability. We conduct extensive
experiments on the knee osteoarthritis dataset from the Osteoarthritis
Initiative (OAI) for pain and Kellgren-Lawrence grade (KLG) prediction at a
future timepoint. We demonstrate the effectiveness of our method by comparing
results from our unified model to specific models that use the same modality
and view combinations during training and evaluation. We also show the benefit
of having extended temporal data and provide post-hoc analysis for a deeper
understanding of each modality/view's importance for different tasks.
\\ ( https://arxiv.org/abs/2403.12211 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12388 (*cross-listing*)
Date: Tue, 19 Mar 2024 02:57:07 GMT   (2798kb,D)

Title: Interpretable User Satisfaction Estimation for Conversational Systems
  with Large Language Models
Authors: Ying-Chun Lin, Jennifer Neville, Jack W. Stokes, Longqi Yang, Tara
  Safavi, Mengting Wan, Scott Counts, Siddharth Suri, Reid Andersen, Xiaofeng
  Xu, Deepak Gupta, Sujay Kumar Jauhar, Xia Song, Georg Buscher, Saurabh
  Tiwary, Brent Hecht, Jaime Teevan
Categories: cs.IR cs.AI
\\
  Accurate and interpretable user satisfaction estimation (USE) is critical for
understanding, evaluating, and continuously improving conversational systems.
Users express their satisfaction or dissatisfaction with diverse conversational
patterns in both general-purpose (ChatGPT and Bing Copilot) and task-oriented
(customer service chatbot) conversational systems. Existing approaches based on
featurized ML models or text embeddings fall short in extracting generalizable
patterns and are hard to interpret. In this work, we show that LLMs can extract
interpretable signals of user satisfaction from their natural language
utterances more effectively than embedding-based approaches. Moreover, an LLM
can be tailored for USE via an iterative prompting framework using supervision
from labeled examples. The resulting method, Supervised Prompting for User
satisfaction Rubrics (SPUR), not only has higher accuracy but is more
interpretable as it scores user satisfaction via learned rubrics with a
detailed breakdown.
\\ ( https://arxiv.org/abs/2403.12388 ,  2798kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12431 (*cross-listing*)
Date: Tue, 19 Mar 2024 04:41:09 GMT   (3157kb,D)

Title: Geometric Constraints in Deep Learning Frameworks: A Survey
Authors: Vibhas K Vats, David J Crandall
Categories: cs.CV cs.AI
Comments: A preprint
\\
  Stereophotogrammetry is an emerging technique of scene understanding. Its
origins go back to at least the 1800s when people first started to investigate
using photographs to measure the physical properties of the world. Since then,
thousands of approaches have been explored. The classic geometric techniques of
Shape from Stereo is built on using geometry to define constraints on scene and
camera geometry and then solving the non-linear systems of equations. More
recent work has taken an entirely different approach, using end-to-end deep
learning without any attempt to explicitly model the geometry. In this survey,
we explore the overlap for geometric-based and deep learning-based frameworks.
We compare and contrast geometry enforcing constraints integrated into a deep
learning framework for depth estimation or other closely related problems. We
present a new taxonomy for prevalent geometry enforcing constraints used in
modern deep learning frameworks. We also present insightful observations and
potential future research directions.
\\ ( https://arxiv.org/abs/2403.12431 ,  3157kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12462 (*cross-listing*)
Date: Tue, 19 Mar 2024 05:37:26 GMT   (5327kb,D)

Title: Topological Representations of Heterogeneous Learning Dynamics of
  Recurrent Spiking Neural Networks
Authors: Biswadeep Chakraborty, Saibal Mukhopadhyay
Categories: cs.NE cs.AI
Comments: Accepted in IEEE World Congress on Computational Intelligence (IEEE
  WCCI) 2024
\\
  Spiking Neural Networks (SNNs) have become an essential paradigm in
neuroscience and artificial intelligence, providing brain-inspired computation.
Recent advances in literature have studied the network representations of deep
neural networks. However, there has been little work that studies
representations learned by SNNs, especially using unsupervised local learning
methods like spike-timing dependent plasticity (STDP). Recent work by
\cite{barannikov2021representation} has introduced a novel method to compare
topological mappings of learned representations called Representation Topology
Divergence (RTD). Though useful, this method is engineered particularly for
feedforward deep neural networks and cannot be used for recurrent networks like
Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to
measure the difference between distributed representations of RSNN models with
different learning methods. We propose a novel reformulation of RSNNs using
feedforward autoencoder networks with skip connections to help us compute the
RTD for recurrent networks. Thus, we investigate the learning capabilities of
RSNN trained using STDP and the role of heterogeneity in the synaptic dynamics
in learning such representations. We demonstrate that heterogeneous STDP in
RSNNs yield distinct representations than their homogeneous and surrogate
gradient-based supervised learning counterparts. Our results provide insights
into the potential of heterogeneous SNN models, aiding the development of more
efficient and biologically plausible hybrid artificial intelligence systems.
\\ ( https://arxiv.org/abs/2403.12462 ,  5327kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12463 (*cross-listing*)
Date: Tue, 24 Oct 2023 18:26:25 GMT   (795kb)

Title: Reinforcement learning based local path planning for mobile robot
Authors: Mehmet Gok, Mehmet Tekerek, Hamza Aydemir
Categories: cs.RO cs.AI
Comments: 5 Pages, 10 figures, Presented in; Interdisciplinary Conference on
  Mechanics, Computers and Electrics ANKARA/TURKEY 27-28 November 2021
Journal-ref: Interdisciplinary Conference on Mechanics, Computers and
  Electrics, 27-28 Nov. 2021, Ankara
\\
  Different methods are used for a mobile robot to go to a specific target
location. These methods work in different ways for online and offline
scenarios. In the offline scenario, an environment map is created once, and
fixed path planning is made on this map to reach the target. Path planning
algorithms such as A* and RRT (Rapidly-Exploring Random Tree) are the examples
of offline methods. The most obvious situation here is the need to re-plan the
path for changing conditions of the loaded map. On the other hand, in the
online scenario, the robot moves dynamically to a given target without using a
map by using the perceived data coming from the sensors. Approaches such as SFM
(Social Force Model) are used in online systems. However, these methods suffer
from the requirement of a lot of dynamic sensing data. Thus, it can be said
that the need for re-planning and mapping in offline systems and various system
design requirements in online systems are the subjects that focus on autonomous
mobile robot research. Recently, deep neural network powered Q-Learning methods
are used as an emerging solution to the aforementioned problems in mobile robot
navigation. In this study, machine learning algorithms with deep Q-Learning
(DQN) and Deep DQN architectures, are evaluated for the solution of the
problems presented above to realize path planning of an autonomous mobile robot
to avoid obstacles.
\\ ( https://arxiv.org/abs/2403.12463 ,  795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12488 (*cross-listing*)
Date: Tue, 19 Mar 2024 06:54:33 GMT   (2306kb,D)

Title: DetToolChain: A New Prompting Paradigm to Unleash Detection Ability of
  MLLM
Authors: Yixuan Wu, Yizhou Wang, Shixiang Tang, Wenhao Wu, Tong He, Wanli
  Ouyang, Jian Wu, Philip Torr
Categories: cs.CV cs.AI
\\
  We present DetToolChain, a novel prompting paradigm, to unleash the zero-shot
object detection ability of multimodal large language models (MLLMs), such as
GPT-4V and Gemini. Our approach consists of a detection prompting toolkit
inspired by high-precision detection priors and a new Chain-of-Thought to
implement these prompts. Specifically, the prompts in the toolkit are designed
to guide the MLLM to focus on regional information (e.g., zooming in), read
coordinates according to measure standards (e.g., overlaying rulers and
compasses), and infer from the contextual information (e.g., overlaying scene
graphs). Building upon these tools, the new detection chain-of-thought can
automatically decompose the task into simple subtasks, diagnose the
predictions, and plan for progressive box refinements. The effectiveness of our
framework is demonstrated across a spectrum of detection tasks, especially hard
cases. Compared to existing state-of-the-art methods, GPT-4V with our
DetToolChain improves state-of-the-art object detectors by +21.5% AP50 on MS
COCO Novel class set for open-vocabulary detection, +24.23% Acc on RefCOCO val
set for zero-shot referring expression comprehension, +14.5% AP on D-cube
describe object detection FULL setting.
\\ ( https://arxiv.org/abs/2403.12488 ,  2306kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12503 (*cross-listing*)
Date: Tue, 19 Mar 2024 07:10:58 GMT   (130kb,D)

Title: Securing Large Language Models: Threats, Vulnerabilities and Responsible
  Practices
Authors: Sara Abdali, Richard Anarfi, CJ Barberan, Jia He
Categories: cs.CR cs.AI cs.LG
\\
  Large language models (LLMs) have significantly transformed the landscape of
Natural Language Processing (NLP). Their impact extends across a diverse
spectrum of tasks, revolutionizing how we approach language understanding and
generations. Nevertheless, alongside their remarkable utility, LLMs introduce
critical security and risk considerations. These challenges warrant careful
examination to ensure responsible deployment and safeguard against potential
vulnerabilities. This research paper thoroughly investigates security and
privacy concerns related to LLMs from five thematic perspectives: security and
privacy concerns, vulnerabilities against adversarial attacks, potential harms
caused by misuses of LLMs, mitigation strategies to address these challenges
while identifying limitations of current strategies. Lastly, the paper
recommends promising avenues for future research to enhance the security and
risk management of LLMs.
\\ ( https://arxiv.org/abs/2403.12503 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12510 (*cross-listing*)
Date: Tue, 19 Mar 2024 07:24:54 GMT   (39865kb,D)

Title: Generalized Consistency Trajectory Models for Image Manipulation
Authors: Beomsu Kim and Jaemin Kim and Jeongsol Kim and Jong Chul Ye
Categories: cs.CV cs.AI cs.LG
\\
  Diffusion-based generative models excel in unconditional generation, as well
as on applied tasks such as image editing and restoration. The success of
diffusion models lies in the iterative nature of diffusion: diffusion breaks
down the complex process of mapping noise to data into a sequence of simple
denoising tasks. Moreover, we are able to exert fine-grained control over the
generation process by injecting guidance terms into each denoising step.
However, the iterative process is also computationally intensive, often taking
from tens up to thousands of function evaluations. Although consistency
trajectory models (CTMs) enable traversal between any time points along the
probability flow ODE (PFODE) and score inference with a single function
evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this
work aims to unlock the full potential of CTMs by proposing generalized CTMs
(GCTMs), which translate between arbitrary distributions via ODEs. We discuss
the design space of GCTMs and demonstrate their efficacy in various image
manipulation tasks such as image-to-image translation, restoration, and
editing. Code: \url{https://github.com/1202kbs/GCTM}
\\ ( https://arxiv.org/abs/2403.12510 ,  39865kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12533 (*cross-listing*)
Date: Tue, 19 Mar 2024 08:09:44 GMT   (4166kb)

Title: To Help or Not to Help: LLM-based Attentive Support for Human-Robot
  Group Interactions
Authors: Daniel Tanneberg, Felix Ocker, Stephan Hasler, Joerg Deigmoeller, Anna
  Belardinelli, Chao Wang, Heiko Wersing, Bernhard Sendhoff, Michael Gienger
Categories: cs.RO cs.AI
Comments: 8 pages, 5 figures
ACM-class: I.2.8; I.2.9
\\
  How can a robot provide unobtrusive physical support within a group of
humans? We present Attentive Support, a novel interaction concept for robots to
support a group of humans. It combines scene perception, dialogue acquisition,
situation understanding, and behavior generation with the common-sense
reasoning capabilities of Large Language Models (LLMs). In addition to
following user instructions, Attentive Support is capable of deciding when and
how to support the humans, and when to remain silent to not disturb the group.
With a diverse set of scenarios, we show and evaluate the robot's attentive
behavior, which supports and helps the humans when required, while not
disturbing if no help is needed.
\\ ( https://arxiv.org/abs/2403.12533 ,  4166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12552 (*cross-listing*)
Date: Tue, 19 Mar 2024 08:54:52 GMT   (3347kb,D)

Title: M2DA: Multi-Modal Fusion Transformer Incorporating Driver Attention for
  Autonomous Driving
Authors: Dongyang Xu, Haokun Li, Qingfan Wang, Ziying Song, Lei Chen and
  Hanming Deng
Categories: cs.CV cs.AI cs.RO
\\
  End-to-end autonomous driving has witnessed remarkable progress. However, the
extensive deployment of autonomous vehicles has yet to be realized, primarily
due to 1) inefficient multi-modal environment perception: how to integrate data
from multi-modal sensors more efficiently; 2) non-human-like scene
understanding: how to effectively locate and predict critical risky agents in
traffic scenarios like an experienced driver. To overcome these challenges, in
this paper, we propose a Multi-Modal fusion transformer incorporating Driver
Attention (M2DA) for autonomous driving. To better fuse multi-modal data and
achieve higher alignment between different modalities, a novel
Lidar-Vision-Attention-based Fusion (LVAFusion) module is proposed. By
incorporating driver attention, we empower the human-like scene understanding
ability to autonomous vehicles to identify crucial areas within complex
scenarios precisely and ensure safety. We conduct experiments on the CARLA
simulator and achieve state-of-the-art performance with less data in
closed-loop benchmarks. Source codes are available at
https://anonymous.4open.science/r/M2DA-4772.
\\ ( https://arxiv.org/abs/2403.12552 ,  3347kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12568 (*cross-listing*)
Date: Tue, 19 Mar 2024 09:22:50 GMT   (5895kb,D)

Title: Memory-Efficient and Secure DNN Inference on TrustZone-enabled Consumer
  IoT Devices
Authors: Xueshuo Xie, Haoxu Wang, Zhaolong Jian, Tao Li, Wei Wang, Zhiwei Xu,
  Guiling Wang
Categories: cs.CR cs.AI
\\
  Edge intelligence enables resource-demanding Deep Neural Network (DNN)
inference without transferring original data, addressing concerns about data
privacy in consumer Internet of Things (IoT) devices. For privacy-sensitive
applications, deploying models in hardware-isolated trusted execution
environments (TEEs) becomes essential. However, the limited secure memory in
TEEs poses challenges for deploying DNN inference, and alternative techniques
like model partitioning and offloading introduce performance degradation and
security issues. In this paper, we present a novel approach for advanced model
deployment in TrustZone that ensures comprehensive privacy preservation during
model inference. We design a memory-efficient management method to support
memory-demanding inference in TEEs. By adjusting the memory priority, we
effectively mitigate memory leakage risks and memory overlap conflicts,
resulting in 32 lines of code alterations in the trusted operating system.
Additionally, we leverage two tiny libraries: S-Tinylib (2,538 LoCs), a tiny
deep learning library, and Tinylibm (827 LoCs), a tiny math library, to support
efficient inference in TEEs. We implemented a prototype on Raspberry Pi 3B+ and
evaluated it using three well-known lightweight DNN models. The experimental
results demonstrate that our design significantly improves inference speed by
3.13 times and reduces power consumption by over 66.5% compared to non-memory
optimization method in TEEs.
\\ ( https://arxiv.org/abs/2403.12568 ,  5895kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12572 (*cross-listing*)
Date: Tue, 19 Mar 2024 09:30:56 GMT   (311kb,D)

Title: Compound Expression Recognition via Multi Model Ensemble
Authors: Jun Yu, Jichao Zhu, Wangyuan Zhu
Categories: cs.CV cs.AI
\\
  Compound Expression Recognition (CER) plays a crucial role in interpersonal
interactions. Due to the existence of Compound Expressions , human emotional
expressions are complex, requiring consideration of both local and global
facial expressions to make judgments. In this paper, to address this issue, we
propose a solution based on ensemble learning methods for Compound Expression
Recognition. Specifically, our task is classification, where we train three
expression classification models based on convolutional networks, Vision
Transformers, and multi-scale local attention networks. Then, through model
ensemble using late fusion, we merge the outputs of multiple models to predict
the final result. Our method achieves high accuracy on RAF-DB and is able to
recognize expressions through zero-shot on certain portions of C-EXPR-DB.
\\ ( https://arxiv.org/abs/2403.12572 ,  311kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12574 (*cross-listing*)
Date: Tue, 19 Mar 2024 09:34:11 GMT   (4202kb,D)

Title: EAS-SNN: End-to-End Adaptive Sampling and Representation for Event-based
  Detection with Recurrent Spiking Neural Networks
Authors: Ziming Wang, Ziling Wang, Huaning Li, Lang Qin, Runhao Jiang, De Ma
  and Huajin Tang
Categories: cs.CV cs.AI cs.NE
\\
  Event cameras, with their high dynamic range and temporal resolution, are
ideally suited for object detection, especially under scenarios with motion
blur and challenging lighting conditions. However, while most existing
approaches prioritize optimizing spatiotemporal representations with advanced
detection backbones and early aggregation functions, the crucial issue of
adaptive event sampling remains largely unaddressed. Spiking Neural Networks
(SNNs), which operate on an event-driven paradigm through sparse spike
communication, emerge as a natural fit for addressing this challenge. In this
study, we discover that the neural dynamics of spiking neurons align closely
with the behavior of an ideal temporal event sampler. Motivated by this
insight, we propose a novel adaptive sampling module that leverages recurrent
convolutional SNNs enhanced with temporal memory, facilitating a fully
end-to-end learnable framework for event-based detection. Additionally, we
introduce Residual Potential Dropout (RPD) and Spike-Aware Training (SAT) to
regulate potential distribution and address performance degradation encountered
in spike-based sampling modules. Through rigorous testing on neuromorphic
datasets for event-based detection, our approach demonstrably surpasses
existing state-of-the-art spike-based methods, achieving superior performance
with significantly fewer parameters and time steps. For instance, our method
achieves a 4.4\% mAP improvement on the Gen1 dataset, while requiring 38\%
fewer parameters and three time steps. Moreover, the applicability and
effectiveness of our adaptive sampling methodology extend beyond SNNs, as
demonstrated through further validation on conventional non-spiking detection
models.
\\ ( https://arxiv.org/abs/2403.12574 ,  4202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12588 (*cross-listing*)
Date: Tue, 19 Mar 2024 09:47:54 GMT   (11kb)

Title: Machine Learning of the Prime Distribution
Authors: Alexander Kolpakov, Aidan Rocke
Categories: cs.IT cs.AI cs.LG math.IT math.NT
Comments: 10 pages; parts of arXiv:2308.10817 reworked and amended; author's
  draft; accepted in PLOS ONE
DOI: 10.1371/journal.pone.0301240
\\
  In the present work we use maximum entropy methods to derive several theorems
in probabilistic number theory, including a version of the Hardy-Ramanujan
Theorem. We also provide a theoretical argument explaining the experimental
observations of Y.-H. He about the learnability of primes, and posit that the
Erd\H{o}s-Kac law would very unlikely be discovered by current machine learning
techniques. Numerical experiments that we perform corroborate our theoretical
findings.
\\ ( https://arxiv.org/abs/2403.12588 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12589 (*cross-listing*)
Date: Tue, 19 Mar 2024 09:48:18 GMT   (18718kb,D)

Title: FootstepNet: an Efficient Actor-Critic Method for Fast On-line Bipedal
  Footstep Planning and Forecasting
Authors: Cl\'ement Gaspard, Gr\'egoire Passault, M\'elodie Daniel, Olivier Ly
Categories: cs.RO cs.AI
\\
  Designing a humanoid locomotion controller is challenging and classically
split up in sub-problems. Footstep planning is one of those, where the sequence
of footsteps is defined. Even in simpler environments, finding a minimal
sequence, or even a feasible sequence, yields a complex optimization problem.
In the literature, this problem is usually addressed by search-based algorithms
(e.g. variants of A*). However, such approaches are either computationally
expensive or rely on hand-crafted tuning of several parameters. In this work,
at first, we propose an efficient footstep planning method to navigate in local
environments with obstacles, based on state-of-the art Deep Reinforcement
Learning (DRL) techniques, with very low computational requirements for on-line
inference. Our approach is heuristic-free and relies on a continuous set of
actions to generate feasible footsteps. In contrast, other methods necessitate
the selection of a relevant discrete set of actions. Second, we propose a
forecasting method, allowing to quickly estimate the number of footsteps
required to reach different candidates of local targets. This approach relies
on inherent computations made by the actor-critic DRL architecture. We
demonstrate the validity of our approach with simulation results, and by a
deployment on a kid-size humanoid robot during the RoboCup 2023 competition.
\\ ( https://arxiv.org/abs/2403.12589 ,  18718kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12631 (*cross-listing*)
Date: Tue, 19 Mar 2024 10:59:21 GMT   (737kb)

Title: PointGrasp: Point Cloud-based Grasping for Tendon-driven Soft Robotic
  Glove Applications
Authors: Chen Hu, Shirui Lyu, Eojin Rho, Daekyum Kim, Shan Luo, and Letizia
  Gionfrida
Categories: cs.RO cs.AI
Comments: 6 pages, 8 figures, conference
ACM-class: I.2; I.4
\\
  Controlling hand exoskeletons to assist individuals with grasping tasks poses
a challenge due to the difficulty in understanding user intentions. We propose
that most daily grasping tasks during activities of daily living (ADL) can be
deduced by analyzing object geometries (simple and complex) from 3D point
clouds. The study introduces PointGrasp, a real-time system designed for
identifying household scenes semantically, aiming to support and enhance
assistance during ADL for tailored end-to-end grasping tasks. The system
comprises an RGB-D camera with an inertial measurement unit and a
microprocessor integrated into a tendon-driven soft robotic glove. The RGB-D
camera processes 3D scenes at a rate exceeding 30 frames per second. The
proposed pipeline demonstrates an average RMSE of 0.8 $\pm$ 0.39 cm for simple
and 0.11 $\pm$ 0.06 cm for complex geometries. Within each mode, it identifies
and pinpoints reachable objects. This system shows promise in end-to-end
vision-driven robotic-assisted rehabilitation manual tasks.
\\ ( https://arxiv.org/abs/2403.12631 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12649 (*cross-listing*)
Date: Tue, 19 Mar 2024 11:34:15 GMT   (1082kb,D)

Title: InBox: Recommendation with Knowledge Graph using Interest Box Embedding
Authors: Zezhong Xu, Yincen Qu, Wen Zhang, Lei Liang, Huajun Chen
Categories: cs.IR cs.AI
Comments: VLDB 2024 under submission
\\
  Knowledge graphs (KGs) have become vitally important in modern recommender
systems, effectively improving performance and interpretability. Fundamentally,
recommender systems aim to identify user interests based on historical
interactions and recommend suitable items. However, existing works overlook two
key challenges: (1) an interest corresponds to a potentially large set of
related items, and (2) the lack of explicit, fine-grained exploitation of KG
information and interest connectivity. This leads to an inability to reflect
distinctions between entities and interests when modeling them in a single way.
Additionally, the granularity of concepts in the knowledge graphs used for
recommendations tends to be coarse, failing to match the fine-grained nature of
user interests. This homogenization limits the precise exploitation of
knowledge graph data and interest connectivity. To address these limitations,
we introduce a novel embedding-based model called InBox. Specifically, various
knowledge graph entities and relations are embedded as points or boxes, while
user interests are modeled as boxes encompassing interaction history.
Representing interests as boxes enables containing collections of item points
related to that interest. We further propose that an interest comprises diverse
basic concepts, and box intersection naturally supports concept combination.
Across three training steps, InBox significantly outperforms state-of-the-art
methods like HAKG and KGIN on recommendation tasks. Further analysis provides
meaningful insights into the variable value of different KG data for
recommendations. In summary, InBox advances recommender systems through
box-based interest and concept modeling for sophisticated knowledge graph
exploitation.
\\ ( https://arxiv.org/abs/2403.12649 ,  1082kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12660 (*cross-listing*)
Date: Tue, 19 Mar 2024 11:49:35 GMT   (210kb,D)

Title: ERASE: Benchmarking Feature Selection Methods for Deep Recommender
  Systems
Authors: Pengyue Jia, Yejing Wang, Zhaocheng Du, Xiangyu Zhao, Yichao Wang, Bo
  Chen, Wanyu Wang, Huifeng Guo, Ruiming Tang
Categories: cs.IR cs.AI
\\
  Deep Recommender Systems (DRS) are increasingly dependent on a large number
of feature fields for more precise recommendations. Effective feature selection
methods are consequently becoming critical for further enhancing the accuracy
and optimizing storage efficiencies to align with the deployment demands. This
research area, particularly in the context of DRS, is nascent and faces three
core challenges. Firstly, variant experimental setups across research papers
often yield unfair comparisons, obscuring practical insights. Secondly, the
existing literature's lack of detailed analysis on selection attributes, based
on large-scale datasets and a thorough comparison among selection techniques
and DRS backbones, restricts the generalizability of findings and impedes
deployment on DRS. Lastly, research often focuses on comparing the peak
performance achievable by feature selection methods, an approach that is
typically computationally infeasible for identifying the optimal
hyperparameters and overlooks evaluating the robustness and stability of these
methods. To bridge these gaps, this paper presents ERASE, a comprehensive
bEnchmaRk for feAture SElection for DRS. ERASE comprises a thorough evaluation
of eleven feature selection methods, covering both traditional and deep
learning approaches, across four public datasets, private industrial datasets,
and a real-world commercial platform, achieving significant enhancement. Our
code is available online for ease of reproduction.
\\ ( https://arxiv.org/abs/2403.12660 ,  210kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12671 (*cross-listing*)
Date: Tue, 19 Mar 2024 12:13:33 GMT   (125kb,D)

Title: Enhancing Security of AI-Based Code Synthesis with GitHub Copilot via
  Cheap and Efficient Prompt-Engineering
Authors: Jakub Res, Ivan Homoliak, Martin Pere\v{s}\'ini, Ale\v{s} Smr\v{c}ka,
  Kamil Malinka, Petr Hanacek
Categories: cs.CR cs.AI
\\
  AI assistants for coding are on the rise. However one of the reasons
developers and companies avoid harnessing their full potential is the
questionable security of the generated code. This paper first reviews the
current state-of-the-art and identifies areas for improvement on this issue.
Then, we propose a systematic approach based on prompt-altering methods to
achieve better code security of (even proprietary black-box) AI-based code
generators such as GitHub Copilot, while minimizing the complexity of the
application from the user point-of-view, the computational resources, and
operational costs. In sum, we propose and evaluate three prompt altering
methods: (1) scenario-specific, (2) iterative, and (3) general clause, while we
discuss their combination. Contrary to the audit of code security, the latter
two of the proposed methods require no expert knowledge from the user. We
assess the effectiveness of the proposed methods on the GitHub Copilot using
the OpenVPN project in realistic scenarios, and we demonstrate that the
proposed methods reduce the number of insecure generated code samples by up to
16\% and increase the number of secure code by up to 8\%. Since our approach
does not require access to the internals of the AI models, it can be in general
applied to any AI-based code synthesizer, not only GitHub Copilot.
\\ ( https://arxiv.org/abs/2403.12671 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12706 (*cross-listing*)
Date: Tue, 19 Mar 2024 13:08:54 GMT   (8116kb,D)

Title: AnimateDiff-Lightning: Cross-Model Diffusion Distillation
Authors: Shanchuan Lin, Xiao Yang
Categories: cs.CV cs.AI
\\
  We present AnimateDiff-Lightning for lightning-fast video generation. Our
model uses progressive adversarial diffusion distillation to achieve new
state-of-the-art in few-step video generation. We discuss our modifications to
adapt it for the video modality. Furthermore, we propose to simultaneously
distill the probability flow of multiple base diffusion models, resulting in a
single distilled motion module with broader style compatibility. We are pleased
to release our distilled AnimateDiff-Lightning model for the community's use.
\\ ( https://arxiv.org/abs/2403.12706 ,  8116kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12723 (*cross-listing*)
Date: Tue, 19 Mar 2024 13:41:11 GMT   (34kb)

Title: Python Fuzzing for Trustworthy Machine Learning Frameworks
Authors: Ilya Yegorov, Eli Kobrin, Darya Parygina, Alexey Vishnyakov, Andrey
  Fedotov
Categories: cs.CR cs.AI cs.SE
Journal-ref: Zapiski Nauchnykh Seminarov Sankt-Peterburgskogo Otdeleniya
  Matematicheskogo Instituta im. V. A. Steklova Rossiiskoi Akademii Nauk 530
  (2023) 38-50
\\
  Ensuring the security and reliability of machine learning frameworks is
crucial for building trustworthy AI-based systems. Fuzzing, a popular technique
in secure software development lifecycle (SSDLC), can be used to develop secure
and robust software. Popular machine learning frameworks such as PyTorch and
TensorFlow are complex and written in multiple programming languages including
C/C++ and Python. We propose a dynamic analysis pipeline for Python projects
using the Sydr-Fuzz toolset. Our pipeline includes fuzzing, corpus
minimization, crash triaging, and coverage collection. Crash triaging and
severity estimation are important steps to ensure that the most critical
vulnerabilities are addressed promptly. Furthermore, the proposed pipeline is
integrated in GitLab CI. To identify the most vulnerable parts of the machine
learning frameworks, we analyze their potential attack surfaces and develop
fuzz targets for PyTorch, TensorFlow, and related projects such as h5py.
Applying our dynamic analysis pipeline to these targets, we were able to
discover 3 new bugs and propose fixes for them.
\\ ( https://arxiv.org/abs/2403.12723 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12730 (*cross-listing*)
Date: Tue, 19 Mar 2024 13:45:34 GMT   (173kb,D)

Title: What Does Evaluation of Explainable Artificial Intelligence Actually
  Tell Us? A Case for Compositional and Contextual Validation of XAI Building
  Blocks
Authors: Kacper Sokol and Julia E. Vogt
Categories: cs.HC cs.AI
Comments: Published in Extended Abstracts of the 2024 CHI Conference on Human
  Factors in Computing Systems (CHI EA '24)
DOI: 10.1145/3613905.3651047
\\
  Despite significant progress, evaluation of explainable artificial
intelligence remains elusive and challenging. In this paper we propose a
fine-grained validation framework that is not overly reliant on any one facet
of these sociotechnical systems, and that recognises their inherent modular
structure: technical building blocks, user-facing explanatory artefacts and
social communication protocols. While we concur that user studies are
invaluable in assessing the quality and effectiveness of explanation
presentation and delivery strategies from the explainees' perspective in a
particular deployment context, the underlying explanation generation mechanisms
require a separate, predominantly algorithmic validation strategy that accounts
for the technical and human-centred desiderata of their (numerical) outputs.
Such a comprehensive sociotechnical utility-based evaluation framework could
allow to systematically reason about the properties and downstream influence of
different building blocks from which explainable artificial intelligence
systems are composed -- accounting for a diverse range of their engineering and
social aspects -- in view of the anticipated use case.
\\ ( https://arxiv.org/abs/2403.12730 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12748 (*cross-listing*)
Date: Tue, 19 Mar 2024 14:11:26 GMT   (6181kb,D)

Title: Building Brain Tumor Segmentation Networks with User-Assisted Filter
  Estimation and Selection
Authors: Matheus A. Cerqueira, Fl\'avia Sprenger, Bernardo C. A. Teixeira and
  Alexandre X. Falc\~ao
Categories: cs.CV cs.AI
Comments: 10 pages, 5 figures, 2 tables, 24 references, manuscript of
  conference paper
MSC-class: 68T07, 68T45
DOI: 10.1117/12.2669770
\\
  Brain tumor image segmentation is a challenging research topic in which
deep-learning models have presented the best results. However, the traditional
way of training those models from many pre-annotated images leaves several
unanswered questions. Hence methodologies, such as Feature Learning from Image
Markers (FLIM), have involved an expert in the learning loop to reduce human
effort in data annotation and build models sufficiently deep for a given
problem. FLIM has been successfully used to create encoders, estimating the
filters of all convolutional layers from patches centered at marker voxels. In
this work, we present Multi-Step (MS) FLIM - a user-assisted approach to
estimating and selecting the most relevant filters from multiple FLIM
executions. MS-FLIM is used only for the first convolutional layer, and the
results already indicate improvement over FLIM. For evaluation, we build a
simple U-shaped encoder-decoder network, named sU-Net, for glioblastoma
segmentation using T1Gd and FLAIR MRI scans, varying the encoder's training
method, using FLIM, MS-FLIM, and backpropagation algorithm. Also, we compared
these sU-Nets with two State-Of-The-Art (SOTA) deep-learning models using two
datasets. The results show that the sU-Net based on MS-FLIM outperforms the
other training methods and achieves effectiveness within the standard
deviations of the SOTA models.
\\ ( https://arxiv.org/abs/2403.12748 ,  6181kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12777 (*cross-listing*)
Date: Tue, 19 Mar 2024 14:44:54 GMT   (24440kb,D)

Title: Discover and Mitigate Multiple Biased Subgroups in Image Classifiers
Authors: Zeliang Zhang, Mingqian Feng, Zhiheng Li, Chenliang Xu
Categories: cs.CV cs.AI
\\
  Machine learning models can perform well on in-distribution data but often
fail on biased subgroups that are underrepresented in the training data,
hindering the robustness of models for reliable applications. Such subgroups
are typically unknown due to the absence of subgroup labels. Discovering biased
subgroups is the key to understanding models' failure modes and further
improving models' robustness. Most previous works of subgroup discovery make an
implicit assumption that models only underperform on a single biased subgroup,
which does not hold on in-the-wild data where multiple biased subgroups exist.
  In this work, we propose Decomposition, Interpretation, and Mitigation (DIM),
a novel method to address a more challenging but also more practical problem of
discovering multiple biased subgroups in image classifiers. Our approach
decomposes the image features into multiple components that represent multiple
subgroups. This decomposition is achieved via a bilinear dimension reduction
method, Partial Least Square (PLS), guided by useful supervision from the image
classifier. We further interpret the semantic meaning of each subgroup
component by generating natural language descriptions using vision-language
foundation models. Finally, DIM mitigates multiple biased subgroups
simultaneously via two strategies, including the data- and model-centric
strategies. Extensive experiments on CIFAR-100 and Breeds datasets demonstrate
the effectiveness of DIM in discovering and mitigating multiple biased
subgroups. Furthermore, DIM uncovers the failure modes of the classifier on
Hard ImageNet, showcasing its broader applicability to understanding model bias
in image classifiers. The code is available at
https://github.com/ZhangAIPI/DIM.
\\ ( https://arxiv.org/abs/2403.12777 ,  24440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12816 (*cross-listing*)
Date: Tue, 19 Mar 2024 15:15:19 GMT   (9022kb,D)

Title: Re-identification from histopathology images
Authors: Jonathan Ganz, Jonas Ammeling, Samir Jabari, Katharina Breininger,
  Marc Aubreville
Categories: cs.CV cs.AI
Comments: 20 pages, 7 figures, 2 tables
\\
  In numerous studies, deep learning algorithms have proven their potential for
the analysis of histopathology images, for example, for revealing the subtypes
of tumors or the primary origin of metastases. These models require large
datasets for training, which must be anonymized to prevent possible patient
identity leaks. This study demonstrates that even relatively simple deep
learning algorithms can re-identify patients in large histopathology datasets
with substantial accuracy. We evaluated our algorithms on two TCIA datasets
including lung squamous cell carcinoma (LSCC) and lung adenocarcinoma (LUAD).
We also demonstrate the algorithm's performance on an in-house dataset of
meningioma tissue. We predicted the source patient of a slide with F1 scores of
50.16 % and 52.30 % on the LSCC and LUAD datasets, respectively, and with 62.31
% on our meningioma dataset. Based on our findings, we formulated a risk
assessment scheme to estimate the risk to the patient's privacy prior to
publication.
\\ ( https://arxiv.org/abs/2403.12816 ,  9022kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12823 (*cross-listing*)
Date: Tue, 19 Mar 2024 15:24:49 GMT   (83kb)

Title: Answer Set Programming for Flexible Payroll Management
Authors: Benjamin Callewaert, Joost Vennekens
Categories: cs.LO cs.AI
Comments: Under consideration in Theory and Practice of Logic Programming
  (TPLP)
\\
  Payroll management is a critical business task that is subject to a large
number of rules, which vary widely between companies, sectors, and countries.
Moreover, the rules are often complex and change regularly. Therefore, payroll
management systems must be flexible in design. In this paper, we suggest an
approach based on a flexible Answer Set Programming (ASP) model and an
easy-to-read tabular representation based on the Decision Model and Notation
(DMN) standard. It allows HR consultants to represent complex rules without the
need for a software engineer, and to ultimately design payroll systems for a
variety of different scenarios. We show how the multi-shot solving capabilities
of the clingo ASP system can be used to reach the performance that is necessary
to handle real-world instances.
\\ ( https://arxiv.org/abs/2403.12823 ,  83kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12853 (*cross-listing*)
Date: Tue, 19 Mar 2024 15:57:32 GMT   (16902kb,D)

Title: RASP: A Drone-based Reconfigurable Actuation and Sensing Platform
  Towards Ambient Intelligent Systems
Authors: Minghui Zhao, Junxi Xia, Kaiyuan Hou, Yanchen Liu, Stephen Xia,
  Xiaofan Jiang
Categories: cs.RO cs.AI cs.HC
\\
  Realizing consumer-grade drones that are as useful as robot vacuums
throughout our homes or personal smartphones in our daily lives requires drones
to sense, actuate, and respond to general scenarios that may arise. Towards
this vision, we propose RASP, a modular and reconfigurable sensing and
actuation platform that allows drones to autonomously swap onboard sensors and
actuators in only 25 seconds, allowing a single drone to quickly adapt to a
diverse range of tasks. RASP consists of a mechanical layer to physically swap
sensor modules, an electrical layer to maintain power and communication lines
to the sensor/actuator, and a software layer to maintain a common interface
between the drone and any sensor module in our platform. Leveraging recent
advances in large language and visual language models, we further introduce the
architecture, implementation, and real-world deployments of a personal
assistant system utilizing RASP. We demonstrate that RASP can enable a diverse
range of useful tasks in home, office, lab, and other indoor settings.
\\ ( https://arxiv.org/abs/2403.12853 ,  16902kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12891 (*cross-listing*)
Date: Tue, 19 Mar 2024 16:40:57 GMT   (18217kb,D)

Title: Adaptive Visual Imitation Learning for Robotic Assisted Feeding Across
  Varied Bowl Configurations and Food Types
Authors: Rui Liu, Amisha Bhaskar, Pratap Tokekar
Categories: cs.RO cs.AI cs.CV
\\
  In this study, we introduce a novel visual imitation network with a spatial
attention module for robotic assisted feeding (RAF). The goal is to acquire
(i.e., scoop) food items from a bowl. However, achieving robust and adaptive
food manipulation is particularly challenging. To deal with this, we propose a
framework that integrates visual perception with imitation learning to enable
the robot to handle diverse scenarios during scooping. Our approach, named AVIL
(adaptive visual imitation learning), exhibits adaptability and robustness
across different bowl configurations in terms of material, size, and position,
as well as diverse food types including granular, semi-solid, and liquid, even
in the presence of distractors. We validate the effectiveness of our approach
by conducting experiments on a real robot. We also compare its performance with
a baseline. The results demonstrate improvement over the baseline across all
scenarios, with an enhancement of up to 2.5 times in terms of a success metric.
Notably, our model, trained solely on data from a transparent glass bowl
containing granular cereals, showcases generalization ability when tested
zero-shot on other bowl configurations with different types of food.
\\ ( https://arxiv.org/abs/2403.12891 ,  18217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12900 (*cross-listing*)
Date: Tue, 19 Mar 2024 16:53:53 GMT   (1146kb,D)

Title: Toward Sustainable GenAI using Generation Directives for Carbon-Friendly
  Large Language Model Inference
Authors: Baolin Li, Yankai Jiang, Vijay Gadepally, Devesh Tiwari
Categories: cs.DC cs.AI cs.CL cs.LG
\\
  The rapid advancement of Generative Artificial Intelligence (GenAI) across
diverse sectors raises significant environmental concerns, notably the carbon
emissions from their cloud and high performance computing (HPC) infrastructure.
This paper presents Sprout, an innovative framework designed to address these
concerns by reducing the carbon footprint of generative Large Language Model
(LLM) inference services. Sprout leverages the innovative concept of
"generation directives" to guide the autoregressive generation process, thereby
enhancing carbon efficiency. Our proposed method meticulously balances the need
for ecological sustainability with the demand for high-quality generation
outcomes. Employing a directive optimizer for the strategic assignment of
generation directives to user prompts and an original offline quality
evaluator, Sprout demonstrates a significant reduction in carbon emissions by
over 40% in real-world evaluations using the Llama2 LLM and global electricity
grid data. This research marks a critical step toward aligning AI technology
with sustainable practices, highlighting the potential for mitigating
environmental impacts in the rapidly expanding domain of generative artificial
intelligence.
\\ ( https://arxiv.org/abs/2403.12900 ,  1146kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12910 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:08:24 GMT   (42527kb,D)

Title: Yell At Your Robot: Improving On-the-Fly from Language Corrections
Authors: Lucy Xiaoyang Shi, Zheyuan Hu, Tony Z. Zhao, Archit Sharma, Karl
  Pertsch, Jianlan Luo, Sergey Levine, Chelsea Finn
Categories: cs.RO cs.AI cs.LG
Comments: Project website: https://yay-robot.github.io/
\\
  Hierarchical policies that combine language and low-level control have been
shown to perform impressively long-horizon robotic tasks, by leveraging either
zero-shot high-level planners like pretrained language and vision-language
models (LLMs/VLMs) or models trained on annotated robotic demonstrations.
However, for complex and dexterous skills, attaining high success rates on
long-horizon tasks still represents a major challenge -- the longer the task
is, the more likely it is that some stage will fail. Can humans help the robot
to continuously improve its long-horizon task performance through intuitive and
natural feedback? In this paper, we make the following observation: high-level
policies that index into sufficiently rich and expressive low-level
language-conditioned skills can be readily supervised with human feedback in
the form of language corrections. We show that even fine-grained corrections,
such as small movements ("move a bit to the left"), can be effectively
incorporated into high-level policies, and that such corrections can be readily
obtained from humans observing the robot and making occasional suggestions.
This framework enables robots not only to rapidly adapt to real-time language
feedback, but also incorporate this feedback into an iterative training scheme
that improves the high-level policy's ability to correct errors in both
low-level execution and high-level decision-making purely from verbal feedback.
Our evaluation on real hardware shows that this leads to significant
performance improvement in long-horizon, dexterous manipulation tasks without
the need for any additional teleoperation. Videos and code are available at
https://yay-robot.github.io/.
\\ ( https://arxiv.org/abs/2403.12910 ,  42527kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12943 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:47:37 GMT   (4658kb,D)

Title: Vid2Robot: End-to-end Video-conditioned Policy Learning with
  Cross-Attention Transformers
Authors: Vidhi Jain, Maria Attarian, Nikhil J Joshi, Ayzaan Wahid, Danny
  Driess, Quan Vuong, Pannag R Sanketi, Pierre Sermanet, Stefan Welker,
  Christine Chan, Igor Gilitschenski, Yonatan Bisk, Debidatta Dwibedi
Categories: cs.RO cs.AI
Comments: Robot learning: Imitation Learning, Robot Perception, Sensing &
  Vision, Grasping & Manipulation
\\
  While large-scale robotic systems typically rely on textual instructions for
tasks, this work explores a different approach: can robots infer the task
directly from observing humans? This shift necessitates the robot's ability to
decode human intent and translate it into executable actions within its
physical constraints and environment. We introduce Vid2Robot, a novel
end-to-end video-based learning framework for robots. Given a video
demonstration of a manipulation task and current visual observations, Vid2Robot
directly produces robot actions. This is achieved through a unified
representation model trained on a large dataset of human video and robot
trajectory. The model leverages cross-attention mechanisms to fuse prompt video
features to the robot's current state and generate appropriate actions that
mimic the observed task. To further improve policy performance, we propose
auxiliary contrastive losses that enhance the alignment between human and robot
video representations. We evaluate Vid2Robot on real-world robots,
demonstrating a 20% improvement in performance compared to other
video-conditioned policies when using human demonstration videos. Additionally,
our model exhibits emergent capabilities, such as successfully transferring
observed motions from one object to another, and long-horizon composition, thus
showcasing its potential for real-world applications. Project website:
vid2robot.github.io
\\ ( https://arxiv.org/abs/2403.12943 ,  4658kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12952 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:54:34 GMT   (361kb,D)

Title: Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization
  with Vision-Language Models
Authors: Elaine Sui, Xiaohan Wang, Serena Yeung-Levy
Categories: cs.CV cs.AI cs.LG
\\
  Advancements in vision-language models (VLMs) have propelled the field of
computer vision, particularly in the zero-shot learning setting. Despite their
promise, the effectiveness of these models often diminishes due to domain
shifts in test environments. To address this, we introduce the Test-Time
Prototype Shifting (TPS) framework, a pioneering approach designed to adapt
VLMs to test datasets using unlabeled test inputs. Our method is based on the
notion of modulating per-class prototypes in the shared embedding space. By
pre-computing and caching prototypes generated with the pre-trained text
encoder, TPS not only facilitates optimization-free prototype reuse for
subsequent predictions but also enables seamless integration with current
advancements in prompt engineering. At test-time, TPS dynamically learns shift
vectors for each prototype based solely on the given test sample, effectively
bridging the domain gap and enhancing classification accuracy. A notable aspect
of our framework is its significantly reduced memory and computational demands
when compared to conventional text-prompt tuning methods. Extensive evaluations
across 15 datasets involving natural distribution shifts and cross-dataset
generalization demonstrate TPS's superior performance, achieving
state-of-the-art results while reducing resource requirements.
\\ ( https://arxiv.org/abs/2403.12952 ,  361kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12959 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:58:02 GMT   (24594kb,D)

Title: WHAC: World-grounded Humans and Cameras
Authors: Wanqi Yin, Zhongang Cai, Ruisi Wang, Fanzhou Wang, Chen Wei, Haiyi
  Mei, Weiye Xiao, Zhitao Yang, Qingping Sun, Atsushi Yamashita, Ziwei Liu and
  Lei Yang
Categories: cs.CV cs.AI cs.GR cs.LG cs.RO
Comments: Homepage: https://wqyin.github.io/projects/WHAC/
\\
  Estimating human and camera trajectories with accurate scale in the world
coordinate system from a monocular video is a highly desirable yet challenging
and ill-posed problem. In this study, we aim to recover expressive parametric
human models (i.e., SMPL-X) and corresponding camera poses jointly, by
leveraging the synergy between three critical players: the world, the human,
and the camera. Our approach is founded on two key observations. Firstly,
camera-frame SMPL-X estimation methods readily recover absolute human depth.
Secondly, human motions inherently provide absolute spatial cues. By
integrating these insights, we introduce a novel framework, referred to as
WHAC, to facilitate world-grounded expressive human pose and shape estimation
(EHPS) alongside camera pose estimation, without relying on traditional
optimization techniques. Additionally, we present a new synthetic dataset,
WHAC-A-Mole, which includes accurately annotated humans and cameras, and
features diverse interactive human motions as well as realistic camera
trajectories. Extensive experiments on both standard and newly established
benchmarks highlight the superiority and efficacy of our framework. We will
make the code and dataset publicly available.
\\ ( https://arxiv.org/abs/2403.12959 ,  24594kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12961 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:59:09 GMT   (29330kb,D)

Title: TexTile: A Differentiable Metric for Texture Tileability
Authors: Carlos Rodriguez-Pardo, Dan Casas, Elena Garces, Jorge Lopez-Moreno
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: CVPR 2024. Project page: https://mslab.es/projects/TexTile/
MSC-class: 68T07 (Primary) 68T45, 68U05 (Secondary)
ACM-class: I.2.6; I.4.10; I.3.3; I.5.4; I.5.1; I.3.7; I.3.8; I.2.10
\\
  We introduce TexTile, a novel differentiable metric to quantify the degree
upon which a texture image can be concatenated with itself without introducing
repeating artifacts (i.e., the tileability). Existing methods for tileable
texture synthesis focus on general texture quality, but lack explicit analysis
of the intrinsic repeatability properties of a texture. In contrast, our
TexTile metric effectively evaluates the tileable properties of a texture,
opening the door to more informed synthesis and analysis of tileable textures.
Under the hood, TexTile is formulated as a binary classifier carefully built
from a large dataset of textures of different styles, semantics, regularities,
and human annotations.Key to our method is a set of architectural modifications
to baseline pre-train image classifiers to overcome their shortcomings at
measuring tileability, along with a custom data augmentation and training
regime aimed at increasing robustness and accuracy. We demonstrate that TexTile
can be plugged into different state-of-the-art texture synthesis methods,
including diffusion-based strategies, and generate tileable textures while
keeping or even improving the overall texture quality. Furthermore, we show
that TexTile can objectively evaluate any tileable texture synthesis method,
whereas the current mix of existing metrics produces uncorrelated scores which
heavily hinders progress in the field.
\\ ( https://arxiv.org/abs/2403.12961 ,  29330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12083 (*cross-listing*)
Date: Thu, 7 Mar 2024 09:59:15 GMT   (2289kb,D)

Title: Presenting Terrorizer: an algorithm for consolidating company names in
  patent assignees
Authors: Grazia Sveva Ascione, Valerio Sterzi
Categories: cs.IR cs.CL
ACM-class: I.2; J.4
\\
  The problem of disambiguation of company names poses a significant challenge
in extracting useful information from patents. This issue biases research
outcomes as it mostly underestimates the number of patents attributed to
companies, particularly multinational corporations which file patents under a
plethora of names, including alternate spellings of the same entity and,
eventually, companies' subsidiaries. To date, addressing these challenges has
relied on labor-intensive dictionary based or string matching approaches,
leaving the problem of patents' assignee harmonization on large datasets mostly
unresolved. To bridge this gap, this paper describes the Terrorizer algorithm,
a text-based algorithm that leverages natural language processing (NLP),
network theory, and rule-based techniques to harmonize the variants of company
names recorded as patent assignees. In particular, the algorithm follows the
tripartite structure of its antecedents, namely parsing, matching and filtering
stage, adding an original "knowledge augmentation" phase which is used to
enrich the information available on each assignee name. We use Terrorizer on a
set of 325'917 companies' names who are assignees of patents granted by the
USPTO from 2005 to 2022. The performance of Terrorizer is evaluated on four
gold standard datasets. This validation step shows us two main things: the
first is that the performance of Terrorizer is similar over different kind of
datasets, proving that our algorithm generalizes well. Second, when comparing
its performance with the one of the algorithm currently used in PatentsView for
the same task (Monath et al., 2021), it achieves a higher F1 score. Finally, we
use the Tree-structured Parzen Estimator (TPE) optimization algorithm for the
hyperparameters' tuning. Our final result is a reduction in the initial set of
names of over 42%.
\\ ( https://arxiv.org/abs/2403.12083 ,  2289kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12416 (*cross-listing*)
Date: Tue, 19 Mar 2024 03:59:14 GMT   (760kb,D)

Title: Eye-gaze Guided Multi-modal Alignment Framework for Radiology
Authors: Chong Ma, Hanqi Jiang, Wenting Chen, Zihao Wu, Xiaowei Yu, Fang Zeng,
  Lei Guo, Dajiang Zhu, Tuo Zhang, Dinggang Shen, Tianming Liu, Xiang Li
Categories: cs.CV cs.CL
Comments: 12 pages, 4 figures
MSC-class: 68T07
ACM-class: I.2.0; I.4.0; I.5.4; I.7.0
\\
  In multi-modal frameworks, the alignment of cross-modal features presents a
significant challenge. The predominant approach in multi-modal pre-training
emphasizes either global or local alignment between modalities, utilizing
extensive datasets. This bottom-up driven method often suffers from a lack of
interpretability, a critical concern in radiology. Previous studies have
integrated high-level labels in medical images or text, but these still rely on
manual annotation, a costly and labor-intensive process. Our work introduces a
novel approach by using eye-gaze data, collected synchronously by radiologists
during diagnostic evaluations. This data, indicating radiologists' focus areas,
naturally links chest X-rays to diagnostic texts. We propose the Eye-gaze
Guided Multi-modal Alignment (EGMA) framework to harness eye-gaze data for
better alignment of image and text features, aiming to reduce reliance on
manual annotations and thus cut training costs. Our model demonstrates robust
performance, outperforming other state-of-the-art methods in zero-shot
classification and retrieval tasks. The incorporation of easily-obtained
eye-gaze data during routine radiological diagnoses signifies a step towards
minimizing manual annotation dependency. Additionally, we explore the impact of
varying amounts of eye-gaze data on model performance, highlighting the
feasibility and utility of integrating this auxiliary data into multi-modal
pre-training.
\\ ( https://arxiv.org/abs/2403.12416 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12964 (*cross-listing*)
Date: Tue, 19 Mar 2024 17:59:39 GMT   (6950kb,D)

Title: Negative Yields Positive: Unified Dual-Path Adapter for Vision-Language
  Models
Authors: Ce Zhang, Simon Stepputtis, Katia Sycara, Yaqi Xie
Categories: cs.CV cs.CL
\\
  Recently, large-scale pre-trained Vision-Language Models (VLMs) have
demonstrated great potential in learning open-world visual representations, and
exhibit remarkable performance across a wide range of downstream tasks through
efficient fine-tuning. In this work, we innovatively introduce the concept of
dual learning into fine-tuning VLMs, i.e., we not only learn what an image is,
but also what an image isn't. Building on this concept, we introduce a novel
DualAdapter approach to enable dual-path adaptation of VLMs from both positive
and negative perspectives with only limited annotated samples. In the inference
stage, our DualAdapter performs unified predictions by simultaneously
conducting complementary positive selection and negative exclusion across
target classes, thereby enhancing the overall recognition accuracy of VLMs in
downstream tasks. Our extensive experimental results across 15 datasets
validate that the proposed DualAdapter outperforms existing state-of-the-art
methods on both few-shot learning and domain generalization tasks while
achieving competitive computational efficiency. Code is available at
https://github.com/zhangce01/DualAdapter.
\\ ( https://arxiv.org/abs/2403.12964 ,  6950kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12044 (*cross-listing*)
Date: Fri, 27 Oct 2023 16:41:29 GMT   (1070kb,D)

Title: Mobile Application for Oral Disease Detection using Federated Learning
Authors: Shankara Narayanan V, Sneha Varsha M, Syed Ashfaq Ahmed, Guruprakash J
Categories: cs.CV cs.LG
\\
  The mouth, often regarded as a window to the internal state of the body,
plays an important role in reflecting one's overall health. Poor oral hygiene
has far-reaching consequences, contributing to severe conditions like heart
disease, cancer, and diabetes, while inadequate care leads to discomfort, pain,
and costly treatments. Federated Learning (FL) for object detection can be
utilized for this use case due to the sensitivity of the oral image data of the
patients. FL ensures data privacy by storing the images used for object
detection on the local device and trains the model on the edge. The updated
weights are federated to a central server where all the collected weights are
updated via The Federated Averaging algorithm. Finally, we have developed a
mobile app named OralH which provides user-friendly solutions, allowing people
to conduct self-assessments through mouth scans and providing quick oral health
insights. Upon detection of the issues, the application alerts the user about
potential oral health concerns or diseases and provides details about dental
clinics in the user's locality. Designed as a Progressive Web Application
(PWA), the platform ensures ubiquitous access, catering to users across devices
for a seamless experience. The application aims to provide state-of-the-art
segmentation and detection techniques, leveraging the YOLOv8 object detection
model to identify oral hygiene issues and diseases. This study deals with the
benefits of leveraging FL in healthcare with promising real-world results.
\\ ( https://arxiv.org/abs/2403.12044 ,  1070kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12063 (*cross-listing*)
Date: Fri, 9 Feb 2024 02:23:47 GMT   (33852kb,D)

Title: Consistency Models Improve Diffusion Inverse Solvers
Authors: Tongda Xu, Ziran Zhu, Dailan He, Yuanyuan Wang, Ming Sun, Ning Li,
  Hongwei Qin, Yan Wang, Jingjing Liu, Ya-Qin Zhang
Categories: cs.CV cs.LG
\\
  Diffusion inverse solvers (DIS) aim to find an image $x$ that lives on the
diffusion prior while satisfying the constraint $f(x) = y$, given an operator
$f(.)$ and measurement $y$. Most non-linear DIS use posterior mean
$\hat{x}_{0|t}=\mathbb{E}[x_0|x_t]$ to evaluate $f(.)$ and minimize the
distance $||f(\hat{x}_{0|t})-y||^2$. Previous works show that posterior
mean-based distance is biased; instead, posterior sample $x_{0|t}\sim
p_{\theta}(x_0|x_t)$ promises a better candidate. In this paper, we first
clarify when is posterior sample better: $1)$ When $f(.)$ is linear, the
distance with posterior mean is as good as single posterior sample, thus
preferable as it does not require Monte Carlo; $2)$ When $f(.)$ is non-linear,
the distance using posterior sample is better. As previous approximations to
posterior sample do not look like a real image, we propose to use consistency
model (CM) as a high quality approximation. In addition, we propose a new
family of DIS using pure CM. Empirically, we show that replacing posterior mean
by CM improves DIS performance on non-linear $f(.)$ (e.g. semantic
segmentation, image captioning). Further, our pure CM inversion works well for
both linear and non-linear $f(.)$.
\\ ( https://arxiv.org/abs/2403.12063 ,  33852kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12066 (*cross-listing*)
Date: Fri, 9 Feb 2024 17:12:04 GMT   (36005kb,D)

Title: Adapting SAM for Volumetric X-Ray Data-sets of Arbitrary Sizes
Authors: Roland Gruber and Steffen R\"uger and Thomas Wittenberg
Categories: cs.CV cs.LG
\\
  Objective: We propose a new approach for volumetric instance segmentation in
X-ray Computed Tomography (CT) data for Non-Destructive Testing (NDT) by
combining the Segment Anything Model (SAM) with tile-based Flood Filling
Networks (FFN). Our work evaluates the performance of SAM on volumetric NDT
data-sets and demonstrates its effectiveness to segment instances in
challenging imaging scenarios. Methods: We implemented and evaluated techniques
to extend the image-based SAM algorithm fo the use with volumetric data-sets,
enabling the segmentation of three-dimensional objects using FFN's spatially
adaptability. The tile-based approach for SAM leverages FFN's capabilities to
segment objects of any size. We also explore the use of dense prompts to guide
SAM in combining segmented tiles for improved segmentation accuracy. Results:
Our research indicates the potential of combining SAM with FFN for volumetric
instance segmentation tasks, particularly in NDT scenarios and segmenting large
entities and objects. Conclusion: While acknowledging remaining limitations,
our study provides insights and establishes a foundation for advancements in
instance segmentation in NDT scenarios.
\\ ( https://arxiv.org/abs/2403.12066 ,  36005kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12068 (*cross-listing*)
Date: Sun, 11 Feb 2024 11:51:32 GMT   (990kb)

Title: Process mining for self-regulated learning assessment in e-learning
Authors: R. Cerezo, A. Bogarin, M. Esteban, and C. Romero
Categories: cs.CY cs.LG
Journal-ref: Journal of Computing on Higher Education (2020); 32:74-88
DOI: 10.1007/s12528-019-09225-y
\\
  Content assessment has broadly improved in e-learning scenarios in recent
decades. However, the eLearning process can give rise to a spatial and temporal
gap that poses interesting challenges for assessment of not only content, but
also students' acquisition of core skills such as self-regulated learning. Our
objective was to discover students' self-regulated learning processes during an
eLearning course by using Process Mining Techniques. We applied a new algorithm
in the educational domain called Inductive Miner over the interaction traces
from 101 university students in a course given over one semester on the Moodle
2.0 platform. Data was extracted from the platform's event logs with 21629
traces in order to discover students' self-regulation models that contribute to
improving the instructional process. The Inductive Miner algorithm discovered
optimal models in terms of fitness for both Pass and Fail students in this
dataset, as well as models at a certain level of granularity that can be
interpreted in educational terms, which are the most important achievement in
model discovery. We can conclude that although students who passed did not
follow the instructors' suggestions exactly, they did follow the logic of a
successful self-regulated learning process as opposed to their failing
classmates. The Process Mining models also allow us to examine which specific
actions the students performed, and it was particularly interesting to see a
high presence of actions related to forum-supported collaborative learning in
the Pass group and an absence of those in the Fail group.
\\ ( https://arxiv.org/abs/2403.12068 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12072 (*cross-listing*)
Date: Tue, 13 Feb 2024 15:23:21 GMT   (7469kb,D)

Title: Floralens: a Deep Learning Model for the Portuguese Native Flora
Authors: Ant\'onio Filgueiras, Eduardo R. B. Marques, Lu\'is M. B. Lopes,
  Miguel Marques, Hugo Silva
Categories: cs.CV cs.LG
\\
  Machine-learning techniques, namely deep convolutional neural networks, are
pivotal for image-based identification of biological species in many Citizen
Science platforms. However, the construction of critically sized and sampled
datasets to train the networks and the choice of the network architectures
itself remains little documented and, therefore, does not lend itself to be
easily replicated. In this paper, we develop a streamlined methodology for
building datasets for biological taxa from publicly available research-grade
datasets and for deriving models from these datasets using off-the-shelf deep
convolutional neural networks such as those provided by Google's AutoML Vision
cloud service. Our case study is the Portuguese native flora, anchored in a
high-quality dataset, provided by the Sociedade Portuguesa de Bot\^anica,
scaled up by adding sampled data from iNaturalist, Pl@ntNet, and
Observation.org. We find that with a careful dataset design, off-the-shelf
machine-learning cloud services produce accurate models with relatively little
effort that rival those provided by state-of-the-art citizen science platforms.
The best model we derived, dubbed Floralens, has been integrated into the
public website of Project Biolens, where we gather models for other taxa as
well. The dataset used to train the model and its namesake is publicly
available on Zenodo.
\\ ( https://arxiv.org/abs/2403.12072 ,  7469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12074 (*cross-listing*)
Date: Wed, 14 Feb 2024 17:23:16 GMT   (2923kb)

Title: Beyond Quantities: Machine Learning-based Characterization of Inequality
  in Infrastructure Quality Provision in Cities
Authors: Bo Li, Ali Mostafavi
Categories: cs.CY cs.LG
\\
  The objective of this study is to characterize inequality in infrastructure
quality across urban areas. While a growing of body of literature has
recognized the importance of characterizing infrastructure inequality in cities
and provided quantified metrics to inform urban development plans, the majority
of the existing approaches focus primarily on measuring the quantity of
infrastructure, assuming that more infrastructure is better. Also, the existing
research focuses primarily on index-based approaches in which the status of
infrastructure provision in urban areas is determined based on assumed
subjective weights. The focus on infrastructure quantity and use of indices
obtained from subjective weights has hindered the ability to properly examine
infrastructure inequality as it pertains to urban inequality and environmental
justice considerations. Recognizing this gap, we propose a machine
learning-based approach in which infrastructure features that shape
environmental hazard exposure are identified and we use the weights obtained by
the model to calculate an infrastructure quality provision for spatial areas of
cities and accordingly, quantify the extent of inequality in infrastructure
quality. The implementation of the model in five metropolitan areas in the U.S.
demonstrates the capability of the proposed approach in characterizing
inequality in infrastructure quality and capturing city-specific differences in
the weights of infrastructure features. The results also show that areas in
which low-income populations reside have lower infrastructure quality
provision, suggesting the lower infrastructure quality provision as a
determinant of urban disparities. Accordingly, the proposed approach can be
effectively used to inform integrated urban design strategies to promote
infrastructure equity and environmental justice based on data-driven and
machine intelligence-based insights.
\\ ( https://arxiv.org/abs/2403.12074 ,  2923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12079 (*cross-listing*)
Date: Fri, 1 Mar 2024 17:14:41 GMT   (630kb)

Title: Beyond Beats: A Recipe to Song Popularity? A machine learning approach
Authors: Niklas Sebastian, Jung and Florian Mayer
Categories: cs.IR cs.LG stat.AP
Comments: 14 pages
\\
  Music popularity prediction has garnered significant attention in both
industry and academia, fuelled by the rise of data-driven algorithms and
streaming platforms like Spotify. This study aims to explore the predictive
power of various machine learning models in forecasting song popularity using a
dataset comprising 30,000 songs spanning different genres from 1957 to 2020.
Methods: We employ Ordinary Least Squares (OLS), Multivariate Adaptive
Regression Splines (MARS), Random Forest, and XGBoost algorithms to analyse
song characteristics and their impact on popularity. Results: Ordinary Least
Squares (OLS) regression analysis reveals genre as the primary influencer of
popularity, with notable trends over time. MARS modelling highlights the
complex relationship between variables, particularly with features like
instrumentalness and duration. Random Forest and XGBoost models underscore the
importance of genre, especially EDM, in predicting popularity. Despite
variations in performance, Random Forest emerges as the most effective model,
improving prediction accuracy by 7.1% compared to average scores. Despite the
importance of genre, predicting song popularity remains challenging, as
observed variations in music-related features suggest complex interactions
between genre and other factors. Consequently, while certain characteristics
like loudness and song duration may impact popularity scores, accurately
predicting song success remains elusive.
\\ ( https://arxiv.org/abs/2403.12079 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12080 (*cross-listing*)
Date: Sat, 2 Mar 2024 04:13:46 GMT   (5360kb,D)

Title: Evaluating Terrain-Dependent Performance for Martian Frost Detection in
  Visible Satellite Observations
Authors: Gary Doran, Serina Diniega, Steven Lu, Mark Wronkiewicz, Kiri L.
  Wagstaff
Categories: cs.CV cs.LG
Comments: This is the author's version of the work. It is posted here for your
  personal use. Not for redistribution. The definitive Version of Record was
  published in: SIGKDD DeepSpatial '22, August 15, 2022, Washington, DC
\\
  Seasonal frosting and defrosting on the surface of Mars is hypothesized to
drive both climate processes and the formation and evolution of
geomorphological features such as gullies. Past studies have focused on
manually analyzing the behavior of the frost cycle in the northern mid-latitude
region of Mars using high-resolution visible observations from orbit. Extending
these studies globally requires automating the detection of frost using data
science techniques such as convolutional neural networks. However, visible
indications of frost presence can vary significantly depending on the geologic
context on which the frost is superimposed. In this study, we (1) present a
novel approach for spatially partitioning data to reduce biases in model
performance estimation, (2) illustrate how geologic context affects automated
frost detection, and (3) propose mitigations to observed biases in automated
frost detection.
\\ ( https://arxiv.org/abs/2403.12080 ,  5360kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12115 (*cross-listing*)
Date: Mon, 18 Mar 2024 15:43:45 GMT   (3705kb,D)

Title: Deep learning automates Cobb angle measurement compared with
  multi-expert observers
Authors: Keyu Li, Hanxue Gu, Roy Colglazier, Robert Lark, Elizabeth Hubbard,
  Robert French, Denise Smith, Jikai Zhang, Erin McCrum, Anthony Catanzano,
  Joseph Cao, Leah Waldman, Maciej A. Mazurowski, Benjamin Alman
Categories: eess.IV cs.CV cs.LG
Comments: 17 pages, 5 figures
\\
  Scoliosis, a prevalent condition characterized by abnormal spinal curvature
leading to deformity, requires precise assessment methods for effective
diagnosis and management. The Cobb angle is a widely used scoliosis
quantification method that measures the degree of curvature between the tilted
vertebrae. Yet, manual measuring of Cobb angles is time-consuming and
labor-intensive, fraught with significant interobserver and intraobserver
variability. To address these challenges and the lack of interpretability found
in certain existing automated methods, we have created fully automated software
that not only precisely measures the Cobb angle but also provides clear
visualizations of these measurements. This software integrates deep neural
network-based spine region detection and segmentation, spine centerline
identification, pinpointing the most significantly tilted vertebrae, and direct
visualization of Cobb angles on the original images. Upon comparison with the
assessments of 7 expert readers, our algorithm exhibited a mean deviation in
Cobb angle measurements of 4.17 degrees, notably surpassing the manual
approach's average intra-reader discrepancy of 5.16 degrees. The algorithm also
achieved intra-class correlation coefficients (ICC) exceeding 0.96 and Pearson
correlation coefficients above 0.944, reflecting robust agreement with expert
assessments and superior measurement reliability. Through the comprehensive
reader study and statistical analysis, we believe this algorithm not only
ensures a higher consensus with expert readers but also enhances
interpretability and reproducibility during assessments. It holds significant
promise for clinical application, potentially aiding physicians in more
accurate scoliosis assessment and diagnosis, thereby improving patient care.
\\ ( https://arxiv.org/abs/2403.12115 ,  3705kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12116 (*cross-listing*)
Date: Mon, 18 Mar 2024 16:14:28 GMT   (2509kb,D)

Title: Unsupervised End-to-End Training with a Self-Defined Bio-Inspired Target
Authors: Dongshu Liu, J\'er\'emie Laydevant, Adrien Pontlevy, Damien Querlioz,
  Julie Grollier
Categories: cs.NE cs.ET cs.LG
\\
  Current unsupervised learning methods depend on end-to-end training via deep
learning techniques such as self-supervised learning, with high computational
requirements, or employ layer-by-layer training using bio-inspired approaches
like Hebbian learning, using local learning rules incompatible with supervised
learning. Both approaches are problematic for edge AI hardware that relies on
sparse computational resources and would strongly benefit from alternating
between unsupervised and supervised learning phases - thus leveraging widely
available unlabeled data from the environment as well as labeled training
datasets. To solve this challenge, in this work, we introduce a 'self-defined
target' that uses Winner-Take-All (WTA) selectivity at the network's final
layer, complemented by regularization through biologically inspired homeostasis
mechanism. This approach, framework-agnostic and compatible with both global
(Backpropagation) and local (Equilibrium propagation) learning rules, achieves
a 97.6% test accuracy on the MNIST dataset. Furthermore, we demonstrate that
incorporating a hidden layer enhances classification accuracy and the quality
of learned features across all training methods, showcasing the advantages of
end-to-end unsupervised training. Extending to semi-supervised learning, our
method dynamically adjusts the target according to data availability, reaching
a 96.6% accuracy with just 600 labeled MNIST samples. This result highlights
our 'unsupervised target' strategy's efficacy and flexibility in scenarios
ranging from abundant to no labeled data availability.
\\ ( https://arxiv.org/abs/2403.12116 ,  2509kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12117 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:32:19 GMT   (1435kb,D)

Title: Transfer Learning for T-Cell Response Prediction
Authors: Josua Stadelmaier (University of T\"ubingen), Brandon Malone (NEC
  OncoImmunity), Ralf Eggeling (University of T\"ubingen)
Categories: q-bio.CB cs.LG
Comments: 20 pages, 9 figures. Source code, compiled data, final model, and a
  video presentation are available under
  https://github.com/JosuaStadelmaier/T-cell-response-prediction
\\
  We study the prediction of T-cell response for specific given peptides, which
could, among other applications, be a crucial step towards the development of
personalized cancer vaccines. It is a challenging task due to limited,
heterogeneous training data featuring a multi-domain structure; such data
entail the danger of shortcut learning, where models learn general
characteristics of peptide sources, such as the source organism, rather than
specific peptide characteristics associated with T-cell response.
  Using a transformer model for T-cell response prediction, we show that the
danger of inflated predictive performance is not merely theoretical but occurs
in practice. Consequently, we propose a domain-aware evaluation scheme. We then
study different transfer learning techniques to deal with the multi-domain
structure and shortcut learning. We demonstrate a per-source fine tuning
approach to be effective across a wide range of peptide sources and further
show that our final model outperforms existing state-of-the-art approaches for
predicting T-cell responses for human peptides.
\\ ( https://arxiv.org/abs/2403.12117 ,  1435kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12120 (*cross-listing*)
Date: Mon, 18 Mar 2024 18:00:00 GMT   (1290kb,D)

Title: Light Curve Classification with DistClassiPy: a new distance-based
  classifier
Authors: Siddharth Chaini, Ashish Mahabal, Ajit Kembhavi and Federica B. Bianco
Categories: astro-ph.IM astro-ph.SR cs.LG
Comments: Submitted to Astronomy and Computing. 23 pages, 19 figures
\\
  The rise of synoptic sky surveys has ushered in an era of big data in
time-domain astronomy, making data science and machine learning essential tools
for studying celestial objects. Tree-based (e.g. Random Forests) and deep
learning models represent the current standard in the field. We explore the use
of different distance metrics to aid in the classification of objects. For
this, we developed a new distance metric based classifier called DistClassiPy.
The direct use of distance metrics is an approach that has not been explored in
time-domain astronomy, but distance-based methods can aid in increasing the
interpretability of the classification result and decrease the computational
costs. In particular, we classify light curves of variable stars by comparing
the distances between objects of different classes. Using 18 distance metrics
applied to a catalog of 6,000 variable stars in 10 classes, we demonstrate
classification and dimensionality reduction. We show that this classifier meets
state-of-the-art performance but has lower computational requirements and
improved interpretability. We have made DistClassiPy open-source and accessible
at https://pypi.org/project/distclassipy/ with the goal of broadening its
applications to other classification scenarios within and beyond astronomy.
\\ ( https://arxiv.org/abs/2403.12120 ,  1290kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12158 (*cross-listing*)
Date: Mon, 18 Mar 2024 18:14:54 GMT   (11kb)

Title: Variational Approach for Efficient KL Divergence Estimation in Dirichlet
  Mixture Models
Authors: Samyajoy Pal, Christian Heumann
Categories: stat.ML cs.LG math.ST stat.TH
\\
  This study tackles the efficient estimation of Kullback-Leibler (KL)
Divergence in Dirichlet Mixture Models (DMM), crucial for clustering
compositional data. Despite the significance of DMMs, obtaining an analytically
tractable solution for KL Divergence has proven elusive. Past approaches relied
on computationally demanding Monte Carlo methods, motivating our introduction
of a novel variational approach. Our method offers a closed-form solution,
significantly enhancing computational efficiency for swift model comparisons
and robust estimation evaluations. Validation using real and simulated data
showcases its superior efficiency and accuracy over traditional Monte
Carlo-based methods, opening new avenues for rapid exploration of diverse DMM
models and advancing statistical analyses of compositional data.
\\ ( https://arxiv.org/abs/2403.12158 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12187 (*cross-listing*)
Date: Mon, 18 Mar 2024 18:58:23 GMT   (41kb)

Title: Approximation of RKHS Functionals by Neural Networks
Authors: Tian-Yi Zhou, Namjoon Suh, Guang Cheng, Xiaoming Huo
Categories: stat.ML cs.LG math.ST stat.TH
\\
  Motivated by the abundance of functional data such as time series and images,
there has been a growing interest in integrating such data into neural networks
and learning maps from function spaces to R (i.e., functionals). In this paper,
we study the approximation of functionals on reproducing kernel Hilbert spaces
(RKHS's) using neural networks. We establish the universality of the
approximation of functionals on the RKHS's. Specifically, we derive explicit
error bounds for those induced by inverse multiquadric, Gaussian, and Sobolev
kernels. Moreover, we apply our findings to functional regression, proving that
neural networks can accurately approximate the regression maps in generalized
functional linear models. Existing works on functional learning require
integration-type basis function expansions with a set of pre-specified basis
functions. By leveraging the interpolating orthogonal projections in RKHS's,
our proposed network is much simpler in that we use point evaluations to
replace basis function expansions.
\\ ( https://arxiv.org/abs/2403.12187 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12198 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:13:02 GMT   (13200kb,D)

Title: FLex: Joint Pose and Dynamic Radiance Fields Optimization for Stereo
  Endoscopic Videos
Authors: Florian Philipp Stilz, Mert Asim Karaoglu, Felix Tristram, Nassir
  Navab, Benjamin Busam, Alexander Ladikos
Categories: cs.CV cs.GR cs.LG
\\
  Reconstruction of endoscopic scenes is an important asset for various medical
applications, from post-surgery analysis to educational training. Neural
rendering has recently shown promising results in endoscopic reconstruction
with deforming tissue. However, the setup has been restricted to a static
endoscope, limited deformation, or required an external tracking device to
retrieve camera pose information of the endoscopic camera. With FLex we adress
the challenging setup of a moving endoscope within a highly dynamic environment
of deforming tissue. We propose an implicit scene separation into multiple
overlapping 4D neural radiance fields (NeRFs) and a progressive optimization
scheme jointly optimizing for reconstruction and camera poses from scratch.
This improves the ease-of-use and allows to scale reconstruction capabilities
in time to process surgical videos of 5,000 frames and more; an improvement of
more than ten times compared to the state of the art while being agnostic to
external tracking information. Extensive evaluations on the StereoMIS dataset
show that FLex significantly improves the quality of novel view synthesis while
maintaining competitive pose accuracy.
\\ ( https://arxiv.org/abs/2403.12198 ,  13200kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12203 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:25:57 GMT   (5372kb,D)

Title: Bootstrapping Reinforcement Learning with Imitation for Vision-Based
  Agile Flight
Authors: Jiaxu Xing, Angel Romero, Leonard Bauersfeld, Davide Scaramuzza
Categories: cs.RO cs.CV cs.LG
\\
  We combine the effectiveness of Reinforcement Learning (RL) and the
efficiency of Imitation Learning (IL) in the context of vision-based,
autonomous drone racing. We focus on directly processing visual input without
explicit state estimation. While RL offers a general framework for learning
complex controllers through trial and error, it faces challenges regarding
sample efficiency and computational demands due to the high dimensionality of
visual inputs. Conversely, IL demonstrates efficiency in learning from visual
demonstrations but is limited by the quality of those demonstrations and faces
issues like covariate shift. To overcome these limitations, we propose a novel
training framework combining RL and IL's advantages. Our framework involves
three stages: initial training of a teacher policy using privileged state
information, distilling this policy into a student policy using IL, and
performance-constrained adaptive RL fine-tuning. Our experiments in both
simulated and real-world environments demonstrate that our approach achieves
superior performance and robustness than IL or RL alone in navigating a
quadrotor through a racing course using only visual information without
explicit state estimation.
\\ ( https://arxiv.org/abs/2403.12203 ,  5372kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12206 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:43:00 GMT   (250kb,D)

Title: Useful Compact Representations for Data-Fitting
Authors: Johannes J. Brust
Categories: math.OC cs.LG cs.NA math.NA stat.CO
MSC-class: 65F05, 65F55, 68U05, 15A23, 15A69, 90C06, 90C15, 90C30, 90C53
\\
  For minimization problems without 2nd derivative information, methods that
estimate Hessian ma- trices can be very effective. However, conventional
techniques generate dense matrices that are prohibitive for large problems.
Limited-memory compact representations express the dense arrays in terms of a
low rank representation and have become the state-of-the-art for software
implementations on large deterministic problems. We develop new compact
representations that are parameterized by a choice of vectors and that reduce
to existing well known formulas for special choices. We demonstrate
effectiveness of the compact representations for large eigenvalue computations,
tensor factorizations and nonlinear regressions.
\\ ( https://arxiv.org/abs/2403.12206 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12210 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:51:17 GMT   (395kb,D)

Title: Decomposing Control Lyapunov Functions for Efficient Reinforcement
  Learning
Authors: Antonio Lopez, David Fridovich-Keil
Categories: eess.SY cs.LG cs.SY
\\
  Recent methods using Reinforcement Learning (RL) have proven to be successful
for training intelligent agents in unknown environments. However, RL has not
been applied widely in real-world robotics scenarios. This is because current
state-of-the-art RL methods require large amounts of data to learn a specific
task, leading to unreasonable costs when deploying the agent to collect data in
real-world applications. In this paper, we build from existing work that
reshapes the reward function in RL by introducing a Control Lyapunov Function
(CLF), which is demonstrated to reduce the sample complexity. Still, this
formulation requires knowing a CLF of the system, but due to the lack of a
general method, it is often a challenge to identify a suitable CLF. Existing
work can compute low-dimensional CLFs via a Hamilton-Jacobi reachability
procedure. However, this class of methods becomes intractable on
high-dimensional systems, a problem that we address by using a system
decomposition technique to compute what we call Decomposed Control Lyapunov
Functions (DCLFs). We use the computed DCLF for reward shaping, which we show
improves RL performance. Through multiple examples, we demonstrate the
effectiveness of this approach, where our method finds a policy to successfully
land a quadcopter in less than half the amount of real-world data required by
the state-of-the-art Soft-Actor Critic algorithm.
\\ ( https://arxiv.org/abs/2403.12210 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12213 (*cross-listing*)
Date: Mon, 18 Mar 2024 19:54:59 GMT   (2072kb)

Title: Private graphon estimation via sum-of-squares
Authors: Hongjie Chen, Jingqiu Ding, Tommaso d'Orsi, Yiding Hua, Chih-Hung Liu,
  David Steurer
Categories: cs.DS cs.CC cs.LG stat.ML
Comments: 70 pages, accepted to STOC 2024
\\
  We develop the first pure node-differentially-private algorithms for learning
stochastic block models and for graphon estimation with polynomial running time
for any constant number of blocks. The statistical utility guarantees match
those of the previous best information-theoretic (exponential-time)
node-private mechanisms for these problems. The algorithm is based on an
exponential mechanism for a score function defined in terms of a sum-of-squares
relaxation whose level depends on the number of blocks. The key ingredients of
our results are (1) a characterization of the distance between the block
graphons in terms of a quadratic optimization over the polytope of doubly
stochastic matrices, (2) a general sum-of-squares convergence result for
polynomial optimization over arbitrary polytopes, and (3) a general approach to
perform Lipschitz extensions of score functions as part of the sum-of-squares
algorithmic paradigm.
\\ ( https://arxiv.org/abs/2403.12213 ,  2072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12254 (*cross-listing*)
Date: Mon, 18 Mar 2024 21:07:57 GMT   (5189kb,D)

Title: Adaptive LPD Radar Waveform Design with Generative Deep Learning
Authors: Matthew R. Ziemann and Christopher A. Metzler
Categories: eess.SP cs.LG
Comments: 11 pages, 11 figures
\\
  We propose a novel, learning-based method for adaptively generating low
probability of detection (LPD) radar waveforms that blend into their operating
environment. Our waveforms are designed to follow a distribution that is
indistinguishable from the ambient radio frequency (RF) background -- while
still being effective at ranging and sensing. To do so, we use an unsupervised,
adversarial learning framework; our generator network produces waveforms
designed to confuse a critic network, which is optimized to differentiate
generated waveforms from the background. To ensure our generated waveforms are
still effective for sensing, we introduce and minimize an ambiguity
function-based loss on the generated waveforms. We evaluate the performance of
our method by comparing the single-pulse detectability of our generated
waveforms with traditional LPD waveforms using a separately trained detection
neural network. We find that our method can generate LPD waveforms that reduce
detectability by up to 90% while simultaneously offering improved ambiguity
function (sensing) characteristics. Our framework also provides a mechanism to
trade-off detectability and sensing performance.
\\ ( https://arxiv.org/abs/2403.12254 ,  5189kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12267 (*cross-listing*)
Date: Mon, 18 Mar 2024 21:32:58 GMT   (403kb,D)

Title: Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data
  Quality over Quantity
Authors: Siddharth Joshi, Arnav Jain, Ali Payani and Baharan Mirzasoleiman
Categories: cs.CV cs.LG
Comments: AISTATS 2024, Code:
  https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip
\\
  Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption
datasets learns representations that can achieve remarkable zero-shot
generalization. However, such models require a massive amount of pre-training
data. Improving the quality of the pre-training data has been shown to be much
more effective in improving CLIP's performance than increasing its volume.
Nevertheless, finding small subsets of training data that provably generalize
the best has remained an open question. In this work, we propose the first
theoretically rigorous data selection method for CLIP. We show that subsets
that closely preserve the cross-covariance of the images and captions of the
full data provably achieve a superior generalization performance. Our extensive
experiments on ConceptualCaptions3M and ConceptualCaptions12M demonstrate that
subsets found by \method\ achieve over 2.7x and 1.4x the accuracy of the next
best baseline on ImageNet and its shifted versions. Moreover, we show that our
subsets obtain 1.5x the average accuracy across 11 downstream datasets, of the
next best baseline. The code is available at:
https://github.com/BigML-CS-UCLA/clipcov-data-efficient-clip.
\\ ( https://arxiv.org/abs/2403.12267 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12327 (*cross-listing*)
Date: Mon, 18 Mar 2024 23:45:18 GMT   (485kb,D)

Title: GT-Rain Single Image Deraining Challenge Report
Authors: Howard Zhang and Yunhao Ba and Ethan Yang and Rishi Upadhyay and Alex
  Wong and Achuta Kadambi and Yun Guo and Xueyao Xiao and Xiaoxiong Wang and Yi
  Li and Yi Chang and Luxin Yan and Chaochao Zheng and Luping Wang and Bin Liu
  and Sunder Ali Khowaja and Jiseok Yoon and Ik-Hyun Lee and Zhao Zhang and
  Yanyan Wei and Jiahuan Ren and Suiyi Zhao and Huan Zheng
Categories: cs.CV cs.LG
\\
  This report reviews the results of the GT-Rain challenge on single image
deraining at the UG2+ workshop at CVPR 2023. The aim of this competition is to
study the rainy weather phenomenon in real world scenarios, provide a novel
real world rainy image dataset, and to spark innovative ideas that will further
the development of single image deraining methods on real images. Submissions
were trained on the GT-Rain dataset and evaluated on an extension of the
dataset consisting of 15 additional scenes. Scenes in GT-Rain are comprised of
real rainy image and ground truth image captured moments after the rain had
stopped. 275 participants were registered in the challenge and 55 competed in
the final testing phase.
\\ ( https://arxiv.org/abs/2403.12327 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12338 (*cross-listing*)
Date: Tue, 19 Mar 2024 01:07:35 GMT   (36kb)

Title: Stochastic Halpern iteration in normed spaces and applications to
  reinforcement learning
Authors: Mario Bravo and Juan Pablo Contreras
Categories: math.OC cs.LG stat.ML
Comments: 29 pages
\\
  We analyze the oracle complexity of the stochastic Halpern iteration with
variance reduction, where we aim to approximate fixed-points of nonexpansive
and contractive operators in a normed finite-dimensional space. We show that if
the underlying stochastic oracle is with uniformly bounded variance, our method
exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$,
improving recent rates established for the stochastic Krasnoselskii-Mann
iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$,
which applies to a wide range of algorithms, including all averaged iterations
even with minibatching. Using a suitable modification of our approach, we
derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in
which the operator is a $\gamma$-contraction. As an application, we propose new
synchronous algorithms for average reward and discounted reward Markov decision
processes. In particular, for the average reward, our method improves on the
best-known sample complexity.
\\ ( https://arxiv.org/abs/2403.12338 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12362 (*cross-listing*)
Date: Tue, 19 Mar 2024 02:16:32 GMT   (4794kb,D)

Title: DMAD: Dual Memory Bank for Real-World Anomaly Detection
Authors: Jianlong Hu, Xu Chen, Zhenye Gan, Jinlong Peng, Shengchuan Zhang,
  Jiangning Zhang, Yabiao Wang, Chengjie Wang, Liujuan Cao, Rongrong Ji
Categories: cs.CV cs.LG
\\
  Training a unified model is considered to be more suitable for practical
industrial anomaly detection scenarios due to its generalization ability and
storage efficiency. However, this multi-class setting, which exclusively uses
normal data, overlooks the few but important accessible annotated anomalies in
the real world. To address the challenge of real-world anomaly detection, we
propose a new framework named Dual Memory bank enhanced representation learning
for Anomaly Detection (DMAD). This framework handles both unsupervised and
semi-supervised scenarios in a unified (multi-class) setting. DMAD employs a
dual memory bank to calculate feature distance and feature attention between
normal and abnormal patterns, thereby encapsulating knowledge about normal and
abnormal instances. This knowledge is then used to construct an enhanced
representation for anomaly score learning. We evaluated DMAD on the MVTec-AD
and VisA datasets. The results show that DMAD surpasses current
state-of-the-art methods, highlighting DMAD's capability in handling the
complexities of real-world anomaly detection scenarios.
\\ ( https://arxiv.org/abs/2403.12362 ,  4794kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12367 (*cross-listing*)
Date: Tue, 19 Mar 2024 02:24:16 GMT   (709kb,D)

Title: Semisupervised score based matching algorithm to evaluate the effect of
  public health interventions
Authors: Hongzhe Zhang, Jiasheng Shi and Jing Huang
Categories: stat.ML cs.LG stat.ME
\\
  Multivariate matching algorithms "pair" similar study units in an
observational study to remove potential bias and confounding effects caused by
the absence of randomizations. In one-to-one multivariate matching algorithms,
a large number of "pairs" to be matched could mean both the information from a
large sample and a large number of tasks, and therefore, to best match the
pairs, such a matching algorithm with efficiency and comparatively limited
auxiliary matching knowledge provided through a "training" set of paired units
by domain experts, is practically intriguing.
  We proposed a novel one-to-one matching algorithm based on a quadratic score
function $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$. The weights
$\beta$, which can be interpreted as a variable importance measure, are
designed to minimize the score difference between paired training units while
maximizing the score difference between unpaired training units. Further, in
the typical but intricate case where the training set is much smaller than the
unpaired set, we propose a \underline{s}emisupervised \underline{c}ompanion
\underline{o}ne-\underline{t}o-\underline{o}ne \underline{m}atching
\underline{a}lgorithm (SCOTOMA) that makes the best use of the unpaired units.
The proposed weight estimator is proved to be consistent when the truth
matching criterion is indeed the quadratic score function. When the model
assumptions are violated, we demonstrate that the proposed algorithm still
outperforms some popular competing matching algorithms through a series of
simulations. We applied the proposed algorithm to a real-world study to
investigate the effect of in-person schooling on community Covid-19
transmission rate for policy making purpose.
\\ ( https://arxiv.org/abs/2403.12367 ,  709kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12382 (*cross-listing*)
Date: Tue, 19 Mar 2024 02:47:33 GMT   (4663kb,D)

Title: Low-Trace Adaptation of Zero-shot Self-supervised Blind Image Denoising
Authors: Jintong Hu, Bin Xia, Bingchen Li, Wenming Yang
Categories: eess.IV cs.CV cs.LG
Comments: 11pages, 6 figures
\\
  Deep learning-based denoiser has been the focus of recent development on
image denoising. In the past few years, there has been increasing interest in
developing self-supervised denoising networks that only require noisy images,
without the need for clean ground truth for training. However, a performance
gap remains between current self-supervised methods and their supervised
counterparts. Additionally, these methods commonly depend on assumptions about
noise characteristics, thereby constraining their applicability in real-world
scenarios. Inspired by the properties of the Frobenius norm expansion, we
discover that incorporating a trace term reduces the optimization goal
disparity between self-supervised and supervised methods, thereby enhancing the
performance of self-supervised learning. To exploit this insight, we propose a
trace-constraint loss function and design the low-trace adaptation Noise2Noise
(LoTA-N2N) model that bridges the gap between self-supervised and supervised
learning. Furthermore, we have discovered that several existing self-supervised
denoising frameworks naturally fall within the proposed trace-constraint loss
as subcases. Extensive experiments conducted on natural and confocal image
datasets indicate that our method achieves state-of-the-art performance within
the realm of zero-shot self-supervised image denoising approaches, without
relying on any assumptions regarding the noise.
\\ ( https://arxiv.org/abs/2403.12382 ,  4663kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12384 (*cross-listing*)
Date: Tue, 19 Mar 2024 02:49:32 GMT   (4174kb,D)

Title: An Aligning and Training Framework for Multimodal Recommendations
Authors: Yifan Liu, Kangning Zhang, Xiangyuan Ren, Yanhua Huang, Jiarui Jin,
  Yingjie Qin, Ruilong Su, Ruiwen Xu, Weinan Zhang
Categories: cs.IR cs.LG
Comments: 11 pages
\\
  With the development of multimedia applications, multimodal recommendations
are playing an essential role, as they can leverage rich contexts beyond user
interactions. Existing methods mainly regard multimodal information as an
auxiliary, using them to help learn ID features; however, there exist semantic
gaps among multimodal content features and ID features, for which directly
using multimodal information as an auxiliary would lead to misalignment in
representations of users and items. In this paper, we first systematically
investigate the misalignment issue in multimodal recommendations, and propose a
solution named AlignRec. In AlignRec, the recommendation objective is
decomposed into three alignments, namely alignment within contents, alignment
between content and categorical ID, and alignment between users and items. Each
alignment is characterized by a specific objective function and is integrated
into our multimodal recommendation framework. To effectively train our
AlignRec, we propose starting from pre-training the first alignment to obtain
unified multimodal features and subsequently training the following two
alignments together with these features as input. As it is essential to analyze
whether each multimodal feature helps in training, we design three new classes
of metrics to evaluate intermediate performance. Our extensive experiments on
three real-world datasets consistently verify the superiority of AlignRec
compared to nine baselines. We also find that the multimodal features generated
by AlignRec are better than currently used ones, which are to be open-sourced.
\\ ( https://arxiv.org/abs/2403.12384 ,  4174kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12429 (*cross-listing*)
Date: Tue, 19 Mar 2024 04:36:41 GMT   (2743kb,D)

Title: TransformMix: Learning Transformation and Mixing Strategies from Data
Authors: Tsz-Him Cheung, Dit-Yan Yeung
Categories: cs.CV cs.LG
Comments: 17 pages, 9 figures
\\
  Data augmentation improves the generalization power of deep learning models
by synthesizing more training samples. Sample-mixing is a popular data
augmentation approach that creates additional data by combining existing
samples. Recent sample-mixing methods, like Mixup and Cutmix, adopt simple
mixing operations to blend multiple inputs. Although such a heuristic approach
shows certain performance gains in some computer vision tasks, it mixes the
images blindly and does not adapt to different datasets automatically. A mixing
strategy that is effective for a particular dataset does not often generalize
well to other datasets. If not properly configured, the methods may create
misleading mixed images, which jeopardize the effectiveness of sample-mixing
augmentations. In this work, we propose an automated approach, TransformMix, to
learn better transformation and mixing augmentation strategies from data. In
particular, TransformMix applies learned transformations and mixing masks to
create compelling mixed images that contain correct and important information
for the target tasks. We demonstrate the effectiveness of TransformMix on
multiple datasets in transfer learning, classification, object detection, and
knowledge distillation settings. Experimental results show that our method
achieves better performance as well as efficiency when compared with strong
sample-mixing baselines.
\\ ( https://arxiv.org/abs/2403.12429 ,  2743kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12493 (*cross-listing*)
Date: Tue, 19 Mar 2024 07:02:06 GMT   (1034kb,D)

Title: A Trainable Feature Extractor Module for Deep Neural Networks and
  Scanpath Classification
Authors: Wolfgang Fuhl
Categories: cs.CV cs.LG
\\
  Scanpath classification is an area in eye tracking research with possible
applications in medicine, manufacturing as well as training systems for
students in various domains. In this paper we propose a trainable feature
extraction module for deep neural networks. The purpose of this module is to
transform a scanpath into a feature vector which is directly useable for the
deep neural network architecture. Based on the backpropagated error of the deep
neural network, the feature extraction module adapts its parameters to improve
the classification performance. Therefore, our feature extraction module is
jointly trainable with the deep neural network. The motivation to this feature
extraction module is based on classical histogram-based approaches which
usually compute distributions over a scanpath. We evaluated our module on three
public datasets and compared it to the state of the art approaches.
\\ ( https://arxiv.org/abs/2403.12493 ,  1034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12559 (*cross-listing*)
Date: Tue, 19 Mar 2024 09:14:52 GMT   (15860kb,D)

Title: Confidence Self-Calibration for Multi-Label Class-Incremental Learning
Authors: Kaile Du, Yifan Zhou, Fan Lyu, Yuyang Li, Chen Lu, Guangcan Liu
Categories: cs.CV cs.LG
\\
  The partial label challenge in Multi-Label Class-Incremental Learning (MLCIL)
arises when only the new classes are labeled during training, while past and
future labels remain unavailable. This issue leads to a proliferation of
false-positive errors due to erroneously high confidence multi-label
predictions, exacerbating catastrophic forgetting within the disjoint label
space. In this paper, we aim to refine multi-label confidence calibration in
MLCIL and propose a Confidence Self-Calibration (CSC) approach. Firstly, for
label relationship calibration, we introduce a class-incremental graph
convolutional network that bridges the isolated label spaces by constructing
learnable, dynamically extended label relationship graph. Then, for confidence
calibration, we present a max-entropy regularization for each multi-label
increment, facilitating confidence self-calibration through the penalization of
over-confident output distributions. Our approach attains new state-of-the-art
results in MLCIL tasks on both MS-COCO and PASCAL VOC datasets, with the
calibration of label confidences confirmed through our methodology.
\\ ( https://arxiv.org/abs/2403.12559 ,  15860kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12599 (*cross-listing*)
Date: Tue, 19 Mar 2024 10:09:41 GMT   (1553kb,D)

Title: Preventing Eviction-Caused Homelessness through ML-Informed Distribution
  of Rental Assistance
Authors: Catalina Vajiac, Arun Frey, Joachim Baumann, Abigail Smith, Kasun
  Amarasinghe, Alice Lai, Kit Rodolfa, Rayid Ghani
Categories: cs.CY cs.LG
Comments: Published at AAAI 2024
\\
  Rental assistance programs provide individuals with financial assistance to
prevent housing instabilities caused by evictions and avert homelessness. Since
these programs operate under resource constraints, they must decide who to
prioritize. Typically, funding is distributed by a reactive or first-come-first
serve allocation process that does not systematically consider risk of future
homelessness. We partnered with Allegheny County, PA to explore a proactive
allocation approach that prioritizes individuals facing eviction based on their
risk of future homelessness. Our ML system that uses state and county
administrative data to accurately identify individuals in need of support
outperforms simpler prioritization approaches by at least 20% while being fair
and equitable across race and gender. Furthermore, our approach would identify
28% of individuals who are overlooked by the current process and end up
homeless. Beyond improvements to the rental assistance program in Allegheny
County, this study can inform the development of evidence-based decision
support tools in similar contexts, including lessons about data needs, model
design, evaluation, and field validation.
\\ ( https://arxiv.org/abs/2403.12599 ,  1553kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12650 (*cross-listing*)
Date: Tue, 19 Mar 2024 11:34:40 GMT   (625kb,D)

Title: Adaptive Multilevel Neural Networks for Parametric PDEs with Error
  Estimation
Authors: Janina E. Sch\"utte, Martin Eigel
Categories: math.NA cs.LG cs.NA
MSC-class: 65M55, 68T07
ACM-class: G.1.8
\\
  To solve high-dimensional parameter-dependent partial differential equations
(pPDEs), a neural network architecture is presented. It is constructed to map
parameters of the model data to corresponding finite element solutions. To
improve training efficiency and to enable control of the approximation error,
the network mimics an adaptive finite element method (AFEM). It outputs a
coarse grid solution and a series of corrections as produced in an AFEM,
allowing a tracking of the error decay over successive layers of the network.
The observed errors are measured by a reliable residual based a posteriori
error estimator, enabling the reduction to only few parameters for the
approximation in the output of the network. This leads to a problem adapted
representation of the solution on locally refined grids. Furthermore, each
solution of the AFEM is discretized in a hierarchical basis. For the
architecture, convolutional neural networks (CNNs) are chosen. The hierarchical
basis then allows to handle sparse images for finely discretized meshes.
Additionally, as corrections on finer levels decrease in amplitude, i.e.,
importance for the overall approximation, the accuracy of the network
approximation is allowed to decrease successively. This can either be
incorporated in the number of generated high fidelity samples used for training
or the size of the network components responsible for the fine grid outputs.
The architecture is described and preliminary numerical examples are presented.
\\ ( https://arxiv.org/abs/2403.12650 ,  625kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12659 (*cross-listing*)
Date: Tue, 19 Mar 2024 11:49:08 GMT   (14936kb,D)

Title: Zeolite Adsorption Property Prediction using Deep Learning
Authors: Marko Petkovi\'c, Jos\'e Manuel Vicent-Luna, Vlado Menkovski, Sof\'ia
  Calero
Categories: cond-mat.mtrl-sci cs.LG
\\
  The ability to efficiently predict adsorption properties of zeolites can be
of large benefit in accelerating the design process of novel materials. The
existing configuration space for these materials is wide, while existing
molecular simulation methods are computationally expensive. In this work, we
propose a model which is 4 to 5 orders of magnitude faster at adsorption
properties compared to molecular simulations. To validate the model, we
generated datasets containing various aluminium configurations for the MOR,
MFI, RHO and ITW zeolites along with their heat of adsorptions and Henry
coefficients for CO$_2$, obtained from Monte Carlo simulations. The predictions
obtained from the Machine Learning model are in agreement with the values
obtained from the Monte Carlo simulations, confirming that the model can be
used for property prediction. Furthermore, we show that the model can be used
for identifying adsorption sites. Finally, we evaluate the capability of our
model for generating novel zeolite configurations by using it in combination
with a genetic algorithm.
\\ ( https://arxiv.org/abs/2403.12659 ,  14936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12687 (*cross-listing*)
Date: Tue, 19 Mar 2024 12:45:52 GMT   (457kb,D)

Title: Audio-Visual Compound Expression Recognition Method based on Late
  Modality Fusion and Rule-based Decision
Authors: Elena Ryumina (1), Maxim Markitantov (1), Dmitry Ryumin (1), Heysem
  Kaya (2), Alexey Karpov (1) ((1) St. Petersburg Federal Research Center of
  the Russian Academy of Sciences, (2) Department of Information and Computing
  Sciences, Utrecht University)
Categories: cs.CV cs.LG
Comments: 7 pages, 3 figures
\\
  This paper presents the results of the SUN team for the Compound Expressions
Recognition Challenge of the 6th ABAW Competition. We propose a novel
audio-visual method for compound expression recognition. Our method relies on
emotion recognition models that fuse modalities at the emotion probability
level, while decisions regarding the prediction of compound expressions are
based on predefined rules. Notably, our method does not use any training data
specific to the target task. The method is evaluated in multi-corpus training
and cross-corpus validation setups. Our findings from the challenge demonstrate
that the proposed method can potentially form a basis for development of
intelligent tools for annotating audio-visual data in the context of human's
basic and compound emotions. The source code is publicly available.
\\ ( https://arxiv.org/abs/2403.12687 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12695 (*cross-listing*)
Date: Tue, 19 Mar 2024 12:52:38 GMT   (10354kb,D)

Title: Federated Semi-supervised Learning for Medical Image Segmentation with
  intra-client and inter-client Consistency
Authors: Yubin Zheng, Peng Tang, Tianjie Ju, Weidong Qiu, Bo Yan
Categories: eess.IV cs.CV cs.LG
Comments: Working in progress
\\
  Medical image segmentation plays a vital role in clinic disease diagnosis and
medical image analysis. However, labeling medical images for segmentation task
is tough due to the indispensable domain expertise of radiologists.
Furthermore, considering the privacy and sensitivity of medical images, it is
impractical to build a centralized segmentation dataset from different medical
institutions. Federated learning aims to train a shared model of isolated
clients without local data exchange which aligns well with the scarcity and
privacy characteristics of medical data. To solve the problem of labeling hard,
many advanced semi-supervised methods have been proposed in a centralized data
setting. As for federated learning, how to conduct semi-supervised learning
under this distributed scenario is worth investigating. In this work, we
propose a novel federated semi-supervised learning framework for medical image
segmentation. The intra-client and inter-client consistency learning are
introduced to smooth predictions at the data level and avoid confirmation bias
of local models. They are achieved with the assistance of a Variational
Autoencoder (VAE) trained collaboratively by clients. The added VAE model plays
three roles: 1) extracting latent low-dimensional features of all labeled and
unlabeled data; 2) performing a novel type of data augmentation in calculating
intra-client consistency loss; 3) utilizing the generative ability of itself to
conduct inter-client consistency distillation. The proposed framework is
compared with other federated semi-supervised or self-supervised learning
methods. The experimental results illustrate that our method outperforms the
state-of-the-art method while avoiding a lot of computation and communication
overhead.
\\ ( https://arxiv.org/abs/2403.12695 ,  10354kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12710 (*cross-listing*)
Date: Tue, 19 Mar 2024 13:17:26 GMT   (41391kb,D)

Title: Selective, Interpretable, and Motion Consistent Privacy Attribute
  Obfuscation for Action Recognition
Authors: Filip Ilic, He Zhao, Thomas Pock, Richard P. Wildes
Categories: cs.CV cs.LG
\\
  Concerns for the privacy of individuals captured in public imagery have led
to privacy-preserving action recognition. Existing approaches often suffer from
issues arising through obfuscation being applied globally and a lack of
interpretability. Global obfuscation hides privacy sensitive regions, but also
contextual regions important for action recognition. Lack of interpretability
erodes trust in these new technologies. We highlight the limitations of current
paradigms and propose a solution: Human selected privacy templates that yield
interpretability by design, an obfuscation scheme that selectively hides
attributes and also induces temporal consistency, which is important in action
recognition. Our approach is architecture agnostic and directly modifies input
imagery, while existing approaches generally require architecture training. Our
approach offers more flexibility, as no retraining is required, and outperforms
alternatives on three widely used datasets.
\\ ( https://arxiv.org/abs/2403.12710 ,  41391kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12712 (*cross-listing*)
Date: Tue, 19 Mar 2024 13:19:41 GMT   (27623kb,D)

Title: Addressing Source Scale Bias via Image Warping for Domain Adaptation
Authors: Shen Zheng, Anurag Ghosh, Srinivasa G. Narasimhan
Categories: cs.CV cs.LG
\\
  In visual recognition, scale bias is a key challenge due to the imbalance of
object and image size distribution inherent in real scene datasets.
Conventional solutions involve injecting scale invariance priors, oversampling
the dataset at different scales during training, or adjusting scale at
inference. While these strategies mitigate scale bias to some extent, their
ability to adapt across diverse datasets is limited. Besides, they increase
computational load during training and latency during inference. In this work,
we use adaptive attentional processing -- oversampling salient object regions
by warping images in-place during training. Discovering that shifting the
source scale distribution improves backbone features, we developed a
instance-level warping guidance aimed at object region sampling to mitigate
source scale bias in domain adaptation. Our approach improves adaptation across
geographies, lighting and weather conditions, is agnostic to the task, domain
adaptation algorithm, saliency guidance, and underlying model architecture.
Highlights include +6.1 mAP50 for BDD100K Clear $\rightarrow$ DENSE Foggy, +3.7
mAP50 for BDD100K Day $\rightarrow$ Night, +3.0 mAP50 for BDD100K Clear
$\rightarrow$ Rainy, and +6.3 mIoU for Cityscapes $\rightarrow$ ACDC. Our
approach adds minimal memory during training and has no additional latency at
inference time. Please see Appendix for more results and analysis.
\\ ( https://arxiv.org/abs/2403.12712 ,  27623kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12729 (*cross-listing*)
Date: Mon, 18 Mar 2024 17:46:07 GMT   (8265kb,D)

Title: Posterior Uncertainty Quantification in Neural Networks using Data
  Augmentation
Authors: Luhuan Wu, Sinead Williamson
Categories: stat.ML cs.LG
\\
  In this paper, we approach the problem of uncertainty quantification in deep
learning through a predictive framework, which captures uncertainty in model
parameters by specifying our assumptions about the predictive distribution of
unseen future data. Under this view, we show that deep ensembling
(Lakshminarayanan et al., 2017) is a fundamentally mis-specified model class,
since it assumes that future data are supported on existing observations only
-- a situation rarely encountered in practice. To address this limitation, we
propose MixupMP, a method that constructs a more realistic predictive
distribution using popular data augmentation techniques. MixupMP operates as a
drop-in replacement for deep ensembles, where each ensemble member is trained
on a random simulation from this predictive distribution. Grounded in the
recently-proposed framework of Martingale posteriors (Fong et al., 2023),
MixupMP returns samples from an implicitly defined Bayesian posterior. Our
empirical analysis showcases that MixupMP achieves superior predictive
performance and uncertainty quantification on various image classification
datasets, when compared with existing Bayesian and non-Bayesian approaches.
\\ ( https://arxiv.org/abs/2403.12729 ,  8265kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12732 (*cross-listing*)
Date: Tue, 19 Mar 2024 13:47:35 GMT   (1566kb,D)

Title: Tighter Confidence Bounds for Sequential Kernel Regression
Authors: Hamish Flynn, David Reeb
Categories: stat.ML cs.LG
Comments: 22 pages, 7 figures
\\
  Confidence bounds are an essential tool for rigorously quantifying the
uncertainty of predictions. In this capacity, they can inform the
exploration-exploitation trade-off and form a core component in many sequential
learning and decision-making algorithms. Tighter confidence bounds give rise to
algorithms with better empirical performance and better performance guarantees.
In this work, we use martingale tail bounds and finite-dimensional
reformulations of infinite-dimensional convex programs to establish new
confidence bounds for sequential kernel regression. We prove that our new
confidence bounds are always tighter than existing ones in this setting. We
apply our confidence bounds to the kernel bandit problem, where future actions
depend on the previous history. When our confidence bounds replace existing
ones, the KernelUCB (GP-UCB) algorithm has better empirical performance, a
matching worst-case performance guarantee and comparable computational cost.
Our new confidence bounds can be used as a generic tool to design improved
algorithms for other kernelised learning and decision-making problems.
\\ ( https://arxiv.org/abs/2403.12732 ,  1566kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12820 (*cross-listing*)
Date: Tue, 19 Mar 2024 15:21:00 GMT   (809kb)

Title: A Physics-embedded Deep Learning Framework for Cloth Simulation
Authors: Zhiwei Zhao
Categories: cs.GR cs.LG
Comments: Updates are kept with future progress. The code is available at:
  https://github.com/Furkath/DL_Framework-for-PBS-Cloth-Simulation
ACM-class: I.2.0; I.3.7
\\
  Delicate cloth simulations have long been desired in computer graphics.
Various methods were proposed to improve engaged force interactions, collision
handling, and numerical integrations. Deep learning has the potential to
achieve fast and real-time simulation, but common neural network structures
often demand many parameters to capture cloth dynamics. This paper proposes a
physics-embedded learning framework that directly encodes physical features of
cloth simulation. The convolutional neural network is used to represent spatial
correlations of the mass-spring system, after which three branches are designed
to learn linear, nonlinear, and time derivate features of cloth physics. The
framework can also integrate with other external forces and collision handling
through either traditional simulators or sub neural networks. The model is
tested across different cloth animation cases, without training with new data.
Agreement with baselines and predictive realism successfully validate its
generalization ability. Inference efficiency of the proposed model also defeats
traditional physics simulation. This framework is also designed to easily
integrate with other visual refinement techniques like wrinkle carving, which
leaves significant chances to incorporate prevailing macing learning techniques
in 3D cloth amination.
\\ ( https://arxiv.org/abs/2403.12820 ,  809kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12859 (*cross-listing*)
Date: Tue, 19 Mar 2024 16:03:03 GMT   (1141kb,D)

Title: Primal Methods for Variational Inequality Problems with Functional
  Constraints
Authors: Liang Zhang, Niao He, Michael Muehlebach
Categories: math.OC cs.LG stat.ML
\\
  Constrained variational inequality problems are recognized for their broad
applications across various fields including machine learning and operations
research. First-order methods have emerged as the standard approach for solving
these problems due to their simplicity and scalability. However, they typically
rely on projection or linear minimization oracles to navigate the feasible set,
which becomes computationally expensive in practical scenarios featuring
multiple functional constraints. Existing efforts to tackle such functional
constrained variational inequality problems have centered on primal-dual
algorithms grounded in the Lagrangian function. These algorithms along with
their theoretical analysis often require the existence and prior knowledge of
the optimal Lagrange multipliers. In this work, we propose a simple primal
method, termed Constrained Gradient Method (CGM), for addressing functional
constrained variational inequality problems, without necessitating any
information on the optimal Lagrange multipliers. We establish a non-asymptotic
convergence analysis of the algorithm for variational inequality problems with
monotone operators under smooth constraints. Remarkably, our algorithms match
the complexity of projection-based methods in terms of operator queries for
both monotone and strongly monotone settings, while utilizing significantly
cheaper oracles based on quadratic programming. Furthermore, we provide several
numerical examples to evaluate the efficacy of our algorithms.
\\ ( https://arxiv.org/abs/2403.12859 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12861 (*cross-listing*)
Date: Tue, 19 Mar 2024 16:05:51 GMT   (3814kb,D)

Title: D-Cubed: Latent Diffusion Trajectory Optimisation for Dexterous
  Deformable Manipulation
Authors: Jun Yamada, Shaohong Zhong, Jack Collins, Ingmar Posner
Categories: cs.RO cs.LG
Comments: https://applied-ai-lab.github.io/D-cubed/
\\
  Mastering dexterous robotic manipulation of deformable objects is vital for
overcoming the limitations of parallel grippers in real-world applications.
Current trajectory optimisation approaches often struggle to solve such tasks
due to the large search space and the limited task information available from a
cost function. In this work, we propose D-Cubed, a novel trajectory
optimisation method using a latent diffusion model (LDM) trained from a
task-agnostic play dataset to solve dexterous deformable object manipulation
tasks. D-Cubed learns a skill-latent space that encodes short-horizon actions
in the play dataset using a VAE and trains a LDM to compose the skill latents
into a skill trajectory, representing a long-horizon action trajectory in the
dataset. To optimise a trajectory for a target task, we introduce a novel
gradient-free guided sampling method that employs the Cross-Entropy method
within the reverse diffusion process. In particular, D-Cubed samples a small
number of noisy skill trajectories using the LDM for exploration and evaluates
the trajectories in simulation. Then, D-Cubed selects the trajectory with the
lowest cost for the subsequent reverse process. This effectively explores
promising solution areas and optimises the sampled trajectories towards a
target task throughout the reverse diffusion process. Through empirical
evaluation on a public benchmark of dexterous deformable object manipulation
tasks, we demonstrate that D-Cubed outperforms traditional trajectory
optimisation and competitive baseline approaches by a significant margin. We
further demonstrate that trajectories found by D-Cubed readily transfer to a
real-world LEAP hand on a folding task.
\\ ( https://arxiv.org/abs/2403.12861 ,  3814kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2306.09509
replaced with revised version Tue, 19 Mar 2024 00:05:21 GMT   (22760kb,D)

Title: Granger-Causal Hierarchical Skill Discovery
Authors: Caleb Chuck, Kevin Black, Aditya Arjun, Yuke Zhu, Scott Niekum
Categories: cs.AI cs.RO
Comments: Accepted TMLR 2024
\\ ( https://arxiv.org/abs/2306.09509 ,  22760kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13812
replaced with revised version Tue, 19 Mar 2024 12:29:54 GMT   (3004kb,D)

Title: Dysen-VDM: Empowering Dynamics-aware Text-to-Video Diffusion with LLMs
Authors: Hao Fei, Shengqiong Wu, Wei Ji, Hanwang Zhang, Tat-Seng Chua
Categories: cs.AI cs.CV
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2308.13812 ,  3004kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12996
replaced with revised version Mon, 18 Mar 2024 20:45:17 GMT   (29217kb,D)

Title: RLIF: Interactive Imitation Learning as Reinforcement Learning
Authors: Jianlan Luo, Perry Dong, Yuexiang Zhai, Yi Ma, Sergey Levine
Categories: cs.AI cs.RO
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2311.12996 ,  29217kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10109
replaced with revised version Tue, 19 Mar 2024 16:43:09 GMT   (536kb,D)

Title: Towards Reducing Diagnostic Errors with Interpretable Risk Prediction
Authors: Denis Jered McInerney, William Dickinson, Lucy C. Flynn, Andrea C.
  Young, Geoffrey S. Young, Jan-Willem van de Meent, Byron C. Wallace
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.10109 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09481
replaced with revised version Tue, 19 Mar 2024 16:48:27 GMT   (4223kb,D)

Title: Clinical Reasoning over Tabular Data and Text with Bayesian Networks
Authors: Paloma Rabaey, Johannes Deleu, Stefan Heytens, Thomas Demeester
Categories: cs.AI
Comments: 10 pages, 2 figures
\\ ( https://arxiv.org/abs/2403.09481 ,  4223kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13853
replaced with revised version Tue, 19 Mar 2024 16:08:36 GMT   (0kb,I)

Title: LAGAN: Deep Semi-Supervised Linguistic-Anthropology Classification with
  Conditional Generative Adversarial Neural Network
Authors: Rossi Kamal, Zuzana Kubincova
Categories: cs.CL cs.CY
Comments: This submission has been withdrawn by arXiv administrators as the
  second author was added without their knowledge or consent
\\ ( https://arxiv.org/abs/2301.13853 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08473
replaced with revised version Tue, 19 Mar 2024 07:59:52 GMT   (1964kb,D)

Title: Shared and Private Information Learning in Multimodal Sentiment Analysis
  with Deep Modal Alignment and Self-supervised Multi-Task Learning
Authors: Songning Lai, Jiakang Li, Guinan Guo, Xifeng Hu, Yulong Li, Yuan Tan,
  Zichen Song, Yutong Liu, Zhaoxia Ren, Chun Wan, Danmin Miao and Zhi Liu
Categories: cs.CL cs.CV
Journal-ref: International Joint Conference on Neural Networks (IJCNN) 2024
\\ ( https://arxiv.org/abs/2305.08473 ,  1964kb)
------------------------------------------------------------------------------
\\
arXiv:2305.10314
replaced with revised version Tue, 19 Mar 2024 11:53:15 GMT   (3632kb,D)

Title: LeTI: Learning to Generate from Textual Interactions
Authors: Xingyao Wang, Hao Peng, Reyhaneh Jabbarvand, Heng Ji
Categories: cs.CL cs.AI cs.SE
Comments: NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2305.10314 ,  3632kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16635
replaced with revised version Tue, 19 Mar 2024 16:14:04 GMT   (8436kb,D)

Title: Impossible Distillation: from Low-Quality Model to High-Quality Dataset
  & Model for Summarization and Paraphrasing
Authors: Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu,
  Jillian Fisher, Taylor Sorensen, Yejin Choi
Categories: cs.CL cs.AI cs.LG
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2305.16635 ,  8436kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18026
replaced with revised version Mon, 18 Mar 2024 23:53:50 GMT   (1544kb,D)

Title: Semantic Role Labeling Guided Out-of-distribution Detection
Authors: Jinan Zou, Maihao Guo, Yu Tian, Yuhao Lin, Haiyao Cao, Lingqiao Liu,
  Ehsan Abbasnejad, Javen Qinfeng Shi
Categories: cs.CL
Comments: accepted by COLING 2024
\\ ( https://arxiv.org/abs/2305.18026 ,  1544kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08666
replaced with revised version Tue, 19 Mar 2024 17:01:03 GMT   (2811kb,D)

Title: Radiology-GPT: A Large Language Model for Radiology
Authors: Zhengliang Liu, Aoxiao Zhong, Yiwei Li, Longtao Yang, Chao Ju, Zihao
  Wu, Chong Ma, Peng Shu, Cheng Chen, Sekeun Kim, Haixing Dai, Lin Zhao, Lichao
  Sun, Dajiang Zhu, Jun Liu, Wei Liu, Dinggang Shen, Xiang Li, Quanzheng Li,
  Tianming Liu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2306.08666 ,  2811kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12725
replaced with revised version Tue, 19 Mar 2024 12:30:53 GMT   (3282kb,D)

Title: Generative Multimodal Entity Linking
Authors: Senbao Shi, Zhenran Xu, Baotian Hu, Min Zhang
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2306.12725 ,  3282kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00852
replaced with revised version Tue, 19 Mar 2024 01:30:03 GMT   (2759kb,D)

Title: VOLTA: Improving Generative Diversity by Variational Mutual Information
  Maximizing Autoencoder
Authors: Yueen Ma, Dafeng Chi, Jingjing Li, Kai Song, Yuzheng Zhuang, Irwin
  King
Categories: cs.CL
\\ ( https://arxiv.org/abs/2307.00852 ,  2759kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07269
replaced with revised version Tue, 19 Mar 2024 12:27:33 GMT   (8964kb,D)

Title: EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language
  Models
Authors: Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu,
  Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng
  Ni, Guozhou Zheng, Huajun Chen
Categories: cs.CL cs.AI cs.CV cs.IR cs.LG
Comments: Code: https://github.com/zjunlp/EasyEdit HF Demo:
  https://huggingface.co/spaces/zjunlp/EasyEdit Video:
  https://youtu.be/Gm6T0QaaskU Docs: https://zjunlp.gitbook.io/easyedit
\\ ( https://arxiv.org/abs/2308.07269 ,  8964kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07875
replaced with revised version Tue, 19 Mar 2024 16:50:50 GMT   (536kb,D)

Title: Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language
  Models that Follow Instructions
Authors: Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R\"ottger,
  Dan Jurafsky, Tatsunori Hashimoto, James Zou
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.07875 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13173
replaced with revised version Tue, 19 Mar 2024 17:11:41 GMT   (1660kb,D)

Title: BenLLMEval: A Comprehensive Evaluation into the Potentials and Pitfalls
  of Large Language Models on Bengali NLP
Authors: Mohsinul Kabir, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mir
  Tafseer Nayeem, M Saiful Bari, Enamul Hoque
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024. The first two authors contributed
  equally
\\ ( https://arxiv.org/abs/2309.13173 ,  1660kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13345
replaced with revised version Tue, 19 Mar 2024 09:00:32 GMT   (1019kb,D)

Title: BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling
  Capacities of Large Language Models
Authors: Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen
Categories: cs.CL
Comments: Accepted for the Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING) 2024
\\ ( https://arxiv.org/abs/2309.13345 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00840
replaced with revised version Mon, 18 Mar 2024 19:28:38 GMT   (8061kb,D)

Title: Error Norm Truncation: Robust Training in the Presence of Data Noise for
  Text Generation Models
Authors: Tianjian Li, Haoran Xu, Philipp Koehn, Daniel Khashabi, Kenton Murray
Categories: cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.00840 ,  8061kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03173
replaced with revised version Mon, 18 Mar 2024 21:44:47 GMT   (315kb,D)

Title: $\mathcal{B}$-Coder: Value-Based Deep Reinforcement Learning for Program
  Synthesis
Authors: Zishun Yu, Yunzhe Tao, Liyu Chen, Tao Sun, Hongxia Yang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.03173 ,  315kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08559
replaced with revised version Tue, 19 Mar 2024 13:18:22 GMT   (829kb,D)

Title: Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of
  Language Models with Hypothesis Refinement
Authors: Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin,
  Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, Xiang
  Ren
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.08559 ,  829kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14855
replaced with revised version Mon, 18 Mar 2024 20:11:03 GMT   (7906kb,D)

Title: Contextual Refinement of Translations: Large Language Models for
  Sentence and Document-Level Post-Editing
Authors: Sai Koneru, Miriam Exel, Matthias Huck and Jan Niehues
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2310.14855 ,  7906kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09585
replaced with revised version Tue, 19 Mar 2024 02:20:50 GMT   (9196kb,D)

Title: LifeTox: Unveiling Implicit Toxicity in Life Advice
Authors: Minbeom Kim, Jahyun Koo, Hwanhee Lee, Joonsuk Park, Hwaran Lee, Kyomin
  Jung
Categories: cs.CL
Comments: 11 pages, 5 figures, NAACL 2024
\\ ( https://arxiv.org/abs/2311.09585 ,  9196kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09684
replaced with revised version Tue, 19 Mar 2024 16:27:37 GMT   (8108kb,D)

Title: Do Physicians Know How to Prompt? The Need for Automatic Prompt
  Optimization Help in Clinical Note Generation
Authors: Zonghai Yao, Ahmed Jaafar, Beining Wang, Zhichao Yang, Hong Yu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.09684 ,  8108kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01989
replaced with revised version Mon, 18 Mar 2024 20:09:01 GMT   (2440kb,D)

Title: Revisiting Zero-Shot Abstractive Summarization in the Era of Large
  Language Models from the Perspective of Position Bias
Authors: Anshuman Chhabra, Hadi Askari, Prasant Mohapatra
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2401.01989 ,  2440kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09283
replaced with revised version Tue, 19 Mar 2024 16:23:20 GMT   (8521kb,D)

Title: Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey
Authors: Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2402.09283 ,  8521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10311
replaced with revised version Tue, 19 Mar 2024 16:57:20 GMT   (52kb,D)

Title: The optimal placement of the head in the noun phrase. The case of
  demonstrative, numeral, adjective and noun
Authors: Ramon Ferrer-i-Cancho
Categories: cs.CL physics.soc-ph
Comments: Many typos corrected
\\ ( https://arxiv.org/abs/2402.10311 ,  52kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11453
replaced with revised version Tue, 19 Mar 2024 14:44:22 GMT   (3682kb,D)

Title: MatPlotAgent: Method and Evaluation for LLM-Based Agentic Scientific
  Data Visualization
Authors: Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan,
  Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi,
  Maosong Sun
Categories: cs.CL
Comments: Work in Progress
\\ ( https://arxiv.org/abs/2402.11453 ,  3682kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13605
replaced with revised version Tue, 19 Mar 2024 04:00:56 GMT   (9560kb,D)

Title: KorNAT: LLM Alignment Benchmark for Korean Social Values and Common
  Knowledge
Authors: Jiyoung Lee, Minwoo Kim, Seungho Kim, Junghwan Kim, Seunghyun Won,
  Hwaran Lee, Edward Choi
Categories: cs.CL
Comments: 35 pages, 7 figures, 16 tables
\\ ( https://arxiv.org/abs/2402.13605 ,  9560kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17231
replaced with revised version Tue, 19 Mar 2024 06:25:40 GMT   (630kb,D)

Title: MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical
  Reasoning
Authors: Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.17231 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00241
replaced with revised version Tue, 19 Mar 2024 08:34:05 GMT   (1477kb,D)

Title: CASIMIR: A Corpus of Scientific Articles enhanced with Multiple
  Author-Integrated Revisions
Authors: Leane Jourdan, Florian Boudin, Nicolas Hernandez, Richard Dufour
Categories: cs.CL
Comments: Accepted at LREC-Coling 2024
\\ ( https://arxiv.org/abs/2403.00241 ,  1477kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01811
replaced with revised version Tue, 19 Mar 2024 15:40:52 GMT   (246kb,D)

Title: Enhancing Multi-Domain Automatic Short Answer Grading through an
  Explainable Neuro-Symbolic Pipeline
Authors: Felix K\"unnecke, Anna Filighera, Colin Leong, Tim Steuer
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.01811 ,  246kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02246
replaced with revised version Tue, 19 Mar 2024 03:42:31 GMT   (1273kb)

Title: PHAnToM: Personality Has An Effect on Theory-of-Mind Reasoning in Large
  Language Models
Authors: Fiona Anting Tan, Gerard Christopher Yeo, Fanyou Wu, Weijie Xu, Vinija
  Jain, Aman Chadha, Kokil Jaidka, Yang Liu, See-Kiong Ng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.02246 ,  1273kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05326
replaced with revised version Tue, 19 Mar 2024 12:53:27 GMT   (2938kb,D)

Title: ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in
  Dialogues
Authors: Yiding Liu and Jingjing Wang and Jiamin Luo and Tao Zeng and Guodong
  Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.05326 ,  2938kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05881
replaced with revised version Tue, 19 Mar 2024 03:48:11 GMT   (401kb,D)

Title: KG-Rank: Enhancing Large Language Models for Medical QA with Knowledge
  Graphs and Ranking Techniques
Authors: Rui Yang, Haoran Liu, Edison Marrese-Taylor, Qingcheng Zeng, Yu He Ke,
  Wanxin Li, Lechao Cheng, Qingyu Chen, James Caverlee, Yutaka Matsuo, Irene Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.05881 ,  401kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06097
replaced with revised version Tue, 19 Mar 2024 11:36:26 GMT   (325kb,D)

Title: Can LLM Substitute Human Labeling? A Case Study of Fine-grained Chinese
  Address Entity Recognition Dataset for UAV Delivery
Authors: Yuxuan Yao, Sichun Luo, Haohan Zhao, Guanzhi Deng, Linqi Song
Categories: cs.CL cs.AI cs.IR
Comments: Accepted by TheWebConf'24 (WWW'24) as a Resource Paper
\\ ( https://arxiv.org/abs/2403.06097 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07311
replaced with revised version Tue, 19 Mar 2024 11:08:02 GMT   (2262kb,D)

Title: Knowledge Graph Large Language Model (KG-LLM) for Link Prediction
Authors: Dong Shu, Tianle Chen, Mingyu Jin, Yiting Zhang, Chong Zhang, Mengnan
  Du, Yongfeng Zhang
Categories: cs.CL cs.LG
Comments: 23 pages, 2 figures
\\ ( https://arxiv.org/abs/2403.07311 ,  2262kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09362
replaced with revised version Tue, 19 Mar 2024 05:49:01 GMT   (5260kb,D)

Title: Komodo: A Linguistic Expedition into Indonesia's Regional Languages
Authors: Louis Owen, Vishesh Tripathi, Abhay Kumar, Biddwan Ahmed
Categories: cs.CL
Comments: 30 Pages, 8 Figures, 4 Tables
\\ ( https://arxiv.org/abs/2403.09362 ,  5260kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09891
replaced with revised version Mon, 18 Mar 2024 23:10:24 GMT   (831kb,D)

Title: Fisher Mask Nodes for Language Model Merging
Authors: Thennal D K, Ganesh Nathan, Suchithra M S
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.09891 ,  831kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11183
replaced with revised version Tue, 19 Mar 2024 10:09:20 GMT   (9052kb,D)

Title: Decoding Continuous Character-based Language from Non-invasive Brain
  Recordings
Authors: Cenyuan Zhang, Xiaoqing Zheng, Ruicheng Yin, Shujie Geng, Jianhan Xu,
  Xuan Gao, Changze Lv, Zixuan Ling, Xuanjing Huang, Miao Cao, Jianfeng Feng
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.11183 ,  9052kb)
------------------------------------------------------------------------------
\\
arXiv:2108.08454
replaced with revised version Tue, 19 Mar 2024 08:12:28 GMT   (9161kb,D)

Title: Improving Human Sequential Decision-Making with Reinforcement Learning
Authors: Hamsa Bastani, Osbert Bastani, Wichinpong Park Sinchaisri
Categories: cs.LG cs.HC
\\ ( https://arxiv.org/abs/2108.08454 ,  9161kb)
------------------------------------------------------------------------------
\\
arXiv:2210.12624
replaced with revised version Tue, 19 Mar 2024 15:47:43 GMT   (2957kb,D)

Title: Mitigating Gradient Bias in Multi-objective Learning: A Provably
  Convergent Stochastic Approach
Authors: Heshan Fernando, Han Shen, Miao Liu, Subhajit Chaudhury, Keerthiram
  Murugesan, Tianyi Chen
Categories: cs.LG math.OC stat.ML
Comments: Changed hyper-parameter choice which affects some of the convergence
  rate results in the paper
\\ ( https://arxiv.org/abs/2210.12624 ,  2957kb)
------------------------------------------------------------------------------
\\
arXiv:2212.03281
replaced with revised version Mon, 18 Mar 2024 21:49:35 GMT   (2651kb,D)

Title: Copula Conformal Prediction for Multi-step Time Series Forecasting
Authors: Sophia Sun, Rose Yu
Categories: cs.LG stat.AP
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2212.03281 ,  2651kb)
------------------------------------------------------------------------------
\\
arXiv:2301.00016
replaced with revised version Tue, 19 Mar 2024 16:07:14 GMT   (0kb,I)

Title: Behave-XAI: Deep Explainable Learning of Behavioral Representational
  Data
Authors: Rossi Kamal, Zuzana Kubincova
Categories: cs.LG
Comments: This submission has been withdrawn by arXiv administrators as the
  second author was added without their knowledge or consent
\\ ( https://arxiv.org/abs/2301.00016 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10835
replaced with revised version Tue, 19 Mar 2024 14:08:25 GMT   (833kb,D)

Title: When Layers Play the Lottery, all Tickets Win at Initialization
Authors: Artur Jordao and George Correa de Araujo and Helena de Almeida Maia
  and Helio Pedrini
Categories: cs.LG
Comments: Published at International Conference on Computer Vision Workshop
  (ICCV), 2023
\\ ( https://arxiv.org/abs/2301.10835 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12923
replaced with revised version Mon, 18 Mar 2024 20:15:51 GMT   (4599kb,D)

Title: On student-teacher deviations in distillation: does it pay to disobey?
Authors: Vaishnavh Nagarajan, Aditya Krishna Menon, Srinadh Bhojanapalli,
  Hossein Mobahi, Sanjiv Kumar
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2301.12923 ,  4599kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03094
replaced with revised version Tue, 19 Mar 2024 13:56:07 GMT   (3015kb,D)

Title: PopulAtion Parameter Averaging (PAPA)
Authors: Alexia Jolicoeur-Martineau, Emy Gervais, Kilian Fatras, Yan Zhang,
  Simon Lacoste-Julien
Categories: cs.LG cs.CV
Comments: Blog post: https://ajolicoeur.wordpress.com/papa/, Code:
  https://github.com/SamsungSAILMontreal/PAPA, TMLR journal publication:
  https://openreview.net/forum?id=cPDVjsOytS
\\ ( https://arxiv.org/abs/2304.03094 ,  3015kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08260
replaced with revised version Tue, 19 Mar 2024 13:30:26 GMT   (2828kb,D)

Title: Cross or Wait? Predicting Pedestrian Interaction Outcomes at
  Unsignalized Crossings
Authors: Chi Zhang (1), Amir Hossein Kalantari (2), Yue Yang (2), Zhongjun Ni
  (3), Gustav Markkula (2), Natasha Merat (2), Christian Berger (1) ((1)
  Department of Computer Science and Engineering, University of Gothenburg,
  Sweden, (2) Institute for Transport Studies, University of Leeds, Leeds, UK,
  (3) Department of Science and Technology, Link\"oping University, Campus
  Norrk\"oping, Sweden)
Categories: cs.LG cs.AI
Comments: 8 pages, 7 figures, 9 tables. Accepted in 2023 IEEE Intelligent
  Vehicles Symposium (IV). DOI: 10.1109/IV55152.2023.10186616
MSC-class: 68T40
ACM-class: I.2.10
Journal-ref: C. Zhang et al, "Cross or Wait? Predicting Pedestrian Interaction
  Outcomes at Unsignalized Crossings," 2023 IEEE Intelligent Vehicles Symposium
  (IV), Anchorage, AK, USA, 2023, pp. 1-8
DOI: 10.1109/IV55152.2023.10186616
\\ ( https://arxiv.org/abs/2304.08260 ,  2828kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01604
replaced with revised version Tue, 19 Mar 2024 17:51:12 GMT   (26308kb,D)

Title: The Training Process of Many Deep Networks Explores the Same
  Low-Dimensional Manifold
Authors: Jialin Mao, Itay Griniasty, Han Kheng Teoh, Rahul Ramesh, Rubing Yang,
  Mark K. Transtrum, James P. Sethna, Pratik Chaudhari
Categories: cs.LG cond-mat.dis-nn
Journal-ref: Proceedings of the National Academy of Sciences 121.12 (2024)
DOI: 10.1073/pnas.2310002121
\\ ( https://arxiv.org/abs/2305.01604 ,  26308kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05109
replaced with revised version Tue, 19 Mar 2024 16:39:03 GMT   (3669kb,D)

Title: Yet Another ICU Benchmark: A Flexible Multi-Center Framework for
  Clinical ML
Authors: Robin van de Water, Hendrik Schmidt, Paul Elbers, Patrick Thoral, Bert
  Arnrich, Patrick Rockenschaub
Categories: cs.LG
Comments: Main benchmark: https://github.com/rvandewater/YAIB, Cohort
  generation: https://github.com/rvandewater/YAIB-cohorts, Models:
  https://github.com/rvandewater/YAIB-models. To be published in ICLR 2024
  proceedings
\\ ( https://arxiv.org/abs/2306.05109 ,  3669kb)
------------------------------------------------------------------------------
\\
arXiv:2306.11035
replaced with revised version Mon, 18 Mar 2024 18:55:44 GMT   (456kb,D)

Title: Adversarial Training Should Be Cast as a Non-Zero-Sum Game
Authors: Alexander Robey, Fabian Latorre, George J. Pappas, Hamed Hassani,
  Volkan Cevher
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2306.11035 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04755
replaced with revised version Tue, 19 Mar 2024 00:36:47 GMT   (4847kb,D)

Title: Information decomposition in complex systems via machine learning
Authors: Kieran A. Murphy, Dani S. Bassett
Categories: cs.LG cond-mat.soft cs.IT math.IT physics.data-an
Comments: Project page: https://distributed-information-bottleneck.github.io/
Journal-ref: PNAS 121 (2024) e2312988121
DOI: 0.1073/pnas.2312988121
\\ ( https://arxiv.org/abs/2307.04755 ,  4847kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06966
replaced with revised version Tue, 19 Mar 2024 12:50:38 GMT   (17227kb,D)

Title: Layer-wise Linear Mode Connectivity
Authors: Linara Adilova, Maksym Andriushchenko, Michael Kamp, Asja Fischer,
  Martin Jaggi
Categories: cs.LG
Comments: published at ICLR24
\\ ( https://arxiv.org/abs/2307.06966 ,  17227kb)
------------------------------------------------------------------------------
\\
arXiv:2307.13716
replaced with revised version Tue, 19 Mar 2024 11:21:07 GMT   (28546kb,D)

Title: FedDRL: A Trustworthy Federated Learning Model Fusion Method Based on
  Staged Reinforcement Learning
Authors: Leiming Chen, Weishan Zhang, Cihao Dong, Sibo Qiao, Ziling Huang,
  Yuming Nie, Zhaoxiang Hou, Chee Wei Tan
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2307.13716 ,  28546kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00721
replaced with revised version Tue, 19 Mar 2024 04:31:59 GMT   (2333kb,D)

Title: A Pre-trained Data Deduplication Model based on Active Learning
Authors: Xinyao Liu, Shengdong Du, Fengmao Lv, Hongtao Xue, Jie Hu, and Tianrui
  Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.00721 ,  2333kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15250
replaced with revised version Tue, 19 Mar 2024 08:49:35 GMT   (202kb,D)

Title: The Relative Gaussian Mechanism and its Application to Private Gradient
  Descent
Authors: Hadrien Hendrikx, Paul Mangold and Aur\'elien Bellet
Categories: cs.LG cs.CR math.OC
\\ ( https://arxiv.org/abs/2308.15250 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10668
replaced with revised version Mon, 18 Mar 2024 23:15:47 GMT   (2356kb,D)

Title: Language Modeling Is Compression
Authors: Gr\'egoire Del\'etang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot
  Catt, Tim Genewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang,
  Matthew Aitchison, Laurent Orseau, Marcus Hutter, Joel Veness
Categories: cs.LG cs.AI cs.CL cs.IT math.IT
\\ ( https://arxiv.org/abs/2309.10668 ,  2356kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01174
replaced with revised version Tue, 19 Mar 2024 15:14:39 GMT   (36204kb,D)

Title: Light Schr\"odinger Bridge
Authors: Alexander Korotin, Nikita Gushchin, Evgeny Burnaev
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.01174 ,  36204kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07630
replaced with revised version Tue, 19 Mar 2024 09:52:42 GMT   (750kb,D)

Title: Differentiable Euler Characteristic Transforms for Shape Classification
Authors: Ernst Roell, Bastian Rieck
Categories: cs.LG
Comments: Accepted at ICLR 2024 (https://openreview.net/forum?id=MO632iPq3I)
\\ ( https://arxiv.org/abs/2310.07630 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13391
replaced with revised version Tue, 19 Mar 2024 09:58:29 GMT   (3047kb,D)

Title: Learning Successor Features with Distributed Hebbian Temporal Memory
Authors: Evgenii Dzhivelikian, Petr Kuderov and Aleksandr I. Panov
Categories: cs.LG cs.AI cs.NE
Comments: 20 pages, 9 figures
\\ ( https://arxiv.org/abs/2310.13391 ,  3047kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02629
replaced with revised version Tue, 19 Mar 2024 09:15:36 GMT   (199kb,D)

Title: Pointer Networks with Q-Learning for OP Combinatorial Optimization
Authors: Alessandro Barro
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2311.02629 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02761
replaced with revised version Tue, 19 Mar 2024 14:41:35 GMT   (98kb,D)

Title: One-Shot Strategic Classification Under Unknown Costs
Authors: Elan Rosenfeld, Nir Rosenfeld
Categories: cs.LG cs.GT stat.ML
Comments: Fixed a bug in Algorithm 1, significantly strengthened Theorem 4.2,
  and added Figure 1 to help visualize the lower bound in Theorem 3.2
\\ ( https://arxiv.org/abs/2311.02761 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02971
replaced with revised version Tue, 19 Mar 2024 13:08:59 GMT   (2519kb,D)

Title: TabRepo: A Large Scale Repository of Tabular Model Evaluations and its
  AutoML Applications
Authors: David Salinas and Nick Erickson
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.02971 ,  2519kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04465
replaced with revised version Tue, 19 Mar 2024 01:38:12 GMT   (45777kb,D)

Title: Solving High Frequency and Multi-Scale PDEs with Gaussian Processes
Authors: Shikai Fang, Madison Cooley, Da Long, Shibo Li, Robert Kirby, Shandian
  Zhe
Categories: cs.LG cs.CE
Journal-ref: The Twelfth International Conference on Learning Representations
  (ICLR 2024)
\\ ( https://arxiv.org/abs/2311.04465 ,  45777kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04829
replaced with revised version Tue, 19 Mar 2024 01:39:19 GMT   (1996kb,D)

Title: Functional Bayesian Tucker Decomposition for Continuous-indexed Tensor
  Data
Authors: Shikai Fang, Xin Yu, Zheng Wang, Shibo Li, Mike Kirby, Shandian Zhe
Categories: cs.LG stat.ML
Journal-ref: The Twelfth International Conference on Learning Representations
  (ICLR 2024)
\\ ( https://arxiv.org/abs/2311.04829 ,  1996kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15056
replaced with revised version Tue, 19 Mar 2024 05:38:16 GMT   (2785kb,D)

Title: Accurate and interpretable drug-drug interaction prediction enabled by
  knowledge subgraph learning
Authors: Yaqing Wang and Zaifei Yang and Quanming Yao
Categories: cs.LG cs.AI
Comments: Accepted to Nature Communications (Medicine)
\\ ( https://arxiv.org/abs/2311.15056 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16191
replaced with revised version Tue, 19 Mar 2024 02:32:56 GMT   (1765kb,D)

Title: Learning Multi-Pattern Normalities in the Frequency Domain for Efficient
  Time Series Anomaly Detection
Authors: Feiyi Chen, Yingying zhang, Zhen Qin, Lunting Fan, Renhe Jiang, Yuxuan
  Liang, Qingsong Wen, Shuiguang Deng
Categories: cs.LG cs.AI
Comments: Accepted by IEEE 40th International Conference on Data Engineering
  (ICDE 2024)
\\ ( https://arxiv.org/abs/2311.16191 ,  1765kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03675
replaced with revised version Tue, 19 Mar 2024 15:41:44 GMT   (4183kb)

Title: GeoShapley: A Game Theory Approach to Measuring Spatial Effects in
  Machine Learning Models
Authors: Ziqi Li
Categories: cs.LG stat.ML
Comments: 30 pages, 10 figures, 6 tables
\\ ( https://arxiv.org/abs/2312.03675 ,  4183kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08656
replaced with revised version Tue, 19 Mar 2024 02:17:43 GMT   (4659kb,D)

Title: MaxK-GNN: Extremely Fast GPU Kernel Design for Accelerating Graph Neural
  Networks Training
Authors: Hongwu Peng, Xi Xie, Kaustubh Shivdikar, MD Amit Hasan, Jiahui Zhao,
  Shaoyi Huang, Omer Khan, David Kaeli, Caiwen Ding
Categories: cs.LG cs.AI cs.DC
Comments: ASPLOS 2024 accepted publication
ACM-class: I.2; C.5
\\ ( https://arxiv.org/abs/2312.08656 ,  4659kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09778
replaced with revised version Tue, 19 Mar 2024 16:09:23 GMT   (170kb,D)

Title: Hypergraph-MLP: Learning on Hypergraphs without Message Passing
Authors: Bohan Tang, Siheng Chen, Xiaowen Dong
Categories: cs.LG eess.SP
Comments: Accepted by ICASSP 2024
\\ ( https://arxiv.org/abs/2312.09778 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10467
replaced with revised version Tue, 19 Mar 2024 00:56:14 GMT   (1010kb,D)

Title: TrojFSP: Trojan Insertion in Few-shot Prompt Tuning
Authors: Mengxin Zheng, Jiaqi Xue, Xun Chen, YanShan Wang, Qian Lou, and Lei
  Jiang
Categories: cs.LG
Comments: 9 pages, 2 figures
\\ ( https://arxiv.org/abs/2312.10467 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11730
replaced with revised version Tue, 19 Mar 2024 03:57:19 GMT   (1489kb,D)

Title: Stronger Graph Transformer with Regularized Attention Scores
Authors: Eugene Ku, Swetha Arunraj
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.11730 ,  1489kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12784
replaced with revised version Tue, 19 Mar 2024 08:06:48 GMT   (7095kb,D)

Title: Fast Cell Library Characterization for Design Technology Co-Optimization
  Based on Graph Neural Networks
Authors: Tianliang Ma, Guangxi Fan, Zhihui Deng, Xuguang Sun, Kainlu Low,
  Leilai Shao
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.12784 ,  7095kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15045
replaced with revised version Mon, 18 Mar 2024 21:13:26 GMT   (8535kb,D)

Title: Probabilistic Modeling for Sequences of Sets in Continuous-Time
Authors: Yuxin Chang, Alex Boyd, Padhraic Smyth
Categories: cs.LG stat.ML
Comments: Oral presentation at AISTATS 2024
\\ ( https://arxiv.org/abs/2312.15045 ,  8535kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09516
replaced with revised version Tue, 19 Mar 2024 08:46:35 GMT   (2879kb,D)

Title: Accelerating Data Generation for Neural Operators via Krylov Subspace
  Recycling
Authors: Hong Wang, Zhongkai Hao, Jie Wang, Zijie Geng, Zhen Wang, Bin Li, Feng
  Wu
Categories: cs.LG cs.AI cs.NA math.NA
\\ ( https://arxiv.org/abs/2401.09516 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10191
replaced with revised version Tue, 19 Mar 2024 14:09:31 GMT   (6202kb,D)

Title: Divide and not forget: Ensemble of selectively trained experts in
  Continual Learning
Authors: Grzegorz Rype\'s\'c, Sebastian Cygert, Valeriya Khan, Tomasz
  Trzci\'nski, Bartosz Zieli\'nski, Bart{\l}omiej Twardowski
Categories: cs.LG cs.CV
Comments: Accepted for ICLR 2024 (main track), code is available at:
  https://github.com/grypesc/SEED
\\ ( https://arxiv.org/abs/2401.10191 ,  6202kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02334
replaced with revised version Tue, 19 Mar 2024 11:55:14 GMT   (200kb,D)

Title: Arithmetic Feature Interaction Is Necessary for Deep Tabular Learning
Authors: Yi Cheng, Renjun Hu, Haochao Ying, Xing Shi, Jian Wu, Wei Lin
Categories: cs.LG cs.AI
Comments: 11 pages, 8 figures, to be published to AAAI2024
ACM-class: I.2.4
\\ ( https://arxiv.org/abs/2402.02334 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02464
replaced with revised version Tue, 19 Mar 2024 05:27:08 GMT   (14051kb,D)

Title: A Graph is Worth $K$ Words: Euclideanizing Graph using Pure Transformer
Authors: Zhangyang Gao, Daize Dong, Cheng Tan, Jun Xia, Bozhen Hu, Stan Z. Li
Categories: cs.LG cs.AI cs.SI
\\ ( https://arxiv.org/abs/2402.02464 ,  14051kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04068
replaced with revised version Mon, 18 Mar 2024 21:37:45 GMT   (891kb,D)

Title: Retrieve to Explain: Evidence-driven Predictions with Language Models
Authors: Ravi Patel, Angus Brayne, Rogier Hintzen, Daniel Jaroslawicz,
  Georgiana Neculae, Dane Corneil
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.04068 ,  891kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04906
replaced with revised version Tue, 19 Mar 2024 16:42:48 GMT   (291kb,D)

Title: Conformal Monte Carlo Meta-learners for Predictive Inference of
  Individual Treatment Effects
Authors: Jef Jonkers, Jarne Verhaeghe, Glenn Van Wallendael, Luc Duchateau,
  Sofie Van Hoecke
Categories: cs.LG stat.ML
Comments: 21 pages, 8 figures
\\ ( https://arxiv.org/abs/2402.04906 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05946
replaced with revised version Tue, 19 Mar 2024 08:43:29 GMT   (2016kb,D)

Title: Unveiling Latent Causal Rules: A Temporal Point Process Approach for
  Abnormal Event Explanation
Authors: Yiling Kuang, Chao Yang, Yang Yang, Shuang Li
Categories: cs.LG cs.AI
Comments: Accepted by AISTATS 2024
\\ ( https://arxiv.org/abs/2402.05946 ,  2016kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11137
replaced with revised version Tue, 19 Mar 2024 00:49:24 GMT   (395kb,D)

Title: TuneTables: Context Optimization for Scalable Prior-Data Fitted Networks
Authors: Benjamin Feuer, Robin Tibor Schirrmeister, Valeriia Cherepanova,
  Chinmay Hegde, Frank Hutter, Micah Goldblum, Niv Cohen, Colin White
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.11137 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12518
replaced with revised version Tue, 19 Mar 2024 15:38:29 GMT   (1151kb,D)

Title: Gaussian Process Neural Additive Models
Authors: Wei Zhang, Brian Barr, John Paisley
Categories: cs.LG cs.AI
Comments: Appears at AAAI 2024
\\ ( https://arxiv.org/abs/2402.12518 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14698
replaced with revised version Tue, 19 Mar 2024 14:21:32 GMT   (11000kb,D)

Title: Using construction waste hauling trucks' GPS data to classify
  earthwork-related locations: A Chengdu case study
Authors: Lei Yu, Ke Han
Categories: cs.LG cs.AI
Comments: 12 pages, 8 figures
\\ ( https://arxiv.org/abs/2402.14698 ,  11000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01046
replaced with revised version Mon, 18 Mar 2024 22:11:45 GMT   (4169kb,D)

Title: A Library of Mirrors: Deep Neural Nets in Low Dimensions are Convex
  Lasso Models with Reflection Features
Authors: Emi Zeger, Yifei Wang, Aaron Mishkin, Tolga Ergen, Emmanuel Cand\`es,
  Mert Pilanci
Categories: cs.LG cs.AI cs.NE math.OC stat.ML
\\ ( https://arxiv.org/abs/2403.01046 ,  4169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04720
replaced with revised version Tue, 19 Mar 2024 16:47:44 GMT   (944kb,D)

Title: Rethinking of Encoder-based Warm-start Methods in Hyperparameter
  Optimization
Authors: Dawid P{\l}udowski, Antoni Zajko, Anna Kozak, Katarzyna Wo\'znica
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.04720 ,  944kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08438
replaced with revised version Tue, 19 Mar 2024 10:54:22 GMT   (2020kb,D)

Title: Reproducibility and Geometric Intrinsic Dimensionality: An Investigation
  on Graph Neural Network Research
Authors: Tobias Hille and Maximilian Stubbemann and Tom Hanika
Categories: cs.LG cs.AI
Comments: 39 pages, 9 figures
MSC-class: 68T01 68T07 68T09 51F99
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2403.08438 ,  2020kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09499
replaced with revised version Tue, 19 Mar 2024 13:53:16 GMT   (3808kb,D)

Title: A Reinforcement Learning Approach to Dairy Farm Battery Management using
  Q Learning
Authors: Nawazish Ali, Abdul Wahid, Rachael Shaw, Karl Mason
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.09499 ,  3808kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10903
replaced with revised version Tue, 19 Mar 2024 08:58:00 GMT   (238kb,D)

Title: DTOR: Decision Tree Outlier Regressor to explain anomalies
Authors: Riccardo Crupi, Daniele Regoli, Alessandro Damiano Sabatino,
  Immacolata Marano, Massimiliano Brinis, Luca Albertazzi, Andrea Cirillo,
  Andrea Claudio Cosentini
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2403.10903 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11087
replaced with revised version Tue, 19 Mar 2024 13:37:36 GMT   (540kb,D)

Title: Incorporating Higher-order Structural Information for Graph Clustering
Authors: Qiankun Li, Haobing Liu, Ruobing Jiang, and Tingting Wang
Categories: cs.LG cs.SI
Journal-ref: DASFAA 2024
\\ ( https://arxiv.org/abs/2403.11087 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11366
replaced with revised version Tue, 19 Mar 2024 16:19:49 GMT   (143kb,D)

Title: JORA: JAX Tensor-Parallel LoRA Library for Retrieval Augmented
  Fine-Tuning
Authors: Anique Tahir, Lu Cheng, and Huan Liu
Categories: cs.LG cs.CL cs.DC
\\ ( https://arxiv.org/abs/2403.11366 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11395
replaced with revised version Tue, 19 Mar 2024 09:36:27 GMT   (2451kb)

Title: Automated data processing and feature engineering for deep learning and
  big data applications: a survey
Authors: Alhassan Mumuni and Fuseini Mumuni
Categories: cs.LG cs.AI cs.DB
Comments: Journal of Information and Intelligence (2024)
DOI: 10.1016/j.jiixd.2024.01.002
\\ ( https://arxiv.org/abs/2403.11395 ,  2451kb)
------------------------------------------------------------------------------
\\
arXiv:2209.11812
replaced with revised version Mon, 18 Mar 2024 22:11:33 GMT   (1147kb,D)

Title: Explanations, Fairness, and Appropriate Reliance in Human-AI
  Decision-Making
Authors: Jakob Schoeffer, Maria De-Arteaga, Niklas Kuehl
Categories: cs.HC cs.AI
Comments: ACM CHI Conference on Human Factors in Computing Systems (CHI '24)
DOI: 10.1145/3613904.3642621
\\ ( https://arxiv.org/abs/2209.11812 ,  1147kb)
------------------------------------------------------------------------------
\\
arXiv:2301.00693
replaced with revised version Tue, 19 Mar 2024 16:06:19 GMT   (0kb,I)

Title: Deep Recurrent Learning Through Long Short Term Memory and TOPSIS
Authors: Rossi Kamal, Zuzana Kubincova, Mosaddek Hossain Kamal, Upama Kabir
Categories: cs.SE cs.AI cs.LG
Comments: This submission has been withdrawn by arXiv administrators as the
  second author was added without their knowledge or consent
\\ ( https://arxiv.org/abs/2301.00693 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2302.03788
replaced with revised version Mon, 18 Mar 2024 22:41:04 GMT   (7231kb,D)

Title: Toward a Theory of Causation for Interpreting Neural Code Models
Authors: David N. Palacio and Nathan Cooper and Alvaro Rodriguez and Kevin
  Moran and Denys Poshyvanyk
Categories: cs.SE cs.AI cs.LG stat.ME
\\ ( https://arxiv.org/abs/2302.03788 ,  7231kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01838
replaced with revised version Tue, 19 Mar 2024 16:40:25 GMT   (13485kb,D)

Title: BugNIST - a Large Volumetric Dataset for Object Detection under Domain
  Shift
Authors: Patrick M{\o}ller Jensen, Vedrana Andersen Dahl, Carsten Gundlach,
  Rebecca Engberg, Hans Martin Kjer, Anders Bjorholm Dahl
Categories: cs.CV cs.AI
Comments: 20 pages, 6 figures, 2 tables
ACM-class: I.2.10; I.4.6
\\ ( https://arxiv.org/abs/2304.01838 ,  13485kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06192
replaced with revised version Tue, 19 Mar 2024 16:16:16 GMT   (8078kb,D)

Title: Ada-NAV: Adaptive Trajectory Length-Based Sample Efficient Policy
  Learning for Robotic Navigation
Authors: Bhrij Patel, Kasun Weerakoon, Wesley A. Suttle, Alec Koppel, Brian M.
  Sadler, Tianyi Zhou, Amrit Singh Bedi and Dinesh Manocha
Categories: cs.RO cs.AI cs.LG
Comments: 11 pages, 9 figures, 2 tables
\\ ( https://arxiv.org/abs/2306.06192 ,  8078kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15006 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 23:59:29 GMT   (153kb,D)

Title: DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species
  Genome
Authors: Zhihan Zhou, Yanrong Ji, Weijian Li, Pratik Dutta, Ramana Davuluri,
  Han Liu
Categories: q-bio.GN cs.AI cs.CE cs.CL
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2306.15006 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01187
replaced with revised version Tue, 19 Mar 2024 12:27:37 GMT   (3437kb,D)

Title: SAMAug: Point Prompt Augmentation for Segment Anything Model
Authors: Haixing Dai, Chong Ma, Zhiling Yan, Zhengliang Liu, Enze Shi, Yiwei
  Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao Wu, Fang Zeng, Dajiang Zhu, Wei
  Liu, Quanzheng Li, Lichao Sun, Shu Zhang Tianming Liu, and Xiang Li
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2307.01187 ,  3437kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04792
replaced with revised version Tue, 19 Mar 2024 14:13:53 GMT   (36107kb)

Title: A Fast and Optimal Learning-based Path Planning Method for Planetary
  Rovers
Authors: Yiming Ji, Yang Liu, Guanghu Xie, Boyu Ma, Zongwu Xie, Baoshi Cao
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2308.04792 ,  36107kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00145
replaced with revised version Mon, 18 Mar 2024 18:51:09 GMT   (6062kb,D)

Title: 3D Reconstruction in Noisy Agricultural Environments: A Bayesian
  Optimization Perspective for View Planning
Authors: Athanasios Bacharis, Konstantinos D. Polyzos, Henry J. Nelson,
  Georgios B. Giannakis, Nikolaos Papanikolopoulos
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2310.00145 ,  6062kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11409
replaced with revised version Tue, 19 Mar 2024 14:23:07 GMT   (262kb,D)

Title: LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks
Authors: Andreas Happe, Aaron Kaplan, J\"urgen Cito
Categories: cs.CR cs.AI
Comments: 11 pages
\\ ( https://arxiv.org/abs/2310.11409 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19778
replaced with revised version Mon, 18 Mar 2024 20:31:05 GMT   (795kb,D)

Title: Human-AI collaboration is not very collaborative yet: A taxonomy of
  interaction patterns in AI-assisted decision making from a systematic review
Authors: Catalina Gomez, Sue Min Cho, Shichang Ke, Chien-Ming Huang, and
  Mathias Unberath
Categories: cs.HC cs.AI
Comments: 25 pages; 2 figures
\\ ( https://arxiv.org/abs/2310.19778 ,  795kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05903
replaced with revised version Tue, 19 Mar 2024 10:32:16 GMT   (128kb,D)

Title: Establishing Performance Baselines in Fine-Tuning, Retrieval-Augmented
  Generation and Soft-Prompting for Non-Specialist LLM Users
Authors: Jennifer Dodgson, Lin Nanzheng, Julian Peh, Akira Rafhael Janson
  Pattirane, Alfath Daryl Alhajir, Eko Ridho Dinarto, Joseph Lim, Syed Danyal
  Ahmad
Categories: cs.IR cs.AI
Comments: 10 pages, LaTeX; typos corrected, using the correct term 'system
  prompting' instead of 'soft prompting'
\\ ( https://arxiv.org/abs/2311.05903 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06455
replaced with revised version Tue, 19 Mar 2024 02:59:03 GMT   (40032kb,D)

Title: Aria-NeRF: Multimodal Egocentric View Synthesis
Authors: Jiankai Sun, Jianing Qiu, Chuanyang Zheng, John Tucker, Javier Yu, Mac
  Schwager
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.06455 ,  40032kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15619
replaced with revised version Tue, 19 Mar 2024 17:17:50 GMT   (2847kb,D)

Title: Align before Adapt: Leveraging Entity-to-Region Alignments for
  Generalizable Video Action Recognition
Authors: Yifei Chen, Dapeng Chen, Ruijin Liu, Sai Zhou, Wenyuan Xue, Wei Peng
Categories: cs.CV cs.AI
Comments: Accepted at CVPR 2024
\\ ( https://arxiv.org/abs/2311.15619 ,  2847kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03526
replaced with revised version Tue, 19 Mar 2024 08:03:07 GMT   (6576kb,D)

Title: On the Diversity and Realism of Distilled Dataset: An Efficient Dataset
  Distillation Paradigm
Authors: Peng Sun, Bei Shi, Daiwei Yu, Tao Lin
Categories: cs.CV cs.AI cs.LG
Comments: 17 pages, 20 figures
\\ ( https://arxiv.org/abs/2312.03526 ,  6576kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03799
replaced with revised version Tue, 19 Mar 2024 16:08:37 GMT   (3275kb,D)

Title: Low-power, Continuous Remote Behavioral Localization with Event Cameras
Authors: Friedhelm Hamann, Suman Ghosh, Ignacio Juarez Martinez, Tom Hart, Alex
  Kacelnik, Guillermo Gallego
Categories: cs.CV cs.AI
Comments: 13 pages, 8 figures, 12 tables, Project page:
  https://tub-rip.github.io/eventpenguins/
Journal-ref: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Seattle, 2024
\\ ( https://arxiv.org/abs/2312.03799 ,  3275kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06731
replaced with revised version Tue, 19 Mar 2024 09:13:22 GMT   (8334kb,D)

Title: Genixer: Empowering Multimodal Large Language Models as a Powerful Data
  Generator
Authors: Henry Hengyuan Zhao, Pan Zhou, Mike Zheng Shou
Categories: cs.CV cs.AI
Comments: Technical report
\\ ( https://arxiv.org/abs/2312.06731 ,  8334kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00420
replaced with revised version Tue, 19 Mar 2024 16:56:53 GMT   (34034kb,D)

Title: SynCDR : Training Cross Domain Retrieval Models with Synthetic Data
Authors: Samarth Mishra, Carlos D. Castillo, Hongcheng Wang, Kate Saenko,
  Venkatesh Saligrama
Categories: cs.CV cs.AI
Comments: Pre-print
\\ ( https://arxiv.org/abs/2401.00420 ,  34034kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07603
replaced with revised version Tue, 19 Mar 2024 11:17:00 GMT   (17251kb,D)

Title: Multi-task real-robot data with gaze attention for dual-arm fine
  manipulation
Authors: Heecheol Kim, Yoshiyuki Ohmura, Yasuo Kuniyoshi
Categories: cs.RO cs.AI
Comments: 10 pages, The dataset is available at
  https://sites.google.com/view/multi-task-fine
\\ ( https://arxiv.org/abs/2401.07603 ,  17251kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07931
replaced with revised version Tue, 19 Mar 2024 17:07:40 GMT   (3015kb,D)

Title: Vertical Federated Image Segmentation
Authors: Paul K. Mandal, Cole Leo
Categories: cs.CV cs.AI cs.DC cs.LG
Comments: 11 pages, 5 figures
ACM-class: C.2.4; I.2.8; I.4; I.4.8
\\ ( https://arxiv.org/abs/2401.07931 ,  3015kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01166
replaced with revised version Tue, 19 Mar 2024 08:22:42 GMT   (244kb,D)

Title: A Comprehensive Survey on 3D Content Generation
Authors: Jian Liu, Xiaoshui Huang, Tianyu Huang, Lu Chen, Yuenan Hou, Shixiang
  Tang, Ziwei Liu, Wanli Ouyang, Wangmeng Zuo, Junjun Jiang, Xianming Liu
Categories: cs.CV cs.AI
Comments: under review
\\ ( https://arxiv.org/abs/2402.01166 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04527
replaced with revised version Tue, 19 Mar 2024 14:56:54 GMT   (1902kb,D)

Title: RA-Rec: An Efficient ID Representation Alignment Framework for LLM-based
  Recommendation
Authors: Xiaohan Yu, Li Zhang, Xin Zhao, Yue Wang, Zhongrui Ma
Categories: cs.IR cs.AI
Comments: 10 pages
\\ ( https://arxiv.org/abs/2402.04527 ,  1902kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09450 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 16:17:00 GMT   (18857kb,D)

Title: Guiding Masked Representation Learning to Capture Spatio-Temporal
  Relationship of Electrocardiogram
Authors: Yeongyeon Na, Minje Park, Yunwon Tae, Sunghoon Joo
Categories: eess.SP cs.AI cs.LG
Comments: ICLR 2024. The first three authors contribute equally
\\ ( https://arxiv.org/abs/2402.09450 ,  18857kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13224 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 12:28:13 GMT   (442kb,D)

Title: Controlling Large Electric Vehicle Charging Stations via User Behavior
  Modeling and Stochastic Programming
Authors: Alban Puech, Tristan Rigaut, William Templier, Maud Tournoud
Categories: math.OC cs.AI cs.CE cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2402.13224 ,  442kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18590
replaced with revised version Tue, 19 Mar 2024 07:56:40 GMT   (143kb)

Title: Exploring the Impact of Large Language Models on Recommender Systems: An
  Extensive Review
Authors: Arpita Vats, Vinija Jain, Rahul Raja, Aman Chadha
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2402.18590 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06025
replaced with revised version Tue, 19 Mar 2024 05:58:51 GMT   (13284kb,D)

Title: CarbonNet: How Computer Vision Plays a Role in Climate Change?
  Application: Learning Geomechanics from Subsurface Geometry of CCS to
  Mitigate Global Warming
Authors: Wei Chen, Yunan Li and Yuan Tian
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.06025 ,  13284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06420
replaced with revised version Tue, 19 Mar 2024 17:52:09 GMT   (12781kb,D)

Title: RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic
  Manipulations With Large Language Models
Authors: Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang
Categories: cs.RO cs.AI cs.HC cs.LG
\\ ( https://arxiv.org/abs/2403.06420 ,  12781kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09669
replaced with revised version Mon, 18 Mar 2024 07:02:44 GMT   (4568kb,D)

Title: STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video
  Generative Models
Authors: Pum Jun Kim, Seojun Kim, Jaejun Yoo
Categories: cs.CV cs.AI
Comments: Our work is accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2403.09669 ,  4568kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09673 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 05:29:23 GMT   (19191kb,D)

Title: FoldToken: Learning Protein Language via Vector Quantization and Beyond
Authors: Zhangyang Gao, Cheng Tan, Jue Wang, Yufei Huang, Lirong Wu, Stan Z. Li
Categories: q-bio.BM cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.09673 ,  19191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09920 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 18:04:39 GMT   (3717kb)

Title: Predicting Generalization of AI Colonoscopy Models to Unseen Data
Authors: Joel Shor, Carson McNeil, Yotam Intrator, Joseph R Ledsam, Hiro-o
  Yamano, Daisuke Tsurumaru, Hiroki Kayama, Atsushi Hamabe, Koji Ando,
  Mitsuhiko Ota, Haruei Ogino, Hiroshi Nakase, Kaho Kobayashi, Masaaki Miyo,
  Eiji Oki, Ichiro Takemasa, Ehud Rivlin, Roman Goldenberg
Categories: eess.IV cs.AI cs.CV cs.CY
\\ ( https://arxiv.org/abs/2403.09920 ,  3717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10086
replaced with revised version Tue, 19 Mar 2024 09:30:21 GMT   (430kb)

Title: Large Language Models to Generate System-Level Test Programs Targeting
  Non-functional Properties
Authors: Denis Schwachhofer, Peter Domanski, Steffen Becker, Stefan Wagner,
  Matthias Sauer, Dirk Pfl\"uger, Ilia Polian
Categories: cs.SE cs.AI cs.ET cs.PL
Comments: Testmethoden und Zuverl\"assigkeit von Schaltungen und Systemen, TuZ
  2024
\\ ( https://arxiv.org/abs/2403.10086 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10433
replaced with revised version Tue, 19 Mar 2024 14:12:24 GMT   (1442kb,D)

Title: AI-enhanced Collective Intelligence: The State of the Art and Prospects
Authors: Hao Cui and Taha Yasseri
Categories: cs.CY cs.AI
Comments: 27 pages, 2 figures
\\ ( https://arxiv.org/abs/2403.10433 ,  1442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10462
replaced with revised version Mon, 18 Mar 2024 18:11:46 GMT   (3691kb,D)

Title: Safety Cases: How to Justify the Safety of Advanced AI Systems
Authors: Joshua Clymer, Nick Gabrieli, David Krueger, Thomas Larsen
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2403.10462 ,  3691kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11220
replaced with revised version Tue, 19 Mar 2024 13:02:10 GMT   (17939kb,D)

Title: CPA-Enhancer: Chain-of-Thought Prompted Adaptive Enhancer for Object
  Detection under Unknown Degradations
Authors: Yuwei Zhang, Yan Wu, Yanming Liu, Xinyue Peng
Categories: cs.CV cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.11220 ,  17939kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11492
replaced with revised version Tue, 19 Mar 2024 17:04:35 GMT   (1486kb,D)

Title: SmartRefine: A Scenario-Adaptive Refinement Framework for Efficient
  Motion Prediction
Authors: Yang Zhou, Hao Shao, Letian Wang, Steven L. Waslander, Hongsheng Li,
  Yu Liu
Categories: cs.CV cs.AI cs.RO
Comments: Camera-ready version for CVPR 2024
\\ ( https://arxiv.org/abs/2403.11492 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11755
replaced with revised version Tue, 19 Mar 2024 13:28:27 GMT   (1166kb,D)

Title: Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
Authors: M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub
  Micorek, Mateusz Kozinski, Hilde Kuhene, Horst Possegger
Categories: cs.CV cs.AI cs.LG
Comments: Project Page (Code and Data):
  https://jmiemirza.github.io/Meta-Prompting/
\\ ( https://arxiv.org/abs/2403.11755 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11852
replaced with revised version Tue, 19 Mar 2024 02:46:55 GMT   (559kb,D)

Title: Reinforcement Learning with Latent State Inference for Autonomous
  On-ramp Merging under Observation Delay
Authors: Amin Tabrizian, Zhitong Huang, Peng Wei
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2403.11852 ,  559kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11942
replaced with revised version Tue, 19 Mar 2024 17:20:59 GMT   (598kb,D)

Title: Exploring Facial Expression Recognition through Semi-Supervised
  Pretraining and Temporal Modeling
Authors: Jun Yu, Zhihong Wei, Zhongpeng Cai, Gongpeng Zhao, Zerui Zhang, Yongqi
  Wang, Guochen Xie, Jichao Zhu, Wangyuan Zhu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.11942 ,  598kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12009
replaced with revised version Tue, 19 Mar 2024 07:11:28 GMT   (1418kb)

Title: Leveraging Spatial and Semantic Feature Extraction for Skin Cancer
  Diagnosis with Capsule Networks and Graph Neural Networks
Authors: K. P. Santoso, R. V. H. Ginardi, R. A. Sastrowardoyo, F. A. Madany
Categories: cs.CV cs.AI
Comments: This is the first version of our paper, we gladly expect feedback and
  corrections if there is any mistake within our paper
\\ ( https://arxiv.org/abs/2403.12009 ,  1418kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03132
replaced with revised version Mon, 18 Mar 2024 22:18:02 GMT   (7138kb,D)

Title: T-MARS: Improving Visual Representations by Circumventing Text Feature
  Learning
Authors: Pratyush Maini, Sachin Goyal, Zachary C. Lipton, J. Zico Kolter, Aditi
  Raghunathan
Categories: cs.CV cs.CL cs.LG
Comments: Accepted to ICLR 2024. Oral at ICCV Datacomp 2023
\\ ( https://arxiv.org/abs/2307.03132 ,  7138kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12418
replaced with revised version Tue, 19 Mar 2024 02:17:22 GMT   (963kb,D)

Title: HateModerate: Testing Hate Speech Detectors against Content Moderation
  Policies
Authors: Jiangrui Zheng, Xueqing Liu, Guanqun Yang, Mirazul Haque, Xing Qian,
  Ravishka Rathnasuriya, Wei Yang, Girish Budhrani
Categories: cs.SE cs.CL
Comments: NAACL 2024 Finding
\\ ( https://arxiv.org/abs/2307.12418 ,  963kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07968
replaced with revised version Tue, 19 Mar 2024 01:32:19 GMT   (3493kb,D)

Title: Think, Act, and Ask: Open-World Interactive Personalized Robot
  Navigation
Authors: Yinpei Dai, Run Peng, Sikai Li, Joyce Chai
Categories: cs.RO cs.CL cs.HC
Comments: Video URL: https://www.youtube.com/watch?v=rN5S8QIhhQc Code URL:
  https://github.com/sled-group/navchat
\\ ( https://arxiv.org/abs/2310.07968 ,  3493kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16226
replaced with revised version Tue, 19 Mar 2024 14:17:54 GMT   (7367kb,D)

Title: TiC-CLIP: Continual Training of CLIP Models
Authors: Saurabh Garg, Mehrdad Farajtabar, Hadi Pouransari, Raviteja
  Vemulapalli, Sachin Mehta, Oncel Tuzel, Vaishaal Shankar, Fartash Faghri
Categories: cs.CV cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.16226 ,  7367kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10081
replaced with revised version Tue, 19 Mar 2024 17:51:45 GMT   (47309kb,D)

Title: DRESS: Instructing Large Vision-Language Models to Align and Interact
  with Humans via Natural Language Feedback
Authors: Yangyi Chen, Karan Sikka, Michael Cogswell, Heng Ji, Ajay Divakaran
Categories: cs.CV cs.CL cs.LG
Comments: CVPR 2024. The feedback datasets are released at:
  https://huggingface.co/datasets/YangyiYY/LVLM_NLF
\\ ( https://arxiv.org/abs/2311.10081 ,  47309kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09611
replaced with revised version Tue, 19 Mar 2024 16:37:13 GMT   (14458kb,D)

Title: MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training
Authors: Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen
  Zhang, Philipp Dufter, Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers,
  Anton Belyi, Haotian Zhang, Karanjeet Singh, Doug Kang, Ankur Jain, Hongyu
  H\`e, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang, Chong
  Wang, Nan Du, Tao Lei, Sam Wiseman, Mark Lee, Zirui Wang, Ruoming Pang, Peter
  Grasch, Alexander Toshev, Yinfei Yang
Categories: cs.CV cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.09611 ,  14458kb)
------------------------------------------------------------------------------
\\
arXiv:2205.07473
replaced with revised version Tue, 19 Mar 2024 17:25:14 GMT   (19177kb,D)

Title: Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with
  Dual-Phase Optimization
Authors: Ziming Wang, Shuang Lian, Yuhao Zhang, Xiaoxin Cui, Rui Yan and Huajin
  Tang
Categories: cs.NE cs.LG
Comments: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
DOI: 10.1109/TNNLS.2023.3337176
\\ ( https://arxiv.org/abs/2205.07473 ,  19177kb)
------------------------------------------------------------------------------
\\
arXiv:2207.09350 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 05:33:55 GMT   (675kb,D)

Title: Riemannian Stochastic Gradient Method for Nested Composition
  Optimization
Authors: Dewei Zhang and Sam Davanloo Tajbakhsh
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2207.09350 ,  675kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09353
replaced with revised version Tue, 19 Mar 2024 08:06:35 GMT   (193kb,D)

Title: SFPDML: Securer and Faster Privacy-Preserving Distributed Machine
  Learning based on MKTFHE
Authors: Hongxiao Wang, Zoe L. Jiang, Yanmin Zhao, Siu-Ming Yiu, Peng Yang, Man
  Chen, Zejiu Tan, and Bohan Jin
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2211.09353 ,  193kb)
------------------------------------------------------------------------------
\\
arXiv:2211.10777 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 00:53:45 GMT   (4065kb,D)

Title: Non-Coherent Over-the-Air Decentralized Gradient Descent
Authors: Nicolo Michelusi
Categories: eess.SP cs.IT cs.LG math.IT
Comments: Submitted to the IEEE Transactions on Signal Processing
\\ ( https://arxiv.org/abs/2211.10777 ,  4065kb)
------------------------------------------------------------------------------
\\
arXiv:2212.13992
replaced with revised version Tue, 19 Mar 2024 14:51:32 GMT   (813kb,D)

Title: Social-Aware Clustered Federated Learning with Customized Privacy
  Preservation
Authors: Yuntao Wang, Zhou Su, Yanghe Pan, Tom H Luan, Ruidong Li, and Shui Yu
Categories: cs.CR cs.LG
Comments: This paper has been accepted by IEEE/ACM Transactions on Networking
  in March 2024
DOI: 10.1109/TNET.2024.3379439
\\ ( https://arxiv.org/abs/2212.13992 ,  813kb)
------------------------------------------------------------------------------
\\
arXiv:2301.11187 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 15:18:23 GMT   (79kb)

Title: Smoothed Online Learning for Prediction in Piecewise Affine Systems
Authors: Adam Block, Max Simchowitz, Russ Tedrake
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2301.11187 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05430 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 15:22:18 GMT   (73kb,D)

Title: Oracle-Efficient Smoothed Online Learning for Piecewise Continuous
  Decision Making
Authors: Adam Block, Alexander Rakhlin, and Max Simchowitz
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2302.05430 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00673
replaced with revised version Tue, 19 Mar 2024 15:01:36 GMT   (54kb)

Title: Attribute-Efficient PAC Learning of Low-Degree Polynomial Threshold
  Functions with Nasty Noise
Authors: Shiwei Zeng and Jie Shen
Categories: cs.DS cs.LG stat.ML
Comments: ICML 2023. V2 fixed typos
\\ ( https://arxiv.org/abs/2306.00673 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02960
replaced with revised version Tue, 19 Mar 2024 17:35:51 GMT   (1858kb,D)

Title: Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical
  Flow Estimation
Authors: Shubham Negi, Deepika Sharma, Adarsh Kumar Kosta and Kaushik Roy
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.02960 ,  1858kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08929 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 11:25:09 GMT   (7913kb,D)

Title: Active learning for effective Hamiltonian of super-large-scale atomic
  structures
Authors: Xingyue Ma, Hongying Chen, Ri He, Zhanbo Yu, Sergei Prokhorenko, Zheng
  Wen, Zhicheng Zhong, Jorge I\~niguez, L. Bellaiche, Di Wu, and Yurong Yang
Categories: cond-mat.mtrl-sci cs.LG physics.app-ph physics.comp-ph
Comments: 13 pages, 4 figures
\\ ( https://arxiv.org/abs/2307.08929 ,  7913kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08305 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 18:16:00 GMT   (2040kb,D)

Title: Warped geometric information on the optimisation of Euclidean functions
Authors: Marcelo Hartmann, Bernardo Williams, Hanlin Yu, Mark Girolami,
  Alessandro Barp and Arto Klami
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2308.08305 ,  2040kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12547
replaced with revised version Tue, 19 Mar 2024 11:09:12 GMT   (7179kb,D)

Title: PGA: Personalizing Grasping Agents with Single Human-Robot Interaction
Authors: Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Seoyun Yang, Minjoon Jung,
  Byoung-Tak Zhang
Categories: cs.RO cs.CV cs.LG
Comments: 8 pages, under review
\\ ( https://arxiv.org/abs/2310.12547 ,  7179kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18108 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 13:19:22 GMT   (9485kb,D)

Title: Transductive conformal inference with adaptive scores
Authors: Ulysse Gazin, Gilles Blanchard, Etienne Roquain
Categories: stat.ME cs.LG
Comments: 27 Pages, 8 Figures, 1 Table
\\ ( https://arxiv.org/abs/2310.18108 ,  9485kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18430 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 14:34:10 GMT   (1151kb,D)

Title: MCRAGE: Synthetic Healthcare Data for Fairness
Authors: Keira Behal, Jiayi Chen, Caleb Fikes, and Sophia Xiao
Categories: stat.ML cs.LG
Comments: Keywords: synthetic electronic health records, conditional denoising
  diffusion probabilistic model, healthcare AI, tabular data, fairness,
  synthetic data. This paper is the result of work completed at the 2023 Emory
  University Department of Mathematics REU/RET program under the direction of
  Project Advisor Dr. Xi Yuanzhe. This work is sponsored by NSF DMS 2051019
\\ ( https://arxiv.org/abs/2310.18430 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04417
replaced with revised version Tue, 19 Mar 2024 06:31:52 GMT   (2837kb,D)

Title: Evaluating Emerging AI/ML Accelerators: IPU, RDU, and NVIDIA/AMD GPUs
Authors: Hongwu Peng and Caiwen Ding and Tong Geng and Sutanay Choudhury and
  Kevin Barker and Ang Li
Categories: cs.AR cs.DC cs.LG cs.PF
Comments: ICPE 2024 accepted publication
ACM-class: C.4
\\ ( https://arxiv.org/abs/2311.04417 ,  2837kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08255 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 09:49:01 GMT   (13287kb,D)

Title: OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep
  Learning Methods
Authors: Mikhail Kulyabin, Aleksei Zhdanov, Anastasia Nikiforova, Andrey
  Stepichev, Anna Kuznetsova, Mikhail Ronkin, Vasilii Borisov, Alexander
  Bogachev, Sergey Korotkich, Paul A Constable, and Andreas Maier
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.08255 ,  13287kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08876
replaced with revised version Mon, 18 Mar 2024 20:43:07 GMT   (5154kb,D)

Title: Evaluating the Utility of Conformal Prediction Sets for AI-Advised Image
  Labeling
Authors: Dongping Zhang, Angelos Chatzimparmpas, Negar Kamali, and Jessica
  Hullman
Categories: cs.HC cs.CV cs.LG
Comments: 19 pages, 11 figures, 10 tables. Accepted by ACM CHI 2024
\\ ( https://arxiv.org/abs/2401.08876 ,  5154kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11576 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 12:53:24 GMT   (17981kb,D)

Title: Quantum Architecture Search with Unsupervised Representation Learning
Authors: Yize Sun, Zixin Wu, Yunpu Ma, Volker Tresp
Categories: quant-ph cs.LG
Comments: 9 Pages, quantum architecture search, unsupervised representation
  learning
\\ ( https://arxiv.org/abs/2401.11576 ,  17981kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15771 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 16:46:42 GMT   (111kb,D)

Title: Bayesian Nonparametrics Meets Data-Driven Robust Optimization
Authors: Nicola Bariletto, Nhat Ho
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2401.15771 ,  111kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03094
replaced with revised version Tue, 19 Mar 2024 16:34:28 GMT   (7722kb,D)

Title: Cross-Domain Few-Shot Object Detection via Enhanced Open-Set Object
  Detector
Authors: Yuqian Fu, Yu Wang, Yixuan Pan, Lian Huai, Xingyu Qiu, Zeyu Shangguan,
  Tong Liu, Yanwei Fu, Luc Van Gool, Xingqun Jiang
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.03094 ,  7722kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07793 (*cross-listing*)
replaced with revised version Mon, 18 Mar 2024 20:19:43 GMT   (153kb)

Title: Tuning-Free Stochastic Optimization
Authors: Ahmed Khaled and Chi Jin
Categories: math.OC cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.07793 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14817
replaced with revised version Tue, 19 Mar 2024 07:29:20 GMT   (8468kb,D)

Title: Cameras as Rays: Pose Estimation via Ray Diffusion
Authors: Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan,
  Shubham Tulsiani
Categories: cs.CV cs.LG
Comments: In ICLR 2024 (oral). v2: updated references. Project webpage:
  https://jasonyzhang.com/RayDiffusion
\\ ( https://arxiv.org/abs/2402.14817 ,  8468kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16814
replaced with revised version Mon, 18 Mar 2024 19:19:40 GMT   (22kb,D)

Title: Cut Facets and Cube Facets of Lifted Multicut Polytopes
Authors: Lucas Fabian Naumann, Jannik Irmai, Shengxian Zhao, Bjoern Andres
Categories: cs.DM cs.LG
Comments: 10 pages, 5 figures
\\ ( https://arxiv.org/abs/2402.16814 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02628
replaced with revised version Tue, 19 Mar 2024 02:19:52 GMT   (556kb,D)

Title: Interactive Continual Learning: Fast and Slow Thinking
Authors: Biqing Qi, Xingquan Chen, Junqi Gao, Dong Li, Jianxing Liu, Ligang Wu
  and Bowen Zhou
Categories: cs.CV cs.LG
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2403.02628 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07310 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 15:20:31 GMT   (4426kb,D)

Title: How does promoting the minority fraction affect generalization? A
  theoretical study of the one-hidden-layer neural network on group imbalance
Authors: Hongkang Li, Shuai Zhang, Yihua Zhang, Meng Wang, Sijia Liu, Pin-Yu
  Chen
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2403.07310 ,  4426kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10089
replaced with revised version Tue, 19 Mar 2024 07:58:17 GMT   (2560kb,D)

Title: Approximation and bounding techniques for the Fisher-Rao distances
Authors: Frank Nielsen
Categories: cs.IT cs.CV cs.LG math.IT
Comments: 43 pages
\\ ( https://arxiv.org/abs/2403.10089 ,  2560kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10682 (*cross-listing*)
replaced with revised version Tue, 19 Mar 2024 17:37:39 GMT   (2757kb)

Title: Evaluation of GlassNet for physics-informed machine learning of glass
  stability and glass-forming ability
Authors: Sarah I. Allec, Xiaonan Lu, Daniel R. Cassar, Xuan T. Nguyen, Vinay I.
  Hegde, Thiruvillamalai Mahadevan, Miroslava Peterson, Jincheng Du, Brian J.
  Riley, John D. Vienna, James E. Saal
Categories: cond-mat.mtrl-sci cs.LG
\\ ( https://arxiv.org/abs/2403.10682 ,  2757kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12007
replaced with revised version Tue, 19 Mar 2024 16:55:05 GMT   (1822kb,D)

Title: Defining Effective Engagement For Enhancing Cancer Patients' Well-being
  with Mobile Digital Behavior Change Interventions
Authors: Aneta Lisowska, Szymon Wilk, Laura Locati, Mimma Rizzo, Lucia Sacchi,
  Silvana Quaglini, Matteo Terzaghi, Valentina Tibollo, Mor Peleg
Categories: cs.HC cs.LG
\\ ( https://arxiv.org/abs/2403.12007 ,  1822kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
