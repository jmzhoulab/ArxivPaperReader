paper_240318.txt

Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年3月18日 12:50
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu 14 Mar 24 18:00:00 GMT  to  Fri 15 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.09713
Date: Mon, 11 Mar 2024 15:15:27 GMT   (146kb,D)

Title: A Hybrid Intelligence Method for Argument Mining
Authors: Michiel van der Meer, Enrico Liscio, Catholijn M. Jonker, Aske Plaat,
  Piek Vossen, Pradeep K. Murukannaiah
Categories: cs.AI cs.CL cs.HC
Comments: Submitted to JAIR
\\
  Large-scale survey tools enable the collection of citizen feedback in opinion
corpora. Extracting the key arguments from a large and noisy set of opinions
helps in understanding the opinions quickly and accurately. Fully automated
methods can extract arguments but (1) require large labeled datasets that
induce large annotation costs and (2) work well for known viewpoints, but not
for novel points of view. We propose HyEnA, a hybrid (human + AI) method for
extracting arguments from opinionated texts, combining the speed of automated
processing with the understanding and reasoning capabilities of humans. We
evaluate HyEnA on three citizen feedback corpora. We find that, on the one
hand, HyEnA achieves higher coverage and precision than a state-of-the-art
automated method when compared to a common set of diverse opinions, justifying
the need for human insight. On the other hand, HyEnA requires less human effort
and does not compromise quality compared to (fully manual) expert analysis,
demonstrating the benefit of combining human and artificial intelligence.
\\ ( https://arxiv.org/abs/2403.09713 ,  146kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09742
Date: Wed, 13 Mar 2024 20:12:05 GMT   (192kb,D)

Title: A Short Review on Novel Approaches for Maximum Clique Problem: from
  Classical algorithms to Graph Neural Networks and Quantum algorithms
Authors: Raffaele Marino, Lorenzo Buffoni, Bogdan Zavalnij
Categories: cs.AI cond-mat.dis-nn cs.DS cs.LG math.OC quant-ph
Comments: 24 pages
\\
  This manuscript provides a comprehensive review of the Maximum Clique
Problem, a computational problem that involves finding subsets of vertices in a
graph that are all pairwise adjacent to each other. The manuscript covers in a
simple way classical algorithms for solving the problem and includes a review
of recent developments in graph neural networks and quantum algorithms. The
review concludes with benchmarks for testing classical as well as new learning,
and quantum algorithms.
\\ ( https://arxiv.org/abs/2403.09742 ,  192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09806
Date: Thu, 14 Mar 2024 18:53:44 GMT   (5360kb,D)

Title: xLP: Explainable Link Prediction for Master Data Management
Authors: Balaji Ganesan, Matheen Ahmed Pasha, Srinivasa Parkala, Neeraj R
  Singh, Gayatri Mishra, Sumit Bhatia, Hima Patel, Somashekar Naganna, Sameep
  Mehta
Categories: cs.AI
Comments: 8 pages, 4 figures, NeurIPS 2020 Competition and Demonstration Track.
  arXiv admin note: text overlap with arXiv:2012.05516
\\
  Explaining neural model predictions to users requires creativity. Especially
in enterprise applications, where there are costs associated with users' time,
and their trust in the model predictions is critical for adoption. For link
prediction in master data management, we have built a number of explainability
solutions drawing from research in interpretability, fact verification, path
ranking, neuro-symbolic reasoning and self-explaining AI. In this demo, we
present explanations for link prediction in a creative way, to allow users to
choose explanations they are more comfortable with.
\\ ( https://arxiv.org/abs/2403.09806 ,  5360kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09925
Date: Thu, 14 Mar 2024 23:54:19 GMT   (198kb,D)

Title: Surrogate Assisted Monte Carlo Tree Search in Combinatorial Optimization
Authors: Saeid Amiri, Parisa Zehtabi, Danial Dervovic, Michael Cashmore
Categories: cs.AI
Comments: Accepted to the ICAPS Planning and Scheduling for Financial Services
  (FINPLAN) 2023 workshop
\\
  Industries frequently adjust their facilities network by opening new branches
in promising areas and closing branches in areas where they expect low profits.
In this paper, we examine a particular class of facility location problems. Our
objective is to minimize the loss of sales resulting from the removal of
several retail stores. However, estimating sales accurately is expensive and
time-consuming. To overcome this challenge, we leverage Monte Carlo Tree Search
(MCTS) assisted by a surrogate model that computes evaluations faster. Results
suggest that MCTS supported by a fast surrogate function can generate solutions
faster while maintaining a consistent solution compared to MCTS that does not
benefit from the surrogate function.
\\ ( https://arxiv.org/abs/2403.09925 ,  198kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10112
Date: Fri, 15 Mar 2024 08:55:56 GMT   (182kb,D)

Title: Single- and Multi-Agent Private Active Sensing: A Deep Neuroevolution
  Approach
Authors: George Stamatelis, Angelos-Nikolaos Kanatas, Ioannis Asprogerakas, and
  George C. Alexandropoulos
Categories: cs.AI cs.CR cs.MA cs.NE
Comments: 7 pages, 5 figures, accepted at IEEE ICC 2024 (to be presented)
\\
  In this paper, we focus on one centralized and one decentralized problem of
active hypothesis testing in the presence of an eavesdropper. For the
centralized problem including a single legitimate agent, we present a new
framework based on NeuroEvolution (NE), whereas, for the decentralized problem,
we develop a novel NE-based method for solving collaborative multi-agent tasks,
which interestingly maintains all computational benefits of single-agent NE.
The superiority of the proposed EAHT approaches over conventional active
hypothesis testing policies, as well as learning-based methods, is validated
through numerical investigations in an example use case of anomaly detection
over wireless sensor networks.
\\ ( https://arxiv.org/abs/2403.10112 ,  182kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10167
Date: Fri, 15 Mar 2024 10:20:56 GMT   (43kb,D)

Title: Efficient Detection of Exchangeable Factors in Factor Graphs
Authors: Malte Luttermann, Johann Machemer, Marcel Gehrke
Categories: cs.AI cs.DS
Comments: Extended version of paper accepted to the Proceedings of the 37th
  International FLAIRS Conference (FLAIRS-24)
\\
  To allow for tractable probabilistic inference with respect to domain sizes,
lifted probabilistic inference exploits symmetries in probabilistic graphical
models. However, checking whether two factors encode equivalent semantics and
hence are exchangeable is computationally expensive. In this paper, we
efficiently solve the problem of detecting exchangeable factors in a factor
graph. In particular, we introduce the detection of exchangeable factors (DEFT)
algorithm, which allows us to drastically reduce the computational effort for
checking whether two factors are exchangeable in practice. While previous
approaches iterate all $O(n!)$ permutations of a factor's argument list in the
worst case (where $n$ is the number of arguments of the factor), we prove that
DEFT efficiently identifies restrictions to drastically reduce the number of
permutations and validate the efficiency of DEFT in our empirical evaluation.
\\ ( https://arxiv.org/abs/2403.10167 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10171
Date: Fri, 15 Mar 2024 10:27:17 GMT   (1488kb)

Title: AUTONODE: A Neuro-Graphic Self-Learnable Engine for Cognitive GUI
  Automation
Authors: Arkajit Datta, Tushar Verma, Rajat Chawla
Categories: cs.AI cs.CV
\\
  In recent advancements within the domain of Large Language Models (LLMs),
there has been a notable emergence of agents capable of addressing Robotic
Process Automation (RPA) challenges through enhanced cognitive capabilities and
sophisticated reasoning. This development heralds a new era of scalability and
human-like adaptability in goal attainment. In this context, we introduce
AUTONODE (Autonomous User-interface Transformation through Online Neuro-graphic
Operations and Deep Exploration). AUTONODE employs advanced neuro-graphical
techniques to facilitate autonomous navigation and task execution on web
interfaces, thereby obviating the necessity for predefined scripts or manual
intervention. Our engine empowers agents to comprehend and implement complex
workflows, adapting to dynamic web environments with unparalleled efficiency.
Our methodology synergizes cognitive functionalities with robotic automation,
endowing AUTONODE with the ability to learn from experience. We have integrated
an exploratory module, DoRA (Discovery and mapping Operation for graph
Retrieval Agent), which is instrumental in constructing a knowledge graph that
the engine utilizes to optimize its actions and achieve objectives with minimal
supervision. The versatility and efficacy of AUTONODE are demonstrated through
a series of experiments, highlighting its proficiency in managing a diverse
array of web-based tasks, ranging from data extraction to transaction
processing.
\\ ( https://arxiv.org/abs/2403.10171 ,  1488kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10184
Date: Fri, 15 Mar 2024 10:44:27 GMT   (50kb,D)

Title: Lifted Causal Inference in Relational Domains
Authors: Malte Luttermann, Mattis Hartwig, Tanya Braun, Ralf M\"oller, Marcel
  Gehrke
Categories: cs.AI cs.DS
Comments: Accepted to the Proceedings of the 3rd Conference on Causal Learning
  and Reasoning (CLeaR-24)
\\
  Lifted inference exploits symmetries in probabilistic graphical models by
using a representative for indistinguishable objects, thereby speeding up query
answering while maintaining exact answers. Even though lifting is a
well-established technique for the task of probabilistic inference in
relational domains, it has not yet been applied to the task of causal
inference. In this paper, we show how lifting can be applied to efficiently
compute causal effects in relational domains. More specifically, we introduce
parametric causal factor graphs as an extension of parametric factor graphs
incorporating causal knowledge and give a formal semantics of interventions
therein. We further present the lifted causal inference algorithm to compute
causal effects on a lifted level, thereby drastically speeding up causal
inference compared to propositional inference, e.g., in causal Bayesian
networks. In our empirical evaluation, we demonstrate the effectiveness of our
approach.
\\ ( https://arxiv.org/abs/2403.10184 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10249
Date: Fri, 15 Mar 2024 12:37:12 GMT   (1450kb,D)

Title: A Survey on Game Playing Agents and Large Models: Methods, Applications,
  and Challenges
Authors: Xinrun Xu and Yuxin Wang and Chaoyi Xu and Ziluo Ding and Jiechuan
  Jiang and Zhiming Ding and B\"orje F. Karlsson
Categories: cs.AI
Comments: 13 pages, 3 figures
\\
  The swift evolution of Large-scale Models (LMs), either language-focused or
multi-modal, has garnered extensive attention in both academy and industry. But
despite the surge in interest in this rapidly evolving area, there are scarce
systematic reviews on their capabilities and potential in distinct impactful
scenarios. This paper endeavours to help bridge this gap, offering a thorough
examination of the current landscape of LM usage in regards to complex game
playing scenarios and the challenges still open. Here, we seek to
systematically review the existing architectures of LM-based Agents (LMAs) for
games and summarize their commonalities, challenges, and any other insights.
Furthermore, we present our perspective on promising future research avenues
for the advancement of LMs in games. We hope to assist researchers in gaining a
clear understanding of the field and to generate more interest in this highly
impactful research direction. A corresponding resource, continuously updated,
can be found in our GitHub repository.
\\ ( https://arxiv.org/abs/2403.10249 ,  1450kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10299
Date: Fri, 15 Mar 2024 13:42:00 GMT   (1343kb,D)

Title: A Multi-constraint and Multi-objective Allocation Model for Emergency
  Rescue in IoT Environment
Authors: Xinrun Xu and Zhanbiao Lian and Yurong Wu and Manying Lv and Zhiming
  Ding and Jian Yan and Shang Jiang
Categories: cs.AI
Comments: 5 pages, 5 figures, ISCAS 2024
\\
  Emergency relief operations are essential in disaster aftermaths,
necessitating effective resource allocation to minimize negative impacts and
maximize benefits. In prolonged crises or extensive disasters, a systematic,
multi-cycle approach is key for timely and informed decision-making. Leveraging
advancements in IoT and spatio-temporal data analytics, we've developed the
Multi-Objective Shuffled Gray-Wolf Frog Leaping Model (MSGW-FLM). This
multi-constraint, multi-objective resource allocation model has been rigorously
tested against 28 diverse challenges, showing superior performance in
comparison to established models such as NSGA-II, IBEA, and MOEA/D. MSGW-FLM's
effectiveness is particularly notable in complex, multi-cycle emergency rescue
scenarios, which involve numerous constraints and objectives. This model
represents a significant step forward in optimizing resource distribution in
emergency response situations.
\\ ( https://arxiv.org/abs/2403.10299 ,  1343kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10304
Date: Fri, 15 Mar 2024 13:46:36 GMT   (244kb,D)

Title: KIF: A Framework for Virtual Integration of Heterogeneous Knowledge
  Bases using Wikidata
Authors: Guilherme Lima and Marcelo Machado and Elton Soares and Sandro R.
  Fiorini and Raphael Thiago and Leonardo G. Azevedo and Viviane T. da Silva
  and Renato Cerqueira
Categories: cs.AI cs.DB
\\
  We present a knowledge integration framework (called KIF) that uses Wikidata
as a lingua franca to integrate heterogeneous knowledge bases. These can be
triplestores, relational databases, CSV files, etc., which may or may not use
the Wikidata dialect of RDF. KIF leverages Wikidata's data model and vocabulary
plus user-defined mappings to expose a unified view of the integrated bases
while keeping track of the context and provenance of their statements. The
result is a virtual knowledge base which behaves like an "extended Wikidata"
and which can be queried either through an efficient filter interface or using
SPARQL. We present the design and implementation of KIF, discuss how we have
used it to solve a real integration problem in the domain of chemistry
(involving Wikidata, PubChem, and IBM CIRCA), and present experimental results
on the performance and overhead of KIF.
\\ ( https://arxiv.org/abs/2403.10304 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10415
Date: Fri, 15 Mar 2024 15:49:31 GMT   (1781kb,D)

Title: Gradient based Feature Attribution in Explainable AI: A Technical Review
Authors: Yongjie Wang, Tong Zhang, Xu Guo and Zhiqi Shen
Categories: cs.AI
\\
  The surge in black-box AI models has prompted the need to explain the
internal mechanism and justify their reliability, especially in high-stakes
applications, such as healthcare and autonomous driving. Due to the lack of a
rigorous definition of explainable AI (XAI), a plethora of research related to
explainability, interpretability, and transparency has been developed to
explain and analyze the model from various perspectives. Consequently, with an
exhaustive list of papers, it becomes challenging to have a comprehensive
overview of XAI research from all aspects. Considering the popularity of neural
networks in AI research, we narrow our focus to a specific area of XAI
research: gradient based explanations, which can be directly adopted for neural
network models. In this review, we systematically explore gradient based
explanation methods to date and introduce a novel taxonomy to categorize them
into four distinct classes. Then, we present the essence of technique details
in chronological order and underscore the evolution of algorithms. Next, we
introduce both human and quantitative evaluations to measure algorithm
performance. More importantly, we demonstrate the general challenges in XAI and
specific challenges in gradient based explanations. We hope that this survey
can help researchers understand state-of-the-art progress and their
corresponding disadvantages, which could spark their interest in addressing
these issues in future work.
\\ ( https://arxiv.org/abs/2403.10415 ,  1781kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10502
Date: Fri, 15 Mar 2024 17:40:11 GMT   (491kb,D)

Title: Belief Change based on Knowledge Measures
Authors: Umberto Straccia, Giovanni Casini
Categories: cs.AI
Comments: 48 pages, 3 figures, preprint
\\
  Knowledge Measures (KMs) aim at quantifying the amount of
knowledge/information that a knowledge base carries. On the other hand, Belief
Change (BC) is the process of changing beliefs (in our case, in terms of
contraction, expansion and revision) taking into account a new piece of
knowledge, which possibly may be in contradiction with the current belief. We
propose a new quantitative BC framework that is based on KMs by defining belief
change operators that try to minimise, from an information-theoretic point of
view, the surprise that the changed belief carries. To this end, we introduce
the principle of minimal surprise. In particular, our contributions are (i) a
general information-theoretic approach to KMs for which [1] is a special case;
(ii) KM-based BC operators that satisfy the so-called AGM postulates; and (iii)
a characterisation of any BC operator that satisfies the AGM postulates as a
KM-based BC operator, i.e., any BC operator satisfying the AGM postulates can
be encoded within our quantitative BC framework. We also introduce quantitative
measures that account for the information loss of contraction, information gain
of expansion and information change of revision. We also give a succinct look
into the problem of iterated revision, which deals with the application of a
sequence of revision operations in our framework, and also illustrate how one
may build from our KM-based contraction operator also one not satisfying the
(in)famous recovery postulate, by focusing on the so-called severe withdrawal
model as an illustrative example.
\\ ( https://arxiv.org/abs/2403.10502 ,  491kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09674
Date: Sun, 4 Feb 2024 13:21:19 GMT   (841kb)

Title: Navigating the Peril of Generated Alternative Facts: A ChatGPT-4
  Fabricated Omega Variant Case as a Cautionary Tale in Medical Misinformation
Authors: Malik Sallam, Jan Egger, Rainer Roehrig, Behrus Puladi
Categories: cs.CL cs.CY cs.IR cs.LG
\\
  In an era where artificial intelligence (AI) intertwines with medical
research, the delineation of truth becomes increasingly complex. This study
ostensibly examines a purported novel SARS-CoV-2 variant, dubbed the Omega
variant, showcasing 31 unique mutations in the S gene region. However, the real
undercurrent of this narrative is a demonstration of the ease with which AI,
specifically ChatGPT-4, can fabricate convincing yet entirely fictional
scientific data. The so-called Omega variant was identified in a fully
vaccinated, previously infected 35-year-old male presenting with severe
COVID-19 symptoms. Through a detailed, albeit artificial, genomic analysis and
contact tracing, this study mirrors the rigorous methodology of genuine case
reports, thereby setting the stage for a compelling but entirely constructed
narrative. The entire case study was generated by ChatGPT-4, a large language
model by OpenAI. The fabricated Omega variant features an ensemble of
mutations, including N501Y and E484K, known for enhancing ACE2 receptor
affinity, alongside L452R and P681H, ostensibly indicative of immune evasion.
This variant's contrived interaction dynamics - severe symptoms in a vaccinated
individual versus mild ones in unvaccinated contacts - were designed to mimic
real-world complexities, including suggestions of antibody-dependent
enhancement (ADE). While the Omega variant is a product of AI-generated
fiction, the implications of this exercise are real and profound. The ease with
which AI can generate believable but false scientific information, as
illustrated in this case, raises significant concerns about the potential for
misinformation in medicine. This study, therefore, serves as a cautionary tale,
emphasizing the necessity for critical evaluation of sources, especially in an
age where AI tools like ChatGPT are becoming increasingly sophisticated and
widespread in their use.
\\ ( https://arxiv.org/abs/2403.09674 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09676
Date: Wed, 7 Feb 2024 00:21:46 GMT   (206kb)

Title: Unmasking the Shadows of AI: Investigating Deceptive Capabilities in
  Large Language Models
Authors: Linge Guo
Categories: cs.CL cs.AI
Comments: AI deception, Large Language Models, ChatGPT
\\
  This research critically navigates the intricate landscape of AI deception,
concentrating on deceptive behaviours of Large Language Models (LLMs). My
objective is to elucidate this issue, examine the discourse surrounding it, and
subsequently delve into its categorization and ramifications. The essay
initiates with an evaluation of the AI Safety Summit 2023 (ASS) and
introduction of LLMs, emphasising multidimensional biases that underlie their
deceptive behaviours.The literature review covers four types of deception
categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful
Reasoning, along with the social implications and risks they entail. Lastly, I
take an evaluative stance on various aspects related to navigating the
persistent challenges of the deceptive AI. This encompasses considerations of
international collaborative governance, the reconfigured engagement of
individuals with AI, proposal of practical adjustments, and specific elements
of digital education.
\\ ( https://arxiv.org/abs/2403.09676 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09702
Date: Fri, 8 Mar 2024 13:05:44 GMT   (1128kb,D)

Title: Generator-Guided Crowd Reaction Assessment
Authors: Sohom Ghosh, Chung-Chi Chen, Sudip Kumar Naskar
Categories: cs.CL
Comments: Accepted for publication in The ACM Web Conference WWW'24 Companion
  Short Papers Track, May 13 to 17 2024, Singapore, DOI 10.1145/3589335.3651512
ACM-class: I.2.7
\\
  In the realm of social media, understanding and predicting post reach is a
significant challenge. This paper presents a Crowd Reaction AssessMent (CReAM)
task designed to estimate if a given social media post will receive more
reaction than another, a particularly essential task for digital marketers and
content writers. We introduce the Crowd Reaction Estimation Dataset (CRED),
consisting of pairs of tweets from The White House with comparative measures of
retweet count. The proposed Generator-Guided Estimation Approach (GGEA)
leverages generative Large Language Models (LLMs), such as ChatGPT, FLAN-UL2,
and Claude, to guide classification models for making better predictions. Our
results reveal that a fine-tuned FLANG-RoBERTa model, utilizing a cross-encoder
architecture with tweet content and responses generated by Claude, performs
optimally. We further use a T5-based paraphraser to generate paraphrases of a
given post and demonstrate GGEA's ability to predict which post will elicit the
most reactions. We believe this novel application of LLMs provides a
significant advancement in predicting social media post reach.
\\ ( https://arxiv.org/abs/2403.09702 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09703
Date: Fri, 8 Mar 2024 19:07:47 GMT   (1393kb,D)

Title: Concept-aware Data Construction Improves In-context Learning of Language
  Models
Authors: Michal \v{S}tef\'anik, Marek Kadl\v{c}\'ik, Petr Sojka
Categories: cs.CL cs.AI
\\
  Many recent language models (LMs) are capable of in-context learning (ICL),
manifested in the LMs' ability to perform a new task solely from
natural-language instruction. Previous work curating in-context learners
assumes that ICL emerges from a vast over-parametrization or the scale of
multi-task training. However, recent theoretical work attributes the ICL
ability to concept-dependent training data and creates functional in-context
learners even in small-scale, synthetic settings.
  In this work, we practically explore this newly identified axis of ICL
quality. We propose Concept-aware Training (CoAT), a framework for constructing
training scenarios that make it beneficial for the LM to learn to utilize the
analogical reasoning concepts from demonstrations. We find that by using CoAT,
pre-trained transformers can learn to better utilise new latent concepts from
demonstrations and that such ability makes ICL more robust to the functional
deficiencies of the previous models. Finally, we show that concept-aware
in-context learning is more effective for a majority of new tasks when compared
to traditional instruction tuning, resulting in a performance comparable to the
previous in-context learners using magnitudes of more training data.
\\ ( https://arxiv.org/abs/2403.09703 ,  1393kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09704
Date: Fri, 8 Mar 2024 21:26:49 GMT   (1814kb,D)

Title: Alignment Studio: Aligning Large Language Models to Particular
  Contextual Regulations
Authors: Swapnaja Achintalwar, Ioana Baldini, Djallel Bouneffouf, Joan
  Byamugisha, Maria Chang, Pierre Dognin, Eitan Farchi, Ndivhuwo Makondo,
  Aleksandra Mojsilovic, Manish Nagireddy, Karthikeyan Natesan Ramamurthy,
  Inkit Padhi, Orna Raz, Jesus Rios, Prasanna Sattigeri, Moninder Singh,
  Siphiwe Thwala, Rosario A. Uceda-Sosa, Kush R. Varshney
Categories: cs.CL cs.AI cs.LG
Comments: 7 pages, 5 figures
\\
  The alignment of large language models is usually done by model providers to
add or control behaviors that are common or universally understood across use
cases and contexts. In contrast, in this article, we present an approach and
architecture that empowers application developers to tune a model to their
particular values, social norms, laws and other regulations, and orchestrate
between potentially conflicting requirements in context. We lay out three main
components of such an Alignment Studio architecture: Framers, Instructors, and
Auditors that work in concert to control the behavior of a language model. We
illustrate this approach with a running example of aligning a company's
internal-facing enterprise chatbot to its business conduct guidelines.
\\ ( https://arxiv.org/abs/2403.09704 ,  1814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09705
Date: Fri, 8 Mar 2024 23:46:37 GMT   (2042kb,D)

Title: A Novel Nuanced Conversation Evaluation Framework for Large Language
  Models in Mental Health
Authors: Alexander Marrapese, Basem Suleiman, Imdad Ullah, Juno Kim
Categories: cs.CL cs.AI cs.ET
\\
  Understanding the conversation abilities of Large Language Models (LLMs) can
help lead to its more cautious and appropriate deployment. This is especially
important for safety-critical domains like mental health, where someone's life
may depend on the exact wording of a response to an urgent question. In this
paper, we propose a novel framework for evaluating the nuanced conversation
abilities of LLMs. Within it, we develop a series of quantitative metrics
developed from literature on using psychotherapy conversation analysis
literature. While we ensure that our framework and metrics are transferable by
researchers to relevant adjacent domains, we apply them to the mental health
field. We use our framework to evaluate several popular frontier LLMs,
including some GPT and Llama models, through a verified mental health dataset.
Our results show that GPT4 Turbo can perform significantly more similarly to
verified therapists than other selected LLMs. We conduct additional analysis to
examine how LLM conversation performance varies across specific mental health
topics. Our results indicate that GPT4 Turbo performs well in achieving high
correlation with verified therapists in particular topics such as Parenting and
Relationships. We believe our contributions will help researchers develop
better LLMs that, in turn, will more positively support people's lives.
\\ ( https://arxiv.org/abs/2403.09705 ,  2042kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09706
Date: Sat, 9 Mar 2024 01:13:37 GMT   (7709kb,D)

Title: Schema-Aware Multi-Task Learning for Complex Text-to-SQL
Authors: Yangjun Wu and Han Wang
Categories: cs.CL cs.AI cs.DB
Comments: 8pages
\\
  Conventional text-to-SQL parsers are not good at synthesizing complex SQL
queries that involve multiple tables or columns, due to the challenges inherent
in identifying the correct schema items and performing accurate alignment
between question and schema items. To address the above issue, we present a
schema-aware multi-task learning framework (named MTSQL) for complicated SQL
queries. Specifically, we design a schema linking discriminator module to
distinguish the valid question-schema linkings, which explicitly instructs the
encoder by distinctive linking relations to enhance the alignment quality. On
the decoder side, we define 6-type relationships to describe the connections
between tables and columns (e.g., WHERE_TC), and introduce an operator-centric
triple extractor to recognize those associated schema items with the predefined
relationship. Also, we establish a rule set of grammar constraints via the
predicted triples to filter the proper SQL operators and schema items during
the SQL generation. On Spider, a cross-domain challenging text-to-SQL
benchmark, experimental results indicate that MTSQL is more effective than
baselines, especially in extremely hard scenarios. Moreover, further analyses
verify that our approach leads to promising improvements for complicated SQL
queries.
\\ ( https://arxiv.org/abs/2403.09706 ,  7709kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09708
Date: Sat, 9 Mar 2024 19:18:27 GMT   (3652kb)

Title: Institutional-Level Monitoring of Immune Checkpoint Inhibitor IrAEs
  Using a Novel Natural Language Processing Algorithmic Pipeline
Authors: Michael Shapiro, Herut Dor, Anna Gurevich-Shapiro, Tal Etan, Ido Wolf
Categories: cs.CL cs.LG
\\
  Background: Immune checkpoint inhibitors (ICIs) have revolutionized cancer
treatment but can result in severe immune-related adverse events (IrAEs).
Monitoring IrAEs on a large scale is essential for personalized risk profiling
and assisting in treatment decisions.
  Methods: In this study, we conducted an analysis of clinical notes from
patients who received ICIs at the Tel Aviv Sourasky Medical Center. By
employing a Natural Language Processing algorithmic pipeline, we systematically
identified seven common or severe IrAEs. We examined the utilization of
corticosteroids, treatment discontinuation rates following IrAEs, and
constructed survival curves to visualize the occurrence of adverse events
during treatment.
  Results: Our analysis encompassed 108,280 clinical notes associated with
1,635 patients who had undergone ICI therapy. The detected incidence of IrAEs
was consistent with previous reports, exhibiting substantial variation across
different ICIs. Treatment with corticosteroids varied depending on the specific
IrAE, ranging from 17.3% for thyroiditis to 57.4% for myocarditis. Our
algorithm demonstrated high accuracy in identifying IrAEs, as indicated by an
area under the curve (AUC) of 0.89 for each suspected note and F1 scores of
0.87 or higher for five out of the seven IrAEs examined at the patient level.
  Conclusions: This study presents a novel, large-scale monitoring approach
utilizing deep neural networks for IrAEs. Our method provides accurate results,
enhancing understanding of detrimental consequences experienced by ICI-treated
patients. Moreover, it holds potential for monitoring other medications,
enabling comprehensive post-marketing surveillance to identify susceptible
populations and establish personalized drug safety profiles.
\\ ( https://arxiv.org/abs/2403.09708 ,  3652kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09709
Date: Sat, 9 Mar 2024 23:21:17 GMT   (1741kb,D)

Title: Exploratory Data Analysis on Code-mixed Misogynistic Comments
Authors: Sargam Yadav (1), Abhishek Kaushik (1), Kevin McDaid (1) ((1) Dundalk
  Institute of Technology, Dundalk)
Categories: cs.CL
Comments: This paper is accepted in the 16th ISDSI-Global Conference 2023
  https://isdsi2023.iimranchi.ac.in/
\\
  The problems of online hate speech and cyberbullying have significantly
worsened since the increase in popularity of social media platforms such as
YouTube and Twitter (X). Natural Language Processing (NLP) techniques have
proven to provide a great advantage in automatic filtering such toxic content.
Women are disproportionately more likely to be victims of online abuse.
However, there appears to be a lack of studies that tackle misogyny detection
in under-resourced languages. In this short paper, we present a novel dataset
of YouTube comments in mix-code Hinglish collected from YouTube videos which
have been weak labelled as `Misogynistic' and `Non-misogynistic'.
Pre-processing and Exploratory Data Analysis (EDA) techniques have been applied
on the dataset to gain insights on its characteristics. The process has
provided a better understanding of the dataset through sentiment scores, word
clouds, etc.
\\ ( https://arxiv.org/abs/2403.09709 ,  1741kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09712
Date: Mon, 11 Mar 2024 03:42:03 GMT   (606kb,D)

Title: A Knowledge-Injected Curriculum Pretraining Framework for Question
  Answering
Authors: Xin Lin, Tianhuang Su, Zhenya Huang, Shangzi Xue, Haifeng Liu, Enhong
  Chen
Categories: cs.CL cs.AI
Comments: Accepted by WWW 2024
DOI: 10.1145/3589334.3645406
\\
  Knowledge-based question answering (KBQA) is a key task in NLP research, and
also an approach to access the web data and knowledge, which requires
exploiting knowledge graphs (KGs) for reasoning. In the literature, one
promising solution for KBQA is to incorporate the pretrained language model
(LM) with KGs by generating KG-centered pretraining corpus, which has shown its
superiority. However, these methods often depend on specific techniques and
resources to work, which may not always be available and restrict its
application. Moreover, existing methods focus more on improving language
understanding with KGs, while neglect the more important human-like complex
reasoning. To this end, in this paper, we propose a general Knowledge-Injected
Curriculum Pretraining framework (KICP) to achieve comprehensive KG learning
and exploitation for KBQA tasks, which is composed of knowledge injection (KI),
knowledge adaptation (KA) and curriculum reasoning (CR). Specifically, the KI
module first injects knowledge into the LM by generating KG-centered
pretraining corpus, and generalizes the process into three key steps that could
work with different implementations for flexible application. Next, the KA
module learns knowledge from the generated corpus with LM equipped with an
adapter as well as keeps its original natural language understanding ability to
reduce the negative impacts of the difference between the generated and natural
corpus. Last, to enable the LM with complex reasoning, the CR module follows
human reasoning patterns to construct three corpora with increasing
difficulties of reasoning, and further trains the LM from easy to hard in a
curriculum manner. We provide an implementation of the general framework, and
evaluate the proposed KICP on four real-word datasets. The results demonstrate
that our framework can achieve higher performances.
\\ ( https://arxiv.org/abs/2403.09712 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09714
Date: Mon, 11 Mar 2024 16:54:49 GMT   (3014kb,D)

Title: Linguistic Structure Induction from Language Models
Authors: Omar Momen
Categories: cs.CL cs.AI
Comments: Master's Thesis. Supervised by Laura Kallmeyer and David Arps
\\
  Linear sequences of words are implicitly represented in our brains by
hierarchical structures that organize the composition of words in sentences.
Linguists formalize different frameworks to model this hierarchy; two of the
most common syntactic frameworks are Constituency and Dependency. Constituency
represents sentences as nested groups of phrases, while dependency represents a
sentence by assigning relations between its words. Recently, the pursuit of
intelligent machines has produced Language Models (LMs) capable of solving many
language tasks with a human-level performance. Many studies now question
whether LMs implicitly represent syntactic hierarchies. This thesis focuses on
producing constituency and dependency structures from LMs in an unsupervised
setting. I review the critical methods in this field and highlight a line of
work that utilizes a numerical representation for binary constituency trees
(Syntactic Distance). I present a detailed study on StructFormer (SF) (Shen et
al., 2021), which retrofits a transformer encoder architecture with a parser
network to produce constituency and dependency structures. I present six
experiments to analyze and address this field's challenges; experiments include
investigating the effect of repositioning the parser network within the SF
architecture, evaluating subword-based induced trees, and benchmarking the
models developed in the thesis experiments on linguistic tasks. Models
benchmarking is performed by participating in the BabyLM challenge, published
at CoNLL 2023 (Momen et al., 2023). The results of this thesis encourage
further development in the direction of retrofitting transformer-based models
to induce syntactic structures, supported by the acceptable performance of SF
in different experimental settings and the observed limitations that require
innovative solutions to advance the state of syntactic structure induction.
\\ ( https://arxiv.org/abs/2403.09714 ,  3014kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09718
Date: Tue, 12 Mar 2024 07:25:53 GMT   (581kb)

Title: Comprehensive Implementation of TextCNN for Enhanced Collaboration
  between Natural Language Processing and System Recommendation
Authors: Xiaonan Xu, Zheng Xu, Zhipeng Ling, Zhengyu Jin, ShuQian Du
Categories: cs.CL cs.AI
\\
  Natural Language Processing (NLP) is an important branch of artificial
intelligence that studies how to enable computers to understand, process, and
generate human language. Text classification is a fundamental task in NLP,
which aims to classify text into different predefined categories. Text
classification is the most basic and classic task in natural language
processing, and most of the tasks in natural language processing can be
regarded as classification tasks. In recent years, deep learning has achieved
great success in many research fields, and today, it has also become a standard
technology in the field of NLP, which is widely integrated into text
classification tasks. Unlike numbers and images, text processing emphasizes
fine-grained processing ability. Traditional text classification methods
generally require preprocessing the input model's text data. Additionally, they
also need to obtain good sample features through manual annotation and then use
classical machine learning algorithms for classification. Therefore, this paper
analyzes the application status of deep learning in the three core tasks of NLP
(including text representation, word order modeling, and knowledge
representation). This content explores the improvement and synergy achieved
through natural language processing in the context of text classification,
while also taking into account the challenges posed by adversarial techniques
in text generation, text classification, and semantic parsing. An empirical
study on text classification tasks demonstrates the effectiveness of
interactive integration training, particularly in conjunction with TextCNN,
highlighting the significance of these advancements in text classification
augmentation and enhancement.
\\ ( https://arxiv.org/abs/2403.09718 ,  581kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09719
Date: Tue, 12 Mar 2024 08:40:44 GMT   (261kb)

Title: Mevaker: Conclusion Extraction and Allocation Resources for the Hebrew
  Language
Authors: Vitaly Shalumov, Harel Haskey, Yuval Solaz
Categories: cs.CL cs.AI
\\
  In this paper, we introduce summarization MevakerSumm and conclusion
extraction MevakerConc datasets for the Hebrew language based on the State
Comptroller and Ombudsman of Israel reports, along with two auxiliary datasets.
We accompany these datasets with models for conclusion extraction (HeConE,
HeConEspc) and conclusion allocation (HeCross). All of the code, datasets, and
model checkpoints used in this work are publicly available.
\\ ( https://arxiv.org/abs/2403.09719 ,  261kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09720
Date: Tue, 12 Mar 2024 08:49:31 GMT   (5400kb,D)

Title: Fine-tuning vs Prompting, Can Language Models Understand Human Values?
Authors: Pingwei Sun
Categories: cs.CL cs.AI
\\
  Accurately handling the underlying support values in sentences is crucial for
understanding the speaker's tendencies, yet it poses a challenging task in
natural language understanding (NLU). In this article, we explore the potential
of fine-tuning and prompt tuning in this downstream task, using the Human Value
Detection 2023. Additionally, we attempt to validate whether models can
effectively solve the problem based on the knowledge acquired during the
pre-training stage. Simultaneously, our interest lies in the capabilities of
large language models (LLMs) aligned with RLHF in this task, and some
preliminary attempts are presented.
\\ ( https://arxiv.org/abs/2403.09720 ,  5400kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09721
Date: Tue, 12 Mar 2024 08:58:07 GMT   (21663kb,D)

Title: A Semantic Mention Graph Augmented Model for Document-Level Event
  Argument Extraction
Authors: Jian Zhang, Changlin Yang, Haiping Zhu, Qika Lin, Fangzhi Xu and Jun
  Liu
Categories: cs.CL cs.AI
Comments: Accepted By Coling 2024
\\
  Document-level Event Argument Extraction (DEAE) aims to identify arguments
and their specific roles from an unstructured document. The advanced approaches
on DEAE utilize prompt-based methods to guide pre-trained language models
(PLMs) in extracting arguments from input documents. They mainly concentrate on
establishing relations between triggers and entity mentions within documents,
leaving two unresolved problems: a) independent modeling of entity mentions; b)
document-prompt isolation. To this end, we propose a semantic mention Graph
Augmented Model (GAM) to address these two problems in this paper. Firstly, GAM
constructs a semantic mention graph that captures relations within and between
documents and prompts, encompassing co-existence, co-reference and co-type
relations. Furthermore, we introduce an ensembled graph transformer module to
address mentions and their three semantic relations effectively. Later, the
graph-augmented encoder-decoder module incorporates the relation-specific graph
into the input embedding of PLMs and optimizes the encoder section with
topology information, enhancing the relations comprehensively. Extensive
experiments on the RAMS and WikiEvents datasets demonstrate the effectiveness
of our approach, surpassing baseline methods and achieving a new
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2403.09721 ,  21663kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09722
Date: Tue, 12 Mar 2024 09:03:44 GMT   (407kb)

Title: Prediction of readmission of patients by extracting biomedical concepts
  from clinical texts
Authors: Rasoul Samani, Fahime Shahrokh, Mohammad Dehghani
Categories: cs.CL cs.AI
\\
  Today, the existence of a vast amount of electronic health data has created
potential capacities for conducting studies aiming to improve the medical
services provided to patients and reduce the costs of the healthcare system.
One of the topics that has been receiving attention in the field of medicine in
recent years is the identification of patients who are likely to be
re-hospitalized shortly after being discharged from the hospital. This
identification can help doctors choose appropriate treatment methods, thereby
reducing the rate of patient re-hospitalization and resulting in effective
treatment cost reduction. In this study, the prediction of patient
re-hospitalization using text mining approaches and the processing of discharge
report texts in the patient's electronic file has been discussed. To this end,
the performance of various machine learning models has been evaluated using two
approaches: bag of word and bag of concept, in the process of predicting
patient readmission. Comparing the efficiency of these approaches has shown the
superiority of the random forest model and the bag of concept approach over
other machine learning models and approaches. This research has achieved the
highest score in predicting the probability of patient re-hospitalization, with
a recall score of 68.9%, compared to similar works that have utilized machine
learning models in this field.
\\ ( https://arxiv.org/abs/2403.09722 ,  407kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09724
Date: Tue, 12 Mar 2024 17:07:53 GMT   (548kb,D)

Title: ClaimVer: Explainable Claim-Level Verification and Evidence Attribution
  of Text Through Knowledge Graphs
Authors: Preetam Prabhu Srikar Dammu, Himanshu Naidu, Mouly Dewan, YoungMin
  Kim, Tanya Roosta, Aman Chadha, Chirag Shah
Categories: cs.CL cs.CY cs.LG
\\
  In the midst of widespread misinformation and disinformation through social
media and the proliferation of AI-generated texts, it has become increasingly
difficult for people to validate and trust information they encounter. Many
fact-checking approaches and tools have been developed, but they often lack
appropriate explainability or granularity to be useful in various contexts. A
text validation method that is easy to use, accessible, and can perform
fine-grained evidence attribution has become crucial. More importantly,
building user trust in such a method requires presenting the rationale behind
each prediction, as research shows this significantly influences people's
belief in automated systems. It is also paramount to localize and bring users'
attention to the specific problematic content, instead of providing simple
blanket labels. In this paper, we present $\textit{ClaimVer, a human-centric
framework}$ tailored to meet users' informational and verification needs by
generating rich annotations and thereby reducing cognitive load. Designed to
deliver comprehensive evaluations of texts, it highlights each claim, verifies
it against a trusted knowledge graph (KG), presents the evidence, and provides
succinct, clear explanations for each claim prediction. Finally, our framework
introduces an attribution score, enhancing applicability across a wide range of
downstream tasks.
\\ ( https://arxiv.org/abs/2403.09724 ,  548kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09725
Date: Tue, 12 Mar 2024 17:27:22 GMT   (406kb,D)

Title: RAD-PHI2: Instruction Tuning PHI-2 for Radiology
Authors: Mercy Ranjit, Gopinath Ganapathy, Shaury Srivastav, Tanuja Ganu,
  Srujana Oruganti
Categories: cs.CL cs.AI
ACM-class: J.3
\\
  Small Language Models (SLMs) have shown remarkable performance in general
domain language understanding, reasoning and coding tasks, but their
capabilities in the medical domain, particularly concerning radiology text, is
less explored. In this study, we investigate the application of SLMs for
general radiology knowledge specifically question answering related to
understanding of symptoms, radiological appearances of findings, differential
diagnosis, assessing prognosis, and suggesting treatments w.r.t diseases
pertaining to different organ systems. Additionally, we explore the utility of
SLMs in handling text-related tasks with respect to radiology reports within
AI-driven radiology workflows. We fine-tune Phi-2, a SLM with 2.7 billion
parameters using high-quality educational content from Radiopaedia, a
collaborative online radiology resource. The resulting language model,
RadPhi-2-Base, exhibits the ability to address general radiology queries across
various systems (e.g., chest, cardiac). Furthermore, we investigate Phi-2 for
instruction tuning, enabling it to perform specific tasks. By fine-tuning Phi-2
on both general domain tasks and radiology-specific tasks related to chest
X-ray reports, we create Rad-Phi2. Our empirical results reveal that Rad-Phi2
Base and Rad-Phi2 perform comparably or even outperform larger models such as
Mistral-7B-Instruct-v0.2 and GPT-4 providing concise and precise answers. In
summary, our work demonstrates the feasibility and effectiveness of utilizing
SLMs in radiology workflows both for knowledge related queries as well as for
performing specific tasks related to radiology reports thereby opening up new
avenues for enhancing the quality and efficiency of radiology practice.
\\ ( https://arxiv.org/abs/2403.09725 ,  406kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09727
Date: Tue, 12 Mar 2024 21:06:31 GMT   (4585kb,D)

Title: Investigating the performance of Retrieval-Augmented Generation and
  fine-tuning for the development of AI-driven knowledge-based systems
Authors: Robert Lakatos, Peter Pollner, Andras Hajdu, Tamas Joo
Categories: cs.CL cs.AI cs.LG
\\
  The development of generative large language models (G-LLM) opened up new
opportunities for the development of new types of knowledge-based systems
similar to ChatGPT, Bing, or Gemini. Fine-tuning (FN) and Retrieval-Augmented
Generation (RAG) are the techniques that can be used to implement domain
adaptation for the development of G-LLM-based knowledge systems. In our study,
using ROUGE, BLEU, METEOR scores, and cosine similarity, we compare and examine
the performance of RAG and FN for the GPT-J-6B, OPT-6.7B, LlaMA, LlaMA-2
language models. Based on measurements shown on different datasets, we
demonstrate that RAG-based constructions are more efficient than models
produced with FN. We point out that connecting RAG and FN is not trivial,
because connecting FN models with RAG can cause a decrease in performance.
Furthermore, we outline a simple RAG-based architecture which, on average,
outperforms the FN models by 16% in terms of the ROGUE score, 15% in the case
of the BLEU score, and 53% based on the cosine similarity. This shows the
significant advantage of RAG over FN in terms of hallucination, which is not
offset by the fact that the average 8% better METEOR score of FN models
indicates greater creativity compared to RAG.
\\ ( https://arxiv.org/abs/2403.09727 ,  4585kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09728
Date: Tue, 12 Mar 2024 21:54:34 GMT   (807kb,D)

Title: Simulating Weighted Automata over Sequences and Trees with Transformers
Authors: Michael Rizvi, Maude Lizaire, Clara Lacroce and Guillaume Rabusseau
Categories: cs.CL cs.AI cs.CC
\\
  Transformers are ubiquitous models in the natural language processing (NLP)
community and have shown impressive empirical successes in the past few years.
However, little is understood about how they reason and the limits of their
computational capabilities. These models do not process data sequentially, and
yet outperform sequential neural models such as RNNs. Recent work has shown
that these models can compactly simulate the sequential reasoning abilities of
deterministic finite automata (DFAs). This leads to the following question: can
transformers simulate the reasoning of more complex finite state machines? In
this work, we show that transformers can simulate weighted finite automata
(WFAs), a class of models which subsumes DFAs, as well as weighted tree
automata (WTA), a generalization of weighted automata to tree structured
inputs. We prove these claims formally and provide upper bounds on the sizes of
the transformer models needed as a function of the number of states the target
automata. Empirically, we perform synthetic experiments showing that
transformers are able to learn these compact solutions via standard
gradient-based training.
\\ ( https://arxiv.org/abs/2403.09728 ,  807kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09732
Date: Wed, 13 Mar 2024 02:32:41 GMT   (153kb,D)

Title: PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with
  Cross-consistency
Authors: Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru
  Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao
Categories: cs.CL cs.AI
\\
  Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large
language models (LLM) on in-context learning, achieving significant results.
Nevertheless, they face challenges when dealing with verbose database
information and complex user intentions. This paper presents a two-stage
framework to enhance the performance of current LLM-based natural language to
SQL systems. We first introduce a novel prompt representation, called
reference-enhanced representation, which includes schema information and
randomly sampled cell values from tables to instruct LLMs in generating SQL
queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot
demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After
that, the mentioned entities in PreSQL are parsed to conduct schema linking,
which can significantly compact the useful information. In the second stage,
with the linked schema, we simplify the prompt's schema information and
instruct the LLM to produce the final SQL. Finally, as the post-refinement
module, we propose using cross-consistency across different LLMs rather than
self-consistency within a particular LLM. Our methods achieve new SOTA results
on the Spider benchmark, with an execution accuracy of 87.6%.
\\ ( https://arxiv.org/abs/2403.09732 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09733
Date: Wed, 13 Mar 2024 07:52:31 GMT   (404kb,D)

Title: OverleafCopilot: Empowering Academic Writing in Overleaf with Large
  Language Models
Authors: Haomin Wen, Zhenjie Wei, Yan Lin, Jiyuan Wang, Yuxuan Liang, Huaiyu
  Wan
Categories: cs.CL cs.AI
\\
  The rapid development of Large Language Models (LLMs) has facilitated a
variety of applications from different domains. In this technical report, we
explore the integration of LLMs and the popular academic writing tool,
Overleaf, to enhance the efficiency and quality of academic writing. To achieve
the above goal, there are three challenges: i) including seamless interaction
between Overleaf and LLMs, ii) establishing reliable communication with the LLM
provider, and iii) ensuring user privacy. To address these challenges, we
present OverleafCopilot, the first-ever tool (i.e., a browser extension) that
seamlessly integrates LLMs and Overleaf, enabling researchers to leverage the
power of LLMs while writing papers. Specifically, we first propose an effective
framework to bridge LLMs and Overleaf. Then, we developed PromptGenius, a
website for researchers to easily find and share high-quality up-to-date
prompts. Thirdly, we propose an agent command system to help researchers
quickly build their customizable agents. OverleafCopilot
(https://chromewebstore.google.com/detail/overleaf-copilot/eoadabdpninlhkkbhngoddfjianhlghb
) has been on the Chrome Extension Store, which now serves thousands of
researchers. Additionally, the code of PromptGenius is released at
https://github.com/wenhaomin/ChatGPT-PromptGenius. We believe our work has the
potential to revolutionize academic writing practices, empowering researchers
to produce higher-quality papers in less time.
\\ ( https://arxiv.org/abs/2403.09733 ,  404kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09734
Date: Wed, 13 Mar 2024 09:48:13 GMT   (2082kb,D)

Title: Do Large Language Models Solve ARC Visual Analogies Like People Do?
Authors: Gustaw Opie{\l}ka, Hannes Rosenbusch, Veerle Vijverberg, Claire E.
  Stevenson
Categories: cs.CL cs.AI
\\
  The Abstraction Reasoning Corpus (ARC) is a visual analogical reasoning test
designed for humans and machines (Chollet, 2019). We compared human and large
language model (LLM) performance on a new child-friendly set of ARC items.
Results show that both children and adults outperform most LLMs on these tasks.
Error analysis revealed a similar "fallback" solution strategy in LLMs and
young children, where part of the analogy is simply copied. In addition, we
found two other error types, one based on seemingly grasping key concepts
(e.g., Inside-Outside) and the other based on simple combinations of analogy
input matrices. On the whole, "concept" errors were more common in humans, and
"matrix" errors were more common in LLMs. This study sheds new light on LLM
reasoning ability and the extent to which we can use error analyses and
comparisons with human development to understand how LLMs solve visual
analogies.
\\ ( https://arxiv.org/abs/2403.09734 ,  2082kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09738
Date: Wed, 13 Mar 2024 18:16:21 GMT   (1106kb,D)

Title: Evaluating Large Language Models as Generative User Simulators for
  Conversational Recommendation
Authors: Se-eun Yoon, Zhankui He, Jessica Maria Echterhoff, Julian McAuley
Categories: cs.CL cs.AI cs.IR
\\
  Synthetic users are cost-effective proxies for real users in the evaluation
of conversational recommender systems. Large language models show promise in
simulating human-like behavior, raising the question of their ability to
represent a diverse population of users. We introduce a new protocol to measure
the degree to which language models can accurately emulate human behavior in
conversational recommendation. This protocol is comprised of five tasks, each
designed to evaluate a key property that a synthetic user should exhibit:
choosing which items to talk about, expressing binary preferences, expressing
open-ended preferences, requesting recommendations, and giving feedback.
Through evaluation of baseline simulators, we demonstrate these tasks
effectively reveal deviations of language models from human behavior, and offer
insights on how to reduce the deviations with model selection and prompting
strategies.
\\ ( https://arxiv.org/abs/2403.09738 ,  1106kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09743
Date: Wed, 13 Mar 2024 21:39:39 GMT   (330kb)

Title: The Human Factor in Detecting Errors of Large Language Models: A
  Systematic Literature Review and Future Research Directions
Authors: Christian A. Schiller
Categories: cs.CL cs.AI cs.LG
Comments: 21 papers analysed and synthesized in detail from a total search
  result size of 594 (raw results) / 61 (scanned) / 28 (selected)
\\
  The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for
Artificial Intelligence, introducing Large Language Models (LLMs) to the
mainstream and setting new records in user adoption. LLMs, particularly
ChatGPT, trained on extensive internet data, demonstrate remarkable
conversational capabilities across various domains, suggesting a significant
impact on the workforce. However, these models are susceptible to errors -
"hallucinations" and omissions, generating incorrect or incomplete information.
This poses risks especially in contexts where accuracy is crucial, such as
legal compliance, medicine or fine-grained process frameworks.
  There are both technical and human solutions to cope with this isse. This
paper explores the human factors that enable users to detect errors in LLM
outputs, a critical component in mitigating risks associated with their use in
professional settings. Understanding these factors is essential for
organizations aiming to leverage LLM technology efficiently, guiding targeted
training and deployment strategies to enhance error detection by users. This
approach not only aims to optimize the use of LLMs but also to prevent
potential downstream issues stemming from reliance on inaccurate model
responses. The research emphasizes the balance between technological
advancement and human insight in maximizing the benefits of LLMs while
minimizing the risks, particularly in areas where precision is paramount.
  This paper performs a systematic literature research on this research topic,
analyses and synthesizes the findings, and outlines future research directions.
Literature selection cut-off date is January 11th 2024.
\\ ( https://arxiv.org/abs/2403.09743 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09744
Date: Wed, 13 Mar 2024 23:14:35 GMT   (239kb,D)

Title: Evaluating the Application of Large Language Models to Generate Feedback
  in Programming Education
Authors: Sven Jacobs and Steffen Jaschke
Categories: cs.CL cs.AI cs.CY cs.HC
Comments: accepted at IEEE Global Engineering Education Conference 2024, Kos,
  Greece
\\
  This study investigates the application of large language models,
specifically GPT-4, to enhance programming education. The research outlines the
design of a web application that uses GPT-4 to provide feedback on programming
tasks, without giving away the solution. A web application for working on
programming tasks was developed for the study and evaluated with 51 students
over the course of one semester. The results show that most of the feedback
generated by GPT-4 effectively addressed code errors. However, challenges with
incorrect suggestions and hallucinated issues indicate the need for further
improvements.
\\ ( https://arxiv.org/abs/2403.09744 ,  239kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09747
Date: Thu, 14 Mar 2024 00:35:39 GMT   (1092kb,D)

Title: Re-Search for The Truth: Multi-round Retrieval-augmented Large Language
  Models are Strong Fake News Detectors
Authors: Guanghua Li, Wensheng Lu, Wei Zhang, Defu Lian, Kezhong Lu, Rui Mao,
  Kai Shu, Hao Liao
Categories: cs.CL cs.AI
\\
  The proliferation of fake news has had far-reaching implications on politics,
the economy, and society at large. While Fake news detection methods have been
employed to mitigate this issue, they primarily depend on two essential
elements: the quality and relevance of the evidence, and the effectiveness of
the verdict prediction mechanism. Traditional methods, which often source
information from static repositories like Wikipedia, are limited by outdated or
incomplete data, particularly for emerging or rare claims. Large Language
Models (LLMs), known for their remarkable reasoning and generative
capabilities, introduce a new frontier for fake news detection. However, like
traditional methods, LLM-based solutions also grapple with the limitations of
stale and long-tail knowledge. Additionally, retrieval-enhanced LLMs frequently
struggle with issues such as low-quality evidence retrieval and context length
constraints. To address these challenges, we introduce a novel,
retrieval-augmented LLMs framework--the first of its kind to automatically and
strategically extract key evidence from web sources for claim verification.
Employing a multi-round retrieval strategy, our framework ensures the
acquisition of sufficient, relevant evidence, thereby enhancing performance.
Comprehensive experiments across three real-world datasets validate the
framework's superiority over existing methods. Importantly, our model not only
delivers accurate verdicts but also offers human-readable explanations to
improve result interpretability.
\\ ( https://arxiv.org/abs/2403.09747 ,  1092kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09750
Date: Thu, 14 Mar 2024 05:34:35 GMT   (7818kb,D)

Title: Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge
  in Datasets and Large Language Models
Authors: Zhuoqun Li, Hongyu Lin, Yaojie Lu, Hao Xiang, Xianpei Han, Le Sun
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024 as a short paper
\\
  Declarative knowledge and procedural knowledge are two key parts in
meta-cognitive theory, and these two hold significant importance in
pre-training and inference of LLMs. However, a comprehensive analysis comparing
these two types of knowledge is lacking, primarily due to challenges in
definition, probing and quantitative assessment. In this paper, we explore from
a new perspective by providing ground-truth knowledge for LLMs and evaluating
the effective score. Through extensive experiments with widely-used datasets
and models, we get conclusions: (1) In most tasks, benefits from declarative
knowledge are greater than those from procedural knowledge. (2) Profits of
procedural knowledge are larger than declarative knowledge only in reasoning
tasks with simple logic. (3) As pre-training progresses and size increases,
model ability to utilize both kinds of knowledge significantly improves, but in
different speed. We do detailed analysis for the findings and this can provide
primary guidance for evaluation and enhancement of large language models.
\\ ( https://arxiv.org/abs/2403.09750 ,  7818kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09762
Date: Thu, 14 Mar 2024 15:58:13 GMT   (25kb,D)

Title: Emotional Intelligence Through Artificial Intelligence : NLP and Deep
  Learning in the Analysis of Healthcare Texts
Authors: Prashant Kumar Nag, Amit Bhagat, R. Vishnu Priya, Deepak kumar Khare
Categories: cs.CL cs.AI cs.HC cs.LG cs.NE
\\
  This manuscript presents a methodical examination of the utilization of
Artificial Intelligence in the assessment of emotions in texts related to
healthcare, with a particular focus on the incorporation of Natural Language
Processing and deep learning technologies. We scrutinize numerous research
studies that employ AI to augment sentiment analysis, categorize emotions, and
forecast patient outcomes based on textual information derived from clinical
narratives, patient feedback on medications, and online health discussions. The
review demonstrates noteworthy progress in the precision of algorithms used for
sentiment classification, the prognostic capabilities of AI models for
neurodegenerative diseases, and the creation of AI-powered systems that offer
support in clinical decision-making. Remarkably, the utilization of AI
applications has exhibited an enhancement in personalized therapy plans by
integrating patient sentiment and contributing to the early identification of
mental health disorders. There persist challenges, which encompass ensuring the
ethical application of AI, safeguarding patient confidentiality, and addressing
potential biases in algorithmic procedures. Nevertheless, the potential of AI
to revolutionize healthcare practices is unmistakable, offering a future where
healthcare is not only more knowledgeable and efficient but also more
empathetic and centered around the needs of patients. This investigation
underscores the transformative influence of AI on healthcare, delivering a
comprehensive comprehension of its role in examining emotional content in
healthcare texts and highlighting the trajectory towards a more compassionate
approach to patient care. The findings advocate for a harmonious synergy
between AI's analytical capabilities and the human aspects of healthcare.
\\ ( https://arxiv.org/abs/2403.09762 ,  25kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09832
Date: Thu, 14 Mar 2024 19:39:10 GMT   (2500kb,D)

Title: Scaling Behavior of Machine Translation with Large Language Models under
  Prompt Injection Attacks
Authors: Zhifan Sun and Antonio Valerio Miceli-Barone
Categories: cs.CL
Comments: 15 pages, 18 figures, First Workshop on the Scaling Behavior of Large
  Language Models (SCALE-LLM 2024)
\\
  Large Language Models (LLMs) are increasingly becoming the preferred
foundation platforms for many Natural Language Processing tasks such as Machine
Translation, owing to their quality often comparable to or better than
task-specific models, and the simplicity of specifying the task through natural
language instructions or in-context examples. Their generality, however, opens
them up to subversion by end users who may embed into their requests
instructions that cause the model to behave in unauthorized and possibly unsafe
ways. In this work we study these Prompt Injection Attacks (PIAs) on multiple
families of LLMs on a Machine Translation task, focusing on the effects of
model size on the attack success rates. We introduce a new benchmark data set
and we discover that on multiple language pairs and injected prompts written in
English, larger models under certain conditions may become more susceptible to
successful attacks, an instance of the Inverse Scaling phenomenon (McKenzie et
al., 2023). To our knowledge, this is the first work to study non-trivial LLM
scaling behaviour in a multi-lingual setting.
\\ ( https://arxiv.org/abs/2403.09832 ,  2500kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09849
Date: Thu, 14 Mar 2024 20:17:10 GMT   (29kb,D)

Title: Self-Consistency Boosts Calibration for Math Reasoning
Authors: Ante Wang, Linfeng Song, Ye Tian, Baolin Peng, Lifeng Jin, Haitao Mi,
  Jinsong Su and Dong Yu
Categories: cs.CL cs.AI
\\
  Calibration, which establishes the correlation between accuracy and model
confidence, is important for LLM development. We design three off-the-shelf
calibration methods based on self-consistency (Wang et al., 2022) for math
reasoning tasks. Evaluation on two popular benchmarks (GSM8K and MathQA) using
strong open-source LLMs (Mistral and LLaMA2), our methods better bridge model
confidence and accuracy than existing methods based on p(True) (Kadavath et
al., 2022) or logit (Kadavath et al., 2022).
\\ ( https://arxiv.org/abs/2403.09849 ,  29kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09858
Date: Thu, 14 Mar 2024 20:39:26 GMT   (3228kb,D)

Title: FakeWatch: A Framework for Detecting Fake News to Ensure Credible
  Elections
Authors: Shaina Raza, Tahniat Khan, Drai Paulen-Patterson, Veronica Chatrath,
  Mizanur Rahman, Oluwanifemi Bamgbose
Categories: cs.CL
Comments: arXiv admin note: substantial text overlap with arXiv:2312.03730
\\
  In today's technologically driven world, the rapid spread of fake news,
particularly during critical events like elections, poses a growing threat to
the integrity of information. To tackle this challenge head-on, we introduce
FakeWatch, a comprehensive framework carefully designed to detect fake news.
Leveraging a newly curated dataset of North American election-related news
articles, we construct robust classification models. Our framework integrates a
model hub comprising of both traditional machine learning (ML) techniques and
cutting-edge Language Models (LMs) to discern fake news effectively. Our
overarching objective is to provide the research community with adaptable and
precise classification models adept at identifying the ever-evolving landscape
of misinformation. Quantitative evaluations of fake news classifiers on our
dataset reveal that, while state-of-the-art LMs exhibit a slight edge over
traditional ML models, classical models remain competitive due to their balance
of accuracy and computational efficiency. Additionally, qualitative analyses
shed light on patterns within fake news articles. This research lays the
groundwork for future endeavors aimed at combating misinformation, particularly
concerning electoral processes. We provide our labeled data and model publicly
for use and reproducibility.
\\ ( https://arxiv.org/abs/2403.09858 ,  3228kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09887
Date: Thu, 14 Mar 2024 21:44:48 GMT   (2551kb,D)

Title: Sabi\'a-2: A New Generation of Portuguese Large Language Models
Authors: Thales Sales Almeida, Hugo Abonizio, Rodrigo Nogueira and Ramon Pires
Categories: cs.CL cs.AI
\\
  We introduce Sabi\'a-2, a family of large language models trained on
Portuguese texts. The models are evaluated on a diverse range of exams,
including entry-level tests for Brazilian universities, professional
certification exams, and graduate-level exams for various disciplines such as
accounting, economics, engineering, law and medicine. Our results reveal that
our best model so far, Sabi\'a-2 Medium, matches or surpasses GPT-4's
performance in 23 out of 64 exams and outperforms GPT-3.5 in 58 out of 64
exams. Notably, specialization has a significant impact on a model's
performance without the need to increase its size, allowing us to offer
Sabi\'a-2 Medium at a price per token that is 10 times cheaper than GPT-4.
Finally, we identified that math and coding are key abilities that need
improvement.
\\ ( https://arxiv.org/abs/2403.09887 ,  2551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09891
Date: Thu, 14 Mar 2024 21:52:26 GMT   (830kb,D)

Title: Fisher Mask Nodes for Language Model Merging
Authors: Thennal D K, Ganesh Nathan, Suchithra M S
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-COLING 2024
\\
  Fine-tuning pre-trained models provides significant advantages in downstream
performance. The ubiquitous nature of pre-trained models such as BERT and its
derivatives in natural language processing has also led to a proliferation of
task-specific fine-tuned models. As these models typically only perform one
task well, additional training or ensembling is required in multi-task
scenarios. The growing field of model merging provides a solution, dealing with
the challenge of combining multiple task-specific models into a single
multi-task model. In this study, we introduce a novel model merging method for
Transformers, combining insights from previous work in Fisher-weighted
averaging and the use of Fisher information in model pruning. Utilizing the
Fisher information of mask nodes within the Transformer architecture, we devise
a computationally efficient weighted-averaging scheme. Our method exhibits a
regular and significant performance increase across various models in the BERT
family, outperforming full-scale Fisher-weighted averaging in a fraction of the
computational cost, with baseline performance improvements of up to +6.5 and a
speedup of 57.4x. Our results prove the potential of our method in current
multi-task learning environments and suggest its scalability and adaptability
to new model architectures and learning scenarios.
\\ ( https://arxiv.org/abs/2403.09891 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09892
Date: Thu, 14 Mar 2024 21:55:17 GMT   (997kb,D)

Title: Geographically-Informed Language Identification
Authors: Jonathan Dunn and Lane Edwards-Brown
Categories: cs.CL
\\
  This paper develops an approach to language identification in which the set
of languages considered by the model depends on the geographic origin of the
text in question. Given that many digital corpora can be geo-referenced at the
country level, this paper formulates 16 region-specific models, each of which
contains the languages expected to appear in countries within that region.
These regional models also each include 31 widely-spoken international
languages in order to ensure coverage of these linguae francae regardless of
location. An upstream evaluation using traditional language identification
testing data shows an improvement in f-score ranging from 1.7 points (Southeast
Asia) to as much as 10.4 points (North Africa). A downstream evaluation on
social media data shows that this improved performance has a significant impact
on the language labels which are applied to large real-world corpora. The
result is a highly-accurate model that covers 916 languages at a sample size of
50 characters, the performance improved by incorporating geographic information
into the model.
\\ ( https://arxiv.org/abs/2403.09892 ,  997kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09919
Date: Thu, 14 Mar 2024 23:40:56 GMT   (590kb,D)

Title: Recurrent Drafter for Fast Speculative Decoding in Large Language Models
Authors: Aonan Zhang, Chong Wang, Yi Wang, Xuanyu Zhang, Yunfei Cheng
Categories: cs.CL cs.LG
Comments: 11 pages, 6 figures
\\
  In this paper, we introduce an improved approach of speculative decoding
aimed at enhancing the efficiency of serving large language models. Our method
capitalizes on the strengths of two established techniques: the classic
two-model speculative decoding approach, and the more recent single-model
approach, Medusa. Drawing inspiration from Medusa, our approach adopts a
single-model strategy for speculative decoding. However, our method
distinguishes itself by employing a single, lightweight draft head with a
recurrent dependency design, akin in essence to the small, draft model uses in
classic speculative decoding, but without the complexities of the full
transformer architecture. And because of the recurrent dependency, we can use
beam search to swiftly filter out undesired candidates with the draft head. The
outcome is a method that combines the simplicity of single-model design and
avoids the need to create a data-dependent tree attention structure only for
inference in Medusa. We empirically demonstrate the effectiveness of the
proposed method on several popular open source language models, along with a
comprehensive analysis of the trade-offs involved in adopting this approach.
\\ ( https://arxiv.org/abs/2403.09919 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09963
Date: Fri, 15 Mar 2024 02:04:35 GMT   (292kb,D)

Title: Take Care of Your Prompt Bias! Investigating and Mitigating Prompt Bias
  in Factual Knowledge Extraction
Authors: Ziyang Xu, Keqin Peng, Liang Ding, Dacheng Tao, Xiliang Lu
Categories: cs.CL cs.AI cs.IR
Comments: Accepted by COLING 2024
\\
  Recent research shows that pre-trained language models (PLMs) suffer from
"prompt bias" in factual knowledge extraction, i.e., prompts tend to introduce
biases toward specific labels. However, the extent and impact of prompt bias
within the model remain underexplored. In response, this paper quantifies the
bias with various types of prompts and assesses their impact on different
benchmarks. We show that: 1) all prompts in the experiments exhibit
non-negligible bias, with gradient-based prompts like AutoPrompt and OptiPrompt
displaying significantly higher levels of bias; 2) prompt bias can amplify
benchmark accuracy unreasonably by overfitting the test datasets, especially on
imbalanced datasets like LAMA. Based on these findings, we propose a
representation-based approach to mitigate the prompt bias during inference
time. Specifically, we first estimate the biased representation using
prompt-only querying, and then remove it from the model's internal
representations to generate the debiased representations, which are used to
produce the final debiased outputs. Experiments across various prompts, PLMs,
and benchmarks show that our approach can not only correct the overfitted
performance caused by prompt bias, but also significantly improve the prompt
retrieval capability (up to 10% absolute performance gain). Our findings shed
new light on the underlying predicting mechanisms of prompt-based queries in
PLMs. Hopefully, our plug-and-play approach can be a golden standard to
strengthen PLMs toward reliable knowledge bases. Code and data are released in
https://github.com/FelliYang/PromptBias.
\\ ( https://arxiv.org/abs/2403.09963 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09972
Date: Fri, 15 Mar 2024 02:38:26 GMT   (7374kb,D)

Title: Think Twice Before Assure: Confidence Estimation for Large Language
  Models through Reflection on Multiple Answers
Authors: Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, Tat-Seng
  Chua
Categories: cs.CL
Comments: Under review
\\
  Confidence estimation aiming to evaluate output trustability is crucial for
the application of large language models (LLM), especially the black-box ones.
Existing confidence estimation of LLM is typically not calibrated due to the
overconfidence of LLM on its generated incorrect answers. Existing approaches
addressing the overconfidence issue are hindered by a significant limitation
that they merely consider the confidence of one answer generated by LLM. To
tackle this limitation, we propose a novel paradigm that thoroughly evaluates
the trustability of multiple candidate answers to mitigate the overconfidence
on incorrect answers. Building upon this paradigm, we introduce a two-step
framework, which firstly instructs LLM to reflect and provide justifications
for each answer, and then aggregates the justifications for comprehensive
confidence estimation. This framework can be integrated with existing
confidence estimation approaches for superior calibration. Experimental results
on six datasets of three tasks demonstrate the rationality and effectiveness of
the proposed framework.
\\ ( https://arxiv.org/abs/2403.09972 ,  7374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09997
Date: Fri, 15 Mar 2024 03:43:07 GMT   (1912kb,D)

Title: Identifying Health Risks from Family History: A Survey of Natural
  Language Processing Techniques
Authors: Xiang Dai and Sarvnaz Karimi and Nathan O'Callaghan
Categories: cs.CL
Comments: Under Review
\\
  Electronic health records include information on patients' status and medical
history, which could cover the history of diseases and disorders that could be
hereditary. One important use of family history information is in precision
health, where the goal is to keep the population healthy with preventative
measures. Natural Language Processing (NLP) and machine learning techniques can
assist with identifying information that could assist health professionals in
identifying health risks before a condition is developed in their later years,
saving lives and reducing healthcare costs.
  We survey the literature on the techniques from the NLP field that have been
developed to utilise digital health records to identify risks of familial
diseases. We highlight that rule-based methods are heavily investigated and are
still actively used for family history extraction. Still, more recent efforts
have been put into building neural models based on large-scale pre-trained
language models. In addition to the areas where NLP has successfully been
utilised, we also identify the areas where more research is needed to unlock
the value of patients' records regarding data collection, task formulation and
downstream applications.
\\ ( https://arxiv.org/abs/2403.09997 ,  1912kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10020
Date: Fri, 15 Mar 2024 05:06:21 GMT   (202kb,D)

Title: Lost in Overlap: Exploring Watermark Collision in LLMs
Authors: Yiyang Luo, Ke Lin, Chao Gu
Categories: cs.CL cs.MM
Comments: Short Paper, 4 pages
\\
  The proliferation of large language models (LLMs) in generating content
raises concerns about text copyright. Watermarking methods, particularly
logit-based approaches, embed imperceptible identifiers into text to address
these challenges. However, the widespread use of watermarking across diverse
LLMs has led to an inevitable issue known as watermark collision during common
tasks like question answering and paraphrasing. This study focuses on dual
watermark collisions, where two watermarks are present simultaneously in the
same text. The research demonstrates that watermark collision poses a threat to
detection performance for detectors of both upstream and downstream watermark
algorithms.
\\ ( https://arxiv.org/abs/2403.10020 ,  202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10056
Date: Fri, 15 Mar 2024 06:54:20 GMT   (911kb,D)

Title: Don't Half-listen: Capturing Key-part Information in Continual
  Instruction Tuning
Authors: Yongquan He and Xuancheng Huang and Minghao Tang and Lingxun Meng and
  Xiang Li and Wei Lin and Wenyuan Zhang and Yifu Gao
Categories: cs.CL cs.AI
Comments: 18 pages, 4 figures
\\
  Instruction tuning for large language models (LLMs) can drive them to produce
results consistent with human goals in specific downstream tasks. However, the
process of continual instruction tuning (CIT) for LLMs may bring about the
catastrophic forgetting (CF) problem, where previously learned abilities are
degraded. Recent methods try to alleviate the CF problem by modifying models or
replaying data, which may only remember the surface-level pattern of
instructions and get confused on held-out tasks. In this paper, we propose a
novel continual instruction tuning method based on Key-part Information Gain
(KPIG). Our method computes the information gain on masked parts to dynamically
replay data and refine the training objective, which enables LLMs to capture
task-aware information relevant to the correct response and alleviate
overfitting to general descriptions in instructions. In addition, we propose
two metrics, P-score and V-score, to measure the generalization and
instruction-following abilities of LLMs. Experiments demonstrate our method
achieves superior performance on both seen and held-out tasks.
\\ ( https://arxiv.org/abs/2403.10056 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10065
Date: Fri, 15 Mar 2024 07:15:48 GMT   (1663kb,D)

Title: Triple GNNs: Introducing Syntactic and Semantic Information for
  Conversational Aspect-Based Quadruple Sentiment Analysis
Authors: Binbin Li, Yuqing Li, Siyu Jia, Bingnan Ma, Yu Ding, Zisen Qi,
  Xingbang Tan, Menghan Guo, Shenghui Liu
Categories: cs.CL
Comments: Accepted by CSCWD2024
\\
  Conversational Aspect-Based Sentiment Analysis (DiaASQ) aims to detect
quadruples \{target, aspect, opinion, sentiment polarity\} from given
dialogues. In DiaASQ, elements constituting these quadruples are not
necessarily confined to individual sentences but may span across multiple
utterances within a dialogue. This necessitates a dual focus on both the
syntactic information of individual utterances and the semantic interaction
among them. However, previous studies have primarily focused on coarse-grained
relationships between utterances, thus overlooking the potential benefits of
detailed intra-utterance syntactic information and the granularity of
inter-utterance relationships. This paper introduces the Triple GNNs network to
enhance DiaAsQ. It employs a Graph Convolutional Network (GCN) for modeling
syntactic dependencies within utterances and a Dual Graph Attention Network
(DualGATs) to construct interactions between utterances. Experiments on two
standard datasets reveal that our model significantly outperforms
state-of-the-art baselines. The code is available at
\url{https://github.com/nlperi2b/Triple-GNNs-}.
\\ ( https://arxiv.org/abs/2403.10065 ,  1663kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10081
Date: Fri, 15 Mar 2024 07:45:37 GMT   (8015kb,D)

Title: DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time
  Information Needs of Large Language Models
Authors: Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu
Categories: cs.CL cs.IR
\\
  Dynamic retrieval augmented generation (RAG) paradigm actively decides when
and what to retrieve during the text generation process of Large Language
Models (LLMs). There are two key elements of this paradigm: identifying the
optimal moment to activate the retrieval module (deciding when to retrieve) and
crafting the appropriate query once retrieval is triggered (determining what to
retrieve). However, current dynamic RAG methods fall short in both aspects.
Firstly, the strategies for deciding when to retrieve often rely on static
rules. Moreover, the strategies for deciding what to retrieve typically limit
themselves to the LLM's most recent sentence or the last few tokens, while the
LLM's real-time information needs may span across the entire context. To
overcome these limitations, we introduce a new framework, DRAGIN, i.e., Dynamic
Retrieval Augmented Generation based on the real-time Information Needs of
LLMs. Our framework is specifically designed to make decisions on when and what
to retrieve based on the LLM's real-time information needs during the text
generation process. We evaluate DRAGIN along with existing methods
comprehensively over 4 knowledge-intensive generation datasets. Experimental
results show that DRAGIN achieves superior performance on all tasks,
demonstrating the effectiveness of our method. We have open-sourced all the
code, data, and models in GitHub: https://github.com/oneal2000/DRAGIN/tree/main
\\ ( https://arxiv.org/abs/2403.10081 ,  8015kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10088
Date: Fri, 15 Mar 2024 08:03:49 GMT   (8890kb,D)

Title: Intent-conditioned and Non-toxic Counterspeech Generation using
  Multi-Task Instruction Tuning with RLAIF
Authors: Amey Hengle, Aswini Kumar, Sahajpreet Singh, Anil Bandhakavi, Md Shad
  Akhtar, Tanmoy Chakroborty
Categories: cs.CL cs.AI
\\
  Counterspeech, defined as a response to mitigate online hate speech, is
increasingly used as a non-censorial solution. Addressing hate speech
effectively involves dispelling the stereotypes, prejudices, and biases often
subtly implied in brief, single-sentence statements or abuses. These implicit
expressions challenge language models, especially in seq2seq tasks, as model
performance typically excels with longer contexts. Our study introduces CoARL,
a novel framework enhancing counterspeech generation by modeling the pragmatic
implications underlying social biases in hateful statements. CoARL's first two
phases involve sequential multi-instruction tuning, teaching the model to
understand intents, reactions, and harms of offensive statements, and then
learning task-specific low-rank adapter weights for generating
intent-conditioned counterspeech. The final phase uses reinforcement learning
to fine-tune outputs for effectiveness and non-toxicity. CoARL outperforms
existing benchmarks in intent-conditioned counterspeech generation, showing an
average improvement of 3 points in intent-conformity and 4 points in
argument-quality metrics. Extensive human evaluation supports CoARL's efficacy
in generating superior and more context-appropriate responses compared to
existing systems, including prominent LLMs like ChatGPT.
\\ ( https://arxiv.org/abs/2403.10088 ,  8890kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10131
Date: Fri, 15 Mar 2024 09:26:02 GMT   (760kb,D)

Title: RAFT: Adapting Language Model to Domain Specific RAG
Authors: Tianjun Zhang, Shishir G. Patil, Naman Jain, Sheng Shen, Matei
  Zaharia, Ion Stoica, Joseph E. Gonzalez
Categories: cs.CL cs.AI
\\
  Pretraining Large Language Models (LLMs) on large corpora of textual data is
now a standard paradigm. When using these LLMs for many downstream
applications, it is common to additionally bake in new knowledge (e.g.,
time-critical news, or private domain knowledge) into the pretrained model
either through RAG-based-prompting, or fine-tuning. However, the optimal
methodology for the model to gain such new knowledge remains an open question.
In this paper, we present Retrieval Augmented FineTuning (RAFT), a training
recipe that improves the model's ability to answer questions in a "open-book"
in-domain settings. In RAFT, given a question, and a set of retrieved
documents, we train the model to ignore those documents that don't help in
answering the question, which we call, distractor documents. RAFT accomplishes
this by citing verbatim the right sequence from the relevant document that
would help answer the question. This coupled with RAFT's chain-of-thought-style
response helps improve the model's ability to reason. In domain-specific RAG,
RAFT consistently improves the model's performance across PubMed, HotpotQA, and
Gorilla datasets, presenting a post-training recipe to improve pre-trained LLMs
to in-domain RAG. RAFT's code and demo are open-sourced at
github.com/ShishirPatil/gorilla.
\\ ( https://arxiv.org/abs/2403.10131 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10144
Date: Fri, 15 Mar 2024 09:43:52 GMT   (3023kb,D)

Title: NLP Verification: Towards a General Methodology for Certifying
  Robustness
Authors: Marco Casadio, Tanvi Dinkar, Ekaterina Komendantskaya, Luca Arnaboldi,
  Omri Isac, Matthew L. Daggitt, Guy Katz, Verena Rieser, Oliver Lemon
Categories: cs.CL cs.AI cs.LG cs.LO cs.PL
\\
  Deep neural networks have exhibited substantial success in the field of
Natural Language Processing (NLP) and ensuring their safety and reliability is
crucial: there are safety critical contexts where such models must be robust to
variability or attack, and give guarantees over their output. Unlike Computer
Vision, NLP lacks a unified verification methodology and, despite recent
advancements in literature, they are often light on the pragmatical issues of
NLP verification. In this paper, we make an attempt to distil and evaluate
general components of an NLP verification pipeline, that emerges from the
progress in the field to date. Our contributions are two-fold. Firstly, we give
a general characterisation of verifiable subspaces that result from embedding
sentences into continuous spaces. We identify, and give an effective method to
deal with, the technical challenge of semantic generalisability of verified
subspaces; and propose it as a standard metric in the NLP verification
pipelines (alongside with the standard metrics of model accuracy and model
verifiability). Secondly, we propose a general methodology to analyse the
effect of the embedding gap, a problem that refers to the discrepancy between
verification of geometric subpspaces on the one hand, and semantic meaning of
sentences which the geometric subspaces are supposed to represent, on the other
hand. In extreme cases, poor choices in embedding of sentences may invalidate
verification results. We propose a number of practical NLP methods that can
help to identify the effects of the embedding gap; and in particular we propose
the metric of falsifiability of semantic subpspaces as another fundamental
metric to be reported as part of the NLP verification pipeline. We believe that
together these general principles pave the way towards a more consolidated and
effective development of this new domain.
\\ ( https://arxiv.org/abs/2403.10144 ,  3023kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10185
Date: Fri, 15 Mar 2024 10:46:00 GMT   (2930kb,D)

Title: Can Factual Statements be Deceptive? The DeFaBel Corpus of Belief-based
  Deception
Authors: Aswathy Velutharambath, Amelie W\"uhrl, and Roman Klinger
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  If a person firmly believes in a non-factual statement, such as "The Earth is
flat", and argues in its favor, there is no inherent intention to deceive. As
the argumentation stems from genuine belief, it may be unlikely to exhibit the
linguistic properties associated with deception or lying. This interplay of
factuality, personal belief, and intent to deceive remains an understudied
area. Disentangling the influence of these variables in argumentation is
crucial to gain a better understanding of the linguistic properties attributed
to each of them. To study the relation between deception and factuality, based
on belief, we present the DeFaBel corpus, a crowd-sourced resource of
belief-based deception. To create this corpus, we devise a study in which
participants are instructed to write arguments supporting statements like
"eating watermelon seeds can cause indigestion", regardless of its factual
accuracy or their personal beliefs about the statement. In addition to the
generation task, we ask them to disclose their belief about the statement. The
collected instances are labelled as deceptive if the arguments are in
contradiction to the participants' personal beliefs. Each instance in the
corpus is thus annotated (or implicitly labelled) with personal beliefs of the
author, factuality of the statement, and the intended deceptiveness. The
DeFaBel corpus contains 1031 texts in German, out of which 643 are deceptive
and 388 are non-deceptive. It is the first publicly available corpus for
studying deception in German. In our analysis, we find that people are more
confident in the persuasiveness of their arguments when the statement is
aligned with their belief, but surprisingly less confident when they are
generating arguments in favor of facts. The DeFaBel corpus can be obtained from
https://www.ims.uni-stuttgart.de/data/defabel
\\ ( https://arxiv.org/abs/2403.10185 ,  2930kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10205
Date: Fri, 15 Mar 2024 11:11:57 GMT   (8660kb,D)

Title: Read between the lines -- Functionality Extraction From READMEs
Authors: Prince Kumar, Srikanth Tamilselvam, Dinesh Garg
Categories: cs.CL cs.AI
\\
  While text summarization is a well-known NLP task, in this paper, we
introduce a novel and useful variant of it called functionality extraction from
Git README files. Though this task is a text2text generation at an abstract
level, it involves its own peculiarities and challenges making existing
text2text generation systems not very useful. The motivation behind this task
stems from a recent surge in research and development activities around the use
of large language models for code-related tasks, such as code refactoring, code
summarization, etc. We also release a human-annotated dataset called FuncRead,
and develop a battery of models for the task. Our exhaustive experimentation
shows that small size fine-tuned models beat any baseline models that can be
designed using popular black-box or white-box large language models (LLMs) such
as ChatGPT and Bard. Our best fine-tuned 7 Billion CodeLlama model exhibit 70%
and 20% gain on the F1 score against ChatGPT and Bard respectively.
\\ ( https://arxiv.org/abs/2403.10205 ,  8660kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10214
Date: Fri, 15 Mar 2024 11:32:44 GMT   (4252kb,D)

Title: Enhanced Coherence-Aware Network with Hierarchical Disentanglement for
  Aspect-Category Sentiment Analysis
Authors: Jin Cui, Fumiyo Fukumoto, Xinfeng Wang, Yoshimi Suzuki, Jiyi Li,
  Noriko Tomuro, and Wanzeng Kong
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\
  Aspect-category-based sentiment analysis (ACSA), which aims to identify
aspect categories and predict their sentiments has been intensively studied due
to its wide range of NLP applications. Most approaches mainly utilize
intrasentential features. However, a review often includes multiple different
aspect categories, and some of them do not explicitly appear in the review.
Even in a sentence, there is more than one aspect category with its sentiments,
and they are entangled intra-sentence, which makes the model fail to
discriminately preserve all sentiment characteristics. In this paper, we
propose an enhanced coherence-aware network with hierarchical disentanglement
(ECAN) for ACSA tasks. Specifically, we explore coherence modeling to capture
the contexts across the whole review and to help the implicit aspect and
sentiment identification. To address the issue of multiple aspect categories
and sentiment entanglement, we propose a hierarchical disentanglement module to
extract distinct categories and sentiment features. Extensive experimental and
visualization results show that our ECAN effectively decouples multiple
categories and sentiments entangled in the coherence representations and
achieves state-of-the-art (SOTA) performance. Our codes and data are available
online: \url{https://github.com/cuijin-23/ECAN}.
\\ ( https://arxiv.org/abs/2403.10214 ,  4252kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10237
Date: Fri, 15 Mar 2024 12:08:58 GMT   (754kb)

Title: A comprehensive study on Frequent Pattern Mining and Clustering
  categories for topic detection in Persian text stream
Authors: Elnaz Zafarani-Moattar, Mohammad Reza Kangavari, Amir Masoud Rahmani
Categories: cs.CL
\\
  Topic detection is a complex process and depends on language because it
somehow needs to analyze text. There have been few studies on topic detection
in Persian, and the existing algorithms are not remarkable. Therefore, we aimed
to study topic detection in Persian. The objectives of this study are: 1) to
conduct an extensive study on the best algorithms for topic detection, 2) to
identify necessary adaptations to make these algorithms suitable for the
Persian language, and 3) to evaluate their performance on Persian social
network texts. To achieve these objectives, we have formulated two research
questions: First, considering the lack of research in Persian, what
modifications should be made to existing frameworks, especially those developed
in English, to make them compatible with Persian? Second, how do these
algorithms perform, and which one is superior? There are various topic
detection methods that can be categorized into different categories. Frequent
pattern and clustering are selected for this research, and a hybrid of both is
proposed as a new category. Then, ten methods from these three categories are
selected. All of them are re-implemented from scratch, changed, and adapted
with Persian. These ten methods encompass different types of topic detection
methods and have shown good performance in English. The text of Persian social
network posts is used as the dataset. Additionally, a new multiclass evaluation
criterion, called FS, is used in this paper for the first time in the field of
topic detection. Approximately 1.4 billion tokens are processed during
experiments. The results indicate that if we are searching for keyword-topics
that are easily understandable by humans, the hybrid category is better.
However, if the aim is to cluster posts for further analysis, the frequent
pattern category is more suitable.
\\ ( https://arxiv.org/abs/2403.10237 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10239
Date: Fri, 15 Mar 2024 12:12:54 GMT   (824kb)

Title: A Big Data Approach to Understand Sub-national Determinants of FDI in
  Africa
Authors: A. Fronzetti Colladon, R. Vestrelli, S. Bait, M. M. Schiraldi
Categories: cs.CL econ.EM physics.soc-ph
ACM-class: I.2.7; J.4; H.4.0
\\
  Various macroeconomic and institutional factors hinder FDI inflows, including
corruption, trade openness, access to finance, and political instability.
Existing research mostly focuses on country-level data, with limited
exploration of firm-level data, especially in developing countries. Recognizing
this gap, recent calls for research emphasize the need for qualitative data
analysis to delve into FDI determinants, particularly at the regional level.
This paper proposes a novel methodology, based on text mining and social
network analysis, to get information from more than 167,000 online news
articles to quantify regional-level (sub-national) attributes affecting FDI
ownership in African companies. Our analysis extends information on obstacles
to industrial development as mapped by the World Bank Enterprise Surveys.
Findings suggest that regional (sub-national) structural and institutional
characteristics can play an important role in determining foreign ownership.
\\ ( https://arxiv.org/abs/2403.10239 ,  824kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10258
Date: Fri, 15 Mar 2024 12:47:39 GMT   (8417kb,D)

Title: Is Translation All You Need? A Study on Solving Multilingual Tasks with
  Large Language Models
Authors: Chaoqun Liu, Wenxuan Zhang, Yiran Zhao, Anh Tuan Luu, Lidong Bing
Categories: cs.CL
Comments: 18 pages
\\
  Large language models (LLMs) have demonstrated strong multilingual
capabilities; yet, they are mostly English-centric due to the imbalanced
training corpora. Existing works leverage this phenomenon to improve their
multilingual performances on NLP tasks. In this work, we extend the evaluation
from NLP tasks to real user queries. We find that even though translation into
English can help improve the performance of multilingual NLP tasks for
English-centric LLMs, it may not be optimal for all scenarios. For
culture-related tasks that need deep language understanding, prompting in the
native language proves to be more promising since it can capture the nuances
related to culture and language. Therefore, we advocate for more efforts
towards the development of strong multilingual LLMs instead of just
English-centric LLMs.
\\ ( https://arxiv.org/abs/2403.10258 ,  8417kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10275
Date: Fri, 15 Mar 2024 13:15:23 GMT   (793kb,D)

Title: A Question on the Explainability of Large Language Models and the
  Word-Level Univariate First-Order Plausibility Assumption
Authors: Jeremie Bogaert, Francois-Xavier Standaert
Categories: cs.CL cs.AI
Comments: 7 pages, 10 figures, Accepted and presented at AAAI 2024 (ReLM
  workshop)
\\
  The explanations of large language models have recently been shown to be
sensitive to the randomness used for their training, creating a need to
characterize this sensitivity. In this paper, we propose a characterization
that questions the possibility to provide simple and informative explanations
for such models. To this end, we give statistical definitions for the
explanations' signal, noise and signal-to-noise ratio. We highlight that, in a
typical case study where word-level univariate explanations are analyzed with
first-order statistical tools, the explanations of simple feature-based models
carry more signal and less noise than those of transformer ones. We then
discuss the possibility to improve these results with alternative definitions
of signal and noise that would capture more complex explanations and analysis
methods, while also questioning the tradeoff with their plausibility for
readers.
\\ ( https://arxiv.org/abs/2403.10275 ,  793kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10281
Date: Fri, 15 Mar 2024 13:24:28 GMT   (3158kb,D)

Title: Team Trifecta at Factify5WQA: Setting the Standard in Fact Verification
  with Fine-Tuning
Authors: Shang-Hsuan Chiang, Ming-Chih Lo, Lin-Wei Chao and Wen-Chih Peng
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by AAAI 2024 Workshop: FACTIFY 3.0 - Workshop Series on
  Multimodal Fact-Checking and Hate Speech Detection
\\
  In this paper, we present Pre-CoFactv3, a comprehensive framework comprised
of Question Answering and Text Classification components for fact verification.
Leveraging In-Context Learning, Fine-tuned Large Language Models (LLMs), and
the FakeNet model, we address the challenges of fact verification. Our
experiments explore diverse approaches, comparing different Pre-trained LLMs,
introducing FakeNet, and implementing various ensemble methods. Notably, our
team, Trifecta, secured first place in the AAAI-24 Factify 3.0 Workshop,
surpassing the baseline accuracy by 103% and maintaining a 70% lead over the
second competitor. This success underscores the efficacy of our approach and
its potential contributions to advancing fact verification research.
\\ ( https://arxiv.org/abs/2403.10281 ,  3158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10293
Date: Fri, 15 Mar 2024 13:33:10 GMT   (814kb,D)

Title: MaiBaam: A Multi-Dialectal Bavarian Universal Dependency Treebank
Authors: Verena Blaschke, Barbara Kova\v{c}i\'c, Siyao Peng, Hinrich Sch\"utze,
  Barbara Plank
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Despite the success of the Universal Dependencies (UD) project exemplified by
its impressive language breadth, there is still a lack in `within-language
breadth': most treebanks focus on standard languages. Even for German, the
language with the most annotations in UD, so far no treebank exists for one of
its language varieties spoken by over 10M people: Bavarian. To contribute to
closing this gap, we present the first multi-dialect Bavarian treebank
(MaiBaam) manually annotated with part-of-speech and syntactic dependency
information in UD, covering multiple text genres (wiki, fiction, grammar
examples, social, non-fiction). We highlight the morphosyntactic differences
between the closely-related Bavarian and German and showcase the rich
variability of speakers' orthographies. Our corpus includes 15k tokens,
covering dialects from all Bavarian-speaking areas spanning three countries. We
provide baseline parsing and POS tagging results, which are lower than results
obtained on German and vary substantially between different graph-based
parsers. To support further research on Bavarian syntax, we make our dataset,
language-specific guidelines and code publicly available.
\\ ( https://arxiv.org/abs/2403.10293 ,  814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10301
Date: Fri, 15 Mar 2024 13:43:47 GMT   (10981kb,D)

Title: Uni-SMART: Universal Science Multimodal Analysis and Research
  Transformer
Authors: Hengxing Cai, Xiaochen Cai, Shuwen Yang, Jiankun Wang, Lin Yao,
  Zhifeng Gao, Junhan Chang, Sihang Li, Mingjun Xu, Changxin Wang, Hongshuai
  Wang, Yongge Li, Mujie Lin, Yaqi Li, Yuqi Yin, Linfeng Zhang, Guolin Ke
Categories: cs.CL cs.CV
\\
  In scientific research and its application, scientific literature analysis is
crucial as it allows researchers to build on the work of others. However, the
fast growth of scientific knowledge has led to a massive increase in scholarly
articles, making in-depth literature analysis increasingly challenging and
time-consuming. The emergence of Large Language Models (LLMs) has offered a new
way to address this challenge. Known for their strong abilities in summarizing
texts, LLMs are seen as a potential tool to improve the analysis of scientific
literature. However, existing LLMs have their own limits. Scientific literature
often includes a wide range of multimodal elements, such as molecular
structure, tables, and charts, which are hard for text-focused LLMs to
understand and analyze. This issue points to the urgent need for new solutions
that can fully understand and analyze multimodal content in scientific
literature. To answer this demand, we present Uni-SMART (Universal Science
Multimodal Analysis and Research Transformer), an innovative model designed for
in-depth understanding of multimodal scientific literature. Through rigorous
quantitative evaluation across several domains, Uni-SMART demonstrates superior
performance over leading text-focused LLMs. Furthermore, our exploration
extends to practical applications, including patent infringement detection and
nuanced analysis of charts. These applications not only highlight Uni-SMART's
adaptability but also its potential to revolutionize how we interact with
scientific literature.
\\ ( https://arxiv.org/abs/2403.10301 ,  10981kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10326
Date: Fri, 15 Mar 2024 14:14:26 GMT   (6810kb,D)

Title: CDGP: Automatic Cloze Distractor Generation based on Pre-trained
  Language Model
Authors: Shang-Hsuan Chiang and Ssu-Cheng Wang and Yao-Chung Fan
Categories: cs.CL cs.AI cs.LG
Comments: Findings of short paper, EMNLP 2022
Journal-ref: chiang-etal-2022-cdgp
DOI: 10.18653/v1/2022.findings-emnlp.429
\\
  Manually designing cloze test consumes enormous time and efforts. The major
challenge lies in wrong option (distractor) selection. Having carefully-design
distractors improves the effectiveness of learner ability assessment. As a
result, the idea of automatically generating cloze distractor is motivated. In
this paper, we investigate cloze distractor generation by exploring the
employment of pre-trained language models (PLMs) as an alternative for
candidate distractor generation. Experiments show that the PLM-enhanced model
brings a substantial performance improvement. Our best performing model
advances the state-of-the-art result from 14.94 to 34.17 (NDCG@10 score). Our
code and dataset is available at https://github.com/AndyChiangSH/CDGP.
\\ ( https://arxiv.org/abs/2403.10326 ,  6810kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10338
Date: Fri, 15 Mar 2024 14:25:59 GMT   (1007kb,D)

Title: Investigating grammatical abstraction in language models using few-shot
  learning of novel noun gender
Authors: Priyanka Sukumaran, Conor Houghton, Nina Kazanina
Categories: cs.CL
Comments: EACL 2024; Findings of the Association for Computational Linguistics
\\
  Humans can learn a new word and infer its grammatical properties from very
few examples. They have an abstract notion of linguistic properties like
grammatical gender and agreement rules that can be applied to novel syntactic
contexts and words. Drawing inspiration from psycholinguistics, we conduct a
noun learning experiment to assess whether an LSTM and a decoder-only
transformer can achieve human-like abstraction of grammatical gender in French.
Language models were tasked with learning the gender of a novel noun embedding
from a few examples in one grammatical agreement context and predicting
agreement in another, unseen context. We find that both language models
effectively generalise novel noun gender from one to two learning examples and
apply the learnt gender across agreement contexts, albeit with a bias for the
masculine gender category. Importantly, the few-shot updates were only applied
to the embedding layers, demonstrating that models encode sufficient gender
information within the word embedding space. While the generalisation behaviour
of models suggests that they represent grammatical gender as an abstract
category, like humans, further work is needed to explore the details of how
exactly this is implemented. For a comparative perspective with human
behaviour, we conducted an analogous one-shot novel noun gender learning
experiment, which revealed that native French speakers, like language models,
also exhibited a masculine gender bias and are not excellent one-shot learners
either.
\\ ( https://arxiv.org/abs/2403.10338 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10351
Date: Fri, 15 Mar 2024 14:36:38 GMT   (1236kb,D)

Title: TriSum: Learning Summarization Ability from Large Language Models with
  Structured Rationale
Authors: Pengcheng Jiang, Cao Xiao, Zifeng Wang, Parminder Bhatia, Jimeng Sun,
  Jiawei Han
Categories: cs.CL
Comments: NAACL'24
\\
  The advent of large language models (LLMs) has significantly advanced natural
language processing tasks like text summarization. However, their large size
and computational demands, coupled with privacy concerns in data transmission,
limit their use in resource-constrained and privacy-centric settings. To
overcome this, we introduce TriSum, a framework for distilling LLMs' text
summarization abilities into a compact, local model. Initially, LLMs extract a
set of aspect-triple rationales and summaries, which are refined using a
dual-scoring method for quality. Next, a smaller local model is trained with
these tasks, employing a curriculum learning strategy that evolves from simple
to complex tasks. Our method enhances local model performance on various
benchmarks (CNN/DailyMail, XSum, and ClinicalTrial), outperforming baselines by
4.5%, 8.5%, and 7.4%, respectively. It also improves interpretability by
providing insights into the summarization rationale.
\\ ( https://arxiv.org/abs/2403.10351 ,  1236kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10378
Date: Fri, 15 Mar 2024 15:08:39 GMT   (7091kb,D)

Title: EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for
  Evaluating Vision Language Models
Authors: Rocktim Jyoti Das and Simeon Emilov Hristov and Haonan Li and Dimitar
  Iliyanov Dimitrov and Ivan Koychev and Preslav Nakov
Categories: cs.CL cs.CV
\\
  We introduce EXAMS-V, a new challenging multi-discipline multimodal
multilingual exam benchmark for evaluating vision language models. It consists
of 20,932 multiple-choice questions across 20 school disciplines covering
natural science, social science, and other miscellaneous studies, e.g.,
religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal
features such as text, images, tables, figures, diagrams, maps, scientific
symbols, and equations. The questions come in 11 languages from 7 language
families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering
school exam questions from various countries, with a variety of education
systems. This distinctive approach calls for intricate reasoning across diverse
languages and relies on region-specific knowledge. Solving the problems in the
dataset requires advanced perception and joint reasoning over the text and the
visual content of the image. Our evaluation results demonstrate that this is a
challenging dataset, which is difficult even for advanced vision-text models
such as GPT-4V and Gemini; this underscores the inherent complexity of the
dataset and its significance as a future benchmark.
\\ ( https://arxiv.org/abs/2403.10378 ,  7091kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10381
Date: Fri, 15 Mar 2024 15:10:41 GMT   (6378kb,D)

Title: Monotonic Representation of Numeric Properties in Language Models
Authors: Benjamin Heinzerling and Kentaro Inui
Categories: cs.CL
\\
  Language models (LMs) can express factual knowledge involving numeric
properties such as Karl Popper was born in 1902. However, how this information
is encoded in the model's internal representations is not understood well.
Here, we introduce a simple method for finding and editing representations of
numeric properties such as an entity's birth year. Empirically, we find
low-dimensional subspaces that encode numeric properties monotonically, in an
interpretable and editable fashion. When editing representations along
directions in these subspaces, LM output changes accordingly. For example, by
patching activations along a "birthyear" direction we can make the LM express
an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was
born in 1957, Karl Popper was born in 1968. Property-encoding directions exist
across several numeric properties in all models under consideration, suggesting
the possibility that monotonic representation of numeric properties
consistently emerges during LM pretraining. Code:
https://github.com/bheinzerling/numeric-property-repr
\\ ( https://arxiv.org/abs/2403.10381 ,  6378kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10446
Date: Fri, 15 Mar 2024 16:30:14 GMT   (504kb,D)

Title: Enhancing LLM Factual Accuracy with RAG to Counter Hallucinations: A
  Case Study on Domain-Specific Queries in Private Knowledge-Bases
Authors: Jiarui Li and Ye Yuan and Zehua Zhang
Categories: cs.CL cs.LG
Comments: These authors contributed equally to this work
\\
  We proposed an end-to-end system design towards utilizing Retrieval Augmented
Generation (RAG) to improve the factual accuracy of Large Language Models
(LLMs) for domain-specific and time-sensitive queries related to private
knowledge-bases. Our system integrates RAG pipeline with upstream datasets
processing and downstream performance evaluation. Addressing the challenge of
LLM hallucinations, we finetune models with a curated dataset which originates
from CMU's extensive resources and annotated with the teacher model. Our
experiments demonstrate the system's effectiveness in generating more accurate
answers to domain-specific and time-sensitive inquiries. The results also
revealed the limitations of fine-tuning LLMs with small-scale and skewed
datasets. This research highlights the potential of RAG systems in augmenting
LLMs with external datasets for improved performance in knowledge-intensive
tasks. Our code and models are available on Github.
\\ ( https://arxiv.org/abs/2403.10446 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09701
Date: Thu, 7 Mar 2024 19:39:47 GMT   (2009kb,D)

Title: A Natural Extension To Online Algorithms For Hybrid RL With Limited
  Coverage
Authors: Kevin Tan, Ziping Xu
Categories: cs.LG stat.ML
Comments: Submitted to the reinforcement learning conference
\\
  Hybrid Reinforcement Learning (RL), leveraging both online and offline data,
has garnered recent interest, yet research on its provable benefits remains
sparse. Additionally, many existing hybrid RL algorithms (Song et al., 2023;
Nakamoto et al., 2023; Amortila et al., 2024) impose coverage assumptions on
the offline dataset, but we show that this is unnecessary. A well-designed
online algorithm should "fill in the gaps" in the offline dataset, exploring
states and actions that the behavior policy did not explore. Unlike previous
approaches that focus on estimating the offline data distribution to guide
online exploration (Li et al., 2023b), we show that a natural extension to
standard optimistic online algorithms -- warm-starting them by including the
offline dataset in the experience replay buffer -- achieves similar provable
gains from hybrid data even when the offline dataset does not have
single-policy concentrability. We accomplish this by partitioning the
state-action space into two, bounding the regret on each partition through an
offline and an online complexity measure, and showing that the regret of this
hybrid RL algorithm can be characterized by the best partition -- despite the
algorithm not knowing the partition itself. As an example, we propose
DISC-GOLF, a modification of an existing optimistic online algorithm with
general function approximation called GOLF used in Jin et al. (2021); Xie et
al. (2022a), and show that it demonstrates provable gains over both online-only
and offline-only reinforcement learning, with competitive bounds when
specialized to the tabular, linear and block MDP cases. Numerical simulations
further validate our theory that hybrid data facilitates more efficient
exploration, supporting the potential of hybrid RL in various scenarios.
\\ ( https://arxiv.org/abs/2403.09701 ,  2009kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09749
Date: Thu, 14 Mar 2024 05:02:00 GMT   (742kb,D)

Title: Towards Diverse Perspective Learning with Selection over Multiple
  Temporal Poolings
Authors: Jihyeon Seong, Jungmin Kim, Jaesik Choi
Categories: cs.LG cs.AI
Comments: 17 pages, 9 figures
Journal-ref: AAAI 2024
\\
  In Time Series Classification (TSC), temporal pooling methods that consider
sequential information have been proposed. However, we found that each temporal
pooling has a distinct mechanism, and can perform better or worse depending on
time series data. We term this fixed pooling mechanism a single perspective of
temporal poolings. In this paper, we propose a novel temporal pooling method
with diverse perspective learning: Selection over Multiple Temporal Poolings
(SoM-TP). SoM-TP dynamically selects the optimal temporal pooling among
multiple methods for each data by attention. The dynamic pooling selection is
motivated by the ensemble concept of Multiple Choice Learning (MCL), which
selects the best among multiple outputs. The pooling selection by SoM-TP's
attention enables a non-iterative pooling ensemble within a single classifier.
Additionally, we define a perspective loss and Diverse Perspective Learning
Network (DPLN). The loss works as a regularizer to reflect all the pooling
perspectives from DPLN. Our perspective analysis using Layer-wise Relevance
Propagation (LRP) reveals the limitation of a single perspective and ultimately
demonstrates diverse perspective learning of SoM-TP. We also show that SoM-TP
outperforms CNN models based on other temporal poolings and state-of-the-art
models in TSC with extensive UCR/UEA repositories.
\\ ( https://arxiv.org/abs/2403.09749 ,  742kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09809
Date: Thu, 14 Mar 2024 18:58:06 GMT   (2120kb,D)

Title: Self-Supervised Learning for Time Series: Contrastive or Generative?
Authors: Ziyu Liu, Azadeh Alavi, Minyi Li, Xiang Zhang
Categories: cs.LG cs.AI cs.ET
Comments: Published at the AI4TS Workshop, IJCAI 2023
\\
  Self-supervised learning (SSL) has recently emerged as a powerful approach to
learning representations from large-scale unlabeled data, showing promising
results in time series analysis. The self-supervised representation learning
can be categorized into two mainstream: contrastive and generative. In this
paper, we will present a comprehensive comparative study between contrastive
and generative methods in time series. We first introduce the basic frameworks
for contrastive and generative SSL, respectively, and discuss how to obtain the
supervision signal that guides the model optimization. We then implement
classical algorithms (SimCLR vs. MAE) for each type and conduct a comparative
analysis in fair settings. Our results provide insights into the strengths and
weaknesses of each approach and offer practical recommendations for choosing
suitable SSL methods. We also discuss the implications of our findings for the
broader field of representation learning and propose future research
directions. All the code and data are released at
\url{https://github.com/DL4mHealth/SSL_Comparison}.
\\ ( https://arxiv.org/abs/2403.09809 ,  2120kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09830
Date: Thu, 14 Mar 2024 19:36:07 GMT   (12909kb,D)

Title: Towards the Reusability and Compositionality of Causal Representations
Authors: Davide Talon, Phillip Lippe, Stuart James, Alessio Del Bue and Sara
  Magliacane
Categories: cs.LG cs.AI
Comments: Accepted to the 3rd Conference on Causal Learning and Reasoning
  (CLeaR 2024)
\\
  Causal Representation Learning (CRL) aims at identifying high-level causal
factors and their relationships from high-dimensional observations, e.g.,
images. While most CRL works focus on learning causal representations in a
single environment, in this work we instead propose a first step towards
learning causal representations from temporal sequences of images that can be
adapted in a new environment, or composed across multiple related environments.
In particular, we introduce DECAF, a framework that detects which causal
factors can be reused and which need to be adapted from previously learned
causal representations. Our approach is based on the availability of
intervention targets, that indicate which variables are perturbed at each time
step. Experiments on three benchmark datasets show that integrating our
framework with four state-of-the-art CRL approaches leads to accurate
representations in a new environment with only a few samples.
\\ ( https://arxiv.org/abs/2403.09830 ,  12909kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09857
Date: Thu, 14 Mar 2024 20:34:53 GMT   (1800kb,D)

Title: Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive
  Prompt
Authors: Chenxi Liu, Zhenyi Wang, Tianyi Xiong, Ruibo Chen, Yihan Wu, Junfeng
  Guo, Heng Huang
Categories: cs.LG cs.AI cs.CV
\\
  Few-Shot Class-Incremental Learning (FSCIL) models aim to incrementally learn
new classes with scarce samples while preserving knowledge of old ones.
Existing FSCIL methods usually fine-tune the entire backbone, leading to
overfitting and hindering the potential to learn new classes. On the other
hand, recent prompt-based CIL approaches alleviate forgetting by training
prompts with sufficient data in each task. In this work, we propose a novel
framework named Attention-aware Self-adaptive Prompt (ASP). ASP encourages
task-invariant prompts to capture shared knowledge by reducing specific
information from the attention aspect. Additionally, self-adaptive
task-specific prompts in ASP provide specific information and transfer
knowledge from old classes to new classes with an Information Bottleneck
learning objective. In summary, ASP prevents overfitting on base task and does
not require enormous data in few-shot incremental tasks. Extensive experiments
on three benchmark datasets validate that ASP consistently outperforms
state-of-the-art FSCIL and prompt-based CIL methods in terms of both learning
new classes and mitigating forgetting.
\\ ( https://arxiv.org/abs/2403.09857 ,  1800kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09859
Date: Thu, 14 Mar 2024 20:40:36 GMT   (3358kb,D)

Title: MAMBA: an Effective World Model Approach for Meta-Reinforcement Learning
Authors: Zohar Rimon, Tom Jurgenson, Orr Krupnik, Gilad Adler, Aviv Tamar
Categories: cs.LG
\\
  Meta-reinforcement learning (meta-RL) is a promising framework for tackling
challenging domains requiring efficient exploration. Existing meta-RL
algorithms are characterized by low sample efficiency, and mostly focus on
low-dimensional task distributions. In parallel, model-based RL methods have
been successful in solving partially observable MDPs, of which meta-RL is a
special case. In this work, we leverage this success and propose a new
model-based approach to meta-RL, based on elements from existing
state-of-the-art model-based and meta-RL methods. We demonstrate the
effectiveness of our approach on common meta-RL benchmark domains, attaining
greater return with better sample efficiency (up to $15\times$) while requiring
very little hyperparameter tuning. In addition, we validate our approach on a
slate of more challenging, higher-dimensional domains, taking a step towards
real-world generalizing agents.
\\ ( https://arxiv.org/abs/2403.09859 ,  3358kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09863
Date: Thu, 14 Mar 2024 20:50:03 GMT   (688kb,D)

Title: A Conceptual Framework For White Box Neural Networks
Authors: Maciej Satkiewicz
Categories: cs.LG cs.AI cs.NE
Comments: 11 pages, 7 figures, independent research
\\
  This paper introduces semantic features as a general conceptual framework for
fully explainable neural network layers. A well-motivated proof of concept
model for relevant subproblem of MNIST consists of 4 such layers with the total
of 4.8K learnable parameters. The model is easily interpretable, achieves
human-level adversarial test accuracy with no form of adversarial training,
requires little hyperparameter tuning and can be quickly trained on a single
CPU. The general nature of the technique bears promise for a paradigm shift
towards radically democratised and truly generalizable white box neural
networks. The code is available at
https://github.com/314-Foundation/white-box-nn
\\ ( https://arxiv.org/abs/2403.09863 ,  688kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09867
Date: Thu, 14 Mar 2024 20:59:36 GMT   (320kb)

Title: iBRF: Improved Balanced Random Forest Classifier
Authors: Asif Newaz, Md. Salman Mohosheu, MD. Abdullah al Noman, Dr. Taskeed
  Jabid
Categories: cs.LG
\\
  Class imbalance poses a major challenge in different classification tasks,
which is a frequently occurring scenario in many real-world applications. Data
resampling is considered to be the standard approach to address this issue. The
goal of the technique is to balance the class distribution by generating new
samples or eliminating samples from the data. A wide variety of sampling
techniques have been proposed over the years to tackle this challenging
problem. Sampling techniques can also be incorporated into the ensemble
learning framework to obtain more generalized prediction performance. Balanced
Random Forest (BRF) and SMOTE-Bagging are some of the popular ensemble
approaches. In this study, we propose a modification to the BRF classifier to
enhance the prediction performance. In the original algorithm, the Random
Undersampling (RUS) technique was utilized to balance the bootstrap samples.
However, randomly eliminating too many samples from the data leads to
significant data loss, resulting in a major decline in performance. We propose
to alleviate the scenario by incorporating a novel hybrid sampling approach to
balance the uneven class distribution in each bootstrap sub-sample. Our
proposed hybrid sampling technique, when incorporated into the framework of the
Random Forest classifier, termed as iBRF: improved Balanced Random Forest
classifier, achieves better prediction performance than other sampling
techniques used in imbalanced classification tasks. Experiments were carried
out on 44 imbalanced datasets on which the original BRF classifier produced an
average MCC score of 47.03% and an F1 score of 49.09%. Our proposed algorithm
outperformed the approach by producing a far better MCC score of 53.04% and an
F1 score of 55%. The results obtained signify the superiority of the iBRF
algorithm and its potential to be an effective sampling technique in imbalanced
learning.
\\ ( https://arxiv.org/abs/2403.09867 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09889
Date: Thu, 14 Mar 2024 21:48:00 GMT   (249kb,D)

Title: Generalization of Scaled Deep ResNets in the Mean-Field Regime
Authors: Yihang Chen, Fanghui Liu, Yiping Lu, Grigorios G. Chrysos, Volkan
  Cevher
Categories: cs.LG
Comments: ICLR 2024 (Spotlight)
\\
  Despite the widespread empirical success of ResNet, the generalization
properties of deep ResNet are rarely explored beyond the lazy training regime.
In this work, we investigate \emph{scaled} ResNet in the limit of infinitely
deep and wide neural networks, of which the gradient flow is described by a
partial differential equation in the large-neural network limit, i.e., the
\emph{mean-field} regime. To derive the generalization bounds under this
setting, our analysis necessitates a shift from the conventional time-invariant
Gram matrix employed in the lazy training regime to a time-variant,
distribution-dependent version. To this end, we provide a global lower bound on
the minimum eigenvalue of the Gram matrix under the mean-field regime. Besides,
for the traceability of the dynamic of Kullback-Leibler (KL) divergence, we
establish the linear convergence of the empirical error and estimate the upper
bound of the KL divergence over parameters distribution. Finally, we build the
uniform convergence for generalization bound via Rademacher complexity. Our
results offer new insights into the generalization ability of deep ResNet
beyond the lazy training regime and contribute to advancing the understanding
of the fundamental properties of deep neural networks.
\\ ( https://arxiv.org/abs/2403.09889 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09898
Date: Thu, 14 Mar 2024 22:19:37 GMT   (504kb,D)

Title: TimeMachine: A Time Series is Worth 4 Mambas for Long-term Forecasting
Authors: Md Atik Ahamed, Qiang Cheng
Categories: cs.LG
\\
  Long-term time-series forecasting remains challenging due to the difficulty
in capturing long-term dependencies, achieving linear scalability, and
maintaining computational efficiency. We introduce TimeMachine, an innovative
model that leverages Mamba, a state-space model, to capture long-term
dependencies in multivariate time series data while maintaining linear
scalability and small memory footprints. TimeMachine exploits the unique
properties of time series data to produce salient contextual cues at
multi-scales and leverage an innovative integrated quadruple-Mamba architecture
to unify the handling of channel-mixing and channel-independence situations,
thus enabling effective selection of contents for prediction against global and
local contexts at different scales. Experimentally, TimeMachine achieves
superior performance in prediction accuracy, scalability, and memory
efficiency, as extensively validated using benchmark datasets. Code
availability: https://github.com/Atik-Ahamed/TimeMachine
\\ ( https://arxiv.org/abs/2403.09898 ,  504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09901
Date: Thu, 14 Mar 2024 22:25:37 GMT   (10197kb,D)

Title: Robust Subgraph Learning by Monitoring Early Training Representations
Authors: Sepideh Neshatfar, Salimeh Yasaei Sekeh
Categories: cs.LG cs.CR
\\
  Graph neural networks (GNNs) have attracted significant attention for their
outstanding performance in graph learning and node classification tasks.
However, their vulnerability to adversarial attacks, particularly through
susceptible nodes, poses a challenge in decision-making. The need for robust
graph summarization is evident in adversarial challenges resulting from the
propagation of attacks throughout the entire graph. In this paper, we address
both performance and adversarial robustness in graph input by introducing the
novel technique SHERD (Subgraph Learning Hale through Early Training
Representation Distances). SHERD leverages information from layers of a
partially trained graph convolutional network (GCN) to detect susceptible nodes
during adversarial attacks using standard distance metrics. The method
identifies "vulnerable (bad)" nodes and removes such nodes to form a robust
subgraph while maintaining node classification performance. Through our
experiments, we demonstrate the increased performance of SHERD in enhancing
robustness by comparing the network's performance on original and subgraph
inputs against various baselines alongside existing adversarial attacks. Our
experiments across multiple datasets, including citation datasets such as Cora,
Citeseer, and Pubmed, as well as microanatomical tissue structures of cell
graphs in the placenta, highlight that SHERD not only achieves substantial
improvement in robust performance but also outperforms several baselines in
terms of node classification accuracy and computational complexity.
\\ ( https://arxiv.org/abs/2403.09901 ,  10197kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09904
Date: Thu, 14 Mar 2024 22:29:59 GMT   (2524kb,D)

Title: FedComLoc: Communication-Efficient Distributed Training of Sparse and
  Quantized Models
Authors: Kai Yi, Georg Meinhardt, Laurent Condat, Peter Richt\'arik
Categories: cs.LG cs.AI cs.DC
\\
  Federated Learning (FL) has garnered increasing attention due to its unique
characteristic of allowing heterogeneous clients to process their private data
locally and interact with a central server, while being respectful of privacy.
A critical bottleneck in FL is the communication cost. A pivotal strategy to
mitigate this burden is \emph{Local Training}, which involves running multiple
local stochastic gradient descent iterations between communication phases. Our
work is inspired by the innovative \emph{Scaffnew} algorithm, which has
considerably advanced the reduction of communication complexity in FL. We
introduce FedComLoc (Federated Compressed and Local Training), integrating
practical and effective compression into \emph{Scaffnew} to further enhance
communication efficiency. Extensive experiments, using the popular TopK
compressor and quantization, demonstrate its prowess in substantially reducing
communication overheads in heterogeneous settings.
\\ ( https://arxiv.org/abs/2403.09904 ,  2524kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09930
Date: Fri, 15 Mar 2024 00:09:47 GMT   (15268kb,D)

Title: Quality-Diversity Actor-Critic: Learning High-Performing and Diverse
  Behaviors via Value and Successor Features Critics
Authors: Luca Grillotti, Maxence Faldor, Borja G. Le\'on, Antoine Cully
  (Imperial College London)
Categories: cs.LG cs.AI
Comments: The first two authors contributed equally to this work
\\
  A key aspect of intelligence is the ability to demonstrate a broad spectrum
of behaviors for adapting to unexpected situations. Over the past decade,
advancements in deep reinforcement learning have led to groundbreaking
achievements to solve complex continuous control tasks. However, most
approaches return only one solution specialized for a specific problem. We
introduce Quality-Diversity Actor-Critic (QDAC), an off-policy actor-critic
deep reinforcement learning algorithm that leverages a value function critic
and a successor features critic to learn high-performing and diverse behaviors.
In this framework, the actor optimizes an objective that seamlessly unifies
both critics using constrained optimization to (1) maximize return, while (2)
executing diverse skills. Compared with other Quality-Diversity methods, QDAC
achieves significantly higher performance and more diverse behaviors on six
challenging continuous control locomotion tasks. We also demonstrate that we
can harness the learned skills to adapt better than other baselines to five
perturbed environments. Finally, qualitative analyses showcase a range of
remarkable behaviors, available at: http://bit.ly/qdac.
\\ ( https://arxiv.org/abs/2403.09930 ,  15268kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09940
Date: Fri, 15 Mar 2024 00:45:36 GMT   (1950kb)

Title: Global Convergence Guarantees for Federated Policy Gradient Methods with
  Adversaries
Authors: Swetha Ganesh, Jiayu Chen, Gugan Thoppe, Vaneet Aggarwal
Categories: cs.LG cs.AI math.OC
Comments: 27 pages, 6 figures
\\
  Federated Reinforcement Learning (FRL) allows multiple agents to
collaboratively build a decision making policy without sharing raw
trajectories. However, if a small fraction of these agents are adversarial, it
can lead to catastrophic results. We propose a policy gradient based approach
that is robust to adversarial agents which can send arbitrary values to the
server. Under this setting, our results form the first global convergence
guarantees with general parametrization. These results demonstrate resilience
with adversaries, while achieving sample complexity of order
$\tilde{\mathcal{O}}\left( \frac{1}{\epsilon^2} \left( \frac{1}{N-f} +
\frac{f^2}{(N-f)^2}\right)\right)$, where $N$ is the total number of agents and
$f$ is the number of adversarial agents.
\\ ( https://arxiv.org/abs/2403.09940 ,  1950kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09953
Date: Fri, 15 Mar 2024 01:28:08 GMT   (6841kb,D)

Title: Online GNN Evaluation Under Test-time Graph Distribution Shifts
Authors: Xin Zheng, Dongjin Song, Qingsong Wen, Bo Du, Shirui Pan
Categories: cs.LG
Comments: Accepted by ICLR-2024
\\
  Evaluating the performance of a well-trained GNN model on real-world graphs
is a pivotal step for reliable GNN online deployment and serving. Due to a lack
of test node labels and unknown potential training-test graph data distribution
shifts, conventional model evaluation encounters limitations in calculating
performance metrics (e.g., test error) and measuring graph data-level
discrepancies, particularly when the training graph used for developing GNNs
remains unobserved during test time. In this paper, we study a new research
problem, online GNN evaluation, which aims to provide valuable insights into
the well-trained GNNs's ability to effectively generalize to real-world
unlabeled graphs under the test-time graph distribution shifts. Concretely, we
develop an effective learning behavior discrepancy score, dubbed LeBeD, to
estimate the test-time generalization errors of well-trained GNN models.
Through a novel GNN re-training strategy with a parameter-free optimality
criterion, the proposed LeBeD comprehensively integrates learning behavior
discrepancies from both node prediction and structure reconstruction
perspectives. This enables the effective evaluation of the well-trained GNNs'
ability to capture test node semantics and structural representations, making
it an expressive metric for estimating the generalization error in online GNN
evaluation. Extensive experiments on real-world test graphs under diverse graph
distribution shifts could verify the effectiveness of the proposed method,
revealing its strong correlation with ground-truth test errors on various
well-trained GNN models.
\\ ( https://arxiv.org/abs/2403.09953 ,  6841kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09969
Date: Fri, 15 Mar 2024 02:25:04 GMT   (946kb,D)

Title: Prediction of Vessel Arrival Time to Pilotage Area Using Multi-Data
  Fusion and Deep Learning
Authors: Xiaocai Zhang, Xiuju Fu, Zhe Xiao, Haiyan Xu, Xiaoyang Wei, Jimmy Koh,
  Daichi Ogawa, Zheng Qin
Categories: cs.LG
Comments: The 26th IEEE International Conference on Intelligent Transportation
  Systems (ITSC 2023)
\\
  This paper investigates the prediction of vessels' arrival time to the
pilotage area using multi-data fusion and deep learning approaches. Firstly,
the vessel arrival contour is extracted based on Multivariate Kernel Density
Estimation (MKDE) and clustering. Secondly, multiple data sources, including
Automatic Identification System (AIS), pilotage booking information, and
meteorological data, are fused before latent feature extraction. Thirdly, a
Temporal Convolutional Network (TCN) framework that incorporates a residual
mechanism is constructed to learn the hidden arrival patterns of the vessels.
Extensive tests on two real-world data sets from Singapore have been conducted
and the following promising results have been obtained: 1) fusion of pilotage
booking information and meteorological data improves the prediction accuracy,
with pilotage booking information having a more significant impact; 2) using
discrete embedding for the meteorological data performs better than using
continuous embedding; 3) the TCN outperforms the state-of-the-art baseline
methods in regression tasks, exhibiting Mean Absolute Error (MAE) ranging from
4.58 min to 4.86 min; and 4) approximately 89.41% to 90.61% of the absolute
prediction residuals fall within a time frame of 10 min.
\\ ( https://arxiv.org/abs/2403.09969 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09976
Date: Fri, 15 Mar 2024 02:46:19 GMT   (5265kb,D)

Title: AD3: Implicit Action is the Key for World Models to Distinguish the
  Diverse Visual Distractors
Authors: Yucen Wang, Shenghua Wan, Le Gan, Shuai Feng, De-Chuan Zhan
Categories: cs.LG cs.CV
\\
  Model-based methods have significantly contributed to distinguishing
task-irrelevant distractors for visual control. However, prior research has
primarily focused on heterogeneous distractors like noisy background videos,
leaving homogeneous distractors that closely resemble controllable agents
largely unexplored, which poses significant challenges to existing methods. To
tackle this problem, we propose Implicit Action Generator (IAG) to learn the
implicit actions of visual distractors, and present a new algorithm named
implicit Action-informed Diverse visual Distractors Distinguisher (AD3), that
leverages the action inferred by IAG to train separated world models. Implicit
actions effectively capture the behavior of background distractors, aiding in
distinguishing the task-irrelevant components, and the agent can optimize the
policy within the task-relevant state space. Our method achieves superior
performance on various visual control tasks featuring both heterogeneous and
homogeneous distractors. The indispensable role of implicit actions learned by
IAG is also empirically validated.
\\ ( https://arxiv.org/abs/2403.09976 ,  5265kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10045
Date: Fri, 15 Mar 2024 06:31:03 GMT   (5059kb,D)

Title: Towards Adversarially Robust Dataset Distillation by Curvature
  Regularization
Authors: Eric Xue, Yijiang Li, Haoyang Liu, Yifan Shen, Haohan Wang
Categories: cs.LG cs.CV
Comments: 17 pages, 3 figures
\\
  Dataset distillation (DD) allows datasets to be distilled to fractions of
their original size while preserving the rich distributional information so
that models trained on the distilled datasets can achieve a comparable accuracy
while saving significant computational loads. Recent research in this area has
been focusing on improving the accuracy of models trained on distilled
datasets. In this paper, we aim to explore a new perspective of DD. We study
how to embed adversarial robustness in distilled datasets, so that models
trained on these datasets maintain the high accuracy and meanwhile acquire
better adversarial robustness. We propose a new method that achieves this goal
by incorporating curvature regularization into the distillation process with
much less computational overhead than standard adversarial training. Extensive
empirical experiments suggest that our method not only outperforms standard
adversarial training on both accuracy and robustness with less computation
overhead but is also capable of generating robust distilled datasets that can
withstand various adversarial attacks.
\\ ( https://arxiv.org/abs/2403.10045 ,  5059kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10063
Date: Fri, 15 Mar 2024 07:05:44 GMT   (342kb,D)

Title: Unified Projection-Free Algorithms for Adversarial DR-Submodular
  Optimization
Authors: Mohammad Pedramfar, Yididiya Y. Nadew, Christopher J. Quinn, Vaneet
  Aggarwal
Categories: cs.LG cs.AI cs.CC math.OC
Comments: Accepted for publication at ICLR 2024
\\
  This paper introduces unified projection-free Frank-Wolfe type algorithms for
adversarial continuous DR-submodular optimization, spanning scenarios such as
full information and (semi-)bandit feedback, monotone and non-monotone
functions, different constraints, and types of stochastic queries. For every
problem considered in the non-monotone setting, the proposed algorithms are
either the first with proven sub-linear $\alpha$-regret bounds or have better
$\alpha$-regret bounds than the state of the art, where $\alpha$ is a
corresponding approximation bound in the offline setting. In the monotone
setting, the proposed approach gives state-of-the-art sub-linear
$\alpha$-regret bounds among projection-free algorithms in 7 of the 8
considered cases while matching the result of the remaining case. Additionally,
this paper addresses semi-bandit and bandit feedback for adversarial
DR-submodular optimization, advancing the understanding of this optimization
area.
\\ ( https://arxiv.org/abs/2403.10063 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10097
Date: Fri, 15 Mar 2024 08:26:59 GMT   (1123kb,D)

Title: Adaptive Random Feature Regularization on Fine-tuning Deep Neural
  Networks
Authors: Shin'ya Yamaguchi, Sekitoshi Kanai, Kazuki Adachi, Daiki Chijiwa
Categories: cs.LG cs.AI cs.CV
Comments: Accepted to CVPR 2024
\\
  While fine-tuning is a de facto standard method for training deep neural
networks, it still suffers from overfitting when using small target datasets.
Previous methods improve fine-tuning performance by maintaining knowledge of
the source datasets or introducing regularization terms such as contrastive
loss. However, these methods require auxiliary source information (e.g., source
labels or datasets) or heavy additional computations. In this paper, we propose
a simple method called adaptive random feature regularization (AdaRand).
AdaRand helps the feature extractors of training models to adaptively change
the distribution of feature vectors for downstream classification tasks without
auxiliary source information and with reasonable computation costs. To this
end, AdaRand minimizes the gap between feature vectors and random reference
vectors that are sampled from class conditional Gaussian distributions.
Furthermore, AdaRand dynamically updates the conditional distribution to follow
the currently updated feature extractors and balance the distance between
classes in feature spaces. Our experiments show that AdaRand outperforms the
other fine-tuning regularization, which requires auxiliary source information
and heavy computation costs.
\\ ( https://arxiv.org/abs/2403.10097 ,  1123kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10110
Date: Fri, 15 Mar 2024 08:54:25 GMT   (223kb,D)

Title: Meta Operator for Complex Query Answering on Knowledge Graphs
Authors: Hang Yin, Zihao Wang, Yangqiu Song
Categories: cs.LG cs.AI cs.LO
\\
  Knowledge graphs contain informative factual knowledge but are considered
incomplete. To answer complex queries under incomplete knowledge,
learning-based Complex Query Answering (CQA) models are proposed to directly
learn from the query-answer samples to avoid the direct traversal of incomplete
graph data. Existing works formulate the training of complex query answering
models as multi-task learning and require a large number of training samples.
In this work, we explore the compositional structure of complex queries and
argue that the different logical operator types, rather than the different
complex query types, are the key to improving generalizability. Accordingly, we
propose a meta-learning algorithm to learn the meta-operators with limited data
and adapt them to different instances of operators under various complex
queries. Empirical results show that learning meta-operators is more effective
than learning original CQA or meta-CQA models.
\\ ( https://arxiv.org/abs/2403.10110 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10123
Date: Fri, 15 Mar 2024 09:14:18 GMT   (616kb,D)

Title: Regularization-Based Efficient Continual Learning in Deep State-Space
  Models
Authors: Yuanhang Zhang, Zhidi Lin, Yiyong Sun, Feng Yin, Carsten Fritsche
Categories: cs.LG
Comments: 7 pages, 14 figures
\\
  Deep state-space models (DSSMs) have gained popularity in recent years due to
their potent modeling capacity for dynamic systems. However, existing DSSM
works are limited to single-task modeling, which requires retraining with
historical task data upon revisiting a forepassed task. To address this
limitation, we propose continual learning DSSMs (CLDSSMs), which are capable of
adapting to evolving tasks without catastrophic forgetting. Our proposed
CLDSSMs integrate mainstream regularization-based continual learning (CL)
methods, ensuring efficient updates with constant computational and memory
costs for modeling multiple dynamic systems. We also conduct a comprehensive
cost analysis of each CL method applied to the respective CLDSSMs, and
demonstrate the efficacy of CLDSSMs through experiments on real-world datasets.
The results corroborate that while various competing CL methods exhibit
different merits, the proposed CLDSSMs consistently outperform traditional
DSSMs in terms of effectively addressing catastrophic forgetting, enabling
swift and accurate parameter transfer to new tasks.
\\ ( https://arxiv.org/abs/2403.10123 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10158
Date: Fri, 15 Mar 2024 10:01:19 GMT   (754kb,D)

Title: Functional Graph Convolutional Networks: A unified multi-task and
  multi-modal learning framework to facilitate health and social-care insights
Authors: Tobia Boschi, Francesca Bonin, Rodrigo Ordonez-Hurtado, C\'ecile
  Rosseau, Alessandra Pascale, and John Dinsmore
Categories: cs.LG cs.AI
\\
  This paper introduces a novel Functional Graph Convolutional Network (funGCN)
framework that combines Functional Data Analysis and Graph Convolutional
Networks to address the complexities of multi-task and multi-modal learning in
digital health and longitudinal studies. With the growing importance of health
solutions to improve health care and social support, ensure healthy lives, and
promote well-being at all ages, funGCN offers a unified approach to handle
multivariate longitudinal data for multiple entities and ensures
interpretability even with small sample sizes. Key innovations include
task-specific embedding components that manage different data types, the
ability to perform classification, regression, and forecasting, and the
creation of a knowledge graph for insightful data interpretation. The efficacy
of funGCN is validated through simulation experiments and a real-data
application.
\\ ( https://arxiv.org/abs/2403.10158 ,  754kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10160
Date: Fri, 15 Mar 2024 10:11:26 GMT   (10633kb,D)

Title: Online Policy Learning from Offline Preferences
Authors: Guoxi Zhang and Han Bao and Hisashi Kashima
Categories: cs.LG
\\
  In preference-based reinforcement learning (PbRL), a reward function is
learned from a type of human feedback called preference. To expedite preference
collection, recent works have leveraged \emph{offline preferences}, which are
preferences collected for some offline data. In this scenario, the learned
reward function is fitted on the offline data. If a learning agent exhibits
behaviors that do not overlap with the offline data, the learned reward
function may encounter generalizability issues. To address this problem, the
present study introduces a framework that consolidates offline preferences and
\emph{virtual preferences} for PbRL, which are comparisons between the agent's
behaviors and the offline data. Critically, the reward function can track the
agent's behaviors using the virtual preferences, thereby offering well-aligned
guidance to the agent. Through experiments on continuous control tasks, this
study demonstrates the effectiveness of incorporating the virtual preferences
in PbRL.
\\ ( https://arxiv.org/abs/2403.10160 ,  10633kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10168
Date: Fri, 15 Mar 2024 10:22:48 GMT   (495kb,D)

Title: Explainability through uncertainty: Trustworthy decision-making with
  neural networks
Authors: Arthur Thuy, Dries F. Benoit
Categories: cs.LG cs.CY stat.ML
Comments: Accepted Manuscript version of an article published in the European
  Journal of Operational Research
DOI: 10.1016/j.ejor.2023.09.009
\\
  Uncertainty is a key feature of any machine learning model and is
particularly important in neural networks, which tend to be overconfident. This
overconfidence is worrying under distribution shifts, where the model
performance silently degrades as the data distribution diverges from the
training data distribution. Uncertainty estimation offers a solution to
overconfident models, communicating when the output should (not) be trusted.
Although methods for uncertainty estimation have been developed, they have not
been explicitly linked to the field of explainable artificial intelligence
(XAI). Furthermore, literature in operations research ignores the actionability
component of uncertainty estimation and does not consider distribution shifts.
This work proposes a general uncertainty framework, with contributions being
threefold: (i) uncertainty estimation in ML models is positioned as an XAI
technique, giving local and model-specific explanations; (ii) classification
with rejection is used to reduce misclassifications by bringing a human expert
in the loop for uncertain observations; (iii) the framework is applied to a
case study on neural networks in educational data mining subject to
distribution shifts. Uncertainty as XAI improves the model's trustworthiness in
downstream decision-making tasks, giving rise to more actionable and robust
machine learning systems in operations research.
\\ ( https://arxiv.org/abs/2403.10168 ,  495kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10175
Date: Fri, 15 Mar 2024 10:31:46 GMT   (313kb,D)

Title: A Short Survey on Importance Weighting for Machine Learning
Authors: Masanari Kimura and Hideitsu Hino
Categories: cs.LG cs.AI stat.ML
\\
  Importance weighting is a fundamental procedure in statistics and machine
learning that weights the objective function or probability distribution based
on the importance of the instance in some sense. The simplicity and usefulness
of the idea has led to many applications of importance weighting. For example,
it is known that supervised learning under an assumption about the difference
between the training and test distributions, called distribution shift, can
guarantee statistically desirable properties through importance weighting by
their density ratio. This survey summarizes the broad applications of
importance weighting in machine learning and related research.
\\ ( https://arxiv.org/abs/2403.10175 ,  313kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10182
Date: Fri, 15 Mar 2024 10:38:48 GMT   (6797kb,D)

Title: Reliable uncertainty with cheaper neural network ensembles: a case study
  in industrial parts classification
Authors: Arthur Thuy, Dries F. Benoit
Categories: cs.LG stat.ML
Comments: Submitted to Annals of Operations Research
\\
  In operations research (OR), predictive models often encounter
out-of-distribution (OOD) scenarios where the data distribution differs from
the training data distribution. In recent years, neural networks (NNs) are
gaining traction in OR for their exceptional performance in fields such as
image classification. However, NNs tend to make confident yet incorrect
predictions when confronted with OOD data. Uncertainty estimation offers a
solution to overconfident models, communicating when the output should (not) be
trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR
domain. Deep ensembles, composed of multiple independent NNs, have emerged as a
promising approach, offering not only strong predictive accuracy but also
reliable uncertainty estimation. However, their deployment is challenging due
to substantial computational demands. Recent fundamental research has proposed
more efficient NN ensembles, namely the snapshot, batch, and multi-input
multi-output ensemble. This study is the first to provide a comprehensive
comparison of a single NN, a deep ensemble, and the three efficient NN
ensembles. In addition, we propose a Diversity Quality metric to quantify the
ensembles' performance on the in-distribution and OOD sets in one single
metric. The OR case study discusses industrial parts classification to identify
and manage spare parts, important for timely maintenance of industrial plants.
The results highlight the batch ensemble as a cost-effective and competitive
alternative to the deep ensemble. It outperforms the deep ensemble in both
uncertainty and accuracy while exhibiting a training time speedup of 7x, a test
time speedup of 8x, and 9x memory savings.
\\ ( https://arxiv.org/abs/2403.10182 ,  6797kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10220
Date: Fri, 15 Mar 2024 11:39:12 GMT   (1668kb,D)

Title: From Chaos to Clarity: Time Series Anomaly Detection in Astronomical
  Observations
Authors: Xinli Hao, Yile Chen, Chen Yang, Zhihui Du, Chaohong Ma, Chao Wu,
  Xiaofeng Meng
Categories: cs.LG cs.AI
Comments: accepted by ICDE 2024
\\
  With the development of astronomical facilities, large-scale time series data
observed by these facilities is being collected. Analyzing anomalies in these
astronomical observations is crucial for uncovering potential celestial events
and physical phenomena, thus advancing the scientific research process.
However, existing time series anomaly detection methods fall short in tackling
the unique characteristics of astronomical observations where each star is
inherently independent but interfered by random concurrent noise, resulting in
a high rate of false alarms. To overcome the challenges, we propose AERO, a
novel two-stage framework tailored for unsupervised anomaly detection in
astronomical observations. In the first stage, we employ a Transformer-based
encoder-decoder architecture to learn the normal temporal patterns on each
variate (i.e., star) in alignment with the characteristic of variate
independence. In the second stage, we enhance the graph neural network with a
window-wise graph structure learning to tackle the occurrence of concurrent
noise characterized by spatial and temporal randomness. In this way, AERO is
not only capable of distinguishing normal temporal patterns from potential
anomalies but also effectively differentiating concurrent noise, thus
decreasing the number of false alarms. We conducted extensive experiments on
three synthetic datasets and three real-world datasets. The results demonstrate
that AERO outperforms the compared baselines. Notably, compared to the
state-of-the-art model, AERO improves the F1-score by up to 8.76% and 2.63% on
synthetic and real-world datasets respectively.
\\ ( https://arxiv.org/abs/2403.10220 ,  1668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10231
Date: Fri, 15 Mar 2024 12:00:12 GMT   (15374kb,D)

Title: Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge
  Graphs
Authors: Zhanke Zhou, Yongqi Zhang, Jiangchao Yao, Quanming Yao, Bo Han
Categories: cs.LG cs.AI cs.SI
Comments: 32 pages, 43 figures
\\
  To deduce new facts on a knowledge graph (KG), a link predictor learns from
the graph structure and collects local evidence to find the answer to a given
query. However, existing methods suffer from a severe scalability problem due
to the utilization of the whole KG for prediction, which hinders their promise
on large scale KGs and cannot be directly addressed by vanilla sampling
methods. In this work, we propose the one-shot-subgraph link prediction to
achieve efficient and adaptive prediction. The design principle is that,
instead of directly acting on the whole KG, the prediction procedure is
decoupled into two steps, i.e., (i) extracting only one subgraph according to
the query and (ii) predicting on this single, query dependent subgraph. We
reveal that the non-parametric and computation-efficient heuristics
Personalized PageRank (PPR) can effectively identify the potential answers and
supporting evidence. With efficient subgraph-based prediction, we further
introduce the automated searching of the optimal configurations in both data
and model spaces. Empirically, we achieve promoted efficiency and leading
performances on five large-scale benchmarks. The code is publicly available at:
https://github.com/tmlr-group/one-shot-subgraph.
\\ ( https://arxiv.org/abs/2403.10231 ,  15374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10253
Date: Fri, 15 Mar 2024 12:43:03 GMT   (3226kb,D)

Title: Open Continual Feature Selection via Granular-Ball Knowledge Transfer
Authors: Xuemei Cao, Xin Yang, Shuyin Xia, Guoyin Wang, Tianrui Li
Categories: cs.LG
Comments: 14 pages, 7 figures, 6 tables
\\
  This paper presents a novel framework for continual feature selection (CFS)
in data preprocessing, particularly in the context of an open and dynamic
environment where unknown classes may emerge. CFS encounters two primary
challenges: the discovery of unknown knowledge and the transfer of known
knowledge. To this end, the proposed CFS method combines the strengths of
continual learning (CL) with granular-ball computing (GBC), which focuses on
constructing a granular-ball knowledge base to detect unknown classes and
facilitate the transfer of previously learned knowledge for further feature
selection. CFS consists of two stages: initial learning and open learning. The
former aims to establish an initial knowledge base through multi-granularity
representation using granular-balls. The latter utilizes prior granular-ball
knowledge to identify unknowns, updates the knowledge base for granular-ball
knowledge transfer, reinforces old knowledge, and integrates new knowledge.
Subsequently, we devise an optimal feature subset mechanism that incorporates
minimal new features into the existing optimal subset, often yielding superior
results during each period. Extensive experimental results on public benchmark
datasets demonstrate our method's superiority in terms of both effectiveness
and efficiency compared to state-of-the-art feature selection methods.
\\ ( https://arxiv.org/abs/2403.10253 ,  3226kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10259
Date: Fri, 15 Mar 2024 12:47:45 GMT   (333kb)

Title: Comprehensive Study Of Predictive Maintenance In Industries Using
  Classification Models And LSTM Model
Authors: Saket Maheshwari, Sambhav Tiwari, Shyam Rai, Satyam Vinayak Daman
  Pratap Singh
Categories: cs.LG cs.AI
\\
  In today's technology-driven era, the imperative for predictive maintenance
and advanced diagnostics extends beyond aviation to encompass the
identification of damages, failures, and operational defects in rotating and
moving machines. Implementing such services not only curtails maintenance costs
but also extends machine lifespan, ensuring heightened operational efficiency.
Moreover, it serves as a preventive measure against potential accidents or
catastrophic events. The advent of Artificial Intelligence (AI) has
revolutionized maintenance across industries, enabling more accurate and
efficient prediction and analysis of machine failures, thereby conserving time
and resources. Our proposed study aims to delve into various machine learning
classification techniques, including Support Vector Machine (SVM), Random
Forest, Logistic Regression, and Convolutional Neural Network LSTM-Based, for
predicting and analyzing machine performance. SVM classifies data into
different categories based on their positions in a multidimensional space,
while Random Forest employs ensemble learning to create multiple decision trees
for classification. Logistic Regression predicts the probability of binary
outcomes using input data. The primary objective of the study is to assess
these algorithms' performance in predicting and analyzing machine performance,
considering factors such as accuracy, precision, recall, and F1 score. The
findings will aid maintenance experts in selecting the most suitable machine
learning algorithm for effective prediction and analysis of machine
performance.
\\ ( https://arxiv.org/abs/2403.10259 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10318
Date: Fri, 15 Mar 2024 14:09:46 GMT   (1803kb,D)

Title: Anytime Neural Architecture Search on Tabular Data
Authors: Naili Xing, Shaofeng Cai, Zhaojing Luo, BengChin Ooi, Jian Pei
Categories: cs.LG
\\
  The increasing demand for tabular data analysis calls for transitioning from
manual architecture design to Neural Architecture Search (NAS). This transition
demands an efficient and responsive anytime NAS approach that is capable of
returning current optimal architectures within any given time budget while
progressively enhancing architecture quality with increased budget allocation.
However, the area of research on Anytime NAS for tabular data remains
unexplored. To this end, we introduce ATLAS, the first anytime NAS approach
tailored for tabular data. ATLAS introduces a novel two-phase
filtering-and-refinement optimization scheme with joint optimization, combining
the strengths of both paradigms of training-free and training-based
architecture evaluation. Specifically, in the filtering phase, ATLAS employs a
new zero-cost proxy specifically designed for tabular data to efficiently
estimate the performance of candidate architectures, thereby obtaining a set of
promising architectures. Subsequently, in the refinement phase, ATLAS leverages
a fixed-budget search algorithm to schedule the training of the promising
candidates, so as to accurately identify the optimal architecture. To jointly
optimize the two phases for anytime NAS, we also devise a budget-aware
coordinator that delivers high NAS performance within constraints. Experimental
evaluations demonstrate that our ATLAS can obtain a good-performing
architecture within any predefined time budget and return better architectures
as and when a new time budget is made available. Overall, it reduces the search
time on tabular data by up to 82.75x compared to existing NAS approaches.
\\ ( https://arxiv.org/abs/2403.10318 ,  1803kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10330
Date: Fri, 15 Mar 2024 14:18:21 GMT   (4936kb,D)

Title: Towards Non-Adversarial Algorithmic Recourse
Authors: Tobias Leemann, Martin Pawelczyk, Bardh Prenkaj, Gjergji Kasneci
Categories: cs.LG
\\
  The streams of research on adversarial examples and counterfactual
explanations have largely been growing independently. This has led to several
recent works trying to elucidate their similarities and differences. Most
prominently, it has been argued that adversarial examples, as opposed to
counterfactual explanations, have a unique characteristic in that they lead to
a misclassification compared to the ground truth. However, the computational
goals and methodologies employed in existing counterfactual explanation and
adversarial example generation methods often lack alignment with this
requirement. Using formal definitions of adversarial examples and
counterfactual explanations, we introduce non-adversarial algorithmic recourse
and outline why in high-stakes situations, it is imperative to obtain
counterfactual explanations that do not exhibit adversarial characteristics. We
subsequently investigate how different components in the objective functions,
e.g., the machine learning model or cost function used to measure distance,
determine whether the outcome can be considered an adversarial example or not.
Our experiments on common datasets highlight that these design choices are
often more critical in deciding whether recourse is non-adversarial than
whether recourse or attack algorithms are used. Furthermore, we show that
choosing a robust and accurate machine learning model results in less
adversarial recourse desired in practice.
\\ ( https://arxiv.org/abs/2403.10330 ,  4936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10339
Date: Fri, 15 Mar 2024 14:26:53 GMT   (1638kb,D)

Title: Generation is better than Modification: Combating High Class Homophily
  Variance in Graph Anomaly Detection
Authors: Rui Zhang, Dawei Cheng, Xin Liu, Jie Yang, Yi Ouyang, Xian Wu, Yefeng
  Zheng
Categories: cs.LG
\\
  Graph-based anomaly detection is currently an important research topic in the
field of graph neural networks (GNNs). We find that in graph anomaly detection,
the homophily distribution differences between different classes are
significantly greater than those in homophilic and heterophilic graphs. For the
first time, we introduce a new metric called Class Homophily Variance, which
quantitatively describes this phenomenon. To mitigate its impact, we propose a
novel GNN model named Homophily Edge Generation Graph Neural Network (HedGe).
Previous works typically focused on pruning, selecting or connecting on
original relationships, and we refer to these methods as modifications.
Different from these works, our method emphasizes generating new relationships
with low class homophily variance, using the original relationships as an
auxiliary. HedGe samples homophily adjacency matrices from scratch using a
self-attention mechanism, and leverages nodes that are relevant in the feature
space but not directly connected in the original graph. Additionally, we modify
the loss function to punish the generation of unnecessary heterophilic edges by
the model. Extensive comparison experiments demonstrate that HedGe achieved the
best performance across multiple benchmark datasets, including anomaly
detection and edgeless node classification. The proposed model also improves
the robustness under the novel Heterophily Attack with increased class
homophily variance on other graph classification tasks.
\\ ( https://arxiv.org/abs/2403.10339 ,  1638kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10371
Date: Fri, 15 Mar 2024 15:01:48 GMT   (1991kb)

Title: An Energy-Efficient Ensemble Approach for Mitigating Data Incompleteness
  in IoT Applications
Authors: Yousef AlShehri and Lakshmish Ramaswamy
Categories: cs.LG cs.AI cs.NI
Comments: 8 pages, 8 figures, 1 table, Accepted as a conference paper at IEEE
  INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING IN SMART SYSTEMS AND THE
  INTERNET OF THINGS (DCOSS-IoT 2024)
\\
  Machine Learning (ML) is becoming increasingly important for IoT-based
applications. However, the dynamic and ad-hoc nature of many IoT ecosystems
poses unique challenges to the efficacy of ML algorithms. One such challenge is
data incompleteness, which is manifested as missing sensor readings. Many
factors, including sensor failures and/or network disruption, can cause data
incompleteness. Furthermore, most IoT systems are severely power-constrained.
It is important that we build IoT-based ML systems that are robust against data
incompleteness while simultaneously being energy efficient. This paper presents
an empirical study of SECOE - a recent technique for alleviating data
incompleteness in IoT - with respect to its energy bottlenecks. Towards
addressing the energy bottlenecks of SECOE, we propose ENAMLE - a proactive,
energy-aware technique for mitigating the impact of concurrent missing data.
ENAMLE is unique in the sense that it builds an energy-aware ensemble of
sub-models, each trained with a subset of sensors chosen carefully based on
their correlations. Furthermore, at inference time, ENAMLE adaptively alters
the number of the ensemble of models based on the amount of missing data rate
and the energy-accuracy trade-off. ENAMLE's design includes several novel
mechanisms for minimizing energy consumption while maintaining accuracy. We
present extensive experimental studies on two distinct datasets that
demonstrate the energy efficiency of ENAMLE and its ability to alleviate sensor
failures.
\\ ( https://arxiv.org/abs/2403.10371 ,  1991kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10373
Date: Fri, 15 Mar 2024 15:04:20 GMT   (67kb,D)

Title: Towards a general framework for improving the performance of classifiers
  using XAI methods
Authors: Andrea Apicella, Salvatore Giugliano, Francesco Isgr\`o, Roberto
  Prevete
Categories: cs.LG
\\
  Modern Artificial Intelligence (AI) systems, especially Deep Learning (DL)
models, poses challenges in understanding their inner workings by AI
researchers. eXplainable Artificial Intelligence (XAI) inspects internal
mechanisms of AI models providing explanations about their decisions. While
current XAI research predominantly concentrates on explaining AI systems, there
is a growing interest in using XAI techniques to automatically improve the
performance of AI systems themselves. This paper proposes a general framework
for automatically improving the performance of pre-trained DL classifiers using
XAI methods, avoiding the computational overhead associated with retraining
complex models from scratch. In particular, we outline the possibility of two
different learning strategies for implementing this architecture, which we will
call auto-encoder-based and encoder-decoder-based, and discuss their key
aspects.
\\ ( https://arxiv.org/abs/2403.10373 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10379
Date: Fri, 15 Mar 2024 15:09:13 GMT   (397kb,D)

Title: Regret Minimization via Saddle Point Optimization
Authors: Johannes Kirschner, Seyed Alireza Bakhtiari, Kushagra Chandak,
  Volodymyr Tkachuk, Csaba Szepesv\'ari
Categories: cs.LG
\\
  A long line of works characterizes the sample complexity of regret
minimization in sequential decision-making by min-max programs. In the
corresponding saddle-point game, the min-player optimizes the sampling
distribution against an adversarial max-player that chooses confusing models
leading to large regret. The most recent instantiation of this idea is the
decision-estimation coefficient (DEC), which was shown to provide nearly tight
lower and upper bounds on the worst-case expected regret in structured bandits
and reinforcement learning. By re-parametrizing the offset DEC with the
confidence radius and solving the corresponding min-max program, we derive an
anytime variant of the Estimation-To-Decisions (E2D) algorithm. Importantly,
the algorithm optimizes the exploration-exploitation trade-off online instead
of via the analysis. Our formulation leads to a practical algorithm for finite
model classes and linear feedback models. We further point out connections to
the information ratio, decoupling coefficient and PAC-DEC, and numerically
evaluate the performance of E2D on simple examples.
\\ ( https://arxiv.org/abs/2403.10379 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10404
Date: Fri, 15 Mar 2024 15:37:19 GMT   (1757kb)

Title: A comparative study on machine learning approaches for rock mass
  classification using drilling data
Authors: Tom F. Hansen, Georg H. Erharter, Zhongqiang Liu, Jim Torresen
Categories: cs.LG cs.CV
\\
  Current rock engineering design in drill and blast tunnelling primarily
relies on engineers' observational assessments. Measure While Drilling (MWD)
data, a high-resolution sensor dataset collected during tunnel excavation, is
underutilised, mainly serving for geological visualisation. This study aims to
automate the translation of MWD data into actionable metrics for rock
engineering. It seeks to link data to specific engineering actions, thus
providing critical decision support for geological challenges ahead of the
tunnel face. Leveraging a large and geologically diverse dataset of 500,000
drillholes from 15 tunnels, the research introduces models for accurate rock
mass quality classification in a real-world tunnelling context. Both
conventional machine learning and image-based deep learning are explored to
classify MWD data into Q-classes and Q-values, examples of metrics describing
the stability of the rock mass, using both tabular and image data. The results
indicate that the K-nearest neighbours algorithm in an ensemble with tree-based
models using tabular data, effectively classifies rock mass quality. It
achieves a cross-validated balanced accuracy of 0.86 in classifying rock mass
into the Q-classes A, B, C, D, E1, E2, and 0.95 for a binary classification
with E versus the rest. Classification using a CNN with MWD-images for each
blasting round resulted in a balanced accuracy of 0.82 for binary
classification. Regressing the Q-value from tabular MWD-data achieved
cross-validated R2 and MSE scores of 0.80 and 0.18 for a similar ensemble model
as in classification. High performance in regression and classification boosts
confidence in automated rock mass assessment. Applying advanced modelling on a
unique dataset demonstrates MWD data's value in improving rock mass
classification accuracy and advancing data-driven rock engineering design,
reducing manual intervention.
\\ ( https://arxiv.org/abs/2403.10404 ,  1757kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10416
Date: Fri, 15 Mar 2024 15:51:27 GMT   (45kb,D)

Title: Robust Sparse Estimation for Gaussians with Optimal Error under Huber
  Contamination
Authors: Ilias Diakonikolas, Daniel M. Kane, Sushrut Karmalkar, Ankit Pensia,
  Thanasis Pittas
Categories: cs.LG cs.DS math.ST stat.ML stat.TH
\\
  We study Gaussian sparse estimation tasks in Huber's contamination model with
a focus on mean estimation, PCA, and linear regression. For each of these
tasks, we give the first sample and computationally efficient robust estimators
with optimal error guarantees, within constant factors. All prior efficient
algorithms for these tasks incur quantitatively suboptimal error. Concretely,
for Gaussian robust $k$-sparse mean estimation on $\mathbb{R}^d$ with
corruption rate $\epsilon>0$, our algorithm has sample complexity
$(k^2/\epsilon^2)\mathrm{polylog}(d/\epsilon)$, runs in sample polynomial time,
and approximates the target mean within $\ell_2$-error $O(\epsilon)$. Previous
efficient algorithms inherently incur error $\Omega(\epsilon
\sqrt{\log(1/\epsilon)})$. At the technical level, we develop a novel
multidimensional filtering method in the sparse regime that may find other
applications.
\\ ( https://arxiv.org/abs/2403.10416 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10424
Date: Fri, 15 Mar 2024 15:58:37 GMT   (112kb,D)

Title: Structured Evaluation of Synthetic Tabular Data
Authors: Scott Cheng-Hsin Yang, Baxter Eaves, Michael Schmidt, Ken Swanson, and
  Patrick Shafto
Categories: cs.LG stat.ML
\\
  Tabular data is common yet typically incomplete, small in volume, and
access-restricted due to privacy concerns. Synthetic data generation offers
potential solutions. Many metrics exist for evaluating the quality of synthetic
tabular data; however, we lack an objective, coherent interpretation of the
many metrics. To address this issue, we propose an evaluation framework with a
single, mathematical objective that posits that the synthetic data should be
drawn from the same distribution as the observed data. Through various
structural decomposition of the objective, this framework allows us to reason
for the first time the completeness of any set of metrics, as well as unifies
existing metrics, including those that stem from fidelity considerations,
downstream application, and model-based approaches. Moreover, the framework
motivates model-free baselines and a new spectrum of metrics. We evaluate
structurally informed synthesizers and synthesizers powered by deep learning.
Using our structured framework, we show that synthetic data generators that
explicitly represent tabular structure outperform other methods, especially on
smaller datasets.
\\ ( https://arxiv.org/abs/2403.10424 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10444
Date: Fri, 15 Mar 2024 16:28:22 GMT   (201kb,D)

Title: Optimal Block-Level Draft Verification for Accelerating Speculative
  Decoding
Authors: Ziteng Sun and Jae Hun Ro and Ahmad Beirami and Ananda Theertha Suresh
Categories: cs.LG cs.CL cs.DS cs.IT math.IT
\\
  Speculative decoding has shown to be an effective method for lossless
acceleration of large language models (LLMs) during inference. In each
iteration, the algorithm first uses a smaller model to draft a block of tokens.
The tokens are then verified by the large model in parallel and only a subset
of tokens will be kept to guarantee that the final output follows the
distribution of the large model. In all of the prior speculative decoding
works, the draft verification is performed token-by-token independently. In
this work, we propose a better draft verification algorithm that provides
additional wall-clock speedup without incurring additional computation cost and
draft tokens. We first formulate the draft verification step as a block-level
optimal transport problem. The block-level formulation allows us to consider a
wider range of draft verification algorithms and obtain a higher number of
accepted tokens in expectation in one draft block. We propose a verification
algorithm that achieves the optimal accepted length for the block-level
transport problem. We empirically evaluate our proposed block-level
verification algorithm in a wide range of tasks and datasets, and observe
consistent improvements in wall-clock speedup when compared to token-level
verification algorithm. To the best of our knowledge, our work is the first to
establish improvement over speculative decoding through a better draft
verification algorithm.
\\ ( https://arxiv.org/abs/2403.10444 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10459
Date: Fri, 15 Mar 2024 16:51:24 GMT   (1771kb,D)

Title: Understanding the Double Descent Phenomenon in Deep Learning
Authors: Marc Lafon and Alexandre Thomas
Categories: cs.LG cs.CV stat.ML
\\
  Combining empirical risk minimization with capacity control is a classical
strategy in machine learning when trying to control the generalization gap and
avoid overfitting, as the model class capacity gets larger. Yet, in modern deep
learning practice, very large over-parameterized models (e.g. neural networks)
are optimized to fit perfectly the training data and still obtain great
generalization performance. Past the interpolation point, increasing model
complexity seems to actually lower the test error.
  In this tutorial, we explain the concept of double descent and its
mechanisms. The first section sets the classical statistical learning framework
and introduces the double descent phenomenon. By looking at a number of
examples, section 2 introduces inductive biases that appear to have a key role
in double descent by selecting, among the multiple interpolating solutions, a
smooth empirical risk minimizer. Finally, section 3 explores the double descent
with two linear models, and gives other points of view from recent related
works.
\\ ( https://arxiv.org/abs/2403.10459 ,  1771kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10461
Date: Fri, 15 Mar 2024 16:52:25 GMT   (4093kb,D)

Title: Introducing Adaptive Continuous Adversarial Training (ACAT) to Enhance
  ML Robustness
Authors: Mohamed elShehaby, Aditya Kotha, Ashraf Matrawy
Categories: cs.LG cs.CR cs.NI
\\
  Machine Learning (ML) is susceptible to adversarial attacks that aim to trick
ML models, making them produce faulty predictions. Adversarial training was
found to increase the robustness of ML models against these attacks. However,
in network and cybersecurity, obtaining labeled training and adversarial
training data is challenging and costly. Furthermore, concept drift deepens the
challenge, particularly in dynamic domains like network and cybersecurity, and
requires various models to conduct periodic retraining. This letter introduces
Adaptive Continuous Adversarial Training (ACAT) to continuously integrate
adversarial training samples into the model during ongoing learning sessions,
using real-world detected adversarial data, to enhance model resilience against
evolving adversarial threats. ACAT is an adaptive defense mechanism that
utilizes periodic retraining to effectively counter adversarial attacks while
mitigating catastrophic forgetting. Our approach also reduces the total time
required for adversarial sample detection, especially in environments such as
network security where the rate of attacks could be very high. Traditional
detection processes that involve two stages may result in lengthy procedures.
Experimental results using a SPAM detection dataset demonstrate that with ACAT,
the accuracy of the SPAM filter increased from 69% to over 88% after just three
retraining sessions. Furthermore, ACAT outperforms conventional adversarial
sample detectors, providing faster decision times, up to four times faster in
some cases.
\\ ( https://arxiv.org/abs/2403.10461 ,  4093kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10499
Date: Fri, 15 Mar 2024 17:33:49 GMT   (19445kb,D)

Title: Benchmarking Zero-Shot Robustness of Multimodal Foundation Models: A
  Pilot Study
Authors: Chenguang Wang, Ruoxi Jia, Xin Liu, Dawn Song
Categories: cs.LG cs.AI cs.CL cs.CV
\\
  Pre-training image representations from the raw text about images enables
zero-shot vision transfer to downstream tasks. Through pre-training on millions
of samples collected from the internet, multimodal foundation models, such as
CLIP, produce state-of-the-art zero-shot results that often reach
competitiveness with fully supervised methods without the need for
task-specific training. Besides the encouraging performance on classification
accuracy, it is reported that these models close the robustness gap by matching
the performance of supervised models trained on ImageNet under natural
distribution shift. Because robustness is critical to real-world applications,
especially safety-critical ones, in this paper, we present a comprehensive
evaluation based on a large-scale robustness benchmark covering 7 natural, 3
synthetic distribution shifts, and 11 adversarial attacks. We use CLIP as a
pilot study. We show that CLIP leads to a significant robustness drop compared
to supervised ImageNet models on our benchmark, especially under synthetic
distribution shift and adversarial attacks. Furthermore, data overlap analysis
suggests that the observed robustness under natural distribution shifts could
be attributed, at least in part, to data overlap. In summary, our evaluation
shows a comprehensive evaluation of robustness is necessary; and there is a
significant need to improve the robustness of zero-shot multimodal models.
\\ ( https://arxiv.org/abs/2403.10499 ,  19445kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2307.12807 (*cross-listing*)
Date: Mon, 24 Jul 2023 13:58:15 GMT   (323kb,D)

Title: Comprehending Semantic Types in JSON Data with Graph Neural Networks
Authors: Shuang Wei, Michael J. Mior
Categories: cs.DB cs.AI
\\
  Semantic types are a more powerful and detailed way of describing data than
atomic types such as strings or integers. They establish connections between
columns and concepts from the real world, providing more nuanced and
fine-grained information that can be useful for tasks such as automated data
cleaning, schema matching, and data discovery. Existing deep learning models
trained on large text corpora have been successful at performing single-column
semantic type prediction for relational data. However, in this work, we propose
an extension of the semantic type prediction problem to JSON data, labeling the
types based on JSON Paths. Similar to columns in relational data, JSON Path is
a query language that enables the navigation of complex JSON data structures by
specifying the location and content of the elements. We use a graph neural
network to comprehend the structural information within collections of JSON
documents. Our model outperforms a state-of-the-art existing model in several
cases. These results demonstrate the ability of our model to understand complex
JSON data and its potential usage for JSON-related data processing tasks.
\\ ( https://arxiv.org/abs/2307.12807 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09646 (*cross-listing*)
Date: Wed, 18 Oct 2023 04:00:43 GMT   (4616kb,D)

Title: On Unsupervised Image-to-image translation and GAN stability
Authors: BahaaEddin AlAila, Zahra Jandaghi, Abolfazl Farahani and Mohammad Ziad
  Al-Saad
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  The problem of image-to-image translation is one that is intruiging and
challenging at the same time, for the impact potential it can have on a wide
variety of other computer vision applications like colorization, inpainting,
segmentation and others. Given the high-level of sophistication needed to
extract patterns from one domain and successfully applying them to another,
especially, in a completely unsupervised (unpaired) manner, this problem has
gained much attention as of the last few years. It is one of the first problems
where successful applications to deep generative models, and especially
Generative Adversarial Networks achieved astounding results that are actually
of realworld impact, rather than just a show of theoretical prowess; the such
that has been dominating the GAN world. In this work, we study some of the
failure cases of a seminal work in the field, CycleGAN [1] and hypothesize that
they are GAN-stability related, and propose two general models to try to
alleviate these problems. We also reach the same conclusion of the problem
being ill-posed that has been also circulating in the literature lately.
\\ ( https://arxiv.org/abs/2403.09646 ,  4616kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09668 (*cross-listing*)
Date: Mon, 29 Jan 2024 11:20:19 GMT   (1264kb,D)

Title: Trustworthy Automated Driving through Qualitative Scene Understanding
  and Explanations
Authors: Nassim Belmecheri, Arnaud Gotlieb, Nadjib Lazaar, Helge Spieker
Categories: cs.CV cs.AI
Comments: Transport Research Arena (TRA) 2024
\\
  We present the Qualitative Explainable Graph (QXG): a unified symbolic and
qualitative representation for scene understanding in urban mobility. QXG
enables the interpretation of an automated vehicle's environment using sensor
data and machine learning models. It leverages spatio-temporal graphs and
qualitative constraints to extract scene semantics from raw sensor inputs, such
as LiDAR and camera data, offering an intelligible scene model. Crucially, QXG
can be incrementally constructed in real-time, making it a versatile tool for
in-vehicle explanations and real-time decision-making across various sensor
types. Our research showcases the transformative potential of QXG, particularly
in the context of automated driving, where it elucidates decision rationales by
linking the graph with vehicle actions. These explanations serve diverse
purposes, from informing passengers and alerting vulnerable road users (VRUs)
to enabling post-analysis of prior behaviours.
\\ ( https://arxiv.org/abs/2403.09668 ,  1264kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09669 (*cross-listing*)
Date: Tue, 30 Jan 2024 08:18:20 GMT   (4568kb,D)

Title: STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video
  Generative Models
Authors: Pum Jun Kim, Seojun Kim, Jaejun Yoo
Categories: cs.CV cs.AI
Comments: Our work is accepted to ICLR 2024
\\
  Image generative models have made significant progress in generating
realistic and diverse images, supported by comprehensive guidance from various
evaluation metrics. However, current video generative models struggle to
generate even short video clips, with limited tools that provide insights for
improvements. Current video evaluation metrics are simple adaptations of image
metrics by switching the embeddings with video embedding networks, which may
underestimate the unique characteristics of video. Our analysis reveals that
the widely used Frechet Video Distance (FVD) has a stronger emphasis on the
spatial aspect than the temporal naturalness of video and is inherently
constrained by the input size of the embedding networks used, limiting it to 16
frames. Additionally, it demonstrates considerable instability and diverges
from human evaluations. To address the limitations, we propose STREAM, a new
video evaluation metric uniquely designed to independently evaluate spatial and
temporal aspects. This feature allows comprehensive analysis and evaluation of
video generative models from various perspectives, unconstrained by video
length. We provide analytical and experimental evidence demonstrating that
STREAM provides an effective evaluation tool for both visual and temporal
quality of videos, offering insights into area of improvement for video
generative models. To the best of our knowledge, STREAM is the first evaluation
metric that can separately assess the temporal and spatial aspects of videos.
Our code is available at STREAM.
\\ ( https://arxiv.org/abs/2403.09669 ,  4568kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09671 (*cross-listing*)
Date: Sun, 4 Feb 2024 07:21:45 GMT   (10928kb,D)

Title: CoRaiS: Lightweight Real-Time Scheduler for Multi-Edge Cooperative
  Computing
Authors: Yujiao Hu, Qingmin Jia, Jinchao Chen, Yuan Yao, Yan Pan, Renchao Xie,
  F.Richard Yu
Categories: cs.DC cs.AI
Comments: Under Review
\\
  Multi-edge cooperative computing that combines constrained resources of
multiple edges into a powerful resource pool has the potential to deliver great
benefits, such as a tremendous computing power, improved response time, more
diversified services. However, the mass heterogeneous resources composition and
lack of scheduling strategies make the modeling and cooperating of multi-edge
computing system particularly complicated. This paper first proposes a
system-level state evaluation model to shield the complex hardware
configurations and redefine the different service capabilities at heterogeneous
edges. Secondly, an integer linear programming model is designed to cater for
optimally dispatching the distributed arriving requests. Finally, a
learning-based lightweight real-time scheduler, CoRaiS, is proposed. CoRaiS
embeds the real-time states of multi-edge system and requests information, and
combines the embeddings with a policy network to schedule the requests, so that
the response time of all requests can be minimized. Evaluation results verify
that CoRaiS can make a high-quality scheduling decision in real time, and can
be generalized to other multi-edge computing system, regardless of system
scales. Characteristic validation also demonstrates that CoRaiS successfully
learns to balance loads, perceive real-time state and recognize heterogeneity
while scheduling.
\\ ( https://arxiv.org/abs/2403.09671 ,  10928kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09673 (*cross-listing*)
Date: Sun, 4 Feb 2024 12:18:51 GMT   (19191kb,D)

Title: FoldToken: Learning Protein Language via Vector Quantization and Beyond
Authors: Zhangyang Gao, Cheng Tan, Jue Wang, Yufei Huang, Lirong Wu, Stan Z. Li
Categories: q-bio.BM cs.AI cs.LG
\\
  Is there a foreign language describing protein sequences and structures
simultaneously? Protein structures, represented by continuous 3D points, have
long posed a challenge due to the contrasting modeling paradigms of discrete
sequences. We introduce \textbf{FoldTokenizer} to represent protein
sequence-structure as discrete symbols. This innovative approach involves
projecting residue types and structures into a discrete space, guided by a
reconstruction loss for information preservation. We refer to the learned
discrete symbols as \textbf{FoldToken}, and the sequence of FoldTokens serves
as a new protein language, transforming the protein sequence-structure into a
unified modality. We apply the created protein language on general backbone
inpainting and antibody design tasks, building the first GPT-style model
(\textbf{FoldGPT}) for sequence-structure co-generation with promising results.
Key to our success is the substantial enhancement of the vector quantization
module, Soft Conditional Vector Quantization (\textbf{SoftCVQ}).
\\ ( https://arxiv.org/abs/2403.09673 ,  19191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09680 (*cross-listing*)
Date: Wed, 7 Feb 2024 15:30:23 GMT   (3614kb,D)

Title: Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)
Authors: Jordan Morris
Categories: cs.NE cs.AI cs.LG
Comments: 6 pages, 12 figures, 3 tables
ACM-class: B.6.0; B.7.0; C.1.0; I.2.6
\\
  This paper proposes a machine learning pre-sort stage to traditional
supervised learning using Tsetlin Machines. Initially, N data-points are
identified from the dataset using an expedited genetic algorithm to solve the
maximum dispersion problem. These are then used as the initial placement to run
the K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is
used to align N independent Tsetlin Machines by maximising hamming distance.
For MNIST level classification problems, results demonstrate up to 10%
improvement in accuracy, approx. 383X reduction in training time and approx.
86X reduction in inference time.
\\ ( https://arxiv.org/abs/2403.09680 ,  3614kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09700 (*cross-listing*)
Date: Tue, 5 Mar 2024 22:19:21 GMT   (6953kb,D)

Title: Shapley Values-Powered Framework for Fair Reward Split in Content
  Produced by GenAI
Authors: Alex Glinsky, Alexey Sokolsky
Categories: cs.CV cs.AI
Comments: 35 pages, 31 figures
MSC-class: 91A12, 68T05, 91B32
ACM-class: I.2.6; I.3.3; I.2.0; J.5; J.7
\\
  It is evident that, currently, generative models are surpassed in quality by
human professionals. However, with the advancements in Artificial Intelligence,
this gap will narrow, leading to scenarios where individuals who have dedicated
years of their lives to mastering a skill become obsolete due to their high
costs, which are inherently linked to the time they require to complete a task
-- a task that AI could accomplish in minutes or seconds. To avoid future
social upheavals, we must, even now, contemplate how to fairly assess the
contributions of such individuals in training generative models and how to
compensate them for the reduction or complete loss of their incomes. In this
work, we propose a method to structure collaboration between model developers
and data providers. To achieve this, we employ Shapley Values to quantify the
contribution of artist(s) in an image generated by the Stable Diffusion-v1.5
model and to equitably allocate the reward among them.
\\ ( https://arxiv.org/abs/2403.09700 ,  6953kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09717 (*cross-listing*)
Date: Tue, 12 Mar 2024 07:17:01 GMT   (8579kb,D)

Title: Enhancing Depression-Diagnosis-Oriented Chat with Psychological State
  Tracking
Authors: Yiyang Gu, Yougen Zhou, Qin Chen, Ningning Zhou, Jie Zhou, Aimin Zhou,
  Liang He
Categories: cs.HC cs.AI cs.CL cs.CY
\\
  Depression-diagnosis-oriented chat aims to guide patients in self-expression
to collect key symptoms for depression detection. Recent work focuses on
combining task-oriented dialogue and chitchat to simulate the interview-based
depression diagnosis. Whereas, these methods can not well capture the changing
information, feelings, or symptoms of the patient during dialogues. Moreover,
no explicit framework has been explored to guide the dialogue, which results in
some useless communications that affect the experience. In this paper, we
propose to integrate Psychological State Tracking (POST) within the large
language model (LLM) to explicitly guide depression-diagnosis-oriented chat.
Specifically, the state is adapted from a psychological theoretical model,
which consists of four components, namely Stage, Information, Summary and Next.
We fine-tune an LLM model to generate the dynamic psychological state, which is
further used to assist response generation at each turn to simulate the
psychiatrist. Experimental results on the existing benchmark show that our
proposed method boosts the performance of all subtasks in
depression-diagnosis-oriented chat.
\\ ( https://arxiv.org/abs/2403.09717 ,  8579kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09735 (*cross-listing*)
Date: Wed, 13 Mar 2024 14:26:25 GMT   (398kb)

Title: A Sophisticated Framework for the Accurate Detection of Phishing
  Websites
Authors: Asif Newaz, Farhan Shahriyar Haq, Nadim Ahmed
Categories: cs.CR cs.AI
\\
  Phishing is an increasingly sophisticated form of cyberattack that is
inflicting huge financial damage to corporations throughout the globe while
also jeopardizing individuals' privacy. Attackers are constantly devising new
methods of launching such assaults and detecting them has become a daunting
task. Many different techniques have been suggested, each with its own pros and
cons. While machine learning-based techniques have been most successful in
identifying such attacks, they continue to fall short in terms of performance
and generalizability. This paper proposes a comprehensive methodology for
detecting phishing websites. The goal is to design a system that is capable of
accurately distinguishing phishing websites from legitimate ones and provides
generalized performance over a broad variety of datasets. A combination of
feature selection, greedy algorithm, cross-validation, and deep learning
methods have been utilized to construct a sophisticated stacking ensemble
classifier. Extensive experimentation on four different phishing datasets was
conducted to evaluate the performance of the proposed technique. The proposed
algorithm outperformed the other existing phishing detection models obtaining
accuracy of 97.49%, 98.23%, 97.48%, and 98.20% on dataset-1 (UCI Phishing
Websites Dataset), dataset-2 (Phishing Dataset for Machine Learning: Feature
Evaluation), dataset-3 (Phishing Websites Dataset), and dataset-4 (Web page
phishing detection), respectively. The high accuracy values obtained across all
datasets imply the models' generalizability and effectiveness in the accurate
identification of phishing websites.
\\ ( https://arxiv.org/abs/2403.09735 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09740 (*cross-listing*)
Date: Wed, 13 Mar 2024 18:55:20 GMT   (1020kb,D)

Title: Teaching Machines to Code: Smart Contract Translation with LLMs
Authors: Rabimba Karanjai, Lei Xu, Weidong Shi
Categories: cs.SE cs.AI
\\
  The advent of large language models (LLMs) has marked a significant milestone
in the realm of artificial intelligence, with their capabilities often matching
or surpassing human expertise in various domains. Among these achievements,
their adeptness in translation tasks stands out, closely mimicking the
intricate and preliminary processes undertaken by human translators to ensure
the fidelity and quality of the translated content. Despite the advancements in
utilizing LLMs for translating programming code across different languages, the
domain of smart contract translation, particularly into languages not
previously encountered by the LLM, remains largely unexplored. In our research,
we present a pioneering approach, SolMover, which harnesses the synergy of two
distinct LLMs within a unified framework. This framework is designed to grasp
coding principles and apply this understanding to the translation of code into
an unfamiliar language. Our study delves into the capacity of LLMs to mimic
human learning processes, offering an in-depth evaluation of our methodology
for converting smart contracts written in Solidity to Move, a language with
limited resources. The framework employs one LLM to decipher coding conventions
for the new language, creating a blueprint for the second LLM, which, lacking
planning abilities, possesses coding expertise. The empirical evidence from our
experiments suggests that SolMover substantially enhances performance compared
to gpt-3.5-turbo-1106, and achieves superior results over competitors such as
Palm2 and Mixtral-8x7B-Instruct. Additionally, our analysis highlights the
efficacy of our bug mitigation strategy in elevating code quality across all
models, even outside the SolMover framework.
\\ ( https://arxiv.org/abs/2403.09740 ,  1020kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09751 (*cross-listing*)
Date: Thu, 14 Mar 2024 09:38:12 GMT   (5400kb,D)

Title: What Was Your Prompt? A Remote Keylogging Attack on AI Assistants
Authors: Roy Weiss, Daniel Ayzenshteyn, Guy Amit, Yisroel Mirsky
Categories: cs.CR cs.AI cs.CL
\\
  AI assistants are becoming an integral part of society, used for asking
advice or help in personal and confidential issues. In this paper, we unveil a
novel side-channel that can be used to read encrypted responses from AI
Assistants over the web: the token-length side-channel. We found that many
vendors, including OpenAI and Microsoft, have this side-channel.
  However, inferring the content of a response from a token-length sequence
alone proves challenging. This is because tokens are akin to words, and
responses can be several sentences long leading to millions of grammatically
correct sentences. In this paper, we show how this can be overcome by (1)
utilizing the power of a large language model (LLM) to translate these
sequences, (2) providing the LLM with inter-sentence context to narrow the
search space and (3) performing a known-plaintext attack by fine-tuning the
model on the target model's writing style.
  Using these methods, we were able to accurately reconstruct 29\% of an AI
assistant's responses and successfully infer the topic from 55\% of them. To
demonstrate the threat, we performed the attack on OpenAI's ChatGPT-4 and
Microsoft's Copilot on both browser and API traffic.
\\ ( https://arxiv.org/abs/2403.09751 ,  5400kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09752 (*cross-listing*)
Date: Thu, 14 Mar 2024 11:57:26 GMT   (1040kb,D)

Title: Explainable Machine Learning-Based Security and Privacy Protection
  Framework for Internet of Medical Things Systems
Authors: Ayoub Si-ahmed, Mohammed Ali Al-Garadi, Narhimene Boustia
Categories: cs.CR cs.AI
Comments: 33 pages, 8 figures, journal paper
\\
  The Internet of Medical Things (IoMT) transcends traditional medical
boundaries, enabling a transition from reactive treatment to proactive
prevention. This innovative method revolutionizes healthcare by facilitating
early disease detection and tailored care, particularly in chronic disease
management, where IoMT automates treatments based on real-time health data
collection. Nonetheless, its benefits are countered by significant security
challenges that endanger the lives of its users due to the sensitivity and
value of the processed data, thereby attracting malicious interests. Moreover,
the utilization of wireless communication for data transmission exposes medical
data to interception and tampering by cybercriminals. Additionally, anomalies
may arise due to human errors, network interference, or hardware malfunctions.
In this context, anomaly detection based on Machine Learning (ML) is an
interesting solution, but it comes up against obstacles in terms of
explicability and protection of privacy. To address these challenges, a new
framework for Intrusion Detection Systems (IDS) is introduced, leveraging
Artificial Neural Networks (ANN) for intrusion detection while utilizing
Federated Learning (FL) for privacy preservation. Additionally, eXplainable
Artificial Intelligence (XAI) methods are incorporated to enhance model
explanation and interpretation. The efficacy of the proposed framework is
evaluated and compared with centralized approaches using multiple datasets
containing network and medical data, simulating various attack types impacting
the confidentiality, integrity, and availability of medical and physiological
data. The results obtained offer compelling evidence that the FL method
performs comparably to the centralized method, demonstrating high performance.
Additionally, it affords the dual advantage of safeguarding privacy and
providing model explanation.
\\ ( https://arxiv.org/abs/2403.09752 ,  1040kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09753 (*cross-listing*)
Date: Thu, 14 Mar 2024 12:07:37 GMT   (1016kb,D)

Title: SpokeN-100: A Cross-Lingual Benchmarking Dataset for The Classification
  of Spoken Numbers in Different Languages
Authors: Ren\'e Groh, Nina Goes, Andreas M. Kist
Categories: cs.SD cs.AI eess.AS
Comments: Accepted as a full paper by the tinyML Research Symposium 2024
\\
  Benchmarking plays a pivotal role in assessing and enhancing the performance
of compact deep learning models designed for execution on resource-constrained
devices, such as microcontrollers. Our study introduces a novel, entirely
artificially generated benchmarking dataset tailored for speech recognition,
representing a core challenge in the field of tiny deep learning. SpokeN-100
consists of spoken numbers from 0 to 99 spoken by 32 different speakers in four
different languages, namely English, Mandarin, German and French, resulting in
12,800 audio samples. We determine auditory features and use UMAP (Uniform
Manifold Approximation and Projection for Dimension Reduction) as a
dimensionality reduction method to show the diversity and richness of the
dataset. To highlight the use case of the dataset, we introduce two benchmark
tasks: given an audio sample, classify (i) the used language and/or (ii) the
spoken number. We optimized state-of-the-art deep neural networks and performed
an evolutionary neural architecture search to find tiny architectures optimized
for the 32-bit ARM Cortex-M4 nRF52840 microcontroller. Our results represent
the first benchmark data achieved for SpokeN-100.
\\ ( https://arxiv.org/abs/2403.09753 ,  1016kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09793 (*cross-listing*)
Date: Thu, 14 Mar 2024 18:25:40 GMT   (789kb,D)

Title: Socially Integrated Navigation: A Social Acting Robot with Deep
  Reinforcement Learning
Authors: Daniel Fl\"ogel, Lars Fischer, Thomas Rudolf, Tobias Sch\"urmann and
  S\"oren Hohmann
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
\\
  Mobile robots are being used on a large scale in various crowded situations
and become part of our society. The socially acceptable navigation behavior of
a mobile robot with individual human consideration is an essential requirement
for scalable applications and human acceptance. Deep Reinforcement Learning
(DRL) approaches are recently used to learn a robot's navigation policy and to
model the complex interactions between robots and humans. We propose to divide
existing DRL-based navigation approaches based on the robot's exhibited social
behavior and distinguish between social collision avoidance with a lack of
social behavior and socially aware approaches with explicit predefined social
behavior. In addition, we propose a novel socially integrated navigation
approach where the robot's social behavior is adaptive and emerges from the
interaction with humans. The formulation of our approach is derived from a
sociological definition, which states that social acting is oriented toward the
acting of others. The DRL policy is trained in an environment where other
agents interact socially integrated and reward the robot's behavior
individually. The simulation results indicate that the proposed socially
integrated navigation approach outperforms a socially aware approach in terms
of distance traveled, time to completion, and negative impact on all agents
within the environment.
\\ ( https://arxiv.org/abs/2403.09793 ,  789kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09795 (*cross-listing*)
Date: Thu, 14 Mar 2024 18:27:43 GMT   (933kb,D)

Title: Helpful or Harmful? Exploring the Efficacy of Large Language Models for
  Online Grooming Prevention
Authors: Ellie Prosser and Matthew Edwards
Categories: cs.CR cs.AI cs.CL
\\
  Powerful generative Large Language Models (LLMs) are becoming popular tools
amongst the general public as question-answering systems, and are being
utilised by vulnerable groups such as children. With children increasingly
interacting with these tools, it is imperative for researchers to scrutinise
the safety of LLMs, especially for applications that could lead to serious
outcomes, such as online child safety queries. In this paper, the efficacy of
LLMs for online grooming prevention is explored both for identifying and
avoiding grooming through advice generation, and the impact of prompt design on
model performance is investigated by varying the provided context and prompt
specificity. In results reflecting over 6,000 LLM interactions, we find that no
models were clearly appropriate for online grooming prevention, with an
observed lack of consistency in behaviours, and potential for harmful answer
generation, especially from open-source models. We outline where and how models
fall short, providing suggestions for improvement, and identify prompt designs
that heavily altered model performance in troubling ways, with findings that
can be used to inform best practice usage guides.
\\ ( https://arxiv.org/abs/2403.09795 ,  933kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09810 (*cross-listing*)
Date: Thu, 14 Mar 2024 18:59:10 GMT   (24486kb,D)

Title: LabelAId: Just-in-time AI Interventions for Improving Human Labeling
  Quality and Domain Knowledge in Crowdsourcing Systems
Authors: Chu Li, Zhihan Zhang, Michael Saugstad, Esteban Safranchik, Minchu
  Kulkarni, Xiaoyu Huang, Shwetak Patel, Vikram Iyer, Tim Althoff, Jon E.
  Froehlich
Categories: cs.HC cs.AI cs.LG
DOI: 10.1145/3613904.3642089
\\
  Crowdsourcing platforms have transformed distributed problem-solving, yet
quality control remains a persistent challenge. Traditional quality control
measures, such as prescreening workers and refining instructions, often focus
solely on optimizing economic output. This paper explores just-in-time AI
interventions to enhance both labeling quality and domain-specific knowledge
among crowdworkers. We introduce LabelAId, an advanced inference model
combining Programmatic Weak Supervision (PWS) with FT-Transformers to infer
label correctness based on user behavior and domain knowledge. Our technical
evaluation shows that our LabelAId pipeline consistently outperforms
state-of-the-art ML baselines, improving mistake inference accuracy by 36.7%
with 50 downstream samples. We then implemented LabelAId into Project Sidewalk,
an open-source crowdsourcing platform for urban accessibility. A
between-subjects study with 34 participants demonstrates that LabelAId
significantly enhances label precision without compromising efficiency while
also increasing labeler confidence. We discuss LabelAId's success factors,
limitations, and its generalizability to other crowdsourced science domains.
\\ ( https://arxiv.org/abs/2403.09810 ,  24486kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09847 (*cross-listing*)
Date: Thu, 14 Mar 2024 20:13:26 GMT   (394kb,D)

Title: Forecasting Geoffective Events from Solar Wind Data and Evaluating the
  Most Predictive Features through Machine Learning Approaches
Authors: Sabrina Guastavino, Katsiaryna Bahamazava, Emma Perracchione, Fabiana
  Camattari, Gianluca Audone, Daniele Telloni, Roberto Susino, Gianalfredo
  Nicolini, Silvano Fineschi, Michele Piana, Anna Maria Massone
Categories: physics.space-ph astro-ph.SR cs.AI
MSC-class: 85-08, 68T07, 68T05
\\
  This study addresses the prediction of geomagnetic disturbances by exploiting
machine learning techniques. Specifically, the Long-Short Term Memory recurrent
neural network, which is particularly suited for application over long time
series, is employed in the analysis of in-situ measurements of solar wind
plasma and magnetic field acquired over more than one solar cycle, from $2005$
to $2019$, at the Lagrangian point L$1$. The problem is approached as a binary
classification aiming to predict one hour in advance a decrease in the SYM-H
geomagnetic activity index below the threshold of $-50$ nT, which is generally
regarded as indicative of magnetospheric perturbations. The strong class
imbalance issue is tackled by using an appropriate loss function tailored to
optimize appropriate skill scores in the training phase of the neural network.
Beside classical skill scores, value-weighted skill scores are then employed to
evaluate predictions, suitable in the study of problems, such as the one faced
here, characterized by strong temporal variability. For the first time, the
content of magnetic helicity and energy carried by solar transients, associated
with their detection and likelihood of geo-effectiveness, were considered as
input features of the network architecture. Their predictive capabilities are
demonstrated through a correlation-driven feature selection method to rank the
most relevant characteristics involved in the neural network prediction model.
The optimal performance of the adopted neural network in properly forecasting
the onset of geomagnetic storms, which is a crucial point for giving real
warnings in an operational setting, is finally showed.
\\ ( https://arxiv.org/abs/2403.09847 ,  394kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09861 (*cross-listing*)
Date: Thu, 14 Mar 2024 20:42:23 GMT   (3145kb,D)

Title: NN-Defined Modulator: Reconfigurable and Portable Software Modulator on
  IoT Gateways
Authors: Jiazhao Wang, Wenchao Jiang, Ruofeng Liu, Bin Hu, Demin Gao, Shuai
  Wang
Categories: cs.ET cs.AI
Journal-ref: NSDI 2024
\\
  A physical-layer modulator is a vital component for an IoT gateway to map the
symbols to signals. However, due to the soldered hardware chipsets on the
gateway's motherboards or the diverse toolkits on different platforms for the
software radio, the existing solutions either have limited extensibility or are
platform-specific. Such limitation is hard to ignore when modulation schemes
and hardware platforms have become extremely diverse. This paper presents a new
paradigm of using neural networks as an abstraction layer for physical layer
modulators in IoT gateway devices, referred to as NN-defined modulators. Our
approach addresses the challenges of extensibility and portability for multiple
technologies on various hardware platforms. The proposed NN-defined modulator
uses a model-driven methodology rooted in solid mathematical foundations while
having native support for hardware acceleration and portability to
heterogeneous platforms. We conduct the evaluation of NN-defined modulators on
different platforms, including Nvidia Jetson Nano and Raspberry Pi. Evaluations
demonstrate that our NN-defined modulator effectively operates as conventional
modulators and provides significant efficiency gains (up to $4.7\times$ on
Nvidia Jetson Nano and $1.1\times$ on Raspberry Pi), indicating high
portability. Furthermore, we show the real-world applications using our
NN-defined modulators to generate ZigBee and WiFi packets, which are compliant
with commodity TI CC2650 (ZigBee) and Intel AX201 (WiFi NIC), respectively.
\\ ( https://arxiv.org/abs/2403.09861 ,  3145kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09869 (*cross-listing*)
Date: Thu, 14 Mar 2024 21:00:26 GMT   (382kb,D)

Title: Mind the GAP: Improving Robustness to Subpopulation Shifts with
  Group-Aware Priors
Authors: Tim G. J. Rudner, Ya Shi Zhang, Andrew Gordon Wilson, Julia Kempe
Categories: stat.ML cs.AI cs.LG stat.ME
Comments: Published in Proceedings of the 27th International Conference on
  Artificial Intelligence and Statistics (AISTATS 2024)
\\
  Machine learning models often perform poorly under subpopulation shifts in
the data distribution. Developing methods that allow machine learning models to
better generalize to such shifts is crucial for safe deployment in real-world
settings. In this paper, we develop a family of group-aware prior (GAP)
distributions over neural network parameters that explicitly favor models that
generalize well under subpopulation shifts. We design a simple group-aware
prior that only requires access to a small set of data with group information
and demonstrate that training with this prior yields state-of-the-art
performance -- even when only retraining the final layer of a previously
trained non-robust model. Group aware-priors are conceptually simple,
complementary to existing approaches, such as attribute pseudo labeling and
data reweighting, and open up promising new avenues for harnessing Bayesian
inference to enable robustness to subpopulation shifts.
\\ ( https://arxiv.org/abs/2403.09869 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09871 (*cross-listing*)
Date: Thu, 14 Mar 2024 21:01:06 GMT   (28774kb,D)

Title: ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric
  Thermal Image
Authors: Fangqiang Ding, Yunzhou Zhu, Xiangyu Wen, Chris Xiaoxuan Lu
Categories: cs.CV cs.AI cs.HC cs.LG
Comments: 20 pages, 6 pages, 5 tables
\\
  In this work, we present ThermoHands, a new benchmark for thermal image-based
egocentric 3D hand pose estimation, aimed at overcoming challenges like varying
lighting and obstructions (e.g., handwear). The benchmark includes a diverse
dataset from 28 subjects performing hand-object and hand-virtual interactions,
accurately annotated with 3D hand poses through an automated process. We
introduce a bespoken baseline method, TheFormer, utilizing dual transformer
modules for effective egocentric 3D hand pose estimation in thermal imagery.
Our experimental results highlight TheFormer's leading performance and affirm
thermal imaging's effectiveness in enabling robust 3D hand pose estimation in
adverse conditions.
\\ ( https://arxiv.org/abs/2403.09871 ,  28774kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09920 (*cross-listing*)
Date: Thu, 14 Mar 2024 23:41:00 GMT   (3717kb)

Title: Predicting Generalization of AI Colonoscopy Models to Unseen Data
Authors: Joel Shor, Carson McNeil, Yotam Intrator, Joseph R Ledsam, Hiro-o
  Yamano, Daisuke Tsurumaru, Hiroki Kayama, Atsushi Hamabe, Koji Ando,
  Mitsuhiko Ota, Haruei Ogino, Hiroshi Nakase, Kaho Kobayashi, Masaaki Miyo,
  Eiji Oki, Ichiro Takemasa, Ehud Rivlin, Roman Goldenberg
Categories: cs.CV cs.AI cs.CY
\\
  Background and aims Generalizability of AI colonoscopy algorithms is
important for wider adoption in clinical practice. However, current techniques
for evaluating performance on unseen data require expensive and time-intensive
labels.
  Methods We use a "Masked Siamese Network" (MSN) to identify novel phenomena
in unseen data and predict polyp detector performance. MSN is trained to
predict masked out regions of polyp images, without any labels. We test MSN's
ability to be trained on data only from Israel and detect unseen techniques,
narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes from Japan
(354 videos, 128 hours). We also test MSN's ability to predict performance of
Computer Aided Detection (CADe) of polyps on colonoscopies from both countries,
even though MSN is not trained on data from Japan.
  Results MSN correctly identifies NBI and CE as less similar to Israel
whitelight than Japan whitelight (bootstrapped z-test, |z| > 496, p < 10-8 for
both) using the label-free Frechet distance. MSN detects NBI with 99% accuracy,
predicts CE better than our heuristic (90% vs 79% accuracy) despite being
trained only on whitelight, and is the only method that is robust to noisy
labels. MSN predicts CADe polyp detector performance on in-domain Israel and
out-of-domain Japan colonoscopies (r=0.79, 0.37 respectively). With few
examples of Japan detector performance to train on, MSN prediction of Japan
performance improves (r=0.56).
  Conclusion Our technique can identify distribution shifts in clinical data
and can predict CADe detector performance on unseen data, without labels. Our
self-supervised approach can aid in detecting when data in practice is
different from training, such as between hospitals or data has meaningfully
shifted from training. MSN has potential for application to medical image
domains beyond colonoscopy.
\\ ( https://arxiv.org/abs/2403.09920 ,  3717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09948 (*cross-listing*)
Date: Fri, 15 Mar 2024 01:18:08 GMT   (1822kb)

Title: RadCLIP: Enhancing Radiologic Image Analysis through Contrastive
  Language-Image Pre-training
Authors: Zhixiu Lu, Hailong Li, and Lili He
Categories: cs.CV cs.AI
\\
  The integration of artificial intelligence (AI) with radiology has marked a
transformative era in medical diagnostics. Vision foundation models have been
adopted to enhance radiologic imaging analysis. However, the distinct
complexities of radiological imaging, including the interpretation of 2D and 3D
radiological data, pose unique challenges that existing models, trained on
general non-medical images, fail to address adequately. To bridge this gap and
capitalize on the diagnostic precision required in medical imaging, we
introduce RadCLIP: a pioneering cross-modal foundational model that harnesses
Contrastive Language-Image Pre-training (CLIP) to refine radiologic image
analysis. RadCLIP incorporates a novel 3D slice pooling mechanism tailored for
volumetric image analysis and is trained using a comprehensive and diverse
dataset of radiologic image-text pairs. Our evaluations demonstrate that
RadCLIP effectively aligns radiological images with their corresponding textual
annotations, and in the meantime, offers a robust vision backbone for
radiologic imagery with significant promise.
\\ ( https://arxiv.org/abs/2403.09948 ,  1822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09974 (*cross-listing*)
Date: Fri, 15 Mar 2024 02:40:13 GMT   (21080kb,D)

Title: GET: Unlocking the Multi-modal Potential of CLIP for Generalized
  Category Discovery
Authors: Enguang Wang, Zhimao Peng, Zhengyuan Xie, Xialei Liu, Ming-Ming Cheng
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Given unlabelled datasets containing both old and new categories, generalized
category discovery (GCD) aims to accurately discover new classes while
correctly classifying old classes, leveraging the class concepts learned from
labeled samples. Current GCD methods only use a single visual modality of
information, resulting in poor classification of visually similar classes.
Though certain classes are visually confused, their text information might be
distinct, motivating us to introduce text information into the GCD task.
However, the lack of class names for unlabelled data makes it impractical to
utilize text information. To tackle this challenging problem, in this paper, we
propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings
for unlabelled samples. Specifically, our TES leverages the property that CLIP
can generate aligned vision-language features, converting visual embeddings
into tokens of the CLIP's text encoder to generate pseudo text embeddings.
Besides, we employ a dual-branch framework, through the joint learning and
instance consistency of different modality branches, visual and semantic
information mutually enhance each other, promoting the interaction and fusion
of visual and text embedding space. Our method unlocks the multi-modal
potentials of CLIP and outperforms the baseline methods by a large margin on
all GCD benchmarks, achieving new state-of-the-art. The code will be released
at \url{https://github.com/enguangW/GET}.
\\ ( https://arxiv.org/abs/2403.09974 ,  21080kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09977 (*cross-listing*)
Date: Fri, 15 Mar 2024 02:48:47 GMT   (25694kb,D)

Title: EfficientVMamba: Atrous Selective Scan for Light Weight Visual Mamba
Authors: Xiaohuan Pei, Tao Huang, Chang Xu
Categories: cs.CV cs.AI
\\
  Prior efforts in light-weight model development mainly centered on CNN and
Transformer-based designs yet faced persistent challenges. CNNs adept at local
feature extraction compromise resolution while Transformers offer global reach
but escalate computational demands $\mathcal{O}(N^2)$. This ongoing trade-off
between accuracy and efficiency remains a significant hurdle. Recently, state
space models (SSMs), such as Mamba, have shown outstanding performance and
competitiveness in various tasks such as language modeling and computer vision,
while reducing the time complexity of global information extraction to
$\mathcal{O}(N)$. Inspired by this, this work proposes to explore the potential
of visual state space models in light-weight model design and introduce a novel
efficient model variant dubbed EfficientVMamba. Concretely, our EfficientVMamba
integrates a atrous-based selective scan approach by efficient skip sampling,
constituting building blocks designed to harness both global and local
representational features. Additionally, we investigate the integration between
SSM blocks and convolutions, and introduce an efficient visual state space
block combined with an additional convolution branch, which further elevate the
model performance. Experimental results show that, EfficientVMamba scales down
the computational complexity while yields competitive results across a variety
of vision tasks. For example, our EfficientVMamba-S with $1.3$G FLOPs improves
Vim-Ti with $1.5$G FLOPs by a large margin of $5.6\%$ accuracy on ImageNet.
Code is available at: \url{https://github.com/TerryPei/EfficientVMamba}.
\\ ( https://arxiv.org/abs/2403.09977 ,  25694kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09998 (*cross-listing*)
Date: Fri, 15 Mar 2024 03:45:10 GMT   (1668kb,D)

Title: FBPT: A Fully Binary Point Transformer
Authors: Zhixing Hou, Yuzhang Shang, Yan Yan
Categories: cs.CV cs.AI
Comments: Accepted to ICRA 2024. arXiv admin note: substantial text overlap
  with arXiv:2303.01166
\\
  This paper presents a novel Fully Binary Point Cloud Transformer (FBPT) model
which has the potential to be widely applied and expanded in the fields of
robotics and mobile devices. By compressing the weights and activations of a
32-bit full-precision network to 1-bit binary values, the proposed binary point
cloud Transformer network significantly reduces the storage footprint and
computational resource requirements of neural network models for point cloud
processing tasks, compared to full-precision point cloud networks. However,
achieving a fully binary point cloud Transformer network, where all parts
except the modules specific to the task are binary, poses challenges and
bottlenecks in quantizing the activations of Q, K, V and self-attention in the
attention module, as they do not adhere to simple probability distributions and
can vary with input data. Furthermore, in our network, the binary attention
module undergoes a degradation of the self-attention module due to the uniform
distribution that occurs after the softmax operation. The primary focus of this
paper is on addressing the performance degradation issue caused by the use of
binary point cloud Transformer modules. We propose a novel binarization
mechanism called dynamic-static hybridization. Specifically, our approach
combines static binarization of the overall network model with fine granularity
dynamic binarization of data-sensitive components. Furthermore, we make use of
a novel hierarchical training scheme to obtain the optimal model and
binarization parameters. These above improvements allow the proposed
binarization method to outperform binarization methods applied to convolution
neural networks when used in point cloud Transformer structures. To demonstrate
the superiority of our algorithm, we conducted experiments on two different
tasks: point cloud classification and place recognition.
\\ ( https://arxiv.org/abs/2403.09998 ,  1668kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10014 (*cross-listing*)
Date: Fri, 15 Mar 2024 04:36:44 GMT   (1919kb,D)

Title: NNCTC: Physical Layer Cross-Technology Communication via Neural Networks
Authors: Haoyu Wang, Jiazhao Wang, Demin Gao, Wenchao Jiang
Categories: cs.NI cs.AI
Comments: 12 pages
ACM-class: C.2.2
\\
  Cross-technology communication(CTC) enables seamless interactions between
diverse wireless technologies. Most existing work is based on reversing the
transmission path to identify the appropriate payload to generate the waveform
that the target devices can recognize. However, this method suffers from many
limitations, including dependency on specific technologies and the necessity
for intricate algorithms to mitigate distortion. In this work, we present
NNCTC, a Neural-Network-based Cross-Technology Communication framework inspired
by the adaptability of trainable neural models in wireless communications. By
converting signal processing components within the CTC pipeline into neural
models, the NNCTC is designed for end-to-end training without requiring labeled
data. This enables the NNCTC system to autonomously derive the optimal CTC
payload, which significantly eases the development complexity and showcases the
scalability potential for various CTC links. Particularly, we construct a CTC
system from Wi-Fi to ZigBee. The NNCTC system outperforms the well-recognized
WEBee and WIDE design in error performance, achieving an average packet
reception rate(PRR) of 92.3% and an average symbol error rate(SER) as low as
1.3%.
\\ ( https://arxiv.org/abs/2403.10014 ,  1919kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10024 (*cross-listing*)
Date: Fri, 15 Mar 2024 05:13:38 GMT   (6985kb,D)

Title: MR-MT3: Memory Retaining Multi-Track Music Transcription to Mitigate
  Instrument Leakage
Authors: Hao Hao Tan, Kin Wai Cheuk, Taemin Cho, Wei-Hsiang Liao, Yuki
  Mitsufuji
Categories: cs.SD cs.AI cs.LG cs.MM eess.AS
\\
  This paper presents enhancements to the MT3 model, a state-of-the-art (SOTA)
token-based multi-instrument automatic music transcription (AMT) model. Despite
SOTA performance, MT3 has the issue of instrument leakage, where transcriptions
are fragmented across different instruments. To mitigate this, we propose
MR-MT3, with enhancements including a memory retention mechanism, prior token
sampling, and token shuffling are proposed. These methods are evaluated on the
Slakh2100 dataset, demonstrating improved onset F1 scores and reduced
instrument leakage. In addition to the conventional multi-instrument
transcription F1 score, new metrics such as the instrument leakage ratio and
the instrument detection F1 score are introduced for a more comprehensive
assessment of transcription quality. The study also explores the issue of
domain overfitting by evaluating MT3 on single-instrument monophonic datasets
such as ComMU and NSynth. The findings, along with the source code, are shared
to facilitate future work aimed at refining token-based multi-instrument AMT
models.
\\ ( https://arxiv.org/abs/2403.10024 ,  6985kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10039 (*cross-listing*)
Date: Fri, 15 Mar 2024 06:19:02 GMT   (45083kb,D)

Title: Rethinking Low-quality Optical Flow in Unsupervised Surgical Instrument
  Segmentation
Authors: Peiran Wu, Yang Liu, Jiayu Huo, Gongyu Zhang, Christos Bergeles,
  Rachel Sparks, Prokar Dasgupta, Alejandro Granados, and Sebastien Ourselin
Categories: cs.CV cs.AI
\\
  Video-based surgical instrument segmentation plays an important role in
robot-assisted surgeries. Unlike supervised settings, unsupervised segmentation
relies heavily on motion cues, which are challenging to discern due to the
typically lower quality of optical flow in surgical footage compared to natural
scenes. This presents a considerable burden for the advancement of unsupervised
segmentation techniques. In our work, we address the challenge of enhancing
model performance despite the inherent limitations of low-quality optical flow.
Our methodology employs a three-pronged approach: extracting boundaries
directly from the optical flow, selectively discarding frames with inferior
flow quality, and employing a fine-tuning process with variable frame rates. We
thoroughly evaluate our strategy on the EndoVis2017 VOS dataset and Endovis2017
Challenge dataset, where our model demonstrates promising results, achieving a
mean Intersection-over-Union (mIoU) of 0.75 and 0.72, respectively. Our
findings suggest that our approach can greatly decrease the need for manual
annotations in clinical environments and may facilitate the annotation process
for new datasets. The code is available at
https://github.com/wpr1018001/Rethinking-Low-quality-Optical-Flow.git
\\ ( https://arxiv.org/abs/2403.10039 ,  45083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10041 (*cross-listing*)
Date: Fri, 15 Mar 2024 06:22:32 GMT   (1489kb,D)

Title: Towards Embedding Dynamic Personas in Interactive Robots: Masquerading
  Animated Social Kinematics (MASK)
Authors: Jeongeun Park, Taemoon Jeong, Hyeonseong Kim, Taehyun Byun, Seungyoon
  Shin, Keunjun Choi, Jaewoon Kwon, Taeyoon Lee, Matthew Pan, and Sungjoon Choi
Categories: cs.RO cs.AI
Comments: 4 pages, 3 figures
\\
  This paper presents the design and development of an innovative interactive
robotic system to enhance audience engagement using character-like personas.
Built upon the foundations of persona-driven dialog agents, this work extends
the agent application to the physical realm, employing robots to provide a more
immersive and interactive experience. The proposed system, named the
Masquerading Animated Social Kinematics (MASK), leverages an anthropomorphic
robot which interacts with guests using non-verbal interactions, including
facial expressions and gestures. A behavior generation system based upon a
finite-state machine structure effectively conditions robotic behavior to
convey distinct personas. The MASK framework integrates a perception engine, a
behavior selection engine, and a comprehensive action library to enable
real-time, dynamic interactions with minimal human intervention in behavior
design. Throughout the user subject studies, we examined whether the users
could recognize the intended character in film-character-based persona
conditions. We conclude by discussing the role of personas in interactive
agents and the factors to consider for creating an engaging user experience.
\\ ( https://arxiv.org/abs/2403.10041 ,  1489kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10049 (*cross-listing*)
Date: Fri, 15 Mar 2024 06:42:23 GMT   (1168kb,D)

Title: PPM : A Pre-trained Plug-in Model for Click-through Rate Prediction
Authors: Yuanbo Gao, Peng Lin, Dongyue Wang, Feng Mei, Xiwei Zhao, Sulong Xu,
  Jinghe Hu
Categories: cs.IR cs.AI
Comments: Accepted by ACM Web Conference 2024 (WWW'24)
Report-no: ip6417
DOI: 10.1145/3589335.3648329
\\
  Click-through rate (CTR) prediction is a core task in recommender systems.
Existing methods (IDRec for short) rely on unique identities to represent
distinct users and items that have prevailed for decades. On one hand, IDRec
often faces significant performance degradation on cold-start problem; on the
other hand, IDRec cannot use longer training data due to constraints imposed by
iteration efficiency. Most prior studies alleviate the above problems by
introducing pre-trained knowledge(e.g. pre-trained user model or multi-modal
embeddings). However, the explosive growth of online latency can be attributed
to the huge parameters in the pre-trained model. Therefore, most of them cannot
employ the unified model of end-to-end training with IDRec in industrial
recommender systems, thus limiting the potential of the pre-trained model. To
this end, we propose a $\textbf{P}$re-trained $\textbf{P}$lug-in CTR
$\textbf{M}$odel, namely PPM. PPM employs multi-modal features as input and
utilizes large-scale data for pre-training. Then, PPM is plugged in IDRec model
to enhance unified model's performance and iteration efficiency. Upon
incorporating IDRec model, certain intermediate results within the network are
cached, with only a subset of the parameters participating in training and
serving. Hence, our approach can successfully deploy an end-to-end model
without causing huge latency increases. Comprehensive offline experiments and
online A/B testing at JD E-commerce demonstrate the efficiency and
effectiveness of PPM.
\\ ( https://arxiv.org/abs/2403.10049 ,  1168kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10069 (*cross-listing*)
Date: Fri, 15 Mar 2024 07:19:15 GMT   (12024kb,D)

Title: Boundary Matters: A Bi-Level Active Finetuning Framework
Authors: Han Lu, Yichen Xie, Xiaokang Yang, Junchi Yan
Categories: cs.CV cs.AI
\\
  The pretraining-finetuning paradigm has gained widespread adoption in vision
tasks and other fields, yet it faces the significant challenge of high sample
annotation costs. To mitigate this, the concept of active finetuning has
emerged, aiming to select the most appropriate samples for model finetuning
within a limited budget. Traditional active learning methods often struggle in
this setting due to their inherent bias in batch selection. Furthermore, the
recent active finetuning approach has primarily concentrated on aligning the
distribution of selected subsets with the overall data pool, focusing solely on
diversity. In this paper, we propose a Bi-Level Active Finetuning framework to
select the samples for annotation in one shot, which includes two stages: core
sample selection for diversity, and boundary sample selection for uncertainty.
The process begins with the identification of pseudo-class centers, followed by
an innovative denoising method and an iterative strategy for boundary sample
selection in the high-dimensional feature space, all without relying on
ground-truth labels. Our comprehensive experiments provide both qualitative and
quantitative evidence of our method's efficacy, outperforming all the existing
baselines.
\\ ( https://arxiv.org/abs/2403.10069 ,  12024kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10079 (*cross-listing*)
Date: Fri, 15 Mar 2024 07:45:25 GMT   (12940kb,D)

Title: Learning Physical Dynamics for Object-centric Visual Prediction
Authors: Huilin Xu, Tao Chen, Feng Xu
Categories: cs.CV cs.AI
Comments: 13 pages, 10 figures
\\
  The ability to model the underlying dynamics of visual scenes and reason
about the future is central to human intelligence. Many attempts have been made
to empower intelligent systems with such physical understanding and prediction
abilities. However, most existing methods focus on pixel-to-pixel prediction,
which suffers from heavy computational costs while lacking a deep understanding
of the physical dynamics behind videos. Recently, object-centric prediction
methods have emerged and attracted increasing interest. Inspired by it, this
paper proposes an unsupervised object-centric prediction model that makes
future predictions by learning visual dynamics between objects. Our model
consists of two modules, perceptual, and dynamic module. The perceptual module
is utilized to decompose images into several objects and synthesize images with
a set of object-centric representations. The dynamic module fuses contextual
information, takes environment-object and object-object interaction into
account, and predicts the future trajectory of objects. Extensive experiments
are conducted to validate the effectiveness of the proposed method. Both
quantitative and qualitative experimental results demonstrate that our model
generates higher visual quality and more physically reliable predictions
compared to the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.10079 ,  12940kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10086 (*cross-listing*)
Date: Fri, 15 Mar 2024 08:01:02 GMT   (430kb)

Title: Large Language Models to Generate System-Level Test Programs Targeting
  Non-functional Properties
Authors: Denis Schwachhofer, Peter Domanski, Steffen Becker, Stefan Wagner,
  Matthias Sauer, Dirk Pfl\"uger, Ilia Polian
Categories: cs.SE cs.AI cs.ET cs.PL
Comments: Testmethoden und Zuverl\"assigkeit von Schaltungen und Systemen, TuZ
  2024
\\
  System-Level Test (SLT) has been a part of the test flow for integrated
circuits for over a decade and still gains importance. However, no systematic
approaches exist for test program generation, especially targeting
non-functional properties of the Device under Test (DUT). Currently, test
engineers manually compose test suites from off-the-shelf software,
approximating the end-user environment of the DUT. This is a challenging and
tedious task that does not guarantee sufficient control over non-functional
properties. This paper proposes Large Language Models (LLMs) to generate test
programs. We take a first glance at how pre-trained LLMs perform in test
program generation to optimize non-functional properties of the DUT. Therefore,
we write a prompt to generate C code snippets that maximize the instructions
per cycle of a super-scalar, out-of-order architecture in simulation.
Additionally, we apply prompt and hyperparameter optimization to achieve the
best possible results without further training.
\\ ( https://arxiv.org/abs/2403.10086 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10105 (*cross-listing*)
Date: Fri, 15 Mar 2024 08:50:39 GMT   (2425kb,D)

Title: Belief Aided Navigation using Bayesian Reinforcement Learning for
  Avoiding Humans in Blind Spots
Authors: Jinyeob Kim, Daewon Kwak, Hyunwoo Rim, and Donghan Kim
Categories: cs.RO cs.AI cs.LG
Comments: 8 pages, 4 figures
\\
  Recent research on mobile robot navigation has focused on socially aware
navigation in crowded environments. However, existing methods do not adequately
account for human robot interactions and demand accurate location information
from omnidirectional sensors, rendering them unsuitable for practical
applications. In response to this need, this study introduces a novel
algorithm, BNBRL+, predicated on the partially observable Markov decision
process framework to assess risks in unobservable areas and formulate movement
strategies under uncertainty. BNBRL+ consolidates belief algorithms with
Bayesian neural networks to probabilistically infer beliefs based on the
positional data of humans. It further integrates the dynamics between the
robot, humans, and inferred beliefs to determine the navigation paths and
embeds social norms within the reward function, thereby facilitating socially
aware navigation. Through experiments in various risk laden scenarios, this
study validates the effectiveness of BNBRL+ in navigating crowded environments
with blind spots. The model's ability to navigate effectively in spaces with
limited visibility and avoid obstacles dynamically can significantly improve
the safety and reliability of autonomous vehicles.
\\ ( https://arxiv.org/abs/2403.10105 ,  2425kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10107 (*cross-listing*)
Date: Fri, 15 Mar 2024 08:51:15 GMT   (5191kb,D)

Title: Enhancing Human-Centered Dynamic Scene Understanding via Multiple LLMs
  Collaborated Reasoning
Authors: Hang Zhang, Wenxiao Zhang, Haoxuan Qu, Jun Liu
Categories: cs.CV cs.AI cs.MM
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Human-centered dynamic scene understanding plays a pivotal role in enhancing
the capability of robotic and autonomous systems, in which Video-based
Human-Object Interaction (V-HOI) detection is a crucial task in semantic scene
understanding, aimed at comprehensively understanding HOI relationships within
a video to benefit the behavioral decisions of mobile robots and autonomous
driving systems. Although previous V-HOI detection models have made significant
strides in accurate detection on specific datasets, they still lack the general
reasoning ability like human beings to effectively induce HOI relationships. In
this study, we propose V-HOI Multi-LLMs Collaborated Reasoning (V-HOI MLCR), a
novel framework consisting of a series of plug-and-play modules that could
facilitate the performance of current V-HOI detection models by leveraging the
strong reasoning ability of different off-the-shelf pre-trained large language
models (LLMs). We design a two-stage collaboration system of different LLMs for
the V-HOI task. Specifically, in the first stage, we design a Cross-Agents
Reasoning scheme to leverage the LLM conduct reasoning from different aspects.
In the second stage, we perform Multi-LLMs Debate to get the final reasoning
answer based on the different knowledge in different LLMs. Additionally, we
devise an auxiliary training strategy that utilizes CLIP, a large
vision-language model to enhance the base V-HOI models' discriminative ability
to better cooperate with LLMs. We validate the superiority of our design by
demonstrating its effectiveness in improving the prediction accuracy of the
base V-HOI model via reasoning from multiple perspectives.
\\ ( https://arxiv.org/abs/2403.10107 ,  5191kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10135 (*cross-listing*)
Date: Fri, 15 Mar 2024 09:28:19 GMT   (20194kb,D)

Title: The Whole is Better than the Sum: Using Aggregated Demonstrations in
  In-Context Learning for Sequential Recommendation
Authors: Lei Wang, Ee-Peng Lim
Categories: cs.IR cs.AI cs.CL
Comments: NAACL 2024 (Findings)
\\
  Large language models (LLMs) have shown excellent performance on various NLP
tasks. To use LLMs as strong sequential recommenders, we explore the in-context
learning approach to sequential recommendation. We investigate the effects of
instruction format, task consistency, demonstration selection, and number of
demonstrations. As increasing the number of demonstrations in ICL does not
improve accuracy despite using a long prompt, we propose a novel method called
LLMSRec-Syn that incorporates multiple demonstration users into one aggregated
demonstration. Our experiments on three recommendation datasets show that
LLMSRec-Syn outperforms state-of-the-art LLM-based sequential recommendation
methods. In some cases, LLMSRec-Syn can perform on par with or even better than
supervised learning methods. Our code is publicly available at
https://github.com/demoleiwang/LLMSRec_Syn.
\\ ( https://arxiv.org/abs/2403.10135 ,  20194kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10136 (*cross-listing*)
Date: Fri, 15 Mar 2024 09:33:10 GMT   (2250kb)

Title: Response Style Characterization for Repeated Measures Using the Visual
  Analogue Scale
Authors: Shunsuke Minusa, Tadayuki Matsumura, Kanako Esaki, Yang Shao, Chihiro
  Yoshimura, and Hiroyuki Mizuno
Categories: stat.ME cs.AI
Comments: 13 pages, 7 figures, submitted to IEEE Access
\\
  Self-report measures (e.g., Likert scales) are widely used to evaluate
subjective health perceptions. Recently, the visual analog scale (VAS), a
slider-based scale, has become popular owing to its ability to precisely and
easily assess how people feel. These data can be influenced by the response
style (RS), a user-dependent systematic tendency that occurs regardless of
questionnaire instructions. Despite its importance, especially in
between-individual analysis, little attention has been paid to handling the RS
in the VAS (denoted as response profile (RP)), as it is mainly used for
within-individual monitoring and is less affected by RP. However, VAS
measurements often require repeated self-reports of the same questionnaire
items, making it difficult to apply conventional methods on a Likert scale. In
this study, we developed a novel RP characterization method for various types
of repeatedly measured VAS data. This approach involves the modeling of RP as
distributional parameters ${\theta}$ through a mixture of RS-like
distributions, and addressing the issue of unbalanced data through bootstrap
sampling for treating repeated measures. We assessed the effectiveness of the
proposed method using simulated pseudo-data and an actual dataset from an
empirical study. The assessment of parameter recovery showed that our method
accurately estimated the RP parameter ${\theta}$, demonstrating its robustness.
Moreover, applying our method to an actual VAS dataset revealed the presence of
individual RP heterogeneity, even in repeated VAS measurements, similar to the
findings of the Likert scale. Our proposed method enables RP
heterogeneity-aware VAS data analysis, similar to Likert-scale data analysis.
\\ ( https://arxiv.org/abs/2403.10136 ,  2250kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10164 (*cross-listing*)
Date: Fri, 15 Mar 2024 10:18:06 GMT   (17654kb,D)

Title: CoReEcho: Continuous Representation Learning for 2D+time
  Echocardiography Analysis
Authors: Fadillah Adamsyah Maani, Numan Saeed, Aleksandr Matsun, Mohammad Yaqub
Categories: cs.CV cs.AI cs.LG
\\
  Deep learning (DL) models have been advancing automatic medical image
analysis on various modalities, including echocardiography, by offering a
comprehensive end-to-end training pipeline. This approach enables DL models to
regress ejection fraction (EF) directly from 2D+time echocardiograms, resulting
in superior performance. However, the end-to-end training pipeline makes the
learned representations less explainable. The representations may also fail to
capture the continuous relation among echocardiogram clips, indicating the
existence of spurious correlations, which can negatively affect the
generalization. To mitigate this issue, we propose CoReEcho, a novel training
framework emphasizing continuous representations tailored for direct EF
regression. Our extensive experiments demonstrate that CoReEcho: 1) outperforms
the current state-of-the-art (SOTA) on the largest echocardiography dataset
(EchoNet-Dynamic) with MAE of 3.90 & R2 of 82.44, and 2) provides robust and
generalizable features that transfer more effectively in related downstream
tasks. The code is publicly available at https://github.com/fadamsyah/CoReEcho.
\\ ( https://arxiv.org/abs/2403.10164 ,  17654kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10173 (*cross-listing*)
Date: Fri, 15 Mar 2024 10:28:31 GMT   (3552kb,D)

Title: A Hybrid SNN-ANN Network for Event-based Object Detection with Spatial
  and Temporal Attention
Authors: Soikat Hasan Ahmed, Jan Finkbeiner, Emre Neftci
Categories: cs.CV cs.AI
\\
  Event cameras offer high temporal resolution and dynamic range with minimal
motion blur, making them promising for object detection tasks. While Spiking
Neural Networks (SNNs) are a natural match for event-based sensory data and
enable ultra-energy efficient and low latency inference on neuromorphic
hardware, Artificial Neural Networks (ANNs) tend to display more stable
training dynamics and faster convergence resulting in greater task performance.
Hybrid SNN-ANN approaches are a promising alternative, enabling to leverage the
strengths of both SNN and ANN architectures. In this work, we introduce the
first Hybrid Attention-based SNN-ANN backbone for object detection using event
cameras. We propose a novel Attention-based SNN-ANN bridge module to capture
sparse spatial and temporal relations from the SNN layer and convert them into
dense feature maps for the ANN part of the backbone. Experimental results
demonstrate that our proposed method surpasses baseline hybrid and SNN-based
approaches by significant margins, with results comparable to existing
ANN-based methods. Extensive ablation studies confirm the effectiveness of our
proposed modules and architectural choices. These results pave the way toward a
hybrid SNN-ANN architecture that achieves ANN like performance at a drastically
reduced parameter budget. We implemented the SNN blocks on digital neuromorphic
hardware to investigate latency and power consumption and demonstrate the
feasibility of our approach.
\\ ( https://arxiv.org/abs/2403.10173 ,  3552kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10187 (*cross-listing*)
Date: Fri, 15 Mar 2024 10:48:16 GMT   (16643kb,D)

Title: Grasp Anything: Combining Teacher-Augmented Policy Gradient Learning
  with Instance Segmentation to Grasp Arbitrary Objects
Authors: Malte Mosbach and Sven Behnke
Categories: cs.RO cs.AI cs.LG
\\
  Interactive grasping from clutter, akin to human dexterity, is one of the
longest-standing problems in robot learning. Challenges stem from the
intricacies of visual perception, the demand for precise motor skills, and the
complex interplay between the two. In this work, we present Teacher-Augmented
Policy Gradient (TAPG), a novel two-stage learning framework that synergizes
reinforcement learning and policy distillation. After training a teacher policy
to master the motor control based on object pose information, TAPG facilitates
guided, yet adaptive, learning of a sensorimotor policy, based on object
segmentation. We zero-shot transfer from simulation to a real robot by using
Segment Anything Model for promptable object segmentation. Our trained policies
adeptly grasp a wide variety of objects from cluttered scenarios in simulation
and the real world based on human-understandable prompts. Furthermore, we show
robust zero-shot transfer to novel objects. Videos of our experiments are
available at \url{https://maltemosbach.github.io/grasp_anything}.
\\ ( https://arxiv.org/abs/2403.10187 ,  16643kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10190 (*cross-listing*)
Date: Fri, 15 Mar 2024 10:52:18 GMT   (486kb,D)

Title: Perceptual Quality-based Model Training under Annotator Label
  Uncertainty
Authors: Chen Zhou, Mohit Prabhushankar, Ghassan AlRegib
Categories: cs.CV cs.AI cs.LG
\\
  Annotators exhibit disagreement during data labeling, which can be termed as
annotator label uncertainty. Annotator label uncertainty manifests in
variations of labeling quality. Training with a single low-quality annotation
per sample induces model reliability degradations. In this work, we first
examine the effects of annotator label uncertainty in terms of the model's
generalizability and prediction uncertainty. We observe that the model's
generalizability and prediction uncertainty degrade with the presence of
low-quality noisy labels. Meanwhile, our evaluation of existing uncertainty
estimation algorithms indicates their incapability in response to annotator
label uncertainty. To mitigate performance degradation, prior methods show that
training models with labels collected from multiple independent annotators can
enhance generalizability. However, they require massive annotations. Hence, we
introduce a novel perceptual quality-based model training framework to
objectively generate multiple labels for model training to enhance reliability,
while avoiding massive annotations. Specifically, we first select a subset of
samples with low perceptual quality scores ranked by statistical regularities
of visual signals. We then assign de-aggregated labels to each sample in this
subset to obtain a training set with multiple labels. Our experiments and
analysis demonstrate that training with the proposed framework alleviates the
degradation of generalizability and prediction uncertainty caused by annotator
label uncertainty.
\\ ( https://arxiv.org/abs/2403.10190 ,  486kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10202 (*cross-listing*)
Date: Fri, 15 Mar 2024 11:07:38 GMT   (789kb,D)

Title: Learning on JPEG-LDPC Compressed Images: Classifying with Syndromes
Authors: Ahcen Aliouat, Elsa Dupraz
Categories: eess.IV cs.AI cs.CV cs.IT cs.LG math.IT
Comments: 5 pages, 3 figures, conference paper, submitted to the EUSIPCO 2024
  Conference
MSC-class: 94A08, 94A29, 68P30
ACM-class: I.4.2; I.4.9; E.4; C.2.0; I.2.10
\\
  In goal-oriented communications, the objective of the receiver is often to
apply a Deep-Learning model, rather than reconstructing the original data. In
this context, direct learning over compressed data, without any prior decoding,
holds promise for enhancing the time-efficient execution of inference models at
the receiver. However, conventional entropic-coding methods like Huffman and
Arithmetic break data structure, rendering them unsuitable for learning without
decoding. In this paper, we propose an alternative approach in which entropic
coding is realized with Low-Density Parity Check (LDPC) codes. We hypothesize
that Deep Learning models can more effectively exploit the internal code
structure of LDPC codes. At the receiver, we leverage a specific class of
Recurrent Neural Networks (RNNs), specifically Gated Recurrent Unit (GRU),
trained for image classification. Our numerical results indicate that
classification based on LDPC-coded bit-planes surpasses Huffman and Arithmetic
coding, while necessitating a significantly smaller learning model. This
demonstrates the efficiency of classification directly from LDPC-coded data,
eliminating the need for any form of decompression, even partial, prior to
applying the learning model.
\\ ( https://arxiv.org/abs/2403.10202 ,  789kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10216 (*cross-listing*)
Date: Fri, 15 Mar 2024 11:36:26 GMT   (7853kb,D)

Title: Exploring Optical Flow Inclusion into nnU-Net Framework for Surgical
  Instrument Segmentation
Authors: Marcos Fern\'andez-Rodr\'iguez, Bruno Silva, Sandro Queir\'os, Helena
  R. Torres, Bruno Oliveira, Pedro Morais, Lukas R. Buschle, Jorge
  Correia-Pinto, Estev\~ao Lima, Jo\~ao L. Vila\c{c}a
Categories: cs.CV cs.AI
\\
  Surgical instrument segmentation in laparoscopy is essential for
computer-assisted surgical systems. Despite the Deep Learning progress in
recent years, the dynamic setting of laparoscopic surgery still presents
challenges for precise segmentation. The nnU-Net framework excelled in semantic
segmentation analyzing single frames without temporal information. The
framework's ease of use, including its ability to be automatically configured,
and its low expertise requirements, have made it a popular base framework for
comparisons. Optical flow (OF) is a tool commonly used in video tasks to
estimate motion and represent it in a single frame, containing temporal
information. This work seeks to employ OF maps as an additional input to the
nnU-Net architecture to improve its performance in the surgical instrument
segmentation task, taking advantage of the fact that instruments are the main
moving objects in the surgical field. With this new input, the temporal
component would be indirectly added without modifying the architecture. Using
CholecSeg8k dataset, three different representations of movement were estimated
and used as new inputs, comparing them with a baseline model. Results showed
that the use of OF maps improves the detection of classes with high movement,
even when these are scarce in the dataset. To further improve performance,
future work may focus on implementing other OF-preserving augmentations.
\\ ( https://arxiv.org/abs/2403.10216 ,  7853kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10228 (*cross-listing*)
Date: Fri, 15 Mar 2024 11:58:18 GMT   (1313kb,D)

Title: HawkEye: Training Video-Text LLMs for Grounding Text in Videos
Authors: Yueqian Wang, Xiaojun Meng, Jianxin Liang, Yuxuan Wang, Qun Liu,
  Dongyan Zhao
Categories: cs.CV cs.AI cs.CL
\\
  Video-text Large Language Models (video-text LLMs) have shown remarkable
performance in answering questions and holding conversations on simple videos.
However, they perform almost the same as random on grounding text queries in
long and complicated videos, having little ability to understand and reason
about temporal information, which is the most fundamental difference between
videos and images. In this paper, we propose HawkEye, one of the first
video-text LLMs that can perform temporal video grounding in a fully
text-to-text manner. To collect training data that is applicable for temporal
video grounding, we construct InternVid-G, a large-scale video-text corpus with
segment-level captions and negative spans, with which we introduce two new
time-aware training objectives to video-text LLMs. We also propose a
coarse-grained method of representing segments in videos, which is more robust
and easier for LLMs to learn and follow than other alternatives. Extensive
experiments show that HawkEye is better at temporal video grounding and
comparable on other video-text tasks with existing video-text LLMs, which
verifies its superior video-text multi-modal understanding abilities.
\\ ( https://arxiv.org/abs/2403.10228 ,  1313kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10288 (*cross-listing*)
Date: Fri, 15 Mar 2024 13:29:45 GMT   (244kb,D)

Title: Rough Transformers for Continuous and Efficient Time-Series Modelling
Authors: Fernando Moreno-Pino, \'Alvaro Arroyo, Harrison Waldon, Xiaowen Dong,
  \'Alvaro Cartea
Categories: stat.ML cs.AI cs.LG
\\
  Time-series data in real-world medical settings typically exhibit long-range
dependencies and are observed at non-uniform intervals. In such contexts,
traditional sequence-based recurrent models struggle. To overcome this,
researchers replace recurrent architectures with Neural ODE-based models to
model irregularly sampled data and use Transformer-based architectures to
account for long-range dependencies. Despite the success of these two
approaches, both incur very high computational costs for input sequences of
moderate lengths and greater. To mitigate this, we introduce the Rough
Transformer, a variation of the Transformer model which operates on
continuous-time representations of input sequences and incurs significantly
reduced computational costs, critical for addressing long-range dependencies
common in medical contexts. In particular, we propose multi-view signature
attention, which uses path signatures to augment vanilla attention and to
capture both local and global dependencies in input data, while remaining
robust to changes in the sequence length and sampling frequency. We find that
Rough Transformers consistently outperform their vanilla attention counterparts
while obtaining the benefits of Neural ODE-based models using a fraction of the
computational time and memory resources on synthetic and real-world time-series
tasks.
\\ ( https://arxiv.org/abs/2403.10288 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10327 (*cross-listing*)
Date: Fri, 15 Mar 2024 14:16:10 GMT   (583kb)

Title: Unsupervised Threat Hunting using Continuous Bag-of-Terms-and-Time
  (CBoTT)
Authors: Varol Kayhan, Shivendu Shivendu, Rouzbeh Behnia, Clinton Daniel,
  Manish Agrawal
Categories: cs.CR cs.AI
\\
  Threat hunting is sifting through system logs to detect malicious activities
that might have bypassed existing security measures. It can be performed in
several ways, one of which is based on detecting anomalies. We propose an
unsupervised framework, called continuous bag-of-terms-and-time (CBoTT), and
publish its application programming interface (API) to help researchers and
cybersecurity analysts perform anomaly-based threat hunting among SIEM logs
geared toward process auditing on endpoint devices. Analyses show that our
framework consistently outperforms benchmark approaches. When logs are sorted
by likelihood of being an anomaly (from most likely to least), our approach
identifies anomalies at higher percentiles (between 1.82-6.46) while benchmark
approaches identify the same anomalies at lower percentiles (between
3.25-80.92). This framework can be used by other researchers to conduct
benchmark analyses and cybersecurity analysts to find anomalies in SIEM logs.
\\ ( https://arxiv.org/abs/2403.10327 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10365 (*cross-listing*)
Date: Fri, 15 Mar 2024 14:58:27 GMT   (56kb)

Title: Scalable Algorithms for Individual Preference Stable Clustering
Authors: Ron Mosenzon, Ali Vakilian
Categories: cs.DS cs.AI cs.CY cs.LG
Comments: 59 pages, 9 figures, submitted to AIStats2024
\\
  In this paper, we study the individual preference (IP) stability, which is an
notion capturing individual fairness and stability in clustering. Within this
setting, a clustering is $\alpha$-IP stable when each data point's average
distance to its cluster is no more than $\alpha$ times its average distance to
any other cluster. In this paper, we study the natural local search algorithm
for IP stable clustering. Our analysis confirms a $O(\log n)$-IP stability
guarantee for this algorithm, where $n$ denotes the number of points in the
input. Furthermore, by refining the local search approach, we show it runs in
an almost linear time, $\tilde{O}(nk)$.
\\ ( https://arxiv.org/abs/2403.10365 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10380 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:10:40 GMT   (450kb,D)

Title: BirdSet: A Multi-Task Benchmark for Classification in Avian Bioacoustics
Authors: Lukas Rauch, Raphael Schwinger, Moritz Wirth, Ren\'e Heinrich, Jonas
  Lange, Stefan Kahl, Bernhard Sick, Sven Tomforde, Christoph Scholz
Categories: cs.SD cs.AI eess.AS
Comments: Work in progress, to be submitted @DMLR next month
\\
  Deep learning (DL) models have emerged as a powerful tool in avian
bioacoustics to diagnose environmental health and biodiversity. However,
inconsistencies in research pose notable challenges hindering progress in this
domain. Reliable DL models need to analyze bird calls flexibly across various
species and environments to fully harness the potential of bioacoustics in a
cost-effective passive acoustic monitoring scenario. Data fragmentation and
opacity across studies complicate a comprehensive evaluation of general model
performance. To overcome these challenges, we present the BirdSet benchmark, a
unified framework consolidating research efforts with a holistic approach for
classifying bird vocalizations in avian bioacoustics. BirdSet harmonizes
open-source bird recordings into a curated dataset collection. This unified
approach provides an in-depth understanding of model performance and identifies
potential shortcomings across different tasks. By establishing baseline results
of current models, BirdSet aims to facilitate comparability, guide subsequent
data collection, and increase accessibility for newcomers to avian
bioacoustics.
\\ ( https://arxiv.org/abs/2403.10380 ,  450kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10401 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:34:59 GMT   (6199kb,D)

Title: SculptDiff: Learning Robotic Clay Sculpting from Humans with Goal
  Conditioned Diffusion Policy
Authors: Alison Bartsch, Arvind Car, Charlotte Avra, Amir Barati Farimani
Categories: cs.RO cs.AI
\\
  Manipulating deformable objects remains a challenge within robotics due to
the difficulties of state estimation, long-horizon planning, and predicting how
the object will deform given an interaction. These challenges are the most
pronounced with 3D deformable objects. We propose SculptDiff, a
goal-conditioned diffusion-based imitation learning framework that works with
point cloud state observations to directly learn clay sculpting policies for a
variety of target shapes. To the best of our knowledge this is the first
real-world method that successfully learns manipulation policies for 3D
deformable objects. For sculpting videos and access to our dataset and hardware
CAD models, see the project website:
https://sites.google.com/andrew.cmu.edu/imitation-sculpting/home
\\ ( https://arxiv.org/abs/2403.10401 ,  6199kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10403 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:37:04 GMT   (520kb,D)

Title: Energy Correction Model in the Feature Space for Out-of-Distribution
  Detection
Authors: Marc Lafon and Cl\'ement Rambour and Nicolas Thome
Categories: cs.CV cs.AI cs.LG
Comments: NeurIPS ML Safety Workshop (2022)
\\
  In this work, we study the out-of-distribution (OOD) detection problem
through the use of the feature space of a pre-trained deep classifier. We show
that learning the density of in-distribution (ID) features with an energy-based
models (EBM) leads to competitive detection results. However, we found that the
non-mixing of MCMC sampling during the EBM's training undermines its detection
performance. To overcome this an energy-based correction of a mixture of
class-conditional Gaussian distributions. We obtains favorable results when
compared to a strong baseline like the KNN detector on the CIFAR-10/CIFAR-100
OOD detection benchmarks.
\\ ( https://arxiv.org/abs/2403.10403 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10425 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:58:51 GMT   (2788kb,D)

Title: NeuFlow: Real-time, High-accuracy Optical Flow Estimation on Robots
  Using Edge Devices
Authors: Zhiyong Zhang, Huaizu Jiang, Hanumant Singh
Categories: cs.CV cs.AI cs.RO
\\
  Real-time high-accuracy optical flow estimation is a crucial component in
various applications, including localization and mapping in robotics, object
tracking, and activity recognition in computer vision. While recent
learning-based optical flow methods have achieved high accuracy, they often
come with heavy computation costs. In this paper, we propose a highly efficient
optical flow architecture, called NeuFlow, that addresses both high accuracy
and computational cost concerns. The architecture follows a global-to-local
scheme. Given the features of the input images extracted at different spatial
resolutions, global matching is employed to estimate an initial optical flow on
the 1/16 resolution, capturing large displacement, which is then refined on the
1/8 resolution with lightweight CNN layers for better accuracy. We evaluate our
approach on Jetson Orin Nano and RTX 2080 to demonstrate efficiency
improvements across different computing platforms. We achieve a notable 10x-80x
speedup compared to several state-of-the-art methods, while maintaining
comparable accuracy. Our approach achieves around 30 FPS on edge computing
platforms, which represents a significant breakthrough in deploying complex
computer vision tasks such as SLAM on small robots like drones. The full
training and evaluation code is available at
https://github.com/neufieldrobotics/NeuFlow.
\\ ( https://arxiv.org/abs/2403.10425 ,  2788kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10433 (*cross-listing*)
Date: Fri, 15 Mar 2024 16:11:15 GMT   (1442kb,D)

Title: AI-enhanced Collective Intelligence: The State of the Art and Prospects
Authors: Hao Cui and Taha Yasseri
Categories: cs.CY cs.AI
Comments: 27 pages, 2 figures
\\
  The current societal challenges exceed the capacity of human individual or
collective effort alone. As AI evolves, its role within human collectives is
poised to vary from an assistive tool to a participatory member. Humans and AI
possess complementary capabilities that, when synergized, can achieve a level
of collective intelligence that surpasses the collective capabilities of either
humans or AI in isolation. However, the interactions in human-AI systems are
inherently complex, involving intricate processes and interdependencies. This
review incorporates perspectives from network science to conceptualize a
multilayer representation of human-AI collective intelligence, comprising a
cognition layer, a physical layer, and an information layer. Within this
multilayer network, humans and AI agents exhibit varying characteristics;
humans differ in diversity from surface-level to deep-level attributes, while
AI agents range in degrees of functionality and anthropomorphism. The interplay
among these agents shapes the overall structure and dynamics of the system. We
explore how agents' diversity and interactions influence the system's
collective intelligence. Furthermore, we present an analysis of real-world
instances of AI-enhanced collective intelligence. We conclude by addressing the
potential challenges in AI-enhanced collective intelligence and offer
perspectives on future developments in this field.
\\ ( https://arxiv.org/abs/2403.10433 ,  1442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10438 (*cross-listing*)
Date: Fri, 15 Mar 2024 16:20:51 GMT   (4739kb,D)

Title: Data Ethics Emergency Drill: A Toolbox for Discussing Responsible AI for
  Industry Teams
Authors: Vanessa Aisyahsari Hanschke, Dylan Rees, Merve Alanyali, David
  Hopkinson and Paul Marshall
Categories: cs.HC cs.AI cs.CY
Comments: accepted to CHI 2024
\\
  Researchers urge technology practitioners such as data scientists to consider
the impacts and ethical implications of algorithmic decisions. However, unlike
programming, statistics, and data management, discussion of ethical
implications is rarely included in standard data science training. To begin to
address this gap, we designed and tested a toolbox called the data ethics
emergency drill (DEED) to help data science teams discuss and reflect on the
ethical implications of their work. The DEED is a roleplay of a fictional
ethical emergency scenario that is contextually situated in the team's specific
workplace and applications. This paper outlines the DEED toolbox and describes
three studies carried out with two different data science teams that
iteratively shaped its design. Our findings show that practitioners can apply
lessons learnt from the roleplay to real-life situations, and how the DEED
opened up conversations around ethics and values.
\\ ( https://arxiv.org/abs/2403.10438 ,  4739kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10454 (*cross-listing*)
Date: Fri, 15 Mar 2024 16:42:14 GMT   (19872kb,D)

Title: Partially Observable Task and Motion Planning with Uncertainty and Risk
  Awareness
Authors: Aidan Curtis, George Matheos, Nishad Gothoskar, Vikash Mansinghka,
  Joshua Tenenbaum, Tom\'as Lozano-P\'erez, Leslie Pack Kaelbling
Categories: cs.RO cs.AI
\\
  Integrated task and motion planning (TAMP) has proven to be a valuable
approach to generalizable long-horizon robotic manipulation and navigation
problems. However, the typical TAMP problem formulation assumes full
observability and deterministic action effects. These assumptions limit the
ability of the planner to gather information and make decisions that are
risk-aware. We propose a strategy for TAMP with Uncertainty and Risk Awareness
(TAMPURA) that is capable of efficiently solving long-horizon planning problems
with initial-state and action outcome uncertainty, including problems that
require information gathering and avoiding undesirable and irreversible
outcomes. Our planner reasons under uncertainty at both the abstract task level
and continuous controller level. Given a set of closed-loop goal-conditioned
controllers operating in the primitive action space and a description of their
preconditions and potential capabilities, we learn a high-level abstraction
that can be solved efficiently and then refined to continuous actions for
execution. We demonstrate our approach on several robotics problems where
uncertainty is a crucial factor and show that reasoning under uncertainty in
these problems outperforms previously proposed determinized planning, direct
search, and reinforcement learning strategies. Lastly, we demonstrate our
planner on two real-world robotics problems using recent advancements in
probabilistic perception.
\\ ( https://arxiv.org/abs/2403.10454 ,  19872kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10460 (*cross-listing*)
Date: Fri, 15 Mar 2024 16:51:30 GMT   (2846kb,D)

Title: Online Concurrent Multi-Robot Coverage Path Planning
Authors: Ratijit Mitra and Indranil Saha
Categories: cs.RO cs.AI
\\
  Recently, centralized receding horizon online multi-robot coverage path
planning algorithms have shown remarkable scalability in thoroughly exploring
large, complex, unknown workspaces with many robots. In a horizon, the path
planning and the path execution interleave, meaning when the path planning
occurs for robots with no paths, the robots with outstanding paths do not
execute, and subsequently, when the robots with new or outstanding paths
execute to reach respective goals, path planning does not occur for those
robots yet to get new paths, leading to wastage of both the robotic and the
computation resources. As a remedy, we propose a centralized algorithm that is
not horizon-based. It plans paths at any time for a subset of robots with no
paths, i.e., who have reached their previously assigned goals, while the rest
execute their outstanding paths, thereby enabling concurrent planning and
execution. We formally prove that the proposed algorithm ensures complete
coverage of an unknown workspace and analyze its time complexity. To
demonstrate scalability, we evaluate our algorithm to cover eight large $2$D
grid benchmark workspaces with up to 512 aerial and ground robots,
respectively. A comparison with a state-of-the-art horizon-based algorithm
shows its superiority in completing the coverage with up to 1.6x speedup. For
validation, we perform ROS + Gazebo simulations in six 2D grid benchmark
workspaces with 10 quadcopters and TurtleBots, respectively. We also
successfully conducted one outdoor experiment with three quadcopters and one
indoor with two TurtleBots.
\\ ( https://arxiv.org/abs/2403.10460 ,  2846kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10462 (*cross-listing*)
Date: Fri, 15 Mar 2024 16:53:13 GMT   (3694kb,D)

Title: Safety Cases: Justifying the Safety of Advanced AI Systems
Authors: Joshua Clymer, Nick Gabrieli, David Krueger, Thomas Larsen
Categories: cs.CY cs.AI
\\
  As AI systems become more advanced, companies and regulators will make
difficult decisions about whether it is safe to train and deploy them. To
prepare for these decisions, we investigate how developers could make a 'safety
case,' which is a structured rationale that AI systems are unlikely to cause a
catastrophe. We propose a framework for organizing a safety case and discuss
four categories of arguments to justify safety: total inability to cause a
catastrophe, sufficiently strong control measures, trustworthiness despite
capability to cause harm, and deference to credible AI advisors. We evaluate
concrete examples of arguments in each category and outline how arguments could
be combined to justify that AI systems are safe to deploy.
\\ ( https://arxiv.org/abs/2403.10462 ,  3694kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10482 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:12:57 GMT   (2055kb)

Title: Can a GPT4-Powered AI Agent Be a Good Enough Performance Attribution
  Analyst?
Authors: Bruno de Melo
Categories: q-fin.CP cs.AI q-fin.PM
\\
  Performance attribution analysis, defined as the process of explaining the
drivers of the excess performance of an investment portfolio against a
benchmark, stands as a significant aspect of portfolio management and plays a
crucial role in the investment decision-making process, particularly within the
fund management industry. Rooted in a solid financial and mathematical
framework, the importance and methodologies of this analytical technique are
extensively documented across numerous academic research papers and books. The
integration of large language models (LLMs) and AI agents marks a
groundbreaking development in this field. These agents are designed to automate
and enhance the performance attribution analysis by accurately calculating and
analyzing portfolio performances against benchmarks. In this study, we
introduce the application of an AI Agent for a variety of essential performance
attribution tasks, including the analysis of performance drivers and utilizing
LLMs as calculation engine for multi-level attribution analysis and
question-answer (QA) exercises. Leveraging advanced prompt engineering
techniques such as Chain-of-Thought (CoT) and Plan and Solve (PS), and
employing a standard agent framework from LangChain, the research achieves
promising results: it achieves accuracy rates exceeding 93% in analyzing
performance drivers, attains 100% in multi-level attribution calculations, and
surpasses 84% accuracy in QA exercises that simulate official examination
standards. These findings affirm the impactful role of AI agents, prompt
engineering and evaluation in advancing portfolio management processes,
highlighting a significant advancement in the practical application and
evaluation of AI technologies within the domain.
\\ ( https://arxiv.org/abs/2403.10482 ,  2055kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10487 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:21:39 GMT   (5740kb,D)

Title: Stimulate the Potential of Robots via Competition
Authors: Kangyao Huang, Di Guo, Xinyu Zhang, Xiangyang Ji, Huaping Liu
Categories: cs.RO cs.AI
\\
  It is common for us to feel pressure in a competition environment, which
arises from the desire to obtain success comparing with other individuals or
opponents. Although we might get anxious under the pressure, it could also be a
drive for us to stimulate our potentials to the best in order to keep up with
others. Inspired by this, we propose a competitive learning framework which is
able to help individual robot to acquire knowledge from the competition, fully
stimulating its dynamics potential in the race. Specifically, the competition
information among competitors is introduced as the additional auxiliary signal
to learn advantaged actions. We further build a Multiagent-Race environment,
and extensive experiments are conducted, demonstrating that robots trained in
competitive environments outperform ones that are trained with SoTA algorithms
in single robot environment.
\\ ( https://arxiv.org/abs/2403.10487 ,  5740kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10506 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:45:44 GMT   (21456kb,D)

Title: HumanoidBench: Simulated Humanoid Benchmark for Whole-Body Locomotion
  and Manipulation
Authors: Carmelo Sferrazza, Dun-Ming Huang, Xingyu Lin, Youngwoon Lee, Pieter
  Abbeel
Categories: cs.RO cs.AI cs.LG
\\
  Humanoid robots hold great promise in assisting humans in diverse
environments and tasks, due to their flexibility and adaptability leveraging
human-like morphology. However, research in humanoid robots is often
bottlenecked by the costly and fragile hardware setups. To accelerate
algorithmic research in humanoid robots, we present a high-dimensional,
simulated robot learning benchmark, HumanoidBench, featuring a humanoid robot
equipped with dexterous hands and a variety of challenging whole-body
manipulation and locomotion tasks. Our findings reveal that state-of-the-art
reinforcement learning algorithms struggle with most tasks, whereas a
hierarchical learning baseline achieves superior performance when supported by
robust low-level policies, such as walking or reaching. With HumanoidBench, we
provide the robotics community with a platform to identify the challenges
arising when solving diverse tasks with humanoid robots, facilitating prompt
verification of algorithms and ideas. The open-source code is available at
https://sferrazza.cc/humanoidbench_site.
\\ ( https://arxiv.org/abs/2403.10506 ,  21456kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10516 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:57:06 GMT   (38876kb,D)

Title: FeatUp: A Model-Agnostic Framework for Features at Any Resolution
Authors: Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong
  Zhang, William T. Freeman
Categories: cs.CV cs.AI cs.IR cs.LG
Comments: Accepted to the International Conference on Learning Representations
  (ICLR) 2024
\\
  Deep features are a cornerstone of computer vision research, capturing image
semantics and enabling the community to solve downstream tasks even in the
zero- or few-shot regime. However, these features often lack the spatial
resolution to directly perform dense prediction tasks like segmentation and
depth prediction because models aggressively pool information over large areas.
In this work, we introduce FeatUp, a task- and model-agnostic framework to
restore lost spatial information in deep features. We introduce two variants of
FeatUp: one that guides features with high-resolution signal in a single
forward pass, and one that fits an implicit model to a single image to
reconstruct features at any resolution. Both approaches use a multi-view
consistency loss with deep analogies to NeRFs. Our features retain their
original semantics and can be swapped into existing applications to yield
resolution and performance gains even without re-training. We show that FeatUp
significantly outperforms other feature upsampling and image super-resolution
approaches in class activation map generation, transfer learning for
segmentation and depth prediction, and end-to-end training for semantic
segmentation.
\\ ( https://arxiv.org/abs/2403.10516 ,  38876kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10517 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:57:52 GMT   (1761kb,D)

Title: VideoAgent: Long-form Video Understanding with Large Language Model as
  Agent
Authors: Xiaohan Wang, Yuhui Zhang, Orr Zohar and Serena Yeung-Levy
Categories: cs.CV cs.AI cs.CL cs.IR
\\
  Long-form video understanding represents a significant challenge within
computer vision, demanding a model capable of reasoning over long multi-modal
sequences. Motivated by the human cognitive process for long-form video
understanding, we emphasize interactive reasoning and planning over the ability
to process lengthy visual inputs. We introduce a novel agent-based system,
VideoAgent, that employs a large language model as a central agent to
iteratively identify and compile crucial information to answer a question, with
vision-language foundation models serving as tools to translate and retrieve
visual information. Evaluated on the challenging EgoSchema and NExT-QA
benchmarks, VideoAgent achieves 54.1% and 71.3% zero-shot accuracy with only
8.4 and 8.2 frames used on average. These results demonstrate superior
effectiveness and efficiency of our method over the current state-of-the-art
methods, highlighting the potential of agent-based approaches in advancing
long-form video understanding.
\\ ( https://arxiv.org/abs/2403.10517 ,  1761kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09715 (*cross-listing*)
Date: Mon, 11 Mar 2024 20:45:27 GMT   (526kb,D)

Title: Textual analysis of End User License Agreement for red-flagging
  potentially malicious software
Authors: Behraj Khan, Tahir Syed, Zeshan Khan, Muhammad Rafi
Categories: cs.SE cs.CL cs.CR cs.LG
DOI: 10.1109/ICECCE49384.2020.9179319
\\
  New software and updates are downloaded by end users every day. Each
dowloaded software has associated with it an End Users License Agreements
(EULA), but this is rarely read. An EULA includes information to avoid legal
repercussions. However,this proposes a host of potential problems such as
spyware or producing an unwanted affect in the target system. End users do not
read these EULA's because of length of the document and users find it extremely
difficult to understand. Text summarization is one of the relevant solution to
these kind of problems. This require a solution which can summarize the EULA
and classify the EULA as "Benign" or "Malicious". We propose a solution in
which we have summarize the EULA and classify the EULA as "Benign" or
"Malicious". We extract EULA text of different sofware's then we classify the
text using eight different supervised classifiers. we use ensemble learning to
classify the EULA as benign or malicious using five different text
summarization methods. An accuracy of $95.8$\% shows the effectiveness of the
presented approach.
\\ ( https://arxiv.org/abs/2403.09715 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09792 (*cross-listing*)
Date: Thu, 14 Mar 2024 18:24:55 GMT   (12272kb,D)

Title: Images are Achilles' Heel of Alignment: Exploiting Visual
  Vulnerabilities for Jailbreaking Multimodal Large Language Models
Authors: Yifan Li, Hangyu Guo, Kun Zhou, Wayne Xin Zhao and Ji-Rong Wen
Categories: cs.CV cs.CL
Comments: Work in progress
\\
  In this paper, we study the harmlessness alignment problem of multimodal
large language models~(MLLMs). We conduct a systematic empirical analysis of
the harmlessness performance of representative MLLMs and reveal that the image
input poses the alignment vulnerability of MLLMs. Inspired by this, we propose
a novel jailbreak method named HADES, which hides and amplifies the harmfulness
of the malicious intent within the text input, using meticulously crafted
images. Experimental results show that HADES can effectively jailbreak existing
MLLMs, which achieves an average Attack Success Rate~(ASR) of 90.26% for
LLaVA-1.5 and 71.60% for Gemini Pro Vision. Our code and data will be publicly
released.
\\ ( https://arxiv.org/abs/2403.09792 ,  12272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10059 (*cross-listing*)
Date: Fri, 15 Mar 2024 06:59:43 GMT   (9421kb,D)

Title: Repoformer: Selective Retrieval for Repository-Level Code Completion
Authors: Di Wu, Wasi Uddin Ahmad, Dejiao Zhang, Murali Krishna Ramanathan,
  Xiaofei Ma
Categories: cs.SE cs.CL
\\
  Recent advances in retrieval-augmented generation (RAG) have initiated a new
era in repository-level code completion. However, the invariable use of
retrieval in existing methods exposes issues in both efficiency and robustness,
with a large proportion of the retrieved contexts proving unhelpful or harmful
to code language models (code LMs). To tackle the challenges, this paper
proposes a selective RAG framework where retrieval is avoided when unnecessary.
To power this framework, we design a self-supervised learning approach that
enables a code LM to accurately self-evaluate whether retrieval can improve its
output quality and robustly leverage the potentially noisy retrieved contexts.
Using this LM as both the selective retrieval policy and the generation model,
our framework consistently outperforms the state-of-the-art prompting with an
invariable retrieval approach on diverse benchmarks including RepoEval,
CrossCodeEval, and a new benchmark. Meanwhile, our selective retrieval strategy
results in strong efficiency improvements by as much as 70% inference speedup
without harming the performance. We demonstrate that our framework effectively
accommodates different generation models, retrievers, and programming
languages. These advancements position our framework as an important step
towards more accurate and efficient repository-level code completion.
\\ ( https://arxiv.org/abs/2403.10059 ,  9421kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09672 (*cross-listing*)
Date: Sun, 4 Feb 2024 08:05:58 GMT   (2125kb,D)

Title: COMPRER: A Multimodal Multi-Objective Pretraining Framework for Enhanced
  Medical Image Representation
Authors: Guy Lutsker, Hagai Rossman, Nastya Godiva, Eran Segal
Categories: cs.CV cs.LG
\\
  Substantial advances in multi-modal Artificial Intelligence (AI) facilitate
the combination of diverse medical modalities to achieve holistic health
assessments. We present COMPRER , a novel multi-modal, multi-objective
pretraining framework which enhances medical-image representation, diagnostic
inferences, and prognosis of diseases. COMPRER employs a multi-objective
training framework, where each objective introduces distinct knowledge to the
model. This includes a multimodal loss that consolidates information across
different imaging modalities; A temporal loss that imparts the ability to
discern patterns over time; Medical-measure prediction adds appropriate medical
insights; Lastly, reconstruction loss ensures the integrity of image structure
within the latent space. Despite the concern that multiple objectives could
weaken task performance, our findings show that this combination actually
boosts outcomes on certain tasks. Here, we apply this framework to both fundus
images and carotid ultrasound, and validate our downstream tasks capabilities
by predicting both current and future cardiovascular conditions. COMPRER
achieved higher Area Under the Curve (AUC) scores in evaluating medical
conditions compared to existing models on held-out data. On the
Out-of-distribution (OOD) UK-Biobank dataset COMPRER maintains favorable
performance over well-established models with more parameters, even though
these models were trained on $75\times$ more data than COMPRER. In addition, to
better assess our model's performance in contrastive learning, we introduce a
novel evaluation metric, providing deeper understanding of the effectiveness of
the latent space pairing.
\\ ( https://arxiv.org/abs/2403.09672 ,  2125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09681 (*cross-listing*)
Date: Wed, 7 Feb 2024 17:06:32 GMT   (4244kb,D)

Title: ViT-MUL: A Baseline Study on Recent Machine Unlearning Methods Applied
  to Vision Transformers
Authors: Ikhyun Cho, Changyeon Park, and Julia Hockenmaier
Categories: cs.CV cs.LG
Comments: 7 pages
\\
  Machine unlearning (MUL) is an arising field in machine learning that seeks
to erase the learned information of specific training data points from a
trained model. Despite the recent active research in MUL within computer
vision, the majority of work has focused on ResNet-based models. Given that
Vision Transformers (ViT) have become the predominant model architecture, a
detailed study of MUL specifically tailored to ViT is essential. In this paper,
we present comprehensive experiments on ViTs using recent MUL algorithms and
datasets. We anticipate that our experiments, ablation studies, and findings
could provide valuable insights and inspire further research in this field.
\\ ( https://arxiv.org/abs/2403.09681 ,  4244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09683 (*cross-listing*)
Date: Wed, 7 Feb 2024 20:55:39 GMT   (15923kb,D)

Title: Counterfactual Image Editing
Authors: Yushu Pan, Elias Bareinboim
Categories: cs.CV cs.LG
\\
  Counterfactual image editing is an important task in generative AI, which
asks how an image would look if certain features were different. The current
literature on the topic focuses primarily on changing individual features while
remaining silent about the causal relationships between these features, as
present in the real world. In this paper, we formalize the counterfactual image
editing task using formal language, modeling the causal relationships between
latent generative factors and images through a special type of model called
augmented structural causal models (ASCMs). Second, we show two fundamental
impossibility results: (1) counterfactual editing is impossible from i.i.d.
image samples and their corresponding labels alone; (2) even when the causal
relationships between the latent generative factors and images are available,
no guarantees regarding the output of the model can be provided. Third, we
propose a relaxation for this challenging problem by approximating
non-identifiable counterfactual distributions with a new family of
counterfactual-consistent estimators. This family exhibits the desirable
property of preserving features that the user cares about across both factual
and counterfactual worlds. Finally, we develop an efficient algorithm to
generate counterfactual images by leveraging neural causal models.
\\ ( https://arxiv.org/abs/2403.09683 ,  15923kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09755 (*cross-listing*)
Date: Thu, 14 Mar 2024 14:02:00 GMT   (367kb,D)

Title: Estimating the history of a random recursive tree
Authors: Simon Briend, Christophe Giraud, G\'abor Lugosi and D\'eborah Sulem
Categories: stat.ML cs.LG cs.SI
\\
  This paper studies the problem of estimating the order of arrival of the
vertices in a random recursive tree. Specifically, we study two fundamental
models: the uniform attachment model and the linear preferential attachment
model. We propose an order estimator based on the Jordan centrality measure and
define a family of risk measures to quantify the quality of the ordering
procedure. Moreover, we establish a minimax lower bound for this problem, and
prove that the proposed estimator is nearly optimal. Finally, we numerically
demonstrate that the proposed estimator outperforms degree-based and spectral
ordering procedures.
\\ ( https://arxiv.org/abs/2403.09755 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09758 (*cross-listing*)
Date: Thu, 14 Mar 2024 15:41:15 GMT   (2460kb,D)

Title: Reconstructing Blood Flow in Data-Poor Regimes: A Vasculature Network
  Kernel for Gaussian Process Regression
Authors: Shaghayegh Z. Ashtiani, Mohammad Sarabian, Kaveh Laksari, Hessam
  Babaee
Categories: eess.IV cs.LG
Comments: 20 pages, 9 figures
\\
  Blood flow reconstruction in the vasculature is important for many clinical
applications. However, in clinical settings, the available data are often quite
limited. For instance, Transcranial Doppler ultrasound (TCD) is a noninvasive
clinical tool that is commonly used in the clinical settings to measure blood
velocity waveform at several locations on brain's vasculature. This amount of
data is grossly insufficient for training machine learning surrogate models,
such as deep neural networks or Gaussian process regression. In this work, we
propose a Gaussian process regression approach based on physics-informed
kernels, enabling near-real-time reconstruction of blood flow in data-poor
regimes. We introduce a novel methodology to reconstruct the kernel within the
vascular network, which is a non-Euclidean space. The proposed kernel encodes
both spatiotemporal and vessel-to-vessel correlations, thus enabling blood flow
reconstruction in vessels that lack direct measurements. We demonstrate that
any prediction made with the proposed kernel satisfies the conservation of mass
principle. The kernel is constructed by running stochastic one-dimensional
blood flow simulations, where the stochasticity captures the epistemic
uncertainties, such as lack of knowledge about boundary conditions and
uncertainties in vasculature geometries. We demonstrate the performance of the
model on three test cases, namely, a simple Y-shaped bifurcation, abdominal
aorta, and the Circle of Willis in the brain.
\\ ( https://arxiv.org/abs/2403.09758 ,  2460kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09805 (*cross-listing*)
Date: Thu, 14 Mar 2024 18:52:34 GMT   (3221kb,D)

Title: On the Utility of 3D Hand Poses for Action Recognition
Authors: Md Salman Shamil, Dibyadip Chatterjee, Fadime Sener, Shugao Ma, Angela
  Yao
Categories: cs.CV cs.LG
Comments: Project page: https://s-shamil.github.io/HandFormer/
\\
  3D hand poses are an under-explored modality for action recognition. Poses
are compact yet informative and can greatly benefit applications with limited
compute budgets. However, poses alone offer an incomplete understanding of
actions, as they cannot fully capture objects and environments with which
humans interact. To efficiently model hand-object interactions, we propose
HandFormer, a novel multimodal transformer. HandFormer combines 3D hand poses
at a high temporal resolution for fine-grained motion modeling with sparsely
sampled RGB frames for encoding scene semantics. Observing the unique
characteristics of hand poses, we temporally factorize hand modeling and
represent each joint by its short-term trajectories. This factorized pose
representation combined with sparse RGB samples is remarkably efficient and
achieves high accuracy. Unimodal HandFormer with only hand poses outperforms
existing skeleton-based methods at 5x fewer FLOPs. With RGB, we achieve new
state-of-the-art performance on Assembly101 and H2O with significant
improvements in egocentric action recognition.
\\ ( https://arxiv.org/abs/2403.09805 ,  3221kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09811 (*cross-listing*)
Date: Thu, 14 Mar 2024 18:59:54 GMT   (7348kb)

Title: Adapting OC20-trained EquiformerV2 Models for High-Entropy Materials
Authors: Christian M. Clausen, Jan Rossmeisl, Zachary W. Ulissi
Categories: cond-mat.mtrl-sci cs.LG physics.chem-ph
\\
  Computational high-throughput studies, especially in research on high-entropy
materials and catalysts, are hampered by high-dimensional composition spaces
and myriad structural microstates. They present bottlenecks to the conventional
use of density functional theory calculations, and consequently, the use of
machine-learned potentials is becoming increasingly prevalent in atomic
structure simulations. In this communication, we show the results of adjusting
and fine-tuning the pretrained EquiformerV2 model from the Open Catalyst
Project to infer adsorption energies of *OH and *O on the out-of-domain
high-entropy alloy Ag-Ir-Pd-Pt-Ru. By applying an energy filter based on the
local environment of the binding site the zero-shot inference is markedly
improved and through few-shot fine-tuning the model yields state-of-the-art
accuracy. It is also found that EquiformerV2, assuming the role of general
machine learning potential, is able to inform a smaller, more focused direct
inference model. This knowledge distillation setup boosts performance on
complex binding sites. Collectively, this shows that foundational knowledge
learned from ordered intermetallic structures, can be extrapolated to the
highly disordered structures of solid-solutions. With the vastly accelerated
computational throughput of these models, hitherto infeasible research in the
high-entropy material space is now readily accessible.
\\ ( https://arxiv.org/abs/2403.09811 ,  7348kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09918 (*cross-listing*)
Date: Thu, 14 Mar 2024 23:31:41 GMT   (15294kb,D)

Title: Attention-based Class-Conditioned Alignment for Multi-Source Domain
  Adaptive Object Detection
Authors: Atif Belal and Akhil Meethal and Francisco Perdigon Romero and Marco
  Pedersoli and Eric Granger
Categories: cs.CV cs.LG
\\
  Domain adaptation methods for object detection (OD) strive to mitigate the
impact of distribution shifts by promoting feature alignment across source and
target domains. Multi-source domain adaptation (MSDA) allows leveraging
multiple annotated source datasets, and unlabeled target data to improve the
accuracy and robustness of the detection model. Most state-of-the-art MSDA
methods for OD perform feature alignment in a class-agnostic manner. This is
challenging since the objects have unique modal information due to variations
in object appearance across domains. A recent prototype-based approach proposed
a class-wise alignment, yet it suffers from error accumulation due to noisy
pseudo-labels which can negatively affect adaptation with imbalanced data. To
overcome these limitations, we propose an attention-based class-conditioned
alignment scheme for MSDA that aligns instances of each object category across
domains. In particular, an attention module coupled with an adversarial domain
classifier allows learning domain-invariant and class-specific instance
representations. Experimental results on multiple benchmarking MSDA datasets
indicate that our method outperforms the state-of-the-art methods and is robust
to class imbalance. Our code is available at https://github.com/imatif17/ACIA.
\\ ( https://arxiv.org/abs/2403.09918 ,  15294kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09942 (*cross-listing*)
Date: Fri, 15 Mar 2024 00:52:17 GMT   (1278kb,D)

Title: Attention-Enhanced Hybrid Feature Aggregation Network for 3D Brain Tumor
  Segmentation
Authors: Ziya Ata Yaz{\i}c{\i}, \.Ilkay \"Oks\"uz, Haz{\i}m Kemal Ekenel
Categories: eess.IV cs.CV cs.LG
Comments: Accepted at 9th BrainLes Workshop (BraTS 2023 Challenge) @
  International Conference on Medical Image Computing and Computer Assisted
  Intervention (MICCAI) 2023
\\
  Glioblastoma is a highly aggressive and malignant brain tumor type that
requires early diagnosis and prompt intervention. Due to its heterogeneity in
appearance, developing automated detection approaches is challenging. To
address this challenge, Artificial Intelligence (AI)-driven approaches in
healthcare have generated interest in efficiently diagnosing and evaluating
brain tumors. The Brain Tumor Segmentation Challenge (BraTS) is a platform for
developing and assessing automated techniques for tumor analysis using
high-quality, clinically acquired MRI data. In our approach, we utilized a
multi-scale, attention-guided and hybrid U-Net-shaped model -- GLIMS -- to
perform 3D brain tumor segmentation in three regions: Enhancing Tumor (ET),
Tumor Core (TC), and Whole Tumor (WT). The multi-scale feature extraction
provides better contextual feature aggregation in high resolutions and the Swin
Transformer blocks improve the global feature extraction at deeper levels of
the model. The segmentation mask generation in the decoder branch is guided by
the attention-refined features gathered from the encoder branch to enhance the
important attributes. Moreover, hierarchical supervision is used to train the
model efficiently. Our model's performance on the validation set resulted in
92.19, 87.75, and 83.18 Dice Scores and 89.09, 84.67, and 82.15 Lesion-wise
Dice Scores in WT, TC, and ET, respectively. The code is publicly available at
https://github.com/yaziciz/GLIMS.
\\ ( https://arxiv.org/abs/2403.09942 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09961 (*cross-listing*)
Date: Fri, 15 Mar 2024 01:55:07 GMT   (43856kb,D)

Title: Thermal Earth Model for the Conterminous United States Using an
  Interpolative Physics-Informed Graph Neural Network (InterPIGNN)
Authors: Mohammad J. Aljubran and Roland N. Horne
Categories: physics.geo-ph cs.LG
Comments: The thermal Earth model is made available as feature layers on ArcGIS
  at https://arcg.is/nLzzT0
\\
  This study presents a data-driven spatial interpolation algorithm based on
physics-informed graph neural networks used to develop national
temperature-at-depth maps for the conterminous United States. The model was
trained to approximately satisfy the three-dimensional heat conduction law by
simultaneously predicting subsurface temperature, surface heat flow, and rock
thermal conductivity. In addition to bottomhole temperature measurements, we
incorporated other physical quantities as model inputs, such as depth,
geographic coordinates, elevation, sediment thickness, magnetic anomaly,
gravity anomaly, gamma-ray flux of radioactive elements, seismicity, and
electric conductivity. We constructed surface heat flow, and temperature and
thermal conductivity predictions for depths of 0-7 km at an interval of 1 km
with spatial resolution of 18 km$^2$ per grid cell. Our model showed superior
temperature, surface heat flow and thermal conductivity mean absolute errors of
4.8{\deg} C, 5.817 mW/m$^2$ and 0.022 W/(C-m)$, respectively. The predictions
were visualized in two-dimensional spatial maps across the modeled depths. This
thorough modeling of the Earth's thermal processes is crucial to understanding
subsurface phenomena and exploiting natural underground resources.
\\ ( https://arxiv.org/abs/2403.09961 ,  43856kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10006 (*cross-listing*)
Date: Fri, 15 Mar 2024 04:04:40 GMT   (1861kb,D)

Title: Graph Enhanced Reinforcement Learning for Effective Group Formation in
  Collaborative Problem Solving
Authors: Zheng Fang, Fucai Ke, Jae Young Han, Zhijie Feng, Toby Cai
Categories: cs.CY cs.HC cs.LG cs.SI
\\
  This study addresses the challenge of forming effective groups in
collaborative problem-solving environments. Recognizing the complexity of human
interactions and the necessity for efficient collaboration, we propose a novel
approach leveraging graph theory and reinforcement learning. Our methodology
involves constructing a graph from a dataset where nodes represent
participants, and edges signify the interactions between them. We conceptualize
each participant as an agent within a reinforcement learning framework, aiming
to learn an optimal graph structure that reflects effective group dynamics.
Clustering techniques are employed to delineate clear group structures based on
the learned graph. Our approach provides theoretical solutions based on
evaluation metrics and graph measurements, offering insights into potential
improvements in group effectiveness and reductions in conflict incidences. This
research contributes to the fields of collaborative work and educational
psychology by presenting a data-driven, analytical approach to group formation.
It has practical implications for organizational team building, classroom
settings, and any collaborative scenario where group dynamics are crucial. The
study opens new avenues for exploring the application of graph theory and
reinforcement learning in social and behavioral sciences, highlighting the
potential for empirical validation in future work.
\\ ( https://arxiv.org/abs/2403.10006 ,  1861kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10013 (*cross-listing*)
Date: Fri, 15 Mar 2024 04:35:56 GMT   (1183kb,D)

Title: LyZNet: A Lightweight Python Tool for Learning and Verifying Neural
  Lyapunov Functions and Regions of Attraction
Authors: Jun Liu, Yiming Meng, Maxwell Fitzsimmons, and Ruikun Zhou
Categories: eess.SY cs.LG cs.SY math.OC
Comments: To appear in the 27th ACM International Conference on Hybrid Systems:
  Computation and Control (HSCC 2024). arXiv admin note: text overlap with
  arXiv:2312.09131
\\
  In this paper, we describe a lightweight Python framework that provides
integrated learning and verification of neural Lyapunov functions for stability
analysis. The proposed tool, named LyZNet, learns neural Lyapunov functions
using physics-informed neural networks (PINNs) to solve Zubov's equation and
verifies them using satisfiability modulo theories (SMT) solvers. What
distinguishes this tool from others in the literature is its ability to provide
verified regions of attraction close to the domain of attraction. This is
achieved by encoding Zubov's partial differential equation (PDE) into the PINN
approach. By embracing the non-convex nature of the underlying optimization
problems, we demonstrate that in cases where convex optimization, such as
semidefinite programming, fails to capture the domain of attraction, our neural
network framework proves more successful. The tool also offers automatic
decomposition of coupled nonlinear systems into a network of low-dimensional
subsystems for compositional verification. We illustrate the tool's usage and
effectiveness with several numerical examples, including both non-trivial
low-dimensional nonlinear systems and high-dimensional systems. The repository
of the tool can be found at https://git.uwaterloo.ca/hybrid-systems-lab/lyznet.
\\ ( https://arxiv.org/abs/2403.10013 ,  1183kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10015 (*cross-listing*)
Date: Fri, 15 Mar 2024 04:39:27 GMT   (351kb,D)

Title: Linear optimal transport subspaces for point set classification
Authors: Mohammad Shifat E Rabbi, Naqib Sad Pathan, Shiying Li, Yan Zhuang, Abu
  Hasnat Mohammad Rubaiyat, Gustavo K Rohde
Categories: cs.CV cs.LG
\\
  Learning from point sets is an essential component in many computer vision
and machine learning applications. Native, unordered, and permutation invariant
set structure space is challenging to model, particularly for point set
classification under spatial deformations. Here we propose a framework for
classifying point sets experiencing certain types of spatial deformations, with
a particular emphasis on datasets featuring affine deformations. Our approach
employs the Linear Optimal Transport (LOT) transform to obtain a linear
embedding of set-structured data. Utilizing the mathematical properties of the
LOT transform, we demonstrate its capacity to accommodate variations in point
sets by constructing a convex data space, effectively simplifying point set
classification problems. Our method, which employs a nearest-subspace algorithm
in the LOT space, demonstrates label efficiency, non-iterative behavior, and
requires no hyper-parameter tuning. It achieves competitive accuracies compared
to state-of-the-art methods across various point set classification tasks.
Furthermore, our approach exhibits robustness in out-of-distribution scenarios
where training and test distributions vary in terms of deformation magnitudes.
\\ ( https://arxiv.org/abs/2403.10015 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10042 (*cross-listing*)
Date: Fri, 15 Mar 2024 06:23:30 GMT   (956kb)

Title: Accurate and Data-Efficient Micro-XRD Phase Identification Using
  Multi-Task Learning: Application to Hydrothermal Fluids
Authors: Yanfei Li, Juejing Liu, Xiaodong Zhao, Wenjun Liu, Tong Geng, Ang Li,
  Xin Zhang
Categories: cond-mat.mtrl-sci cs.LG
\\
  Traditional analysis of highly distorted micro-X-ray diffraction ({\mu}-XRD)
patterns from hydrothermal fluid environments is a time-consuming process,
often requiring substantial data preprocessing and labeled experimental data.
This study demonstrates the potential of deep learning with a multitask
learning (MTL) architecture to overcome these limitations. We trained MTL
models to identify phase information in {\mu}-XRD patterns, minimizing the need
for labeled experimental data and masking preprocessing steps. Notably, MTL
models showed superior accuracy compared to binary classification CNNs.
Additionally, introducing a tailored cross-entropy loss function improved MTL
model performance. Most significantly, MTL models tuned to analyze raw and
unmasked XRD patterns achieved close performance to models analyzing
preprocessed data, with minimal accuracy differences. This work indicates that
advanced deep learning architectures like MTL can automate arduous data
handling tasks, streamline the analysis of distorted XRD patterns, and reduce
the reliance on labor-intensive experimental datasets.
\\ ( https://arxiv.org/abs/2403.10042 ,  956kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10070 (*cross-listing*)
Date: Fri, 15 Mar 2024 07:20:21 GMT   (14263kb,D)

Title: A Structure-Preserving Kernel Method for Learning Hamiltonian Systems
Authors: Jianyu Hu, Juan-Pablo Ortega, Daiying Yin
Categories: stat.ML cs.LG math.DS
\\
  A structure-preserving kernel ridge regression method is presented that
allows the recovery of potentially high-dimensional and nonlinear Hamiltonian
functions out of datasets made of noisy observations of Hamiltonian vector
fields. The method proposes a closed-form solution that yields excellent
numerical performances that surpass other techniques proposed in the literature
in this setup. From the methodological point of view, the paper extends kernel
regression methods to problems in which loss functions involving linear
functions of gradients are required and, in particular, a differential
reproducing property and a Representer Theorem are proved in this context. The
relation between the structure-preserving kernel estimator and the Gaussian
posterior mean estimator is analyzed. A full error analysis is conducted that
provides convergence rates using fixed and adaptive regularization parameters.
The good performance of the proposed estimator is illustrated with various
numerical experiments.
\\ ( https://arxiv.org/abs/2403.10070 ,  14263kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10075 (*cross-listing*)
Date: Fri, 15 Mar 2024 07:34:08 GMT   (2099kb)

Title: A survey of synthetic data augmentation methods in computer vision
Authors: Alhassan Mumuni, Fuseini Mumuni and Nana Kobina Gerrar
Categories: cs.CV cs.GR cs.LG
\\
  The standard approach to tackling computer vision problems is to train deep
convolutional neural network (CNN) models using large-scale image datasets
which are representative of the target task. However, in many scenarios, it is
often challenging to obtain sufficient image data for the target task. Data
augmentation is a way to mitigate this challenge. A common practice is to
explicitly transform existing images in desired ways so as to create the
required volume and variability of training data necessary to achieve good
generalization performance. In situations where data for the target domain is
not accessible, a viable workaround is to synthesize training data from
scratch--i.e., synthetic data augmentation. This paper presents an extensive
review of synthetic data augmentation techniques. It covers data synthesis
approaches based on realistic 3D graphics modeling, neural style transfer
(NST), differential neural rendering, and generative artificial intelligence
(AI) techniques such as generative adversarial networks (GANs) and variational
autoencoders (VAEs). For each of these classes of methods, we focus on the
important data generation and augmentation techniques, general scope of
application and specific use-cases, as well as existing limitations and
possible workarounds. Additionally, we provide a summary of common synthetic
datasets for training computer vision models, highlighting the main features,
application domains and supported tasks. Finally, we discuss the effectiveness
of synthetic data augmentation methods. Since this is the first paper to
explore synthetic data augmentation methods in great detail, we are hoping to
equip readers with the necessary background information and in-depth knowledge
of existing methods and their attendant issues.
\\ ( https://arxiv.org/abs/2403.10075 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10089 (*cross-listing*)
Date: Fri, 15 Mar 2024 08:05:16 GMT   (2542kb,D)

Title: Approximation and bounding techniques for the Fisher-Rao distances
Authors: Frank Nielsen
Categories: cs.IT cs.CV cs.LG math.IT
Comments: 38 pages
\\
  The Fisher-Rao distance between two probability distributions of a
statistical model is defined as the Riemannian geodesic distance induced by the
Fisher information metric. In order to calculate the Fisher-Rao distance in
closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and
(2) to integrate the Fisher length element along those geodesics. We consider
several numerically robust approximation and bounding techniques for the
Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao
distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we
describe several generic approximation schemes depending on whether the
Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In
particular, we obtain a generic method to guarantee an arbitrarily small
additive error on the approximation provided that Fisher-Rao pregeodesics and
tight lower and upper bounds are available. Third, we consider the case of
Fisher metrics being Hessian metrics, and report generic tight upper bounds on
the Fisher-Rao distances using techniques of information geometry.
Uniparametric and biparametric statistical models always have Fisher Hessian
metrics, and in general a simple test allows to check whether the Fisher
information matrix yields a Hessian metric or not. Fourth, we consider
elliptical distribution families and show how to apply the above techniques to
these models. We also propose two new distances based either on the Fisher-Rao
lengths of curves serving as proxies of Fisher-Rao geodesics, or based on the
Birkhoff/Hilbert projective cone distance. Last, we consider an alternative
group-theoretic approach for statistical transformation models based on the
notion of maximal invariant which yields insights on the structures of the
Fisher-Rao distance formula which may be used fruitfully in applications.
\\ ( https://arxiv.org/abs/2403.10089 ,  2542kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10153 (*cross-listing*)
Date: Fri, 15 Mar 2024 09:54:04 GMT   (4580kb,D)

Title: Improving Medical Multi-modal Contrastive Learning with Expert
  Annotations
Authors: Yogesh Kumar, Pekka Marttinen
Categories: cs.CV cs.LG
Comments: Under review at a conference
\\
  We introduce eCLIP, an enhanced version of the CLIP model that integrates
expert annotations in the form of radiologist eye-gaze heatmaps. It tackles key
challenges in contrastive multi-modal medical imaging analysis, notably data
scarcity and the "modality gap" -- a significant disparity between image and
text embeddings that diminishes the quality of representations and hampers
cross-modal interoperability. eCLIP integrates a heatmap processor and
leverages mixup augmentation to efficiently utilize the scarce expert
annotations, thus boosting the model's learning effectiveness. eCLIP is
designed to be generally applicable to any variant of CLIP without requiring
any modifications of the core architecture. Through detailed evaluations across
several tasks, including zero-shot inference, linear probing, cross-modal
retrieval, and Retrieval Augmented Generation (RAG) of radiology reports using
a frozen Large Language Model, eCLIP showcases consistent improvements in
embedding quality. The outcomes reveal enhanced alignment and uniformity,
affirming eCLIP's capability to harness high-quality annotations for enriched
multi-modal analysis in the medical imaging domain.
\\ ( https://arxiv.org/abs/2403.10153 ,  4580kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10232 (*cross-listing*)
Date: Fri, 15 Mar 2024 12:00:37 GMT   (1107kb,D)

Title: Matrix Completion via Nonsmooth Regularization of Fully Connected Neural
  Networks
Authors: Sajad Faramarzi, Farzan Haddadi, Sajjad Amini, Masoud Ahookhosh
Categories: cs.IT cs.LG math.IT
\\
  Conventional matrix completion methods approximate the missing values by
assuming the matrix to be low-rank, which leads to a linear approximation of
missing values. It has been shown that enhanced performance could be attained
by using nonlinear estimators such as deep neural networks. Deep fully
connected neural networks (FCNNs), one of the most suitable architectures for
matrix completion, suffer from over-fitting due to their high capacity, which
leads to low generalizability. In this paper, we control over-fitting by
regularizing the FCNN model in terms of the $\ell_{1}$ norm of intermediate
representations and nuclear norm of weight matrices. As such, the resulting
regularized objective function becomes nonsmooth and nonconvex, i.e., existing
gradient-based methods cannot be applied to our model. We propose a variant of
the proximal gradient method and investigate its convergence to a critical
point. In the initial epochs of FCNN training, the regularization terms are
ignored, and through epochs, the effect of that increases. The gradual addition
of nonsmooth regularization terms is the main reason for the better performance
of the deep neural network with nonsmooth regularization terms (DNN-NSR)
algorithm. Our simulations indicate the superiority of the proposed algorithm
in comparison with existing linear and nonlinear algorithms.
\\ ( https://arxiv.org/abs/2403.10232 ,  1107kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10250 (*cross-listing*)
Date: Fri, 15 Mar 2024 12:38:00 GMT   (1272kb,D)

Title: Interpretable Machine Learning for Survival Analysis
Authors: Sophie Hanna Langbein, Mateusz Krzyzi\'nski, Miko{\l}aj Spytek, Hubert
  Baniecki, Przemys{\l}aw Biecek, Marvin N. Wright
Categories: stat.ML cs.LG stat.ME
\\
  With the spread and rapid advancement of black box machine learning models,
the field of interpretable machine learning (IML) or explainable artificial
intelligence (XAI) has become increasingly important over the last decade. This
is particularly relevant for survival analysis, where the adoption of IML
techniques promotes transparency, accountability and fairness in sensitive
areas, such as clinical decision making processes, the development of targeted
therapies, interventions or in other medical or healthcare related contexts.
More specifically, explainability can uncover a survival model's potential
biases and limitations and provide more mathematically sound ways to understand
how and which features are influential for prediction or constitute risk
factors. However, the lack of readily available IML methods may have deterred
medical practitioners and policy makers in public health from leveraging the
full potential of machine learning for predicting time-to-event data. We
present a comprehensive review of the limited existing amount of work on IML
methods for survival analysis within the context of the general IML taxonomy.
In addition, we formally detail how commonly used IML methods, such as such as
individual conditional expectation (ICE), partial dependence plots (PDP),
accumulated local effects (ALE), different feature importance measures or
Friedman's H-interaction statistics can be adapted to survival outcomes. An
application of several IML methods to real data on data on under-5 year
mortality of Ghanaian children from the Demographic and Health Surveys (DHS)
Program serves as a tutorial or guide for researchers, on how to utilize the
techniques in practice to facilitate understanding of model decisions or
predictions.
\\ ( https://arxiv.org/abs/2403.10250 ,  1272kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10266 (*cross-listing*)
Date: Fri, 15 Mar 2024 12:53:50 GMT   (2310kb,D)

Title: DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers
Authors: Xuanlei Zhao, Shenggan Cheng, Zangwei Zheng, Zheming Yang, Ziming Liu,
  Yang You
Categories: cs.DC cs.LG
\\
  Scaling large models with long sequences across applications like language
generation, video generation and multimodal tasks requires efficient sequence
parallelism. However, existing sequence parallelism methods all assume a single
sequence dimension and fail to adapt to multi-dimensional transformer
architectures that perform attention calculations across different dimensions.
This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to
enable efficient sequence parallelism for multi-dimensional transformer models.
The key idea is to dynamically switch the parallelism dimension according to
the current computation stage, leveraging the potential characteristics of
multi-dimensional attention. This dynamic dimension switching allows sequence
parallelism with minimal communication overhead compared to applying
traditional single-dimension parallelism to multi-dimensional models.
Experiments show DSP improves end-to-end throughput by 42.0% to 216.8% over
prior sequence parallelism methods.
\\ ( https://arxiv.org/abs/2403.10266 ,  2310kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10332 (*cross-listing*)
Date: Fri, 15 Mar 2024 14:19:09 GMT   (2149kb,D)

Title: GreedyML: A Parallel Algorithm for Maximizing Submodular Functions
Authors: Shivaram Gopal, S M Ferdous, Hemanta K. Maji and Alex Pothen
Categories: cs.DC cs.DS cs.LG
Comments: 22 pages, 7 figures
\\
  We describe a parallel approximation algorithm for maximizing monotone
submodular functions subject to hereditary constraints on distributed memory
multiprocessors. Our work is motivated by the need to solve submodular
optimization problems on massive data sets, for practical applications in areas
such as data summarization, machine learning, and graph sparsification. Our
work builds on the randomized distributed RandGreedI algorithm, proposed by
Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed
solution by randomly partitioning the data among all the processors and then
employing a single accumulation step in which all processors send their partial
solutions to one processor. However, for large problems, the accumulation step
could exceed the memory available on a processor, and the processor which
performs the accumulation could become a computational bottleneck.
  Here, we propose a generalization of the RandGreedI algorithm that employs
multiple accumulation steps to reduce the memory required. We analyze the
approximation ratio and the time complexity of the algorithm (in the BSP
model). We also evaluate the new GreedyML algorithm on three classes of
problems, and report results from massive data sets with millions of elements.
The results show that the GreedyML algorithm can solve problems where the
sequential Greedy and distributed RandGreedI algorithms fail due to memory
constraints. For certain computationally intensive problems, the GreedyML
algorithm can be faster than the RandGreedI algorithm. The observed
approximation quality of the solutions computed by the GreedyML algorithm
closely matches those obtained by the RandGreedI algorithm on these problems.
\\ ( https://arxiv.org/abs/2403.10332 ,  2149kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10348 (*cross-listing*)
Date: Fri, 15 Mar 2024 14:34:34 GMT   (6282kb,D)

Title: Denoising Task Difficulty-based Curriculum for Training Diffusion Models
Authors: Jin-Young Kim, Hyojun Go, Soonwoo Kwon, Hyun-Gyoon Kim
Categories: cs.CV cs.LG
Comments: 22 pages, 8 figures, 5 tables
\\
  Diffusion-based generative models have emerged as powerful tools in the realm
of generative modeling. Despite extensive research on denoising across various
timesteps and noise levels, a conflict persists regarding the relative
difficulties of the denoising tasks. While various studies argue that lower
timesteps present more challenging tasks, others contend that higher timesteps
are more difficult. To address this conflict, our study undertakes a
comprehensive examination of task difficulties, focusing on convergence
behavior and changes in relative entropy between consecutive probability
distributions across timesteps. Our observational study reveals that denoising
at earlier timesteps poses challenges characterized by slower convergence and
higher relative entropy, indicating increased task difficulty at these lower
timesteps. Building on these observations, we introduce an easy-to-hard
learning scheme, drawing from curriculum learning, to enhance the training
process of diffusion models. By organizing timesteps or noise levels into
clusters and training models with descending orders of difficulty, we
facilitate an order-aware training regime, progressing from easier to harder
denoising tasks, thereby deviating from the conventional approach of training
diffusion models simultaneously across all timesteps. Our approach leads to
improved performance and faster convergence by leveraging the benefits of
curriculum learning, while maintaining orthogonality with existing improvements
in diffusion training techniques. We validate these advantages through
comprehensive experiments in image generation tasks, including unconditional,
class-conditional, and text-to-image generation.
\\ ( https://arxiv.org/abs/2403.10348 ,  6282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10368 (*cross-listing*)
Date: Fri, 15 Mar 2024 14:59:24 GMT   (3227kb,D)

Title: Conformal Predictions for Probabilistically Robust Scalable Machine
  Learning Classification
Authors: Alberto Carlevaro, Teodoro Alamo Cantarero, Fabrizio Dabbene, Maurizio
  Mongelli
Categories: stat.ML cs.CR cs.LG
Comments: 19 pages, 6 figures, journal paper
MSC-class: 68T37 (Primary)
ACM-class: I.5.3
\\
  Conformal predictions make it possible to define reliable and robust learning
algorithms. But they are essentially a method for evaluating whether an
algorithm is good enough to be used in practice. To define a reliable learning
framework for classification from the very beginning of its design, the concept
of scalable classifier was introduced to generalize the concept of classical
classifier by linking it to statistical order theory and probabilistic learning
theory. In this paper, we analyze the similarities between scalable classifiers
and conformal predictions by introducing a new definition of a score function
and defining a special set of input variables, the conformal safety set, which
can identify patterns in the input space that satisfy the error coverage
guarantee, i.e., that the probability of observing the wrong (possibly unsafe)
label for points belonging to this set is bounded by a predefined $\varepsilon$
error level. We demonstrate the practical implications of this framework
through an application in cybersecurity for identifying DNS tunneling attacks.
Our work contributes to the development of probabilistically robust and
reliable machine learning models.
\\ ( https://arxiv.org/abs/2403.10368 ,  3227kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10395 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:27:58 GMT   (35232kb,D)

Title: Isotropic3D: Image-to-3D Generation Based on a Single CLIP Embedding
Authors: Pengkun Liu, Yikai Wang, Fuchun Sun, Jiafang Li, Hang Xiao, Hongxiang
  Xue, Xinzhou Wang
Categories: cs.CV cs.LG
Comments: Project page: https://isotropic3d.github.io/ Source code:
  https://github.com/pkunliu/Isotropic3D
\\
  Encouraged by the growing availability of pre-trained 2D diffusion models,
image-to-3D generation by leveraging Score Distillation Sampling (SDS) is
making remarkable progress. Most existing methods combine novel-view lifting
from 2D diffusion models which usually take the reference image as a condition
while applying hard L2 image supervision at the reference view. Yet heavily
adhering to the image is prone to corrupting the inductive knowledge of the 2D
diffusion model leading to flat or distorted 3D generation frequently. In this
work, we reexamine image-to-3D in a novel perspective and present Isotropic3D,
an image-to-3D generation pipeline that takes only an image CLIP embedding as
input. Isotropic3D allows the optimization to be isotropic w.r.t. the azimuth
angle by solely resting on the SDS loss. The core of our framework lies in a
two-stage diffusion model fine-tuning. Firstly, we fine-tune a text-to-3D
diffusion model by substituting its text encoder with an image encoder, by
which the model preliminarily acquires image-to-image capabilities. Secondly,
we perform fine-tuning using our Explicit Multi-view Attention (EMA) which
combines noisy multi-view images with the noise-free reference image as an
explicit condition. CLIP embedding is sent to the diffusion model throughout
the whole process while reference images are discarded once after fine-tuning.
As a result, with a single image CLIP embedding, Isotropic3D is capable of
generating multi-view mutually consistent images and also a 3D model with more
symmetrical and neat content, well-proportioned geometry, rich colored texture,
and less distortion compared with existing image-to-3D methods while still
preserving the similarity to the reference image to a large extent. The project
page is available at https://isotropic3d.github.io/. The code and models are
available at https://github.com/pkunliu/Isotropic3D.
\\ ( https://arxiv.org/abs/2403.10395 ,  35232kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10408 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:43:02 GMT   (499kb,D)

Title: SocialGenPod: Privacy-Friendly Generative AI Social Web Applications
  with Decentralised Personal Data Stores
Authors: Vidminas Vizgirda (1), Rui Zhao (2), and Naman Goel (2) ((1)
  University of Edinburgh, (2) University of Oxford)
Categories: cs.CR cs.CY cs.IR cs.LG cs.SI
Comments: Demo paper accepted in Companion Proceedings of the ACM Web
  Conference 2024
ACM-class: H.3.4; H.3.5; C.2.4; I.2.1; K.8.1
DOI: 10.1145/3589335.3651251
\\
  We present SocialGenPod, a decentralised and privacy-friendly way of
deploying generative AI Web applications. Unlike centralised Web and data
architectures that keep user data tied to application and service providers, we
show how one can use Solid -- a decentralised Web specification -- to decouple
user data from generative AI applications. We demonstrate SocialGenPod using a
prototype that allows users to converse with different Large Language Models,
optionally leveraging Retrieval Augmented Generation to generate answers
grounded in private documents stored in any Solid Pod that the user is allowed
to access, directly or indirectly. SocialGenPod makes use of Solid access
control mechanisms to give users full control of determining who has access to
data stored in their Pods. SocialGenPod keeps all user data (chat history, app
configuration, personal documents, etc) securely in the user's personal Pod;
separate from specific model or application providers. Besides better privacy
controls, this approach also enables portability across different services and
applications. Finally, we discuss challenges, posed by the large compute
requirements of state-of-the-art models, that future research in this area
should address. Our prototype is open-source and available at:
https://github.com/Vidminas/socialgenpod/.
\\ ( https://arxiv.org/abs/2403.10408 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10423 (*cross-listing*)
Date: Fri, 15 Mar 2024 15:58:20 GMT   (2937kb)

Title: Quantization Avoids Saddle Points in Distributed Optimization
Authors: Yanan Bo and Yongqiang Wang
Categories: math.OC cs.LG
Comments: Accepted as a Research Article to Proceedings of the National Academy
  of Sciences (PNAS)
\\
  Distributed nonconvex optimization underpins key functionalities of numerous
distributed systems, ranging from power systems, smart buildings, cooperative
robots, vehicle networks to sensor networks. Recently, it has also merged as a
promising solution to handle the enormous growth in data and model sizes in
deep learning. A fundamental problem in distributed nonconvex optimization is
avoiding convergence to saddle points, which significantly degrade optimization
accuracy. We discover that the process of quantization, which is necessary for
all digital communications, can be exploited to enable saddle-point avoidance.
More specifically, we propose a stochastic quantization scheme and prove that
it can effectively escape saddle points and ensure convergence to a
second-order stationary point in distributed nonconvex optimization. With an
easily adjustable quantization granularity, the approach allows a user to
control the number of bits sent per iteration and, hence, to aggressively
reduce the communication overhead. Numerical experimental results using
distributed optimization and learning problems on benchmark datasets confirm
the effectiveness of the approach.
\\ ( https://arxiv.org/abs/2403.10423 ,  2937kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10476 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:07:39 GMT   (12537kb,D)

Title: Approximate Nullspace Augmented Finetuning for Robust Vision
  Transformers
Authors: Haoyang Liu, Aditya Singh, Yijiang Li, Haohan Wang
Categories: cs.CV cs.LG
Comments: 21 pages, 8 figures
\\
  Enhancing the robustness of deep learning models, particularly in the realm
of vision transformers (ViTs), is crucial for their real-world deployment. In
this work, we provide a finetuning approach to enhance the robustness of vision
transformers inspired by the concept of nullspace from linear algebra. Our
investigation centers on whether a vision transformer can exhibit resilience to
input variations akin to the nullspace property in linear mappings, implying
that perturbations sampled from this nullspace do not influence the model's
output when added to the input. Firstly, we show that for many pretrained ViTs,
a non-trivial nullspace exists due to the presence of the patch embedding
layer. Secondly, as nullspace is a concept associated with linear algebra, we
demonstrate that it is possible to synthesize approximate nullspace elements
for the non-linear blocks of ViTs employing an optimisation strategy. Finally,
we propose a fine-tuning strategy for ViTs wherein we augment the training data
with synthesized approximate nullspace noise. After finetuning, we find that
the model demonstrates robustness to adversarial and natural image perbutations
alike.
\\ ( https://arxiv.org/abs/2403.10476 ,  12537kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10488 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:23:38 GMT   (627kb,D)

Title: Joint Multimodal Transformer for Dimensional Emotional Recognition in
  the Wild
Authors: Paul Waligora, Osama Zeeshan, Haseeb Aslam, Soufiane Belharbi,
  Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger
Categories: cs.CV cs.LG cs.SD eess.AS
Comments: 5 pages, 1 figure
\\
  Audiovisual emotion recognition (ER) in videos has immense potential over
unimodal performance. It effectively leverages the inter- and intra-modal
dependencies between visual and auditory modalities. This work proposes a novel
audio-visual emotion recognition system utilizing a joint multimodal
transformer architecture with key-based cross-attention. This framework aims to
exploit the complementary nature of audio and visual cues (facial expressions
and vocal patterns) in videos, leading to superior performance compared to
solely relying on a single modality. The proposed model leverages separate
backbones for capturing intra-modal temporal dependencies within each modality
(audio and visual). Subsequently, a joint multimodal transformer architecture
integrates the individual modality embeddings, enabling the model to
effectively capture inter-modal (between audio and visual) and intra-modal
(within each modality) relationships. Extensive evaluations on the challenging
Affwild2 dataset demonstrate that the proposed model significantly outperforms
baseline and state-of-the-art methods in ER tasks.
\\ ( https://arxiv.org/abs/2403.10488 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10497 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:32:02 GMT   (1008kb)

Title: Data-Driven Distributionally Robust Safety Verification Using Barrier
  Certificates and Conditional Mean Embeddings
Authors: Oliver Sch\"on, Zhengang Zhong, and Sadegh Soudjani
Categories: eess.SY cs.LG cs.SY
Comments: 7 pages, 2 figures, accepted to American Control Conference (ACC)
  2024
\\
  Algorithmic verification of realistic systems to satisfy safety and other
temporal requirements has suffered from poor scalability of the employed formal
approaches. To design systems with rigorous guarantees, many approaches still
rely on exact models of the underlying systems. Since this assumption can
rarely be met in practice, models have to be inferred from measurement data or
are bypassed completely. Whilst former usually requires the model structure to
be known a-priori and immense amounts of data to be available, latter gives
rise to a plethora of restrictive mathematical assumptions about the unknown
dynamics. In a pursuit of developing scalable formal verification algorithms
without shifting the problem to unrealistic assumptions, we employ the concept
of barrier certificates, which can guarantee safety of the system, and learn
the certificate directly from a compact set of system trajectories. We use
conditional mean embeddings to embed data from the system into a reproducing
kernel Hilbert space (RKHS) and construct an RKHS ambiguity set that can be
inflated to robustify the result w.r.t. a set of plausible transition kernels.
We show how to solve the resulting program efficiently using sum-of-squares
optimization and a Gaussian process envelope. Our approach lifts the need for
restrictive assumptions on the system dynamics and uncertainty, and suggests an
improvement in the sample complexity of verifying the safety of a system on a
tested case study compared to a state-of-the-art approach.
\\ ( https://arxiv.org/abs/2403.10497 ,  1008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10520 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:59:44 GMT   (48896kb,D)

Title: Strong and Controllable Blind Image Decomposition
Authors: Zeyu Zhang, Junlin Han, Chenhui Gou, Hongdong Li, Liang Zheng
Categories: cs.CV cs.LG eess.IV
Comments: Code: https://github.com/Zhangzeyu97/CBD.git
\\
  Blind image decomposition aims to decompose all components present in an
image, typically used to restore a multi-degraded input image. While fully
recovering the clean image is appealing, in some scenarios, users might want to
retain certain degradations, such as watermarks, for copyright protection. To
address this need, we add controllability to the blind image decomposition
process, allowing users to enter which types of degradation to remove or
retain. We design an architecture named controllable blind image decomposition
network. Inserted in the middle of U-Net structure, our method first decomposes
the input feature maps and then recombines them according to user instructions.
Advantageously, this functionality is implemented at minimal computational
cost: decomposition and recombination are all parameter-free. Experimentally,
our system excels in blind image decomposition tasks and can outputs partially
or fully restored images that well reflect user intentions. Furthermore, we
evaluate and configure different options for the network structure and loss
functions. This, combined with the proposed decomposition-and-recombination
method, yields an efficient and competitive system for blind image
decomposition, compared with current state-of-the-art methods.
\\ ( https://arxiv.org/abs/2403.10520 ,  48896kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2304.09572
replaced with revised version Fri, 15 Mar 2024 15:20:52 GMT   (79kb,D)

Title: An Ecosystem for Personal Knowledge Graphs: A Survey and Research
  Roadmap
Authors: Martin G. Skj{\ae}veland and Krisztian Balog and Nolwenn Bernard and
  Weronika {\L}ajewska and Trond Linjordet
Categories: cs.AI cs.IR
Comments: Published in AI Open, 2024
Journal-ref: An Ecosystem for Personal Knowledge Graphs: A Survey and Research
  Roadmap, M. G. Skj{\ae}veland, K. Balog, N. Bernard, W. {\L}ajewska, and T.
  Linjordet. In: AI Open, 5:55-69, 2024
DOI: 10.1016/j.aiopen.2024.01.003
\\ ( https://arxiv.org/abs/2304.09572 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02427
replaced with revised version Fri, 15 Mar 2024 15:44:11 GMT   (1968kb,D)

Title: Cognitive Architectures for Language Agents
Authors: Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, Thomas L.
  Griffiths
Categories: cs.AI cs.CL cs.LG cs.SC
Comments: v3 is TMLR camera ready version. 19 pages of main content, 5 figures.
  The first two authors contributed equally, order decided by coin flip. A
  CoALA-based repo of recent work on language agents:
  https://github.com/ysymyth/awesome-language-agents
\\ ( https://arxiv.org/abs/2309.02427 ,  1968kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06799
replaced with revised version Fri, 15 Mar 2024 02:37:52 GMT   (20603kb,D)

Title: When Geoscience Meets Foundation Models: Towards General Geoscience
  Artificial Intelligence System
Authors: Hao Zhang and Jin-Jian Xu and Hong-Wei Cui and Lin Li and Yaowen Yang
  and Chao-Sheng Tang and Niklas Boers
Categories: cs.AI physics.geo-ph
Comments: the manuscript is under re-writing
\\ ( https://arxiv.org/abs/2309.06799 ,  20603kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10112
replaced with revised version Fri, 15 Mar 2024 15:38:07 GMT   (2537kb,D)

Title: zrLLM: Zero-Shot Relational Learning on Temporal Knowledge Graphs with
  Large Language Models
Authors: Zifeng Ding, Heling Cai, Jingpei Wu, Yunpu Ma, Ruotong Liao, Bo Xiong,
  Volker Tresp
Categories: cs.AI cs.CL cs.LG
Comments: Accepted to NAACL 2024 main conference
\\ ( https://arxiv.org/abs/2311.10112 ,  2537kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03388
replaced with revised version Fri, 15 Mar 2024 08:16:14 GMT   (4016kb,D)

Title: Delivery Optimized Discovery in Behavioral User Segmentation under
  Budget Constraint
Authors: Harshita Chopra, Atanu R. Sinha, Sunav Choudhary, Ryan A. Rossi,
  Paavan Kumar Indela, Veda Pranav Parwatala, Srinjayee Paul, Aurghya Maiti
Categories: cs.AI cs.IR cs.LG
DOI: 10.1145/3583780.3614839
\\ ( https://arxiv.org/abs/2402.03388 ,  4016kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05359
replaced with revised version Thu, 14 Mar 2024 21:12:42 GMT   (585kb,D)

Title: Prompting Large Language Models with Divide-and-Conquer Program for
  Discerning Problem Solving
Authors: Yizhou Zhang, Lun Du, Defu Cao, Qiang Fu, Yan Liu
Categories: cs.AI cs.CL cs.LG
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.05359 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10877
replaced with revised version Thu, 14 Mar 2024 21:30:42 GMT   (2636kb,D)

Title: Robust agents learn causal world models
Authors: Jonathan Richens, Tom Everitt
Categories: cs.AI cs.LG
Comments: ICLR 2024 (oral). Proofs in appendix simplified
\\ ( https://arxiv.org/abs/2402.10877 ,  2636kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03359
replaced with revised version Fri, 15 Mar 2024 10:32:45 GMT   (20779kb,D)

Title: RACE-SM: Reinforcement Learning Based Autonomous Control for Social
  On-Ramp Merging
Authors: Jordan Poots
Categories: cs.AI cs.LG cs.RO
Comments: Updated explanation of TTC, page 7
\\ ( https://arxiv.org/abs/2403.03359 ,  20779kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04293
replaced with revised version Fri, 15 Mar 2024 03:57:44 GMT   (5088kb,D)

Title: MKF-ADS: Multi-Knowledge Fusion Based Self-supervised Anomaly Detection
  System for Control Area Network
Authors: Pengzhou Cheng, Zongru Wu, and Gongshen Liu
Categories: cs.AI cs.CR
Comments: 14 figures, 5 tables
\\ ( https://arxiv.org/abs/2403.04293 ,  5088kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05407
replaced with revised version Fri, 15 Mar 2024 14:35:35 GMT   (1124kb,D)

Title: Algorithmic Identification of Essential Exogenous Nodes for Causal
  Sufficiency in Brain Networks
Authors: Abdolmahdi Bagheri, Mahdi Dehshiri, Babak Nadjar Araabi, Alireza
  Akhondi Asl
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.05407 ,  1124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07566
replaced with revised version Fri, 15 Mar 2024 09:48:34 GMT   (300kb,D)

Title: An Improved Strategy for Blood Glucose Control Using Multi-Step Deep
  Reinforcement Learning
Authors: Weiwei Gu and Senquan Wang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.07566 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07769
replaced with revised version Fri, 15 Mar 2024 11:44:51 GMT   (556kb)

Title: Transforming Competition into Collaboration: The Revolutionary Role of
  Multi-Agent Systems and Language Models in Modern Organizations
Authors: Carlos Jose Xavier Cruz
Categories: cs.AI cs.CL cs.CY cs.MA
\\ ( https://arxiv.org/abs/2403.07769 ,  556kb)
------------------------------------------------------------------------------
\\
arXiv:2109.10952
replaced with revised version Thu, 14 Mar 2024 18:23:47 GMT   (6488kb,D)

Title: Cross-linguistically Consistent Semantic and Syntactic Annotation of
  Child-directed Speech
Authors: Ida Szubert, Omri Abend, Nathan Schneider, Samuel Gibbon, Louis Mahon,
  Sharon Goldwater and Mark Steedman
Categories: cs.CL
\\ ( https://arxiv.org/abs/2109.10952 ,  6488kb)
------------------------------------------------------------------------------
\\
arXiv:2204.09601
replaced with revised version Fri, 15 Mar 2024 17:59:17 GMT   (728kb)

Title: Extraction of Sleep Information from Clinical Notes of Patients with
  Alzheimer's Disease Using Natural Language Processing
Authors: Sonish Sivarajkumar, Thomas Yu CHow Tam, Haneef Ahamed Mohammad,
  Samual Viggiano, David Oniani, Shyam Visweswaran, Yanshan Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2204.09601 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2208.14602
replaced with revised version Fri, 15 Mar 2024 01:53:58 GMT   (0kb,I)

Title: Continuous QA Learning with Structured Prompts
Authors: Yinhe Zheng
Categories: cs.CL cs.AI cs.LG
Comments: Duplicate of arXiv:2305.06555 (Please cite arXiv:2305.06555 since it
  is the camera ready version)
\\ ( https://arxiv.org/abs/2208.14602 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2212.12192
replaced with revised version Fri, 15 Mar 2024 16:03:46 GMT   (103kb,D)

Title: CinPatent: Datasets for Patent Classification
Authors: Minh-Tien Nguyen, Nhung Bui, Manh Tran-Tien, Linh Le, and Huy-The Vu
Categories: cs.CL
Comments: This paper describes an on-going work
\\ ( https://arxiv.org/abs/2212.12192 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:2303.13466
replaced with revised version Fri, 15 Mar 2024 17:55:52 GMT   (487kb)

Title: Mining Clinical Notes for Physical Rehabilitation Exercise Information:
  Natural Language Processing Algorithm Development and Validation Study
Authors: Sonish Sivarajkumar, Fengyi Gao, Parker E. Denny, Bayan M. Aldhahwani,
  Shyam Visweswaran, Allyn Bove, Yanshan Wang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2303.13466 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17876
replaced with revised version Fri, 15 Mar 2024 12:01:25 GMT   (4331kb,D)

Title: WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset
Authors: Tiago Ribeiro, Stephanie Brandl, Anders S{\o}gaard, Nora Hollenstein
Categories: cs.CL
\\ ( https://arxiv.org/abs/2303.17876 ,  4331kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11657
replaced with revised version Fri, 15 Mar 2024 10:28:13 GMT   (1939kb,D)

Title: Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in
  Large Language Models
Authors: Jiashuo Sun and Yi Luo and Yeyun Gong and Chen Lin and Yelong Shen and
  Jian Guo and Nan Duan
Categories: cs.CL cs.AI
Comments: Accepted by NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2304.11657 ,  1939kb)
------------------------------------------------------------------------------
\\
arXiv:2308.03449
replaced with revised version Fri, 15 Mar 2024 04:51:19 GMT   (15442kb,D)

Title: Accurate Retraining-free Pruning for Pretrained Encoder-based Language
  Models
Authors: Seungcheol Park, Hojun Choi, U Kang
Categories: cs.CL
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2308.03449 ,  15442kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07317
replaced with revised version Thu, 14 Mar 2024 20:56:23 GMT   (1261kb,D)

Title: Platypus: Quick, Cheap, and Powerful Refinement of LLMs
Authors: Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz
Categories: cs.CL
Comments: Workshop on Instruction Tuning and Instruction Following at NeurIPS
  2023
\\ ( https://arxiv.org/abs/2308.07317 ,  1261kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04849
replaced with revised version Thu, 14 Mar 2024 21:46:37 GMT   (457kb,D)

Title: Speech Emotion Recognition with Distilled Prosodic and Linguistic Affect
  Representations
Authors: Debaditya Shome, Ali Etemad
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at ICASSP 2024
\\ ( https://arxiv.org/abs/2309.04849 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11063
replaced with revised version Thu, 14 Mar 2024 22:23:14 GMT   (6641kb,D)

Title: XATU: A Fine-grained Instruction-based Benchmark for Explainable Text
  Updates
Authors: Haopeng Zhang, Hayate Iso, Sairam Gurajada, Nikita Bhutani
Categories: cs.CL
Comments: LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.11063 ,  6641kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00898
replaced with revised version Thu, 14 Mar 2024 19:27:23 GMT   (294kb,D)

Title: Enabling Language Models to Implicitly Learn Self-Improvement
Authors: Ziqi Wang, Le Hou, Tianjian Lu, Yuexin Wu, Yunxuan Li, Hongkun Yu,
  Heng Ji
Categories: cs.CL
Comments: 28 pages, 5 figures, 4 tables
\\ ( https://arxiv.org/abs/2310.00898 ,  294kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05502
replaced with revised version Fri, 15 Mar 2024 02:30:28 GMT   (2976kb,D)

Title: XAL: EXplainable Active Learning Makes Classifiers Better Low-resource
  Learners
Authors: Yun Luo and Zhen Yang and Fandong Meng and Yingjie Li and Fang Guo and
  Qinglin Qi and Jie Zhou and Yue Zhang
Categories: cs.CL
Comments: Accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2310.05502 ,  2976kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13011
replaced with revised version Thu, 14 Mar 2024 18:07:10 GMT   (1484kb,D)

Title: Compositional preference models for aligning LMs
Authors: Dongyoung Go, Tomasz Korbak, Germ\'an Kruszewski, Jos Rozen, Marc
  Dymetman
Categories: cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.13011 ,  1484kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18913
replaced with revised version Fri, 15 Mar 2024 16:39:26 GMT   (5293kb,D)

Title: Debiasing Algorithm through Model Adaptation
Authors: Tomasz Limisiewicz and David Mare\v{c}ek and Tom\'a\v{s} Musil
Categories: cs.CL cs.AI stat.ML
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2310.18913 ,  5293kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19791
replaced with revised version Fri, 15 Mar 2024 16:55:47 GMT   (3056kb,D)

Title: LILO: Learning Interpretable Libraries by Compressing and Documenting
  Code
Authors: Gabriel Grand, Lionel Wong, Maddy Bowers, Theo X. Olausson, Muxin Liu,
  Joshua B. Tenenbaum, Jacob Andreas
Categories: cs.CL cs.AI cs.LG cs.PL
Comments: ICLR 2024 camera-ready
\\ ( https://arxiv.org/abs/2310.19791 ,  3056kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07445
replaced with revised version Fri, 15 Mar 2024 08:30:30 GMT   (1243kb,D)

Title: Think Before You Speak: Cultivating Communication Skills of Large
  Language Models via Inner Monologue
Authors: Junkai Zhou, Liang Pang, Huawei Shen, Xueqi Cheng
Categories: cs.CL cs.AI
Comments: Accepted by NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2311.07445 ,  1243kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07593
replaced with revised version Fri, 15 Mar 2024 08:58:05 GMT   (23384kb,D)

Title: Follow-Up Differential Descriptions: Language Models Resolve Ambiguities
  for Image Classification
Authors: Reza Esfandiarpoor, Stephen H. Bach
Categories: cs.CL cs.CV cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2311.07593 ,  23384kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09022
replaced with revised version Fri, 15 Mar 2024 10:00:04 GMT   (201kb,D)

Title: Exploring the Potential of Large Language Models in Computational
  Argumentation
Authors: Guizhen Chen, Liying Cheng, Luu Anh Tuan, Lidong Bing
Categories: cs.CL
Comments: 20 pages, 3 figures
\\ ( https://arxiv.org/abs/2311.09022 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02436
replaced with revised version Fri, 15 Mar 2024 03:11:44 GMT   (2374kb,D)

Title: MUFFIN: Curating Multi-Faceted Instructions for Improving
  Instruction-Following
Authors: Renze Lou, Kai Zhang, Jian Xie, Yuxuan Sun, Janice Ahn, Hanzi Xu, Yu
  Su, Wenpeng Yin
Categories: cs.CL cs.AI
Comments: ICLR 2024. Data, model, and code are available at:
  https://renzelou.github.io/Muffin/
\\ ( https://arxiv.org/abs/2312.02436 ,  2374kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01989
replaced with revised version Thu, 14 Mar 2024 23:20:33 GMT   (2440kb,D)

Title: Revisiting Zero-Shot Abstractive Summarization in the Era of Large
  Language Models from the Perspective of Position Bias
Authors: Anshuman Chhabra, Hadi Askari, Prasant Mohapatra
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2401.01989 ,  2440kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06443
replaced with revised version Fri, 15 Mar 2024 07:17:10 GMT   (35812kb,D)

Title: BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via
  Graph Representation Pretraining
Authors: Minjun Kim, Seungwoo Song, Youhan Lee, Haneol Jang, Kyungtae Lim
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.06443 ,  35812kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07518
replaced with revised version Fri, 15 Mar 2024 05:10:05 GMT   (3521kb,D)

Title: Survey of Natural Language Processing for Education: Taxonomy,
  Systematic Review, and Future Trends
Authors: Yunshi Lan, Xinyuan Li, Hanyue Du, Xuesong Lu, Ming Gao, Weining Qian,
  Aoying Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.07518 ,  3521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10948
replaced with revised version Fri, 15 Mar 2024 02:02:02 GMT   (1423kb,D)

Title: Zero-shot Explainable Mental Health Analysis on Social Media by
  Incorporating Mental Scales
Authors: Wenyu Li, Yinuo Zhu, Xin Lin, Ming Li, Ziyue Jiang, Ziqian Zeng
Categories: cs.CL cs.AI
Comments: 4 pages,2 figures
\\ ( https://arxiv.org/abs/2402.10948 ,  1423kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15302
replaced with revised version Fri, 15 Mar 2024 17:57:58 GMT   (617kb,D)

Title: How (un)ethical are instruction-centric responses of LLMs? Unveiling the
  vulnerabilities of safety guardrails to harmful queries
Authors: Somnath Banerjee, Sayan Layek, Rima Hazra, Animesh Mukherjee
Categories: cs.CL cs.CR
Comments: Under review.
  {https://huggingface.co/datasets/SoftMINER-Group/TechHazardQA}
\\ ( https://arxiv.org/abs/2402.15302 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16363
replaced with revised version Fri, 15 Mar 2024 01:58:58 GMT   (1642kb,D)

Title: LLM Inference Unveiled: Survey and Roofline Model Insights
Authors: Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao
  Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, Yan Yan, Beidi Chen,
  Guangyu Sun, Kurt Keutzer
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.16363 ,  1642kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01976
replaced with revised version Fri, 15 Mar 2024 13:27:31 GMT   (8174kb,D)

Title: SciAssess: Benchmarking LLM Proficiency in Scientific Literature
  Analysis
Authors: Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin
  Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun
  Wang, Yuqi Yin, Yaqi Li, Linfeng Zhang, Guolin Ke
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.01976 ,  8174kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06412
replaced with revised version Fri, 15 Mar 2024 08:53:31 GMT   (8186kb,D)

Title: CLIcK: A Benchmark Dataset of Cultural and Linguistic Intelligence in
  Korean
Authors: Eunsu Kim, Juyoung Suk, Philhoon Oh, Haneul Yoo, James Thorne, Alice
  Oh
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.06412 ,  8186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06487
replaced with revised version Thu, 14 Mar 2024 23:59:59 GMT   (1550kb,D)

Title: Multilingual Turn-taking Prediction Using Voice Activity Projection
Authors: Koji Inoue, Bing'er Jiang, Erik Ekstedt, Tatsuya Kawahara, Gabriel
  Skantze
Categories: cs.CL cs.SD eess.AS
Comments: This paper has been accepted for presentation at The 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING 2024) and represents the author's version of the work
\\ ( https://arxiv.org/abs/2403.06487 ,  1550kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07378
replaced with revised version Fri, 15 Mar 2024 02:59:10 GMT   (328kb,D)

Title: SVD-LLM: Truncation-aware Singular Value Decomposition for Large
  Language Model Compression
Authors: Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang
Categories: cs.CL cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2403.07378 ,  328kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08281
replaced with revised version Fri, 15 Mar 2024 07:22:31 GMT   (7386kb,D)

Title: Mastering Text, Code and Math Simultaneously via Fusing Highly
  Specialized Language Models
Authors: Ning Ding, Yulin Chen, Ganqu Cui, Xingtai Lv, Ruobing Xie, Bowen Zhou,
  Zhiyuan Liu, Maosong Sun
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.08281 ,  7386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08604
replaced with revised version Fri, 15 Mar 2024 13:23:34 GMT   (10471kb,D)

Title: DevBench: A Comprehensive Benchmark for Software Development
Authors: Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li,
  Shunyu Yao, Chen Qian, Binyuan Hui, Qicheng Zhang, Zhiyin Yu, He Du, Ping
  Yang, Dahua Lin, Chao Peng, Kai Chen
Categories: cs.CL cs.SE
Comments: Our data and code are available at
  https://github.com/open-compass/DevBench
\\ ( https://arxiv.org/abs/2403.08604 ,  10471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09539
replaced with revised version Fri, 15 Mar 2024 02:07:30 GMT   (1651kb,D)

Title: Logits of API-Protected LLMs Leak Proprietary Information
Authors: Matthew Finlayson, Xiang Ren, Swabha Swayamdipta
Categories: cs.CL cs.AI cs.CR cs.LG
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2403.09539 ,  1651kb)
------------------------------------------------------------------------------
\\
arXiv:2106.03725
replaced with revised version Thu, 14 Mar 2024 18:39:42 GMT   (625kb,D)

Title: Stability to Deformations of Manifold Filters and Manifold Neural
  Networks
Authors: Zhiyang Wang, Luana Ruiz, Alejandro Ribeiro
Categories: cs.LG
Comments: 19 pages; 6 figures
\\ ( https://arxiv.org/abs/2106.03725 ,  625kb)
------------------------------------------------------------------------------
\\
arXiv:2106.04379
replaced with revised version Fri, 15 Mar 2024 00:13:09 GMT   (6291kb,D)

Title: Learning Markov State Abstractions for Deep Reinforcement Learning
Authors: Cameron Allen, Neev Parikh, Omer Gottesman, George Konidaris
Categories: cs.LG cs.AI stat.ML
Comments: Fixed typo (see Errata). Code available at
  https://github.com/camall3n/markov-state-abstractions
\\ ( https://arxiv.org/abs/2106.04379 ,  6291kb)
------------------------------------------------------------------------------
\\
arXiv:2210.04442
replaced with revised version Fri, 15 Mar 2024 03:20:47 GMT   (1929kb)

Title: DPAR: Decoupled Graph Neural Networks with Node-Level Differential
  Privacy
Authors: Qiuchen Zhang, Hong kyu Lee, Jing Ma, Jian Lou, Carl Yang, Li Xiong
Categories: cs.LG cs.CR
Comments: Accepted to The 2024 Web Conference
DOI: 10.1145/3589334.3645531
\\ ( https://arxiv.org/abs/2210.04442 ,  1929kb)
------------------------------------------------------------------------------
\\
arXiv:2302.05614
replaced with revised version Fri, 15 Mar 2024 07:29:42 GMT   (6262kb,D)

Title: Cross-domain Random Pre-training with Prototypes for Reinforcement
  Learning
Authors: Xin Liu, Yaran Chen, Haoran Li, Boyu Li and Dongbin Zhao
Categories: cs.LG cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2302.05614 ,  6262kb)
------------------------------------------------------------------------------
\\
arXiv:2305.17297
replaced with revised version Thu, 14 Mar 2024 23:02:53 GMT   (768kb,D)

Title: Double Descent and Overfitting under Noisy Inputs and Distribution Shift
  for Linear Denoisers
Authors: Chinmaya Kausik and Kashvi Srivastava and Rishi Sonthalia
Categories: cs.LG math.ST stat.ML stat.TH
Comments: Complete overhaul of presentation, many new results
\\ ( https://arxiv.org/abs/2305.17297 ,  768kb)
------------------------------------------------------------------------------
\\
arXiv:2306.07261
replaced with revised version Fri, 15 Mar 2024 16:02:26 GMT   (7689kb,D)

Title: Unprocessing Seven Years of Algorithmic Fairness
Authors: Andr\'e F. Cruz, Moritz Hardt
Categories: cs.LG cs.CY
Journal-ref: ICLR 2024
\\ ( https://arxiv.org/abs/2306.07261 ,  7689kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14009
replaced with revised version Thu, 14 Mar 2024 22:54:18 GMT   (1638kb,D)

Title: Boosting Multitask Learning on Graphs through Higher-Order Task
  Affinities
Authors: Dongyue Li, Haotian Ju, Aneesh Sharma, and Hongyang R. Zhang
Categories: cs.LG cs.SI
Comments: 16 pages. Appeared in KDD 2023
DOI: 10.1145/3580305.3599265
\\ ( https://arxiv.org/abs/2306.14009 ,  1638kb)
------------------------------------------------------------------------------
\\
arXiv:2307.07516
replaced with revised version Fri, 15 Mar 2024 15:03:40 GMT   (543kb)

Title: Voting-based Multimodal Automatic Deception Detection
Authors: Lana Touma and Mohammad Al Horani and Manar Tailouni and Anas Dahabiah
  and Khloud Al Jallad
Categories: cs.LG cs.CL cs.CV cs.HC
\\ ( https://arxiv.org/abs/2307.07516 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09552
replaced with revised version Fri, 15 Mar 2024 13:06:41 GMT   (1675kb,D)

Title: Self-Compatibility: Evaluating Causal Discovery without Ground Truth
Authors: Philipp M. Faller, Leena Chennuru Vankadara, Atalanti A. Mastakouri,
  Francesco Locatello, Dominik Janzing
Categories: cs.LG stat.ME stat.ML
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2307.09552 ,  1675kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07061
replaced with revised version Fri, 15 Mar 2024 02:29:24 GMT   (362kb,D)

Title: Machine Unlearning: Solutions and Challenges
Authors: Jie Xu, Zihan Wu, Cong Wang and Xiaohua Jia
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.07061 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13838
replaced with revised version Fri, 15 Mar 2024 15:04:05 GMT   (473kb,D)

Title: Price-Discrimination Game for Distributed Resource Management in
  Federated Learning
Authors: Han Zhang, Halvin Yang and Guopeng Zhang
Categories: cs.LG cs.GT
\\ ( https://arxiv.org/abs/2308.13838 ,  473kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15594
replaced with revised version Thu, 14 Mar 2024 20:47:17 GMT   (483kb)

Title: Learning the greatest common divisor: explaining transformer predictions
Authors: Fran\c{c}ois Charton
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.15594 ,  483kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04284
replaced with revised version Fri, 15 Mar 2024 08:08:52 GMT   (446kb,D)

Title: Viewing the process of generating counterfactuals as a source of
  knowledge: a new approach for explaining classifiers
Authors: Vincent Lemaire, Nathan Le Boudec, Victor Guyomard and Fran\c{c}oise
  Fessant
Categories: cs.LG
Comments: 8 pages
\\ ( https://arxiv.org/abs/2309.04284 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08374
replaced with revised version Thu, 14 Mar 2024 20:15:38 GMT   (2386kb,D)

Title: Understanding the limitations of self-supervised learning for tabular
  anomaly detection
Authors: Kimberly T. Mai, Toby Davies, Lewis D. Griffin
Categories: cs.LG
DOI: 10.1007/s10044-023-01208-1
\\ ( https://arxiv.org/abs/2309.08374 ,  2386kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15551
replaced with revised version Fri, 15 Mar 2024 17:01:24 GMT   (2614kb,D)

Title: DeepRepViz: Identifying Confounders in Deep Learning Model Predictions
Authors: Roshan Prakash Rane, JiHoon Kim, Arjun Umesha, Didem Stark,
  Marc-Andr\'e Schulz, Kerstin Ritter
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2309.15551 ,  2614kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16984
replaced with revised version Thu, 14 Mar 2024 19:28:48 GMT   (6566kb,D)

Title: Consistency Models as a Rich and Efficient Policy Class for
  Reinforcement Learning
Authors: Zihan Ding, Chi Jin
Categories: cs.LG
Comments: published as a paper in ICLR 2024
\\ ( https://arxiv.org/abs/2309.16984 ,  6566kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00535
replaced with revised version Fri, 15 Mar 2024 02:03:21 GMT   (3568kb,D)

Title: JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and
  Attention
Authors: Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, Simon Du
Categories: cs.LG cs.AI cs.CL
Comments: ICLR'24 camera ready. Improve theorem 3 and theorem 4. Polish writing
  and add code link
\\ ( https://arxiv.org/abs/2310.00535 ,  3568kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02025
replaced with revised version Fri, 15 Mar 2024 15:28:11 GMT   (1561kb,D)

Title: DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training
Authors: Aochuan Chen, Yimeng Zhang, Jinghan Jia, James Diffenderfer, Jiancheng
  Liu, Konstantinos Parasyris, Yihua Zhang, Zheng Zhang, Bhavya Kailkhura,
  Sijia Liu
Categories: cs.LG
Comments: Accepted to ICLR'24. Codes are available at
  https://github.com/OPTML-Group/DeepZero
\\ ( https://arxiv.org/abs/2310.02025 ,  1561kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02227
replaced with revised version Fri, 15 Mar 2024 06:00:29 GMT   (16657kb,D)

Title: SNIP: Bridging Mathematical Symbolic and Numeric Realms with Unified
  Pre-training
Authors: Kazem Meidani, Parshin Shojaee, Chandan K. Reddy, Amir Barati Farimani
Categories: cs.LG cs.AI
Comments: ICLR 2024 Spotlight Paper
\\ ( https://arxiv.org/abs/2310.02227 ,  16657kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02970
replaced with revised version Fri, 15 Mar 2024 09:21:33 GMT   (4722kb,D)

Title: Fast, Expressive SE$(n)$ Equivariant Networks through Weight-Sharing in
  Position-Orientation Space
Authors: Erik J Bekkers, Sharvaree Vadgama, Rob D Hesselink, Putri A van der
  Linden, David W Romero
Categories: cs.LG math.GR
Comments: Our code is publicly available at https://github.com/ebekkers/ponita
  . Published at ICLR 2024
\\ ( https://arxiv.org/abs/2310.02970 ,  4722kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07297
replaced with revised version Fri, 15 Mar 2024 03:42:03 GMT   (4486kb,D)

Title: Score Regularized Policy Optimization through Diffusion Behavior
Authors: Huayu Chen, Cheng Lu, Zhengyi Wang, Hang Su, Jun Zhu
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.07297 ,  4486kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07630
replaced with revised version Fri, 15 Mar 2024 17:17:35 GMT   (760kb,D)

Title: Differentiable Euler Characteristic Transforms for Shape Classification
Authors: Ernst Roell, Bastian Rieck
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.07630 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00500
replaced with revised version Fri, 15 Mar 2024 12:05:14 GMT   (14764kb,D)

Title: Intriguing Properties of Data Attribution on Diffusion Models
Authors: Xiaosen Zheng, Tianyu Pang, Chao Du, Jing Jiang, Min Lin
Categories: cs.LG cs.AI cs.CV
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2311.00500 ,  14764kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07604
replaced with revised version Fri, 15 Mar 2024 08:42:39 GMT   (35633kb,D)

Title: Finetuning Text-to-Image Diffusion Models for Fairness
Authors: Xudong Shen, Chao Du, Tianyu Pang, Min Lin, Yongkang Wong, Mohan
  Kankanhalli
Categories: cs.LG cs.AI cs.CV cs.CY
Comments: ICLR 2024 oral presentation
\\ ( https://arxiv.org/abs/2311.07604 ,  35633kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14120
replaced with revised version Fri, 15 Mar 2024 16:13:46 GMT   (1049kb,D)

Title: Weight fluctuations in (deep) linear neural networks and a derivation of
  the inverse-variance flatness relation
Authors: Markus Gross, Arne P. Raulf, Christoph R\"ath
Categories: cs.LG cond-mat.dis-nn cond-mat.stat-mech
Comments: 26 pages, 7 figures
\\ ( https://arxiv.org/abs/2311.14120 ,  1049kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05742
replaced with revised version Fri, 15 Mar 2024 03:17:42 GMT   (2198kb,D)

Title: The Generalization Gap in Offline Reinforcement Learning
Authors: Ishita Mediratta, Qingfei You, Minqi Jiang and Roberta Raileanu
Categories: cs.LG cs.AI
Comments: Published as a conference paper at ICLR 2024; First two authors
  contributed equally
\\ ( https://arxiv.org/abs/2312.05742 ,  2198kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08515
replaced with revised version Fri, 15 Mar 2024 11:00:56 GMT   (9207kb,D)

Title: Simplicial Representation Learning with Neural $k$-Forms
Authors: Kelly Maggs, Celia Hacker, Bastian Rieck
Categories: cs.LG math.AT
Comments: Accepted at ICLR 2024 (https://openreview.net/forum?id=Djw0XhjHZb)
\\ ( https://arxiv.org/abs/2312.08515 ,  9207kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12467
replaced with revised version Fri, 15 Mar 2024 12:57:46 GMT   (29853kb,D)

Title: Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh
  Transformer
Authors: Youn-Yeol Yu, Jeongwhan Choi, Woojin Cho, Kookjin Lee, Nayong Kim,
  Kiseok Chang, Chang-Seung Woo, Ilho Kim, Seok-Woo Lee, Joon-Young Yang,
  Sooyoung Yoon, Noseong Park
Categories: cs.LG cs.AI cs.CE
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2312.12467 ,  29853kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12689
replaced with revised version Fri, 15 Mar 2024 06:51:28 GMT   (7736kb,D)

Title: Energy-based Automated Model Evaluation
Authors: Ru Peng, Heming Zou, Haobo Wang, Yawen Zeng, Zenan Huang, Junbo Zhao
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: ICLR2024 poster paper
\\ ( https://arxiv.org/abs/2401.12689 ,  7736kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01261
replaced with revised version Fri, 15 Mar 2024 14:12:18 GMT   (10716kb,D)

Title: TEDDY: Trimming Edges with Degree-based Discrimination strategY
Authors: Hyunjin Seo, Jihun Yun, Eunho Yang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.01261 ,  10716kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08073
replaced with revised version Fri, 15 Mar 2024 01:18:45 GMT   (511kb,D)

Title: Grounding Data Science Code Generation with Input-Output Specifications
Authors: Yeming Wen, Pengcheng Yin, Kensen Shi, Henryk Michalewski, Swarat
  Chaudhuri, Alex Polozov
Categories: cs.LG cs.PL cs.SE
\\ ( https://arxiv.org/abs/2402.08073 ,  511kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15656
replaced with revised version Fri, 15 Mar 2024 06:47:48 GMT   (1518kb,D)

Title: Learning Semilinear Neural Operators : A Unified Recursive Framework For
  Prediction And Data Assimilation
Authors: Ashutosh Singh, Ricardo Augusto Borsoi, Deniz Erdogmus, Tales Imbiriba
Categories: cs.LG cs.AI cs.NA math.NA
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2402.15656 ,  1518kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01759
replaced with revised version Fri, 15 Mar 2024 02:22:07 GMT   (3406kb,D)

Title: Open-world Machine Learning: A Review and New Outlooks
Authors: Fei Zhu, Shijie Ma, Zhen Cheng, Xu-Yao Zhang, Zhaoxiang Zhang,
  Cheng-Lin Liu
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2403.01759 ,  3406kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02598
replaced with revised version Thu, 14 Mar 2024 22:46:02 GMT   (0kb,I)

Title: Pooling Image Datasets With Multiple Covariate Shift and Imbalance
Authors: Sotirios Panagiotis Chytas, Vishnu Suresh Lokhande, Peiran Li, Vikas
  Singh
Categories: cs.LG cs.CV
Comments: We need to do some fixes of references to make them more precise.
  This paper will be corrected and uploaded again by another group member
\\ ( https://arxiv.org/abs/2403.02598 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03454
replaced with revised version Thu, 14 Mar 2024 20:09:17 GMT   (679kb,D)

Title: Learning Constrained Optimization with Deep Augmented Lagrangian Methods
Authors: James Kotary and Ferdinando Fioretto
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2403.03454 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05713
replaced with revised version Fri, 15 Mar 2024 14:24:40 GMT   (1482kb,D)

Title: tsGT: Stochastic Time Series Modeling With Transformer
Authors: {\L}ukasz Kuci\'nski, Witold Drzewakowski, Mateusz Olko, Piotr
  Kozakowski, {\L}ukasz Maziarka, Marta Emilia Nowakowska, {\L}ukasz Kaiser,
  Piotr Mi{\l}o\'s
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.05713 ,  1482kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08838
replaced with revised version Fri, 15 Mar 2024 06:22:16 GMT   (5631kb,D)

Title: Predictive Clustering of Vessel Behavior Based on Hierarchical
  Trajectory Representation
Authors: Rui Zhang, Hanyue Wu, Zhenzhong Yin, Zhu Xiao, Yong Xiong, and Kezhong
  Liu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.08838 ,  5631kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09303
replaced with revised version Fri, 15 Mar 2024 01:58:19 GMT   (205kb,D)

Title: Rethinking Autoencoders for Medical Anomaly Detection from A Theoretical
  Perspective
Authors: Yu Cai, Hao Chen, Kwang-Ting Cheng
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2403.09303 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2112.07303
replaced with revised version Fri, 15 Mar 2024 14:09:15 GMT   (11423kb,D)

Title: MMO: Meta Multi-Objectivization for Software Configuration Tuning
Authors: Pengzhou Chen and Tao Chen and Miqing Li
Categories: cs.SE cs.AI cs.DC
Comments: 20 figures, 4 tables. journal extension at TSE. arXiv admin note:
  text overlap with arXiv:2106.01331
\\ ( https://arxiv.org/abs/2112.07303 ,  11423kb)
------------------------------------------------------------------------------
\\
arXiv:2207.03341
replaced with revised version Fri, 15 Mar 2024 00:52:40 GMT   (10057kb,D)

Title: Softmax-free Linear Transformers
Authors: Jiachen Lu, Junge Zhang, Xiatian Zhu, Jianfeng Feng, Tao Xiang, Li
  Zhang
Categories: cs.CV cs.AI cs.LG
Comments: Accepted by IJCV. arXiv admin note: substantial text overlap with
  arXiv:2110.11945
DOI: 10.1007/s11263-024-02035-5
\\ ( https://arxiv.org/abs/2207.03341 ,  10057kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07105
replaced with revised version Fri, 15 Mar 2024 08:21:04 GMT   (22021kb,D)

Title: Bridging Implicit and Explicit Geometric Transformation for Single-Image
  View Synthesis
Authors: Byeongjun Park, Hyojun Go, Changick Kim
Categories: cs.CV cs.AI
Comments: TPAMI 2024
\\ ( https://arxiv.org/abs/2209.07105 ,  22021kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10164 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 09:18:20 GMT   (3384kb,D)

Title: Lowering Detection in Sport Climbing Based on Orientation of the Sensor
  Enhanced Quickdraw
Authors: Sadaf Moaveninejad, Andrea Janes, Camillo Porcaro
Categories: eess.SP cs.AI cs.LG cs.RO
Comments: arXiv admin note: substantial text overlap with arXiv:2211.02680
\\ ( https://arxiv.org/abs/2301.10164 ,  3384kb)
------------------------------------------------------------------------------
\\
arXiv:2305.07019
replaced with revised version Thu, 14 Mar 2024 20:30:52 GMT   (1060kb,D)

Title: Musketeer: Joint Training for Multi-task Vision Language Model with Task
  Explanation Prompts
Authors: Zhaoyang Zhang, Yantao Shen, Kunyu Shi, Zhaowei Cai, Jun Fang, Siqi
  Deng, Hao Yang, Davide Modolo, Zhuowen Tu, Stefano Soatto
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2305.07019 ,  1060kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08553
replaced with revised version Fri, 15 Mar 2024 10:53:11 GMT   (2132kb,D)

Title: Distilling Knowledge for Short-to-Long Term Trajectory Prediction
Authors: Sourav Das and Guglielmo Camporese and Shaokang Cheng and Lamberto
  Ballan
Categories: cs.CV cs.AI cs.LG cs.RO
\\ ( https://arxiv.org/abs/2305.08553 ,  2132kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08045
replaced with revised version Fri, 15 Mar 2024 02:29:44 GMT   (12113kb,D)

Title: Traveling Waves Encode the Recent Past and Enhance Sequence Learning
Authors: T. Anderson Keller, Lyle Muller, Terrence Sejnowski, Max Welling
Categories: cs.NE cs.AI cs.LG
\\ ( https://arxiv.org/abs/2309.08045 ,  12113kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14780 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 16:07:43 GMT   (7315kb)

Title: Transferring climate change knowledge
Authors: Francesco Immorlano, Veronika Eyring, Thomas le Monnier de Gouville,
  Gabriele Accarino, Donatello Elia, Giovanni Aloisio and Pierre Gentine
Categories: physics.ao-ph cs.AI cs.LG
\\ ( https://arxiv.org/abs/2309.14780 ,  7315kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04345 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 14:51:10 GMT   (2911kb,D)

Title: Neur2RO: Neural Two-Stage Robust Optimization
Authors: Justin Dumouchelle, Esther Julien, Jannis Kurtz, Elias B. Khalil
Categories: math.OC cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.04345 ,  2911kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06020
replaced with revised version Fri, 15 Mar 2024 13:53:19 GMT   (4150kb,D)

Title: DyST: Towards Dynamic Neural Scene Representations on Real-World Videos
Authors: Maximilian Seitzer, Sjoerd van Steenkiste, Thomas Kipf, Klaus Greff,
  Mehdi S. M. Sajjadi
Categories: cs.CV cs.AI cs.GR cs.LG cs.RO
Comments: ICLR 2024 spotlight. Project website: https://dyst-paper.github.io/
\\ ( https://arxiv.org/abs/2310.06020 ,  4150kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09562
replaced with revised version Thu, 14 Mar 2024 18:18:49 GMT   (12686kb,D)

Title: Does CLIP's Generalization Performance Mainly Stem from High Train-Test
  Similarity?
Authors: Prasanna Mayilvahanan, Thadd\"aus Wiedemer, Evgenia Rusak, Matthias
  Bethge, Wieland Brendel
Categories: cs.CV cs.AI cs.LG
Comments: ICLR 2024 camera-ready version
\\ ( https://arxiv.org/abs/2310.09562 ,  12686kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10329
replaced with revised version Fri, 15 Mar 2024 12:43:41 GMT   (10956kb,D)

Title: High-fidelity Person-centric Subject-to-Image Synthesis
Authors: Yibin Wang and Weizhong Zhang and Jianwei Zheng and Cheng Jin
Categories: cs.CV cs.AI
Comments: Accepted by CVPR2024
\\ ( https://arxiv.org/abs/2311.10329 ,  10956kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17248 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 13:35:01 GMT   (2728kb,D)

Title: Deep Regularized Compound Gaussian Network for Solving Linear Inverse
  Problems
Authors: Carter Lyons, Raghu G. Raj, and Margaret Cheney
Categories: eess.SP cs.AI cs.NA math.NA
Comments: Published IEEE TCI. Main article has 16 pages, 7 figures, 3 tables,
  and 1 algorithm. Supplementary material has 4 pages and 5 figures
Journal-ref: in IEEE Transactions on Computational Imaging, vol. 10, pp.
  399-414, 2024
DOI: 10.1109/TCI.2024.336939
\\ ( https://arxiv.org/abs/2311.17248 ,  2728kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02256
replaced with revised version Thu, 14 Mar 2024 20:49:25 GMT   (18680kb,D)

Title: EMDM: Efficient Motion Diffusion Model for Fast and High-Quality Motion
  Generation
Authors: Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang,
  Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, Lingjie Liu
Categories: cs.CV cs.AI cs.GR
Comments: Project Page: https://frank-zy-dou.github.io/projects/EMDM/index.html
\\ ( https://arxiv.org/abs/2312.02256 ,  18680kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02521
replaced with revised version Fri, 15 Mar 2024 04:37:32 GMT   (48604kb,D)

Title: Retrieving Conditions from Reference Images for Diffusion Models
Authors: Haoran Tang, Xin Zhou, Jieren Deng, Zhihong Pan, Hao Tian, Pratik
  Chaudhari
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.02521 ,  48604kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14125
replaced with revised version Thu, 14 Mar 2024 18:08:11 GMT   (41255kb,D)

Title: VideoPoet: A Large Language Model for Zero-Shot Video Generation
Authors: Dan Kondratyuk and Lijun Yu and Xiuye Gu and Jos\'e Lezama and
  Jonathan Huang and Rachel Hornung and Hartwig Adam and Hassan Akbari and Yair
  Alon and Vighnesh Birodkar and Yong Cheng and Ming-Chang Chiu and Josh Dillon
  and Irfan Essa and Agrim Gupta and Meera Hahn and Anja Hauth and David Hendon
  and Alonso Martinez and David Minnen and David Ross and Grant Schindler and
  Mikhail Sirotenko and Kihyuk Sohn and Krishna Somandepalli and Huisheng Wang
  and Jimmy Yan and Ming-Hsuan Yang and Xuan Yang and Bryan Seybold and Lu
  Jiang
Categories: cs.CV cs.AI
Comments: Project page: http://sites.research.google/videopoet/
\\ ( https://arxiv.org/abs/2312.14125 ,  41255kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00587 (*cross-listing*)
replaced with revised version Thu, 14 Mar 2024 19:02:51 GMT   (502kb,D)

Title: Brain Tumor Segmentation Based on Deep Learning, Attention Mechanisms,
  and Energy-Based Uncertainty Prediction
Authors: Zachary Schwehr and Sriman Achanta
Categories: eess.IV cs.AI
Comments: 11 pages, 6 figures, code available at
  https://github.com/WeToTheMoon/BrainTumorSegmentation, submitted to Computers
  in Biology and Medicine
\\ ( https://arxiv.org/abs/2401.00587 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2401.07348
replaced with revised version Fri, 15 Mar 2024 17:10:29 GMT   (411kb)

Title: Generative AI in EU Law: Liability, Privacy, Intellectual Property, and
  Cybersecurity
Authors: Claudio Novelli, Federico Casolari, Philipp Hacker, Giorgio Spedicato,
  Luciano Floridi
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2401.07348 ,  411kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16352
replaced with revised version Fri, 15 Mar 2024 12:19:09 GMT   (3776kb,D)

Title: Adversarial Training on Purification (AToP): Advancing Both Robustness
  and Generalization
Authors: Guang Lin, Chao Li, Jianhai Zhang, Toshihisa Tanaka, Qibin Zhao
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2401.16352 ,  3776kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02498 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 08:44:40 GMT   (393kb,D)

Title: Fully Differentiable Correlation-driven 2D/3D Registration for X-ray to
  CT Image Fusion
Authors: Minheng Chen, Zhirun Zhang, Shuheng Gu, Zhangyang Ge and Youyong Kong
Categories: eess.IV cs.AI cs.CV
Comments: ISBI 2024
\\ ( https://arxiv.org/abs/2402.02498 ,  393kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15011
replaced with revised version Thu, 14 Mar 2024 23:52:30 GMT   (3940kb,D)

Title: A Conversational Brain-Artificial Intelligence Interface
Authors: Anja Meunier, Michal Robert \v{Z}\'ak, Lucas Munz, Sofiya Garkot,
  Manuel Eder, Jiachen Xu, Moritz Grosse-Wentrup
Categories: cs.HC cs.AI eess.SP
Comments: 16 pages (39 with supplementary meterial), 6 figures
\\ ( https://arxiv.org/abs/2402.15011 ,  3940kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00895
replaced with revised version Fri, 15 Mar 2024 00:40:50 GMT   (591kb,D)

Title: End-to-End Graph-Sequential Representation Learning for Accurate
  Recommendations
Authors: Vladimir Baikalov, Evgeny Frolov
Categories: cs.IR cs.AI cs.LG
Comments: 4 pages, 1 figure, submitted to WWW'24, short-paper track
\\ ( https://arxiv.org/abs/2403.00895 ,  591kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03230 (*cross-listing*)
replaced with revised version Thu, 14 Mar 2024 23:32:15 GMT   (3575kb,D)

Title: Large language models surpass human experts in predicting neuroscience
  results
Authors: Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin K. Nejad, Felipe
  Y\'a\~nez, Bati Yilmaz, Kangjoo Lee, Alexandra O. Cohen, Valentina
  Borghesani, Anton Pashkov, Daniele Marinazzo, Jonathan Nicholas, Alessandro
  Salatiello, Ilia Sucholutsky, Pasquale Minervini, Sepehr Razavi, Roberta
  Rocca, Elkhan Yusifov, Tereza Okalova, Nianlong Gu, Martin Ferianc, Mikail
  Khona, Kaustubh R. Patil, Pui-Shee Lee, Rui Mata, Nicholas E. Myers, Jennifer
  K Bizley, Sebastian Musslick, Isil Poyraz Bilgin, Guiomar Niso, Justin M.
  Ales, Michael Gaebler, N Apurva Ratan Murty, Leyla Loued-Khenissi, Anna
  Behler, Chloe M. Hall, Jessica Dafflon, Sherry Dongqi Bao, Bradley C. Love
Categories: q-bio.NC cs.AI
\\ ( https://arxiv.org/abs/2403.03230 ,  3575kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04473
replaced with revised version Fri, 15 Mar 2024 06:51:30 GMT   (11936kb,D)

Title: TextMonkey: An OCR-Free Large Multimodal Model for Understanding
  Document
Authors: Yuliang Liu, Biao Yang, Qiang Liu, Zhang Li, Zhiyin Ma, Shuo Zhang,
  Xiang Bai
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.04473 ,  11936kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04701
replaced with revised version Fri, 15 Mar 2024 11:43:21 GMT   (45787kb,D)

Title: ObjectCompose: Evaluating Resilience of Vision-Based Models on
  Object-to-Background Compositional Changes
Authors: Hashmat Shadab Malik, Muhammad Huzaifa, Muzammal Naseer, Salman Khan,
  Fahad Shahbaz Khan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.04701 ,  45787kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05050
replaced with revised version Fri, 15 Mar 2024 01:30:13 GMT   (10747kb,D)

Title: DyRoNet: A Low-Rank Adapter Enhanced Dynamic Routing Network for
  Streaming Perception
Authors: Xiang Huang, Zhi-Qi Cheng, Jun-Yan He, Chenyang Li, Wangmeng Xiang,
  Baigui Sun, Xiao Wu
Categories: cs.CV cs.AI cs.MM
\\ ( https://arxiv.org/abs/2403.05050 ,  10747kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05589
replaced with revised version Fri, 15 Mar 2024 13:00:08 GMT   (2387kb,D)

Title: Ergonomic Design of Computer Laboratory Furniture: Mismatch Analysis
  Utilizing Anthropometric Data of University Students
Authors: Anik Kumar Saha, Md Abrar Jahin, Md. Rafiquzzaman, and M. F. Mridha
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2403.05589 ,  2387kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07911
replaced with revised version Thu, 14 Mar 2024 18:37:53 GMT   (2714kb)

Title: Standing on FURM ground -- A framework for evaluating Fair, Useful, and
  Reliable AI Models in healthcare systems
Authors: Alison Callahan, Duncan McElfresh, Juan M. Banda, Gabrielle Bunney,
  Danton Char, Jonathan Chen, Conor K. Corbin, Debadutta Dash, Norman L.
  Downing, Sneha S. Jain, Nikesh Kotecha, Jonathan Masterson, Michelle M.
  Mello, Keith Morse, Srikar Nallan, Abby Pandya, Anurang Revri, Aditya Sharma,
  Christopher Sharp, Rahul Thapa, Michael Wornow, Alaa Youssef, Michael A.
  Pfeffer, Nigam H. Shah
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2403.07911 ,  2714kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08017
replaced with revised version Thu, 14 Mar 2024 18:40:34 GMT   (2965kb,D)

Title: Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI
Authors: Vladimir Zaigrajew, Hubert Baniecki, Lukasz Tulczyjew, Agata M.
  Wijata, Jakub Nalepa, Nicolas Long\'ep\'e, Przemyslaw Biecek
Categories: cs.CV cs.AI
Comments: 14 pages, 9 figures, ICLR 2024 Machine Learning for Remote Sensing
  (ML4RS) Workshop
\\ ( https://arxiv.org/abs/2403.08017 ,  2965kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02992
replaced with revised version Fri, 15 Mar 2024 04:38:21 GMT   (12549kb,D)

Title: Kosmos-G: Generating Images in Context with Multimodal Large Language
  Models
Authors: Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu
  Wei
Categories: cs.CV cs.CL
Comments: Code: https://aka.ms/Kosmos-G Project Page:
  https://xichenpan.github.io/kosmosg
\\ ( https://arxiv.org/abs/2310.02992 ,  12549kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13607
replaced with revised version Fri, 15 Mar 2024 11:19:30 GMT   (2053kb,D)

Title: CODIS: Benchmarking Context-Dependent Visual Comprehension for
  Multimodal Large Language Models
Authors: Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li,
  Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong
  Sun, Yang Liu
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2402.13607 ,  2053kb)
------------------------------------------------------------------------------
\\
arXiv:2402.19119
replaced with revised version Thu, 14 Mar 2024 19:18:43 GMT   (45158kb,D)

Title: VIXEN: Visual Text Comparison Network for Image Difference Captioning
Authors: Alexander Black and Jing Shi and Yifei Fan and Tu Bui and John
  Collomosse
Categories: cs.CV cs.CL
Comments: AAAI 2024
\\ ( https://arxiv.org/abs/2402.19119 ,  45158kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06199
replaced with revised version Fri, 15 Mar 2024 17:48:05 GMT   (10906kb,D)

Title: Mipha: A Comprehensive Overhaul of Multimodal Assistant with Small
  Language Models
Authors: Minjie Zhu, Yichen Zhu, Xin Liu, Ning Liu, Zhiyuan Xu, Chaomin Shen,
  Yaxin Peng, Zhicai Ou, Feifei Feng, Jian Tang
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2403.06199 ,  10906kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07910
replaced with revised version Fri, 15 Mar 2024 09:30:29 GMT   (1807kb,D)

Title: MAGPIE: Multi-Task Media-Bias Analysis Generalization for Pre-Trained
  Identification of Expressions
Authors: Tom\'a\v{s} Horych, Martin Wessel, Jan Philip Wahle, Terry Ruas,
  Jerome Wa{\ss}muth, Andr\'e Greiner-Petter, Akiko Aizawa, Bela Gipp, Timo
  Spinde
Categories: cs.CY cs.CL
\\ ( https://arxiv.org/abs/2403.07910 ,  1807kb)
------------------------------------------------------------------------------
\\
arXiv:2208.10922
replaced with revised version Fri, 15 Mar 2024 08:48:04 GMT   (3166kb,D)

Title: StyleTalker: One-shot Style-based Audio-driven Talking Head Video
  Generation
Authors: Dongchan Min, Minyoung Song, Eunji Ko, Sung Ju Hwang
Categories: cs.CV cs.LG eess.AS eess.IV
\\ ( https://arxiv.org/abs/2208.10922 ,  3166kb)
------------------------------------------------------------------------------
\\
arXiv:2303.00935
replaced with revised version Thu, 14 Mar 2024 20:04:33 GMT   (6903kb,D)

Title: Learning to Detect Slip through Tactile Estimation of the Contact Force
  Field and its Entropy
Authors: Xiaohai Hu, Aparajit Venkatesh, Guiliang Zheng, and Xu Chen
Categories: cs.RO cs.LG
Comments: 8 pages, 7 figures, to be submitted
\\ ( https://arxiv.org/abs/2303.00935 ,  6903kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08752
replaced with revised version Fri, 15 Mar 2024 10:13:42 GMT   (2192kb,D)

Title: A Matter of Annotation: An Empirical Study on In Situ and Self-Recall
  Activity Annotations from Wearable Sensors
Authors: Alexander Hoelzemann, Kristof Van Laerhoven
Categories: cs.HC cs.LG
\\ ( https://arxiv.org/abs/2305.08752 ,  2192kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18702 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 03:48:23 GMT   (2406kb,D)

Title: Adversarial Adaptive Sampling: Unify PINN and Optimal Transport for the
  Approximation of PDEs
Authors: Kejun Tang, Jiayu Zhai, Xiaoliang Wan, Chao Yang
Categories: stat.ML cs.LG cs.NA math.NA
Comments: ICLR, 2024
\\ ( https://arxiv.org/abs/2305.18702 ,  2406kb)
------------------------------------------------------------------------------
\\
arXiv:2306.01776 (*cross-listing*)
replaced with revised version Thu, 14 Mar 2024 22:46:25 GMT   (1827kb,D)

Title: From Zero to Turbulence: Generative Modeling for 3D Flow Simulation
Authors: Marten Lienen, David L\"udke, Jan Hansen-Palmus, Stephan G\"unnemann
Categories: physics.flu-dyn cs.LG
Comments: Published at ICLR 2024
\\ ( https://arxiv.org/abs/2306.01776 ,  1827kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07212 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 01:13:38 GMT   (598kb,D)

Title: Automated ensemble method for pediatric brain tumor segmentation
Authors: Shashidhar Reddy Javaji, Sovesh Mohapatra, Advait Gosai, Gottfried
  Schlaug
Categories: eess.IV cs.CV cs.LG
Comments: Accepted at MICCAI BrainLes Workshop 2023
\\ ( https://arxiv.org/abs/2308.07212 ,  598kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14396
replaced with revised version Fri, 15 Mar 2024 17:03:05 GMT   (10461kb,D)

Title: Guess & Sketch: Language Model Guided Transpilation
Authors: Celine Lee, Abdulrahman Mahmoud, Michal Kurek, Simone Campanoni, David
  Brooks, Stephen Chong, Gu-Yeon Wei, Alexander M. Rush
Categories: cs.SE cs.LG cs.PL
\\ ( https://arxiv.org/abs/2309.14396 ,  10461kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05725 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 15:09:24 GMT   (263kb,D)

Title: Post-hoc Bias Scoring Is Optimal For Fair Classification
Authors: Wenlong Chen, Yegor Klochkov, Yang Liu
Categories: stat.ML cs.LG
Comments: Published at The Twelfth International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2310.05725 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08391 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 02:01:17 GMT   (118kb,D)

Title: How Many Pretraining Tasks Are Needed for In-Context Learning of Linear
  Regression?
Authors: Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu,
  Peter L. Bartlett
Categories: stat.ML cs.LG
Comments: ICLR 2024 Camera Ready
\\ ( https://arxiv.org/abs/2310.08391 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2311.14024
replaced with revised version Fri, 15 Mar 2024 05:27:42 GMT   (4887kb,D)

Title: Creating and Leveraging a Synthetic Dataset of Cloud Optical Thickness
  Measures for Cloud Detection in MSI
Authors: Aleksis Pirinen, Nosheen Abid, Nuria Agues Paszkowsky, Thomas Ohlson
  Timoudas, Ronald Scheirer, Chiara Ceccobello, Gy\"orgy Kov\'acs, Anders
  Persson
Categories: cs.CV cs.LG
Comments: Published in the journal Remote Sensing (2024). Code, data and models
  available at https://github.com/aleksispi/ml-cloud-opt-thick
\\ ( https://arxiv.org/abs/2311.14024 ,  4887kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01490
replaced with revised version Thu, 14 Mar 2024 23:24:46 GMT   (11161kb,D)

Title: GAPS: Geometry-Aware, Physics-Based, Self-Supervised Neural Garment
  Draping
Authors: Ruochen Chen, Liming Chen, Shaifali Parashar
Categories: cs.CV cs.GR cs.LG
\\ ( https://arxiv.org/abs/2312.01490 ,  11161kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07977 (*cross-listing*)
replaced with revised version Thu, 14 Mar 2024 19:26:00 GMT   (44244kb,D)

Title: Modeling non-genetic information dynamics in cells using reservoir
  computing
Authors: Dipesh Niraula (1), Issam El Naqa (1), Jack Adam Tuszynski (2), and
  Robert A. Gatenby (3) ((1) Department of Machine Learning, Moffitt Cancer
  Center, Tampa, FL, USA (2) Departments of Physics and Oncology, University of
  Alberta, Edmonton, AB, CAN (3) Departments of Radiology and Integrated
  Mathematical Oncology, Moffitt Cancer Center, Tampa, FL, USA)
Categories: q-bio.CB cs.LG physics.bio-ph
Comments: Main text: 18 pages, 1 table, and 8 figures; Supplementary materials:
  14 pages, 18 figures; Link to Source code and Data included
\\ ( https://arxiv.org/abs/2312.07977 ,  44244kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01085
replaced with revised version Fri, 15 Mar 2024 05:34:36 GMT   (2183kb,D)

Title: Imperio: Language-Guided Backdoor Attacks for Arbitrary Model Control
Authors: Ka-Ho Chow, Wenqi Wei, Lei Yu
Categories: cs.CR cs.LG
\\ ( https://arxiv.org/abs/2401.01085 ,  2183kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16433
replaced with revised version Thu, 14 Mar 2024 21:08:34 GMT   (18312kb,D)

Title: Within-basket Recommendation via Neural Pattern Associator
Authors: Kai Luo, Tianshu Shen, Lan Yao, Ga Wu, Aaron Liblong, Istvan
  Fehervari, Ruijian An, Jawad Ahmed, Harshit Mishra, Charu Pujari
Categories: cs.IR cs.LG
Comments: 13 pages, 9 figures
\\ ( https://arxiv.org/abs/2401.16433 ,  18312kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00038 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 12:47:51 GMT   (707kb,D)

Title: Detecting Brain Tumors through Multimodal Neural Networks
Authors: Antonio Curci and Andrea Esposito
Categories: eess.IV cs.CV cs.LG q-bio.QM
Comments: Presented at NeroPRAI 2024 (co-located with ICPRAM 2024). This
  version did not undergo peer review: refer to the open access version of
  record (see DOI)
ACM-class: I.5.4
Journal-ref: Proceedings of the 13th International Conference on Pattern
  Recognition Applications and Methods (ICPRAM 2024) - NeroPRAI 2024
DOI: 10.5220/0012608600003654
\\ ( https://arxiv.org/abs/2402.00038 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02340
replaced with revised version Thu, 14 Mar 2024 23:29:47 GMT   (544kb,D)

Title: Learning Semantic Proxies from Visual Prompts for Parameter-Efficient
  Fine-Tuning in Deep Metric Learning
Authors: Li Ren, Chen Chen, Liqiang Wang, Kien Hua
Categories: cs.CV cs.LG
Comments: Published in ICLR 2024
\\ ( https://arxiv.org/abs/2402.02340 ,  544kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03230 (*cross-listing*)
replaced with revised version Thu, 14 Mar 2024 20:11:10 GMT   (4388kb,D)

Title: Architecture Analysis and Benchmarking of 3D U-shaped Deep Learning
  Models for Thoracic Anatomical Segmentation
Authors: Arash Harirpoush, Amirhossein Rasoulian, Marta Kersten-Oertel, and
  Yiming Xiao
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2402.03230 ,  4388kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10242
replaced with revised version Fri, 15 Mar 2024 15:22:50 GMT   (222kb)

Title: Signed Diverse Multiplex Networks: Clustering and Inference
Authors: Marianna Pensky
Categories: cs.SI cs.LG stat.ME
Comments: 8 figures
\\ ( https://arxiv.org/abs/2402.10242 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14315 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 05:33:48 GMT   (3074kb,D)

Title: Structure-Based Drug Design via 3D Molecular Generative Pre-training and
  Sampling
Authors: Yuwei Yang, Siqi Ouyang, Xueyu Hu, Mingyue Zheng, Hao Zhou, Lei Li
Categories: q-bio.BM cs.LG
\\ ( https://arxiv.org/abs/2402.14315 ,  3074kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14982
replaced with revised version Thu, 14 Mar 2024 22:43:13 GMT   (1209kb,D)

Title: Human Brain Exhibits Distinct Patterns When Listening to Fake Versus
  Real Audio: Preliminary Evidence
Authors: Mahsa Salehi, Kalin Stefanov, Ehsan Shareghi
Categories: cs.SD cs.LG eess.AS q-bio.NC
Comments: 9 pages, 4 figures, 3 tables
\\ ( https://arxiv.org/abs/2402.14982 ,  1209kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07013 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 05:46:37 GMT   (673kb,D)

Title: AdaNovo: Adaptive \emph{De Novo} Peptide Sequencing with Conditional
  Mutual Information
Authors: Jun Xia, Shaorong Chen, Jingbo Zhou, Tianze Ling, Wenjie Du, Sizhe
  Liu, Stan Z. Li
Categories: q-bio.QM cs.LG q-bio.BM
\\ ( https://arxiv.org/abs/2403.07013 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07706
replaced with revised version Fri, 15 Mar 2024 15:46:31 GMT   (5390kb,D)

Title: Fast and Simple Explainability for Point Cloud Networks
Authors: Meir Yossef Levi and Guy Gilboa
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.07706 ,  5390kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07925 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 00:21:25 GMT   (16314kb,D)

Title: Physics-informed generative model for drug-like molecule conformers
Authors: David C. Williams and Neil Inala
Categories: q-bio.BM cs.LG physics.chem-ph
Comments: To appear in the Journal of Chemical Information and Modeling
DOI: 10.1021/acs.jcim.3c01816
\\ ( https://arxiv.org/abs/2403.07925 ,  16314kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09429 (*cross-listing*)
replaced with revised version Fri, 15 Mar 2024 09:03:41 GMT   (816kb,D)

Title: VISA: Variational Inference with Sequential Sample-Average
  Approximations
Authors: Heiko Zimmermann, Christian A. Naesseth, Jan-Willem van de Meent
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2403.09429 ,  816kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
