Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年4月8日 11:13
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu  4 Apr 24 18:00:00 GMT  to  Fri  5 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.03893
Date: Fri, 5 Apr 2024 05:02:12 GMT   (570kb,D)

Title: KGExplainer: Towards Exploring Connected Subgraph Explanations for
  Knowledge Graph Completion
Authors: Tengfei Ma, Xiang song, Wen Tao, Mufei Li, Jiani Zhang, Xiaoqin Pan,
  Jianxin Lin, Bosheng Song, xiangxiang Zeng
Categories: cs.AI
Comments: 13 pages, 7 figures, 11 tables. Under Review
\\
  Knowledge graph completion (KGC) aims to alleviate the inherent
incompleteness of knowledge graphs (KGs), which is a critical task for various
applications, such as recommendations on the web. Although knowledge graph
embedding (KGE) models have demonstrated superior predictive performance on KGC
tasks, these models infer missing links in a black-box manner that lacks
transparency and accountability, preventing researchers from developing
accountable models. Existing KGE-based explanation methods focus on exploring
key paths or isolated edges as explanations, which is information-less to
reason target prediction. Additionally, the missing ground truth leads to these
explanation methods being ineffective in quantitatively evaluating explored
explanations. To overcome these limitations, we propose KGExplainer, a
model-agnostic method that identifies connected subgraph explanations and
distills an evaluator to assess them quantitatively. KGExplainer employs a
perturbation-based greedy search algorithm to find key connected subgraphs as
explanations within the local structure of target predictions. To evaluate the
quality of the explored explanations, KGExplainer distills an evaluator from
the target KGE model. By forwarding the explanations to the evaluator, our
method can examine the fidelity of them. Extensive experiments on benchmark
datasets demonstrate that KGExplainer yields promising improvement and achieves
an optimal ratio of 83.3% in human evaluation.
\\ ( https://arxiv.org/abs/2404.03893 ,  570kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03978
Date: Fri, 5 Apr 2024 09:19:55 GMT   (3240kb,D)

Title: Random Walk in Random Permutation Set Theory
Authors: Jiefeng Zhou, Zhen Li, Yong Deng
Categories: cs.AI cs.IT math.IT
Comments: 27 pages, 8 figures
\\
  Random walk is an explainable approach for modeling natural processes at the
molecular level. The Random Permutation Set Theory (RPST) serves as a framework
for uncertainty reasoning, extending the applicability of Dempster-Shafer
Theory. Recent explorations indicate a promising link between RPST and random
walk. In this study, we conduct an analysis and construct a random walk model
based on the properties of RPST, with Monte Carlo simulations of such random
walk. Our findings reveal that the random walk generated through RPST exhibits
characteristics similar to those of a Gaussian random walk and can be
transformed into a Wiener process through a specific limiting scaling
procedure. This investigation establishes a novel connection between RPST and
random walk theory, thereby not only expanding the applicability of RPST, but
also demonstrating the potential for combining the strengths of both approaches
to improve problem-solving abilities.
\\ ( https://arxiv.org/abs/2404.03978 ,  3240kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04106
Date: Fri, 5 Apr 2024 14:02:04 GMT   (3175kb,D)

Title: Intervention-Assisted Policy Gradient Methods for Online Stochastic
  Queuing Network Optimization: Technical Report
Authors: Jerrod Wigmore, Brooke Shrader, Eytan Modiano
Categories: cs.AI cs.LG
Comments: 25 pages, 6 Figures
ACM-class: F.2.2; I.2.6
\\
  Deep Reinforcement Learning (DRL) offers a powerful approach to training
neural network control policies for stochastic queuing networks (SQN). However,
traditional DRL methods rely on offline simulations or static datasets,
limiting their real-world application in SQN control. This work proposes Online
Deep Reinforcement Learning-based Controls (ODRLC) as an alternative, where an
intelligent agent interacts directly with a real environment and learns an
optimal control policy from these online interactions. SQNs present a challenge
for ODRLC due to the unbounded nature of the queues within the network
resulting in an unbounded state-space. An unbounded state-space is particularly
challenging for neural network policies as neural networks are notoriously poor
at extrapolating to unseen states. To address this challenge, we propose an
intervention-assisted framework that leverages strategic interventions from
known stable policies to ensure the queue sizes remain bounded. This framework
combines the learning power of neural networks with the guaranteed stability of
classical control policies for SQNs. We introduce a method to design these
intervention-assisted policies to ensure strong stability of the network.
Furthermore, we extend foundational DRL theorems for intervention-assisted
policies and develop two practical algorithms specifically for ODRLC of SQNs.
Finally, we demonstrate through experiments that our proposed algorithms
outperform both classical control approaches and prior ODRLC algorithms.
\\ ( https://arxiv.org/abs/2404.04106 ,  3175kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04108
Date: Fri, 5 Apr 2024 14:04:07 GMT   (985kb,D)

Title: Large language models as oracles for instantiating ontologies with
  domain-specific knowledge
Authors: Giovanni Ciatto and Andrea Agiollo and Matteo Magnini and Andrea
  Omicini
Categories: cs.AI cs.CL cs.IR cs.LG cs.LO
\\
  Background. Endowing intelligent systems with semantic data commonly requires
designing and instantiating ontologies with domain-specific knowledge.
Especially in the early phases, those activities are typically performed
manually by human experts possibly leveraging on their own experience. The
resulting process is therefore time-consuming, error-prone, and often biased by
the personal background of the ontology designer. Objective. To mitigate that
issue, we propose a novel domain-independent approach to automatically
instantiate ontologies with domain-specific knowledge, by leveraging on large
language models (LLMs) as oracles. Method. Starting from (i) an initial schema
composed by inter-related classes andproperties and (ii) a set of query
templates, our method queries the LLM multi- ple times, and generates instances
for both classes and properties from its replies. Thus, the ontology is
automatically filled with domain-specific knowledge, compliant to the initial
schema. As a result, the ontology is quickly and automatically enriched with
manifold instances, which experts may consider to keep, adjust, discard, or
complement according to their own needs and expertise. Contribution. We
formalise our method in general way and instantiate it over various LLMs, as
well as on a concrete case study. We report experiments rooted in the
nutritional domain where an ontology of food meals and their ingredients is
semi-automatically instantiated from scratch, starting from a categorisation of
meals and their relationships. There, we analyse the quality of the generated
ontologies and compare ontologies attained by exploiting different LLMs.
Finally, we provide a SWOT analysis of the proposed method.
\\ ( https://arxiv.org/abs/2404.04108 ,  985kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03732
Date: Thu, 4 Apr 2024 18:01:21 GMT   (350kb,D)

Title: SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based
  Classification for Hallucination Detection
Authors: Bradley P. Allen and Fina Polat and Paul Groth
Categories: cs.CL cs.AI
Comments: 6 pages, 6 figures, 4 tables, camera-ready copy, accepted to the 18th
  International Workshop on Semantic Evaluation (SemEval-2024), for associated
  code and data see https://github.com/bradleypallen/shroom
\\
  We describe the University of Amsterdam Intelligent Data Engineering Lab
team's entry for the SemEval-2024 Task 6 competition. The SHROOM-INDElab system
builds on previous work on using prompt programming and in-context learning
with large language models (LLMs) to build classifiers for hallucination
detection, and extends that work through the incorporation of context-specific
definition of task, role, and target concept, and automated generation of
examples for use in a few-shot prompting approach. The resulting system
achieved fourth-best and sixth-best performance in the model-agnostic track and
model-aware tracks for Task 6, respectively, and evaluation using the
validation sets showed that the system's classification decisions were
consistent with those of the crowd-sourced human labellers. We further found
that a zero-shot approach provided better accuracy than a few-shot approach
using automatically generated examples. Code for the system described in this
paper is available on Github.
\\ ( https://arxiv.org/abs/2404.03732 ,  350kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03818
Date: Thu, 4 Apr 2024 21:57:11 GMT   (178kb,D)

Title: PRobELM: Plausibility Ranking Evaluation for Language Models
Authors: Zhangdie Yuan, Chenxi Whitehouse, Eric Chamoun, Rami Aly and Andreas
  Vlachos
Categories: cs.CL
\\
  This paper introduces PRobELM (Plausibility Ranking Evaluation for Language
Models), a benchmark designed to assess language models' ability to discern
more plausible from less plausible scenarios through their parametric
knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or
truthfulness, and others such as COPA explore plausible scenarios without
explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by
evaluating models' capabilities to prioritise plausible scenarios that leverage
world knowledge over less plausible alternatives. This design allows us to
assess the potential of language models for downstream use cases such as
literature-based discovery where the focus is on identifying information that
is likely but not yet known. Our benchmark is constructed from a dataset
curated from Wikidata edit histories, tailored to align the temporal bounds of
the training data for the evaluated models. PRobELM facilitates the evaluation
of language models across multiple prompting types, including statement, text
completion, and question-answering. Experiments with 10 models of various sizes
and architectures on the relationship between model scales, training recency,
and plausibility performance, reveal that factual accuracy does not directly
correlate with plausibility performance and that up-to-date training data
enhances plausibility assessment across different model architectures.
\\ ( https://arxiv.org/abs/2404.03818 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03820
Date: Thu, 4 Apr 2024 22:31:58 GMT   (348kb,D)

Title: CantTalkAboutThis: Aligning Language Models to Stay on Topic in
  Dialogues
Authors: Makesh Narsimhan Sreedhar, Traian Rebedea, Shaona Ghosh and
  Christopher Parisien
Categories: cs.CL
\\
  Recent advancements in instruction-tuning datasets have predominantly focused
on specific tasks like mathematical or logical reasoning. There has been a
notable gap in data designed for aligning language models to maintain topic
relevance in conversations - a critical aspect for deploying chatbots to
production. We introduce the CantTalkAboutThis dataset to help language models
remain focused on the subject at hand during task-oriented interactions. It
consists of synthetic dialogues on a wide range of conversation topics from
different domains. These dialogues are interspersed with distractor turns that
intentionally divert the chatbot from the predefined topic. Fine-tuning
language models on this dataset helps make them resilient to deviating from the
role assigned and improves their ability to maintain topical coherence compared
to general-purpose instruction-tuned LLMs like GPT-4-turbo and
Mixtral-Instruct. Additionally, preliminary observations suggest that training
models on this dataset also enhance their performance on fine-grained
instruction following tasks.
\\ ( https://arxiv.org/abs/2404.03820 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03862
Date: Fri, 5 Apr 2024 02:27:09 GMT   (6621kb,D)

Title: Verifiable by Design: Aligning Language Models to Quote from
  Pre-Training Data
Authors: Jingyu Zhang, Marc Marone, Tianjian Li, Benjamin Van Durme, Daniel
  Khashabi
Categories: cs.CL
\\
  For humans to trust the fluent generations of large language models (LLMs),
they must be able to verify their correctness against trusted, external
sources. Recent efforts aim to increase verifiability through citations of
retrieved documents or post-hoc provenance. However, such citations are prone
to mistakes that further complicate their verifiability. To address these
limitations, we tackle the verifiability goal with a different philosophy: we
trivialize the verification process by developing models that quote verbatim
statements from trusted sources in pre-training data. We propose Quote-Tuning,
which demonstrates the feasibility of aligning LLMs to leverage memorized
information and quote from pre-training data. Quote-Tuning quantifies quoting
against large corpora with efficient membership inference tools, and uses the
amount of quotes as an implicit reward signal to construct a synthetic
preference dataset for quoting, without any human annotation. Next, the target
model is aligned to quote using preference optimization algorithms.
Experimental results show that Quote-Tuning significantly increases the
percentage of LLM generation quoted verbatim from high-quality pre-training
documents by 55% to 130% relative to untuned models while maintaining response
quality. Further experiments demonstrate that Quote-Tuning generalizes quoting
to out-of-domain data, is applicable in different tasks, and provides
additional benefits to truthfulness. Quote-Tuning not only serves as a
hassle-free method to increase quoting but also opens up avenues for improving
LLM trustworthiness through better verifiability.
\\ ( https://arxiv.org/abs/2404.03862 ,  6621kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03865
Date: Fri, 5 Apr 2024 02:35:43 GMT   (1500kb,D)

Title: FFN-SkipLLM: A Hidden Gem for Autoregressive Decoding with Adaptive Feed
  Forward Skipping
Authors: Ajay Jaiswal, Bodun Hu, Lu Yin, Yeonju Ro, Shiwei Liu, Tianlong Chen,
  Aditya Akella
Categories: cs.CL cs.LG
Comments: arXiv admin note: text overlap with arXiv:2310.01382
\\
  Autoregressive Large Language Models (e.g., LLaMa, GPTs) are omnipresent
achieving remarkable success in language understanding and generation. However,
such impressive capability typically comes with a substantial model size, which
presents significant challenges for autoregressive token-by-token generation.
To mitigate computation overload incurred during generation, several early-exit
and layer-dropping strategies have been proposed. Despite some promising
success due to the redundancy across LLMs layers on metrics like Rough-L/BLUE,
our careful knowledge-intensive evaluation unveils issues such as generation
collapse, hallucination of wrong facts, and noticeable performance drop even at
the trivial exit ratio of 10-15% of layers. We attribute these errors primarily
to ineffective handling of the KV cache through state copying during
early-exit. In this work, we observed the saturation of computationally
expensive feed-forward blocks of LLM layers and proposed FFN-SkipLLM, which is
a novel fine-grained skip strategy of autoregressive LLMs. More specifically,
FFN-SkipLLM is an input-adaptive feed-forward skipping strategy that can skip
25-30% of FFN blocks of LLMs with marginal change in performance on
knowledge-intensive generation tasks without any requirement to handle KV
cache. Our extensive experiments and ablation across benchmarks like MT-Bench,
Factoid-QA, and variable-length text summarization illustrate how our simple
and ease-at-use method can facilitate faster autoregressive decoding.
\\ ( https://arxiv.org/abs/2404.03865 ,  1500kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03868
Date: Fri, 5 Apr 2024 02:53:51 GMT   (106kb,D)

Title: Extract, Define, Canonicalize: An LLM-based Framework for Knowledge
  Graph Construction
Authors: Bowen Zhang, Harold Soh
Categories: cs.CL cs.AI cs.LG
Comments: 15 pages, 2 figures
\\
  In this work, we are interested in automated methods for knowledge graph
creation (KGC) from input text. Progress on large language models (LLMs) has
prompted a series of recent works applying them to KGC, e.g., via zero/few-shot
prompting. Despite successes on small domain-specific datasets, these models
face difficulties scaling up to text common in many real-world applications. A
principal issue is that in prior methods, the KG schema has to be included in
the LLM prompt to generate valid triplets; larger and more complex schema
easily exceed the LLMs' context window length. To address this problem, we
propose a three-phase framework named Extract-Define-Canonicalize (EDC): open
information extraction followed by schema definition and post-hoc
canonicalization. EDC is flexible in that it can be applied to settings where a
pre-defined target schema is available and when it is not; in the latter case,
it constructs a schema automatically and applies self-canonicalization. To
further improve performance, we introduce a trained component that retrieves
schema elements relevant to the input text; this improves the LLMs' extraction
performance in a retrieval-augmented generation-like manner. We demonstrate on
three KGC benchmarks that EDC is able to extract high-quality triplets without
any parameter tuning and with significantly larger schemas compared to prior
works.
\\ ( https://arxiv.org/abs/2404.03868 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03881
Date: Fri, 5 Apr 2024 04:04:23 GMT   (10698kb,D)

Title: A Bi-consolidating Model for Joint Relational Triple Extraction
Authors: Xiaocheng Luo, Yanping Chen, Ruixue Tang, Ruizhang Huang, Yongbin Qin
Categories: cs.CL
\\
  Current methods to extract relational triples directly make a prediction
based on a possible entity pair in a raw sentence without depending on entity
recognition. The task suffers from a serious semantic overlapping problem, in
which several relation triples may share one or two entities in a sentence. It
is weak to learn discriminative semantic features relevant to a relation
triple. In this paper, based on a two-dimensional sentence representation, a
bi-consolidating model is proposed to address this problem by simultaneously
reinforcing the local and global semantic features relevant to a relation
triple. This model consists of a local consolidation component and a global
consolidation component. The first component uses a pixel difference
convolution to enhance semantic information of a possible triple representation
from adjacent regions and mitigate noise in neighbouring neighbours. The second
component strengthens the triple representation based a channel attention and a
spatial attention, which has the advantage to learn remote semantic
dependencies in a sentence. They are helpful to improve the performance of both
entity identification and relation type classification in relation triple
extraction. After evaluated on several publish datasets, it achieves
competitive performance. Analytical experiments demonstrate the effectiveness
of our model for relational triple extraction and give motivation for other
natural language processing tasks.
\\ ( https://arxiv.org/abs/2404.03881 ,  10698kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03887
Date: Fri, 5 Apr 2024 04:25:47 GMT   (735kb,D)

Title: SAAS: Solving Ability Amplification Strategy for Enhanced Mathematical
  Reasoning in Large Language Models
Authors: Hyeonwoo Kim, Gyoungjin Gim, Yungi Kim, Jihoo Kim, Byungju Kim,
  Wonseok Lee, Chanjun Park
Categories: cs.CL cs.AI
\\
  This study presents a novel learning approach designed to enhance both
mathematical reasoning and problem-solving abilities of Large Language Models
(LLMs). We focus on integrating the Chain-of-Thought (CoT) and the
Program-of-Thought (PoT) learning, hypothesizing that prioritizing the learning
of mathematical reasoning ability is helpful for the amplification of
problem-solving ability. Thus, the initial learning with CoT is essential for
solving challenging mathematical problems. To this end, we propose a sequential
learning approach, named SAAS (Solving Ability Amplification Strategy), which
strategically transitions from CoT learning to PoT learning. Our empirical
study, involving an extensive performance comparison using several benchmarks,
demonstrates that our SAAS achieves state-of-the-art (SOTA) performance. The
results underscore the effectiveness of our sequential learning approach,
marking a significant advancement in the field of mathematical reasoning in
LLMs.
\\ ( https://arxiv.org/abs/2404.03887 ,  735kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03912
Date: Fri, 5 Apr 2024 06:35:31 GMT   (423kb,D)

Title: Forget NLI, Use a Dictionary: Zero-Shot Topic Classification for
  Low-Resource Languages with Application to Luxembourgish
Authors: Fred Philippy, Shohreh Haddadan and Siwen Guo
Categories: cs.CL cs.AI
Comments: 3rd Annual Meeting of the ELRA/ISCA Special Interest Group on
  Under-resourced Languages (SIGUL 2024)
\\
  In NLP, zero-shot classification (ZSC) is the task of assigning labels to
textual data without any labeled examples for the target classes. A common
method for ZSC is to fine-tune a language model on a Natural Language Inference
(NLI) dataset and then use it to infer the entailment between the input
document and the target labels. However, this approach faces certain
challenges, particularly for languages with limited resources. In this paper,
we propose an alternative solution that leverages dictionaries as a source of
data for ZSC. We focus on Luxembourgish, a low-resource language spoken in
Luxembourg, and construct two new topic relevance classification datasets based
on a dictionary that provides various synonyms, word translations and example
sentences. We evaluate the usability of our dataset and compare it with the
NLI-based approach on two topic classification tasks in a zero-shot manner. Our
results show that by using the dictionary-based dataset, the trained models
outperform the ones following the NLI-based approach for ZSC. While we focus on
a single low-resource language in this study, we believe that the efficacy of
our approach can also transfer to other languages where such a dictionary is
available.
\\ ( https://arxiv.org/abs/2404.03912 ,  423kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03921
Date: Fri, 5 Apr 2024 07:07:15 GMT   (234kb,D)

Title: Simple Techniques for Enhancing Sentence Embeddings in Generative
  Language Models
Authors: Bowen Zhang, Kehua Chang, Chunping Li
Categories: cs.CL
Comments: Work in Progress
\\
  Sentence Embedding stands as a fundamental task within the realm of Natural
Language Processing, finding extensive application in search engines, expert
systems, and question-and-answer platforms. With the continuous evolution of
large language models such as LLaMA and Mistral, research on sentence embedding
has recently achieved notable breakthroughs. However, these advancements mainly
pertain to fine-tuning scenarios, leaving explorations into computationally
efficient direct inference methods for sentence representation in a nascent
stage. This paper endeavors to bridge this research gap. Through comprehensive
experimentation, we challenge the widely held belief in the necessity of an
Explicit One-word Limitation for deriving sentence embeddings from Pre-trained
Language Models (PLMs). We demonstrate that this approach, while beneficial for
generative models under direct inference scenario, is not imperative for
discriminative models or the fine-tuning of generative PLMs. This discovery
sheds new light on the design of manual templates in future studies. Building
upon this insight, we propose two innovative prompt engineering techniques
capable of further enhancing the expressive power of PLMs' raw embeddings:
Pretended Chain of Thought and Knowledge Enhancement. We confirm their
effectiveness across various PLM types and provide a detailed exploration of
the underlying factors contributing to their success.
\\ ( https://arxiv.org/abs/2404.03921 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03938
Date: Fri, 5 Apr 2024 07:57:03 GMT   (687kb,D)

Title: Data Augmentation with In-Context Learning and Comparative Evaluation in
  Math Word Problem Solving
Authors: Gulsum Yigit and Mehmet Fatih Amasyali
Categories: cs.CL
Comments: Accepted in SN Computer Science
\\
  Math Word Problem (MWP) solving presents a challenging task in Natural
Language Processing (NLP). This study aims to provide MWP solvers with a more
diverse training set, ultimately improving their ability to solve various math
problems. We propose several methods for data augmentation by modifying the
problem texts and equations, such as synonym replacement, rule-based: question
replacement, and rule based: reversing question methodologies over two English
MWP datasets. This study extends by introducing a new in-context learning
augmentation method, employing the Llama-7b language model. This approach
involves instruction-based prompting for rephrasing the math problem texts.
Performance evaluations are conducted on 9 baseline models, revealing that
augmentation methods outperform baseline models. Moreover, concatenating
examples generated by various augmentation methods further improves
performance.
\\ ( https://arxiv.org/abs/2404.03938 ,  687kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03977
Date: Fri, 5 Apr 2024 09:18:50 GMT   (785kb,D)

Title: SEME at SemEval-2024 Task 2: Comparing Masked and Generative Language
  Models on Natural Language Inference for Clinical Trials
Authors: Mathilde Aguiar, Pierre Zweigenbaum, Nona Naderi
Categories: cs.CL
\\
  This paper describes our submission to Task 2 of SemEval-2024: Safe
Biomedical Natural Language Inference for Clinical Trials. The Multi-evidence
Natural Language Inference for Clinical Trial Data (NLI4CT) consists of a
Textual Entailment (TE) task focused on the evaluation of the consistency and
faithfulness of Natural Language Inference (NLI) models applied to Clinical
Trial Reports (CTR). We test 2 distinct approaches, one based on finetuning and
ensembling Masked Language Models and the other based on prompting Large
Language Models using templates, in particular, using Chain-Of-Thought and
Contrastive Chain-Of-Thought. Prompting Flan-T5-large in a 2-shot setting leads
to our best system that achieves 0.57 F1 score, 0.64 Faithfulness, and 0.56
Consistency.
\\ ( https://arxiv.org/abs/2404.03977 ,  785kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03987
Date: Fri, 5 Apr 2024 09:48:00 GMT   (1978kb,D)

Title: Investigating the Robustness of Modelling Decisions for Few-Shot
  Cross-Topic Stance Detection: A Preregistered Study
Authors: Myrthe Reuver and Suzan Verberne and Antske Fokkens
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024: cite the published version when
  available
\\
  For a viewpoint-diverse news recommender, identifying whether two news
articles express the same viewpoint is essential. One way to determine "same or
different" viewpoint is stance detection. In this paper, we investigate the
robustness of operationalization choices for few-shot stance detection, with
special attention to modelling stance across different topics. Our experiments
test pre-registered hypotheses on stance detection. Specifically, we compare
two stance task definitions (Pro/Con versus Same Side Stance), two LLM
architectures (bi-encoding versus cross-encoding), and adding Natural Language
Inference knowledge, with pre-trained RoBERTa models trained with shots of 100
examples from 7 different stance detection datasets. Some of our hypotheses and
claims from earlier work can be confirmed, while others give more inconsistent
results. The effect of the Same Side Stance definition on performance differs
per dataset and is influenced by other modelling choices. We found no
relationship between the number of training topics in the training shots and
performance. In general, cross-encoding out-performs bi-encoding, and adding
NLI training to our models gives considerable improvement, but these results
are not consistent across all datasets. Our results indicate that it is
essential to include multiple datasets and systematic modelling experiments
when aiming to find robust modelling choices for the concept `stance'.
\\ ( https://arxiv.org/abs/2404.03987 ,  1978kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04003
Date: Fri, 5 Apr 2024 10:26:42 GMT   (324kb,D)

Title: BuDDIE: A Business Document Dataset for Multi-task Information
  Extraction
Authors: Ran Zmigrod, Dongsheng Wang, Mathieu Sibue, Yulong Pei, Petr Babkin,
  Ivan Brugere, Xiaomo Liu, Nacho Navarro, Antony Papadimitriou, William
  Watson, Zhiqiang Ma, Armineh Nourbakhsh, Sameena Shah
Categories: cs.CL
\\
  The field of visually rich document understanding (VRDU) aims to solve a
multitude of well-researched NLP tasks in a multi-modal domain. Several
datasets exist for research on specific tasks of VRDU such as document
classification (DC), key entity extraction (KEE), entity linking, visual
question answering (VQA), inter alia. These datasets cover documents like
invoices and receipts with sparse annotations such that they support one or two
co-related tasks (e.g., entity extraction and entity linking). Unfortunately,
only focusing on a single specific of documents or task is not representative
of how documents often need to be processed in the wild - where variety in
style and requirements is expected. In this paper, we introduce BuDDIE
(Business Document Dataset for Information Extraction), the first multi-task
dataset of 1,665 real-world business documents that contains rich and dense
annotations for DC, KEE, and VQA. Our dataset consists of publicly available
business entity documents from US state government websites. The documents are
structured and vary in their style and layout across states and types (e.g.,
forms, certificates, reports, etc.). We provide data variety and quality
metrics for BuDDIE as well as a series of baselines for each task. Our
baselines cover traditional textual, multi-modal, and large language model
approaches to VRDU.
\\ ( https://arxiv.org/abs/2404.04003 ,  324kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04022
Date: Fri, 5 Apr 2024 11:06:07 GMT   (2848kb)

Title: Good Books are Complex Matters: Gauging Complexity Profiles Across
  Diverse Categories of Perceived Literary Quality
Authors: Yuri Bizzoni, Pascale Feldkamp, Ida Marie Lassen, Mia Jacobsen, Mads
  Rosendahl Thomsen and Kristoffer Nielbo
Categories: cs.CL
\\
  In this study, we employ a classification approach to show that different
categories of literary "quality" display unique linguistic profiles, leveraging
a corpus that encompasses titles from the Norton Anthology, Penguin Classics
series, and the Open Syllabus project, contrasted against contemporary
bestsellers, Nobel prize winners and recipients of prestigious literary awards.
Our analysis reveals that canonical and so called high-brow texts exhibit
distinct textual features when compared to other quality categories such as
bestsellers and popular titles as well as to control groups, likely responding
to distinct (but not mutually exclusive) models of quality. We apply a classic
machine learning approach, namely Random Forest, to distinguish quality novels
from "control groups", achieving up to 77\% F1 scores in differentiating
between the categories. We find that quality category tend to be easier to
distinguish from control groups than from other quality categories, suggesting
than literary quality features might be distinguishable but shared through
quality proxies.
\\ ( https://arxiv.org/abs/2404.04022 ,  2848kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04031
Date: Fri, 5 Apr 2024 11:24:41 GMT   (2879kb,D)

Title: Willkommens-Merkel, Chaos-Johnson, and Tore-Klose: Modeling the
  Evaluative Meaning of German Personal Name Compounds
Authors: Annerose Eichel, Tana Deeg, Andr\'e Blessing, Milena Belosevic, Sabine
  Arndt-Lappe, Sabine Schulte im Walde
Categories: cs.CL cs.CY
Comments: Accepted at LREC-COLING 2024
\\
  We present a comprehensive computational study of the under-investigated
phenomenon of personal name compounds (PNCs) in German such as
Willkommens-Merkel ('Welcome-Merkel'). Prevalent in news, social media, and
political discourse, PNCs are hypothesized to exhibit an evaluative function
that is reflected in a more positive or negative perception as compared to the
respective personal full name (such as Angela Merkel). We model 321 PNCs and
their corresponding full names at discourse level, and show that PNCs bear an
evaluative nature that can be captured through a variety of computational
methods. Specifically, we assess through valence information whether a PNC is
more positively or negatively evaluative than the person's name, by applying
and comparing two approaches using (i) valence norms and (ii) pretrained
language models (PLMs). We further enrich our data with personal,
domain-specific, and extra-linguistic information and perform a range of
regression analyses revealing that factors including compound and modifier
valence, domain, and political party membership influence how a PNC is
evaluated.
\\ ( https://arxiv.org/abs/2404.04031 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04035
Date: Fri, 5 Apr 2024 11:37:40 GMT   (9069kb,D)

Title: A Dataset for Physical and Abstract Plausibility and Sources of Human
  Disagreement
Authors: Annerose Eichel, Sabine Schulte im Walde
Categories: cs.CL
Comments: Accepted at The 17th Linguistic Annotation Workshop
\\
  We present a novel dataset for physical and abstract plausibility of events
in English. Based on naturally occurring sentences extracted from Wikipedia, we
infiltrate degrees of abstractness, and automatically generate perturbed
pseudo-implausible events. We annotate a filtered and balanced subset for
plausibility using crowd-sourcing, and perform extensive cleansing to ensure
annotation quality. In-depth quantitative analyses indicate that annotators
favor plausibility over implausibility and disagree more on implausible events.
Furthermore, our plausibility dataset is the first to capture abstractness in
events to the same extent as concreteness, and we find that event abstractness
has an impact on plausibility ratings: more concrete event participants trigger
a perception of implausibility.
\\ ( https://arxiv.org/abs/2404.04035 ,  9069kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04042
Date: Fri, 5 Apr 2024 11:52:02 GMT   (8067kb,D)

Title: Teaching Llama a New Language Through Cross-Lingual Knowledge Transfer
Authors: Hele-Andra Kuulmets, Taido Purason, Agnes Luhtaru, Mark Fishel
Categories: cs.CL
\\
  This paper explores cost-efficient methods to adapt pretrained Large Language
Models (LLMs) to new lower-resource languages, with a specific focus on
Estonian. Leveraging the Llama 2 model, we investigate the impact of combining
cross-lingual instruction-tuning with additional monolingual pretraining. Our
results demonstrate that even a relatively small amount of additional
monolingual pretraining followed by cross-lingual instruction-tuning
significantly enhances results on Estonian. Furthermore, we showcase
cross-lingual knowledge transfer from high-quality English instructions to
Estonian, resulting in improvements in commonsense reasoning and multi-turn
conversation capabilities. Our best model, named \textsc{Llammas}, represents
the first open-source instruction-following LLM for Estonian. Additionally, we
publish Alpaca-est, the first general task instruction dataset for Estonia.
These contributions mark the initial progress in the direction of developing
open-source LLMs for Estonian.
\\ ( https://arxiv.org/abs/2404.04042 ,  8067kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04067
Date: Fri, 5 Apr 2024 12:51:37 GMT   (2004kb,D)

Title: CLUE: A Clinical Language Understanding Evaluation for LLMs
Authors: Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen
  Kora\c{s}, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek
Categories: cs.CL cs.AI cs.LG
\\
  Large Language Models (LLMs) have shown the potential to significantly
contribute to patient care, diagnostics, and administrative processes. Emerging
biomedical LLMs address healthcare-specific challenges, including privacy
demands and computational constraints. However, evaluation of these models has
primarily been limited to non-clinical tasks, which do not reflect the
complexity of practical clinical applications. Additionally, there has been no
thorough comparison between biomedical and general-domain LLMs for clinical
tasks. To fill this gap, we present the Clinical Language Understanding
Evaluation (CLUE), a benchmark tailored to evaluate LLMs on real-world clinical
tasks. CLUE includes two novel datasets derived from MIMIC IV discharge letters
and four existing tasks designed to test the practical applicability of LLMs in
healthcare settings. Our evaluation covers several biomedical and general
domain LLMs, providing insights into their clinical performance and
applicability. CLUE represents a step towards a standardized approach to
evaluating and developing LLMs in healthcare to align future model development
with the real-world needs of clinical application. We publish our evaluation
and data generation scripts: https://github.com/dadaamin/CLUE
\\ ( https://arxiv.org/abs/2404.04067 ,  2004kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04068
Date: Fri, 5 Apr 2024 12:51:48 GMT   (412kb,D)

Title: Assessing the quality of information extraction
Authors: Filip Seitl, Tom\'a\v{s} Kov\'a\v{r}\'ik, Soheyla Mirshahi, Jan
  Kry\v{s}t\r{u}fek, Rastislav Dujava, Mat\'u\v{s} Ondrei\v{c}ka, Herbert
  Ullrich, Petr Gronat
Categories: cs.CL
\\
  Advances in large language models have notably enhanced the efficiency of
information extraction from unstructured and semi-structured data sources. As
these technologies become integral to various applications, establishing an
objective measure for the quality of information extraction becomes imperative.
However, the scarcity of labeled data presents significant challenges to this
endeavor. In this paper, we introduce an automatic framework to assess the
quality of the information extraction and its completeness. The framework
focuses on information extraction in the form of entity and its properties. We
discuss how to handle the input/output size limitations of the large language
models and analyze their performance when iteratively extracting the
information. Finally, we introduce metrics to evaluate the quality of the
extraction and provide an extensive discussion on how to interpret the metrics.
\\ ( https://arxiv.org/abs/2404.04068 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04103
Date: Fri, 5 Apr 2024 13:59:12 GMT   (57kb,D)

Title: Improving Factual Accuracy of Neural Table-to-Text Output by Addressing
  Input Problems in ToTTo
Authors: Barkavi Sundararajan, Somayajulu Sripada, Ehud Reiter
Categories: cs.CL
Comments: Added link to human evaluation guidelines and error annotations
\\
  Neural Table-to-Text models tend to hallucinate, producing texts that contain
factual errors. We investigate whether such errors in the output can be traced
back to problems with the input. We manually annotated 1,837 texts generated by
multiple models in the politics domain of the ToTTo dataset. We identify the
input problems that are responsible for many output errors and show that fixing
these inputs reduces factual errors by between 52% and 76% (depending on the
model). In addition, we observe that models struggle in processing tabular
inputs that are structured in a non-standard way, particularly when the input
lacks distinct row and column values or when the column headers are not
correctly mapped to corresponding values.
\\ ( https://arxiv.org/abs/2404.04103 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04113
Date: Fri, 5 Apr 2024 14:13:55 GMT   (204kb,D)

Title: BEAR: A Unified Framework for Evaluating Relational Knowledge in Causal
  and Masked Language Models
Authors: Jacek Wiland, Max Ploner, Alan Akbik
Categories: cs.CL
Comments: NAACL 2024
\\
  Knowledge probing assesses to which degree a language model (LM) has
successfully learned relational knowledge during pre-training. Probing is an
inexpensive way to compare LMs of different sizes and training configurations.
However, previous approaches rely on the objective function used in
pre-training LMs and are thus applicable only to masked or causal LMs. As a
result, comparing different types of LMs becomes impossible. To address this,
we propose an approach that uses an LM's inherent ability to estimate the
log-likelihood of any given textual statement. We carefully design an
evaluation dataset of 7,731 instances (40,916 in a larger variant) from which
we produce alternative statements for each relational fact, one of which is
correct. We then evaluate whether an LM correctly assigns the highest
log-likelihood to the correct statement. Our experimental evaluation of 22
common LMs shows that our proposed framework, BEAR, can effectively probe for
knowledge across different LM types. We release the BEAR datasets and an
open-source framework that implements the probing approach to the research
community to facilitate the evaluation and development of LMs.
\\ ( https://arxiv.org/abs/2404.04113 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04167
Date: Fri, 5 Apr 2024 15:20:02 GMT   (2398kb,D)

Title: Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model
Authors: Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang
  Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui
  Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, and Ge Zhang
Categories: cs.CL cs.AI
\\
  In this study, we introduce CT-LLM, a 2B large language model (LLM) that
illustrates a pivotal shift towards prioritizing the Chinese language in
developing LLMs. Uniquely initiated from scratch, CT-LLM diverges from the
conventional methodology by primarily incorporating Chinese textual data,
utilizing an extensive corpus of 1,200 billion tokens, including 800 billion
Chinese tokens, 300 billion English tokens, and 100 billion code tokens. This
strategic composition facilitates the model's exceptional proficiency in
understanding and processing Chinese, a capability further enhanced through
alignment techniques. Demonstrating remarkable performance on the CHC-Bench,
CT-LLM excels in Chinese language tasks, and showcases its adeptness in English
through SFT. This research challenges the prevailing paradigm of training LLMs
predominantly on English corpora and then adapting them to other languages,
broadening the horizons for LLM training methodologies. By open-sourcing the
full process of training a Chinese LLM, including a detailed data processing
procedure with the obtained Massive Appropriate Pretraining Chinese Corpus
(MAP-CC), a well-chosen multidisciplinary Chinese Hard Case Benchmark
(CHC-Bench), and the 2B-size Chinese Tiny LLM (CT-LLM), we aim to foster
further exploration and innovation in both academia and industry, paving the
way for more inclusive and versatile language models.
\\ ( https://arxiv.org/abs/2404.04167 ,  2398kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04169
Date: Fri, 5 Apr 2024 15:22:02 GMT   (2314kb,D)

Title: Do Sentence Transformers Learn Quasi-Geospatial Concepts from General
  Text?
Authors: Ilya Ilyankou, Aldo Lipani, Stefano Cavazzi, Xiaowei Gao and James
  Haworth
Categories: cs.CL cs.LG
Comments: Presented at the Second International Workshop on Geographic
  Information Extraction from Texts at ECIR 2024
  (https://geo-ext.github.io/GeoExT2024/program/)
\\
  Sentence transformers are language models designed to perform semantic
search. This study investigates the capacity of sentence transformers,
fine-tuned on general question-answering datasets for asymmetric semantic
search, to associate descriptions of human-generated routes across Great
Britain with queries often used to describe hiking experiences. We find that
sentence transformers have some zero-shot capabilities to understand
quasi-geospatial concepts, such as route types and difficulty, suggesting their
potential utility for routing recommendation systems.
\\ ( https://arxiv.org/abs/2404.04169 ,  2314kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04204
Date: Fri, 5 Apr 2024 16:29:58 GMT   (498kb,D)

Title: Social Skill Training with Large Language Models
Authors: Diyi Yang, Caleb Ziems, William Held, Omar Shaikh, Michael S.
  Bernstein, John Mitchell
Categories: cs.CL cs.HC
\\
  People rely on social skills like conflict resolution to communicate
effectively and to thrive in both work and personal life. However, practice
environments for social skills are typically out of reach for most people. How
can we make social skill training more available, accessible, and inviting?
Drawing upon interdisciplinary research from communication and psychology, this
perspective paper identifies social skill barriers to enter specialized fields.
Then we present a solution that leverages large language models for social
skill training via a generic framework. Our AI Partner, AI Mentor framework
merges experiential learning with realistic practice and tailored feedback.
This work ultimately calls for cross-disciplinary innovation to address the
broader implications for workforce development and social equality.
\\ ( https://arxiv.org/abs/2404.04204 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04212
Date: Fri, 5 Apr 2024 16:42:28 GMT   (8111kb,D)

Title: Unlocking Parameter-Efficient Fine-Tuning for Low-Resource Language
  Translation
Authors: Tong Su, Xin Peng, Sarubi Thillainathan, David Guzm\'an, Surangika
  Ranathunga, En-Shiun Annie Lee
Categories: cs.CL
Comments: Accepted to the Findings of NAACL 2024
\\
  Parameter-efficient fine-tuning (PEFT) methods are increasingly vital in
adapting large-scale pre-trained language models for diverse tasks, offering a
balance between adaptability and computational efficiency. They are important
in Low-Resource Language (LRL) Neural Machine Translation (NMT) to enhance
translation accuracy with minimal resources. However, their practical
effectiveness varies significantly across different languages. We conducted
comprehensive empirical experiments with varying LRL domains and sizes to
evaluate the performance of 8 PEFT methods with in total of 15 architectures
using the SacreBLEU score. We showed that 6 PEFT architectures outperform the
baseline for both in-domain and out-domain tests and the Houlsby+Inversion
adapter has the best performance overall, proving the effectiveness of PEFT
methods.
\\ ( https://arxiv.org/abs/2404.04212 ,  8111kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04221
Date: Fri, 5 Apr 2024 17:10:33 GMT   (9563kb,D)

Title: How Lexical is Bilingual Lexicon Induction?
Authors: Harsh Kohli, Helian Feng, Nicholas Dronen, Calvin McCarter, Sina
  Moeini, Ali Kebarighotbi
Categories: cs.CL
Comments: 8 pages, 4 figures. Paper accepted at NAACL Findings 2024
\\
  In contemporary machine learning approaches to bilingual lexicon induction
(BLI), a model learns a mapping between the embedding spaces of a language
pair. Recently, retrieve-and-rank approach to BLI has achieved state of the art
results on the task. However, the problem remains challenging in low-resource
settings, due to the paucity of data. The task is complicated by factors such
as lexical variation across languages. We argue that the incorporation of
additional lexical information into the recent retrieve-and-rank approach
should improve lexicon induction. We demonstrate the efficacy of our proposed
approach on XLING, improving over the previous state of the art by an average
of 2\% across all language pairs.
\\ ( https://arxiv.org/abs/2404.04221 ,  9563kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04232
Date: Fri, 5 Apr 2024 17:26:22 GMT   (2346kb,D)

Title: Benchmarking and Improving Compositional Generalization of Multi-aspect
  Controllable Text Generation
Authors: Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian,
  Zhendong Mao
Categories: cs.CL
\\
  Compositional generalization, representing the model's ability to generate
text with new attribute combinations obtained by recombining single attributes
from the training data, is a crucial property for multi-aspect controllable
text generation (MCTG) methods. Nonetheless, a comprehensive compositional
generalization evaluation benchmark of MCTG is still lacking. We propose
CompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets and a
crafted three-dimensional evaluation protocol, to holistically evaluate the
compositional generalization of MCTG approaches. We observe that existing MCTG
works generally confront a noticeable performance drop in compositional
testing. To mitigate this issue, we introduce Meta-MCTG, a training framework
incorporating meta-learning, where we enable models to learn how to generalize
by simulating compositional generalization scenarios in the training phase. We
demonstrate the effectiveness of Meta-MCTG through achieving obvious
improvement (by at most 3.64%) for compositional testing performance in 94.4%
cases.
\\ ( https://arxiv.org/abs/2404.04232 ,  2346kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04237
Date: Fri, 5 Apr 2024 17:36:26 GMT   (2383kb,D)

Title: Cleared for Takeoff? Compositional & Conditional Reasoning may be the
  Achilles Heel to (Flight-Booking) Language Agents
Authors: Harsh Kohli, Huan Sun
Categories: cs.CL
Comments: 18 pages, 17 figures, 3 tables. Paper under review
\\
  The rapid progress of large language models (LLMs) has seen them excel and
frequently surpass human performance on standard benchmarks. This has enabled
many downstream applications, such as LLM agents, to rely on their
sophisticated reasoning to navigate complex task requirements. However, LLMs
are known to unexpectedly falter in simple tasks and under seemingly
straightforward circumstances - underscoring the need for better and more
diverse evaluation setups to measure their true capabilities. To this end, we
choose to study compositional and conditional reasoning, two cornerstones of
human cognition, and introduce GroundCocoa - a lexically diverse benchmark
connecting these reasoning skills to the real-world problem of flight booking.
Our task involves aligning detailed user preferences with available flight
options presented in a multiple-choice format. Results indicate a significant
disparity in performance among current state-of-the-art LLMs with even the best
performing model, GPT-4 Turbo, not exceeding 67% accuracy despite advanced
prompting techniques.
\\ ( https://arxiv.org/abs/2404.04237 ,  2383kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03659
Date: Wed, 17 Jan 2024 15:51:36 GMT   (312kb,D)

Title: Federated Unlearning for Human Activity Recognition
Authors: Kongyang Chen, Dongping zhang, Yaping Chai, Weibin Zhang, Shaowei
  Wang, Jiaxing Shen
Categories: cs.LG cs.CR
\\
  The rapid evolution of Internet of Things (IoT) technology has spurred the
widespread adoption of Human Activity Recognition (HAR) in various daily life
domains. Federated Learning (FL) is frequently utilized to build a global HAR
model by aggregating user contributions without transmitting raw individual
data. Despite substantial progress in user privacy protection with FL,
challenges persist. Regulations like the General Data Protection Regulation
(GDPR) empower users to request data removal, raising a new query in FL: How
can a HAR client request data removal without compromising other clients'
privacy? In response, we propose a lightweight machine unlearning method for
refining the FL HAR model by selectively removing a portion of a client's
training data. Our method employs a third-party dataset unrelated to model
training. Using KL divergence as a loss function for fine-tuning, we aim to
align the predicted probability distribution on forgotten data with the
third-party dataset. Additionally, we introduce a membership inference
evaluation method to assess unlearning effectiveness. Experimental results
across diverse datasets show our method achieves unlearning accuracy comparable
to \textit{retraining} methods, resulting in speedups ranging from hundreds to
thousands.
\\ ( https://arxiv.org/abs/2404.03659 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03660
Date: Wed, 24 Jan 2024 04:41:03 GMT   (8631kb,D)

Title: Machine Learning in Proton Exchange Membrane Water Electrolysis -- Part
  I: A Knowledge-Integrated Framework
Authors: Xia Chen, Alexander Rex, Janis Woelke, Christoph Eckert, Boris
  Bensmann, Richard Hanke-Rauschenbach, Philipp Geyer
Categories: cs.LG cs.CE
Comments: 21 pages, 7 figures
MSC-class: 68U01
ACM-class: H.1; J.2; J.6
\\
  In this study, we propose to adopt a novel framework, Knowledge-integrated
Machine Learning, for advancing Proton Exchange Membrane Water Electrolysis
(PEMWE) development. Given the significance of PEMWE in green hydrogen
production and the inherent challenges in optimizing its performance, our
framework aims to meld data-driven models with domain-specific insights
systematically to address the domain challenges. We first identify the
uncertainties originating from data acquisition conditions, data-driven model
mechanisms, and domain expertise, highlighting their complementary
characteristics in carrying information from different perspectives. Building
upon this foundation, we showcase how to adeptly decompose knowledge and
extract unique information to contribute to the data augmentation, modeling
process, and knowledge discovery. We demonstrate a hierarchical three-level
framework, termed the "Ladder of Knowledge-integrated Machine Learning", in the
PEMWE context, applying it to three case studies within a context of cell
degradation analysis to affirm its efficacy in interpolation, extrapolation,
and information representation. This research lays the groundwork for more
knowledge-informed enhancements in ML applications in engineering.
\\ ( https://arxiv.org/abs/2404.03660 ,  8631kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03678
Date: Thu, 28 Mar 2024 09:51:28 GMT   (33189kb,D)

Title: Machine learning augmented diagnostic testing to identify sources of
  variability in test performance
Authors: Christopher J. Banks, Aeron Sanchez, Vicki Stewart, Kate Bowen, Graham
  Smith, Rowland R. Kao
Categories: cs.LG q-bio.PE stat.AP stat.ML
\\
  Diagnostic tests which can detect pre-clinical or sub-clinical infection, are
one of the most powerful tools in our armoury of weapons to control infectious
diseases. Considerable effort has been therefore paid to improving diagnostic
testing for human, plant and animal diseases, including strategies for
targeting the use of diagnostic tests towards individuals who are more likely
to be infected. Here, we follow other recent proposals to further refine this
concept, by using machine learning to assess the situational risk under which a
diagnostic test is applied to augment its interpretation . We develop this to
predict the occurrence of breakdowns of cattle herds due to bovine
tuberculosis, exploiting the availability of exceptionally detailed testing
records. We show that, without compromising test specificity, test sensitivity
can be improved so that the proportion of infected herds detected by the skin
test, improves by over 16 percentage points. While many risk factors are
associated with increased risk of becoming infected, of note are several
factors which suggest that, in some herds there is a higher risk of infection
going undetected, including effects that are correlated to the veterinary
practice conducting the test, and number of livestock moved off the herd.
\\ ( https://arxiv.org/abs/2404.03678 ,  33189kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03683
Date: Mon, 1 Apr 2024 06:50:52 GMT   (2903kb,D)

Title: Stream of Search (SoS): Learning to Search in Language
Authors: Kanishk Gandhi, Denise Lee, Gabriel Grand, Muxin Liu, Winson Cheng,
  Archit Sharma, Noah D. Goodman
Categories: cs.LG cs.AI cs.CL
\\
  Language models are rarely shown fruitful mistakes while training. They then
struggle to look beyond the next token, suffering from a snowballing of errors
and struggling to predict the consequence of their actions several steps ahead.
In this paper, we show how language models can be taught to search by
representing the process of search in language, as a flattened string -- a
stream of search (SoS). We propose a unified language for search that captures
an array of different symbolic search strategies. We demonstrate our approach
using the simple yet difficult game of Countdown, where the goal is to combine
input numbers with arithmetic operations to reach a target number. We pretrain
a transformer-based language model from scratch on a dataset of streams of
search generated by heuristic solvers. We find that SoS pretraining increases
search accuracy by 25% over models trained to predict only the optimal search
trajectory. We further finetune this model with two policy improvement methods:
Advantage-Induced Policy Alignment (APA) and Self-Taught Reasoner (STaR). The
finetuned SoS models solve 36% of previously unsolved problems, including
problems that cannot be solved by any of the heuristic solvers. Our results
indicate that language models can learn to solve problems via search,
self-improve to flexibly use different search strategies, and potentially
discover new ones.
\\ ( https://arxiv.org/abs/2404.03683 ,  2903kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03686
Date: Mon, 1 Apr 2024 20:41:28 GMT   (319kb)

Title: Securing Social Spaces: Harnessing Deep Learning to Eradicate
  Cyberbullying
Authors: Rohan Biswas, Kasturi Ganguly, Arijit Das, Diganta Saha
Categories: cs.LG cs.AI cs.CY
\\
  In today's digital world, cyberbullying is a serious problem that can harm
the mental and physical health of people who use social media. This paper
explains just how serious cyberbullying is and how it really affects
indi-viduals exposed to it. It also stresses how important it is to find better
ways to detect cyberbullying so that online spaces can be safer. Plus, it talks
about how making more accurate tools to spot cyberbullying will be really
helpful in the future. Our paper introduces a deep learning-based ap-proach,
primarily employing BERT and BiLSTM architectures, to effective-ly address
cyberbullying. This approach is designed to analyse large vol-umes of posts and
predict potential instances of cyberbullying in online spaces. Our results
demonstrate the superiority of the hateBERT model, an extension of BERT focused
on hate speech detection, among the five mod-els, achieving an accuracy rate of
89.16%. This research is a significant con-tribution to "Computational
Intelligence for Social Transformation," prom-ising a safer and more inclusive
digital landscape.
\\ ( https://arxiv.org/abs/2404.03686 ,  319kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03687
Date: Mon, 1 Apr 2024 20:44:28 GMT   (118kb,D)

Title: DRIVE: Dual Gradient-Based Rapid Iterative Pruning
Authors: Dhananjay Saikumar, Blesson Varghese
Categories: cs.LG cs.CV
\\
  Modern deep neural networks (DNNs) consist of millions of parameters,
necessitating high-performance computing during training and inference. Pruning
is one solution that significantly reduces the space and time complexities of
DNNs. Traditional pruning methods that are applied post-training focus on
streamlining inference, but there are recent efforts to leverage sparsity early
on by pruning before training. Pruning methods, such as iterative
magnitude-based pruning (IMP) achieve up to a 90% parameter reduction while
retaining accuracy comparable to the original model. However, this leads to
impractical runtime as it relies on multiple train-prune-reset cycles to
identify and eliminate redundant parameters. In contrast, training agnostic
early pruning methods, such as SNIP and SynFlow offer fast pruning but fall
short of the accuracy achieved by IMP at high sparsities. To bridge this gap,
we present Dual Gradient-Based Rapid Iterative Pruning (DRIVE), which leverages
dense training for initial epochs to counteract the randomness inherent at the
initialization. Subsequently, it employs a unique dual gradient-based metric
for parameter ranking. It has been experimentally demonstrated for VGG and
ResNet architectures on CIFAR-10/100 and Tiny ImageNet, and ResNet on ImageNet
that DRIVE consistently has superior performance over other training-agnostic
early pruning methods in accuracy. Notably, DRIVE is 43$\times$ to 869$\times$
faster than IMP for pruning.
\\ ( https://arxiv.org/abs/2404.03687 ,  118kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03693
Date: Wed, 3 Apr 2024 02:41:16 GMT   (3085kb,D)

Title: Improve Knowledge Distillation via Label Revision and Data Selection
Authors: Weichao Lan, Yiu-ming Cheung, Qing Xu, Buhua Liu, Zhikai Hu, Mengke
  Li, Zhenghua Chen
Categories: cs.LG cs.AI
\\
  Knowledge distillation (KD) has become a widely used technique in the field
of model compression, which aims to transfer knowledge from a large teacher
model to a lightweight student model for efficient network development. In
addition to the supervision of ground truth, the vanilla KD method regards the
predictions of the teacher as soft labels to supervise the training of the
student model. Based on vanilla KD, various approaches have been developed to
further improve the performance of the student model. However, few of these
previous methods have considered the reliability of the supervision from
teacher models. Supervision from erroneous predictions may mislead the training
of the student model. This paper therefore proposes to tackle this problem from
two aspects: Label Revision to rectify the incorrect supervision and Data
Selection to select appropriate samples for distillation to reduce the impact
of erroneous supervision. In the former, we propose to rectify the teacher's
inaccurate predictions using the ground truth. In the latter, we introduce a
data selection technique to choose suitable training samples to be supervised
by the teacher, thereby reducing the impact of incorrect predictions to some
extent. Experiment results demonstrate the effectiveness of our proposed
method, and show that our method can be combined with other distillation
approaches, improving their performance.
\\ ( https://arxiv.org/abs/2404.03693 ,  3085kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03701
Date: Thu, 4 Apr 2024 00:49:05 GMT   (433kb,D)

Title: Predictive Analytics of Varieties of Potatoes
Authors: Fabiana Ferracina, Bala Krishnamoorthy, Mahantesh Halappanavar,
  Shengwei Hu, Vidyasagar Sathuvalli
Categories: cs.LG stat.ML
Comments: 19 pages, 3 figures, submitted to Precision Agriculture
\\
  We explore the application of machine learning algorithms to predict the
suitability of Russet potato clones for advancement in breeding trials.
Leveraging data from manually collected trials in the state of Oregon, we
investigate the potential of a wide variety of state-of-the-art binary
classification models. We conduct a comprehensive analysis of the dataset that
includes preprocessing, feature engineering, and imputation to address missing
values. We focus on several key metrics such as accuracy, F1-score, and
Matthews correlation coefficient (MCC) for model evaluation. The top-performing
models, namely the multi-layer perceptron (MLPC), histogram-based gradient
boosting classifier (HGBC), and a support vector machine (SVC), demonstrate
consistent and significant results. Variable selection further enhances model
performance and identifies influential features in predicting trial outcomes.
The findings emphasize the potential of machine learning in streamlining the
selection process for potato varieties, offering benefits such as increased
efficiency, substantial cost savings, and judicious resource utilization. Our
study contributes insights into precision agriculture and showcases the
relevance of advanced technologies for informed decision-making in breeding
programs.
\\ ( https://arxiv.org/abs/2404.03701 ,  433kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03702
Date: Thu, 4 Apr 2024 02:43:56 GMT   (1944kb,D)

Title: Personalized Federated Learning for Spatio-Temporal Forecasting: A Dual
  Semantic Alignment-Based Contrastive Approach
Authors: Qingxiang Liu, Sheng Sun, Yuxuan Liang, Jingjing Xue, Min Liu
Categories: cs.LG cs.AI
\\
  The existing federated learning (FL) methods for spatio-temporal forecasting
fail to capture the inherent spatio-temporal heterogeneity, which calls for
personalized FL (PFL) methods to model the spatio-temporally variant patterns.
While contrastive learning approach is promising in addressing spatio-temporal
heterogeneity, the existing methods are noneffective in determining negative
pairs and can hardly apply to PFL paradigm. To tackle this limitation, we
propose a novel PFL method, named Federated dUal sEmantic aLignment-based
contraStive learning (FUELS), which can adaptively align positive and negative
pairs based on semantic similarity, thereby injecting precise spatio-temporal
heterogeneity into the latent representation space by auxiliary contrastive
tasks. From temporal perspective, a hard negative filtering module is
introduced to dynamically align heterogeneous temporal representations for the
supplemented intra-client contrastive task. From spatial perspective, we design
lightweight-but-efficient prototypes as client-level semantic representations,
based on which the server evaluates spatial similarity and yields
client-customized global prototypes for the supplemented inter-client
contrastive task. Extensive experiments demonstrate that FUELS outperforms
state-of-the-art methods, with communication cost decreasing by around 94%.
\\ ( https://arxiv.org/abs/2404.03702 ,  1944kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03704
Date: Thu, 4 Apr 2024 09:02:17 GMT   (656kb)

Title: Improvement of Performance in Freezing of Gait detection in Parkinsons
  Disease using Transformer networks and a single waist worn triaxial
  accelerometer
Authors: Luis Sigcha, Luigi Borz\`i, Ignacio Pav\'on, N\'elson Costa, Susana
  Costa, Pedro Arezes, Juan-Manuel L\'opez, Guillermo De Arcas
Categories: cs.LG cs.AI eess.SP
Journal-ref: Engineering Applications of Artificial Intelligence Volume 116,
  November 2022, 105482
DOI: 10.1016/j.engappai.2022.105482
\\
  Freezing of gait (FOG) is one of the most incapacitating symptoms in
Parkinsons disease, affecting more than 50 percent of patients in advanced
stages of the disease. The presence of FOG may lead to falls and a loss of
independence with a consequent reduction in the quality of life. Wearable
technology and artificial intelligence have been used for automatic FOG
detection to optimize monitoring. However, differences between laboratory and
daily-life conditions present challenges for the implementation of reliable
detection systems. Consequently, improvement of FOG detection methods remains
important to provide accurate monitoring mechanisms intended for free-living
and real-time use. This paper presents advances in automatic FOG detection
using a single body-worn triaxial accelerometer and a novel classification
algorithm based on Transformers and convolutional networks. This study was
performed with data from 21 patients who manifested FOG episodes while
performing activities of daily living in a home setting. Results indicate that
the proposed FOG-Transformer can bring a significant improvement in FOG
detection using leave-one-subject-out cross-validation (LOSO CV). These results
bring opportunities for the implementation of accurate monitoring systems for
use in ambulatory or home settings.
\\ ( https://arxiv.org/abs/2404.03704 ,  656kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03707
Date: Thu, 4 Apr 2024 10:54:38 GMT   (776kb,D)

Title: Investigating the Robustness of Counterfactual Learning to Rank Models:
  A Reproducibility Study
Authors: Zechun Niu, Jiaxin Mao, Qingyao Ai, and Ji-Rong Wen
Categories: cs.LG cs.AI cs.IR
\\
  Counterfactual learning to rank (CLTR) has attracted extensive attention in
the IR community for its ability to leverage massive logged user interaction
data to train ranking models. While the CLTR models can be theoretically
unbiased when the user behavior assumption is correct and the propensity
estimation is accurate, their effectiveness is usually empirically evaluated
via simulation-based experiments due to a lack of widely-available,
large-scale, real click logs. However, the mainstream simulation-based
experiments are somewhat limited as they often feature a single, deterministic
production ranker and simplified user simulation models to generate the
synthetic click logs. As a result, the robustness of CLTR models in complex and
diverse situations is largely unknown and needs further investigation.
  To address this problem, in this paper, we aim to investigate the robustness
of existing CLTR models in a reproducibility study with extensive
simulation-based experiments that (1) use both deterministic and stochastic
production rankers, each with different ranking performance, and (2) leverage
multiple user simulation models with different user behavior assumptions. We
find that the DLA models and IPS-DCM show better robustness under various
simulation settings than IPS-PBM and PRS with offline propensity estimation.
Besides, the existing CLTR models often fail to outperform the naive click
baselines when the production ranker has relatively high ranking performance or
certain randomness, which suggests an urgent need for developing new CLTR
algorithms that work for these settings.
\\ ( https://arxiv.org/abs/2404.03707 ,  776kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03710
Date: Thu, 4 Apr 2024 13:43:17 GMT   (1074kb,D)

Title: Self-organized arrival system for urban air mobility
Authors: Martin Waltz, Ostap Okhrin, Michael Schultz
Categories: cs.LG cs.AI
\\
  Urban air mobility is an innovative mode of transportation in which electric
vertical takeoff and landing (eVTOL) vehicles operate between nodes called
vertiports. We outline a self-organized vertiport arrival system based on deep
reinforcement learning. The airspace around the vertiport is assumed to be
circular, and the vehicles can freely operate inside. Each aircraft is
considered an individual agent and follows a shared policy, resulting in
decentralized actions that are based on local information. We investigate the
development of the reinforcement learning policy during training and illustrate
how the algorithm moves from suboptimal local holding patterns to a safe and
efficient final policy. The latter is validated in simulation-based scenarios
and also deployed on small-scale unmanned aerial vehicles to showcase its
real-world usability.
\\ ( https://arxiv.org/abs/2404.03710 ,  1074kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03713
Date: Thu, 4 Apr 2024 17:46:20 GMT   (40934kb,D)

Title: Explaining Explainability: Understanding Concept Activation Vectors
Authors: Angus Nicolson, Lisa Schut, J. Alison Noble, Yarin Gal
Categories: cs.LG cs.AI cs.CV cs.HC
Comments: (54 pages, 39 figures)
ACM-class: I.2.6
\\
  Recent interpretability methods propose using concept-based explanations to
translate the internal representations of deep learning models into a language
that humans are familiar with: concepts. This requires understanding which
concepts are present in the representation space of a neural network. One
popular method for finding concepts is Concept Activation Vectors (CAVs), which
are learnt using a probe dataset of concept exemplars. In this work, we
investigate three properties of CAVs. CAVs may be: (1) inconsistent between
layers, (2) entangled with different concepts, and (3) spatially dependent.
Each property provides both challenges and opportunities in interpreting
models. We introduce tools designed to detect the presence of these properties,
provide insight into how they affect the derived explanations, and provide
recommendations to minimise their impact. Understanding these properties can be
used to our advantage. For example, we introduce spatially dependent CAVs to
test if a model is translation invariant with respect to a specific concept and
class. Our experiments are performed on ImageNet and a new synthetic dataset,
Elements. Elements is designed to capture a known ground truth relationship
between concepts and classes. We release this dataset to facilitate further
research in understanding and evaluating interpretability methods.
\\ ( https://arxiv.org/abs/2404.03713 ,  40934kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03715
Date: Thu, 4 Apr 2024 17:56:41 GMT   (288kb,D)

Title: Direct Nash Optimization: Teaching Language Models to Self-Improve with
  General Preferences
Authors: Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed
  Awadallah, Tengyang Xie
Categories: cs.LG cs.AI cs.CL
\\
  This paper studies post-training large language models (LLMs) using
preference feedback from a powerful oracle to help a model iteratively improve
over itself. The typical approach for post-training LLMs involves Reinforcement
Learning from Human Feedback (RLHF), which traditionally separates reward
learning and subsequent policy optimization. However, such a reward
maximization approach is limited by the nature of "point-wise" rewards (such as
Bradley-Terry model), which fails to express complex intransitive or cyclic
preference relations. While advances on RLHF show reward learning and policy
optimization can be merged into a single contrastive objective for stability,
they yet still remain tethered to the reward maximization framework. Recently,
a new wave of research sidesteps the reward maximization presumptions in favor
of directly optimizing over "pair-wise" or general preferences. In this paper,
we introduce Direct Nash Optimization (DNO), a provable and scalable algorithm
that marries the simplicity and stability of contrastive learning with
theoretical generality from optimizing general preferences. Because DNO is a
batched on-policy algorithm using a regression-based objective, its
implementation is straightforward and efficient. Moreover, DNO enjoys monotonic
improvement across iterations that help it improve even over a strong teacher
(such as GPT-4). In our experiments, a resulting 7B parameter Orca-2.5 model
aligned by DNO achieves the state-of-the-art win-rate against GPT-4-Turbo of
33% on AlpacaEval 2.0 (even after controlling for response length), an absolute
gain of 26% (7% to 33%) over the initializing model. It outperforms models with
far more parameters, including Mistral Large, Self-Rewarding LM (70B
parameters), and older versions of GPT-4.
\\ ( https://arxiv.org/abs/2404.03715 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03759
Date: Thu, 4 Apr 2024 19:06:29 GMT   (8348kb,D)

Title: Localized Distributional Robustness in Submodular Multi-Task Subset
  Selection
Authors: Ege C. Kaya, Abolfazl Hashemi
Categories: cs.LG eess.SP math.OC
Comments: 35 pages, 7 figures. A preliminary version of this article was
  presented at the 2023 Allerton Conference on Communication, Control, and
  Computing. This version was submitted to IEEE Transactions on Signal
  Processing
\\
  In this work, we approach the problem of multi-task submodular optimization
with the perspective of local distributional robustness, within the
neighborhood of a reference distribution which assigns an importance score to
each task. We initially propose to introduce a regularization term which makes
use of the relative entropy to the standard multi-task objective. We then
demonstrate through duality that this novel formulation itself is equivalent to
the maximization of a submodular function, which may be efficiently carried out
through standard greedy selection methods. This approach bridges the existing
gap in the optimization of performance-robustness trade-offs in multi-task
subset selection. To numerically validate our theoretical results, we test the
proposed method in two different setting, one involving the selection of
satellites in low Earth orbit constellations in the context of a sensor
selection problem, and the other involving an image summarization task using
neural networks. Our method is compared with two other algorithms focused on
optimizing the performance of the worst-case task, and on directly optimizing
the performance on the reference distribution itself. We conclude that our
novel formulation produces a solution that is locally distributional robust,
and computationally inexpensive.
\\ ( https://arxiv.org/abs/2404.03759 ,  8348kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03764
Date: Sat, 30 Mar 2024 07:32:58 GMT   (6193kb,D)

Title: CONCERT: Covariate-Elaborated Robust Local Information Transfer with
  Conditional Spike-and-Slab Prior
Authors: Ruqian Zhang, Yijiao Zhang, Annie Qu, Zhongyi Zhu, Juan Shen
Categories: cs.LG stat.ME stat.ML
Comments: 31 pages, 22 figures
\\
  The popularity of transfer learning stems from the fact that it can borrow
information from useful auxiliary datasets. Existing statistical transfer
learning methods usually adopt a global similarity measure between the source
data and the target data, which may lead to inefficiency when only local
information is shared. In this paper, we propose a novel Bayesian transfer
learning method named "CONCERT" to allow robust local information transfer for
high-dimensional data analysis. A novel conditional spike-and-slab prior is
introduced in the joint distribution of target and source parameters for
information transfer. By incorporating covariate-specific priors, we can
characterize the local similarities and make the sources work collaboratively
to help improve the performance on the target. Distinguished from existing
work, CONCERT is a one-step procedure, which achieves variable selection and
information transfer simultaneously. Variable selection consistency is
established for our CONCERT. To make our algorithm scalable, we adopt the
variational Bayes framework to facilitate implementation. Extensive experiments
and a genetic data analysis demonstrate the validity and the advantage of
CONCERT over existing cutting-edge transfer learning methods. We also extend
our CONCERT to the logistical models with numerical studies showing its
superiority over other methods.
\\ ( https://arxiv.org/abs/2404.03764 ,  6193kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03774
Date: Thu, 4 Apr 2024 19:35:41 GMT   (392kb,D)

Title: Exploration is Harder than Prediction: Cryptographically Separating
  Reinforcement Learning from Supervised Learning
Authors: Noah Golowich, Ankur Moitra, Dhruv Rohatgi
Categories: cs.LG cs.CC cs.CR cs.DS
Comments: 112 pages, 3 figures
\\
  Supervised learning is often computationally easy in practice. But to what
extent does this mean that other modes of learning, such as reinforcement
learning (RL), ought to be computationally easy by extension? In this work we
show the first cryptographic separation between RL and supervised learning, by
exhibiting a class of block MDPs and associated decoding functions where
reward-free exploration is provably computationally harder than the associated
regression problem. We also show that there is no computationally efficient
algorithm for reward-directed RL in block MDPs, even when given access to an
oracle for this regression problem.
  It is known that being able to perform regression in block MDPs is necessary
for finding a good policy; our results suggest that it is not sufficient. Our
separation lower bound uses a new robustness property of the Learning Parities
with Noise (LPN) hardness assumption, which is crucial in handling the
dependent nature of RL data. We argue that separations and oracle lower bounds,
such as ours, are a more meaningful way to prove hardness of learning because
the constructions better reflect the practical reality that supervised learning
by itself is often not the computational bottleneck.
\\ ( https://arxiv.org/abs/2404.03774 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03775
Date: Thu, 4 Apr 2024 19:36:47 GMT   (86kb)

Title: A Systems Theoretic Approach to Online Machine Learning
Authors: Anli du Preez, Peter A. Beling, Tyler Cody
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: Accepted by the 18th Annual IEEE International Systems Conference
  (SysCon)
\\
  The machine learning formulation of online learning is incomplete from a
systems theoretic perspective. Typically, machine learning research emphasizes
domains and tasks, and a problem solving worldview. It focuses on algorithm
parameters, features, and samples, and neglects the perspective offered by
considering system structure and system behavior or dynamics. Online learning
is an active field of research and has been widely explored in terms of
statistical theory and computational algorithms, however, in general, the
literature still lacks formal system theoretical frameworks for modeling online
learning systems and resolving systems-related concept drift issues.
Furthermore, while the machine learning formulation serves to classify methods
and literature, the systems theoretic formulation presented herein serves to
provide a framework for the top-down design of online learning systems,
including a novel definition of online learning and the identification of key
design parameters. The framework is formulated in terms of input-output systems
and is further divided into system structure and system behavior. Concept drift
is a critical challenge faced in online learning, and this work formally
approaches it as part of the system behavior characteristics. Healthcare
provider fraud detection using machine learning is used as a case study
throughout the paper to ground the discussion in a real-world online learning
challenge.
\\ ( https://arxiv.org/abs/2404.03775 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03784
Date: Thu, 4 Apr 2024 19:55:11 GMT   (657kb,D)

Title: Layerwise Early Stopping for Test Time Adaptation
Authors: Sabyasachi Sahoo, Mostafa ElAraby, Jonas Ngnawe, Yann Pequignot,
  Frederic Precioso, Christian Gagne
Categories: cs.LG cs.AI cs.CV
Comments: 14 pages, 5 figures
\\
  Test Time Adaptation (TTA) addresses the problem of distribution shift by
enabling pretrained models to learn new features on an unseen domain at test
time. However, it poses a significant challenge to maintain a balance between
learning new features and retaining useful pretrained features. In this paper,
we propose Layerwise EArly STopping (LEAST) for TTA to address this problem.
The key idea is to stop adapting individual layers during TTA if the features
being learned do not appear beneficial for the new domain. For that purpose, we
propose using a novel gradient-based metric to measure the relevance of the
current learnt features to the new domain without the need for supervised
labels. More specifically, we propose to use this metric to determine
dynamically when to stop updating each layer during TTA. This enables a more
balanced adaptation, restricted to layers benefiting from it, and only for a
certain number of steps. Such an approach also has the added effect of limiting
the forgetting of pretrained features useful for dealing with new domains.
Through extensive experiments, we demonstrate that Layerwise Early Stopping
improves the performance of existing TTA approaches across multiple datasets,
domain shifts, model architectures, and TTA losses.
\\ ( https://arxiv.org/abs/2404.03784 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03800
Date: Thu, 4 Apr 2024 20:44:56 GMT   (575kb,D)

Title: Learning Social Fairness Preferences from Non-Expert Stakeholder
  Opinions in Kidney Placement
Authors: Mukund Telukunta, Sukruth Rao, Gabriella Stickney, Venkata Sriram
  Siddardh Nadendla, Casey Canfield
Categories: cs.LG cs.HC
Comments: Submitted to CHIL (Conference on Health, Inference, and Learning)
  2024
\\
  Modern kidney placement incorporates several intelligent recommendation
systems which exhibit social discrimination due to biases inherited from
training data. Although initial attempts were made in the literature to study
algorithmic fairness in kidney placement, these methods replace true outcomes
with surgeons' decisions due to the long delays involved in recording such
outcomes reliably. However, the replacement of true outcomes with surgeons'
decisions disregards expert stakeholders' biases as well as social opinions of
other stakeholders who do not possess medical expertise. This paper alleviates
the latter concern and designs a novel fairness feedback survey to evaluate an
acceptance rate predictor (ARP) that predicts a kidney's acceptance rate in a
given kidney-match pair. The survey is launched on Prolific, a crowdsourcing
platform, and public opinions are collected from 85 anonymous crowd
participants. A novel social fairness preference learning algorithm is proposed
based on minimizing social feedback regret computed using a novel logit-based
fairness feedback model. The proposed model and learning algorithm are both
validated using simulation experiments as well as Prolific data. Public
preferences towards group fairness notions in the context of kidney placement
have been estimated and discussed in detail. The specific ARP tested in the
Prolific survey has been deemed fair by the participants.
\\ ( https://arxiv.org/abs/2404.03800 ,  575kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03827
Date: Thu, 4 Apr 2024 23:05:30 GMT   (9865kb,D)

Title: Uniform Memory Retrieval with Larger Capacity for Modern Hopfield Models
Authors: Dennis Wu, Jerry Yao-Chieh Hu, Teng-Yun Hsiao, Han Liu
Categories: cs.LG cs.AI stat.ML
Comments: 64 pages; Code available at https://github.com/MAGICS-LAB/UHop
\\
  We propose a two-stage memory retrieval dynamics for modern Hopfield models,
termed $\mathtt{U\text{-}Hop}$, with enhanced memory capacity. Our key
contribution is a learnable feature map $\Phi$ which transforms the Hopfield
energy function into a kernel space. This transformation ensures convergence
between the local minima of energy and the fixed points of retrieval dynamics
within the kernel space. Consequently, the kernel norm induced by $\Phi$ serves
as a novel similarity measure. It utilizes the stored memory patterns as
learning data to enhance memory capacity across all modern Hopfield models.
Specifically, we accomplish this by constructing a separation loss
$\mathcal{L}_\Phi$ that separates the local minima of kernelized energy by
separating stored memory patterns in kernel space. Methodologically,
$\mathtt{U\text{-}Hop}$ memory retrieval process consists of:
\textbf{(Stage~I.)} minimizing separation loss for a more uniformed memory
(local minimum) distribution, followed by \textbf{(Stage~II.)} standard
Hopfield energy minimization for memory retrieval. This results in a
significant reduction of possible meta-stable states in the Hopfield energy
function, thus enhancing memory capacity by preventing memory confusion.
Empirically, with real-world datasets, we demonstrate that
$\mathtt{U\text{-}Hop}$ outperforms all existing modern Hopfield models and
SOTA similarity measures, achieving substantial improvements in both
associative memory retrieval and deep learning tasks.
\\ ( https://arxiv.org/abs/2404.03827 ,  9865kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03828
Date: Thu, 4 Apr 2024 23:08:43 GMT   (3281kb,D)

Title: Outlier-Efficient Hopfield Layers for Large Transformer-Based Models
Authors: Jerry Yao-Chieh Hu, Pei-Hsuan Chang, Robin Luo, Hong-Yu Chen, Weijian
  Li, Wei-Po Wang, Han Liu
Categories: cs.LG cs.AI stat.ML
Comments: 48 pages; Code available at https://github.com/MAGICS-LAB/OutEffHop
\\
  We introduce an Outlier-Efficient Modern Hopfield Model (termed
$\mathtt{OutEffHop}$) and use it to address the outlier-induced challenge of
quantizing gigantic transformer-based models. Our main contribution is a novel
associative memory model facilitating \textit{outlier-efficient} associative
memory retrievals. Interestingly, this memory model manifests a model-based
interpretation of an outlier-efficient attention mechanism
($\text{Softmax}_1$): it is an approximation of the memory retrieval process of
$\mathtt{OutEffHop}$. Methodologically, this allows us to debut novel
outlier-efficient Hopfield layers a powerful attention alternative with
superior post-quantization performance. Theoretically, the Outlier-Efficient
Modern Hopfield Model retains and improves the desirable properties of the
standard modern Hopfield models, including fixed point convergence and
exponential storage capacity. Empirically, we demonstrate the proposed model's
efficacy across large-scale transformer-based and Hopfield-based models
(including BERT, OPT, ViT and STanHop-Net), benchmarking against
state-of-the-art methods including $\mathtt{Clipped\_Softmax}$ and
$\mathtt{Gated\_Attention}$. Notably, $\mathtt{OutEffHop}$ achieves on average
$\sim$22+\% reductions in both average kurtosis and maximum infinity norm of
model outputs accross 4 models.
\\ ( https://arxiv.org/abs/2404.03828 ,  3281kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03830
Date: Thu, 4 Apr 2024 23:13:32 GMT   (4046kb,D)

Title: BiSHop: Bi-Directional Cellular Learning for Tabular Data with
  Generalized Sparse Modern Hopfield Model
Authors: Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar
  Gilani, Hsi-Sheng Goan, Han Liu
Categories: cs.LG cs.AI stat.ML
Comments: 40 page; Code available at https://github.com/MAGICS-LAB/BiSHop
\\
  We introduce the \textbf{B}i-Directional \textbf{S}parse \textbf{Hop}field
Network (\textbf{BiSHop}), a novel end-to-end framework for deep tabular
learning. BiSHop handles the two major challenges of deep tabular learning:
non-rotationally invariant data structure and feature sparsity in tabular data.
Our key motivation comes from the recent established connection between
associative memory and attention mechanisms. Consequently, BiSHop uses a
dual-component approach, sequentially processing data both column-wise and
row-wise through two interconnected directional learning modules.
Computationally, these modules house layers of generalized sparse modern
Hopfield layers, a sparse extension of the modern Hopfield model with adaptable
sparsity. Methodologically, BiSHop facilitates multi-scale representation
learning, capturing both intra-feature and inter-feature interactions, with
adaptive sparsity at each scale. Empirically, through experiments on diverse
real-world datasets, we demonstrate that BiSHop surpasses current SOTA methods
with significantly less HPO runs, marking it a robust solution for deep tabular
learning.
\\ ( https://arxiv.org/abs/2404.03830 ,  4046kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03833
Date: Thu, 4 Apr 2024 23:30:01 GMT   (652kb,D)

Title: An ExplainableFair Framework for Prediction of Substance Use Disorder
  Treatment Completion
Authors: Mary M. Lucas, Xiaoyang Wang, Chia-Hsuan Chang, Christopher C. Yang,
  Jacqueline E. Braughton, and Quyen M. Ngo
Categories: cs.LG cs.CY
Comments: Accepted to the IEEE International Conference on Healthcare
  Informatics (IEEE ICHI 2024)
\\
  Fairness of machine learning models in healthcare has drawn increasing
attention from clinicians, researchers, and even at the highest level of
government. On the other hand, the importance of developing and deploying
interpretable or explainable models has been demonstrated, and is essential to
increasing the trustworthiness and likelihood of adoption of these models. The
objective of this study was to develop and implement a framework for addressing
both these issues - fairness and explainability. We propose an explainable
fairness framework, first developing a model with optimized performance, and
then using an in-processing approach to mitigate model biases relative to the
sensitive attributes of race and sex. We then explore and visualize
explanations of the model changes that lead to the fairness enhancement process
through exploring the changes in importance of features. Our resulting-fairness
enhanced models retain high sensitivity with improved fairness and explanations
of the fairness-enhancement that may provide helpful insights for healthcare
providers to guide clinical decision-making and resource allocation.
\\ ( https://arxiv.org/abs/2404.03833 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03854
Date: Fri, 5 Apr 2024 01:17:25 GMT   (20898kb,D)

Title: Mitigating Heterogeneity in Federated Multimodal Learning with
  Biomedical Vision-Language Pre-training
Authors: Zitao Shuai, Liyue Shen
Categories: cs.LG cs.CL cs.CV
\\
  Vision-language pre-training (VLP) has arised as an efficient scheme for
multimodal representation learning, but it requires large-scale multimodal data
for pre-training, making it an obstacle especially for biomedical applications.
To overcome the data limitation, federated learning (FL) can be a promising
strategy to scale up the dataset for biomedical VLP while protecting data
privacy. However, client data are often heterogeneous in real-world scenarios,
and we observe that local training on heterogeneous client data would distort
the multimodal representation learning and lead to biased cross-modal
alignment. To address this challenge, we propose Federated distributional
Robust Guidance-Based (FedRGB) learning framework for federated VLP with
robustness to data heterogeneity. Specifically, we utilize a guidance-based
local training scheme to reduce feature distortions, and employ a
distribution-based min-max optimization to learn unbiased cross-modal
alignment. The experiments on real-world datasets show our method successfully
promotes efficient federated multimodal learning for biomedical VLP with data
heterogeneity.
\\ ( https://arxiv.org/abs/2404.03854 ,  20898kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03869
Date: Fri, 5 Apr 2024 03:02:57 GMT   (13288kb,D)

Title: Heterogeneous Multi-Agent Reinforcement Learning for Zero-Shot Scalable
  Collaboration
Authors: Xudong Guo, Daming Shi, Junjie Yu, Wenhui Fan
Categories: cs.LG cs.AI cs.MA cs.RO cs.SY eess.SY
\\
  The rise of multi-agent systems, especially the success of multi-agent
reinforcement learning (MARL), is reshaping our future across diverse domains
like autonomous vehicle networks. However, MARL still faces significant
challenges, particularly in achieving zero-shot scalability, which allows
trained MARL models to be directly applied to unseen tasks with varying numbers
of agents. In addition, real-world multi-agent systems usually contain agents
with different functions and strategies, while the existing scalable MARL
methods only have limited heterogeneity. To address this, we propose a novel
MARL framework named Scalable and Heterogeneous Proximal Policy Optimization
(SHPPO), integrating heterogeneity into parameter-shared PPO-based MARL
networks. we first leverage a latent network to adaptively learn strategy
patterns for each agent. Second, we introduce a heterogeneous layer for
decision-making, whose parameters are specifically generated by the learned
latent variables. Our approach is scalable as all the parameters are shared
except for the heterogeneous layer, and gains both inter-individual and
temporal heterogeneity at the same time. We implement our approach based on the
state-of-the-art backbone PPO-based algorithm as SHPPO, while our approach is
agnostic to the backbone and can be seamlessly plugged into any
parameter-shared MARL method. SHPPO exhibits superior performance over the
baselines such as MAPPO and HAPPO in classic MARL environments like Starcraft
Multi-Agent Challenge (SMAC) and Google Research Football (GRF), showcasing
enhanced zero-shot scalability and offering insights into the learned latent
representation's impact on team performance by visualization.
\\ ( https://arxiv.org/abs/2404.03869 ,  13288kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03870
Date: Fri, 5 Apr 2024 03:11:24 GMT   (873kb)

Title: Optimizing Convolutional Neural Networks for Identifying Invasive
  Pollinator Apis Mellifera and Finding a Ligand drug to Protect California's
  Biodiversity
Authors: Arnav Swaroop
Categories: cs.LG q-bio.BM q-bio.QM
Comments: 13 pages total
\\
  In North America, there are many diverse species of native bees crucial for
the environment, who are the primary pollinators of most native floral species.
The Californian agriculture industry imports European honeybees (Apis
Mellifera) primarily for pollinating almonds. Unfortunately, this has resulted
in the unintended consequence of disrupting the native ecosystem and
threatening many native bee species as they are outcompeted for food. Our first
step for protecting the native species is identification with the use of a
Convolutional Neural Network (CNN) to differentiate common native bee species
from invasive ones. Removing invasive colonies efficiently without harming
native species is difficult as pesticides cause myriad diseases in native
species. Our approach seeks to prevent the formation of new queens, causing the
colony's collapse. Workers secrete royal jelly, a substance that causes
fertility and longevity; it is fed to future honeybee queens. Targeting the
production of this substance is safe as no native species use it; small organic
molecules (ligands) prevent the proteins Apisimin and MRJP1 from combining and
producing an oligomer used to form the substance. Ideal ligands bind to only
one of these proteins preventing them from joining together: they have a high
affinity for one receptor and a significantly lower affinity for the other. We
optimized the CNN to provide a framework for creating Machine Learning models
that excel at differentiating between subspecies of insects by measuring the
effects of image alteration and class grouping on model performance. The CNN is
able to achieve an accuracy of 82% in differentiating between invasive and
native bee species; 3 ligands have been identified as effective. Our new
approach offers a promising solution to curb the spread of invasive bees within
California through an identification and neutralization method.
\\ ( https://arxiv.org/abs/2404.03870 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03888
Date: Fri, 5 Apr 2024 04:34:43 GMT   (897kb,D)

Title: A proximal policy optimization based intelligent home solar management
Authors: Kode Creer, Imitiaz Parvez
Categories: cs.LG cs.AI
\\
  In the smart grid, the prosumers can sell unused electricity back to the
power grid, assuming the prosumers own renewable energy sources and storage
units. The maximizing of their profits under a dynamic electricity market is a
problem that requires intelligent planning. To address this, we propose a
framework based on Proximal Policy Optimization (PPO) using recurrent rewards.
By using the information about the rewards modeled effectively with PPO to
maximize our objective, we were able to get over 30\% improvement over the
other naive algorithms in accumulating total profits. This shows promise in
getting reinforcement learning algorithms to perform tasks required to plan
their actions in complex domains like financial markets. We also introduce a
novel method for embedding longs based on soliton waves that outperformed
normal embedding in our use case with random floating point data augmentation.
\\ ( https://arxiv.org/abs/2404.03888 ,  897kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03908
Date: Fri, 5 Apr 2024 06:15:58 GMT   (2453kb,D)

Title: Multi-Task Learning for Lung sound & Lung disease classification
Authors: Suma K V, Deepali Koppad, Preethi Kumar, Neha A Kantikar, Surabhi
  Ramesh
Categories: cs.LG cs.AI cs.SD
\\
  In recent years, advancements in deep learning techniques have considerably
enhanced the efficiency and accuracy of medical diagnostics. In this work, a
novel approach using multi-task learning (MTL) for the simultaneous
classification of lung sounds and lung diseases is proposed. Our proposed model
leverages MTL with four different deep learning models such as 2D CNN,
ResNet50, MobileNet and Densenet to extract relevant features from the lung
sound recordings. The ICBHI 2017 Respiratory Sound Database was employed in the
current study. The MTL for MobileNet model performed better than the other
models considered, with an accuracy of74\% for lung sound analysis and 91\% for
lung diseases classification. Results of the experimentation demonstrate the
efficacy of our approach in classifying both lung sounds and lung diseases
concurrently.
  In this study,using the demographic data of the patients from the database,
risk level computation for Chronic Obstructive Pulmonary Disease is also
carried out. For this computation, three machine learning algorithms namely
Logistic Regression, SVM and Random Forest classifierswere employed. Among
these ML algorithms, the Random Forest classifier had the highest accuracy of
92\%.This work helps in considerably reducing the physician's burden of not
just diagnosing the pathology but also effectively communicating to the patient
about the possible causes or outcomes.
\\ ( https://arxiv.org/abs/2404.03908 ,  2453kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03969
Date: Fri, 5 Apr 2024 09:05:37 GMT   (4799kb,D)

Title: Transformers for molecular property prediction: Lessons learned from the
  past five years
Authors: Afnan Sultan, Jochen Sieg, Miriam Mathea, and Andrea Volkamer
Categories: cs.LG cs.CL q-bio.QM
\\
  Molecular Property Prediction (MPP) is vital for drug discovery, crop
protection, and environmental science. Over the last decades, diverse
computational techniques have been developed, from using simple physical and
chemical properties and molecular fingerprints in statistical models and
classical machine learning to advanced deep learning approaches. In this
review, we aim to distill insights from current research on employing
transformer models for MPP. We analyze the currently available models and
explore key questions that arise when training and fine-tuning a transformer
model for MPP. These questions encompass the choice and scale of the
pre-training data, optimal architecture selections, and promising pre-training
objectives. Our analysis highlights areas not yet covered in current research,
inviting further exploration to enhance the field's understanding.
Additionally, we address the challenges in comparing different models,
emphasizing the need for standardized data splitting and robust statistical
analysis.
\\ ( https://arxiv.org/abs/2404.03969 ,  4799kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03988
Date: Fri, 5 Apr 2024 09:50:00 GMT   (1968kb,D)

Title: Model Selection with Model Zoo via Graph Learning
Authors: Ziyu Li, Hilco van der Wilk, Danning Zhan, Megha Khosla, Alessandro
  Bozzon and Rihan Hai
Categories: cs.LG cs.SI
Comments: Accepted at 40th IEEE International Conference on Data Engineering
  (ICDE 2024)
\\
  Pre-trained deep learning (DL) models are increasingly accessible in public
repositories, i.e., model zoos. Given a new prediction task, finding the best
model to fine-tune can be computationally intensive and costly, especially when
the number of pre-trained models is large. Selecting the right pre-trained
models is crucial, yet complicated by the diversity of models from various
model families (like ResNet, Vit, Swin) and the hidden relationships between
models and datasets. Existing methods, which utilize basic information from
models and datasets to compute scores indicating model performance on target
datasets, overlook the intrinsic relationships, limiting their effectiveness in
model selection. In this study, we introduce TransferGraph, a novel framework
that reformulates model selection as a graph learning problem. TransferGraph
constructs a graph using extensive metadata extracted from models and datasets,
while capturing their inherent relationships. Through comprehensive experiments
across 16 real datasets, both images and texts, we demonstrate TransferGraph's
effectiveness in capturing essential model-dataset relationships, yielding up
to a 32% improvement in correlation between predicted performance and the
actual fine-tuning results compared to the state-of-the-art methods.
\\ ( https://arxiv.org/abs/2404.03988 ,  1968kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03992
Date: Fri, 5 Apr 2024 10:02:32 GMT   (1042kb)

Title: Rolling the dice for better deep learning performance: A study of
  randomness techniques in deep neural networks
Authors: Mohammed Ghaith Altarabichi, S{\l}awomir Nowaczyk, Sepideh Pashami,
  Peyman Sheikholharam Mashhadi, Julia Handl
Categories: cs.LG cs.AI cs.CV cs.NE
Journal-ref: Information Sciences, p.120500 (2024)
DOI: 10.1016/j.ins.2024.120500
\\
  This paper investigates how various randomization techniques impact Deep
Neural Networks (DNNs). Randomization, like weight noise and dropout, aids in
reducing overfitting and enhancing generalization, but their interactions are
poorly understood. The study categorizes randomness techniques into four types
and proposes new methods: adding noise to the loss function and random masking
of gradient updates. Using Particle Swarm Optimizer (PSO) for hyperparameter
optimization, it explores optimal configurations across MNIST, FASHION-MNIST,
CIFAR10, and CIFAR100 datasets. Over 30,000 configurations are evaluated,
revealing data augmentation and weight initialization randomness as main
performance contributors. Correlation analysis shows different optimizers
prefer distinct randomization types. The complete implementation and dataset
are available on GitHub.
\\ ( https://arxiv.org/abs/2404.03992 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03997
Date: Fri, 5 Apr 2024 10:19:04 GMT   (5793kb,D)

Title: Demonstration Guided Multi-Objective Reinforcement Learning
Authors: Junlin Lu, Patrick Mannion, Karl Mason
Categories: cs.LG cs.AI
\\
  Multi-objective reinforcement learning (MORL) is increasingly relevant due to
its resemblance to real-world scenarios requiring trade-offs between multiple
objectives. Catering to diverse user preferences, traditional reinforcement
learning faces amplified challenges in MORL. To address the difficulty of
training policies from scratch in MORL, we introduce demonstration-guided
multi-objective reinforcement learning (DG-MORL). This novel approach utilizes
prior demonstrations, aligns them with user preferences via corner weight
support, and incorporates a self-evolving mechanism to refine suboptimal
demonstrations. Our empirical studies demonstrate DG-MORL's superiority over
existing MORL algorithms, establishing its robustness and efficacy,
particularly under challenging conditions. We also provide an upper bound of
the algorithm's sample complexity.
\\ ( https://arxiv.org/abs/2404.03997 ,  5793kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04001
Date: Fri, 5 Apr 2024 10:25:26 GMT   (579kb,D)

Title: Approximate UMAP allows for high-rate online visualization of
  high-dimensional data streams
Authors: Peter Wassenaar, Pierre Guetschel, Michael Tangermann
Categories: cs.LG cs.AI cs.HC eess.SP
Comments: 6 pages, 3 figures, submitted to the Graz BCI conference 2024
ACM-class: I.5.3; I.5.3; J.4
\\
  In the BCI field, introspection and interpretation of brain signals are
desired for providing feedback or to guide rapid paradigm prototyping but are
challenging due to the high noise level and dimensionality of the signals. Deep
neural networks are often introspected by transforming their learned feature
representations into 2- or 3-dimensional subspace visualizations using
projection algorithms like Uniform Manifold Approximation and Projection
(UMAP). Unfortunately, these methods are computationally expensive, making the
projection of data streams in real-time a non-trivial task. In this study, we
introduce a novel variant of UMAP, called approximate UMAP (aUMAP). It aims at
generating rapid projections for real-time introspection. To study its
suitability for real-time projecting, we benchmark the methods against standard
UMAP and its neural network counterpart parametric UMAP. Our results show that
approximate UMAP delivers projections that replicate the projection space of
standard UMAP while decreasing projection speed by an order of magnitude and
maintaining the same training time.
\\ ( https://arxiv.org/abs/2404.04001 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04002
Date: Fri, 5 Apr 2024 10:25:40 GMT   (1741kb,D)

Title: Continual Learning with Weight Interpolation
Authors: J\k{e}drzej Kozal and Jan Wasilewski and Bartosz Krawczyk and
  Micha{\l} Wo\'zniak
Categories: cs.LG
\\
  Continual learning poses a fundamental challenge for modern machine learning
systems, requiring models to adapt to new tasks while retaining knowledge from
previous ones. Addressing this challenge necessitates the development of
efficient algorithms capable of learning from data streams and accumulating
knowledge over time. This paper proposes a novel approach to continual learning
utilizing the weight consolidation method. Our method, a simple yet powerful
technique, enhances robustness against catastrophic forgetting by interpolating
between old and new model weights after each novel task, effectively merging
two models to facilitate exploration of local minima emerging after arrival of
new concepts. Moreover, we demonstrate that our approach can complement
existing rehearsal-based replay approaches, improving their accuracy and
further mitigating the forgetting phenomenon. Additionally, our method provides
an intuitive mechanism for controlling the stability-plasticity trade-off.
Experimental results showcase the significant performance enhancement to
state-of-the-art experience replay algorithms the proposed weight consolidation
approach offers. Our algorithm can be downloaded from
https://github.com/jedrzejkozal/weight-interpolation-cl.
\\ ( https://arxiv.org/abs/2404.04002 ,  1741kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04057
Date: Fri, 5 Apr 2024 12:30:19 GMT   (46890kb,D)

Title: Score identity Distillation: Exponentially Fast Distillation of
  Pretrained Diffusion Models for One-Step Generation
Authors: Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, Hai Huang
Categories: cs.LG cs.AI cs.CV stat.ML
\\
  We introduce Score identity Distillation (SiD), an innovative data-free
method that distills the generative capabilities of pretrained diffusion models
into a single-step generator. SiD not only facilitates an exponentially fast
reduction in Fr\'echet inception distance (FID) during distillation but also
approaches or even exceeds the FID performance of the original teacher
diffusion models. By reformulating forward diffusion processes as semi-implicit
distributions, we leverage three score-related identities to create an
innovative loss mechanism. This mechanism achieves rapid FID reduction by
training the generator using its own synthesized images, eliminating the need
for real data or reverse-diffusion-based generation, all accomplished within
significantly shortened generation time. Upon evaluation across four benchmark
datasets, the SiD algorithm demonstrates high iteration efficiency during
distillation and surpasses competing distillation approaches, whether they are
one-step or few-step, data-free, or dependent on training data, in terms of
generation quality. This achievement not only redefines the benchmarks for
efficiency and effectiveness in diffusion distillation but also in the broader
field of diffusion-based generation. Our PyTorch implementation will be
publicly accessible on GitHub.
\\ ( https://arxiv.org/abs/2404.04057 ,  46890kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04062
Date: Fri, 5 Apr 2024 12:37:08 GMT   (2442kb,D)

Title: Derivative-free tree optimization for complex systems
Authors: Ye Wei, Bo Peng, Ruiwen Xie, Yangtao Chen, Yu Qin, Peng Wen, Stefan
  Bauer, Po-Yen Tung
Categories: cs.LG math.OC
Comments: 39 pages, 3 figures
\\
  A tremendous range of design tasks in materials, physics, and biology can be
formulated as finding the optimum of an objective function depending on many
parameters without knowing its closed-form expression or the derivative.
Traditional derivative-free optimization techniques often rely on strong
assumptions about objective functions, thereby failing at optimizing non-convex
systems beyond 100 dimensions. Here, we present a tree search method for
derivative-free optimization that enables accelerated optimal design of
high-dimensional complex systems. Specifically, we introduce stochastic tree
expansion, dynamic upper confidence bound, and short-range backpropagation
mechanism to evade local optimum, iteratively approximating the global optimum
using machine learning models. This development effectively confronts the
dimensionally challenging problems, achieving convergence to global optima
across various benchmark functions up to 2,000 dimensions, surpassing the
existing methods by 10- to 20-fold. Our method demonstrates wide applicability
to a wide range of real-world complex systems spanning materials, physics, and
biology, considerably outperforming state-of-the-art algorithms. This enables
efficient autonomous knowledge discovery and facilitates self-driving virtual
laboratories. Although we focus on problems within the realm of natural
science, the advancements in optimization techniques achieved herein are
applicable to a broader spectrum of challenges across all quantitative
disciplines.
\\ ( https://arxiv.org/abs/2404.04062 ,  2442kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04064
Date: Fri, 5 Apr 2024 12:41:53 GMT   (317kb,D)

Title: Fusing Dictionary Learning and Support Vector Machines for Unsupervised
  Anomaly Detection
Authors: Paul Irofti and Iulian-Andrei H\^iji and Andrei P\u{a}tra\c{s}cu and
  Nicolae Cleju
Categories: cs.LG cs.CR cs.NA math.NA
\\
  We study in this paper the improvement of one-class support vector machines
(OC-SVM) through sparse representation techniques for unsupervised anomaly
detection. As Dictionary Learning (DL) became recently a common analysis
technique that reveals hidden sparse patterns of data, our approach uses this
insight to endow unsupervised detection with more control on pattern finding
and dimensions. We introduce a new anomaly detection model that unifies the
OC-SVM and DL residual functions into a single composite objective,
subsequently solved through K-SVD-type iterative algorithms. A closed-form of
the alternating K-SVD iteration is explicitly derived for the new composite
model and practical implementable schemes are discussed. The standard DL model
is adapted for the Dictionary Pair Learning (DPL) context, where the usual
sparsity constraints are naturally eliminated. Finally, we extend both
objectives to the more general setting that allows the use of kernel functions.
The empirical convergence properties of the resulting algorithms are provided
and an in-depth analysis of their parametrization is performed while also
demonstrating their numerical performance in comparison with existing methods.
\\ ( https://arxiv.org/abs/2404.04064 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04070
Date: Fri, 5 Apr 2024 12:54:09 GMT   (2363kb,D)

Title: Hierarchical Neural Additive Models for Interpretable Demand Forecasts
Authors: Leif Feddersen, Catherine Cleophas
Categories: cs.LG cs.HC
\\
  Demand forecasts are the crucial basis for numerous business decisions,
ranging from inventory management to strategic facility planning. While machine
learning (ML) approaches offer accuracy gains, their interpretability and
acceptance are notoriously lacking. Addressing this dilemma, we introduce
Hierarchical Neural Additive Models for time series (HNAM). HNAM expands upon
Neural Additive Models (NAM) by introducing a time-series specific additive
model with a level and interacting covariate components.
  Covariate interactions are only allowed according to a user-specified
interaction hierarchy. For example, weekday effects may be estimated
independently of other covariates, whereas a holiday effect may depend on the
weekday and an additional promotion may depend on both former covariates that
are lower in the interaction hierarchy.
  Thereby, HNAM yields an intuitive forecasting interface in which analysts can
observe the contribution for each known covariate. We evaluate the proposed
approach and benchmark its performance against other state-of-the-art machine
learning and statistical models extensively on real-world retail data. The
results reveal that HNAM offers competitive prediction performance whilst
providing plausible explanations.
\\ ( https://arxiv.org/abs/2404.04070 ,  2363kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04102
Date: Fri, 5 Apr 2024 13:58:51 GMT   (1919kb,D)

Title: Robust Preference Optimization with Provable Noise Tolerance for LLMs
Authors: Xize Liang, Chao Chen, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng
  Wu, Jieping Ye
Categories: cs.LG cs.AI cs.CL
\\
  The preference alignment aims to enable large language models (LLMs) to
generate responses that conform to human values, which is essential for
developing general AI systems. Ranking-based methods -- a promising class of
alignment approaches -- learn human preferences from datasets containing
response pairs by optimizing the log-likelihood margins between preferred and
dis-preferred responses. However, due to the inherent differences in
annotators' preferences, ranking labels of comparisons for response pairs are
unavoidably noisy. This seriously hurts the reliability of existing
ranking-based methods. To address this problem, we propose a provably
noise-tolerant preference alignment method, namely RObust Preference
Optimization (ROPO). To the best of our knowledge, ROPO is the first preference
alignment method with noise-tolerance guarantees. The key idea of ROPO is to
dynamically assign conservative gradient weights to response pairs with high
label uncertainty, based on the log-likelihood margins between the responses.
By effectively suppressing the gradients of noisy samples, our weighting
strategy ensures that the expected risk has the same gradient direction
independent of the presence and proportion of noise. Experiments on three
open-ended text generation tasks with four base models ranging in size from
2.8B to 13B demonstrate that ROPO significantly outperforms existing
ranking-based methods.
\\ ( https://arxiv.org/abs/2404.04102 ,  1919kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04111
Date: Fri, 5 Apr 2024 14:08:57 GMT   (4032kb,D)

Title: The Unreasonable Effectiveness Of Early Discarding After One Epoch In
  Neural Network Hyperparameter Optimization
Authors: Romain Egele, Felix Mohr, Tom Viering, Prasanna Balaprakash
Categories: cs.LG
\\
  To reach high performance with deep learning, hyperparameter optimization
(HPO) is essential. This process is usually time-consuming due to costly
evaluations of neural networks. Early discarding techniques limit the resources
granted to unpromising candidates by observing the empirical learning curves
and canceling neural network training as soon as the lack of competitiveness of
a candidate becomes evident. Despite two decades of research, little is
understood about the trade-off between the aggressiveness of discarding and the
loss of predictive performance. Our paper studies this trade-off for several
commonly used discarding techniques such as successive halving and learning
curve extrapolation. Our surprising finding is that these commonly used
techniques offer minimal to no added value compared to the simple strategy of
discarding after a constant number of epochs of training. The chosen number of
epochs depends mostly on the available compute budget. We call this approach
i-Epoch (i being the constant number of epochs with which neural networks are
trained) and suggest to assess the quality of early discarding techniques by
comparing how their Pareto-Front (in consumed training epochs and predictive
performance) complement the Pareto-Front of i-Epoch.
\\ ( https://arxiv.org/abs/2404.04111 ,  4032kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04118
Date: Fri, 5 Apr 2024 14:18:06 GMT   (6290kb,D)

Title: GNNBENCH: Fair and Productive Benchmarking for Single-GPU GNN System
Authors: Yidong Gong and Pradeep Kumar
Categories: cs.LG cs.DC
\\
  We hypothesize that the absence of a standardized benchmark has allowed
several fundamental pitfalls in GNN System design and evaluation that the
community has overlooked. In this work, we propose GNNBench, a plug-and-play
benchmarking platform focused on system innovation. GNNBench presents a new
protocol to exchange their captive tensor data, supports custom classes in
System APIs, and allows automatic integration of the same system module to many
deep learning frameworks, such as PyTorch and TensorFlow. To demonstrate the
importance of such a benchmark framework, we integrated several GNN systems.
Our results show that integration with GNNBench helped us identify several
measurement issues that deserve attention from the community.
\\ ( https://arxiv.org/abs/2404.04118 ,  6290kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04126
Date: Fri, 5 Apr 2024 14:23:43 GMT   (173kb,D)

Title: Generalizable Temperature Nowcasting with Physics-Constrained RNNs for
  Predictive Maintenance of Wind Turbine Components
Authors: Johannes Exenberger, Matteo Di Salvo, Thomas Hirsch, Franz Wotawa,
  Gerald Schweiger
Categories: cs.LG cs.SY eess.SY
Comments: Published at ICLR 2024 Tackling Climate Change with Machine Learning
  Workshop
\\
  Machine learning plays an important role in the operation of current wind
energy production systems. One central application is predictive maintenance to
increase efficiency and lower electricity costs by reducing downtimes.
Integrating physics-based knowledge in neural networks to enforce their
physical plausibilty is a promising method to improve current approaches, but
incomplete system information often impedes their application in real world
scenarios. We describe a simple and efficient way for physics-constrained deep
learning-based predictive maintenance for wind turbine gearbox bearings with
partial system knowledge. The approach is based on temperature nowcasting
constrained by physics, where unknown system coefficients are treated as
learnable neural network parameters. Results show improved generalization
performance to unseen environments compared to a baseline neural network, which
is especially important in low data scenarios often encountered in real-world
applications.
\\ ( https://arxiv.org/abs/2404.04126 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04199
Date: Fri, 5 Apr 2024 16:13:35 GMT   (5716kb,D)

Title: Exploring Probabilistic Models for Semi-supervised Learning
Authors: Jianfeng Wang
Categories: cs.LG
Comments: PhD Thesis, University of Oxford
\\
  This thesis studies advanced probabilistic models, including both their
theoretical foundations and practical applications, for different
semi-supervised learning (SSL) tasks. The proposed probabilistic methods are
able to improve the safety of AI systems in real applications by providing
reliable uncertainty estimates quickly, and at the same time, achieve
competitive performance compared to their deterministic counterparts. The
experimental results indicate that the methods proposed in the thesis have
great value in safety-critical areas, such as the autonomous driving or medical
imaging analysis domain, and pave the way for the future discovery of highly
effective and efficient probabilistic approaches in the SSL sector.
\\ ( https://arxiv.org/abs/2404.04199 ,  5716kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04205
Date: Fri, 5 Apr 2024 16:30:45 GMT   (1091kb,D)

Title: Enhancing IoT Intelligence: A Transformer-based Reinforcement Learning
  Methodology
Authors: Gaith Rjoub, Saidul Islam, Jamal Bentahar, Mohammed Amin Almaiah, Rana
  Alrawashdeh
Categories: cs.LG cs.AI
\\
  The proliferation of the Internet of Things (IoT) has led to an explosion of
data generated by interconnected devices, presenting both opportunities and
challenges for intelligent decision-making in complex environments. Traditional
Reinforcement Learning (RL) approaches often struggle to fully harness this
data due to their limited ability to process and interpret the intricate
patterns and dependencies inherent in IoT applications. This paper introduces a
novel framework that integrates transformer architectures with Proximal Policy
Optimization (PPO) to address these challenges. By leveraging the
self-attention mechanism of transformers, our approach enhances RL agents'
capacity for understanding and acting within dynamic IoT environments, leading
to improved decision-making processes. We demonstrate the effectiveness of our
method across various IoT scenarios, from smart home automation to industrial
control systems, showing marked improvements in decision-making efficiency and
adaptability. Our contributions include a detailed exploration of the
transformer's role in processing heterogeneous IoT data, a comprehensive
evaluation of the framework's performance in diverse environments, and a
benchmark against traditional RL methods. The results indicate significant
advancements in enabling RL agents to navigate the complexities of IoT
ecosystems, highlighting the potential of our approach to revolutionize
intelligent automation and decision-making in the IoT landscape.
\\ ( https://arxiv.org/abs/2404.04205 ,  1091kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04224
Date: Fri, 5 Apr 2024 17:15:48 GMT   (1112kb,D)

Title: Active Causal Learning for Decoding Chemical Complexities with Targeted
  Interventions
Authors: Zachary R. Fox and Ayana Ghosh
Categories: cs.LG physics.chem-ph physics.data-an q-bio.BM
\\
  Predicting and enhancing inherent properties based on molecular structures is
paramount to design tasks in medicine, materials science, and environmental
management. Most of the current machine learning and deep learning approaches
have become standard for predictions, but they face challenges when applied
across different datasets due to reliance on correlations between molecular
representation and target properties. These approaches typically depend on
large datasets to capture the diversity within the chemical space, facilitating
a more accurate approximation, interpolation, or extrapolation of the chemical
behavior of molecules. In our research, we introduce an active learning
approach that discerns underlying cause-effect relationships through strategic
sampling with the use of a graph loss function. This method identifies the
smallest subset of the dataset capable of encoding the most information
representative of a much larger chemical space. The identified causal relations
are then leveraged to conduct systematic interventions, optimizing the design
task within a chemical space that the models have not encountered previously.
While our implementation focused on the QM9 quantum-chemical dataset for a
specific design task-finding molecules with a large dipole moment-our active
causal learning approach, driven by intelligent sampling and interventions,
holds potential for broader applications in molecular, materials design and
discovery.
\\ ( https://arxiv.org/abs/2404.04224 ,  1112kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04234
Date: Fri, 5 Apr 2024 17:29:47 GMT   (1218kb,D)

Title: player2vec: A Language Modeling Approach to Understand Player Behavior
  in Games
Authors: Tianze Wang, Maryam Honari-Jahromi, Styliani Katsarou, Olga Mikheeva,
  Theodoros Panagiotakopoulos, Sahar Asadi, Oleg Smirnov
Categories: cs.LG cs.AI cs.CL
\\
  Methods for learning latent user representations from historical behavior
logs have gained traction for recommendation tasks in e-commerce, content
streaming, and other settings. However, this area still remains relatively
underexplored in video and mobile gaming contexts. In this work, we present a
novel method for overcoming this limitation by extending a long-range
Transformer model from the natural language processing domain to player
behavior data. We discuss specifics of behavior tracking in games and propose
preprocessing and tokenization approaches by viewing in-game events in an
analogous way to words in sentences, thus enabling learning player
representations in a self-supervised manner in the absence of ground-truth
annotations. We experimentally demonstrate the efficacy of the proposed
approach in fitting the distribution of behavior events by evaluating intrinsic
language modeling metrics. Furthermore, we qualitatively analyze the emerging
structure of the learned embedding space and show its value for generating
insights into behavior patterns to inform downstream applications.
\\ ( https://arxiv.org/abs/2404.04234 ,  1218kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04240
Date: Fri, 5 Apr 2024 17:41:52 GMT   (2236kb,D)

Title: Dynamic Conditional Optimal Transport through Simulation-Free Flows
Authors: Gavin Kerrigan, Giosue Migliorini, Padhraic Smyth
Categories: cs.LG
\\
  We study the geometry of conditional optimal transport (COT) and prove a
dynamical formulation which generalizes the Benamou-Brenier Theorem. With these
tools, we propose a simulation-free flow-based method for conditional
generative modeling. Our method couples an arbitrary source distribution to a
specified target distribution through a triangular COT plan. We build on the
framework of flow matching to train a conditional generative model by
approximating the geodesic path of measures induced by this COT plan. Our
theory and methods are applicable in the infinite-dimensional setting, making
them well suited for inverse problems. Empirically, we demonstrate our proposed
method on two image-to-image translation tasks and an infinite-dimensional
Bayesian inverse problem.
\\ ( https://arxiv.org/abs/2404.04240 ,  2236kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04253
Date: Fri, 5 Apr 2024 17:58:37 GMT   (2850kb,D)

Title: Growing Q-Networks: Solving Continuous Control Tasks with Adaptive
  Control Resolution
Authors: Tim Seyde, Peter Werner, Wilko Schwarting, Markus Wulfmeier, Daniela
  Rus
Categories: cs.LG cs.AI cs.RO
\\
  Recent reinforcement learning approaches have shown surprisingly strong
capabilities of bang-bang policies for solving continuous control benchmarks.
The underlying coarse action space discretizations often yield favourable
exploration characteristics while final performance does not visibly suffer in
the absence of action penalization in line with optimal control theory. In
robotics applications, smooth control signals are commonly preferred to reduce
system wear and energy efficiency, but action costs can be detrimental to
exploration during early training. In this work, we aim to bridge this
performance gap by growing discrete action spaces from coarse to fine control
resolution, taking advantage of recent results in decoupled Q-learning to scale
our approach to high-dimensional action spaces up to dim(A) = 38. Our work
indicates that an adaptive control resolution in combination with value
decomposition yields simple critic-only algorithms that yield surprisingly
strong performance on continuous control tasks.
\\ ( https://arxiv.org/abs/2404.04253 ,  2850kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.03662 (*cross-listing*)
Date: Thu, 15 Feb 2024 06:19:02 GMT   (446kb,D)

Title: X-lifecycle Learning for Cloud Incident Management using LLMs
Authors: Drishti Goel, Fiza Husain, Aditya Singh, Supriyo Ghosh, Anjaly
  Parayil, Chetan Bansal, Xuchao Zhang and Saravan Rajmohan
Categories: cs.NI cs.AI
\\
  Incident management for large cloud services is a complex and tedious process
and requires significant amount of manual efforts from on-call engineers
(OCEs). OCEs typically leverage data from different stages of the software
development lifecycle [SDLC] (e.g., codes, configuration, monitor data, service
properties, service dependencies, trouble-shooting documents, etc.) to generate
insights for detection, root causing and mitigating of incidents. Recent
advancements in large language models [LLMs] (e.g., ChatGPT, GPT-4, Gemini)
created opportunities to automatically generate contextual recommendations to
the OCEs assisting them to quickly identify and mitigate critical issues.
However, existing research typically takes a silo-ed view for solving a certain
task in incident management by leveraging data from a single stage of SDLC. In
this paper, we demonstrate that augmenting additional contextual data from
different stages of SDLC improves the performance of two critically important
and practically challenging tasks: (1) automatically generating root cause
recommendations for dependency failure related incidents, and (2) identifying
ontology of service monitors used for automatically detecting incidents. By
leveraging 353 incident and 260 monitor dataset from Microsoft, we demonstrate
that augmenting contextual information from different stages of the SDLC
improves the performance over State-of-The-Art methods.
\\ ( https://arxiv.org/abs/2404.03662 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03664 (*cross-listing*)
Date: Fri, 16 Feb 2024 10:56:15 GMT   (8250kb,D)

Title: LLMs in the Heart of Differential Testing: A Case Study on a Medical
  Rule Engine
Authors: Erblin Isaku, Christoph Laaber, Hassan Sartaj, Shaukat Ali, Thomas
  Schwitalla, Jan F. Nyg{\aa}rd
Categories: cs.SE cs.AI
Comments: 12 pages, 6 figures, 4 tables, 1 listing
\\
  The Cancer Registry of Norway (CRN) uses an automated cancer registration
support system (CaReSS) to support core cancer registry activities, i.e, data
capture, data curation, and producing data products and statistics for various
stakeholders. GURI is a core component of CaReSS, which is responsible for
validating incoming data with medical rules. Such medical rules are manually
implemented by medical experts based on medical standards, regulations, and
research. Since large language models (LLMs) have been trained on a large
amount of public information, including these documents, they can be employed
to generate tests for GURI. Thus, we propose an LLM-based test generation and
differential testing approach (LLMeDiff) to test GURI. We experimented with
four different LLMs, two medical rule engine implementations, and 58 real
medical rules to investigate the hallucination, success, time efficiency, and
robustness of the LLMs to generate tests, and these tests' ability to find
potential issues in GURI. Our results showed that GPT-3.5 hallucinates the
least, is the most successful, and is generally the most robust; however, it
has the worst time efficiency. Our differential testing revealed 22 medical
rules where implementation inconsistencies were discovered (e.g., regarding
handling rule versions). Finally, we provide insights for practitioners and
researchers based on the results.
\\ ( https://arxiv.org/abs/2404.03664 ,  8250kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03665 (*cross-listing*)
Date: Fri, 16 Feb 2024 16:46:10 GMT   (292kb)

Title: Serial Parallel Reliability Redundancy Allocation Optimization for
  Energy Efficient and Fault Tolerant Cloud Computing
Authors: Gutha Jaya Krishna
Categories: cs.DC cs.AI
Comments: 5 Pages, 1 Figure, 2 Tables
MSC-class: 68W50
ACM-class: I.2.11
\\
  Serial-parallel redundancy is a reliable way to ensure service and systems
will be available in cloud computing. That method involves making copies of the
same system or program, with only one remaining active. When an error occurs,
the inactive copy can step in as a backup right away, this provides continuous
performance and uninterrupted operation. This approach is called parallel
redundancy, otherwise known as active-active redundancy, and its exceptional
when it comes to strategy. It creates duplicates of a system or service that
are all running at once. By doing this fault tolerance increases since if one
copy fails, the workload can be distributed across any replica thats
functioning properly. Reliability allocation depends on features in a system
and the availability and fault tolerance you want from it. Serial redundancy or
parallel redundancies can be applied to increase the dependability of systems
and services. To demonstrate how well this concept works, we looked into fixed
serial parallel reliability redundancy allocation issues followed by using an
innovative hybrid optimization technique to find the best possible allocation
for peak dependability. We then measured our findings against other research.
\\ ( https://arxiv.org/abs/2404.03665 ,  292kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03673 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:40:22 GMT   (3497kb,D)

Title: RL for Consistency Models: Faster Reward Guided Text-to-Image Generation
Authors: Owen Oertell, Jonathan D. Chang, Yiyi Zhang, Kiant\'e Brantley, Wen
  Sun
Categories: cs.CV cs.AI cs.LG
Comments: 17 pages, 8 figures, 1 table
\\
  Reinforcement learning (RL) has improved guided image generation with
diffusion models by directly optimizing rewards that capture image quality,
aesthetics, and instruction following capabilities. However, the resulting
generative policies inherit the same iterative sampling process of diffusion
models that causes slow generation. To overcome this limitation, consistency
models proposed learning a new class of generative models that directly map
noise to data, resulting in a model that can generate an image in as few as one
sampling iteration. In this work, to optimize text-to-image generative models
for task specific rewards and enable fast training and inference, we propose a
framework for fine-tuning consistency models via RL. Our framework, called
Reinforcement Learning for Consistency Model (RLCM), frames the iterative
inference process of a consistency model as an RL procedure. RLCM improves upon
RL fine-tuned diffusion models on text-to-image generation capabilities and
trades computation during inference time for sample quality. Experimentally, we
show that RLCM can adapt text-to-image consistency models to objectives that
are challenging to express with prompting, such as image compressibility, and
those derived from human feedback, such as aesthetic quality. Comparing to RL
finetuned diffusion models, RLCM trains significantly faster, improves the
quality of the generation measured under the reward objectives, and speeds up
the inference procedure by generating high quality images with as few as two
inference steps. Our code is available at https://rlcm.owenoertell.com
\\ ( https://arxiv.org/abs/2404.03673 ,  3497kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03685 (*cross-listing*)
Date: Mon, 1 Apr 2024 13:12:27 GMT   (330kb,D)

Title: Cooperative Evolutionary Pressure and Diminishing Returns Might Explain
  the Fermi Paradox: On What Super-AIs Are Like
Authors: Daniel Vallstrom
Categories: physics.soc-ph cs.AI
Comments: 22 pages, 1 figure
\\
  With an evolutionary approach, the basis of morality can be explained as
adaptations to problems of cooperation. With 'evolution' taken in a broad
sense, evolving AIs that satisfy the conditions for evolution to apply will be
subject to the same cooperative evolutionary pressure as biological entities.
Here the adaptiveness of increased cooperation as material safety and wealth
increase is discussed -- for humans, for other societies, and for AIs.
Diminishing beneficial returns from increased access to material resources also
suggests the possibility that, on the whole, there will be no incentive to for
instance colonize entire galaxies, thus providing a possible explanation of the
Fermi paradox, wondering where everybody is. It is further argued that old
societies could engender, give way to, super-AIs, since it is likely that
super-AIs are feasible, and fitter. Closing is an aside on effective ways for
morals and goals to affect life and society, emphasizing environments,
cultures, and laws, and exemplified by how to eat.
  Appended are an algorithm for colonizing for example a galaxy quickly, models
of the evolution of cooperation and fairness under diminishing returns, and
software for simulating signaling development. It is also noted that there can
be no exponential colonization or reproduction, for mathematical reasons, as
each entity takes up a certain amount of space.
\\ ( https://arxiv.org/abs/2404.03685 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03703 (*cross-listing*)
Date: Thu, 4 Apr 2024 07:49:39 GMT   (343kb,D)

Title: Mitigating analytical variability in fMRI results with style transfer
Authors: Elodie Germani (EMPENN, LACODAM), Elisa Fromont (LACODAM), Camille
  Maumet (EMPENN)
Categories: eess.IV cs.AI cs.CV cs.LG
\\
  We propose a novel approach to improve the reproducibility of neuroimaging
results by converting statistic maps across different functional MRI pipelines.
We make the assumption that pipelines can be considered as a style component of
data and propose to use different generative models, among which, Diffusion
Models (DM) to convert data between pipelines. We design a new DM-based
unsupervised multi-domain image-to-image transition framework and constrain the
generation of 3D fMRI statistic maps using the latent space of an auxiliary
classifier that distinguishes statistic maps from different pipelines. We
extend traditional sampling techniques used in DM to improve the transition
performance. Our experiments demonstrate that our proposed methods are
successful: pipelines can indeed be transferred, providing an important source
of data augmentation for future medical studies.
\\ ( https://arxiv.org/abs/2404.03703 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03709 (*cross-listing*)
Date: Thu, 4 Apr 2024 11:51:26 GMT   (5kb)

Title: Proceedings 12th International Workshop on Theorem proving components
  for Educational software
Authors: Julien Narboux (University of Strasbourg, France), Walther Neuper
  (JKU, Johannes Kepler University), Pedro Quaresma (University of Coimbra,
  Portugal)
Categories: cs.LO cs.AI cs.LG
Journal-ref: EPTCS 400, 2024
DOI: 10.4204/EPTCS.400
\\
  The ThEdu series pursues the smooth transition from an intuitive way of doing
mathematics at secondary school to a more formal approach to the subject in
STEM education, while favouring software support for this transition by
exploiting the power of theorem-proving technologies. What follows is a brief
description of how the present volume contributes to this enterprise.
  The 12th International Workshop on Theorem Proving Components for Educational
Software(ThEdu'23), was a satellite event of the 29th international Conference
on Automated Deduction (CADE 2023), July 1-4, 2023, Rome, Italy. ThEdu'23 was
very successful, with one invited talk, by Yves Bertot (Inria, France), "The
challenges of using Type Theory to teach Mathematics", and seven regular
contributions. An open call for papers was then issued, to which eight
contributions were submitted. Seven submissions have been accepted by our
reviewers, who jointly produced at least three careful reports on each of the
contributions. The resulting revised papers are collected in the present
volume.
  We, the volume editors, hope that this collection of papers will further
promote the development of theorem-proving based software, and that it will
allow to improve the mutual understanding between computer scientists,
mathematicians and stakeholders in education.
  PC Chairs:Julien Narboux (University of Strasbourg, France); Walther Neuper
(JKU, Johannes Kepler University, Linz, Austria); Pedro Quaresma (University of
Coimbra, Portugal)
\\ ( https://arxiv.org/abs/2404.03709 ,  5kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03714 (*cross-listing*)
Date: Thu, 4 Apr 2024 17:53:08 GMT   (1376kb,D)

Title: SpikeExplorer: hardware-oriented Design Space Exploration for Spiking
  Neural Networks on FPGA
Authors: Dario Padovano, Alessio Carpegna, Alessandro Savino and Stefano Di
  Carlo
Categories: cs.NE cs.AI
\\
  One of today's main concerns is to bring Artificial Intelligence power to
embedded systems for edge applications. The hardware resources and power
consumption required by state-of-the-art models are incompatible with the
constrained environments observed in edge systems, such as IoT nodes and
wearable devices. Spiking Neural Networks (SNNs) can represent a solution in
this sense: inspired by neuroscience, they reach unparalleled power and
resource efficiency when run on dedicated hardware accelerators. However, when
designing such accelerators, the amount of choices that can be taken is huge.
This paper presents SpikExplorer, a modular and flexible Python tool for
hardware-oriented Automatic Design Space Exploration to automate the
configuration of FPGA accelerators for SNNs. Using Bayesian optimizations,
SpikerExplorer enables hardware-centric multi-objective optimization,
supporting factors such as accuracy, area, latency, power, and various
combinations during the exploration process. The tool searches the optimal
network architecture, neuron model, and internal and training parameters,
trying to reach the desired constraints imposed by the user. It allows for a
straightforward network configuration, providing the full set of explored
points for the user to pick the trade-off that best fits the needs. The
potential of SpikExplorer is showcased using three benchmark datasets. It
reaches 95.8% accuracy on the MNIST dataset, with a power consumption of
180mW/image and a latency of 0.12 ms/image, making it a powerful tool for
automatically optimizing SNNs.
\\ ( https://arxiv.org/abs/2404.03714 ,  1376kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03745 (*cross-listing*)
Date: Thu, 4 Apr 2024 18:34:32 GMT   (664kb,D)

Title: Fakes of Varying Shades: How Warning Affects Human Perception and
  Engagement Regarding LLM Hallucinations
Authors: Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee
Categories: cs.HC cs.AI cs.CL
\\
  The widespread adoption and transformative effects of large language models
(LLMs) have sparked concerns regarding their capacity to produce inaccurate and
fictitious content, referred to as `hallucinations'. Given the potential risks
associated with hallucinations, humans should be able to identify them. This
research aims to understand the human perception of LLM hallucinations by
systematically varying the degree of hallucination (genuine, minor
hallucination, major hallucination) and examining its interaction with warning
(i.e., a warning of potential inaccuracies: absent vs. present). Participants
(N=419) from Prolific rated the perceived accuracy and engaged with content
(e.g., like, dislike, share) in a Q/A format. Results indicate that humans rank
content as truthful in the order genuine > minor hallucination > major
hallucination and user engagement behaviors mirror this pattern. More
importantly, we observed that warning improves hallucination detection without
significantly affecting the perceived truthfulness of genuine content. We
conclude by offering insights for future tools to aid human detection of
hallucinations.
\\ ( https://arxiv.org/abs/2404.03745 ,  664kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03746 (*cross-listing*)
Date: Thu, 4 Apr 2024 18:35:25 GMT   (390kb,D)

Title: GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query
  Reformulation
Authors: Kaustubh Dhole and Eugene Agichtein
Categories: cs.IR cs.AI cs.CL
Comments: Accepted at ECIR 2024
DOI: 10.1007/978-3-031-56063-7_24
\\
  Query Reformulation(QR) is a set of techniques used to transform a user's
original search query to a text that better aligns with the user's intent and
improves their search experience. Recently, zero-shot QR has been shown to be a
promising approach due to its ability to exploit knowledge inherent in large
language models. By taking inspiration from the success of ensemble prompting
strategies which have benefited many tasks, we investigate if they can help
improve query reformulation. In this context, we propose an ensemble based
prompting technique, GenQREnsemble which leverages paraphrases of a zero-shot
instruction to generate multiple sets of keywords ultimately improving
retrieval performance. We further introduce its post-retrieval variant,
GenQREnsembleRF to incorporate pseudo relevant feedback. On evaluations over
four IR benchmarks, we find that GenQREnsemble generates better reformulations
with relative nDCG@10 improvements up to 18% and MAP improvements upto 24% over
the previous zero-shot state-of-art. On the MSMarco Passage Ranking task,
GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback,
and 9% nDCG@10 using relevant feedback documents.
\\ ( https://arxiv.org/abs/2404.03746 ,  390kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03753 (*cross-listing*)
Date: Thu, 4 Apr 2024 18:44:33 GMT   (635kb,D)

Title: A Reinforcement Learning based Reset Policy for CDCL SAT Solvers
Authors: Chunxiao Li, Charlie Liu, Jonathan Chung, Zhengyang (John) Lu, Piyush
  Jha, Vijay Ganesh
Categories: cs.LO cs.AI cs.LG
\\
  Restart policy is an important technique used in modern Conflict-Driven
Clause Learning (CDCL) solvers, wherein some parts of the solver state are
erased at certain intervals during the run of the solver. In most solvers,
variable activities are preserved across restart boundaries, resulting in
solvers continuing to search parts of the assignment tree that are not far from
the one immediately prior to a restart. To enable the solver to search possibly
"distant" parts of the assignment tree, we study the effect of resets, a
variant of restarts which not only erases the assignment trail, but also
randomizes the activity scores of the variables of the input formula after
reset, thus potentially enabling a better global exploration of the search
space.
  In this paper, we model the problem of whether to trigger reset as a
multi-armed bandit (MAB) problem, and propose two reinforcement learning (RL)
based adaptive reset policies using the Upper Confidence Bound (UCB) and
Thompson sampling algorithms. These two algorithms balance the
exploration-exploitation tradeoff by adaptively choosing arms (reset vs. no
reset) based on their estimated rewards during the solver's run. We implement
our reset policies in four baseline SOTA CDCL solvers and compare the baselines
against the reset versions on Satcoin benchmarks and SAT Competition instances.
Our results show that RL-based reset versions outperform the corresponding
baseline solvers on both Satcoin and the SAT competition instances, suggesting
that our RL policy helps to dynamically and profitably adapt the reset
frequency for any given input instance. We also introduce the concept of a
partial reset, where at least a constant number of variable activities are
retained across reset boundaries. Building on previous results, we show that
there is an exponential separation between O(1) vs. $\Omega(n)$-length partial
resets.
\\ ( https://arxiv.org/abs/2404.03753 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03789 (*cross-listing*)
Date: Thu, 4 Apr 2024 20:04:12 GMT   (6916kb,D)

Title: Quantifying Uncertainty in Motion Prediction with Variational Bayesian
  Mixture
Authors: Juanwu Lu, Can Cui, Yunsheng Ma, Aniket Bera, Ziran Wang
Categories: cs.CV cs.AI
Comments: Accepted at CVPR 2024
\\
  Safety and robustness are crucial factors in developing trustworthy
autonomous vehicles. One essential aspect of addressing these factors is to
equip vehicles with the capability to predict future trajectories for all
moving objects in the surroundings and quantify prediction uncertainties. In
this paper, we propose the Sequential Neural Variational Agent (SeNeVA), a
generative model that describes the distribution of future trajectories for a
single moving object. Our approach can distinguish Out-of-Distribution data
while quantifying uncertainty and achieving competitive performance compared to
state-of-the-art methods on the Argoverse 2 and INTERACTION datasets.
Specifically, a 0.446 meters minimum Final Displacement Error, a 0.203 meters
minimum Average Displacement Error, and a 5.35% Miss Rate are achieved on the
INTERACTION test set. Extensive qualitative and quantitative analysis is also
provided to evaluate the proposed model. Our open-source code is available at
https://github.com/PurdueDigitalTwin/seneva.
\\ ( https://arxiv.org/abs/2404.03789 ,  6916kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03799 (*cross-listing*)
Date: Thu, 4 Apr 2024 20:42:49 GMT   (17350kb,D)

Title: Language-Guided Instance-Aware Domain-Adaptive Panoptic Segmentation
Authors: Elham Amin Mansour, Ozan Unal, Suman Saha, Benjamin Bejar, Luc Van
  Gool
Categories: cs.CV cs.AI
\\
  The increasing relevance of panoptic segmentation is tied to the advancements
in autonomous driving and AR/VR applications. However, the deployment of such
models has been limited due to the expensive nature of dense data annotation,
giving rise to unsupervised domain adaptation (UDA). A key challenge in
panoptic UDA is reducing the domain gap between a labeled source and an
unlabeled target domain while harmonizing the subtasks of semantic and instance
segmentation to limit catastrophic interference. While considerable progress
has been achieved, existing approaches mainly focus on the adaptation of
semantic segmentation. In this work, we focus on incorporating instance-level
adaptation via a novel instance-aware cross-domain mixing strategy IMix. IMix
significantly enhances the panoptic quality by improving instance segmentation
performance. Specifically, we propose inserting high-confidence predicted
instances from the target domain onto source images, retaining the
exhaustiveness of the resulting pseudo-labels while reducing the injected
confirmation bias. Nevertheless, such an enhancement comes at the cost of
degraded semantic performance, attributed to catastrophic forgetting. To
mitigate this issue, we regularize our semantic branch by employing CLIP-based
domain alignment (CDA), exploiting the domain-robustness of natural language
prompts. Finally, we present an end-to-end model incorporating these two
mechanisms called LIDAPS, achieving state-of-the-art results on all popular
panoptic UDA benchmarks.
\\ ( https://arxiv.org/abs/2404.03799 ,  17350kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03836 (*cross-listing*)
Date: Thu, 4 Apr 2024 23:38:45 GMT   (24177kb,D)

Title: PARIS3D: Reasoning-based 3D Part Segmentation Using Large Multimodal
  Model
Authors: Amrin Kareem, Jean Lahoud, and Hisham Cholakkal
Categories: cs.CV cs.AI
Comments: 14 pages
\\
  Recent advancements in 3D perception systems have significantly improved
their ability to perform visual recognition tasks such as segmentation.
However, these systems still heavily rely on explicit human instruction to
identify target objects or categories, lacking the capability to actively
reason and comprehend implicit user intentions. We introduce a novel
segmentation task known as reasoning part segmentation for 3D objects, aiming
to output a segmentation mask based on complex and implicit textual queries
about specific parts of a 3D object. To facilitate evaluation and benchmarking,
we present a large 3D dataset comprising over 60k instructions paired with
corresponding ground-truth part segmentation annotations specifically curated
for reasoning-based 3D part segmentation. We propose a model that is capable of
segmenting parts of 3D objects based on implicit textual queries and generating
natural language explanations corresponding to 3D object segmentation requests.
Experiments show that our method achieves competitive performance to models
that use explicit queries, with the additional abilities to identify part
concepts, reason about them, and complement them with world knowledge. Our
source code, dataset, and trained models are available at
https://github.com/AmrinKareem/PARIS3D.
\\ ( https://arxiv.org/abs/2404.03836 ,  24177kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03838 (*cross-listing*)
Date: Thu, 4 Apr 2024 23:50:18 GMT   (1389kb,D)

Title: A Block-Coordinate Descent EMO Algorithm: Theoretical and Empirical
  Analysis
Authors: Benjamin Doerr, Joshua Knowles, Aneta Neumann, Frank Neumann
Categories: cs.NE cs.AI
Comments: Accepted at GECCO 2024
\\
  We consider whether conditions exist under which block-coordinate descent is
asymptotically efficient in evolutionary multi-objective optimization,
addressing an open problem. Block-coordinate descent, where an optimization
problem is decomposed into $k$ blocks of decision variables and each of the
blocks is optimized (with the others fixed) in a sequence, is a technique used
in some large-scale optimization problems such as airline scheduling, however
its use in multi-objective optimization is less studied. We propose a
block-coordinate version of GSEMO and compare its running time to the standard
GSEMO algorithm. Theoretical and empirical results on a bi-objective test
function, a variant of LOTZ, serve to demonstrate the existence of cases where
block-coordinate descent is faster. The result may yield wider insights into
this class of algorithms.
\\ ( https://arxiv.org/abs/2404.03838 ,  1389kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03891 (*cross-listing*)
Date: Fri, 5 Apr 2024 04:58:34 GMT   (2201kb,D)

Title: Can only LLMs do Reasoning?: Potential of Small Language Models in Task
  Planning
Authors: Gawon Choi, Hyemin Ahn
Categories: cs.RO cs.AI cs.LG
Comments: 8 pages, 11 figures
\\
  In robotics, the use of Large Language Models (LLMs) is becoming prevalent,
especially for understanding human commands. In particular, LLMs are utilized
as domain-agnostic task planners for high-level human commands. LLMs are
capable of Chain-of-Thought (CoT) reasoning, and this allows LLMs to be task
planners. However, we need to consider that modern robots still struggle to
perform complex actions, and the domains where robots can be deployed are
limited in practice. This leads us to pose a question: If small LMs can be
trained to reason in chains within a single domain, would even small LMs be
good task planners for the robots? To train smaller LMs to reason in chains, we
build `COmmand-STeps datasets' (COST) consisting of high-level commands along
with corresponding actionable low-level steps, via LLMs. We release not only
our datasets but also the prompt templates used to generate them, to allow
anyone to build datasets for their domain. We compare GPT3.5 and GPT4 with the
finetuned GPT2 for task domains, in tabletop and kitchen environments, and the
result shows that GPT2-medium is comparable to GPT3.5 for task planning in a
specific domain. Our dataset, code, and more output samples can be found in
https://github.com/Gawon-Choi/small-LMs-Task-Planning
\\ ( https://arxiv.org/abs/2404.03891 ,  2201kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03892 (*cross-listing*)
Date: Fri, 5 Apr 2024 05:00:21 GMT   (2914kb,D)

Title: Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and
  Integration of Convolutional Neural Networks and Explainable AI
Authors: Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir
Categories: cs.CV cs.AI cs.LG eess.IV
\\
  The study introduces an integrated framework combining Convolutional Neural
Networks (CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced
diagnosis of breast cancer using the CBIS-DDSM dataset. Utilizing a fine-tuned
ResNet50 architecture, our investigation not only provides effective
differentiation of mammographic images into benign and malignant categories but
also addresses the opaque "black-box" nature of deep learning models by
employing XAI methodologies, namely Grad-CAM, LIME, and SHAP, to interpret CNN
decision-making processes for healthcare professionals. Our methodology
encompasses an elaborate data preprocessing pipeline and advanced data
augmentation techniques to counteract dataset limitations, and transfer
learning using pre-trained networks, such as VGG-16, DenseNet and ResNet was
employed. A focal point of our study is the evaluation of XAI's effectiveness
in interpreting model predictions, highlighted by utilising the Hausdorff
measure to assess the alignment between AI-generated explanations and expert
annotations quantitatively. This approach plays a critical role for XAI in
promoting trustworthiness and ethical fairness in AI-assisted diagnostics. The
findings from our research illustrate the effective collaboration between CNNs
and XAI in advancing diagnostic methods for breast cancer, thereby facilitating
a more seamless integration of advanced AI technologies within clinical
settings. By enhancing the interpretability of AI-driven decisions, this work
lays the groundwork for improved collaboration between AI systems and medical
practitioners, ultimately enriching patient care. Furthermore, the implications
of our research extend well beyond the current methodologies, advocating for
subsequent inquiries into the integration of multimodal data and the refinement
of AI explanations to satisfy the needs of clinical practice.
\\ ( https://arxiv.org/abs/2404.03892 ,  2914kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03894 (*cross-listing*)
Date: Fri, 5 Apr 2024 05:03:39 GMT   (8449kb,D)

Title: Holon: a cybernetic interface for bio-semiotics
Authors: Jon McCormack and Elliott Wilson
Categories: cs.SD cs.AI cs.MA eess.AS
Comments: Paper accepted at ISEA 24, The 29th International Symposium on
  Electronic Art, Brisbane, Australia, 21-29 June 2024
ACM-class: I.2.11; J.5
\\
  This paper presents an interactive artwork, "Holon", a collection of 130
autonomous, cybernetic organisms that listen and make sound in collaboration
with the natural environment. The work was developed for installation on water
at a heritage-listed dock in Melbourne, Australia. Conceptual issues informing
the work are presented, along with a detailed technical overview of the
implementation. Individual holons are of three types, inspired by biological
models of animal communication: composer/generators, collector/critics and
disruptors. Collectively, Holon integrates and occupies elements of the
acoustic spectrum in collaboration with human and non-human agents.
\\ ( https://arxiv.org/abs/2404.03894 ,  8449kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03900 (*cross-listing*)
Date: Fri, 5 Apr 2024 05:46:20 GMT   (4617kb,D)

Title: Nonparametric Modern Hopfield Models
Authors: Jerry Yao-Chieh Hu, Bo-Yu Chen, Dennis Wu, Feng Ruan, Han Liu
Categories: stat.ML cs.AI cs.LG cs.NE
Comments: 59 pages; Code available at
  https://github.com/MAGICS-LAB/NonparametricHopfield
\\
  We present a nonparametric construction for deep learning compatible modern
Hopfield models and utilize this framework to debut an efficient variant. Our
key contribution stems from interpreting the memory storage and retrieval
processes in modern Hopfield models as a nonparametric regression problem
subject to a set of query-memory pairs. Crucially, our framework not only
recovers the known results from the original dense modern Hopfield model but
also fills the void in the literature regarding efficient modern Hopfield
models, by introducing \textit{sparse-structured} modern Hopfield models with
sub-quadratic complexity. We establish that this sparse model inherits the
appealing theoretical properties of its dense analogue -- connection with
transformer attention, fixed point convergence and exponential memory capacity
-- even without knowing details of the Hopfield energy function. Additionally,
we showcase the versatility of our framework by constructing a family of modern
Hopfield models as extensions, including linear, random masked, top-$K$ and
positive random feature modern Hopfield models. Empirically, we validate the
efficacy of our framework in both synthetic and realistic settings.
\\ ( https://arxiv.org/abs/2404.03900 ,  4617kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03913 (*cross-listing*)
Date: Fri, 5 Apr 2024 06:41:27 GMT   (26991kb,D)

Title: Concept Weaver: Enabling Multi-Concept Fusion in Text-to-Image Models
Authors: Gihyun Kwon, Simon Jenni, Dingzeyu Li, Joon-Young Lee, Jong Chul Ye,
  Fabian Caba Heilbron
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024
\\
  While there has been significant progress in customizing text-to-image
generation models, generating images that combine multiple personalized
concepts remains challenging. In this work, we introduce Concept Weaver, a
method for composing customized text-to-image diffusion models at inference
time. Specifically, the method breaks the process into two steps: creating a
template image aligned with the semantics of input prompts, and then
personalizing the template using a concept fusion strategy. The fusion strategy
incorporates the appearance of the target concepts into the template image
while retaining its structural details. The results indicate that our method
can generate multiple custom concepts with higher identity fidelity compared to
alternative approaches. Furthermore, the method is shown to seamlessly handle
more than two concepts and closely follow the semantic meaning of the input
prompt without blending appearances across different subjects.
\\ ( https://arxiv.org/abs/2404.03913 ,  26991kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03995 (*cross-listing*)
Date: Fri, 5 Apr 2024 10:11:08 GMT   (3688kb,D)

Title: Balancing Progress and Responsibility: A Synthesis of Sustainability
  Trade-Offs of AI-Based Systems
Authors: Apoorva Nalini Pradeep Kumar, Justus Bogner, Markus Funke, Patricia
  Lago
Categories: cs.SE cs.AI
Comments: Accepted for publication at the 8th International Workshop on Green
  and Sustainable Software (GREENS'24), collocated with ICSA'24
\\
  Recent advances in artificial intelligence (AI) capabilities have increased
the eagerness of companies to integrate AI into software systems. While AI can
be used to have a positive impact on several dimensions of sustainability, this
is often overshadowed by its potential negative influence. While many studies
have explored sustainability factors in isolation, there is insufficient
holistic coverage of potential sustainability benefits or costs that
practitioners need to consider during decision-making for AI adoption. We
therefore aim to synthesize trade-offs related to sustainability in the context
of integrating AI into software systems. We want to make the sustainability
benefits and costs of integrating AI more transparent and accessible for
practitioners.
  The study was conducted in collaboration with a Dutch financial organization.
We first performed a rapid review that led to the inclusion of 151 research
papers. Afterward, we conducted six semi-structured interviews to enrich the
data with industry perspectives. The combined results showcase the potential
sustainability benefits and costs of integrating AI. The labels synthesized
from the review regarding potential sustainability benefits were clustered into
16 themes, with "energy management" being the most frequently mentioned one. 11
themes were identified in the interviews, with the top mentioned theme being
"employee wellbeing". Regarding sustainability costs, the review discovered
seven themes, with "deployment issues" being the most popular one, followed by
"ethics & society". "Environmental issues" was the top theme from the
interviews. Our results provide valuable insights to organizations and
practitioners for understanding the potential sustainability implications of
adopting AI.
\\ ( https://arxiv.org/abs/2404.03995 ,  3688kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03996 (*cross-listing*)
Date: Fri, 5 Apr 2024 10:15:24 GMT   (2855kb)

Title: Fast Genetic Algorithm for feature selection -- A qualitative
  approximation approach
Authors: Mohammed Ghaith Altarabichi, S{\l}awomir Nowaczyk, Sepideh Pashami,
  Peyman Sheikholharam Mashhadi
Categories: cs.NE cs.AI cs.LG
Journal-ref: Expert Systems with Applications, 211, p.118528 (2023)
DOI: 10.1016/j.eswa.2022.118528
\\
  Evolutionary Algorithms (EAs) are often challenging to apply in real-world
settings since evolutionary computations involve a large number of evaluations
of a typically expensive fitness function. For example, an evaluation could
involve training a new machine learning model. An approximation (also known as
meta-model or a surrogate) of the true function can be used in such
applications to alleviate the computation cost. In this paper, we propose a
two-stage surrogate-assisted evolutionary approach to address the computational
issues arising from using Genetic Algorithm (GA) for feature selection in a
wrapper setting for large datasets. We define 'Approximation Usefulness' to
capture the necessary conditions to ensure correctness of the EA computations
when an approximation is used. Based on this definition, we propose a procedure
to construct a lightweight qualitative meta-model by the active selection of
data instances. We then use a meta-model to carry out the feature selection
task. We apply this procedure to the GA-based algorithm CHC (Cross generational
elitist selection, Heterogeneous recombination and Cataclysmic mutation) to
create a Qualitative approXimations variant, CHCQX. We show that CHCQX
converges faster to feature subset solutions of significantly higher accuracy
(as compared to CHC), particularly for large datasets with over 100K instances.
We also demonstrate the applicability of the thinking behind our approach more
broadly to Swarm Intelligence (SI), another branch of the Evolutionary
Computation (EC) paradigm with results of PSOQX, a qualitative approximation
adaptation of the Particle Swarm Optimization (PSO) method. A GitHub repository
with the complete implementation is available.
\\ ( https://arxiv.org/abs/2404.03996 ,  2855kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04095 (*cross-listing*)
Date: Fri, 5 Apr 2024 13:44:39 GMT   (9397kb,D)

Title: Dynamic Prompt Optimizing for Text-to-Image Generation
Authors: Wenyi Mo, Tianyu Zhang, Yalong Bai, Bing Su, Ji-Rong Wen and Qing Yang
Categories: cs.CV cs.AI
Comments: Accepted to CVPR 2024
\\
  Text-to-image generative models, specifically those based on diffusion models
like Imagen and Stable Diffusion, have made substantial advancements. Recently,
there has been a surge of interest in the delicate refinement of text prompts.
Users assign weights or alter the injection time steps of certain words in the
text prompts to improve the quality of generated images. However, the success
of fine-control prompts depends on the accuracy of the text prompts and the
careful selection of weights and time steps, which requires significant manual
intervention. To address this, we introduce the \textbf{P}rompt
\textbf{A}uto-\textbf{E}diting (PAE) method. Besides refining the original
prompts for image generation, we further employ an online reinforcement
learning strategy to explore the weights and injection time steps of each word,
leading to the dynamic fine-control prompts. The reward function during
training encourages the model to consider aesthetic score, semantic
consistency, and user preferences. Experimental results demonstrate that our
proposed method effectively improves the original prompts, generating visually
more appealing images while maintaining semantic alignment. Code is available
at https://github.com/Mowenyii/PAE.
\\ ( https://arxiv.org/abs/2404.04095 ,  9397kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04139 (*cross-listing*)
Date: Fri, 5 Apr 2024 14:37:49 GMT   (2106kb)

Title: Precision Guided Approach to Mitigate Data Poisoning Attacks in
  Federated Learning
Authors: K Naveen Kumar, C Krishna Mohan, and Aravind Machiry
Categories: cs.CR cs.AI
Comments: 14 pages, 11 figures, 5 tables, Accepted in ACM CODASPY 2024
\\
  Federated Learning (FL) is a collaborative learning paradigm enabling
participants to collectively train a shared machine learning model while
preserving the privacy of their sensitive data. Nevertheless, the inherent
decentralized and data-opaque characteristics of FL render its susceptibility
to data poisoning attacks. These attacks introduce malformed or malicious
inputs during local model training, subsequently influencing the global model
and resulting in erroneous predictions. Current FL defense strategies against
data poisoning attacks either involve a trade-off between accuracy and
robustness or necessitate the presence of a uniformly distributed root dataset
at the server. To overcome these limitations, we present FedZZ, which harnesses
a zone-based deviating update (ZBDU) mechanism to effectively counter data
poisoning attacks in FL. Further, we introduce a precision-guided methodology
that actively characterizes these client clusters (zones), which in turn aids
in recognizing and discarding malicious updates at the server. Our evaluation
of FedZZ across two widely recognized datasets: CIFAR10 and EMNIST, demonstrate
its efficacy in mitigating data poisoning attacks, surpassing the performance
of prevailing state-of-the-art methodologies in both single and multi-client
attack scenarios and varying attack volumes. Notably, FedZZ also functions as a
robust client selection strategy, even in highly non-IID and attack-free
scenarios. Moreover, in the face of escalating poisoning rates, the model
accuracy attained by FedZZ displays superior resilience compared to existing
techniques. For instance, when confronted with a 50% presence of malicious
clients, FedZZ sustains an accuracy of 67.43%, while the accuracy of the
second-best solution, FL-Defender, diminishes to 43.36%.
\\ ( https://arxiv.org/abs/2404.04139 ,  2106kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04159 (*cross-listing*)
Date: Fri, 5 Apr 2024 15:11:09 GMT   (1392kb,D)

Title: Noisy Label Processing for Classification: A Survey
Authors: Mengting Li, Chuang Zhu
Categories: cs.CV cs.AI
\\
  In recent years, deep neural networks (DNNs) have gained remarkable
achievement in computer vision tasks, and the success of DNNs often depends
greatly on the richness of data. However, the acquisition process of data and
high-quality ground truth requires a lot of manpower and money. In the long,
tedious process of data annotation, annotators are prone to make mistakes,
resulting in incorrect labels of images, i.e., noisy labels. The emergence of
noisy labels is inevitable. Moreover, since research shows that DNNs can easily
fit noisy labels, the existence of noisy labels will cause significant damage
to the model training process. Therefore, it is crucial to combat noisy labels
for computer vision tasks, especially for classification tasks. In this survey,
we first comprehensively review the evolution of different deep learning
approaches for noisy label combating in the image classification task. In
addition, we also review different noise patterns that have been proposed to
design robust algorithms. Furthermore, we explore the inner pattern of
real-world label noise and propose an algorithm to generate a synthetic label
noise pattern guided by real-world data. We test the algorithm on the
well-known real-world dataset CIFAR-10N to form a new real-world data-guided
synthetic benchmark and evaluate some typical noise-robust methods on the
benchmark.
\\ ( https://arxiv.org/abs/2404.04159 ,  1392kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04219 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:05:45 GMT   (10205kb,D)

Title: Continual Policy Distillation of Reinforcement Learning-based
  Controllers for Soft Robotic In-Hand Manipulation
Authors: Lanpei Li and Enrico Donato and Vincenzo Lomonaco and Egidio Falotico
Categories: cs.RO cs.AI cs.LG
Comments: Accepted for presentation at IEEE RoboSoft 2024
\\
  Dexterous manipulation, often facilitated by multi-fingered robotic hands,
holds solid impact for real-world applications. Soft robotic hands, due to
their compliant nature, offer flexibility and adaptability during object
grasping and manipulation. Yet, benefits come with challenges, particularly in
the control development for finger coordination. Reinforcement Learning (RL)
can be employed to train object-specific in-hand manipulation policies, but
limiting adaptability and generalizability. We introduce a Continual Policy
Distillation (CPD) framework to acquire a versatile controller for in-hand
manipulation, to rotate different objects in shape and size within a
four-fingered soft gripper. The framework leverages Policy Distillation (PD) to
transfer knowledge from expert policies to a continually evolving student
policy network. Exemplar-based rehearsal methods are then integrated to
mitigate catastrophic forgetting and enhance generalization. The performance of
the CPD framework over various replay strategies demonstrates its effectiveness
in consolidating knowledge from multiple experts and achieving versatile and
adaptive behaviours for in-hand manipulation tasks.
\\ ( https://arxiv.org/abs/2404.04219 ,  10205kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04220 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:06:03 GMT   (5853kb,D)

Title: Multi-modal perception for soft robotic interactions using generative
  models
Authors: Enrico Donato and Egidio Falotico and Thomas George Thuruthel
Categories: cs.RO cs.AI cs.LG
Comments: Accepted for presentation at IEEE RoboSoft 2024
\\
  Perception is essential for the active interaction of physical agents with
the external environment. The integration of multiple sensory modalities, such
as touch and vision, enhances this perceptual process, creating a more
comprehensive and robust understanding of the world. Such fusion is
particularly useful for highly deformable bodies such as soft robots.
Developing a compact, yet comprehensive state representation from multi-sensory
inputs can pave the way for the development of complex control strategies. This
paper introduces a perception model that harmonizes data from diverse
modalities to build a holistic state representation and assimilate essential
information. The model relies on the causality between sensory input and
robotic actions, employing a generative model to efficiently compress fused
information and predict the next observation. We present, for the first time, a
study on how touch can be predicted from vision and proprioception on soft
robots, the importance of the cross-modal generation and why this is essential
for soft robotic interactions in unstructured environments.
\\ ( https://arxiv.org/abs/2404.04220 ,  5853kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04242 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:45:07 GMT   (11591kb,D)

Title: Physical Property Understanding from Language-Embedded Feature Fields
Authors: Albert J. Zhai, Yuan Shen, Emily Y. Chen, Gloria X. Wang, Xinlei Wang,
  Sheng Wang, Kaiyu Guan, Shenlong Wang
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: CVPR 2024. Project page (with code):
  https://ajzhai.github.io/NeRF2Physics/
\\
  Can computers perceive the physical properties of objects solely through
vision? Research in cognitive science and vision science has shown that humans
excel at identifying materials and estimating their physical properties based
purely on visual appearance. In this paper, we present a novel approach for
dense prediction of the physical properties of objects using a collection of
images. Inspired by how humans reason about physics through vision, we leverage
large language models to propose candidate materials for each object. We then
construct a language-embedded point cloud and estimate the physical properties
of each 3D point using a zero-shot kernel regression approach. Our method is
accurate, annotation-free, and applicable to any object in the open world.
Experiments demonstrate the effectiveness of the proposed approach in various
physical property reasoning tasks, such as estimating the mass of common
objects, as well as other properties like friction and hardness.
\\ ( https://arxiv.org/abs/2404.04242 ,  11591kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04243 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:45:22 GMT   (41186kb,D)

Title: Identity Decoupling for Multi-Subject Personalization of Text-to-Image
  Models
Authors: Sangwon Jang, Jaehyeong Jo, Kimin Lee, Sung Ju Hwang
Categories: cs.CV cs.AI
Comments: Preprint. Project page: https://mudi-t2i.github.io/
\\
  Text-to-image diffusion models have shown remarkable success in generating a
personalized subject based on a few reference images. However, current methods
struggle with handling multiple subjects simultaneously, often resulting in
mixed identities with combined attributes from different subjects. In this
work, we present MuDI, a novel framework that enables multi-subject
personalization by effectively decoupling identities from multiple subjects.
Our main idea is to utilize segmented subjects generated by the Segment
Anything Model for both training and inference, as a form of data augmentation
for training and initialization for the generation process. Our experiments
demonstrate that MuDI can produce high-quality personalized images without
identity mixing, even for highly similar subjects as shown in Figure 1. In
human evaluation, MuDI shows twice as many successes for personalizing multiple
subjects without identity mixing over existing baselines and is preferred over
70% compared to the strongest baseline. More results are available at
https://mudi-t2i.github.io/.
\\ ( https://arxiv.org/abs/2404.04243 ,  41186kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04251 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:57:16 GMT   (32828kb,D)

Title: Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt
  Coherence Metrics with T2IScoreScore (TS2)
Authors: Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya
  Sharma, William Yang Wang
Categories: cs.CV cs.AI cs.CL
Comments: 15 pages main, 9 pages appendices, 16 figures, 3 tables
\\
  With advances in the quality of text-to-image (T2I) models has come interest
in benchmarking their prompt faithfulness-the semantic coherence of generated
images to the prompts they were conditioned on. A variety of T2I faithfulness
metrics have been proposed, leveraging advances in cross-modal embeddings and
vision-language models (VLMs). However, these metrics are not rigorously
compared and benchmarked, instead presented against few weak baselines by
correlation to human Likert scores over a set of easy-to-discriminate images.
  We introduce T2IScoreScore (TS2), a curated set of semantic error graphs
containing a prompt and a set increasingly erroneous images. These allow us to
rigorously judge whether a given prompt faithfulness metric can correctly order
images with respect to their objective error count and significantly
discriminate between different error nodes, using meta-metric scores derived
from established statistical tests. Surprisingly, we find that the
state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we
tested fail to significantly outperform simple feature-based metrics like
CLIPScore, particularly on a hard subset of naturally-occurring T2I model
errors. TS2 will enable the development of better T2I prompt faithfulness
metrics through more rigorous comparison of their conformity to expected
orderings and separations under objective criteria.
\\ ( https://arxiv.org/abs/2404.04251 ,  32828kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04254 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:58:52 GMT   (1958kb,D)

Title: Watermark-based Detection and Attribution of AI-Generated Content
Authors: Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong
Categories: cs.CR cs.AI cs.CL cs.CV cs.LG
\\
  Several companies--such as Google, Microsoft, and OpenAI--have deployed
techniques to watermark AI-generated content to enable proactive detection.
However, existing literature mainly focuses on user-agnostic detection.
Attribution aims to further trace back the user of a generative-AI service who
generated a given content detected as AI-generated. Despite its growing
importance, attribution is largely unexplored. In this work, we aim to bridge
this gap by providing the first systematic study on watermark-based, user-aware
detection and attribution of AI-generated content. Specifically, we
theoretically study the detection and attribution performance via rigorous
probabilistic analysis. Moreover, we develop an efficient algorithm to select
watermarks for the users to enhance attribution performance. Both our
theoretical and empirical results show that watermark-based detection and
attribution inherit the accuracy and (non-)robustness properties of the
watermarking method.
\\ ( https://arxiv.org/abs/2404.04254 ,  1958kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03676 (*cross-listing*)
Date: Thu, 15 Feb 2024 15:15:11 GMT   (933kb)

Title: Neural Information Organizing and Processing -- Neural Machines
Authors: Iosif Iulian Petrila
Categories: cs.NE cs.CL cs.LG
\\
  The informational synthesis of neural structures, processes, parameters and
characteristics that allow a unified description and modeling as neural
machines of natural and artificial neural systems is presented. The general
informational parameters as the global quantitative measure of the neural
systems computing potential as absolute and relative neural power were
proposed. Neural information organizing and processing follows the way in which
nature manages neural information by developing functions, functionalities and
circuits related to different internal or peripheral components and also to the
whole system through a non-deterministic memorization, fragmentation and
aggregation of afferent and efferent information, deep neural information
processing representing multiple alternations of fragmentation and aggregation
stages. The relevant neural characteristics were integrated into a neural
machine type model that incorporates unitary also peripheral or interface
components as the central ones. The proposed approach allows overcoming the
technical constraints in artificial computational implementations of neural
information processes and also provides a more relevant description of natural
ones.
\\ ( https://arxiv.org/abs/2404.03676 ,  933kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03823 (*cross-listing*)
Date: Thu, 4 Apr 2024 22:52:41 GMT   (4190kb,D)

Title: An Investigation into Misuse of Java Security APIs by Large Language
  Models
Authors: Zahra Mousavi, Chadni Islam, Kristen Moore, Alsharif Abuadbba,
  Muhammad Ali Babar
Categories: cs.CR cs.CL cs.CY
Comments: This paper has been accepted by ACM ASIACCS 2024
\\
  The increasing trend of using Large Language Models (LLMs) for code
generation raises the question of their capability to generate trustworthy
code. While many researchers are exploring the utility of code generation for
uncovering software vulnerabilities, one crucial but often overlooked aspect is
the security Application Programming Interfaces (APIs). APIs play an integral
role in upholding software security, yet effectively integrating security APIs
presents substantial challenges. This leads to inadvertent misuse by
developers, thereby exposing software to vulnerabilities. To overcome these
challenges, developers may seek assistance from LLMs. In this paper, we
systematically assess ChatGPT's trustworthiness in code generation for security
API use cases in Java. To conduct a thorough evaluation, we compile an
extensive collection of 48 programming tasks for 5 widely used security APIs.
We employ both automated and manual approaches to effectively detect security
API misuse in the code generated by ChatGPT for these tasks. Our findings are
concerning: around 70% of the code instances across 30 attempts per task
contain security API misuse, with 20 distinct misuse types identified.
Moreover, for roughly half of the tasks, this rate reaches 100%, indicating
that there is a long way to go before developers can rely on ChatGPT to
securely implement security API code.
\\ ( https://arxiv.org/abs/2404.03823 ,  4190kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04066 (*cross-listing*)
Date: Fri, 5 Apr 2024 12:45:10 GMT   (7254kb,D)

Title: VoicePilot: Harnessing LLMs as Speech Interfaces for Physically
  Assistive Robots
Authors: Akhil Padmanabha, Jessie Yuan, Janavi Gupta, Zulekha Karachiwalla,
  Carmel Majidi, Henny Admoni, Zackory Erickson
Categories: cs.RO cs.CL cs.HC
\\
  Physically assistive robots present an opportunity to significantly increase
the well-being and independence of individuals with motor impairments or other
forms of disability who are unable to complete activities of daily living.
Speech interfaces, especially ones that utilize Large Language Models (LLMs),
can enable individuals to effectively and naturally communicate high-level
commands and nuanced preferences to robots. Frameworks for integrating LLMs as
interfaces to robots for high level task planning and code generation have been
proposed, but fail to incorporate human-centric considerations which are
essential while developing assistive interfaces. In this work, we present a
framework for incorporating LLMs as speech interfaces for physically assistive
robots, constructed iteratively with 3 stages of testing involving a feeding
robot, culminating in an evaluation with 11 older adults at an independent
living facility. We use both quantitative and qualitative data from the final
study to validate our framework and additionally provide design guidelines for
using LLMs as speech interfaces for assistive robots. Videos and supporting
files are located on our project website:
https://sites.google.com/andrew.cmu.edu/voicepilot/
\\ ( https://arxiv.org/abs/2404.04066 ,  7254kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04125 (*cross-listing*)
Date: Thu, 4 Apr 2024 17:58:02 GMT   (37862kb,D)

Title: No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency
  Determines Multimodal Model Performance
Authors: Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip
  H.S. Torr, Adel Bibi, Samuel Albanie, Matthias Bethge
Categories: cs.CV cs.CL cs.LG
Comments: Extended version of the short paper accepted at DPFM, ICLR'24
\\
  Web-crawled pretraining datasets underlie the impressive "zero-shot"
evaluation performance of multimodal models, such as CLIP for
classification/retrieval and Stable-Diffusion for image generation. However, it
is unclear how meaningful the notion of "zero-shot" generalization is for such
multimodal models, as it is not known to what extent their pretraining datasets
encompass the downstream concepts targeted for during "zero-shot" evaluation.
In this work, we ask: How is the performance of multimodal models on downstream
concepts influenced by the frequency of these concepts in their pretraining
datasets? We comprehensively investigate this question across 34 models and
five standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M,
LAION-Aesthetics), generating over 300GB of data artifacts. We consistently
find that, far from exhibiting "zero-shot" generalization, multimodal models
require exponentially more data to achieve linear improvements in downstream
"zero-shot" performance, following a sample inefficient log-linear scaling
trend. This trend persists even when controlling for sample-level similarity
between pretraining and downstream datasets, and testing on purely synthetic
data distributions. Furthermore, upon benchmarking models on long-tailed data
sampled based on our analysis, we demonstrate that multimodal models across the
board perform poorly. We contribute this long-tail test set as the "Let it
Wag!" benchmark to further research in this direction. Taken together, our
study reveals an exponential need for training data which implies that the key
to "zero-shot" generalization capabilities under large-scale training paradigms
remains to be found.
\\ ( https://arxiv.org/abs/2404.04125 ,  37862kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04163 (*cross-listing*)
Date: Fri, 5 Apr 2024 15:16:16 GMT   (128kb,D)

Title: Dwell in the Beginning: How Language Models Embed Long Documents for
  Dense Retrieval
Authors: Jo\~ao Coelho, Bruno Martins, Jo\~ao Magalh\~aes, Jamie Callan,
  Chenyan Xiong
Categories: cs.IR cs.CL
\\
  This study investigates the existence of positional biases in
Transformer-based models for text representation learning, particularly in the
context of web document retrieval. We build on previous research that
demonstrated loss of information in the middle of input sequences for causal
language models, extending it to the domain of representation learning. We
examine positional biases at various stages of training for an encoder-decoder
model, including language model pre-training, contrastive pre-training, and
contrastive fine-tuning. Experiments with the MS-MARCO document collection
reveal that after contrastive pre-training the model already generates
embeddings that better capture early contents of the input, with fine-tuning
further aggravating this effect.
\\ ( https://arxiv.org/abs/2404.04163 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03017 (*cross-listing*)
Date: Wed, 3 Apr 2024 18:57:54 GMT   (3194kb,D)

Title: Distributionally Robust Policy and Lyapunov-Certificate Learning
Authors: Kehan Long, Jorge Cortes and Nikolay Atanasov
Categories: eess.SY cs.LG cs.RO cs.SY math.OC
Comments: Submitted to IEEE Open Journal of Control Systems
\\
  This article presents novel methods for synthesizing distributionally robust
stabilizing neural controllers and certificates for control systems under model
uncertainty. A key challenge in designing controllers with stability guarantees
for uncertain systems is the accurate determination of and adaptation to shifts
in model parametric uncertainty during online deployment. We tackle this with a
novel distributionally robust formulation of the Lyapunov derivative chance
constraint ensuring a monotonic decrease of the Lyapunov certificate. To avoid
the computational complexity involved in dealing with the space of probability
measures, we identify a sufficient condition in the form of deterministic
convex constraints that ensures the Lyapunov derivative constraint is
satisfied. We integrate this condition into a loss function for training a
neural network-based controller and show that, for the resulting closed-loop
system, the global asymptotic stability of its equilibrium can be certified
with high confidence, even with Out-of-Distribution (OoD) model uncertainties.
To demonstrate the efficacy and efficiency of the proposed methodology, we
compare it with an uncertainty-agnostic baseline approach and several
reinforcement learning approaches in two control problems in simulation.
\\ ( https://arxiv.org/abs/2404.03017 ,  3194kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03689 (*cross-listing*)
Date: Tue, 2 Apr 2024 03:13:05 GMT   (14086kb,D)

Title: A Tutorial on Gaussian Process Learning-based Model Predictive Control
Authors: Jie Wang, Youmin Zhang
Categories: cs.RO cs.LG cs.SY eess.SY
\\
  This tutorial provides a systematic introduction to Gaussian process
learning-based model predictive control (GP-MPC), an advanced approach
integrating Gaussian process (GP) with model predictive control (MPC) for
enhanced control in complex systems. It begins with GP regression fundamentals,
illustrating how it enriches MPC with enhanced predictive accuracy and robust
handling of uncertainties. A central contribution of this tutorial is the first
detailed, systematic mathematical formulation of GP-MPC in literature, focusing
on deriving the approximation of means and variances propagation for GP
multi-step predictions. Practical applications in robotics control, such as
path-following for mobile robots in challenging terrains and mixed-vehicle
platooning, are discussed to demonstrate the real-world effectiveness and
adaptability of GP-MPC. This tutorial aims to make GP-MPC accessible to
researchers and practitioners, enriching the learning-based control field with
in-depth theoretical and practical insights and fostering further innovations
in complex system control.
\\ ( https://arxiv.org/abs/2404.03689 ,  14086kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03696 (*cross-listing*)
Date: Wed, 3 Apr 2024 15:17:29 GMT   (3461kb,D)

Title: Convolutional variational autoencoders for secure lossy image
  compression in remote sensing
Authors: Alessandro Giuliano, S. Andrew Gadsden, Waleed Hilal, John Yawney
Categories: eess.IV cs.LG
Comments: Accepted at SPIE Defense + Commercial Sensing: {Paper
  No.13062-18,Tracking No. DCS24-DCS314-13}
Report-no: Paper No. 13062-18
\\
  The volume of remote sensing data is experiencing rapid growth, primarily due
to the plethora of space and air platforms equipped with an array of sensors.
Due to limited hardware and battery constraints the data is transmitted back to
Earth for processing. The large amounts of data along with security concerns
call for new compression and encryption techniques capable of preserving
reconstruction quality while minimizing the transmission cost of this data back
to Earth. This study investigates image compression based on convolutional
variational autoencoders (CVAE), which are capable of substantially reducing
the volume of transmitted data while guaranteeing secure lossy image
reconstruction. CVAEs have been demonstrated to outperform conventional
compression methods such as JPEG2000 by a substantial margin on compression
benchmark datasets. The proposed model draws on the strength of the CVAEs
capability to abstract data into highly insightful latent spaces, and combining
it with the utilization of an entropy bottleneck is capable of finding an
optimal balance between compressibility and reconstruction quality. The balance
is reached by optimizing over a composite loss function that represents the
rate-distortion curve.
\\ ( https://arxiv.org/abs/2404.03696 ,  3461kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03706 (*cross-listing*)
Date: Thu, 4 Apr 2024 10:36:56 GMT   (36694kb,D)

Title: Bi-level Guided Diffusion Models for Zero-Shot Medical Imaging Inverse
  Problems
Authors: Hossein Askari, Fred Roosta, Hongfu Sun
Categories: eess.IV cs.LG
Comments: 19 pages, 14 figures
\\
  In the realm of medical imaging, inverse problems aim to infer high-quality
images from incomplete, noisy measurements, with the objective of minimizing
expenses and risks to patients in clinical settings. The Diffusion Models have
recently emerged as a promising approach to such practical challenges, proving
particularly useful for the zero-shot inference of images from partially
acquired measurements in Magnetic Resonance Imaging (MRI) and Computed
Tomography (CT). A central challenge in this approach, however, is how to guide
an unconditional prediction to conform to the measurement information. Existing
methods rely on deficient projection or inefficient posterior score
approximation guidance, which often leads to suboptimal performance. In this
paper, we propose \underline{\textbf{B}}i-level \underline{G}uided
\underline{D}iffusion \underline{M}odels ({BGDM}), a zero-shot imaging
framework that efficiently steers the initial unconditional prediction through
a \emph{bi-level} guidance strategy. Specifically, BGDM first approximates an
\emph{inner-level} conditional posterior mean as an initial
measurement-consistent reference point and then solves an \emph{outer-level}
proximal optimization objective to reinforce the measurement consistency. Our
experimental findings, using publicly available MRI and CT medical datasets,
reveal that BGDM is more effective and efficient compared to the baselines,
faithfully generating high-fidelity medical images and substantially reducing
hallucinatory artifacts in cases of severe degradation.
\\ ( https://arxiv.org/abs/2404.03706 ,  36694kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03708 (*cross-listing*)
Date: Thu, 4 Apr 2024 11:22:58 GMT   (3780kb)

Title: Dendrites endow artificial neural networks with accurate, robust and
  parameter-efficient learning
Authors: Spyridon Chavlis, Panayiota Poirazi
Categories: cs.NE cs.LG q-bio.NC
Comments: 50 pages, 6 main and 2 supplementary figures, 2 main and 3
  supplementary tables
\\
  Artificial neural networks (ANNs) are at the core of most Deep learning (DL)
algorithms that successfully tackle complex problems like image recognition,
autonomous driving, and natural language processing. However, unlike biological
brains who tackle similar problems in a very efficient manner, DL algorithms
require a large number of trainable parameters, making them energy-intensive
and prone to overfitting. Here, we show that a new ANN architecture that
incorporates the structured connectivity and restricted sampling properties of
biological dendrites counteracts these limitations. We find that dendritic ANNs
are more robust to overfitting and outperform traditional ANNs on several image
classification tasks while using significantly fewer trainable parameters. This
is achieved through the adoption of a different learning strategy, whereby most
of the nodes respond to several classes, unlike classical ANNs that strive for
class-specificity. These findings suggest that the incorporation of dendrites
can make learning in ANNs precise, resilient, and parameter-efficient and shed
new light on how biological features can impact the learning strategies of
ANNs.
\\ ( https://arxiv.org/abs/2404.03708 ,  3780kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03729 (*cross-listing*)
Date: Thu, 4 Apr 2024 18:00:15 GMT   (17905kb,D)

Title: JUICER: Data-Efficient Imitation Learning for Robotic Assembly
Authors: Lars Ankile and Anthony Simeonov and Idan Shenfeld and Pulkit Agrawal
Categories: cs.RO cs.LG
\\
  While learning from demonstrations is powerful for acquiring visuomotor
policies, high-performance imitation without large demonstration datasets
remains challenging for tasks requiring precise, long-horizon manipulation.
This paper proposes a pipeline for improving imitation learning performance
with a small human demonstration budget. We apply our approach to assembly
tasks that require precisely grasping, reorienting, and inserting multiple
parts over long horizons and multiple task phases. Our pipeline combines
expressive policy architectures and various techniques for dataset expansion
and simulation-based data augmentation. These help expand dataset support and
supervise the model with locally corrective actions near bottleneck regions
requiring high precision. We demonstrate our pipeline on four furniture
assembly tasks in simulation, enabling a manipulator to assemble up to five
parts over nearly 2500 time steps directly from RGB images, outperforming
imitation and data augmentation baselines.
\\ ( https://arxiv.org/abs/2404.03729 ,  17905kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03761 (*cross-listing*)
Date: Thu, 4 Apr 2024 19:07:21 GMT   (566kb,D)

Title: Learning smooth functions in high dimensions: from sparse polynomials to
  deep neural networks
Authors: Ben Adcock, Simone Brugiapaglia, Nick Dexter, Sebastian Moraga
Categories: math.NA cs.LG cs.NA
\\
  Learning approximations to smooth target functions of many variables from
finite sets of pointwise samples is an important task in scientific computing
and its many applications in computational science and engineering. Despite
well over half a century of research on high-dimensional approximation, this
remains a challenging problem. Yet, significant advances have been made in the
last decade towards efficient methods for doing this, commencing with so-called
sparse polynomial approximation methods and continuing most recently with
methods based on Deep Neural Networks (DNNs). In tandem, there have been
substantial advances in the relevant approximation theory and analysis of these
techniques. In this work, we survey this recent progress. We describe the
contemporary motivations for this problem, which stem from parametric models
and computational uncertainty quantification; the relevant function classes,
namely, classes of infinite-dimensional, Banach-valued, holomorphic functions;
fundamental limits of learnability from finite data for these classes; and
finally, sparse polynomial and DNN methods for efficiently learning such
functions from finite data. For the latter, there is currently a significant
gap between the approximation theory of DNNs and the practical performance of
deep learning. Aiming to narrow this gap, we develop the topic of practical
existence theory, which asserts the existence of dimension-independent DNN
architectures and training strategies that achieve provably near-optimal
generalization errors in terms of the amount of training data.
\\ ( https://arxiv.org/abs/2404.03761 ,  566kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03769 (*cross-listing*)
Date: Thu, 4 Apr 2024 19:28:38 GMT   (82kb)

Title: On Extending the Automatic Test Markup Language (ATML) for Machine
  Learning
Authors: Tyler Cody, Bingtong Li, Peter A. Beling
Categories: cs.SE cs.LG cs.SY eess.SY
Comments: Accepted by the 18th Annual IEEE International Systems Conference
  (SysCon)
\\
  This paper addresses the urgent need for messaging standards in the
operational test and evaluation (T&E) of machine learning (ML) applications,
particularly in edge ML applications embedded in systems like robots,
satellites, and unmanned vehicles. It examines the suitability of the IEEE
Standard 1671 (IEEE Std 1671), known as the Automatic Test Markup Language
(ATML), an XML-based standard originally developed for electronic systems, for
ML application testing. The paper explores extending IEEE Std 1671 to encompass
the unique challenges of ML applications, including the use of datasets and
dependencies on software. Through modeling various tests such as adversarial
robustness and drift detection, this paper offers a framework adaptable to
specific applications, suggesting that minor modifications to ATML might
suffice to address the novelties of ML. This paper differentiates ATML's focus
on testing from other ML standards like Predictive Model Markup Language (PMML)
or Open Neural Network Exchange (ONNX), which concentrate on ML model
specification. We conclude that ATML is a promising tool for effective, near
real-time operational T&E of ML applications, an essential aspect of AI
lifecycle management, safety, and governance.
\\ ( https://arxiv.org/abs/2404.03769 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03804 (*cross-listing*)
Date: Thu, 4 Apr 2024 20:51:37 GMT   (985kb,D)

Title: TransformerLSR: Attentive Joint Model of Longitudinal Data, Survival,
  and Recurrent Events with Concurrent Latent Structure
Authors: Zhiyue Zhang, Yao Zhao, Yanxun Xu
Categories: stat.ML cs.LG stat.AP stat.ME
\\
  In applications such as biomedical studies, epidemiology, and social
sciences, recurrent events often co-occur with longitudinal measurements and a
terminal event, such as death. Therefore, jointly modeling longitudinal
measurements, recurrent events, and survival data while accounting for their
dependencies is critical. While joint models for the three components exist in
statistical literature, many of these approaches are limited by heavy
parametric assumptions and scalability issues. Recently, incorporating deep
learning techniques into joint modeling has shown promising results. However,
current methods only address joint modeling of longitudinal measurements at
regularly-spaced observation times and survival events, neglecting recurrent
events. In this paper, we develop TransformerLSR, a flexible transformer-based
deep modeling and inference framework to jointly model all three components
simultaneously. TransformerLSR integrates deep temporal point processes into
the joint modeling framework, treating recurrent and terminal events as two
competing processes dependent on past longitudinal measurements and recurrent
event times. Additionally, TransformerLSR introduces a novel trajectory
representation and model architecture to potentially incorporate a priori
knowledge of known latent structures among concurrent longitudinal variables.
We demonstrate the effectiveness and necessity of TransformerLSR through
simulation studies and analyzing a real-world medical dataset on patients after
kidney transplantation.
\\ ( https://arxiv.org/abs/2404.03804 ,  985kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03813 (*cross-listing*)
Date: Thu, 4 Apr 2024 21:39:47 GMT   (21kb)

Title: Agnostic Tomography of Stabilizer Product States
Authors: Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang
Categories: quant-ph cs.LG
Comments: 20 pages
\\
  We define a quantum learning task called agnostic tomography, where given
copies of an arbitrary state $\rho$ and a class of quantum states
$\mathcal{C}$, the goal is to output a succinct description of a state that
approximates $\rho$ at least as well as any state in $\mathcal{C}$ (up to some
small error $\varepsilon$). This task generalizes ordinary quantum tomography
of states in $\mathcal{C}$ and is more challenging because the learning
algorithm must be robust to perturbations of $\rho$.
  We give an efficient agnostic tomography algorithm for the class
$\mathcal{C}$ of $n$-qubit stabilizer product states. Assuming $\rho$ has
fidelity at least $\tau$ with a stabilizer product state, the algorithm runs in
time $n^{O(1 + \log(1/\tau))} / \varepsilon^2$. This runtime is quasipolynomial
in all parameters, and polynomial if $\tau$ is a constant.
\\ ( https://arxiv.org/abs/2404.03813 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03843 (*cross-listing*)
Date: Fri, 5 Apr 2024 00:25:37 GMT   (11551kb,D)

Title: Scaling Motion Forecasting Models with Ensemble Distillation
Authors: Scott Ettinger, Kratarth Goel, Avikalp Srivastava, Rami Al-Rfou
Categories: cs.RO cs.LG
Comments: 11 pages, 14 figures
\\
  Motion forecasting has become an increasingly critical component of
autonomous robotic systems. Onboard compute budgets typically limit the
accuracy of real-time systems. In this work we propose methods of improving
motion forecasting systems subject to limited compute budgets by combining
model ensemble and distillation techniques. The use of ensembles of deep neural
networks has been shown to improve generalization accuracy in many application
domains. We first demonstrate significant performance gains by creating a large
ensemble of optimized single models. We then develop a generalized framework to
distill motion forecasting model ensembles into small student models which
retain high performance with a fraction of the computing cost. For this study
we focus on the task of motion forecasting using real world data from
autonomous driving systems. We develop ensemble models that are very
competitive on the Waymo Open Motion Dataset (WOMD) and Argoverse leaderboards.
From these ensembles, we train distilled student models which have high
performance at a fraction of the compute costs. These experiments demonstrate
distillation from ensembles as an effective method for improving accuracy of
predictive models for robotic systems with limited compute budgets.
\\ ( https://arxiv.org/abs/2404.03843 ,  11551kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03876 (*cross-listing*)
Date: Fri, 5 Apr 2024 03:51:19 GMT   (1302kb,D)

Title: Increasing Fairness in Classification of Out of Distribution Data for
  Facial Recognition
Authors: Gianluca Barone and Aashrit Cunchala and Rudy Nunez
Categories: cs.CV cs.CY cs.LG
Comments: 18 pages, 6 tables, 6 figures
\\
  Standard classification theory assumes that the distribution of images in the
test and training sets are identical. Unfortunately, real-life scenarios
typically feature unseen data ("out-of-distribution data") which is different
from data in the training distribution("in-distribution"). This issue is most
prevalent in social justice problems where data from under-represented groups
may appear in the test data without representing an equal proportion of the
training data. This may result in a model returning confidently wrong decisions
and predictions. We are interested in the following question: Can the
performance of a neural network improve on facial images of out-of-distribution
data when it is trained simultaneously on multiple datasets of in-distribution
data? We approach this problem by incorporating the Outlier Exposure model and
investigate how the model's performance changes when other datasets of facial
images were implemented. We observe that the accuracy and other metrics of the
model can be increased by applying Outlier Exposure, incorporating a trainable
weight parameter to increase the machine's emphasis on outlier images, and by
re-weighting the importance of different class labels. We also experimented
with whether sorting the images and determining outliers via image features
would have more of an effect on the metrics than sorting by average pixel
value. Our goal was to make models not only more accurate but also more fair by
scanning a more expanded range of images. We also tested the datasets in
reverse order to see whether a more fair dataset with balanced features has an
effect on the model's accuracy.
\\ ( https://arxiv.org/abs/2404.03876 ,  1302kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03898 (*cross-listing*)
Date: Fri, 5 Apr 2024 05:42:23 GMT   (964kb,D)

Title: VoltaVision: A Transfer Learning model for electronic component
  classification
Authors: Anas Mohammad Ishfaqul Muktadir Osmani, Taimur Rahman, Salekul Islam
Categories: cs.CV cs.LG
Comments: Tiny Paper at ICLR 2024
\\
  In this paper, we analyze the effectiveness of transfer learning on
classifying electronic components. Transfer learning reuses pre-trained models
to save time and resources in building a robust classifier rather than learning
from scratch. Our work introduces a lightweight CNN, coined as VoltaVision, and
compares its performance against more complex models. We test the hypothesis
that transferring knowledge from a similar task to our target domain yields
better results than state-of-the-art models trained on general datasets. Our
dataset and code for this work are available at
https://github.com/AnasIshfaque/VoltaVision.
\\ ( https://arxiv.org/abs/2404.03898 ,  964kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03936 (*cross-listing*)
Date: Fri, 5 Apr 2024 07:44:17 GMT   (1171kb)

Title: Deep Learning for Satellite Image Time Series Analysis: A Review
Authors: Lynn Miller, Charlotte Pelletier, Geoffrey I. Webb
Categories: cs.CV cs.LG eess.IV
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Earth observation (EO) satellite missions have been providing detailed images
about the state of the Earth and its land cover for over 50 years. Long term
missions, such as NASA's Landsat, Terra, and Aqua satellites, and more
recently, the ESA's Sentinel missions, record images of the entire world every
few days. Although single images provide point-in-time data, repeated images of
the same area, or satellite image time series (SITS) provide information about
the changing state of vegetation and land use. These SITS are useful for
modeling dynamic processes and seasonal changes such as plant phenology. They
have potential benefits for many aspects of land and natural resource
management, including applications in agricultural, forest, water, and disaster
management, urban planning, and mining. However, the resulting satellite image
time series (SITS) are complex, incorporating information from the temporal,
spatial, and spectral dimensions. Therefore, deep learning methods are often
deployed as they can analyze these complex relationships. This review presents
a summary of the state-of-the-art methods of modelling environmental,
agricultural, and other Earth observation variables from SITS data using deep
learning methods. We aim to provide a resource for remote sensing experts
interested in using deep learning techniques to enhance Earth observation
models with temporal information.
\\ ( https://arxiv.org/abs/2404.03936 ,  1171kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03948 (*cross-listing*)
Date: Fri, 5 Apr 2024 08:27:36 GMT   (2260kb,D)

Title: Re-pseudonymization Strategies for Smart Meter Data Are Not Robust to
  Deep Learning Profiling Attacks
Authors: Ana-Maria Cretu, Miruna Rusu, Yves-Alexandre de Montjoye
Categories: cs.CR cs.LG
Comments: Extended version, including the Appendix, of a paper with the same
  title which will appear in the Proceedings of the Fourteenth ACM Conference
  on Data and Application Security and Privacy (CODASPY '24). The first two
  authors contributed equally
DOI: 10.1145/3626232.3653272
\\
  Smart meters, devices measuring the electricity and gas consumption of a
household, are currently being deployed at a fast rate throughout the world.
The data they collect are extremely useful, including in the fight against
climate change. However, these data and the information that can be inferred
from them are highly sensitive. Re-pseudonymization, i.e., the frequent
replacement of random identifiers over time, is widely used to share smart
meter data while mitigating the risk of re-identification. We here show how, in
spite of re-pseudonymization, households' consumption records can be pieced
together with high accuracy in large-scale datasets. We propose the first deep
learning-based profiling attack against re-pseudonymized smart meter data. Our
attack combines neural network embeddings, which are used to extract features
from weekly consumption records and are tailored to the smart meter
identification task, with a nearest neighbor classifier. We evaluate six neural
networks architectures as the embedding model. Our results suggest that the
Transformer and CNN-LSTM architectures vastly outperform previous methods as
well as other architectures, successfully identifying the correct household
73.4% of the time among 5139 households based on electricity and gas
consumption records (54.5% for electricity only). We further show that the
features extracted by the embedding model maintain their effectiveness when
transferred to a set of users disjoint from the one used to train the model.
Finally, we extensively evaluate the robustness of our results. Taken together,
our results strongly suggest that even frequent re-pseudonymization strategies
can be reversed, strongly limiting their ability to prevent re-identification
in practice.
\\ ( https://arxiv.org/abs/2404.03948 ,  2260kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03991 (*cross-listing*)
Date: Fri, 5 Apr 2024 10:01:31 GMT   (1149kb,D)

Title: Towards Efficient and Accurate CT Segmentation via Edge-Preserving
  Probabilistic Downsampling
Authors: Shahzad Ali, Yu Rim Lee, Soo Young Park, Won Young Tak, Soon Ki Jung
Categories: eess.IV cs.CV cs.LG
Comments: 5 pages (4 figures, 1 table); This work has been submitted to the
  IEEE Signal Processing Letters. Copyright may be transferred without notice,
  after which this version may no longer be accessible
\\
  Downsampling images and labels, often necessitated by limited resources or to
expedite network training, leads to the loss of small objects and thin
boundaries. This undermines the segmentation network's capacity to interpret
images accurately and predict detailed labels, resulting in diminished
performance compared to processing at original resolutions. This situation
exemplifies the trade-off between efficiency and accuracy, with higher
downsampling factors further impairing segmentation outcomes. Preserving
information during downsampling is especially critical for medical image
segmentation tasks. To tackle this challenge, we introduce a novel method named
Edge-preserving Probabilistic Downsampling (EPD). It utilizes class uncertainty
within a local window to produce soft labels, with the window size dictating
the downsampling factor. This enables a network to produce quality predictions
at low resolutions. Beyond preserving edge details more effectively than
conventional nearest-neighbor downsampling, employing a similar algorithm for
images, it surpasses bilinear interpolation in image downsampling, enhancing
overall performance. Our method significantly improved Intersection over Union
(IoU) to 2.85%, 8.65%, and 11.89% when downsampling data to 1/2, 1/4, and 1/8,
respectively, compared to conventional interpolation methods.
\\ ( https://arxiv.org/abs/2404.03991 ,  1149kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04049 (*cross-listing*)
Date: Fri, 5 Apr 2024 12:05:20 GMT   (1387kb,D)

Title: Cycle Life Prediction for Lithium-ion Batteries: Machine Learning and
  More
Authors: Joachim Schaeffer, Giacomo Galuppini, Jinwook Rhyu, Patrick A.
  Asinger, Robin Droop, Rolf Findeisen, and Richard D. Braatz
Categories: eess.SY cs.LG cs.SY
Comments: 6 pages, 3 figures, accepted for ACC 2024
\\
  Batteries are dynamic systems with complicated nonlinear aging, highly
dependent on cell design, chemistry, manufacturing, and operational conditions.
Prediction of battery cycle life and estimation of aging states is important to
accelerate battery R&D, testing, and to further the understanding of how
batteries degrade. Beyond testing, battery management systems rely on real-time
models and onboard diagnostics and prognostics for safe operation. Estimating
the state of health and remaining useful life of a battery is important to
optimize performance and use resources optimally.
  This tutorial begins with an overview of first-principles, machine learning,
and hybrid battery models. Then, a typical pipeline for the development of
interpretable machine learning models is explained and showcased for cycle life
prediction from laboratory testing data. We highlight the challenges of machine
learning models, motivating the incorporation of physics in hybrid modeling
approaches, which are needed to decipher the aging trajectory of batteries but
require more data and further work on the physics of battery degradation. The
tutorial closes with a discussion on generalization and further research
directions.
\\ ( https://arxiv.org/abs/2404.04049 ,  1387kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04072 (*cross-listing*)
Date: Fri, 5 Apr 2024 12:58:07 GMT   (104kb,D)

Title: Label Propagation for Zero-shot Classification with Vision-Language
  Models
Authors: Vladan Stojni\'c, Yannis Kalantidis, Giorgos Tolias
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\
  Vision-Language Models (VLMs) have demonstrated impressive performance on
zero-shot classification, i.e. classification when provided merely with a list
of class names. In this paper, we tackle the case of zero-shot classification
in the presence of unlabeled data. We leverage the graph structure of the
unlabeled data and introduce ZLaP, a method based on label propagation (LP)
that utilizes geodesic distances for classification. We tailor LP to graphs
containing both text and image features and further propose an efficient method
for performing inductive inference based on a dual solution and a
sparsification step. We perform extensive experiments to evaluate the
effectiveness of our method on 14 common datasets and show that ZLaP
outperforms the latest related works. Code:
https://github.com/vladan-stojnic/ZLaP
\\ ( https://arxiv.org/abs/2404.04072 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04140 (*cross-listing*)
Date: Fri, 5 Apr 2024 14:39:13 GMT   (25870kb,D)

Title: Improving Detection in Aerial Images by Capturing Inter-Object
  Relationships
Authors: Botao Ren, Botian Xu, Yifan Pu, Jingyi Wang, Zhidong Deng
Categories: cs.CV cs.LG
\\
  In many image domains, the spatial distribution of objects in a scene
exhibits meaningful patterns governed by their semantic relationships. In most
modern detection pipelines, however, the detection proposals are processed
independently, overlooking the underlying relationships between objects. In
this work, we introduce a transformer-based approach to capture these
inter-object relationships to refine classification and regression outcomes for
detected objects. Building on two-stage detectors, we tokenize the region of
interest (RoI) proposals to be processed by a transformer encoder. Specific
spatial and geometric relations are incorporated into the attention weights and
adaptively modulated and regularized. Experimental results demonstrate that the
proposed method achieves consistent performance improvement on three benchmarks
including DOTA-v1.0, DOTA-v1.5, and HRSC 2016, especially ranking first on both
DOTA-v1.5 and HRSC 2016. Specifically, our new method has an increase of 1.59
mAP on DOTA-v1.0, 4.88 mAP on DOTA-v1.5, and 2.1 mAP on HRSC 2016,
respectively, compared to the baselines.
\\ ( https://arxiv.org/abs/2404.04140 ,  25870kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04173 (*cross-listing*)
Date: Fri, 5 Apr 2024 15:32:49 GMT   (3835kb,D)

Title: H3DFact: Heterogeneous 3D Integrated CIM for Factorization with
  Holographic Perceptual Representations
Authors: Zishen Wan, Che-Kai Liu, Mohamed Ibrahim, Hanchen Yang, Samuel
  Spetalnick, Tushar Krishna, Arijit Raychowdhury
Categories: cs.AR cs.LG
Comments: 2024 Design Automation and Test in Europe (DATE); The first two
  authors have equal contributions
\\
  Disentangling attributes of various sensory signals is central to human-like
perception and reasoning and a critical task for higher-order cognitive and
neuro-symbolic AI systems. An elegant approach to represent this intricate
factorization is via high-dimensional holographic vectors drawing on
brain-inspired vector symbolic architectures. However, holographic
factorization involves iterative computation with high-dimensional
matrix-vector multiplications and suffers from non-convergence problems.
  In this paper, we present H3DFact, a heterogeneous 3D integrated in-memory
compute engine capable of efficiently factorizing high-dimensional holographic
representations. H3DFact exploits the computation-in-superposition capability
of holographic vectors and the intrinsic stochasticity associated with
memristive-based 3D compute-in-memory. Evaluated on large-scale factorization
and perceptual problems, H3DFact demonstrates superior capability in
factorization accuracy and operational capacity by up to five orders of
magnitude, with 5.5x compute density, 1.2x energy efficiency improvements, and
5.9x less silicon footprint compared to iso-capacity 2D designs.
\\ ( https://arxiv.org/abs/2404.04173 ,  3835kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04188 (*cross-listing*)
Date: Fri, 5 Apr 2024 16:01:21 GMT   (403kb)

Title: Reliable Feature Selection for Adversarially Robust Cyber-Attack
  Detection
Authors: Jo\~ao Vitorino, Miguel Silva, Eva Maia, Isabel Pra\c{c}a
Categories: cs.CR cs.LG cs.NI
Comments: 24 pages, 17 tables, Annals of Telecommunications journal. arXiv
  admin note: substantial text overlap with arXiv:2402.16912
\\
  The growing cybersecurity threats make it essential to use high-quality data
to train Machine Learning (ML) models for network traffic analysis, without
noisy or missing data. By selecting the most relevant features for cyber-attack
detection, it is possible to improve both the robustness and computational
efficiency of the models used in a cybersecurity system. This work presents a
feature selection and consensus process that combines multiple methods and
applies them to several network datasets. Two different feature sets were
selected and were used to train multiple ML models with regular and adversarial
training. Finally, an adversarial evasion robustness benchmark was performed to
analyze the reliability of the different feature sets and their impact on the
susceptibility of the models to adversarial examples. By using an improved
dataset with more data diversity, selecting the best time-related features and
a more specific feature set, and performing adversarial training, the ML models
were able to achieve a better adversarially robust generalization. The
robustness of the models was significantly improved without their
generalization to regular traffic flows being affected, without increases of
false alarms, and without requiring too many computational resources, which
enables a reliable detection of suspicious activity and perturbed traffic flows
in enterprise computer networks.
\\ ( https://arxiv.org/abs/2404.04188 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04225 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:16:10 GMT   (12416kb,D)

Title: Twins in rotational spectroscopy: Does a rotational spectrum uniquely
  identify a molecule?
Authors: Marcus Schwarting, Nathan A. Seifert, Michael J. Davis, Ben Blaiszik,
  Ian Foster, and Kirill Prozument
Categories: physics.chem-ph cs.LG
\\
  Rotational spectroscopy is the most accurate method for determining
structures of molecules in the gas phase. It is often assumed that a rotational
spectrum is a unique "fingerprint" of a molecule. The availability of large
molecular databases and the development of artificial intelligence methods for
spectroscopy makes the testing of this assumption timely. In this paper, we
pose the determination of molecular structures from rotational spectra as an
inverse problem. Within this framework, we adopt a funnel-based approach to
search for molecular twins, which are two or more molecules, which have similar
rotational spectra but distinctly different molecular structures. We
demonstrate that there are twins within standard levels of computational
accuracy by generating rotational constants for many molecules from several
large molecular databases, indicating the inverse problem is ill-posed.
However, some twins can be distinguished by increasing the accuracy of the
theoretical methods or by performing additional experiments.
\\ ( https://arxiv.org/abs/2404.04225 ,  12416kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04245 (*cross-listing*)
Date: Fri, 5 Apr 2024 17:51:58 GMT   (2947kb)

Title: Evaluating Adversarial Robustness: A Comparison Of FGSM, Carlini-Wagner
  Attacks, And The Role of Distillation as Defense Mechanism
Authors: Trilokesh Ranjan Sarkar, Nilanjan Das, Pralay Sankar Maitra, Bijoy
  Some, Ritwik Saha, Orijita Adhikary, Bishal Bose, Jaydip Sen
Categories: cs.CR cs.CV cs.LG
Comments: This report pertains to the Capstone Project done by Group 1 of the
  Fall batch of 2023 students at Praxis Tech School, Kolkata, India. The
  reports consists of 35 pages and it includes 15 figures and 10 tables. This
  is the preprint which will be submitted to to an IEEE international
  conference for review
\\
  This technical report delves into an in-depth exploration of adversarial
attacks specifically targeted at Deep Neural Networks (DNNs) utilized for image
classification. The study also investigates defense mechanisms aimed at
bolstering the robustness of machine learning models. The research focuses on
comprehending the ramifications of two prominent attack methodologies: the Fast
Gradient Sign Method (FGSM) and the Carlini-Wagner (CW) approach. These attacks
are examined concerning three pre-trained image classifiers: Resnext50_32x4d,
DenseNet-201, and VGG-19, utilizing the Tiny-ImageNet dataset. Furthermore, the
study proposes the robustness of defensive distillation as a defense mechanism
to counter FGSM and CW attacks. This defense mechanism is evaluated using the
CIFAR-10 dataset, where CNN models, specifically resnet101 and Resnext50_32x4d,
serve as the teacher and student models, respectively. The proposed defensive
distillation model exhibits effectiveness in thwarting attacks such as FGSM.
However, it is noted to remain susceptible to more sophisticated techniques
like the CW attack. The document presents a meticulous validation of the
proposed scheme. It provides detailed and comprehensive results, elucidating
the efficacy and limitations of the defense mechanisms employed. Through
rigorous experimentation and analysis, the study offers insights into the
dynamics of adversarial attacks on DNNs, as well as the effectiveness of
defensive strategies in mitigating their impact.
\\ ( https://arxiv.org/abs/2404.04245 ,  2947kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2308.11066
replaced with revised version Fri, 5 Apr 2024 11:53:41 GMT   (10697kb,D)

Title: CSM-H-R: A Context Modeling Framework in Supporting Reasoning Automation
  for Interoperable Intelligent Systems and Privacy Protection
Authors: Songhui Yue, Xiaoyan Hong, and Randy K. Smith
Categories: cs.AI cs.SY eess.SY
Comments: 13 pages, 10 figures, Keywords: Automation, Context Dynamism, Context
  Modeling, Context Reasoning, Intelligent System, Interoperability, Privacy
  Protection, System Integration
\\ ( https://arxiv.org/abs/2308.11066 ,  10697kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06629
replaced with revised version Thu, 4 Apr 2024 18:08:52 GMT   (2096kb,D)

Title: The Relational Bottleneck as an Inductive Bias for Efficient Abstraction
Authors: Taylor W. Webb, Steven M. Frankland, Awni Altabaa, Simon Segert,
  Kamesh Krishnamurthy, Declan Campbell, Jacob Russin, Tyler Giallanza, Zack
  Dulberg, Randall O'Reilly, John Lafferty, Jonathan D. Cohen
Categories: cs.AI cs.NE
\\ ( https://arxiv.org/abs/2309.06629 ,  2096kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08395
replaced with revised version Fri, 5 Apr 2024 12:59:31 GMT   (1566kb,D)

Title: Learning by Self-Explaining
Authors: Wolfgang Stammer, Felix Friedrich, David Steinmann, Manuel Brack,
  Hikaru Shindo and Kristian Kersting
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2309.08395 ,  1566kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05452
replaced with revised version Fri, 5 Apr 2024 08:07:59 GMT   (961kb,D)

Title: Parrot Mind: Towards Explaining the Complex Task Reasoning of Pretrained
  Large Language Models with Template-Content Structure
Authors: Haotong Yang and Fanxu Meng and Zhouchen Lin and Muhan Zhang
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2310.05452 ,  961kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10167
replaced with revised version Fri, 5 Apr 2024 16:02:40 GMT   (34kb,D)

Title: Efficient Detection of Exchangeable Factors in Factor Graphs
Authors: Malte Luttermann, Johann Machemer, Marcel Gehrke
Categories: cs.AI cs.DS
Comments: Extended version of paper accepted to the Proceedings of the 37th
  International FLAIRS Conference (FLAIRS-24)
\\ ( https://arxiv.org/abs/2403.10167 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02568
replaced with revised version Fri, 5 Apr 2024 09:04:49 GMT   (287kb,D)

Title: Less is More: Understanding Word-level Textual Adversarial Attack via
  n-gram Frequency Descend
Authors: Ning Lu, Shengcai Liu, Zhirui Zhang, Qi Wang, Haifeng Liu, Ke Tang
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: To be published in: 2024 IEEE Conference on Artificial Intelligence
  (CAI 2024)
\\ ( https://arxiv.org/abs/2302.02568 ,  287kb)
------------------------------------------------------------------------------
\\
arXiv:2303.16854
replaced with revised version Fri, 5 Apr 2024 15:19:19 GMT   (254kb,D)

Title: AnnoLLM: Making Large Language Models to Be Better Crowdsourced
  Annotators
Authors: Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen
  Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2303.16854 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10783
replaced with revised version Fri, 5 Apr 2024 01:27:49 GMT   (1160kb,D)

Title: Zero- and Few-Shot Prompting with LLMs: A Comparative Study with
  Fine-tuned Models for Bangla Sentiment Analysis
Authors: Md. Arid Hasan, Shudipta Das, Afiyat Anjum, Firoj Alam, Anika Anjum,
  Avijit Sarker, Sheak Rashed Haider Noori
Categories: cs.CL cs.LG
Comments: Accepted at LREC-COLING 2024. Zero-Shot Prompting, Few-Shot
  Prompting, LLMs, Comparative Study, Fine-tuned Models, Bangla, Sentiment
  Analysis
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2308.10783 ,  1160kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07759
replaced with revised version Fri, 5 Apr 2024 07:53:45 GMT   (7315kb,D)

Title: PROGrasp: Pragmatic Human-Robot Communication for Object Grasping
Authors: Gi-Cheon Kang, Junghyun Kim, Jaein Kim, Byoung-Tak Zhang
Categories: cs.CL cs.RO
Comments: ICRA 2024
\\ ( https://arxiv.org/abs/2309.07759 ,  7315kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08963
replaced with revised version Thu, 4 Apr 2024 21:57:12 GMT   (10353kb,D)

Title: Struc-Bench: Are Large Language Models Really Good at Generating Complex
  Structured Data?
Authors: Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou,
  Arman Cohan, Mark Gerstein
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.08963 ,  10353kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12288
replaced with revised version Thu, 4 Apr 2024 21:25:17 GMT   (1336kb,D)

Title: The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"
Authors: Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper
  Stickland, Tomasz Korbak, Owain Evans
Categories: cs.CL cs.AI cs.LG
Comments: 18 pages, 10 figures
\\ ( https://arxiv.org/abs/2309.12288 ,  1336kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17157
replaced with revised version Fri, 5 Apr 2024 16:18:14 GMT   (9705kb,D)

Title: LatticeGen: A Cooperative Framework which Hides Generated Text in a
  Lattice for Privacy-Aware Generation on Cloud
Authors: Mengke Zhang, Tianxing He, Tianle Wang, Lu Mi, Fatemehsadat
  Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.17157 ,  9705kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00176
replaced with revised version Thu, 4 Apr 2024 20:18:57 GMT   (1650kb,D)

Title: ChipNeMo: Domain-Adapted LLMs for Chip Design
Authors: Mingjie Liu, Teodor-Dumitru Ene, Robert Kirby, Chris Cheng, Nathaniel
  Pinckney, Rongjian Liang, Jonah Alben, Himyanshu Anand, Sanmitra Banerjee,
  Ismet Bayraktaroglu, Bonita Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri,
  Sharon Clay, Bill Dally, Laura Dang, Parikshit Deshpande, Siddhanth Dhodhi,
  Sameer Halepete, Eric Hill, Jiashang Hu, Sumit Jain, Ankit Jindal, Brucek
  Khailany, George Kokai, Kishor Kunal, Xiaowei Li, Charley Lind, Hao Liu,
  Stuart Oberman, Sujeet Omar, Ghasem Pasandi, Sreedhar Pratty, Jonathan
  Raiman, Ambar Sarkar, Zhengjiang Shao, Hanfei Sun, Pratik P Suthar, Varun
  Tej, Walker Turner, Kaizhe Xu, Haoxing Ren
Categories: cs.CL
Comments: Updated results for ChipNeMo-70B model
\\ ( https://arxiv.org/abs/2311.00176 ,  1650kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09559
replaced with revised version Fri, 5 Apr 2024 17:19:04 GMT   (7974kb,D)

Title: Prompt-based Pseudo-labeling Strategy for Sample-Efficient
  Semi-Supervised Extractive Summarization
Authors: Gaurav Sahu, Olga Vechtomova, Issam H. Laradji
Categories: cs.CL cs.AI
Comments: 8 pages, 6 figures, 3 tables
\\ ( https://arxiv.org/abs/2311.09559 ,  7974kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09635
replaced with revised version Thu, 4 Apr 2024 18:54:01 GMT   (10107kb,D)

Title: Evaluating In-Context Learning of Libraries for Code Generation
Authors: Arkil Patel, Siva Reddy, Dzmitry Bahdanau, Pradeep Dasigi
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2311.09635 ,  10107kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10797
replaced with revised version Fri, 5 Apr 2024 06:13:15 GMT   (493kb,D)

Title: TaCo: Enhancing Cross-Lingual Transfer for Low-Resource Languages in
  LLMs through Translation-Assisted Chain-of-Thought Processes
Authors: Bibek Upadhayay and Vahid Behzadan
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2311.10797 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02972
replaced with revised version Fri, 5 Apr 2024 09:32:57 GMT   (383kb,D)

Title: REE-HDSC: Recognizing Extracted Entities for the Historical Database
  Suriname Curacao
Authors: Erik Tjong Kim Sang
Categories: cs.CL
Comments: 24 pages
\\ ( https://arxiv.org/abs/2401.02972 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14295
replaced with revised version Fri, 5 Apr 2024 11:40:50 GMT   (2290kb,D)

Title: Demystifying Chains, Trees, and Graphs of Thoughts
Authors: Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger,
  Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz
  Kwa\'sniewski, J\"urgen M\"uller, Lukas Gianinazzi, Ales Kubicek, Hubert
  Niewiadomski, Aidan O'Mahony, Onur Mutlu, Torsten Hoefler
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.14295 ,  2290kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00157
replaced with revised version Fri, 5 Apr 2024 04:06:51 GMT   (6945kb,D)

Title: Large Language Models for Mathematical Reasoning: Progresses and
  Challenges
Authors: Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, Wenpeng Yin
Categories: cs.CL
Comments: EACL 2024 Student Research Workshop, 8 pages
\\ ( https://arxiv.org/abs/2402.00157 ,  6945kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01740
replaced with revised version Thu, 4 Apr 2024 19:54:07 GMT   (9063kb,D)

Title: Compensatory Biases Under Cognitive Load: Reducing Selection Bias in
  Large Language Models
Authors: J. E. Eicher and R. F. Irgoli\v{c}
Categories: cs.CL cs.AI
Comments: 27 pages, 23 figures
ACM-class: I.2.0
\\ ( https://arxiv.org/abs/2402.01740 ,  9063kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09949
replaced with revised version Thu, 4 Apr 2024 22:50:25 GMT   (119kb,D)

Title: Multi-word Tokenization for Sequence Compression
Authors: Leonidas Gee, Leonardo Rigutini, Marco Ernandes, Andrea Zugarini
Categories: cs.CL cs.LG
Comments: The 2023 Conference on Empirical Methods in Natural Language
  Processing (EMNLP 2023)
Journal-ref: Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing: Industry Track
DOI: 10.18653/v1/2023.emnlp-industry.58
\\ ( https://arxiv.org/abs/2402.09949 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13917
replaced with revised version Fri, 5 Apr 2024 05:26:05 GMT   (5169kb,D)

Title: Could We Have Had Better Multilingual LLMs If English Was Not the
  Central Language?
Authors: Ryandito Diandaru, Lucky Susanto, Zilu Tang, Ayu Purwarianti, Derry
  Wijaya
Categories: cs.CL cs.AI
Comments: TDLE 2024
\\ ( https://arxiv.org/abs/2402.13917 ,  5169kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04460
replaced with revised version Fri, 5 Apr 2024 11:11:01 GMT   (4078kb,D)

Title: Pearl: A Review-driven Persona-Knowledge Grounded Conversational
  Recommendation Dataset
Authors: Minjin Kim, Minju Kim, Hana Kim, Beong-woo Kwak, Soyeon Chun, Hyunseo
  Kim, SeongKu Kang, Youngjae Yu, Jinyoung Yeo, Dongha Lee
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.04460 ,  4078kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14932
replaced with revised version Fri, 5 Apr 2024 10:15:09 GMT   (1083kb,D)

Title: Attention-Driven Reasoning: Unlocking the Potential of Large Language
  Models
Authors: Bingli Liao, Danilo Vasconcellos Vargas
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.14932 ,  1083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14989
replaced with revised version Fri, 5 Apr 2024 07:49:09 GMT   (8023kb,D)

Title: MasonTigers at SemEval-2024 Task 8: Performance Analysis of
  Transformer-based Models on Machine-Generated Text Detection
Authors: Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Dhiman Goswami, Al
  Nahian Bin Emran, Amrita Ganguly, Ozlem Uzuner
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.14989 ,  8023kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01054
replaced with revised version Fri, 5 Apr 2024 02:47:41 GMT   (640kb,D)

Title: Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language
  Model Alignment
Authors: Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2404.01054 ,  640kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01744
replaced with revised version Fri, 5 Apr 2024 11:26:28 GMT   (1646kb,D)

Title: Octopus v2: On-device language model for super agent
Authors: Wei Chen, Zhiyuan Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.01744 ,  1646kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02000
replaced with revised version Fri, 5 Apr 2024 09:07:00 GMT   (53kb)

Title: Africa-Centric Self-Supervised Pre-Training for Multilingual Speech
  Representation in a Sub-Saharan Context
Authors: Antoine Caubri\`ere and Elodie Gauthier
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: To appear in AfricaNLP 2024
\\ ( https://arxiv.org/abs/2404.02000 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02124
replaced with revised version Fri, 5 Apr 2024 00:33:07 GMT   (6948kb,D)

Title: Exploring Automated Distractor Generation for Math Multiple-choice
  Questions via Large Language Models
Authors: Wanyong Feng, Jaewook Lee, Hunter McNichols, Alexander Scarlatos,
  Digory Smith, Simon Woodhead, Nancy Otero Ornelas, Andrew Lan
Categories: cs.CL
Comments: NAACL 2024 findings
\\ ( https://arxiv.org/abs/2404.02124 ,  6948kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02456
replaced with revised version Fri, 5 Apr 2024 04:55:24 GMT   (421kb,D)

Title: PhonologyBench: Evaluating Phonological Skills of Large Language Models
Authors: Ashima Suvarna, Harshita Khandelwal, Nanyun Peng
Categories: cs.CL cs.AI cs.LG cs.SD eess.AS
Comments: 17 pages, 7 figures, 6 tables
\\ ( https://arxiv.org/abs/2404.02456 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03528
replaced with revised version Fri, 5 Apr 2024 09:35:50 GMT   (284kb,D)

Title: BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with
  Semantic Neural Graph Filtering
Authors: Azmine Toushik Wasi and Taki Hasan Rafi and Raima Islam and Dong-Kyu
  Chae
Categories: cs.CL cs.IR cs.LG cs.NE cs.SI
Comments: 7 pages, 3 figures. Accepted to The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)
\\ ( https://arxiv.org/abs/2404.03528 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03563
replaced with revised version Fri, 5 Apr 2024 08:55:20 GMT   (7921kb,D)

Title: EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German
Authors: Regina Stodden
Categories: cs.CL
Comments: Code and resources available at https://github.com/rstodden/easse-de
\\ ( https://arxiv.org/abs/2404.03563 ,  7921kb)
------------------------------------------------------------------------------
\\
arXiv:2207.14800
replaced with revised version Fri, 5 Apr 2024 16:10:34 GMT   (1303kb,D)

Title: Contrastive UCB: Provably Efficient Contrastive Self-Supervised Learning
  in Online Reinforcement Learning
Authors: Shuang Qiu, Lingxiao Wang, Chenjia Bai, Zhuoran Yang, Zhaoran Wang
Categories: cs.LG stat.ML
Comments: ICML 2022
\\ ( https://arxiv.org/abs/2207.14800 ,  1303kb)
------------------------------------------------------------------------------
\\
arXiv:2212.13847
replaced with revised version Fri, 5 Apr 2024 11:06:42 GMT   (7417kb,D)

Title: Heterogeneous Graph Contrastive Learning with Meta-path Contexts and
  Adaptively Weighted Negative Samples
Authors: Jianxiang Yu, Qingqing Ge, Xiang Li, Aoying Zhou
Categories: cs.LG
Comments: This paper has been accepted by TKDE as a regular paper
DOI: 10.1109/TKDE.2024.3377431
\\ ( https://arxiv.org/abs/2212.13847 ,  7417kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11046
replaced with revised version Thu, 4 Apr 2024 19:08:45 GMT   (1686kb,D)

Title: Difference of Submodular Minimization via DC Programming
Authors: Marwa El Halabi, George Orfanides, Tim Hoheisel
Categories: cs.LG cs.DM cs.DS math.OC stat.ML
Comments: Removed minor errors in Proposition 2.7, Theorem 4.3 and Corollary
  4.4. Key results unchanged (see Erratum on p.4). Also fixed typos
Journal-ref: Proceedings of the 40th International Conference on Machine
  Learning, Honolulu, Hawaii, USA. PMLR 202, 2023
\\ ( https://arxiv.org/abs/2305.11046 ,  1686kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14912
replaced with revised version Fri, 5 Apr 2024 09:42:14 GMT   (3425kb,D)

Title: SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from
  Regularized Modeling Perspective
Authors: Yu-Bang Zheng, Xi-Le Zhao, Junhua Zeng, Chao Li, Qibin Zhao, Heng-Chao
  Li, Ting-Zhu Huang
Categories: cs.LG
Comments: This paper is accepted by CVPR 2024 as a Poster (Highlight)
\\ ( https://arxiv.org/abs/2305.14912 ,  3425kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15215
replaced with revised version Thu, 4 Apr 2024 19:30:22 GMT   (2858kb,D)

Title: Shadow Cones: A Generalized Framework for Partial Order Embeddings
Authors: Tao Yu, Toni J.B. Liu, Albert Tseng, Christopher De Sa
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.15215 ,  2858kb)
------------------------------------------------------------------------------
\\
arXiv:2306.14975
replaced with revised version Fri, 5 Apr 2024 10:45:19 GMT   (4592kb,D)

Title: The Underlying Scaling Laws and Universal Statistical Structure of
  Complex Datasets
Authors: Noam Levi and Yaron Oz
Categories: cs.LG cond-mat.dis-nn hep-th math.PR stat.ML
Comments: 21 pages, 9 figures
\\ ( https://arxiv.org/abs/2306.14975 ,  4592kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05194
replaced with revised version Fri, 5 Apr 2024 11:14:39 GMT   (6683kb,D)

Title: Evaluating Pedestrian Trajectory Prediction Methods with Respect to
  Autonomous Driving
Authors: Nico Uhlemann, Felix Fent, Markus Lienkamp
Categories: cs.LG cs.RO
Comments: Accepted in IEEE Transactions on Intelligent Transportation Systems
  (T-ITS); 11 pages, 6 figures, 4 tables
DOI: 10.1109/TITS.2024.3386195
\\ ( https://arxiv.org/abs/2308.05194 ,  6683kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20092
replaced with revised version Fri, 5 Apr 2024 14:35:26 GMT   (2534kb,D)

Title: The Missing U for Efficient Diffusion Models
Authors: Sergio Calvo-Ordonez, Chun-Wun Cheng, Jiahao Huang, Lipei Zhang, Guang
  Yang, Carola-Bibiane Schonlieb, Angelica I Aviles-Rivero
Categories: cs.LG cs.CV
Comments: 23 pages, 14 figures, Accepted at Transactions of Machine Learning
  Research (04/2024)
\\ ( https://arxiv.org/abs/2310.20092 ,  2534kb)
------------------------------------------------------------------------------
\\
arXiv:2311.00875
replaced with revised version Thu, 4 Apr 2024 23:30:37 GMT   (1998kb,D)

Title: Learning Collective Behaviors from Observation
Authors: Jinchao Feng and Ming Zhong
Categories: cs.LG cs.MA math.DS
\\ ( https://arxiv.org/abs/2311.00875 ,  1998kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00111
replaced with revised version Fri, 5 Apr 2024 15:44:08 GMT   (7625kb,D)

Title: Multimodal Learning for Materials
Authors: Viggo Moro, Charlotte Loh, Rumen Dangovski, Ali Ghorashi, Andrew Ma,
  Zhuo Chen, Peter Y. Lu, Thomas Christensen, Marin Solja\v{c}i\'c
Categories: cs.LG cond-mat.mtrl-sci
Comments: 11 pages, 4 figures
\\ ( https://arxiv.org/abs/2312.00111 ,  7625kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00502
replaced with revised version Fri, 5 Apr 2024 11:19:12 GMT   (6490kb,D)

Title: Which Augmentation Should I Use? An Empirical Investigation of
  Augmentations for Self-Supervised Phonocardiogram Representation Learning
Authors: Aristotelis Ballas, Vasileios Papapanagiotou and Christos Diou
Categories: cs.LG cs.SD q-bio.QM
Comments: PREPRINT Manuscript under review
\\ ( https://arxiv.org/abs/2312.00502 ,  6490kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08063
replaced with revised version Fri, 5 Apr 2024 13:42:27 GMT   (5999kb,D)

Title: Estimation of Concept Explanations Should be Uncertainty Aware
Authors: Vihari Piratla, Juyeon Heo, Katherine M. Collins, Sukriti Singh,
  Adrian Weller
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2312.08063 ,  5999kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00867
replaced with revised version Fri, 5 Apr 2024 09:15:05 GMT   (286kb,D)

Title: Tensor Networks for Explainable Machine Learning in Cybersecurity
Authors: Borja Aizpurua, Samuel Palmer, Roman Orus
Categories: cs.LG cs.AI quant-ph
Comments: 9 pages, 9 figures, 2 table, minor typos corrected
\\ ( https://arxiv.org/abs/2401.00867 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02723
replaced with revised version Fri, 5 Apr 2024 07:12:16 GMT   (0kb,I)

Title: Predicting Traffic Flow with Federated Learning and Graph Neural with
  Asynchronous Computations Network
Authors: Muhammad Yaqub, Shahzad Ahmad, Malik Abdul Manan, Imran Shabir Chuhan
Categories: cs.LG cs.CV
Comments: I request to withdraw my paper from arXiv due to significant updates
  and improvements identified post-submission. These enhancements will
  substantially elevate the work's quality and impact. I plan to resubmit the
  revised paper upon completion of these updates. Thank you for accommodating
  this request
\\ ( https://arxiv.org/abs/2401.02723 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15587
replaced with revised version Fri, 5 Apr 2024 10:32:57 GMT   (677kb)

Title: Hyperedge Interaction-aware Hypergraph Neural Network
Authors: Rongping Ye, Xiaobing Pei, Haoran Yang, Ruiqi Wang
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2401.15587 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02081
replaced with revised version Fri, 5 Apr 2024 10:19:43 GMT   (1010kb,D)

Title: Risk-Sensitive Diffusion for Perturbation-Robust Optimization
Authors: Yangming Li, Max Ruiz Luyten, Mihaela van der Schaar
Categories: cs.LG
Comments: Under review paper
\\ ( https://arxiv.org/abs/2402.02081 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04520
replaced with revised version Thu, 4 Apr 2024 21:56:56 GMT   (37kb)

Title: On Computational Limits of Modern Hopfield Models: A Fine-Grained
  Complexity Analysis
Authors: Jerry Yao-Chieh Hu, Thomas Lin, Zhao Song, Han Liu
Categories: cs.LG cs.AI stat.ML
Comments: 31 pages; v2: fix typos; v3: fix typos, add clarifications, add
  references
\\ ( https://arxiv.org/abs/2402.04520 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07087
replaced with revised version Fri, 5 Apr 2024 15:42:13 GMT   (13171kb,D)

Title: Self-Correcting Self-Consuming Loops for Generative Model Training
Authors: Nate Gillman, Michael Freeman, Daksh Aggarwal, Chia-Hong Hsu, Calvin
  Luo, Yonglong Tian, Chen Sun
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: This new version contains updated mathematical results (c.f. Remark
  4.4), as well as experiments for an additional generative modeling task.
  Paper under submission; code is available at
  https://nategillman.com/sc-sc.html
\\ ( https://arxiv.org/abs/2402.07087 ,  13171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08787
replaced with revised version Fri, 5 Apr 2024 02:17:54 GMT   (356kb,D)

Title: Rethinking Machine Unlearning for Large Language Models
Authors: Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie
  Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R. Varshney,
  Mohit Bansal, Sanmi Koyejo, Yang Liu
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2402.08787 ,  356kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10291
replaced with revised version Fri, 5 Apr 2024 03:09:07 GMT   (3563kb,D)

Title: An Evaluation of Real-time Adaptive Sampling Change Point Detection
  Algorithm using KCUSUM
Authors: Vijayalakshmi Saravanan, Perry Siehien, Shinjae Yoo, Hubertus Van Dam,
  Thomas Flynn, Christopher Kelly, Khaled Z Ibrahim
Categories: cs.LG stat.ML
Comments: 16 pages. arXiv admin note: text overlap with arXiv:1903.01661
MSC-class: CCS
\\ ( https://arxiv.org/abs/2402.10291 ,  3563kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01628
replaced with revised version Fri, 5 Apr 2024 14:15:13 GMT   (143kb)

Title: Recent Advances, Applications, and Open Challenges in Machine Learning
  for Health: Reflections from Research Roundtables at ML4H 2023 Symposium
Authors: Hyewon Jeong, Sarah Jabbour, Yuzhe Yang, Rahul Thapta, Hussein
  Mozannar, William Jongwon Han, Nikita Mehandru, Michael Wornow, Vladislav
  Lialin, Xin Liu, Alejandro Lozano, Jiacheng Zhu, Rafal Dariusz Kocielnik,
  Keith Harrigian, Haoran Zhang, Edward Lee, Milos Vukadinovic, Aparna
  Balagopalan, Vincent Jeanselme, Katherine Matton, Ilker Demirel, Jason Fries,
  Parisa Rashidi, Brett Beaulieu-Jones, Xuhai Orson Xu, Matthew McDermott,
  Tristan Naumann, Monica Agrawal, Marinka Zitnik, Berk Ustun, Edward Choi,
  Kristen Yeom, Gamze Gursoy, Marzyeh Ghassemi, Emma Pierson, George Chen,
  Sanjat Kanjilal, Michael Oberst, Linying Zhang, Harvineet Singh, Tom
  Hartvigsen, Helen Zhou, Chinasa T. Okolo
Categories: cs.LG
Comments: ML4H 2023, Research Roundtables
\\ ( https://arxiv.org/abs/2403.01628 ,  143kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06563
replaced with revised version Fri, 5 Apr 2024 06:39:34 GMT   (67kb,D)

Title: Unraveling the Mystery of Scaling Laws: Part I
Authors: Hui Su, Zhi Tian, Xiaoyu Shen, Xunliang Cai
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2403.06563 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12553
replaced with revised version Fri, 5 Apr 2024 16:28:18 GMT   (2839kb,D)

Title: Pretraining Codomain Attention Neural Operators for Solving Multiphysics
  PDEs
Authors: Md Ashiqur Rahman, Robert Joseph George, Mogab Elleithy, Daniel
  Leibovici, Zongyi Li, Boris Bonev, Colin White, Julius Berner, Raymond A.
  Yeh, Jean Kossaifi, Kamyar Azizzadenesheli, Anima Anandkumar
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.12553 ,  2839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13793
replaced with revised version Fri, 5 Apr 2024 12:26:11 GMT   (1042kb,D)

Title: Evaluating Frontier Models for Dangerous Capabilities
Authors: Mary Phuong, Matthew Aitchison, Elliot Catt, Sarah Cogan, Alexandre
  Kaskasoli, Victoria Krakovna, David Lindner, Matthew Rahtz, Yannis Assael,
  Sarah Hodkinson, Heidi Howard, Tom Lieberum, Ramana Kumar, Maria Abi Raad,
  Albert Webson, Lewis Ho, Sharon Lin, Sebastian Farquhar, Marcus Hutter,
  Gregoire Deletang, Anian Ruoss, Seliem El-Sayed, Sasha Brown, Anca Dragan,
  Rohin Shah, Allan Dafoe, Toby Shevlane
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.13793 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13869
replaced with revised version Fri, 5 Apr 2024 15:48:03 GMT   (5450kb,D)

Title: Accurately Predicting Probabilities of Safety-Critical Rare Events for
  Intelligent Systems
Authors: Ruoxuan Bai, Jingxuan Yang, Weiduo Gong, Yi Zhang, Qiujing Lu and Shuo
  Feng
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.13869 ,  5450kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15263
replaced with revised version Thu, 4 Apr 2024 19:09:21 GMT   (1399kb,D)

Title: Federated Bayesian Deep Learning: The Application of Statistical
  Aggregation Methods to Bayesian Models
Authors: John Fischer, Marko Orescanin, Justin Loomis, Patrick McClure
Categories: cs.LG stat.ML
Comments: 22 pages, 9 figures
\\ ( https://arxiv.org/abs/2403.15263 ,  1399kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17753
replaced with revised version Fri, 29 Mar 2024 06:48:37 GMT   (31494kb,D)

Title: CCDSReFormer: Traffic Flow Prediction with a Criss-Crossed Dual-Stream
  Enhanced Rectified Transformer Model
Authors: Zhiqi Shao, Michael G.H. Bell, Ze Wang, D. Glenn Geers, Xusheng Yao,
  and Junbin Gao
Categories: cs.LG
Comments: 18 pages
ACM-class: I.2.0
\\ ( https://arxiv.org/abs/2403.17753 ,  31494kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00477
replaced with revised version Fri, 5 Apr 2024 05:06:39 GMT   (422kb,D)

Title: DE-HNN: An effective neural model for Circuit Netlist representation
Authors: Zhishang Luo, Truong Son Hy, Puoya Tabaghi, Donghyeon Koh, Michael
  Defferrard, Elahe Rezaei, Ryan Carey, Rhett Davis, Rajeev Jain, Yusu Wang
Categories: cs.LG cs.AR
\\ ( https://arxiv.org/abs/2404.00477 ,  422kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00498
replaced with revised version Fri, 5 Apr 2024 00:09:00 GMT   (1888kb,D)

Title: 94% on CIFAR-10 in 3.29 Seconds on a Single GPU
Authors: Keller Jordan
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2404.00498 ,  1888kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00712
replaced with revised version Fri, 5 Apr 2024 02:18:29 GMT   (2179kb,D)

Title: Survey of Computerized Adaptive Testing: A Machine Learning Perspective
Authors: Qi Liu, Yan Zhuang, Haoyang Bi, Zhenya Huang, Weizhe Huang, Jiatong
  Li, Junhao Yu, Zirui Liu, Zirui Hu, Yuting Hong, Zachary A. Pardos, Haiping
  Ma, Mengxiao Zhu, Shijin Wang, Enhong Chen
Categories: cs.LG cs.AI cs.CY cs.IR
\\ ( https://arxiv.org/abs/2404.00712 ,  2179kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01365
replaced with revised version Fri, 5 Apr 2024 14:31:21 GMT   (35261kb,D)

Title: Prompt-prompted Mixture of Experts for Efficient LLM Generation
Authors: Harry Dong, Beidi Chen, Yuejie Chi
Categories: cs.LG cs.AI cs.CL
Comments: Revision 1: Updated abstract with code link; re-ran top-k + sampling
  rows in Table 4, conclusions unchanged
\\ ( https://arxiv.org/abs/2404.01365 ,  35261kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03037
replaced with revised version Fri, 5 Apr 2024 00:53:07 GMT   (10397kb,D)

Title: Model-based Reinforcement Learning for Parameterized Action Spaces
Authors: Renhao Zhang, Haotian Fu, Yilin Miao, George Konidaris
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2404.03037 ,  10397kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00003 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 03:25:04 GMT   (3870kb,D)

Title: Detecting Heart Disease from Multi-View Ultrasound Images via Supervised
  Attention Multiple Instance Learning
Authors: Zhe Huang, Benjamin S. Wessler, Michael C.Hughes
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Echocardiogram; multiple-instance learning; self-supervised learning;
  semi-supervised learning; medical imaging
Journal-ref: MLHC 2023
\\ ( https://arxiv.org/abs/2306.00003 ,  3870kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00040
replaced with revised version Thu, 4 Apr 2024 19:41:09 GMT   (11271kb,D)

Title: DisCo: Disentangled Control for Realistic Human Dance Generation
Authors: Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin,
  Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, Lijuan Wang
Categories: cs.CV cs.AI
Comments: Accepted by CVPR24
\\ ( https://arxiv.org/abs/2307.00040 ,  11271kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20550
replaced with revised version Fri, 5 Apr 2024 05:29:29 GMT   (8538kb,D)

Title: CapsFusion: Rethinking Image-Text Data at Scale
Authors: Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao,
  Xinlong Wang, Jingjing Liu
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: CVPR 2024. Code & Dataset: https://github.com/baaivision/CapsFusion
\\ ( https://arxiv.org/abs/2310.20550 ,  8538kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08577
replaced with revised version Fri, 5 Apr 2024 17:37:36 GMT   (33611kb,D)

Title: Finding AI-Generated Faces in the Wild
Authors: Gonzalo J. Aniano Porcile, Jack Gindi, Shivansh Mundra, James R.
  Verbus, Hany Farid
Categories: cs.CV cs.AI
Comments: to be published as: G.J.A. Porcile, J. Gindi, S. Mundra, J.R. Verbus,
  and H. Farid, Finding AI-Generated Faces in the Wild, Workshop on Media
  Forensics at CVPR, 2024
\\ ( https://arxiv.org/abs/2311.08577 ,  33611kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08537
replaced with revised version Thu, 4 Apr 2024 18:39:50 GMT   (73kb)

Title: Object-Centric Conformance Alignments with Synchronization (Extended
  Version)
Authors: Alessandro Gianola and Marco Montali and Sarah Winkler
Categories: cs.LO cs.AI
\\ ( https://arxiv.org/abs/2312.08537 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14264
replaced with revised version Thu, 4 Apr 2024 18:44:47 GMT   (1505kb)

Title: Experimental demonstration of magnetic tunnel junction-based
  computational random-access memory
Authors: Yang Lv, Brandon R. Zink, Robert P. Bloom, H\"usrev C{\i}lasun, Pravin
  Khanal, Salonik Resch, Zamshed Chowdhury, Ali Habiboglu, Weigang Wang, Sachin
  S. Sapatnekar, Ulya Karpuzcu, Jian-Ping Wang
Categories: cs.ET cond-mat.mes-hall cs.AI cs.AR cs.SY eess.SY
\\ ( https://arxiv.org/abs/2312.14264 ,  1505kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11322
replaced with revised version Fri, 5 Apr 2024 11:51:58 GMT   (1804kb,D)

Title: SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for
  Spiking Neural Network-based Autonomous Agents
Authors: Rachmad Vidya Wicaksana Putra, Muhammad Shafique
Categories: cs.NE cs.AI cs.LG
Comments: 8 pages, 13 figures, 2 tables
\\ ( https://arxiv.org/abs/2402.11322 ,  1804kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15472
replaced with revised version Fri, 5 Apr 2024 11:32:24 GMT   (1103kb,D)

Title: Enhancing Programming Education with ChatGPT: A Case Study on Student
  Perceptions and Interactions in a Python Course
Authors: Boxaun Ma, Li Chen and Shin'ichi Konomi
Categories: cs.CY cs.AI cs.PL
\\ ( https://arxiv.org/abs/2403.15472 ,  1103kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16081
replaced with revised version Fri, 5 Apr 2024 06:14:57 GMT   (976kb)

Title: The Interplay of Learning, Analytics, and Artificial Intelligence in
  Education
Authors: Mutlu Cukurova
Categories: cs.CY cs.AI
Comments: 20 pages, 7 figures, this paper is based on the keynote talk given by
  the author at the ACM International Conference on Learning Analytics &
  Knowledge (LAK) 2024 in Kyoto, Japan.
  https://www.solaresearch.org/events/lak/lak24/keynotes/
\\ ( https://arxiv.org/abs/2403.16081 ,  976kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00185
replaced with revised version Fri, 5 Apr 2024 16:10:44 GMT   (43677kb,D)

Title: On Inherent Adversarial Robustness of Active Vision Systems
Authors: Amitangshu Mukherjee, Timur Ibrayev, and Kaushik Roy
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2404.00185 ,  43677kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02249
replaced with revised version Fri, 5 Apr 2024 02:13:30 GMT   (327kb,D)

Title: RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction
Authors: Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang,
  Shu-Tao Xia
Categories: cs.IR cs.AI cs.LG cs.SI
Comments: Accepted to The ACM Web Conference 2024 (WWW'24, short paper). Data
  and code are available
DOI: 10.1145/3589335.3651550
\\ ( https://arxiv.org/abs/2404.02249 ,  327kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02548
replaced with revised version Fri, 5 Apr 2024 07:05:06 GMT   (468kb,D)

Title: AI-Tutoring in Software Engineering Education
Authors: Eduard Frankford, Clemens Sauerwein, Patrick Bassner, Stephan Krusche
  and Ruth Breu
Categories: cs.SE cs.AI
Comments: 11 pages, 5 figures
DOI: 10.1145/3639474.3640061
\\ ( https://arxiv.org/abs/2404.02548 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02817
replaced with revised version Fri, 5 Apr 2024 09:06:00 GMT   (18395kb,D)

Title: A Survey of Optimization-based Task and Motion Planning: From Classical
  To Learning Approaches
Authors: Zhigen Zhao, Shuo Cheng, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu,
  Ye Zhao
Categories: cs.RO cs.AI
Comments: 24 pages, 12 figures, submitted for review
\\ ( https://arxiv.org/abs/2404.02817 ,  18395kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03493
replaced with revised version Fri, 5 Apr 2024 11:42:57 GMT   (1276kb,D)

Title: A Methodology to Study the Impact of Spiking Neural Network Parameters
  considering Event-Based Automotive Data
Authors: Iqra Bano, Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Muhammad
  Shafique
Categories: cs.NE cs.AI cs.LG cs.RO
Comments: 7 pages, 13 figures, 1 table
\\ ( https://arxiv.org/abs/2404.03493 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03635
replaced with revised version Fri, 5 Apr 2024 17:27:34 GMT   (5336kb,D)

Title: WorDepth: Variational Language Prior for Monocular Depth Estimation
Authors: Ziyao Zeng, Daniel Wang, Fengyu Yang, Hyoungseob Park, Yangchao Wu,
  Stefano Soatto, Byung-Woo Hong, Dong Lao, Alex Wong
Categories: cs.CV cs.AI cs.CL cs.LG cs.MM
\\ ( https://arxiv.org/abs/2404.03635 ,  5336kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03052
replaced with revised version Fri, 5 Apr 2024 04:33:23 GMT   (20853kb,D)

Title: Visual Program Distillation: Distilling Tools and Programmatic Reasoning
  into Vision-Language Models
Authors: Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji
  Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman
Categories: cs.CV cs.CL
Comments: CVPR 2024 Oral
\\ ( https://arxiv.org/abs/2312.03052 ,  20853kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16862
replaced with revised version Thu, 4 Apr 2024 18:53:58 GMT   (5205kb,D)

Title: TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones
Authors: Zhengqing Yuan, Zhaoxu Li, Weiran Huang, Yanfang Ye, Lichao Sun
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2312.16862 ,  5205kb)
------------------------------------------------------------------------------
\\
arXiv:1808.02933 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 14:00:42 GMT   (72024kb,D)

Title: Sequential Monte Carlo Bandits
Authors: I\~nigo Urteaga and Chris H. Wiggins
Categories: stat.ML cs.LG stat.CO
Comments: The software used for this study is publicly available at
  https://github.com/iurteaga/bandits
MSC-class: 62L05, 62L12, 62L20, 62M05
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/1808.02933 ,  72024kb)
------------------------------------------------------------------------------
\\
arXiv:2012.09561 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 08:09:39 GMT   (108kb)

Title: Estimating Mixed-Memberships Using the Symmetric Laplacian Inverse
  Matrix
Authors: Huan Qing and Jingli Wang
Categories: stat.ML cs.LG
Journal-ref: Journal of the Korean Statistical Society. 2023 Mar;52(1):248-64
\\ ( https://arxiv.org/abs/2012.09561 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2112.04389
replaced with revised version Fri, 5 Apr 2024 08:37:35 GMT   (886kb)

Title: Mixed membership distribution-free model
Authors: Huan Qing and Jingli Wang
Categories: cs.SI cs.LG physics.soc-ph stat.ML
Journal-ref: Knowledge and Information Systems. 2024 Feb;66(2):879-904
\\ ( https://arxiv.org/abs/2112.04389 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2201.12900
replaced with revised version Thu, 4 Apr 2024 21:54:18 GMT   (285kb,D)

Title: Learning Optimal Topology for Ad-hoc Robot Networks
Authors: Matin Macktoobian, Zhan Shu, Qing Zhao
Categories: cs.RO cs.LG
Comments: This version is the one published in IEEE Robotics and Automation
  Letters
Journal-ref: IEEE Robotics and Automation Letters, 2023
DOI: 10.1109/LRA.2023.3246845
\\ ( https://arxiv.org/abs/2201.12900 ,  285kb)
------------------------------------------------------------------------------
\\
arXiv:2209.01710
replaced with revised version Tue, 28 Nov 2023 17:59:46 GMT   (3896kb,D)

Title: Perception Simplex: Verifiable Collision Avoidance in Autonomous
  Vehicles Amidst Obstacle Detection Faults
Authors: Ayoosh Bansal, Hunmin Kim, Simon Yu, Bo Li, Naira Hovakimyan, Marco
  Caccamo and Lui Sha
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: arXiv admin note: substantial text overlap with arXiv:2208.14403
ACM-class: D.2.11; I.2.9; C.4; J.7
\\ ( https://arxiv.org/abs/2209.01710 ,  3896kb)
------------------------------------------------------------------------------
\\
arXiv:2209.14089 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 08:44:22 GMT   (348kb,D)

Title: Combining Reinforcement Learning and Tensor Networks, with an
  Application to Dynamical Large Deviations
Authors: Edward Gillman, Dominic C. Rose and Juan P. Garrahan
Categories: cond-mat.stat-mech cs.LG
Comments: [v1]: Combined main text of 6 pages, 3 figures and supplemental
  materials of 7 pages, 1 figure. [v2]: Accepted version, Phys. Rev. Lett.
  Combined main text of 8 pages, 4 figures and supplemental materials of 5
  pages
\\ ( https://arxiv.org/abs/2209.14089 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13409 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 02:26:02 GMT   (147kb)

Title: Efficient Learning of Quantum States Prepared With Few Non-Clifford
  Gates
Authors: Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang
Categories: quant-ph cs.LG
Comments: 54 pages. Merged v3 with arXiv:2308.07175. This version now subsumes
  arXiv:2308.07175
\\ ( https://arxiv.org/abs/2305.13409 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10560 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 08:38:13 GMT   (790kb)

Title: Post-variational quantum neural networks
Authors: Po-Wei Huang, Patrick Rebentrost
Categories: quant-ph cs.LG
Comments: 20 pages, 8 figures; updated numerical experiments
\\ ( https://arxiv.org/abs/2307.10560 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07175 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 21:27:11 GMT   (0kb,I)

Title: Efficient Learning of Quantum States Prepared With Few Non-Clifford
  Gates II: Single-Copy Measurements
Authors: Sabee Grewal, Vishnu Iyer, William Kretschmer, Daniel Liang
Categories: quant-ph cs.LG
Comments: This work has been merged into arXiv:2305.13409
\\ ( https://arxiv.org/abs/2308.07175 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03842 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 11:10:27 GMT   (868kb,D)

Title: Early warning indicators via latent stochastic dynamical systems
Authors: Lingyu Feng, Ting Gao, Wang Xiao and Jinqiao Duan
Categories: stat.ML cs.LG
DOI: 10.1063/5.0195042
\\ ( https://arxiv.org/abs/2309.03842 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17496 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 00:40:28 GMT   (1575kb,D)

Title: Tackling Interference Induced by Data Training Loops in A/B Tests: A
  Weighted Training Approach
Authors: Nian Si
Categories: stat.ME cs.LG econ.EM
\\ ( https://arxiv.org/abs/2310.17496 ,  1575kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09491 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 20:18:27 GMT   (6578kb,D)

Title: Spatial Bayesian Neural Networks
Authors: Andrew Zammit-Mangion, Michael D. Kaminski, Ba-Hien Tran, Maurizio
  Filippone, Noel Cressie
Categories: stat.ML cs.LG
Comments: 35 pages, 21 figures
\\ ( https://arxiv.org/abs/2311.09491 ,  6578kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11167 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 22:54:08 GMT   (2219kb,D)

Title: Benchmarking Machine Learning Models for Quantum Error Correction
Authors: Yue Zhao
Categories: quant-ph cs.LG
Comments: This is a preliminary version of the paper and is subject to further
  revisions
\\ ( https://arxiv.org/abs/2311.11167 ,  2219kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12337
replaced with revised version Thu, 4 Apr 2024 19:04:55 GMT   (9469kb,D)

Title: pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable
  Generalizable 3D Reconstruction
Authors: David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann
Categories: cs.CV cs.LG
Comments: Project page: https://dcharatan.github.io/pixelsplat
\\ ( https://arxiv.org/abs/2312.12337 ,  9469kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01779 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 14:57:56 GMT   (48569kb,D)

Title: Plug-and-Play image restoration with Stochastic deNOising REgularization
Authors: Marien Renaud, Jean Prost, Arthur Leclaire, Nicolas Papadakis
Categories: eess.IV cs.CV cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.01779 ,  48569kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06646 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 15:27:07 GMT   (1804kb)

Title: Diffusion Model-based Probabilistic Downscaling for 180-year East Asian
  Climate Reconstruction
Authors: Fenghua Ling, Zeyu Lu, Jing-Jia Luo, Lei Bai, Swadhin K. Behera,
  Dachao Jin, Baoxiang Pan, Huidong Jiang and Toshio Yamagata
Categories: physics.ao-ph cs.LG physics.geo-ph
\\ ( https://arxiv.org/abs/2402.06646 ,  1804kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11950 (*cross-listing*)
replaced with revised version Fri, 5 Apr 2024 08:51:55 GMT   (911kb)

Title: A novel molecule generative model of VAE combined with Transformer for
  unseen structure generation
Authors: Yasuhiro Yoshikai, Tadahaya Mizuno, Shumpei Nemoto, Hiroyuki Kusuhara
Categories: q-bio.BM cs.LG physics.chem-ph
Comments: 23 pages, 9 figures
ACM-class: J.2; I.2.7
\\ ( https://arxiv.org/abs/2402.11950 ,  911kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15584
replaced with revised version Fri, 5 Apr 2024 17:01:34 GMT   (896kb,D)

Title: State Space Models for Event Cameras
Authors: Nikola Zubi\'c, Mathias Gehrig, Davide Scaramuzza
Categories: cs.CV cs.LG
Comments: 18 pages, 5 figures, 6 tables, CVPR 2024 Camera Ready paper
Journal-ref: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
  Seattle, 2024
\\ ( https://arxiv.org/abs/2402.15584 ,  896kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06546
replaced with revised version Fri, 5 Apr 2024 12:35:06 GMT   (3670kb,D)

Title: OMH: Structured Sparsity via Optimally Matched Hierarchy for
  Unsupervised Semantic Segmentation
Authors: Baran Ozaydin, Tong Zhang, Deblina Bhattacharjee, Sabine S\"usstrunk,
  Mathieu Salzmann
Categories: cs.CV cs.LG
Comments: 11 pages
\\ ( https://arxiv.org/abs/2403.06546 ,  3670kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18578 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 22:38:02 GMT   (1618kb,D)

Title: SteinGen: Generating Fidelitous and Diverse Graph Samples
Authors: Gesine Reinert and Wenkai Xu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2403.18578 ,  1618kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20106
replaced with revised version Fri, 5 Apr 2024 10:29:00 GMT   (6190kb,D)

Title: Learning Enriched Features via Selective State Spaces Model for
  Efficient Image Deblurring
Authors: Hu Gao, Depeng Dang
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.20106 ,  6190kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02072
replaced with revised version Fri, 5 Apr 2024 14:48:43 GMT   (4540kb,D)

Title: EGTR: Extracting Graph from Transformer for Scene Graph Generation
Authors: Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park
Categories: cs.CV cs.LG
Comments: CVPR 2024 (Oral)
\\ ( https://arxiv.org/abs/2404.02072 ,  4540kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
