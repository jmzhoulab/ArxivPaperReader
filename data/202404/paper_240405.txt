Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年4月5日 12:11
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Wed  3 Apr 24 18:00:00 GMT  to  Thu  4 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.03054
Date: Wed, 3 Apr 2024 20:38:22 GMT   (1547kb,D)

Title: Data-Driven Goal Recognition Design for General Behavioral Agents
Authors: Robert Kasumba, Guanghui Yu, Chien-Ju Ho, Sarah Keren, William Yeoh
Categories: cs.AI cs.LG
\\
  Goal recognition design aims to make limited modifications to decision-making
environments with the goal of making it easier to infer the goals of agents
acting within those environments. Although various research efforts have been
made in goal recognition design, existing approaches are computationally
demanding and often assume that agents are (near-)optimal in their
decision-making. To address these limitations, we introduce a data-driven
approach to goal recognition design that can account for agents with general
behavioral models. Following existing literature, we use worst-case
distinctiveness ($\textit{wcd}$) as a measure of the difficulty in inferring
the goal of an agent in a decision-making environment. Our approach begins by
training a machine learning model to predict the $\textit{wcd}$ for a given
environment and the agent behavior model. We then propose a gradient-based
optimization framework that accommodates various constraints to optimize
decision-making environments for enhanced goal recognition. Through extensive
simulations, we demonstrate that our approach outperforms existing methods in
reducing $\textit{wcd}$ and enhancing runtime efficiency in conventional
setups, and it also adapts to scenarios not previously covered in the
literature, such as those involving flexible budget constraints, more complex
environments, and suboptimal agent behavior. Moreover, we have conducted
human-subject experiments which confirm that our method can create environments
that facilitate efficient goal recognition from real-world human
decision-makers.
\\ ( https://arxiv.org/abs/2404.03054 ,  1547kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03441
Date: Thu, 4 Apr 2024 13:39:06 GMT   (683kb,D)

Title: Benchmarking ChatGPT on Algorithmic Reasoning
Authors: Sean McLeish, Avi Schwarzschild, Tom Goldstein
Categories: cs.AI cs.CL cs.LG
\\
  We evaluate ChatGPT's ability to solve algorithm problems from the CLRS
benchmark suite that is designed for GNNs. The benchmark requires the use of a
specified classical algorithm to solve a given problem. We find that ChatGPT
outperforms specialist GNN models, using Python to successfully solve these
problems. This raises new points in the discussion about learning algorithms
with neural networks.
\\ ( https://arxiv.org/abs/2404.03441 ,  683kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03499
Date: Thu, 4 Apr 2024 14:57:32 GMT   (6974kb,D)

Title: Comprehensible Artificial Intelligence on Knowledge Graphs: A survey
Authors: Simon Schramm and Christoph Wehner and Ute Schmid
Categories: cs.AI
DOI: 10.1016/j.websem.2023.100806
\\
  Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research.
\\ ( https://arxiv.org/abs/2404.03499 ,  6974kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03502
Date: Thu, 4 Apr 2024 15:06:23 GMT   (57kb,D)

Title: AI and the Problem of Knowledge Collapse
Authors: Andrew J. Peterson
Categories: cs.AI cs.CY
Comments: 16 pages, 7 figures
ACM-class: I.2.0
\\
  While artificial intelligence has the potential to process vast amounts of
data, generate new insights, and unlock greater productivity, its widespread
adoption may entail unforeseen consequences. We identify conditions under which
AI, by reducing the cost of access to certain modes of knowledge, can
paradoxically harm public understanding. While large language models are
trained on vast amounts of diverse data, they naturally generate output towards
the 'center' of the distribution. This is generally useful, but widespread
reliance on recursive AI systems could lead to a process we define as
"knowledge collapse", and argue this could harm innovation and the richness of
human understanding and culture. However, unlike AI models that cannot choose
what data they are trained on, humans may strategically seek out diverse forms
of knowledge if they perceive them to be worthwhile. To investigate this, we
provide a simple model in which a community of learners or innovators choose to
use traditional methods or to rely on a discounted AI-assisted process and
identify conditions under which knowledge collapse occurs. In our default
model, a 20% discount on AI-generated content generates public beliefs 2.3
times further from the truth than when there is no discount. Finally, based on
the results, we consider further research directions to counteract such
outcomes.
\\ ( https://arxiv.org/abs/2404.03502 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03624
Date: Thu, 4 Apr 2024 17:46:32 GMT   (3992kb,D)

Title: Standardizing Knowledge Engineering Practices with a Reference
  Architecture
Authors: Bradley P. Allen and Filip Ilievski
Categories: cs.AI cs.SE
Comments: 23 pages, 4 figures, 2 tables, camera-ready version, accepted for
  Transactions on Graph Data and Knowledge (TGDK)
\\
  Knowledge engineering is the process of creating and maintaining
knowledge-producing systems. Throughout the history of computer science and AI,
knowledge engineering workflows have been widely used given the importance of
high-quality knowledge for reliable intelligent agents. Meanwhile, the scope of
knowledge engineering, as apparent from its target tasks and use cases, has
been shifting, together with its paradigms such as expert systems, semantic
web, and language modeling. The intended use cases and supported user
requirements between these paradigms have not been analyzed globally, as new
paradigms often satisfy prior pain points while possibly introducing new ones.
The recent abstraction of systemic patterns into a boxology provides an opening
for aligning the requirements and use cases of knowledge engineering with the
systems, components, and software that can satisfy them best. This paper
proposes a vision of harmonizing the best practices in the field of knowledge
engineering by leveraging the software engineering methodology of creating
reference architectures. We describe how a reference architecture can be
iteratively designed and implemented to associate user needs with recurring
systemic patterns, building on top of existing knowledge engineering workflows
and boxologies. We provide a six-step roadmap that can enable the development
of such an architecture, providing an initial design and outcome of the
definition of architectural scope, selection of information sources, and
analysis. We expect that following through on this vision will lead to
well-grounded reference architectures for knowledge engineering, will advance
the ongoing initiatives of organizing the neurosymbolic knowledge engineering
space, and will build new links to the software architectures and data science
communities.
\\ ( https://arxiv.org/abs/2404.03624 ,  3992kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02929
Date: Tue, 2 Apr 2024 09:54:51 GMT   (719kb,D)

Title: Using Large Language Models to Understand Telecom Standards
Authors: Athanasios Karapantelakis and Mukesh Shakur and Alexandros Nikou and
  Farnaz Moradi and Christian Orlog and Fitsum Gaim and Henrik Holm and
  Doumitrou Daniil Nimara and Vincent Huang
Categories: cs.CL cs.AI
Comments: Accepted to ICMLCN 2024, Stockholm, May 2024
\\
  The Third Generation Partnership Project (3GPP) has successfully introduced
standards for global mobility. However, the volume and complexity of these
standards has increased over time, thus complicating access to relevant
information for vendors and service providers. Use of Generative Artificial
Intelligence (AI) and in particular Large Language Models (LLMs), may provide
faster access to relevant information. In this paper, we evaluate the
capability of state-of- art LLMs to be used as Question Answering (QA)
assistants for 3GPP document reference. Our contribution is threefold. First,
we provide a benchmark and measuring methods for evaluating performance of
LLMs. Second, we do data preprocessing and fine-tuning for one of these LLMs
and provide guidelines to increase accuracy of the responses that apply to all
LLMs. Third, we provide a model of our own, TeleRoBERTa, that performs on- par
with foundation LLMs but with an order of magnitude less number of parameters.
Results show that LLMs can be used as a credible reference tool on telecom
technical documents, and thus have potential for a number of different
applications from troubleshooting and maintenance, to network operations and
software product development.
\\ ( https://arxiv.org/abs/2404.02929 ,  719kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02931
Date: Tue, 2 Apr 2024 16:42:44 GMT   (7812kb,D)

Title: READ: Improving Relation Extraction from an ADversarial Perspective
Authors: Dawei Li, William Hogan, Jingbo Shang
Categories: cs.CL cs.AI
Comments: Accepted by findings of NAACL 2024
\\
  Recent works in relation extraction (RE) have achieved promising benchmark
accuracy; however, our adversarial attack experiments show that these works
excessively rely on entities, making their generalization capability
questionable. To address this issue, we propose an adversarial training method
specifically designed for RE. Our approach introduces both sequence- and
token-level perturbations to the sample and uses a separate perturbation
vocabulary to improve the search for entity and context perturbations.
Furthermore, we introduce a probabilistic strategy for leaving clean tokens in
the context during adversarial training. This strategy enables a larger attack
budget for entities and coaxes the model to leverage relational patterns
embedded in the context. Extensive experiments show that compared to various
adversarial training methods, our method significantly improves both the
accuracy and robustness of the model. Additionally, experiments on different
data availability settings highlight the effectiveness of our method in
low-resource scenarios. We also perform in-depth analyses of our proposed
method and provide further hints. We will release our code at
https://github.com/David-Li0406/READ.
\\ ( https://arxiv.org/abs/2404.02931 ,  7812kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02934
Date: Wed, 3 Apr 2024 02:16:37 GMT   (142kb,D)

Title: GreedLlama: Performance of Financial Value-Aligned Large Language Models
  in Moral Reasoning
Authors: Jeffy Yu, Maximilian Huber, Kevin Tang
Categories: cs.CL cs.AI cs.CY cs.LG
Comments: 9 pages, 1 figure
\\
  This paper investigates the ethical implications of aligning Large Language
Models (LLMs) with financial optimization, through the case study of
GreedLlama, a model fine-tuned to prioritize economically beneficial outcomes.
By comparing GreedLlama's performance in moral reasoning tasks to a base Llama2
model, our results highlight a concerning trend: GreedLlama demonstrates a
marked preference for profit over ethical considerations, making morally
appropriate decisions at significantly lower rates than the base model in
scenarios of both low and high moral ambiguity. In low ambiguity situations,
GreedLlama's ethical decisions decreased to 54.4%, compared to the base model's
86.9%, while in high ambiguity contexts, the rate was 47.4% against the base
model's 65.1%. These findings emphasize the risks of single-dimensional value
alignment in LLMs, underscoring the need for integrating broader ethical values
into AI development to ensure decisions are not solely driven by financial
incentives. The study calls for a balanced approach to LLM deployment,
advocating for the incorporation of ethical considerations in models intended
for business applications, particularly in light of the absence of regulatory
oversight.
\\ ( https://arxiv.org/abs/2404.02934 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02935
Date: Wed, 3 Apr 2024 02:52:07 GMT   (446kb,D)

Title: KnowHalu: Hallucination Detection via Multi-Form Knowledge Based Factual
  Checking
Authors: Jiawei Zhang, Chejian Xu, Yu Gai, Freddy Lecue, Dawn Song, Bo Li
Categories: cs.CL cs.AI cs.LG
\\
  This paper introduces KnowHalu, a novel approach for detecting hallucinations
in text generated by large language models (LLMs), utilizing step-wise
reasoning, multi-formulation query, multi-form knowledge for factual checking,
and fusion-based detection mechanism. As LLMs are increasingly applied across
various domains, ensuring that their outputs are not hallucinated is critical.
Recognizing the limitations of existing approaches that either rely on the
self-consistency check of LLMs or perform post-hoc fact-checking without
considering the complexity of queries or the form of knowledge, KnowHalu
proposes a two-phase process for hallucination detection. In the first phase,
it identifies non-fabrication hallucinations--responses that, while factually
correct, are irrelevant or non-specific to the query. The second phase,
multi-form based factual checking, contains five key steps: reasoning and query
decomposition, knowledge retrieval, knowledge optimization, judgment
generation, and judgment aggregation. Our extensive evaluations demonstrate
that KnowHalu significantly outperforms SOTA baselines in detecting
hallucinations across diverse tasks, e.g., improving by 15.65% in QA tasks and
5.50% in summarization tasks, highlighting its effectiveness and versatility in
detecting hallucinations in LLM-generated content.
\\ ( https://arxiv.org/abs/2404.02935 ,  446kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02936
Date: Wed, 3 Apr 2024 04:25:01 GMT   (353kb,D)

Title: Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large
  Language Models
Authors: Jingyang Zhang, Jingwei Sun, Eric Yeats, Yang Ouyang, Martin Kuo,
  Jianyi Zhang, Hao Yang, Hai Li
Categories: cs.CL cs.LG
Comments: Work in progress; project page is available at
  https://zjysteven.github.io/mink-plus-plus/
\\
  The problem of pre-training data detection for large language models (LLMs)
has received growing attention due to its implications in critical issues like
copyright violation and test data contamination. The current state-of-the-art
approach, Min-K%, measures the raw token probability which we argue may not be
the most informative signal. Instead, we propose Min-K%++ to normalize the
token probability with statistics of the categorical distribution over the
whole vocabulary, which accurately reflects the relative likelihood of the
target token compared with other candidate tokens in the vocabulary.
Theoretically, we back up our method by showing that the statistic it estimates
is explicitly optimized during LLM training, thus serving as a reliable
indicator for detecting training data. Empirically, on the WikiMIA benchmark,
Min-K%++ outperforms the SOTA Min-K% by 6.2% to 10.5% in detection AUROC
averaged over five models. On the more challenging MIMIR benchmark, Min-K%++
consistently improves upon Min-K% and performs on par with reference-based
method, despite not requiring an extra reference model.
\\ ( https://arxiv.org/abs/2404.02936 ,  353kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02983
Date: Wed, 3 Apr 2024 18:09:33 GMT   (1466kb,D)

Title: Towards a Fully Interpretable and More Scalable RSA Model for Metaphor
  Understanding
Authors: Gaia Carenini, Luca Bischetti, Walter Schaeken, Valentina Bambini
Categories: cs.CL
\\
  The Rational Speech Act (RSA) model provides a flexible framework to model
pragmatic reasoning in computational terms. However, state-of-the-art RSA
models are still fairly distant from modern machine learning techniques and
present a number of limitations related to their interpretability and
scalability. Here, we introduce a new RSA framework for metaphor understanding
that addresses these limitations by providing an explicit formula - based on
the mutually shared information between the speaker and the listener - for the
estimation of the communicative goal and by learning the rationality parameter
using gradient-based methods. The model was tested against 24 metaphors, not
limited to the conventional $\textit{John-is-a-shark}$ type. Results suggest an
overall strong positive correlation between the distributions generated by the
model and the interpretations obtained from the human behavioral data, which
increased when the intended meaning capitalized on properties that were
inherent to the vehicle concept. Overall, findings suggest that metaphor
processing is well captured by a typicality-based Bayesian model, even when
more scalable and interpretable, opening up possible applications to other
pragmatic phenomena and novel uses for increasing Large Language Models
interpretability. Yet, results highlight that the more creative nuances of
metaphorical meaning, not strictly encoded in the lexical concepts, are a
challenging aspect for machines.
\\ ( https://arxiv.org/abs/2404.02983 ,  1466kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03021
Date: Wed, 3 Apr 2024 19:14:45 GMT   (91kb)

Title: Blessing or curse? A survey on the Impact of Generative AI on Fake News
Authors: Alexander Loth, Martin Kappes, Marc-Oliver Pahl
Categories: cs.CL cs.AI
Comments: 16 pages, 2 figures. Submitted to ACM Transactions on Intelligent
  Systems and Technology (ACM TIST)
ACM-class: I.2.7
\\
  Fake news significantly influence our society. They impact consumers, voters,
and many other societal groups. While Fake News exist for a centuries,
Generative AI brings fake news on a new level. It is now possible to automate
the creation of masses of high-quality individually targeted Fake News. On the
other end, Generative AI can also help detecting Fake News. Both fields are
young but developing fast.
  This survey provides a comprehensive examination of the research and
practical use of Generative AI for Fake News detection and creation in 2024.
Following the Structured Literature Survey approach, the paper synthesizes
current results in the following topic clusters 1) enabling technologies, 2)
creation of Fake News, 3) case study social media as most relevant distribution
channel, 4) detection of Fake News, and 5) deepfakes as upcoming technology.
  The article also identifies current challenges and open issues.
\\ ( https://arxiv.org/abs/2404.03021 ,  91kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03022
Date: Wed, 3 Apr 2024 19:17:43 GMT   (380kb,D)

Title: BCAmirs at SemEval-2024 Task 4: Beyond Words: A Multimodal and
  Multilingual Exploration of Persuasion in Memes
Authors: Amirhossein Abaskohi, Amirhossein Dabiriaghdam, Lele Wang, Giuseppe
  Carenini
Categories: cs.CL cs.CV cs.IT cs.LG math.IT
Comments: 11 pages, 5 tables, 2 figures, Proceedings of the 18th International
  Workshop on Semantic Evaluation (SemEval-2024) @ NAACL 2024
\\
  Memes, combining text and images, frequently use metaphors to convey
persuasive messages, shaping public opinion. Motivated by this, our team
engaged in SemEval-2024 Task 4, a hierarchical multi-label classification task
designed to identify rhetorical and psychological persuasion techniques
embedded within memes. To tackle this problem, we introduced a caption
generation step to assess the modality gap and the impact of additional
semantic information from images, which improved our result. Our best model
utilizes GPT-4 generated captions alongside meme text to fine-tune RoBERTa as
the text encoder and CLIP as the image encoder. It outperforms the baseline by
a large margin in all 12 subtasks. In particular, it ranked in top-3 across all
languages in Subtask 2a, and top-4 in Subtask 2b, demonstrating quantitatively
strong performance. The improvement achieved by the introduced intermediate
step is likely attributable to the metaphorical essence of images that
challenges visual encoders. This highlights the potential for improving
abstract visual semantics encoding.
\\ ( https://arxiv.org/abs/2404.03022 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03028
Date: Wed, 3 Apr 2024 19:31:56 GMT   (18255kb,D)

Title: An Incomplete Loop: Deductive, Inductive, and Abductive Learning in
  Large Language Models
Authors: Emmy Liu, Graham Neubig, Jacob Andreas
Categories: cs.CL
\\
  Modern language models (LMs) can learn to perform new tasks in different
ways: in instruction following, the target task is described explicitly in
natural language; in few-shot prompting, the task is specified implicitly with
a small number of examples; in instruction inference, LMs are presented with
in-context examples and are then prompted to generate a natural language task
description before making predictions. Each of these procedures may be thought
of as invoking a different form of reasoning: instruction following involves
deductive reasoning, few-shot prompting involves inductive reasoning, and
instruction inference involves abductive reasoning. How do these different
capabilities relate? Across four LMs (from the gpt and llama families) and two
learning problems (involving arithmetic functions and machine translation) we
find a strong dissociation between the different types of reasoning: LMs can
sometimes learn effectively from few-shot prompts even when they are unable to
explain their own prediction rules; conversely, they sometimes infer useful
task descriptions while completely failing to learn from human-generated
descriptions of the same task. Our results highlight the non-systematic nature
of reasoning even in some of today's largest LMs, and underscore the fact that
very different learning mechanisms may be invoked by seemingly similar
prompting procedures.
\\ ( https://arxiv.org/abs/2404.03028 ,  18255kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03036
Date: Wed, 3 Apr 2024 19:47:33 GMT   (6960kb,D)

Title: MuLan: A Study of Fact Mutability in Language Models
Authors: Constanza Fierro, Nicolas Garneau, Emanuele Bugliarello, Yova
  Kementchedjhieva, Anders S{\o}gaard
Categories: cs.CL
\\
  Facts are subject to contingencies and can be true or false in different
circumstances. One such contingency is time, wherein some facts mutate over a
given period, e.g., the president of a country or the winner of a championship.
Trustworthy language models ideally identify mutable facts as such and process
them accordingly. We create MuLan, a benchmark for evaluating the ability of
English language models to anticipate time-contingency, covering both 1:1 and
1:N relations. We hypothesize that mutable facts are encoded differently than
immutable ones, hence being easier to update. In a detailed evaluation of six
popular large language models, we consistently find differences in the LLMs'
confidence, representations, and update behavior, depending on the mutability
of a fact. Our findings should inform future work on the injection of and
induction of time-contingent knowledge to/from LLMs.
\\ ( https://arxiv.org/abs/2404.03036 ,  6960kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03049
Date: Wed, 3 Apr 2024 20:30:38 GMT   (60kb)

Title: Language, Environment, and Robotic Navigation
Authors: Johnathan E. Avery
Categories: cs.CL
\\
  This paper explores the integration of linguistic inputs within robotic
navigation systems, drawing upon the symbol interdependency hypothesis to
bridge the divide between symbolic and embodied cognition. It examines previous
work incorporating language and semantics into Neural Network (NN) and
Simultaneous Localization and Mapping (SLAM) approaches, highlighting how these
integrations have advanced the field. By contrasting abstract symbol
manipulation with sensory- motor grounding, we propose a unified framework
where language functions both as an abstract communicative system and as a
grounded representation of perceptual experiences. Our review of cognitive
models of distributional semantics and their application to autonomous agents
underscores the transformative potential of language-integrated systems.
\\ ( https://arxiv.org/abs/2404.03049 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03052
Date: Wed, 3 Apr 2024 20:35:36 GMT   (98kb)

Title: GPT-DETOX: An In-Context Learning-Based Paraphraser for Text
  Detoxification
Authors: Ali Pesaranghader, Nikhil Verma, Manasa Bharadwaj
Categories: cs.CL
Comments: 7 pages, 8 tables. Published in: 2023 International Conference on
  Machine Learning and Applications (ICMLA)
DOI: 10.1109/ICMLA58977.2023.00230
\\
  Harmful and offensive communication or content is detrimental to social
bonding and the mental state of users on social media platforms. Text
detoxification is a crucial task in natural language processing (NLP), where
the goal is removing profanity and toxicity from text while preserving its
content. Supervised and unsupervised learning are common approaches for
designing text detoxification solutions. However, these methods necessitate
fine-tuning, leading to computational overhead. In this paper, we propose
GPT-DETOX as a framework for prompt-based in-context learning for text
detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting
techniques for detoxifying input sentences. To generate few-shot prompts, we
propose two methods: word-matching example selection (WMES) and
context-matching example selection (CMES). We additionally take into account
ensemble in-context learning (EICL) where the ensemble is shaped by base
prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA
as benchmark detoxification datasets. Our experimental results show that the
zero-shot solution achieves promising performance, while our best few-shot
setting outperforms the state-of-the-art models on ParaDetox and shows
comparable results on APPDIA. Our EICL solutions obtain the greatest
performance, adding at least 10% improvement, against both datasets.
\\ ( https://arxiv.org/abs/2404.03052 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03073
Date: Wed, 3 Apr 2024 21:29:40 GMT   (82kb,D)

Title: Mai Ho'om\=auna i ka 'Ai: Language Models Improve Automatic Speech
  Recognition in Hawaiian
Authors: Kaavya Chaparala and Guido Zarrella and Bruce Torres Fischer and Larry
  Kimura and Oiwi Parker Jones
Categories: cs.CL cs.LG cs.SD eess.AS
\\
  In this paper we address the challenge of improving Automatic Speech
Recognition (ASR) for a low-resource language, Hawaiian, by incorporating large
amounts of independent text data into an ASR foundation model, Whisper. To do
this, we train an external language model (LM) on ~1.5M words of Hawaiian text.
We then use the LM to rescore Whisper and compute word error rates (WERs) on a
manually curated test set of labeled Hawaiian data. As a baseline, we use
Whisper without an external LM. Experimental results reveal a small but
significant improvement in WER when ASR outputs are rescored with a Hawaiian
LM. The results support leveraging all available data in the development of ASR
systems for underrepresented languages.
\\ ( https://arxiv.org/abs/2404.03073 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03080
Date: Wed, 3 Apr 2024 21:46:14 GMT   (4005kb,D)

Title: Construction of Functional Materials Knowledge Graph in
  Multidisciplinary Materials Science via Large Language Model
Authors: Yanpeng Ye, Jie Ren, Shaozhou Wang, Yuwei Wan, Imran Razzak, Tong Xie,
  Wenjie Zhang
Categories: cs.CL cs.AI
Comments: 11 pages, 5 figures
\\
  The convergence of materials science and artificial intelligence has unlocked
new opportunities for gathering, analyzing, and generating novel materials
sourced from extensive scientific literature. Despite the potential benefits,
persistent challenges such as manual annotation, precise extraction, and
traceability issues remain. Large language models have emerged as promising
solutions to address these obstacles. This paper introduces Functional
Materials Knowledge Graph (FMKG), a multidisciplinary materials science
knowledge graph. Through the utilization of advanced natural language
processing techniques, extracting millions of entities to form triples from a
corpus comprising all high-quality research papers published in the last
decade. It organizes unstructured information into nine distinct labels,
covering Name, Formula, Acronym, Structure/Phase, Properties, Descriptor,
Synthesis, Characterization Method, Application, and Domain, seamlessly
integrating papers' Digital Object Identifiers. As the latest structured
database for functional materials, FMKG acts as a powerful catalyst for
expediting the development of functional materials and a fundation for building
a more comprehensive material knowledge graph using full paper text.
Furthermore, our research lays the groundwork for practical text-mining-based
knowledge management systems, not only in intricate materials systems but also
applicable to other specialized domains.
\\ ( https://arxiv.org/abs/2404.03080 ,  4005kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03092
Date: Wed, 3 Apr 2024 22:13:04 GMT   (27799kb,D)

Title: Unsupervised, Bottom-up Category Discovery for Symbol Grounding with a
  Curious Robot
Authors: Catherine Henry, Casey Kennington
Categories: cs.CL cs.RO
Comments: 10 pages
\\
  Towards addressing the Symbol Grounding Problem and motivated by early
childhood language development, we leverage a robot which has been equipped
with an approximate model of curiosity with particular focus on bottom-up
building of unsupervised categories grounded in the physical world. That is,
rather than starting with a top-down symbol (e.g., a word referring to an
object) and providing meaning through the application of predetermined samples,
the robot autonomously and gradually breaks up its exploration space into a
series of increasingly specific unlabeled categories at which point an external
expert may optionally provide a symbol association. We extend prior work by
using a robot that can observe the visual world, introducing a higher
dimensional sensory space, and using a more generalizable method of category
building. Our experiments show that the robot learns categories based on
actions and what it visually observes, and that those categories can be
symbolically grounded into.https://info.arxiv.org/help/prep#comments
\\ ( https://arxiv.org/abs/2404.03092 ,  27799kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03098
Date: Wed, 3 Apr 2024 22:39:33 GMT   (1971kb,D)

Title: Exploring the Trade-off Between Model Performance and Explanation
  Plausibility of Text Classifiers Using Human Rationales
Authors: Lucas E. Resck, Marcos M. Raimundo, Jorge Poco
Categories: cs.CL cs.AI cs.LG
Comments: 27 pages, 22 figures, 8 tables; to appear in NAACL Findings 2024;
  code and data available at
  https://github.com/visual-ds/plausible-nlp-explanations
\\
  Saliency post-hoc explainability methods are important tools for
understanding increasingly complex NLP models. While these methods can reflect
the model's reasoning, they may not align with human intuition, making the
explanations not plausible. In this work, we present a methodology for
incorporating rationales, which are text annotations explaining human
decisions, into text classification models. This incorporation enhances the
plausibility of post-hoc explanations while preserving their faithfulness. Our
approach is agnostic to model architectures and explainability methods. We
introduce the rationales during model training by augmenting the standard
cross-entropy loss with a novel loss function inspired by contrastive learning.
By leveraging a multi-objective optimization algorithm, we explore the
trade-off between the two loss functions and generate a Pareto-optimal frontier
of models that balance performance and plausibility. Through extensive
experiments involving diverse models, datasets, and explainability methods, we
demonstrate that our approach significantly enhances the quality of model
explanations without causing substantial (sometimes negligible) degradation in
the original model's performance.
\\ ( https://arxiv.org/abs/2404.03098 ,  1971kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03134
Date: Thu, 4 Apr 2024 01:07:14 GMT   (4283kb,D)

Title: Robust Pronoun Use Fidelity with English LLMs: Are they Reasoning,
  Repeating, or Just Biased?
Authors: Vagrant Gautam, Eileen Bingert, Dawei Zhu, Anne Lauscher, Dietrich
  Klakow
Categories: cs.CL cs.CY
\\
  Robust, faithful and harm-free pronoun use for individuals is an important
goal for language models as their use increases, but prior work tends to study
only one or two of these components at a time. To measure progress towards the
combined goal, we introduce the task of pronoun use fidelity: given a context
introducing a co-referring entity and pronoun, the task is to reuse the correct
pronoun later, independent of potential distractors. We present a
carefully-designed dataset of over 5 million instances to evaluate pronoun use
fidelity in English, and we use it to evaluate 37 popular large language models
across architectures (encoder-only, decoder-only and encoder-decoder) and
scales (11M-70B parameters). We find that while models can mostly faithfully
reuse previously-specified pronouns in the presence of no distractors, they are
significantly worse at processing she/her/her, singular they and neopronouns.
Additionally, models are not robustly faithful to pronouns, as they are easily
distracted. With even one additional sentence containing a distractor pronoun,
accuracy drops on average by 34%. With 5 distractor sentences, accuracy drops
by 52% for decoder-only models and 13% for encoder-only models. We show that
widely-used large language models are still brittle, with large gaps in
reasoning and in processing different pronouns in a setting that is very simple
for humans, and we encourage researchers in bias and reasoning to bridge them.
\\ ( https://arxiv.org/abs/2404.03134 ,  4283kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03150
Date: Thu, 4 Apr 2024 01:50:20 GMT   (38kb,D)

Title: NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation
  using Few-Shot Multi-Choice QA
Authors: Anish Pahilajani, Samyak Rajesh Jain, Devasha Trivedi
Categories: cs.CL cs.AI
\\
  This paper presents our submission to the SemEval 2024 Task 5: The Legal
Argument Reasoning Task in Civil Procedure. We present two approaches to
solving the task of legal answer validation, given an introduction to the case,
a question and an answer candidate. Firstly, we fine-tuned pre-trained
BERT-based models and found that models trained on domain knowledge perform
better. Secondly, we performed few-shot prompting on GPT models and found that
reformulating the answer validation task to be a multiple-choice QA task
remarkably improves the performance of the model. Our best submission is a
BERT-based model that achieved the 7th place out of 20.
\\ ( https://arxiv.org/abs/2404.03150 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03163
Date: Thu, 4 Apr 2024 02:31:05 GMT   (1082kb,D)

Title: Uncertainty in Language Models: Assessment through Rank-Calibration
Authors: Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed Hassani, Insup
  Lee, Osbert Bastani, Edgar Dobriban
Categories: cs.CL cs.AI cs.LG stat.ML
\\
  Language Models (LMs) have shown promising performance in natural language
generation. However, as LMs often generate incorrect or hallucinated responses,
it is crucial to correctly quantify their uncertainty in responding to given
inputs. In addition to verbalized confidence elicited via prompting, many
uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based
measures) have been proposed. However, these measures can differ greatly, and
it is unclear how to compare them, partly because they take values over
different ranges ($e.g.$, $[0,\infty)$ or $[0,1]$). In this work, we address
this issue by developing a novel and practical framework, termed
$Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs.
Our key tenet is that higher uncertainty (or lower confidence) should imply
lower generation quality, on average. Rank-calibration quantifies deviations
from this ideal relationship in a principled manner, without requiring ad hoc
binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The
broad applicability and the granular interpretability of our methods are
demonstrated empirically.
\\ ( https://arxiv.org/abs/2404.03163 ,  1082kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03184
Date: Thu, 4 Apr 2024 03:50:34 GMT   (480kb,D)

Title: The Death of Feature Engineering? BERT with Linguistic Features on SQuAD
  2.0
Authors: Jiawei Li, Yue Zhang
Categories: cs.CL cs.AI
\\
  Machine reading comprehension is an essential natural language processing
task, which takes into a pair of context and query and predicts the
corresponding answer to query. In this project, we developed an end-to-end
question answering model incorporating BERT and additional linguistic features.
We conclude that the BERT base model will be improved by incorporating the
features. The EM score and F1 score are improved 2.17 and 2.14 compared with
BERT(base). Our best single model reaches EM score 76.55 and F1 score 79.97 in
the hidden test set. Our error analysis also shows that the linguistic
architecture can help model understand the context better in that it can locate
answers that BERT only model predicted "No Answer" wrongly.
\\ ( https://arxiv.org/abs/2404.03184 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03189
Date: Thu, 4 Apr 2024 04:20:04 GMT   (860kb,D)

Title: The Probabilities Also Matter: A More Faithful Metric for Faithfulness
  of Free-Text Explanations in Large Language Models
Authors: Noah Y. Siegel, Oana-Maria Camburu, Nicolas Heess, Maria Perez-Ortiz
Categories: cs.CL cs.AI
Comments: 19 pages, 2 figures
\\
  In order to oversee advanced AI systems, it is important to understand their
underlying decision-making process. When prompted, large language models (LLMs)
can provide natural language explanations or reasoning traces that sound
plausible and receive high ratings from human annotators. However, it is
unclear to what extent these explanations are faithful, i.e., truly capture the
factors responsible for the model's predictions. In this work, we introduce
Correlational Explanatory Faithfulness (CEF), a metric that can be used in
faithfulness tests based on input interventions. Previous metrics used in such
tests take into account only binary changes in the predictions. Our metric
accounts for the total shift in the model's predicted label distribution, more
accurately reflecting the explanations' faithfulness. We then introduce the
Correlational Counterfactual Test (CCT) by instantiating CEF on the
Counterfactual Test (CT) from Atanasova et al. (2023). We evaluate the
faithfulness of free-text explanations generated by few-shot-prompted LLMs from
the Llama2 family on three NLP tasks. We find that our metric measures aspects
of faithfulness which the CT misses.
\\ ( https://arxiv.org/abs/2404.03189 ,  860kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03196
Date: Thu, 4 Apr 2024 04:49:46 GMT   (3409kb,D)

Title: Okay, Let's Do This! Modeling Event Coreference with Generated
  Rationales and Knowledge Distillation
Authors: Abhijnan Nath, Shadi Manafi, Avyakta Chelle and Nikhil Krishnaswamy
Categories: cs.CL
Comments: To be published in NAACL 2024 Main
\\
  In NLP, Event Coreference Resolution (ECR) is the task of connecting event
clusters that refer to the same underlying real-life event, usually via neural
systems. In this work, we investigate using abductive free-text rationales
(FTRs) generated by modern autoregressive LLMs as distant supervision of
smaller student models for cross-document coreference (CDCR) of events. We
implement novel rationale-oriented event clustering and knowledge distillation
methods for event coreference scoring that leverage enriched information from
the FTRs for improved CDCR without additional annotation or expensive document
clustering. Our model using coreference specific knowledge distillation
achieves SOTA B3 F1 on the ECB+ and GVC corpora and we establish a new baseline
on the AIDA Phase 1 corpus. Our code can be found at
https://github.com/csu-signal/llama_cdcr
\\ ( https://arxiv.org/abs/2404.03196 ,  3409kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03259
Date: Thu, 4 Apr 2024 07:31:56 GMT   (1721kb)

Title: Enhancing the Performance of Aspect-Based Sentiment Analysis Systems
Authors: Chen Li, Jinli Zhang, Huidong Tang, Peng Ju, Debo Cheng, Yasuhiko
  Morimoto
Categories: cs.CL cs.AI
\\
  Aspect-based sentiment analysis aims to predict sentiment polarity with fine
granularity. While Graph Convolutional Networks (GCNs) are widely utilized for
sentimental feature extraction, their naive application for syntactic feature
extraction can compromise information preservation. This study introduces an
innovative edge-enhanced GCN, named SentiSys, to navigate the syntactic graph
while preserving intact feature information, leading to enhanced performance.
Specifically,we first integrate a bidirectional long short-term memory
(Bi-LSTM) network and a self-attention-based transformer. This combination
facilitates effective text encoding, preventing the loss of information and
predicting long dependency text. A bidirectional GCN (Bi-GCN) with message
passing is then employed to encode relationships between entities.
Additionally, unnecessary information is filtered out using an aspect-specific
masking technique. To validate the effectiveness of our proposed model, we
conduct extensive evaluation experiments and ablation studies on four benchmark
datasets. The results consistently demonstrate improved performance in
aspect-based sentiment analysis when employing SentiSys. This approach
successfully addresses the challenges associated with syntactic feature
extraction, highlighting its potential for advancing sentiment analysis
methodologies.
\\ ( https://arxiv.org/abs/2404.03259 ,  1721kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03278
Date: Thu, 4 Apr 2024 08:04:24 GMT   (7368kb,D)

Title: Evaluating Document Simplification: On the Importance of Separately
  Assessing Simplicity and Meaning Preservation
Authors: Liam Cripwell, Jo\"el Legrand, Claire Gardent
Categories: cs.CL
Comments: Accepted to READI Workshop 2024
\\
  Text simplification intends to make a text easier to read while preserving
its core meaning. Intuitively and as shown in previous works, these two
dimensions (simplification and meaning preservation) are often-times inversely
correlated. An overly conservative text will fail to simplify sufficiently,
whereas extreme simplification will degrade meaning preservation. Yet, popular
evaluation metrics either aggregate meaning preservation and simplification
into a single score (SARI, LENS), or target meaning preservation alone
(BERTScore, QuestEval). Moreover, these metrics usually require a set of
references and most previous work has only focused on sentence-level
simplification. In this paper, we focus on the evaluation of document-level
text simplification and compare existing models using distinct metrics for
meaning preservation and simplification. We leverage existing metrics from
similar tasks and introduce a reference-less metric variant for simplicity,
showing that models are mostly biased towards either simplification or meaning
preservation, seldom performing well on both dimensions. Making use of the fact
that the metrics we use are all reference-less, we also investigate the
performance of existing models when applied to unseen data (where reference
simplifications are unavailable).
\\ ( https://arxiv.org/abs/2404.03278 ,  7368kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03301
Date: Thu, 4 Apr 2024 08:52:25 GMT   (551kb,D)

Title: Probing Large Language Models for Scalar Adjective Lexical Semantics and
  Scalar Diversity Pragmatics
Authors: Fangru Lin, Daniel Altshuler, Janet B. Pierrehumbert
Categories: cs.CL
Comments: Accepted for the 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024)
\\
  Scalar adjectives pertain to various domain scales and vary in intensity
within each scale (e.g. certain is more intense than likely on the likelihood
scale). Scalar implicatures arise from the consideration of alternative
statements which could have been made. They can be triggered by scalar
adjectives and require listeners to reason pragmatically about them. Some
scalar adjectives are more likely to trigger scalar implicatures than others.
This phenomenon is referred to as scalar diversity. In this study, we probe
different families of Large Language Models such as GPT-4 for their knowledge
of the lexical semantics of scalar adjectives and one specific aspect of their
pragmatics, namely scalar diversity. We find that they encode rich
lexical-semantic information about scalar adjectives. However, the rich
lexical-semantic knowledge does not entail a good understanding of scalar
diversity. We also compare current models of different sizes and complexities
and find that larger models are not always better. Finally, we explain our
probing results by leveraging linguistic intuitions and model training
objectives.
\\ ( https://arxiv.org/abs/2404.03301 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03302
Date: Thu, 4 Apr 2024 08:52:30 GMT   (203kb,D)

Title: How Easily do Irrelevant Inputs Skew the Responses of Large Language
  Models?
Authors: Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao
Categories: cs.CL
Comments: Work in progress
\\
  By leveraging the retrieval of information from external knowledge databases,
Large Language Models (LLMs) exhibit enhanced capabilities for accomplishing
many knowledge-intensive tasks. However, due to the inherent flaws of current
retrieval systems, there might exist irrelevant information within those
retrieving top-ranked passages. In this work, we present a comprehensive
investigation into the robustness of LLMs to different types of irrelevant
information under various conditions. We initially introduce a framework to
construct high-quality irrelevant information that ranges from semantically
unrelated, partially related, and related to questions. Furthermore, our
analysis demonstrates that the constructed irrelevant information not only
scores highly on similarity metrics, being highly retrieved by existing
systems, but also bears semantic connections to the context. Our investigation
reveals that current LLMs still face challenges in discriminating highly
semantically related information and can be easily distracted by these
irrelevant yet misleading contents. Besides, we also find that current
solutions for handling irrelevant information have limitations in improving the
robustness of LLMs to such distractions. Resources are available at
https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information.
\\ ( https://arxiv.org/abs/2404.03302 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03304
Date: Thu, 4 Apr 2024 08:56:48 GMT   (12010kb,D)

Title: Concept -- An Evaluation Protocol on Conversation Recommender Systems
  with System- and User-centric Factors
Authors: Chen Huang, Peixin Qin, Yang Deng, Wenqiang Lei, Jiancheng Lv,
  Tat-Seng Chua
Categories: cs.CL cs.AI
Comments: 27 pages, 18 tables, and 10 figures
\\
  The conversational recommendation system (CRS) has been criticized regarding
its user experience in real-world scenarios, despite recent significant
progress achieved in academia. Existing evaluation protocols for CRS may
prioritize system-centric factors such as effectiveness and fluency in
conversation while neglecting user-centric aspects. Thus, we propose a new and
inclusive evaluation protocol, Concept, which integrates both system- and
user-centric factors. We conceptualise three key characteristics in
representing such factors and further divide them into six primary abilities.
To implement Concept, we adopt a LLM-based user simulator and evaluator with
scoring rubrics that are tailored for each primary ability. Our protocol,
Concept, serves a dual purpose. First, it provides an overview of the pros and
cons in current CRS models. Second, it pinpoints the problem of low usability
in the "omnipotent" ChatGPT and offers a comprehensive reference guide for
evaluating CRS, thereby setting the foundation for CRS improvement.
\\ ( https://arxiv.org/abs/2404.03304 ,  12010kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03312
Date: Thu, 4 Apr 2024 09:17:22 GMT   (152kb,D)

Title: M3TCM: Multi-modal Multi-task Context Model for Utterance Classification
  in Motivational Interviews
Authors: Sayed Muddashir Hossain, Jan Alexandersson, Philipp M\"uller
Categories: cs.CL cs.SD eess.AS
Comments: Accepted for publication at LREC-COLING'24
\\
  Accurate utterance classification in motivational interviews is crucial to
automatically understand the quality and dynamics of client-therapist
interaction, and it can serve as a key input for systems mediating such
interactions. Motivational interviews exhibit three important characteristics.
First, there are two distinct roles, namely client and therapist. Second, they
are often highly emotionally charged, which can be expressed both in text and
in prosody. Finally, context is of central importance to classify any given
utterance. Previous works did not adequately incorporate all of these
characteristics into utterance classification approaches for mental health
dialogues. In contrast, we present M3TCM, a Multi-modal, Multi-task Context
Model for utterance classification. Our approach for the first time employs
multi-task learning to effectively model both joint and individual components
of therapist and client behaviour. Furthermore, M3TCM integrates information
from the text and speech modality as well as the conversation context. With our
novel approach, we outperform the state of the art for utterance classification
on the recently introduced AnnoMI dataset with a relative improvement of 20%
for the client- and by 15% for therapist utterance classification. In extensive
ablation studies, we quantify the improvement resulting from each contribution.
\\ ( https://arxiv.org/abs/2404.03312 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03324
Date: Thu, 4 Apr 2024 09:48:14 GMT   (728kb,D)

Title: A Comparative Analysis of Word-Level Metric Differential Privacy:
  Benchmarking The Privacy-Utility Trade-off
Authors: Stephen Meisenbacher, Nihildev Nandakumar, Alexandra Klymenko and
  Florian Matthes
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  The application of Differential Privacy to Natural Language Processing
techniques has emerged in relevance in recent years, with an increasing number
of studies published in established NLP outlets. In particular, the adaptation
of Differential Privacy for use in NLP tasks has first focused on the
$\textit{word-level}$, where calibrated noise is added to word embedding
vectors to achieve "noisy" representations. To this end, several
implementations have appeared in the literature, each presenting an alternative
method of achieving word-level Differential Privacy. Although each of these
includes its own evaluation, no comparative analysis has been performed to
investigate the performance of such methods relative to each other. In this
work, we conduct such an analysis, comparing seven different algorithms on two
NLP tasks with varying hyperparameters, including the $\textit{epsilon
($\varepsilon$)}$ parameter, or privacy budget. In addition, we provide an
in-depth analysis of the results with a focus on the privacy-utility trade-off,
as well as open-source our implementation code for further reproduction. As a
result of our analysis, we give insight into the benefits and challenges of
word-level Differential Privacy, and accordingly, we suggest concrete steps
forward for the research field.
\\ ( https://arxiv.org/abs/2404.03324 ,  728kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03344
Date: Thu, 4 Apr 2024 10:18:03 GMT   (1346kb,D)

Title: Schroedinger's Threshold: When the AUC doesn't predict Accuracy
Authors: Juri Opitz
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  The Area Under Curve measure (AUC) seems apt to evaluate and compare diverse
models, possibly without calibration. An important example of AUC application
is the evaluation and benchmarking of models that predict faithfulness of
generated text. But we show that the AUC yields an academic and optimistic
notion of accuracy that can misalign with the actual accuracy observed in
application, yielding significant changes in benchmark rankings. To paint a
more realistic picture of downstream model performance (and prepare a model for
actual application), we explore different calibration modes, testing
calibration data and method.
\\ ( https://arxiv.org/abs/2404.03344 ,  1346kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03353
Date: Thu, 4 Apr 2024 10:45:07 GMT   (3547kb,D)

Title: Towards Pareto Optimal Throughput in Small Language Model Serving
Authors: Pol G.Recasens and Yue Zhu and Chen Wang and Eun Kyung Lee and Olivier
  Tardieu and Alaa Youssef and Jordi Torres and Josep Ll. Berral
Categories: cs.CL
Comments: It is going to be published at EuroMLSys'24
DOI: 10.1145/3642970.3655832
\\
  Large language models (LLMs) have revolutionized the state-of-the-art of many
different natural language processing tasks. Although serving LLMs is
computationally and memory demanding, the rise of Small Language Models (SLMs)
offers new opportunities for resource-constrained users, who now are able to
serve small models with cutting-edge performance. In this paper, we present a
set of experiments designed to benchmark SLM inference at performance and
energy levels. Our analysis provides a new perspective in serving, highlighting
that the small memory footprint of SLMs allows for reaching the Pareto-optimal
throughput within the resource capacity of a single accelerator. In this
regard, we present an initial set of findings demonstrating how model
replication can effectively improve resource utilization for serving SLMs.
\\ ( https://arxiv.org/abs/2404.03353 ,  3547kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03361
Date: Thu, 4 Apr 2024 11:03:33 GMT   (2522kb,D)

Title: nicolay-r at SemEval-2024 Task 3: Using Flan-T5 for Reasoning Emotion
  Cause in Conversations with Chain-of-Thought on Emotion States
Authors: Nicolay Rusnachenko, Huizhi Liang
Categories: cs.CL
Comments: Ranked 3rd-4th place (F1-proportional) and 5th place (F1-strict) in
  SemEval'24 Task 3, Subtask 1, to appear in SemEval-2024 proceedings
\\
  Emotion expression is one of the essential traits of conversations. It may be
self-related or caused by another speaker. The variety of reasons may serve as
a source of the further emotion causes: conversation history, speaker's
emotional state, etc. Inspired by the most recent advances in Chain-of-Thought,
in this work, we exploit the existing three-hop reasoning approach (THOR) to
perform large language model instruction-tuning for answering: emotion states
(THOR-state), and emotion caused by one speaker to the other (THOR-cause). We
equip THOR-cause with the reasoning revision (rr) for devising a reasoning path
in fine-tuning. In particular, we rely on the annotated speaker emotion states
to revise reasoning path. Our final submission, based on Flan-T5-base (250M)
and the rule-based span correction technique, preliminary tuned with THOR-state
and fine-tuned with THOR-cause-rr on competition training data, results in 3rd
and 4th places (F1-proportional) and 5th place (F1-strict) among 15
participating teams. Our THOR implementation fork is publicly available:
https://github.com/nicolay-r/THOR-ECAC
\\ ( https://arxiv.org/abs/2404.03361 ,  2522kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03381
Date: Thu, 4 Apr 2024 11:27:54 GMT   (7603kb,D)

Title: Learning to Plan and Generate Text with Citations
Authors: Constanza Fierro, Reinald Kim Amplayo, Fantine Huot, Nicola De Cao,
  Joshua Maynez, Shashi Narayan, Mirella Lapata
Categories: cs.CL
\\
  The increasing demand for the deployment of LLMs in information-seeking
scenarios has spurred efforts in creating verifiable systems, which generate
responses to queries along with supporting evidence. In this paper, we explore
the attribution capabilities of plan-based models which have been recently
shown to improve the faithfulness, grounding, and controllability of generated
text. We conceptualize plans as a sequence of questions which serve as
blueprints of the generated content and its organization. We propose two
attribution models that utilize different variants of blueprints, an
abstractive model where questions are generated from scratch, and an extractive
model where questions are copied from the input. Experiments on long-form
question-answering show that planning consistently improves attribution
quality. Moreover, the citations generated by blueprint models are more
accurate compared to those obtained from LLM-based pipelines lacking a planning
component.
\\ ( https://arxiv.org/abs/2404.03381 ,  7603kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03414
Date: Thu, 4 Apr 2024 12:46:37 GMT   (4019kb,D)

Title: Can Small Language Models Help Large Language Models Reason Better?:
  LM-Guided Chain-of-Thought
Authors: Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, Kai-Wei
  Chang, Chengwei Su
Categories: cs.CL cs.AI
Comments: This paper is accepted to LREC-COLING 2024
\\
  We introduce a novel framework, LM-Guided CoT, that leverages a lightweight
(i.e., <1B) language model (LM) for guiding a black-box large (i.e., >10B) LM
in reasoning tasks. Specifically, the lightweight LM first generates a
rationale for each input instance. The Frozen large LM is then prompted to
predict a task output based on the rationale generated by the lightweight LM.
Our approach is resource-efficient in the sense that it only requires training
the lightweight LM. We optimize the model through 1) knowledge distillation and
2) reinforcement learning from rationale-oriented and task-oriented reward
signals. We assess our method with multi-hop extractive question answering (QA)
benchmarks, HotpotQA, and 2WikiMultiHopQA. Experimental results show that our
approach outperforms all baselines regarding answer prediction accuracy. We
also find that reinforcement learning helps the model to produce higher-quality
rationales with improved QA performance.
\\ ( https://arxiv.org/abs/2404.03414 ,  4019kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03428
Date: Thu, 4 Apr 2024 13:15:28 GMT   (282kb,D)

Title: Edisum: Summarizing and Explaining Wikipedia Edits at Scale
Authors: Marija \v{S}akota, Isaac Johnson, Guosheng Feng, Robert West
Categories: cs.CL
\\
  An edit summary is a succinct comment written by a Wikipedia editor
explaining the nature of, and reasons for, an edit to a Wikipedia page. Edit
summaries are crucial for maintaining the encyclopedia: they are the first
thing seen by content moderators and help them decide whether to accept or
reject an edit. Additionally, edit summaries constitute a valuable data source
for researchers. Unfortunately, as we show, for many edits, summaries are
either missing or incomplete. To overcome this problem and help editors write
useful edit summaries, we propose a model for recommending edit summaries
generated by a language model trained to produce good edit summaries given the
representation of an edit diff. This is a challenging task for multiple
reasons, including mixed-quality training data, the need to understand not only
what was changed in the article but also why it was changed, and efficiency
requirements imposed by the scale of Wikipedia. We address these challenges by
curating a mix of human and synthetically generated training data and
fine-tuning a generative language model sufficiently small to be used on
Wikipedia at scale. Our model performs on par with human editors. Commercial
large language models are able to solve this task better than human editors,
but would be too expensive to run on Wikipedia at scale. More broadly, this
paper showcases how language modeling technology can be used to support humans
in maintaining one of the largest and most visible projects on the Web.
\\ ( https://arxiv.org/abs/2404.03428 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03429
Date: Thu, 4 Apr 2024 13:22:28 GMT   (1989kb,D)

Title: Scaffolding Language Learning via Multi-modal Tutoring Systems with
  Pedagogical Instructions
Authors: Zhengyuan Liu, Stella Xin Yin, Carolyn Lee, Nancy F. Chen
Categories: cs.CL
\\
  Intelligent tutoring systems (ITSs) that imitate human tutors and aim to
provide immediate and customized instructions or feedback to learners have
shown their effectiveness in education. With the emergence of generative
artificial intelligence, large language models (LLMs) further entitle the
systems to complex and coherent conversational interactions. These systems
would be of great help in language education as it involves developing skills
in communication, which, however, drew relatively less attention. Additionally,
due to the complicated cognitive development at younger ages, more endeavors
are needed for practical uses. Scaffolding refers to a teaching technique where
teachers provide support and guidance to students for learning and developing
new concepts or skills. It is an effective way to support diverse learning
needs, goals, processes, and outcomes. In this work, we investigate how
pedagogical instructions facilitate the scaffolding in ITSs, by conducting a
case study on guiding children to describe images for language learning. We
construct different types of scaffolding tutoring systems grounded in four
fundamental learning theories: knowledge construction, inquiry-based learning,
dialogic teaching, and zone of proximal development. For qualitative and
quantitative analyses, we build and refine a seven-dimension rubric to evaluate
the scaffolding process. In our experiment on GPT-4V, we observe that LLMs
demonstrate strong potential to follow pedagogical instructions and achieve
self-paced learning in different student groups. Moreover, we extend our
evaluation framework from a manual to an automated approach, paving the way to
benchmark various conversational tutoring systems.
\\ ( https://arxiv.org/abs/2404.03429 ,  1989kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03437
Date: Thu, 4 Apr 2024 13:36:01 GMT   (5572kb,D)

Title: Knowledge Graph Representation for Political Information Sources
Authors: Tinatin Osmonova, Alexey Tikhonov and Ivan P. Yamshchikov
Categories: cs.CL cs.SI
\\
  With the rise of computational social science, many scholars utilize data
analysis and natural language processing tools to analyze social media, news
articles, and other accessible data sources for examining political and social
discourse. Particularly, the study of the emergence of echo-chambers due to the
dissemination of specific information has become a topic of interest in mixed
methods research areas. In this paper, we analyze data collected from two news
portals, Breitbart News (BN) and New York Times (NYT) to prove the hypothesis
that the formation of echo-chambers can be partially explained on the level of
an individual information consumption rather than a collective topology of
individuals' social networks. Our research findings are presented through
knowledge graphs, utilizing a dataset spanning 11.5 years gathered from BN and
NYT media portals. We demonstrate that the application of knowledge
representation techniques to the aforementioned news streams highlights,
contrary to common assumptions, shows relative "internal" neutrality of both
sources and polarizing attitude towards a small fraction of entities.
Additionally, we argue that such characteristics in information sources lead to
fundamental disparities in audience worldviews, potentially acting as a
catalyst for the formation of echo-chambers.
\\ ( https://arxiv.org/abs/2404.03437 ,  5572kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03471
Date: Thu, 4 Apr 2024 14:24:06 GMT   (884kb,D)

Title: Reevaluating Bias Detection in Language Models: The Role of Implicit
  Norm
Authors: Farnaz Kohankhaki, Jacob-Junqi Tian, David Emerson, Laleh
  Seyyed-Kalantari, Faiza Khan Khattak
Categories: cs.CL cs.CY cs.LG
\\
  Large language models (LLMs), trained on vast datasets, can carry biases that
manifest in various forms, from overt discrimination to implicit stereotypes.
One facet of bias is performance disparities in LLMs, often harming
underprivileged groups, such as racial minorities. A common approach to
quantifying bias is to use template-based bias probes, which explicitly state
group membership (e.g. White) and evaluate if the outcome of a task, sentiment
analysis for instance, is invariant to the change of group membership (e.g.
change White race to Black). This approach is widely used in bias
quantification. However, in this work, we find evidence of an unexpectedly
overlooked consequence of using template-based probes for LLM bias
quantification. We find that in doing so, text examples associated with White
ethnicities appear to be classified as exhibiting negative sentiment at
elevated rates. We hypothesize that the scenario arises artificially through a
mismatch between the pre-training text of LLMs and the templates used to
measure bias through reporting bias, unstated norms that imply group membership
without explicit statement. Our finding highlights the potential misleading
impact of varying group membership through explicit mention in bias
quantification
\\ ( https://arxiv.org/abs/2404.03471 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03486
Date: Thu, 4 Apr 2024 14:40:07 GMT   (737kb,D)

Title: Generative AI and Teachers - For Us or Against Us? A Case Study
Authors: Jenny Pettersson, Elias Hult, Tim Eriksson and Tosin Adewumi
Categories: cs.CL
Comments: 7 pages, 3 figures
\\
  We present insightful results of a survey on the adoption of generative
artificial intelligence (GenAI) by university teachers in their teaching
activities. The transformation of education by GenAI, particularly large
language models (LLMs), has been presenting both opportunities and challenges,
including cheating by students. We prepared the online survey according to best
practices and the questions were created by the authors, who have pedagogy
experience. The survey contained 12 questions and a pilot study was first
conducted. The survey was then sent to all teachers in multiple departments
across different campuses of the university of interest in Sweden: Lule{\aa}
University of Technology. The survey was available in both Swedish and English.
The results show that 35 teachers (more than half) use GenAI out of 67
respondents. Preparation is the teaching activity with the most frequency that
GenAI is used for and ChatGPT is the most commonly used GenAI. 59% say it has
impacted their teaching, however, 55% say there should be legislation around
the use of GenAI, especially as inaccuracies and cheating are the biggest
concerns.
\\ ( https://arxiv.org/abs/2404.03486 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03491
Date: Thu, 4 Apr 2024 14:45:26 GMT   (2075kb,D)

Title: A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded
  Dialogue Generation
Authors: Jifan Yu, Xiaohan Zhang, Yifan Xu, Xuanyu Lei, Zijun Yao, Jing Zhang,
  Lei Hou, Juanzi Li
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024
\\
  Empowered by the large-scale pretrained language models, existing dialogue
systems have demonstrated impressive performance conducting fluent and
natural-sounding conversations. However, they are still plagued by the
hallucination problem, causing unpredictable factual errors in the generated
responses. Recently, knowledge-grounded dialogue generation models, that
intentionally invoke external knowledge resources to more informative
responses, are also proven to be effective in reducing hallucination. Following
the idea of getting high-quality knowledge, a few efforts have achieved pretty
good performance on this issue. As some inevitable knowledge noises may also
lead to hallucinations, it is emergent to investigate the reason and future
directions for building noise-tolerant methods in KGD tasks. In this paper, we
analyze the causal story behind this problem with counterfactual reasoning
methods. Based on the causal effect analysis, we propose a possible solution
for alleviating the hallucination in KGD by exploiting the dialogue-knowledge
interaction. Experimental results of our example implementation show that this
method can reduce hallucination without disrupting other dialogue performance,
while keeping adaptive to different generation models. We hope our efforts can
support and call for more attention to developing lightweight techniques
towards robust and trusty dialogue systems.
\\ ( https://arxiv.org/abs/2404.03491 ,  2075kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03514
Date: Thu, 4 Apr 2024 15:21:22 GMT   (8985kb,D)

Title: Learn When (not) to Trust Language Models: A Privacy-Centric Adaptive
  Model-Aware Approach
Authors: Chengkai Huang, Rui Wang, Kaige Xie, Tong Yu, Lina Yao
Categories: cs.CL cs.AI
\\
  Retrieval-augmented large language models (LLMs) have been remarkably
competent in various NLP tasks. Despite their great success, the knowledge
provided by the retrieval process is not always useful for improving the model
prediction, since in some samples LLMs may already be quite knowledgeable and
thus be able to answer the question correctly without retrieval. Aiming to save
the cost of retrieval, previous work has proposed to determine when to do/skip
the retrieval in a data-aware manner by analyzing the LLMs' pretraining data.
However, these data-aware methods pose privacy risks and memory limitations,
especially when requiring access to sensitive or extensive pretraining data.
Moreover, these methods offer limited adaptability under fine-tuning or
continual learning settings. We hypothesize that token embeddings are able to
capture the model's intrinsic knowledge, which offers a safer and more
straightforward way to judge the need for retrieval without the privacy risks
associated with accessing pre-training data. Moreover, it alleviates the need
to retain all the data utilized during model pre-training, necessitating only
the upkeep of the token embeddings. Extensive experiments and in-depth analyses
demonstrate the superiority of our model-aware approach.
\\ ( https://arxiv.org/abs/2404.03514 ,  8985kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03528
Date: Thu, 4 Apr 2024 15:31:21 GMT   (293kb,D)

Title: BanglaAutoKG: Automatic Bangla Knowledge Graph Construction with
  Semantic Neural Graph Filtering
Authors: Azmine Toushik Wasi and Taki Hasan Rafi and Raima Islam and Dong-Kyu
  Chae
Categories: cs.CL cs.IR cs.LG cs.NE cs.SI
Comments: 7 pages, 3 figures. Accepted to The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
  (LREC-COLING 2024)
\\
  Knowledge Graphs (KGs) have proven essential in information processing and
reasoning applications because they link related entities and give context-rich
information, supporting efficient information retrieval and knowledge
discovery; presenting information flow in a very effective manner. Despite
being widely used globally, Bangla is relatively underrepresented in KGs due to
a lack of comprehensive datasets, encoders, NER (named entity recognition)
models, POS (part-of-speech) taggers, and lemmatizers, hindering efficient
information processing and reasoning applications in the language. Addressing
the KG scarcity in Bengali, we propose BanglaAutoKG, a pioneering framework
that is able to automatically construct Bengali KGs from any Bangla text. We
utilize multilingual LLMs to understand various languages and correlate
entities and relations universally. By employing a translation dictionary to
identify English equivalents and extracting word features from pre-trained BERT
models, we construct the foundational KG. To reduce noise and align word
embeddings with our goal, we employ graph-based polynomial filters. Lastly, we
implement a GNN-based semantic filter, which elevates contextual understanding
and trims unnecessary edges, culminating in the formation of the definitive KG.
Empirical findings and case studies demonstrate the universal effectiveness of
our model, capable of autonomously constructing semantically enriched KGs from
any text.
\\ ( https://arxiv.org/abs/2404.03528 ,  293kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03532
Date: Thu, 4 Apr 2024 15:36:53 GMT   (4535kb,D)

Title: Evaluating Generative Language Models in Information Extraction as
  Subjective Question Correction
Authors: Yuchen Fan, Yantao Liu, Zijun Yao, Jifan Yu, Lei Hou, Juanzi Li
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024, short paper
\\
  Modern Large Language Models (LLMs) have showcased remarkable prowess in
various tasks necessitating sophisticated cognitive behaviors. Nevertheless, a
paradoxical performance discrepancy is observed, where these models
underperform in seemingly elementary tasks like relation extraction and event
extraction due to two issues in conventional evaluation. (1) The imprecision of
existing evaluation metrics that struggle to effectively gauge semantic
consistency between model outputs and ground truth, and (2) The inherent
incompleteness of evaluation benchmarks, primarily due to restrictive human
annotation schemas, resulting in underestimated LLM performances. Inspired by
the principles in subjective question correction, we propose a new evaluation
method, SQC-Score. This method innovatively utilizes LLMs, fine-tuned through
subjective question correction data, to refine matching between model outputs
and golden labels. Additionally, by incorporating a Natural Language Inference
(NLI) model, SQC-Score enriches golden labels, addressing benchmark
incompleteness by acknowledging correct yet previously omitted answers. Results
on three information extraction tasks show that SQC-Score is more preferred by
human annotators than the baseline metrics. Utilizing SQC-Score, we conduct a
comprehensive evaluation of the state-of-the-art LLMs and provide insights for
future research for information extraction. Dataset and associated codes can be
accessed at https://github.com/THU-KEG/SQC-Score.
\\ ( https://arxiv.org/abs/2404.03532 ,  4535kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03555
Date: Thu, 4 Apr 2024 16:07:06 GMT   (859kb,D)

Title: From News to Summaries: Building a Hungarian Corpus for Extractive and
  Abstractive Summarization
Authors: Botond Barta, Dorina Lakatos, Attila Nagy, Mil\'an Konor Nyist, Judit
  \'Acs
Categories: cs.CL
\\
  Training summarization models requires substantial amounts of training data.
However for less resourceful languages like Hungarian, openly available models
and datasets are notably scarce. To address this gap our paper introduces
HunSum-2 an open-source Hungarian corpus suitable for training abstractive and
extractive summarization models. The dataset is assembled from segments of the
Common Crawl corpus undergoing thorough cleaning, preprocessing and
deduplication. In addition to abstractive summarization we generate
sentence-level labels for extractive summarization using sentence similarity.
We train baseline models for both extractive and abstractive summarization
using the collected dataset. To demonstrate the effectiveness of the trained
models, we perform both quantitative and qualitative evaluation. Our dataset,
models and code are publicly available, encouraging replication, further
research, and real-world applications across various domains.
\\ ( https://arxiv.org/abs/2404.03555 ,  859kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03558
Date: Thu, 4 Apr 2024 16:15:23 GMT   (4706kb,D)

Title: How does Multi-Task Training Affect Transformer In-Context Capabilities?
  Investigations with Function Classes
Authors: Harmon Bhasin, Timothy Ossowski, Yiqiao Zhong, Junjie Hu
Categories: cs.CL cs.LG
Comments: Accepted to NAACL 2024
\\
  Large language models (LLM) have recently shown the extraordinary ability to
perform unseen tasks based on few-shot examples provided as text, also known as
in-context learning (ICL). While recent works have attempted to understand the
mechanisms driving ICL, few have explored training strategies that incentivize
these models to generalize to multiple tasks. Multi-task learning (MTL) for
generalist models is a promising direction that offers transfer learning
potential, enabling large parameterized models to be trained from simpler,
related tasks. In this work, we investigate the combination of MTL with ICL to
build models that efficiently learn tasks while being robust to
out-of-distribution examples. We propose several effective curriculum learning
strategies that allow ICL models to achieve higher data efficiency and more
stable convergence. Our experiments reveal that ICL models can effectively
learn difficult tasks by training on progressively harder tasks while mixing in
prior tasks, denoted as mixed curriculum in this work. Our code and models are
available at https://github.com/harmonbhasin/curriculum_learning_icl .
\\ ( https://arxiv.org/abs/2404.03558 ,  4706kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03561
Date: Thu, 4 Apr 2024 16:16:53 GMT   (7768kb,D)

Title: Select and Summarize: Scene Saliency for Movie Script Summarization
Authors: Rohit Saxena, Frank Keller
Categories: cs.CL
Comments: NAACL 2024 Findings
\\
  Abstractive summarization for long-form narrative texts such as movie scripts
is challenging due to the computational and memory constraints of current
language models. A movie script typically comprises a large number of scenes;
however, only a fraction of these scenes are salient, i.e., important for
understanding the overall narrative. The salience of a scene can be
operationalized by considering it as salient if it is mentioned in the summary.
Automatically identifying salient scenes is difficult due to the lack of
suitable datasets. In this work, we introduce a scene saliency dataset that
consists of human-annotated salient scenes for 100 movies. We propose a
two-stage abstractive summarization approach which first identifies the salient
scenes in script and then generates a summary using only those scenes. Using
QA-based evaluation, we show that our model outperforms previous
state-of-the-art summarization methods and reflects the information content of
a movie more accurately than a model that takes the whole movie script as
input.
\\ ( https://arxiv.org/abs/2404.03561 ,  7768kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03563
Date: Thu, 4 Apr 2024 16:18:37 GMT   (7925kb,D)

Title: EASSE-DE: Easier Automatic Sentence Simplification Evaluation for German
Authors: Regina Stodden
Categories: cs.CL
Comments: Code and resources available at https://github.com/rstodden/easse-de
\\
  In this work, we propose EASSE-multi, a framework for easier automatic
sentence evaluation for languages other than English. Compared to the original
EASSE framework, EASSE-multi does not focus only on English. It contains
tokenizers and versions of text simplification evaluation metrics which are
suitable for multiple languages. In this paper, we exemplify the usage of
EASSE-multi for German TS, resulting in EASSE-DE. Further, we compare text
simplification results when evaluating with different language or tokenization
settings of the metrics. Based on this, we formulate recommendations on how to
make the evaluation of (German) TS models more transparent and better
comparable. The code of EASSE-multi and its German specialisation (EASSE-DE)
can be found at https://github.com/rstodden/easse-de.
\\ ( https://arxiv.org/abs/2404.03563 ,  7925kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03565
Date: Thu, 4 Apr 2024 16:20:34 GMT   (10979kb,D)

Title: Personalized LLM Response Generation with Parameterized Memory Injection
Authors: Kai Zhang, Lizhi Qing, Yangyang Kang, Xiaozhong Liu
Categories: cs.CL
\\
  Large Language Models (LLMs) have exhibited remarkable proficiency in
comprehending and generating natural language. On the other hand, personalized
LLM response generation holds the potential to offer substantial benefits for
individuals in critical areas such as medical. Existing research has explored
memory-augmented methods to prompt the LLM with pre-stored user-specific
knowledge for personalized response generation in terms of new queries. We
contend that such paradigm is unable to perceive fine-granularity information.
In this study, we propose a novel \textbf{M}emory-\textbf{i}njected approach
using parameter-efficient fine-tuning (PEFT) and along with a Bayesian
Optimisation searching strategy to achieve \textbf{L}LM
\textbf{P}ersonalization(\textbf{MiLP}).
\\ ( https://arxiv.org/abs/2404.03565 ,  10979kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03577
Date: Thu, 4 Apr 2024 16:40:11 GMT   (3883kb,D)

Title: Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning
  Skills in Large Language Models
Authors: Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei
  Hou, Juanzi Li
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024 as long paper
\\
  Providing knowledge documents for large language models (LLMs) has emerged as
a promising solution to update the static knowledge inherent in their
parameters. However, knowledge in the document may conflict with the memory of
LLMs due to outdated or incorrect knowledge in the LLMs' parameters. This leads
to the necessity of examining the capability of LLMs to assimilate supplemental
external knowledge that conflicts with their memory. While previous studies
have explained to what extent LLMs extract conflicting knowledge from the
provided text, they neglect the necessity to reason with conflicting knowledge.
Furthermore, there lack a detailed analysis on strategies to enable LLMs to
resolve conflicting knowledge via prompting, decoding strategy, and supervised
fine-tuning. To address these limitations, we construct a new dataset, dubbed
KNOT, for knowledge conflict resolution examination in the form of question
answering. KNOT facilitates in-depth analysis by dividing reasoning with
conflicting knowledge into three levels: (1) Direct Extraction, which directly
extracts conflicting knowledge to answer questions. (2) Explicit Reasoning,
which reasons with conflicting knowledge when the reasoning path is explicitly
provided in the question. (3) Implicit Reasoning, where reasoning with
conflicting knowledge requires LLMs to infer the reasoning path independently
to answer questions. We also conduct extensive experiments on KNOT to establish
empirical guidelines for LLMs to utilize conflicting knowledge in complex
circumstances. Dataset and associated codes can be accessed at
https://github.com/THU-KEG/KNOT .
\\ ( https://arxiv.org/abs/2404.03577 ,  3883kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03592
Date: Thu, 4 Apr 2024 17:00:37 GMT   (1456kb,D)

Title: ReFT: Representation Finetuning for Language Models
Authors: Zhengxuan Wu and Aryaman Arora and Zheng Wang and Atticus Geiger and
  Dan Jurafsky and Christopher D. Manning and Christopher Potts
Categories: cs.CL cs.AI cs.LG
Comments: 40 pages, preprint
\\
  Parameter-efficient fine-tuning (PEFT) methods seek to adapt large models via
updates to a small number of weights. However, much prior interpretability work
has shown that representations encode rich semantic information, suggesting
that editing representations might be a more powerful alternative. Here, we
pursue this hypothesis by developing a family of $\textbf{Representation
Finetuning (ReFT)}$ methods. ReFT methods operate on a frozen base model and
learn task-specific interventions on hidden representations. We define a strong
instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT). LoReFT is
a drop-in replacement for existing PEFTs and learns interventions that are
10x-50x more parameter-efficient than prior state-of-the-art PEFTs. We showcase
LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks,
Alpaca-Eval v1.0, and GLUE. In all these evaluations, LoReFT delivers the best
balance of efficiency and performance, and almost always outperforms
state-of-the-art PEFTs. We release a generic ReFT training library publicly at
https://github.com/stanfordnlp/pyreft.
\\ ( https://arxiv.org/abs/2404.03592 ,  1456kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03598
Date: Thu, 4 Apr 2024 17:09:52 GMT   (256kb,D)

Title: Intent Detection and Entity Extraction from BioMedical Literature
Authors: Ankan Mullick, Mukur Gupta, Pawan Goyal
Categories: cs.CL
Comments: Accepted to CL4Health LREC-COLING 2024
\\
  Biomedical queries have become increasingly prevalent in web searches,
reflecting the growing interest in accessing biomedical literature. Despite
recent research on large-language models (LLMs) motivated by endeavours to
attain generalized intelligence, their efficacy in replacing task and
domain-specific natural language understanding approaches remains questionable.
In this paper, we address this question by conducting a comprehensive empirical
evaluation of intent detection and named entity recognition (NER) tasks from
biomedical text. We show that Supervised Fine Tuned approaches are still
relevant and more effective than general-purpose LLMs. Biomedical transformer
models such as PubMedBERT can surpass ChatGPT on NER task with only 5
supervised examples.
\\ ( https://arxiv.org/abs/2404.03598 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03602
Date: Thu, 4 Apr 2024 17:19:47 GMT   (906kb,D)

Title: Evaluating LLMs at Detecting Errors in LLM Responses
Authors: Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn,
  Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth
  Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui
  Zhang
Categories: cs.CL
Comments: Benchmark and code: https://github.com/psunlpgroup/ReaLMistake
\\
  With Large Language Models (LLMs) being widely used across various tasks,
detecting errors in their responses is increasingly crucial. However, little
research has been conducted on error detection of LLM responses. Collecting
error annotations on LLM responses is challenging due to the subjective nature
of many NLP tasks, and thus previous research focuses on tasks of little
practical value (e.g., word sorting) or limited error types (e.g., faithfulness
in summarization). This work introduces ReaLMistake, the first error detection
benchmark consisting of objective, realistic, and diverse errors made by LLMs.
ReaLMistake contains three challenging and meaningful tasks that introduce
objectively assessable errors in four categories (reasoning correctness,
instruction-following, context-faithfulness, and parameterized knowledge),
eliciting naturally observed and diverse errors in responses of GPT-4 and Llama
2 70B annotated by experts. We use ReaLMistake to evaluate error detectors
based on 12 LLMs. Our findings show: 1) Top LLMs like GPT-4 and Claude 3 detect
errors made by LLMs at very low recall, and all LLM-based error detectors
perform much worse than humans. 2) Explanations by LLM-based error detectors
lack reliability. 3) LLMs-based error detection is sensitive to small changes
in prompts but remains challenging to improve. 4) Popular approaches to
improving LLMs, including self-consistency and majority vote, do not improve
the error detection performance. Our benchmark and code are provided at
https://github.com/psunlpgroup/ReaLMistake.
\\ ( https://arxiv.org/abs/2404.03602 ,  906kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03608
Date: Thu, 4 Apr 2024 17:31:32 GMT   (711kb,D)

Title: Sailor: Open Language Models for South-East Asia
Authors: Longxu Dou, Qian Liu, Guangtao Zeng, Jia Guo, Jiahui Zhou, Wei Lu, Min
  Lin
Categories: cs.CL cs.AI
Comments: Code is available at https://github.com/sail-sg/sailor-llm
\\
  We present Sailor, a family of open language models ranging from 0.5B to 7B
parameters, tailored for South-East Asian (SEA) languages. These models are
continually pre-trained from Qwen1.5, a great language model for multilingual
use cases. From Qwen1.5, Sailor models accept 200B to 400B tokens, primarily
covering the languages of English, Chinese, Vietnamese, Thai, Indonesian,
Malay, and Lao. The training leverages several techniques, including BPE
dropout for improving the model robustness, aggressive data cleaning and
deduplication, and small proxy models to optimize data mixture. Experimental
results on four typical tasks indicate that Sailor models demonstrate strong
performance across different benchmarks, including commonsense reasoning,
question answering, reading comprehension and examination. Embracing the
open-source spirit, we share our insights through this report to spark a wider
interest in developing large language models for multilingual use cases.
\\ ( https://arxiv.org/abs/2404.03608 ,  711kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03622
Date: Thu, 4 Apr 2024 17:45:08 GMT   (12911kb,D)

Title: Visualization-of-Thought Elicits Spatial Reasoning in Large Language
  Models
Authors: Wenshan Wu, Shaoguang Mao, Yadong Zhang, Yan Xia, Li Dong, Lei Cui,
  Furu Wei
Categories: cs.CL
\\
  Large language models (LLMs) have exhibited impressive performance in
language comprehension and various reasoning tasks. However, their abilities in
spatial reasoning, a crucial aspect of human cognition, remain relatively
unexplored. Human possess a remarkable ability to create mental images of
unseen objects and actions through a process known as \textbf{the Mind's Eye},
enabling the imagination of the unseen world. Inspired by this cognitive
capacity, we propose Visualization-of-Thought (\textbf{VoT}) prompting. VoT
aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces,
thereby guiding subsequent reasoning steps. We employed VoT for multi-hop
spatial reasoning tasks, including natural language navigation, visual
navigation, and visual tiling in 2D grid worlds. Experimental results
demonstrated that VoT significantly enhances the spatial reasoning abilities of
LLMs. Notably, VoT outperformed existing multimodal large language models
(MLLMs) in these tasks. While VoT works surprisingly well on LLMs, the ability
to generate \textit{mental images} to facilitate spatial reasoning resembles
the mind's eye process, suggesting its potential viability in MLLMs.
\\ ( https://arxiv.org/abs/2404.03622 ,  12911kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03623
Date: Thu, 4 Apr 2024 17:45:59 GMT   (5378kb,D)

Title: Unveiling LLMs: The Evolution of Latent Representations in a Temporal
  Knowledge Graph
Authors: Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea
  Passerini
Categories: cs.CL cs.AI cs.CY
Comments: Preprint. Under review. 10 pages, 7 figures
\\
  Large Language Models (LLMs) demonstrate an impressive capacity to recall a
vast range of common factual knowledge information. However, unravelling the
underlying reasoning of LLMs and explaining their internal mechanisms of
exploiting this factual knowledge remain active areas of investigation. Our
work analyzes the factual knowledge encoded in the latent representation of
LLMs when prompted to assess the truthfulness of factual claims. We propose an
end-to-end framework that jointly decodes the factual knowledge embedded in the
latent space of LLMs from a vector space to a set of ground predicates and
represents its evolution across the layers using a temporal knowledge graph.
Our framework relies on the technique of activation patching which intervenes
in the inference computation of a model by dynamically altering its latent
representations. Consequently, we neither rely on external models nor training
processes. We showcase our framework with local and global interpretability
analyses using two claim verification datasets: FEVER and CLIMATE-FEVER. The
local interpretability analysis exposes different latent errors from
representation to multi-hop reasoning errors. On the other hand, the global
analysis uncovered patterns in the underlying evolution of the model's factual
knowledge (e.g., store-and-seek factual information). By enabling graph-based
analyses of the latent representations, this work represents a step towards the
mechanistic interpretability of LLMs.
\\ ( https://arxiv.org/abs/2404.03623 ,  5378kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03626
Date: Thu, 4 Apr 2024 17:48:28 GMT   (320kb,D)

Title: Training LLMs over Neurally Compressed Text
Authors: Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam
  Roberts, Jascha Sohl-Dickstein, and Noah Constant
Categories: cs.CL cs.LG
\\
  In this paper, we explore the idea of training large language models (LLMs)
over highly compressed text. While standard subword tokenizers compress text by
a small factor, neural text compressors can achieve much higher rates of
compression. If it were possible to train LLMs directly over neurally
compressed text, this would confer advantages in training and serving
efficiency, as well as easier handling of long text spans. The main obstacle to
this goal is that strong compression tends to produce opaque outputs that are
not well-suited for learning. In particular, we find that text na\"ively
compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome
this, we propose Equal-Info Windows, a novel compression technique whereby text
is segmented into blocks that each compress to the same bit length. Using this
method, we demonstrate effective learning over neurally compressed text that
improves with scale, and outperforms byte-level baselines by a wide margin on
perplexity and inference speed benchmarks. While our method delivers worse
perplexity than subword tokenizers for models trained with the same parameter
count, it has the benefit of shorter sequence lengths. Shorter sequence lengths
require fewer autoregressive generation steps, and reduce latency. Finally, we
provide extensive analysis of the properties that contribute to learnability,
and offer concrete suggestions for how to further improve the performance of
high-compression tokenizers.
\\ ( https://arxiv.org/abs/2404.03626 ,  320kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03646
Date: Thu, 4 Apr 2024 17:58:31 GMT   (1287kb,D)

Title: Locating and Editing Factual Associations in Mamba
Authors: Arnab Sen Sharma and David Atkinson and David Bau
Categories: cs.CL
\\
  We investigate the mechanisms of factual recall in the Mamba state space
model. Our work is inspired by previous findings in autoregressive transformer
language models suggesting that their knowledge recall is localized to
particular modules at specific token locations; we therefore ask whether
factual recall in Mamba can be similarly localized. To investigate this, we
conduct four lines of experiments on Mamba. First, we apply causal tracing or
interchange interventions to localize key components inside Mamba that are
responsible for recalling facts, revealing that specific components within
middle layers show strong causal effects at the last token of the subject,
while the causal effect of intervening on later layers is most pronounced at
the last token of the prompt, matching previous findings on autoregressive
transformers. Second, we show that rank-one model editing methods can
successfully insert facts at specific locations, again resembling findings on
transformer models. Third, we examine the linearity of Mamba's representations
of factual relations. Finally we adapt attention-knockout techniques to Mamba
to dissect information flow during factual recall. We compare Mamba directly to
a similar-sized transformer and conclude that despite significant differences
in architectural approach, when it comes to factual recall, the two
architectures share many similarities.
\\ ( https://arxiv.org/abs/2404.03646 ,  1287kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03648
Date: Thu, 4 Apr 2024 17:58:40 GMT   (27538kb,D)

Title: AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web
  Navigating Agent
Authors: Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo
  Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang
Categories: cs.CL
\\
  Large language models (LLMs) have fueled many intelligent agent tasks, such
as web navigation -- but most existing agents perform far from satisfying in
real-world webpages due to three factors: (1) the versatility of actions on
webpages, (2) HTML text exceeding model processing capacity, and (3) the
complexity of decision-making due to the open-domain nature of web. In light of
the challenge, we develop AutoWebGLM, a GPT-4-outperforming automated web
navigation agent built upon ChatGLM3-6B. Inspired by human browsing patterns,
we design an HTML simplification algorithm to represent webpages, preserving
vital information succinctly. We employ a hybrid human-AI method to build web
browsing data for curriculum training. Then, we bootstrap the model by
reinforcement learning and rejection sampling to further facilitate webpage
comprehension, browser operations, and efficient task decomposition by itself.
For testing, we establish a bilingual benchmark -- AutoWebBench -- for
real-world web browsing tasks. We evaluate AutoWebGLM across diverse web
navigation benchmarks, revealing its improvements but also underlying
challenges to tackle real environments. Related code, model, and data will be
released at \url{https://github.com/THUDM/AutoWebGLM}.
\\ ( https://arxiv.org/abs/2404.03648 ,  27538kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02926
Date: Mon, 1 Apr 2024 23:09:52 GMT   (721kb,D)

Title: A High Order Solver for Signature Kernels
Authors: Maud Lemercier and Terry Lyons
Categories: cs.LG math.AP
MSC-class: 60L10, 60L20
\\
  Signature kernels are at the core of several machine learning algorithms for
analysing multivariate time series. The kernel of two bounded variation paths
(such as piecewise linear interpolations of time series data) is typically
computed by solving a Goursat problem for a hyperbolic partial differential
equation (PDE) in two independent time variables. However, this approach
becomes considerably less practical for highly oscillatory input paths, as they
have to be resolved at a fine enough scale to accurately recover their
signature kernel, resulting in significant time and memory complexities. To
mitigate this issue, we first show that the signature kernel of a broader class
of paths, known as \emph{smooth rough paths}, also satisfies a PDE, albeit in
the form of a system of coupled equations. We then use this result to introduce
new algorithms for the numerical approximation of signature kernels. As bounded
variation paths (and more generally geometric $p$-rough paths) can be
approximated by piecewise smooth rough paths, one can replace the PDE with
rapidly varying coefficients in the original Goursat problem by an explicit
system of coupled equations with piecewise constant coefficients derived from
the first few iterated integrals of the original input paths. While this
approach requires solving more equations, they do not require looking back at
the complex and fine structure of the initial paths, which significantly
reduces the computational complexity associated with the analysis of highly
oscillatory time series.
\\ ( https://arxiv.org/abs/2404.02926 ,  721kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02937
Date: Wed, 3 Apr 2024 07:14:15 GMT   (5136kb,D)

Title: Explainable Traffic Flow Prediction with Large Language Models
Authors: Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhua, and Hao (Frank)
  Yang
Categories: cs.LG cs.AI
Comments: 25pages, 8 figures
\\
  Traffic flow prediction provides essential future views in the intelligent
transportation system. Explainable predictions offer valuable insights into the
factors influencing traffic patterns, which help urban planners, traffic
engineers, and policymakers make informed decisions about infrastructure
development, traffic management strategies, and public transportation planning.
Despite their widespread popularity and commendable accuracy, prediction
methods grounded in deep learning frequently disappoint in terms of
transparency and interpretability. Recently, the availability of large-scale
spatio-temporal data and the development of large language models (LLMs) have
opened up new opportunities for urban traffic prediction. With the popularity
of LLMs, people witnessed the potential reasoning and generating ability of
foundation models in various tasks. Considering text as input and output, LLMs
have advantages in generating more intuitive and interpretable predictions.
Hence, this work introduces TP-LLM, an explainable foundation-model-based
method for traffic prediction, aiming at more direct and reasonable
forecasting. TP-LLM presents a framework to unify multi-modality factors as
language-based inputs, TP-LLM avoids complex spatial-temporal data programming
and outperforms state-of-art baselines merely under fine-tuning foundation
models. Also, TP-LLM can generate input-dependency explanations for more
confident prediction and can be easily generalized to different city dynamics
for zero-shot prediction with a similar framework. These findings demonstrate
the potential of LLMs for explainable traffic prediction.
\\ ( https://arxiv.org/abs/2404.02937 ,  5136kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02942
Date: Wed, 3 Apr 2024 12:38:12 GMT   (4011kb,D)

Title: Decision Predicate Graphs: Enhancing Interpretability in Tree Ensembles
Authors: Leonardo Arrighi, Luca Pennella, Gabriel Marques Tavares, Sylvio
  Barbon Junior
Categories: cs.LG cs.AI
\\
  Understanding the decisions of tree-based ensembles and their relationships
is pivotal for machine learning model interpretation. Recent attempts to
mitigate the human-in-the-loop interpretation challenge have explored the
extraction of the decision structure underlying the model taking advantage of
graph simplification and path emphasis. However, while these efforts enhance
the visualisation experience, they may either result in a visually complex
representation or compromise the interpretability of the original ensemble
model. In addressing this challenge, especially in complex scenarios, we
introduce the Decision Predicate Graph (DPG) as a model-agnostic tool to
provide a global interpretation of the model. DPG is a graph structure that
captures the tree-based ensemble model and learned dataset details, preserving
the relations among features, logical decisions, and predictions towards
emphasising insightful points. Leveraging well-known graph theory concepts,
such as the notions of centrality and community, DPG offers additional
quantitative insights into the model, complementing visualisation techniques,
expanding the problem space descriptions, and offering diverse possibilities
for extensions. Empirical experiments demonstrate the potential of DPG in
addressing traditional benchmarks and complex classification scenarios.
\\ ( https://arxiv.org/abs/2404.02942 ,  4011kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02943
Date: Wed, 3 Apr 2024 13:31:49 GMT   (2861kb,D)

Title: Learning in Convolutional Neural Networks Accelerated by Transfer
  Entropy
Authors: Adrian Moldovan, Angel Ca\c{t}aron, R\u{a}zvan Andonie
Categories: cs.LG cs.AI cs.IT math.IT
Journal-ref: Entropy - MDPI, Year 2021, Number 9, Article Number 1218, PubMedID
  34573843, ISSN 1099-4300
DOI: 10.3390/e23091218
\\
  Recently, there is a growing interest in applying Transfer Entropy (TE) in
quantifying the effective connectivity between artificial neurons. In a
feedforward network, the TE can be used to quantify the relationships between
neuron output pairs located in different layers. Our focus is on how to include
the TE in the learning mechanisms of a Convolutional Neural Network (CNN)
architecture. We introduce a novel training mechanism for CNN architectures
which integrates the TE feedback connections. Adding the TE feedback parameter
accelerates the training process, as fewer epochs are needed. On the flip side,
it adds computational overhead to each epoch. According to our experiments on
CNN classifiers, to achieve a reasonable computational overhead--accuracy
trade-off, it is efficient to consider only the inter-neural information
transfer of a random subset of the neuron pairs from the last two fully
connected layers. The TE acts as a smoothing factor, generating stability and
becoming active only periodically, not after processing each input sample.
Therefore, we can consider the TE is in our model a slowly changing
meta-parameter.
\\ ( https://arxiv.org/abs/2404.02943 ,  2861kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02944
Date: Wed, 3 Apr 2024 13:32:44 GMT   (5355kb,D)

Title: Foundation Models for Structural Health Monitoring
Authors: Luca Benfenati, Daniele Jahier Pagliari, Luca Zanatta, Yhorman
  Alexander Bedoya Velez, Andrea Acquaviva, Massimo Poncino, Enrico Macii, Luca
  Benini, Alessio Burrello
Categories: cs.LG cs.AI cs.SY eess.SY
Comments: 16 pages, 4 tables, 9 figures
ACM-class: I.2.1; I.2.3
\\
  Structural Health Monitoring (SHM) is a critical task for ensuring the safety
and reliability of civil infrastructures, typically realized on bridges and
viaducts by means of vibration monitoring. In this paper, we propose for the
first time the use of Transformer neural networks, with a Masked Auto-Encoder
architecture, as Foundation Models for SHM. We demonstrate the ability of these
models to learn generalizable representations from multiple large datasets
through self-supervised pre-training, which, coupled with task-specific
fine-tuning, allows them to outperform state-of-the-art traditional methods on
diverse tasks, including Anomaly Detection (AD) and Traffic Load Estimation
(TLE). We then extensively explore model size versus accuracy trade-offs and
experiment with Knowledge Distillation (KD) to improve the performance of
smaller Transformers, enabling their embedding directly into the SHM edge
nodes. We showcase the effectiveness of our foundation models using data from
three operational viaducts. For AD, we achieve a near-perfect 99.9% accuracy
with a monitoring time span of just 15 windows. In contrast, a state-of-the-art
method based on Principal Component Analysis (PCA) obtains its first good
result (95.03% accuracy) only considering 120 windows. On two different TLE
tasks, our models obtain state-of-the-art performance on multiple evaluation
metrics (R$^2$ score, MAE% and MSE%). On the first benchmark, we achieve an
R$^2$ score of 0.97 and 0.85 for light and heavy vehicle traffic, respectively,
while the best previous approach stops at 0.91 and 0.84. On the second one, we
achieve an R$^2$ score of 0.54 versus the 0.10 of the best existing method.
\\ ( https://arxiv.org/abs/2404.02944 ,  5355kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02945
Date: Wed, 3 Apr 2024 14:14:08 GMT   (2076kb,D)

Title: Optimizing the Deployment of Tiny Transformers on Low-Power MCUs
Authors: Victor J.B. Jung, Alessio Burrello, Moritz Scherer, Francesco Conti,
  Luca Benini
Categories: cs.LG cs.AI cs.DC cs.PF
Comments: Pre-print manuscript submitted for review to the IEEE Transactions on
  Computers
\\
  Transformer networks are rapidly becoming SotA in many fields, such as NLP
and CV. Similarly to CNN, there is a strong push for deploying Transformer
models at the extreme edge, ultimately fitting the tiny power budget and memory
footprint of MCUs. However, the early approaches in this direction are mostly
ad-hoc, platform, and model-specific. This work aims to enable and optimize the
flexible, multi-platform deployment of encoder Tiny Transformers on commercial
MCUs. We propose a complete framework to perform end-to-end deployment of
Transformer models onto single and multi-core MCUs. Our framework provides an
optimized library of kernels to maximize data reuse and avoid unnecessary data
marshaling operations into the crucial attention block. A novel MHSA inference
schedule, named Fused-Weight Self-Attention, is introduced, fusing the linear
projection weights offline to further reduce the number of operations and
parameters. Furthermore, to mitigate the memory peak reached by the computation
of the attention map, we present a Depth-First Tiling scheme for MHSA. We
evaluate our framework on three different MCU classes exploiting ARM and RISC-V
ISA, namely the STM32H7, the STM32L4, and GAP9 (RV32IMC-XpulpV2). We reach an
average of 4.79x and 2.0x lower latency compared to SotA libraries CMSIS-NN
(ARM) and PULP-NN (RISC-V), respectively. Moreover, we show that our MHSA
depth-first tiling scheme reduces the memory peak by up to 6.19x, while the
fused-weight attention can reduce the runtime by 1.53x, and number of
parameters by 25%. We report significant improvements across several Tiny
Transformers: for instance, when executing a transformer block for the task of
radar-based hand-gesture recognition on GAP9, we achieve a latency of 0.14ms
and energy consumption of 4.92 micro-joules, 2.32x lower than the SotA PULP-NN
library on the same platform.
\\ ( https://arxiv.org/abs/2404.02945 ,  2076kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02947
Date: Wed, 3 Apr 2024 15:06:09 GMT   (11503kb,D)

Title: DNN Memory Footprint Reduction via Post-Training Intra-Layer
  Multi-Precision Quantization
Authors: Behnam Ghavami, Amin Kamjoo, Lesley Shannon, Steve Wilton
Categories: cs.LG cs.AI
Comments: The 25th International Symposium on Quality Electronic Design
  (ISQED'24)
\\
  The imperative to deploy Deep Neural Network (DNN) models on
resource-constrained edge devices, spurred by privacy concerns, has become
increasingly apparent. To facilitate the transition from cloud to edge
computing, this paper introduces a technique that effectively reduces the
memory footprint of DNNs, accommodating the limitations of resource-constrained
edge devices while preserving model accuracy. Our proposed technique, named
Post-Training Intra-Layer Multi-Precision Quantization (PTILMPQ), employs a
post-training quantization approach, eliminating the need for extensive
training data. By estimating the importance of layers and channels within the
network, the proposed method enables precise bit allocation throughout the
quantization process. Experimental results demonstrate that PTILMPQ offers a
promising solution for deploying DNNs on edge devices with restricted memory
resources. For instance, in the case of ResNet50, it achieves an accuracy of
74.57\% with a memory footprint of 9.5 MB, representing a 25.49\% reduction
compared to previous similar methods, with only a minor 1.08\% decrease in
accuracy.
\\ ( https://arxiv.org/abs/2404.02947 ,  11503kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02948
Date: Wed, 3 Apr 2024 15:06:43 GMT   (419kb,D)

Title: PiSSA: Principal Singular Values and Singular Vectors Adaptation of
  Large Language Models
Authors: Fanxu Meng, Zhaohui Wang, Muhan Zhang
Categories: cs.LG cs.AI
\\
  As the parameters of LLMs expand, the computational cost of fine-tuning the
entire model becomes prohibitive. To address this challenge, we introduce a
PEFT method, Principal Singular values and Singular vectors Adaptation (PiSSA),
which optimizes a significantly reduced parameter space while achieving or
surpassing the performance of full-parameter fine-tuning. PiSSA is inspired by
Intrinsic SAID, which suggests that pre-trained, over-parametrized models
inhabit a space of low intrinsic dimension. Consequently, PiSSA represents a
matrix W within the model by the product of two trainable matrices A and B,
plus a residual matrix $W^{res}$ for error correction. SVD is employed to
factorize W, and the principal singular values and vectors of W are utilized to
initialize A and B. The residual singular values and vectors initialize the
residual matrix $W^{res}$, which keeps frozen during fine-tuning. Notably,
PiSSA shares the same architecture with LoRA. However, LoRA approximates Delta
W through the product of two matrices, A, initialized with Gaussian noise, and
B, initialized with zeros, while PiSSA initializes A and B with principal
singular values and vectors of the original matrix W. PiSSA can better
approximate the outcomes of full-parameter fine-tuning at the beginning by
changing the essential parts while freezing the "noisy" parts. In comparison,
LoRA freezes the original matrix and updates the "noise". This distinction
enables PiSSA to convergence much faster than LoRA and also achieve better
performance in the end. Due to the same architecture, PiSSA inherits many of
LoRA's advantages, such as parameter efficiency and compatibility with
quantization. Leveraging a fast SVD method, the initialization of PiSSA takes
only a few seconds, inducing negligible cost of switching LoRA to PiSSA.
\\ ( https://arxiv.org/abs/2404.02948 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02949
Date: Wed, 3 Apr 2024 17:56:28 GMT   (14768kb,D)

Title: The SaTML '24 CNN Interpretability Competition: New Innovations for
  Concept-Level Interpretability
Authors: Stephen Casper, Jieun Yun, Joonhyuk Baek, Yeseong Jung, Minhwan Kim,
  Kiwan Kwon, Saerom Park, Hayden Moore, David Shriver, Marissa Connor, Keltin
  Grimes, Angus Nicolson, Arush Tagade, Jessica Rumbelow, Hieu Minh Nguyen,
  Dylan Hadfield-Menell
Categories: cs.LG cs.AI
Comments: Competition for SaTML 2024
\\
  Interpretability techniques are valuable for helping humans understand and
oversee AI systems. The SaTML 2024 CNN Interpretability Competition solicited
novel methods for studying convolutional neural networks (CNNs) at the ImageNet
scale. The objective of the competition was to help human crowd-workers
identify trojans in CNNs. This report showcases the methods and results of four
featured competition entries. It remains challenging to help humans reliably
diagnose trojans via interpretability tools. However, the competition's entries
have contributed new techniques and set a new record on the benchmark from
Casper et al., 2023.
\\ ( https://arxiv.org/abs/2404.02949 ,  14768kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02954
Date: Wed, 3 Apr 2024 18:00:00 GMT   (3195kb,D)

Title: Deep Generative Models through the Lens of the Manifold Hypothesis: A
  Survey and New Connections
Authors: Gabriel Loaiza-Ganem, Brendan Leigh Ross, Rasa Hosseinzadeh, Anthony
  L. Caterini, Jesse C. Cresswell
Categories: cs.LG cs.AI stat.ML
\\
  In recent years there has been increased interest in understanding the
interplay between deep generative models (DGMs) and the manifold hypothesis.
Research in this area focuses on understanding the reasons why commonly-used
DGMs succeed or fail at learning distributions supported on unknown
low-dimensional manifolds, as well as developing new models explicitly designed
to account for manifold-supported data. This manifold lens provides both
clarity as to why some DGMs (e.g. diffusion models and some generative
adversarial networks) empirically surpass others (e.g. likelihood-based models
such as variational autoencoders, normalizing flows, or energy-based models) at
sample generation, and guidance for devising more performant DGMs. We carry out
the first survey of DGMs viewed through this lens, making two novel
contributions along the way. First, we formally establish that numerical
instability of high-dimensional likelihoods is unavoidable when modelling
low-dimensional data. We then show that DGMs on learned representations of
autoencoders can be interpreted as approximately minimizing Wasserstein
distance: this result, which applies to latent diffusion models, helps justify
their outstanding empirical results. The manifold lens provides a rich
perspective from which to understand DGMs, which we aim to make more accessible
and widespread.
\\ ( https://arxiv.org/abs/2404.02954 ,  3195kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02986
Date: Wed, 3 Apr 2024 18:14:23 GMT   (3634kb,D)

Title: Universal Functional Regression with Neural Operator Flows
Authors: Yaozhong Shi, Angela F. Gao, Zachary E. Ross, Kamyar Azizzadenesheli
Categories: cs.LG stat.ML
\\
  Regression on function spaces is typically limited to models with Gaussian
process priors. We introduce the notion of universal functional regression, in
which we aim to learn a prior distribution over non-Gaussian function spaces
that remains mathematically tractable for functional regression. To do this, we
develop Neural Operator Flows (OpFlow), an infinite-dimensional extension of
normalizing flows. OpFlow is an invertible operator that maps the (potentially
unknown) data function space into a Gaussian process, allowing for exact
likelihood estimation of functional point evaluations. OpFlow enables robust
and accurate uncertainty quantification via drawing posterior samples of the
Gaussian process and subsequently mapping them into the data function space. We
empirically study the performance of OpFlow on regression and generation tasks
with data generated from Gaussian processes with known posterior forms and
non-Gaussian processes, as well as real-world earthquake seismograms with an
unknown closed-form distribution.
\\ ( https://arxiv.org/abs/2404.02986 ,  3634kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03011
Date: Wed, 3 Apr 2024 18:48:45 GMT   (1644kb,D)

Title: Transfer learning applications for anomaly detection in wind turbines
Authors: Cyriana M.A. Roelofs and Christian G\"uck and Stefan Faulstich
Categories: cs.LG cs.AI
Comments: 16 pages, 7 figures, preprint submitted to Energy&AI
ACM-class: I.2
\\
  Anomaly detection in wind turbines typically involves using normal behaviour
models to detect faults early. However, training autoencoder models for each
turbine is time-consuming and resource intensive. Thus, transfer learning
becomes essential for wind turbines with limited data or applications with
limited computational resources. This study examines how cross-turbine transfer
learning can be applied to autoencoder-based anomaly detection. Here,
autoencoders are combined with constant thresholds for the reconstruction error
to determine if input data contains an anomaly. The models are initially
trained on one year's worth of data from one or more source wind turbines. They
are then fine-tuned using smaller amounts of data from another turbine. Three
methods for fine-tuning are investigated: adjusting the entire autoencoder,
only the decoder, or only the threshold of the model. The performance of the
transfer learning models is compared to baseline models that were trained on
one year's worth of data from the target wind turbine. The results of the tests
conducted in this study indicate that models trained on data of multiple wind
turbines do not improve the anomaly detection capability compared to models
trained on data of one source wind turbine. In addition, modifying the model's
threshold can lead to comparable or even superior performance compared to the
baseline, whereas fine-tuning the decoder or autoencoder further enhances the
models' performance.
\\ ( https://arxiv.org/abs/2404.03011 ,  1644kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03012
Date: Wed, 3 Apr 2024 18:50:14 GMT   (4941kb,D)

Title: Spectral Clustering in Convex and Constrained Settings
Authors: Swarup Ranjan Behera and Vijaya V. Saradhi
Categories: cs.LG
ACM-class: I.2.7
\\
  Spectral clustering methods have gained widespread recognition for their
effectiveness in clustering high-dimensional data. Among these techniques,
constrained spectral clustering has emerged as a prominent approach,
demonstrating enhanced performance by integrating pairwise constraints.
However, the application of such constraints to semidefinite spectral
clustering, a variant that leverages semidefinite programming to optimize
clustering objectives, remains largely unexplored. In this paper, we introduce
a novel framework for seamlessly integrating pairwise constraints into
semidefinite spectral clustering. Our methodology systematically extends the
capabilities of semidefinite spectral clustering to capture complex data
structures, thereby addressing real-world clustering challenges more
effectively. Additionally, we extend this framework to encompass both active
and self-taught learning scenarios, further enhancing its versatility and
applicability. Empirical studies conducted on well-known datasets demonstrate
the superiority of our proposed framework over existing spectral clustering
methods, showcasing its robustness and scalability across diverse datasets and
learning settings. By bridging the gap between constrained learning and
semidefinite spectral clustering, our work contributes to the advancement of
spectral clustering techniques, offering researchers and practitioners a
versatile tool for addressing complex clustering challenges in various
real-world applications. Access to the data, code, and experimental results is
provided for further exploration (https://github.com/swarupbehera/SCCCS).
\\ ( https://arxiv.org/abs/2404.03012 ,  4941kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03037
Date: Wed, 3 Apr 2024 19:48:13 GMT   (10397kb,D)

Title: Model-based Reinforcement Learning for Parameterized Action Spaces
Authors: Renhao Zhang, Haotian Fu, Yilin Miao, George Konidaris
Categories: cs.LG cs.AI
\\
  We propose a novel model-based reinforcement learning algorithm -- Dynamics
Learning and predictive control with Parameterized Actions (DLPA) -- for
Parameterized Action Markov Decision Processes (PAMDPs). The agent learns a
parameterized-action-conditioned dynamics model and plans with a modified Model
Predictive Path Integral control. We theoretically quantify the difference
between the generated trajectory and the optimal trajectory during planning in
terms of the value they achieved through the lens of Lipschitz Continuity. Our
empirical results on several standard benchmarks show that our algorithm
achieves superior sample efficiency and asymptotic performance than
state-of-the-art PAMDP methods.
\\ ( https://arxiv.org/abs/2404.03037 ,  10397kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03044
Date: Wed, 3 Apr 2024 20:08:15 GMT   (505kb)

Title: The Artificial Intelligence Ontology: LLM-assisted construction of AI
  concept hierarchies
Authors: Marcin P. Joachimiak, Mark A. Miller, J. Harry Caufield, Ryan Ly, Nomi
  L. Harris, Andrew Tritt, Christopher J. Mungall, Kristofer E. Bouchard
Categories: cs.LG cs.AI
\\
  The Artificial Intelligence Ontology (AIO) is a systematization of artificial
intelligence (AI) concepts, methodologies, and their interrelations. Developed
via manual curation, with the additional assistance of large language models
(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a
comprehensive framework that encompasses both technical and ethical aspects of
AI technologies. The primary audience for AIO includes AI researchers,
developers, and educators seeking standardized terminology and concepts within
the AI domain. The ontology is structured around six top-level branches:
Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to
support the modular composition of AI methods and facilitate a deeper
understanding of deep learning architectures and ethical considerations in AI.
  AIO's development utilized the Ontology Development Kit (ODK) for its
creation and maintenance, with its content being dynamically updated through
AI-driven curation support. This approach not only ensures the ontology's
relevance amidst the fast-paced advancements in AI but also significantly
enhances its utility for researchers, developers, and educators by simplifying
the integration of new AI concepts and methodologies.
  The ontology's utility is demonstrated through the annotation of AI methods
data in a catalog of AI research publications and the integration into the
BioPortal ontology resource, highlighting its potential for cross-disciplinary
research. The AIO ontology is open source and is available on GitHub
(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal
(https://bioportal.bioontology.org/ontologies/AIO).
\\ ( https://arxiv.org/abs/2404.03044 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03050
Date: Wed, 3 Apr 2024 20:34:18 GMT   (541kb,D)

Title: ANOVA-boosting for Random Fourier Features
Authors: Daniel Potts and Laura Weidensager
Categories: cs.LG cs.NA math.NA stat.ML
\\
  We propose two algorithms for boosting random Fourier feature models for
approximating high-dimensional functions. These methods utilize the classical
and generalized analysis of variance (ANOVA) decomposition to learn low-order
functions, where there are few interac- tions between the variables. Our
algorithms are able to find an index set of important input variables and
variable interactions reliably. Furthermore, we generalize already existing
random Fourier feature models to an ANOVA setting, where terms of different
order can be used. Our algorithms have the advantage of interpretability,
meaning that the influence of every input variable is known in the learned
model, even for dependent input variables. We give theoretical as well as
numerical results that our algorithms perform well for sensitivity analysis.
The ANOVA-boosting step reduces the approximation error of existing methods
significantly.
\\ ( https://arxiv.org/abs/2404.03050 ,  541kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03058
Date: Wed, 3 Apr 2024 20:50:48 GMT   (147kb,D)

Title: Automatic Extraction of Linguistic Description from Fuzzy Rule Base
Authors: Krzysztof Siminski, Konrad Wnuk
Categories: cs.LG cs.AI
\\
  Neuro-fuzzy systems are a technique of explainable artificial intelligence
(XAI). They elaborate knowledge models as a set of fuzzy rules. Fuzzy sets are
crucial components of fuzzy rules. They are used to model linguistic terms. In
this paper, we present an automatic extraction of fuzzy rules in the natural
English language. Full implementation is available free from a public
repository.
\\ ( https://arxiv.org/abs/2404.03058 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03081
Date: Wed, 3 Apr 2024 21:47:02 GMT   (35kb)

Title: First-order PDES for Graph Neural Networks: Advection And Burgers
  Equation Models
Authors: Yifan Qu, Oliver Krzysik, Hans De Sterck, Omer Ege Kara
Categories: cs.LG cs.NA math.NA
\\
  Graph Neural Networks (GNNs) have established themselves as the preferred
methodology in a multitude of domains, ranging from computer vision to
computational biology, especially in contexts where data inherently conform to
graph structures. While many existing methods have endeavored to model GNNs
using various techniques, a prevalent challenge they grapple with is the issue
of over-smoothing. This paper presents new Graph Neural Network models that
incorporate two first-order Partial Differential Equations (PDEs). These models
do not increase complexity but effectively mitigate the over-smoothing problem.
Our experimental findings highlight the capacity of our new PDE model to
achieve comparable results with higher-order PDE models and fix the
over-smoothing problem up to 64 layers. These results underscore the
adaptability and versatility of GNNs, indicating that unconventional approaches
can yield outcomes on par with established techniques.
\\ ( https://arxiv.org/abs/2404.03081 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03082
Date: Wed, 3 Apr 2024 21:47:44 GMT   (59kb)

Title: Machine Learning and Data Analysis Using Posets: A Survey
Authors: Arnauld Mesinga Mwafise
Categories: cs.LG
MSC-class: 06A06
\\
  Posets are discrete mathematical structures which are ubiquitous in a broad
range of data analysis and machine learning applications. Research connecting
posets to the data science domain has been ongoing for many years. In this
paper, a comprehensive review of a wide range of studies on data analysis amd
machine learning using posets are examined in terms of their theory, algorithms
and applications. In addition, the applied lattice theory domain of formal
concept analysis will also be highlighted in terms of its machine learning
applications.
\\ ( https://arxiv.org/abs/2404.03082 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03084
Date: Wed, 3 Apr 2024 21:55:17 GMT   (1535kb,D)

Title: Rethinking Teacher-Student Curriculum Learning through the Cooperative
  Mechanics of Experience
Authors: Manfred Diaz and Liam Paull and Andrea Tacchetti
Categories: cs.LG cs.AI cs.GT
\\
  Teacher-Student Curriculum Learning (TSCL) is a curriculum learning framework
that draws inspiration from human cultural transmission and learning. It
involves a teacher algorithm shaping the learning process of a learner
algorithm by exposing it to controlled experiences. Despite its success,
understanding the conditions under which TSCL is effective remains challenging.
In this paper, we propose a data-centric perspective to analyze the underlying
mechanics of the teacher-student interactions in TSCL. We leverage cooperative
game theory to describe how the composition of the set of experiences presented
by the teacher to the learner, as well as their order, influences the
performance of the curriculum that is found by TSCL approaches. To do so, we
demonstrate that for every TSCL problem, there exists an equivalent cooperative
game, and several key components of the TSCL framework can be reinterpreted
using game-theoretic principles. Through experiments covering supervised
learning, reinforcement learning, and classical games, we estimate the
cooperative values of experiences and use value-proportional curriculum
mechanisms to construct curricula, even in cases where TSCL struggles. The
framework and experimental setup we present in this work represent a novel
foundation for a deeper exploration of TSCL, shedding light on its underlying
mechanisms and providing insights into its broader applicability in machine
learning.
\\ ( https://arxiv.org/abs/2404.03084 ,  1535kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03088
Date: Wed, 3 Apr 2024 22:03:28 GMT   (1227kb,D)

Title: Robust Federated Learning for Wireless Networks: A Demonstration with
  Channel Estimation
Authors: Zexin Fang, Bin Han, and Hans D. Schotten
Categories: cs.LG cs.AI cs.NI eess.SP
Comments: Submitted to IEEE GLOBECOM 2024
\\
  Federated learning (FL) offers a privacy-preserving collaborative approach
for training models in wireless networks, with channel estimation emerging as a
promising application. Despite extensive studies on FL-empowered channel
estimation, the security concerns associated with FL require meticulous
attention. In a scenario where small base stations (SBSs) serve as local models
trained on cached data, and a macro base station (MBS) functions as the global
model setting, an attacker can exploit the vulnerability of FL, launching
attacks with various adversarial attacks or deployment tactics. In this paper,
we analyze such vulnerabilities, corresponding solutions were brought forth,
and validated through simulation.
\\ ( https://arxiv.org/abs/2404.03088 ,  1227kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03099
Date: Wed, 3 Apr 2024 22:42:37 GMT   (2043kb,D)

Title: Composite Bayesian Optimization In Function Spaces Using NEON - Neural
  Epistemic Operator Networks
Authors: Leonardo Ferreira Guilhoto, Paris Perdikaris
Categories: cs.LG cs.AI cs.CE cs.IT math.IT stat.ML
MSC-class: 68T37
ACM-class: J.2; I.2.6
\\
  Operator learning is a rising field of scientific computing where inputs or
outputs of a machine learning model are functions defined in
infinite-dimensional spaces. In this paper, we introduce NEON (Neural Epistemic
Operator Networks), an architecture for generating predictions with uncertainty
using a single operator network backbone, which presents orders of magnitude
less trainable parameters than deep ensembles of comparable performance. We
showcase the utility of this method for sequential decision-making by examining
the problem of composite Bayesian Optimization (BO), where we aim to optimize a
function $f=g\circ h$, where $h:X\to C(\mathcal{Y},\mathbb{R}^{d_s})$ is an
unknown map which outputs elements of a function space, and $g:
C(\mathcal{Y},\mathbb{R}^{d_s})\to \mathbb{R}$ is a known and cheap-to-compute
functional. By comparing our approach to other state-of-the-art methods on toy
and real world scenarios, we demonstrate that NEON achieves state-of-the-art
performance while requiring orders of magnitude less trainable parameters.
\\ ( https://arxiv.org/abs/2404.03099 ,  2043kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03105
Date: Wed, 3 Apr 2024 23:07:24 GMT   (100kb,D)

Title: Methodology for Interpretable Reinforcement Learning for Optimizing
  Mechanical Ventilation
Authors: Joo Seung Lee, Malini Mahendra and Anil Aswani
Categories: cs.LG math.OC
\\
  Mechanical ventilation is a critical life-support intervention that uses a
machine to deliver controlled air and oxygen to a patient's lungs, assisting or
replacing spontaneous breathing. While several data-driven approaches have been
proposed to optimize ventilator control strategies, they often lack
interpretability and agreement with general domain knowledge. This paper
proposes a methodology for interpretable reinforcement learning (RL) using
decision trees for mechanical ventilation control. Using a causal,
nonparametric model-based off-policy evaluation, we evaluate the policies in
their ability to gain increases in SpO2 while avoiding aggressive ventilator
settings which are known to cause ventilator induced lung injuries and other
complications. Numerical experiments using MIMIC-III data on the stays of real
patients' intensive care unit stays demonstrate that the decision tree policy
outperforms the behavior cloning policy and is comparable to state-of-the-art
RL policy. Future work concerns better aligning the cost function with medical
objectives to generate deeper clinical insights.
\\ ( https://arxiv.org/abs/2404.03105 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03115
Date: Wed, 3 Apr 2024 23:38:31 GMT   (2073kb,D)

Title: Deep Learning-Based Weather-Related Power Outage Prediction with
  Socio-Economic and Power Infrastructure Data
Authors: Xuesong Wang, Nina Fatehi, Caisheng Wang, Masoud H. Nazari
Categories: cs.LG cs.SY eess.SY
Comments: Accepted in 2024 IEEE PES General Meeting, Seattle, Washington (PES
  GM 2024)
\\
  This paper presents a deep learning-based approach for hourly power outage
probability prediction within census tracts encompassing a utility company's
service territory. Two distinct deep learning models, conditional Multi-Layer
Perceptron (MLP) and unconditional MLP, were developed to forecast power outage
probabilities, leveraging a rich array of input features gathered from publicly
available sources including weather data, weather station locations, power
infrastructure maps, socio-economic and demographic statistics, and power
outage records. Given a one-hour-ahead weather forecast, the models predict the
power outage probability for each census tract, taking into account both the
weather prediction and the location's characteristics. The deep learning models
employed different loss functions to optimize prediction performance. Our
experimental results underscore the significance of socio-economic factors in
enhancing the accuracy of power outage predictions at the census tract level.
\\ ( https://arxiv.org/abs/2404.03115 ,  2073kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03139
Date: Thu, 4 Apr 2024 01:24:27 GMT   (2891kb,D)

Title: Theoretical and Empirical Insights into the Origins of Degree Bias in
  Graph Neural Networks
Authors: Arjun Subramonian, Jian Kang, Yizhou Sun
Categories: cs.LG cs.SI
\\
  Graph Neural Networks (GNNs) often perform better for high-degree nodes than
low-degree nodes on node classification tasks. This degree bias can reinforce
social marginalization by, e.g., sidelining authors of lowly-cited papers when
predicting paper topics in citation networks. While researchers have proposed
numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38
degree bias papers that these hypotheses are often not rigorously validated,
and can even be contradictory. Thus, we provide an analysis of the origins of
degree bias in message-passing GNNs with different graph filters. We prove that
high-degree test nodes tend to have a lower probability of misclassification
regardless of how GNNs are trained. Moreover, we show that degree bias arises
from a variety of factors that are associated with a node's degree (e.g.,
homophily of neighbors, diversity of neighbors). Furthermore, we show that
during training, some GNNs may adjust their loss on low-degree nodes more
slowly than on high-degree nodes; however, with sufficiently many epochs of
training, message-passing GNNs can achieve their maximum possible training
accuracy, which is not significantly limited by their expressive power.
Throughout our analysis, we connect our findings to previously-proposed
hypotheses for the origins of degree bias, supporting and unifying some while
drawing doubt to others. We validate our theoretical findings on 8 common
real-world networks, and based on our theoretical and empirical insights,
describe a roadmap to alleviate degree bias.
\\ ( https://arxiv.org/abs/2404.03139 ,  2891kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03147
Date: Thu, 4 Apr 2024 01:42:28 GMT   (163kb,D)

Title: Eigenpruning
Authors: Tom\'as Vergara-Browne, \'Alvaro Soto, Akiko Aizawa
Categories: cs.LG cs.AI
\\
  We introduce eigenpruning, a method that removes singular values from weight
matrices in an LLM to improve its performance in a particular task. This method
is inspired by interpretability methods designed to automatically find
subnetworks of a model which solve a specific task. In our tests, the pruned
model outperforms the original model by a large margin, while only requiring
minimal computation to prune the weight matrices. In the case of a small
synthetic task in integer multiplication, the Phi-2 model can improve its
accuracy in the test set from 13.75% to 97.50%. Interestingly, these results
seem to indicate the existence of a computation path that can solve the task
very effectively, but it was not being used by the original model. Finally, we
plan to open-source our implementation in the camera-ready version of our work.
\\ ( https://arxiv.org/abs/2404.03147 ,  163kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03176
Date: Thu, 4 Apr 2024 03:20:35 GMT   (1354kb,D)

Title: Information-Theoretic Generalization Bounds for Deep Neural Networks
Authors: Haiyun He, Christina Lee Yu, Ziv Goldfeld
Categories: cs.LG cs.IT math.IT
Comments: 25 pages, 5 figures
\\
  Deep neural networks (DNNs) exhibit an exceptional capacity for
generalization in practical applications. This work aims to capture the effect
and benefits of depth for supervised learning via information-theoretic
generalization bounds. We first derive two hierarchical bounds on the
generalization error in terms of the Kullback-Leibler (KL) divergence or the
1-Wasserstein distance between the train and test distributions of the network
internal representations. The KL divergence bound shrinks as the layer index
increases, while the Wasserstein bound implies the existence of a layer that
serves as a generalization funnel, which attains a minimal 1-Wasserstein
distance. Analytic expressions for both bounds are derived under the setting of
binary Gaussian classification with linear DNNs. To quantify the contraction of
the relevant information measures when moving deeper into the network, we
analyze the strong data processing inequality (SDPI) coefficient between
consecutive layers of three regularized DNN models: Dropout, DropConnect, and
Gaussian noise injection. This enables refining our generalization bounds to
capture the contraction as a function of the network architecture parameters.
Specializing our results to DNNs with a finite parameter space and the Gibbs
algorithm reveals that deeper yet narrower network architectures generalize
better in those examples, although how broadly this statement applies remains a
question.
\\ ( https://arxiv.org/abs/2404.03176 ,  1354kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03180
Date: Thu, 4 Apr 2024 03:29:41 GMT   (5039kb,D)

Title: Goldfish: An Efficient Federated Unlearning Framework
Authors: Houzhe Wang, Xiaojie Zhu, Chi Chen, Paulo Esteves-Ver\'issimo
Categories: cs.LG cs.CR
\\
  With recent legislation on the right to be forgotten, machine unlearning has
emerged as a crucial research area. It facilitates the removal of a user's data
from federated trained machine learning models without the necessity for
retraining from scratch. However, current machine unlearning algorithms are
confronted with challenges of efficiency and validity.To address the above
issues, we propose a new framework, named Goldfish. It comprises four modules:
basic model, loss function, optimization, and extension. To address the
challenge of low validity in existing machine unlearning algorithms, we propose
a novel loss function. It takes into account the loss arising from the
discrepancy between predictions and actual labels in the remaining dataset.
Simultaneously, it takes into consideration the bias of predicted results on
the removed dataset. Moreover, it accounts for the confidence level of
predicted results. Additionally, to enhance efficiency, we adopt knowledge
distillation technique in basic model and introduce an optimization module that
encompasses the early termination mechanism guided by empirical risk and the
data partition mechanism. Furthermore, to bolster the robustness of the
aggregated model, we propose an extension module that incorporates a mechanism
using adaptive distillation temperature to address the heterogeneity of user
local data and a mechanism using adaptive weight to handle the variety in the
quality of uploaded models. Finally, we conduct comprehensive experiments to
illustrate the effectiveness of proposed approach.
\\ ( https://arxiv.org/abs/2404.03180 ,  5039kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03200
Date: Thu, 4 Apr 2024 05:08:51 GMT   (631kb,D)

Title: Future-Proofing Class Incremental Learning
Authors: Quentin Jodelet, Xin Liu, Yin Jun Phua, Tsuyoshi Murata
Categories: cs.LG cs.CV
\\
  Exemplar-Free Class Incremental Learning is a highly challenging setting
where replay memory is unavailable. Methods relying on frozen feature
extractors have drawn attention recently in this setting due to their
impressive performances and lower computational costs. However, those methods
are highly dependent on the data used to train the feature extractor and may
struggle when an insufficient amount of classes are available during the first
incremental step. To overcome this limitation, we propose to use a pre-trained
text-to-image diffusion model in order to generate synthetic images of future
classes and use them to train the feature extractor. Experiments on the
standard benchmarks CIFAR100 and ImageNet-Subset demonstrate that our proposed
method can be used to improve state-of-the-art methods for exemplar-free class
incremental learning, especially in the most difficult settings where the first
incremental step only contains few classes. Moreover, we show that using
synthetic samples of future classes achieves higher performance than using real
data from different classes, paving the way for better and less costly
pre-training methods for incremental learning.
\\ ( https://arxiv.org/abs/2404.03200 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03208
Date: Thu, 4 Apr 2024 05:30:03 GMT   (2175kb)

Title: Multimodal hierarchical multi-task deep learning framework for jointly
  predicting and explaining Alzheimer disease progression
Authors: Sayantan Kumar, Sean Yu, Thomas Kannampallil, Andrew Michelson,
  Aristeidis Sotiras, Philip Payne
Categories: cs.LG
Comments: Preprint submitted to Journal of Medical Informatics Association
  (JAMIA). 34 pages, 6 figures, 3 tables
\\
  Early identification of Mild Cognitive Impairment (MCI) subjects who will
eventually progress to Alzheimer Disease (AD) is challenging. Existing deep
learning models are mostly single-modality single-task models predicting risk
of disease progression at a fixed timepoint. We proposed a multimodal
hierarchical multi-task learning approach which can monitor the risk of disease
progression at each timepoint of the visit trajectory. Longitudinal visit data
from multiple modalities (MRI, cognition, and clinical data) were collected
from MCI individuals of the Alzheimer Disease Neuroimaging Initiative (ADNI)
dataset. Our hierarchical model predicted at every timepoint a set of
neuropsychological composite cognitive function scores as auxiliary tasks and
used the forecasted scores at every timepoint to predict the future risk of
disease. Relevance weights for each composite function provided explanations
about potential factors for disease progression. Our proposed model performed
better than state-of-the-art baselines in predicting AD progression risk and
the composite scores. Ablation study on the number of modalities demonstrated
that imaging and cognition data contributed most towards the outcome. Model
explanations at each timepoint can inform clinicians 6 months in advance the
potential cognitive function decline that can lead to progression to AD in
future. Our model monitored their risk of AD progression every 6 months
throughout the visit trajectory of individuals. The hierarchical learning of
auxiliary tasks allowed better optimization and allowed longitudinal
explanations for the outcome. Our framework is flexible with the number of
input modalities and the selection of auxiliary tasks and hence can be
generalized to other clinical problems too.
\\ ( https://arxiv.org/abs/2404.03208 ,  2175kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03211
Date: Thu, 4 Apr 2024 05:35:59 GMT   (149kb,D)

Title: Convergence Conditions of Online Regularized Statistical Learning in
  Reproducing Kernel Hilbert Space With Non-Stationary Data
Authors: Xiwei Zhang and Tao Li
Categories: cs.LG cs.SY eess.SY
\\
  We study the convergence of recursive regularized learning algorithms in the
reproducing kernel Hilbert space (RKHS) with dependent and non-stationary
online data streams. Firstly, we study the mean square asymptotic stability of
a class of random difference equations in RKHS, whose non-homogeneous terms are
martingale difference sequences dependent on the homogeneous ones. Secondly, we
introduce the concept of random Tikhonov regularization path, and show that if
the regularization path is slowly time-varying in some sense, then the output
of the algorithm is consistent with the regularization path in mean square.
Furthermore, if the data streams also satisfy the RKHS persistence of
excitation condition, i.e. there exists a fixed length of time period, such
that each eigenvalue of the conditional expectation of the operators induced by
the input data accumulated over every time period has a uniformly positive
lower bound with respect to time, then the output of the algorithm is
consistent with the unknown function in mean square. Finally, for the case with
independent and non-identically distributed data streams, the algorithm
achieves the mean square consistency provided the marginal probability measures
induced by the input data are slowly time-varying and the average measure over
each fixed-length time period has a uniformly strictly positive lower bound.
\\ ( https://arxiv.org/abs/2404.03211 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03222
Date: Thu, 4 Apr 2024 06:10:57 GMT   (1369kb,D)

Title: Enabling Clean Energy Resilience with Machine Learning-Empowered
  Underground Hydrogen Storage
Authors: Alvaro Carbonero, Shaowen Mao, Mohamed Mehana
Categories: cs.LG cs.SY eess.SY
Comments: 10 pages, 4 figures, accepted proposal track paper at ICLR CCAI
  workshop
\\
  To address the urgent challenge of climate change, there is a critical need
to transition away from fossil fuels towards sustainable energy systems, with
renewable energy sources playing a pivotal role. However, the inherent
variability of renewable energy, without effective storage solutions, often
leads to imbalances between energy supply and demand. Underground Hydrogen
Storage (UHS) emerges as a promising long-term storage solution to bridge this
gap, yet its widespread implementation is impeded by the high computational
costs associated with high fidelity UHS simulations. This paper introduces UHS
from a data-driven perspective and outlines a roadmap for integrating machine
learning into UHS, thereby facilitating the large-scale deployment of UHS.
\\ ( https://arxiv.org/abs/2404.03222 ,  1369kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03240
Date: Thu, 4 Apr 2024 06:56:32 GMT   (1340kb,D)

Title: Knowledge-Based Convolutional Neural Network for the Simulation and
  Prediction of Two-Phase Darcy Flows
Authors: Zakaria Elabid, Daniel Busby, Abdenour Hadid
Categories: cs.LG physics.flu-dyn
\\
  Physics-informed neural networks (PINNs) have gained significant prominence
as a powerful tool in the field of scientific computing and simulations. Their
ability to seamlessly integrate physical principles into deep learning
architectures has revolutionized the approaches to solving complex problems in
physics and engineering. However, a persistent challenge faced by mainstream
PINNs lies in their handling of discontinuous input data, leading to
inaccuracies in predictions. This study addresses these challenges by
incorporating the discretized forms of the governing equations into the PINN
framework. We propose to combine the power of neural networks with the dynamics
imposed by the discretized differential equations. By discretizing the
governing equations, the PINN learns to account for the discontinuities and
accurately capture the underlying relationships between inputs and outputs,
improving the accuracy compared to traditional interpolation techniques.
Moreover, by leveraging the power of neural networks, the computational cost
associated with numerical simulations is substantially reduced. We evaluate our
model on a large-scale dataset for the prediction of pressure and saturation
fields demonstrating high accuracies compared to non-physically aware models.
\\ ( https://arxiv.org/abs/2404.03240 ,  1340kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03263
Date: Thu, 4 Apr 2024 07:38:11 GMT   (1840kb,D)

Title: On the Surprising Efficacy of Distillation as an Alternative to
  Pre-Training Small Models
Authors: Sean Farhat, Deming Chen
Categories: cs.LG cs.AI
Comments: ICLR 2024. 5th Workshop on Practical ML for Low Resource Settings
  (PML4LRS). Code can be found at https://github.com/sfarhat/dapt
\\
  In this paper, we propose that small models may not need to absorb the cost
of pre-training to reap its benefits. Instead, they can capitalize on the
astonishing results achieved by modern, enormous models to a surprising degree.
We observe that, when distilled on a task from a pre-trained teacher model, a
small model can achieve or surpass the performance it would achieve if it was
pre-trained then finetuned on that task. To allow this phenomenon to be easily
leveraged, we establish a connection reducing knowledge distillation to modern
contrastive learning, opening two doors: (1) vastly different model
architecture pairings can work for the distillation, and (2) most contrastive
learning algorithms rooted in the theory of Noise Contrastive Estimation can be
easily applied and used. We demonstrate this paradigm using pre-trained teacher
models from open-source model hubs, Transformer and convolution based model
combinations, and a novel distillation algorithm that massages the
Alignment/Uniformity perspective of contrastive learning by Wang & Isola (2020)
into a distillation objective. We choose this flavor of contrastive learning
due to its low computational cost, an overarching theme of this work. We also
observe that this phenomenon tends not to occur if the task is data-limited.
However, this can be alleviated by leveraging yet another scale-inspired
development: large, pre-trained generative models for dataset augmentation.
Again, we use an open-source model, and our rudimentary prompts are sufficient
to boost the small model`s performance. Thus, we highlight a training method
for small models that is up to 94% faster than the standard pre-training
paradigm without sacrificing performance. For practitioners discouraged from
fully utilizing modern foundation datasets for their small models due to the
prohibitive scale, we believe our work keeps that door open.
\\ ( https://arxiv.org/abs/2404.03263 ,  1840kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03272
Date: Thu, 4 Apr 2024 07:49:09 GMT   (3012kb,D)

Title: Cryptographic Hardness of Score Estimation
Authors: Min Jae Song
Categories: cs.LG cs.CC cs.CR math.ST stat.ML stat.TH
Comments: 28 pages
\\
  We show that $L^2$-accurate score estimation, in the absence of strong
assumptions on the data distribution, is computationally hard even when sample
complexity is polynomial in the relevant problem parameters. Our reduction
builds on the result of Chen et al. (ICLR 2023), who showed that the problem of
generating samples from an unknown data distribution reduces to $L^2$-accurate
score estimation. Our hard-to-estimate distributions are the "Gaussian
pancakes" distributions, originally due to Diakonikolas et al. (FOCS 2017),
which have been shown to be computationally indistinguishable from the standard
Gaussian under widely believed hardness assumptions from lattice-based
cryptography (Bruna et al., STOC 2021; Gupte et al., FOCS 2022).
\\ ( https://arxiv.org/abs/2404.03272 ,  3012kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03273
Date: Thu, 4 Apr 2024 07:55:46 GMT   (2195kb,D)

Title: Gaussian-Smoothed Sliced Probability Divergences
Authors: Mokhtar Z. Alaya (LMAC), Alain Rakotomamonjy (LITIS), Maxime Berar
  (LITIS), Gilles Gasso (LITIS)
Categories: cs.LG math.ST stat.ML stat.TH
Comments: arXiv admin note: substantial text overlap with arXiv:2110.10524
\\
  Gaussian smoothed sliced Wasserstein distance has been recently introduced
for comparing probability distributions, while preserving privacy on the data.
It has been shown that it provides performances similar to its non-smoothed
(non-private) counterpart. However, the computationaland statistical properties
of such a metric have not yet been well-established. This work investigates the
theoretical properties of this distance as well as those of generalized
versions denoted as Gaussian-smoothed sliced divergences. We first show that
smoothing and slicing preserve the metric property and the weak topology. To
study the sample complexity of such divergences, we then introduce
$\hat{\hat\mu}_{n}$ the double empirical distribution for the
smoothed-projected $\mu$. The distribution $\hat{\hat\mu}_{n}$ is a result of a
double sampling process: one from sampling according to the origin distribution
$\mu$ and the second according to the convolution of the projection of $\mu$ on
the unit sphere and the Gaussian smoothing. We particularly focus on the
Gaussian smoothed sliced Wasserstein distance and prove that it converges with
a rate $O(n^{-1/2})$. We also derive other properties, including continuity, of
different divergences with respect to the smoothing parameter. We support our
theoretical findings with empirical studies in the context of
privacy-preserving domain adaptation.
\\ ( https://arxiv.org/abs/2404.03273 ,  2195kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03290
Date: Thu, 4 Apr 2024 08:24:57 GMT   (2856kb,D)

Title: Learning-to-Optimize with PAC-Bayesian Guarantees: Theoretical
  Considerations and Practical Implementation
Authors: Michael Sucker, Jalal Fadili and Peter Ochs
Categories: cs.LG math.OC
\\
  We use the PAC-Bayesian theory for the setting of learning-to-optimize. To
the best of our knowledge, we present the first framework to learn optimization
algorithms with provable generalization guarantees (PAC-Bayesian bounds) and
explicit trade-off between convergence guarantees and convergence speed, which
contrasts with the typical worst-case analysis. Our learned optimization
algorithms provably outperform related ones derived from a (deterministic)
worst-case analysis. The results rely on PAC-Bayesian bounds for general,
possibly unbounded loss-functions based on exponential families. Then, we
reformulate the learning procedure into a one-dimensional minimization problem
and study the possibility to find a global minimum. Furthermore, we provide a
concrete algorithmic realization of the framework and new methodologies for
learning-to-optimize, and we conduct four practically relevant experiments to
support our theory. With this, we showcase that the provided learning framework
yields optimization algorithms that provably outperform the state-of-the-art by
orders of magnitude.
\\ ( https://arxiv.org/abs/2404.03290 ,  2856kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03299
Date: Thu, 4 Apr 2024 08:48:30 GMT   (9252kb,D)

Title: SiloFuse: Cross-silo Synthetic Data Generation with Latent Tabular
  Diffusion Models
Authors: Aditya Shankar, Hans Brouwer, Rihan Hai, Lydia Chen
Categories: cs.LG cs.CR cs.DB cs.DC
Comments: Accepted at 40th IEEE International Conference on Data Engineering
  (ICDE 2024)
\\
  Synthetic tabular data is crucial for sharing and augmenting data across
silos, especially for enterprises with proprietary data. However, existing
synthesizers are designed for centrally stored data. Hence, they struggle with
real-world scenarios where features are distributed across multiple silos,
necessitating on-premise data storage. We introduce SiloFuse, a novel
generative framework for high-quality synthesis from cross-silo tabular data.
To ensure privacy, SiloFuse utilizes a distributed latent tabular diffusion
architecture. Through autoencoders, latent representations are learned for each
client's features, masking their actual values. We employ stacked distributed
training to improve communication efficiency, reducing the number of rounds to
a single step. Under SiloFuse, we prove the impossibility of data
reconstruction for vertically partitioned synthesis and quantify privacy risks
through three attacks using our benchmark framework. Experimental results on
nine datasets showcase SiloFuse's competence against centralized
diffusion-based synthesizers. Notably, SiloFuse achieves 43.8 and 29.8 higher
percentage points over GANs in resemblance and utility. Experiments on
communication show stacked training's fixed cost compared to the growing costs
of end-to-end training as the number of training iterations increases.
Additionally, SiloFuse proves robust to feature permutations and varying
numbers of clients.
\\ ( https://arxiv.org/abs/2404.03299 ,  9252kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03309
Date: Thu, 4 Apr 2024 09:08:04 GMT   (295kb,D)

Title: Optimistic Online Non-stochastic Control via FTRL
Authors: Naram Mhaisen and George Iosifidis
Categories: cs.LG math.OC
\\
  This paper brings the concept of "optimism" to the new and promising
framework of online Non-stochastic Control (NSC). Namely, we study how can NSC
benefit from a prediction oracle of unknown quality responsible for forecasting
future costs. The posed problem is first reduced to an optimistic learning with
delayed feedback problem, which is handled through the Optimistic Follow the
Regularized Leader (OFTRL) algorithmic family. This reduction enables the
design of OptFTRL-C, the first Disturbance Action Controller (DAC) with
optimistic policy regret bounds. These new bounds are commensurate with the
oracle's accuracy, ranging from $\mathcal{O}(1)$ for perfect predictions to the
order-optimal $\mathcal{O}(\sqrt{T})$ even when all predictions fail. By
addressing the challenge of incorporating untrusted predictions into control
systems, our work contributes to the advancement of the NSC framework and paves
the way towards effective and robust learning-based controllers.
\\ ( https://arxiv.org/abs/2404.03309 ,  295kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03320
Date: Thu, 4 Apr 2024 09:35:48 GMT   (1890kb,D)

Title: Exploring Lightweight Federated Learning for Distributed Load
  Forecasting
Authors: Abhishek Duttagupta, Jin Zhao, Shanker Shreejith
Categories: cs.LG cs.SY eess.SY
DOI: 10.1109/SmartGridComm57358.2023.10333889
\\
  Federated Learning (FL) is a distributed learning scheme that enables deep
learning to be applied to sensitive data streams and applications in a
privacy-preserving manner. This paper focuses on the use of FL for analyzing
smart energy meter data with the aim to achieve comparable accuracy to
state-of-the-art methods for load forecasting while ensuring the privacy of
individual meter data. We show that with a lightweight fully connected deep
neural network, we are able to achieve forecasting accuracy comparable to
existing schemes, both at each meter source and at the aggregator, by utilising
the FL framework. The use of lightweight models further reduces the energy and
resource consumption caused by complex deep-learning models, making this
approach ideally suited for deployment across resource-constrained smart meter
systems. With our proposed lightweight model, we are able to achieve an overall
average load forecasting RMSE of 0.17, with the model having a negligible
energy overhead of 50 mWh when performing training and inference on an Arduino
Uno platform.
\\ ( https://arxiv.org/abs/2404.03320 ,  1890kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03329
Date: Thu, 4 Apr 2024 09:55:11 GMT   (970kb)

Title: MPOFI: Multichannel Partially Observed Functional Modeling for Defect
  Classification with Imbalanced Dataset via Deep Metric Learning
Authors: Yukun Xie, Juan Du, Chen Zhang
Categories: cs.LG eess.SP stat.ML
\\
  In modern manufacturing, most of the product lines are conforming. Few
products are nonconforming but with different defect types. The identification
of defect types can help further root cause diagnosis of production lines. With
the sensing development, continuous signals of process variables can be
collected in high resolution, which can be regarded as multichannel functional
data. They have abundant information to characterize the process and help
identify the defect types. Motivated by a real example from the pipe tightening
process, we target at detect classification when each sample is a multichannel
functional data. However, the available samples for each defect type are
limited and imbalanced. Moreover, the functions are partially observed since
the pre-tightening process before the pipe tightening process is unobserved. To
classify the defect samples based on imbalanced, multichannel, and partially
observed functional data is very important but challenging. Thus, we propose an
innovative framework known as "Multichannel Partially Observed Functional
Modeling for Defect Classification with an Imbalanced Dataset" (MPOFI). The
framework leverages the power of deep metric learning in conjunction with a
neural network specially crafted for processing functional data. This paper
introduces a neural network explicitly tailored for handling multichannel and
partially observed functional data, complemented by developing a corresponding
loss function for training on imbalanced datasets. The results from a
real-world case study demonstrate the superior accuracy of our framework when
compared to existing benchmarks.
\\ ( https://arxiv.org/abs/2404.03329 ,  970kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03348
Date: Thu, 4 Apr 2024 10:28:55 GMT   (1188kb,D)

Title: Knowledge Distillation-Based Model Extraction Attack using Private
  Counterfactual Explanations
Authors: Fatima Ezzeddine, Omran Ayoub, Silvia Giordano
Categories: cs.LG cs.AI cs.CR cs.CY
Comments: 15 pages
\\
  In recent years, there has been a notable increase in the deployment of
machine learning (ML) models as services (MLaaS) across diverse production
software applications. In parallel, explainable AI (XAI) continues to evolve,
addressing the necessity for transparency and trustworthiness in ML models. XAI
techniques aim to enhance the transparency of ML models by providing insights,
in terms of the model's explanations, into their decision-making process.
Simultaneously, some MLaaS platforms now offer explanations alongside the ML
prediction outputs. This setup has elevated concerns regarding vulnerabilities
in MLaaS, particularly in relation to privacy leakage attacks such as model
extraction attacks (MEA). This is due to the fact that explanations can unveil
insights about the inner workings of the model which could be exploited by
malicious users. In this work, we focus on investigating how model
explanations, particularly Generative adversarial networks (GANs)-based
counterfactual explanations (CFs), can be exploited for performing MEA within
the MLaaS platform. We also delve into assessing the effectiveness of
incorporating differential privacy (DP) as a mitigation strategy. To this end,
we first propose a novel MEA methodology based on Knowledge Distillation (KD)
to enhance the efficiency of extracting a substitute model of a target model
exploiting CFs. Then, we advise an approach for training CF generators
incorporating DP to generate private CFs. We conduct thorough experimental
evaluations on real-world datasets and demonstrate that our proposed KD-based
MEA can yield a high-fidelity substitute model with reduced queries with
respect to baseline approaches. Furthermore, our findings reveal that the
inclusion of a privacy layer impacts the performance of the explainer, the
quality of CFs, and results in a reduction in the MEA performance.
\\ ( https://arxiv.org/abs/2404.03348 ,  1188kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03359
Date: Thu, 4 Apr 2024 10:56:30 GMT   (5996kb,D)

Title: REACT: Revealing Evolutionary Action Consequence Trajectories for
  Interpretable Reinforcement Learning
Authors: Philipp Altmann, C\'eline Davignon, Maximilian Zorn, Fabian Ritz,
  Claudia Linnhoff-Popien, Thomas Gabor
Categories: cs.LG cs.AI cs.NE
Comments: 12 pages, 12 figures
\\
  To enhance the interpretability of Reinforcement Learning (RL), we propose
Revealing Evolutionary Action Consequence Trajectories (REACT). In contrast to
the prevalent practice of validating RL models based on their optimal behavior
learned during training, we posit that considering a range of edge-case
trajectories provides a more comprehensive understanding of their inherent
behavior. To induce such scenarios, we introduce a disturbance to the initial
state, optimizing it through an evolutionary algorithm to generate a diverse
population of demonstrations. To evaluate the fitness of trajectories, REACT
incorporates a joint fitness function that encourages both local and global
diversity in the encountered states and chosen actions. Through assessments
with policies trained for varying durations in discrete and continuous
environments, we demonstrate the descriptive power of REACT. Our results
highlight its effectiveness in revealing nuanced aspects of RL models' behavior
beyond optimal performance, thereby contributing to improved interpretability.
\\ ( https://arxiv.org/abs/2404.03359 ,  5996kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03368
Date: Thu, 4 Apr 2024 11:09:49 GMT   (765kb,D)

Title: Graph Neural Networks for Electric and Hydraulic Data Fusion to Enhance
  Short-term Forecasting of Pumped-storage Hydroelectricity
Authors: Raffael Theiler, Olga Fink
Categories: cs.LG cs.SY eess.SP eess.SY
Comments: 11 pages, 8 figures, conference
\\
  Pumped-storage hydropower plants (PSH) actively participate in grid
power-frequency control and therefore often operate under dynamic conditions,
which results in rapidly varying system states. Predicting these dynamically
changing states is essential for comprehending the underlying sensor and
machine conditions. This understanding aids in detecting anomalies and faults,
ensuring the reliable operation of the connected power grid, and in identifying
faulty and miscalibrated sensors. PSH are complex, highly interconnected
systems encompassing electrical and hydraulic subsystems, each characterized by
their respective underlying networks that can individually be represented as
graphs. To take advantage of this relational inductive bias, graph neural
networks (GNNs) have been separately applied to state forecasting tasks in the
individual subsystems, but without considering their interdependencies. In PSH,
however, these subsystems depend on the same control input, making their
operations highly interdependent and interconnected. Consequently, hydraulic
and electrical sensor data should be fused across PSH subsystems to improve
state forecasting accuracy. This approach has not been explored in GNN
literature yet because many available PSH graphs are limited to their
respective subsystem boundaries, which makes the method unsuitable to be
applied directly. In this work, we introduce the application of
spectral-temporal graph neural networks, which leverage self-attention
mechanisms to concurrently capture and learn meaningful subsystem
interdependencies and the dynamic patterns observed in electric and hydraulic
sensors. Our method effectively fuses data from the PSH's subsystems by
operating on a unified, system-wide graph, learned directly from the data, This
approach leads to demonstrably improved state forecasting performance and
enhanced generalizability.
\\ ( https://arxiv.org/abs/2404.03368 ,  765kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03380
Date: Thu, 4 Apr 2024 11:26:51 GMT   (172kb,D)

Title: On the Theoretical Expressive Power and the Design Space of Higher-Order
  Graph Transformers
Authors: Cai Zhou, Rose Yu, Yusu Wang
Categories: cs.LG cs.CG math.GN
Comments: Accepted to AISTATS 2024. 40 pages
\\
  Graph transformers have recently received significant attention in graph
learning, partly due to their ability to capture more global interaction via
self-attention. Nevertheless, while higher-order graph neural networks have
been reasonably well studied, the exploration of extending graph transformers
to higher-order variants is just starting. Both theoretical understanding and
empirical results are limited. In this paper, we provide a systematic study of
the theoretical expressive power of order-$k$ graph transformers and sparse
variants. We first show that, an order-$k$ graph transformer without additional
structural information is less expressive than the $k$-Weisfeiler Lehman
($k$-WL) test despite its high computational cost. We then explore strategies
to both sparsify and enhance the higher-order graph transformers, aiming to
improve both their efficiency and expressiveness. Indeed, sparsification based
on neighborhood information can enhance the expressive power, as it provides
additional information about input graph structures. In particular, we show
that a natural neighborhood-based sparse order-$k$ transformer model is not
only computationally efficient, but also expressive -- as expressive as $k$-WL
test. We further study several other sparse graph attention models that are
computationally efficient and provide their expressiveness analysis. Finally,
we provide experimental results to show the effectiveness of the different
sparsification strategies.
\\ ( https://arxiv.org/abs/2404.03380 ,  172kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03382
Date: Thu, 4 Apr 2024 11:29:05 GMT   (1729kb,D)

Title: DIDA: Denoised Imitation Learning based on Domain Adaptation
Authors: Kaichen Huang, Hai-Hang Sun, Shenghua Wan, Minghao Shao, Shuai Feng,
  Le Gan, De-Chuan Zhan
Categories: cs.LG cs.AI
\\
  Imitating skills from low-quality datasets, such as sub-optimal
demonstrations and observations with distractors, is common in real-world
applications. In this work, we focus on the problem of Learning from Noisy
Demonstrations (LND), where the imitator is required to learn from data with
noise that often occurs during the processes of data collection or
transmission. Previous IL methods improve the robustness of learned policies by
injecting an adversarially learned Gaussian noise into pure expert data or
utilizing additional ranking information, but they may fail in the LND setting.
To alleviate the above problems, we propose Denoised Imitation learning based
on Domain Adaptation (DIDA), which designs two discriminators to distinguish
the noise level and expertise level of data, facilitating a feature encoder to
learn task-related but domain-agnostic representations. Experiment results on
MuJoCo demonstrate that DIDA can successfully handle challenging imitation
tasks from demonstrations with various types of noise, outperforming most
baseline methods.
\\ ( https://arxiv.org/abs/2404.03382 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03411
Date: Thu, 4 Apr 2024 12:38:14 GMT   (53kb)

Title: Red Teaming GPT-4V: Are GPT-4V Safe Against Uni/Multi-Modal Jailbreak
  Attacks?
Authors: Shuo Chen, Zhen Han, Bailan He, Zifeng Ding, Wenqian Yu, Philip Torr,
  Volker Tresp, Jindong Gu
Categories: cs.LG cs.CL cs.CR
Comments: technical report
\\
  Various jailbreak attacks have been proposed to red-team Large Language
Models (LLMs) and revealed the vulnerable safeguards of LLMs. Besides, some
methods are not limited to the textual modality and extend the jailbreak attack
to Multimodal Large Language Models (MLLMs) by perturbing the visual input.
However, the absence of a universal evaluation benchmark complicates the
performance reproduction and fair comparison. Besides, there is a lack of
comprehensive evaluation of closed-source state-of-the-art (SOTA) models,
especially MLLMs, such as GPT-4V. To address these issues, this work first
builds a comprehensive jailbreak evaluation dataset with 1445 harmful questions
covering 11 different safety policies. Based on this dataset, extensive
red-teaming experiments are conducted on 11 different LLMs and MLLMs, including
both SOTA proprietary models and open-source models. We then conduct a deep
analysis of the evaluated results and find that (1) GPT4 and GPT-4V demonstrate
better robustness against jailbreak attacks compared to open-source LLMs and
MLLMs. (2) Llama2 and Qwen-VL-Chat are more robust compared to other
open-source models. (3) The transferability of visual jailbreak methods is
relatively limited compared to textual jailbreak methods. The dataset and code
can be found here
https://anonymous.4open.science/r/red_teaming_gpt4-C1CE/README.md .
\\ ( https://arxiv.org/abs/2404.03411 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03419
Date: Thu, 4 Apr 2024 12:54:13 GMT   (76kb,D)

Title: Integrating Hyperparameter Search into GramML
Authors: Hern\'an Ceferino V\'azquez, Jorge Sanchez, Rafael Carrascosa
Categories: cs.LG cs.AI
\\
  Automated Machine Learning (AutoML) has become increasingly popular in recent
years due to its ability to reduce the amount of time and expertise required to
design and develop machine learning systems. This is very important for the
practice of machine learning, as it allows building strong baselines quickly,
improving the efficiency of the data scientists, and reducing the time to
production. However, despite the advantages of AutoML, it faces several
challenges, such as defining the solutions space and exploring it efficiently.
Recently, some approaches have been shown to be able to do it using tree-based
search algorithms and context-free grammars. In particular, GramML presents a
model-free reinforcement learning approach that leverages pipeline
configuration grammars and operates using Monte Carlo tree search. However, one
of the limitations of GramML is that it uses default hyperparameters, limiting
the search problem to finding optimal pipeline structures for the available
data preprocessors and models. In this work, we propose an extension to GramML
that supports larger search spaces including hyperparameter search. We
evaluated the approach using an OpenML benchmark and found significant
improvements compared to other state-of-the-art techniques.
\\ ( https://arxiv.org/abs/2404.03419 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03426
Date: Thu, 4 Apr 2024 13:09:26 GMT   (589kb,D)

Title: Accurate estimation of feature importance faithfulness for tree models
Authors: Mateusz Gajewski, Adam Karczmarz, Mateusz Rapicki, Piotr Sankowski
Categories: cs.LG
\\
  In this paper, we consider a perturbation-based metric of predictive
faithfulness of feature rankings (or attributions) that we call PGI squared.
When applied to decision tree-based regression models, the metric can be
computed accurately and efficiently for arbitrary independent feature
perturbation distributions. In particular, the computation does not involve
Monte Carlo sampling that has been typically used for computing similar metrics
and which is inherently prone to inaccuracies. Moreover, we propose a method of
ranking features by their importance for the tree model's predictions based on
PGI squared. Our experiments indicate that in some respects, the method may
identify the globally important features better than the state-of-the-art SHAP
explainer
\\ ( https://arxiv.org/abs/2404.03426 ,  589kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03434
Date: Thu, 4 Apr 2024 13:27:22 GMT   (106kb,D)

Title: Learning From Simplicial Data Based on Random Walks and 1D Convolutions
Authors: Florian Frantzen, Michael T. Schaub
Categories: cs.LG
\\
  Triggered by limitations of graph-based deep learning methods in terms of
computational expressivity and model flexibility, recent years have seen a
surge of interest in computational models that operate on higher-order
topological domains such as hypergraphs and simplicial complexes. While the
increased expressivity of these models can indeed lead to a better
classification performance and a more faithful representation of the underlying
system, the computational cost of these higher-order models can increase
dramatically. To this end, we here explore a simplicial complex neural network
learning architecture based on random walks and fast 1D convolutions (SCRaWl),
in which we can adjust the increase in computational cost by varying the length
and number of random walks considered while accounting for higher-order
relationships. Importantly, due to the random walk-based design, the
expressivity of the proposed architecture is provably incomparable to that of
existing message-passing simplicial neural networks. We empirically evaluate
SCRaWl on real-world datasets and show that it outperforms other simplicial
neural networks.
\\ ( https://arxiv.org/abs/2404.03434 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03473
Date: Thu, 4 Apr 2024 14:26:47 GMT   (106kb)

Title: Generalization Bounds for Message Passing Networks on Mixture of
  Graphons
Authors: Sohir Maskey, Gitta Kutyniok, Ron Levie
Categories: cs.LG
\\
  We study the generalization capabilities of Message Passing Neural Networks
(MPNNs), a prevalent class of Graph Neural Networks (GNN). We derive
generalization bounds specifically for MPNNs with normalized sum aggregation
and mean aggregation. Our analysis is based on a data generation model
incorporating a finite set of template graphons. Each graph within this
framework is generated by sampling from one of the graphons with a certain
degree of perturbation. In particular, we extend previous MPNN generalization
results to a more realistic setting, which includes the following
modifications: 1) we analyze simple random graphs with Bernoulli-distributed
edges instead of weighted graphs; 2) we sample both graphs and graph signals
from perturbed graphons instead of clean graphons; and 3) we analyze sparse
graphs instead of dense graphs. In this more realistic and challenging
scenario, we provide a generalization bound that decreases as the average
number of nodes in the graphs increases. Our results imply that MPNNs with
higher complexity than the size of the training set can still generalize
effectively, as long as the graphs are sufficiently large.
\\ ( https://arxiv.org/abs/2404.03473 ,  106kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03495
Date: Thu, 4 Apr 2024 14:50:50 GMT   (8100kb,D)

Title: About Test-time training for outlier detection
Authors: Simon Kl\"uttermann and Emmanuel M\"uller
Categories: cs.LG
\\
  In this paper, we introduce DOUST, our method applying test-time training for
outlier detection, significantly improving the detection performance. After
thoroughly evaluating our algorithm on common benchmark datasets, we discuss a
common problem and show that it disappears with a large enough test set. Thus,
we conclude that under reasonable conditions, our algorithm can reach almost
supervised performance even when no labeled outliers are given.
\\ ( https://arxiv.org/abs/2404.03495 ,  8100kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03524
Date: Thu, 4 Apr 2024 15:29:50 GMT   (119kb,D)

Title: Approximate Gradient Coding for Privacy-Flexible Federated Learning with
  Non-IID Data
Authors: Okko Makkonen, Sampo Niemel\"a, Camilla Hollanti, Serge Kas Hanna
Categories: cs.LG cs.CR cs.DC cs.IT math.IT stat.ML
\\
  This work focuses on the challenges of non-IID data and stragglers/dropouts
in federated learning. We introduce and explore a privacy-flexible paradigm
that models parts of the clients' local data as non-private, offering a more
versatile and business-oriented perspective on privacy. Within this framework,
we propose a data-driven strategy for mitigating the effects of label
heterogeneity and client straggling on federated learning. Our solution
combines both offline data sharing and approximate gradient coding techniques.
Through numerical simulations using the MNIST dataset, we demonstrate that our
approach enables achieving a deliberate trade-off between privacy and utility,
leading to improved model convergence and accuracy while using an adaptable
portion of non-private data.
\\ ( https://arxiv.org/abs/2404.03524 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03578
Date: Thu, 4 Apr 2024 16:40:22 GMT   (71kb)

Title: Distributionally Robust Reinforcement Learning with Interactive Data
  Collection: Fundamental Hardness and Near-Optimal Algorithm
Authors: Miao Lu, Han Zhong, Tong Zhang, Jose Blanchet
Categories: cs.LG stat.ML
\\
  The sim-to-real gap, which represents the disparity between training and
testing environments, poses a significant challenge in reinforcement learning
(RL). A promising approach to addressing this challenge is distributionally
robust RL, often framed as a robust Markov decision process (RMDP). In this
framework, the objective is to find a robust policy that achieves good
performance under the worst-case scenario among all environments within a
pre-specified uncertainty set centered around the training environment. Unlike
previous work, which relies on a generative model or a pre-collected offline
dataset enjoying good coverage of the deployment environment, we tackle robust
RL via interactive data collection, where the learner interacts with the
training environment only and refines the policy through trial and error. In
this robust RL paradigm, two main challenges emerge: managing distributional
robustness while striking a balance between exploration and exploitation during
data collection. Initially, we establish that sample-efficient learning without
additional assumptions is unattainable owing to the curse of support shift;
i.e., the potential disjointedness of the distributional supports between the
training and testing environments. To circumvent such a hardness result, we
introduce the vanishing minimal value assumption to RMDPs with a
total-variation (TV) distance robust set, postulating that the minimal value of
the optimal robust value function is zero. We prove that such an assumption
effectively eliminates the support shift issue for RMDPs with a TV distance
robust set, and present an algorithm with a provable sample complexity
guarantee. Our work makes the initial step to uncovering the inherent
difficulty of robust RL via interactive data collection and sufficient
conditions for designing a sample-efficient algorithm accompanied by sharp
sample complexity analysis.
\\ ( https://arxiv.org/abs/2404.03578 ,  71kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03586
Date: Thu, 4 Apr 2024 16:52:17 GMT   (3597kb,D)

Title: Leveraging Interpolation Models and Error Bounds for Verifiable
  Scientific Machine Learning
Authors: Tyler Chang and Andrew Gillette and Romit Maulik
Categories: cs.LG stat.ML
\\
  Effective verification and validation techniques for modern scientific
machine learning workflows are challenging to devise. Statistical methods are
abundant and easily deployed, but often rely on speculative assumptions about
the data and methods involved. Error bounds for classical interpolation
techniques can provide mathematically rigorous estimates of accuracy, but often
are difficult or impractical to determine computationally. In this work, we
present a best-of-both-worlds approach to verifiable scientific machine
learning by demonstrating that (1) multiple standard interpolation techniques
have informative error bounds that can be computed or estimated efficiently;
(2) comparative performance among distinct interpolants can aid in validation
goals; (3) deploying interpolation methods on latent spaces generated by deep
learning techniques enables some interpretability for black-box models. We
present a detailed case study of our approach for predicting lift-drag ratios
from airfoil images. Code developed for this work is available in a public
Github repository.
\\ ( https://arxiv.org/abs/2404.03586 ,  3597kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03596
Date: Thu, 4 Apr 2024 17:05:42 GMT   (680kb,D)

Title: Laser Learning Environment: A new environment for coordination-critical
  multi-agent tasks
Authors: Yannick Molinghen, Rapha\"el Avalos, Mark Van Achter, Ann Now\'e, Tom
  Lenaerts
Categories: cs.LG cs.AI cs.MA
Comments: Pre-print, 21 pages
\\
  We introduce the Laser Learning Environment (LLE), a collaborative
multi-agent reinforcement learning environment in which coordination is
central. In LLE, agents depend on each other to make progress
(interdependence), must jointly take specific sequences of actions to succeed
(perfect coordination), and accomplishing those joint actions does not yield
any intermediate reward (zero-incentive dynamics). The challenge of such
problems lies in the difficulty of escaping state space bottlenecks caused by
interdependence steps since escaping those bottlenecks is not rewarded. We test
multiple state-of-the-art value-based MARL algorithms against LLE and show that
they consistently fail at the collaborative task because of their inability to
escape state space bottlenecks, even though they successfully achieve perfect
coordination. We show that Q-learning extensions such as prioritized experience
replay and n-steps return hinder exploration in environments with
zero-incentive dynamics, and find that intrinsic curiosity with random network
distillation is not sufficient to escape those bottlenecks. We demonstrate the
need for novel methods to solve this problem and the relevance of LLE as
cooperative MARL benchmark.
\\ ( https://arxiv.org/abs/2404.03596 ,  680kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03605
Date: Thu, 4 Apr 2024 17:25:30 GMT   (7569kb,D)

Title: Mitigating the Impact of Outlier Channels for Language Model
  Quantization with Activation Regularization
Authors: Aniruddha Nrusimha, Mayank Mishra, Naigang Wang, Dan Alistarh,
  Rameswar Panda, Yoon Kim
Categories: cs.LG cs.CL
\\
  We consider the problem of accurate quantization for language models, where
both the weights and activations are uniformly quantized to 4 bits per
parameter, the lowest bitwidth format natively supported by GPU hardware. In
this context, the key challenge is activation quantization: it is known that
language models contain outlier channels whose values on average are orders of
magnitude higher than than other channels, which prevents accurate low-bitwidth
quantization with known techniques. We systematically study this phenomena and
find that these outlier channels emerge early in training, and that they occur
more frequently in layers with residual streams. We then propose a simple
strategy which regularizes a layer's inputs via quantization-aware training
(QAT) and its outputs via activation kurtosis regularization. We show that
regularizing both the inputs and outputs is crucial for preventing a model's
"migrating" the difficulty in input quantization to the weights, which makes
post-training quantization (PTQ) of weights more difficult. When combined with
weight PTQ, we show that our approach can obtain a W4A4 model that performs
competitively to the standard-precision W16A16 baseline.
\\ ( https://arxiv.org/abs/2404.03605 ,  7569kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03617
Date: Thu, 4 Apr 2024 17:39:41 GMT   (1954kb,D)

Title: On the Efficiency of Convolutional Neural Networks
Authors: Andrew Lavin
Categories: cs.LG cs.CV
\\
  Since the breakthrough performance of AlexNet in 2012, convolutional neural
networks (convnets) have grown into extremely powerful vision models. Deep
learning researchers have used convnets to produce accurate results that were
unachievable a decade ago. Yet computer scientists make computational
efficiency their primary objective. Accuracy with exorbitant cost is not
acceptable; an algorithm must also minimize its computational requirements.
Confronted with the daunting computation that convnets use, deep learning
researchers also became interested in efficiency. Researchers applied
tremendous effort to find the convnet architectures that have the greatest
efficiency. However, skepticism grew among researchers and engineers alike
about the relevance of arithmetic complexity. Contrary to the prevailing view
that latency and arithmetic complexity are irreconcilable, a simple formula
relates both through computational efficiency. This insight enabled us to
co-optimize the separate factors that determine latency. We observed that the
degenerate conv2d layers that produce the best accuracy-complexity trade-off
also have low operational intensity. Therefore, kernels that implement these
layers use significant memory resources. We solved this optimization problem
with block-fusion kernels that implement all layers of a residual block,
thereby creating temporal locality, avoiding communication, and reducing
workspace size. Our ConvFirst model with block-fusion kernels ran approximately
four times as fast as the ConvNeXt baseline with PyTorch Inductor, at equal
accuracy on the ImageNet-1K classification task. Our unified approach to
convnet efficiency envisions a new era of models and kernels that achieve
greater accuracy at lower cost.
\\ ( https://arxiv.org/abs/2404.03617 ,  1954kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.02254 (*cross-listing*)
Date: Tue, 2 Apr 2024 19:21:28 GMT   (264kb)

Title: On Stronger Computational Separations Between Multimodal and Unimodal
  Machine Learning
Authors: Ari Karchmer
Categories: stat.ML cs.AI cs.LG
\\
  In multimodal machine learning, multiple modalities of data (e.g., text and
images) are combined to facilitate the learning of a better machine learning
model, which remains applicable to a corresponding unimodal task (e.g., text
generation). Recently, multimodal machine learning has enjoyed huge empirical
success (e.g. GPT-4). Motivated to develop theoretical justification for this
empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal
learning, and considers possible separations between theoretical models of
multimodal and unimodal learning. In particular, Lu (ALT '24) shows a
computational separation, which is relevant to worst-case instances of the
learning task.
  In this paper, we give a stronger average-case computational separation,
where for "typical" instances of the learning task, unimodal learning is
computationally hard, but multimodal learning is easy. We then question how
"organic" the average-case separation is. Would it be encountered in practice?
To this end, we prove that under natural conditions, any given computational
separation between average-case unimodal and multimodal learning tasks implies
a corresponding cryptographic key agreement protocol. We suggest to interpret
this as evidence that very strong computational advantages of multimodal
learning may arise infrequently in practice, since they exist only for the
"pathological" case of inherently cryptographic distributions. However, this
does not apply to possible (super-polynomial) statistical advantages.
\\ ( https://arxiv.org/abs/2404.02254 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02912 (*cross-listing*)
Date: Mon, 4 Mar 2024 10:40:09 GMT   (28kb)

Title: Probabilistic Generating Circuits -- Demystified
Authors: Sanyam Agarwal, Markus Bl\"aser
Categories: cs.CC cs.AI
\\
  Zhang et al. (ICML 2021, PLMR 139, pp. 12447-1245) introduced probabilistic
generating circuits (PGCs) as a probabilistic model to unify probabilistic
circuits (PCs) and determinantal point processes (DPPs). At a first glance,
PGCs store a distribution in a very different way, they compute the probability
generating polynomial instead of the probability mass function and it seems
that this is the main reason why PGCs are more powerful than PCs or DPPs.
However, PGCs also allow for negative weights, whereas classical PCs assume
that all weights are nonnegative. One of the main insights of our paper is that
the negative weights are responsible for the power of PGCs and not the
different representation. PGCs are PCs in disguise, in particular, we show how
to transform any PGC into a PC with negative weights with only polynomial
blowup.
  PGCs were defined by Zhang et al. only for binary random variables. As our
second main result, we show that there is a good reason for this: we prove that
PGCs for categorial variables with larger image size do not support tractable
marginalization unless NP = P. On the other hand, we show that we can model
categorial variables with larger image size as PC with negative weights
computing set-multilinear polynomials. These allow for tractable
marginalization. In this sense, PCs with negative weights strictly subsume
PGCs.
\\ ( https://arxiv.org/abs/2404.02912 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02923 (*cross-listing*)
Date: Sun, 31 Mar 2024 01:20:01 GMT   (2060kb,D)

Title: An Unsupervised Adversarial Autoencoder for Cyber Attack Detection in
  Power Distribution Grids
Authors: Mehdi Jabbari Zideh, Mohammad Reza Khalghani, and Sarika Khushalani
  Solanki
Categories: cs.CR cs.AI cs.LG cs.SY eess.SY
\\
  Detection of cyber attacks in smart power distribution grids with unbalanced
configurations poses challenges due to the inherent nonlinear nature of these
uncertain and stochastic systems. It originates from the intermittent
characteristics of the distributed energy resources (DERs) generation and load
variations. Moreover, the unknown behavior of cyber attacks, especially false
data injection attacks (FDIAs) in the distribution grids with complex temporal
correlations and the limited amount of labeled data increases the vulnerability
of the grids and imposes a high risk in the secure and reliable operation of
the grids. To address these challenges, this paper proposes an unsupervised
adversarial autoencoder (AAE) model to detect FDIAs in unbalanced power
distribution grids integrated with DERs, i.e., PV systems and wind generation.
The proposed method utilizes long short-term memory (LSTM) in the structure of
the autoencoder to capture the temporal dependencies in the time-series
measurements and leverages the power of generative adversarial networks (GANs)
for better reconstruction of the input data. The advantage of the proposed
data-driven model is that it can detect anomalous points for the system
operation without reliance on abstract models or mathematical representations.
To evaluate the efficacy of the approach, it is tested on IEEE 13-bus and
123-bus systems with historical meteorological data (wind speed, ambient
temperature, and solar irradiance) as well as historical real-world load data
under three types of data falsification functions. The comparison of the
detection results of the proposed model with other unsupervised learning
methods verifies its superior performance in detecting cyber attacks in
unbalanced power distribution grids.
\\ ( https://arxiv.org/abs/2404.02923 ,  2060kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02928 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:49:35 GMT   (20018kb,D)

Title: Jailbreaking Prompt Attack: A Controllable Adversarial Attack against
  Diffusion Models
Authors: Jiachen Ma, Anda Cao, Zhiqing Xiao, Jie Zhang, Chao Ye, Junbo Zhao
Categories: cs.CR cs.AI
\\
  The fast advance of the image generation community has attracted attention
worldwide. The safety issue needs to be further scrutinized and studied. There
have been a few works around this area mostly achieving a post-processing
design, model-specific, or yielding suboptimal image quality generation.
Despite that, in this article, we discover a black-box attack method that
enjoys three merits. It enables (i)-attacks both directed and semantic-driven
that theoretically and practically pose a hazard to this vast user community,
(ii)-surprisingly surpasses the white-box attack in a black-box manner and
(iii)-without requiring any post-processing effort. Core to our approach is
inspired by the concept guidance intriguing property of Classifier-Free
guidance (CFG) in T2I models, and we discover that conducting frustratingly
simple guidance in the CLIP embedding space, coupled with the semantic loss and
an additionally sensitive word list works very well. Moreover, our results
expose and highlight the vulnerabilities in existing defense mechanisms.
\\ ( https://arxiv.org/abs/2404.02928 ,  20018kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02933 (*cross-listing*)
Date: Wed, 3 Apr 2024 01:09:41 GMT   (1112kb,D)

Title: NL2KQL: From Natural Language to Kusto Query
Authors: Amir H. Abdi, Xinye Tang, Jeremias Eichelbaum, Mahan Das, Alex Klein,
  Nihal Irmak Pakis, William Blum, Daniel L Mace, Tanvi Raja, Namrata
  Padmanabhan, Ye Xing
Categories: cs.DB cs.AI cs.CL
\\
  Data is growing rapidly in volume and complexity. Proficiency in database
query languages is pivotal for crafting effective queries. As coding assistants
become more prevalent, there is significant opportunity to enhance database
query languages. The Kusto Query Language (KQL) is a widely used query language
for large semi-structured data such as logs, telemetries, and time-series for
big data analytics platforms. This paper introduces NL2KQL an innovative
framework that uses large language models (LLMs) to convert natural language
queries (NLQs) to KQL queries. The proposed NL2KQL framework includes several
key components: Schema Refiner which narrows down the schema to its most
pertinent elements; the Few-shot Selector which dynamically selects relevant
examples from a few-shot dataset; and the Query Refiner which repairs syntactic
and semantic errors in KQL queries. Additionally, this study outlines a method
for generating large datasets of synthetic NLQ-KQL pairs which are valid within
a specific database contexts. To validate NL2KQL's performance, we utilize an
array of online (based on query execution) and offline (based on query parsing)
metrics. Through ablation studies, the significance of each framework component
is examined, and the datasets used for benchmarking are made publicly
available. This work is the first of its kind and is compared with available
baselines to demonstrate its effectiveness.
\\ ( https://arxiv.org/abs/2404.02933 ,  1112kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02990 (*cross-listing*)
Date: Wed, 3 Apr 2024 18:20:41 GMT   (38623kb,D)

Title: ASAP: Interpretable Analysis and Summarization of AI-generated Image
  Patterns at Scale
Authors: Jinbin Huang, Chen Chen, Aditi Mishra, Bum Chul Kwon, Zhicheng Liu,
  Chris Bryan
Categories: cs.CV cs.AI cs.HC
Comments: 9 pages, 6 figures
\\
  Generative image models have emerged as a promising technology to produce
realistic images. Despite potential benefits, concerns grow about its misuse,
particularly in generating deceptive images that could raise significant
ethical, legal, and societal issues. Consequently, there is growing demand to
empower users to effectively discern and comprehend patterns of AI-generated
images. To this end, we developed ASAP, an interactive visualization system
that automatically extracts distinct patterns of AI-generated images and allows
users to interactively explore them via various views. To uncover fake
patterns, ASAP introduces a novel image encoder, adapted from CLIP, which
transforms images into compact "distilled" representations, enriched with
information for differentiating authentic and fake images. These
representations generate gradients that propagate back to the attention maps of
CLIP's transformer block. This process quantifies the relative importance of
each pixel to image authenticity or fakeness, exposing key deceptive patterns.
ASAP enables the at scale interactive analysis of these patterns through
multiple, coordinated visualizations. This includes a representation overview
with innovative cell glyphs to aid in the exploration and qualitative
evaluation of fake patterns across a vast array of images, as well as a pattern
view that displays authenticity-indicating patterns in images and quantifies
their impact. ASAP supports the analysis of cutting-edge generative models with
the latest architectures, including GAN-based models like proGAN and diffusion
models like the latent diffusion model. We demonstrate ASAP's usefulness
through two usage scenarios using multiple fake image detection benchmark
datasets, revealing its ability to identify and understand hidden patterns in
AI-generated images, especially in detecting fake human faces produced by
diffusion-based techniques.
\\ ( https://arxiv.org/abs/2404.02990 ,  38623kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03023 (*cross-listing*)
Date: Wed, 3 Apr 2024 19:18:25 GMT   (38kb)

Title: Toward Safe Evolution of Artificial Intelligence (AI) based
  Conversational Agents to Support Adolescent Mental and Sexual Health
  Knowledge Discovery
Authors: Jinkyung Park, Vivek Singh, Pamela Wisniewski
Categories: cs.HC cs.AI
Comments: This paper has been peer-reviewed and presented at the "CHI 2024
  Workshop on Child-centred AI Design, May 11, 2024, Honolulu, HI, USA."
\\
  Following the recent release of various Artificial Intelligence (AI) based
Conversation Agents (CAs), adolescents are increasingly using CAs for
interactive knowledge discovery on sensitive topics, including mental and
sexual health topics. Exploring such sensitive topics through online search has
been an essential part of adolescent development, and CAs can support their
knowledge discovery on such topics through human-like dialogues. Yet,
unintended risks have been documented with adolescents' interactions with
AI-based CAs, such as being exposed to inappropriate content, false
information, and/or being given advice that is detrimental to their mental and
physical well-being (e.g., to self-harm). In this position paper, we discuss
the current landscape and opportunities for CAs to support adolescents' mental
and sexual health knowledge discovery. We also discuss some of the challenges
related to ensuring the safety of adolescents when interacting with CAs
regarding sexual and mental health topics. We call for a discourse on how to
set guardrails for the safe evolution of AI-based CAs for adolescents.
\\ ( https://arxiv.org/abs/2404.03023 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03027 (*cross-listing*)
Date: Wed, 3 Apr 2024 19:23:18 GMT   (4573kb,D)

Title: JailBreakV-28K: A Benchmark for Assessing the Robustness of MultiModal
  Large Language Models against Jailbreak Attacks
Authors: Weidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo, Chaowei Xiao
Categories: cs.CR cs.AI cs.CL
\\
  With the rapid advancements in Multimodal Large Language Models (MLLMs),
securing these models against malicious inputs while align- ing them with human
values has emerged as a critical challenge. In this paper, we investigate an
important and unexplored question of whether techniques that successfully
jailbreak Large Language Models (LLMs) can be equally effective in jailbreaking
MLLMs. To explore this issue, we in- troduce JailBreakV-28K, a pioneering
benchmark designed to assess the transferability of LLM jailbreak techniques to
MLLMs, thereby evaluat- ing the robustness of MLLMs against diverse jailbreak
attacks. Utilizing a dataset of 2, 000 malicious queries that is also proposed
in this paper, we generate 20, 000 text-based jailbreak prompts using advanced
jailbreak attacks on LLMs, alongside 8, 000 image-based jailbreak inputs from
recent MLLMs jailbreak attacks, our comprehensive dataset includes 28, 000 test
cases across a spectrum of adversarial scenarios. Our evaluation of 10 open-
source MLLMs reveals a notably high Attack Success Rate (ASR) for attacks
transferred from LLMs, highlighting a critical vulnerability in MLLMs that
stems from their text-processing capabilities. Our findings underscore the
urgent need for future research to address alignment vulnerabilities in MLLMs
from both textual and visual inputs.
\\ ( https://arxiv.org/abs/2404.03027 ,  4573kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03085 (*cross-listing*)
Date: Wed, 3 Apr 2024 21:55:44 GMT   (19385kb,D)

Title: Talaria: Interactively Optimizing Machine Learning Models for Efficient
  Inference
Authors: Fred Hohman, Chaoqun Wang, Jinmook Lee, Jochen G\"ortler, Dominik
  Moritz, Jeffrey P Bigham, Zhile Ren, Cecile Foret, Qi Shan, Xiaoyi Zhang
Categories: cs.HC cs.AI cs.LG
Comments: Proceedings of the 2024 ACM CHI Conference on Human Factors in
  Computing Systems
DOI: 10.1145/3613904.3642628
\\
  On-device machine learning (ML) moves computation from the cloud to personal
devices, protecting user privacy and enabling intelligent user experiences.
However, fitting models on devices with limited resources presents a major
technical challenge: practitioners need to optimize models and balance hardware
metrics such as model size, latency, and power. To help practitioners create
efficient ML models, we designed and developed Talaria: a model visualization
and optimization system. Talaria enables practitioners to compile models to
hardware, interactively visualize model statistics, and simulate optimizations
to test the impact on inference metrics. Since its internal deployment two
years ago, we have evaluated Talaria using three methodologies: (1) a log
analysis highlighting its growth of 800+ practitioners submitting 3,600+
models; (2) a usability survey with 26 users assessing the utility of 20
Talaria features; and (3) a qualitative interview with the 7 most active users
about their experience using Talaria.
\\ ( https://arxiv.org/abs/2404.03085 ,  19385kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03114 (*cross-listing*)
Date: Wed, 3 Apr 2024 23:33:56 GMT   (335kb,D)

Title: Testing the Effect of Code Documentation on Large Language Model Code
  Understanding
Authors: William Macke and Michael Doyle
Categories: cs.SE cs.AI cs.CL
Comments: 7 pages, 5 figures, 2 tables. Accepted as a Findings paper in the
  "Generation" track to NAACL 2024. MITRE Public Release Case Number 23-4132
\\
  Large Language Models (LLMs) have demonstrated impressive abilities in recent
years with regards to code generation and understanding. However, little work
has investigated how documentation and other code properties affect an LLM's
ability to understand and generate code or documentation. We present an
empirical analysis of how underlying properties of code or documentation can
affect an LLM's capabilities. We show that providing an LLM with "incorrect"
documentation can greatly hinder code understanding, while incomplete or
missing documentation does not seem to significantly affect an LLM's ability to
understand code.
\\ ( https://arxiv.org/abs/2404.03114 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03133 (*cross-listing*)
Date: Thu, 4 Apr 2024 00:58:19 GMT   (238kb,D)

Title: A Framework for Guided Motion Planning
Authors: Amnon Attali, Stav Ashur, Isaac Burton Love, Courtney McBeth, James
  Motes, Marco Morales, Nancy M. Amato
Categories: cs.RO cs.AI
\\
  Randomized sampling based algorithms are widely used in robot motion planning
due to the problem's intractability, and are experimentally effective on a wide
range of problem instances. Most variants bias their sampling using various
heuristics related to the known underlying structure of the search space. In
this work, we formalize the intuitive notion of guided search by defining the
concept of a guiding space. This new language encapsulates many seemingly
distinct prior methods under the same framework, and allows us to reason about
guidance, a previously obscured core contribution of different algorithms. We
suggest an information theoretic method to evaluate guidance, which
experimentally matches intuition when tested on known algorithms in a variety
of environments. The language and evaluation of guidance suggests improvements
to existing methods, and allows for simple hybrid algorithms that combine
guidance from multiple sources.
\\ ( https://arxiv.org/abs/2404.03133 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03164 (*cross-listing*)
Date: Thu, 4 Apr 2024 02:32:58 GMT   (1179kb)

Title: Does Knowledge Graph Really Matter for Recommender Systems?
Authors: Haonan Zhang, Dongxia Wang, Zhu Sun, Yanhui Li, Youcheng Sun, Huizhi
  Liang, Wenhai Wang
Categories: cs.IR cs.AI cs.LG
\\
  Recommender systems (RSs) are designed to provide personalized
recommendations to users. Recently, knowledge graphs (KGs) have been widely
introduced in RSs to improve recommendation accuracy. In this study, however,
we demonstrate that RSs do not necessarily perform worse even if the KG is
downgraded to the user-item interaction graph only (or removed). We propose an
evaluation framework KG4RecEval to systematically evaluate how much a KG
contributes to the recommendation accuracy of a KG-based RS, using our defined
metric KGER (KG utilization efficiency in recommendation). We consider the
scenarios where knowledge in a KG gets completely removed, randomly distorted
and decreased, and also where recommendations are for cold-start users. Our
extensive experiments on four commonly used datasets and a number of
state-of-the-art KG-based RSs reveal that: to remove, randomly distort or
decrease knowledge does not necessarily decrease recommendation accuracy, even
for cold-start users. These findings inspire us to rethink how to better
utilize knowledge from existing KGs, whereby we discuss and provide insights
into what characteristics of datasets and KG-based RSs may help improve KG
utilization efficiency.
\\ ( https://arxiv.org/abs/2404.03164 ,  1179kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03190 (*cross-listing*)
Date: Thu, 4 Apr 2024 04:22:25 GMT   (4048kb,D)

Title: Adaptive Discrete Disparity Volume for Self-supervised Monocular Depth
  Estimation
Authors: Jianwei Ren
Categories: cs.CV cs.AI cs.RO
\\
  In self-supervised monocular depth estimation tasks, discrete disparity
prediction has been proven to attain higher quality depth maps than common
continuous methods. However, current discretization strategies often divide
depth ranges of scenes into bins in a handcrafted and rigid manner, limiting
model performance. In this paper, we propose a learnable module, Adaptive
Discrete Disparity Volume (ADDV), which is capable of dynamically sensing depth
distributions in different RGB images and generating adaptive bins for them.
Without any extra supervision, this module can be integrated into existing CNN
architectures, allowing networks to produce representative values for bins and
a probability volume over them. Furthermore, we introduce novel training
strategies - uniformizing and sharpening - through a loss term and temperature
parameter, respectively, to provide regularizations under self-supervised
conditions, preventing model degradation or collapse. Empirical results
demonstrate that ADDV effectively processes global information, generating
appropriate bins for various scenes and producing higher quality depth maps
compared to handcrafted methods.
\\ ( https://arxiv.org/abs/2404.03190 ,  4048kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03204 (*cross-listing*)
Date: Thu, 4 Apr 2024 05:15:07 GMT   (233kb,D)

Title: RALL-E: Robust Codec Language Modeling with Chain-of-Thought Prompting
  for Text-to-Speech Synthesis
Authors: Detai Xin, Xu Tan, Kai Shen, Zeqian Ju, Dongchao Yang, Yuancheng Wang,
  Shinnosuke Takamichi, Hiroshi Saruwatari, Shujie Liu, Jinyu Li, Sheng Zhao
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
\\
  We present RALL-E, a robust language modeling method for text-to-speech (TTS)
synthesis. While previous work based on large language models (LLMs) shows
impressive performance on zero-shot TTS, such methods often suffer from poor
robustness, such as unstable prosody (weird pitch and rhythm/duration) and a
high word error rate (WER), due to the autoregressive prediction style of
language models. The core idea behind RALL-E is chain-of-thought (CoT)
prompting, which decomposes the task into simpler steps to enhance the
robustness of LLM-based TTS. To accomplish this idea, RALL-E first predicts
prosody features (pitch and duration) of the input text and uses them as
intermediate conditions to predict speech tokens in a CoT style. Second, RALL-E
utilizes the predicted duration prompt to guide the computing of self-attention
weights in Transformer to enforce the model to focus on the corresponding
phonemes and prosody features when predicting speech tokens. Results of
comprehensive objective and subjective evaluations demonstrate that, compared
to a powerful baseline method VALL-E, RALL-E significantly improves the WER of
zero-shot TTS from $6.3\%$ (without reranking) and $2.1\%$ (with reranking) to
$2.8\%$ and $1.0\%$, respectively. Furthermore, we demonstrate that RALL-E
correctly synthesizes sentences that are hard for VALL-E and reduces the error
rate from $68\%$ to $4\%$.
\\ ( https://arxiv.org/abs/2404.03204 ,  233kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03239 (*cross-listing*)
Date: Thu, 4 Apr 2024 06:54:44 GMT   (3688kb,D)

Title: Exploring Emotions in Multi-componential Space using Interactive VR
  Games
Authors: Rukshani Somarathna, Gelareh Mohammadi
Categories: cs.HC cs.AI cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Emotion understanding is a complex process that involves multiple components.
The ability to recognise emotions not only leads to new context awareness
methods but also enhances system interaction's effectiveness by perceiving and
expressing emotions. Despite the attention to discrete and dimensional models,
neuroscientific evidence supports those emotions as being complex and
multi-faceted. One framework that resonated well with such findings is the
Component Process Model (CPM), a theory that considers the complexity of
emotions with five interconnected components: appraisal, expression,
motivation, physiology and feeling. However, the relationship between CPM and
discrete emotions has not yet been fully explored. Therefore, to better
understand emotions underlying processes, we operationalised a data-driven
approach using interactive Virtual Reality (VR) games and collected multimodal
measures (self-reports, physiological and facial signals) from 39 participants.
We used Machine Learning (ML) methods to identify the unique contributions of
each component to emotion differentiation. Our results showed the role of
different components in emotion differentiation, with the model including all
components demonstrating the most significant contribution. Moreover, we found
that at least five dimensions are needed to represent the variation of emotions
in our dataset. These findings also have implications for using VR environments
in emotion research and highlight the role of physiological signals in emotion
recognition within such environments.
\\ ( https://arxiv.org/abs/2404.03239 ,  3688kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03253 (*cross-listing*)
Date: Thu, 4 Apr 2024 07:19:31 GMT   (706kb,D)

Title: A dataset of primary nasopharyngeal carcinoma MRI with multi-modalities
  segmentation
Authors: Yin Li, Qi Chen, Kai Wang, Meige Li, Liping Si, Yingwei Guo, Yu Xiong,
  Qixing Wang, Yang Qin, Ling Xu, Patrick van der Smagt, Jun Tang, Nutan Chen
Categories: eess.IV cs.AI cs.CV cs.LG
\\
  Multi-modality magnetic resonance imaging data with various sequences
facilitate the early diagnosis, tumor segmentation, and disease staging in the
management of nasopharyngeal carcinoma (NPC). The lack of publicly available,
comprehensive datasets limits advancements in diagnosis, treatment planning,
and the development of machine learning algorithms for NPC. Addressing this
critical need, we introduce the first comprehensive NPC MRI dataset,
encompassing MR axial imaging of 277 primary NPC patients. This dataset
includes T1-weighted, T2-weighted, and contrast-enhanced T1-weighted sequences,
totaling 831 scans. In addition to the corresponding clinical data, manually
annotated and labeled segmentations by experienced radiologists offer
high-quality data resources from untreated primary NPC.
\\ ( https://arxiv.org/abs/2404.03253 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03264 (*cross-listing*)
Date: Thu, 4 Apr 2024 07:39:55 GMT   (2377kb,D)

Title: Foundation Model for Advancing Healthcare: Challenges, Opportunities,
  and Future Directions
Authors: Yuting He, Fuxiang Huang, Xinrui Jiang, Yuxiang Nie, Minghao Wang,
  Jiguang Wang, Hao Chen
Categories: cs.CY cs.AI
\\
  Foundation model, which is pre-trained on broad data and is able to adapt to
a wide range of tasks, is advancing healthcare. It promotes the development of
healthcare artificial intelligence (AI) models, breaking the contradiction
between limited AI models and diverse healthcare practices. Much more
widespread healthcare scenarios will benefit from the development of a
healthcare foundation model (HFM), improving their advanced intelligent
healthcare services. Despite the impending widespread deployment of HFMs, there
is currently a lack of clear understanding about how they work in the
healthcare field, their current challenges, and where they are headed in the
future. To answer these questions, a comprehensive and deep survey of the
challenges, opportunities, and future directions of HFMs is presented in this
survey. It first conducted a comprehensive overview of the HFM including the
methods, data, and applications for a quick grasp of the current progress.
Then, it made an in-depth exploration of the challenges present in data,
algorithms, and computing infrastructures for constructing and widespread
application of foundation models in healthcare. This survey also identifies
emerging and promising directions in this field for future development. We
believe that this survey will enhance the community's comprehension of the
current progress of HFM and serve as a valuable source of guidance for future
development in this field. The latest HFM papers and related resources are
maintained on our website:
https://github.com/YutingHe-list/Awesome-Foundation-Models-for-Advancing-Healthcare.
\\ ( https://arxiv.org/abs/2404.03264 ,  2377kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03275 (*cross-listing*)
Date: Thu, 4 Apr 2024 07:59:24 GMT   (11165kb,D)

Title: DELTA: Decomposed Efficient Long-Term Robot Task Planning using Large
  Language Models
Authors: Yuchen Liu, Luigi Palmieri, Sebastian Koch, Ilche Georgievski, Marco
  Aiello
Categories: cs.RO cs.AI
\\
  Recent advancements in Large Language Models (LLMs) have sparked a revolution
across various research fields. In particular, the integration of common-sense
knowledge from LLMs into robot task and motion planning has been proven to be a
game-changer, elevating performance in terms of explainability and downstream
task efficiency to unprecedented heights. However, managing the vast knowledge
encapsulated within these large models has posed challenges, often resulting in
infeasible plans generated by LLM-based planning systems due to hallucinations
or missing domain information. To overcome these challenges and obtain even
greater planning feasibility and computational efficiency, we propose a novel
LLM-driven task planning approach called DELTA. For achieving better grounding
from environmental topology into actionable knowledge, DELTA leverages the
power of scene graphs as environment representations within LLMs, enabling the
fast generation of precise planning problem descriptions. For obtaining higher
planning performance, we use LLMs to decompose the long-term task goals into an
autoregressive sequence of sub-goals for an automated task planner to solve.
Our contribution enables a more efficient and fully automatic task planning
pipeline, achieving higher planning success rates and significantly shorter
planning times compared to the state of the art.
\\ ( https://arxiv.org/abs/2404.03275 ,  11165kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03276 (*cross-listing*)
Date: Thu, 4 Apr 2024 08:00:12 GMT   (1306kb,D)

Title: A Deep Reinforcement Learning Approach for Security-Aware Service
  Acquisition in IoT
Authors: Marco Arazzi, Serena Nicolazzo, Antonino Nocera
Categories: cs.CR cs.AI
\\
  The novel Internet of Things (IoT) paradigm is composed of a growing number
of heterogeneous smart objects and services that are transforming architectures
and applications, increasing systems' complexity, and the need for reliability
and autonomy. In this context, both smart objects and services are often
provided by third parties which do not give full transparency regarding the
security and privacy of the features offered. Although machine-based Service
Level Agreements (SLA) have been recently leveraged to establish and share
policies in Cloud-based scenarios, and also in the IoT context, the issue of
making end users aware of the overall system security levels and the
fulfillment of their privacy requirements through the provision of the
requested service remains a challenging task. To tackle this problem, we
propose a complete framework that defines suitable levels of privacy and
security requirements in the acquisition of services in IoT, according to the
user needs. Through the use of a Reinforcement Learning based solution, a user
agent, inside the environment, is trained to choose the best smart objects
granting access to the target services. Moreover, the solution is designed to
guarantee deadline requirements and user security and privacy needs. Finally,
to evaluate the correctness and the performance of the proposed approach we
illustrate an extensive experimental analysis.
\\ ( https://arxiv.org/abs/2404.03276 ,  1306kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03323 (*cross-listing*)
Date: Thu, 4 Apr 2024 09:43:43 GMT   (24647kb,D)

Title: Sparse Concept Bottleneck Models: Gumbel Tricks in Contrastive Learning
Authors: Andrei Semenov, Vladimir Ivanov, Aleksandr Beznosikov, Alexander
  Gasnikov
Categories: cs.CV cs.AI
Comments: 23 pages, 1 algorithm, 36 figures
MSC-class: I.2.6, I.2.10, I.4.10, I.5.1, I.5.4, I.5.5
ACM-class: I.2.6; I.2.10; I.4.10; I.5.1; I.5.4; I.5.5
\\
  We propose a novel architecture and method of explainable classification with
Concept Bottleneck Models (CBMs). While SOTA approaches to Image Classification
task work as a black box, there is a growing demand for models that would
provide interpreted results. Such a models often learn to predict the
distribution over class labels using additional description of this target
instances, called concepts. However, existing Bottleneck methods have a number
of limitations: their accuracy is lower than that of a standard model and CBMs
require an additional set of concepts to leverage. We provide a framework for
creating Concept Bottleneck Model from pre-trained multi-modal encoder and new
CLIP-like architectures. By introducing a new type of layers known as Concept
Bottleneck Layers, we outline three methods for training them: with
$\ell_1$-loss, contrastive loss and loss function based on Gumbel-Softmax
distribution (Sparse-CBM), while final FC layer is still trained with
Cross-Entropy. We show a significant increase in accuracy using sparse hidden
layers in CLIP-based bottleneck models. Which means that sparse representation
of concepts activation vector is meaningful in Concept Bottleneck Models.
Moreover, with our Concept Matrix Search algorithm we can improve CLIP
predictions on complex datasets without any additional training or fine-tuning.
The code is available at: https://github.com/Andron00e/SparseCBM.
\\ ( https://arxiv.org/abs/2404.03323 ,  24647kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03325 (*cross-listing*)
Date: Thu, 4 Apr 2024 09:52:22 GMT   (1572kb,D)

Title: Embodied Neuromorphic Artificial Intelligence for Robotics:
  Perspectives, Challenges, and Research Development Stack
Authors: Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Fakhreddine Zayer,
  Jorge Dias, Muhammad Shafique
Categories: cs.RO cs.AI cs.LG cs.NE
Comments: 8 pages, 9 figures, 1 table
\\
  Robotic technologies have been an indispensable part for improving human
productivity since they have been helping humans in completing diverse,
complex, and intensive tasks in a fast yet accurate and efficient way.
Therefore, robotic technologies have been deployed in a wide range of
applications, ranging from personal to industrial use-cases. However, current
robotic technologies and their computing paradigm still lack embodied
intelligence to efficiently interact with operational environments, respond
with correct/expected actions, and adapt to changes in the environments. Toward
this, recent advances in neuromorphic computing with Spiking Neural Networks
(SNN) have demonstrated the potential to enable the embodied intelligence for
robotics through bio-plausible computing paradigm that mimics how the
biological brain works, known as "neuromorphic artificial intelligence (AI)".
However, the field of neuromorphic AI-based robotics is still at an early
stage, therefore its development and deployment for solving real-world problems
expose new challenges in different design aspects, such as accuracy,
adaptability, efficiency, reliability, and security. To address these
challenges, this paper will discuss how we can enable embodied neuromorphic AI
for robotic systems through our perspectives: (P1) Embodied intelligence based
on effective learning rule, training mechanism, and adaptability; (P2)
Cross-layer optimizations for energy-efficient neuromorphic computing; (P3)
Representative and fair benchmarks; (P4) Low-cost reliability and safety
enhancements; (P5) Security and privacy for neuromorphic computing; and (P6) A
synergistic development for energy-efficient and robust neuromorphic-based
robotics. Furthermore, this paper identifies research challenges and
opportunities, as well as elaborates our vision for future research development
toward embodied neuromorphic AI for robotics.
\\ ( https://arxiv.org/abs/2404.03325 ,  1572kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03354 (*cross-listing*)
Date: Thu, 4 Apr 2024 10:45:23 GMT   (703kb,D)

Title: A Comprehensive Survey on Self-Supervised Learning for Recommendation
Authors: Xubin Ren, Wei Wei, Lianghao Xia and Chao Huang
Categories: cs.IR cs.AI
\\
  Recommender systems play a crucial role in tackling the challenge of
information overload by delivering personalized recommendations based on
individual user preferences. Deep learning techniques, such as RNNs, GNNs, and
Transformer architectures, have significantly propelled the advancement of
recommender systems by enhancing their comprehension of user behaviors and
preferences. However, supervised learning methods encounter challenges in
real-life scenarios due to data sparsity, resulting in limitations in their
ability to learn representations effectively. To address this, self-supervised
learning (SSL) techniques have emerged as a solution, leveraging inherent data
structures to generate supervision signals without relying solely on labeled
data. By leveraging unlabeled data and extracting meaningful representations,
recommender systems utilizing SSL can make accurate predictions and
recommendations even when confronted with data sparsity. In this paper, we
provide a comprehensive review of self-supervised learning frameworks designed
for recommender systems, encompassing a thorough analysis of over 170 papers.
We conduct an exploration of nine distinct scenarios, enabling a comprehensive
understanding of SSL-enhanced recommenders in different contexts. For each
domain, we elaborate on different self-supervised learning paradigms, namely
contrastive learning, generative learning, and adversarial learning, so as to
present technical details of how SSL enhances recommender systems in various
contexts. We consistently maintain the related open-source materials at
https://github.com/HKUDS/Awesome-SSLRec-Papers.
\\ ( https://arxiv.org/abs/2404.03354 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03386 (*cross-listing*)
Date: Thu, 4 Apr 2024 11:37:55 GMT   (3050kb,D)

Title: SENSOR: Imitate Third-Person Expert's Behaviors via Active Sensoring
Authors: Kaichen Huang, Minghao Shao, Shenghua Wan, Hai-Hang Sun, Shuai Feng,
  Le Gan, De-Chuan Zhan
Categories: cs.RO cs.AI cs.LG
\\
  In many real-world visual Imitation Learning (IL) scenarios, there is a
misalignment between the agent's and the expert's perspectives, which might
lead to the failure of imitation. Previous methods have generally solved this
problem by domain alignment, which incurs extra computation and storage costs,
and these methods fail to handle the \textit{hard cases} where the viewpoint
gap is too large. To alleviate the above problems, we introduce active
sensoring in the visual IL setting and propose a model-based SENSory imitatOR
(SENSOR) to automatically change the agent's perspective to match the expert's.
SENSOR jointly learns a world model to capture the dynamics of latent states, a
sensor policy to control the camera, and a motor policy to control the agent.
Experiments on visual locomotion tasks show that SENSOR can efficiently
simulate the expert's perspective and strategy, and outperforms most baseline
methods.
\\ ( https://arxiv.org/abs/2404.03386 ,  3050kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03418 (*cross-listing*)
Date: Thu, 4 Apr 2024 12:51:28 GMT   (40kb)

Title: Permissible Knowledge Pooling
Authors: Huimin Dong
Categories: cs.LO cs.AI
\\
  Information pooling has been extensively formalised across various logical
frameworks in distributed systems, characterized by diverse information-sharing
patterns. These approaches generally adopt an intersection perspective,
aggregating all possible information, regardless of whether it is known or
unknown to the agents. In contrast, this work adopts a unique stance,
emphasising that sharing knowledge means distributing what is known, rather
than what remains uncertain. This paper introduces a dynamic logic for
knowledge pooling or sharing and further discusses a potential framework for
permissible knowledge pooling.
\\ ( https://arxiv.org/abs/2404.03418 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03425 (*cross-listing*)
Date: Thu, 4 Apr 2024 13:06:25 GMT   (7313kb,D)

Title: ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State
  Space Model
Authors: Hongruixuan Chen and Jian Song and Chengxi Han and Junshi Xia and
  Naoto Yokoya
Categories: eess.IV cs.AI cs.CV
\\
  Convolutional neural networks (CNN) and Transformers have made impressive
progress in the field of remote sensing change detection (CD). However, both
architectures have their inherent shortcomings. Recently, the Mamba
architecture, based on spatial state models, has shown remarkable performance
in a series of natural language processing tasks, which can effectively
compensate for the shortcomings of the above two architectures. In this paper,
we explore for the first time the potential of the Mamba architecture for
remote sensing change detection tasks. We tailor the corresponding frameworks,
called MambaBCD, MambaSCD, and MambaBDA, for binary change detection (BCD),
semantic change detection (SCD), and building damage assessment (BDA),
respectively. All three frameworks adopt the cutting-edge visual Mamba
architecture as the encoder, which allows full learning of global spatial
contextual information from the input images. For the change decoder, which is
available in all three architectures, we propose three spatio-temporal
relationship modeling mechanisms, which can be naturally combined with the
Mamba architecture and fully utilize its attribute to achieve spatio-temporal
interaction of multi-temporal features and obtain accurate change information.
On five benchmark datasets, our proposed frameworks outperform current CNN- and
Transformer-based approaches without using any complex strategies or tricks,
fully demonstrating the potential of the Mamba architecture. Specifically, we
obtained 83.11%, 88.39% and 94.19% F1 scores on the three BCD datasets SYSU,
LEVIR-CD+, and WHU-CD; on the SCD dataset SECOND, we obtained 24.04% SeK; and
on the xBD dataset, we obtained 81.41% overall F1 score. The source code will
be available in https://github.com/ChenHongruixuan/MambaCD
\\ ( https://arxiv.org/abs/2404.03425 ,  7313kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03474 (*cross-listing*)
Date: Thu, 4 Apr 2024 14:26:58 GMT   (6094kb,D)

Title: Performance of computer vision algorithms for fine-grained
  classification using crowdsourced insect images
Authors: Rita Pucci, Vincent J. Kalkman, and Dan Stowell
Categories: cs.CV cs.AI
\\
  With fine-grained classification, we identify unique characteristics to
distinguish among classes of the same super-class. We are focusing on species
recognition in Insecta, as they are critical for biodiversity monitoring and at
the base of many ecosystems. With citizen science campaigns, billions of images
are collected in the wild. Once these are labelled, experts can use them to
create distribution maps. However, the labelling process is time-consuming,
which is where computer vision comes in. The field of computer vision offers a
wide range of algorithms, each with its strengths and weaknesses; how do we
identify the algorithm that is in line with our application? To answer this
question, we provide a full and detailed evaluation of nine algorithms among
deep convolutional networks (CNN), vision transformers (ViT), and
locality-based vision transformers (LBVT) on 4 different aspects:
classification performance, embedding quality, computational cost, and gradient
activity. We offer insights that we haven't yet had in this domain proving to
which extent these algorithms solve the fine-grained tasks in Insecta. We found
that the ViT performs the best on inference speed and computational cost while
the LBVT outperforms the others on performance and embedding quality; the CNN
provide a trade-off among the metrics.
\\ ( https://arxiv.org/abs/2404.03474 ,  6094kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03493 (*cross-listing*)
Date: Thu, 4 Apr 2024 14:48:26 GMT   (1276kb,D)

Title: A Methodology to Study the Impact of Spiking Neural Network Parameters
  considering Event-Based Automotive Data
Authors: Iqra Bano, Rachmad Vidya Wicaksana Putra, Alberto Marchisio, Muhammad
  Shafique
Categories: cs.NE cs.AI cs.LG cs.RO
Comments: 7 pages, 13 figures, 1 table
\\
  Autonomous Driving (AD) systems are considered as the future of human
mobility and transportation. Solving computer vision tasks such as image
classification and object detection/segmentation, with high accuracy and low
power/energy consumption, is highly needed to realize AD systems in real life.
These requirements can potentially be satisfied by Spiking Neural Networks
(SNNs). However, the state-of-the-art works in SNN-based AD systems still focus
on proposing network models that can achieve high accuracy, and they have not
systematically studied the roles of SNN parameters when used for learning
event-based automotive data. Therefore, we still lack understanding of how to
effectively develop SNN models for AD systems. Toward this, we propose a novel
methodology to systematically study and analyze the impact of SNN parameters
considering event-based automotive data, then leverage this analysis for
enhancing SNN developments. To do this, we first explore different settings of
SNN parameters that directly affect the learning mechanism (i.e., batch size,
learning rate, neuron threshold potential, and weight decay), then analyze the
accuracy results. Afterward, we propose techniques that jointly improve SNN
accuracy and reduce training time. Experimental results show that our
methodology can improve the SNN models for AD systems than the
state-of-the-art, as it achieves higher accuracy (i.e., 86%) for the NCARS
dataset, and it can also achieve iso-accuracy (i.e., ~85% with standard
deviation less than 0.5%) while speeding up the training time by 1.9x. In this
manner, our research work provides a set of guidelines for SNN parameter
enhancements, thereby enabling the practical developments of SNN-based AD
systems.
\\ ( https://arxiv.org/abs/2404.03493 ,  1276kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03543 (*cross-listing*)
Date: Thu, 4 Apr 2024 15:49:49 GMT   (2063kb,D)

Title: CodeEditorBench: Evaluating Code Editing Capability of Large Language
  Models
Authors: Jiawei Guo, Ziming Li, Xueling Liu, Kaijing Ma, Tianyu Zheng,
  Zhouliang Yu, Ding Pan, Yizhi LI, Ruibo Liu, Yue Wang, Shuyue Guo, Xingwei
  Qu, Xiang Yue, Ge Zhang, Wenhu Chen, Jie Fu
Categories: cs.SE cs.AI cs.CL cs.LG
\\
  Large Language Models (LLMs) for code are rapidly evolving, with code editing
emerging as a critical capability. We introduce CodeEditorBench, an evaluation
framework designed to rigorously assess the performance of LLMs in code editing
tasks, including debugging, translating, polishing, and requirement switching.
Unlike existing benchmarks focusing solely on code generation, CodeEditorBench
emphasizes real-world scenarios and practical aspects of software development.
We curate diverse coding challenges and scenarios from five sources, covering
various programming languages, complexity levels, and editing tasks. Evaluation
of 19 LLMs reveals that closed-source models (particularly Gemini-Ultra and
GPT-4), outperform open-source models in CodeEditorBench, highlighting
differences in model performance based on problem types and prompt
sensitivities. CodeEditorBench aims to catalyze advancements in LLMs by
providing a robust platform for assessing code editing capabilities. We will
release all prompts and datasets to enable the community to expand the dataset
and benchmark emerging LLMs. By introducing CodeEditorBench, we contribute to
the advancement of LLMs in code editing and provide a valuable resource for
researchers and practitioners.
\\ ( https://arxiv.org/abs/2404.03543 ,  2063kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03549 (*cross-listing*)
Date: Thu, 4 Apr 2024 15:56:23 GMT   (2378kb,D)

Title: Alzheimer's disease detection in PSG signals
Authors: Lorena Gallego-Vi\~nar\'as (1), Juan Miguel Mira-Tom\'as (1), Anna
  Michela-Gaeta (3), Gerard Pinol-Ripoll (4), Ferr\'an Barb\'e (4 and 5), Pablo
  M. Olmos (2 and 6), Arrate Mu\~noz-Barrutia (1 and 2) ((1) Bioengineering
  Department, Universidad Carlos III de Madrid, (2) Instituto de
  Investigaci\'on Sanitaria Gregorio Mara\~n\'on (IiSGM), (3) Department of
  Pulmunology, Hospital Universitario Severo Ochoa, (4) Hospital Universitari
  Santa Maria Lleida, (5) Hospital Universitari Arnau de Vilanova, (6) Signal
  Processing Group (GTS), Universidad Carlos III de Madrid)
Categories: eess.SP cs.AI
Comments: 12 pages, 14 figures. Submitted to IEEE Biomedical and Health
  Informatics for publication
MSC-class: 68T07 (Primary), 68T05, 92B20 (Secondary)
ACM-class: I.2.1
\\
  Alzheimer's disease (AD) and sleep disorders exhibit a close association,
where disruptions in sleep patterns often precede the onset of Mild Cognitive
Impairment (MCI) and early-stage AD. This study delves into the potential of
utilizing sleep-related electroencephalography (EEG) signals acquired through
polysomnography (PSG) for the early detection of AD. Our primary focus is on
exploring semi-supervised Deep Learning techniques for the classification of
EEG signals due to the clinical scenario characterized by the limited data
availability. The methodology entails testing and comparing the performance of
semi-supervised SMATE and TapNet models, benchmarked against the supervised XCM
model, and unsupervised Hidden Markov Models (HMMs). The study highlights the
significance of spatial and temporal analysis capabilities, conducting
independent analyses of each sleep stage. Results demonstrate the effectiveness
of SMATE in leveraging limited labeled data, achieving stable metrics across
all sleep stages, and reaching 90% accuracy in its supervised form. Comparative
analyses reveal SMATE's superior performance over TapNet and HMM, while XCM
excels in supervised scenarios with an accuracy range of 92 - 94%. These
findings underscore the potential of semi-supervised models in early AD
detection, particularly in overcoming the challenges associated with the
scarcity of labeled data. Ablation tests affirm the critical role of
spatio-temporal feature extraction in semi-supervised predictive performance,
and t-SNE visualizations validate the model's proficiency in distinguishing AD
patterns. Overall, this research contributes to the advancement of AD detection
through innovative Deep Learning approaches, highlighting the crucial role of
semi-supervised learning in addressing data limitations.
\\ ( https://arxiv.org/abs/2404.03549 ,  2378kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03574 (*cross-listing*)
Date: Thu, 4 Apr 2024 16:38:49 GMT   (45072kb,D)

Title: TinyVQA: Compact Multimodal Deep Neural Network for Visual Question
  Answering on Resource-Constrained Devices
Authors: Hasib-Al Rashid, Argho Sarkar, Aryya Gangopadhyay, Maryam Rahnemoonfar
  and Tinoosh Mohsenin
Categories: cs.CV cs.AI cs.LG
Comments: Accepted as a full paper by the tinyML Research Symposium 2024
\\
  Traditional machine learning models often require powerful hardware, making
them unsuitable for deployment on resource-limited devices. Tiny Machine
Learning (tinyML) has emerged as a promising approach for running machine
learning models on these devices, but integrating multiple data modalities into
tinyML models still remains a challenge due to increased complexity, latency,
and power consumption. This paper proposes TinyVQA, a novel multimodal deep
neural network for visual question answering tasks that can be deployed on
resource-constrained tinyML hardware. TinyVQA leverages a supervised
attention-based model to learn how to answer questions about images using both
vision and language modalities. Distilled knowledge from the supervised
attention-based VQA model trains the memory aware compact TinyVQA model and low
bit-width quantization technique is employed to further compress the model for
deployment on tinyML devices. The TinyVQA model was evaluated on the FloodNet
dataset, which is used for post-disaster damage assessment. The compact model
achieved an accuracy of 79.5%, demonstrating the effectiveness of TinyVQA for
real-world applications. Additionally, the model was deployed on a Crazyflie
2.0 drone, equipped with an AI deck and GAP8 microprocessor. The TinyVQA model
achieved low latencies of 56 ms and consumes 693 mW power while deployed on the
tiny drone, showcasing its suitability for resource-constrained embedded
systems.
\\ ( https://arxiv.org/abs/2404.03574 ,  45072kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03587 (*cross-listing*)
Date: Thu, 4 Apr 2024 16:52:48 GMT   (4317kb,D)

Title: Anticipate & Collab: Data-driven Task Anticipation and Knowledge-driven
  Planning for Human-robot Collaboration
Authors: Shivam Singh, Karthik Swaminathan, Raghav Arora, Ramandeep Singh,
  Ahana Datta, Dipanjan Das, Snehasis Banerjee, Mohan Sridharan, Madhava
  Krishna
Categories: cs.RO cs.AI
\\
  An agent assisting humans in daily living activities can collaborate more
effectively by anticipating upcoming tasks. Data-driven methods represent the
state of the art in task anticipation, planning, and related problems, but
these methods are resource-hungry and opaque. Our prior work introduced a proof
of concept framework that used an LLM to anticipate 3 high-level tasks that
served as goals for a classical planning system that computed a sequence of
low-level actions for the agent to achieve these goals. This paper describes
DaTAPlan, our framework that significantly extends our prior work toward
human-robot collaboration. Specifically, DaTAPlan planner computes actions for
an agent and a human to collaboratively and jointly achieve the tasks
anticipated by the LLM, and the agent automatically adapts to unexpected
changes in human action outcomes and preferences. We evaluate DaTAPlan
capabilities in a realistic simulation environment, demonstrating accurate task
anticipation, effective human-robot collaboration, and the ability to adapt to
unexpected changes. Project website: https://dataplan-hrc.github.io
\\ ( https://arxiv.org/abs/2404.03587 ,  4317kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03590 (*cross-listing*)
Date: Thu, 4 Apr 2024 16:58:26 GMT   (6118kb,D)

Title: SemGrasp: Semantic Grasp Generation via Language Aligned Discretization
Authors: Kailin Li, Jingbo Wang, Lixin Yang, Cewu Lu, Bo Dai
Categories: cs.CV cs.AI
\\
  Generating natural human grasps necessitates consideration of not just object
geometry but also semantic information. Solely depending on object shape for
grasp generation confines the applications of prior methods in downstream
tasks. This paper presents a novel semantic-based grasp generation method,
termed SemGrasp, which generates a static human grasp pose by incorporating
semantic information into the grasp representation. We introduce a discrete
representation that aligns the grasp space with semantic space, enabling the
generation of grasp postures in accordance with language instructions. A
Multimodal Large Language Model (MLLM) is subsequently fine-tuned, integrating
object, grasp, and language within a unified semantic space. To facilitate the
training of SemGrasp, we have compiled a large-scale, grasp-text-aligned
dataset named CapGrasp, featuring about 260k detailed captions and 50k diverse
grasps. Experimental findings demonstrate that SemGrasp efficiently generates
natural human grasps in alignment with linguistic intentions. Our code, models,
and dataset are available publicly at: https://kailinli.github.io/SemGrasp.
\\ ( https://arxiv.org/abs/2404.03590 ,  6118kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03606 (*cross-listing*)
Date: Thu, 4 Apr 2024 17:25:31 GMT   (223kb,D)

Title: Analyzing Musical Characteristics of National Anthems in Relation to
  Global Indices
Authors: S M Rakib Hasan, Aakar Dhakal, Ms. Ayesha Siddiqua, Mohammad Mominur
  Rahman, Md Maidul Islam, Mohammed Arfat Raihan Chowdhury, S M Masfequier
  Rahman Swapno, SM Nuruzzaman Nobel
Categories: cs.SD cs.AI cs.IR eess.AS
\\
  Music plays a huge part in shaping peoples' psychology and behavioral
patterns. This paper investigates the connection between national anthems and
different global indices with computational music analysis and statistical
correlation analysis. We analyze national anthem musical data to determine
whether certain musical characteristics are associated with peace, happiness,
suicide rate, crime rate, etc. To achieve this, we collect national anthems
from 169 countries and use computational music analysis techniques to extract
pitch, tempo, beat, and other pertinent audio features. We then compare these
musical characteristics with data on different global indices to ascertain
whether a significant correlation exists. Our findings indicate that there may
be a correlation between the musical characteristics of national anthems and
the indices we investigated. The implications of our findings for music
psychology and policymakers interested in promoting social well-being are
discussed. This paper emphasizes the potential of musical data analysis in
social research and offers a novel perspective on the relationship between
music and social indices. The source code and data are made open-access for
reproducibility and future research endeavors. It can be accessed at
http://bit.ly/na_code.
\\ ( https://arxiv.org/abs/2404.03606 ,  223kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03611 (*cross-listing*)
Date: Thu, 4 Apr 2024 17:34:21 GMT   (561kb,D)

Title: InsectMamba: Insect Pest Classification with State Space Model
Authors: Qianning Wang, Chenglin Wang, Zhixin Lai, Yucheng Zhou
Categories: cs.CV cs.AI
Comments: 13 pages, 5 figures
\\
  The classification of insect pests is a critical task in agricultural
technology, vital for ensuring food security and environmental sustainability.
However, the complexity of pest identification, due to factors like high
camouflage and species diversity, poses significant obstacles. Existing methods
struggle with the fine-grained feature extraction needed to distinguish between
closely related pest species. Although recent advancements have utilized
modified network structures and combined deep learning approaches to improve
accuracy, challenges persist due to the similarity between pests and their
surroundings. To address this problem, we introduce InsectMamba, a novel
approach that integrates State Space Models (SSMs), Convolutional Neural
Networks (CNNs), Multi-Head Self-Attention mechanism (MSA), and Multilayer
Perceptrons (MLPs) within Mix-SSM blocks. This integration facilitates the
extraction of comprehensive visual features by leveraging the strengths of each
encoding strategy. A selective module is also proposed to adaptively aggregate
these features, enhancing the model's ability to discern pest characteristics.
InsectMamba was evaluated against strong competitors across five insect pest
classification datasets. The results demonstrate its superior performance and
verify the significance of each model component by an ablation study.
\\ ( https://arxiv.org/abs/2404.03611 ,  561kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03635 (*cross-listing*)
Date: Thu, 4 Apr 2024 17:54:33 GMT   (5336kb,D)

Title: WorDepth: Variational Language Prior for Monocular Depth Estimation
Authors: Ziyao Zeng, Daniel Wang, Fengyu Yang, Hyoungseob Park, Yangchao Wu,
  Stefano Soatto, Byung-Woo Hong, Dong Lao, Alex Wong
Categories: cs.CV cs.AI cs.CL cs.LG cs.MM
\\
  Three-dimensional (3D) reconstruction from a single image is an ill-posed
problem with inherent ambiguities, i.e. scale. Predicting a 3D scene from text
description(s) is similarly ill-posed, i.e. spatial arrangements of objects
described. We investigate the question of whether two inherently ambiguous
modalities can be used in conjunction to produce metric-scaled reconstructions.
To test this, we focus on monocular depth estimation, the problem of predicting
a dense depth map from a single image, but with an additional text caption
describing the scene. To this end, we begin by encoding the text caption as a
mean and standard deviation; using a variational framework, we learn the
distribution of the plausible metric reconstructions of 3D scenes corresponding
to the text captions as a prior. To "select" a specific reconstruction or depth
map, we encode the given image through a conditional sampler that samples from
the latent space of the variational text encoder, which is then decoded to the
output depth map. Our approach is trained alternatingly between the text and
image branches: in one optimization step, we predict the mean and standard
deviation from the text description and sample from a standard Gaussian, and in
the other, we sample using a (image) conditional sampler. Once trained, we
directly predict depth from the encoded text using the conditional sampler. We
demonstrate our approach on indoor (NYUv2) and outdoor (KITTI) scenarios, where
we show that language can consistently improve performance in both.
\\ ( https://arxiv.org/abs/2404.03635 ,  5336kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03647 (*cross-listing*)
Date: Thu, 4 Apr 2024 17:58:38 GMT   (505kb,D)

Title: Capabilities of Large Language Models in Control Engineering: A
  Benchmark Study on GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra
Authors: Darioush Kevian, Usman Syed, Xingang Guo, Aaron Havens, Geir Dullerud,
  Peter Seiler, Lianhui Qin, Bin Hu
Categories: math.OC cs.AI cs.LG
\\
  In this paper, we explore the capabilities of state-of-the-art large language
models (LLMs) such as GPT-4, Claude 3 Opus, and Gemini 1.0 Ultra in solving
undergraduate-level control problems. Controls provides an interesting case
study for LLM reasoning due to its combination of mathematical theory and
engineering design. We introduce ControlBench, a benchmark dataset tailored to
reflect the breadth, depth, and complexity of classical control design. We use
this dataset to study and evaluate the problem-solving abilities of these LLMs
in the context of control engineering. We present evaluations conducted by a
panel of human experts, providing insights into the accuracy, reasoning, and
explanatory prowess of LLMs in control engineering. Our analysis reveals the
strengths and limitations of each LLM in the context of classical control, and
our results imply that Claude 3 Opus has become the state-of-the-art LLM for
solving undergraduate control problems. Our study serves as an initial step
towards the broader goal of employing artificial general intelligence in
control engineering.
\\ ( https://arxiv.org/abs/2404.03647 ,  505kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03653 (*cross-listing*)
Date: Thu, 4 Apr 2024 17:59:46 GMT   (17151kb,D)

Title: CoMat: Aligning Text-to-Image Diffusion Model with Image-to-Text Concept
  Matching
Authors: Dongzhi Jiang, Guanglu Song, Xiaoshi Wu, Renrui Zhang, Dazhong Shen,
  Zhuofan Zong, Yu Liu, Hongsheng Li
Categories: cs.CV cs.AI cs.CL
Comments: Project Page: https://caraj7.github.io/comat
\\
  Diffusion models have demonstrated great success in the field of
text-to-image generation. However, alleviating the misalignment between the
text prompts and images is still challenging. The root reason behind the
misalignment has not been extensively investigated. We observe that the
misalignment is caused by inadequate token attention activation. We further
attribute this phenomenon to the diffusion model's insufficient condition
utilization, which is caused by its training paradigm. To address the issue, we
propose CoMat, an end-to-end diffusion model fine-tuning strategy with an
image-to-text concept matching mechanism. We leverage an image captioning model
to measure image-to-text alignment and guide the diffusion model to revisit
ignored tokens. A novel attribute concentration module is also proposed to
address the attribute binding problem. Without any image or human preference
data, we use only 20K text prompts to fine-tune SDXL to obtain CoMat-SDXL.
Extensive experiments show that CoMat-SDXL significantly outperforms the
baseline model SDXL in two text-to-image alignment benchmarks and achieves
start-of-the-art performance.
\\ ( https://arxiv.org/abs/2404.03653 ,  17151kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03657 (*cross-listing*)
Date: Thu, 4 Apr 2024 17:59:58 GMT   (5352kb,D)

Title: OW-VISCap: Open-World Video Instance Segmentation and Captioning
Authors: Anwesa Choudhuri, Girish Chowdhary, Alexander G. Schwing
Categories: cs.CV cs.AI
Comments: Project page: https://anwesachoudhuri.github.io/OpenWorldVISCap/
\\
  Open-world video instance segmentation is an important video understanding
task. Yet most methods either operate in a closed-world setting, require an
additional user-input, or use classic region-based proposals to identify never
before seen objects. Further, these methods only assign a one-word label to
detected objects, and don't generate rich object-centric descriptions. They
also often suffer from highly overlapping predictions. To address these issues,
we propose Open-World Video Instance Segmentation and Captioning (OW-VISCap),
an approach to jointly segment, track, and caption previously seen or unseen
objects in a video. For this, we introduce open-world object queries to
discover never before seen objects without additional user-input. We generate
rich and descriptive object-centric captions for each detected object via a
masked attention augmented LLM input. We introduce an inter-query contrastive
loss to ensure that the object queries differ from one another. Our generalized
approach matches or surpasses state-of-the-art on three tasks: open-world video
instance segmentation on the BURST dataset, dense video object captioning on
the VidSTG dataset, and closed-world video instance segmentation on the OVIS
dataset.
\\ ( https://arxiv.org/abs/2404.03657 ,  5352kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03048 (*cross-listing*)
Date: Wed, 3 Apr 2024 20:29:40 GMT   (527kb,D)

Title: Decentralised Moderation for Interoperable Social Networks: A
  Conversation-based Approach for Pleroma and the Fediverse
Authors: Vibhor Agarwal, Aravindh Raman, Nishanth Sastry, Ahmed M. Abdelmoniem,
  Gareth Tyson, Ignacio Castro
Categories: cs.CY cs.CL
Comments: Accepted at International AAAI Conference on Web and Social Media
  (ICWSM) 2024. Please cite accordingly!
\\
  The recent development of decentralised and interoperable social networks
(such as the "fediverse") creates new challenges for content moderators. This
is because millions of posts generated on one server can easily "spread" to
another, even if the recipient server has very different moderation policies.
An obvious solution would be to leverage moderation tools to automatically tag
(and filter) posts that contravene moderation policies, e.g. related to toxic
speech. Recent work has exploited the conversational context of a post to
improve this automatic tagging, e.g. using the replies to a post to help
classify if it contains toxic speech. This has shown particular potential in
environments with large training sets that contain complete conversations.
This, however, creates challenges in a decentralised context, as a single
conversation may be fragmented across multiple servers. Thus, each server only
has a partial view of an entire conversation because conversations are often
federated across servers in a non-synchronized fashion. To address this, we
propose a decentralised conversation-aware content moderation approach suitable
for the fediverse. Our approach employs a graph deep learning model (GraphNLI)
trained locally on each server. The model exploits local data to train a model
that combines post and conversational information captured through random walks
to detect toxicity. We evaluate our approach with data from Pleroma, a major
decentralised and interoperable micro-blogging network containing 2 million
conversations. Our model effectively detects toxicity on larger instances,
exclusively trained using their local post information (0.8837 macro-F1). Our
approach has considerable scope to improve moderation in decentralised and
interoperable social networks such as Pleroma or Mastodon.
\\ ( https://arxiv.org/abs/2404.03048 ,  527kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03086 (*cross-listing*)
Date: Wed, 3 Apr 2024 22:01:26 GMT   (47kb,D)

Title: Auditing the Use of Language Models to Guide Hiring Decisions
Authors: Johann D. Gaebler, Sharad Goel, Aziz Huq, Prasanna Tambe
Categories: stat.AP cs.CL
\\
  Regulatory efforts to protect against algorithmic bias have taken on
increased urgency with rapid advances in large language models (LLMs), which
are machine learning models that can achieve performance rivaling human experts
on a wide array of tasks. A key theme of these initiatives is algorithmic
"auditing," but current regulations -- as well as the scientific literature --
provide little guidance on how to conduct these assessments. Here we propose
and investigate one approach for auditing algorithms: correspondence
experiments, a widely applied tool for detecting bias in human judgements. In
the employment context, correspondence experiments aim to measure the extent to
which race and gender impact decisions by experimentally manipulating elements
of submitted application materials that suggest an applicant's demographic
traits, such as their listed name. We apply this method to audit candidate
assessments produced by several state-of-the-art LLMs, using a novel corpus of
applications to K-12 teaching positions in a large public school district. We
find evidence of moderate race and gender disparities, a pattern largely robust
to varying the types of application material input to the models, as well as
the framing of the task to the LLMs. We conclude by discussing some important
limitations of correspondence experiments for auditing algorithms.
\\ ( https://arxiv.org/abs/2404.03086 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03161 (*cross-listing*)
Date: Thu, 4 Apr 2024 02:22:37 GMT   (9422kb,D)

Title: BioVL-QR: Egocentric Biochemical Video-and-Language Dataset Using Micro
  QR Codes
Authors: Taichi Nishimura and Koki Yamamoto and Yuto Haneji and Keiya Kajimura
  and Chihiro Nishiwaki and Eriko Daikoku and Natsuko Okuda and Fumihito Ono
  and Hirotaka Kameko and Shinsuke Mori
Categories: cs.CV cs.CL cs.MM
Comments: 6 pages
\\
  This paper introduces a biochemical vision-and-language dataset, which
consists of 24 egocentric experiment videos, corresponding protocols, and
video-and-language alignments. The key challenge in the wet-lab domain is
detecting equipment, reagents, and containers is difficult because the lab
environment is scattered by filling objects on the table and some objects are
indistinguishable. Therefore, previous studies assume that objects are manually
annotated and given for downstream tasks, but this is costly and
time-consuming. To address this issue, this study focuses on Micro QR Codes to
detect objects automatically. From our preliminary study, we found that
detecting objects only using Micro QR Codes is still difficult because the
researchers manipulate objects, causing blur and occlusion frequently. To
address this, we also propose a novel object labeling method by combining a
Micro QR Code detector and an off-the-shelf hand object detector. As one of the
applications of our dataset, we conduct the task of generating protocols from
experiment videos and find that our approach can generate accurate protocols.
\\ ( https://arxiv.org/abs/2404.03161 ,  9422kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03192 (*cross-listing*)
Date: Thu, 4 Apr 2024 04:23:19 GMT   (8149kb,D)

Title: Do Large Language Models Rank Fairly? An Empirical Study on the Fairness
  of LLMs as Rankers
Authors: Yuan Wang, Xuyang Wu, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang
Categories: cs.IR cs.CL
Comments: Accepted at NAACL 2024 Main Conference
\\
  The integration of Large Language Models (LLMs) in information retrieval has
raised a critical reevaluation of fairness in the text-ranking models. LLMs,
such as GPT models and Llama2, have shown effectiveness in natural language
understanding tasks, and prior works (e.g., RankGPT) have also demonstrated
that the LLMs exhibit better performance than the traditional ranking models in
the ranking task. However, their fairness remains largely unexplored. This
paper presents an empirical study evaluating these LLMs using the TREC Fair
Ranking dataset, focusing on the representation of binary protected attributes
such as gender and geographic location, which are historically underrepresented
in search outcomes. Our analysis delves into how these LLMs handle queries and
documents related to these attributes, aiming to uncover biases in their
ranking algorithms. We assess fairness from both user and content perspectives,
contributing an empirical benchmark for evaluating LLMs as the fair ranker.
\\ ( https://arxiv.org/abs/2404.03192 ,  8149kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02988 (*cross-listing*)
Date: Wed, 3 Apr 2024 18:16:47 GMT   (2912kb,D)

Title: Risk-averse Learning with Non-Stationary Distributions
Authors: Siyi Wang, Zifan Wang, Xinlei Yi, Michael M. Zavlanos, Karl H.
  Johansson, Sandra Hirche
Categories: eess.SY cs.LG cs.SY
\\
  Considering non-stationary environments in online optimization enables
decision-maker to effectively adapt to changes and improve its performance over
time. In such cases, it is favorable to adopt a strategy that minimizes the
negative impact of change to avoid potentially risky situations. In this paper,
we investigate risk-averse online optimization where the distribution of the
random cost changes over time. We minimize risk-averse objective function using
the Conditional Value at Risk (CVaR) as risk measure. Due to the difficulty in
obtaining the exact CVaR gradient, we employ a zeroth-order optimization
approach that queries the cost function values multiple times at each iteration
and estimates the CVaR gradient using the sampled values. To facilitate the
regret analysis, we use a variation metric based on Wasserstein distance to
capture time-varying distributions. Given that the distribution variation is
sub-linear in the total number of episodes, we show that our designed learning
algorithm achieves sub-linear dynamic regret with high probability for both
convex and strongly convex functions. Moreover, theoretical results suggest
that increasing the number of samples leads to a reduction in the dynamic
regret bounds until the sampling number reaches a specific limit. Finally, we
provide numerical experiments of dynamic pricing in a parking lot to illustrate
the efficacy of the designed algorithm.
\\ ( https://arxiv.org/abs/2404.02988 ,  2912kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03010 (*cross-listing*)
Date: Wed, 3 Apr 2024 18:42:19 GMT   (33043kb,D)

Title: Skeleton Recall Loss for Connectivity Conserving and Resource Efficient
  Segmentation of Thin Tubular Structures
Authors: Yannick Kirchhoff, Maximilian R. Rokuss, Saikat Roy, Balint Kovacs,
  Constantin Ulrich, Tassilo Wald, Maximilian Zenk, Philipp Vollmuth, Jens
  Kleesiek, Fabian Isensee, Klaus Maier-Hein
Categories: eess.IV cs.CV cs.LG
\\
  Accurately segmenting thin tubular structures, such as vessels, nerves, roads
or concrete cracks, is a crucial task in computer vision. Standard deep
learning-based segmentation loss functions, such as Dice or Cross-Entropy,
focus on volumetric overlap, often at the expense of preserving structural
connectivity or topology. This can lead to segmentation errors that adversely
affect downstream tasks, including flow calculation, navigation, and structural
inspection. Although current topology-focused losses mark an improvement, they
introduce significant computational and memory overheads. This is particularly
relevant for 3D data, rendering these losses infeasible for larger volumes as
well as increasingly important multi-class segmentation problems. To mitigate
this, we propose a novel Skeleton Recall Loss, which effectively addresses
these challenges by circumventing intensive GPU-based calculations with
inexpensive CPU operations. It demonstrates overall superior performance to
current state-of-the-art approaches on five public datasets for
topology-preserving segmentation, while substantially reducing computational
overheads by more than 90%. In doing so, we introduce the first multi-class
capable loss function for thin structure segmentation, excelling in both
efficiency and efficacy for topology-preservation.
\\ ( https://arxiv.org/abs/2404.03010 ,  33043kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03019 (*cross-listing*)
Date: Wed, 3 Apr 2024 19:03:15 GMT   (403kb,D)

Title: GeoT: Tensor Centric Library for Graph Neural Network via Efficient
  Segment Reduction on GPU
Authors: Zhongming Yu, Genghan Zhang, Hanxian Huang, Xin Chen, Jishen Zhao
Categories: cs.DC cs.LG
\\
  In recent years, Graph Neural Networks (GNNs) have ignited a surge of
innovation, significantly enhancing the processing of geometric data structures
such as graphs, point clouds, and meshes. As the domain continues to evolve, a
series of frameworks and libraries are being developed to push GNN efficiency
to new heights. While graph-centric libraries have achieved success in the
past, the advent of efficient tensor compilers has highlighted the urgent need
for tensor-centric libraries. Yet, efficient tensor-centric frameworks for GNNs
remain scarce due to unique challenges and limitations encountered when
implementing segment reduction in GNN contexts.
  We introduce GeoT, a cutting-edge tensor-centric library designed
specifically for GNNs via efficient segment reduction. GeoT debuts innovative
parallel algorithms that not only introduce new design principles but also
expand the available design space. Importantly, GeoT is engineered for
straightforward fusion within a computation graph, ensuring compatibility with
contemporary tensor-centric machine learning frameworks and compilers. Setting
a new performance benchmark, GeoT marks a considerable advancement by
showcasing an average operator speedup of 1.80x and an end-to-end speedup of
1.68x.
\\ ( https://arxiv.org/abs/2404.03019 ,  403kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03101 (*cross-listing*)
Date: Wed, 3 Apr 2024 22:51:54 GMT   (3578kb,D)

Title: MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large
  Neighborhoods Search
Authors: Weizhe Chen, Sven Koenig, Bistra Dilkina
Categories: cs.MA cs.LG
\\
  Cooperative multi-agent reinforcement learning (MARL) has been an
increasingly important research topic in the last half-decade because of its
great potential for real-world applications. Because of the curse of
dimensionality, the popular "centralized training decentralized execution"
framework requires a long time in training, yet still cannot converge
efficiently. In this paper, we propose a general training framework, MARL-LNS,
to algorithmically address these issues by training on alternating subsets of
agents using existing deep MARL algorithms as low-level trainers, while not
involving any additional parameters to be trained. Based on this framework, we
provide three algorithm variants based on the framework: random large
neighborhood search (RLNS), batch large neighborhood search (BLNS), and
adaptive large neighborhood search (ALNS), which alternate the subsets of
agents differently. We test our algorithms on both the StarCraft Multi-Agent
Challenge and Google Research Football, showing that our algorithms can
automatically reduce at least 10% of training time while reaching the same
final skill level as the original algorithm.
\\ ( https://arxiv.org/abs/2404.03101 ,  3578kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03171 (*cross-listing*)
Date: Thu, 4 Apr 2024 03:03:38 GMT   (13174kb,D)

Title: Multi-modal Learning for WebAssembly Reverse Engineering
Authors: Hanxian Huang and Jishen Zhao
Categories: cs.SE cs.LG cs.PL
Comments: Accepted by ISSTA '24
DOI: 10.1145/3650212.3652141
\\
  The increasing adoption of WebAssembly (Wasm) for performance-critical and
security-sensitive tasks drives the demand for WebAssembly program
comprehension and reverse engineering. Recent studies have introduced machine
learning (ML)-based WebAssembly reverse engineering tools. Yet, the
generalization of task-specific ML solutions remains challenging, because their
effectiveness hinges on the availability of an ample supply of high-quality
task-specific labeled data. Moreover, previous works overlook the high-level
semantics present in source code and its documentation. Acknowledging the
abundance of available source code with documentation, which can be compiled
into WebAssembly, we propose to learn representations of them concurrently and
harness their mutual relationships for effective WebAssembly reverse
engineering.
  In this paper, we present WasmRev, the first multi-modal pre-trained language
model for WebAssembly reverse engineering. WasmRev is pre-trained using
self-supervised learning on a large-scale multi-modal corpus encompassing
source code, code documentation and the compiled WebAssembly, without requiring
labeled data. WasmRev incorporates three tailored multi-modal pre-training
tasks to capture various characteristics of WebAssembly and cross-modal
relationships. WasmRev is only trained once to produce general-purpose
representations that can broadly support WebAssembly reverse engineering tasks
through few-shot fine-tuning with much less labeled data, improving data
efficiency. We fine-tune WasmRev onto three important reverse engineering
tasks: type recovery, function purpose identification and WebAssembly
summarization. Our results show that WasmRev pre-trained on the corpus of
multi-modal samples establishes a robust foundation for these tasks, achieving
high task accuracy and outperforming the state-of-the-art ML methods for
WebAssembly reverse engineering.
\\ ( https://arxiv.org/abs/2404.03171 ,  13174kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03188 (*cross-listing*)
Date: Thu, 4 Apr 2024 04:16:31 GMT   (527kb)

Title: Classification of Nasopharyngeal Cases using DenseNet Deep Learning
  Architecture
Authors: W. S. H. M. W. Ahmad, M. F. A. Fauzi, M. K. Abdullahi, Jenny T. H.
  Lee, N. S. A. Basry, A Yahaya, A. M. Ismail, A. Adam, Elaine W. L. Chan, F.
  S. Abas
Categories: eess.IV cs.CV cs.LG
Comments: This article has been accepted in the Journal of Engineering Science
  and Technology (JESTEC) and awaiting publication
\\
  Nasopharyngeal carcinoma (NPC) is one of the understudied yet deadliest
cancers in South East Asia. In Malaysia, the prevalence is identified mainly in
Sarawak, among the ethnic of Bidayuh. NPC is often late-diagnosed because it is
asymptomatic at the early stage. There are several tissue representations from
the nasopharynx biopsy, such as nasopharyngeal inflammation (NPI), lymphoid
hyperplasia (LHP), nasopharyngeal carcinoma (NPC) and normal tissue. This paper
is our first initiative to identify the difference between NPC, NPI and normal
cases. Seven whole slide images (WSIs) with gigapixel resolutions from seven
different patients and two hospitals were experimented with using two test
setups, consisting of a different set of images. The tissue regions are patched
into smaller blocks and classified using DenseNet architecture with 21 dense
layers. Two tests are carried out, each for proof of concept (Test 1) and
real-test scenario (Test 2). The accuracy achieved for NPC class is 94.8% for
Test 1 and 67.0% for Test 2.
\\ ( https://arxiv.org/abs/2404.03188 ,  527kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03225 (*cross-listing*)
Date: Thu, 4 Apr 2024 06:20:22 GMT   (147kb,D)

Title: FACTUAL: A Novel Framework for Contrastive Learning Based Robust SAR
  Image Classification
Authors: Xu Wang, Tian Ye, Rajgopal Kannan, Viktor Prasanna
Categories: cs.CV cs.LG
Comments: 2024 IEEE Radar Conference
\\
  Deep Learning (DL) Models for Synthetic Aperture Radar (SAR) Automatic Target
Recognition (ATR), while delivering improved performance, have been shown to be
quite vulnerable to adversarial attacks. Existing works improve robustness by
training models on adversarial samples. However, by focusing mostly on attacks
that manipulate images randomly, they neglect the real-world feasibility of
such attacks. In this paper, we propose FACTUAL, a novel Contrastive Learning
framework for Adversarial Training and robust SAR classification. FACTUAL
consists of two components: (1) Differing from existing works, a novel
perturbation scheme that incorporates realistic physical adversarial attacks
(such as OTSA) to build a supervised adversarial pre-training network. This
network utilizes class labels for clustering clean and perturbed images
together into a more informative feature space. (2) A linear classifier
cascaded after the encoder to use the computed representations to predict the
target labels. By pre-training and fine-tuning our model on both clean and
adversarial samples, we show that our model achieves high prediction accuracy
on both cases. Our model achieves 99.7% accuracy on clean samples, and 89.6% on
perturbed samples, both outperforming previous state-of-the-art methods.
\\ ( https://arxiv.org/abs/2404.03225 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03227 (*cross-listing*)
Date: Thu, 4 Apr 2024 06:24:11 GMT   (1483kb,D)

Title: Decentralized Learning Strategies for Estimation Error Minimization with
  Graph Neural Networks
Authors: Xingran Chen, Navid NaderiAlizadeh, Alejandro Ribeiro, Shirin Saeedi
  Bidokhti
Categories: eess.SP cs.LG
\\
  We address the challenge of sampling and remote estimation for autoregressive
Markovian processes in a multi-hop wireless network with
statistically-identical agents. Agents cache the most recent samples from
others and communicate over wireless collision channels governed by an
underlying graph topology. Our goal is to minimize time-average estimation
error and/or age of information with decentralized scalable sampling and
transmission policies, considering both oblivious (where decision-making is
independent of the physical processes) and non-oblivious policies (where
decision-making depends on physical processes). We prove that in oblivious
policies, minimizing estimation error is equivalent to minimizing the age of
information. The complexity of the problem, especially the multi-dimensional
action spaces and arbitrary network topologies, makes theoretical methods for
finding optimal transmission policies intractable. We optimize the policies
using a graphical multi-agent reinforcement learning framework, where each
agent employs a permutation-equivariant graph neural network architecture.
Theoretically, we prove that our proposed framework exhibits desirable
transferability properties, allowing transmission policies trained on small- or
moderate-size networks to be executed effectively on large-scale topologies.
Numerical experiments demonstrate that (i) Our proposed framework outperforms
state-of-the-art baselines; (ii) The trained policies are transferable to
larger networks, and their performance gains increase with the number of
agents; (iii) The training procedure withstands non-stationarity even if we
utilize independent learning techniques; and, (iv) Recurrence is pivotal in
both independent learning and centralized training and decentralized execution,
and improves the resilience to non-stationarity in independent learning.
\\ ( https://arxiv.org/abs/2404.03227 ,  1483kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03250 (*cross-listing*)
Date: Thu, 4 Apr 2024 07:09:43 GMT   (707kb)

Title: Multi-task learning via robust regularized clustering with non-convex
  group penalties
Authors: Akira Okazaki, Shuichi Kawano
Categories: stat.ME cs.LG stat.ML
Comments: 32 pages
\\
  Multi-task learning (MTL) aims to improve estimation and prediction
performance by sharing common information among related tasks. One natural
assumption in MTL is that tasks are classified into clusters based on their
characteristics. However, existing MTL methods based on this assumption often
ignore outlier tasks that have large task-specific components or no relation to
other tasks. To address this issue, we propose a novel MTL method called
Multi-Task Learning via Robust Regularized Clustering (MTLRRC). MTLRRC
incorporates robust regularization terms inspired by robust convex clustering,
which is further extended to handle non-convex and group-sparse penalties. The
extension allows MTLRRC to simultaneously perform robust task clustering and
outlier task detection. The connection between the extended robust clustering
and the multivariate M-estimator is also established. This provides an
interpretation of the robustness of MTLRRC against outlier tasks. An efficient
algorithm based on a modified alternating direction method of multipliers is
developed for the estimation of the parameters. The effectiveness of MTLRRC is
demonstrated through simulation studies and application to real data.
\\ ( https://arxiv.org/abs/2404.03250 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03310 (*cross-listing*)
Date: Thu, 4 Apr 2024 09:12:13 GMT   (4567kb)

Title: Site-specific Deterministic Temperature and Humidity Forecasts with
  Explainable and Reliable Machine Learning
Authors: MengMeng Han, Tennessee Leeuwenburg, Brad Murphy
Categories: physics.ao-ph cs.LG
Comments: 27 Pages, 16 Figures, 11 Tables
\\
  Site-specific weather forecasts are essential to accurate prediction of power
demand and are consequently of great interest to energy operators. However,
weather forecasts from current numerical weather prediction (NWP) models lack
the fine-scale detail to capture all important characteristics of localised
real-world sites. Instead they provide weather information representing a
rectangular gridbox (usually kilometres in size). Even after post-processing
and bias correction, area-averaged information is usually not optimal for
specific sites. Prior work on site optimised forecasts has focused on linear
methods, weighted consensus averaging, time-series methods, and others. Recent
developments in machine learning (ML) have prompted increasing interest in
applying ML as a novel approach towards this problem. In this study, we
investigate the feasibility of optimising forecasts at sites by adopting the
popular machine learning model gradient boosting decision tree, supported by
the Python version of the XGBoost package. Regression trees have been trained
with historical NWP and site observations as training data, aimed at predicting
temperature and dew point at multiple site locations across Australia. We
developed a working ML framework, named 'Multi-SiteBoost' and initial testing
results show a significant improvement compared with gridded values from
bias-corrected NWP models. The improvement from XGBoost is found to be
comparable with non-ML methods reported in literature. With the insights
provided by SHapley Additive exPlanations (SHAP), this study also tests various
approaches to understand the ML predictions and increase the reliability of the
forecasts generated by ML.
\\ ( https://arxiv.org/abs/2404.03310 ,  4567kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03331 (*cross-listing*)
Date: Thu, 4 Apr 2024 09:57:29 GMT   (898kb,D)

Title: LancBiO: dynamic Lanczos-aided bilevel optimization via Krylov subspace
Authors: Bin Gao, Yan Yang, Ya-xiang Yuan
Categories: math.OC cs.LG stat.ML
Comments: 35 pages, 11 figures, 1 table
\\
  Bilevel optimization, with broad applications in machine learning, has an
intricate hierarchical structure. Gradient-based methods have emerged as a
common approach to large-scale bilevel problems. However, the computation of
the hyper-gradient, which involves a Hessian inverse vector product, confines
the efficiency and is regarded as a bottleneck. To circumvent the inverse, we
construct a sequence of low-dimensional approximate Krylov subspaces with the
aid of the Lanczos process. As a result, the constructed subspace is able to
dynamically and incrementally approximate the Hessian inverse vector product
with less effort and thus leads to a favorable estimate of the hyper-gradient.
Moreover, we propose a~provable subspace-based framework for bilevel problems
where one central step is to solve a small-size tridiagonal linear system. To
the best of our knowledge, this is the first time that subspace techniques are
incorporated into bilevel optimization. This successful trial not only enjoys
$\mathcal{O}(\epsilon^{-1})$ convergence rate but also demonstrates efficiency
in a synthetic problem and two deep learning tasks.
\\ ( https://arxiv.org/abs/2404.03331 ,  898kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03340 (*cross-listing*)
Date: Thu, 4 Apr 2024 10:10:38 GMT   (15514kb,D)

Title: Meta Invariance Defense Towards Generalizable Robustness to Unknown
  Adversarial Attacks
Authors: Lei Zhang, Yuhang Zhou, Yi Yang, Xinbo Gao
Categories: cs.CV cs.CR cs.LG
Comments: Accepted by IEEE TPAMI in 2024
\\
  Despite providing high-performance solutions for computer vision tasks, the
deep neural network (DNN) model has been proved to be extremely vulnerable to
adversarial attacks. Current defense mainly focuses on the known attacks, but
the adversarial robustness to the unknown attacks is seriously overlooked.
Besides, commonly used adaptive learning and fine-tuning technique is
unsuitable for adversarial defense since it is essentially a zero-shot problem
when deployed. Thus, to tackle this challenge, we propose an attack-agnostic
defense method named Meta Invariance Defense (MID). Specifically, various
combinations of adversarial attacks are randomly sampled from a manually
constructed Attacker Pool to constitute different defense tasks against unknown
attacks, in which a student encoder is supervised by multi-consistency
distillation to learn the attack-invariant features via a meta principle. The
proposed MID has two merits: 1) Full distillation from pixel-, feature- and
prediction-level between benign and adversarial samples facilitates the
discovery of attack-invariance. 2) The model simultaneously achieves robustness
to the imperceptible adversarial perturbations in high-level image
classification and attack-suppression in low-level robust image regeneration.
Theoretical and empirical studies on numerous benchmarks such as ImageNet
verify the generalizable robustness and superiority of MID under various
attacks.
\\ ( https://arxiv.org/abs/2404.03340 ,  15514kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03372 (*cross-listing*)
Date: Thu, 4 Apr 2024 11:16:16 GMT   (66kb,D)

Title: Elementary Analysis of Policy Gradient Methods
Authors: Jiacai Liu, Wenye Li, Ke Wei
Categories: math.OC cs.LG
\\
  Projected policy gradient under the simplex parameterization, policy gradient
and natural policy gradient under the softmax parameterization, are fundamental
algorithms in reinforcement learning. There have been a flurry of recent
activities in studying these algorithms from the theoretical aspect. Despite
this, their convergence behavior is still not fully understood, even given the
access to exact policy evaluations. In this paper, we focus on the discounted
MDP setting and conduct a systematic study of the aforementioned policy
optimization methods. Several novel results are presented, including 1) global
linear convergence of projected policy gradient for any constant step size, 2)
sublinear convergence of softmax policy gradient for any constant step size, 3)
global linear convergence of softmax natural policy gradient for any constant
step size, 4) global linear convergence of entropy regularized softmax policy
gradient for a wider range of constant step sizes than existing result, 5)
tight local linear convergence rate of entropy regularized natural policy
gradient, and 6) a new and concise local quadratic convergence rate of soft
policy iteration without the assumption on the stationary distribution under
the optimal policy. New and elementary analysis techniques have been developed
to establish these results.
\\ ( https://arxiv.org/abs/2404.03372 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03446 (*cross-listing*)
Date: Thu, 4 Apr 2024 13:46:52 GMT   (6817kb,D)

Title: SP$^2$OT: Semantic-Regularized Progressive Partial Optimal Transport for
  Imbalanced Clustering
Authors: Chuyu Zhang, Hui Ren, Xuming He
Categories: cs.CV cs.LG
Comments: under review. arXiv admin note: substantial text overlap with
  arXiv:2401.09266
\\
  Deep clustering, which learns representation and semantic clustering without
labels information, poses a great challenge for deep learning-based approaches.
Despite significant progress in recent years, most existing methods focus on
uniformly distributed datasets, significantly limiting the practical
applicability of their methods. In this paper, we propose a more practical
problem setting named deep imbalanced clustering, where the underlying classes
exhibit an imbalance distribution. To address this challenge, we introduce a
novel optimal transport-based pseudo-label learning framework. Our framework
formulates pseudo-label generation as a Semantic-regularized Progressive
Partial Optimal Transport (SP$^2$OT) problem, which progressively transports
each sample to imbalanced clusters under several prior distribution and
semantic relation constraints, thus generating high-quality and imbalance-aware
pseudo-labels. To solve SP$^2$OT, we develop a Majorization-Minimization-based
optimization algorithm. To be more precise, we employ the strategy of
majorization to reformulate the SP$^2$OT problem into a Progressive Partial
Optimal Transport problem, which can be transformed into an unbalanced optimal
transport problem with augmented constraints and can be solved efficiently by a
fast matrix scaling algorithm. Experiments on various datasets, including a
human-curated long-tailed CIFAR100, challenging ImageNet-R, and large-scale
subsets of fine-grained iNaturalist2018 datasets, demonstrate the superiority
of our method.
\\ ( https://arxiv.org/abs/2404.03446 ,  6817kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03453 (*cross-listing*)
Date: Thu, 4 Apr 2024 13:57:44 GMT   (117kb)

Title: Conditioning of Banach Space Valued Gaussian Random Variables: An
  Approximation Approach Based on Martingales
Authors: Ingo Steinwart
Categories: math.PR cs.LG math.ST stat.TH
Comments: 50 pages plus 22 pages of supplemental material
\\
  In this paper we investigate the conditional distributions of two Banach
space valued, jointly Gaussian random variables. These conditional
distributions are again Gaussian and their means and covariances are determined
by a general approximation scheme based upon a martingale idea. We then apply
our general results to the case of Gaussian processes with continuous paths
conditioned to partial observations of their paths.
\\ ( https://arxiv.org/abs/2404.03453 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03506 (*cross-listing*)
Date: Thu, 4 Apr 2024 15:10:13 GMT   (293kb,D)

Title: CountARFactuals - Generating plausible model-agnostic counterfactual
  explanations with adversarial random forests
Authors: Susanne Dandl, Kristin Blesch, Timo Freiesleben, Gunnar K\"onig, Jan
  Kapar, Bernd Bischl, Marvin Wright
Categories: stat.ML cs.LG
Comments: SD, KB, TB, and GK contributed equally as first authors
\\
  Counterfactual explanations elucidate algorithmic decisions by pointing to
scenarios that would have led to an alternative, desired outcome. Giving
insight into the model's behavior, they hint users towards possible actions and
give grounds for contesting decisions. As a crucial factor in achieving these
goals, counterfactuals must be plausible, i.e., describing realistic
alternative scenarios within the data manifold. This paper leverages a recently
developed generative modeling technique -- adversarial random forests (ARFs) --
to efficiently generate plausible counterfactuals in a model-agnostic way. ARFs
can serve as a plausibility measure or directly generate counterfactual
explanations. Our ARF-based approach surpasses the limitations of existing
methods that aim to generate plausible counterfactual explanations: It is easy
to train and computationally highly efficient, handles continuous and
categorical data naturally, and allows integrating additional desiderata such
as sparsity in a straightforward manner.
\\ ( https://arxiv.org/abs/2404.03506 ,  293kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2303.05038
replaced with revised version Thu, 4 Apr 2024 05:37:52 GMT   (7479kb,D)

Title: Exploiting Contextual Structure to Generate Useful Auxiliary Tasks
Authors: Benedict Quartey, Ankit Shah, George Konidaris
Categories: cs.AI cs.LG cs.RO
\\ ( https://arxiv.org/abs/2303.05038 ,  7479kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04797
replaced with revised version Thu, 4 Apr 2024 12:59:14 GMT   (35145kb,D)

Title: Set-Type Belief Propagation with Applications to Poisson Multi-Bernoulli
  SLAM
Authors: Hyowon Kim, Angel F. Garc\'ia-Fern\'andez, Yu Ge, Yuxuan Xia, Lennart
  Svensson, Henk Wymeersch
Categories: cs.AI eess.SP
Comments: 17 pages, 7 figures
\\ ( https://arxiv.org/abs/2305.04797 ,  35145kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11432
replaced with revised version Thu, 4 Apr 2024 01:32:04 GMT   (4914kb,D)

Title: A Survey on Large Language Model based Autonomous Agents
Authors: Lei Wang and Chen Ma and Xueyang Feng and Zeyu Zhang and Hao Yang and
  Jingsen Zhang and Zhiyuan Chen and Jiakai Tang and Xu Chen and Yankai Lin and
  Wayne Xin Zhao and Zhewei Wei and Ji-Rong Wen
Categories: cs.AI cs.CL
Comments: 35 pages, 5 figures, 3 tables, has been accepted by frontiers of
  computer science (FCS), doi={10.1007/s11704-024-40231-1}
DOI: 10.1007/s11704-024-40231-1
\\ ( https://arxiv.org/abs/2308.11432 ,  4914kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09825
replaced with revised version Wed, 3 Apr 2024 19:47:54 GMT   (5481kb,D)

Title: Bias of AI-Generated Content: An Examination of News Produced by Large
  Language Models
Authors: Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao,
  Xiaohang Zhao
Categories: cs.AI
\\ ( https://arxiv.org/abs/2309.09825 ,  5481kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16146
replaced with revised version Thu, 4 Apr 2024 05:59:22 GMT   (843kb,D)

Title: T-COL: Generating Counterfactual Explanations for General User
  Preferences on Variable Machine Learning Systems
Authors: Ming Wang, Daling Wang, Wenfang Wu, Shi Feng, Yifei Zhang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2309.16146 ,  843kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11753
replaced with revised version Thu, 4 Apr 2024 08:06:03 GMT   (20kb)

Title: Recording and Describing Poker Hands
Authors: Juho Kim
Categories: cs.AI
Comments: 8 pages, submission to 2024 IEEE Conference on Games
\\ ( https://arxiv.org/abs/2312.11753 ,  20kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13433
replaced with revised version Thu, 4 Apr 2024 07:40:31 GMT   (3534kb,D)

Title: AgentGroupChat: An Interactive Group Chat Simulacra For Better Eliciting
  Emergent Behavior
Authors: Zhouhong Gu, Xiaoxuan Zhu, Haoran Guo, Lin Zhang, Yin Cai, Hao Shen,
  Jiangjie Chen, Zheyu Ye, Yifei Dai, Yan Gao, Yao Hu, Hongwei Feng, Yanghua
  Xiao
Categories: cs.AI cs.CL cs.CY
\\ ( https://arxiv.org/abs/2403.13433 ,  3534kb)
------------------------------------------------------------------------------
\\
arXiv:2007.06257
replaced with revised version Thu, 4 Apr 2024 07:17:11 GMT   (258kb,D)

Title: Rewiring the Transformer with Depth-Wise LSTMs
Authors: Hongfei Xu and Yang Song and Qiuhui Liu and Josef van Genabith and
  Deyi Xiong
Categories: cs.CL
\\ ( https://arxiv.org/abs/2007.06257 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2301.05487
replaced with revised version Thu, 4 Apr 2024 07:43:16 GMT   (521kb,D)

Title: FUN with Fisher: Improving Generalization of Adapter-Based Cross-lingual
  Transfer with Scheduled Unfreezing
Authors: Chen Cecilia Liu, Jonas Pfeiffer, Ivan Vuli\'c, Iryna Gurevych
Categories: cs.CL
\\ ( https://arxiv.org/abs/2301.05487 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2302.12239
replaced with revised version Thu, 4 Apr 2024 08:26:54 GMT   (18365kb,D)

Title: What Makes a Language Easy to Deep-Learn?
Authors: Lukas Galke, Yoav Ram, Limor Raviv
Categories: cs.CL
Comments: 16 pages + supplementary material
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2302.12239 ,  18365kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14364
replaced with revised version Wed, 3 Apr 2024 23:57:12 GMT   (704kb,D)

Title: CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to
  Guardrail Models for Virtual Assistants
Authors: Albert Yu Sun, Varun Nair, Elliot Schumacher, Anitha Kannan
Categories: cs.CL cs.AI cs.LG
Comments: To appear in NAACL 2024
\\ ( https://arxiv.org/abs/2304.14364 ,  704kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19358
replaced with revised version Thu, 4 Apr 2024 03:04:12 GMT   (2077kb,D)

Title: Stable Anisotropic Regularization
Authors: William Rudman and Carsten Eickhoff
Categories: cs.CL cs.AI cs.LG
Comments: Camera-ready version for ICLR 2024
\\ ( https://arxiv.org/abs/2305.19358 ,  2077kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08833
replaced with revised version Thu, 4 Apr 2024 15:16:57 GMT   (1920kb,D)

Title: CMB: A Comprehensive Medical Benchmark in Chinese
Authors: Xidong Wang, Guiming Hardy Chen, Dingjie Song, Zhiyi Zhang, Zhihong
  Chen, Qingying Xiao, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, Haizhou
  Li
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2308.08833 ,  1920kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11696
replaced with revised version Thu, 4 Apr 2024 16:23:56 GMT   (11305kb,D)

Title: LLM-based Medical Assistant Personalization with Short- and Long-Term
  Memory Coordination
Authors: Kai Zhang, Yangyang Kang, Fubang Zhao, Xiaozhong Liu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.11696 ,  11305kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00492
replaced with revised version Thu, 4 Apr 2024 16:30:31 GMT   (9338kb,D)

Title: From Language Modeling to Instruction Following: Understanding the
  Behavior Shift in LLMs after Instruction Tuning
Authors: Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang,
  Ninghao Liu, Dong Yu
Categories: cs.CL cs.AI cs.LG
Comments: Accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2310.00492 ,  9338kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06202
replaced with revised version Wed, 3 Apr 2024 18:59:10 GMT   (2593kb,D)

Title: GPT-who: An Information Density-based Machine-Generated Text Detector
Authors: Saranya Venkatraman, Adaku Uchendu, Dongwon Lee
Categories: cs.CL
Comments: To appear in Findings of the Association for Computational
  Linguistics: NAACL 2024
\\ ( https://arxiv.org/abs/2310.06202 ,  2593kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14558
replaced with revised version Wed, 3 Apr 2024 21:36:08 GMT   (4928kb,D)

Title: AlpaCare:Instruction-tuned Large Language Models for Medical Application
Authors: Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, Linda
  Ruth Petzold
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.14558 ,  4928kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03301
replaced with revised version Thu, 4 Apr 2024 17:41:12 GMT   (3907kb,D)

Title: Ziya2: Data-centric Learning is All LLMs Need
Authors: Ruyi Gan, Ziwei Wu, Renliang Sun, Junyu Lu, Xiaojun Wu, Dixiang Zhang,
  Kunhao Pan, Junqing He, Yuanhe Tian, Ping Yang, Qi Yang, Hao Wang, Jiaxing
  Zhang, Yan Song
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.03301 ,  3907kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08252
replaced with revised version Thu, 4 Apr 2024 11:37:01 GMT   (456kb,D)

Title: REST: Retrieval-Based Speculative Decoding
Authors: Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: NAACL 2024, camera ready
\\ ( https://arxiv.org/abs/2311.08252 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09206
replaced with revised version Thu, 4 Apr 2024 17:10:25 GMT   (21178kb,D)

Title: TableLlama: Towards Open Large Generalist Models for Tables
Authors: Tianshu Zhang, Xiang Yue, Yifei Li, Huan Sun
Categories: cs.CL
Comments: NAACL 2024 long paper
\\ ( https://arxiv.org/abs/2311.09206 ,  21178kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09390
replaced with revised version Thu, 4 Apr 2024 11:26:17 GMT   (819kb,D)

Title: LEEETs-Dial: Linguistic Entrainment in End-to-End Task-oriented Dialogue
  systems
Authors: Nalin Kumar and Ond\v{r}ej Du\v{s}ek
Categories: cs.CL
Comments: Accepted to NAACL Findings 2024
\\ ( https://arxiv.org/abs/2311.09390 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09579
replaced with revised version Thu, 4 Apr 2024 01:00:16 GMT   (1681kb,D)

Title: Crafting In-context Examples according to LMs' Parametric Knowledge
Authors: Yoonsang Lee, Pranav Atreya, Xi Ye, Eunsol Choi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09579 ,  1681kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09741
replaced with revised version Thu, 4 Apr 2024 09:10:34 GMT   (183kb,D)

Title: P^3SUM: Preserving Author's Perspective in News Summarization with
  Diffusion Language Models
Authors: Yuhan Liu, Shangbin Feng, Xiaochuang Han, Vidhisha Balachandran, Chan
  Young Park, Sachin Kumar, Yulia Tsvetkov
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.09741 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09783
replaced with revised version Wed, 3 Apr 2024 23:29:03 GMT   (475kb,D)

Title: Investigating Data Contamination in Modern Benchmarks for Large Language
  Models
Authors: Chunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Gerstein, Arman Cohan
Categories: cs.CL cs.AI
Comments: NAACL 2024 Version
\\ ( https://arxiv.org/abs/2311.09783 ,  475kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04372
replaced with revised version Thu, 4 Apr 2024 05:20:11 GMT   (1341kb,D)

Title: LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language
  Model Programs
Authors: Yunsheng Ma, Can Cui, Xu Cao, Wenqian Ye, Peiran Liu, Juanwu Lu, Amr
  Abdelraouf, Rohit Gupta, Kyungtae Han, Aniket Bera, James M. Rehg, Ziran Wang
Categories: cs.CL cs.AI
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2312.04372 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15166
replaced with revised version Thu, 4 Apr 2024 01:53:38 GMT   (467kb,D)

Title: SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective
  Depth Up-Scaling
Authors: Dahyun Kim, Chanjun Park, Sanghoon Kim, Wonsung Lee, Wonho Song, Yunsu
  Kim, Hyeonwoo Kim, Yungi Kim, Hyeonju Lee, Jihoo Kim, Changbae Ahn, Seonghoon
  Yang, Sukyung Lee, Hyunbyung Park, Gyoungjin Gim, Mikyoung Cha, Hwalsuk Lee,
  Sunghun Kim
Categories: cs.CL cs.AI cs.LG
Comments: accepted to NAACL 2024 Industry Track
\\ ( https://arxiv.org/abs/2312.15166 ,  467kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09432
replaced with revised version Thu, 4 Apr 2024 13:27:38 GMT   (12769kb,D)

Title: RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language
  Models
Authors: Meiling Tao, Xuechen Liang, Tianyu Shi, Lei Yu, Yiting Xie
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.09432 ,  12769kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15496
replaced with revised version Thu, 4 Apr 2024 03:15:15 GMT   (2176kb,D)

Title: Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue
  Summarization
Authors: Jianfei Xiao, Yancan Chen, Yimin Ou, Hanyi Yu, Kai Shu, Yiyong Xiao
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.15496 ,  2176kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17377
replaced with revised version Thu, 4 Apr 2024 17:28:38 GMT   (7917kb,D)

Title: Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion
  Tokens
Authors: Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh
  Hajishirzi
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.17377 ,  7917kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01216
replaced with revised version Thu, 4 Apr 2024 02:15:39 GMT   (280kb,D)

Title: API Is Enough: Conformal Prediction for Large Language Models Without
  Logit-Access
Authors: Jiayuan Su, Jing Luo, Hongwei Wang, Lu Cheng
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.01216 ,  280kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02975
replaced with revised version Thu, 4 Apr 2024 01:07:24 GMT   (0kb,I)

Title: A General and Flexible Multi-concept Parsing Framework for Multilingual
  Semantic Matching
Authors: Dong Yao, Asaad Alghamdi, Qingrong Xia, Xiaoye Qu, Xinyu Duan, Zhefeng
  Wang, Yi Zheng, Baoxing Huai, Peilun Cheng, Zhou Zhao
Categories: cs.CL cs.AI
Comments: There are some copyright issues between Huawei and Tonomus
\\ ( https://arxiv.org/abs/2403.02975 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04182
replaced with revised version Thu, 4 Apr 2024 13:48:19 GMT   (190kb,D)

Title: Metric-aware LLM inference for regression and scoring
Authors: Michal Lukasik, Harikrishna Narasimhan, Aditya Krishna Menon, Felix
  Yu, Sanjiv Kumar
Categories: cs.CL cs.AI
Comments: 15 pages
\\ ( https://arxiv.org/abs/2403.04182 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09057
replaced with revised version Wed, 3 Apr 2024 18:08:30 GMT   (3528kb,D)

Title: A Continued Pretrained LLM Approach for Automatic Medical Note
  Generation
Authors: Dong Yuan, Eti Rastogi, Gautam Naik, Sree Prasanna Rajagopal, Sagar
  Goyal, Fen Zhao, Bharath Chintagunta, Jeff Ward
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2403.09057 ,  3528kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14171
replaced with revised version Thu, 4 Apr 2024 05:13:56 GMT   (2312kb,D)

Title: MMIDR: Teaching Large Language Model to Interpret Multimodal
  Misinformation via Knowledge Distillation
Authors: Longzheng Wang, Xiaohan Xu, Lei Zhang, Jiarui Lu, Yongxiu Xu, Hongbo
  Xu, Minghao Tang, Chuang Zhang
Categories: cs.CL
Comments: 10 pages, 3 figures
\\ ( https://arxiv.org/abs/2403.14171 ,  2312kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18802
replaced with revised version Wed, 3 Apr 2024 20:54:11 GMT   (238kb,D)

Title: Long-form factuality in large language models
Authors: Jerry Wei and Chengrun Yang and Xinying Song and Yifeng Lu and Nathan
  Hu and Jie Huang and Dustin Tran and Daiyi Peng and Ruibo Liu and Da Huang
  and Cosmo Du and Quoc V. Le
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.18802 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18933
replaced with revised version Thu, 4 Apr 2024 15:30:14 GMT   (5359kb,D)

Title: SemEval Task 1: Semantic Textual Relatedness for African and Asian
  Languages
Authors: Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris
  Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir
  Araujo, Meriem Beloucif, Christine De Kock, Oumaima Hourrane, Manish
  Shrivastava, Thamar Solorio, Nirmal Surange, Krishnapriya Vishnubhotla, Seid
  Muhie Yimam, Saif M. Mohammad
Categories: cs.CL
Comments: SemEval 2024 Task Description Paper. arXiv admin note: text overlap
  with arXiv:2402.08638
\\ ( https://arxiv.org/abs/2403.18933 ,  5359kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20145
replaced with revised version Thu, 4 Apr 2024 10:36:48 GMT   (7068kb,D)

Title: Fine-tuning Large Language Models for Automated Diagnostic Screening
  Summaries
Authors: Manjeet Yadav, Nilesh Kumar Sahu, Mudita Chaturvedi, Snehil Gupta,
  Haroon R Lone
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.20145 ,  7068kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00450
replaced with revised version Thu, 4 Apr 2024 05:33:07 GMT   (9121kb,D)

Title: Planning and Editing What You Retrieve for Enhanced Tool Learning
Authors: Tenghao Huang, Dongwon Jung, Muhao Chen
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: This paper is accepted at NAACL-Findings 2024
\\ ( https://arxiv.org/abs/2404.00450 ,  9121kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00826
replaced with revised version Thu, 4 Apr 2024 04:21:34 GMT   (7460kb,D)

Title: Extracting Social Determinants of Health from Pediatric Patient Notes
  Using Large Language Models: Novel Corpus and Methods
Authors: Yujuan Fu, Giridhar Kaushik Ramachandran, Nicholas J Dobbins, Namu
  Park, Michael Leu, Abby R. Rosenberg, Kevin Lybarger, Fei Xia, Ozlem Uzuner,
  Meliha Yetisgen
Categories: cs.CL
Comments: 12 pages, 2 figures and 3 tables. Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2404.00826 ,  7460kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01616
replaced with revised version Thu, 4 Apr 2024 01:51:22 GMT   (251kb,D)

Title: Transforming LLMs into Cross-modal and Cross-lingual Retrieval Systems
Authors: Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer,
  Siddharth Dalmia, Gustavo Hernandez Abrego
Categories: cs.CL cs.IR cs.SD eess.AS
\\ ( https://arxiv.org/abs/2404.01616 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01663
replaced with revised version Thu, 4 Apr 2024 12:40:03 GMT   (11503kb,D)

Title: CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small
  Language Models
Authors: Xuechen Liang and Meiling Tao, Tianyu Shi, Yiting Xie
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.01663 ,  11503kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02053
replaced with revised version Thu, 4 Apr 2024 08:05:37 GMT   (13579kb,D)

Title: BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights
Authors: Enmin Zhu, Jerome Yen
Categories: cs.CL cs.CE q-fin.ST
\\ ( https://arxiv.org/abs/2404.02053 ,  13579kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02060
replaced with revised version Thu, 4 Apr 2024 00:01:25 GMT   (3625kb,D)

Title: Long-context LLMs Struggle with Long In-context Learning
Authors: Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2404.02060 ,  3625kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02421
replaced with revised version Thu, 4 Apr 2024 04:52:37 GMT   (9171kb,D)

Title: Revisiting subword tokenization: A case study on affixal negation in
  large language models
Authors: Thinh Hung Truong, Yulia Otmakhova, Karin Verspoor, Trevor Cohn,
  Timothy Baldwin
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2404.02421 ,  9171kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02540
replaced with revised version Thu, 4 Apr 2024 04:17:27 GMT   (220kb)

Title: CSEPrompts: A Benchmark of Introductory Computer Science Prompts
Authors: Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo,
  Christian Newman, Tharindu Ranasinghe, Marcos Zampieri
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.02540 ,  220kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02761
replaced with revised version Thu, 4 Apr 2024 04:34:31 GMT   (102kb,D)

Title: AQuA - Combining Experts' and Non-Experts' Views To Assess Deliberation
  Quality in Online Discussions Using LLMs
Authors: Maike Behrendt, Stefan Sylvius Wagner, Marc Ziegele, Lena Wilms, Anke
  Stoll, Dominique Heinbach and Stefan Harmeling
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2404.02761 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:1911.10829
replaced with revised version Thu, 4 Apr 2024 09:30:55 GMT   (2080kb,D)

Title: Neural Random Forest Imitation
Authors: Christoph Reinders and Bodo Rosenhahn
Categories: cs.LG stat.ML
Comments: Published as part of "Two Worlds in One Network: Fusing Deep Learning
  and Random Forests for Classification and Object Detection" in Volunteered
  Geographic Information, Springer Nature Switzerland
DOI: 10.1007/978-3-031-35374-1_5
\\ ( https://arxiv.org/abs/1911.10829 ,  2080kb)
------------------------------------------------------------------------------
\\
arXiv:2010.08114
replaced with revised version Thu, 4 Apr 2024 03:12:50 GMT   (1668kb,D)

Title: Distributed Representations of Entities in Open-World Knowledge Graphs
Authors: Lingbing Guo, Zhuo Chen, Jiaoyan Chen, Yichi Zhang, Zequn Sun, Zhongpo
  Bo, Yin Fang, Xiaoze Liu, Huajun Chen, Wen Zhang
Categories: cs.LG cs.AI cs.CL
Comments: Knowledge-Based Systems 2024
\\ ( https://arxiv.org/abs/2010.08114 ,  1668kb)
------------------------------------------------------------------------------
\\
arXiv:2106.10761
replaced with revised version Wed, 3 Apr 2024 19:39:10 GMT   (55kb)

Title: Generalization in the Face of Adaptivity: A Bayesian Perspective
Authors: Moshe Shenfeld and Katrina Ligett
Categories: cs.LG stat.ML
Journal-ref: Shenfeld, M., & Ligett, K. (2024). Generalization in the face of
  adaptivity: A Bayesian perspective. Advances in Neural Information Processing
  Systems, 36
\\ ( https://arxiv.org/abs/2106.10761 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2301.06625
replaced with revised version Wed, 3 Apr 2024 21:24:18 GMT   (2830kb,D)

Title: A Transformer-based Diffusion Probabilistic Model for Heart Rate and
  Blood Pressure Forecasting in Intensive Care Unit
Authors: Ping Chang, Huayu Li, Stuart F. Quan, Shuyang Lu, Shu-Fen Wung, Janet
  Roveda and Ao Li
Categories: cs.LG
DOI: 10.1016/j.cmpb.2024.108060
\\ ( https://arxiv.org/abs/2301.06625 ,  2830kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01189
replaced with revised version Thu, 4 Apr 2024 14:35:35 GMT   (16207kb,D)

Title: Reinforcement learning-based estimation for partial differential
  equations
Authors: Saviz Mowlavi, Mouhacine Benosman
Categories: cs.LG cs.SY eess.SY
Comments: 24 pages, 15 figures
\\ ( https://arxiv.org/abs/2302.01189 ,  16207kb)
------------------------------------------------------------------------------
\\
arXiv:2302.01925
replaced with revised version Wed, 3 Apr 2024 21:24:50 GMT   (9906kb,D)

Title: Learning a Fourier Transform for Linear Relative Positional Encodings in
  Transformers
Authors: Krzysztof Marcin Choromanski, Shanda Li, Valerii Likhosherstov, Kumar
  Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tamas Sarlos, Thomas
  Weingarten, Adrian Weller
Categories: cs.LG
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2302.01925 ,  9906kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01923
replaced with revised version Thu, 4 Apr 2024 16:54:25 GMT   (1317kb,D)

Title: Fairness Improvement with Multiple Protected Attributes: How Far Are We?
Authors: Zhenpeng Chen and Jie M. Zhang and Federica Sarro and Mark Harman
Categories: cs.LG cs.AI cs.CY cs.SE
Comments: Accepted by the 46th International Conference on Software Engineering
  (ICSE 2024). Please include ICSE in any citations
\\ ( https://arxiv.org/abs/2308.01923 ,  1317kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12388
replaced with revised version Thu, 4 Apr 2024 03:38:32 GMT   (256kb,D)

Title: Missing Data Imputation Based on Structural Equation Modeling Enhanced
  with Self-Attention
Authors: Ou Deng, Qun Jin
Categories: cs.LG
Comments: The source code for the experiments is publicly available at:
  https://github.com/oudeng/SESA/
\\ ( https://arxiv.org/abs/2308.12388 ,  256kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07402
replaced with revised version Thu, 4 Apr 2024 07:08:25 GMT   (1524kb,D)

Title: Semi-supervised Domain Adaptation on Graphs with Contrastive Learning
  and Minimax Entropy
Authors: Jiaren Xiao, Quanyu Dai, Xiao Shen, Xiaochen Xie, Jing Dai, James Lam,
  Ka-Wai Kwok
Categories: cs.LG
Journal-ref: Neurocomputing (2024)
DOI: 10.1016/j.neucom.2024.127469
\\ ( https://arxiv.org/abs/2309.07402 ,  1524kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08339
replaced with revised version Wed, 3 Apr 2024 13:36:16 GMT   (106404kb,D)

Title: A Theoretical and Empirical Study on the Convergence of Adam with an
  "Exact" Constant Step Size in Non-Convex Settings
Authors: Alokendu Mazumder, Rishabh Sabharwal, Manan Tayal, Bhartendu Kumar,
  Punit Rathore
Categories: cs.LG math.OC
Comments: 48 pages including proofs and extended experiments
\\ ( https://arxiv.org/abs/2309.08339 ,  106404kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08546
replaced with revised version Thu, 4 Apr 2024 12:32:43 GMT   (413kb,D)

Title: Towards Robust Continual Learning with Bayesian Adaptive Moment
  Regularization
Authors: Jack Foster and Alexandra Brintrup
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.08546 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12545
replaced with revised version Thu, 4 Apr 2024 15:29:25 GMT   (46kb)

Title: Provably Robust and Plausible Counterfactual Explanations for Neural
  Networks via Robust Optimisation
Authors: Junqi Jiang, Jianglin Lan, Francesco Leofante, Antonio Rago, Francesca
  Toni
Categories: cs.LG cs.AI
Comments: Accepted at ACML 2023, camera-ready version
\\ ( https://arxiv.org/abs/2309.12545 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00763
replaced with revised version Thu, 4 Apr 2024 01:52:32 GMT   (496kb,D)

Title: Data-Efficient Strategies for Probabilistic Voltage Envelopes under
  Network Contingencies
Authors: Parikshit Pareek, Deepjyoti Deka and Sidhant Misra
Categories: cs.LG cs.SY eess.SY
Comments: 10 Pages
\\ ( https://arxiv.org/abs/2310.00763 ,  496kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00965
replaced with revised version Thu, 4 Apr 2024 13:40:51 GMT   (16171kb,D)

Title: Effective Learning with Node Perturbation in Deep Neural Networks
Authors: Sander Dalm, Marcel van Gerven, Nasir Ahmad
Categories: cs.LG
Comments: 14 pages, 9 figures
\\ ( https://arxiv.org/abs/2310.00965 ,  16171kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12508
replaced with revised version Thu, 4 Apr 2024 07:45:38 GMT   (15593kb,D)

Title: SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency
  in Both Image Classification and Generation
Authors: Chongyu Fan, Jiancheng Liu, Yihua Zhang, Eric Wong, Dennis Wei, Sijia
  Liu
Categories: cs.LG cs.AI
Comments: Accepted by ICLR 2024 as a Spotlight paper
\\ ( https://arxiv.org/abs/2310.12508 ,  15593kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19454
replaced with revised version Thu, 4 Apr 2024 09:38:42 GMT   (2112kb,AD)

Title: MMM and MMMSynth: Clustering of heterogeneous tabular data, and
  synthetic data generation
Authors: Chandrani Kumari and Rahul Siddharthan
Categories: cs.LG stat.ML
Comments: 17 pages, 5 figures
\\ ( https://arxiv.org/abs/2310.19454 ,  2112kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15161
replaced with revised version Thu, 4 Apr 2024 14:12:11 GMT   (592kb,D)

Title: Hessian Aware Low-Rank Weight Perturbation for Continual Learning
Authors: Jiaqi Li, Rui Wang, Yuanhao Lai, Changjian Shui, Sabyasachi Sahoo,
  Charles X. Ling, Shichun Yang, Boyu Wang, Christian Gagn\'e, Fan Zhou
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.15161 ,  592kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01037
replaced with revised version Wed, 3 Apr 2024 21:44:48 GMT   (1280kb,D)

Title: Eliciting Latent Knowledge from Quirky Language Models
Authors: Alex Mallen, Madeline Brumley, Julia Kharchenko, and Nora Belrose
Categories: cs.LG cs.AI cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2312.01037 ,  1280kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04609
replaced with revised version Thu, 4 Apr 2024 06:31:36 GMT   (2115kb,D)

Title: Short-term prediction of construction waste transport activities using
  AI-Truck
Authors: Meng Xu, Ke Han
Categories: cs.LG cs.AI
Comments: 11 pages, 15 figures
\\ ( https://arxiv.org/abs/2312.04609 ,  2115kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14694
replaced with revised version Wed, 3 Apr 2024 23:57:52 GMT   (780kb)

Title: TA-RNN: an Attention-based Time-aware Recurrent Neural Network
  Architecture for Electronic Health Records
Authors: Mohammad Al Olaimat, Serdar Bozdag (for the Alzheimer's Disease
  Neuroimaging Initiative)
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.14694 ,  780kb)
------------------------------------------------------------------------------
\\
arXiv:2401.18079
replaced with revised version Thu, 4 Apr 2024 17:45:34 GMT   (1157kb,D)

Title: KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache
  Quantization
Authors: Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Michael W. Mahoney,
  Yakun Sophia Shao, Kurt Keutzer, Amir Gholami
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.18079 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09201
replaced with revised version Thu, 4 Apr 2024 11:22:20 GMT   (39kb)

Title: Better-than-KL PAC-Bayes Bounds
Authors: Ilja Kuzborskij, Kwang-Sung Jun, Yulian Wu, Kyoungseok Jang, Francesco
  Orabona
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.09201 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11887
replaced with revised version Thu, 4 Apr 2024 10:08:25 GMT   (3821kb,D)

Title: Generative Semi-supervised Graph Anomaly Detection
Authors: Hezhe Qiao, Qingsong Wen, Xiaoli Li, Ee-Peng Lim, Guansong Pang
Categories: cs.LG
Comments: 13 pages, 10 figures
\\ ( https://arxiv.org/abs/2402.11887 ,  3821kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13332
replaced with revised version Thu, 4 Apr 2024 14:02:47 GMT   (959kb,D)

Title: Causal hybrid modeling with double machine learning
Authors: Kai-Hendrik Cohrs, Gherardo Varando, Nuno Carvalhais, Markus
  Reichstein, Gustau Camps-Valls
Categories: cs.LG stat.ME
\\ ( https://arxiv.org/abs/2402.13332 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14698
replaced with revised version Thu, 4 Apr 2024 11:41:04 GMT   (17013kb,D)

Title: Using construction waste hauling trucks' GPS data to classify
  earthwork-related locations: A Chengdu case study
Authors: Lei Yu, Ke Han
Categories: cs.LG cs.AI
Comments: 12 pages, 8 figures
\\ ( https://arxiv.org/abs/2402.14698 ,  17013kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02821
replaced with revised version Thu, 4 Apr 2024 12:47:28 GMT   (217kb,D)

Title: An Adaptive Hydropower Management Approach for Downstream Ecosystem
  Preservation
Authors: C. Coelho, M. Jing, M. Fernanda P. Costa, L.L. Ferr\'as
Categories: cs.LG cs.CE math.OC
ACM-class: J.2; I.5.1; G.1.6
\\ ( https://arxiv.org/abs/2403.02821 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03219
replaced with revised version Wed, 3 Apr 2024 21:49:42 GMT   (39kb)

Title: LC-Tsallis-INF: Generalized Best-of-Both-Worlds Linear Contextual
  Bandits
Authors: Masahiro Kato and Shinji Ito
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.03219 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13032
replaced with revised version Thu, 4 Apr 2024 11:26:58 GMT   (1422kb,D)

Title: Hybrid Unsupervised Learning Strategy for Monitoring Industrial Batch
  Processes
Authors: Christian W. Frey
Categories: cs.LG cs.SY eess.SP eess.SY
Comments: 6 pages, 4 figures, \c{opyright} 2024 Christian W. frey. This work
  has been accepted to IFAC for publication under a Creative Commons Licence
  CC-BY-NC-ND
\\ ( https://arxiv.org/abs/2403.13032 ,  1422kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16612
replaced with revised version Thu, 4 Apr 2024 12:35:33 GMT   (8352kb,D)

Title: Calibrating Bayesian UNet++ for Sub-Seasonal Forecasting
Authors: Busra Asan, Abdullah Akg\"ul, Alper Unal, Melih Kandemir, Gozde Unal
Categories: cs.LG cs.CV
Comments: Accepted as a workshop paper at "ICLR 2024 Tackling Climate Change
  with Machine Learning"
\\ ( https://arxiv.org/abs/2403.16612 ,  8352kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20208
replaced with revised version Thu, 4 Apr 2024 06:28:25 GMT   (712kb,D)

Title: Unleashing the Potential of Large Language Models for Predictive Tabular
  Tasks in Data Science
Authors: Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu
Categories: cs.LG cs.AI
Comments: 10 pages
\\ ( https://arxiv.org/abs/2403.20208 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01965
replaced with revised version Thu, 4 Apr 2024 10:54:04 GMT   (427kb,D)

Title: Towards Leveraging AutoML for Sustainable Deep Learning: A
  Multi-Objective HPO Approach on Deep Shift Neural Networks
Authors: Leona Hennig, Tanja Tornede, Marius Lindauer
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2404.01965 ,  427kb)
------------------------------------------------------------------------------
\\
arXiv:1902.11122
replaced with revised version Thu, 4 Apr 2024 11:34:52 GMT   (204kb,D)

Title: Deep Learning in Cardiology
Authors: Paschalis Bizopoulos and Dimitrios Koutsouris
Categories: cs.CV cs.AI cs.LG
Comments: 27 pages, 2 figures, 10 tables
Journal-ref: IEEE Reviews in Biomedical Engineering 12 (2019): 168-193
DOI: 10.1109/RBME.2018.2885714
\\ ( https://arxiv.org/abs/1902.11122 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10827
replaced with revised version Wed, 3 Apr 2024 18:38:24 GMT   (870kb,D)

Title: AutoML in The Wild: Obstacles, Workarounds, and Expectations
Authors: Yuan Sun, Qiurong Song, Xinning Gui, Fenglong Ma, Ting Wang
Categories: cs.HC cs.AI
Comments: In Proceedings of the 2023 CHI Conference on Human Factors in
  Computing Systems (CHI'23), April 23-28, 2023, Hamburg, Germany
DOI: 10.1145/3544548.3581082
\\ ( https://arxiv.org/abs/2302.10827 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00006
replaced with revised version Thu, 4 Apr 2024 10:06:34 GMT   (5778kb,D)

Title: Truncated Affinity Maximization: One-class Homophily Modeling for Graph
  Anomaly Detection
Authors: Hezhe Qiao and Guansong Pang
Categories: cs.SI cs.AI cs.LG
Comments: Accepted at NeurIPS 2023
\\ ( https://arxiv.org/abs/2306.00006 ,  5778kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06647
replaced with revised version Thu, 4 Apr 2024 04:07:48 GMT   (3087kb,D)

Title: DeepIPCv2: LiDAR-powered Robust Environmental Perception and
  Navigational Control for Autonomous Vehicle
Authors: Oskar Natan, Jun Miura
Categories: cs.RO cs.AI cs.CV
\\ ( https://arxiv.org/abs/2307.06647 ,  3087kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02058
replaced with revised version Thu, 4 Apr 2024 12:26:03 GMT   (1970kb,D)

Title: Incorporating Recklessness to Collaborative Filtering based Recommender
  Systems
Authors: Diego P\'erez-L\'opez, Fernando Ortega, \'Angel Gonz\'alez-Prieto,
  Jorge Due\~nas-Ler\'in
Categories: cs.IR cs.AI cs.LG stat.ML
Comments: 15 pages, 4 figures, 2 tables
MSC-class: Primary: 68T05, Secondary: 68T42, 62M20
ACM-class: I.2; I.5
\\ ( https://arxiv.org/abs/2308.02058 ,  1970kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02793
replaced with revised version Thu, 4 Apr 2024 05:10:06 GMT   (1039kb,D)

Title: Crowdsourcing Fraud Detection over Heterogeneous Temporal MMMA Graph
Authors: Zequan Xu, Qihang Sun, Shaofeng Hu, Jieming Shi, Hui Li
Categories: cs.SI cs.AI
Comments: Full technical report for our DASFAA 2024 paper: Crowdsourcing Fraud
  Detection over Heterogeneous Temporal MMMA Graph
\\ ( https://arxiv.org/abs/2308.02793 ,  1039kb)
------------------------------------------------------------------------------
\\
arXiv:2308.02935
replaced with revised version Thu, 4 Apr 2024 07:56:59 GMT   (7140kb,D)

Title: Bias Behind the Wheel: Fairness Analysis of Autonomous Driving Systems
Authors: Xinyue Li and Zhenpeng Chen and Jie M. Zhang and Federica Sarro and
  Ying Zhang and Xuanzhe Liu
Categories: cs.CY cs.AI cs.CV cs.SE
\\ ( https://arxiv.org/abs/2308.02935 ,  7140kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13234
replaced with revised version Thu, 4 Apr 2024 10:08:10 GMT   (6666kb,D)

Title: Decoding Natural Images from EEG for Object Recognition
Authors: Yonghao Song, Bingchuan Liu, Xiang Li, Nanlin Shi, Yijun Wang,
  Xiaorong Gao
Categories: cs.HC cs.AI eess.SP q-bio.NC
Comments: ICLR, 2024
\\ ( https://arxiv.org/abs/2308.13234 ,  6666kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04077
replaced with revised version Wed, 3 Apr 2024 20:53:45 GMT   (9790kb,D)

Title: SayNav: Grounding Large Language Models for Dynamic Planning to
  Navigation in New Environments
Authors: Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu
  and Alvaro Velasquez
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2309.04077 ,  9790kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14162
replaced with revised version Thu, 4 Apr 2024 13:29:25 GMT   (38923kb,D)

Title: Data Upcycling Knowledge Distillation for Image Super-Resolution
Authors: Yun Zhang, Wei Li, Simiao Li, Hanting Chen, Zhijun Tu, Wenjia Wang,
  Bingyi Jing, Shaohui Lin and Jie Hu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2309.14162 ,  38923kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00156
replaced with revised version Thu, 4 Apr 2024 02:03:20 GMT   (25001kb,D)

Title: Learning Generalizable Tool-use Skills through Trajectory Generation
Authors: Carl Qi, Yilin Wu, Lifan Yu, Haoyue Liu, Bowen Jiang, Xingyu Lin,
  David Held
Categories: cs.RO cs.AI
ACM-class: I.2.9
\\ ( https://arxiv.org/abs/2310.00156 ,  25001kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02003
replaced with revised version Thu, 4 Apr 2024 01:53:27 GMT   (7904kb,D)

Title: L2MAC: Large Language Model Automatic Computer for Extensive Code
  Generation
Authors: Samuel Holt, Max Ruiz Luyten, Mihaela van der Schaar
Categories: cs.SE cs.AI cs.LG cs.PL
Comments: Published in The Twelfth International Conference on Learning
  Representations (ICLR), 2024. Copyright 2023 by the author(s)
ACM-class: I.2.7; I.2.6; I.2.5; D.2.2; D.2.3; D.3.4
\\ ( https://arxiv.org/abs/2310.02003 ,  7904kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04621
replaced with revised version Wed, 3 Apr 2024 21:29:05 GMT   (5508kb,D)

Title: Model Compression in Practice: Lessons Learned from Practitioners
  Creating On-device Machine Learning Experiences
Authors: Fred Hohman, Mary Beth Kery, Donghao Ren, Dominik Moritz
Categories: cs.HC cs.AI cs.LG
Comments: Proceedings of the 2024 ACM CHI Conference on Human Factors in
  Computing Systems
DOI: 10.1145/3613904.3642109
\\ ( https://arxiv.org/abs/2310.04621 ,  5508kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10599
replaced with revised version Wed, 3 Apr 2024 19:32:34 GMT   (6720kb,D)

Title: Chatbots as social companions: How people perceive consciousness, human
  likeness, and social health benefits in machines
Authors: Rose E. Guingrich, Michael S. A. Graziano
Categories: cs.HC cs.AI
Comments: 18 pages, 3 figures, presented at ASSC '26 at NYU
\\ ( https://arxiv.org/abs/2311.10599 ,  6720kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10776
replaced with revised version Thu, 4 Apr 2024 10:57:56 GMT   (3136kb,D)

Title: Chemist-X: Large Language Model-empowered Agent for Reaction Condition
  Recommendation in Chemical Synthesis
Authors: Kexin Chen, Junyou Li, Kunyi Wang, Yuyang Du, Jiahui Yu, Jiamin Lu,
  Lanqing Li, Jiezhong Qiu, Jianzhang Pan, Yi Huang, Qun Fang, Pheng Ann Heng,
  Guangyong Chen
Categories: cs.IR cs.AI
\\ ( https://arxiv.org/abs/2311.10776 ,  3136kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18138
replaced with revised version Thu, 4 Apr 2024 00:16:08 GMT   (559kb,D)

Title: Algorithmic Persuasion Through Simulation
Authors: Keegan Harris, Nicole Immorlica, Brendan Lucier, Aleksandrs Slivkins
Categories: cs.GT cs.AI econ.TH
\\ ( https://arxiv.org/abs/2311.18138 ,  559kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10170
replaced with revised version Thu, 4 Apr 2024 13:51:56 GMT   (9703kb,D)

Title: UINav: A Practical Approach to Train On-Device Automation Agents
Authors: Wei Li, Fu-Lin Hsu, Will Bishop, Folawiyo Campbell-Ajala, Max Lin,
  Oriana Riva
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2312.10170 ,  9703kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03737 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 13:18:55 GMT   (5967kb,D)

Title: Can Large Language Models Beat Wall Street? Unveiling the Potential of
  AI in Stock Selection
Authors: Georgios Fatouros, Konstantinos Metaxas, John Soldatos, Dimosthenis
  Kyriazis
Categories: q-fin.CP cs.AI cs.CE cs.CL cs.LG
Comments: 17 pages, 12 figures, 12 tables
MSC-class: 68T07, 68T50, 91G10, 91G15
ACM-class: I.2.1; I.2.7; J.4
\\ ( https://arxiv.org/abs/2401.03737 ,  5967kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11322
replaced with revised version Thu, 4 Apr 2024 13:55:05 GMT   (1804kb,D)

Title: SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for
  Spiking Neural Network-based Autonomous Agents
Authors: Rachmad Vidya Wicaksana Putra, Muhammad Shafique
Categories: cs.NE cs.AI cs.LG
Comments: 8 pages, 13 figures, 2 tables
\\ ( https://arxiv.org/abs/2402.11322 ,  1804kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14151
replaced with revised version Wed, 3 Apr 2024 20:11:02 GMT   (21545kb,D)

Title: BIRCO: A Benchmark of Information Retrieval Tasks with Complex
  Objectives
Authors: Xiaoyue Wang, Jianyou Wang, Weili Cao, Kaicheng Wang, Ramamohan
  Paturi, Leon Bergen
Categories: cs.IR cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.14151 ,  21545kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01598 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 16:12:51 GMT   (20358kb,D)

Title: APISR: Anime Production Inspired Real-World Anime Super-Resolution
Authors: Boyang Wang, Fengyu Yang, Xihang Yu, Chao Zhang, Hanbin Zhao
Categories: eess.IV cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.01598 ,  20358kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14680
replaced with revised version Thu, 4 Apr 2024 15:34:37 GMT   (1033kb)

Title: Trust in AI: Progress, Challenges, and Future Directions
Authors: Saleh Afroogh, Ali Akbari, Evan Malone, Mohammadali Kargar, Hananeh
  Alambeigi
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2403.14680 ,  1033kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16760
replaced with revised version Thu, 4 Apr 2024 14:51:56 GMT   (363kb)

Title: As Good As A Coin Toss: Human detection of AI-generated images, videos,
  audio, and audiovisual stimuli
Authors: Di Cooke, Abigail Edwards, Sophia Barkoff, and Kathryn Kelly
Categories: cs.HC cs.AI cs.SD eess.AS
Comments: For study pre-registration, see https://osf.io/fnhr3
MSC-class: 68T01
ACM-class: I.2
\\ ( https://arxiv.org/abs/2403.16760 ,  363kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01685
replaced with revised version Thu, 4 Apr 2024 00:36:18 GMT   (515kb,D)

Title: A Methodology for Improving Accuracy of Embedded Spiking Neural Networks
  through Kernel Size Scaling
Authors: Rachmad Vidya Wicaksana Putra, Muhammad Shafique
Categories: cs.NE cs.AI cs.LG
Comments: 3 pages, 3 figures
\\ ( https://arxiv.org/abs/2404.01685 ,  515kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02656
replaced with revised version Thu, 4 Apr 2024 13:30:59 GMT   (9623kb,D)

Title: Non-negative Subspace Feature Representation for Few-shot Learning in
  Medical Imaging
Authors: Keqiang Fan, Xiaohao Cai and Mahesan Niranjan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2404.02656 ,  9623kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02807 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 12:54:30 GMT   (7001kb,D)

Title: An Optimization Framework to Personalize Passive Cardiac Mechanics
Authors: Lei Shi, Ian Chen, Hiroo Takayama, Vijay Vedula
Categories: physics.med-ph cs.AI
\\ ( https://arxiv.org/abs/2404.02807 ,  7001kb)
------------------------------------------------------------------------------
\\
arXiv:2211.06073
replaced with revised version Thu, 4 Apr 2024 09:58:35 GMT   (9599kb,D)

Title: SceneFake: An Initial Dataset and Benchmarks for Scene Fake Audio
  Detection
Authors: Jiangyan Yi and Chenglong Wang and Jianhua Tao and Chu Yuan Zhang and
  Cunhang Fan and Zhengkun Tian and Haoxin Ma and Ruibo Fu
Categories: cs.SD cs.CL eess.AS
Comments: Accepted by Pattern Recognition, 1 April 2024
\\ ( https://arxiv.org/abs/2211.06073 ,  9599kb)
------------------------------------------------------------------------------
\\
arXiv:2304.07235 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 13:24:36 GMT   (569kb,D)

Title: Mapping of attention mechanisms to a generalized Potts model
Authors: Riccardo Rende, Federica Gerace, Alessandro Laio, Sebastian Goldt
Categories: cond-mat.dis-nn cond-mat.stat-mech cs.CL stat.ML
Comments: 5 pages, 3 figures
\\ ( https://arxiv.org/abs/2304.07235 ,  569kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16842
replaced with revised version Thu, 4 Apr 2024 15:18:30 GMT   (3950kb,D)

Title: RELIC: Investigating Large Language Model Responses using
  Self-Consistency
Authors: Furui Cheng, Vil\'em Zouhar, Simran Arora, Mrinmaya Sachan, Hendrik
  Strobelt and Mennatallah El-Assady
Categories: cs.HC cs.CL
DOI: 10.1145/3613904.3641904
\\ ( https://arxiv.org/abs/2311.16842 ,  3950kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02822
replaced with revised version Thu, 4 Apr 2024 11:23:59 GMT   (57kb)

Title: Identifying Climate Targets in National Laws and Policies using Machine
  Learning
Authors: Matyas Juhasz, Tina Marchand, Roshan Melwani, Kalyan Dutia, Sarah
  Goodenough, Harrison Pim, Henry Franks
Categories: cs.CY cs.CL cs.LG
\\ ( https://arxiv.org/abs/2404.02822 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2010.13187 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 02:47:09 GMT   (16330kb,D)

Title: Improving the Reconstruction of Disentangled Representation Learners via
  Multi-Stage Modeling
Authors: Akash Srivastava, Yamini Bansal, Yukun Ding, Cole Lincoln Hurwitz, Kai
  Xu, Bernhard Egger, Prasanna Sattigeri, Joshua B. Tenenbaum, Phuong Le, Arun
  Prakash R, Nengfeng Zhou, Joel Vaughan, Yaquan Wang, Anwesha Bhattacharyya,
  Kristjan Greenewald, David D. Cox, Dan Gutfreund
Categories: stat.ML cs.CV cs.LG
\\ ( https://arxiv.org/abs/2010.13187 ,  16330kb)
------------------------------------------------------------------------------
\\
arXiv:2203.13856 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 00:13:42 GMT   (2879kb,D)

Title: Robust deep learning for eye fundus images: Bridging real and synthetic
  data for enhancing generalization
Authors: Guilherme C. Oliveira, Gustavo H. Rosa, Daniel C. G. Pedronette,
  Jo\~ao P. Papa, Himeesh Kumar, Leandro A. Passos, Dinesh Kumar
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by the Biomedical Signal Processing and Control
Journal-ref: Biomedical Signal Processing and Control, 94 (2024), 106263
DOI: 10.1016/j.bspc.2024.106263
\\ ( https://arxiv.org/abs/2203.13856 ,  2879kb)
------------------------------------------------------------------------------
\\
arXiv:2205.07739 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 12:14:30 GMT   (2858kb,D)

Title: A replica analysis of Self-Training of Linear Classifier
Authors: Takashi Takahashi
Categories: stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.LG math.ST stat.TH
Comments: 65 pages, 13 figures
\\ ( https://arxiv.org/abs/2205.07739 ,  2858kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00856 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 21:26:30 GMT   (647kb)

Title: Risk-Adaptive Approaches to Stochastic Optimization: A Survey
Authors: Johannes O. Royset
Categories: math.OC cs.LG stat.ML
MSC-class: 46N10, 52B55, 65K05, 68Q32, 90C25, 91A26, 91B05, 91G70
\\ ( https://arxiv.org/abs/2212.00856 ,  647kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08424 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 16:24:19 GMT   (1125kb,D)

Title: Long-term Forecasting with TiDE: Time-series Dense Encoder
Authors: Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen and
  Rose Yu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2304.08424 ,  1125kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10294 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 13:45:51 GMT   (2424kb)

Title: OptoGPT: A Foundation Model for Inverse Design in Optical Multilayer
  Thin Film Structures
Authors: Taigao Ma, Haozhu Wang, L. Jay Guo
Categories: physics.optics cs.LG
Comments: 25 pages, 8 figures
\\ ( https://arxiv.org/abs/2304.10294 ,  2424kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05494
replaced with revised version Wed, 3 Apr 2024 21:55:46 GMT   (10149kb,D)

Title: Adversarial Evasion Attacks Practicality in Networks: Testing the Impact
  of Dynamic Learning
Authors: Mohamed el Shehaby and Ashraf Matrawy
Categories: cs.CR cs.LG cs.NI
\\ ( https://arxiv.org/abs/2306.05494 ,  10149kb)
------------------------------------------------------------------------------
\\
arXiv:2306.12965 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 23:01:53 GMT   (3693kb,D)

Title: Improved Financial Forecasting via Quantum Machine Learning
Authors: Sohum Thakkar (1), Skander Kazdaghli (1), Natansh Mathur (1 and 2),
  Iordanis Kerenidis (1 and 2), Andr\'e J. Ferreira-Martins (3), Samurai Brito
  (3) ((1) QC Ware Corp, (2) IRIF - Universit\'e Paris Cit\'e and CNRS, (3)
  Ita\'u Unibanco)
Categories: q-fin.ST cs.LG quant-ph
Comments: The version of record of this article was submitted for publication
  in Quantum Machine Intelligence (https://link.springer.com/journal/42484)
\\ ( https://arxiv.org/abs/2306.12965 ,  3693kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17825
replaced with revised version Thu, 4 Apr 2024 00:00:12 GMT   (6146kb,D)

Title: Scalable tensor methods for nonuniform hypergraphs
Authors: Sinan G. Aksoy, Ilya Amburg, Stephen J. Young
Categories: math.NA cs.LG cs.NA cs.SI math.CO physics.soc-ph
MSC-class: 05C65, 15A69, 05C50, 05C85
\\ ( https://arxiv.org/abs/2306.17825 ,  6146kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02633 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 16:27:08 GMT   (6583kb,D)

Title: Hybrid Ground-State Quantum Algorithms based on Neural Schr\"odinger
  Forging
Authors: Paulin de Schoulepnikoff, Oriel Kiss, Sofia Vallecorsa, Giuseppe
  Carleo and Michele Grossi
Categories: quant-ph cond-mat.stat-mech cs.LG
Comments: 12 pages, 9 figures, 5 pages supplemental material
Journal-ref: Phys. Rev. Research 6, 023021 (2024)
DOI: 10.1103/PhysRevResearch.6.023021
\\ ( https://arxiv.org/abs/2307.02633 ,  6583kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12301
replaced with revised version Thu, 4 Apr 2024 04:11:05 GMT   (5588kb,D)

Title: Image Outlier Detection Without Training using RANSAC
Authors: Chen-Han Tsai, Yu-Shao Peng
Categories: cs.CV cs.IR cs.LG
\\ ( https://arxiv.org/abs/2307.12301 ,  5588kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05780
replaced with revised version Thu, 4 Apr 2024 01:43:42 GMT   (2982kb,D)

Title: Real-time Control of Electric Autonomous Mobility-on-Demand Systems via
  Graph Reinforcement Learning
Authors: Aaryan Singhal, Daniele Gammelli, Justin Luke, Karthik Gopalakrishnan,
  Dominik Helmreich, Marco Pavone
Categories: eess.SY cs.LG cs.RO cs.SY
Comments: 9 pages, revised SF travel data, includes additional experimental
  results, content and clarification revisions per reviewer feedback, and typo
  fixes
\\ ( https://arxiv.org/abs/2311.05780 ,  2982kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04542
replaced with revised version Thu, 4 Apr 2024 16:43:44 GMT   (1120kb,D)

Title: SoK: Unintended Interactions among Machine Learning Defenses and Risks
Authors: Vasisht Duddu, Sebastian Szyller, N. Asokan
Categories: cs.CR cs.LG
Comments: IEEE Symposium on Security and Privacy (S&P) 2024
\\ ( https://arxiv.org/abs/2312.04542 ,  1120kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11337 (*cross-listing*)
replaced with revised version Thu, 4 Apr 2024 14:26:06 GMT   (513kb,D)

Title: Challenges for Reinforcement Learning in Quantum Circuit Design
Authors: Philipp Altmann, Jonas Stein, Michael K\"olle, Adelina B\"arligea,
  Thomas Gabor, Thomy Phan, Sebastian Feld, Claudia Linnhoff-Popien
Categories: quant-ph cs.LG
Comments: 10 pages, 3 figures
\\ ( https://arxiv.org/abs/2312.11337 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01275
replaced with revised version Thu, 4 Apr 2024 14:21:13 GMT   (8934kb,D)

Title: Parametric-Task MAP-Elites
Authors: Timoth\'ee Anne, Jean-Baptiste Mouret
Categories: cs.NE cs.LG
DOI: 10.1145/3638529.3653993
\\ ( https://arxiv.org/abs/2402.01275 ,  8934kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02795
replaced with revised version Thu, 4 Apr 2024 02:41:11 GMT   (213kb,D)

Title: A Learning-Based Caching Mechanism for Edge Content Delivery
Authors: Hoda Torabi, Hamzeh Khazaei, Marin Litoiu
Categories: cs.NI cs.DC cs.LG
DOI: 10.1145/3629526.3645037
\\ ( https://arxiv.org/abs/2402.02795 ,  213kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05141 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 21:25:49 GMT   (498kb,D)

Title: Tensor Completion via Integer Optimization
Authors: Xin Chen, Sukanya Kudva, Yongzheng Dai, Anil Aswani, Chen Chen
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2402.05141 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14817
replaced with revised version Thu, 4 Apr 2024 16:27:06 GMT   (8469kb,D)

Title: Cameras as Rays: Pose Estimation via Ray Diffusion
Authors: Jason Y. Zhang, Amy Lin, Moneish Kumar, Tzu-Hsuan Yang, Deva Ramanan,
  Shubham Tulsiani
Categories: cs.CV cs.LG
Comments: In ICLR 2024 (oral). v2-3: updated references. Project webpage:
  https://jasonyzhang.com/RayDiffusion
\\ ( https://arxiv.org/abs/2402.14817 ,  8469kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03149
replaced with revised version Thu, 4 Apr 2024 05:23:39 GMT   (130kb,D)

Title: Robust Federated Learning Mitigates Client-side Training Data
  Distribution Inference Attacks
Authors: Yichang Xu, Ming Yin, Minghong Fang, Neil Zhenqiang Gong
Categories: cs.CR cs.DC cs.LG
Comments: To appear in The Web Conference 2024 (WWW '24)
\\ ( https://arxiv.org/abs/2403.03149 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07648
replaced with revised version Wed, 3 Apr 2024 20:07:33 GMT   (2710kb,D)

Title: Characterization of Large Language Model Development in the Datacenter
Authors: Qinghao Hu, Zhisheng Ye, Zerui Wang, Guoteng Wang, Meng Zhang,
  Qiaoling Chen, Peng Sun, Dahua Lin, Xiaolin Wang, Yingwei Luo, Yonggang Wen,
  Tianwei Zhang
Categories: cs.DC cs.LG
\\ ( https://arxiv.org/abs/2403.07648 ,  2710kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17729
replaced with revised version Thu, 4 Apr 2024 14:29:34 GMT   (6004kb,D)

Title: EulerFormer: Sequential User Behavior Modeling with Complex Vector
  Attention
Authors: Zhen Tian, Wayne Xin Zhao, Changwang Zhang, Xin Zhao, Zhongrui Ma and
  Ji-Rong Wen
Categories: cs.IR cs.LG
Comments: Accepted for publication in SIGIR'24
\\ ( https://arxiv.org/abs/2403.17729 ,  6004kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19612
replaced with revised version Thu, 4 Apr 2024 14:44:23 GMT   (11952kb,D)

Title: ILPO-NET: Network for the invariant recognition of arbitrary volumetric
  patterns in 3D
Authors: Dmitrii Zhemchuzhnikov and Sergei Grudinin
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.19612 ,  11952kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01436 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 21:08:40 GMT   (262kb)

Title: Convergence Guarantees for RMSProp and Adam in Generalized-smooth
  Non-convex Optimization with Affine Noise Variance
Authors: Qi Zhang, Yi Zhou, Shaofeng Zou
Categories: stat.ML cs.LG math.OC
\\ ( https://arxiv.org/abs/2404.01436 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01518
replaced with revised version Thu, 4 Apr 2024 02:06:15 GMT   (1197kb,D)

Title: Temporally Consistent Unbalanced Optimal Transport for Unsupervised
  Action Segmentation
Authors: Ming Xu, Stephen Gould
Categories: cs.CV cs.LG eess.IV
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2404.01518 ,  1197kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02072
replaced with revised version Thu, 4 Apr 2024 00:59:51 GMT   (4540kb,D)

Title: EGTR: Extracting Graph from Transformer for Scene Graph Generation
Authors: Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2404.02072 ,  4540kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
