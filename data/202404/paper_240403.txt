Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年4月3日 13:14
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon  1 Apr 24 18:00:00 GMT  to  Tue  2 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.01308
Date: Mon, 4 Mar 2024 08:38:55 GMT   (77kb,D)

Title: Learning to Solve Job Shop Scheduling under Uncertainty
Authors: Guillaume Infantes and St\'ephanie Roussel and Pierre Pereira and
  Antoine Jacquet and Emmanuel Benazera
Categories: cs.AI cs.LG stat.ML
Comments: To be published at CPAIOR 2024
\\
  Job-Shop Scheduling Problem (JSSP) is a combinatorial optimization problem
where tasks need to be scheduled on machines in order to minimize criteria such
as makespan or delay. To address more realistic scenarios, we associate a
probability distribution with the duration of each task. Our objective is to
generate a robust schedule, i.e. that minimizes the average makespan. This
paper introduces a new approach that leverages Deep Reinforcement Learning
(DRL) techniques to search for robust solutions, emphasizing JSSPs with
uncertain durations. Key contributions of this research include: (1)
advancements in DRL applications to JSSPs, enhancing generalization and
scalability, (2) a novel method for addressing JSSPs with uncertain durations.
The Wheatley approach, which integrates Graph Neural Networks (GNNs) and DRL,
is made publicly available for further research and applications.
\\ ( https://arxiv.org/abs/2404.01308 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01503
Date: Mon, 1 Apr 2024 22:10:12 GMT   (191kb,D)

Title: Some Orders Are Important: Partially Preserving Orders in Top-Quality
  Planning
Authors: Michael Katz, Junkyu Lee, Jungkoo Kang, Shirin Sohrabi
Categories: cs.AI
Comments: To appear at SoCS 2024
\\
  The ability to generate multiple plans is central to using planning in
real-life applications. Top-quality planners generate sets of such top-cost
plans, allowing flexibility in determining equivalent ones. In terms of the
order between actions in a plan, the literature only considers two extremes --
either all orders are important, making each plan unique, or all orders are
unimportant, treating two plans differing only in the order of actions as
equivalent. To allow flexibility in selecting important orders, we propose
specifying a subset of actions the orders between which are important,
interpolating between the top-quality and unordered top-quality planning
problems. We explore the ways of adapting partial order reduction search
pruning techniques to address this new computational problem and present
experimental evaluations demonstrating the benefits of exploiting such
techniques in this setting.
\\ ( https://arxiv.org/abs/2404.01503 ,  191kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01526
Date: Mon, 1 Apr 2024 23:19:01 GMT   (1472kb)

Title: Categorical semiotics: Foundations for Knowledge Integration
Authors: Carlos Leandro
Categories: cs.AI
Comments: 71 pages, 15 figures. arXiv admin note: substantial text overlap with
  arXiv:1604.02790
\\
  The integration of knowledge extracted from diverse models, whether described
by domain experts or generated by machine learning algorithms, has historically
been challenged by the absence of a suitable framework for specifying and
integrating structures, learning processes, data transformations, and data
models or rules. In this work, we extend algebraic specification methods to
address these challenges within such a framework.
  In our work, we tackle the challenging task of developing a comprehensive
framework for defining and analyzing deep learning architectures. We believe
that previous efforts have fallen short by failing to establish a clear
connection between the constraints a model must adhere to and its actual
implementation.
  Our methodology employs graphical structures that resemble Ehresmann's
sketches, interpreted within a universe of fuzzy sets. This approach offers a
unified theory that elegantly encompasses both deterministic and
non-deterministic neural network designs. Furthermore, we highlight how this
theory naturally incorporates fundamental concepts from computer science and
automata theory. Our extended algebraic specification framework, grounded in
graphical structures akin to Ehresmann's sketches, offers a promising solution
for integrating knowledge across disparate models and domains. By bridging the
gap between domain-specific expertise and machine-generated insights, we pave
the way for more comprehensive, collaborative, and effective approaches to
knowledge integration and modeling.
\\ ( https://arxiv.org/abs/2404.01526 ,  1472kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01677
Date: Tue, 2 Apr 2024 06:28:44 GMT   (289kb,D)

Title: Towards Generalizable and Faithful Logic Reasoning over Natural Language
  via Resolution Refutation
Authors: Zhouhao Sun, Xiao Ding, Li Du, Bibo Cai, Jinglong Gao, Ting Liu, Qin
  Bing
Categories: cs.AI cs.CL
Comments: Coling 2024
\\
  Large language models (LLMs) have achieved significant performance in various
natural language reasoning tasks. However, they still struggle with performing
first-order logic reasoning over formal logical theories expressed in natural
language. This is because the previous LLMs-based reasoning systems have the
theoretical incompleteness issue. As a result, it can only address a limited
set of simple reasoning problems, which significantly decreases their
generalization ability. To address this issue, we propose a novel framework,
named Generalizable and Faithful Reasoner (GFaiR), which introduces the
paradigm of resolution refutation. Resolution refutation has the capability to
solve all first-order logic reasoning problems by extending reasoning rules and
employing the principle of proof by contradiction, so our system's completeness
can be improved by introducing resolution refutation. Experimental results
demonstrate that our system outperforms previous works by achieving
state-of-the-art performances in complex scenarios while maintaining
performances in simple scenarios. Besides, we observe that GFaiR is faithful to
its reasoning process.
\\ ( https://arxiv.org/abs/2404.01677 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01794
Date: Tue, 2 Apr 2024 09:55:30 GMT   (416kb,D)

Title: Imitation Game: A Model-based and Imitation Learning Deep Reinforcement
  Learning Hybrid
Authors: Eric MSP Veith, Torben Logemann, Aleksandr Berezin, Arlena
  Well{\ss}ow, Stephan Balduin
Categories: cs.AI
Comments: Accepted as publication at MSCPES '24
\\
  Autonomous and learning systems based on Deep Reinforcement Learning have
firmly established themselves as a foundation for approaches to creating
resilient and efficient Cyber-Physical Energy Systems. However, most current
approaches suffer from two distinct problems: Modern model-free algorithms such
as Soft Actor Critic need a high number of samples to learn a meaningful
policy, as well as a fallback to ward against concept drifts (e. g.,
catastrophic forgetting). In this paper, we present the work in progress
towards a hybrid agent architecture that combines model-based Deep
Reinforcement Learning with imitation learning to overcome both problems.
\\ ( https://arxiv.org/abs/2404.01794 ,  416kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02039
Date: Tue, 2 Apr 2024 15:34:18 GMT   (1225kb,D)

Title: A Survey on Large Language Model-Based Game Agents
Authors: Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu,
  Ramana Kompella, Ling Liu
Categories: cs.AI
\\
  The development of game agents holds a critical role in advancing towards
Artificial General Intelligence (AGI). The progress of LLMs and their
multimodal counterparts (MLLMs) offers an unprecedented opportunity to evolve
and empower game agents with human-like decision-making capabilities in complex
computer game environments. This paper provides a comprehensive overview of
LLM-based game agents from a holistic viewpoint. First, we introduce the
conceptual architecture of LLM-based game agents, centered around six essential
functional components: perception, memory, thinking, role-playing, action, and
learning. Second, we survey existing representative LLM-based game agents
documented in the literature with respect to methodologies and adaptation
agility across six genres of games, including adventure, communication,
competition, cooperation, simulation, and crafting & exploration games.
Finally, we present an outlook of future research and development directions in
this burgeoning field. A curated list of relevant papers is maintained and made
accessible at: https://github.com/git-disl/awesome-LLM-game-agent-papers.
\\ ( https://arxiv.org/abs/2404.02039 ,  1225kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02078
Date: Tue, 2 Apr 2024 16:25:30 GMT   (1367kb,D)

Title: Advancing LLM Reasoning Generalists with Preference Trees
Authors: Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng,
  Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou,
  Hao Peng, Zhiyuan Liu, Maosong Sun
Categories: cs.AI cs.CL cs.LG
Comments: Models and data are available at https://github.com/OpenBMB/Eurus
\\
  We introduce Eurus, a suite of large language models (LLMs) optimized for
reasoning. Finetuned from Mistral-7B and CodeLlama-70B, Eurus models achieve
state-of-the-art results among open-source models on a diverse set of
benchmarks covering mathematics, code generation, and logical reasoning
problems. Notably, Eurus-70B beats GPT-3.5 Turbo in reasoning through a
comprehensive benchmarking across 12 tests covering five tasks, and achieves a
33.3% pass@1 accuracy on LeetCode and 32.6% on TheoremQA, two challenging
benchmarks, substantially outperforming existing open-source models by margins
more than 13.3%. The strong performance of Eurus can be primarily attributed to
UltraInteract, our newly-curated large-scale, high-quality alignment dataset
specifically designed for complex reasoning tasks. UltraInteract can be used in
both supervised fine-tuning and preference learning. For each instruction, it
includes a preference tree consisting of (1) reasoning chains with diverse
planning strategies in a unified format, (2) multi-turn interaction
trajectories with the environment and the critique, and (3) pairwise data to
facilitate preference learning. UltraInteract allows us to conduct an in-depth
exploration of preference learning for reasoning tasks. Our investigation
reveals that some well-established preference learning algorithms may be less
suitable for reasoning tasks compared to their effectiveness in general
conversations. Inspired by this, we derive a novel reward modeling objective
which, together with UltraInteract, leads to a strong reward model.
\\ ( https://arxiv.org/abs/2404.02078 ,  1367kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01317
Date: Wed, 27 Mar 2024 13:40:09 GMT   (234kb,D)

Title: Intelligent Learning Rate Distribution to reduce Catastrophic Forgetting
  in Transformers
Authors: Philip Kenneweg, Alexander Schulz, Sarah Schr\"oder, Barbara Hammer
Categories: cs.CL cs.AI cs.LG
DOI: 10.1007/978-3-031-21753-1_25
\\
  Pretraining language models on large text corpora is a common practice in
natural language processing. Fine-tuning of these models is then performed to
achieve the best results on a variety of tasks. In this paper, we investigate
the problem of catastrophic forgetting in transformer neural networks and
question the common practice of fine-tuning with a flat learning rate for the
entire network in this context. We perform a hyperparameter optimization
process to find learning rate distributions that are better than a flat
learning rate. We combine the learning rate distributions thus found and show
that they generalize to better performance with respect to the problem of
catastrophic forgetting. We validate these learning rate distributions with a
variety of NLP benchmarks from the GLUE dataset.
\\ ( https://arxiv.org/abs/2404.01317 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01322
Date: Thu, 28 Mar 2024 15:53:45 GMT   (2871kb,D)

Title: A Review of Multi-Modal Large Language and Vision Models
Authors: Kilian Carolan and Laura Fennelly and Alan F. Smeaton
Categories: cs.CL cs.AI
Comments: 33 pages, 1 figure
\\
  Large Language Models (LLMs) have recently emerged as a focal point of
research and application, driven by their unprecedented ability to understand
and generate text with human-like quality. Even more recently, LLMs have been
extended into multi-modal large language models (MM-LLMs) which extends their
capabilities to deal with image, video and audio information, in addition to
text. This opens up applications like text-to-video generation, image
captioning, text-to-speech, and more and is achieved either by retro-fitting an
LLM with multi-modal capabilities, or building a MM-LLM from scratch. This
paper provides an extensive review of the current state of those LLMs with
multi-modal capabilities as well as the very recent MM-LLMs. It covers the
historical development of LLMs especially the advances enabled by
transformer-based architectures like OpenAI's GPT series and Google's BERT, as
well as the role of attention mechanisms in enhancing model performance. The
paper includes coverage of the major and most important of the LLMs and MM-LLMs
and also covers the techniques of model tuning, including fine-tuning and
prompt engineering, which tailor pre-trained models to specific tasks or
domains. Ethical considerations and challenges, such as data bias and model
misuse, are also analysed to underscore the importance of responsible AI
development and deployment. Finally, we discuss the implications of open-source
versus proprietary models in AI research. Through this review, we provide
insights into the transformative potential of MM-LLMs in various applications.
\\ ( https://arxiv.org/abs/2404.01322 ,  2871kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01327
Date: Fri, 29 Mar 2024 12:10:21 GMT   (1611kb,D)

Title: Entertainment chatbot for the digital inclusion of elderly people
  without abstraction capabilities
Authors: Silvia Garc\'ia-M\'endez, Francisco de Arriba-P\'erez, Francisco J.
  Gonz\'alez-Casta\~no, Jos\'e A. Regueiro-Janeiro, Felipe Gil-Casti\~neira
Categories: cs.CL cs.AI cs.HC cs.LG
DOI: 10.1109/ACCESS.2021.3080837
\\
  Current language processing technologies allow the creation of conversational
chatbot platforms. Even though artificial intelligence is still too immature to
support satisfactory user experience in many mass market domains,
conversational interfaces have found their way into ad hoc applications such as
call centres and online shopping assistants. However, they have not been
applied so far to social inclusion of elderly people, who are particularly
vulnerable to the digital divide. Many of them relieve their loneliness with
traditional media such as TV and radio, which are known to create a feeling of
companionship. In this paper we present the EBER chatbot, designed to reduce
the digital gap for the elderly. EBER reads news in the background and adapts
its responses to the user's mood. Its novelty lies in the concept of
"intelligent radio", according to which, instead of simplifying a digital
information system to make it accessible to the elderly, a traditional channel
they find familiar -- background news -- is augmented with interactions via
voice dialogues. We make it possible by combining Artificial Intelligence
Modelling Language, automatic Natural Language Generation and Sentiment
Analysis. The system allows accessing digital content of interest by combining
words extracted from user answers to chatbot questions with keywords extracted
from the news items. This approach permits defining metrics of the abstraction
capabilities of the users depending on a spatial representation of the word
space. To prove the suitability of the proposed solution we present results of
real experiments conducted with elderly people that provided valuable insights.
Our approach was considered satisfactory during the tests and improved the
information search capabilities of the participants.
\\ ( https://arxiv.org/abs/2404.01327 ,  1611kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01331
Date: Fri, 29 Mar 2024 21:32:50 GMT   (608kb,D)

Title: LLaVA-Gemma: Accelerating Multimodal Foundation Models with a Compact
  Language Model
Authors: Musashi Hinck, Matthew L. Olson, David Cobbley, Shao-Yen Tseng,
  Vasudev Lal
Categories: cs.CL cs.AI
Comments: Authors 1 and 2 contributed equally. Models available at
  https://huggingface.co/intel/llava-gemma-2b/ and
  \url{https://huggingface.co/intel/llava-gemma-7b/
\\
  We train a suite of multimodal foundation models (MMFM) using the popular
LLaVA framework with the recently released Gemma family of large language
models (LLMs). Of particular interest is the 2B parameter Gemma model, which
provides opportunities to construct capable small-scale MMFMs. In line with
findings from other papers in this space, we test the effect of ablating three
design features: pretraining the connector, utilizing a more powerful image
backbone, and increasing the size of the language backbone. The resulting
models, which we call LLaVA-Gemma, exhibit moderate performance on an array of
evaluations, but fail to improve past the current comparably sized SOTA models.
Closer analysis of performance shows mixed effects; skipping pretraining tends
to reduce performance, larger vision models sometimes improve performance, and
increasing language model size has inconsistent effects. We publicly release
training recipes, code and weights for our models for the LLaVA-Gemma models.
\\ ( https://arxiv.org/abs/2404.01331 ,  608kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01332
Date: Fri, 29 Mar 2024 22:49:43 GMT   (3929kb)

Title: Wait, It's All Token Noise? Always Has Been: Interpreting LLM Behavior
  Using Shapley Value
Authors: Behnam Mohammadi
Categories: cs.CL cs.AI cs.LG
\\
  The emergence of large language models (LLMs) has opened up exciting
possibilities for simulating human behavior and cognitive processes, with
potential applications in various domains, including marketing research and
consumer behavior analysis. However, the validity of utilizing LLMs as
stand-ins for human subjects remains uncertain due to glaring divergences that
suggest fundamentally different underlying processes at play and the
sensitivity of LLM responses to prompt variations. This paper presents a novel
approach based on Shapley values from cooperative game theory to interpret LLM
behavior and quantify the relative contribution of each prompt component to the
model's output. Through two applications-a discrete choice experiment and an
investigation of cognitive biases-we demonstrate how the Shapley value method
can uncover what we term "token noise" effects, a phenomenon where LLM
decisions are disproportionately influenced by tokens providing minimal
informative content. This phenomenon raises concerns about the robustness and
generalizability of insights obtained from LLMs in the context of human
behavior simulation. Our model-agnostic approach extends its utility to
proprietary LLMs, providing a valuable tool for marketers and researchers to
strategically optimize prompts and mitigate apparent cognitive biases. Our
findings underscore the need for a more nuanced understanding of the factors
driving LLM responses before relying on them as substitutes for human subjects
in research settings. We emphasize the importance of researchers reporting
results conditioned on specific prompt templates and exercising caution when
drawing parallels between human behavior and LLMs.
\\ ( https://arxiv.org/abs/2404.01332 ,  3929kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01334
Date: Sat, 30 Mar 2024 12:13:57 GMT   (8094kb,D)

Title: Augmenting NER Datasets with LLMs: Towards Automated and Refined
  Annotation
Authors: Yuji Naraki, Ryosuke Yamaki, Yoshikazu Ikeda, Takafumi Horie, Hiroki
  Naganuma
Categories: cs.CL cs.LG
\\
  In the field of Natural Language Processing (NLP), Named Entity Recognition
(NER) is recognized as a critical technology, employed across a wide array of
applications. Traditional methodologies for annotating datasets for NER models
are challenged by high costs and variations in dataset quality. This research
introduces a novel hybrid annotation approach that synergizes human effort with
the capabilities of Large Language Models (LLMs). This approach not only aims
to ameliorate the noise inherent in manual annotations, such as omissions,
thereby enhancing the performance of NER models, but also achieves this in a
cost-effective manner. Additionally, by employing a label mixing strategy, it
addresses the issue of class imbalance encountered in LLM-based annotations.
Through an analysis across multiple datasets, this method has been consistently
shown to provide superior performance compared to traditional annotation
methods, even under constrained budget conditions. This study illuminates the
potential of leveraging LLMs to improve dataset quality, introduces a novel
technique to mitigate class imbalances, and demonstrates the feasibility of
achieving high-performance NER in a cost-effective way.
\\ ( https://arxiv.org/abs/2404.01334 ,  8094kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01336
Date: Sat, 30 Mar 2024 14:39:09 GMT   (12049kb,D)

Title: FineFake: A Knowledge-Enriched Dataset for Fine-Grained Multi-Domain
  Fake News Detecction
Authors: Ziyi Zhou, Xiaoming Zhang, Litian Zhang, Jiacheng Liu, Xi Zhang,
  Chaozhuo Li
Categories: cs.CL cs.AI cs.MM
\\
  Existing benchmarks for fake news detection have significantly contributed to
the advancement of models in assessing the authenticity of news content.
However, these benchmarks typically focus solely on news pertaining to a single
semantic topic or originating from a single platform, thereby failing to
capture the diversity of multi-domain news in real scenarios. In order to
understand fake news across various domains, the external knowledge and
fine-grained annotations are indispensable to provide precise evidence and
uncover the diverse underlying strategies for fabrication, which are also
ignored by existing benchmarks. To address this gap, we introduce a novel
multi-domain knowledge-enhanced benchmark with fine-grained annotations, named
\textbf{FineFake}. FineFake encompasses 16,909 data samples spanning six
semantic topics and eight platforms. Each news item is enriched with
multi-modal content, potential social context, semi-manually verified common
knowledge, and fine-grained annotations that surpass conventional binary
labels. Furthermore, we formulate three challenging tasks based on FineFake and
propose a knowledge-enhanced domain adaptation network. Extensive experiments
are conducted on FineFake under various scenarios, providing accurate and
reliable benchmarks for future endeavors. The entire FineFake project is
publicly accessible as an open-source repository at
\url{https://github.com/Accuser907/FineFake}.
\\ ( https://arxiv.org/abs/2404.01336 ,  12049kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01337
Date: Sat, 30 Mar 2024 16:40:10 GMT   (857kb,D)

Title: Detection of Temporality at Discourse Level on Financial News by
  Combining Natural Language Processing and Machine Learning
Authors: Silvia Garc\'ia-M\'endez, Francisco de Arriba-P\'erez, Ana
  Barros-Vila, Francisco J. Gonz\'alez-Casta\~no
Categories: cs.CL cs.CE cs.IR cs.LG q-fin.ST
DOI: 10.1016/j.eswa.2022.116648
\\
  Finance-related news such as Bloomberg News, CNN Business and Forbes are
valuable sources of real data for market screening systems. In news, an expert
shares opinions beyond plain technical analyses that include context such as
political, sociological and cultural factors. In the same text, the expert
often discusses the performance of different assets. Some key statements are
mere descriptions of past events while others are predictions. Therefore,
understanding the temporality of the key statements in a text is essential to
separate context information from valuable predictions. We propose a novel
system to detect the temporality of finance-related news at discourse level
that combines Natural Language Processing and Machine Learning techniques, and
exploits sophisticated features such as syntactic and semantic dependencies.
More specifically, we seek to extract the dominant tenses of the main
statements, which may be either explicit or implicit. We have tested our system
on a labelled dataset of finance-related news annotated by researchers with
knowledge in the field. Experimental results reveal a high detection precision
compared to an alternative rule-based baseline approach. Ultimately, this
research contributes to the state-of-the-art of market screening by identifying
predictive knowledge for financial decision making.
\\ ( https://arxiv.org/abs/2404.01337 ,  857kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01338
Date: Sat, 30 Mar 2024 17:49:34 GMT   (558kb,D)

Title: Automatic detection of relevant information, predictions and forecasts
  in financial news through topic modelling with Latent Dirichlet Allocation
Authors: Silvia Garc\'ia-M\'endez, Francisco de Arriba-P\'erez, Ana
  Barros-Vila, Francisco J. Gonz\'alez-Casta\~no, Enrique Costa-Montenegro
Categories: cs.CL cs.CE cs.IR cs.LG q-fin.ST
DOI: 10.1007/s10489-023-04452-4
\\
  Financial news items are unstructured sources of information that can be
mined to extract knowledge for market screening applications. Manual extraction
of relevant information from the continuous stream of finance-related news is
cumbersome and beyond the skills of many investors, who, at most, can follow a
few sources and authors. Accordingly, we focus on the analysis of financial
news to identify relevant text and, within that text, forecasts and
predictions. We propose a novel Natural Language Processing (NLP) system to
assist investors in the detection of relevant financial events in unstructured
textual sources by considering both relevance and temporality at the discursive
level. Firstly, we segment the text to group together closely related text.
Secondly, we apply co-reference resolution to discover internal dependencies
within segments. Finally, we perform relevant topic modelling with Latent
Dirichlet Allocation (LDA) to separate relevant from less relevant text and
then analyse the relevant text using a Machine Learning-oriented temporal
approach to identify predictions and speculative statements. We created an
experimental data set composed of 2,158 financial news items that were manually
labelled by NLP researchers to evaluate our solution. The ROUGE-L values for
the identification of relevant text and predictions/forecasts were 0.662 and
0.982, respectively. To our knowledge, this is the first work to jointly
consider relevance and temporality at the discursive level. It contributes to
the transfer of human associative discourse capabilities to expert systems
through the combination of multi-paragraph topic segmentation and co-reference
resolution to separate author expression patterns, topic modelling with LDA to
detect relevant text, and discursive temporality analysis to identify forecasts
and predictions within this text.
\\ ( https://arxiv.org/abs/2404.01338 ,  558kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01339
Date: Sun, 31 Mar 2024 00:38:02 GMT   (994kb,D)

Title: Humane Speech Synthesis through Zero-Shot Emotion and Disfluency
  Generation
Authors: Rohan Chaudhury, Mihir Godbole, Aakash Garg, Jinsil Hwaryoung Seo
Categories: cs.CL cs.AI cs.HC
Comments: 10 pages, 1 figure, for associated code and media files, see
  https://github.com/Rohan-Chaudhury/Humane-Speech-Synthesis-through-Zero-Shot-Emotion-and-Disfluency-Generation
\\
  Contemporary conversational systems often present a significant limitation:
their responses lack the emotional depth and disfluent characteristic of human
interactions. This absence becomes particularly noticeable when users seek more
personalized and empathetic interactions. Consequently, this makes them seem
mechanical and less relatable to human users. Recognizing this gap, we embarked
on a journey to humanize machine communication, to ensure AI systems not only
comprehend but also resonate. To address this shortcoming, we have designed an
innovative speech synthesis pipeline. Within this framework, a cutting-edge
language model introduces both human-like emotion and disfluencies in a
zero-shot setting. These intricacies are seamlessly integrated into the
generated text by the language model during text generation, allowing the
system to mirror human speech patterns better, promoting more intuitive and
natural user interactions. These generated elements are then adeptly
transformed into corresponding speech patterns and emotive sounds using a
rule-based approach during the text-to-speech phase. Based on our experiments,
our novel system produces synthesized speech that's almost indistinguishable
from genuine human communication, making each interaction feel more personal
and authentic.
\\ ( https://arxiv.org/abs/2404.01339 ,  994kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01342
Date: Sun, 31 Mar 2024 06:28:15 GMT   (42619kb,D)

Title: DiffAgent: Fast and Accurate Text-to-Image API Selection with Large
  Language Model
Authors: Lirui Zhao, Yue Yang, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao,
  Ping Luo, Rongrong Ji
Categories: cs.CL cs.AI
Comments: Published as a conference paper at CVPR 2024
\\
  Text-to-image (T2I) generative models have attracted significant attention
and found extensive applications within and beyond academic research. For
example, the Civitai community, a platform for T2I innovation, currently hosts
an impressive array of 74,492 distinct models. However, this diversity presents
a formidable challenge in selecting the most appropriate model and parameters,
a process that typically requires numerous trials. Drawing inspiration from the
tool usage research of large language models (LLMs), we introduce DiffAgent, an
LLM agent designed to screen the accurate selection in seconds via API calls.
DiffAgent leverages a novel two-stage training framework, SFTA, enabling it to
accurately align T2I API responses with user input in accordance with human
preferences. To train and evaluate DiffAgent's capabilities, we present
DABench, a comprehensive dataset encompassing an extensive range of T2I APIs
from the community. Our evaluations reveal that DiffAgent not only excels in
identifying the appropriate T2I API but also underscores the effectiveness of
the SFTA training framework. Codes are available at
https://github.com/OpenGVLab/DiffAgent.
\\ ( https://arxiv.org/abs/2404.01342 ,  42619kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01343
Date: Sun, 31 Mar 2024 07:11:48 GMT   (3723kb,D)

Title: CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
Authors: Jingzhe Shi, Jialuo Li, Qinwei Ma, Zaiwen Yang, Huan Ma, Lei Li
Categories: cs.CL cs.AI
Comments: 14 pages
\\
  Businesses and software platforms are increasingly turning to Large Language
Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance
with file access or as reasoning agents for customer service. However, current
LLM-based customer service models have limited integration with customer
profiles and lack the operational capabilities necessary for effective service.
Moreover, existing API integrations emphasize diversity over the precision and
error avoidance essential in real-world customer service scenarios. To address
these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile
in existing System), designed to: (1) efficiently utilize existing databases or
systems for accessing user information or interacting with these systems
following existing guidelines; (2) provide accurate and reasonable responses or
carry out required operations in the system while avoiding harmful operations;
and (3) leverage a combination of small and large LLMs to achieve satisfying
performance at a reasonable inference cost. We introduce a practical dataset,
the CPHOS-dataset, which includes a database, guiding files, and QA pairs
collected from CPHOS, an online platform that facilitates the organization of
simulated Physics Olympiads for high school teachers and students. We have
conducted extensive experiments to validate the performance of our proposed
CHOPS architecture using the CPHOS-dataset, with the aim of demonstrating how
LLMs can enhance or serve as alternatives to human customer service. Our code
and dataset will be open-sourced soon.
\\ ( https://arxiv.org/abs/2404.01343 ,  3723kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01344
Date: Sun, 31 Mar 2024 08:10:45 GMT   (1126kb,D)

Title: Mind Your Neighbours: Leveraging Analogous Instances for Rhetorical Role
  Labeling for Legal Documents
Authors: T.Y.S.S Santosh, Hassan Sarwat, Ahmed Abdou, Matthias Grabmair
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Rhetorical Role Labeling (RRL) of legal judgments is essential for various
tasks, such as case summarization, semantic search and argument mining.
However, it presents challenges such as inferring sentence roles from context,
interrelated roles, limited annotated data, and label imbalance. This study
introduces novel techniques to enhance RRL performance by leveraging knowledge
from semantically similar instances (neighbours). We explore inference-based
and training-based approaches, achieving remarkable improvements in challenging
macro-F1 scores. For inference-based methods, we explore interpolation
techniques that bolster label predictions without re-training. While in
training-based methods, we integrate prototypical learning with our novel
discourse-aware contrastive method that work directly on embedding spaces.
Additionally, we assess the cross-domain applicability of our methods,
demonstrating their effectiveness in transferring knowledge across diverse
legal domains.
\\ ( https://arxiv.org/abs/2404.01344 ,  1126kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01345
Date: Sun, 31 Mar 2024 09:52:25 GMT   (651kb)

Title: Enhancing Bangla Fake News Detection Using Bidirectional Gated Recurrent
  Units and Deep Learning Techniques
Authors: Utsha Roy, Mst. Sazia Tahosin, Md. Mahedi Hassan, Taminul Islam, Fahim
  Imtiaz, Md Rezwane Sadik, Yassine Maleh, Rejwan Bin Sulaiman, Md. Simul Hasan
  Talukder
Categories: cs.CL cs.LG
Comments: Accepted for publication in the 7th International Conference on
  Networking, Intelligent Systems & Security. The conference Proceedings will
  be published by ACM International Conference Proceeding Series (ICPS) ISBN
  N{\deg}: 979-8-4007-0019-4. 8 pages, 11 figures
\\
  The rise of fake news has made the need for effective detection methods,
including in languages other than English, increasingly important. The study
aims to address the challenges of Bangla which is considered a less important
language. To this end, a complete dataset containing about 50,000 news items is
proposed. Several deep learning models have been tested on this dataset,
including the bidirectional gated recurrent unit (GRU), the long short-term
memory (LSTM), the 1D convolutional neural network (CNN), and hybrid
architectures. For this research, we assessed the efficacy of the model
utilizing a range of useful measures, including recall, precision, F1 score,
and accuracy. This was done by employing a big application. We carry out
comprehensive trials to show the effectiveness of these models in identifying
bogus news in Bangla, with the Bidirectional GRU model having a stunning
accuracy of 99.16%. Our analysis highlights the importance of dataset balance
and the need for continual improvement efforts to a substantial degree. This
study makes a major contribution to the creation of Bangla fake news detecting
systems with limited resources, thereby setting the stage for future
improvements in the detection process.
\\ ( https://arxiv.org/abs/2404.01345 ,  651kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01349
Date: Sun, 31 Mar 2024 22:22:53 GMT   (3175kb,D)

Title: Fairness in Large Language Models: A Taxonomic Survey
Authors: Zhibo Chu, Zichong Wang, Wenbin Zhang
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have demonstrated remarkable success across
various domains. However, despite their promising performance in numerous
real-world applications, most of these algorithms lack fairness considerations.
Consequently, they may lead to discriminatory outcomes against certain
communities, particularly marginalized populations, prompting extensive study
in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in
traditional machine learning, entails exclusive backgrounds, taxonomies, and
fulfillment techniques. To this end, this survey presents a comprehensive
overview of recent advances in the existing literature concerning fair LLMs.
Specifically, a brief introduction to LLMs is provided, followed by an analysis
of factors contributing to bias in LLMs. Additionally, the concept of fairness
in LLMs is discussed categorically, summarizing metrics for evaluating bias in
LLMs and existing algorithms for promoting fairness. Furthermore, resources for
evaluating bias in LLMs, including toolkits and datasets, are summarized.
Finally, existing research challenges and open questions are discussed.
\\ ( https://arxiv.org/abs/2404.01349 ,  3175kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01361
Date: Mon, 1 Apr 2024 13:16:34 GMT   (10031kb,D)

Title: LLM Attributor: Interactive Visual Attribution for LLM Generation
Authors: Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling,
  ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng
Categories: cs.CL cs.AI cs.HC cs.LG
Comments: 8 pages, 3 figures, For a video demo, see
  https://youtu.be/mIG2MDQKQxM
\\
  While large language models (LLMs) have shown remarkable capability to
generate convincing text across diverse domains, concerns around its potential
risks have highlighted the importance of understanding the rationale behind
text generation. We present LLM Attributor, a Python library that provides
interactive visualizations for training data attribution of an LLM's text
generation. Our library offers a new way to quickly attribute an LLM's text
generation to training data points to inspect model behaviors, enhance its
trustworthiness, and compare model-generated text with user-provided text. We
describe the visual and interactive design of our tool and highlight usage
scenarios for LLaMA2 models fine-tuned with two different datasets: online
articles about recent disasters and finance-related question-answer pairs.
Thanks to LLM Attributor's broad support for computational notebooks, users can
easily integrate it into their workflow to interactively visualize attributions
of their models. For easier access and extensibility, we open-source LLM
Attributor at https://github.com/poloclub/ LLM-Attribution. The video demo is
available at https://youtu.be/mIG2MDQKQxM.
\\ ( https://arxiv.org/abs/2404.01361 ,  10031kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01399
Date: Mon, 1 Apr 2024 18:10:05 GMT   (11125kb,D)

Title: Developing Safe and Responsible Large Language Models -- A Comprehensive
  Framework
Authors: Shaina Raza, Oluwanifemi Bamgbose, Shardul Ghuge, Fatemeh Tavakoli,
  Deepak John Reji
Categories: cs.CL
\\
  Given the growing concerns around the safety and risks of Large Language
Models (LLMs), it is essential to develop methods for mitigating these issues.
We introduce Safe and Responsible Large Language Model (SR$_{\text{LLM}}$) , a
model designed to enhance the safety of language generation using LLMs. Our
approach incorporates a comprehensive LLM safety risk taxonomy and utilizes a
dataset annotated by experts that align with this taxonomy. SR$_{\text{LLM}}$
is designed to identify potentially unsafe content and produce benign
variations. It employs instruction-based and parameter-efficient fine-tuning
methods, making the model not only effective in enhancing safety but also
resource-efficient and straightforward to adjust. Through our testing on five
benchmark datasets and two proprietary datasets, we observed notable reductions
in the generation of unsafe content. Moreover, following the implementation of
safety measures, there was a significant improvement in the production of safe
content. We detail our fine-tuning processes and how we benchmark safety for
SR$_{\text{LLM}}$ with the community engagement and promote the responsible
advancement of LLMs. All the data and code are available anonymous at
https://github.com/shainarazavi/Safe-Responsible-LLM .
\\ ( https://arxiv.org/abs/2404.01399 ,  11125kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01430
Date: Mon, 1 Apr 2024 19:04:17 GMT   (1419kb,D)

Title: Position-Aware Parameter Efficient Fine-Tuning Approach for Reducing
  Positional Bias in LLMs
Authors: Zheng Zhang, Fan Yang, Ziyan Jiang, Zheng Chen, Zhengyang Zhao,
  Chengyuan Ma, Liang Zhao, Yang Liu
Categories: cs.CL cs.AI cs.LG
\\
  Recent advances in large language models (LLMs) have enhanced their ability
to process long input contexts. This development is particularly crucial for
tasks that involve retrieving knowledge from an external datastore, which can
result in long inputs. However, recent studies show a positional bias in LLMs,
demonstrating varying performance depending on the location of useful
information within the input sequence. In this study, we conduct extensive
experiments to investigate the root causes of positional bias. Our findings
indicate that the primary contributor to LLM positional bias stems from the
inherent positional preferences of different models. We demonstrate that merely
employing prompt-based solutions is inadequate for overcoming the positional
preferences. To address this positional bias issue of a pre-trained LLM, we
developed a Position-Aware Parameter Efficient Fine-Tuning (PAPEFT) approach
which is composed of a data augmentation technique and a parameter efficient
adapter, enhancing a uniform attention distribution across the input context.
Our experiments demonstrate that the proposed approach effectively reduces
positional bias, improving LLMs' effectiveness in handling long context
sequences for various tasks that require externally retrieved knowledge.
\\ ( https://arxiv.org/abs/2404.01430 ,  1419kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01439
Date: Mon, 1 Apr 2024 19:22:58 GMT   (2061kb)

Title: Creating emoji lexica from unsupervised sentiment analysis of their
  descriptions
Authors: Milagros Fern\'andez-Gavilanes, Jonathan Juncal-Mart\'inez, Silvia
  Garc\'ia-M\'endez, Enrique Costa-Montenegro, Francisco Javier
  Gonz\'alez-Casta\~no
Categories: cs.CL cs.AI cs.LG
DOI: 10.1016/j.eswa.2018.02.043
\\
  Online media, such as blogs and social networking sites, generate massive
volumes of unstructured data of great interest to analyze the opinions and
sentiments of individuals and organizations. Novel approaches beyond Natural
Language Processing are necessary to quantify these opinions with polarity
metrics. So far, the sentiment expressed by emojis has received little
attention. The use of symbols, however, has boomed in the past four years.
About twenty billion are typed in Twitter nowadays, and new emojis keep
appearing in each new Unicode version, making them increasingly relevant to
sentiment analysis tasks. This has motivated us to propose a novel approach to
predict the sentiments expressed by emojis in online textual messages, such as
tweets, that does not require human effort to manually annotate data and saves
valuable time for other analysis tasks. For this purpose, we automatically
constructed a novel emoji sentiment lexicon using an unsupervised sentiment
analysis system based on the definitions given by emoji creators in Emojipedia.
Additionally, we automatically created lexicon variants by also considering the
sentiment distribution of the informal texts accompanying emojis. All these
lexica are evaluated and compared regarding the improvement obtained by
including them in sentiment analysis of the annotated datasets provided by
Kralj Novak et al. (2015). The results confirm the competitiveness of our
approach.
\\ ( https://arxiv.org/abs/2404.01439 ,  2061kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01443
Date: Mon, 1 Apr 2024 19:28:52 GMT   (426kb)

Title: Enterprise Use Cases Combining Knowledge Graphs and Natural Language
  Processing
Authors: Phillip Schneider, Tim Schopf, Juraj Vladika, Florian Matthes
Categories: cs.CL
Comments: 16 pages
\\
  Knowledge management is a critical challenge for enterprises in today's
digital world, as the volume and complexity of data being generated and
collected continue to grow incessantly. Knowledge graphs (KG) emerged as a
promising solution to this problem by providing a flexible, scalable, and
semantically rich way to organize and make sense of data. This paper builds
upon a recent survey of the research literature on combining KGs and Natural
Language Processing (NLP). Based on selected application scenarios from
enterprise context, we discuss synergies that result from such a combination.
We cover various approaches from the three core areas of KG construction,
reasoning as well as KG-based NLP tasks. In addition to explaining innovative
enterprise use cases, we assess their maturity in terms of practical
applicability and conclude with an outlook on emergent application areas for
the future.
\\ ( https://arxiv.org/abs/2404.01443 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01453
Date: Mon, 1 Apr 2024 19:56:41 GMT   (882kb,D)

Title: Unveiling Divergent Inductive Biases of LLMs on Temporal Data
Authors: Sindhu Kishore, Hangfeng He
Categories: cs.CL cs.AI
\\
  Unraveling the intricate details of events in natural language necessitates a
subtle understanding of temporal dynamics. Despite the adeptness of Large
Language Models (LLMs) in discerning patterns and relationships from data,
their inherent comprehension of temporal dynamics remains a formidable
challenge. This research meticulously explores these intrinsic challenges
within LLMs, with a specific emphasis on evaluating the performance of GPT-3.5
and GPT-4 models in the analysis of temporal data. Employing two distinct
prompt types, namely Question Answering (QA) format and Textual Entailment (TE)
format, our analysis probes into both implicit and explicit events. The
findings underscore noteworthy trends, revealing disparities in the performance
of GPT-3.5 and GPT-4. Notably, biases toward specific temporal relationships
come to light, with GPT-3.5 demonstrating a preference for "AFTER'' in the QA
format for both implicit and explicit events, while GPT-4 leans towards
"BEFORE''. Furthermore, a consistent pattern surfaces wherein GPT-3.5 tends
towards "TRUE'', and GPT-4 exhibits a preference for "FALSE'' in the TE format
for both implicit and explicit events. This persistent discrepancy between
GPT-3.5 and GPT-4 in handling temporal data highlights the intricate nature of
inductive bias in LLMs, suggesting that the evolution of these models may not
merely mitigate bias but may introduce new layers of complexity.
\\ ( https://arxiv.org/abs/2404.01453 ,  882kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01461
Date: Mon, 1 Apr 2024 20:15:06 GMT   (611kb,D)

Title: Will the Real Linda Please Stand up...to Large Language Models?
  Examining the Representativeness Heuristic in LLMs
Authors: Pengda Wang, Zilin Xiao, Hanjie Chen, Frederick L. Oswald
Categories: cs.CL cs.HC
Comments: work in progress
\\
  Although large language models (LLMs) have demonstrated remarkable
proficiency in understanding text and generating human-like text, they may
exhibit biases acquired from training data in doing so. Specifically, LLMs may
be susceptible to a common cognitive trap in human decision-making called the
representativeness heuristic. This is a concept in psychology that refers to
judging the likelihood of an event based on how closely it resembles a
well-known prototype or typical example versus considering broader facts or
statistical evidence. This work investigates the impact of the
representativeness heuristic on LLM reasoning. We created REHEAT
(Representativeness Heuristic AI Testing), a dataset containing a series of
problems spanning six common types of representativeness heuristics.
Experiments reveal that four LLMs applied to REHEAT all exhibited
representativeness heuristic biases. We further identify that the model's
reasoning steps are often incorrectly based on a stereotype rather than the
problem's description. Interestingly, the performance improves when adding a
hint in the prompt to remind the model of using its knowledge. This suggests
the uniqueness of the representativeness heuristic compared to traditional
biases. It can occur even when LLMs possess the correct knowledge while failing
in a cognitive trap. This highlights the importance of future research focusing
on the representativeness heuristic in model reasoning and decision-making and
on developing solutions to address it.
\\ ( https://arxiv.org/abs/2404.01461 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01474
Date: Mon, 1 Apr 2024 20:50:13 GMT   (160kb,D)

Title: Finding Replicable Human Evaluations via Stable Ranking Probability
Authors: Parker Riley, Daniel Deutsch, George Foster, Viresh Ratnakar, Ali
  Dabirmoghaddam, Markus Freitag
Categories: cs.CL
Comments: To appear at NAACL 2024
\\
  Reliable human evaluation is critical to the development of successful
natural language generation models, but achieving it is notoriously difficult.
Stability is a crucial requirement when ranking systems by quality: consistent
ranking of systems across repeated evaluations is not just desirable, but
essential. Without it, there is no reliable foundation for hill-climbing or
product launch decisions. In this paper, we use machine translation and its
state-of-the-art human evaluation framework, MQM, as a case study to understand
how to set up reliable human evaluations that yield stable conclusions. We
investigate the optimal configurations for item allocation to raters, number of
ratings per item, and score normalization. Our study on two language pairs
provides concrete recommendations for designing replicable human evaluation
studies. We also collect and release the largest publicly available dataset of
multi-segment translations rated by multiple professional translators,
consisting of nearly 140,000 segment annotations across two language pairs.
\\ ( https://arxiv.org/abs/2404.01474 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01481
Date: Mon, 1 Apr 2024 21:02:18 GMT   (682kb,D)

Title: A Study on Scaling Up Multilingual News Framing Analysis
Authors: Syeda Sabrina Akter and Antonios Anastasopoulos
Categories: cs.CL
Comments: accepted at NAACL 2024
\\
  Media framing is the study of strategically selecting and presenting specific
aspects of political issues to shape public opinion. Despite its relevance to
almost all societies around the world, research has been limited due to the
lack of available datasets and other resources. This study explores the
possibility of dataset creation through crowdsourcing, utilizing non-expert
annotators to develop training corpora. We first extend framing analysis beyond
English news to a multilingual context (12 typologically diverse languages)
through automatic translation. We also present a novel benchmark in Bengali and
Portuguese on the immigration and same-sex marriage domains. Additionally, we
show that a system trained on our crowd-sourced dataset, combined with other
existing ones, leads to a 5.32 percentage point increase from the baseline,
showing that crowdsourcing is a viable option. Last, we study the performance
of large language models (LLMs) for this task, finding that task-specific
fine-tuning is a better approach than employing bigger non-specialized models.
\\ ( https://arxiv.org/abs/2404.01481 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01490
Date: Mon, 1 Apr 2024 21:21:15 GMT   (456kb,D)

Title: AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for
  Multilingual Semantic Textual Relatedness
Authors: Miaoran Zhang, Mingyang Wang, Jesujoba O. Alabi, Dietrich Klakow
Categories: cs.CL
Comments: To appear at the SemEval-2024 workshop (NAACL 2024)
\\
  This paper presents our system developed for the SemEval-2024 Task 1:
Semantic Textual Relatedness for African and Asian Languages. The shared task
aims at measuring the semantic textual relatedness between pairs of sentences,
with a focus on a range of under-represented languages. In this work, we
propose using machine translation for data augmentation to address the
low-resource challenge of limited training data. Moreover, we apply
task-adaptive pre-training on unlabeled task data to bridge the gap between
pre-training and task adaptation. For model training, we investigate both full
fine-tuning and adapter-based tuning, and adopt the adapter framework for
effective zero-shot cross-lingual transfer. We achieve competitive results in
the shared task: our system performs the best among all ranked teams in both
subtask A (supervised learning) and subtask C (cross-lingual transfer).
\\ ( https://arxiv.org/abs/2404.01490 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01532
Date: Mon, 1 Apr 2024 23:46:00 GMT   (8044kb,D)

Title: Set-Aligning Framework for Auto-Regressive Event Temporal Graph
  Generation
Authors: Xingwei Tan, Yuxiang Zhou, Gabriele Pergola, Yulan He
Categories: cs.CL cs.IR
Comments: Accepted to NAACL 2024. 9 + 10 pages
\\
  Event temporal graphs have been shown as convenient and effective
representations of complex temporal relations between events in text. Recent
studies, which employ pre-trained language models to auto-regressively generate
linearised graphs for constructing event temporal graphs, have shown promising
results. However, these methods have often led to suboptimal graph generation
as the linearised graphs exhibit set characteristics which are instead treated
sequentially by language models. This discrepancy stems from the conventional
text generation objectives, leading to erroneous penalisation of correct
predictions caused by the misalignment of elements in target sequences. To
address these challenges, we reframe the task as a conditional set generation
problem, proposing a Set-aligning Framework tailored for the effective
utilisation of Large Language Models (LLMs). The framework incorporates data
augmentations and set-property regularisations designed to alleviate text
generation loss penalties associated with the linearised graph edge sequences,
thus encouraging the generation of more relation edges. Experimental results
show that our framework surpasses existing baselines for event temporal graph
generation. Furthermore, under zero-shot settings, the structural knowledge
introduced through our framework notably improves model generalisation,
particularly when the training examples available are limited.
\\ ( https://arxiv.org/abs/2404.01532 ,  8044kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01536
Date: Tue, 2 Apr 2024 00:02:00 GMT   (2963kb,D)

Title: Laying Anchors: Semantically Priming Numerals in Language Modeling
Authors: Mandar Sharma, Rutuja Murlidhar Taware, Pravesh Koirala, Nikhil
  Muralidhar, Naren Ramakrishnan
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to the findings of NAACL 2024
\\
  Off-the-shelf pre-trained language models have become the de facto standard
in NLP pipelines for a multitude of downstream tasks. However, the inability of
these models to properly encode numerals limits their performance on tasks
requiring numeric comprehension. We introduce strategies to semantically prime
numerals in any corpus by generating anchors governed by the distribution of
numerals in said corpus, thereby enabling mathematically grounded
representations of these numeral tokens. We establish the superiority of our
proposed techniques through evaluation on a range of numeracy tasks for both
in-domain (seen) and out-domain (unseen) numerals. Further, we expand our
empirical evaluations to numerals ranging from 1 to 10 billion, a significantly
broader range compared to previous studies of the same nature, and we
demonstrate significant improvements in the mathematical grounding of our
learned embeddings.
\\ ( https://arxiv.org/abs/2404.01536 ,  2963kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01549
Date: Tue, 2 Apr 2024 01:29:28 GMT   (539kb,D)

Title: Octopus: On-device language model for function calling of software APIs
Authors: Wei Chen, Zhiyuan Li, Mingyuan Ma
Categories: cs.CL cs.SE
\\
  In the rapidly evolving domain of artificial intelligence, Large Language
Models (LLMs) play a crucial role due to their advanced text processing and
generation abilities. This study introduces a new strategy aimed at harnessing
on-device LLMs in invoking software APIs. We meticulously compile a dataset
derived from software API documentation and apply fine-tuning to LLMs with
capacities of 2B, 3B and 7B parameters, specifically to enhance their
proficiency in software API interactions. Our approach concentrates on refining
the models' grasp of API structures and syntax, significantly enhancing the
accuracy of API function calls. Additionally, we propose \textit{conditional
masking} techniques to ensure outputs in the desired formats and reduce error
rates while maintaining inference speeds. We also propose a novel benchmark
designed to evaluate the effectiveness of LLMs in API interactions,
establishing a foundation for subsequent research. Octopus, the fine-tuned
model, is proved to have better performance than GPT-4 for the software APIs
calling. This research aims to advance automated software development and API
integration, representing substantial progress in aligning LLM capabilities
with the demands of practical software engineering applications.
\\ ( https://arxiv.org/abs/2404.01549 ,  539kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01569
Date: Tue, 2 Apr 2024 02:03:28 GMT   (6994kb,D)

Title: Evaluating Large Language Models Using Contrast Sets: An Experimental
  Approach
Authors: Manish Sanwal
Categories: cs.CL cs.AI cs.LG
DOI: 10.13140/RG.2.2.19948.33928
\\
  In the domain of Natural Language Inference (NLI), especially in tasks
involving the classification of multiple input texts, the Cross-Entropy Loss
metric is widely employed as a standard for error measurement. However, this
metric falls short in effectively evaluating a model's capacity to understand
language entailments. In this study, we introduce an innovative technique for
generating a contrast set for the Stanford Natural Language Inference (SNLI)
dataset. Our strategy involves the automated substitution of verbs, adverbs,
and adjectives with their synonyms to preserve the original meaning of
sentences. This method aims to assess whether a model's performance is based on
genuine language comprehension or simply on pattern recognition. We conducted
our analysis using the ELECTRA-small model. The model achieved an accuracy of
89.9% on the conventional SNLI dataset but showed a reduced accuracy of 72.5%
on our contrast set, indicating a substantial 17% decline. This outcome led us
to conduct a detailed examination of the model's learning behaviors. Following
this, we improved the model's resilience by fine-tuning it with a
contrast-enhanced training dataset specifically designed for SNLI, which
increased its accuracy to 85.5% on the contrast sets. Our findings highlight
the importance of incorporating diverse linguistic expressions into datasets
for NLI tasks. We hope that our research will encourage the creation of more
inclusive datasets, thereby contributing to the development of NLI models that
are both more sophisticated and effective.
\\ ( https://arxiv.org/abs/2404.01569 ,  6994kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01582
Date: Mon, 1 Apr 2024 12:20:34 GMT   (569kb,D)

Title: BERT-Enhanced Retrieval Tool for Homework Plagiarism Detection System
Authors: Jiarong Xian, Jibao Yuan, Peiwei Zheng, Dexian Chen
Categories: cs.CL cs.AI cs.IR
\\
  Text plagiarism detection task is a common natural language processing task
that aims to detect whether a given text contains plagiarism or copying from
other texts. In existing research, detection of high level plagiarism is still
a challenge due to the lack of high quality datasets. In this paper, we propose
a plagiarized text data generation method based on GPT-3.5, which produces
32,927 pairs of text plagiarism detection datasets covering a wide range of
plagiarism methods, bridging the gap in this part of research. Meanwhile, we
propose a plagiarism identification method based on Faiss with BERT with high
efficiency and high accuracy. Our experiments show that the performance of this
model outperforms other models in several metrics, including 98.86\%, 98.90%,
98.86%, and 0.9888 for Accuracy, Precision, Recall, and F1 Score, respectively.
At the end, we also provide a user-friendly demo platform that allows users to
upload a text library and intuitively participate in the plagiarism analysis.
\\ ( https://arxiv.org/abs/2404.01582 ,  569kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01588
Date: Tue, 2 Apr 2024 02:30:27 GMT   (9291kb,D)

Title: Hallucination Diversity-Aware Active Learning for Text Summarization
Authors: Yu Xia, Xu Liu, Tong Yu, Sungchul Kim, Ryan A. Rossi, Anup Rao, Tung
  Mai, Shuai Li
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to NAACL 2024
\\
  Large Language Models (LLMs) have shown propensity to generate hallucinated
outputs, i.e., texts that are factually incorrect or unsupported. Existing
methods for alleviating hallucinations typically require costly human
annotations to identify and correct hallucinations in LLM outputs. Moreover,
most of these methods focus on a specific type of hallucination, e.g., entity
or token errors, which limits their effectiveness in addressing various types
of hallucinations exhibited in LLM outputs. To our best knowledge, in this
paper we propose the first active learning framework to alleviate LLM
hallucinations, reducing costly human annotations of hallucination needed. By
measuring fine-grained hallucinations from errors in semantic frame, discourse
and content verifiability in text summarization, we propose HAllucination
Diversity-Aware Sampling (HADAS) to select diverse hallucinations for
annotations in active learning for LLM finetuning. Extensive experiments on
three datasets and different backbone models demonstrate advantages of our
method in effectively and efficiently mitigating LLM hallucinations.
\\ ( https://arxiv.org/abs/2404.01588 ,  9291kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01589
Date: Tue, 2 Apr 2024 02:30:47 GMT   (36kb)

Title: Classifying Cancer Stage with Open-Source Clinical Large Language Models
Authors: Chia-Hsuan Chang, Mary M. Lucas, Grace Lu-Yao, Christopher C. Yang
Categories: cs.CL cs.AI
Comments: accepted in the IEEE International Conference on Healthcare
  Informatics (IEEE ICHI 2024)
\\
  Cancer stage classification is important for making treatment and care
management plans for oncology patients. Information on staging is often
included in unstructured form in clinical, pathology, radiology and other
free-text reports in the electronic health record system, requiring extensive
work to parse and obtain. To facilitate the extraction of this information,
previous NLP approaches rely on labeled training datasets, which are
labor-intensive to prepare. In this study, we demonstrate that without any
labeled training data, open-source clinical large language models (LLMs) can
extract pathologic tumor-node-metastasis (pTNM) staging information from
real-world pathology reports. Our experiments compare LLMs and a BERT-based
model fine-tuned using the labeled data. Our findings suggest that while LLMs
still exhibit subpar performance in Tumor (T) classification, with the
appropriate adoption of prompting strategies, they can achieve comparable
performance on Metastasis (M) classification and improved performance on Node
(N) classification.
\\ ( https://arxiv.org/abs/2404.01589 ,  36kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01602
Date: Tue, 2 Apr 2024 02:46:18 GMT   (1474kb,D)

Title: Helmsman of the Masses? Evaluate the Opinion Leadership of Large
  Language Models in the Werewolf Game
Authors: Silin Du, Xiaowei Zhang
Categories: cs.CL cs.AI cs.HC
Comments: 32 pages, 6 figures, 21 tables
\\
  Large language models (LLMs) have exhibited memorable strategic behaviors in
social deductive games. However, the significance of opinion leadership
exhibited by LLM-based agents has been overlooked, which is crucial for
practical applications in multi-agent and human-AI interaction settings.
Opinion leaders are individuals who have a noticeable impact on the beliefs and
behaviors of others within a social group. In this work, we employ the Werewolf
game as a simulation platform to assess the opinion leadership of LLMs. The
game features the role of the Sheriff, tasked with summarizing arguments and
recommending decision options, and therefore serves as a credible proxy for an
opinion leader. We develop a framework integrating the Sheriff role and devise
two novel metrics for evaluation based on the critical characteristics of
opinion leaders. The first metric measures the reliability of the opinion
leader, and the second assesses the influence of the opinion leader on other
players' decisions. We conduct extensive experiments to evaluate LLMs of
different scales. In addition, we collect a Werewolf question-answering dataset
(WWQA) to assess and enhance LLM's grasp of the game rules, and we also
incorporate human participants for further analysis. The results suggest that
the Werewolf game is a suitable test bed to evaluate the opinion leadership of
LLMs and few LLMs possess the capacity for opinion leadership.
\\ ( https://arxiv.org/abs/2404.01602 ,  1474kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01616
Date: Tue, 2 Apr 2024 03:42:28 GMT   (122kb,D)

Title: Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems
Authors: Frank Palma Gomez, Ramon Sanabria, Yun-hsuan Sung, Daniel Cer,
  Siddharth Dalmia, Gustavo Hernandez Abrego
Categories: cs.CL cs.IR cs.SD eess.AS
\\
  Large language models (LLMs) are trained on text-only data that go far beyond
the languages with paired speech and text data. At the same time, Dual Encoder
(DE) based retrieval systems project queries and documents into the same
embedding space and have demonstrated their success in retrieval and bi-text
mining. To match speech and text in many languages, we propose using LLMs to
initialize multi-modal DE retrieval systems. Unlike traditional methods, our
system doesn't require speech data during LLM pre-training and can exploit
LLM's multilingual text understanding capabilities to match speech and text in
languages unseen during retrieval training. Our multi-modal LLM-based retrieval
system is capable of matching speech and text in 102 languages despite only
training on 21 languages. Our system outperforms previous systems trained
explicitly on all 102 languages. We achieve a 10% absolute improvement in
Recall@1 averaged across these languages. Additionally, our model demonstrates
cross-lingual speech and text matching, which is further enhanced by readily
available machine translation data.
\\ ( https://arxiv.org/abs/2404.01616 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01626
Date: Tue, 2 Apr 2024 04:27:54 GMT   (4140kb,D)

Title: Entity Disambiguation via Fusion Entity Decoding
Authors: Junxiong Wang, Ali Mousavi, Omar Attia, Saloni Potdar, Alexander M.
  Rush, Umar Farooq Minhas, Yunyao Li
Categories: cs.CL cs.IR
Comments: Accepted at NAACL'24 main
\\
  Entity disambiguation (ED), which links the mentions of ambiguous entities to
their referent entities in a knowledge base, serves as a core component in
entity linking (EL). Existing generative approaches demonstrate improved
accuracy compared to classification approaches under the standardized ZELDA
benchmark. Nevertheless, generative approaches suffer from the need for
large-scale pre-training and inefficient generation. Most importantly, entity
descriptions, which could contain crucial information to distinguish similar
entities from each other, are often overlooked. We propose an encoder-decoder
model to disambiguate entities with more detailed entity descriptions. Given
text and candidate entities, the encoder learns interactions between the text
and each candidate entity, producing representations for each entity candidate.
The decoder then fuses the representations of entity candidates together and
selects the correct entity. Our experiments, conducted on various entity
disambiguation benchmarks, demonstrate the strong and robust performance of
this model, particularly +1.5% in the ZELDA benchmark compared with GENRE.
Furthermore, we integrate this approach into the retrieval/reader framework and
observe +1.5% improvements in end-to-end entity linking in the GERBIL benchmark
compared with EntQA.
\\ ( https://arxiv.org/abs/2404.01626 ,  4140kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01651
Date: Tue, 2 Apr 2024 05:36:41 GMT   (7947kb,D)

Title: NLP Systems That Can't Tell Use from Mention Censor Counterspeech, but
  Teaching the Distinction Helps
Authors: Kristina Gligoric and Myra Cheng and Lucia Zheng and Esin Durmus and
  Dan Jurafsky
Categories: cs.CL cs.CY cs.HC cs.SI
Comments: NAACL 2024 (Main conference)
\\
  The use of words to convey speaker's intent is traditionally distinguished
from the `mention' of words for quoting what someone said, or pointing out
properties of a word. Here we show that computationally modeling this
use-mention distinction is crucial for dealing with counterspeech online.
Counterspeech that refutes problematic content often mentions harmful language
but is not harmful itself (e.g., calling a vaccine dangerous is not the same as
expressing disapproval of someone for calling vaccines dangerous). We show that
even recent language models fail at distinguishing use from mention, and that
this failure propagates to two key downstream tasks: misinformation and hate
speech detection, resulting in censorship of counterspeech. We introduce
prompting mitigations that teach the use-mention distinction, and show they
reduce these errors. Our work highlights the importance of the use-mention
distinction for NLP and CSS and offers ways to address it.
\\ ( https://arxiv.org/abs/2404.01651 ,  7947kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01652
Date: Tue, 2 Apr 2024 05:44:50 GMT   (493kb,D)

Title: Towards Better Generalization in Open-Domain Question Answering by
  Mitigating Context Memorization
Authors: Zixuan Zhang, Revanth Gangi Reddy, Kevin Small, Tong Zhang, Heng Ji
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 Findings
\\
  Open-domain Question Answering (OpenQA) aims at answering factual questions
with an external large-scale knowledge corpus. However, real-world knowledge is
not static; it updates and evolves continually. Such a dynamic characteristic
of knowledge poses a vital challenge for these models, as the trained models
need to constantly adapt to the latest information to make sure that the
answers remain accurate. In addition, it is still unclear how well an OpenQA
model can transfer to completely new knowledge domains. In this paper, we
investigate the generalization performance of a retrieval-augmented QA model in
two specific scenarios: 1) adapting to updated versions of the same knowledge
corpus; 2) switching to completely different knowledge domains. We observe that
the generalization challenges of OpenQA models stem from the reader's
over-reliance on memorizing the knowledge from the external corpus, which
hinders the model from generalizing to a new knowledge corpus. We introduce
Corpus-Invariant Tuning (CIT), a simple but effective training strategy, to
mitigate the knowledge over-memorization by controlling the likelihood of
retrieved contexts during training. Extensive experimental results on multiple
OpenQA benchmarks show that CIT achieves significantly better generalizability
without compromising the model's performance in its original corpus and domain.
\\ ( https://arxiv.org/abs/2404.01652 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01657
Date: Tue, 2 Apr 2024 05:59:43 GMT   (431kb,D)

Title: Release of Pre-Trained Models for the Japanese Language
Authors: Kei Sawada, Tianyu Zhao, Makoto Shing, Kentaro Mitsui, Akio Kaga,
  Yukiya Hono, Toshiaki Wakatsuki, Koh Mitsuda
Categories: cs.CL cs.AI cs.CV cs.LG eess.AS
Comments: 9 pages, 1 figure, 5 tables, accepted for LREC-COLING 2024. Models
  are publicly available at https://huggingface.co/rinna
\\
  AI democratization aims to create a world in which the average person can
utilize AI techniques. To achieve this goal, numerous research institutes have
attempted to make their results accessible to the public. In particular, large
pre-trained models trained on large-scale data have shown unprecedented
potential, and their release has had a significant impact. However, most of the
released models specialize in the English language, and thus, AI
democratization in non-English-speaking communities is lagging significantly.
To reduce this gap in AI access, we released Generative Pre-trained Transformer
(GPT), Contrastive Language and Image Pre-training (CLIP), Stable Diffusion,
and Hidden-unit Bidirectional Encoder Representations from Transformers
(HuBERT) pre-trained in Japanese. By providing these models, users can freely
interface with AI that aligns with Japanese cultural values and ensures the
identity of Japanese culture, thus enhancing the democratization of AI.
Additionally, experiments showed that pre-trained models specialized for
Japanese can efficiently achieve high performance in Japanese tasks.
\\ ( https://arxiv.org/abs/2404.01657 ,  431kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01663
Date: Tue, 2 Apr 2024 06:07:35 GMT   (13583kb,D)

Title: CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small
  Language Models
Authors: Xuechen Liang and Meiling Tao, Tianyu Shi, Yiting Xie
Categories: cs.CL
\\
  Open large language models (LLMs) have significantly advanced the field of
natural language processing, showcasing impressive performance across various
tasks.Despite the significant advancements in LLMs, their effective operation
still relies heavily on human input to accurately guide the dialogue flow, with
agent tuning being a crucial optimization technique that involves human
adjustments to the model for better response to such guidance.Addressing this
dependency, our work introduces the TinyAgent model, trained on a meticulously
curated high-quality dataset. We also present the Collaborative Multi-Agent
Tuning (CMAT) framework, an innovative system designed to augment language
agent capabilities through adaptive weight updates based on environmental
feedback. This framework fosters collaborative learning and real-time
adaptation among multiple intelligent agents, enhancing their context-awareness
and long-term memory. In this research, we propose a new communication agent
framework that integrates multi-agent systems with environmental feedback
mechanisms, offering a scalable method to explore cooperative behaviors.
Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5,
despite having fewer parameters, signifying a substantial improvement in the
efficiency and effectiveness of LLMs.
\\ ( https://arxiv.org/abs/2404.01663 ,  13583kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01667
Date: Tue, 2 Apr 2024 06:14:54 GMT   (918kb,D)

Title: METAL: Towards Multilingual Meta-Evaluation
Authors: Rishav Hada, Varun Gumma, Mohamed Ahmed, Kalika Bali, Sunayana Sitaram
Categories: cs.CL
Comments: Accepted to NAACL 2024 findings
\\
  With the rising human-like precision of Large Language Models (LLMs) in
numerous tasks, their utilization in a variety of real-world applications is
becoming more prevalent. Several studies have shown that LLMs excel on many
standard NLP benchmarks. However, it is challenging to evaluate LLMs due to
test dataset contamination and the limitations of traditional metrics. Since
human evaluations are difficult to collect, there is a growing interest in the
community to use LLMs themselves as reference-free evaluators for subjective
metrics. However, past work has shown that LLM-based evaluators can exhibit
bias and have poor alignment with human judgments. In this study, we propose a
framework for an end-to-end assessment of LLMs as evaluators in multilingual
scenarios. We create a carefully curated dataset, covering 10 languages
containing native speaker judgments for the task of summarization. This dataset
is created specifically to evaluate LLM-based evaluators, which we refer to as
meta-evaluation (METAL). We compare the performance of LLM-based evaluators
created using GPT-3.5-Turbo, GPT-4, and PaLM2. Our results indicate that
LLM-based evaluators based on GPT-4 perform the best across languages, while
GPT-3.5-Turbo performs poorly. Additionally, we perform an analysis of the
reasoning provided by LLM-based evaluators and find that it often does not
match the reasoning provided by human judges.
\\ ( https://arxiv.org/abs/2404.01667 ,  918kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01679
Date: Tue, 2 Apr 2024 06:31:17 GMT   (9830kb,D)

Title: Event Detection from Social Media for Epidemic Prediction
Authors: Tanmay Parekh, Anh Mac, Jiarui Yu, Yuxuan Dong, Syed Shahriar, Bonnie
  Liu, Eric Yang, Kuan-Hao Huang, Wei Wang, Nanyun Peng, Kai-Wei Chang
Categories: cs.CL cs.SI
Comments: Accepted at NAACL 2024
\\
  Social media is an easy-to-access platform providing timely updates about
societal trends and events. Discussions regarding epidemic-related events such
as infections, symptoms, and social interactions can be crucial for informing
policymaking during epidemic outbreaks. In our work, we pioneer exploiting
Event Detection (ED) for better preparedness and early warnings of any upcoming
epidemic by developing a framework to extract and analyze epidemic-related
events from social media posts. To this end, we curate an epidemic event
ontology comprising seven disease-agnostic event types and construct a Twitter
dataset SPEED with human-annotated events focused on the COVID-19 pandemic.
Experimentation reveals how ED models trained on COVID-based SPEED can
effectively detect epidemic events for three unseen epidemics of Monkeypox,
Zika, and Dengue; while models trained on existing ED datasets fail miserably.
Furthermore, we show that reporting sharp increases in the extracted events by
our framework can provide warnings 4-9 weeks earlier than the WHO epidemic
declaration for Monkeypox. This utility of our framework lays the foundations
for better preparedness against emerging epidemics.
\\ ( https://arxiv.org/abs/2404.01679 ,  9830kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01701
Date: Tue, 2 Apr 2024 07:09:44 GMT   (206kb,D)

Title: On the Role of Summary Content Units in Text Summarization Evaluation
Authors: Marcel Nawrath, Agnieszka Nowak, Tristan Ratz, Danilo C. Walenta, Juri
  Opitz, Leonardo F. R. Ribeiro, Jo\~ao Sedoc, Daniel Deutsch, Simon Mille,
  Yixin Liu, Lining Zhang, Sebastian Gehrmann, Saad Mahamood, Miruna Clinciu,
  Khyathi Chandu, Yufang Hou
Categories: cs.CL
Comments: 10 Pages, 3 Figures, 3 Tables, camera ready version accepted at NAACL
  2024
\\
  At the heart of the Pyramid evaluation method for text summarization lie
human written summary content units (SCUs). These SCUs are concise sentences
that decompose a summary into small facts. Such SCUs can be used to judge the
quality of a candidate summary, possibly partially automated via natural
language inference (NLI) systems. Interestingly, with the aim to fully automate
the Pyramid evaluation, Zhang and Bansal (2021) show that SCUs can be
approximated by automatically generated semantic role triplets (STUs). However,
several questions currently lack answers, in particular: i) Are there other
ways of approximating SCUs that can offer advantages? ii) Under which
conditions are SCUs (or their approximations) offering the most value? In this
work, we examine two novel strategies to approximate SCUs: generating SCU
approximations from AMR meaning representations (SMUs) and from large language
models (SGUs), respectively. We find that while STUs and SMUs are competitive,
the best approximation quality is achieved by SGUs. We also show through a
simple sentence-decomposition baseline (SSUs) that SCUs (and their
approximations) offer the most value when ranking short summaries, but may not
help as much when ranking systems or longer summaries.
\\ ( https://arxiv.org/abs/2404.01701 ,  206kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01706
Date: Tue, 2 Apr 2024 07:43:12 GMT   (1179kb,D)

Title: Polarity Calibration for Opinion Summarization
Authors: Yuanyuan Lei, Kaiqiang Song, Sangwoo Cho, Xiaoyang Wang, Ruihong
  Huang, Dong Yu
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\
  Opinion summarization is automatically generating summaries from a variety of
subjective information, such as product reviews or political opinions. The
challenge of opinions summarization lies in presenting divergent or even
conflicting opinions. We conduct an analysis of previous summarization models,
which reveals their inclination to amplify the polarity bias, emphasizing the
majority opinions while ignoring the minority opinions. To address this issue
and make the summarizer express both sides of opinions, we introduce the
concept of polarity calibration, which aims to align the polarity of output
summary with that of input text. Specifically, we develop a reinforcement
training approach for polarity calibration. This approach feeds the polarity
distance between output summary and input text as reward into the summarizer,
and also balance polarity calibration with content preservation and language
naturality. We evaluate our Polarity Calibration model (PoCa) on two types of
opinions summarization tasks: summarizing product reviews and political
opinions articles. Automatic and human evaluation demonstrate that our approach
can mitigate the polarity mismatch between output summary and input text, as
well as maintain the content semantic and language quality.
\\ ( https://arxiv.org/abs/2404.01706 ,  1179kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01713
Date: Tue, 2 Apr 2024 07:57:05 GMT   (5149kb,D)

Title: Generative AI for Immersive Communication: The Next Frontier in
  Internet-of-Senses Through 6G
Authors: Nassim Sehad, Lina Bariah, Wassim Hamidouche, Hamed Hellaoui, Riku
  J\"antti, and M\'erouane Debbah
Categories: cs.CL cs.AI cs.HC cs.MM cs.NI
\\
  Over the past two decades, the Internet-of-Things (IoT) has been a
transformative concept, and as we approach 2030, a new paradigm known as the
Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR),
IoS seeks to provide multi-sensory experiences, acknowledging that in our
physical reality, our perception extends far beyond just sight and sound; it
encompasses a range of senses. This article explores existing technologies
driving immersive multi-sensory media, delving into their capabilities and
potential applications. This exploration includes a comparative analysis
between conventional immersive media streaming and a proposed use case that
lever- ages semantic communication empowered by generative Artificial
Intelligence (AI). The focal point of this analysis is the substantial
reduction in bandwidth consumption by 99.93% in the proposed scheme. Through
this comparison, we aim to underscore the practical applications of generative
AI for immersive media while addressing the challenges and outlining future
trajectories.
\\ ( https://arxiv.org/abs/2404.01713 ,  5149kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01715
Date: Tue, 2 Apr 2024 07:57:19 GMT   (2656kb,D)

Title: EMONA: Event-level Moral Opinions in News Articles
Authors: Yuanyuan Lei, Md Messal Monem Miah, Ayesha Qamar, Sai Ramana Reddy,
  Jonathan Tong, Haotian Xu, Ruihong Huang
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\
  Most previous research on moral frames has focused on social media short
texts, little work has explored moral sentiment within news articles. In news
articles, authors often express their opinions or political stance through
moral judgment towards events, specifically whether the event is right or wrong
according to social moral rules. This paper initiates a new task to understand
moral opinions towards events in news articles. We have created a new dataset,
EMONA, and annotated event-level moral opinions in news articles. This dataset
consists of 400 news articles containing over 10k sentences and 45k events,
among which 9,613 events received moral foundation labels. Extracting event
morality is a challenging task, as moral judgment towards events can be very
implicit. Baseline models were built for event moral identification and
classification. In addition, we also conduct extrinsic evaluations to integrate
event-level moral opinions into three downstream tasks. The statistical
analysis and experiments show that moral opinions of events can serve as
informative features for identifying ideological bias or subjective events.
\\ ( https://arxiv.org/abs/2404.01715 ,  2656kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01720
Date: Tue, 2 Apr 2024 08:14:27 GMT   (1062kb,D)

Title: Self-Improvement Programming for Temporal Knowledge Graph Question
  Answering
Authors: Zhuo Chen, Zhao Zhang, Zixuan Li, Fei Wang, Yutao Zeng, Xiaolong Jin
  and Yongjun Xu
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024 (long paper)
\\
  Temporal Knowledge Graph Question Answering (TKGQA) aims to answer questions
with temporal intent over Temporal Knowledge Graphs (TKGs). The core challenge
of this task lies in understanding the complex semantic information regarding
multiple types of time constraints (e.g., before, first) in questions. Existing
end-to-end methods implicitly model the time constraints by learning time-aware
embeddings of questions and candidate answers, which is far from understanding
the question comprehensively. Motivated by semantic-parsing-based approaches
that explicitly model constraints in questions by generating logical forms with
symbolic operators, we design fundamental temporal operators for time
constraints and introduce a novel self-improvement Programming method for TKGQA
(Prog-TQA). Specifically, Prog-TQA leverages the in-context learning ability of
Large Language Models (LLMs) to understand the combinatory time constraints in
the questions and generate corresponding program drafts with a few examples
given. Then, it aligns these drafts to TKGs with the linking module and
subsequently executes them to generate the answers. To enhance the ability to
understand questions, Prog-TQA is further equipped with a self-improvement
strategy to effectively bootstrap LLMs using high-quality self-generated
drafts. Extensive experiments demonstrate the superiority of the proposed
Prog-TQA on MultiTQ and CronQuestions datasets, especially in the Hits@1
metric.
\\ ( https://arxiv.org/abs/2404.01720 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01722
Date: Tue, 2 Apr 2024 08:16:03 GMT   (892kb,D)

Title: Sentence-level Media Bias Analysis with Event Relation Graph
Authors: Yuanyuan Lei, Ruihong Huang
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\
  Media outlets are becoming more partisan and polarized nowadays. In this
paper, we identify media bias at the sentence level, and pinpoint bias
sentences that intend to sway readers' opinions. As bias sentences are often
expressed in a neutral and factual way, considering broader context outside a
sentence can help reveal the bias. In particular, we observe that events in a
bias sentence need to be understood in associations with other events in the
document. Therefore, we propose to construct an event relation graph to
explicitly reason about event-event relations for sentence-level bias
identification. The designed event relation graph consists of events as nodes
and four common types of event relations: coreference, temporal, causal, and
subevent relations. Then, we incorporate event relation graph for bias
sentences identification in two steps: an event-aware language model is built
to inject the events and event relations knowledge into the basic language
model via soft labels; further, a relation-aware graph attention network is
designed to update sentence embedding with events and event relations
information based on hard labels. Experiments on two benchmark datasets
demonstrate that our approach with the aid of event relation graph improves
both precision and recall of bias sentence identification.
\\ ( https://arxiv.org/abs/2404.01722 ,  892kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01744
Date: Tue, 2 Apr 2024 09:01:32 GMT   (1595kb,D)

Title: Octopus v2: On-device language model for super agent
Authors: Wei Chen, Zhiyuan Li
Categories: cs.CL
\\
  Language models have shown effectiveness in a variety of software
applications, particularly in tasks related to automatic workflow. These models
possess the crucial ability to call functions, which is essential in creating
AI agents. Despite the high performance of large-scale language models in cloud
environments, they are often associated with concerns over privacy and cost.
Current on-device models for function calling face issues with latency and
accuracy. Our research presents a new method that empowers an on-device model
with 2 billion parameters to surpass the performance of GPT-4 in both accuracy
and latency, and decrease the context length by 95\%. When compared to Llama-7B
with a RAG-based function calling mechanism, our method enhances latency by
35-fold. This method reduces the latency to levels deemed suitable for
deployment across a variety of edge devices in production environments,
aligning with the performance requisites for real-world applications.
\\ ( https://arxiv.org/abs/2404.01744 ,  1595kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01753
Date: Tue, 2 Apr 2024 09:11:58 GMT   (6975kb,D)

Title: M2SA: Multimodal and Multilingual Model for Sentiment Analysis of Tweets
Authors: Gaurish Thakkar, Sherzod Hakimov, Marko Tadi\'c
Categories: cs.CL
Journal-ref: LREC-COLING 2024 - The 2024 Joint International Conference on
  Computational Linguistics, Language Resources and Evaluation
\\
  In recent years, multimodal natural language processing, aimed at learning
from diverse data types, has garnered significant attention. However, there
needs to be more clarity when it comes to analysing multimodal tasks in
multi-lingual contexts. While prior studies on sentiment analysis of tweets
have predominantly focused on the English language, this paper addresses this
gap by transforming an existing textual Twitter sentiment dataset into a
multimodal format through a straightforward curation process. Our work opens up
new avenues for sentiment-related research within the research community.
Additionally, we conduct baseline experiments utilising this augmented dataset
and report the findings. Notably, our evaluations reveal that when comparing
unimodal and multimodal configurations, using a sentiment-tuned large language
model as a text encoder performs exceptionally well.
\\ ( https://arxiv.org/abs/2404.01753 ,  6975kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01767
Date: Tue, 2 Apr 2024 09:31:14 GMT   (14532kb,D)

Title: Class-Incremental Few-Shot Event Detection
Authors: Kailin Zhao, Xiaolong Jin, Long Bai, Jiafeng Guo, Xueqi Cheng
Categories: cs.CL
\\
  Event detection is one of the fundamental tasks in information extraction and
knowledge graph. However, a realistic event detection system often needs to
deal with new event classes constantly. These new classes usually have only a
few labeled instances as it is time-consuming and labor-intensive to annotate a
large number of unlabeled instances. Therefore, this paper proposes a new task,
called class-incremental few-shot event detection. Nevertheless, this task
faces two problems, i.e., old knowledge forgetting and new class overfitting.
To solve these problems, this paper further presents a novel knowledge
distillation and prompt learning based method, called Prompt-KD. Specifically,
to handle the forgetting problem about old knowledge, Prompt-KD develops an
attention based multi-teacher knowledge distillation framework, where the
ancestor teacher model pre-trained on base classes is reused in all learning
sessions, and the father teacher model derives the current student model via
adaptation. On the other hand, in order to cope with the few-shot learning
scenario and alleviate the corresponding new class overfitting problem,
Prompt-KD is also equipped with a prompt learning mechanism. Extensive
experiments on two benchmark datasets, i.e., FewEvent and MAVEN, demonstrate
the superior performance of Prompt-KD.
\\ ( https://arxiv.org/abs/2404.01767 ,  14532kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01768
Date: Tue, 2 Apr 2024 09:31:32 GMT   (17266kb,D)

Title: Auditing Large Language Models for Enhanced Text-Based Stereotype
  Detection and Probing-Based Bias Evaluation
Authors: Zekun Wu, Sahan Bulathwela, Maria Perez-Ortiz, Adriano Soares
  Koshiyama
Categories: cs.CL cs.AI
Comments: Under reviewed as a conference paper at COLM 2024
\\
  Recent advancements in Large Language Models (LLMs) have significantly
increased their presence in human-facing Artificial Intelligence (AI)
applications. However, LLMs could reproduce and even exacerbate stereotypical
outputs from training data. This work introduces the Multi-Grain Stereotype
(MGS) dataset, encompassing 51,867 instances across gender, race, profession,
religion, and stereotypical text, collected by fusing multiple previously
publicly available stereotype detection datasets. We explore different machine
learning approaches aimed at establishing baselines for stereotype detection,
and fine-tune several language models of various architectures and model sizes,
presenting in this work a series of stereotypes classifier models for English
text trained on MGS. To understand whether our stereotype detectors capture
relevant features (aligning with human common sense) we utilise a variety of
explanainable AI tools, including SHAP, LIME, and BertViz, and analyse a series
of example cases discussing the results. Finally, we develop a series of
stereotype elicitation prompts and evaluate the presence of stereotypes in text
generation tasks with popular LLMs, using one of our best performing previously
presented stereotypes detectors. Our experiments yielded several key findings:
i) Training stereotype detectors in a multi-dimension setting yields better
results than training multiple single-dimension classifiers.ii) The integrated
MGS Dataset enhances both the in-dataset and cross-dataset generalisation
ability of stereotype detectors compared to using the datasets separately. iii)
There is a reduction in stereotypes in the content generated by GPT Family LLMs
with newer versions.
\\ ( https://arxiv.org/abs/2404.01768 ,  17266kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01785
Date: Tue, 2 Apr 2024 09:49:07 GMT   (1683kb,D)

Title: Can Humans Identify Domains?
Authors: Maria Barrett and Max M\"uller-Eberstein and Elisa Bassignana and
  Amalie Brogaard Pauli and Mike Zhang and Rob van der Goot
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  Textual domain is a crucial property within the Natural Language Processing
(NLP) community due to its effects on downstream model performance. The concept
itself is, however, loosely defined and, in practice, refers to any
non-typological property, such as genre, topic, medium or style of a document.
We investigate the core notion of domains via human proficiency in identifying
related intrinsic textual properties, specifically the concepts of genre
(communicative purpose) and topic (subject matter). We publish our annotations
in *TGeGUM*: A collection of 9.1k sentences from the GUM dataset (Zeldes, 2017)
with single sentence and larger context (i.e., prose) annotations for one of 11
genres (source type), and its topic/subtopic as per the Dewey Decimal library
classification system (Dewey, 1979), consisting of 10/100 hierarchical topics
of increased granularity. Each instance is annotated by three annotators, for a
total of 32.7k annotations, allowing us to examine the level of human
disagreement and the relative difficulty of each annotation task. With a
Fleiss' kappa of at most 0.53 on the sentence level and 0.66 at the prose
level, it is evident that despite the ubiquity of domains in NLP, there is
little human consensus on how to define them. By training classifiers to
perform the same task, we find that this uncertainty also extends to NLP
models.
\\ ( https://arxiv.org/abs/2404.01785 ,  1683kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01786
Date: Tue, 2 Apr 2024 09:49:53 GMT   (1706kb)

Title: Generative AI-Based Text Generation Methods Using Pre-Trained GPT-2
  Model
Authors: Rohit Pandey, Hetvi Waghela, Sneha Rakshit, Aparna Rangari, Anjali
  Singh, Rahul Kumar, Ratnadeep Ghosal, Jaydip Sen
Categories: cs.CL
Comments: This report pertains to the Capstone Project done by Group 5 of the
  Fall batch of 2023 students at Praxis Tech School, Kolkata, India. The
  reports consists of 57 pages and it includes 17 figures and 8 tables. This is
  the preprint which will be submitted to IEEE CONIT 2024 for review
\\
  This work delved into the realm of automatic text generation, exploring a
variety of techniques ranging from traditional deterministic approaches to more
modern stochastic methods. Through analysis of greedy search, beam search,
top-k sampling, top-p sampling, contrastive searching, and locally typical
searching, this work has provided valuable insights into the strengths,
weaknesses, and potential applications of each method. Each text-generating
method is evaluated using several standard metrics and a comparative study has
been made on the performance of the approaches. Finally, some future directions
of research in the field of automatic text generation are also identified.
\\ ( https://arxiv.org/abs/2404.01786 ,  1706kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01799
Date: Tue, 2 Apr 2024 09:58:57 GMT   (130kb,D)

Title: PATCH -- Psychometrics-AssisTed benCHmarking of Large Language Models: A
  Case Study of Mathematics Proficiency
Authors: Qixiang Fang, Daniel L. Oberski, Dong Nguyen
Categories: cs.CL cs.CY
\\
  Many existing benchmarks of large (multimodal) language models (LLMs) focus
on measuring LLMs' academic proficiency, often with also an interest in
comparing model performance with human test takers. While these benchmarks have
proven key to the development of LLMs, they suffer from several limitations,
including questionable measurement quality (e.g., Do they measure what they are
supposed to in a reliable way?), lack of quality assessment on the item level
(e.g., Are some items more important or difficult than others?) and unclear
human population reference (e.g., To whom can the model be compared?). In
response to these challenges, we propose leveraging knowledge from
psychometrics - a field dedicated to the measurement of latent variables like
academic proficiency - into LLM benchmarking. We make three primary
contributions. First, we introduce PATCH: a novel framework for
Psychometrics-AssisTed benCHmarking of LLMs. PATCH addresses the aforementioned
limitations, presenting a new direction for LLM benchmark research. Second, we
implement PATCH by measuring GPT-4 and Gemini-Pro-Vision's proficiency in 8th
grade mathematics against 56 human populations. We show that adopting a
psychometrics-based approach yields evaluation outcomes that diverge from those
based on existing benchmarking practices. Third, we release 4 datasets to
support measuring and comparing LLM proficiency in grade school mathematics and
science against human populations.
\\ ( https://arxiv.org/abs/2404.01799 ,  130kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01854
Date: Tue, 2 Apr 2024 11:32:58 GMT   (7643kb,D)

Title: IndoCulture: Exploring Geographically-Influenced Cultural Commonsense
  Reasoning Across Eleven Indonesian Provinces
Authors: Fajri Koto and Rahmad Mahendra and Nurul Aisyah and Timothy Baldwin
Categories: cs.CL
\\
  Although commonsense reasoning is greatly shaped by cultural and geographical
factors, previous studies on language models have predominantly centered on
English cultures, potentially resulting in an Anglocentric bias. In this paper,
we introduce IndoCulture, aimed at understanding the influence of geographical
factors on language model reasoning ability, with a specific emphasis on the
diverse cultures found within eleven Indonesian provinces. In contrast to prior
works that relied on templates (Yin et al., 2022) and online scrapping (Fung et
al., 2024), we created IndoCulture by asking local people to manually develop
the context and plausible options based on predefined topics. Evaluations of 23
language models reveal several insights: (1) even the best open-source model
struggles with an accuracy of 53.2%, (2) models often provide more accurate
predictions for specific provinces, such as Bali and West Java, and (3) the
inclusion of location contexts enhances performance, especially in larger
models like GPT-4, emphasizing the significance of geographical context in
commonsense reasoning.
\\ ( https://arxiv.org/abs/2404.01854 ,  7643kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01856
Date: Tue, 2 Apr 2024 11:34:12 GMT   (331kb,D)

Title: Poro 34B and the Blessing of Multilinguality
Authors: Risto Luukkonen, Jonathan Burdge, Elaine Zosa, Aarne Talman, Ville
  Komulainen, V\"ain\"o Hatanp\"a\"a, Peter Sarlin, Sampo Pyysalo
Categories: cs.CL
\\
  The pretraining of state-of-the-art large language models now requires
trillions of words of text, which is orders of magnitude more than available
for the vast majority of languages. While including text in more than one
language is an obvious way to acquire more pretraining data, multilinguality is
often seen as a curse, and most model training efforts continue to focus
near-exclusively on individual large languages. We believe that multilinguality
can be a blessing and that it should be possible to substantially improve over
the capabilities of monolingual models for small languages through multilingual
training. In this study, we introduce Poro 34B, a 34 billion parameter model
trained for 1 trillion tokens of Finnish, English, and programming languages,
and demonstrate that a multilingual training approach can produce a model that
not only substantially advances over the capabilities of existing models for
Finnish, but also excels in translation and is competitive in its class in
generating English and programming languages. We release the model parameters,
scripts, and data under open licenses at
https://huggingface.co/LumiOpen/Poro-34B.
\\ ( https://arxiv.org/abs/2404.01856 ,  331kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01860
Date: Tue, 2 Apr 2024 11:38:11 GMT   (395kb,D)

Title: Self-StrAE at SemEval-2024 Task 1: Making Self-Structuring AutoEncoders
  Learn More With Less
Authors: Mattia Opper and N. Siddharth
Categories: cs.CL
Comments: SemEval 2024
\\
  This paper presents two simple improvements to the Self-Structuring
AutoEncoder (Self-StrAE). Firstly, we show that including reconstruction to the
vocabulary as an auxiliary objective improves representation quality. Secondly,
we demonstrate that increasing the number of independent channels leads to
significant improvements in embedding quality, while simultaneously reducing
the number of parameters. Surprisingly, we demonstrate that this trend can be
followed to the extreme, even to point of reducing the total number of
non-embedding parameters to seven. Our system can be pre-trained from scratch
with as little as 10M tokens of input data, and proves effective across
English, Spanish and Afrikaans.
\\ ( https://arxiv.org/abs/2404.01860 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01869
Date: Tue, 2 Apr 2024 11:46:31 GMT   (381kb,D)

Title: Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language
  Models -- A Survey
Authors: Philipp Mondorf and Barbara Plank
Categories: cs.CL cs.AI
Comments: 26 pages, 2 figures
\\
  Large language models (LLMs) have recently shown impressive performance on
tasks involving reasoning, leading to a lively debate on whether these models
possess reasoning capabilities similar to humans. However, despite these
successes, the depth of LLMs' reasoning abilities remains uncertain. This
uncertainty partly stems from the predominant focus on task performance,
measured through shallow accuracy metrics, rather than a thorough investigation
of the models' reasoning behavior. This paper seeks to address this gap by
providing a comprehensive review of studies that go beyond task accuracy,
offering deeper insights into the models' reasoning processes. Furthermore, we
survey prevalent methodologies to evaluate the reasoning behavior of LLMs,
emphasizing current trends and efforts towards more nuanced reasoning analyses.
Our review suggests that LLMs tend to rely on surface-level patterns and
correlations in their training data, rather than on genuine reasoning
abilities. Additionally, we identify the need for further research that
delineates the key differences between human and LLM-based reasoning. Through
this survey, we aim to shed light on the complex reasoning processes within
LLMs.
\\ ( https://arxiv.org/abs/2404.01869 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01903
Date: Tue, 2 Apr 2024 12:44:44 GMT   (86kb,D)

Title: Activation Steering for Robust Type Prediction in CodeLLMs
Authors: Francesca Lucchetti and Arjun Guha
Categories: cs.CL cs.LG cs.PL
Comments: 16 pages, 7 figures
\\
  Contemporary LLMs pretrained on code are capable of succeeding at a wide
variety of programming tasks. However, their performance is very sensitive to
syntactic features, such as the names of variables and types, the structure of
code, and presence of type hints. We contribute an inference-time technique to
make CodeLLMs more robust to syntactic distractors that are semantically
irrelevant. Our methodology relies on activation steering, which involves
editing internal model activations to steer the model towards the correct
prediction. We contribute a novel way to construct steering vectors by taking
inspiration from mutation testing, which constructs minimal semantics-breaking
code edits. In contrast, we construct steering vectors from
semantics-preserving code edits. We apply our approach to the task of type
prediction for the gradually typed languages Python and TypeScript. This
approach corrects up to 90% of type mispredictions. Finally, we show that
steering vectors calculated from Python activations reliably correct type
mispredictions in TypeScript, and vice versa. This result suggests that LLMs
may be learning to transfer knowledge of types across programming languages.
\\ ( https://arxiv.org/abs/2404.01903 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01907
Date: Tue, 2 Apr 2024 12:49:22 GMT   (270kb,D)

Title: Humanizing Machine-Generated Content: Evading AI-Text Detection through
  Adversarial Attack
Authors: Ying Zhou, Ben He, Le Sun
Categories: cs.CL cs.CR cs.LG
Comments: Accepted by COLING 2024
\\
  With the development of large language models (LLMs), detecting whether text
is generated by a machine becomes increasingly challenging in the face of
malicious use cases like the spread of false information, protection of
intellectual property, and prevention of academic plagiarism. While
well-trained text detectors have demonstrated promising performance on unseen
test data, recent research suggests that these detectors have vulnerabilities
when dealing with adversarial attacks such as paraphrasing. In this paper, we
propose a framework for a broader class of adversarial attacks, designed to
perform minor perturbations in machine-generated content to evade detection. We
consider two attack settings: white-box and black-box, and employ adversarial
learning in dynamic scenarios to assess the potential enhancement of the
current detection model's robustness against such attacks. The empirical
results reveal that the current detection models can be compromised in as
little as 10 seconds, leading to the misclassification of machine-generated
text as human-written content. Furthermore, we explore the prospect of
improving the model's robustness over iterative adversarial learning. Although
some improvements in model robustness are observed, practical applications
still face significant challenges. These findings shed light on the future
development of AI-text detectors, emphasizing the need for more accurate and
robust detection methods.
\\ ( https://arxiv.org/abs/2404.01907 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01914
Date: Tue, 2 Apr 2024 13:05:41 GMT   (4510kb,D)

Title: SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity
  Recognition of Unseen Entities
Authors: Hyunjong Ok, Taeho Kil, Sukmin Seo, Jaeho Lee
Categories: cs.CL cs.AI
Comments: 13 pages, 7 figures, NAACL 2024
\\
  Recent advances in named entity recognition (NER) have pushed the boundary of
the task to incorporate visual signals, leading to many variants, including
multi-modal NER (MNER) or grounded MNER (GMNER). A key challenge to these tasks
is that the model should be able to generalize to the entities unseen during
the training, and should be able to handle the training samples with noisy
annotations. To address this obstacle, we propose SCANNER (Span CANdidate
detection and recognition for NER), a model capable of effectively handling all
three NER variants. SCANNER is a two-stage structure; we extract entity
candidates in the first stage and use it as a query to get knowledge,
effectively pulling knowledge from various sources. We can boost our
performance by utilizing this entity-centric extracted knowledge to address
unseen entities. Furthermore, to tackle the challenges arising from noisy
annotations in NER datasets, we introduce a novel self-distillation method,
enhancing the robustness and accuracy of our model in processing training data
with inherent uncertainties. Our approach demonstrates competitive performance
on the NER benchmark and surpasses existing methods on both MNER and GMNER
benchmarks. Further analysis shows that the proposed distillation and knowledge
utilization methods improve the performance of our model on various benchmarks.
\\ ( https://arxiv.org/abs/2404.01914 ,  4510kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01921
Date: Tue, 2 Apr 2024 13:15:07 GMT   (4005kb,D)

Title: A Rationale-centric Counterfactual Data Augmentation Method for
  Cross-Document Event Coreference Resolution
Authors: Bowen Ding, Qingkai Min, Shengkun Ma, Yingjie Li, Linyi Yang and Yue
  Zhang
Categories: cs.CL
Comments: Accepted to NAACL-24 Main
\\
  Based on Pre-trained Language Models (PLMs), event coreference resolution
(ECR) systems have demonstrated outstanding performance in clustering
coreferential events across documents. However, the existing system exhibits an
excessive reliance on the `triggers lexical matching' spurious pattern in the
input mention pair text. We formalize the decision-making process of the
baseline ECR system using a Structural Causal Model (SCM), aiming to identify
spurious and causal associations (i.e., rationales) within the ECR task.
Leveraging the debiasing capability of counterfactual data augmentation, we
develop a rationale-centric counterfactual data augmentation method with
LLM-in-the-loop. This method is specialized for pairwise input in the ECR
system, where we conduct direct interventions on triggers and context to
mitigate the spurious association while emphasizing the causation. Our approach
achieves state-of-the-art performance on three popular cross-document ECR
benchmarks and demonstrates robustness in out-of-domain scenarios.
\\ ( https://arxiv.org/abs/2404.01921 ,  4005kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01923
Date: Tue, 2 Apr 2024 13:17:36 GMT   (2367kb,D)

Title: SGSH: Stimulate Large Language Models with Skeleton Heuristics for
  Knowledge Base Question Generation
Authors: Shasha Guo, Lizi Liao, Jing Zhang, Yanling Wang, Cuiping Li, Hong Chen
Categories: cs.CL cs.AI
Comments: Accepted by NAACL 2024 Findings
\\
  Knowledge base question generation (KBQG) aims to generate natural language
questions from a set of triplet facts extracted from KB. Existing methods have
significantly boosted the performance of KBQG via pre-trained language models
(PLMs) thanks to the richly endowed semantic knowledge. With the advance of
pre-training techniques, large language models (LLMs) (e.g., GPT-3.5)
undoubtedly possess much more semantic knowledge. Therefore, how to effectively
organize and exploit the abundant knowledge for KBQG becomes the focus of our
study. In this work, we propose SGSH--a simple and effective framework to
Stimulate GPT-3.5 with Skeleton Heuristics to enhance KBQG. The framework
incorporates "skeleton heuristics", which provides more fine-grained guidance
associated with each input to stimulate LLMs to generate optimal questions,
encompassing essential elements like the question phrase and the auxiliary
verb.More specifically, we devise an automatic data construction strategy
leveraging ChatGPT to construct a skeleton training dataset, based on which we
employ a soft prompting approach to train a BART model dedicated to generating
the skeleton associated with each input. Subsequently, skeleton heuristics are
encoded into the prompt to incentivize GPT-3.5 to generate desired questions.
Extensive experiments demonstrate that SGSH derives the new state-of-the-art
performance on the KBQG tasks.
\\ ( https://arxiv.org/abs/2404.01923 ,  2367kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01940
Date: Tue, 2 Apr 2024 13:33:23 GMT   (297kb,D)

Title: Towards Better Understanding of Cybercrime: The Role of Fine-Tuned LLMs
  in Translation
Authors: Veronica Valeros and Anna \v{S}irokova and Carlos Catania and
  Sebastian Garcia
Categories: cs.CL
Comments: 9 pages, 4 figures
\\
  Understanding cybercrime communications is paramount for cybersecurity
defence. This often involves translating communications into English for
processing, interpreting, and generating timely intelligence. The problem is
that translation is hard. Human translation is slow, expensive, and scarce.
Machine translation is inaccurate and biased. We propose using fine-tuned Large
Language Models (LLM) to generate translations that can accurately capture the
nuances of cybercrime language. We apply our technique to public chats from the
NoName057(16) Russian-speaking hacktivist group. Our results show that our
fine-tuned LLM model is better, faster, more accurate, and able to capture
nuances of the language. Our method shows it is possible to achieve
high-fidelity translations and significantly reduce costs by a factor ranging
from 430 to 23,000 compared to a human translator.
\\ ( https://arxiv.org/abs/2404.01940 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01953
Date: Tue, 2 Apr 2024 13:47:52 GMT   (981kb,D)

Title: Classifying Graphemes in English Words Through the Application of a
  Fuzzy Inference System
Authors: Samuel Rose and Chandrasekhar Kambhampati
Categories: cs.CL cs.LO
\\
  In Linguistics, a grapheme is a written unit of a writing system
corresponding to a phonological sound. In Natural Language Processing tasks,
written language is analysed through two different mediums, word analysis, and
character analysis. This paper focuses on a third approach, the analysis of
graphemes. Graphemes have advantages over word and character analysis by being
self-contained representations of phonetic sounds. Due to the nature of
splitting a word into graphemes being based on complex, non-binary rules, the
application of fuzzy logic would provide a suitable medium upon which to
predict the number of graphemes in a word. This paper proposes the application
of a Fuzzy Inference System to split words into their graphemes. This Fuzzy
Inference System results in a correct prediction of the number of graphemes in
a word 50.18% of the time, with 93.51% being within a margin of +- 1 from the
correct classification. Given the variety in language, graphemes are tied with
pronunciation and therefore can change depending on a regional accent/dialect,
the +- 1 accuracy represents the impreciseness of grapheme classification when
regional variances are accounted for. To give a baseline of comparison, a
second method involving a recursive IPA mapping exercise using a pronunciation
dictionary was developed to allow for comparisons to be made.
\\ ( https://arxiv.org/abs/2404.01953 ,  981kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01954
Date: Tue, 2 Apr 2024 13:48:49 GMT   (8273kb,D)

Title: HyperCLOVA X Technical Report
Authors: Kang Min Yoo, Jaegeun Han, Sookyo In, Heewon Jeon, Jisu Jeong, Jaewook
  Kang, Hyunwook Kim, Kyung-Min Kim, Munhyong Kim, Sungju Kim, Donghyun Kwak,
  Hanock Kwak, Se Jung Kwon, Bado Lee, Dongsoo Lee, Gichang Lee, Jooho Lee,
  Baeseong Park, Seongjin Shin, Joonsang Yu, Seolki Baek, Sumin Byeon, Eungsup
  Cho, Dooseok Choe, Jeesung Han, Youngkyun Jin, Hyein Jun, Jaeseung Jung,
  Chanwoong Kim, Jinhong Kim, Jinuk Kim, Dokyeong Lee, Dongwook Park, Jeong Min
  Sohn, Sujung Han, Jiae Heo, Sungju Hong, Mina Jeon, Hyunhoon Jung, Jungeun
  Jung, Wangkyo Jung, Chungjoon Kim, Hyeri Kim, Jonghyun Kim, Min Young Kim,
  Soeun Lee, Joonhee Park, Jieun Shin, Sojin Yang, Jungsoon Yoon, Hwaran Lee,
  Sanghwan Bae, Jeehwan Cha, Donghoon Ham, Youngki Hong, Yunki Hong, Myunggeun
  Ji, Yeguk Jin, Chansong Jo, Shinyoung Joo, Seunghwan Jung, et al. (316
  additional authors not shown)
Categories: cs.CL cs.AI
Comments: 44 pages
\\
  We introduce HyperCLOVA X, a family of large language models (LLMs) tailored
to the Korean language and culture, along with competitive capabilities in
English, math, and coding. HyperCLOVA X was trained on a balanced mix of
Korean, English, and code data, followed by instruction-tuning with
high-quality human-annotated datasets while abiding by strict safety guidelines
reflecting our commitment to responsible AI. The model is evaluated across
various benchmarks, including comprehensive reasoning, knowledge, commonsense,
factuality, coding, math, chatting, instruction-following, and harmlessness, in
both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in
Korean backed by a deep understanding of the language and cultural nuances.
Further analysis of the inherent bilingual nature and its extension to
multilingualism highlights the model's cross-lingual proficiency and strong
generalization ability to untargeted languages, including machine translation
between several language pairs and cross-lingual inference tasks. We believe
that HyperCLOVA X can provide helpful guidance for regions or countries in
developing their sovereign LLMs.
\\ ( https://arxiv.org/abs/2404.01954 ,  8273kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01961
Date: Tue, 2 Apr 2024 13:55:05 GMT   (7283kb,D)

Title: Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument
  Reasoning in Civil Procedures with GPT4
Authors: Dan Schumacher and Anthony Rios
Categories: cs.CL
Comments: Accepted to SemEval@NAACL 2024
\\
  In this paper, we present our system for the SemEval Task 5, The Legal
Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning
is an essential skill that all law students must master. Moreover, it is
important to develop natural language processing solutions that can reason
about a question given terse domain-specific contextual information. Our system
explores a prompt-based solution using GPT4 to reason over legal arguments. We
also evaluate an ensemble of prompting strategies, including chain-of-thought
reasoning and in-context learning. Overall, our system results in a Macro F1 of
.8095 on the validation dataset and .7315 (5th out of 21 teams) on the final
test set. Code for this project is available at
https://github.com/danschumac1/CivilPromptReasoningGPT4.
\\ ( https://arxiv.org/abs/2404.01961 ,  7283kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01991
Date: Tue, 2 Apr 2024 14:31:14 GMT   (847kb,D)

Title: Kallaama: A Transcribed Speech Dataset about Agriculture in the Three
  Most Widely Spoken Languages in Senegal
Authors: Elodie Gauthier, Aminata Ndiaye, Abdoulaye Guiss\'e
Categories: cs.CL
Comments: To appear in RAIL 2024
\\
  This work is part of the Kallaama project, whose objective is to produce and
disseminate national languages corpora for speech technologies developments, in
the field of agriculture. Except for Wolof, which benefits from some language
data for natural language processing, national languages of Senegal are largely
ignored by language technology providers. However, such technologies are keys
to the protection, promotion and teaching of these languages. Kallaama focuses
on the 3 main spoken languages by Senegalese people: Wolof, Pulaar and Sereer.
These languages are widely spoken by the population, with around 10 million of
native Senegalese speakers, not to mention those outside the country. However,
they remain under-resourced in terms of machine-readable data that can be used
for automatic processing and language technologies, all the more so in the
agricultural sector. We release a transcribed speech dataset containing 125
hours of recordings, about agriculture, in each of the above-mentioned
languages. These resources are specifically designed for Automatic Speech
Recognition purpose, including traditional approaches. To build such
technologies, we provide textual corpora in Wolof and Pulaar, and a
pronunciation lexicon containing 49,132 entries from the Wolof dataset.
\\ ( https://arxiv.org/abs/2404.01991 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01992
Date: Tue, 2 Apr 2024 14:35:08 GMT   (2615kb,D)

Title: Dissecting Paraphrases: The Impact of Prompt Syntax and supplementary
  Information on Knowledge Retrieval from Pretrained Language Models
Authors: Stephan Linzbach, Dimitar Dimitrov, Laura Kallmeyer, Kilian Evang,
  Hajira Jabeen, and Stefan Dietze
Categories: cs.CL
Comments: Accepted for NAACL 2024
\\
  Pre-trained Language Models (PLMs) are known to contain various kinds of
knowledge. One method to infer relational knowledge is through the use of
cloze-style prompts, where a model is tasked to predict missing subjects or
objects. Typically, designing these prompts is a tedious task because small
differences in syntax or semantics can have a substantial impact on knowledge
retrieval performance. Simultaneously, evaluating the impact of either prompt
syntax or information is challenging due to their interdependence. We designed
CONPARE-LAMA - a dedicated probe, consisting of 34 million distinct prompts
that facilitate comparison across minimal paraphrases. These paraphrases follow
a unified meta-template enabling the controlled variation of syntax and
semantics across arbitrary relations. CONPARE-LAMA enables insights into the
independent impact of either syntactical form or semantic information of
paraphrases on the knowledge retrieval performance of PLMs. Extensive knowledge
retrieval experiments using our probe reveal that prompts following clausal
syntax have several desirable properties in comparison to appositive syntax: i)
they are more useful when querying PLMs with a combination of supplementary
information, ii) knowledge is more consistently recalled across different
combinations of supplementary information, and iii) they decrease response
uncertainty when retrieving known facts. In addition, range information can
boost knowledge retrieval performance more than domain information, even though
domain information is more reliably helpful across syntactic forms.
\\ ( https://arxiv.org/abs/2404.01992 ,  2615kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02000
Date: Tue, 2 Apr 2024 14:43:36 GMT   (53kb)

Title: Africa-Centric Self-Supervised Pre-Training for Multilingual Speech
  Representation in a Sub-Saharan Context
Authors: Antoine Caubri\`ere and Elodie Gauthier
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: To appear in AfricaNLP 2024
\\
  We present the first self-supervised multilingual speech model trained
exclusively on African speech. The model learned from nearly 60 000 hours of
unlabeled speech segments in 21 languages and dialects spoken in sub-Saharan
Africa. On the SSA subset of the FLEURS-102 dataset, our approach based on a
HuBERT$_{base}$ (0.09B) architecture shows competitive results, for ASR
downstream task, compared to the w2v-bert-51 (0.6B) pre-trained model proposed
in the FLEURS benchmark, while being more efficient by using 7x less data and
6x less parameters. Furthermore, in the context of a LID downstream task, our
approach outperforms FLEURS baselines accuracy by over 22\%.
\\ ( https://arxiv.org/abs/2404.02000 ,  53kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02009
Date: Tue, 2 Apr 2024 14:53:41 GMT   (662kb,D)

Title: Preuve de concept d'un bot vocal dialoguant en wolof
Authors: Elodie Gauthier, Papa-S\'ega Wade, Thierry Moudenc, Patrice Collen,
  Emilie De Neef, Oumar Ba, Ndeye Khoyane Cama, Cheikh Ahmadou Bamba Kebe,
  Ndeye Aissatou Gningue, Thomas Mendo'o Aristide
Categories: cs.CL cs.HC
Comments: in French language
Journal-ref: Actes de la 29e Conf\'erence sur le Traitement Automatique des
  Langues Naturelles. Volume 1 : conf\'erence principale (Est\`eve et al.,
  JEP/TALN/RECITAL 2022)
\\
  This paper presents the proof-of-concept of the first automatic voice
assistant ever built in Wolof language, the main vehicular language spoken in
Senegal. This voicebot is the result of a collaborative research project
between Orange Innovation in France, Orange Senegal (aka Sonatel) and ADNCorp,
a small IT company based in Dakar, Senegal. The purpose of the voicebot is to
provide information to Orange customers about the Sargal loyalty program of
Orange Senegal by using the most natural mean to communicate: speech. The
voicebot receives in input the customer's oral request that is then processed
by a SLU system to reply to the customer's request using audio recordings. The
first results of this proof-of-concept are encouraging as we achieved 22\% of
WER for the ASR task and 78\% of F1-score on the NLU task.
\\ ( https://arxiv.org/abs/2404.02009 ,  662kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02013
Date: Tue, 2 Apr 2024 14:55:47 GMT   (543kb,D)

Title: Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi,
  Tamil, and Indian English Online Spaces
Authors: Advaitha Vetagiri, Gyandeep Kalita, Eisha Halder, Chetna Taparia,
  Partha Pakray, Riyanka Manna
Categories: cs.CL
\\
  Online gender-based harassment is a widespread issue limiting the free
expression and participation of women and marginalized genders in digital
spaces. Detecting such abusive content can enable platforms to curb this
menace. We participated in the Gendered Abuse Detection in Indic Languages
shared task at ICON2023 that provided datasets of annotated Twitter posts in
English, Hindi and Tamil for building classifiers to identify gendered abuse.
Our team CNLP-NITS-PP developed an ensemble approach combining CNN and BiLSTM
networks that can effectively model semantic and sequential patterns in textual
data. The CNN captures localized features indicative of abusive language
through its convolution filters applied on embedded input text. To determine
context-based offensiveness, the BiLSTM analyzes this sequence for dependencies
among words and phrases. Multiple variations were trained using FastText and
GloVe word embeddings for each language dataset comprising over 7,600
crowdsourced annotations across labels for explicit abuse, targeted minority
attacks and general offences. The validation scores showed strong performance
across f1-measures, especially for English 0.84. Our experiments reveal how
customizing embeddings and model hyperparameters can improve detection
capability. The proposed architecture ranked 1st in the competition, proving
its ability to handle real-world noisy text with code-switching. This technique
has a promising scope as platforms aim to combat cyber harassment facing Indic
language internet users. Our Code is at
https://github.com/advaithavetagiri/CNLP-NITS-PP
\\ ( https://arxiv.org/abs/2404.02013 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02022
Date: Tue, 2 Apr 2024 15:10:11 GMT   (11290kb,D)

Title: Improving Retrieval Augmented Open-Domain Question-Answering with
  Vectorized Contexts
Authors: Zhuo Chen, Xinyu Wang, Yong Jiang, Pengjun Xie, Fei Huang, Kewei Tu
Categories: cs.CL
\\
  In the era of large language models, applying techniques such as Retrieval
Augmented Generation can better address Open-Domain Question-Answering
problems. Due to constraints including model sizes and computing resources, the
length of context is often limited, and it becomes challenging to empower the
model to cover overlong contexts while answering questions from open domains.
This paper proposes a general and convenient method to covering longer contexts
in Open-Domain Question-Answering tasks. It leverages a small encoder language
model that effectively encodes contexts, and the encoding applies
cross-attention with origin inputs. With our method, the origin language models
can cover several times longer contexts while keeping the computing
requirements close to the baseline. Our experiments demonstrate that after
fine-tuning, there is improved performance across two held-in datasets, four
held-out datasets, and also in two In Context Learning settings.
\\ ( https://arxiv.org/abs/2404.02022 ,  11290kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02037
Date: Tue, 2 Apr 2024 15:32:32 GMT   (10820kb,D)

Title: MultiParaDetox: Extending Text Detoxification with Parallel Data to New
  Languages
Authors: Daryna Dementieva, Nikolay Babakov and Alexander Panchenko
Categories: cs.CL cs.AI
Comments: Accepted to NAACL2024
\\
  Text detoxification is a textual style transfer (TST) task where a text is
paraphrased from a toxic surface form, e.g. featuring rude words, to the
neutral register. Recently, text detoxification methods found their
applications in various task such as detoxification of Large Language Models
(LLMs) (Leong et al., 2023; He et al., 2024; Tang et al., 2023) and toxic
speech combating in social networks (Deng et al., 2023; Mun et al., 2023;
Agarwal et al., 2023). All these applications are extremely important to ensure
safe communication in modern digital worlds. However, the previous approaches
for parallel text detoxification corpora collection -- ParaDetox (Logacheva et
al., 2022) and APPADIA (Atwell et al., 2022) -- were explored only in
monolingual setup. In this work, we aim to extend ParaDetox pipeline to
multiple languages presenting MultiParaDetox to automate parallel
detoxification corpus collection for potentially any language. Then, we
experiment with different text detoxification models -- from unsupervised
baselines to LLMs and fine-tuned models on the presented parallel corpora --
showing the great benefit of parallel corpus presence to obtain
state-of-the-art text detoxification models for any language.
\\ ( https://arxiv.org/abs/2404.02037 ,  10820kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02043
Date: Tue, 2 Apr 2024 15:37:09 GMT   (7150kb,D)

Title: Ukrainian Texts Classification: Exploration of Cross-lingual Knowledge
  Transfer Approaches
Authors: Daryna Dementieva, Valeriia Khylenko and Georg Groh
Categories: cs.CL cs.AI
\\
  Despite the extensive amount of labeled datasets in the NLP text
classification field, the persistent imbalance in data availability across
various languages remains evident. Ukrainian, in particular, stands as a
language that still can benefit from the continued refinement of cross-lingual
methodologies. Due to our knowledge, there is a tremendous lack of Ukrainian
corpora for typical text classification tasks. In this work, we leverage the
state-of-the-art advances in NLP, exploring cross-lingual knowledge transfer
methods avoiding manual data curation: large multilingual encoders and
translation systems, LLMs, and language adapters. We test the approaches on
three text classification tasks -- toxicity classification, formality
classification, and natural language inference -- providing the "recipe" for
the optimal setups.
\\ ( https://arxiv.org/abs/2404.02043 ,  7150kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02053
Date: Tue, 2 Apr 2024 15:50:10 GMT   (13579kb,D)

Title: BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights
Authors: Enmin Zhu
Categories: cs.CL cs.CE q-fin.ST
\\
  This paper explores the intersection of Natural Language Processing (NLP) and
financial analysis, focusing on the impact of sentiment analysis in stock price
prediction. We employ BERTopic, an advanced NLP technique, to analyze the
sentiment of topics derived from stock market comments. Our methodology
integrates this sentiment analysis with various deep learning models, renowned
for their effectiveness in time series and stock prediction tasks. Through
comprehensive experiments, we demonstrate that incorporating topic sentiment
notably enhances the performance of these models. The results indicate that
topics in stock market comments provide implicit, valuable insights into stock
market volatility and price trends. This study contributes to the field by
showcasing the potential of NLP in enriching financial analysis and opens up
avenues for further research into real-time sentiment analysis and the
exploration of emotional and contextual aspects of market sentiment. The
integration of advanced NLP techniques like BERTopic with traditional financial
analysis methods marks a step forward in developing more sophisticated tools
for understanding and predicting market behaviors.
\\ ( https://arxiv.org/abs/2404.02053 ,  13579kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02054
Date: Tue, 2 Apr 2024 15:50:55 GMT   (959kb,D)

Title: Deconstructing In-Context Learning: Understanding Prompts via Corruption
Authors: Namrata Shivagunde, Vladislav Lialin, Sherin Muckatira, Anna Rumshisky
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024 main conference. The code is available
  at https://github.com/text-machine-lab/Understanding_prompts_via_corruption
\\
  The ability of large language models (LLMs) to "learn in context" based on
the provided prompt has led to an explosive growth in their use, culminating in
the proliferation of AI assistants such as ChatGPT, Claude, and Bard. These AI
assistants are known to be robust to minor prompt modifications, mostly due to
alignment techniques that use human feedback. In contrast, the underlying
pre-trained LLMs they use as a backbone are known to be brittle in this
respect. Building high-quality backbone models remains a core challenge, and a
common approach to assessing their quality is to conduct few-shot evaluation.
Such evaluation is notorious for being highly sensitive to minor prompt
modifications, as well as the choice of specific in-context examples. Prior
work has examined how modifying different elements of the prompt can affect
model performance. However, these earlier studies tended to concentrate on a
limited number of specific prompt attributes and often produced contradictory
results. Additionally, previous research either focused on models with fewer
than 15 billion parameters or exclusively examined black-box models like GPT-3
or PaLM, making replication challenging. In the present study, we decompose the
entire prompt into four components: task description, demonstration inputs,
labels, and inline instructions provided for each demonstration. We investigate
the effects of structural and semantic corruptions of these elements on model
performance. We study models ranging from 1.5B to 70B in size, using ten
datasets covering classification and generation tasks. We find that repeating
text within the prompt boosts model performance, and bigger models ($\geq$30B)
are more sensitive to the semantics of the prompt. Finally, we observe that
adding task and inline instructions to the demonstrations enhances model
performance even when the instructions are semantically corrupted.
\\ ( https://arxiv.org/abs/2404.02054 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02060
Date: Tue, 2 Apr 2024 15:59:11 GMT   (3640kb,D)

Title: Long-context LLMs Struggle with Long In-context Learning
Authors: Tianle Li, Ge Zhang, Quy Duc Do, Xiang Yue, Wenhu Chen
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) have made significant strides in handling long
sequences exceeding 32K tokens. However, their performance evaluation has
largely been confined to metrics like perplexity and synthetic tasks, which may
not fully capture their abilities in more nuanced, real-world scenarios. This
study introduces a specialized benchmark (LIConBench) focusing on long
in-context learning within the realm of extreme-label classification. We
meticulously selected six datasets with a label range spanning 28 to 174
classes covering different input (few-shot demonstration) length from 2K to
50K. Our benchmark requires LLMs to comprehend the entire input to recognize
the massive label spaces to make correct prediction. We evaluate 13
long-context LLMs on our benchmarks. We find that the long-context LLMs perform
relatively well under the token length of 20K and the performance benefits from
utilizing the long context window. However, after the context window exceeds
20K, most LLMs except GPT-4 will dip dramatically. This suggests a notable gap
in current LLM capabilities for processing and understanding long, context-rich
sequences. Further analysis revealed a tendency among models to favor
predictions for labels presented towards the end at the sequence. Their ability
to reason over multiple pieces in the long sequence is yet to be improved. Our
study reveals that long context understanding and reasoning is still a
challenging task for the existing LLMs. We believe LIConBench could serve as a
more realistic evaluation for the future long context LLMs.
\\ ( https://arxiv.org/abs/2404.02060 ,  3640kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02068
Date: Tue, 2 Apr 2024 16:10:29 GMT   (1680kb,D)

Title: Using Interpretation Methods for Model Enhancement
Authors: Zhuo Chen, Chengyue Jiang, Kewei Tu
Categories: cs.CL
Comments: EMNLP 2023
Journal-ref: Proceedings of the 2023 Conference on Empirical Methods in Natural
  Language Processing, pages 424-438
\\
  In the age of neural natural language processing, there are plenty of works
trying to derive interpretations of neural models. Intuitively, when gold
rationales exist during training, one can additionally train the model to match
its interpretation with the rationales. However, this intuitive idea has not
been fully explored. In this paper, we propose a framework of utilizing
interpretation methods and gold rationales to enhance models. Our framework is
very general in the sense that it can incorporate various interpretation
methods. Previously proposed gradient-based methods can be shown as an instance
of our framework. We also propose two novel instances utilizing two other types
of interpretation methods, erasure/replace-based and extractor-based methods,
for model enhancement. We conduct comprehensive experiments on a variety of
tasks. Experimental results show that our framework is effective especially in
low-resource settings in enhancing models with various interpretation methods,
and our two newly-proposed methods outperform gradient-based methods in most
settings. Code is available at https://github.com/Chord-Chen-30/UIMER.
\\ ( https://arxiv.org/abs/2404.02068 ,  1680kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02088
Date: Tue, 2 Apr 2024 16:32:49 GMT   (371kb,D)

Title: LastResort at SemEval-2024 Task 3: Exploring Multimodal Emotion Cause
  Pair Extraction as Sequence Labelling Task
Authors: Suyash Vardhan Mathur, Akshett Rai Jindal, Hardik Mittal, Manish
  Shrivastava
Categories: cs.CL cs.SD eess.AS
\\
  Conversation is the most natural form of human communication, where each
utterance can range over a variety of possible emotions. While significant work
has been done towards the detection of emotions in text, relatively little work
has been done towards finding the cause of the said emotions, especially in
multimodal settings. SemEval 2024 introduces the task of Multimodal Emotion
Cause Analysis in Conversations, which aims to extract emotions reflected in
individual utterances in a conversation involving multiple modalities (textual,
audio, and visual modalities) along with the corresponding utterances that were
the cause for the emotion. In this paper, we propose models that tackle this
task as an utterance labeling and a sequence labeling problem and perform a
comparative study of these models, involving baselines using different
encoders, using BiLSTM for adding contextual information of the conversation,
and finally adding a CRF layer to try to model the inter-dependencies between
adjacent utterances more effectively. In the official leaderboard for the task,
our architecture was ranked 8th, achieving an F1-score of 0.1759 on the
leaderboard.
\\ ( https://arxiv.org/abs/2404.02088 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02103
Date: Tue, 2 Apr 2024 17:00:11 GMT   (1257kb,D)

Title: CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions
  for RAG systems
Authors: Sara Rosenthal, Avirup Sil, Radu Florian, Salim Roukos
Categories: cs.CL
Comments: 25 pages
\\
  Retrieval Augmented Generation (RAG) has become a popular application for
large language models. It is preferable that successful RAG systems provide
accurate answers that are supported by being grounded in a passage without any
hallucinations. While considerable work is required for building a full RAG
pipeline, being able to benchmark performance is also necessary. We present
ClapNQ, a benchmark Long-form Question Answering dataset for the full RAG
pipeline. ClapNQ includes long answers with grounded gold passages from Natural
Questions (NQ) and a corpus to perform either retrieval, generation, or the
full RAG pipeline. The ClapNQ answers are concise, 3x smaller than the full
passage, and cohesive, with multiple pieces of the passage that are not
contiguous. RAG models must adapt to these properties to be successful at
ClapNQ. We present baseline experiments and analysis for ClapNQ that highlight
areas where there is still significant room for improvement in grounded RAG.
CLAPNQ is publicly available at https://github.com/primeqa/clapnq
\\ ( https://arxiv.org/abs/2404.02103 ,  1257kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02115
Date: Tue, 2 Apr 2024 17:18:48 GMT   (369kb,D)

Title: GINopic: Topic Modeling with Graph Isomorphism Network
Authors: Suman Adhya, Debarshi Kumar Sanyal
Categories: cs.CL cs.LG
Comments: Accepted as a long paper for NAACL 2024 main conference
\\
  Topic modeling is a widely used approach for analyzing and exploring large
document collections. Recent research efforts have incorporated pre-trained
contextualized language models, such as BERT embeddings, into topic modeling.
However, they often neglect the intrinsic informational value conveyed by
mutual dependencies between words. In this study, we introduce GINopic, a topic
modeling framework based on graph isomorphism networks to capture the
correlation between words. By conducting intrinsic (quantitative as well as
qualitative) and extrinsic evaluations on diverse benchmark datasets, we
demonstrate the effectiveness of GINopic compared to existing topic models and
highlight its potential for advancing topic modeling.
\\ ( https://arxiv.org/abs/2404.02115 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02124
Date: Tue, 2 Apr 2024 17:31:58 GMT   (6948kb,D)

Title: Exploring Automated Distractor Generation for Math Multiple-choice
  Questions via Large Language Models
Authors: Wanyong Feng, Jaewook Lee, Hunter McNichols, Alexander Scarlatos,
  Digory Smith, Simon Woodhead, Nancy Otero Ornelas, Andrew Lan
Categories: cs.CL
Comments: NAACL 2024 findings
\\
  Multiple-choice questions (MCQs) are ubiquitous in almost all levels of
education since they are easy to administer, grade, and are a reliable format
in assessments and practices. One of the most important aspects of MCQs is the
distractors, i.e., incorrect options that are designed to target common errors
or misconceptions among real students. To date, the task of crafting
high-quality distractors largely remains a labor and time-intensive process for
teachers and learning content designers, which has limited scalability. In this
work, we study the task of automated distractor generation in the domain of
math MCQs and explore a wide variety of large language model (LLM)-based
approaches, from in-context learning to fine-tuning. We conduct extensive
experiments using a real-world math MCQ dataset and find that although LLMs can
generate some mathematically valid distractors, they are less adept at
anticipating common errors or misconceptions among real students.
\\ ( https://arxiv.org/abs/2404.02124 ,  6948kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02126
Date: Tue, 2 Apr 2024 17:33:00 GMT   (8582kb,D)

Title: Rematch: Robust and Efficient Matching of Local Knowledge Graphs to
  Improve Structural and Semantic Similarity
Authors: Zoher Kachwala, Jisun An, Haewoon Kwak, Filippo Menczer
Categories: cs.CL cs.IR
Comments: To be published in NAACL24 proceedings
\\
  Knowledge graphs play a pivotal role in various applications, such as
question-answering and fact-checking. Abstract Meaning Representation (AMR)
represents text as knowledge graphs. Evaluating the quality of these graphs
involves matching them structurally to each other and semantically to the
source text. Existing AMR metrics are inefficient and struggle to capture
semantic similarity. We also lack a systematic evaluation benchmark for
assessing structural similarity between AMR graphs. To overcome these
limitations, we introduce a novel AMR similarity metric, rematch, alongside a
new evaluation for structural similarity called RARE. Among state-of-the-art
metrics, rematch ranks second in structural similarity; and first in semantic
similarity by 1--5 percentage points on the STS-B and SICK-R benchmarks.
Rematch is also five times faster than the next most efficient metric.
\\ ( https://arxiv.org/abs/2404.02126 ,  8582kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02127
Date: Tue, 2 Apr 2024 17:33:34 GMT   (8601kb,D)

Title: FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data
  Mixtures for Legal Reasoning
Authors: Joel Niklaus, Lucia Zheng, Arya D. McCarthy, Christopher Hahn, Brian
  M. Rosen, Peter Henderson, Daniel E. Ho, Garrett Honke, Percy Liang,
  Christopher Manning
Categories: cs.CL cs.AI cs.LG
MSC-class: 68T50
ACM-class: I.2
\\
  Instruction tuning is an important step in making language models useful for
direct user interaction. However, many legal tasks remain out of reach for most
open LLMs and there do not yet exist any large scale instruction datasets for
the domain. This critically limits research in this application area. In this
work, we curate LawInstruct, a large legal instruction dataset, covering 17
jurisdictions, 24 languages and a total of 12M examples. We present evidence
that domain-specific pretraining and instruction tuning improve performance on
LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the
baseline. However, the effect does not generalize across all tasks, training
regimes, model sizes, and other factors. LawInstruct is a resource for
accelerating the development of models with stronger information processing and
decision making capabilities in the legal domain.
\\ ( https://arxiv.org/abs/2404.02127 ,  8601kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01306
Date: Wed, 28 Feb 2024 22:21:47 GMT   (8124kb,D)

Title: NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for
  Large Language Models
Authors: Amit Dhurandhar, Tejaswini Pedapati, Ronny Luss, Soham Dan, Aurelie
  Lozano, Payel Das and Georgios Kollias
Categories: cs.LG cs.CL
\\
  Transformer-based Language Models have become ubiquitous in Natural Language
Processing (NLP) due to their impressive performance on various tasks. However,
expensive training as well as inference remains a significant impediment to
their widespread applicability. While enforcing sparsity at various levels of
the model architecture has found promise in addressing scaling and efficiency
issues, there remains a disconnect between how sparsity affects network
topology. Inspired by brain neuronal networks, we explore sparsity approaches
through the lens of network topology. Specifically, we exploit mechanisms seen
in biological networks, such as preferential attachment and redundant synapse
pruning, and show that principled, model-agnostic sparsity approaches are
performant and efficient across diverse NLP tasks, spanning both classification
(such as natural language inference) and generation (summarization, machine
translation), despite our sole objective not being optimizing performance.
NeuroPrune is competitive with (or sometimes superior to) baselines on
performance and can be up to $10$x faster in terms of training time for a given
level of sparsity, simultaneously exhibiting measurable improvements in
inference time in many cases.
\\ ( https://arxiv.org/abs/2404.01306 ,  8124kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01335
Date: Sat, 30 Mar 2024 13:25:11 GMT   (21519kb,D)

Title: Generative AI for Architectural Design: A Literature Review
Authors: Chengyuan Li, Tianyu Zhang, Xusheng Du, Ye Zhang, Haoran Xie
Categories: cs.LG cs.AI
Comments: 32 pages, 20 figures
\\
  Generative Artificial Intelligence (AI) has pioneered new methodological
paradigms in architectural design, significantly expanding the innovative
potential and efficiency of the design process. This paper explores the
extensive applications of generative AI technologies in architectural design, a
trend that has benefited from the rapid development of deep generative models.
This article provides a comprehensive review of the basic principles of
generative AI and large-scale models and highlights the applications in the
generation of 2D images, videos, and 3D models. In addition, by reviewing the
latest literature from 2020, this paper scrutinizes the impact of generative AI
technologies at different stages of architectural design, from generating
initial architectural 3D forms to producing final architectural imagery. The
marked trend of research growth indicates an increasing inclination within the
architectural design community towards embracing generative AI, thereby
catalyzing a shared enthusiasm for research. These research cases and
methodologies have not only proven to enhance efficiency and innovation
significantly but have also posed challenges to the conventional boundaries of
architectural creativity. Finally, we point out new directions for design
innovation and articulate fresh trajectories for applying generative AI in the
architectural domain. This article provides the first comprehensive literature
review about generative AI for architectural design, and we believe this work
can facilitate more research work on this significant topic in architecture.
\\ ( https://arxiv.org/abs/2404.01335 ,  21519kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01340
Date: Sun, 31 Mar 2024 02:46:27 GMT   (5634kb,D)

Title: From Similarity to Superiority: Channel Clustering for Time Series
  Forecasting
Authors: Jialin Chen, Jan Eric Lenssen, Aosong Feng, Weihua Hu, Matthias Fey,
  Leandros Tassiulas, Jure Leskovec, Rex Ying
Categories: cs.LG cs.AI
Comments: 20 pages, 6 figures
\\
  Time series forecasting has attracted significant attention in recent
decades. Previous studies have demonstrated that the Channel-Independent (CI)
strategy improves forecasting performance by treating different channels
individually, while it leads to poor generalization on unseen instances and
ignores potentially necessary interactions between channels. Conversely, the
Channel-Dependent (CD) strategy mixes all channels with even irrelevant and
indiscriminate information, which, however, results in oversmoothing issues and
limits forecasting accuracy. There is a lack of channel strategy that
effectively balances individual channel treatment for improved forecasting
performance without overlooking essential interactions between channels.
Motivated by our observation of a correlation between the time series model's
performance boost against channel mixing and the intrinsic similarity on a pair
of channels, we developed a novel and adaptable Channel Clustering Module
(CCM). CCM dynamically groups channels characterized by intrinsic similarities
and leverages cluster identity instead of channel identity, combining the best
of CD and CI worlds. Extensive experiments on real-world datasets demonstrate
that CCM can (1) boost the performance of CI and CD models by an average margin
of 2.4% and 7.2% on long-term and short-term forecasting, respectively; (2)
enable zero-shot forecasting with mainstream time series forecasting models;
(3) uncover intrinsic time series patterns among channels and improve
interpretability of complex time series models.
\\ ( https://arxiv.org/abs/2404.01340 ,  5634kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01341
Date: Sun, 31 Mar 2024 05:04:38 GMT   (8522kb,D)

Title: Block-Diagonal Guided DBSCAN Clustering
Authors: Zheng Xing and Weibing Zhao
Categories: cs.LG cs.AI cs.DS
Comments: arXiv admin note: text overlap with arXiv:2009.04552 by other authors
\\
  Cluster analysis plays a crucial role in database mining, and one of the most
widely used algorithms in this field is DBSCAN. However, DBSCAN has several
limitations, such as difficulty in handling high-dimensional large-scale data,
sensitivity to input parameters, and lack of robustness in producing clustering
results. This paper introduces an improved version of DBSCAN that leverages the
block-diagonal property of the similarity graph to guide the clustering
procedure of DBSCAN. The key idea is to construct a graph that measures the
similarity between high-dimensional large-scale data points and has the
potential to be transformed into a block-diagonal form through an unknown
permutation, followed by a cluster-ordering procedure to generate the desired
permutation. The clustering structure can be easily determined by identifying
the diagonal blocks in the permuted graph. We propose a gradient descent-based
method to solve the proposed problem. Additionally, we develop a DBSCAN-based
points traversal algorithm that identifies clusters with high densities in the
graph and generates an augmented ordering of clusters. The block-diagonal
structure of the graph is then achieved through permutation based on the
traversal order, providing a flexible foundation for both automatic and
interactive cluster analysis. We introduce a split-and-refine algorithm to
automatically search for all diagonal blocks in the permuted graph with
theoretically optimal guarantees under specific cases. We extensively evaluate
our proposed approach on twelve challenging real-world benchmark clustering
datasets and demonstrate its superior performance compared to the
state-of-the-art clustering method on every dataset.
\\ ( https://arxiv.org/abs/2404.01341 ,  8522kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01351
Date: Mon, 1 Apr 2024 04:21:49 GMT   (1219kb,D)

Title: AETTA: Label-Free Accuracy Estimation for Test-Time Adaptation
Authors: Taeckyung Lee, Sorn Chottananurak, Taesik Gong, Sung-Ju Lee
Categories: cs.LG cs.AI cs.CV
Comments: Accepted to CVPR 2024
\\
  Test-time adaptation (TTA) has emerged as a viable solution to adapt
pre-trained models to domain shifts using unlabeled test data. However, TTA
faces challenges of adaptation failures due to its reliance on blind adaptation
to unknown test samples in dynamic scenarios. Traditional methods for
out-of-distribution performance estimation are limited by unrealistic
assumptions in the TTA context, such as requiring labeled data or re-training
models. To address this issue, we propose AETTA, a label-free accuracy
estimation algorithm for TTA. We propose the prediction disagreement as the
accuracy estimate, calculated by comparing the target model prediction with
dropout inferences. We then improve the prediction disagreement to extend the
applicability of AETTA under adaptation failures. Our extensive evaluation with
four baselines and six TTA methods demonstrates that AETTA shows an average of
19.8%p more accurate estimation compared with the baselines. We further
demonstrate the effectiveness of accuracy estimation with a model recovery case
study, showcasing the practicality of our model recovery based on accuracy
estimation. The source code is available at https://github.com/taeckyung/AETTA.
\\ ( https://arxiv.org/abs/2404.01351 ,  1219kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01353
Date: Mon, 1 Apr 2024 07:35:15 GMT   (212kb,D)

Title: Efficiently Distilling LLMs for Edge Applications
Authors: Achintya Kundu, Fabian Lim, Aaron Chew, Laura Wynter, Penny Chong,
  Rhui Dih Lee
Categories: cs.LG cs.AI cs.CL
Comments: This paper has been accepted for publication in NAACL 2024 (Industry
  Track)
\\
  Supernet training of LLMs is of great interest in industrial applications as
it confers the ability to produce a palette of smaller models at constant cost,
regardless of the number of models (of different size / latency) produced. We
propose a new method called Multistage Low-rank Fine-tuning of
Super-transformers (MLFS) for parameter-efficient supernet training. We show
that it is possible to obtain high-quality encoder models that are suitable for
commercial edge applications, and that while decoder-only models are resistant
to a comparable degree of compression, decoders can be effectively sliced for a
significant reduction in training time.
\\ ( https://arxiv.org/abs/2404.01353 ,  212kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01356
Date: Mon, 1 Apr 2024 09:29:16 GMT   (2150kb,D)

Title: The Double-Edged Sword of Input Perturbations to Robust Accurate
  Fairness
Authors: Xuran Li, Peng Wu, Yanting Chen, Xingjun Ma, Zhen Zhang, Kaixiang Dong
Categories: cs.LG cs.AI cs.CY
\\
  Deep neural networks (DNNs) are known to be sensitive to adversarial input
perturbations, leading to a reduction in either prediction accuracy or
individual fairness. To jointly characterize the susceptibility of prediction
accuracy and individual fairness to adversarial perturbations, we introduce a
novel robustness definition termed robust accurate fairness. Informally, robust
accurate fairness requires that predictions for an instance and its similar
counterparts consistently align with the ground truth when subjected to input
perturbations. We propose an adversarial attack approach dubbed RAFair to
expose false or biased adversarial defects in DNN, which either deceive
accuracy or compromise individual fairness. Then, we show that such adversarial
instances can be effectively addressed by carefully designed benign
perturbations, correcting their predictions to be accurate and fair. Our work
explores the double-edged sword of input perturbations to robust accurate
fairness in DNN and the potential of using benign perturbations to correct
adversarial instances.
\\ ( https://arxiv.org/abs/2404.01356 ,  2150kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01364
Date: Mon, 1 Apr 2024 17:34:18 GMT   (949kb,D)

Title: Information Plane Analysis Visualization in Deep Learning via Transfer
  Entropy
Authors: Adrian Moldovan, Angel Cataron, Razvan Andonie
Categories: cs.LG cs.AI cs.HC cs.IT math.IT
Journal-ref: 2023 27th International Conference Information Visualisation (IV),
  pages 278-285
DOI: 10.1109/IV60283.2023.00055
\\
  In a feedforward network, Transfer Entropy (TE) can be used to measure the
influence that one layer has on another by quantifying the information transfer
between them during training. According to the Information Bottleneck
principle, a neural model's internal representation should compress the input
data as much as possible while still retaining sufficient information about the
output. Information Plane analysis is a visualization technique used to
understand the trade-off between compression and information preservation in
the context of the Information Bottleneck method by plotting the amount of
information in the input data against the compressed representation. The claim
that there is a causal link between information-theoretic compression and
generalization, measured by mutual information, is plausible, but results from
different studies are conflicting. In contrast to mutual information, TE can
capture temporal relationships between variables. To explore such links, in our
novel approach we use TE to quantify information transfer between neural layers
and perform Information Plane analysis. We obtained encouraging experimental
results, opening the possibility for further investigations.
\\ ( https://arxiv.org/abs/2404.01364 ,  949kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01365
Date: Mon, 1 Apr 2024 17:56:06 GMT   (35261kb,D)

Title: Prompt-prompted Mixture of Experts for Efficient LLM Generation
Authors: Harry Dong, Beidi Chen, Yuejie Chi
Categories: cs.LG cs.AI cs.CL
\\
  With the development of transformer-based large language models (LLMs), they
have been applied to many fields due to their remarkable utility, but this
comes at a considerable computational cost at deployment. Fortunately, some
methods such as pruning or constructing a mixture of experts (MoE) aim at
exploiting sparsity in transformer feedforward (FF) blocks to gain boosts in
speed and reduction in memory requirements. However, these techniques can be
very costly and inflexible in practice, as they often require training or are
restricted to specific types of architectures. To address this, we introduce
GRIFFIN, a novel training-free MoE that selects unique FF experts at the
sequence level for efficient generation across a plethora of LLMs with
different non-ReLU activation functions. This is possible due to a critical
observation that many trained LLMs naturally produce highly structured FF
activation patterns within a sequence, which we call flocking. Despite our
method's simplicity, we show with 50\% of the FF parameters, GRIFFIN maintains
the original model's performance with little to no degradation on a variety of
classification and generation tasks, all while improving latency (e.g.
1.25$\times$ speed-up in Llama 2 13B on an NVIDIA L40). Code will be available
at https://github.com/hdong920/GRIFFIN.
\\ ( https://arxiv.org/abs/2404.01365 ,  35261kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01413
Date: Mon, 1 Apr 2024 18:31:24 GMT   (5804kb,D)

Title: Is Model Collapse Inevitable? Breaking the Curse of Recursion by
  Accumulating Real and Synthetic Data
Authors: Matthias Gerstgrasser, Rylan Schaeffer, Apratim Dey, Rafael Rafailov,
  Henry Sleight, John Hughes, Tomasz Korbak, Rajashree Agrawal, Dhruv Pai,
  Andrey Gromov, Daniel A. Roberts, Diyi Yang, David L. Donoho, Sanmi Koyejo
Categories: cs.LG cs.AI cs.CL cs.ET stat.ML
\\
  The proliferation of generative models, combined with pretraining on
web-scale data, raises a timely question: what happens when these models are
trained on their own generated outputs? Recent investigations into model-data
feedback loops discovered that such loops can lead to model collapse, a
phenomenon where performance progressively degrades with each model-fitting
iteration until the latest model becomes useless. However, several recent
papers studying model collapse assumed that new data replace old data over time
rather than assuming data accumulate over time. In this paper, we compare these
two settings and show that accumulating data prevents model collapse. We begin
by studying an analytically tractable setup in which a sequence of linear
models are fit to the previous models' predictions. Previous work showed if
data are replaced, the test error increases linearly with the number of
model-fitting iterations; we extend this result by proving that if data instead
accumulate, the test error has a finite upper bound independent of the number
of iterations. We next empirically test whether accumulating data similarly
prevents model collapse by pretraining sequences of language models on text
corpora. We confirm that replacing data does indeed cause model collapse, then
demonstrate that accumulating data prevents model collapse; these results hold
across a range of model sizes, architectures and hyperparameters. We further
show that similar results hold for other deep generative models on real data:
diffusion models for molecule generation and variational autoencoders for image
generation. Our work provides consistent theoretical and empirical evidence
that data accumulation mitigates model collapse.
\\ ( https://arxiv.org/abs/2404.01413 ,  5804kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01462
Date: Mon, 1 Apr 2024 20:16:21 GMT   (4848kb,D)

Title: OpenChemIE: An Information Extraction Toolkit For Chemistry Literature
Authors: Vincent Fan and Yujie Qian and Alex Wang and Amber Wang and Connor W.
  Coley and Regina Barzilay
Categories: cs.LG cs.CL cs.IR
Comments: To be submitted to the Journal of Chemical Information and Modeling
\\
  Information extraction from chemistry literature is vital for constructing
up-to-date reaction databases for data-driven chemistry. Complete extraction
requires combining information across text, tables, and figures, whereas prior
work has mainly investigated extracting reactions from single modalities. In
this paper, we present OpenChemIE to address this complex challenge and enable
the extraction of reaction data at the document level. OpenChemIE approaches
the problem in two steps: extracting relevant information from individual
modalities and then integrating the results to obtain a final list of
reactions. For the first step, we employ specialized neural models that each
address a specific task for chemistry information extraction, such as parsing
molecules or reactions from text or figures. We then integrate the information
from these modules using chemistry-informed algorithms, allowing for the
extraction of fine-grained reaction data from reaction condition and substrate
scope investigations. Our machine learning models attain state-of-the-art
performance when evaluated individually, and we meticulously annotate a
challenging dataset of reaction schemes with R-groups to evaluate our pipeline
as a whole, achieving an F1 score of 69.5%. Additionally, the reaction
extraction results of \ours attain an accuracy score of 64.3% when directly
compared against the Reaxys chemical database. We provide OpenChemIE freely to
the public as an open-source package, as well as through a web interface.
\\ ( https://arxiv.org/abs/2404.01462 ,  4848kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01466
Date: Mon, 1 Apr 2024 20:33:29 GMT   (7418kb,D)

Title: TS-CausalNN: Learning Temporal Causal Relations from Non-linear
  Non-stationary Time Series Data
Authors: Omar Faruque, Sahara Ali, Xue Zheng, Jianwu Wang
Categories: cs.LG stat.ME
\\
  The growing availability and importance of time series data across various
domains, including environmental science, epidemiology, and economics, has led
to an increasing need for time-series causal discovery methods that can
identify the intricate relationships in the non-stationary, non-linear, and
often noisy real world data. However, the majority of current time series
causal discovery methods assume stationarity and linear relations in data,
making them infeasible for the task. Further, the recent deep learning-based
methods rely on the traditional causal structure learning approaches making
them computationally expensive. In this paper, we propose a Time-Series Causal
Neural Network (TS-CausalNN) - a deep learning technique to discover
contemporaneous and lagged causal relations simultaneously. Our proposed
architecture comprises (i) convolutional blocks comprising parallel custom
causal layers, (ii) acyclicity constraint, and (iii) optimization techniques
using the augmented Lagrangian approach. In addition to the simple parallel
design, an advantage of the proposed model is that it naturally handles the
non-stationarity and non-linearity of the data. Through experiments on multiple
synthetic and real world datasets, we demonstrate the empirical proficiency of
our proposed approach as compared to several state-of-the-art methods. The
inferred graphs for the real world dataset are in good agreement with the
domain understanding.
\\ ( https://arxiv.org/abs/2404.01466 ,  7418kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01475
Date: Mon, 1 Apr 2024 20:56:25 GMT   (3330kb,D)

Title: Are large language models superhuman chemists?
Authors: Adrian Mirza, Nawaf Alampara, Sreekanth Kunchapu, Benedict Emoekabu,
  Aswanth Krishnan, Mara Wilhelmi, Macjonathan Okereke, Juliane Eberhardt, Amir
  Mohammad Elahi, Maximilian Greiner, Caroline T. Holick, Tanya Gupta, Mehrdad
  Asgari, Christina Glaubitz, Lea C. Klepsch, Yannik K\"oster, Jakob Meyer,
  Santiago Miret, Tim Hoffmann, Fabian Alexander Kreth, Michael Ringleb, Nicole
  Roesner, Ulrich S. Schubert, Leanne M. Stafast, Dinga Wonanke, Michael
  Pieler, Philippe Schwaller, Kevin Maik Jablonka
Categories: cs.LG cond-mat.mtrl-sci cs.AI physics.chem-ph
\\
  Large language models (LLMs) have gained widespread interest due to their
ability to process human language and perform tasks on which they have not been
explicitly trained. This is relevant for the chemical sciences, which face the
problem of small and diverse datasets that are frequently in the form of text.
LLMs have shown promise in addressing these issues and are increasingly being
harnessed to predict chemical properties, optimize reactions, and even design
and conduct experiments autonomously. However, we still have only a very
limited systematic understanding of the chemical reasoning capabilities of
LLMs, which would be required to improve models and mitigate potential harms.
Here, we introduce "ChemBench," an automated framework designed to rigorously
evaluate the chemical knowledge and reasoning abilities of state-of-the-art
LLMs against the expertise of human chemists. We curated more than 7,000
question-answer pairs for a wide array of subfields of the chemical sciences,
evaluated leading open and closed-source LLMs, and found that the best models
outperformed the best human chemists in our study on average. The models,
however, struggle with some chemical reasoning tasks that are easy for human
experts and provide overconfident, misleading predictions, such as about
chemicals' safety profiles. These findings underscore the dual reality that,
although LLMs demonstrate remarkable proficiency in chemical tasks, further
research is critical to enhancing their safety and utility in chemical
sciences. Our findings also indicate a need for adaptations to chemistry
curricula and highlight the importance of continuing to develop evaluation
frameworks to improve safe and useful LLMs.
\\ ( https://arxiv.org/abs/2404.01475 ,  3330kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01487
Date: Mon, 1 Apr 2024 21:12:44 GMT   (3240kb,D)

Title: Explainable AI Integrated Feature Engineering for Wildfire Prediction
Authors: Di Fan, Ayan Biswas, James Paul Ahrens
Categories: cs.LG
Comments: arXiv admin note: text overlap with arXiv:2307.09615 by other authors
\\
  Wildfires present intricate challenges for prediction, necessitating the use
of sophisticated machine learning techniques for effective
modeling\cite{jain2020review}. In our research, we conducted a thorough
assessment of various machine learning algorithms for both classification and
regression tasks relevant to predicting wildfires. We found that for
classifying different types or stages of wildfires, the XGBoost model
outperformed others in terms of accuracy and robustness. Meanwhile, the Random
Forest regression model showed superior results in predicting the extent of
wildfire-affected areas, excelling in both prediction error and explained
variance. Additionally, we developed a hybrid neural network model that
integrates numerical data and image information for simultaneous classification
and regression. To gain deeper insights into the decision-making processes of
these models and identify key contributing features, we utilized eXplainable
Artificial Intelligence (XAI) techniques, including TreeSHAP, LIME, Partial
Dependence Plots (PDP), and Gradient-weighted Class Activation Mapping
(Grad-CAM). These interpretability tools shed light on the significance and
interplay of various features, highlighting the complex factors influencing
wildfire predictions. Our study not only demonstrates the effectiveness of
specific machine learning models in wildfire-related tasks but also underscores
the critical role of model transparency and interpretability in environmental
science applications.
\\ ( https://arxiv.org/abs/2404.01487 ,  3240kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01517
Date: Mon, 1 Apr 2024 22:53:09 GMT   (16707kb,D)

Title: Addressing Heterogeneity in Federated Load Forecasting with
  Personalization Layers
Authors: Shourya Bose, Yu Zhang, Kibaek Kim
Categories: cs.LG eess.SP
\\
  The advent of smart meters has enabled pervasive collection of energy
consumption data for training short-term load forecasting models. In response
to privacy concerns, federated learning (FL) has been proposed as a
privacy-preserving approach for training, but the quality of trained models
degrades as client data becomes heterogeneous. In this paper we propose the use
of personalization layers for load forecasting in a general framework called
PL-FL. We show that PL-FL outperforms FL and purely local training, while
requiring lower communication bandwidth than FL. This is done through extensive
simulations on three different datasets from the NREL ComStock repository.
\\ ( https://arxiv.org/abs/2404.01517 ,  16707kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01542
Date: Tue, 2 Apr 2024 00:54:38 GMT   (8958kb,D)

Title: Predicting the Performance of Foundation Models via
  Agreement-on-the-Line
Authors: Aman Mehra, Rahul Saxena, Taeyoun Kim, Christina Baek, Zico Kolter,
  Aditi Raghunathan
Categories: cs.LG
\\
  Estimating the out-of-distribution performance in regimes where labels are
scarce is critical to safely deploy foundation models. Recently, it was shown
that ensembles of neural networks observe the phenomena
``agreement-on-the-line'', which can be leveraged to reliably predict OOD
performance without labels. However, in contrast to classical neural networks
that are trained on in-distribution data from scratch for numerous epochs,
foundation models undergo minimal finetuning from heavily pretrained weights,
which may reduce the ensemble diversity needed to observe
agreement-on-the-line. In our work, we demonstrate that when lightly finetuning
multiple runs from a $\textit{single}$ foundation model, the choice of
randomness during training (linear head initialization, data ordering, and data
subsetting) can lead to drastically different levels of agreement-on-the-line
in the resulting ensemble. Surprisingly, only random head initialization is
able to reliably induce agreement-on-the-line in finetuned foundation models
across vision and language benchmarks. Second, we demonstrate that ensembles of
$\textit{multiple}$ foundation models pretrained on different datasets but
finetuned on the same task can also show agreement-on-the-line. In total, by
careful construction of a diverse ensemble, we can utilize
agreement-on-the-line-based methods to predict the OOD performance of
foundation models with high precision.
\\ ( https://arxiv.org/abs/2404.01542 ,  8958kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01578
Date: Tue, 2 Apr 2024 02:13:00 GMT   (196kb,D)

Title: GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection
Authors: Namyong Park, Ryan Rossi, Xing Wang, Antoine Simoulin, Nesreen Ahmed,
  Christos Faloutsos
Categories: cs.LG cs.SI
Comments: NeurIPS 2023
\\
  The choice of a graph learning (GL) model (i.e., a GL algorithm and its
hyperparameter settings) has a significant impact on the performance of
downstream tasks. However, selecting the right GL model becomes increasingly
difficult and time consuming as more and more GL models are developed.
Accordingly, it is of great significance and practical value to equip users of
GL with the ability to perform a near-instantaneous selection of an effective
GL model without manual intervention. Despite the recent attempts to tackle
this important problem, there has been no comprehensive benchmark environment
to evaluate the performance of GL model selection methods. To bridge this gap,
we present GLEMOS in this work, a comprehensive benchmark for instantaneous GL
model selection that makes the following contributions. (i) GLEMOS provides
extensive benchmark data for fundamental GL tasks, i.e., link prediction and
node classification, including the performances of 366 models on 457 graphs on
these tasks. (ii) GLEMOS designs multiple evaluation settings, and assesses how
effectively representative model selection techniques perform in these
different settings. (iii) GLEMOS is designed to be easily extended with new
models, new graphs, and new performance records. (iv) Based on the experimental
results, we discuss the limitations of existing approaches and highlight future
research directions. To promote research on this significant problem, we make
the benchmark data and code publicly available at
https://github.com/facebookresearch/glemos.
\\ ( https://arxiv.org/abs/2404.01578 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01595
Date: Tue, 2 Apr 2024 02:36:21 GMT   (546kb,D)

Title: Propensity Score Alignment of Unpaired Multimodal Data
Authors: Johnny Xi, Jason Hartford
Categories: cs.LG stat.ME stat.ML
\\
  Multimodal representation learning techniques typically rely on paired
samples to learn common representations, but paired samples are challenging to
collect in fields such as biology where measurement devices often destroy the
samples. This paper presents an approach to address the challenge of aligning
unpaired samples across disparate modalities in multimodal representation
learning. We draw an analogy between potential outcomes in causal inference and
potential views in multimodal observations, which allows us to use Rubin's
framework to estimate a common space in which to match samples. Our approach
assumes we collect samples that are experimentally perturbed by treatments, and
uses this to estimate a propensity score from each modality, which encapsulates
all shared information between a latent state and treatment and can be used to
define a distance between samples. We experiment with two alignment techniques
that leverage this distance -- shared nearest neighbours (SNN) and optimal
transport (OT) matching -- and find that OT matching results in significant
improvements over state-of-the-art alignment approaches in both a synthetic
multi-modal setting and in real-world data from NeurIPS Multimodal Single-Cell
Integration Challenge.
\\ ( https://arxiv.org/abs/2404.01595 ,  546kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01598
Date: Tue, 2 Apr 2024 02:39:17 GMT   (5250kb,D)

Title: Extremum-Seeking Action Selection for Accelerating Policy Optimization
Authors: Ya-Chien Chang and Sicun Gao
Categories: cs.LG cs.AI cs.RO
\\
  Reinforcement learning for control over continuous spaces typically uses
high-entropy stochastic policies, such as Gaussian distributions, for local
exploration and estimating policy gradient to optimize performance. Many
robotic control problems deal with complex unstable dynamics, where applying
actions that are off the feasible control manifolds can quickly lead to
undesirable divergence. In such cases, most samples taken from the ambient
action space generate low-value trajectories that hardly contribute to policy
improvement, resulting in slow or failed learning. We propose to improve action
selection in this model-free RL setting by introducing additional adaptive
control steps based on Extremum-Seeking Control (ESC). On each action sampled
from stochastic policies, we apply sinusoidal perturbations and query for
estimated Q-values as the response signal. Based on ESC, we then dynamically
improve the sampled actions to be closer to nearby optima before applying them
to the environment. Our methods can be easily added in standard policy
optimization to improve learning efficiency, which we demonstrate in various
control learning environments.
\\ ( https://arxiv.org/abs/2404.01598 ,  5250kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01601
Date: Tue, 2 Apr 2024 02:45:12 GMT   (3973kb,D)

Title: What Can Transformer Learn with Varying Depth? Case Studies on Sequence
  Learning Tasks
Authors: Xingwu Chen, Difan Zou
Categories: cs.LG
\\
  We study the capabilities of the transformer architecture with varying depth.
Specifically, we designed a novel set of sequence learning tasks to
systematically evaluate and comprehend how the depth of transformer affects its
ability to perform memorization, reasoning, generalization, and contextual
generalization. We show a transformer with only one attention layer can excel
in memorization but falls short in other tasks. Then, we show that exhibiting
reasoning and generalization ability requires the transformer to have at least
two attention layers, while context generalization ability may necessitate
three attention layers. Additionally, we identify a class of simple operations
that a single attention layer can execute, and show that the complex tasks can
be approached as the combinations of these simple operations and thus can be
resolved by stacking multiple attention layers. This sheds light on studying
more practical and complex tasks beyond our design. Numerical experiments
corroborate our theoretical findings.
\\ ( https://arxiv.org/abs/2404.01601 ,  3973kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01611
Date: Tue, 2 Apr 2024 03:18:28 GMT   (318kb)

Title: Audio Simulation for Sound Source Localization in Virtual Evironment
Authors: Yi Di Yuan, Swee Liang Wong, Jonathan Pan
Categories: cs.LG cs.SD eess.AS
Comments: 2024 IEEE World Forum on Public Safety Technology
\\
  Non-line-of-sight localization in signal-deprived environments is a
challenging yet pertinent problem. Acoustic methods in such predominantly
indoor scenarios encounter difficulty due to the reverberant nature. In this
study, we aim to locate sound sources to specific locations within a virtual
environment by leveraging physically grounded sound propagation simulations and
machine learning methods. This process attempts to overcome the issue of data
insufficiency to localize sound sources to their location of occurrence
especially in post-event localization. We achieve 0.786+/- 0.0136 F1-score
using an audio transformer spectrogram approach.
\\ ( https://arxiv.org/abs/2404.01611 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01632
Date: Tue, 2 Apr 2024 04:33:03 GMT   (884kb,D)

Title: Enhancing Functional Safety in Automotive AMS Circuits through
  Unsupervised Machine Learning
Authors: Ayush Arunachalam, Ian Kintz, Suvadeep Banerjee, Arnab Raha, Xiankun
  Jin, Fei Su, Viswanathan Pillai Prasanth, Rubin A. Parekhji, Suriyaprakash
  Natarajan, and Kanad Basu
Categories: cs.LG cs.SY eess.SY
Comments: 12 pages, 12 figures
\\
  Given the widespread use of safety-critical applications in the automotive
field, it is crucial to ensure the Functional Safety (FuSa) of circuits and
components within automotive systems. The Analog and Mixed-Signal (AMS)
circuits prevalent in these systems are more vulnerable to faults induced by
parametric perturbations, noise, environmental stress, and other factors, in
comparison to their digital counterparts. However, their continuous signal
characteristics present an opportunity for early anomaly detection, enabling
the implementation of safety mechanisms to prevent system failure. To address
this need, we propose a novel framework based on unsupervised machine learning
for early anomaly detection in AMS circuits. The proposed approach involves
injecting anomalies at various circuit locations and individual components to
create a diverse and comprehensive anomaly dataset, followed by the extraction
of features from the observed circuit signals. Subsequently, we employ
clustering algorithms to facilitate anomaly detection. Finally, we propose a
time series framework to enhance and expedite anomaly detection performance.
Our approach encompasses a systematic analysis of anomaly abstraction at
multiple levels pertaining to the automotive domain, from hardware- to
block-level, where anomalies are injected to create diverse fault scenarios. By
monitoring the system behavior under these anomalous conditions, we capture the
propagation of anomalies and their effects at different abstraction levels,
thereby potentially paving the way for the implementation of reliable safety
mechanisms to ensure the FuSa of automotive SoCs. Our experimental findings
indicate that our approach achieves 100% anomaly detection accuracy and
significantly optimizes the associated latency by 5X, underscoring the
effectiveness of our devised solution.
\\ ( https://arxiv.org/abs/2404.01632 ,  884kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01642
Date: Tue, 2 Apr 2024 05:16:59 GMT   (562kb)

Title: ADVREPAIR:Provable Repair of Adversarial Attack
Authors: Zhiming Chi, Jianan Ma, Pengfei Yang, Cheng-Chao Huang, Renjue Li,
  Xiaowei Huang and Lijun Zhang
Categories: cs.LG cs.CR
\\
  Deep neural networks (DNNs) are increasingly deployed in safety-critical
domains, but their vulnerability to adversarial attacks poses serious safety
risks. Existing neuron-level methods using limited data lack efficacy in fixing
adversaries due to the inherent complexity of adversarial attack mechanisms,
while adversarial training, leveraging a large number of adversarial samples to
enhance robustness, lacks provability. In this paper, we propose ADVREPAIR, a
novel approach for provable repair of adversarial attacks using limited data.
By utilizing formal verification, ADVREPAIR constructs patch modules that, when
integrated with the original network, deliver provable and specialized repairs
within the robustness neighborhood. Additionally, our approach incorporates a
heuristic mechanism for assigning patch modules, allowing this defense against
adversarial attacks to generalize to other inputs. ADVREPAIR demonstrates
superior efficiency, scalability and repair success rate. Different from
existing DNN repair methods, our repair can generalize to general inputs,
thereby improving the robustness of the neural network globally, which
indicates a significant breakthrough in the generalization capability of
ADVREPAIR.
\\ ( https://arxiv.org/abs/2404.01642 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01646
Date: Tue, 2 Apr 2024 05:30:54 GMT   (2499kb,D)

Title: Transformer meets wcDTW to improve real-time battery bids: A new
  approach to scenario selection
Authors: Sujal Bhavsar, Vera Zaychik Moffitt, Justin Appleby
Categories: cs.LG cs.CY
\\
  Stochastic battery bidding in real-time energy markets is a nuanced process,
with its efficacy depending on the accuracy of forecasts and the representative
scenarios chosen for optimization. In this paper, we introduce a pioneering
methodology that amalgamates Transformer-based forecasting with weighted
constrained Dynamic Time Warping (wcDTW) to refine scenario selection. Our
approach harnesses the predictive capabilities of Transformers to foresee
Energy prices, while wcDTW ensures the selection of pertinent historical
scenarios by maintaining the coherence between multiple uncertain products.
Through extensive simulations in the PJM market for July 2023, our method
exhibited a 10% increase in revenue compared to the conventional method,
highlighting its potential to revolutionize battery bidding strategies in
real-time markets.
\\ ( https://arxiv.org/abs/2404.01646 ,  2499kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01650
Date: Tue, 2 Apr 2024 05:34:33 GMT   (702kb,D)

Title: Test-Time Model Adaptation with Only Forward Passes
Authors: Shuaicheng Niu, Chunyan Miao, Guohao Chen, Pengcheng Wu, Peilin Zhao
Categories: cs.LG
Comments: 17 pages, 3 figures, 14 tables
\\
  Test-time adaptation has proven effective in adapting a given trained model
to unseen test samples with potential distribution shifts. However, in
real-world scenarios, models are usually deployed on resource-limited devices,
e.g., FPGAs, and are often quantized and hard-coded with non-modifiable
parameters for acceleration. In light of this, existing methods are often
infeasible since they heavily depend on computation-intensive backpropagation
for model updating that may be not supported. To address this, we propose a
test-time Forward-Only Adaptation (FOA) method. In FOA, we seek to solely learn
a newly added prompt (as model's input) via a derivative-free covariance matrix
adaptation evolution strategy. To make this strategy work stably under our
online unsupervised setting, we devise a novel fitness function by measuring
test-training statistic discrepancy and model prediction entropy. Moreover, we
design an activation shifting scheme that directly tunes the model activations
for shifted test samples, making them align with the source training domain,
thereby further enhancing adaptation performance. Without using any
backpropagation and altering model weights, FOA runs on quantized 8-bit ViT
outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving
an up to 24-fold memory reduction on ImageNet-C. The source code will be
released.
\\ ( https://arxiv.org/abs/2404.01650 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01676
Date: Tue, 2 Apr 2024 06:28:22 GMT   (3540kb,D)

Title: Incentives in Private Collaborative Machine Learning
Authors: Rachael Hwee Ling Sim, Yehong Zhang, Trong Nghia Hoang, Xinyi Xu,
  Bryan Kian Hsiang Low and Patrick Jaillet
Categories: cs.LG
Comments: Accepted to NeurIPS 2023
\\
  Collaborative machine learning involves training models on data from multiple
parties but must incentivize their participation. Existing data valuation
methods fairly value and reward each party based on shared data or model
parameters but neglect the privacy risks involved. To address this, we
introduce differential privacy (DP) as an incentive. Each party can select its
required DP guarantee and perturb its sufficient statistic (SS) accordingly.
The mediator values the perturbed SS by the Bayesian surprise it elicits about
the model parameters. As our valuation function enforces a privacy-valuation
trade-off, parties are deterred from selecting excessive DP guarantees that
reduce the utility of the grand coalition's model. Finally, the mediator
rewards each party with different posterior samples of the model parameters.
Such rewards still satisfy existing incentives like fairness but additionally
preserve DP and a high similarity to the grand coalition's posterior. We
empirically demonstrate the effectiveness and practicality of our approach on
synthetic and real-world datasets.
\\ ( https://arxiv.org/abs/2404.01676 ,  3540kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01693
Date: Tue, 2 Apr 2024 06:53:45 GMT   (669kb,D)

Title: HeMeNet: Heterogeneous Multichannel Equivariant Network for Protein
  Multitask Learning
Authors: Rong Han, Wenbing Huang, Lingxiao Luo, Xinyan Han, Jiaming Shen,
  Zhiqiang Zhang, Jun Zhou, Ting Chen
Categories: cs.LG
\\
  Understanding and leveraging the 3D structures of proteins is central to a
variety of biological and drug discovery tasks. While deep learning has been
applied successfully for structure-based protein function prediction tasks,
current methods usually employ distinct training for each task. However, each
of the tasks is of small size, and such a single-task strategy hinders the
models' performance and generalization ability. As some labeled 3D protein
datasets are biologically related, combining multi-source datasets for
larger-scale multi-task learning is one way to overcome this problem. In this
paper, we propose a neural network model to address multiple tasks jointly upon
the input of 3D protein structures. In particular, we first construct a
standard structure-based multi-task benchmark called Protein-MT, consisting of
6 biologically relevant tasks, including affinity prediction and property
prediction, integrated from 4 public datasets. Then, we develop a novel graph
neural network for multi-task learning, dubbed Heterogeneous Multichannel
Equivariant Network (HeMeNet), which is E(3) equivariant and able to capture
heterogeneous relationships between different atoms. Besides, HeMeNet can
achieve task-specific learning via the task-aware readout mechanism. Extensive
evaluations on our benchmark verify the effectiveness of multi-task learning,
and our model generally surpasses state-of-the-art models.
\\ ( https://arxiv.org/abs/2404.01693 ,  669kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01695
Date: Tue, 2 Apr 2024 06:56:21 GMT   (2234kb,D)

Title: Selective Temporal Knowledge Graph Reasoning
Authors: Zhongni Hou, Xiaolong Jin, Zixuan Li, Long Bai, Jiafeng Guo, Xueqi
  Cheng
Categories: cs.LG
\\
  Temporal Knowledge Graph (TKG), which characterizes temporally evolving facts
in the form of (subject, relation, object, timestamp), has attracted much
attention recently. TKG reasoning aims to predict future facts based on given
historical ones. However, existing TKG reasoning models are unable to abstain
from predictions they are uncertain, which will inevitably bring risks in
real-world applications. Thus, in this paper, we propose an abstention
mechanism for TKG reasoning, which helps the existing models make selective,
instead of indiscriminate, predictions. Specifically, we develop a confidence
estimator, called Confidence Estimator with History (CEHis), to enable the
existing TKG reasoning models to first estimate their confidence in making
predictions, and then abstain from those with low confidence. To do so, CEHis
takes two kinds of information into consideration, namely, the certainty of the
current prediction and the accuracy of historical predictions. Experiments with
representative TKG reasoning models on two benchmark datasets demonstrate the
effectiveness of the proposed CEHis.
\\ ( https://arxiv.org/abs/2404.01695 ,  2234kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01712
Date: Tue, 2 Apr 2024 07:54:18 GMT   (1454kb,D)

Title: Efficient Online Unlearning via Hessian-Free Recollection of Individual
  Data Statistics
Authors: Xinbao Qiao, Meng Zhang, Ming Tang, Ermin Wei
Categories: cs.LG cs.AI
Comments: 24 pages, 8 figures
\\
  Machine unlearning strives to uphold the data owners' right to be forgotten
by enabling models to selectively forget specific data. Recent methods suggest
that one approach of data forgetting is by precomputing and storing statistics
carrying second-order information to improve computational and memory
efficiency. However, they rely on restrictive assumptions and the
computation/storage suffer from the curse of model parameter dimensionality,
making it challenging to apply to most deep neural networks. In this work, we
propose a Hessian-free online unlearning method. We propose to maintain a
statistical vector for each data point, computed through affine stochastic
recursion approximation of the difference between retrained and learned models.
Our proposed algorithm achieves near-instantaneous online unlearning as it only
requires a vector addition operation. Based on the strategy that recollecting
statistics for forgetting data, the proposed method significantly reduces the
unlearning runtime. Experimental studies demonstrate that the proposed scheme
surpasses existing results by orders of magnitude in terms of time and memory
costs, while also enhancing accuracy.
\\ ( https://arxiv.org/abs/2404.01712 ,  1454kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01714
Date: Tue, 2 Apr 2024 07:57:17 GMT   (1124kb,D)

Title: Conjugate-Gradient-like Based Adaptive Moment Estimation Optimization
  Algorithm for Deep Learning
Authors: Jiawu Tian, Liwei Xu, Xiaowei Zhang, Yongqi Li
Categories: cs.LG cs.AI cs.CV math.OC
Comments: 32 pages, 13 figures
\\
  Training deep neural networks is a challenging task. In order to speed up
training and enhance the performance of deep neural networks, we rectify the
vanilla conjugate gradient as conjugate-gradient-like and incorporate it into
the generic Adam, and thus propose a new optimization algorithm named
CG-like-Adam for deep learning. Specifically, both the first-order and the
second-order moment estimation of generic Adam are replaced by the
conjugate-gradient-like. Convergence analysis handles the cases where the
exponential moving average coefficient of the first-order moment estimation is
constant and the first-order moment estimation is unbiased. Numerical
experiments show the superiority of the proposed algorithm based on the
CIFAR10/100 dataset.
\\ ( https://arxiv.org/abs/2404.01714 ,  1124kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01730
Date: Tue, 2 Apr 2024 08:40:07 GMT   (109kb,D)

Title: Asymptotics of Language Model Alignment
Authors: Joy Qiping Yang and Salman Salamatian and Ziteng Sun and Ananda
  Theertha Suresh and Ahmad Beirami
Categories: cs.LG cs.IT math.IT stat.ML
\\
  Let $p$ denote a generative language model. Let $r$ denote a reward model
that returns a scalar that captures the degree at which a draw from $p$ is
preferred. The goal of language model alignment is to alter $p$ to a new
distribution $\phi$ that results in a higher expected reward while keeping
$\phi$ close to $p.$ A popular alignment method is the KL-constrained
reinforcement learning (RL), which chooses a distribution $\phi_\Delta$ that
maximizes $E_{\phi_{\Delta}} r(y)$ subject to a relative entropy constraint
$KL(\phi_\Delta || p) \leq \Delta.$ Another simple alignment method is
best-of-$N$, where $N$ samples are drawn from $p$ and one with highest reward
is selected. In this paper, we offer a closed-form characterization of the
optimal KL-constrained RL solution. We demonstrate that any alignment method
that achieves a comparable trade-off between KL divergence and reward must
approximate the optimal KL-constrained RL solution in terms of relative
entropy. To further analyze the properties of alignment methods, we introduce
two simplifying assumptions: we let the language model be memoryless, and the
reward model be linear. Although these assumptions may not reflect complex
real-world scenarios, they enable a precise characterization of the asymptotic
behavior of both the best-of-$N$ alignment, and the KL-constrained RL method,
in terms of information-theoretic quantities. We prove that the reward of the
optimal KL-constrained RL solution satisfies a large deviation principle, and
we fully characterize its rate function. We also show that the rate of growth
of the scaled cumulants of the reward is characterized by a proper Renyi cross
entropy. Finally, we show that best-of-$N$ is asymptotically equivalent to
KL-constrained RL solution by proving that their expected rewards are
asymptotically equal, and concluding that the two distributions must be close
in KL divergence.
\\ ( https://arxiv.org/abs/2404.01730 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01748
Date: Tue, 2 Apr 2024 09:04:56 GMT   (1103kb,D)

Title: Global Mapping of Exposure and Physical Vulnerability Dynamics in Least
  Developed Countries using Remote Sensing and Machine Learning
Authors: Joshua Dimasaka, Christian Gei{\ss}, Emily So
Categories: cs.LG cs.CV
Comments: This is the camera-ready paper for the accepted poster at the 2nd
  Machine Learning for Remote Sensing Workshop, 12th International Conference
  on Learning Representations (ICLR) in Vienna, Austria, on the 11th of May
  2024. Access the poster here: https://zenodo.org/doi/10.5281/zenodo.10903886
  Watch the video version of our poster here: https://youtu.be/N6ithJeCF4M
\\
  As the world marked the midterm of the Sendai Framework for Disaster Risk
Reduction 2015-2030, many countries are still struggling to monitor their
climate and disaster risk because of the expensive large-scale survey of the
distribution of exposure and physical vulnerability and, hence, are not on
track in reducing risks amidst the intensifying effects of climate change. We
present an ongoing effort in mapping this vital information using machine
learning and time-series remote sensing from publicly available Sentinel-1 SAR
GRD and Sentinel-2 Harmonized MSI. We introduce the development of
"OpenSendaiBench" consisting of 47 countries wherein most are least developed
(LDCs), trained ResNet-50 deep learning models, and demonstrated the region of
Dhaka, Bangladesh by mapping the distribution of its informal constructions. As
a pioneering effort in auditing global disaster risk over time, this paper aims
to advance the area of large-scale risk quantification in informing our
collective long-term efforts in reducing climate and disaster risk.
\\ ( https://arxiv.org/abs/2404.01748 ,  1103kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01769
Date: Tue, 2 Apr 2024 09:31:51 GMT   (889kb,D)

Title: Unifying Qualitative and Quantitative Safety Verification of
  DNN-Controlled Systems
Authors: Dapeng Zhi, Peixin Wang, Si Liu, Luke Ong, Min Zhang
Categories: cs.LG
Comments: This work is a technical report for the paper with the same name to
  appear in the 36th International Conference on Computer Aided Verification
  (CAV 2024)
\\
  The rapid advance of deep reinforcement learning techniques enables the
oversight of safety-critical systems through the utilization of Deep Neural
Networks (DNNs). This underscores the pressing need to promptly establish
certified safety guarantees for such DNN-controlled systems. Most of the
existing verification approaches rely on qualitative approaches, predominantly
employing reachability analysis. However, qualitative verification proves
inadequate for DNN-controlled systems as their behaviors exhibit stochastic
tendencies when operating in open and adversarial environments. In this paper,
we propose a novel framework for unifying both qualitative and quantitative
safety verification problems of DNN-controlled systems. This is achieved by
formulating the verification tasks as the synthesis of valid neural barrier
certificates (NBCs). Initially, the framework seeks to establish almost-sure
safety guarantees through qualitative verification. In cases where qualitative
verification fails, our quantitative verification method is invoked, yielding
precise lower and upper bounds on probabilistic safety across both infinite and
finite time horizons. To facilitate the synthesis of NBCs, we introduce their
$k$-inductive variants. We also devise a simulation-guided approach for
training NBCs, aiming to achieve tightness in computing precise certified lower
and upper bounds. We prototype our approach into a tool called $\textsf{UniQQ}$
and showcase its efficacy on four classic DNN-controlled systems.
\\ ( https://arxiv.org/abs/2404.01769 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01804
Date: Tue, 2 Apr 2024 10:06:21 GMT   (4729kb,D)

Title: Neuromorphic Wireless Device-Edge Co-Inference via the Directed
  Information Bottleneck
Authors: Yuzhen Ke, Zoran Utkovski, Mehdi Heshmati, Osvaldo Simeone, Johannes
  Dommel, Slawomir Stanczak
Categories: cs.LG cs.IT cs.NE math.IT
Comments: 8 pages
\\
  An important use case of next-generation wireless systems is device-edge
co-inference, where a semantic task is partitioned between a device and an edge
server. The device carries out data collection and partial processing of the
data, while the remote server completes the given task based on information
received from the device. It is often required that processing and
communication be run as efficiently as possible at the device, while more
computing resources are available at the edge. To address such scenarios, we
introduce a new system solution, termed neuromorphic wireless device-edge
co-inference. According to it, the device runs sensing, processing, and
communication units using neuromorphic hardware, while the server employs
conventional radio and computing technologies. The proposed system is designed
using a transmitter-centric information-theoretic criterion that targets a
reduction of the communication overhead, while retaining the most relevant
information for the end-to-end semantic task of interest. Numerical results on
standard data sets validate the proposed architecture, and a preliminary
testbed realization is reported.
\\ ( https://arxiv.org/abs/2404.01804 ,  4729kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01805
Date: Tue, 2 Apr 2024 10:06:30 GMT   (555kb,D)

Title: Improved Text Emotion Prediction Using Combined Valence and Arousal
  Ordinal Classification
Authors: Michael Mitsios, Georgios Vamvoukakis, Georgia Maniati, Nikolaos
  Ellinas, Georgios Dimitriou, Konstantinos Markopoulos, Panos Kakoulidis,
  Alexandra Vioni, Myrsini Christidou, Junkwang Oh, Gunu Jho, Inchul Hwang,
  Georgios Vardaxoglou, Aimilios Chalamandaris, Pirros Tsiakoulis, Spyros
  Raptis
Categories: cs.LG
\\
  Emotion detection in textual data has received growing interest in recent
years, as it is pivotal for developing empathetic human-computer interaction
systems. This paper introduces a method for categorizing emotions from text,
which acknowledges and differentiates between the diversified similarities and
distinctions of various emotions. Initially, we establish a baseline by
training a transformer-based model for standard emotion classification,
achieving state-of-the-art performance. We argue that not all
misclassifications are of the same importance, as there are perceptual
similarities among emotional classes. We thus redefine the emotion labeling
problem by shifting it from a traditional classification model to an ordinal
classification one, where discrete emotions are arranged in a sequential order
according to their valence levels. Finally, we propose a method that performs
ordinal classification in the two-dimensional emotion space, considering both
valence and arousal scales. The results show that our approach not only
preserves high accuracy in emotion prediction but also significantly reduces
the magnitude of errors in cases of misclassification.
\\ ( https://arxiv.org/abs/2404.01805 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01822
Date: Tue, 2 Apr 2024 10:32:21 GMT   (948kb,D)

Title: A (More) Realistic Evaluation Setup for Generalisation of Community
  Models on Malicious Content Detection
Authors: Ivo Verhoeven, Pushkar Mishra, Rahel Beloch, Helen Yannakoudakis,
  Ekaterina Shutova
Categories: cs.LG cs.CL cs.SI
Comments: To be published at Findings of NAACL 2024
\\
  Community models for malicious content detection, which take into account the
context from a social graph alongside the content itself, have shown remarkable
performance on benchmark datasets. Yet, misinformation and hate speech continue
to propagate on social media networks. This mismatch can be partially
attributed to the limitations of current evaluation setups that neglect the
rapid evolution of online content and the underlying social graph. In this
paper, we propose a novel evaluation setup for model generalisation based on
our few-shot subgraph sampling approach. This setup tests for generalisation
through few labelled examples in local explorations of a larger graph,
emulating more realistic application settings. We show this to be a challenging
inductive setup, wherein strong performance on the training graph is not
indicative of performance on unseen tasks, domains, or graph structures.
Lastly, we show that graph meta-learners trained with our proposed few-shot
subgraph sampling outperform standard community models in the inductive setup.
We make our code publicly available.
\\ ( https://arxiv.org/abs/2404.01822 ,  948kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01828
Date: Tue, 2 Apr 2024 10:41:51 GMT   (2407kb,D)

Title: Defense without Forgetting: Continual Adversarial Defense with
  Anisotropic & Isotropic Pseudo Replay
Authors: Yuhang Zhou, Zhongyun Hua
Categories: cs.LG cs.AI
\\
  Deep neural networks have demonstrated susceptibility to adversarial attacks.
Adversarial defense techniques often focus on one-shot setting to maintain
robustness against attack. However, new attacks can emerge in sequences in
real-world deployment scenarios. As a result, it is crucial for a defense model
to constantly adapt to new attacks, but the adaptation process can lead to
catastrophic forgetting of previously defended against attacks. In this paper,
we discuss for the first time the concept of continual adversarial defense
under a sequence of attacks, and propose a lifelong defense baseline called
Anisotropic \& Isotropic Replay (AIR), which offers three advantages: (1)
Isotropic replay ensures model consistency in the neighborhood distribution of
new data, indirectly aligning the output preference between old and new tasks.
(2) Anisotropic replay enables the model to learn a compromise data manifold
with fresh mixed semantics for further replay constraints and potential future
attacks. (3) A straightforward regularizer mitigates the 'plasticity-stability'
trade-off by aligning model output between new and old tasks. Experiment
results demonstrate that AIR can approximate or even exceed the empirical
performance upper bounds achieved by Joint Training.
\\ ( https://arxiv.org/abs/2404.01828 ,  2407kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01847
Date: Tue, 2 Apr 2024 11:12:42 GMT   (4645kb,D)

Title: Accelerating Transformer Pre-Training with 2:4 Sparsity
Authors: Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, Jun Zhu
Categories: cs.LG
\\
  Training large Transformers is slow, but recent innovations on GPU
architecture gives us an advantage. NVIDIA Ampere GPUs can execute a
fine-grained 2:4 sparse matrix multiplication twice as fast as its dense
equivalent. In the light of this property, we comprehensively investigate the
feasibility of accelerating feed-forward networks (FFNs) of Transformers in
pre-training. First, we define a "flip rate" to monitor the stability of a 2:4
training process. Utilizing this metric, we suggest two techniques to preserve
accuracy: to modify the sparse-refined straight-through estimator by applying
the mask decay term on gradients, and to enhance the model's quality by a
simple yet effective dense fine-tuning procedure near the end of pre-training.
Besides, we devise two effective techniques to practically accelerate training:
to calculate transposable 2:4 mask by convolution, and to accelerate gated
activation functions by reducing GPU L2 cache miss. Experiments show that a
combination of our methods reaches the best performance on multiple
Transformers among different 2:4 training methods, while actual acceleration
can be observed on different shapes of Transformer block.
\\ ( https://arxiv.org/abs/2404.01847 ,  4645kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01853
Date: Tue, 2 Apr 2024 11:30:22 GMT   (3212kb,D)

Title: Pairwise Similarity Distribution Clustering for Noisy Label Learning
Authors: Sihan Bai
Categories: cs.LG cs.CV
\\
  Noisy label learning aims to train deep neural networks using a large amount
of samples with noisy labels, whose main challenge comes from how to deal with
the inaccurate supervision caused by wrong labels. Existing works either take
the label correction or sample selection paradigm to involve more samples with
accurate labels into the training process. In this paper, we propose a simple
yet effective sample selection algorithm, termed as Pairwise Similarity
Distribution Clustering~(PSDC), to divide the training samples into one clean
set and another noisy set, which can power any of the off-the-shelf
semi-supervised learning regimes to further train networks for different
downstream tasks. Specifically, we take the pairwise similarity between sample
pairs to represent the sample structure, and the Gaussian Mixture Model~(GMM)
to model the similarity distribution between sample pairs belonging to the same
noisy cluster, therefore each sample can be confidently divided into the clean
set or noisy set. Even under severe label noise rate, the resulting data
partition mechanism has been proved to be more robust in judging the label
confidence in both theory and practice. Experimental results on various
benchmark datasets, such as CIFAR-10, CIFAR-100 and Clothing1M, demonstrate
significant improvements over state-of-the-art methods.
\\ ( https://arxiv.org/abs/2404.01853 ,  3212kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01857
Date: Tue, 2 Apr 2024 11:35:05 GMT   (45kb,D)

Title: Detecting Gender Bias in Course Evaluations
Authors: Sarah Lindau and Linnea Nilsson
Categories: cs.LG cs.CL
Comments: Presented at the Swedish Language Technology Conference 2022. 5
  pages, 2 figures, 5 tables
\\
  An outtake from the findnings of a master thesis studying gender bias in
course evaluations through the lense of machine learning and nlp. We use
different methods to examine and explore the data and find differences in what
students write about courses depending on gender of the examiner. Data from
English and Swedish courses are evaluated and compared, in order to capture
more nuance in the gender bias that might be found. Here we present the results
from the work so far, but this is an ongoing project and there is more work to
do.
\\ ( https://arxiv.org/abs/2404.01857 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01863
Date: Tue, 2 Apr 2024 11:40:38 GMT   (18173kb,D)

Title: Confidence-aware Reward Optimization for Fine-tuning Text-to-Image
  Models
Authors: Kyuyoung Kim, Jongheon Jeong, Minyong An, Mohammad Ghavamzadeh,
  Krishnamurthy Dvijotham, Jinwoo Shin, Kimin Lee
Categories: cs.LG cs.AI
Comments: ICLR 2024
\\
  Fine-tuning text-to-image models with reward functions trained on human
feedback data has proven effective for aligning model behavior with human
intent. However, excessive optimization with such reward models, which serve as
mere proxy objectives, can compromise the performance of fine-tuned models, a
phenomenon known as reward overoptimization. To investigate this issue in
depth, we introduce the Text-Image Alignment Assessment (TIA2) benchmark, which
comprises a diverse collection of text prompts, images, and human annotations.
Our evaluation of several state-of-the-art reward models on this benchmark
reveals their frequent misalignment with human assessment. We empirically
demonstrate that overoptimization occurs notably when a poorly aligned reward
model is used as the fine-tuning objective. To address this, we propose
TextNorm, a simple method that enhances alignment based on a measure of reward
model confidence estimated across a set of semantically contrastive text
prompts. We demonstrate that incorporating the confidence-calibrated rewards in
fine-tuning effectively reduces overoptimization, resulting in twice as many
wins in human evaluation for text-image alignment compared against the baseline
reward models.
\\ ( https://arxiv.org/abs/2404.01863 ,  18173kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01872
Date: Tue, 2 Apr 2024 11:55:50 GMT   (4972kb,D)

Title: Fast and Adaptive Questionnaires for Voting Advice Applications
Authors: Fynn Bachmann, Cristina Sarasua, Abraham Bernstein
Categories: cs.LG cs.HC cs.IT math.IT
Comments: 16 pages plus appendix, submitted to ECML/PKDD 2024
\\
  The effectiveness of Voting Advice Applications (VAA) is often compromised by
the length of their questionnaires. To address user fatigue and incomplete
responses, some applications (such as the Swiss Smartvote) offer a condensed
version of their questionnaire. However, these condensed versions can not
ensure the accuracy of recommended parties or candidates, which we show to
remain below 40%. To tackle these limitations, this work introduces an adaptive
questionnaire approach that selects subsequent questions based on users'
previous answers, aiming to enhance recommendation accuracy while reducing the
number of questions posed to the voters. Our method uses an encoder and decoder
module to predict missing values at any completion stage, leveraging a
two-dimensional latent space reflective of political science's traditional
methods for visualizing political orientations. Additionally, a selector module
is proposed to determine the most informative subsequent question based on the
voter's current position in the latent space and the remaining unanswered
questions. We validated our approach using the Smartvote dataset from the Swiss
Federal elections in 2019, testing various spatial models and selection methods
to optimize the system's predictive accuracy. Our findings indicate that
employing the IDEAL model both as encoder and decoder, combined with a
PosteriorRMSE method for question selection, significantly improves the
accuracy of recommendations, achieving 74% accuracy after asking the same
number of questions as in the condensed version.
\\ ( https://arxiv.org/abs/2404.01872 ,  4972kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01877
Date: Tue, 2 Apr 2024 12:05:02 GMT   (1676kb,D)

Title: Procedural Fairness in Machine Learning
Authors: Ziming Wang, Changwu Huang, Xin Yao
Categories: cs.LG
Comments: 14 pages
\\
  Fairness in machine learning (ML) has received much attention. However,
existing studies have mainly focused on the distributive fairness of ML models.
The other dimension of fairness, i.e., procedural fairness, has been neglected.
In this paper, we first define the procedural fairness of ML models, and then
give formal definitions of individual and group procedural fairness. We propose
a novel metric to evaluate the group procedural fairness of ML models, called
$GPF_{FAE}$, which utilizes a widely used explainable artificial intelligence
technique, namely feature attribution explanation (FAE), to capture the
decision process of the ML models. We validate the effectiveness of $GPF_{FAE}$
on a synthetic dataset and eight real-world datasets. Our experiments reveal
the relationship between procedural and distributive fairness of the ML model.
Based on our analysis, we propose a method for identifying the features that
lead to the procedural unfairness of the model and propose two methods to
improve procedural fairness after identifying unfair features. Our experimental
results demonstrate that we can accurately identify the features that lead to
procedural unfairness in the ML model, and both of our proposed methods can
significantly improve procedural fairness with a slight impact on model
performance, while also improving distributive fairness.
\\ ( https://arxiv.org/abs/2404.01877 ,  1676kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01930
Date: Tue, 2 Apr 2024 13:23:54 GMT   (37kb)

Title: Adaptive Combinatorial Maximization: Beyond Approximate Greedy Policies
Authors: Shlomi Weitzman and Sivan Sabato
Categories: cs.LG cs.DM stat.ML
Comments: ALT 2024
Journal-ref: "Adaptive Combinatorial Maximization: Beyond Approximate Greedy
  Policies", Shlomi Weitzman and Sivan Sabato, Proceedings of the 35th
  International Conference on Algorithmic Learning Theory, PMLR 237:1-20, 2024
\\
  We study adaptive combinatorial maximization, which is a core challenge in
machine learning, with applications in active learning as well as many other
domains. We study the Bayesian setting, and consider the objectives of
maximization under a cardinality constraint and minimum cost coverage. We
provide new comprehensive approximation guarantees that subsume previous
results, as well as considerably strengthen them. Our approximation guarantees
simultaneously support the maximal gain ratio as well as near-submodular
utility functions, and include both maximization under a cardinality constraint
and a minimum cost coverage guarantee. In addition, we provided an
approximation guarantee for a modified prior, which is crucial for obtaining
active learning guarantees that do not depend on the smallest probability in
the prior. Moreover, we discover a new parameter of adaptive selection
policies, which we term the "maximal gain ratio". We show that this parameter
is strictly less restrictive than the greedy approximation parameter that has
been used in previous approximation guarantees, and show that it can be used to
provide stronger approximation guarantees than previous results. In particular,
we show that the maximal gain ratio is never larger than the greedy
approximation factor of a policy, and that it can be considerably smaller. This
provides a new insight into the properties that make a policy useful for
adaptive combinatorial maximization.
\\ ( https://arxiv.org/abs/2404.01930 ,  37kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01936
Date: Tue, 2 Apr 2024 13:31:19 GMT   (3190kb,D)

Title: Settling Time vs. Accuracy Tradeoffs for Clustering Big Data
Authors: Andrew Draganov, David Saulpic, Chris Schwiegelshohn
Categories: cs.LG cs.DS
\\
  We study the theoretical and practical runtime limits of k-means and k-median
clustering on large datasets. Since effectively all clustering methods are
slower than the time it takes to read the dataset, the fastest approach is to
quickly compress the data and perform the clustering on the compressed
representation. Unfortunately, there is no universal best choice for
compressing the number of points - while random sampling runs in sublinear time
and coresets provide theoretical guarantees, the former does not enforce
accuracy while the latter is too slow as the numbers of points and clusters
grow. Indeed, it has been conjectured that any sensitivity-based coreset
construction requires super-linear time in the dataset size. We examine this
relationship by first showing that there does exist an algorithm that obtains
coresets via sensitivity sampling in effectively linear time - within
log-factors of the time it takes to read the data. Any approach that
significantly improves on this must then resort to practical heuristics,
leading us to consider the spectrum of sampling strategies across both real and
artificial datasets in the static and streaming settings. Through this, we show
the conditions in which coresets are necessary for preserving cluster validity
as well as the settings in which faster, cruder sampling strategies are
sufficient. As a result, we provide a comprehensive theoretical and practical
blueprint for effective clustering regardless of data size. Our code is
publicly available and has scripts to recreate the experiments.
\\ ( https://arxiv.org/abs/2404.01936 ,  3190kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01958
Date: Tue, 2 Apr 2024 13:54:05 GMT   (7846kb,D)

Title: MESEN: Exploit Multimodal Data to Design Unimodal Human Activity
  Recognition with Few Labels
Authors: Lilin Xu, Chaojie Gu, Rui Tan, Shibo He, Jiming Chen
Categories: cs.LG
Comments: Accepted to the 21th ACM Conference on Embedded Networked Sensor
  Systems (SenSys 2023)
\\
  Human activity recognition (HAR) will be an essential function of various
emerging applications. However, HAR typically encounters challenges related to
modality limitations and label scarcity, leading to an application gap between
current solutions and real-world requirements. In this work, we propose MESEN,
a multimodal-empowered unimodal sensing framework, to utilize unlabeled
multimodal data available during the HAR model design phase for unimodal HAR
enhancement during the deployment phase. From a study on the impact of
supervised multimodal fusion on unimodal feature extraction, MESEN is designed
to feature a multi-task mechanism during the multimodal-aided pre-training
stage. With the proposed mechanism integrating cross-modal feature contrastive
learning and multimodal pseudo-classification aligning, MESEN exploits
unlabeled multimodal data to extract effective unimodal features for each
modality. Subsequently, MESEN can adapt to downstream unimodal HAR with only a
few labeled samples. Extensive experiments on eight public multimodal datasets
demonstrate that MESEN achieves significant performance improvements over
state-of-the-art baselines in enhancing unimodal HAR by exploiting multimodal
data.
\\ ( https://arxiv.org/abs/2404.01958 ,  7846kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01965
Date: Tue, 2 Apr 2024 14:03:37 GMT   (425kb,D)

Title: Towards Leveraging AutoML for Sustainable Deep Learning: A
  Multi-Objective HPO Approach on Deep Shift Neural Networks
Authors: Leona Hennig, Tanja Tornede, Marius Lindauer
Categories: cs.LG cs.AI
\\
  Deep Learning (DL) has advanced various fields by extracting complex patterns
from large datasets. However, the computational demands of DL models pose
environmental and resource challenges. Deep shift neural networks (DSNNs) offer
a solution by leveraging shift operations to reduce computational complexity at
inference. Following the insights from standard DNNs, we are interested in
leveraging the full potential of DSNNs by means of AutoML techniques. We study
the impact of hyperparameter optimization (HPO) to maximize DSNN performance
while minimizing resource consumption. Since this combines multi-objective (MO)
optimization with accuracy and energy consumption as potentially complementary
objectives, we propose to combine state-of-the-art multi-fidelity (MF) HPO with
multi-objective optimization. Experimental results demonstrate the
effectiveness of our approach, resulting in models with over 80\% in accuracy
and low computational cost. Overall, our method accelerates efficient model
development while enabling sustainable AI applications.
\\ ( https://arxiv.org/abs/2404.01965 ,  425kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01975
Date: Tue, 2 Apr 2024 14:16:57 GMT   (1275kb,D)

Title: DSGNN: A Dual-View Supergrid-Aware Graph Neural Network for Regional Air
  Quality Estimation
Authors: Xin Zhang, Ling Chen, Xing Tang, Hongyu Shi
Categories: cs.LG
Comments: Submitted to TKDE, 12 pages and 8 figures
\\
  Air quality estimation can provide air quality for target regions without air
quality stations, which is useful for the public. Existing air quality
estimation methods divide the study area into disjointed grid regions, and
apply 2D convolution to model the spatial dependencies of adjacent grid regions
based on the first law of geography, failing to model the spatial dependencies
of distant grid regions. To this end, we propose a Dual-view Supergrid-aware
Graph Neural Network (DSGNN) for regional air quality estimation, which can
model the spatial dependencies of distant grid regions from dual views (i.e.,
satellite-derived aerosol optical depth (AOD) and meteorology). Specifically,
images are utilized to represent the regional data (i.e., AOD data and
meteorology data). The dual-view supergrid learning module is introduced to
generate supergrids in a parameterized way. Based on the dual-view supergrids,
the dual-view implicit correlation encoding module is introduced to learn the
correlations between pairwise supergrids. In addition, the dual-view message
passing network is introduced to implement the information interaction on the
supergrid graphs and images. Extensive experiments on two real-world datasets
demonstrate that DSGNN achieves the state-of-the-art performances on the air
quality estimation task, outperforming the best baseline by an average of
19.64% in MAE.
\\ ( https://arxiv.org/abs/2404.01975 ,  1275kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01981
Date: Tue, 2 Apr 2024 14:19:30 GMT   (304kb)

Title: Zero-Shot Multi-Lingual Speaker Verification in Clinical Trials
Authors: Ali Akram, Marija Stanojevic, Malikeh Ehghaghi, Jekaterina Novikova
Categories: cs.LG cs.SD eess.AS
\\
  Due to the substantial number of clinicians, patients, and data collection
environments involved in clinical trials, gathering data of superior quality
poses a significant challenge. In clinical trials, patients are assessed based
on their speech data to detect and monitor cognitive and mental health
disorders. We propose using these speech recordings to verify the identities of
enrolled patients and identify and exclude the individuals who try to enroll
multiple times in the same trial. Since clinical studies are often conducted
across different countries, creating a system that can perform speaker
verification in diverse languages without additional development effort is
imperative. We evaluate pre-trained TitaNet, ECAPA-TDNN, and SpeakerNet models
by enrolling and testing with speech-impaired patients speaking English,
German, Danish, Spanish, and Arabic languages. Our results demonstrate that
tested models can effectively generalize to clinical speakers, with less than
2.7% EER for European Languages and 8.26% EER for Arabic. This represents a
significant step in developing more versatile and efficient speaker
verification systems for cognitive and mental health clinical trials that can
be used across a wide range of languages and dialects, substantially reducing
the effort required to develop speaker verification systems for multiple
languages. We also evaluate how speech tasks and number of speakers involved in
the trial influence the performance and show that the type of speech tasks
impacts the model performance.
\\ ( https://arxiv.org/abs/2404.01981 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02003
Date: Tue, 2 Apr 2024 14:44:02 GMT   (777kb,D)

Title: AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug
  Design
Authors: Xinze Li, Penglei Wang, Tianfan Fu, Wenhao Gao, Chengtao Li, Leilei
  Shi, Junhong Liu
Categories: cs.LG
\\
  Structure-based drug design (SBDD), which aims to generate molecules that can
bind tightly to the target protein, is an essential problem in drug discovery,
and previous approaches have achieved initial success. However, most existing
methods still suffer from invalid local structure or unrealistic conformation
issues, which are mainly due to the poor leaning of bond angles or torsional
angles. To alleviate these problems, we propose AUTODIFF, a diffusion-based
fragment-wise autoregressive generation model. Specifically, we design a novel
molecule assembly strategy named conformal motif that preserves the
conformation of local structures of molecules first, then we encode the
interaction of the protein-ligand complex with an SE(3)-equivariant
convolutional network and generate molecules motif-by-motif with diffusion
modeling. In addition, we also improve the evaluation framework of SBDD by
constraining the molecular weights of the generated molecules in the same
range, together with some new metrics, which make the evaluation more fair and
practical. Extensive experiments on CrossDocked2020 demonstrate that our
approach outperforms the existing models in generating realistic molecules with
valid structures and conformations while maintaining high binding affinity.
\\ ( https://arxiv.org/abs/2404.02003 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02047
Date: Tue, 2 Apr 2024 15:39:14 GMT   (289kb,D)

Title: Universal representations for financial transactional data: embracing
  local, global, and external contexts
Authors: Alexandra Bazarova, Maria Kovaleva, Ilya Kuleshov, Evgenia
  Romanenkova, Alexander Stepikin, Alexandr Yugay, Dzhambulat Mollaev, Ivan
  Kireev, Andrey Savchenko, Alexey Zaytsev
Categories: cs.LG cs.AI
\\
  Effective processing of financial transactions is essential for banking data
analysis. However, in this domain, most methods focus on specialized solutions
to stand-alone problems instead of constructing universal representations
suitable for many problems. We present a representation learning framework that
addresses diverse business challenges. We also suggest novel generative models
that account for data specifics, and a way to integrate external information
into a client's representation, leveraging insights from other customers'
actions. Finally, we offer a benchmark, describing representation quality
globally, concerning the entire transaction history; locally, reflecting the
client's current state; and dynamically, capturing representation evolution
over time. Our generative approach demonstrates superior performance in local
tasks, with an increase in ROC-AUC of up to 14\% for the next MCC prediction
task and up to 46\% for downstream tasks from existing contrastive baselines.
Incorporating external information improves the scores by an additional 20\%.
\\ ( https://arxiv.org/abs/2404.02047 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02052
Date: Tue, 2 Apr 2024 15:49:03 GMT   (254kb,D)

Title: Noise Masking Attacks and Defenses for Pretrained Speech Models
Authors: Matthew Jagielski, Om Thakkar, Lun Wang
Categories: cs.LG
Comments: accepted to ICASSP 2024
\\
  Speech models are often trained on sensitive data in order to improve model
performance, leading to potential privacy leakage. Our work considers noise
masking attacks, introduced by Amid et al. 2022, which attack automatic speech
recognition (ASR) models by requesting a transcript of an utterance which is
partially replaced with noise. They show that when a record has been seen at
training time, the model will transcribe the noisy record with its memorized
sensitive transcript. In our work, we extend these attacks beyond ASR models,
to attack pretrained speech encoders. Our method fine-tunes the encoder to
produce an ASR model, and then performs noise masking on this model, which we
find recovers private information from the pretraining data, despite the model
never having seen transcripts at pretraining time! We show how to improve the
precision of these attacks and investigate a number of countermeasures to our
attacks.
\\ ( https://arxiv.org/abs/2404.02052 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02058
Date: Tue, 2 Apr 2024 15:57:32 GMT   (198kb,D)

Title: Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1:
  Framework and Benchmarks
Authors: Jackson Burns and William Green
Categories: cs.LG physics.chem-ph
\\
  Quantitative Structure Property Relationship studies aim to define a mapping
between molecular structure and arbitrary quantities of interest. This was
historically accomplished via the development of descriptors which requires
significant domain expertise and struggles to generalize. Thus the field has
morphed into Molecular Property Prediction and been given over to learned
representations which are highly generalizable. The paper introduces fastprop,
a DeepQSPR framework which uses a cogent set of molecular level descriptors to
meet and exceed the performance of learned representations on diverse datasets
in dramatically less time. fastprop is freely available on github at
github.com/JacksonBurns/fastprop.
\\ ( https://arxiv.org/abs/2404.02058 ,  198kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02108
Date: Tue, 2 Apr 2024 17:08:23 GMT   (38kb)

Title: Variance-Reduced Policy Gradient Approaches for Infinite Horizon Average
  Reward Markov Decision Processes
Authors: Swetha Ganesh, Washim Uddin Mondal, Vaneet Aggarwal
Categories: cs.LG
Comments: 34 pages
\\
  We present two Policy Gradient-based methods with general parameterization in
the context of infinite horizon average reward Markov Decision Processes. The
first approach employs Implicit Gradient Transport for variance reduction,
ensuring an expected regret of the order $\tilde{\mathcal{O}}(T^{3/5})$. The
second approach, rooted in Hessian-based techniques, ensures an expected regret
of the order $\tilde{\mathcal{O}}(\sqrt{T})$. These results significantly
improve the state of the art of the problem, which achieves a regret of
$\tilde{\mathcal{O}}(T^{3/4})$.
\\ ( https://arxiv.org/abs/2404.02108 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02112
Date: Tue, 2 Apr 2024 17:13:04 GMT   (502kb,D)

Title: ImageNot: A contrast with ImageNet preserves model rankings
Authors: Olawale Salaudeen and Moritz Hardt
Categories: cs.LG cs.CV
\\
  We introduce ImageNot, a dataset designed to match the scale of ImageNet
while differing drastically in other aspects. We show that key model
architectures developed for ImageNet over the years rank identically when
trained and evaluated on ImageNot to how they rank on ImageNet. This is true
when training models from scratch or fine-tuning them. Moreover, the relative
improvements of each model over earlier models strongly correlate in both
datasets. We further give evidence that ImageNot has a similar utility as
ImageNet for transfer learning purposes. Our work demonstrates a surprising
degree of external validity in the relative performance of image classification
models. This stands in contrast with absolute accuracy numbers that typically
drop sharply even under small changes to a dataset.
\\ ( https://arxiv.org/abs/2404.02112 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02113
Date: Tue, 2 Apr 2024 17:13:22 GMT   (16853kb,D)

Title: Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL
Authors: Golnaz Mesbahi, Olya Mastikhina, Parham Mohammad Panahi, Martha White,
  Adam White
Categories: cs.LG
\\
  In continual or lifelong reinforcement learning access to the environment
should be limited. If we aspire to design algorithms that can run for
long-periods of time, continually adapting to new, unexpected situations then
we must be willing to deploy our agents without tuning their hyperparameters
over the agent's entire lifetime. The standard practice in deep RL -- and even
continual RL -- is to assume unfettered access to deployment environment for
the full lifetime of the agent. This paper explores the notion that progress in
lifelong RL research has been held back by inappropriate empirical
methodologies. In this paper we propose a new approach for tuning and
evaluating lifelong RL agents where only one percent of the experiment data can
be used for hyperparameter tuning. We then conduct an empirical study of DQN
and Soft Actor Critic across a variety of continuing and non-stationary
domains. We find both methods generally perform poorly when restricted to
one-percent tuning, whereas several algorithmic mitigations designed to
maintain network plasticity perform surprising well. In addition, we find that
properties designed to measure the network's ability to learn continually
indeed correlate with performance under one-percent tuning.
\\ ( https://arxiv.org/abs/2404.02113 ,  16853kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.01319 (*cross-listing*)
Date: Thu, 28 Mar 2024 03:46:56 GMT   (2709kb,D)

Title: Information Cascade Prediction under Public Emergencies: A Survey
Authors: Qi Zhang, Guang Wang, Li Lin, Kaiwen Xia, Shuai Wang
Categories: cs.SI cs.AI cs.CY
\\
  With the advent of the era of big data, massive information, expert
experience, and high-accuracy models bring great opportunities to the
information cascade prediction of public emergencies. However, the involvement
of specialist knowledge from various disciplines has resulted in a primarily
application-specific focus (e.g., earthquakes, floods, infectious diseases) for
information cascade prediction of public emergencies. The lack of a unified
prediction framework poses a challenge for classifying intersectional
prediction methods across different application fields. This survey paper
offers a systematic classification and summary of information cascade modeling,
prediction, and application. We aim to help researchers identify cutting-edge
research and comprehend models and methods of information cascade prediction
under public emergencies. By summarizing open issues and outlining future
directions in this field, this paper has the potential to be a valuable
resource for researchers conducting further studies on predicting information
cascades.
\\ ( https://arxiv.org/abs/2404.01319 ,  2709kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01320 (*cross-listing*)
Date: Thu, 28 Mar 2024 12:29:25 GMT   (3801kb,D)

Title: Graph-Based Optimisation of Network Expansion in a Dockless Bike Sharing
  System
Authors: Mark Roantree, Niamh Murphi, Dinh Viet Cuong, Vuong Minh Ngo
Categories: cs.SI cs.AI cs.CY
Comments: Accepted to publish in The 2024 IEEE 40th International Conference on
  Data Engineering Workshops (ICDEW&DASC-2024), pp. 1-8
\\
  Bike-sharing systems (BSSs) are deployed in over a thousand cities worldwide
and play an important role in many urban transportation systems. BSSs alleviate
congestion, reduce pollution and promote physical exercise. It is essential to
explore the spatiotemporal patterns of bike-sharing demand, as well as the
factors that influence these patterns, in order to optimise system operational
efficiency. In this study, an optimised geo-temporal graph is constructed using
trip data from Moby Bikes, a dockless BSS operator. The process of optimising
the graph unveiled prime locations for erecting new stations during future
expansions of the BSS. The Louvain algorithm, a community detection technique,
is employed to uncover usage patterns at different levels of temporal
granularity. The community detection results reveal largely self-contained
sub-networks that exhibit similar usage patterns at their respective levels of
temporal granularity. Overall, this study reinforces that BSSs are
intrinsically spatiotemporal systems, with community presence driven by
spatiotemporal dynamics. These findings may aid operators in improving
redistribution efficiency.
\\ ( https://arxiv.org/abs/2404.01320 ,  3801kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01352 (*cross-listing*)
Date: Mon, 1 Apr 2024 05:12:55 GMT   (23965kb,D)

Title: VortexViz: Finding Vortex Boundaries by Learning from Particle
  Trajectories
Authors: Akila de Silva, Nicholas Tee, Omkar Ghanekar, Fahim Hasan Khan,
  Gregory Dusek, James Davis, Alex Pang
Categories: physics.flu-dyn cs.AI cs.CV cs.GR
Comments: Under review
\\
  Vortices are studied in various scientific disciplines, offering insights
into fluid flow behavior. Visualizing the boundary of vortices is crucial for
understanding flow phenomena and detecting flow irregularities. This paper
addresses the challenge of accurately extracting vortex boundaries using deep
learning techniques. While existing methods primarily train on velocity
components, we propose a novel approach incorporating particle trajectories
(streamlines or pathlines) into the learning process. By leveraging the
regional/local characteristics of the flow field captured by streamlines or
pathlines, our methodology aims to enhance the accuracy of vortex boundary
extraction.
\\ ( https://arxiv.org/abs/2404.01352 ,  23965kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01358 (*cross-listing*)
Date: Mon, 1 Apr 2024 09:48:14 GMT   (1229kb,D)

Title: Utilizing AI and Social Media Analytics to Discover Adverse Side Effects
  of GLP-1 Receptor Agonists
Authors: Alon Bartal, Kathleen M. Jagodnik, Nava Pliskin, Abraham Seidmann
Categories: q-bio.QM cs.AI cs.CL cs.IR cs.LG cs.SI
Comments: 19 pages, 7 figures, 3 tables, 1 Appendix table
MSC-class: 62
\\
  Adverse side effects (ASEs) of drugs, revealed after FDA approval, pose a
threat to patient safety. To promptly detect overlooked ASEs, we developed a
digital health methodology capable of analyzing massive public data from social
media, published clinical research, manufacturers' reports, and ChatGPT. We
uncovered ASEs associated with the glucagon-like peptide 1 receptor agonists
(GLP-1 RA), a market expected to grow exponentially to $133.5 billion USD by
2030. Using a Named Entity Recognition (NER) model, our method successfully
detected 21 potential ASEs overlooked upon FDA approval, including irritability
and numbness. Our data-analytic approach revolutionizes the detection of
unreported ASEs associated with newly deployed drugs, leveraging cutting-edge
AI-driven social media analytics. It can increase the safety of new drugs in
the marketplace by unlocking the power of social media to support regulators
and manufacturers in the rapid discovery of hidden ASE risks.
\\ ( https://arxiv.org/abs/2404.01358 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01359 (*cross-listing*)
Date: Mon, 1 Apr 2024 10:35:35 GMT   (1148kb)

Title: Parallel Proportional Fusion of Spiking Quantum Neural Network for
  Optimizing Image Classification
Authors: Zuyu Xu, Kang Shen, Pengnian Cai, Tao Yang, Yuanming Hu, Shixian Chen,
  Yunlai Zhu, Zuheng Wu, Yuehua Dai, Jun Wang, Fei Yang
Categories: quant-ph cs.AI cs.NE
\\
  The recent emergence of the hybrid quantum-classical neural network (HQCNN)
architecture has garnered considerable attention due to the potential
advantages associated with integrating quantum principles to enhance various
facets of machine learning algorithms and computations. However, the current
investigated serial structure of HQCNN, wherein information sequentially passes
from one network to another, often imposes limitations on the trainability and
expressivity of the network. In this study, we introduce a novel architecture
termed Parallel Proportional Fusion of Quantum and Spiking Neural Networks
(PPF-QSNN). The dataset information is simultaneously fed into both the spiking
neural network and the variational quantum circuits, with the outputs
amalgamated in proportion to their individual contributions. We systematically
assess the impact of diverse PPF-QSNN parameters on network performance for
image classification, aiming to identify the optimal configuration. Numerical
results on the MNIST dataset unequivocally illustrate that our proposed
PPF-QSNN outperforms both the existing spiking neural network and the serial
quantum neural network across metrics such as accuracy, loss, and robustness.
This study introduces a novel and effective amalgamation approach for HQCNN,
thereby laying the groundwork for the advancement and application of quantum
advantage in artificial intelligent computations.
\\ ( https://arxiv.org/abs/2404.01359 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01363 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:32:22 GMT   (14692kb,D)

Title: AIOps Solutions for Incident Management: Technical Guidelines and A
  Comprehensive Literature Review
Authors: Youcef Remil and Anes Bendimerad and Romain Mathonat and Mehdi Kaytoue
Categories: cs.OS cs.AI cs.SE
\\
  The management of modern IT systems poses unique challenges, necessitating
scalability, reliability, and efficiency in handling extensive data streams.
Traditional methods, reliant on manual tasks and rule-based approaches, prove
inefficient for the substantial data volumes and alerts generated by IT
systems. Artificial Intelligence for Operating Systems (AIOps) has emerged as a
solution, leveraging advanced analytics like machine learning and big data to
enhance incident management. AIOps detects and predicts incidents, identifies
root causes, and automates healing actions, improving quality and reducing
operational costs. However, despite its potential, the AIOps domain is still in
its early stages, decentralized across multiple sectors, and lacking
standardized conventions. Research and industrial contributions are distributed
without consistent frameworks for data management, target problems,
implementation details, requirements, and capabilities. This study proposes an
AIOps terminology and taxonomy, establishing a structured incident management
procedure and providing guidelines for constructing an AIOps framework. The
research also categorizes contributions based on criteria such as incident
management tasks, application areas, data sources, and technical approaches.
The goal is to provide a comprehensive review of technical and research aspects
in AIOps for incident management, aiming to structure knowledge, identify gaps,
and establish a foundation for future developments in the field.
\\ ( https://arxiv.org/abs/2404.01363 ,  14692kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01397 (*cross-listing*)
Date: Mon, 1 Apr 2024 18:08:58 GMT   (810kb,D)

Title: Object-conditioned Bag of Instances for Few-Shot Personalized Instance
  Recognition
Authors: Umberto Michieli, Jijoong Moon, Daehyun Kim, Mete Ozay
Categories: cs.CV cs.AI cs.RO
Comments: ICASSP 2024. Copyright 2024 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses, in any
  current or future media, including reprinting/republishing this material for
  advertising or promotional purposes, creating new collective works, for
  resale or redistribution to servers or lists, or reuse of any copyrighted
  component of this work in other
\\
  Nowadays, users demand for increased personalization of vision systems to
localize and identify personal instances of objects (e.g., my dog rather than
dog) from a few-shot dataset only. Despite outstanding results of deep networks
on classical label-abundant benchmarks (e.g., those of the latest YOLOv8 model
for standard object detection), they struggle to maintain within-class
variability to represent different instances rather than object categories
only. We construct an Object-conditioned Bag of Instances (OBoI) based on
multi-order statistics of extracted features, where generic object detection
models are extended to search and identify personal instances from the OBoI's
metric space, without need for backpropagation. By relying on multi-order
statistics, OBoI achieves consistent superior accuracy in distinguishing
different instances. In the results, we achieve 77.1% personal object
recognition accuracy in case of 18 personal instances, showing about 12%
relative gain over the state of the art.
\\ ( https://arxiv.org/abs/2404.01397 ,  810kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01402 (*cross-listing*)
Date: Mon, 1 Apr 2024 18:12:09 GMT   (2923kb,D)

Title: ContactHandover: Contact-Guided Robot-to-Human Object Handover
Authors: Zixi Wang, Zeyi Liu, Nicolas Ouporov, Shuran Song
Categories: cs.RO cs.AI cs.CV
Comments: Project website:
  https://clairezixiwang.github.io/ContactHandover.github.io/
\\
  Robot-to-human object handover is an important step in many human robot
collaboration tasks. A successful handover requires the robot to maintain a
stable grasp on the object while making sure the human receives the object in a
natural and easy-to-use manner. We propose ContactHandover, a robot to human
handover system that consists of two phases: a contact-guided grasping phase
and an object delivery phase. During the grasping phase, ContactHandover
predicts both 6-DoF robot grasp poses and a 3D affordance map of human contact
points on the object. The robot grasp poses are reranked by penalizing those
that block human contact points, and the robot executes the highest ranking
grasp. During the delivery phase, the robot end effector pose is computed by
maximizing human contact points close to the human while minimizing the human
arm joint torques and displacements. We evaluate our system on 27 diverse
household objects and show that our system achieves better visibility and
reachability of human contacts to the receiver compared to several baselines.
More results can be found on
https://clairezixiwang.github.io/ContactHandover.github.io
\\ ( https://arxiv.org/abs/2404.01402 ,  2923kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01409 (*cross-listing*)
Date: Mon, 1 Apr 2024 18:26:29 GMT   (1310kb,D)

Title: OVFoodSeg: Elevating Open-Vocabulary Food Image Segmentation via
  Image-Informed Textual Representation
Authors: Xiongwei Wu, Sicheng Yu, Ee-Peng Lim, Chong-Wah Ngo
Categories: cs.CV cs.AI cs.MM
Comments: CVPR 2024; 12 pages
\\
  In the realm of food computing, segmenting ingredients from images poses
substantial challenges due to the large intra-class variance among the same
ingredients, the emergence of new ingredients, and the high annotation costs
associated with large food segmentation datasets. Existing approaches primarily
utilize a closed-vocabulary and static text embeddings setting. These methods
often fall short in effectively handling the ingredients, particularly new and
diverse ones. In response to these limitations, we introduce OVFoodSeg, a
framework that adopts an open-vocabulary setting and enhances text embeddings
with visual context. By integrating vision-language models (VLMs), our approach
enriches text embedding with image-specific information through two innovative
modules, eg, an image-to-text learner FoodLearner and an Image-Informed Text
Encoder. The training process of OVFoodSeg is divided into two stages: the
pre-training of FoodLearner and the subsequent learning phase for segmentation.
The pre-training phase equips FoodLearner with the capability to align visual
information with corresponding textual representations that are specifically
related to food, while the second phase adapts both the FoodLearner and the
Image-Informed Text Encoder for the segmentation task. By addressing the
deficiencies of previous models, OVFoodSeg demonstrates a significant
improvement, achieving an 4.9\% increase in mean Intersection over Union (mIoU)
on the FoodSeg103 dataset, setting a new milestone for food image segmentation.
\\ ( https://arxiv.org/abs/2404.01409 ,  1310kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01438 (*cross-listing*)
Date: Mon, 1 Apr 2024 19:22:43 GMT   (4420kb)

Title: Generation and Detection of Sign Language Deepfakes - A Linguistic and
  Visual Analysis
Authors: Shahzeb Naeem, Muhammad Riyyan Khan, Usman Tariq, Abhinav Dhall,
  Carlos Ivan Colon and Hasan Al-Nashash
Categories: cs.CV cs.AI
Comments: 13 pages, 13 figures, Computer Vision and Image Understanding Journal
\\
  A question in the realm of deepfakes is slowly emerging pertaining to whether
we can go beyond facial deepfakes and whether it would be beneficial to
society. Therefore, this research presents a positive application of deepfake
technology in upper body generation, while performing sign-language for the
Deaf and Hard of Hearing (DHoH) community. The resulting videos are later
vetted with a sign language expert. This is particularly helpful, given the
intricate nature of sign language, a scarcity of sign language experts, and
potential benefits for health and education. The objectives of this work
encompass constructing a reliable deepfake dataset, evaluating its technical
and visual credibility through computer vision and natural language processing
models, and assessing the plausibility of the generated content. With over 1200
videos, featuring both previously seen and unseen individuals for the
generation model, using the help of a sign language expert, we establish a
deepfake dataset in sign language that can further be utilized to detect fake
videos that may target certain people of determination.
\\ ( https://arxiv.org/abs/2404.01438 ,  4420kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01440 (*cross-listing*)
Date: Mon, 1 Apr 2024 19:23:00 GMT   (5219kb,D)

Title: Neural Implicit Representation for Building Digital Twins of Unknown
  Articulated Objects
Authors: Yijia Weng, Bowen Wen, Jonathan Tremblay, Valts Blukis, Dieter Fox,
  Leonidas Guibas, Stan Birchfield
Categories: cs.CV cs.AI cs.GR cs.RO
Comments: CVPR 2024
\\
  We address the problem of building digital twins of unknown articulated
objects from two RGBD scans of the object at different articulation states. We
decompose the problem into two stages, each addressing distinct aspects. Our
method first reconstructs object-level shape at each state, then recovers the
underlying articulation model including part segmentation and joint
articulations that associate the two states. By explicitly modeling point-level
correspondences and exploiting cues from images, 3D reconstructions, and
kinematics, our method yields more accurate and stable results compared to
prior work. It also handles more than one movable part and does not rely on any
object shape or structure priors. Project page:
https://github.com/NVlabs/DigitalTwinArt
\\ ( https://arxiv.org/abs/2404.01440 ,  5219kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01446 (*cross-listing*)
Date: Mon, 1 Apr 2024 19:33:41 GMT   (19126kb,D)

Title: Finding Regions of Interest in Whole Slide Images Using Multiple
  Instance Learning
Authors: Martim Afonso, Praphulla M. S. Bhawsar, Monjoy Saha, Jonas S. Almeida
  and Arlindo L. Oliveira
Categories: cs.CV cs.AI
\\
  Whole Slide Images (WSI), obtained by high-resolution digital scanning of
microscope slides at multiple scales, are the cornerstone of modern Digital
Pathology. However, they represent a particular challenge to
AI-based/AI-mediated analysis because pathology labeling is typically done at
slide-level, instead of tile-level. It is not just that medical diagnostics is
recorded at the specimen level, the detection of oncogene mutation is also
experimentally obtained, and recorded by initiatives like The Cancer Genome
Atlas (TCGA), at the slide level. This configures a dual challenge: a)
accurately predicting the overall cancer phenotype and b) finding out what
cellular morphologies are associated with it at the tile level. To address
these challenges, a weakly supervised Multiple Instance Learning (MIL) approach
was explored for two prevalent cancer types, Invasive Breast Carcinoma
(TCGA-BRCA) and Lung Squamous Cell Carcinoma (TCGA-LUSC). This approach was
explored for tumor detection at low magnification levels and TP53 mutations at
various levels. Our results show that a novel additive implementation of MIL
matched the performance of reference implementation (AUC 0.96), and was only
slightly outperformed by Attention MIL (AUC 0.97). More interestingly from the
perspective of the molecular pathologist, these different AI architectures
identify distinct sensitivities to morphological features (through the
detection of Regions of Interest, RoI) at different amplification levels.
Tellingly, TP53 mutation was most sensitive to features at the higher
applications where cellular morphology is resolved.
\\ ( https://arxiv.org/abs/2404.01446 ,  19126kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01459 (*cross-listing*)
Date: Mon, 1 Apr 2024 20:13:28 GMT   (2361kb)

Title: Game-Theoretic Deep Reinforcement Learning to Minimize Carbon Emissions
  and Energy Costs for AI Inference Workloads in Geo-Distributed Data Centers
Authors: Ninad Hogade, Sudeep Pasricha
Categories: cs.DC cs.AI cs.LG
Comments: arXiv admin note: text overlap with arXiv:2106.00066
\\
  Data centers are increasingly using more energy due to the rise in Artificial
Intelligence (AI) workloads, which negatively impacts the environment and
raises operational costs. Reducing operating expenses and carbon emissions
while maintaining performance in data centers is a challenging problem. This
work introduces a unique approach combining Game Theory (GT) and Deep
Reinforcement Learning (DRL) for optimizing the distribution of AI inference
workloads in geo-distributed data centers to reduce carbon emissions and cloud
operating (energy + data transfer) costs. The proposed technique integrates the
principles of non-cooperative Game Theory into a DRL framework, enabling data
centers to make intelligent decisions regarding workload allocation while
considering the heterogeneity of hardware resources, the dynamic nature of
electricity prices, inter-data center data transfer costs, and carbon
footprints. We conducted extensive experiments comparing our game-theoretic DRL
(GT-DRL) approach with current DRL-based and other optimization techniques. The
results demonstrate that our strategy outperforms the state-of-the-art in
reducing carbon emissions and minimizing cloud operating costs without
compromising computational performance. This work has significant implications
for achieving sustainability and cost-efficiency in data centers handling AI
inference workloads across diverse geographic locations.
\\ ( https://arxiv.org/abs/2404.01459 ,  2361kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01464 (*cross-listing*)
Date: Mon, 1 Apr 2024 20:25:04 GMT   (37862kb,D)

Title: Data-Efficient Unsupervised Interpolation Without Any Intermediate Frame
  for 4D Medical Images
Authors: JungEun Kim, Hangyul Yoon, Geondo Park, Kyungsu Kim, Eunho Yang
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: CVPR 2024
\\
  4D medical images, which represent 3D images with temporal information, are
crucial in clinical practice for capturing dynamic changes and monitoring
long-term disease progression. However, acquiring 4D medical images poses
challenges due to factors such as radiation exposure and imaging duration,
necessitating a balance between achieving high temporal resolution and
minimizing adverse effects. Given these circumstances, not only is data
acquisition challenging, but increasing the frame rate for each dataset also
proves difficult. To address this challenge, this paper proposes a simple yet
effective Unsupervised Volumetric Interpolation framework, UVI-Net. This
framework facilitates temporal interpolation without the need for any
intermediate frames, distinguishing it from the majority of other existing
unsupervised methods. Experiments on benchmark datasets demonstrate significant
improvements across diverse evaluation metrics compared to unsupervised and
supervised baselines. Remarkably, our approach achieves this superior
performance even when trained with a dataset as small as one, highlighting its
exceptional robustness and efficiency in scenarios with sparse supervision.
This positions UVI-Net as a compelling alternative for 4D medical imaging,
particularly in settings where data availability is limited. The source code is
available at https://github.com/jungeun122333/UVI-Net.
\\ ( https://arxiv.org/abs/2404.01464 ,  37862kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01476 (*cross-listing*)
Date: Mon, 1 Apr 2024 20:58:24 GMT   (32483kb,D)

Title: TraveLER: A Multi-LMM Agent Framework for Video Question-Answering
Authors: Chuyi Shang, Amos You, Sanjay Subramanian, Trevor Darrell, Roei Herzig
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Recently, Large Multimodal Models (LMMs) have made significant progress in
video question-answering using a frame-wise approach by leveraging large-scale,
image-based pretraining in a zero-shot manner. While image-based methods for
videos have shown impressive performance, a current limitation is that they
often overlook how key timestamps are selected and cannot adjust when incorrect
timestamps are identified. Moreover, they are unable to extract details
relevant to the question, instead providing general descriptions of the frame.
To overcome this, we design a multi-LMM agent framework that travels along the
video, iteratively collecting relevant information from keyframes through
interactive question-asking until there is sufficient information to answer the
question. Specifically, we propose TraveLER, a model that can create a plan to
"Traverse" through the video, ask questions about individual frames to "Locate"
and store key information, and then "Evaluate" if there is enough information
to answer the question. Finally, if there is not enough information, our method
is able to "Replan" based on its collected knowledge. Through extensive
experiments, we find that the proposed TraveLER approach improves performance
on several video question-answering benchmarks, such as NExT-QA, STAR, and
Perception Test, without the need to fine-tune on specific datasets.
\\ ( https://arxiv.org/abs/2404.01476 ,  32483kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01486 (*cross-listing*)
Date: Mon, 1 Apr 2024 21:11:43 GMT   (17497kb,D)

Title: QuAD: Query-based Interpretable Neural Motion Planning for Autonomous
  Driving
Authors: Sourav Biswas, Sergio Casas, Quinlan Sykora, Ben Agro, Abbas Sadat,
  Raquel Urtasun
Categories: cs.RO cs.AI cs.CV cs.LG
\\
  A self-driving vehicle must understand its environment to determine the
appropriate action. Traditional autonomy systems rely on object detection to
find the agents in the scene. However, object detection assumes a discrete set
of objects and loses information about uncertainty, so any errors compound when
predicting the future behavior of those agents. Alternatively, dense occupancy
grid maps have been utilized to understand free-space. However, predicting a
grid for the entire scene is wasteful since only certain spatio-temporal
regions are reachable and relevant to the self-driving vehicle. We present a
unified, interpretable, and efficient autonomy framework that moves away from
cascading modules that first perceive, then predict, and finally plan. Instead,
we shift the paradigm to have the planner query occupancy at relevant
spatio-temporal points, restricting the computation to those regions of
interest. Exploiting this representation, we evaluate candidate trajectories
around key factors such as collision avoidance, comfort, and progress for
safety and interpretability. Our approach achieves better highway driving
quality than the state-of-the-art in high-fidelity closed-loop simulations.
\\ ( https://arxiv.org/abs/2404.01486 ,  17497kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01492 (*cross-listing*)
Date: Mon, 1 Apr 2024 21:28:50 GMT   (38960kb,D)

Title: Modality Translation for Object Detection Adaptation Without Forgetting
  Prior Knowledge
Authors: Heitor Rapela Medeiros, Masih Aminbeidokhti, Fidel Guerrero Pena,
  David Latortue, Eric Granger, and Marco Pedersoli
Categories: cs.CV cs.AI
\\
  A common practice in deep learning consists of training large neural networks
on massive datasets to perform accurately for different domains and tasks.
While this methodology may work well in numerous application areas, it only
applies across modalities due to a larger distribution shift in data captured
using different sensors. This paper focuses on the problem of adapting a large
object detection model to one or multiple modalities while being efficient. To
do so, we propose ModTr as an alternative to the common approach of fine-tuning
large models. ModTr consists of adapting the input with a small transformation
network trained to minimize the detection loss directly. The original model can
therefore work on the translated inputs without any further change or
fine-tuning to its parameters. Experimental results on translating from IR to
RGB images on two well-known datasets show that this simple ModTr approach
provides detectors that can perform comparably or better than the standard
fine-tuning without forgetting the original knowledge. This opens the doors to
a more flexible and efficient service-based detection pipeline in which,
instead of using a different detector for each modality, a unique and unaltered
server is constantly running, where multiple modalities with the corresponding
translations can query it. Code: https://github.com/heitorrapela/ModTr.
\\ ( https://arxiv.org/abs/2404.01492 ,  38960kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01509 (*cross-listing*)
Date: Mon, 1 Apr 2024 22:25:48 GMT   (2896kb,D)

Title: Can Biases in ImageNet Models Explain Generalization?
Authors: Paul Gavrikov and Janis Keuper
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: Accepted at CVPR2024
\\
  The robust generalization of models to rare, in-distribution (ID) samples
drawn from the long tail of the training distribution and to
out-of-training-distribution (OOD) samples is one of the major challenges of
current deep learning methods. For image classification, this manifests in the
existence of adversarial attacks, the performance drops on distorted images,
and a lack of generalization to concepts such as sketches. The current
understanding of generalization in neural networks is very limited, but some
biases that differentiate models from human vision have been identified and
might be causing these limitations. Consequently, several attempts with varying
success have been made to reduce these biases during training to improve
generalization. We take a step back and sanity-check these attempts. Fixing the
architecture to the well-established ResNet-50, we perform a large-scale study
on 48 ImageNet models obtained via different training methods to understand how
and if these biases - including shape bias, spectral biases, and critical bands
- interact with generalization. Our extensive study results reveal that
contrary to previous findings, these biases are insufficient to accurately
predict the generalization of a model holistically. We provide access to all
checkpoints and evaluation code at
https://github.com/paulgavrikov/biases_vs_generalization
\\ ( https://arxiv.org/abs/2404.01509 ,  2896kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01524 (*cross-listing*)
Date: Mon, 1 Apr 2024 23:11:15 GMT   (8376kb,D)

Title: On Train-Test Class Overlap and Detection for Image Retrieval
Authors: Chull Hwan Song, Jooyoung Yoon, Taebaek Hwang, Shunghyun Choi, Yeong
  Hyeon Gu, Yannis Avrithis
Categories: cs.CV cs.AI
Comments: CVPR2024 Accepted
\\
  How important is it for training and evaluation sets to not have class
overlap in image retrieval? We revisit Google Landmarks v2 clean, the most
popular training set, by identifying and removing class overlap with Revisited
Oxford and Paris [34], the most popular evaluation set. By comparing the
original and the new RGLDv2-clean on a benchmark of reproduced state-of-the-art
methods, our findings are striking. Not only is there a dramatic drop in
performance, but it is inconsistent across methods, changing the ranking.What
does it take to focus on objects or interest and ignore background clutter when
indexing? Do we need to train an object detector and the representation
separately? Do we need location supervision? We introduce Single-stage
Detect-to-Retrieve (CiDeR), an end-to-end, single-stage pipeline to detect
objects of interest and extract a global image representation. We outperform
previous state-of-the-art on both existing training sets and the new
RGLDv2-clean. Our dataset is available at
https://github.com/dealicious-inc/RGLDv2-clean.
\\ ( https://arxiv.org/abs/2404.01524 ,  8376kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01548 (*cross-listing*)
Date: Tue, 2 Apr 2024 01:28:44 GMT   (1021kb,D)

Title: mChartQA: A universal benchmark for multimodal Chart Question Answer
  based on Vision-Language Alignment and Reasoning
Authors: Jingxuan Wei, Nan Xu, Guiyong Chang, Yin Luo, BiHui Yu, Ruifeng Guo
Categories: cs.CV cs.AI
\\
  In the fields of computer vision and natural language processing, multimodal
chart question-answering, especially involving color, structure, and textless
charts, poses significant challenges. Traditional methods, which typically
involve either direct multimodal processing or a table-to-text conversion
followed by language model analysis, have limitations in effectively handling
these complex scenarios. This paper introduces a novel multimodal chart
question-answering model, specifically designed to address these intricate
tasks. Our model integrates visual and linguistic processing, overcoming the
constraints of existing methods. We adopt a dual-phase training approach: the
initial phase focuses on aligning image and text representations, while the
subsequent phase concentrates on optimizing the model's interpretative and
analytical abilities in chart-related queries. This approach has demonstrated
superior performance on multiple public datasets, particularly in handling
color, structure, and textless chart questions, indicating its effectiveness in
complex multimodal tasks.
\\ ( https://arxiv.org/abs/2404.01548 ,  1021kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01551 (*cross-listing*)
Date: Tue, 2 Apr 2024 01:30:41 GMT   (4976kb,D)

Title: Multi-Agent Reinforcement Learning with Control-Theoretic Safety
  Guarantees for Dynamic Network Bridging
Authors: Raffaele Galliera, Konstantinos Mitsopoulos, Niranjan Suri, Raffaele
  Romagnoli
Categories: cs.MA cs.AI cs.LG cs.NI cs.SY eess.SY
Comments: 7 pages, 21 equations, 3 figures, 1 algorithm, and 1 table
\\
  Addressing complex cooperative tasks in safety-critical environments poses
significant challenges for Multi-Agent Systems, especially under conditions of
partial observability. This work introduces a hybrid approach that integrates
Multi-Agent Reinforcement Learning with control-theoretic methods to ensure
safe and efficient distributed strategies. Our contributions include a novel
setpoint update algorithm that dynamically adjusts agents' positions to
preserve safety conditions without compromising the mission's objectives.
Through experimental validation, we demonstrate significant advantages over
conventional MARL strategies, achieving comparable task performance with zero
safety violations. Our findings indicate that integrating safe control with
learning approaches not only enhances safety compliance but also achieves good
performance in mission objectives.
\\ ( https://arxiv.org/abs/2404.01551 ,  4976kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01557 (*cross-listing*)
Date: Tue, 2 Apr 2024 01:45:03 GMT   (2012kb,D)

Title: Distributed Autonomous Swarm Formation for Dynamic Network Bridging
Authors: Raffaele Galliera, Thies M\"ohlenhof, Alessandro Amato, Daniel Duran,
  Kristen Brent Venable, Niranjan Suri
Categories: cs.MA cs.AI cs.LG cs.RO
Comments: 6 pages, 3 figures, 1 table, 1 algorithm
\\
  Effective operation and seamless cooperation of robotic systems are a
fundamental component of next-generation technologies and applications. In
contexts such as disaster response, swarm operations require coordinated
behavior and mobility control to be handled in a distributed manner, with the
quality of the agents' actions heavily relying on the communication between
them and the underlying network. In this paper, we formulate the problem of
dynamic network bridging in a novel Decentralized Partially Observable Markov
Decision Process (Dec-POMDP), where a swarm of agents cooperates to form a link
between two distant moving targets. Furthermore, we propose a Multi-Agent
Reinforcement Learning (MARL) approach for the problem based on Graph
Convolutional Reinforcement Learning (DGN) which naturally applies to the
networked, distributed nature of the task. The proposed method is evaluated in
a simulated environment and compared to a centralized heuristic baseline
showing promising results. Moreover, a further step in the direction of
sim-to-real transfer is presented, by additionally evaluating the proposed
approach in a near Live Virtual Constructive (LVC) UAV framework.
\\ ( https://arxiv.org/abs/2404.01557 ,  2012kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01558 (*cross-listing*)
Date: Tue, 2 Apr 2024 01:45:57 GMT   (366kb,D)

Title: Automated User Story Generation with Test Case Specification Using Large
  Language Model
Authors: Tajmilur Rahman and Yuecai Zhu
Categories: cs.SE cs.AI
Comments: 10 pages including 2 pages of Appendix
\\
  Modern Software Engineering era is moving fast with the assistance of
artificial intelligence (AI), especially Large Language Models (LLM).
Researchers have already started automating many parts of the software
development workflow. Requirements Engineering (RE) is a crucial phase that
begins the software development cycle through multiple discussions on a
proposed scope of work documented in different forms. RE phase ends with a list
of user-stories for each unit task identified through discussions and usually
these are created and tracked on a project management tool such as Jira,
AzurDev etc. In this research we developed a tool "GeneUS" using GPT-4.0 to
automatically create user stories from requirements document which is the
outcome of the RE phase. The output is provided in JSON format leaving the
possibilities open for downstream integration to the popular project management
tools. Analyzing requirements documents takes significant effort and multiple
meetings with stakeholders. We believe, automating this process will certainly
reduce additional load off the software engineers, and increase the
productivity since they will be able to utilize their time on other prioritized
tasks.
\\ ( https://arxiv.org/abs/2404.01558 ,  366kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01596 (*cross-listing*)
Date: Tue, 2 Apr 2024 02:36:31 GMT   (3045kb,D)

Title: PhysORD: A Neuro-Symbolic Approach for Physics-infused Motion Prediction
  in Off-road Driving
Authors: Zhipeng Zhao, Bowen Li, Yi Du, Taimeng Fu, Chen Wang
Categories: cs.RO cs.AI
\\
  Motion prediction is critical for autonomous off-road driving, however, it
presents significantly more challenges than on-road driving because of the
complex interaction between the vehicle and the terrain. Traditional
physics-based approaches encounter difficulties in accurately modeling dynamic
systems and external disturbance. In contrast, data-driven neural networks
require extensive datasets and struggle with explicitly capturing the
fundamental physical laws, which can easily lead to poor generalization. By
merging the advantages of both methods, neuro-symbolic approaches present a
promising direction. These methods embed physical laws into neural models,
potentially significantly improving generalization capabilities. However, no
prior works were evaluated in real-world settings for off-road driving. To
bridge this gap, we present PhysORD, a neural-symbolic approach integrating the
conservation law, i.e., the Euler-Lagrange equation, into data-driven neural
models for motion prediction in off-road driving. Our experiments showed that
PhysORD can accurately predict vehicle motion and tolerate external disturbance
by modeling uncertainties. It outperforms existing methods both in accuracy and
efficiency and demonstrates data-efficient learning and generalization ability
in long-term prediction.
\\ ( https://arxiv.org/abs/2404.01596 ,  3045kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01620 (*cross-listing*)
Date: Tue, 2 Apr 2024 04:07:22 GMT   (1159kb)

Title: Voice EHR: Introducing Multimodal Audio Data for Health
Authors: James Anibal, Hannah Huth, Ming Li, Lindsey Hazen, Yen Minh Lam,
  Nguyen Thi Thu Hang, Michael Kleinman, Shelley Ost, Christopher Jackson,
  Laura Sprabery, Cheran Elangovan, Balaji Krishnaiah, Lee Akst, Ioan Lina,
  Iqbal Elyazar, Lenny Ekwati, Stefan Jansen, Richard Nduwayezu, Charisse
  Garcia, Jeffrey Plum, Jacqueline Brenner, Miranda Song, Emily Ricotta, David
  Clifton, C. Louise Thwaites, Yael Bensoussan, Bradford Wood
Categories: cs.SD cs.AI cs.CY eess.AS
Comments: 18 pages, 2 figures, 7 tables
\\
  Large AI models trained on audio data may have the potential to rapidly
classify patients, enhancing medical decision-making and potentially improving
outcomes through early detection. Existing technologies depend on limited
datasets using expensive recording equipment in high-income, English-speaking
countries. This challenges deployment in resource-constrained, high-volume
settings where audio data may have a profound impact. This report introduces a
novel data type and a corresponding collection system that captures health data
through guided questions using only a mobile/web application. This application
ultimately results in an audio electronic health record (voice EHR) which may
contain complex biomarkers of health from conventional voice/respiratory
features, speech patterns, and language with semantic meaning - compensating
for the typical limitations of unimodal clinical datasets. This report
introduces a consortium of partners for global work, presents the application
used for data collection, and showcases the potential of informative voice EHR
to advance the scalability and diversity of audio AI.
\\ ( https://arxiv.org/abs/2404.01620 ,  1159kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01622 (*cross-listing*)
Date: Tue, 2 Apr 2024 04:11:37 GMT   (1659kb)

Title: Gen4DS: Workshop on Data Storytelling in an Era of Generative AI
Authors: Xingyu Lan, Leni Yang, Zezhong Wang, Danqing Shi, Sheelagh Carpendale
Categories: cs.HC cs.AI cs.GR
\\
  Storytelling is an ancient and precious human ability that has been
rejuvenated in the digital age. Over the last decade, there has been a notable
surge in the recognition and application of data storytelling, both in academia
and industry. Recently, the rapid development of generative AI has brought new
opportunities and challenges to this field, sparking numerous new questions.
These questions may not necessarily be quickly transformed into papers, but we
believe it is necessary to promptly discuss them to help the community better
clarify important issues and research agendas for the future. We thus invite
you to join our workshop (Gen4DS) to discuss questions such as: How can
generative AI facilitate the creation of data stories? How might generative AI
alter the workflow of data storytellers? What are the pitfalls and risks of
incorporating AI in storytelling? We have designed both paper presentations and
interactive activities (including hands-on creation, group discussion pods, and
debates on controversial issues) for the workshop. We hope that participants
will learn about the latest advances and pioneering work in data storytelling,
engage in critical conversations with each other, and have an enjoyable,
unforgettable, and meaningful experience at the event.
\\ ( https://arxiv.org/abs/2404.01622 ,  1659kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01628 (*cross-listing*)
Date: Tue, 2 Apr 2024 04:29:01 GMT   (1552kb,D)

Title: Learning Equi-angular Representations for Online Continual Learning
Authors: Minhyuk Seo, Hyunseo Koh, Wonje Jeung, Minjae Lee, San Kim, Hankook
  Lee, Sungjun Cho, Sungik Choi, Hyunwoo Kim, Jonghyun Choi
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024
\\
  Online continual learning suffers from an underfitted solution due to
insufficient training for prompt model update (e.g., single-epoch training). To
address the challenge, we propose an efficient online continual learning method
using the neural collapse phenomenon. In particular, we induce neural collapse
to form a simplex equiangular tight frame (ETF) structure in the representation
space so that the continuously learned model with a single epoch can better fit
to the streamed data by proposing preparatory data training and residual
correction in the representation space. With an extensive set of empirical
validations using CIFAR-10/100, TinyImageNet, ImageNet-200, and ImageNet-1K, we
show that our proposed method outperforms state-of-the-art methods by a
noticeable margin in various online continual learning scenarios such as
disjoint and Gaussian scheduled continuous (i.e., boundary-free) data setups.
\\ ( https://arxiv.org/abs/2404.01628 ,  1552kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01636 (*cross-listing*)
Date: Tue, 2 Apr 2024 04:53:39 GMT   (32406kb,D)

Title: Learning to Control Camera Exposure via Reinforcement Learning
Authors: Kyunghyun Lee, Ukcheol Shin, Byeong-Uk Lee
Categories: cs.CV cs.AI cs.LG cs.RO cs.SY eess.SY
Comments: Accepted at CVPR 2024, *First two authors contributed equally to this
  work. Project page link: https://sites.google.com/view/drl-ae
\\
  Adjusting camera exposure in arbitrary lighting conditions is the first step
to ensure the functionality of computer vision applications. Poorly adjusted
camera exposure often leads to critical failure and performance degradation.
Traditional camera exposure control methods require multiple convergence steps
and time-consuming processes, making them unsuitable for dynamic lighting
conditions. In this paper, we propose a new camera exposure control framework
that rapidly controls camera exposure while performing real-time processing by
exploiting deep reinforcement learning. The proposed framework consists of four
contributions: 1) a simplified training ground to simulate real-world's diverse
and dynamic lighting changes, 2) flickering and image attribute-aware reward
design, along with lightweight state design for real-time processing, 3) a
static-to-dynamic lighting curriculum to gradually improve the agent's
exposure-adjusting capability, and 4) domain randomization techniques to
alleviate the limitation of the training ground and achieve seamless
generalization in the wild.As a result, our proposed method rapidly reaches a
desired exposure level within five steps with real-time processing (1 ms).
Also, the acquired images are well-exposed and show superiority in various
computer vision tasks, such as feature extraction and object detection.
\\ ( https://arxiv.org/abs/2404.01636 ,  32406kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01643 (*cross-listing*)
Date: Tue, 2 Apr 2024 05:19:27 GMT   (9924kb,D)

Title: A Closer Look at Spatial-Slice Features Learning for COVID-19 Detection
Authors: Chih-Chung Hsu, Chia-Ming Lee, Yang Fan Chiang, Yi-Shiuan Chou,
  Chih-Yu Jiang, Shen-Chieh Tai and Chi-Han Tsai
Categories: cs.CV cs.AI
Comments: Submitted to DEF-AI-MIA workshop. arXiv admin note: text overlap with
  arXiv:2403.11230
\\
  Conventional Computed Tomography (CT) imaging recognition faces two
significant challenges: (1) There is often considerable variability in the
resolution and size of each CT scan, necessitating strict requirements for the
input size and adaptability of models. (2) CT-scan contains large number of
out-of-distribution (OOD) slices. The crucial features may only be present in
specific spatial regions and slices of the entire CT scan. How can we
effectively figure out where these are located? To deal with this, we introduce
an enhanced Spatial-Slice Feature Learning (SSFL++) framework specifically
designed for CT scan. It aim to filter out a OOD data within whole CT scan,
enabling our to select crucial spatial-slice for analysis by reducing 70%
redundancy totally. Meanwhile, we proposed Kernel-Density-based slice Sampling
(KDS) method to improve the stability when training and inference stage,
therefore speeding up the rate of convergence and boosting performance. As a
result, the experiments demonstrate the promising performance of our model
using a simple EfficientNet-2D (E2D) model, even with only 1% of the training
data. The efficacy of our approach has been validated on the COVID-19-CT-DB
datasets provided by the DEF-AI-MIA workshop, in conjunction with CVPR 2024.
Our source code will be made available.
\\ ( https://arxiv.org/abs/2404.01643 ,  9924kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01654 (*cross-listing*)
Date: Tue, 2 Apr 2024 05:53:34 GMT   (2353kb,D)

Title: AI WALKUP: A Computer-Vision Approach to Quantifying MDS-UPDRS in
  Parkinson's Disease
Authors: Xiang Xiang, Zihan Zhang, Jing Ma, Yao Deng
Categories: cs.CV cs.AI eess.IV eess.SP
Comments: Technical report for AI WALKUP, an APP winning 3rd Prize of 2022 HUST
  GS AI Innovation and Design Competition
\\
  Parkinson's Disease (PD) is the second most common neurodegenerative
disorder. The existing assessment method for PD is usually the Movement
Disorder Society - Unified Parkinson's Disease Rating Scale (MDS-UPDRS) to
assess the severity of various types of motor symptoms and disease progression.
However, manual assessment suffers from high subjectivity, lack of consistency,
and high cost and low efficiency of manual communication. We want to use a
computer vision based solution to capture human pose images based on a camera,
reconstruct and perform motion analysis using algorithms, and extract the
features of the amount of motion through feature engineering. The proposed
approach can be deployed on different smartphones, and the video recording and
artificial intelligence analysis can be done quickly and easily through our
APP.
\\ ( https://arxiv.org/abs/2404.01654 ,  2353kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01685 (*cross-listing*)
Date: Tue, 2 Apr 2024 06:42:14 GMT   (515kb,D)

Title: A Methodology for Improving Accuracy of Embedded Spiking Neural Networks
  through Kernel Size Scaling
Authors: Rachmad Vidya Wicaksana Putra, Muhammad Shafique
Categories: cs.NE cs.AI cs.LG
Comments: 3 pages, 3 figures
\\
  Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption
for machine learning-based applications due to their sparse spike-based
operations. Currently, most of the SNN architectures need a significantly
larger model size to achieve higher accuracy, which is not suitable for
resource-constrained embedded applications. Therefore, developing SNNs that can
achieve high accuracy with acceptable memory footprint is highly needed. Toward
this, we propose a novel methodology that improves the accuracy of SNNs through
kernel size scaling. Its key steps include investigating the impact of
different kernel sizes on the accuracy, devising new sets of kernel sizes,
generating SNN architectures based on the selected kernel sizes, and analyzing
the accuracy-memory trade-offs for SNN model selection. The experimental
results show that our methodology achieves higher accuracy than
state-of-the-art (93.24% accuracy for CIFAR10 and 70.84% accuracy for CIFAR100)
with less than 10M parameters and up to 3.45x speed-up of searching time,
thereby making it suitable for embedded applications.
\\ ( https://arxiv.org/abs/2404.01685 ,  515kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01709 (*cross-listing*)
Date: Tue, 2 Apr 2024 07:49:08 GMT   (29257kb,D)

Title: Upsample Guidance: Scale Up Diffusion Models without Training
Authors: Juno Hwang, Yong-Hyun Park and Junghyo Jo
Categories: cs.CV cs.AI
Comments: 15 pages, 15 Figures
\\
  Diffusion models have demonstrated superior performance across various
generative tasks including images, videos, and audio. However, they encounter
difficulties in directly generating high-resolution samples. Previously
proposed solutions to this issue involve modifying the architecture, further
training, or partitioning the sampling process into multiple stages. These
methods have the limitation of not being able to directly utilize pre-trained
models as-is, requiring additional work. In this paper, we introduce upsample
guidance, a technique that adapts pretrained diffusion model (e.g., $512^2$) to
generate higher-resolution images (e.g., $1536^2$) by adding only a single term
in the sampling process. Remarkably, this technique does not necessitate any
additional training or relying on external models. We demonstrate that upsample
guidance can be applied to various models, such as pixel-space, latent space,
and video diffusion models. We also observed that the proper selection of
guidance scale can improve image quality, fidelity, and prompt alignment.
\\ ( https://arxiv.org/abs/2404.01709 ,  29257kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01716 (*cross-listing*)
Date: Tue, 2 Apr 2024 08:01:05 GMT   (108kb,D)

Title: Effective internal language model training and fusion for factorized
  transducer model
Authors: Jinxi Guo, Niko Moritz, Yingyi Ma, Frank Seide, Chunyang Wu, Jay
  Mahadeokar, Ozlem Kalinli, Christian Fuegen, Mike Seltzer
Categories: eess.AS cs.AI cs.CL cs.LG
Comments: Accepted to ICASSP 2024
\\
  The internal language model (ILM) of the neural transducer has been widely
studied. In most prior work, it is mainly used for estimating the ILM score and
is subsequently subtracted during inference to facilitate improved integration
with external language models. Recently, various of factorized transducer
models have been proposed, which explicitly embrace a standalone internal
language model for non-blank token prediction. However, even with the adoption
of factorized transducer models, limited improvement has been observed compared
to shallow fusion. In this paper, we propose a novel ILM training and decoding
strategy for factorized transducer models, which effectively combines the
blank, acoustic and ILM scores. Our experiments show a 17% relative improvement
over the standard decoding method when utilizing a well-trained ILM and the
proposed decoding strategy on LibriSpeech datasets. Furthermore, when compared
to a strong RNN-T baseline enhanced with external LM fusion, the proposed model
yields a 5.5% relative improvement on general-sets and an 8.9% WER reduction
for rare words. The proposed model can achieve superior performance without
relying on external language models, rendering it highly efficient for
production use-cases. To further improve the performance, we propose a novel
and memory-efficient ILM-fusion-aware minimum word error rate (MWER) training
method which improves ILM integration significantly.
\\ ( https://arxiv.org/abs/2404.01716 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01740 (*cross-listing*)
Date: Tue, 2 Apr 2024 08:59:58 GMT   (12319kb,D)

Title: Weakly-supervised Audio Separation via Bi-modal Semantic Similarity
Authors: Tanvir Mahmud, Saeed Amizadeh, Kazuhito Koishida, Diana Marculescu
Categories: cs.SD cs.AI eess.AS
Comments: Tech report. Accepted in ICLR-2024
\\
  Conditional sound separation in multi-source audio mixtures without having
access to single source sound data during training is a long standing
challenge. Existing mix-and-separate based methods suffer from significant
performance drop with multi-source training mixtures due to the lack of
supervision signal for single source separation cases during training. However,
in the case of language-conditional audio separation, we do have access to
corresponding text descriptions for each audio mixture in our training data,
which can be seen as (rough) representations of the audio samples in the
language modality. To this end, in this paper, we propose a generic bi-modal
separation framework which can enhance the existing unsupervised frameworks to
separate single-source signals in a target modality (i.e., audio) using the
easily separable corresponding signals in the conditioning modality (i.e.,
language), without having access to single-source samples in the target
modality during training. We empirically show that this is well within reach if
we have access to a pretrained joint embedding model between the two modalities
(i.e., CLAP). Furthermore, we propose to incorporate our framework into two
fundamental scenarios to enhance separation performance. First, we show that
our proposed methodology significantly improves the performance of purely
unsupervised baselines by reducing the distribution shift between training and
test samples. In particular, we show that our framework can achieve 71% boost
in terms of Signal-to-Distortion Ratio (SDR) over the baseline, reaching 97.5%
of the supervised learning performance. Second, we show that we can further
improve the performance of the supervised learning itself by 17% if we augment
it by our proposed weakly-supervised framework, that enables a powerful
semi-supervised framework for audio separation.
\\ ( https://arxiv.org/abs/2404.01740 ,  12319kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01741 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:00:45 GMT   (1933kb,D)

Title: Intrusion Tolerance for Networked Systems through Two-Level Feedback
  Control
Authors: Kim Hammar and Rolf Stadler
Categories: cs.DC cs.AI cs.CR cs.GT cs.SY eess.SY
Comments: Preprint to appear in the conference proceedings of the 54th
  IEEE/IFIP Dependable Systems and Networks Conference (DSN'24)
\\
  We formulate intrusion tolerance for a system with service replicas as a
two-level optimal control problem. On the local level node controllers perform
intrusion recovery, and on the global level a system controller manages the
replication factor. The local and global control problems can be formulated as
classical problems in operations research, namely, the machine replacement
problem and the inventory replenishment problem. Based on this formulation, we
design TOLERANCE, a novel control architecture for intrusion-tolerant systems.
We prove that the optimal control strategies on both levels have threshold
structure and design efficient algorithms for computing them. We implement and
evaluate TOLERANCE in an emulation environment where we run 10 types of network
intrusions. The results show that TOLERANCE can improve service availability
and reduce operational cost compared with state-of-the-art intrusion-tolerant
systems.
\\ ( https://arxiv.org/abs/2404.01741 ,  1933kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01745 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:01:58 GMT   (683kb,D)

Title: Unleash the Potential of CLIP for Video Highlight Detection
Authors: Donghoon Han, Seunghyeon Seo, Eunhwan Park, Seong-Uk Nam, Nojun Kwak
Categories: cs.CV cs.AI
\\
  Multimodal and large language models (LLMs) have revolutionized the
utilization of open-world knowledge, unlocking novel potentials across various
tasks and applications. Among these domains, the video domain has notably
benefited from their capabilities. In this paper, we present Highlight-CLIP
(HL-CLIP), a method designed to excel in the video highlight detection task by
leveraging the pre-trained knowledge embedded in multimodal models. By simply
fine-tuning the multimodal encoder in combination with our innovative saliency
pooling technique, we have achieved the state-of-the-art performance in the
highlight detection task, the QVHighlight Benchmark, to the best of our
knowledge.
\\ ( https://arxiv.org/abs/2404.01745 ,  683kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01746 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:04:06 GMT   (3271kb,D)

Title: Towards Scalable & Efficient Interaction-Aware Planning in Autonomous
  Vehicles using Knowledge Distillation
Authors: Piyush Gupta, David Isele, Sangjae Bae
Categories: cs.RO cs.AI cs.LG
\\
  Real-world driving involves intricate interactions among vehicles navigating
through dense traffic scenarios. Recent research focuses on enhancing the
interaction awareness of autonomous vehicles to leverage these interactions in
decision-making. These interaction-aware planners rely on neural-network-based
prediction models to capture inter-vehicle interactions, aiming to integrate
these predictions with traditional control techniques such as Model Predictive
Control. However, this integration of deep learning-based models with
traditional control paradigms often results in computationally demanding
optimization problems, relying on heuristic methods. This study introduces a
principled and efficient method for combining deep learning with constrained
optimization, employing knowledge distillation to train smaller and more
efficient networks, thereby mitigating complexity. We demonstrate that these
refined networks maintain the problem-solving efficacy of larger models while
significantly accelerating optimization. Specifically, in the domain of
interaction-aware trajectory planning for autonomous vehicles, we illustrate
that training a smaller prediction network using knowledge distillation speeds
up optimization without sacrificing accuracy.
\\ ( https://arxiv.org/abs/2404.01746 ,  3271kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01752 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:07:12 GMT   (22600kb,D)

Title: Safe Interval RRT* for Scalable Multi-Robot Path Planning in Continuous
  Space
Authors: Joonyeol Sim, Joonkyung Kim, Changjoo Nam
Categories: cs.RO cs.AI cs.MA
\\
  In this paper, we consider the problem of Multi-Robot Path Planning (MRPP) in
continuous space to find conflict-free paths. The difficulty of the problem
arises from two primary factors. First, the involvement of multiple robots
leads to combinatorial decision-making, which escalates the search space
exponentially. Second, the continuous space presents potentially infinite
states and actions. For this problem, we propose a two-level approach where the
low level is a sampling-based planner Safe Interval RRT* (SI-RRT*) that finds a
collision-free trajectory for individual robots. The high level can use any
method that can resolve inter-robot conflicts where we employ two
representative methods that are Prioritized Planning (SI-CPP) and Conflict
Based Search (SI-CCBS). Experimental results show that SI-RRT* can find a
high-quality solution quickly with a small number of samples. SI-CPP exhibits
improved scalability while SI-CCBS produces higher-quality solutions compared
to the state-of-the-art planners for continuous space. Compared to the most
scalable existing algorithm, SI-CPP achieves a success rate that is up to 94%
higher with 100 robots while maintaining solution quality (i.e., flowtime, the
sum of travel times of all robots) without significant compromise. SI-CPP also
decreases the makespan up to 45%. SI-CCBS decreases the flowtime by 9% compared
to the competitor, albeit exhibiting a 14% lower success rate.
\\ ( https://arxiv.org/abs/2404.01752 ,  22600kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01754 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:12:21 GMT   (2364kb,D)

Title: Peer-aided Repairer: Empowering Large Language Models to Repair Advanced
  Student Assignments
Authors: Qianhui Zhao, Fang Liu, Li Zhang, Yang Liu, Zhen Yan, Zhenghao Chen,
  Yufei Zhou, Jing Jiang, Ge Li
Categories: cs.SE cs.AI
Comments: On-going work
\\
  Automated generation of feedback on programming assignments holds significant
benefits for programming education, especially when it comes to advanced
assignments. Automated Program Repair techniques, especially Large Language
Model based approaches, have gained notable recognition for their potential to
fix introductory assignments. However, the programs used for evaluation are
relatively simple. It remains unclear how existing approaches perform in
repairing programs from higher-level programming courses. To address these
limitations, we curate a new advanced student assignment dataset named
Defects4DS from a higher-level programming course. Subsequently, we identify
the challenges related to fixing bugs in advanced assignments. Based on the
analysis, we develop a framework called PaR that is powered by the LLM. PaR
works in three phases: Peer Solution Selection, Multi-Source Prompt Generation,
and Program Repair. Peer Solution Selection identifies the closely related peer
programs based on lexical, semantic, and syntactic criteria. Then Multi-Source
Prompt Generation adeptly combines multiple sources of information to create a
comprehensive and informative prompt for the last Program Repair stage. The
evaluation on Defects4DS and another well-investigated ITSP dataset reveals
that PaR achieves a new state-of-the-art performance, demonstrating impressive
improvements of 19.94% and 15.2% in repair rate compared to prior
state-of-the-art LLM- and symbolic-based approaches, respectively
\\ ( https://arxiv.org/abs/2404.01754 ,  2364kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01775 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:40:22 GMT   (21802kb,AD)

Title: A noisy elephant in the room: Is your out-of-distribution detector
  robust to label noise?
Authors: Galadrielle Humblot-Renaux and Sergio Escalera and Thomas B. Moeslund
Categories: cs.CV cs.AI cs.LG
Comments: Accepted at CVPR 2024
\\
  The ability to detect unfamiliar or unexpected images is essential for safe
deployment of computer vision systems. In the context of classification, the
task of detecting images outside of a model's training domain is known as
out-of-distribution (OOD) detection. While there has been a growing research
interest in developing post-hoc OOD detection methods, there has been
comparably little discussion around how these methods perform when the
underlying classifier is not trained on a clean, carefully curated dataset. In
this work, we take a closer look at 20 state-of-the-art OOD detection methods
in the (more realistic) scenario where the labels used to train the underlying
classifier are unreliable (e.g. crowd-sourced or web-scraped labels). Extensive
experiments across different datasets, noise types & levels, architectures and
checkpointing strategies provide insights into the effect of class label noise
on OOD detection, and show that poor separation between incorrectly classified
ID samples vs. OOD samples is an overlooked yet important limitation of
existing methods. Code: https://github.com/glhr/ood-labelnoise
\\ ( https://arxiv.org/abs/2404.01775 ,  21802kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01812 (*cross-listing*)
Date: Tue, 2 Apr 2024 10:15:06 GMT   (977kb,D)

Title: Uncertainty-aware Active Learning of NeRF-based Object Models for Robot
  Manipulators using Visual and Re-orientation Actions
Authors: Saptarshi Dasgupta, Akshat Gupta, Shreshth Tuli, Rohan Paul
Categories: cs.RO cs.AI
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  Manipulating unseen objects is challenging without a 3D representation, as
objects generally have occluded surfaces. This requires physical interaction
with objects to build their internal representations. This paper presents an
approach that enables a robot to rapidly learn the complete 3D model of a given
object for manipulation in unfamiliar orientations. We use an ensemble of
partially constructed NeRF models to quantify model uncertainty to determine
the next action (a visual or re-orientation action) by optimizing
informativeness and feasibility. Further, our approach determines when and how
to grasp and re-orient an object given its partial NeRF model and re-estimates
the object pose to rectify misalignments introduced during the interaction.
Experiments with a simulated Franka Emika Robot Manipulator operating in a
tabletop environment with benchmark objects demonstrate an improvement of (i)
14% in visual reconstruction quality (PSNR), (ii) 20% in the geometric/depth
reconstruction of the object surface (F-score) and (iii) 71% in the task
success rate of manipulating objects a-priori unseen orientations/stable
configurations in the scene; over current methods. The project page can be
found here: https://actnerf.github.io.
\\ ( https://arxiv.org/abs/2404.01812 ,  977kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01833 (*cross-listing*)
Date: Tue, 2 Apr 2024 10:45:49 GMT   (2771kb,D)

Title: Great, Now Write an Article About That: The Crescendo Multi-Turn LLM
  Jailbreak Attack
Authors: Mark Russinovich and Ahmed Salem and Ronen Eldan
Categories: cs.CR cs.AI
\\
  Large Language Models (LLMs) have risen significantly in popularity and are
increasingly being adopted across multiple applications. These LLMs are heavily
aligned to resist engaging in illegal or unethical topics as a means to avoid
contributing to responsible AI harms. However, a recent line of attacks, known
as "jailbreaks", seek to overcome this alignment. Intuitively, jailbreak
attacks aim to narrow the gap between what the model can do and what it is
willing to do. In this paper, we introduce a novel jailbreak attack called
Crescendo. Unlike existing jailbreak methods, Crescendo is a multi-turn
jailbreak that interacts with the model in a seemingly benign manner. It begins
with a general prompt or question about the task at hand and then gradually
escalates the dialogue by referencing the model's replies, progressively
leading to a successful jailbreak. We evaluate Crescendo on various public
systems, including ChatGPT, Gemini Pro, Gemini-Ultra, LlaMA-2 70b Chat, and
Anthropic Chat. Our results demonstrate the strong efficacy of Crescendo, with
it achieving high attack success rates across all evaluated models and tasks.
Furthermore, we introduce Crescendomation, a tool that automates the Crescendo
attack, and our evaluation showcases its effectiveness against state-of-the-art
models.
\\ ( https://arxiv.org/abs/2404.01833 ,  2771kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01849 (*cross-listing*)
Date: Tue, 2 Apr 2024 11:22:53 GMT   (2690kb,D)

Title: EV2Gym: A Flexible V2G Simulator for EV Smart Charging Research and
  Benchmarking
Authors: Stavros Orfanoudakis, Cesar Diaz-Londono, Yunus E. Y{\i}lmaz, Peter
  Palensky, Pedro P. Vergara
Categories: cs.SE cs.AI
Comments: 10 pages, 9 figures, and 6 tables
\\
  As electric vehicle (EV) numbers rise, concerns about the capacity of current
charging and power grid infrastructure grow, necessitating the development of
smart charging solutions. While many smart charging simulators have been
developed in recent years, only a few support the development of Reinforcement
Learning (RL) algorithms in the form of a Gym environment, and those that do
usually lack depth in modeling Vehicle-to-Grid (V2G) scenarios. To address the
aforementioned issues, this paper introduces the EV2Gym, a realistic simulator
platform for the development and assessment of small and large-scale smart
charging algorithms within a standardized platform. The proposed simulator is
populated with comprehensive EV, charging station, power transformer, and EV
behavior models validated using real data. EV2Gym has a highly customizable
interface empowering users to choose from pre-designed case studies or craft
their own customized scenarios to suit their specific requirements. Moreover,
it incorporates a diverse array of RL, mathematical programming, and heuristic
algorithms to speed up the development and benchmarking of new solutions. By
offering a unified and standardized platform, EV2Gym aims to provide
researchers and practitioners with a robust environment for advancing and
assessing smart charging algorithms.
\\ ( https://arxiv.org/abs/2404.01849 ,  2690kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01855 (*cross-listing*)
Date: Tue, 2 Apr 2024 11:33:04 GMT   (456kb,D)

Title: Where to Move Next: Zero-shot Generalization of LLMs for Next POI
  Recommendation
Authors: Shanshan Feng, Haoming Lyu, Caishun Chen, Yew-Soon Ong
Categories: cs.IR cs.AI
\\
  Next Point-of-interest (POI) recommendation provides valuable suggestions for
users to explore their surrounding environment. Existing studies rely on
building recommendation models from large-scale users' check-in data, which is
task-specific and needs extensive computational resources. Recently, the
pretrained large language models (LLMs) have achieved significant advancements
in various NLP tasks and have also been investigated for recommendation
scenarios. However, the generalization abilities of LLMs still are unexplored
to address the next POI recommendations, where users' geographical movement
patterns should be extracted. Although there are studies that leverage LLMs for
next-item recommendations, they fail to consider the geographical influence and
sequential transitions. Hence, they cannot effectively solve the next POI
recommendation task. To this end, we design novel prompting strategies and
conduct empirical studies to assess the capability of LLMs, e.g., ChatGPT, for
predicting a user's next check-in. Specifically, we consider several essential
factors in human movement behaviors, including user geographical preference,
spatial distance, and sequential transitions, and formulate the recommendation
task as a ranking problem. Through extensive experiments on two widely used
real-world datasets, we derive several key findings. Empirical evaluations
demonstrate that LLMs have promising zero-shot recommendation abilities and can
provide accurate and reasonable predictions. We also reveal that LLMs cannot
accurately comprehend geographical context information and are sensitive to the
order of presentation of candidate POIs, which shows the limitations of LLMs
and necessitates further research on robust human mobility reasoning
mechanisms.
\\ ( https://arxiv.org/abs/2404.01855 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01878 (*cross-listing*)
Date: Tue, 2 Apr 2024 12:08:26 GMT   (3495kb,D)

Title: Real, fake and synthetic faces - does the coin have three sides?
Authors: Shahzeb Naeem, Ramzi Al-Sharawi, Muhammad Riyyan Khan, Usman Tariq,
  Abhinav Dhall and Hasan Al-Nashash
Categories: cs.CV cs.AI
\\
  With the ever-growing power of generative artificial intelligence, deepfake
and artificially generated (synthetic) media have continued to spread online,
which creates various ethical and moral concerns regarding their usage. To
tackle this, we thus present a novel exploration of the trends and patterns
observed in real, deepfake and synthetic facial images. The proposed analysis
is done in two parts: firstly, we incorporate eight deep learning models and
analyze their performances in distinguishing between the three classes of
images. Next, we look to further delve into the similarities and differences
between these three sets of images by investigating their image properties both
in the context of the entire image as well as in the context of specific
regions within the image. ANOVA test was also performed and provided further
clarity amongst the patterns associated between the images of the three
classes. From our findings, we observe that the investigated deeplearning
models found it easier to detect synthetic facial images, with the ViT Patch-16
model performing best on this task with a class-averaged sensitivity,
specificity, precision, and accuracy of 97.37%, 98.69%, 97.48%, and 98.25%,
respectively. This observation was supported by further analysis of various
image properties. We saw noticeable differences across the three category of
images. This analysis can help us build better algorithms for facial image
generation, and also shows that synthetic, deepfake and real face images are
indeed three different classes.
\\ ( https://arxiv.org/abs/2404.01878 ,  3495kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01889 (*cross-listing*)
Date: Tue, 2 Apr 2024 12:28:40 GMT   (48263kb,D)

Title: RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image
  Enhancement
Authors: Tatiana Gaintseva, Marting Benning, Gregory Slabaugh
Categories: cs.CV cs.AI
\\
  In this paper we propose a novel modification of Contrastive Language-Image
Pre-Training (CLIP) guidance for the task of unsupervised backlit image
enhancement. Our work builds on the state-of-the-art CLIP-LIT approach, which
learns a prompt pair by constraining the text-image similarity between a prompt
(negative/positive sample) and a corresponding image (backlit image/well-lit
image) in the CLIP embedding space. Learned prompts then guide an image
enhancement network. Based on the CLIP-LIT framework, we propose two novel
methods for CLIP guidance. First, we show that instead of tuning prompts in the
space of text embeddings, it is possible to directly tune their embeddings in
the latent space without any loss in quality. This accelerates training and
potentially enables the use of additional encoders that do not have a text
encoder. Second, we propose a novel approach that does not require any prompt
tuning. Instead, based on CLIP embeddings of backlit and well-lit images from
training data, we compute the residual vector in the embedding space as a
simple difference between the mean embeddings of the well-lit and backlit
images. This vector then guides the enhancement network during training,
pushing a backlit image towards the space of well-lit images. This approach
further dramatically reduces training time, stabilizes training and produces
high quality enhanced images without artifacts, both in supervised and
unsupervised training regimes. Additionally, we show that residual vectors can
be interpreted, revealing biases in training data, and thereby enabling
potential bias correction.
\\ ( https://arxiv.org/abs/2404.01889 ,  48263kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01897 (*cross-listing*)
Date: Tue, 2 Apr 2024 12:36:40 GMT   (533kb,D)

Title: Continuous Spiking Graph Neural Networks
Authors: Nan Yin, Mengzhu Wan, Li Shen, Hitesh Laxmichand Patel, Baopu Li, Bin
  Gu, Huan Xiong
Categories: cs.NE cs.AI cs.LG
\\
  Continuous graph neural networks (CGNNs) have garnered significant attention
due to their ability to generalize existing discrete graph neural networks
(GNNs) by introducing continuous dynamics. They typically draw inspiration from
diffusion-based methods to introduce a novel propagation scheme, which is
analyzed using ordinary differential equations (ODE). However, the
implementation of CGNNs requires significant computational power, making them
challenging to deploy on battery-powered devices. Inspired by recent spiking
neural networks (SNNs), which emulate a biological inference process and
provide an energy-efficient neural architecture, we incorporate the SNNs with
CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks
(COS-GNN). We employ SNNs for graph node representation at each time step,
which are further integrated into the ODE process along with time. To enhance
information preservation and mitigate information loss in SNNs, we introduce
the high-order structure of COS-GNN, which utilizes the second-order ODE for
spiking representation and continuous propagation. Moreover, we provide the
theoretical proof that COS-GNN effectively mitigates the issues of exploding
and vanishing gradients, enabling us to capture long-range dependencies between
nodes. Experimental results on graph-based learning tasks demonstrate the
effectiveness of the proposed COS-GNN over competitive baselines.
\\ ( https://arxiv.org/abs/2404.01897 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01925 (*cross-listing*)
Date: Tue, 2 Apr 2024 13:19:45 GMT   (2733kb,D)

Title: Improving Bird's Eye View Semantic Segmentation by Task Decomposition
Authors: Tianhao Zhao, Yongcan Chen, Yu Wu, Tianyang Liu, Bo Du, Peilun Xiao,
  Shi Qiu, Hongda Yang, Guozhen Li, Yi Yang, Yutian Lin
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024
\\
  Semantic segmentation in bird's eye view (BEV) plays a crucial role in
autonomous driving. Previous methods usually follow an end-to-end pipeline,
directly predicting the BEV segmentation map from monocular RGB inputs.
However, the challenge arises when the RGB inputs and BEV targets from distinct
perspectives, making the direct point-to-point predicting hard to optimize. In
this paper, we decompose the original BEV segmentation task into two stages,
namely BEV map reconstruction and RGB-BEV feature alignment. In the first
stage, we train a BEV autoencoder to reconstruct the BEV segmentation maps
given corrupted noisy latent representation, which urges the decoder to learn
fundamental knowledge of typical BEV patterns. The second stage involves
mapping RGB input images into the BEV latent space of the first stage, directly
optimizing the correlations between the two views at the feature level. Our
approach simplifies the complexity of combining perception and generation into
distinct steps, equipping the model to handle intricate and challenging scenes
effectively. Besides, we propose to transform the BEV segmentation map from the
Cartesian to the polar coordinate system to establish the column-wise
correspondence between RGB images and BEV maps. Moreover, our method requires
neither multi-scale features nor camera intrinsic parameters for depth
estimation and saves computational overhead. Extensive experiments on nuScenes
and Argoverse show the effectiveness and efficiency of our method. Code is
available at https://github.com/happytianhao/TaDe.
\\ ( https://arxiv.org/abs/2404.01925 ,  2733kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01976 (*cross-listing*)
Date: Tue, 2 Apr 2024 14:16:59 GMT   (20003kb,D)

Title: Joint-Task Regularization for Partially Labeled Multi-Task Learning
Authors: Kento Nishi, Junsik Kim, Wanhua Li, Hanspeter Pfister
Categories: cs.CV cs.AI cs.LG
Comments: Accepted paper to CVPR 2024 (main conference)
\\
  Multi-task learning has become increasingly popular in the machine learning
field, but its practicality is hindered by the need for large, labeled
datasets. Most multi-task learning methods depend on fully labeled datasets
wherein each input example is accompanied by ground-truth labels for all target
tasks. Unfortunately, curating such datasets can be prohibitively expensive and
impractical, especially for dense prediction tasks which require per-pixel
labels for each image. With this in mind, we propose Joint-Task Regularization
(JTR), an intuitive technique which leverages cross-task relations to
simultaneously regularize all tasks in a single joint-task latent space to
improve learning when data is not fully labeled for all tasks. JTR stands out
from existing approaches in that it regularizes all tasks jointly rather than
separately in pairs -- therefore, it achieves linear complexity relative to the
number of tasks while previous methods scale quadratically. To demonstrate the
validity of our approach, we extensively benchmark our method across a wide
variety of partially labeled scenarios based on NYU-v2, Cityscapes, and
Taskonomy.
\\ ( https://arxiv.org/abs/2404.01976 ,  20003kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01986 (*cross-listing*)
Date: Tue, 2 Apr 2024 14:22:54 GMT   (5012kb,D)

Title: Predicting the Intention to Interact with a Service Robot:the Role of
  Gaze Cues
Authors: Simone Arreghini, Gabriele Abbate, Alessandro Giusti, Antonio Paolillo
Categories: cs.RO cs.AI cs.LG
\\
  For a service robot, it is crucial to perceive as early as possible that an
approaching person intends to interact: in this case, it can proactively enact
friendly behaviors that lead to an improved user experience. We solve this
perception task with a sequence-to-sequence classifier of a potential user
intention to interact, which can be trained in a self-supervised way. Our main
contribution is a study of the benefit of features representing the person's
gaze in this context. Extensive experiments on a novel dataset show that the
inclusion of gaze cues significantly improves the classifier performance (AUROC
increases from 84.5% to 91.2%); the distance at which an accurate
classification can be achieved improves from 2.4 m to 3.2 m. We also quantify
the system's ability to adapt to new environments without external supervision.
Qualitative experiments show practical applications with a waiter robot.
\\ ( https://arxiv.org/abs/2404.01986 ,  5012kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02018 (*cross-listing*)
Date: Tue, 2 Apr 2024 15:08:35 GMT   (1420kb,D)

Title: Large Language Models for Orchestrating Bimanual Robots
Authors: Kun Chu, Xufeng Zhao, Cornelius Weber, Mengdi Li, Wenhao Lu, and
  Stefan Wermter
Categories: cs.RO cs.AI
Comments: The project website can be found at http://labor-agent.github.io
\\
  Although there has been rapid progress in endowing robots with the ability to
solve complex manipulation tasks, generating control policies for bimanual
robots to solve tasks involving two hands is still challenging because of the
difficulties in effective temporal and spatial coordination. With emergent
abilities in terms of step-by-step reasoning and in-context learning, Large
Language Models (LLMs) have taken control of a variety of robotic tasks.
However, the nature of language communication via a single sequence of discrete
symbols makes LLM-based coordination in continuous space a particular challenge
for bimanual tasks. To tackle this challenge for the first time by an LLM, we
present LAnguage-model-based Bimanual ORchestration (LABOR), an agent utilizing
an LLM to analyze task configurations and devise coordination control policies
for addressing long-horizon bimanual tasks. In the simulated environment, the
LABOR agent is evaluated through several everyday tasks on the NICOL humanoid
robot. Reported success rates indicate that overall coordination efficiency is
close to optimal performance, while the analysis of failure causes, classified
into spatial and temporal coordination and skill selection, shows that these
vary over tasks. The project website can be found at
http://labor-agent.github.io
\\ ( https://arxiv.org/abs/2404.02018 ,  1420kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02062 (*cross-listing*)
Date: Tue, 2 Apr 2024 16:01:18 GMT   (286kb,D)

Title: Digital Forgetting in Large Language Models: A Survey of Unlearning
  Methods
Authors: Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David
  S\'anchez, Josep Domingo-Ferrer, Guillem Collell, Kuan Eeik Tan
Categories: cs.CR cs.AI cs.LG
Comments: 70 pages
MSC-class: 68
ACM-class: K.4.1; I.2.6; I.2.7
\\
  The objective of digital forgetting is, given a model with undesirable
knowledge or behavior, obtain a new model where the detected issues are no
longer present. The motivations for forgetting include privacy protection,
copyright protection, elimination of biases and discrimination, and prevention
of harmful content generation. Effective digital forgetting has to be effective
(meaning how well the new model has forgotten the undesired
knowledge/behavior), retain the performance of the original model on the
desirable tasks, and be scalable (in particular forgetting has to be more
efficient than retraining from scratch on just the tasks/data to be retained).
This survey focuses on forgetting in large language models (LLMs). We first
provide background on LLMs, including their components, the types of LLMs, and
their usual training pipeline. Second, we describe the motivations, types, and
desired properties of digital forgetting. Third, we introduce the approaches to
digital forgetting in LLMs, among which unlearning methodologies stand out as
the state of the art. Fourth, we provide a detailed taxonomy of machine
unlearning methods for LLMs, and we survey and compare current approaches.
Fifth, we detail datasets, models and metrics used for the evaluation of
forgetting, retaining and runtime. Sixth, we discuss challenges in the area.
Finally, we provide some concluding remarks.
\\ ( https://arxiv.org/abs/2404.02062 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02063 (*cross-listing*)
Date: Tue, 2 Apr 2024 16:04:31 GMT   (6624kb,D)

Title: SPMamba: State-space model is all you need in speech separation
Authors: Kai Li, Guo Chen
Categories: cs.SD cs.AI eess.AS
Comments: Technical Report. Work in progress. Code is available at
  https://github.com/JusperLee/SPMamba
\\
  In speech separation, both CNN- and Transformer-based models have
demonstrated robust separation capabilities, garnering significant attention
within the research community. However, CNN-based methods have limited
modelling capability for long-sequence audio, leading to suboptimal separation
performance. Conversely, Transformer-based methods are limited in practical
applications due to their high computational complexity. Notably, within
computer vision, Mamba-based methods have been celebrated for their formidable
performance and reduced computational requirements. In this paper, we propose a
network architecture for speech separation using a state-space model, namely
SPMamba. We adopt the TF-GridNet model as the foundational framework and
substitute its Transformer component with a bidirectional Mamba module, aiming
to capture a broader range of contextual information. Our experimental results
reveal an important role in the performance aspects of Mamba-based models.
SPMamba demonstrates superior performance with a significant advantage over
existing separation models in a dataset built on Librispeech. Notably, SPMamba
achieves a substantial improvement in separation quality, with a 2.42 dB
enhancement in SI-SNRi compared to the TF-GridNet. The source code for SPMamba
is publicly accessible at https://github.com/JusperLee/SPMamba .
\\ ( https://arxiv.org/abs/2404.02063 ,  6624kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02067 (*cross-listing*)
Date: Tue, 2 Apr 2024 16:07:50 GMT   (2477kb,D)

Title: Red-Teaming Segment Anything Model
Authors: Krzysztof Jankowski, Bartlomiej Sobieski, Mateusz Kwiatkowski, Jakub
  Szulc, Michal Janik, Hubert Baniecki, Przemyslaw Biecek
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024 - The 4th Workshop of Adversarial Machine Learning on
  Computer Vision: Robustness of Foundation Models
\\
  Foundation models have emerged as pivotal tools, tackling many complex tasks
through pre-training on vast datasets and subsequent fine-tuning for specific
applications. The Segment Anything Model is one of the first and most
well-known foundation models for computer vision segmentation tasks. This work
presents a multi-faceted red-teaming analysis that tests the Segment Anything
Model against challenging tasks: (1) We analyze the impact of style transfer on
segmentation masks, demonstrating that applying adverse weather conditions and
raindrops to dashboard images of city roads significantly distorts generated
masks. (2) We focus on assessing whether the model can be used for attacks on
privacy, such as recognizing celebrities' faces, and show that the model
possesses some undesired knowledge in this task. (3) Finally, we check how
robust the model is to adversarial attacks on segmentation masks under text
prompts. We not only show the effectiveness of popular white-box attacks and
resistance to black-box attacks but also introduce a novel approach - Focused
Iterative Gradient Attack (FIGA) that combines white-box approaches to
construct an efficient attack resulting in a smaller number of modified pixels.
All of our testing methods and analyses indicate a need for enhanced safety
measures in foundation models for image segmentation.
\\ ( https://arxiv.org/abs/2404.02067 ,  2477kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02090 (*cross-listing*)
Date: Tue, 2 Apr 2024 16:35:52 GMT   (24kb)

Title: Already Moderate Population Sizes Provably Yield Strong Robustness to
  Noise
Authors: Denis Antipov, Benjamin Doerr, Alexandra Ivanova
Categories: cs.NE cs.AI
Comments: Full version of the same-titled paper accepted at GECCO 2024
\\
  Experience shows that typical evolutionary algorithms can cope well with
stochastic disturbances such as noisy function evaluations.
  In this first mathematical runtime analysis of the $(1+\lambda)$ and
$(1,\lambda)$ evolutionary algorithms in the presence of prior bit-wise noise,
we show that both algorithms can tolerate constant noise probabilities without
increasing the asymptotic runtime on the OneMax benchmark. For this, a
population size $\lambda$ suffices that is at least logarithmic in the problem
size $n$. The only previous result in this direction regarded the less
realistic one-bit noise model, required a population size super-linear in the
problem size, and proved a runtime guarantee roughly cubic in the noiseless
runtime for the OneMax benchmark. Our significantly stronger results are based
on the novel proof argument that the noiseless offspring can be seen as a
biased uniform crossover between the parent and the noisy offspring. We are
optimistic that the technical lemmas resulting from this insight will find
applications also in future mathematical runtime analyses of evolutionary
algorithms.
\\ ( https://arxiv.org/abs/2404.02090 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02151 (*cross-listing*)
Date: Tue, 2 Apr 2024 17:58:27 GMT   (2233kb,D)

Title: Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
Authors: Maksym Andriushchenko and Francesco Croce and Nicolas Flammarion
Categories: cs.CR cs.AI cs.LG stat.ML
\\
  We show that even the most recent safety-aligned LLMs are not robust to
simple adaptive jailbreaking attacks. First, we demonstrate how to successfully
leverage access to logprobs for jailbreaking: we initially design an
adversarial prompt template (sometimes adapted to the target LLM), and then we
apply random search on a suffix to maximize the target logprob (e.g., of the
token "Sure"), potentially with multiple restarts. In this way, we achieve
nearly 100\% attack success rate -- according to GPT-4 as a judge -- on
GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was
adversarially trained against the GCG attack. We also show how to jailbreak all
Claude models -- that do not expose logprobs -- via either a transfer or
prefilling attack with 100\% success rate. In addition, we show how to use
random search on a restricted set of tokens for finding trojan strings in
poisoned models -- a task that shares many similarities with jailbreaking --
which is the algorithm that brought us the first place in the SaTML'24 Trojan
Detection Competition. The common theme behind these attacks is that adaptivity
is crucial: different models are vulnerable to different prompting templates
(e.g., R2D2 is very sensitive to in-context learning prompts), some models have
unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and
in some settings it is crucial to restrict the token search space based on
prior knowledge (e.g., for trojan detection). We provide the code, prompts, and
logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.
\\ ( https://arxiv.org/abs/2404.02151 ,  2233kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02157 (*cross-listing*)
Date: Tue, 2 Apr 2024 17:59:10 GMT   (22406kb,D)

Title: Segment Any 3D Object with Language
Authors: Seungjun Lee, Yuyang Zhao, Gim Hee Lee
Categories: cs.CV cs.AI
Comments: Project Page: https://cvrp-sole.github.io
\\
  In this paper, we investigate Open-Vocabulary 3D Instance Segmentation
(OV-3DIS) with free-form language instructions. Earlier works that rely on only
annotated base categories for training suffer from limited generalization to
unseen novel categories. Recent works mitigate poor generalizability to novel
categories by generating class-agnostic masks or projecting generalized masks
from 2D to 3D, but disregard semantic or geometry information, leading to
sub-optimal performance. Instead, generating generalizable but semantic-related
masks directly from 3D point clouds would result in superior outcomes. In this
paper, we introduce Segment any 3D Object with LanguagE (SOLE), which is a
semantic and geometric-aware visual-language learning framework with strong
generalizability by generating semantic-related masks directly from 3D point
clouds. Specifically, we propose a multimodal fusion network to incorporate
multimodal semantics in both backbone and decoder. In addition, to align the 3D
segmentation model with various language instructions and enhance the mask
quality, we introduce three types of multimodal associations as supervision.
Our SOLE outperforms previous methods by a large margin on ScanNetv2,
ScanNet200, and Replica benchmarks, and the results are even close to the
fully-supervised counterpart despite the absence of class annotations in the
training. Furthermore, extensive qualitative results demonstrate the
versatility of our SOLE to language instructions.
\\ ( https://arxiv.org/abs/2404.02157 ,  22406kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01737 (*cross-listing*)
Date: Tue, 2 Apr 2024 08:53:51 GMT   (534kb,D)

Title: Transfer Learning from Whisper for Microscopic Intelligibility
  Prediction
Authors: Paul Best, Santiago Cuervo, Ricard Marxer
Categories: eess.AS cs.CL cs.SD
\\
  Macroscopic intelligibility models predict the expected human word-error-rate
for a given speech-in-noise stimulus. In contrast, microscopic intelligibility
models aim to make fine-grained predictions about listeners' perception, e.g.
predicting phonetic or lexical responses. State-of-the-art macroscopic models
use transfer learning from large scale deep learning models for speech
processing, whereas such methods have rarely been used for microscopic
modeling. In this paper, we study the use of transfer learning from Whisper, a
state-of-the-art deep learning model for automatic speech recognition, for
microscopic intelligibility prediction at the level of lexical responses. Our
method outperforms the considered baselines, even in a zero-shot setup, and
yields a relative improvement of up to 66\% when fine-tuned to predict
listeners' responses. Our results showcase the promise of large scale deep
learning based methods for microscopic intelligibility prediction.
\\ ( https://arxiv.org/abs/2404.01737 ,  534kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01800 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:59:49 GMT   (199kb,D)

Title: Sentiment Analysis of Citations in Scientific Articles Using ChatGPT:
  Identifying Potential Biases and Conflicts of Interest
Authors: Walid Hariri
Categories: cs.DL cs.CL
\\
  Scientific articles play a crucial role in advancing knowledge and informing
research directions. One key aspect of evaluating scientific articles is the
analysis of citations, which provides insights into the impact and reception of
the cited works. This article introduces the innovative use of large language
models, particularly ChatGPT, for comprehensive sentiment analysis of citations
within scientific articles. By leveraging advanced natural language processing
(NLP) techniques, ChatGPT can discern the nuanced positivity or negativity of
citations, offering insights into the reception and impact of cited works.
Furthermore, ChatGPT's capabilities extend to detecting potential biases and
conflicts of interest in citations, enhancing the objectivity and reliability
of scientific literature evaluation. This study showcases the transformative
potential of artificial intelligence (AI)-powered tools in enhancing citation
analysis and promoting integrity in scholarly research.
\\ ( https://arxiv.org/abs/2404.01800 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01994 (*cross-listing*)
Date: Tue, 2 Apr 2024 14:40:04 GMT   (19807kb,D)

Title: DELAN: Dual-Level Alignment for Vision-and-Language Navigation by
  Cross-Modal Contrastive Learning
Authors: Mengfei Du, Binhao Wu, Jiwen Zhang, Zhihao Fan, Zejun Li, Ruipu Luo,
  Xuanjing Huang, Zhongyu Wei
Categories: cs.CV cs.CL cs.LG
Comments: Accepted by LREC-COLING 2024
\\
  Vision-and-Language navigation (VLN) requires an agent to navigate in unseen
environment by following natural language instruction. For task completion, the
agent needs to align and integrate various navigation modalities, including
instruction, observation and navigation history. Existing works primarily
concentrate on cross-modal attention at the fusion stage to achieve this
objective. Nevertheless, modality features generated by disparate uni-encoders
reside in their own spaces, leading to a decline in the quality of cross-modal
fusion and decision. To address this problem, we propose a Dual-levEL AligNment
(DELAN) framework by cross-modal contrastive learning. This framework is
designed to align various navigation-related modalities before fusion, thereby
enhancing cross-modal interaction and action decision-making. Specifically, we
divide the pre-fusion alignment into dual levels: instruction-history level and
landmark-observation level according to their semantic correlations. We also
reconstruct a dual-level instruction for adaptation to the dual-level
alignment. As the training signals for pre-fusion alignment are extremely
limited, self-supervised contrastive learning strategies are employed to
enforce the matching between different modalities. Our approach seamlessly
integrates with the majority of existing models, resulting in improved
navigation performance on various VLN benchmarks, including R2R, R4R, RxR and
CVDN.
\\ ( https://arxiv.org/abs/2404.01994 ,  19807kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02138 (*cross-listing*)
Date: Tue, 2 Apr 2024 17:49:40 GMT   (1661kb,D)

Title: Topic-based Watermarks for LLM-Generated Text
Authors: Alexander Nemecek, Yuzhou Jiang, and Erman Ayday
Categories: cs.CR cs.CL cs.LG
\\
  Recent advancements of large language models (LLMs) have resulted in
indistinguishable text outputs comparable to human-generated text. Watermarking
algorithms are potential tools that offer a way to differentiate between LLM-
and human-generated text by embedding detectable signatures within
LLM-generated output. However, current watermarking schemes lack robustness
against known attacks against watermarking algorithms. In addition, they are
impractical considering an LLM generates tens of thousands of text outputs per
day and the watermarking algorithm needs to memorize each output it generates
for the detection to work. In this work, focusing on the limitations of current
watermarking schemes, we propose the concept of a "topic-based watermarking
algorithm" for LLMs. The proposed algorithm determines how to generate tokens
for the watermarked LLM output based on extracted topics of an input prompt or
the output of a non-watermarked LLM. Inspired from previous work, we propose
using a pair of lists (that are generated based on the specified extracted
topic(s)) that specify certain tokens to be included or excluded while
generating the watermarked output of the LLM. Using the proposed watermarking
algorithm, we show the practicality of a watermark detection algorithm.
Furthermore, we discuss a wide range of attacks that can emerge against
watermarking algorithms for LLMs and the benefit of the proposed watermarking
scheme for the feasibility of modeling a potential attacker considering its
benefit vs. loss.
\\ ( https://arxiv.org/abs/2404.02138 ,  1661kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01318 (*cross-listing*)
Date: Thu, 28 Mar 2024 02:44:02 GMT   (1333kb,D)

Title: JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large
  Language Models
Authors: Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym
  Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas
  Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, Eric Wong
Categories: cs.CR cs.LG
\\
  Jailbreak attacks cause large language models (LLMs) to generate harmful,
unethical, or otherwise objectionable content. Evaluating these attacks
presents a number of challenges, which the current collection of benchmarks and
evaluation techniques do not adequately address. First, there is no clear
standard of practice regarding jailbreaking evaluation. Second, existing works
compute costs and success rates in incomparable ways. And third, numerous works
are not reproducible, as they withhold adversarial prompts, involve
closed-source code, or rely on evolving proprietary APIs. To address these
challenges, we introduce JailbreakBench, an open-sourced benchmark with the
following components: (1) a new jailbreaking dataset containing 100 unique
behaviors, which we call JBB-Behaviors; (2) an evolving repository of
state-of-the-art adversarial prompts, which we refer to as jailbreak artifacts;
(3) a standardized evaluation framework that includes a clearly defined threat
model, system prompts, chat templates, and scoring functions; and (4) a
leaderboard that tracks the performance of attacks and defenses for various
LLMs. We have carefully considered the potential ethical implications of
releasing this benchmark, and believe that it will be a net positive for the
community. Over time, we will expand and adapt the benchmark to reflect
technical and methodological advances in the research community.
\\ ( https://arxiv.org/abs/2404.01318 ,  1333kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01330 (*cross-listing*)
Date: Fri, 29 Mar 2024 15:27:28 GMT   (2036kb,D)

Title: Holo-VQVAE: VQ-VAE for phase-only holograms
Authors: Joohyun Park, Hyeongyeop Kang
Categories: cs.CV cs.GR cs.LG eess.IV
\\
  Holography stands at the forefront of visual technology innovation, offering
immersive, three-dimensional visualizations through the manipulation of light
wave amplitude and phase. Contemporary research in hologram generation has
predominantly focused on image-to-hologram conversion, producing holograms from
existing images. These approaches, while effective, inherently limit the scope
of innovation and creativity in hologram generation. In response to this
limitation, we present Holo-VQVAE, a novel generative framework tailored for
phase-only holograms (POHs). Holo-VQVAE leverages the architecture of Vector
Quantized Variational AutoEncoders, enabling it to learn the complex
distributions of POHs. Furthermore, it integrates the Angular Spectrum Method
into the training process, facilitating learning in the image domain. This
framework allows for the generation of unseen, diverse holographic content
directly from its intricately learned latent space without requiring
pre-existing images. This pioneering work paves the way for groundbreaking
applications and methodologies in holographic content creation, opening a new
era in the exploration of holographic content.
\\ ( https://arxiv.org/abs/2404.01330 ,  2036kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01360 (*cross-listing*)
Date: Mon, 1 Apr 2024 11:42:43 GMT   (1867kb,D)

Title: Harnessing Data and Physics for Deep Learning Phase Recovery
Authors: Kaiqiang Wang, Edmund Y. Lam
Categories: eess.IV cs.LG physics.optics
Comments: 26 pages, 10 figures
\\
  Phase recovery, calculating the phase of a light wave from its intensity
measurements, is essential for various applications, such as coherent
diffraction imaging, adaptive optics, and biomedical imaging. It enables the
reconstruction of an object's refractive index distribution or topography as
well as the correction of imaging system aberrations. In recent years, deep
learning has been proven to be highly effective in addressing phase recovery
problems. Two main deep learning phase recovery strategies are data-driven (DD)
with supervised learning mode and physics-driven (PD) with self-supervised
learning mode. DD and PD achieve the same goal in different ways and lack the
necessary study to reveal similarities and differences. Therefore, in this
paper, we comprehensively compare these two deep learning phase recovery
strategies in terms of time consumption, accuracy, generalization ability,
ill-posedness adaptability, and prior capacity. What's more, we propose a
co-driven (CD) strategy of combining datasets and physics for the balance of
high- and low-frequency information. The codes for DD, PD, and CD are publicly
available at https://github.com/kqwang/DLPR.
\\ ( https://arxiv.org/abs/2404.01360 ,  1867kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01367 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:59:48 GMT   (47498kb,D)

Title: Bigger is not Always Better: Scaling Properties of Latent Diffusion
  Models
Authors: Kangfu Mei and Zhengzhong Tu and Mauricio Delbracio and Hossein Talebi
  and Vishal M. Patel and Peyman Milanfar
Categories: cs.CV cs.LG
\\
  We study the scaling properties of latent diffusion models (LDMs) with an
emphasis on their sampling efficiency. While improved network architecture and
inference algorithms have shown to effectively boost sampling efficiency of
diffusion models, the role of model size -- a critical determinant of sampling
efficiency -- has not been thoroughly examined. Through empirical analysis of
established text-to-image diffusion models, we conduct an in-depth
investigation into how model size influences sampling efficiency across varying
sampling steps. Our findings unveil a surprising trend: when operating under a
given inference budget, smaller models frequently outperform their larger
equivalents in generating high-quality results. Moreover, we extend our study
to demonstrate the generalizability of the these findings by applying various
diffusion samplers, exploring diverse downstream tasks, evaluating
post-distilled models, as well as comparing performance relative to training
compute. These findings open up new pathways for the development of LDM scaling
strategies which can be employed to enhance generative capabilities within
limited inference budgets.
\\ ( https://arxiv.org/abs/2404.01367 ,  47498kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01436 (*cross-listing*)
Date: Mon, 1 Apr 2024 19:17:45 GMT   (262kb)

Title: Convergence Guarantees for RMSProp and Adam in Generalized-smooth
  Non-convex Optimization with Affine Noise Variance
Authors: Qi Zhang, Yi Zhou, Shaofeng Zou
Categories: stat.ML cs.LG math.OC
\\
  This paper provides the first tight convergence analyses for RMSProp and Adam
in non-convex optimization under the most relaxed assumptions of
coordinate-wise generalized smoothness and affine noise variance. We first
analyze RMSProp, which is a special case of Adam with adaptive learning rates
but without first-order momentum. Specifically, to solve the challenges due to
dependence among adaptive update, unbounded gradient estimate and Lipschitz
constant, we demonstrate that the first-order term in the descent lemma
converges and its denominator is upper bounded by a function of gradient norm.
Based on this result, we show that RMSProp with proper hyperparameters
converges to an $\epsilon$-stationary point with an iteration complexity of
$\mathcal O(\epsilon^{-4})$. We then generalize our analysis to Adam, where the
additional challenge is due to a mismatch between the gradient and first-order
momentum. We develop a new upper bound on the first-order term in the descent
lemma, which is also a function of the gradient norm. We show that Adam with
proper hyperparameters converges to an $\epsilon$-stationary point with an
iteration complexity of $\mathcal O(\epsilon^{-4})$. Our complexity results for
both RMSProp and Adam match with the complexity lower bound established in
\cite{arjevani2023lower}.
\\ ( https://arxiv.org/abs/2404.01436 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01448 (*cross-listing*)
Date: Mon, 1 Apr 2024 19:41:33 GMT   (16208kb)

Title: Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT
  Reconstruction
Authors: Jiacheng Xie, Hua-Chieh Shao, Yunxiang Li, You Zhang
Categories: physics.med-ph cs.LG
Comments: 20 pages, 8 figures, submitted to Physics in Medicine & Biology
\\
  Cone-beam computed tomography (CBCT) is widely used in image-guided
radiotherapy. Reconstructing CBCTs from limited-angle acquisitions (LA-CBCT) is
highly desired for improved imaging efficiency, dose reduction, and better
mechanical clearance. LA-CBCT reconstruction, however, suffers from severe
under-sampling artifacts, making it a highly ill-posed inverse problem.
Diffusion models can generate data/images by reversing a data-noising process
through learned data distributions; and can be incorporated as a
denoiser/regularizer in LA-CBCT reconstruction. In this study, we developed a
diffusion model-based framework, prior frequency-guided diffusion model
(PFGDM), for robust and structure-preserving LA-CBCT reconstruction. PFGDM uses
a conditioned diffusion model as a regularizer for LA-CBCT reconstruction, and
the condition is based on high-frequency information extracted from
patient-specific prior CT scans which provides a strong anatomical prior for
LA-CBCT reconstruction. Specifically, we developed two variants of PFGDM
(PFGDM-A and PFGDM-B) with different conditioning schemes. PFGDM-A applies the
high-frequency CT information condition until a pre-optimized iteration step,
and drops it afterwards to enable both similar and differing CT/CBCT anatomies
to be reconstructed. PFGDM-B, on the other hand, continuously applies the prior
CT information condition in every reconstruction step, while with a decaying
mechanism, to gradually phase out the reconstruction guidance from the prior CT
scans. The two variants of PFGDM were tested and compared with current
available LA-CBCT reconstruction solutions, via metrics including PSNR and
SSIM. PFGDM outperformed all traditional and diffusion model-based methods.
PFGDM reconstructs high-quality LA-CBCTs under very-limited gantry angles,
allowing faster and more flexible CBCT scans with dose reductions.
\\ ( https://arxiv.org/abs/2404.01448 ,  16208kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01501 (*cross-listing*)
Date: Mon, 1 Apr 2024 21:49:05 GMT   (1661kb,D)

Title: MosquitoFusion: A Multiclass Dataset for Real-Time Detection of
  Mosquitoes, Swarms, and Breeding Sites Using Deep Learning
Authors: Md. Faiyaz Abdullah Sayeedi, Fahim Hafiz, Md Ashiqur Rahman
Categories: cs.CV cs.LG
\\
  In this paper, we present an integrated approach to real-time mosquito
detection using our multiclass dataset (MosquitoFusion) containing 1204 diverse
images and leverage cutting-edge technologies, specifically computer vision, to
automate the identification of Mosquitoes, Swarms, and Breeding Sites. The
pre-trained YOLOv8 model, trained on this dataset, achieved a mean Average
Precision (mAP@50) of 57.1%, with precision at 73.4% and recall at 50.5%. The
integration of Geographic Information Systems (GIS) further enriches the depth
of our analysis, providing valuable insights into spatial patterns. The dataset
and code are available at https://github.com/faiyazabdullah/MosquitoFusion.
\\ ( https://arxiv.org/abs/2404.01501 ,  1661kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01518 (*cross-listing*)
Date: Mon, 1 Apr 2024 22:53:47 GMT   (1212kb,D)

Title: Temporally Consistent Unbalanced Optimal Transport for Unsupervised
  Action Segmentation
Authors: Ming Xu, Stephen Gould
Categories: cs.CV cs.LG eess.IV
Comments: Accepted to CVPR 2024
\\
  We propose a novel approach to the action segmentation task for long,
untrimmed videos, based on solving an optimal transport problem. By encoding a
temporal consistency prior into a Gromov-Wasserstein problem, we are able to
decode a temporally consistent segmentation from a noisy affinity/matching cost
matrix between video frames and action classes. Unlike previous approaches, our
method does not require knowing the action order for a video to attain temporal
consistency. Furthermore, our resulting (fused) Gromov-Wasserstein problem can
be efficiently solved on GPUs using a few iterations of projected mirror
descent. We demonstrate the effectiveness of our method in an unsupervised
learning setting, where our method is used to generate pseudo-labels for
self-training. We evaluate our segmentation approach and unsupervised learning
pipeline on the Breakfast, 50-Salads, YouTube Instructions and Desktop Assembly
datasets, yielding state-of-the-art results for the unsupervised video action
segmentation task.
\\ ( https://arxiv.org/abs/2404.01518 ,  1212kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01521 (*cross-listing*)
Date: Mon, 1 Apr 2024 23:01:07 GMT   (673kb,D)

Title: Fair MP-BOOST: Fair and Interpretable Minipatch Boosting
Authors: Camille Olivia Little and Genevera I. Allen
Categories: stat.ML cs.LG
\\
  Ensemble methods, particularly boosting, have established themselves as
highly effective and widely embraced machine learning techniques for tabular
data. In this paper, we aim to leverage the robust predictive power of
traditional boosting methods while enhancing fairness and interpretability. To
achieve this, we develop Fair MP-Boost, a stochastic boosting scheme that
balances fairness and accuracy by adaptively learning features and observations
during training. Specifically, Fair MP-Boost sequentially samples small subsets
of observations and features, termed minipatches (MP), according to adaptively
learned feature and observation sampling probabilities. We devise these
probabilities by combining loss functions, or by combining feature importance
scores to address accuracy and fairness simultaneously. Hence, Fair MP-Boost
prioritizes important and fair features along with challenging instances, to
select the most relevant minipatches for learning. The learned probability
distributions also yield intrinsic interpretations of feature importance and
important observations in Fair MP-Boost. Through empirical evaluation of
simulated and benchmark datasets, we showcase the interpretability, accuracy,
and fairness of Fair MP-Boost.
\\ ( https://arxiv.org/abs/2404.01521 ,  673kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01530 (*cross-listing*)
Date: Mon, 1 Apr 2024 23:34:28 GMT   (4074kb,D)

Title: ML KPI Prediction in 5G and B5G Networks
Authors: Nguyen Phuc Tran, Oscar Delgado, Brigitte Jaumard, Fadi Bishay
Categories: cs.NI cs.LG cs.SY eess.SY
Journal-ref: 2023 Joint European Conference on Networks and Communications & 6G
  Summit (EuCNC/6G Summit)
DOI: 10.1109/EuCNC/6GSummit58263.2023.10188363
\\
  Network operators are facing new challenges when meeting the needs of their
customers. The challenges arise due to the rise of new services, such as HD
video streaming, IoT, autonomous driving, etc., and the exponential growth of
network traffic. In this context, 5G and B5G networks have been evolving to
accommodate a wide range of applications and use cases. Additionally, this
evolution brings new features, like the ability to create multiple end-to-end
isolated virtual networks using network slicing. Nevertheless, to ensure the
quality of service, operators must maintain and optimize their networks in
accordance with the key performance indicators (KPIs) and the slice
service-level agreements (SLAs).
  In this paper, we introduce a machine learning (ML) model used to estimate
throughput in 5G and B5G networks with end-to-end (E2E) network slices. Then,
we combine the predicted throughput with the current network state to derive an
estimate of other network KPIs, which can be used to further improve service
assurance. To assess the efficiency of our solution, a performance metric was
proposed. Numerical evaluations demonstrate that our KPI prediction model
outperforms those derived from other methods with the same or nearly the same
computational time.
\\ ( https://arxiv.org/abs/2404.01530 ,  4074kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01574 (*cross-listing*)
Date: Tue, 2 Apr 2024 02:08:29 GMT   (2007kb,D)

Title: Multi-granular Adversarial Attacks against Black-box Neural Ranking
  Models
Authors: Yu-An Liu, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan,
  Xueqi Cheng
Categories: cs.IR cs.CR cs.LG
Comments: Accepted by SIGIR 2024
\\
  Adversarial ranking attacks have gained increasing attention due to their
success in probing vulnerabilities, and, hence, enhancing the robustness, of
neural ranking models. Conventional attack methods employ perturbations at a
single granularity, e.g., word-level or sentence-level, to a target document.
However, limiting perturbations to a single level of granularity may reduce the
flexibility of creating adversarial examples, thereby diminishing the potential
threat of the attack. Therefore, we focus on generating high-quality
adversarial examples by incorporating multi-granular perturbations. Achieving
this objective involves tackling a combinatorial explosion problem, which
requires identifying an optimal combination of perturbations across all
possible levels of granularity, positions, and textual pieces. To address this
challenge, we transform the multi-granular adversarial attack into a sequential
decision-making process, where perturbations in the next attack step are
influenced by the perturbed document in the current attack step. Since the
attack process can only access the final state without direct intermediate
signals, we use reinforcement learning to perform multi-granular attacks.
During the reinforcement learning process, two agents work cooperatively to
identify multi-granular vulnerabilities as attack targets and organize
perturbation candidates into a final perturbation sequence. Experimental
results show that our attack method surpasses prevailing baselines in both
attack effectiveness and imperceptibility.
\\ ( https://arxiv.org/abs/2404.01574 ,  2007kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01608 (*cross-listing*)
Date: Tue, 2 Apr 2024 03:06:25 GMT   (706kb)

Title: FAIRM: Learning invariant representations for algorithmic fairness and
  domain generalization with minimax optimality
Authors: Sai Li and Linjun Zhang
Categories: stat.ML cs.LG stat.ME
\\
  Machine learning methods often assume that the test data have the same
distribution as the training data. However, this assumption may not hold due to
multiple levels of heterogeneity in applications, raising issues in algorithmic
fairness and domain generalization. In this work, we address the problem of
fair and generalizable machine learning by invariant principles. We propose a
training environment-based oracle, FAIRM, which has desirable fairness and
domain generalization properties under a diversity-type condition. We then
provide an empirical FAIRM with finite-sample theoretical guarantees under weak
distributional assumptions. We then develop efficient algorithms to realize
FAIRM in linear models and demonstrate the nonasymptotic performance with
minimax optimality. We evaluate our method in numerical experiments with
synthetic data and MNIST data and show that it outperforms its counterparts.
\\ ( https://arxiv.org/abs/2404.01608 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01617 (*cross-listing*)
Date: Tue, 2 Apr 2024 03:43:55 GMT   (4244kb,D)

Title: LLM-ABR: Designing Adaptive Bitrate Algorithms via Large Language Models
Authors: Zhiyuan He, Aashish Gottipati, Lili Qiu, Francis Y. Yan, Xufang Luo,
  Kenuo Xu, Yuqing Yang
Categories: cs.NI cs.LG cs.MM
\\
  We present LLM-ABR, the first system that utilizes the generative
capabilities of large language models (LLMs) to autonomously design adaptive
bitrate (ABR) algorithms tailored for diverse network characteristics.
Operating within a reinforcement learning framework, LLM-ABR empowers LLMs to
design key components such as states and neural network architectures. We
evaluate LLM-ABR across diverse network settings, including broadband,
satellite, 4G, and 5G. LLM-ABR consistently outperforms default ABR algorithms.
\\ ( https://arxiv.org/abs/2404.01617 ,  4244kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01645 (*cross-listing*)
Date: Tue, 2 Apr 2024 05:30:39 GMT   (1038kb,D)

Title: ContrastCAD: Contrastive Learning-based Representation Learning for
  Computer-Aided Design Models
Authors: Minseop Jung, Minseong Kim, Jibum Kim
Categories: cs.CV cs.LG
\\
  The success of Transformer-based models has encouraged many researchers to
learn CAD models using sequence-based approaches. However, learning CAD models
is still a challenge, because they can be represented as complex shapes with
long construction sequences. Furthermore, the same CAD model can be expressed
using different CAD construction sequences. We propose a novel contrastive
learning-based approach, named ContrastCAD, that effectively captures semantic
information within the construction sequences of the CAD model. ContrastCAD
generates augmented views using dropout techniques without altering the shape
of the CAD model. We also propose a new CAD data augmentation method, called a
Random Replace and Extrude (RRE) method, to enhance the learning performance of
the model when training an imbalanced training CAD dataset. Experimental
results show that the proposed RRE augmentation method significantly enhances
the learning performance of Transformer-based autoencoders, even for complex
CAD models having very long construction sequences. The proposed ContrastCAD
model is shown to be robust to permutation changes of construction sequences
and performs better representation learning by generating representation spaces
where similar CAD models are more closely clustered. Our codes are available at
https://github.com/cm8908/ContrastCAD.
\\ ( https://arxiv.org/abs/2404.01645 ,  1038kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01669 (*cross-listing*)
Date: Tue, 2 Apr 2024 06:18:41 GMT   (85kb,D)

Title: How COVID-19 has Impacted the Anti-Vaccine Discourse: A Large-Scale
  Twitter Study Spanning Pre-COVID and Post-COVID Era
Authors: Soham Poddar, Rajdeep Mukherjee, Subhendu Khatuya, Niloy Ganguly,
  Saptarshi Ghosh
Categories: cs.SI cs.CY cs.LG
Comments: This work has been accepted to appear at the 18th International AAAI
  Conference on Web and Social Media (ICWSM), 2024
\\
  The debate around vaccines has been going on for decades, but the COVID-19
pandemic showed how crucial it is to understand and mitigate anti-vaccine
sentiments. While the pandemic may be over, it is still important to understand
how the pandemic affected the anti-vaccine discourse, and whether the arguments
against non-COVID vaccines (e.g., Flu, MMR, IPV, HPV vaccines) have also
changed due to the pandemic. This study attempts to answer these questions
through a large-scale study of anti-vaccine posts on Twitter. Almost all prior
works that utilized social media to understand anti-vaccine opinions considered
only the three broad stances of Anti-Vax, Pro-Vax, and Neutral. There has not
been any effort to identify the specific reasons/concerns behind the anti-vax
sentiments (e.g., side-effects, conspiracy theories, political reasons) on
social media at scale. In this work, we propose two novel methods for
classifying tweets into 11 different anti-vax concerns -- a discriminative
approach (entailment-based) and a generative approach (based on instruction
tuning of LLMs) -- which outperform several strong baselines. We then apply
this classifier on anti-vaccine tweets posted over a 5-year period (Jan 2018 -
Jan 2023) to understand how the COVID-19 pandemic has impacted the anti-vaccine
concerns among the masses. We find that the pandemic has made the anti-vaccine
discourse far more complex than in the pre-COVID times, and increased the
variety of concerns being voiced. Alarmingly, we find that concerns about COVID
vaccines are now being projected onto the non-COVID vaccines, thus making more
people hesitant in taking vaccines in the post-COVID era.
\\ ( https://arxiv.org/abs/2404.01669 ,  85kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01697 (*cross-listing*)
Date: Tue, 2 Apr 2024 06:58:41 GMT   (3576kb,D)

Title: Preventing Model Collapse in Gaussian Process Latent Variable Models
Authors: Ying Li, Zhidi Lin, Feng Yin, Michael Minyi Zhang
Categories: stat.ML cs.LG
\\
  Gaussian process latent variable models (GPLVMs) are a versatile family of
unsupervised learning models, commonly used for dimensionality reduction.
However, common challenges in modeling data with GPLVMs include inadequate
kernel flexibility and improper selection of the projection noise, which leads
to a type of model collapse characterized primarily by vague latent
representations that do not reflect the underlying structure of the data. This
paper addresses these issues by, first, theoretically examining the impact of
the projection variance on model collapse through the lens of a linear GPLVM.
Second, we address the problem of model collapse due to inadequate kernel
flexibility by integrating the spectral mixture (SM) kernel and a
differentiable random Fourier feature (RFF) kernel approximation, which ensures
computational scalability and efficiency through off-the-shelf automatic
differentiation tools for learning the kernel hyperparameters, projection
variance, and latent representations within the variational inference
framework. The proposed GPLVM, named advisedRFLVM, is evaluated across diverse
datasets and consistently outperforms various salient competing models,
including state-of-the-art variational autoencoders (VAEs) and GPLVM variants,
in terms of informative latent representations and missing data imputation.
\\ ( https://arxiv.org/abs/2404.01697 ,  3576kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01790 (*cross-listing*)
Date: Tue, 2 Apr 2024 09:53:20 GMT   (715kb,D)

Title: Super-Resolution Analysis for Landfill Waste Classification
Authors: Matias Molina, Rita P. Ribeiro, Bruno Veloso, Jo\~ao Gama
Categories: cs.CV cs.LG
Comments: This article has been accepted by the Symposium on Intelligent Data
  Analysis (IDA 2024)
\\
  Illegal landfills are a critical issue due to their environmental, economic,
and public health impacts. This study leverages aerial imagery for
environmental crime monitoring. While advances in artificial intelligence and
computer vision hold promise, the challenge lies in training models with
high-resolution literature datasets and adapting them to open-access
low-resolution images. Considering the substantial quality differences and
limited annotation, this research explores the adaptability of models across
these domains. Motivated by the necessity for a comprehensive evaluation of
waste detection algorithms, it advocates cross-domain classification and
super-resolution enhancement to analyze the impact of different image
resolutions on waste classification as an evaluation to combat the
proliferation of illegal landfills. We observed performance improvements by
enhancing image quality but noted an influence on model sensitivity,
necessitating careful threshold fine-tuning.
\\ ( https://arxiv.org/abs/2404.01790 ,  715kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01814 (*cross-listing*)
Date: Tue, 2 Apr 2024 10:16:30 GMT   (2101kb,D)

Title: A neural network-based approach to hybrid systems identification for
  control
Authors: Filippo Fabiani, Bartolomeo Stellato, Daniele Masti, Paul J. Goulart
Categories: eess.SY cs.LG cs.SY math.OC
\\
  We consider the problem of designing a machine learning-based model of an
unknown dynamical system from a finite number of (state-input)-successor state
data points, such that the model obtained is also suitable for optimal control
design. We propose a specific neural network (NN) architecture that yields a
hybrid system with piecewise-affine dynamics that is differentiable with
respect to the network's parameters, thereby enabling the use of
derivative-based training procedures. We show that a careful choice of our NN's
weights produces a hybrid system model with structural properties that are
highly favourable when used as part of a finite horizon optimal control problem
(OCP). Specifically, we show that optimal solutions with strong local
optimality guarantees can be computed via nonlinear programming, in contrast to
classical OCPs for general hybrid systems which typically require mixed-integer
optimization. In addition to being well-suited for optimal control design,
numerical simulations illustrate that our NN-based technique enjoys very
similar performance to state-of-the-art system identification methodologies for
hybrid systems and it is competitive on nonlinear benchmarks.
\\ ( https://arxiv.org/abs/2404.01814 ,  2101kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01830 (*cross-listing*)
Date: Tue, 2 Apr 2024 10:42:44 GMT   (58kb,D)

Title: Doubly-Robust Off-Policy Evaluation with Estimated Logging Policy
Authors: Kyungbok Lee, Myunghee Cho Paik
Categories: stat.ML cs.LG
\\
  We introduce a novel doubly-robust (DR) off-policy evaluation (OPE) estimator
for Markov decision processes, DRUnknown, designed for situations where both
the logging policy and the value function are unknown. The proposed estimator
initially estimates the logging policy and then estimates the value function
model by minimizing the asymptotic variance of the estimator while considering
the estimating effect of the logging policy. When the logging policy model is
correctly specified, DRUnknown achieves the smallest asymptotic variance within
the class containing existing OPE estimators. When the value function model is
also correctly specified, DRUnknown is optimal as its asymptotic variance
reaches the semiparametric lower bound. We present experimental results
conducted in contextual bandits and reinforcement learning to compare the
performance of DRUnknown with that of existing methods.
\\ ( https://arxiv.org/abs/2404.01830 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01832 (*cross-listing*)
Date: Tue, 2 Apr 2024 10:44:55 GMT   (253kb,D)

Title: When does Subagging Work?
Authors: Christos Revelas, Otilia Boldea, Bas J.M. Werker
Categories: stat.ML cs.LG
\\
  We study the effectiveness of subagging, or subsample aggregating, on
regression trees, a popular non-parametric method in machine learning. First,
we give sufficient conditions for pointwise consistency of trees. We formalize
that (i) the bias depends on the diameter of cells, hence trees with few splits
tend to be biased, and (ii) the variance depends on the number of observations
in cells, hence trees with many splits tend to have large variance. While these
statements for bias and variance are known to hold globally in the covariate
space, we show that, under some constraints, they are also true locally.
Second, we compare the performance of subagging to that of trees across
different numbers of splits. We find that (1) for any given number of splits,
subagging improves upon a single tree, and (2) this improvement is larger for
many splits than it is for few splits. However, (3) a single tree grown at
optimal size can outperform subagging if the size of its individual trees is
not optimally chosen. This last result goes against common practice of growing
large randomized trees to eliminate bias and then averaging to reduce variance.
\\ ( https://arxiv.org/abs/2404.01832 ,  253kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01867 (*cross-listing*)
Date: Tue, 2 Apr 2024 11:44:37 GMT   (30453kb,D)

Title: Active Exploration in Bayesian Model-based Reinforcement Learning for
  Robot Manipulation
Authors: Carlos Plou, Ana C. Murillo, Ruben Martinez-Cantin
Categories: cs.RO cs.LG
\\
  Efficiently tackling multiple tasks within complex environment, such as those
found in robot manipulation, remains an ongoing challenge in robotics and an
opportunity for data-driven solutions, such as reinforcement learning (RL).
Model-based RL, by building a dynamic model of the robot, enables data reuse
and transfer learning between tasks with the same robot and similar
environment. Furthermore, data gathering in robotics is expensive and we must
rely on data efficient approaches such as model-based RL, where policy learning
is mostly conducted on cheaper simulations based on the learned model.
Therefore, the quality of the model is fundamental for the performance of the
posterior tasks. In this work, we focus on improving the quality of the model
and maintaining the data efficiency by performing active learning of the
dynamic model during a preliminary exploration phase based on maximize
information gathering. We employ Bayesian neural network models to represent,
in a probabilistic way, both the belief and information encoded in the dynamic
model during exploration. With our presented strategies we manage to actively
estimate the novelty of each transition, using this as the exploration reward.
In this work, we compare several Bayesian inference methods for neural
networks, some of which have never been used in a robotics context, and
evaluate them in a realistic robot manipulation setup. Our experiments show the
advantages of our Bayesian model-based RL approach, with similar quality in the
results than relevant alternatives with much lower requirements regarding robot
execution steps. Unlike related previous studies that focused the validation
solely on toy problems, our research takes a step towards more realistic
setups, tackling robotic arm end-tasks.
\\ ( https://arxiv.org/abs/2404.01867 ,  30453kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01875 (*cross-listing*)
Date: Tue, 2 Apr 2024 11:59:58 GMT   (18108kb,D)

Title: Satellite Federated Edge Learning: Architecture Design and Convergence
  Analysis
Authors: Yuanming Shi, Li Zeng, Jingyang Zhu, Yong Zhou, Chunxiao Jiang, Khaled
  B. Letaief
Categories: eess.SP cs.DC cs.IT cs.LG math.IT
Comments: 16 pages, 15 figures
\\
  The proliferation of low-earth-orbit (LEO) satellite networks leads to the
generation of vast volumes of remote sensing data which is traditionally
transferred to the ground server for centralized processing, raising privacy
and bandwidth concerns. Federated edge learning (FEEL), as a distributed
machine learning approach, has the potential to address these challenges by
sharing only model parameters instead of raw data. Although promising, the
dynamics of LEO networks, characterized by the high mobility of satellites and
short ground-to-satellite link (GSL) duration, pose unique challenges for FEEL.
Notably, frequent model transmission between the satellites and ground incurs
prolonged waiting time and large transmission latency. This paper introduces a
novel FEEL algorithm, named FEDMEGA, tailored to LEO mega-constellation
networks. By integrating inter-satellite links (ISL) for intra-orbit model
aggregation, the proposed algorithm significantly reduces the usage of low data
rate and intermittent GSL. Our proposed method includes a ring all-reduce based
intra-orbit aggregation mechanism, coupled with a network flow-based
transmission scheme for global model aggregation, which enhances transmission
efficiency. Theoretical convergence analysis is provided to characterize the
algorithm performance. Extensive simulations show that our FEDMEGA algorithm
outperforms existing satellite FEEL algorithms, exhibiting an approximate 30%
improvement in convergence rate.
\\ ( https://arxiv.org/abs/2404.01875 ,  18108kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01883 (*cross-listing*)
Date: Tue, 2 Apr 2024 12:15:37 GMT   (811kb,D)

Title: Adversarial Combinatorial Bandits with Switching Costs
Authors: Yanyan Dong and Vincent Y. F. Tan
Categories: stat.ML cs.LG
Comments: The work has been accepted in IEEE Transactions on Information
  Theory. https://ieeexplore.ieee.org/document/10487974
DOI: 10.1109/TIT.2024.3384033
\\
  We study the problem of adversarial combinatorial bandit with a switching
cost $\lambda$ for a switch of each selected arm in each round, considering
both the bandit feedback and semi-bandit feedback settings. In the oblivious
adversarial case with $K$ base arms and time horizon $T$, we derive lower
bounds for the minimax regret and design algorithms to approach them. To prove
these lower bounds, we design stochastic loss sequences for both feedback
settings, building on an idea from previous work in Dekel et al. (2014). The
lower bound for bandit feedback is $ \tilde{\Omega}\big( (\lambda
K)^{\frac{1}{3}} (TI)^{\frac{2}{3}}\big)$ while that for semi-bandit feedback
is $ \tilde{\Omega}\big( (\lambda K I)^{\frac{1}{3}} T^{\frac{2}{3}}\big)$
where $I$ is the number of base arms in the combinatorial arm played in each
round. To approach these lower bounds, we design algorithms that operate in
batches by dividing the time horizon into batches to restrict the number of
switches between actions. For the bandit feedback setting, where only the total
loss of the combinatorial arm is observed, we introduce the Batched-Exp2
algorithm which achieves a regret upper bound of $\tilde{O}\big((\lambda
K)^{\frac{1}{3}}T^{\frac{2}{3}}I^{\frac{4}{3}}\big)$ as $T$ tends to infinity.
In the semi-bandit feedback setting, where all losses for the combinatorial arm
are observed, we propose the Batched-BROAD algorithm which achieves a regret
upper bound of $\tilde{O}\big( (\lambda K)^{\frac{1}{3}}
(TI)^{\frac{2}{3}}\big)$.
\\ ( https://arxiv.org/abs/2404.01883 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01932 (*cross-listing*)
Date: Tue, 2 Apr 2024 13:25:16 GMT   (3590kb,D)

Title: Bridging Language, Vision and Action: Multimodal VAEs in Robotic
  Manipulation Tasks
Authors: Gabriela Sejnova, Michal Vavrecka, Karla Stepanova
Categories: cs.RO cs.LG
Comments: 7 pages, 5 figures, 2 tables, conference
\\
  In this work, we focus on unsupervised vision-language-action mapping in the
area of robotic manipulation. Recently, multiple approaches employing
pre-trained large language and vision models have been proposed for this task.
However, they are computationally demanding and require careful fine-tuning of
the produced outputs. A more lightweight alternative would be the
implementation of multimodal Variational Autoencoders (VAEs) which can extract
the latent features of the data and integrate them into a joint representation,
as has been demonstrated mostly on image-image or image-text data for the
state-of-the-art models. Here we explore whether and how can multimodal VAEs be
employed in unsupervised robotic manipulation tasks in a simulated environment.
Based on the obtained results, we propose a model-invariant training
alternative that improves the models' performance in a simulator by up to 55%.
Moreover, we systematically evaluate the challenges raised by the individual
tasks such as object or robot position variability, number of distractors or
the task length. Our work thus also sheds light on the potential benefits and
limitations of using the current multimodal VAEs for unsupervised learning of
robotic motion trajectories based on vision and language.
\\ ( https://arxiv.org/abs/2404.01932 ,  3590kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01946 (*cross-listing*)
Date: Tue, 2 Apr 2024 13:42:29 GMT   (2101kb,D)

Title: Synthetic Data for Robust Stroke Segmentation
Authors: Liam Chalcroft and Ioannis Pappas and Cathy J. Price and John
  Ashburner
Categories: eess.IV cs.CV cs.LG
\\
  Deep learning-based semantic segmentation in neuroimaging currently requires
high-resolution scans and extensive annotated datasets, posing significant
barriers to clinical applicability. We present a novel synthetic framework for
the task of lesion segmentation, extending the capabilities of the established
SynthSeg approach to accommodate large heterogeneous pathologies with
lesion-specific augmentation strategies. Our method trains deep learning
models, demonstrated here with the UNet architecture, using label maps derived
from healthy and stroke datasets, facilitating the segmentation of both healthy
tissue and pathological lesions without sequence-specific training data.
Evaluated against in-domain and out-of-domain (OOD) datasets, our framework
demonstrates robust performance, rivaling current methods within the training
domain and significantly outperforming them on OOD data. This contribution
holds promise for advancing medical imaging analysis in clinical settings,
especially for stroke pathology, by enabling reliable segmentation across
varied imaging sequences with reduced dependency on large annotated corpora.
Code and weights available at https://github.com/liamchalcroft/SynthStroke.
\\ ( https://arxiv.org/abs/2404.01946 ,  2101kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01959 (*cross-listing*)
Date: Tue, 2 Apr 2024 13:54:22 GMT   (19414kb,D)

Title: Bi-LORA: A Vision-Language Approach for Synthetic Image Detection
Authors: Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdenour
  Hadid, Abdelmalik Taleb-Ahmed
Categories: cs.CV cs.CR cs.LG
\\
  Advancements in deep image synthesis techniques, such as generative
adversarial networks (GANs) and diffusion models (DMs), have ushered in an era
of generating highly realistic images. While this technological progress has
captured significant interest, it has also raised concerns about the potential
difficulty in distinguishing real images from their synthetic counterparts.
This paper takes inspiration from the potent convergence capabilities between
vision and language, coupled with the zero-shot nature of vision-language
models (VLMs). We introduce an innovative method called Bi-LORA that leverages
VLMs, combined with low-rank adaptation (LORA) tuning techniques, to enhance
the precision of synthetic image detection for unseen model-generated images.
The pivotal conceptual shift in our methodology revolves around reframing
binary classification as an image captioning task, leveraging the distinctive
capabilities of cutting-edge VLM, notably bootstrapping language image
pre-training (BLIP2). Rigorous and comprehensive experiments are conducted to
validate the effectiveness of our proposed approach, particularly in detecting
unseen diffusion-generated images from unknown diffusion-based generative
models during training, showcasing robustness to noise, and demonstrating
generalization capabilities to GANs. The obtained results showcase an
impressive average accuracy of 93.41% in synthetic image detection on unseen
generation models. The code and models associated with this research can be
publicly accessed at https://github.com/Mamadou-Keita/VLM-DETECT.
\\ ( https://arxiv.org/abs/2404.01959 ,  19414kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01964 (*cross-listing*)
Date: Tue, 2 Apr 2024 13:57:30 GMT   (28903kb,D)

Title: CAM-Based Methods Can See through Walls
Authors: Magamed Taimeskhanov and Ronan Sicre and Damien Garreau
Categories: cs.CV cs.LG
Comments: 25 pages, 9 figures
\\
  CAM-based methods are widely-used post-hoc interpretability method that
produce a saliency map to explain the decision of an image classification
model. The saliency map highlights the important areas of the image relevant to
the prediction. In this paper, we show that most of these methods can
incorrectly attribute an important score to parts of the image that the model
cannot see. We show that this phenomenon occurs both theoretically and
experimentally. On the theory side, we analyze the behavior of GradCAM on a
simple masked CNN model at initialization. Experimentally, we train a VGG-like
model constrained to not use the lower part of the image and nevertheless
observe positive scores in the unseen part of the image. This behavior is
evaluated quantitatively on two new datasets. We believe that this is
problematic, potentially leading to mis-interpretation of the model's behavior.
\\ ( https://arxiv.org/abs/2404.01964 ,  28903kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01998 (*cross-listing*)
Date: Tue, 2 Apr 2024 14:41:42 GMT   (45709kb,D)

Title: Specularity Factorization for Low-Light Enhancement
Authors: Saurabh Saini and P J Narayanan
Categories: cs.CV cs.LG
Comments: CVPR 2024, Pages: 8(main)+4(references)+17(supp) = 29
\\
  We present a new additive image factorization technique that treats images to
be composed of multiple latent specular components which can be simply
estimated recursively by modulating the sparsity during decomposition. Our
model-driven {\em RSFNet} estimates these factors by unrolling the optimization
into network layers requiring only a few scalars to be learned. The resultant
factors are interpretable by design and can be fused for different image
enhancement tasks via a network or combined directly by the user in a
controllable fashion. Based on RSFNet, we detail a zero-reference Low Light
Enhancement (LLE) application trained without paired or unpaired supervision.
Our system improves the state-of-the-art performance on standard benchmarks and
achieves better generalization on multiple other datasets. We also integrate
our factors with other task specific fusion networks for applications like
deraining, deblurring and dehazing with negligible overhead thereby
highlighting the multi-domain and multi-task generalizability of our proposed
RSFNet. The code and data is released for reproducibility on the project
homepage.
\\ ( https://arxiv.org/abs/2404.01998 ,  45709kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01999 (*cross-listing*)
Date: Tue, 2 Apr 2024 14:42:52 GMT   (1993kb,D)

Title: Emergence of Chemotactic Strategies with Multi-Agent Reinforcement
  Learning
Authors: Samuel Tovey, Christoph Lohrmann and Christian Holm
Categories: physics.bio-ph cs.LG cs.MA
Comments: 12 pages, 6 figures
\\
  Reinforcement learning (RL) is a flexible and efficient method for
programming micro-robots in complex environments. Here we investigate whether
reinforcement learning can provide insights into biological systems when
trained to perform chemotaxis. Namely, whether we can learn about how
intelligent agents process given information in order to swim towards a target.
We run simulations covering a range of agent shapes, sizes, and swim speeds to
determine if the physical constraints on biological swimmers, namely Brownian
motion, lead to regions where reinforcement learners' training fails. We find
that the RL agents can perform chemotaxis as soon as it is physically possible
and, in some cases, even before the active swimming overpowers the stochastic
environment. We study the efficiency of the emergent policy and identify
convergence in agent size and swim speeds. Finally, we study the strategy
adopted by the reinforcement learning algorithm to explain how the agents
perform their tasks. To this end, we identify three emerging dominant
strategies and several rare approaches taken. These strategies, whilst
producing almost identical trajectories in simulation, are distinct and give
insight into the possible mechanisms behind which biological agents explore
their environment and respond to changing conditions.
\\ ( https://arxiv.org/abs/2404.01999 ,  1993kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02040 (*cross-listing*)
Date: Tue, 2 Apr 2024 15:34:47 GMT   (56kb,D)

Title: Transformers as Transducers
Authors: Lena Strobl, Dana Angluin, David Chiang, Jonathan Rawski, Ashish
  Sabharwal
Categories: cs.FL cs.LG
\\
  We study the sequence-to-sequence mapping capacity of transformers by
relating them to finite transducers, and find that they can express
surprisingly large classes of transductions. We do so using variants of RASP, a
programming language designed to help people "think like transformers," as an
intermediate representation. We extend the existing Boolean variant B-RASP to
sequence-to-sequence functions and show that it computes exactly the
first-order rational functions (such as string rotation). Then, we introduce
two new extensions. B-RASP[pos] enables calculations on positions (such as
copying the first half of a string) and contains all first-order regular
functions. S-RASP adds prefix sum, which enables additional arithmetic
operations (such as squaring a string) and contains all first-order polyregular
functions. Finally, we show that masked average-hard attention transformers can
simulate S-RASP. A corollary of our results is a new proof that transformer
decoders are Turing-complete.
\\ ( https://arxiv.org/abs/2404.02040 ,  56kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02072 (*cross-listing*)
Date: Tue, 2 Apr 2024 16:20:02 GMT   (4540kb,D)

Title: EGTR: Extracting Graph from Transformer for Scene Graph Generation
Authors: Jinbae Im, JeongYeon Nam, Nokyung Park, Hyungmin Lee, Seunghyun Park
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\
  Scene Graph Generation (SGG) is a challenging task of detecting objects and
predicting relationships between objects. After DETR was developed, one-stage
SGG models based on a one-stage object detector have been actively studied.
However, complex modeling is used to predict the relationship between objects,
and the inherent relationship between object queries learned in the multi-head
self-attention of the object detector has been neglected. We propose a
lightweight one-stage SGG model that extracts the relation graph from the
various relationships learned in the multi-head self-attention layers of the
DETR decoder. By fully utilizing the self-attention by-products, the relation
graph can be extracted effectively with a shallow relation extraction head.
Considering the dependency of the relation extraction task on the object
detection task, we propose a novel relation smoothing technique that adjusts
the relation label adaptively according to the quality of the detected objects.
By the relation smoothing, the model is trained according to the continuous
curriculum that focuses on object detection task at the beginning of training
and performs multi-task learning as the object detection performance gradually
improves. Furthermore, we propose a connectivity prediction task that predicts
whether a relation exists between object pairs as an auxiliary task of the
relation extraction. We demonstrate the effectiveness and efficiency of our
method for the Visual Genome and Open Image V6 datasets. Our code is publicly
available at https://github.com/naver-ai/egtr .
\\ ( https://arxiv.org/abs/2404.02072 ,  4540kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02141 (*cross-listing*)
Date: Tue, 2 Apr 2024 17:53:28 GMT   (2172kb,D)

Title: Robustly estimating heterogeneity in factorial data using Rashomon
  Partitions
Authors: Aparajithan Venkateswaran and Anirudh Sankar and Arun G. Chandrasekhar
  and Tyler H. McCormick
Categories: stat.ME cs.LG econ.EM stat.CO stat.ML
\\
  Many statistical analyses, in both observational data and randomized control
trials, ask: how does the outcome of interest vary with combinations of
observable covariates? How do various drug combinations affect health outcomes,
or how does technology adoption depend on incentives and demographics? Our goal
is to partition this factorial space into ``pools'' of covariate combinations
where the outcome differs across the pools (but not within a pool). Existing
approaches (i) search for a single ``optimal'' partition under assumptions
about the association between covariates or (ii) sample from the entire set of
possible partitions. Both these approaches ignore the reality that, especially
with correlation structure in covariates, many ways to partition the covariate
space may be statistically indistinguishable, despite very different
implications for policy or science. We develop an alternative perspective,
called Rashomon Partition Sets (RPSs). Each item in the RPS partitions the
space of covariates using a tree-like geometry. RPSs incorporate all partitions
that have posterior values near the maximum a posteriori partition, even if
they offer substantively different explanations, and do so using a prior that
makes no assumptions about associations between covariates. This prior is the
$\ell_0$ prior, which we show is minimax optimal. Given the RPS we calculate
the posterior of any measurable function of the feature effects vector on
outcomes, conditional on being in the RPS. We also characterize approximation
error relative to the entire posterior and provide bounds on the size of the
RPS. Simulations demonstrate this framework allows for robust conclusions
relative to conventional regularization techniques. We apply our method to
three empirical settings: price effects on charitable giving, chromosomal
structure (telomere length), and the introduction of microfinance.
\\ ( https://arxiv.org/abs/2404.02141 ,  2172kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2305.06104
replaced with revised version Tue, 2 Apr 2024 07:11:01 GMT   (3454kb,D)

Title: Few-shot Link Prediction on N-ary Facts
Authors: Jiyao Wei, Saiping Guan, Xiaolong Jin, Jiafeng Guo, and Xueqi Cheng
Categories: cs.AI cs.IR cs.LG
\\ ( https://arxiv.org/abs/2305.06104 ,  3454kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09591
replaced with revised version Tue, 2 Apr 2024 08:55:51 GMT   (14729kb,D)

Title: Saliency strikes back: How filtering out high frequencies improves
  white-box explanations
Authors: Sabine Muzellec, Thomas Fel, Victor Boutin, L\'eo and\'eol, Rufin
  VanRullen, Thomas Serre
Categories: cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2307.09591 ,  14729kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11482
replaced with revised version Tue, 2 Apr 2024 03:36:57 GMT   (680kb,D)

Title: Meta Prompting for AI Systems
Authors: Yifan Zhang, Yang Yuan, Andrew Chi-Chih Yao
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2311.11482 ,  680kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05209
replaced with revised version Tue, 2 Apr 2024 17:55:52 GMT   (2625kb,D)

Title: HALO: An Ontology for Representing and Categorizing Hallucinations in
  Large Language Models
Authors: Navapat Nananukul, Mayank Kejriwal
Categories: cs.AI cs.CL
Comments: This paper has been accepted and orally presented in "SPIE Defense +
  Commercial Sensing (DCS 2024)" in National Harbor, Maryland, April 2024
\\ ( https://arxiv.org/abs/2312.05209 ,  2625kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09699
replaced with revised version Tue, 2 Apr 2024 10:09:15 GMT   (1503kb,D)

Title: Social, Legal, Ethical, Empathetic, and Cultural Rules: Compilation and
  Reasoning (Extended Version)
Authors: Nicolas Troquard, Martina De Sanctis, Paola Inverardi, Patrizio
  Pelliccione, Gian Luca Scoccia
Categories: cs.AI
Comments: In proceedings of the 38th Annual AAAI Conference on Artificial
  Intelligence
DOI: 10.1609/aaai.v38i20.30245
\\ ( https://arxiv.org/abs/2312.09699 ,  1503kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04429
replaced with revised version Tue, 2 Apr 2024 05:50:00 GMT   (5750kb,D)

Title: i-Rebalance: Personalized Vehicle Repositioning for Supply Demand
  Balance
Authors: Haoyang Chen, Peiyan Sun, Qiyuan Song, Wanyuan Wang, Weiwei Wu, Wencan
  Zhang, Guanyu Gao, Yan Lyu
Categories: cs.AI cs.MA
\\ ( https://arxiv.org/abs/2401.04429 ,  5750kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01118
replaced with revised version Tue, 2 Apr 2024 15:46:35 GMT   (7000kb,D)

Title: PokeLLMon: A Human-Parity Agent for Pokemon Battles with Large Language
  Models
Authors: Sihao Hu, Tiansheng Huang, Ling Liu
Categories: cs.AI cs.CL
Comments: 10 pages
\\ ( https://arxiv.org/abs/2402.01118 ,  7000kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04483
replaced with revised version Tue, 2 Apr 2024 07:57:16 GMT   (7803kb,D)

Title: GraphInstruct: Empowering Large Language Models with Graph Understanding
  and Reasoning Capability
Authors: Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi
  Jiang, Xing Xie
Categories: cs.AI cs.CL
Comments: 9 pages
\\ ( https://arxiv.org/abs/2403.04483 ,  7803kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12627
replaced with revised version Tue, 2 Apr 2024 13:54:47 GMT   (88kb,D)

Title: Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training
  AI Models on Coq Code
Authors: Andreas Florath
Categories: cs.AI cs.LO
Comments: 11 pages
\\ ( https://arxiv.org/abs/2403.12627 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16206
replaced with revised version Tue, 2 Apr 2024 01:52:13 GMT   (526kb)

Title: Rumor Detection with a novel graph neural network approach
Authors: Tianrui Liu, Qi Cai, Changxin Xu, Bo Hong, Fanghao Ni, Yuxin Qiao, and
  Tsungwei Yang
Categories: cs.AI
Comments: 10 pages, 5 figures
\\ ( https://arxiv.org/abs/2403.16206 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20234
replaced with revised version Tue, 2 Apr 2024 09:26:43 GMT   (11713kb,D)

Title: Artificial Neural Networks-based Real-time Classification of ENG Signals
  for Implanted Nerve Interfaces
Authors: Antonio Coviello, Francesco Linsalata, Umberto Spagnolini, Maurizio
  Magarini
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.20234 ,  11713kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01266
replaced with revised version Tue, 2 Apr 2024 15:46:13 GMT   (3038kb,D)

Title: IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic
  Representations
Authors: Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani
  Yogatama, Robin Jia, Willie Neiswanger
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2404.01266 ,  3038kb)
------------------------------------------------------------------------------
\\
arXiv:2206.11612
replaced with revised version Mon, 1 Apr 2024 18:18:51 GMT   (741kb,D)

Title: Constructing Cross-lingual Consumer Health Vocabulary with
  Word-Embedding from Comparable User Generated Content
Authors: Chia-Hsuan Chang, Lei Wang, Christopher C. Yang
Categories: cs.CL cs.IR
Comments: accepted in the IEEE International Conference on Healthcare
  Informatics (IEEE ICHI 2024)
\\ ( https://arxiv.org/abs/2206.11612 ,  741kb)
------------------------------------------------------------------------------
\\
arXiv:2207.12571
replaced with revised version Tue, 2 Apr 2024 00:33:27 GMT   (1345kb,D)

Title: Innovations in Neural Data-to-text Generation: A Survey
Authors: Mandar Sharma, Ajay Gogineni, Naren Ramakrishnan
Categories: cs.CL
Comments: Accepted to ACM Transactions on Intelligent Systems and Technology
  2024
\\ ( https://arxiv.org/abs/2207.12571 ,  1345kb)
------------------------------------------------------------------------------
\\
arXiv:2208.10160
replaced with revised version Tue, 2 Apr 2024 07:00:39 GMT   (4293kb,D)

Title: PANDA: Prompt Transfer Meets Knowledge Distillation for Efficient Model
  Adaptation
Authors: Qihuang Zhong, Liang Ding, Juhua Liu, Bo Du and Dacheng Tao
Categories: cs.CL
Comments: Accepted by IEEE TKDE
\\ ( https://arxiv.org/abs/2208.10160 ,  4293kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17513
replaced with revised version Tue, 2 Apr 2024 11:29:58 GMT   (41kb,D)

Title: Improving the Diproche CNL through Autoformalization via Large Language
  Models
Authors: Merlin Carl
Categories: cs.CL cs.LO
\\ ( https://arxiv.org/abs/2303.17513 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06186
replaced with revised version Tue, 2 Apr 2024 11:34:15 GMT   (43kb,D)

Title: Using Large Language Models for (De-)Formalization and Natural
  Argumentation Exercises for Beginner's Students
Authors: Merlin Carl
Categories: cs.CL math.LO
\\ ( https://arxiv.org/abs/2304.06186 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01181
replaced with revised version Tue, 2 Apr 2024 01:56:56 GMT   (5626kb,D)

Title: A Paradigm Shift: The Future of Machine Translation Lies with Large
  Language Models
Authors: Chenyang Lyu, Zefeng Du, Jitao Xu, Yitao Duan, Minghao Wu, Teresa
  Lynn, Alham Fikri Aji, Derek F. Wong, Siyou Liu, Longyue Wang
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2305.01181 ,  5626kb)
------------------------------------------------------------------------------
\\
arXiv:2305.02547
replaced with revised version Tue, 2 Apr 2024 06:06:53 GMT   (2097kb,D)

Title: PersonaLLM: Investigating the Ability of Large Language Models to
  Express Personality Traits
Authors: Hang Jiang, Xiajie Zhang, Xubo Cao, Cynthia Breazeal, Deb Roy, Jad
  Kabbara
Categories: cs.CL cs.AI cs.HC
Comments: First version in 05/2023. Accepted at NAACL Findings 2024
\\ ( https://arxiv.org/abs/2305.02547 ,  2097kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14610
replaced with revised version Tue, 2 Apr 2024 02:55:33 GMT   (7632kb,D)

Title: This Land is {Your, My} Land: Evaluating Geopolitical Biases in Language
  Models
Authors: Bryan Li, Samar Haider, Chris Callison-Burch
Categories: cs.CL
Comments: NAACL 2024 main conference
\\ ( https://arxiv.org/abs/2305.14610 ,  7632kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10558
replaced with revised version Tue, 2 Apr 2024 04:38:21 GMT   (456kb,D)

Title: Instruction-following Evaluation through Verbalizer Manipulation
Authors: Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay
  Srinivasan, Hongxia Jin
Categories: cs.CL
Comments: NAACL 2024 findings
\\ ( https://arxiv.org/abs/2307.10558 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15780
replaced with revised version Tue, 2 Apr 2024 10:59:51 GMT   (543kb,D)

Title: LLM-Rec: Personalized Recommendation via Prompting Large Language Models
Authors: Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, Qifan Wang, Si
  Zhang, Ren Chen, Christopher Leung, Jiajie Tang, Jiebo Luo
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2307.15780 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08747
replaced with revised version Tue, 2 Apr 2024 09:05:51 GMT   (3075kb,D)

Title: An Empirical Study of Catastrophic Forgetting in Large Language Models
  During Continual Fine-tuning
Authors: Yun Luo and Zhen Yang and Fandong Meng and Yafu Li and Jie Zhou and
  Yue Zhang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2308.08747 ,  3075kb)
------------------------------------------------------------------------------
\\
arXiv:2309.00779
replaced with revised version Tue, 2 Apr 2024 16:52:03 GMT   (14673kb,D)

Title: Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights,
  and Duties
Authors: Taylor Sorensen, Liwei Jiang, Jena Hwang, Sydney Levine, Valentina
  Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula,
  Maarten Sap, John Tasioulas, Yejin Choi
Categories: cs.CL cs.AI
Comments: Proceedings of the AAAI Conference on Artificial Intelligence, 38
Journal-ref: Vol. 38 No. 18: AAAI-24 Technical Tracks 18; 2024; 19937-19947
DOI: 10.1609/aaai.v38i18.29970
\\ ( https://arxiv.org/abs/2309.00779 ,  14673kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07852
replaced with revised version Tue, 2 Apr 2024 01:07:05 GMT   (10130kb,D)

Title: ExpertQA: Expert-Curated Questions and Attributed Answers
Authors: Chaitanya Malaviya, Subin Lee, Sihao Chen, Elizabeth Sieber, Mark
  Yatskar, Dan Roth
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024. Dataset & code is available at
  https://github.com/chaitanyamalaviya/expertqa
\\ ( https://arxiv.org/abs/2309.07852 ,  10130kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08832
replaced with revised version Tue, 2 Apr 2024 09:36:24 GMT   (7744kb,D)

Title: SLIDE: Reference-free Evaluation for Machine Translation using a Sliding
  Document Window
Authors: Vikas Raunak, Tom Kocmi, Matt Post
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2309.08832 ,  7744kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08943
replaced with revised version Mon, 1 Apr 2024 20:38:15 GMT   (8055kb,D)

Title: Contextual Label Projection for Cross-Lingual Structure Extraction
Authors: Tanmay Parekh, I-Hung Hsu, Kuan-Hao Huang, Kai-Wei Chang, Nanyun Peng
Categories: cs.CL
Comments: Accepted at NAACL 2024
\\ ( https://arxiv.org/abs/2309.08943 ,  8055kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12053
replaced with revised version Tue, 2 Apr 2024 06:04:16 GMT   (8051kb,D)

Title: AceGPT, Localizing Large Language Models in Arabic
Authors: Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie
  Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Juncai He, Ziche Liu,
  Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun,
  Xiang Wan, Haizhou Li, Jinchao Xu
Categories: cs.CL
Comments: Accepted to NAACL main conference.
  https://github.com/FreedomIntelligence/AceGPT
\\ ( https://arxiv.org/abs/2309.12053 ,  8051kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15028
replaced with revised version Tue, 2 Apr 2024 17:51:49 GMT   (2096kb,D)

Title: Don't throw away your value model! Generating more preferable text with
  Value-Guided Monte-Carlo Tree Search decoding
Authors: Jiacheng Liu, Andrew Cohen, Ramakanth Pasunuru, Yejin Choi, Hannaneh
  Hajishirzi, Asli Celikyilmaz
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2309.15028 ,  2096kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17157
replaced with revised version Mon, 1 Apr 2024 21:43:50 GMT   (9705kb,D)

Title: LatticeGen: A Cooperative Framework which Hides Generated Text in a
  Lattice for Privacy-Aware Generation on Cloud
Authors: Mengke Zhang, Tianxing He, Tianle Wang, Lu Mi, Fatemehsadat
  Mireshghallah, Binyi Chen, Hao Wang, Yulia Tsvetkov
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.17157 ,  9705kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05746
replaced with revised version Tue, 2 Apr 2024 04:12:53 GMT   (1097kb,D)

Title: Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and
  Execution of LLM Agents in an Auction Arena
Authors: Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, Kyle
  Richardson
Categories: cs.CL cs.AI
Comments: Preprint
\\ ( https://arxiv.org/abs/2310.05746 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05861
replaced with revised version Tue, 2 Apr 2024 17:37:42 GMT   (4457kb,D)

Title: Rephrase, Augment, Reason: Visual Grounding of Questions for
  Vision-Language Models
Authors: Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: ICLR 2024 camera-ready (23 pages), Code:
  https://github.com/archiki/RepARe
\\ ( https://arxiv.org/abs/2310.05861 ,  4457kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06404
replaced with revised version Tue, 2 Apr 2024 11:28:40 GMT   (1602kb,D)

Title: Hexa: Self-Improving for Knowledge-Grounded Dialogue System
Authors: Daejin Jo, Daniel Wontae Nam, Gunsoo Han, Kyoung-Woon On, Taehwan
  Kwon, Seungeun Rho, Sungwoong Kim
Categories: cs.CL cs.AI cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2310.06404 ,  1602kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11722
replaced with revised version Tue, 2 Apr 2024 02:48:22 GMT   (1590kb,D)

Title: Quantifying Self-diagnostic Atomic Knowledge in Chinese Medical
  Foundation Model: A Computational Analysis
Authors: Yaxin Fan, Feng Jiang, Benyou Wang, Peifeng Li, Haizhou Li
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2310.11722 ,  1590kb)
------------------------------------------------------------------------------
\\
arXiv:2310.12558
replaced with revised version Mon, 1 Apr 2024 21:55:06 GMT   (3353kb,D)

Title: Large Language Models Help Humans Verify Truthfulness -- Except When
  They Are Convincingly Wrong
Authors: Chenglei Si, Navita Goyal, Sherry Tongshuang Wu, Chen Zhao, Shi Feng,
  Hal Daum\'e III, Jordan Boyd-Graber
Categories: cs.CL cs.HC
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2310.12558 ,  3353kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20620
replaced with revised version Tue, 2 Apr 2024 15:56:46 GMT   (8028kb,D)

Title: The Unreasonable Effectiveness of Random Target Embeddings for
  Continuous-Output Neural Machine Translation
Authors: Evgeniia Tokarchuk and Vlad Niculae
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2310.20620 ,  8028kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01449
replaced with revised version Mon, 1 Apr 2024 18:02:57 GMT   (521kb,D)

Title: TopicGPT: A Prompt-based Topic Modeling Framework
Authors: Chau Minh Pham, Alexander Hoyle, Simeng Sun, Philip Resnik, Mohit
  Iyyer
Categories: cs.CL
Comments: Accepted to NAACL 2024 (Main conference)
\\ ( https://arxiv.org/abs/2311.01449 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03663
replaced with revised version Tue, 2 Apr 2024 02:27:12 GMT   (851kb,D)

Title: Principles from Clinical Research for NLP Model Generalization
Authors: Aparna Elangovan, Jiayuan He, Yuan Li, Karin Verspoor
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.03663 ,  851kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04453
replaced with revised version Tue, 2 Apr 2024 07:53:07 GMT   (316kb,D)

Title: Lewis's Signaling Game as beta-VAE For Natural Word Lengths and Segments
Authors: Ryo Ueda, Tadahiro Taniguchi
Categories: cs.CL
Comments: ICLR2024 camera-ready
\\ ( https://arxiv.org/abs/2311.04453 ,  316kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06899
replaced with revised version Tue, 2 Apr 2024 05:15:19 GMT   (1157kb,D)

Title: Flames: Benchmarking Value Alignment of LLMs in Chinese
Authors: Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun,
  Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, Dahua
  Lin
Categories: cs.CL cs.AI
Comments: Accepted to the NAACL 2024
\\ ( https://arxiv.org/abs/2311.06899 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07362
replaced with revised version Tue, 2 Apr 2024 04:12:43 GMT   (3715kb,D)

Title: Volcano: Mitigating Multimodal Hallucination through Self-Feedback
  Guided Revision
Authors: Seongyun Lee and Sue Hyun Park and Yongrae Jo and Minjoon Seo
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2311.07362 ,  3715kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08323
replaced with revised version Mon, 1 Apr 2024 23:10:59 GMT   (7755kb,D)

Title: The taste of IPA: Towards open-vocabulary keyword spotting and forced
  alignment in any language
Authors: Jian Zhu, Changbing Yang, Farhan Samir, Jahurul Islam
Categories: cs.CL cs.SD eess.AS
Comments: NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2311.08323 ,  7755kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08505
replaced with revised version Tue, 2 Apr 2024 03:36:32 GMT   (8488kb,D)

Title: Semi-Structured Chain-of-Thought: Integrating Multiple Sources of
  Knowledge for Improved Language Model Reasoning
Authors: Xin Su, Tiep Le, Steven Bethard, Phillip Howard
Categories: cs.CL
Comments: NAACL 2024 main conference
\\ ( https://arxiv.org/abs/2311.08505 ,  8488kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09060
replaced with revised version Tue, 2 Apr 2024 06:32:40 GMT   (9462kb,D)

Title: Do Localization Methods Actually Localize Memorized Data in LLMs? A Tale
  of Two Benchmarks
Authors: Ting-Yun Chang, Jesse Thomason, and Robin Jia
Categories: cs.CL
Comments: accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2311.09060 ,  9462kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09066
replaced with revised version Tue, 2 Apr 2024 05:43:27 GMT   (62kb,D)

Title: Identifying Self-Disclosures of Use, Misuse and Addiction in
  Community-based Social Media Posts
Authors: Chenghao Yang, Tuhin Chakrabarty, Karli R Hochstatter, Melissa N
  Slavin, Nabila El-Bassel, Smaranda Muresan
Categories: cs.CL
Comments: NAACL 2023 Findings (Camera-Ready Version). Codes and Data are
  available at https://github.com/yangalan123/OpioidID
\\ ( https://arxiv.org/abs/2311.09066 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09336
replaced with revised version Tue, 2 Apr 2024 16:39:13 GMT   (2540kb,D)

Title: LLMRefine: Pinpointing and Refining Large Language Models via
  Fine-Grained Actionable Feedback
Authors: Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj Juraska, Biao Zhang,
  Zhongtao Liu, William Yang Wang, Lei Li, and Markus Freitag
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.09336 ,  2540kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09447
replaced with revised version Tue, 2 Apr 2024 15:21:55 GMT   (2239kb,D)

Title: How Trustworthy are Open-Source LLMs? An Assessment under Malicious
  Demonstrations Shows their Vulnerabilities
Authors: Lingbo Mo, Boshi Wang, Muhao Chen, Huan Sun
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2311.09447 ,  2239kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09558
replaced with revised version Tue, 2 Apr 2024 00:59:36 GMT   (5129kb,D)

Title: What if you said that differently?: How Explanation Formats Affect Human
  Feedback Efficacy and User Perception
Authors: Chaitanya Malaviya, Subin Lee, Dan Roth, Mark Yatskar
Categories: cs.CL
Comments: Accepted to NAACL 2024. Code & data available at
  https://github.com/chaitanyamalaviya/rationale_formats
\\ ( https://arxiv.org/abs/2311.09558 ,  5129kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09615
replaced with revised version Tue, 2 Apr 2024 06:23:23 GMT   (7989kb,D)

Title: On Retrieval Augmentation and the Limitations of Language Model Training
Authors: Ting-Rui Chiang, Xinyan Velocity Yu, Joshua Robinson, Ollie Liu,
  Isabelle Lee, Dani Yogatama
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.09615 ,  7989kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09648
replaced with revised version Tue, 2 Apr 2024 02:31:59 GMT   (5271kb,D)

Title: Event Causality Is Key to Computational Story Understanding
Authors: Yidan Sun, Qin Chao, Boyang Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.09648 ,  5271kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09718
replaced with revised version Mon, 1 Apr 2024 20:51:03 GMT   (3550kb,D)

Title: You don't need a personality test to know these models are unreliable:
  Assessing the Reliability of Large Language Models on Psychometric
  Instruments
Authors: Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan, Lajanugen
  Logeswaran, Moontae Lee, Dallas Card, David Jurgens
Categories: cs.CL cs.AI
Comments: Camera-ready version for NAACL 2024. First two authors contributed
  equally
\\ ( https://arxiv.org/abs/2311.09718 ,  3550kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09782
replaced with revised version Tue, 2 Apr 2024 17:16:40 GMT   (9603kb,D)

Title: More Samples or More Prompts? Exploring Effective In-Context Sampling
  for LLM Few-Shot Prompt Engineering
Authors: Bingsheng Yao, Guiming Chen, Ruishi Zou, Yuxuan Lu, Jiachen Li, Shao
  Zhang, Yisi Sang, Sijia Liu, James Hendler, Dakuo Wang
Categories: cs.CL
Comments: Accepted at NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2311.09782 ,  9603kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04556
replaced with revised version Tue, 2 Apr 2024 14:35:40 GMT   (207kb,D)

Title: Large Language Models for Mathematicians
Authors: Simon Frieder, Julius Berner, Philipp Petersen, Thomas Lukasiewicz
Categories: cs.CL cs.AI cs.LG math.HO
Journal-ref: International Mathematical News 254 (2023) 1-20
\\ ( https://arxiv.org/abs/2312.04556 ,  207kb)
------------------------------------------------------------------------------
\\
arXiv:2312.07751
replaced with revised version Tue, 2 Apr 2024 14:30:12 GMT   (8355kb,D)

Title: Large Human Language Models: A Need and the Challenges
Authors: Nikita Soni, H. Andrew Schwartz, Jo\~ao Sedoc, Niranjan
  Balasubramanian
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.07751 ,  8355kb)
------------------------------------------------------------------------------
\\
arXiv:2312.15548
replaced with revised version Tue, 2 Apr 2024 09:36:35 GMT   (424kb,D)

Title: YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal
  Information Extraction
Authors: Xinglin Xiao, Yijie Wang, Nan Xu, Yuqi Wang, Hanxuan Yang, Minzheng
  Wang, Yin Luo, Lei Wang, Wenji Mao, Daniel Zeng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.15548 ,  424kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17044
replaced with revised version Tue, 2 Apr 2024 04:56:52 GMT   (6589kb,D)

Title: Length Extrapolation of Transformers: A Survey from the Perspective of
  Positional Encoding
Authors: Liang Zhao, Xiaocheng Feng, Xiachong Feng, Dongliang Xu, Qing Yang,
  Hongtao Liu, Bing Qin, Ting Liu
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2312.17044 ,  6589kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03729
replaced with revised version Mon, 1 Apr 2024 20:56:11 GMT   (312kb,D)

Title: The Butterfly Effect of Altering Prompts: How Small Changes and
  Jailbreaks Affect Large Language Model Performance
Authors: Abel Salinas and Fred Morstatter
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.03729 ,  312kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03804
replaced with revised version Tue, 2 Apr 2024 01:45:11 GMT   (23706kb,D)

Title: TeleChat Technical Report
Authors: Zhongjiang He, Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao,
  Yuyao Huang, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang,
  Xin Wang, Luwen Pu, Huinan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng
  Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang,
  Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi,
  Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang,
  Shuangyong Song
Categories: cs.CL cs.AI
Comments: 28 pages, 2 figures
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2401.03804 ,  23706kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05249
replaced with revised version Tue, 2 Apr 2024 06:42:52 GMT   (8307kb,D)

Title: CASA: Causality-driven Argument Sufficiency Assessment
Authors: Xiao Liu, Yansong Feng, Kai-Wei Chang
Categories: cs.CL
Comments: Accepted by NAACL 2024 main conference. Project website:
  https://xxxiaol.github.io/CASA/
\\ ( https://arxiv.org/abs/2401.05249 ,  8307kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17514
replaced with revised version Mon, 1 Apr 2024 20:40:40 GMT   (171kb,D)

Title: How Useful is Continued Pre-Training for Generative Unsupervised Domain
  Adaptation?
Authors: Rheeya Uppaal, Yixuan Li, Junjie Hu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.17514 ,  171kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07625
replaced with revised version Tue, 2 Apr 2024 04:17:30 GMT   (194kb,D)

Title: Autonomous Data Selection with Language Models for Mathematical Texts
Authors: Yifan Zhang, Yifan Luo, Yang Yuan, Andrew Chi-Chih Yao
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.07625 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10962
replaced with revised version Tue, 2 Apr 2024 17:13:24 GMT   (1811kb,D)

Title: Measuring and Controlling Instruction (In)Stability in Language Model
  Dialogs
Authors: Kenneth Li, Tianle Liu, Naomi Bashkansky, David Bau, Fernanda
  Vi\'egas, Hanspeter Pfister, Martin Wattenberg
Categories: cs.CL cs.AI cs.LG
Comments: Code: https://github.com/likenneth/persona_drift
\\ ( https://arxiv.org/abs/2402.10962 ,  1811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13604
replaced with revised version Tue, 2 Apr 2024 14:56:18 GMT   (6568kb,D)

Title: Breaking the HISCO Barrier: Automatic Occupational Standardization with
  OccCANINE
Authors: Christian M{\o}ller Dahl, Torben Johansen, Christian Vedel
Categories: cs.CL econ.EM
Comments: All code and guides on how to use OccCANINE is available on GitHub
  https://github.com/christianvedels/OccCANINE
ACM-class: I.2.7; I.7.0
\\ ( https://arxiv.org/abs/2402.13604 ,  6568kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14320
replaced with revised version Tue, 2 Apr 2024 04:23:44 GMT   (150kb,D)

Title: Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve
  Knowledge Base Question Answering
Authors: Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Yueting
  Zhuang
Categories: cs.CL cs.AI
Comments: 8 pages
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.14320 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16611
replaced with revised version Mon, 1 Apr 2024 19:58:43 GMT   (184kb,D)

Title: Understanding the Dataset Practitioners Behind Large Language Model
  Development
Authors: Crystal Qian, Emily Reif, Minsuk Kahng
Categories: cs.CL cs.AI cs.HC
Comments: 7 pages, 2 figures. To be published in In Extended Abstracts of the
  CHI Conference on Human Factors in Computing Systems (CHI EA '24). Revised to
  reflect updates from CHI LBW reviewer feedback
DOI: 10.1145/3613905.3651007
\\ ( https://arxiv.org/abs/2402.16611 ,  184kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18700
replaced with revised version Tue, 2 Apr 2024 02:38:31 GMT   (1215kb,D)

Title: Learning to Compress Prompt in Natural Language Formats
Authors: Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen,
  Xia Hu
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.18700 ,  1215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04798
replaced with revised version Tue, 2 Apr 2024 14:52:37 GMT   (6784kb,D)

Title: JMI at SemEval 2024 Task 3: Two-step approach for multimodal ECAC using
  in-context learning with GPT and instruction-tuned Llama models
Authors: Arefa, Mohammed Abbas Ansari, Chandni Saxena, Tanvir Ahmad
Categories: cs.CL cs.LG
Comments: Paper Accepted at SemEval 2024
\\ ( https://arxiv.org/abs/2403.04798 ,  6784kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09057
replaced with revised version Mon, 1 Apr 2024 22:48:56 GMT   (3528kb,D)

Title: A Continued Pretrained LLM Approach for Automatic Medical Note
  Generation
Authors: Dong Yuan, Eti Rastogi, Gautam Naik, Sree Prasanna Rajagopal, Sagar
  Goyal, Fen Zhao, Bharath Chintagunta, Jeff Ward
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2403.09057 ,  3528kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09516
replaced with revised version Tue, 2 Apr 2024 14:14:24 GMT   (9877kb,D)

Title: Leveraging Prototypical Representations for Mitigating Social Bias
  without Demographic Information
Authors: Shadi Iskander, Kira Radinsky, Yonatan Belinkov
Categories: cs.CL cs.CY cs.LG
\\ ( https://arxiv.org/abs/2403.09516 ,  9877kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11896
replaced with revised version Tue, 2 Apr 2024 10:07:39 GMT   (154kb,D)

Title: Investigating Markers and Drivers of Gender Bias in Machine Translations
Authors: Peter J Barclay and Ashkan Sami (Edinburgh Napier University)
Categories: cs.CL cs.CY cs.SE
Comments: This work has been accepted to SANER 2024;
  https://conf.researchr.org/home/saner-2024 REVISION: Some minor enhancements
  have been made to references. Wording in the Acknowledgement section has been
  clarified, and slightly abridged to allow the paper to fit again onto an even
  number of pages. In all versions, the body text, sections I - VI, is
  identical
\\ ( https://arxiv.org/abs/2403.11896 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11904
replaced with revised version Tue, 2 Apr 2024 10:25:34 GMT   (8048kb,D)

Title: CICLe: Conformal In-Context Learning for Largescale Multi-Class Food
  Risk Classification
Authors: Korbinian Randl, John Pavlopoulos, Aron Henriksson, Tony Lindgren
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.11904 ,  8048kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13485
replaced with revised version Tue, 2 Apr 2024 12:31:27 GMT   (530kb,D)

Title: An Entropy-based Text Watermarking Detection Method
Authors: Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King
Categories: cs.CL
Comments: 8 pages, 5 figures, submitted to ARR Feb 2024
\\ ( https://arxiv.org/abs/2403.13485 ,  530kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16792
replaced with revised version Tue, 2 Apr 2024 08:17:12 GMT   (4732kb,D)

Title: Iterative Refinement of Project-Level Code Context for Precise Code
  Generation with Compiler Feedback
Authors: Zhangqian Bi, Yao Wan, Zheng Wang, Hongyu Zhang, Batu Guan, Fangxin
  Lu, Zili Zhang, Yulei Sui, Xuanhua Shi, Hai Jin
Categories: cs.CL cs.SE
\\ ( https://arxiv.org/abs/2403.16792 ,  4732kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17407
replaced with revised version Tue, 2 Apr 2024 04:15:36 GMT   (2083kb,D)

Title: Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens
Authors: S M Jishanul Islam, Sadia Ahmmed and Sahid Hossain Mustakim
Categories: cs.CL cs.AI cs.LG
Comments: Updated missing references to the dataset and corrected some
  sentences in sections 1 and 2. This work became the champion of the Bhashamul
  challenge
ACM-class: F.2.2; I.2.7
\\ ( https://arxiv.org/abs/2403.17407 ,  2083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17516
replaced with revised version Tue, 2 Apr 2024 12:05:41 GMT   (8628kb,D)

Title: MapGuide: A Simple yet Effective Method to Reconstruct Continuous
  Language from Brain Activities
Authors: Xinpei Zhao, Jingyuan Sun, Shaonan Wang, Jing Ye, Xiaohan Zhang,
  Chengqing Zong
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 main conference
\\ ( https://arxiv.org/abs/2403.17516 ,  8628kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17860
replaced with revised version Tue, 2 Apr 2024 12:25:57 GMT   (286kb,D)

Title: Exploring LLMs as a Source of Targeted Synthetic Textual Data to
  Minimize High Confidence Misclassifications
Authors: Philip Lippmann, Matthijs T.J. Spaan, Jie Yang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.17860 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18105
replaced with revised version Mon, 1 Apr 2024 18:47:45 GMT   (2691kb,D)

Title: Large Language Models for Education: A Survey and Outlook
Authors: Shen Wang, Tianlong Xu, Hang Li, Chaoli Zhang, Joleen Liang, Jiliang
  Tang, Philip S. Yu, Qingsong Wen
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.18105 ,  2691kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18802
replaced with revised version Mon, 1 Apr 2024 21:02:37 GMT   (238kb,D)

Title: Long-form factuality in large language models
Authors: Jerry Wei and Chengrun Yang and Xinying Song and Yifeng Lu and Nathan
  Hu and Jie Huang and Dustin Tran and Daiyi Peng and Ruibo Liu and Da Huang
  and Cosmo Du and Quoc V. Le
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.18802 ,  238kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18933
replaced with revised version Tue, 2 Apr 2024 09:26:00 GMT   (5257kb,D)

Title: SemEval Task 1: Semantic Textual Relatedness for African and Asian
  Languages
Authors: Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris
  Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir
  Araujo, Meriem Beloucif, Christine De Kock, Oumaima Hourrane, Manish
  Shrivastava, Thamar Solorio, Nirmal Surange, Krishnapriya Vishnubhotla, Seid
  Muhie Yimam, Saif M. Mohammad
Categories: cs.CL
Comments: SemEval 2024 Task Description Paper. arXiv admin note: text overlap
  with arXiv:2402.08638
\\ ( https://arxiv.org/abs/2403.18933 ,  5257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19275
replaced with revised version Tue, 2 Apr 2024 10:59:23 GMT   (5155kb,D)

Title: Knowledge Boundary and Persona Dynamic Shape A Better Social Media Agent
Authors: Junkai Zhou, Liang Pang, Ya Jing, Jia Gu, Huawei Shen, Xueqi Cheng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.19275 ,  5155kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19399
replaced with revised version Tue, 2 Apr 2024 12:08:25 GMT   (54kb,D)

Title: KazParC: Kazakh Parallel Corpus for Machine Translation
Authors: Rustem Yeshpanov, Alina Polonskaya, Huseyin Atakan Varol
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.19399 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00511
replaced with revised version Tue, 2 Apr 2024 16:46:24 GMT   (3057kb,D)

Title: MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in
  Conversations with Multimodal Language Models
Authors: Zebang Cheng, Fuqiang Niu, Yuxiang Lin, Zhi-Qi Cheng, Bowen Zhang,
  Xiaojiang Peng
Categories: cs.CL cs.CV cs.MM
Comments: Ranked 3rd in SemEval '24 Task 3 with F1 of 0.3435, close to 1st &
  2nd by 0.0339 & 0.0025
\\ ( https://arxiv.org/abs/2404.00511 ,  3057kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01232
replaced with revised version Tue, 2 Apr 2024 15:03:33 GMT   (405kb,D)

Title: Open-Vocabulary Federated Learning with Multimodal Prototyping
Authors: Huimin Zeng, Zhenrui Yue, Dong Wang
Categories: cs.CL cs.CV
Comments: Accepted at NAACL 2024
\\ ( https://arxiv.org/abs/2404.01232 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2010.08755
replaced with revised version Tue, 2 Apr 2024 02:09:15 GMT   (7614kb,D)

Title: Variational Dynamic for Self-Supervised Exploration in Deep
  Reinforcement Learning
Authors: Chenjia Bai, Peng Liu, Kaiyu Liu, Lingxiao Wang, Yingnan Zhao, Lei Han
Categories: cs.LG cs.CV cs.RO
Comments: IEEE Transactions on Neural Networks and Learning Systems (TNNLS)
  2021
DOI: 10.1109/TNNLS.2021.3129160
\\ ( https://arxiv.org/abs/2010.08755 ,  7614kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00654
replaced with revised version Tue, 2 Apr 2024 06:58:50 GMT   (6715kb,D)

Title: Distributional Drift Adaptation with Temporal Conditional Variational
  Autoencoder for Multivariate Time Series Forecasting
Authors: Hui He, Qi Zhang, Kun Yi, Kaize Shi, Zhendong Niu and Longbing Cao
Categories: cs.LG
Comments: 16 pages, 7 figures, accepted by IEEE Transactions on Neural Networks
  and Learning Systems (TNNLS)
MSC-class: 68Txx
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2209.00654 ,  6715kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07397
replaced with revised version Mon, 1 Apr 2024 19:28:12 GMT   (308kb,D)

Title: From algorithms to action: improving patient care requires causality
Authors: Wouter A.C. van Amsterdam, Pim A. de Jong, Joost J.C. Verhoeff, Tim
  Leiner, Rajesh Ranganath
Categories: cs.LG cs.CY stat.ML
\\ ( https://arxiv.org/abs/2209.07397 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01301
replaced with revised version Tue, 2 Apr 2024 13:52:53 GMT   (103kb)

Title: GIDN: A Lightweight Graph Inception Diffusion Network for High-efficient
  Link Prediction
Authors: Zixiao Wang, Yuluo Guo, Jin Zhao, Yu Zhang, Hui Yu, Xiaofei Liao, Biao
  Wang, Ting Yu
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2210.01301 ,  103kb)
------------------------------------------------------------------------------
\\
arXiv:2210.12494
replaced with revised version Tue, 2 Apr 2024 06:37:59 GMT   (727kb,D)

Title: Learning The Likelihood Test With One-Class Classifiers
Authors: Francesco Ardizzon and Stefano Tomasin
Categories: cs.LG eess.SP stat.ML
Comments: 12 pages, 6 figure, submitted to IEEE Access
\\ ( https://arxiv.org/abs/2210.12494 ,  727kb)
------------------------------------------------------------------------------
\\
arXiv:2211.00713
replaced with revised version Tue, 2 Apr 2024 14:22:26 GMT   (30753kb,D)

Title: MAgNET: A Graph U-Net Architecture for Mesh-Based Simulations
Authors: Saurabh Deshpande, St\'ephane P.A. Bordas, Jakub Lengiewicz
Categories: cs.LG cs.CE
Journal-ref: Engineering Applications of Artificial Intelligence, Volume 133,
  Part B, 2024, 108055
DOI: 10.1016/j.engappai.2024.108055
\\ ( https://arxiv.org/abs/2211.00713 ,  30753kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09929
replaced with revised version Tue, 2 Apr 2024 00:44:45 GMT   (3125kb,D)

Title: Contrastive Credibility Propagation for Reliable Semi-Supervised
  Learning
Authors: Brody Kutt, Pralay Ramteke, Xavier Mignot, Pamela Toman, Nandini
  Ramanan, Sujit Rokka Chhetri, Shan Huang, Min Du, William Hewlett
Categories: cs.LG
Comments: Accepted to AAAI '24
\\ ( https://arxiv.org/abs/2211.09929 ,  3125kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04823
replaced with revised version Mon, 1 Apr 2024 22:51:21 GMT   (9078kb,D)

Title: Hierarchical Generative Adversarial Imitation Learning with Mid-level
  Input Generation for Autonomous Driving on Urban Environments
Authors: Gustavo Claudio Karl Couto and Eric Aislan Antonelo
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/2302.04823 ,  9078kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11068
replaced with revised version Mon, 1 Apr 2024 22:17:22 GMT   (65kb)

Title: Low Rank Matrix Completion via Robust Alternating Minimization in Nearly
  Linear Time
Authors: Yuzhou Gu, Zhao Song, Junze Yin, Lichen Zhang
Categories: cs.LG cs.DS math.OC stat.ML
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2302.11068 ,  65kb)
------------------------------------------------------------------------------
\\
arXiv:2303.01978
replaced with revised version Mon, 1 Apr 2024 20:01:10 GMT   (14813kb,D)

Title: Robust One-Class Classification with Signed Distance Function using
  1-Lipschitz Neural Networks
Authors: Louis Bethune, Paul Novello, Thibaut Boissin, Guillaume Coiffier,
  Mathieu Serrurier, Quentin Vincenot, Andres Troya-Galvis
Categories: cs.LG
Comments: 27 pages, 11 figures, International Conference on Machine Learning
  2023, (ICML 2023)
\\ ( https://arxiv.org/abs/2303.01978 ,  14813kb)
------------------------------------------------------------------------------
\\
arXiv:2303.02311
replaced with revised version Tue, 2 Apr 2024 14:34:37 GMT   (2170kb,D)

Title: Traffic State Estimation from Vehicle Trajectories with Anisotropic
  Gaussian Processes
Authors: Fan Wu, Zhanhong Cheng, Huiyu Chen, Tony Z. Qiu, and Lijun Sun
Categories: cs.LG stat.AP
\\ ( https://arxiv.org/abs/2303.02311 ,  2170kb)
------------------------------------------------------------------------------
\\
arXiv:2303.07557
replaced with revised version Tue, 2 Apr 2024 17:01:05 GMT   (3587kb,D)

Title: Lifelong Continual Learning for Anomaly Detection: New Challenges,
  Perspectives, and Insights
Authors: Kamil Faber, Roberto Corizzo, Bartlomiej Sniezynski, Nathalie
  Japkowicz
Categories: cs.LG cs.AI
Journal-ref: IEEE Access, vol. 12, pp. 41364-41380, 2024
DOI: 10.1109/ACCESS.2024.3377690
\\ ( https://arxiv.org/abs/2303.07557 ,  3587kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14304
replaced with revised version Mon, 1 Apr 2024 22:30:22 GMT   (464kb,D)

Title: Solving Attention Kernel Regression Problem via Pre-conditioner
Authors: Zhao Song, Junze Yin, Lichen Zhang
Categories: cs.LG
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2308.14304 ,  464kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13775
replaced with revised version Mon, 1 Apr 2024 22:59:31 GMT   (7936kb,D)

Title: The Rashomon Importance Distribution: Getting RID of Unstable, Single
  Model-based Variable Importance
Authors: Jon Donnelly, Srikar Katta, Cynthia Rudin, Edward P. Browne
Categories: cs.LG q-bio.GN stat.ML
Comments: Appeared in NeurIPS 2023 as a spotlight paper
\\ ( https://arxiv.org/abs/2309.13775 ,  7936kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00429
replaced with revised version Tue, 2 Apr 2024 14:09:40 GMT   (9565kb,D)

Title: On the Stability of Iterative Retraining of Generative Models on their
  own Data
Authors: Quentin Bertrand, Avishek Joey Bose, Alexandre Duplessis, Marco
  Jiralerspong, and Gauthier Gidel
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.00429 ,  9565kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04948
replaced with revised version Tue, 2 Apr 2024 04:39:08 GMT   (2670kb,D)

Title: TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series
  Forecasting
Authors: Defu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen
  Ye, Yan Liu
Categories: cs.LG cs.CL
Comments: Accepted by ICLR 2024. Camera Ready Version
\\ ( https://arxiv.org/abs/2310.04948 ,  2670kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05169
replaced with revised version Mon, 1 Apr 2024 19:57:59 GMT   (1853kb,D)

Title: Investigating the Ability of PINNs To Solve Burgers' PDE Near
  Finite-Time BlowUp
Authors: Dibyakanti Kumar, Anirbit Mukherjee
Categories: cs.LG cs.NA math.AP math.NA
\\ ( https://arxiv.org/abs/2310.05169 ,  1853kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08049
replaced with revised version Tue, 2 Apr 2024 01:54:53 GMT   (7347kb,D)

Title: Is attention required for ICL? Exploring the Relationship Between Model
  Architecture and In-Context Learning Ability
Authors: Ivan Lee, Nan Jiang, Taylor Berg-Kirkpatrick
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.08049 ,  7347kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01483
replaced with revised version Tue, 2 Apr 2024 13:53:20 GMT   (43788kb,D)

Title: FedSN: A Novel Federated Learning Framework over LEO Satellite Networks
Authors: Zheng Lin, Zhe Chen, Zihan Fang, Xianhao Chen, Xiong Wang, and Yue Gao
Categories: cs.LG cs.AI cs.DC
Comments: 14 pages, 17 figures
\\ ( https://arxiv.org/abs/2311.01483 ,  43788kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03564
replaced with revised version Mon, 1 Apr 2024 18:26:36 GMT   (50kb)

Title: Low-Rank MDPs with Continuous Action Spaces
Authors: Andrew Bennett, Nathan Kallus, Miruna Oprescu
Categories: cs.LG cs.AI stat.ML
Comments: 25 pages, AISTATS 2024
Journal-ref: PMLR, Volume 238, 2024
\\ ( https://arxiv.org/abs/2311.03564 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18817
replaced with revised version Tue, 2 Apr 2024 05:43:18 GMT   (181kb,D)

Title: Dichotomy of Early and Late Phase Implicit Biases Can Provably Induce
  Grokking
Authors: Kaifeng Lyu, Jikai Jin, Zhiyuan Li, Simon S. Du, Jason D. Lee, Wei Hu
Categories: cs.LG cs.AI
Comments: Published as a conference paper at ICLR 2024; 40 pages, 4 figures
\\ ( https://arxiv.org/abs/2311.18817 ,  181kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01995
replaced with revised version Tue, 2 Apr 2024 03:10:33 GMT   (8230kb,D)

Title: Online Uniform Risk Times Sampling: First Approximation Algorithms,
  Learning Augmentation with Full Confidence Interval Integration
Authors: Xueqing Liu, Kyra Gan, Esmaeil Keyvanshokooh, Susan Murphy
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2402.01995 ,  8230kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04012
replaced with revised version Mon, 1 Apr 2024 20:26:01 GMT   (247kb,D)

Title: Temporal Cross-Attention for Dynamic Embedding and Tokenization of
  Multimodal Electronic Health Records
Authors: Yingbo Ma, Suraj Kolla, Dhruv Kaliraman, Victoria Nolan, Zhenhong Hu,
  Ziyuan Guan, Yuanfang Ren, Brooke Armfield, Tezcan Ozrazgat-Baslanti, Tyler
  J. Loftus, Parisa Rashidi, Azra Bihorac, Benjamin Shickel
Categories: cs.LG
Comments: ICLR 2024 Workshop on Learning From Time Series for Health. 10 pages,
  3 figures
\\ ( https://arxiv.org/abs/2403.04012 ,  247kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07503
replaced with revised version Tue, 2 Apr 2024 11:20:22 GMT   (2773kb,D)

Title: Constrained Optimal Fuel Consumption of HEV: A Constrained Reinforcement
  Learning Approach
Authors: Shuchang Yan
Categories: cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2403.07503 ,  2773kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10842
replaced with revised version Mon, 1 Apr 2024 18:37:10 GMT   (692kb)

Title: Twin Transformer using Gated Dynamic Learnable Attention mechanism for
  Fault Detection and Diagnosis in the Tennessee Eastman Process
Authors: Mohammad Ali Labbaf-Khaniki, Mohammad Manthouri, Hanieh Ajami
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.10842 ,  692kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11144
replaced with revised version Tue, 2 Apr 2024 09:03:37 GMT   (442kb,D)

Title: Is Mamba Effective for Time Series Forecasting?
Authors: Zihan Wang and Fanheng Kong and Shi Feng and Ming Wang and Han Zhao
  and Daling Wang and Yifei Zhang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.11144 ,  442kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14735
replaced with revised version Tue, 2 Apr 2024 12:38:02 GMT   (2112kb,D)

Title: Foundation Models for Time Series Analysis: A Tutorial and Survey
Authors: Yuxuan Liang, Haomin Wen, Yuqi Nie, Yushan Jiang, Ming Jin, Dongjin
  Song, Shirui Pan, Qingsong Wen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.14735 ,  2112kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16591
replaced with revised version Tue, 2 Apr 2024 14:28:06 GMT   (1243kb,D)

Title: Deciphering the Interplay between Local Differential Privacy, Average
  Bayesian Privacy, and Maximum Bayesian Privacy
Authors: Xiaojin Zhang, Yulin Fei, Wei Chen
Categories: cs.LG cs.AI cs.CR
\\ ( https://arxiv.org/abs/2403.16591 ,  1243kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16930
replaced with revised version Tue, 2 Apr 2024 13:33:06 GMT   (525kb,D)

Title: FLIGAN: Enhancing Federated Learning with Incomplete Data using GAN
Authors: Paul Joe Maliakel, Shashikant Ilager, Ivona Brandic
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.16930 ,  525kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17637
replaced with revised version Tue, 2 Apr 2024 12:17:30 GMT   (2847kb,D)

Title: PeersimGym: An Environment for Solving the Task Offloading Problem with
  Reinforcement Learning
Authors: Frederico Metelo, Stevo Rackovi\'c, Pedro \'Akos Costa, Cl\'audia
  Soares
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.17637 ,  2847kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17886
replaced with revised version Tue, 2 Apr 2024 17:48:54 GMT   (4072kb,D)

Title: Neural Embedding Compression For Efficient Multi-Task Earth Observation
  Modelling
Authors: Carlos Gomes and Thomas Brunschwiler
Categories: cs.LG
Comments: Published at IGARSS 2024
\\ ( https://arxiv.org/abs/2403.17886 ,  4072kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19837
replaced with revised version Mon, 1 Apr 2024 22:34:37 GMT   (1929kb,D)

Title: Concept-based Analysis of Neural Networks via Vision-Language Models
Authors: Ravi Mangal, Nina Narodytska, Divya Gopinath, Boyue Caroline Hu,
  Anirban Roy, Susmit Jha, Corina Pasareanu
Categories: cs.LG cs.AI cs.CL cs.CV cs.LO
\\ ( https://arxiv.org/abs/2403.19837 ,  1929kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00228
replaced with revised version Tue, 2 Apr 2024 01:16:20 GMT   (4339kb,D)

Title: InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
Authors: Yan-Shuo Liang, Wu-Jun Li
Categories: cs.LG cs.AI cs.CV
Comments: Accepted by the 2024 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR 2024)
\\ ( https://arxiv.org/abs/2404.00228 ,  4339kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00462
replaced with revised version Tue, 2 Apr 2024 17:39:26 GMT   (3285kb,D)

Title: Zero-shot Safety Prediction for Autonomous Robots with Foundation World
  Models
Authors: Zhenjiang Mao, Siqi Dai, Yuang Geng and Ivan Ruchkin
Categories: cs.LG cs.RO
\\ ( https://arxiv.org/abs/2404.00462 ,  3285kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00521
replaced with revised version Tue, 2 Apr 2024 07:15:34 GMT   (6508kb,D)

Title: CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz
  continuity constrAIned Normalization
Authors: Yao Ni, Piotr Koniusz
Categories: cs.LG cs.CV
Comments: Accepted by CVPR2024, 26 pages full version
\\ ( https://arxiv.org/abs/2404.00521 ,  6508kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01041
replaced with revised version Tue, 2 Apr 2024 06:49:33 GMT   (1811kb,D)

Title: Can LLMs get help from other LLMs without revealing private information?
Authors: Florian Hartmann, Duc-Hieu Tran, Peter Kairouz, Victor C\u{a}rbune,
  Blaise Aguera y Arcas
Categories: cs.LG cs.AI cs.CR cs.MA
\\ ( https://arxiv.org/abs/2404.01041 ,  1811kb)
------------------------------------------------------------------------------
\\
arXiv:2208.08626 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 21:36:20 GMT   (20488kb,D)

Title: CP-PINNs: Data-Driven Changepoints Detection in PDEs Using Online
  Optimized Physics-Informed Neural Networks
Authors: Zhikang Dong, Pawel Polak
Categories: stat.ML cs.AI cs.LG cs.NA math.DS math.NA
\\ ( https://arxiv.org/abs/2208.08626 ,  20488kb)
------------------------------------------------------------------------------
\\
arXiv:2212.08546 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 23:15:00 GMT   (305kb,D)

Title: Estimating truncation effects of quantum bosonic systems using sampling
  algorithms
Authors: Masanori Hanada, Junyu Liu, Enrico Rinaldi, Masaki Tezuka
Categories: quant-ph cs.AI cs.LG hep-lat hep-th
Comments: 22 pages, 4 figures
Report-no: RIKEN-iTHEMS-Report-23
Journal-ref: Mach. Learn.: Sci. Technol. 4 045021, 2023
DOI: 10.1088/2632-2153/ad035c
\\ ( https://arxiv.org/abs/2212.08546 ,  305kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13418
replaced with revised version Tue, 2 Apr 2024 11:03:02 GMT   (21012kb,D)

Title: BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete
  Annotations
Authors: Yuanhong Chen, Yuyuan Liu, Chong Wang, Michael Elliott, Chun Fung
  Kwok, Carlos Pena-Solorzano, Yu Tian, Fengbei Liu, Helen Frazer, Davis J.
  McCarthy, Gustavo Carneiro
Categories: cs.CV cs.AI cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2301.13418 ,  21012kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13447
replaced with revised version Mon, 1 Apr 2024 20:18:04 GMT   (4693kb,D)

Title: Benchmarking Model Predictive Control Algorithms in Building
  Optimization Testing Framework (BOPTEST)
Authors: Saman Mostafavi and Chihyeon Song and Aayushman Sharma and Raman Goyal
  and Alejandro Brito
Categories: eess.SY cs.AI cs.LG cs.SY
Journal-ref: Proceedings of the 18th IBPSA Conference, Shanghai, China, Sept.
  4-6, 2023
DOI: 10.26868/25222708.2023.1371
\\ ( https://arxiv.org/abs/2301.13447 ,  4693kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10827
replaced with revised version Tue, 2 Apr 2024 02:54:04 GMT   (3792kb,D)

Title: Generative AI in the Wild: Prospects, Challenges, and Strategies
Authors: Yuan Sun, Eunchae Jang, Fenglong Ma, Ting Wang
Categories: cs.HC cs.AI
DOI: 10.1145/3613904.3642160
\\ ( https://arxiv.org/abs/2302.10827 ,  3792kb)
------------------------------------------------------------------------------
\\
arXiv:2304.02595 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 12:38:25 GMT   (4665kb,D)

Title: Bayesian neural networks via MCMC: a Python-based tutorial
Authors: Rohitash Chandra, Royce Chen, Joshua Simmons
Categories: stat.ML cs.AI cs.LG stat.CO
\\ ( https://arxiv.org/abs/2304.02595 ,  4665kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10550
replaced with revised version Mon, 1 Apr 2024 22:57:46 GMT   (2342kb,D)

Title: Deep transfer learning for intrusion detection in industrial control
  networks: A comprehensive review
Authors: Hamza Kheddar, Yassine Himeur, Ali Ismail Awad
Categories: cs.CR cs.AI cs.LG cs.NI cs.SY eess.SY
Journal-ref: Journal of Network and Computer Applications, Elsevier, 2023
DOI: 10.1016/j.jnca.2023.103760
\\ ( https://arxiv.org/abs/2304.10550 ,  2342kb)
------------------------------------------------------------------------------
\\
arXiv:2305.05726
replaced with revised version Tue, 2 Apr 2024 09:52:41 GMT   (18822kb,D)

Title: Vision-Language Models in Remote Sensing: Current Progress and Future
  Trends
Authors: Xiang Li, Congcong Wen, Yuan Hu, Zhenghang Yuan, Xiao Xiang Zhu
Categories: cs.CV cs.AI
Comments: Accepted by IEEE Geoscience and Remote Sensing Magazine
\\ ( https://arxiv.org/abs/2305.05726 ,  18822kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18607
replaced with revised version Tue, 2 Apr 2024 00:48:11 GMT   (943kb,D)

Title: How Effective Are Neural Networks for Fixing Security Vulnerabilities
Authors: Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin
  Tan, Petr Babkin, Sameena Shah
Categories: cs.SE cs.AI cs.CR
Comments: This paper was accepted in the proceedings of the 32nd ACM SIGSOFT
  International Symposium on Software Testing and Analysis (ISSTA 2023), and
  was presented at the conference, that was held in Seattle, USA, 17-21 July
  2023
DOI: 10.1145/3597926.3598135
\\ ( https://arxiv.org/abs/2305.18607 ,  943kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06077
replaced with revised version Tue, 2 Apr 2024 16:19:22 GMT   (4634kb,D)

Title: Semantically-Prompted Language Models Improve Visual Descriptions
Authors: Michael Ogezi, Bradley Hauer, Grzegorz Kondrak
Categories: cs.CV cs.AI cs.CL
Comments: To appear at NAACL 2024
\\ ( https://arxiv.org/abs/2306.06077 ,  4634kb)
------------------------------------------------------------------------------
\\
arXiv:2306.06189
replaced with revised version Mon, 1 Apr 2024 19:14:25 GMT   (1468kb,D)

Title: FasterViT: Fast Vision Transformers with Hierarchical Attention
Authors: Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, Jose M.
  Alvarez, Jan Kautz, Pavlo Molchanov
Categories: cs.CV cs.AI cs.LG
Comments: ICLR'24 Accepted Paper
\\ ( https://arxiv.org/abs/2306.06189 ,  1468kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05385 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 08:31:51 GMT   (4049kb,D)

Title: Learned Kernels for Sparse, Interpretable, and Efficient Medical Time
  Series Processing
Authors: Sully F. Chen, Zhicheng Guo, Cheng Ding, Xiao Hu, Cynthia Rudin
Categories: eess.SP cs.AI cs.LG
Comments: 26 pages, 9 figures
\\ ( https://arxiv.org/abs/2307.05385 ,  4049kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01390
replaced with revised version Tue, 2 Apr 2024 05:20:01 GMT   (4865kb,D)

Title: Bridging the Projection Gap: Overcoming Projection Bias Through
  Parameterized Distance Learning
Authors: Chong Zhang, Mingyu Jin, Qinkai Yu, Haochen Xue, Shreyank N Gowda,
  Xiaobo Jin
Categories: cs.CV cs.AI
Comments: 18 pages, 9 figures
\\ ( https://arxiv.org/abs/2309.01390 ,  4865kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08730 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 13:35:59 GMT   (502kb,D)

Title: MusiLingo: Bridging Music and Text with Pre-trained Language Models for
  Music Captioning and Query Response
Authors: Zihao Deng, Yinghao Ma, Yudong Liu, Rongchen Guo, Ge Zhang, Wenhu
  Chen, Wenhao Huang, Emmanouil Benetos
Categories: eess.AS cs.AI cs.CL cs.MM cs.SD
Journal-ref: 2024 Annual Conference of the North American Chapter of the
  Association for Computational Linguistics
\\ ( https://arxiv.org/abs/2309.08730 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11653
replaced with revised version Tue, 2 Apr 2024 01:32:06 GMT   (1111kb,D)

Title: "It's a Fair Game", or Is It? Examining How Users Navigate Disclosure
  Risks and Benefits When Using LLM-Based Conversational Agents
Authors: Zhiping Zhang, Michelle Jia, Hao-Ping Lee, Bingsheng Yao, Sauvik Das,
  Ada Lerner, Dakuo Wang, Tianshi Li
Categories: cs.HC cs.AI cs.CR
Comments: 26 pages, 5 figures
DOI: 10.1145/3613904.3642385
\\ ( https://arxiv.org/abs/2309.11653 ,  1111kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09690
replaced with revised version Tue, 2 Apr 2024 06:02:07 GMT   (2683kb,D)

Title: Configuration Validation with Large Language Models
Authors: Xinyu Lian, Yinfang Chen, Runxiang Cheng, Jie Huang, Parth Thakkar,
  Minjia Zhang, Tianyin Xu
Categories: cs.SE cs.AI cs.OS
\\ ( https://arxiv.org/abs/2310.09690 ,  2683kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11807 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 10:06:26 GMT   (12608kb,D)

Title: Learning quantum properties from short-range correlations using
  multi-task networks
Authors: Ya-Dong Wu, Yan Zhu, Yuexuan Wang and Giulio Chiribella
Categories: quant-ph cs.AI
\\ ( https://arxiv.org/abs/2310.11807 ,  12608kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19654
replaced with revised version Tue, 2 Apr 2024 00:12:21 GMT   (9499kb,D)

Title: MCAD: Multi-teacher Cross-modal Alignment Distillation for efficient
  image-text retrieval
Authors: Youbo Lei, Feifei He, Chen Chen, Yingbin Mo, Si Jia Li, Defeng Xie,
  Haonan Lu
Categories: cs.CV cs.AI
Comments: Accepted by NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2310.19654 ,  9499kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03381
replaced with revised version Tue, 2 Apr 2024 06:31:59 GMT   (1094kb,D)

Title: Separating and Learning Latent Confounders to Enhancing User Preferences
  Modeling
Authors: Hangtong Xu and Yuanbo Xu and Yongjian Yang
Categories: cs.IR cs.AI cs.LG stat.ME
Comments: Accepted by DASFAA 2024
\\ ( https://arxiv.org/abs/2311.03381 ,  1094kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12304
replaced with revised version Tue, 2 Apr 2024 03:35:02 GMT   (1675kb,D)

Title: Discovering Effective Policies for Land-Use Planning with Neuroevolution
Authors: Risto Miikkulainen, Olivier Francon, Daniel Young, Elliot Meyerson,
  Clemens Schwingshackl, Jacob Bieker, Hugo Cunha, and Babak Hodjat
Categories: cs.NE cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.12304 ,  1675kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18403
replaced with revised version Tue, 2 Apr 2024 11:17:49 GMT   (2251kb,D)

Title: Corrupting Convolution-based Unlearnable Datasets with Pixel-based Image
  Transformations
Authors: Xianlong Wang, Shengshan Hu, Minghui Li, Zhifei Yu, Ziqi Zhou, Leo Yu
  Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2311.18403 ,  2251kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00057
replaced with revised version Tue, 2 Apr 2024 14:28:26 GMT   (10015kb,D)

Title: VA3: Virtually Assured Amplification Attack on Probabilistic Copyright
  Protection for Text-to-Image Generative Models
Authors: Xiang Li, Qianli Shen, Kenji Kawaguchi
Categories: cs.CR cs.AI cs.CV cs.MM
Comments: 18 pages, 9 figures. Accept to CVPR 2024
\\ ( https://arxiv.org/abs/2312.00057 ,  10015kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01855
replaced with revised version Tue, 2 Apr 2024 16:12:11 GMT   (33027kb,D)

Title: Modular Control Architecture for Safe Marine Navigation: Reinforcement
  Learning and Predictive Safety Filters
Authors: Aksel Vaaler and Svein Jostein Husa and Daniel Menges and Thomas
  Nakken Larsen and Adil Rasheed
Categories: cs.RO cs.AI
Comments: 15 pages, 15 figures
\\ ( https://arxiv.org/abs/2312.01855 ,  33027kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02139
replaced with revised version Mon, 1 Apr 2024 18:55:16 GMT   (45575kb,D)

Title: DiffiT: Diffusion Vision Transformers for Image Generation
Authors: Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, Arash Vahdat
Categories: cs.CV cs.AI cs.LG
Comments: Revised Tech report
\\ ( https://arxiv.org/abs/2312.02139 ,  45575kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02957
replaced with revised version Tue, 2 Apr 2024 05:12:10 GMT   (1065kb,D)

Title: Classification for everyone : Building geography agnostic models for
  fairer recognition
Authors: Akshat Jindal, Shreya Singh, Soham Gadgil
Categories: cs.CV cs.AI cs.CY cs.LG
Comments: typos corrected, references added
\\ ( https://arxiv.org/abs/2312.02957 ,  1065kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03517
replaced with revised version Tue, 2 Apr 2024 08:40:54 GMT   (28361kb,D)

Title: FRDiff : Feature Reuse for Universal Training-free Acceleration of
  Diffusion Models
Authors: Junhyuk So, Jungwon Lee, Eunhyeok Park
Categories: cs.CV cs.AI
Comments: Work in progress. Project page :
  https://jungwon-lee.github.io/Project_FRDiff/
\\ ( https://arxiv.org/abs/2312.03517 ,  28361kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10170
replaced with revised version Tue, 2 Apr 2024 17:25:57 GMT   (9702kb,D)

Title: UINav: A Practical Approach to Train On-Device Automation Agents
Authors: Wei Li, Fu-Lin Hsu, Will Bishop, Folawiyo Campbell-Ajala, Max Lin,
  Oriana Riva
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2312.10170 ,  9702kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16476
replaced with revised version Tue, 2 Apr 2024 13:25:04 GMT   (45663kb,D)

Title: SVGDreamer: Text Guided SVG Generation with Diffusion Model
Authors: Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024. project link:
  https://ximinng.github.io/SVGDreamer-project/
\\ ( https://arxiv.org/abs/2312.16476 ,  45663kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04728
replaced with revised version Tue, 2 Apr 2024 08:29:09 GMT   (12074kb,D)

Title: Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar
  Creation
Authors: Xiyi Chen, Marko Mihajlovic, Shaofei Wang, Sergey Prokudin, Siyu Tang
Categories: cs.CV cs.AI
Comments: [CVPR 2024] Project page:
  https://xiyichen.github.io/morphablediffusion/
\\ ( https://arxiv.org/abs/2401.04728 ,  12074kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09286
replaced with revised version Tue, 2 Apr 2024 17:18:19 GMT   (6457kb,D)

Title: Deployable Reinforcement Learning with Variable Control Rate
Authors: Dong Wang and Giovanni Beltrame
Categories: cs.RO cs.AI
Comments: Paper for AAAI-DAI 2024 workshop
\\ ( https://arxiv.org/abs/2401.09286 ,  6457kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00089
replaced with revised version Tue, 2 Apr 2024 10:05:33 GMT   (7821kb)

Title: SCAPE: Searching Conceptual Architecture Prompts using Evolution
Authors: Soo Ling Lim, Peter J Bentley, Fuyuki Ishikawa
Categories: cs.NE cs.AI
Comments: 8 pages
MSC-class: 68W50, 68T07
ACM-class: G.1.6; I.2.10
Journal-ref: IEEE Congress on Evolutionary Computation (IEEE World Congress on
  Computational Intelligence 2024), Yokohama, Japan
\\ ( https://arxiv.org/abs/2402.00089 ,  7821kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09871
replaced with revised version Tue, 2 Apr 2024 16:15:35 GMT   (16868kb,D)

Title: MuChin: A Chinese Colloquial Description Benchmark for Evaluating
  Language Models in the Field of Music
Authors: Zihao Wang, Shuyu Li, Tao Zhang, Qi Wang, Pengfei Yu, Jinyang Luo, Yan
  Liu, Ming Xi, Kejun Zhang
Categories: cs.SD cs.AI cs.MM eess.AS
MSC-class: 68Txx(Primary)14F05, 91Fxx(Secondary)
ACM-class: I.2.7; J.5
\\ ( https://arxiv.org/abs/2402.09871 ,  16868kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12393
replaced with revised version Tue, 2 Apr 2024 09:16:14 GMT   (67kb,D)

Title: On Automating Video Game Regression Testing by Planning and Learning
Authors: Tom\'a\v{s} Balyo, G. Michael Youngblood, Filip Dvo\v{r}\'ak,
  Luk\'a\v{s} Chrpa, Roman Bart\'ak
Categories: cs.HC cs.AI
\\ ( https://arxiv.org/abs/2402.12393 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17971
replaced with revised version Tue, 2 Apr 2024 09:32:51 GMT   (16671kb,D)

Title: All in an Aggregated Image for In-Image Learning
Authors: Lei Wang, Wanyu Xu, Zhiqiang Hu, Yihuai Lan, Shan Dong, Hao Wang, Roy
  Ka-Wei Lee, Ee-Peng Lim
Categories: cs.CV cs.AI cs.CL
Comments: Preprint
\\ ( https://arxiv.org/abs/2402.17971 ,  16671kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05030
replaced with revised version Mon, 1 Apr 2024 21:32:18 GMT   (1140kb,D)

Title: Defending Against Unforeseen Failure Modes with Latent Adversarial
  Training
Authors: Stephen Casper, Lennart Schulze, Oam Patel, Dylan Hadfield-Menell
Categories: cs.CR cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.05030 ,  1140kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07711
replaced with revised version Tue, 2 Apr 2024 06:38:18 GMT   (812kb,D)

Title: SSM Meets Video Diffusion Models: Efficient Video Generation with
  Structured State Spaces
Authors: Yuta Oshima, Shohei Taniguchi, Masahiro Suzuki, Yutaka Matsuo
Categories: cs.CV cs.AI
Comments: Accepted as a workshop paper at ICLR 2024
\\ ( https://arxiv.org/abs/2403.07711 ,  812kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07888
replaced with revised version Tue, 2 Apr 2024 14:47:23 GMT   (658kb,D)

Title: Cross-modality debiasing: using language to mitigate sub-population
  shifts in imaging
Authors: Yijiang Pang, Bao Hoang, Jiayu Zhou
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.07888 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10516
replaced with revised version Mon, 1 Apr 2024 20:57:45 GMT   (38877kb,D)

Title: FeatUp: A Model-Agnostic Framework for Features at Any Resolution
Authors: Stephanie Fu, Mark Hamilton, Laura Brandt, Axel Feldman, Zhoutong
  Zhang, William T. Freeman
Categories: cs.CV cs.AI cs.IR cs.LG
Comments: Accepted to the International Conference on Learning Representations
  (ICLR) 2024
\\ ( https://arxiv.org/abs/2403.10516 ,  38877kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11487
replaced with revised version Tue, 2 Apr 2024 04:27:55 GMT   (9669kb,D)

Title: Can LLMs Generate Human-Like Wayfinding Instructions? Towards
  Platform-Agnostic Embodied Instruction Synthesis
Authors: Vishnu Sashank Dorbala, Sanjoy Chowdhury, Dinesh Manocha
Categories: cs.RO cs.AI
Comments: 14 Pages
\\ ( https://arxiv.org/abs/2403.11487 ,  9669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13002
replaced with revised version Tue, 2 Apr 2024 09:38:05 GMT   (5251kb,D)

Title: AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models
Authors: Shuo Jiang, Jianxi Luo
Categories: cs.HC cs.AI cs.CL
Comments: 13pages, 6 figures
ACM-class: I.2.7; I.2.1
\\ ( https://arxiv.org/abs/2403.13002 ,  5251kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15769 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 13:16:46 GMT   (4036kb,D)

Title: FusionINN: Invertible Image Fusion for Brain Tumor Monitoring
Authors: Nishant Kumar, Ziyan Tao, Jaikirat Singh, Yang Li, Peiwen Sun, Binghui
  Zhao, Stefan Gumhold
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: Source code available at https://github.com/nish03/FusionINN
\\ ( https://arxiv.org/abs/2403.15769 ,  4036kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16003
replaced with revised version Tue, 2 Apr 2024 13:31:41 GMT   (560kb,D)

Title: Diverse Representation Embedding for Lifelong Person Re-Identification
Authors: Shiben Liu, Huijie Fan, Qiang Wang, Xiai Chen, Zhi Han, and Yandong
  Tang
Categories: cs.CV cs.AI
Comments: 11 pages,7 Tables,3 Figures
\\ ( https://arxiv.org/abs/2403.16003 ,  560kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16209
replaced with revised version Tue, 2 Apr 2024 01:57:00 GMT   (955kb)

Title: Image Captioning in news report scenario
Authors: Tianrui Liu, Qi Cai, Changxin Xu, Bo Hong, Jize Xiong, Yuxin Qiao, and
  Tsungwei Yang
Categories: cs.CV cs.AI
Comments: 10 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.16209 ,  955kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16578
replaced with revised version Tue, 2 Apr 2024 09:55:02 GMT   (3411kb,D)

Title: SegICL: A Universal In-context Learning Framework for Enhanced
  Segmentation in Medical Imaging
Authors: Lingdong Shen, Fangxin Shang, Yehui Yang, Xiaoshuang Huang, Shiming
  Xiang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.16578 ,  3411kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17012
replaced with revised version Tue, 2 Apr 2024 06:35:04 GMT   (426kb)

Title: Evolution and Efficiency in Neural Architecture Search: Bridging the Gap
  Between Expert Design and Automated Optimization
Authors: Fanfei Meng, Chen-Ao Wang, Lele Zhang
Categories: cs.NE cs.AI
Comments: 7 Pages, Double Column
Journal-ref: Journal of Mathematical Techniques and Computational Mathematics,
  2024, Volume 3, Issue 3
\\ ( https://arxiv.org/abs/2403.17012 ,  426kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18807
replaced with revised version Mon, 1 Apr 2024 18:26:22 GMT   (8994kb,D)

Title: ECoDepth: Effective Conditioning of Diffusion Models for Monocular Depth
  Estimation
Authors: Suraj Patni, Aradhye Agarwal, Chetan Arora
Categories: cs.CV cs.AI cs.LG
Comments: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
  2024
\\ ( https://arxiv.org/abs/2403.18807 ,  8994kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00026
replaced with revised version Tue, 2 Apr 2024 15:42:05 GMT   (761kb,D)

Title: Ink and Individuality: Crafting a Personalised Narrative in the Age of
  LLMs
Authors: Azmine Toushik Wasi and Raima Islam and Mst Rafia Islam
Categories: cs.HC cs.AI cs.CL cs.IR cs.LG
Comments: 4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive
  Writing Assistants, a hybrid event co-located with The ACM CHI Conference on
  Human Factors in Computing Systems (CHI 2024) Openreview:
  https://openreview.net/forum?id=Pwk5suNA6P
\\ ( https://arxiv.org/abs/2404.00026 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00027
replaced with revised version Tue, 2 Apr 2024 15:40:21 GMT   (629kb,D)

Title: LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership
  and Reasoning
Authors: Azmine Toushik Wasi and Mst Rafia Islam and Raima Islam
Categories: cs.HC cs.AI cs.CL cs.CY cs.LG
Comments: 4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive
  Writing Assistants, a hybrid event co-located with The ACM CHI Conference on
  Human Factors in Computing Systems (CHI 2024) Openreview:
  https://openreview.net/forum?id=wS0Or6FOyz
\\ ( https://arxiv.org/abs/2404.00027 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00600
replaced with revised version Tue, 2 Apr 2024 06:05:29 GMT   (21kb,D)

Title: AI Act and Large Language Models (LLMs): When critical issues and
  privacy impact require human and ethical oversight
Authors: Nicola Fabiano
Categories: cs.CY cs.AI cs.CL
\\ ( https://arxiv.org/abs/2404.00600 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00636
replaced with revised version Tue, 2 Apr 2024 11:31:50 GMT   (3860kb,D)

Title: Learning to Generate Conditional Tri-plane for 3D-aware Expression
  Controllable Portrait Animation
Authors: Taekyung Ki, Dongchan Min and Gyeongsu Chae
Categories: cs.CV cs.AI cs.MM
Comments: Project page: https://export3d.github.io
\\ ( https://arxiv.org/abs/2404.00636 ,  3860kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00675
replaced with revised version Tue, 2 Apr 2024 10:59:05 GMT   (10963kb,D)

Title: LLM meets Vision-Language Models for Zero-Shot One-Class Classification
Authors: Yassir Bendou, Giulia Lioi, Bastien Pasdeloup, Lukas Mauch, Ghouthi
  Boukli Hacene, Fabien Cardinaux and Vincent Gripon
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2404.00675 ,  10963kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00722
replaced with revised version Tue, 2 Apr 2024 13:15:36 GMT   (8107kb,D)

Title: DRCT: Saving Image Super-resolution away from Information Bottleneck
Authors: Chih-Chung Hsu, Chia-Ming Lee, and Yi-Shiuan Chou
Categories: cs.CV cs.AI
Comments: Submitted to NTIRE 2024
\\ ( https://arxiv.org/abs/2404.00722 ,  8107kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01030
replaced with revised version Tue, 2 Apr 2024 03:36:28 GMT   (62kb)

Title: Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and
  Mitigation
Authors: Yixin Wan, Arjun Subramonian, Anaelia Ovalle, Zongyu Lin, Ashima
  Suvarna, Christina Chance, Hritik Bansal, Rebecca Pattichis, Kai-Wei Chang
Categories: cs.CV cs.AI cs.CY
\\ ( https://arxiv.org/abs/2404.01030 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01258
replaced with revised version Tue, 2 Apr 2024 12:47:49 GMT   (3427kb,D)

Title: Direct Preference Optimization of Video Large Multimodal Models from
  Language Model Reward
Authors: Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu,
  Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and
  Yiming Yang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2404.01258 ,  3427kb)
------------------------------------------------------------------------------
\\
arXiv:2310.16781
replaced with revised version Tue, 2 Apr 2024 05:50:21 GMT   (4266kb,D)

Title: Kiki or Bouba? Sound Symbolism in Vision-and-Language Models
Authors: Morris Alper and Hadar Averbuch-Elor
Categories: cs.CV cs.CL cs.LG
Comments: Accepted to NeurIPS 2023 (spotlight). Project webpage:
  https://kiki-bouba.github.io/
\\ ( https://arxiv.org/abs/2310.16781 ,  4266kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09612
replaced with revised version Tue, 2 Apr 2024 00:11:50 GMT   (1114kb,D)

Title: Efficient End-to-End Visual Document Understanding with Rationale
  Distillation
Authors: Wang Zhu, Alekh Agarwal, Mandar Joshi, Robin Jia, Jesse Thomason,
  Kristina Toutanova
Categories: cs.CV cs.CL
Comments: Accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2311.09612 ,  1114kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10208
replaced with revised version Tue, 2 Apr 2024 09:20:50 GMT   (4471kb,D)

Title: MM-Interleaved: Interleaved Image-Text Generative Modeling via
  Multi-modal Feature Synchronizer
Authors: Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai
  Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, Hongsheng Li, Yu Qiao, Jifeng
  Dai
Categories: cs.CV cs.CL
Comments: 20 pages, 9 figures, 17 tables
\\ ( https://arxiv.org/abs/2401.10208 ,  4471kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15479
replaced with revised version Tue, 2 Apr 2024 15:28:07 GMT   (3565kb,D)

Title: Navigating the Post-API Dilemma | Search Engine Results Pages Present a
  Biased View of Social Media Data
Authors: Amrit Poudel, Tim Weninger
Categories: cs.IR cs.CL cs.SI
Comments: Proceedings of the ACM Web Conference 2024 (WWW '24)
\\ ( https://arxiv.org/abs/2401.15479 ,  3565kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12997
replaced with revised version Tue, 2 Apr 2024 12:56:53 GMT   (267kb,D)

Title: Towards Trustworthy Reranking: A Simple yet Effective Abstention
  Mechanism
Authors: Hippolyte Gisserot-Boukhlef, Manuel Faysse, Emmanuel Malherbe,
  C\'eline Hudelot, Pierre Colombo
Categories: cs.IR cs.CL
\\ ( https://arxiv.org/abs/2402.12997 ,  267kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02090
replaced with revised version Mon, 1 Apr 2024 20:03:38 GMT   (2373kb,D)

Title: Modeling Multimodal Social Interactions: New Challenges and Baselines
  with Densely Aligned Representations
Authors: Sangmin Lee, Bolin Lai, Fiona Ryan, Bikram Boote, James M. Rehg
Categories: cs.CV cs.CL cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2403.02090 ,  2373kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10576
replaced with revised version Tue, 2 Apr 2024 08:46:42 GMT   (1607kb,D)

Title: Ignore Me But Don't Replace Me: Utilizing Non-Linguistic Elements for
  Pretraining on the Cybersecurity Domain
Authors: Eugene Jang, Jian Cui, Dayeon Yim, Youngjin Jin, Jin-Woo Chung,
  Seungwon Shin, Yongjae Lee
Categories: cs.CR cs.CL cs.LG
Comments: To appear in NAACL Findings 2024
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2403.10576 ,  1607kb)
------------------------------------------------------------------------------
\\
arXiv:2204.06062 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 20:19:49 GMT   (1479kb,D)

Title: Local and global topological complexity measures OF ReLU neural network
  functions
Authors: J. Elisenda Grigsby and Kathryn Lindsey and Marissa Masden
Categories: math.AT cs.CG cs.LG math.GT
Comments: 40 pages, 8 figures; Sections 5 and 6 from v1 removed from v2. We
  plan to use constructions in those sections for a follow-up paper with a more
  computational focus
MSC-class: 57R70, 57Q99, 52B70, 52C35
\\ ( https://arxiv.org/abs/2204.06062 ,  1479kb)
------------------------------------------------------------------------------
\\
arXiv:2206.09563
replaced with revised version Mon, 1 Apr 2024 21:47:26 GMT   (21072kb,D)

Title: Scalable Distributed Algorithms for Size-Constrained Submodular
  Maximization in the MapReduce and Adaptive Complexity Models
Authors: Tonmoy Dey, Yixin Chen, Alan Kuhnle
Categories: cs.DS cs.DC cs.LG
Comments: 35 pages, 5 figures
\\ ( https://arxiv.org/abs/2206.09563 ,  21072kb)
------------------------------------------------------------------------------
\\
arXiv:2208.10851
replaced with revised version Tue, 2 Apr 2024 13:49:07 GMT   (3794kb,D)

Title: Bayesian Floor Field: Transferring people flow predictions across
  environments
Authors: Francesco Verdoja, Tomasz Piotr Kucner, Ville Kyrki
Categories: cs.RO cs.LG
Comments: Submitted to 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS)
\\ ( https://arxiv.org/abs/2208.10851 ,  3794kb)
------------------------------------------------------------------------------
\\
arXiv:2211.09321
replaced with revised version Tue, 2 Apr 2024 00:33:42 GMT   (26852kb,D)

Title: Interpretable Dimensionality Reduction by Feature Preserving Manifold
  Approximation and Projection
Authors: Yang Yang, Hongjian Sun, Jialei Gong, Di Yu
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2211.09321 ,  26852kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04320
replaced with revised version Tue, 2 Apr 2024 07:58:41 GMT   (562kb)

Title: A 65nm 8b-Activation 8b-Weight SRAM-Based Charge-Domain
  Computing-in-Memory Macro Using A Fully-Parallel Analog Adder Network and A
  Single-ADC Interface
Authors: Guodong Yin, Mufeng Zhou, Yiming Chen, Wenjun Tang, Zekun Yang,
  Mingyen Lee, Xirui Du, Jinshan Yue, Jiaxin Liu, Huazhong Yang, Yongpan Liu,
  Xueqing Li
Categories: cs.AR cs.LG
Comments: Accepted by IEEE 48th European Solid-State Circuits Conference
  (ESSCIRC 2022)
\\ ( https://arxiv.org/abs/2212.04320 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10180 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 15:40:16 GMT   (22483kb,D)

Title: Samplet basis pursuit: Multiresolution scattered data approximation with
  sparsity constraints
Authors: Davide Baroli, Helmut Harbrecht, and Michael Multerer
Categories: stat.ML cs.LG cs.NA math.NA
\\ ( https://arxiv.org/abs/2306.10180 ,  22483kb)
------------------------------------------------------------------------------
\\
arXiv:2309.15642 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 11:44:04 GMT   (5646kb,D)

Title: Efficient tensor network simulation of IBM's largest quantum processors
Authors: Siddhartha Patra, Saeed S. Jahromi, Sukhbinder Singh, Roman Orus
Categories: quant-ph cond-mat.str-el cs.CE cs.LG
Comments: 7 pages, 8 figures, revised version
Journal-ref: Phys. Rev. Research 6, 013326 (2024)
\\ ( https://arxiv.org/abs/2309.15642 ,  5646kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09852
replaced with revised version Tue, 2 Apr 2024 10:35:12 GMT   (1839kb,D)

Title: Short vs. Long-term Coordination of Drones: When Distributed
  Optimization Meets Deep Reinforcement Learning
Authors: Chuhao Qin and Evangelos Pournaras
Categories: cs.RO cs.LG cs.MA
Comments: 12 pages, 13 figures
\\ ( https://arxiv.org/abs/2311.09852 ,  1839kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16080 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 15:56:38 GMT   (7148kb,D)

Title: XLB: A differentiable massively parallel lattice Boltzmann library in
  Python
Authors: Mohammadmehdi Ataei and Hesam Salehipour
Categories: physics.comp-ph cs.CE cs.LG
DOI: 10.1016/j.cpc.2024.109187
\\ ( https://arxiv.org/abs/2311.16080 ,  7148kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12337
replaced with revised version Tue, 2 Apr 2024 17:23:16 GMT   (9304kb,D)

Title: pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable
  Generalizable 3D Reconstruction
Authors: David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann
Categories: cs.CV cs.LG
Comments: Project page: https://dcharatan.github.io/pixelsplat
\\ ( https://arxiv.org/abs/2312.12337 ,  9304kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02283
replaced with revised version Mon, 1 Apr 2024 18:27:34 GMT   (695kb,D)

Title: DEM: A Method for Certifying Deep Neural Network Classifier Outputs in
  Aerospace
Authors: Guy Katz, Natan Levy, Idan Refaeli and Raz Yerushalmi
Categories: cs.SE cs.LG
Comments: 15 pages
\\ ( https://arxiv.org/abs/2401.02283 ,  695kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03862 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 18:09:08 GMT   (3480kb,D)

Title: End-to-End Crystal Structure Prediction from Powder X-Ray Diffraction
Authors: Qingsi Lai, Lin Yao, Zhifeng Gao, Siyuan Liu, Hongshuai Wang, Shuqi
  Lu, Di He, Liwei Wang, Cheng Wang and Guolin Ke
Categories: physics.chem-ph cs.LG
\\ ( https://arxiv.org/abs/2401.03862 ,  3480kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10375
replaced with revised version Tue, 2 Apr 2024 01:31:24 GMT   (1666kb,D)

Title: Vulnerabilities of Foundation Model Integrated Federated Learning Under
  Adversarial Threats
Authors: Chen Wu, Xi Li, Jiaqi Wang
Categories: cs.CR cs.DC cs.LG
Comments: Chen Wu and Xi Li are equal contribution. The corresponding author is
  Jiaqi Wang
\\ ( https://arxiv.org/abs/2401.10375 ,  1666kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12362 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 17:30:38 GMT   (882kb,D)

Title: VC dimension of Graph Neural Networks with Pfaffian activation functions
Authors: Giuseppe Alessio D'Inverno, Monica Bianchini, Franco Scarselli
Categories: stat.ML cs.LG
Comments: 35 pages, 9 figures
\\ ( https://arxiv.org/abs/2401.12362 ,  882kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01542 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 20:11:07 GMT   (10158kb,D)

Title: Learning Collective Variables with Synthetic Data Augmentation through
  Physics-inspired Geodesic Interpolation
Authors: Soojung Yang, Juno Nam, Johannes C. B. Dietschreit, Rafael
  G\'omez-Bombarelli
Categories: physics.chem-ph cs.LG q-bio.BM
\\ ( https://arxiv.org/abs/2402.01542 ,  10158kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04498 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 21:58:10 GMT   (1019kb,D)

Title: Pathspace Kalman Filters with Dynamic Process Uncertainty for Analyzing
  Time-course Data
Authors: Chaitra Agrahar, William Poole, Simone Bianco, Hana El-Samad
Categories: stat.ML cs.LG q-bio.QM
Comments: 32 pages, 8 figures, Submitted for review
\\ ( https://arxiv.org/abs/2402.04498 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11425 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 02:36:56 GMT   (330kb,D)

Title: Online Local False Discovery Rate Control: A Resource Allocation
  Approach
Authors: Ruicheng Ao, Hongyu Chen, David Simchi-Levi, Feng Zhu
Categories: stat.ME cs.LG math.OC math.PR
\\ ( https://arxiv.org/abs/2402.11425 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11705 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 03:04:09 GMT   (2706kb,D)

Title: Learning Memory Kernels in Generalized Langevin Equations
Authors: Quanjun Lang, Jianfeng Lu
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.11705 ,  2706kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14961
replaced with revised version Tue, 2 Apr 2024 14:02:07 GMT   (7664kb,D)

Title: Reinforcement Learning with Elastic Time Steps
Authors: Dong Wang and Giovanni Beltrame
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2402.14961 ,  7664kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03849 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 17:11:45 GMT   (3632kb,D)

Title: MedMamba: Vision Mamba for Medical Image Classification
Authors: Yubiao Yue, Zhenzhang Li
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.03849 ,  3632kb)
------------------------------------------------------------------------------
\\
arXiv:2403.03954
replaced with revised version Tue, 2 Apr 2024 08:59:57 GMT   (27492kb,D)

Title: 3D Diffusion Policy: Generalizable Visuomotor Policy Learning via Simple
  3D Representations
Authors: Yanjie Ze, Gu Zhang, Kangning Zhang, Chenyuan Hu, Muhan Wang, Huazhe
  Xu
Categories: cs.RO cs.CV cs.LG
Comments: Videos, code, and data: https://3d-diffusion-policy.github.io
\\ ( https://arxiv.org/abs/2403.03954 ,  27492kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10488
replaced with revised version Tue, 2 Apr 2024 15:34:04 GMT   (5682kb,D)

Title: Joint Multimodal Transformer for Emotion Recognition in the Wild
Authors: Paul Waligora, Haseeb Aslam, Osama Zeeshan, Soufiane Belharbi,
  Alessandro Lameiras Koerich, Marco Pedersoli, Simon Bacon, Eric Granger
Categories: cs.CV cs.LG cs.SD eess.AS
Comments: 10 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.10488 ,  5682kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19964
replaced with revised version Tue, 2 Apr 2024 02:34:22 GMT   (10605kb,D)

Title: FairRAG: Fair Human Generation via Fair Retrieval Augmentation
Authors: Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi
  Deng
Categories: cs.CV cs.CY cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2403.19964 ,  10605kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00015 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 05:54:55 GMT   (554kb,D)

Title: Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning
Authors: Javier Mancilla, Andr\'e Sequeira, Iraitz Montalb\'an, Tomas Tagliani,
  Francisco Llaneza, Claudio Beiza
Categories: q-fin.RM cs.LG q-fin.ST quant-ph stat.ML
Comments: Preprint
\\ ( https://arxiv.org/abs/2404.00015 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00640
replaced with revised version Tue, 2 Apr 2024 10:53:41 GMT   (31398kb,D)

Title: Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize
  Configuration Errors via Logs
Authors: Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li and Zibin Zheng
Categories: cs.SE cs.LG
Comments: 13 pages, accepted by ISSTA 2024 (The 33rd ACM SIGSOFT International
  Symposium on Software Testing and Analysis)
DOI: 10.1145/3650212.3652106
\\ ( https://arxiv.org/abs/2404.00640 ,  31398kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
