Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年4月10日 12:06
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Mon  8 Apr 24 18:00:00 GMT  to  Tue  9 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.05735
Date: Sun, 24 Mar 2024 16:00:16 GMT   (34kb)

Title: A Python Framework for Neutrosophic Sets and Mappings
Authors: Giorgio Nordo, Saeid Jafari, Arif Mehmood, Bhimraj Basumatary
Categories: cs.AI
Comments: 38 PAGES
MSC-class: 03E72, 08A72, 03B52
Journal-ref: Neutrosophic Sets and Systems 65, 2024
\\
  In this paper we present an open source framework developed in Python and
consisting of three distinct classes designed to manipulate in a simple and
intuitive way both symbolic representations of neutrosophic sets over universes
of various types as well as mappings between them. The capabilities offered by
this framework extend and generalize previous attempts to provide software
solutions to the manipulation of neutrosophic sets such as those proposed by
Salama et al., Saranya et al., El-Ghareeb, Topal et al. and Sleem. The code is
described in detail and many examples and use cases are also provided.
\\ ( https://arxiv.org/abs/2404.05735 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05893
Date: Mon, 8 Apr 2024 22:29:53 GMT   (572kb)

Title: Use of a Structured Knowledge Base Enhances Metadata Curation by Large
  Language Models
Authors: Sowmya S. Sundaram, Benjamin Solomon, Avani Khatri, Anisha Laumas,
  Purvesh Khatri, Mark A. Musen
Categories: cs.AI cs.CL cs.IR
\\
  Metadata play a crucial role in ensuring the findability, accessibility,
interoperability, and reusability of datasets. This paper investigates the
potential of large language models (LLMs), specifically GPT-4, to improve
adherence to metadata standards. We conducted experiments on 200 random data
records describing human samples relating to lung cancer from the NCBI
BioSample repository, evaluating GPT-4's ability to suggest edits for adherence
to metadata standards. We computed the adherence accuracy of field name-field
value pairs through a peer review process, and we observed a marginal average
improvement in adherence to the standard data dictionary from 79% to 80%
(p<0.01). We then prompted GPT-4 with domain information in the form of the
textual descriptions of CEDAR templates and recorded a significant improvement
to 97% from 79% (p<0.01). These results indicate that, while LLMs may not be
able to correct legacy metadata to ensure satisfactory adherence to standards
when unaided, they do show promise for use in automated metadata curation when
integrated with a structured knowledge base.
\\ ( https://arxiv.org/abs/2404.05893 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06325
Date: Tue, 9 Apr 2024 14:03:38 GMT   (1352kb,D)

Title: Automatically Learning HTN Methods from Landmarks
Authors: Ruoxi Li, Dana Nau, Mark Roberts, Morgan Fine-Morris
Categories: cs.AI
Comments: This work has been submitted to FLAIRS-24
\\
  Hierarchical Task Network (HTN) planning usually requires a domain engineer
to provide manual input about how to decompose a planning problem. Even
HTN-MAKER, a well-known method-learning algorithm, requires a domain engineer
to annotate the tasks with information about what to learn. We introduce
CURRICULAMA, an HTN method learning algorithm that completely automates the
learning process. It uses landmark analysis to compose annotated tasks and
leverages curriculum learning to order the learning of methods from simpler to
more complex. This eliminates the need for manual input, resolving a core issue
with HTN-MAKER. We prove CURRICULAMA's soundness, and show experimentally that
it has a substantially similar convergence rate in learning a complete set of
methods to HTN-MAKER.
\\ ( https://arxiv.org/abs/2404.06325 ,  1352kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06345
Date: Tue, 9 Apr 2024 14:33:16 GMT   (7436kb,D)

Title: AgentsCoDriver: Large Language Model Empowered Collaborative Driving
  with Lifelong Learning
Authors: Senkang Hu, Zhengru Fang, Zihan Fang, Xianhao Chen, Yuguang Fang
Categories: cs.AI cs.RO
\\
  Connected and autonomous driving is developing rapidly in recent years.
However, current autonomous driving systems, which are primarily based on
data-driven approaches, exhibit deficiencies in interpretability,
generalization, and continuing learning capabilities. In addition, the
single-vehicle autonomous driving systems lack of the ability of collaboration
and negotiation with other vehicles, which is crucial for the safety and
efficiency of autonomous driving systems. In order to address these issues, we
leverage large language models (LLMs) to develop a novel framework,
AgentsCoDriver, to enable multiple vehicles to conduct collaborative driving.
AgentsCoDriver consists of five modules: observation module, reasoning engine,
cognitive memory module, reinforcement reflection module, and communication
module. It can accumulate knowledge, lessons, and experiences over time by
continuously interacting with the environment, thereby making itself capable of
lifelong learning. In addition, by leveraging the communication module,
different agents can exchange information and realize negotiation and
collaboration in complex traffic environments. Extensive experiments are
conducted and show the superiority of AgentsCoDriver.
\\ ( https://arxiv.org/abs/2404.06345 ,  7436kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06370
Date: Tue, 9 Apr 2024 15:06:25 GMT   (1455kb)

Title: Enhancing Decision Analysis with a Large Language Model: pyDecision a
  Comprehensive Library of MCDA Methods in Python
Authors: Valdecy Pereira, Marcio Pereira Basilio, Carlos Henrique Tarjano
  SantosCarlos Henrique Tarjano Santos
Categories: cs.AI
Comments: 23 pages, 2 figures
\\
  Purpose: Multicriteria decision analysis (MCDA) has become increasingly
essential for decision-making in complex environments. In response to this
need, the pyDecision library, implemented in Python and available at
https://bit.ly/3tLFGtH, has been developed to provide a comprehensive and
accessible collection of MCDA methods. Methods: The pyDecision offers 70 MCDA
methods, including AHP, TOPSIS, and the PROMETHEE and ELECTRE families. Beyond
offering a vast range of techniques, the library provides visualization tools
for more intuitive results interpretation. In addition to these features,
pyDecision has integrated ChatGPT, an advanced Large Language Model, where
decision-makers can use ChatGPT to discuss and compare the outcomes of
different methods, providing a more interactive and intuitive understanding of
the solutions. Findings: Large Language Models are undeniably potent but can
sometimes be a double-edged sword. Its answers may be misleading without
rigorous verification of its outputs, especially for researchers lacking deep
domain expertise. It's imperative to approach its insights with a discerning
eye and a solid foundation in the relevant field. Originality: With the
integration of MCDA methods and ChatGPT, pyDecision is a significant
contribution to the scientific community, as it is an invaluable resource for
researchers, practitioners, and decision-makers navigating complex
decision-making problems and seeking the most appropriate solutions based on
MCDA methods.
\\ ( https://arxiv.org/abs/2404.06370 ,  1455kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06405
Date: Tue, 9 Apr 2024 15:54:00 GMT   (2503kb,D)

Title: Wu's Method can Boost Symbolic AI to Rival Silver Medalists and
  AlphaGeometry to Outperform Gold Medalists at IMO Geometry
Authors: Shiven Sinha, Ameya Prabhu, Ponnurangam Kumaraguru, Siddharth Bhat,
  Matthias Bethge
Categories: cs.AI cs.CG cs.CL cs.LG
Comments: Work in Progress. Released for wider feedback
\\
  Proving geometric theorems constitutes a hallmark of visual reasoning
combining both intuitive and logical skills. Therefore, automated theorem
proving of Olympiad-level geometry problems is considered a notable milestone
in human-level automated reasoning. The introduction of AlphaGeometry, a
neuro-symbolic model trained with 100 million synthetic samples, marked a major
breakthrough. It solved 25 of 30 International Mathematical Olympiad (IMO)
problems whereas the reported baseline based on Wu's method solved only ten. In
this note, we revisit the IMO-AG-30 Challenge introduced with AlphaGeometry,
and find that Wu's method is surprisingly strong. Wu's method alone can solve
15 problems, and some of them are not solved by any of the other methods. This
leads to two key findings: (i) Combining Wu's method with the classic synthetic
methods of deductive databases and angle, ratio, and distance chasing solves 21
out of 30 methods by just using a CPU-only laptop with a time limit of 5
minutes per problem. Essentially, this classic method solves just 4 problems
less than AlphaGeometry and establishes the first fully symbolic baseline
strong enough to rival the performance of an IMO silver medalist. (ii) Wu's
method even solves 2 of the 5 problems that AlphaGeometry failed to solve.
Thus, by combining AlphaGeometry with Wu's method we set a new state-of-the-art
for automated theorem proving on IMO-AG-30, solving 27 out of 30 problems, the
first AI method which outperforms an IMO gold medalist.
\\ ( https://arxiv.org/abs/2404.06405 ,  2503kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06411
Date: Tue, 9 Apr 2024 16:01:24 GMT   (992kb,D)

Title: AgentQuest: A Modular Benchmark Framework to Measure Progress and
  Improve LLM Agents
Authors: Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril
  Gashteovski, David Friede, Roberto Bifulco, Carolin Lawrence
Categories: cs.AI cs.CL
Comments: Accepted at the 2024 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies
  (NAACL-HLT 2024)
\\
  The advances made by Large Language Models (LLMs) have led to the pursuit of
LLM agents that can solve intricate, multi-step reasoning tasks. As with any
research pursuit, benchmarking and evaluation are key corner stones to
efficient and reliable progress. However, existing benchmarks are often narrow
and simply compute overall task success. To face these issues, we propose
AgentQuest -- a framework where (i) both benchmarks and metrics are modular and
easily extensible through well documented and easy-to-use APIs; (ii) we offer
two new evaluation metrics that can reliably track LLM agent progress while
solving a task. We exemplify the utility of the metrics on two use cases
wherein we identify common failure points and refine the agent architecture to
obtain a significant performance increase. Together with the research
community, we hope to extend AgentQuest further and therefore we make it
available under https://github.com/nec-research/agentquest.
\\ ( https://arxiv.org/abs/2404.06411 ,  992kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06474
Date: Tue, 9 Apr 2024 17:25:47 GMT   (24102kb,D)

Title: Autonomous Evaluation and Refinement of Digital Agents
Authors: Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine,
  and Alane Suhr
Categories: cs.AI
Comments: Code at https://github.com/Berkeley-NLP/Agent-Eval-Refine
\\
  We show that domain-general automatic evaluators can significantly improve
the performance of agents for web navigation and device control. We experiment
with multiple evaluation models that trade off between inference cost,
modularity of design, and accuracy. We validate the performance of these models
in several popular benchmarks for digital agents, finding between 74.4 and
92.9% agreement with oracle evaluation metrics. Finally, we use these
evaluators to improve the performance of existing agents via fine-tuning and
inference-time guidance. Without any additional supervision, we improve
state-of-the-art performance by 29% on the popular benchmark WebArena, and
achieve a 75% relative improvement in a challenging domain transfer scenario.
\\ ( https://arxiv.org/abs/2404.06474 ,  24102kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05829
Date: Mon, 8 Apr 2024 19:48:36 GMT   (3417kb,D)

Title: SambaLingo: Teaching Large Language Models New Languages
Authors: Zoltan Csaki, Bo Li, Jonathan Li, Qiantong Xu, Pian Pawakapan, Leon
  Zhang, Yun Du, Hengyu Zhao, Changran Hu, Urmish Thakker
Categories: cs.CL cs.AI cs.LG
Comments: 23 pages
\\
  Despite the widespread availability of LLMs, there remains a substantial gap
in their capabilities and availability across diverse languages. One approach
to address these issues has been to take an existing pre-trained LLM and
continue to train it on new languages. While prior works have experimented with
language adaptation, many questions around best practices and methodology have
not been covered. In this paper, we present a comprehensive investigation into
the adaptation of LLMs to new languages. Our study covers the key components in
this process, including vocabulary extension, direct preference optimization
and the data scarcity problem for human alignment in low-resource languages. We
scale these experiments across 9 languages and 2 parameter scales (7B and 70B).
We compare our models against Llama 2, Aya-101, XGLM, BLOOM and existing
language experts, outperforming all prior published baselines. Additionally,
all evaluation code and checkpoints are made public to facilitate future
research.
\\ ( https://arxiv.org/abs/2404.05829 ,  3417kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05839
Date: Mon, 8 Apr 2024 20:05:25 GMT   (104kb,D)

Title: \'UFAL LatinPipe at EvaLatin 2024: Morphosyntactic Analysis of Latin
Authors: Milan Straka, Jana Strakov\'a, Federica Gamba
Categories: cs.CL
Comments: Accepted to EvaLatin 2024
\\
  We present LatinPipe, the winning submission to the EvaLatin 2024 Dependency
Parsing shared task. Our system consists of a fine-tuned concatenation of base
and large pre-trained LMs, with a dot-product attention head for parsing and
softmax classification heads for morphology to jointly learn both dependency
parsing and morphological analysis. It is trained by sampling from seven
publicly available Latin corpora, utilizing additional harmonization of
annotations to achieve a more unified annotation style. Before fine-tuning, we
train the system for a few initial epochs with frozen weights. We also add
additional local relative contextualization by stacking the BiLSTM layers on
top of the Transformer(s). Finally, we ensemble output probability
distributions from seven randomly instantiated networks for the final
submission. The code is available at
https://github.com/ufal/evalatin2024-latinpipe.
\\ ( https://arxiv.org/abs/2404.05839 ,  104kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05866
Date: Mon, 8 Apr 2024 20:58:06 GMT   (1400kb,D)

Title: GeniL: A Multilingual Dataset on Generalizing Language
Authors: Aida Mostafazadeh Davani, Sagar Gubbi, Sunipa Dev, Shachi Dave,
  Vinodkumar Prabhakaran
Categories: cs.CL
\\
  LLMs are increasingly transforming our digital ecosystem, but they often
inherit societal biases learned from their training data, for instance
stereotypes associating certain attributes with specific identity groups. While
whether and how these biases are mitigated may depend on the specific use
cases, being able to effectively detect instances of stereotype perpetuation is
a crucial first step. Current methods to assess presence of stereotypes in
generated language rely on simple template or co-occurrence based measures,
without accounting for the variety of sentential contexts they manifest in. We
argue that understanding the sentential context is crucial for detecting
instances of generalization. We distinguish two types of generalizations: (1)
language that merely mentions the presence of a generalization ("people think
the French are very rude"), and (2) language that reinforces such a
generalization ("as French they must be rude"), from non-generalizing context
("My French friends think I am rude"). For meaningful stereotype evaluations,
we need to reliably distinguish such instances of generalizations. We introduce
the new task of detecting generalization in language, and build GeniL, a
multilingual dataset of over 50K sentences from 9 languages (English, Arabic,
Bengali, Spanish, French, Hindi, Indonesian, Malay, and Portuguese) annotated
for instances of generalizations. We demonstrate that the likelihood of a
co-occurrence being an instance of generalization is usually low, and varies
across different languages, identity groups, and attributes. We build
classifiers to detect generalization in language with an overall PR-AUC of
58.7, with varying degrees of performance across languages. Our research
provides data and tools to enable a nuanced understanding of stereotype
perpetuation, a crucial step towards more inclusive and responsible language
technologies.
\\ ( https://arxiv.org/abs/2404.05866 ,  1400kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05875
Date: Mon, 8 Apr 2024 21:15:36 GMT   (1343kb,D)

Title: CodecLM: Aligning Language Models with Tailored Synthetic Data
Authors: Zifeng Wang, Chun-Liang Li, Vincent Perot, Long T. Le, Jin Miao,
  Zizhao Zhang, Chen-Yu Lee, Tomas Pfister
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to Findings of NAACL 2024
\\
  Instruction tuning has emerged as the key in aligning large language models
(LLMs) with specific task instructions, thereby mitigating the discrepancy
between the next-token prediction objective and users' actual goals. To reduce
the labor and time cost to collect or annotate data by humans, researchers
start to explore the use of LLMs to generate instruction-aligned synthetic
data. Recent works focus on generating diverse instructions and applying LLM to
increase instruction complexity, often neglecting downstream use cases. It
remains unclear how to tailor high-quality data to elicit better
instruction-following abilities in different target instruction distributions
and LLMs. To this end, we introduce CodecLM, a general framework for adaptively
generating high-quality synthetic data for LLM alignment with different
downstream instruction distributions and LLMs. Drawing on the Encode-Decode
principles, we use LLMs as codecs to guide the data generation process. We
first encode seed instructions into metadata, which are concise keywords
generated on-the-fly to capture the target instruction distribution, and then
decode metadata to create tailored instructions. We also introduce Self-Rubrics
and Contrastive Filtering during decoding to tailor data-efficient samples.
Extensive experiments on four open-domain instruction following benchmarks
validate the effectiveness of CodecLM over the current state-of-the-arts.
\\ ( https://arxiv.org/abs/2404.05875 ,  1343kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05880
Date: Mon, 8 Apr 2024 21:26:22 GMT   (8137kb,D)

Title: Eraser: Jailbreaking Defense in Large Language Models via Unlearning
  Harmful Knowledge
Authors: Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen,
  Huiping Zhuang, Cen Chen
Categories: cs.CL
\\
  Jailbreaking attacks can enable Large Language Models (LLMs) to bypass the
safeguard and generate harmful content. Existing jailbreaking defense methods
have failed to address the fundamental issue that harmful knowledge resides
within the model, leading to potential jailbreak risks for LLMs. In this paper,
we propose a novel defense method called Eraser, which mainly includes three
goals: unlearning harmful knowledge, retaining general knowledge, and
maintaining safety alignment. The intuition is that if an LLM forgets the
specific knowledge required to answer a harmful question, it will no longer
have the ability to answer harmful questions. The training of Erase does not
actually require the model's own harmful knowledge, and it can benefit from
unlearning general answers related to harmful queries, which means it does not
need assistance from the red team. The experimental results show that Eraser
can significantly reduce the jailbreaking success rate for various attacks
without compromising the general capabilities of the model.
\\ ( https://arxiv.org/abs/2404.05880 ,  8137kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05892
Date: Mon, 8 Apr 2024 22:20:59 GMT   (5572kb,AD)

Title: Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence
Authors: Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric
  Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou,
  Przemys{\l}aw Kazienko, Kranthi Kiran GV, Jan Koco\'n, Bart{\l}omiej Koptyra,
  Satyapriya Krishna, Ronald McClelland Jr., Niklas Muennighoff, Fares Obeid,
  Atsushi Saito, Guangyu Song, Haoqin Tu, Stanis{\l}aw Wo\'zniak, Ruichong
  Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Rui-Jie Zhu
Categories: cs.CL cs.AI
\\
  We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon
the RWKV (RWKV-4) architecture. Our architectural design advancements include
multi-headed matrix-valued states and a dynamic recurrence mechanism that
improve expressivity while maintaining the inference efficiency characteristics
of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a
fast tokenizer based on greedy matching for enhanced multilinguality. We
trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two
Finch models with 1.6 and 3.1 billion parameters and find that they achieve
competitive performance across a wide variety of benchmarks. We release all our
models on HuggingFace under the Apache 2.0 license. Models at:
https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM
Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code
at: https://github.com/RWKV/RWKV-infctx-trainer
\\ ( https://arxiv.org/abs/2404.05892 ,  5572kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05902
Date: Mon, 8 Apr 2024 23:10:47 GMT   (7511kb,D)

Title: WILBUR: Adaptive In-Context Learning for Robust and Accurate Web Agents
Authors: Michael Lutz, Arth Bohra, Manvel Saroyan, Artem Harutyunyan, Giovanni
  Campagna
Categories: cs.CL cs.AI
\\
  In the realm of web agent research, achieving both generalization and
accuracy remains a challenging problem. Due to high variance in website
structure, existing approaches often fail. Moreover, existing fine-tuning and
in-context learning techniques fail to generalize across multiple websites. We
introduce Wilbur, an approach that uses a differentiable ranking model and a
novel instruction synthesis technique to optimally populate a black-box large
language model's prompt with task demonstrations from previous runs. To
maximize end-to-end success rates, we also propose an intelligent backtracking
mechanism that learns and recovers from its mistakes. Finally, we show that our
ranking model can be trained on data from a generative auto-curriculum which
samples representative goals from an LLM, runs the agent, and automatically
evaluates it, with no manual annotation. Wilbur achieves state-of-the-art
results on the WebVoyager benchmark, beating text-only models by 8% overall,
and up to 36% on certain websites. On the same benchmark, Wilbur is within 5%
of a strong multi-modal model despite only receiving textual inputs, and
further analysis reveals a substantial number of failures are due to
engineering challenges of operating the web.
\\ ( https://arxiv.org/abs/2404.05902 ,  7511kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05904
Date: Mon, 8 Apr 2024 23:16:22 GMT   (1447kb,D)

Title: The Hallucinations Leaderboard -- An Open Effort to Measure
  Hallucinations in Large Language Models
Authors: Giwon Hong, Aryo Pradipta Gema, Rohit Saxena, Xiaotang Du, Ping Nie,
  Yu Zhao, Laura Perez-Beltrachini, Max Ryabinin, Xuanli He, Pasquale Minervini
Categories: cs.CL
\\
  Large Language Models (LLMs) have transformed the Natural Language Processing
(NLP) landscape with their remarkable ability to understand and generate
human-like text. However, these models are prone to ``hallucinations'' --
outputs that do not align with factual reality or the input context. This paper
introduces the Hallucinations Leaderboard, an open initiative to quantitatively
measure and compare the tendency of each model to produce hallucinations. The
leaderboard uses a comprehensive set of benchmarks focusing on different
aspects of hallucinations, such as factuality and faithfulness, across various
tasks, including question-answering, summarisation, and reading comprehension.
Our analysis provides insights into the performance of different models,
guiding researchers and practitioners in choosing the most reliable models for
their applications.
\\ ( https://arxiv.org/abs/2404.05904 ,  1447kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05943
Date: Tue, 9 Apr 2024 01:55:05 GMT   (13350kb,D)

Title: Interplay of Machine Translation, Diacritics, and Diacritization
Authors: Wei-Rui Chen, Ife Adebara, Muhammad Abdul-Mageed
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 Main Conference
\\
  We investigate two research questions: (1) how do machine translation (MT)
and diacritization influence the performance of each other in a multi-task
learning setting (2) the effect of keeping (vs. removing) diacritics on MT
performance. We examine these two questions in both high-resource (HR) and
low-resource (LR) settings across 55 different languages (36 African languages
and 19 European languages). For (1), results show that diacritization
significantly benefits MT in the LR scenario, doubling or even tripling
performance for some languages, but harms MT in the HR scenario. We find that
MT harms diacritization in LR but benefits significantly in HR for some
languages. For (2), MT performance is similar regardless of diacritics being
kept or removed. In addition, we propose two classes of metrics to measure the
complexity of a diacritical system, finding these metrics to correlate
positively with the performance of our diacritization models. Overall, our work
provides insights for developing MT and diacritization systems under different
data size conditions and may have implications that generalize beyond the 55
languages we investigate.
\\ ( https://arxiv.org/abs/2404.05943 ,  13350kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05955
Date: Tue, 9 Apr 2024 02:29:39 GMT   (3164kb,D)

Title: VisualWebBench: How Far Have Multimodal LLMs Evolved in Web Page
  Understanding and Grounding?
Authors: Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig,
  Yuanzhi Li, and Xiang Yue
Categories: cs.CL cs.AI
\\
  Multimodal Large Language models (MLLMs) have shown promise in web-related
tasks, but evaluating their performance in the web domain remains a challenge
due to the lack of comprehensive benchmarks. Existing benchmarks are either
designed for general multimodal tasks, failing to capture the unique
characteristics of web pages, or focus on end-to-end web agent tasks, unable to
measure fine-grained abilities such as OCR, understanding, and grounding. In
this paper, we introduce \bench{}, a multimodal benchmark designed to assess
the capabilities of MLLMs across a variety of web tasks. \bench{} consists of
seven tasks, and comprises 1.5K human-curated instances from 139 real websites,
covering 87 sub-domains. We evaluate 14 open-source MLLMs, Gemini Pro, Claude-3
series, and GPT-4V(ision) on \bench{}, revealing significant challenges and
performance gaps. Further analysis highlights the limitations of current MLLMs,
including inadequate grounding in text-rich environments and subpar performance
with low-resolution image inputs. We believe \bench{} will serve as a valuable
resource for the research community and contribute to the creation of more
powerful and versatile MLLMs for web-related applications.
\\ ( https://arxiv.org/abs/2404.05955 ,  3164kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05961
Date: Tue, 9 Apr 2024 02:51:05 GMT   (3854kb,D)

Title: LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders
Authors: Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach, Dzmitry
  Bahdanau, Nicolas Chapados, Siva Reddy
Categories: cs.CL cs.AI
\\
  Large decoder-only language models (LLMs) are the state-of-the-art models on
most of today's NLP tasks and benchmarks. Yet, the community is only slowly
adopting these models for text embedding tasks, which require rich
contextualized representations. In this work, we introduce LLM2Vec, a simple
unsupervised approach that can transform any decoder-only LLM into a strong
text encoder. LLM2Vec consists of three simple steps: 1) enabling bidirectional
attention, 2) masked next token prediction, and 3) unsupervised contrastive
learning. We demonstrate the effectiveness of LLM2Vec by applying it to 3
popular LLMs ranging from 1.3B to 7B parameters and evaluate the transformed
models on English word- and sequence-level tasks. We outperform encoder-only
models by a large margin on word-level tasks and reach a new unsupervised
state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB).
Moreover, when combining LLM2Vec with supervised contrastive learning, we
achieve state-of-the-art performance on MTEB among models that train only on
publicly available data. Our strong empirical results and extensive analysis
demonstrate that LLMs can be effectively transformed into universal text
encoders in a parameter-efficient manner without the need for expensive
adaptation or synthetic GPT-4 generated data.
\\ ( https://arxiv.org/abs/2404.05961 ,  3854kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05966
Date: Tue, 9 Apr 2024 02:53:14 GMT   (4433kb,D)

Title: THOUGHTSCULPT: Reasoning with Intermediate Revision and Search
Authors: Yizhou Chi, Kevin Yang, Dan Klein
Categories: cs.CL cs.AI
Comments: Code and data available at https://github.com/cyzus/thoughtsculpt
\\
  We present THOUGHTSCULPT, a general reasoning and search method for tasks
with outputs that can be decomposed into components. THOUGHTSCULPT explores a
search tree of potential solutions using Monte Carlo Tree Search (MCTS),
building solutions one action at a time and evaluating according to any
domain-specific heuristic, which in practice is often simply an LLM evaluator.
Critically, our action space includes revision actions: THOUGHTSCULPT may
choose to revise part of its previous output rather than continuing to build
the rest of its output. Empirically, THOUGHTSCULPT outperforms state-of-the-art
reasoning methods across three challenging tasks: Story Outline Improvement (up
to +30% interestingness), Mini-Crosswords Solving (up to +16% word success
rate), and Constrained Generation (up to +10% concept coverage).
\\ ( https://arxiv.org/abs/2404.05966 ,  4433kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05970
Date: Tue, 9 Apr 2024 02:58:05 GMT   (1408kb,D)

Title: Optimization Methods for Personalizing Large Language Models through
  Retrieval Augmentation
Authors: Alireza Salemi, Surya Kallumadi, Hamed Zamani
Categories: cs.CL cs.IR
\\
  This paper studies retrieval-augmented approaches for personalizing large
language models (LLMs), which potentially have a substantial impact on various
applications and domains. We propose the first attempt to optimize the
retrieval models that deliver a limited number of personal documents to large
language models for the purpose of personalized generation. We develop two
optimization algorithms that solicit feedback from the downstream personalized
generation tasks for retrieval optimization--one based on reinforcement
learning whose reward function is defined using any arbitrary metric for
personalized generation and another based on knowledge distillation from the
downstream LLM to the retrieval model. This paper also introduces a pre- and
post-generation retriever selection model that decides what retriever to choose
for each LLM input. Extensive experiments on diverse tasks from the language
model personalization (LaMP) benchmark reveal statistically significant
improvements in six out of seven datasets.
\\ ( https://arxiv.org/abs/2404.05970 ,  1408kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05989
Date: Tue, 9 Apr 2024 03:47:48 GMT   (520kb,D)

Title: Event-enhanced Retrieval in Real-time Search
Authors: Yanan Zhang, Xiaoling Bai, Tianhua Zhou
Categories: cs.CL cs.IR
Comments: LREC-COLING 2024
\\
  The embedding-based retrieval (EBR) approach is widely used in mainstream
search engine retrieval systems and is crucial in recent retrieval-augmented
methods for eliminating LLM illusions. However, existing EBR models often face
the "semantic drift" problem and insufficient focus on key information, leading
to a low adoption rate of retrieval results in subsequent steps. This issue is
especially noticeable in real-time search scenarios, where the various
expressions of popular events on the Internet make real-time retrieval heavily
reliant on crucial event information. To tackle this problem, this paper
proposes a novel approach called EER, which enhances real-time retrieval
performance by improving the dual-encoder model of traditional EBR. We
incorporate contrastive learning to accompany pairwise learning for encoder
optimization. Furthermore, to strengthen the focus on critical event
information in events, we include a decoder module after the document encoder,
introduce a generative event triplet extraction scheme based on prompt-tuning,
and correlate the events with query encoder optimization through comparative
learning. This decoder module can be removed during inference. Extensive
experiments demonstrate that EER can significantly improve the real-time search
retrieval performance. We believe that this approach will provide new
perspectives in the field of information retrieval. The codes and dataset are
available at https://github.com/open-event-hub/Event-enhanced_Retrieval .
\\ ( https://arxiv.org/abs/2404.05989 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06001
Date: Tue, 9 Apr 2024 04:11:25 GMT   (1090kb,D)

Title: Privacy Preserving Prompt Engineering: A Survey
Authors: Kennedy Edemacu, Xintao Wu
Categories: cs.CL
Comments: 23 pages, 8 figures
\\
  Pre-trained language models (PLMs) have demonstrated significant proficiency
in solving a wide range of general natural language processing (NLP) tasks.
Researchers have observed a direct correlation between the performance of these
models and their sizes. As a result, the sizes of these models have notably
expanded in recent years, persuading researchers to adopt the term large
language models (LLMs) to characterize the larger-sized PLMs. The increased
size is accompanied by a distinct capability known as in-context learning
(ICL), which represents a specialized form of prompting. This enables the
utilization of LLMs for specific downstream tasks by presenting them with
demonstration examples while keeping the model parameters frozen. Although
interesting, privacy concerns have become a major obstacle in its widespread
usage. Multiple studies have examined the privacy risks linked to ICL and
prompting in general, and have devised techniques to alleviate these risks.
Thus, there is a necessity to organize these mitigation techniques for the
benefit of the community. This survey provides a systematic overview of the
privacy protection methods employed during ICL and prompting in general. We
review, analyze, and compare different methods under this paradigm.
Furthermore, we provide a summary of the resources accessible for the
development of these frameworks. Finally, we discuss the limitations of these
frameworks and offer a detailed examination of the promising areas that
necessitate further exploration.
\\ ( https://arxiv.org/abs/2404.06001 ,  1090kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06003
Date: Tue, 9 Apr 2024 04:17:51 GMT   (4460kb,D)

Title: FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation
  of Large Language Models
Authors: Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye,
  Jindong Wang, Yue Zhang, Shikun Zhang
Categories: cs.CL cs.AI
Comments: We open-source all our code at:
  https://github.com/WisdomShell/FreeEval
\\
  The rapid development of large language model (LLM) evaluation methodologies
and datasets has led to a profound challenge: integrating state-of-the-art
evaluation techniques cost-effectively while ensuring reliability,
reproducibility, and efficiency. Currently, there is a notable absence of a
unified and adaptable framework that seamlessly integrates various evaluation
approaches. Moreover, the reliability of evaluation findings is often
questionable due to potential data contamination, with the evaluation
efficiency commonly overlooked when facing the substantial costs associated
with LLM inference. In response to these challenges, we introduce FreeEval, a
modular and scalable framework crafted to enable trustworthy and efficient
automatic evaluations of LLMs. Firstly, FreeEval's unified abstractions
simplify the integration and improve the transparency of diverse evaluation
methodologies, encompassing dynamic evaluation that demand sophisticated LLM
interactions. Secondly, the framework integrates meta-evaluation techniques
like human evaluation and data contamination detection, which, along with
dynamic evaluation modules in the platform, enhance the fairness of the
evaluation outcomes. Lastly, FreeEval is designed with a high-performance
infrastructure, including distributed computation and caching strategies,
enabling extensive evaluations across multi-node, multi-GPU clusters for
open-source and proprietary LLMs.
\\ ( https://arxiv.org/abs/2404.06003 ,  4460kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06017
Date: Tue, 9 Apr 2024 04:55:24 GMT   (1953kb,D)

Title: Identifying Shopping Intent in Product QA for Proactive Recommendations
Authors: Besnik Fetahu, Nachshon Cohen, Elad Haramaty, Liane Lewin-Eytan, Oleg
  Rokhlenko, Shervin Malmasi
Categories: cs.CL
Comments: Accepted at IronGraphs@ECIR'2024
\\
  Voice assistants have become ubiquitous in smart devices allowing users to
instantly access information via voice questions. While extensive research has
been conducted in question answering for voice search, little attention has
been paid on how to enable proactive recommendations from a voice assistant to
its users. This is a highly challenging problem that often leads to user
friction, mainly due to recommendations provided to the users at the wrong
time. We focus on the domain of e-commerce, namely in identifying Shopping
Product Questions (SPQs), where the user asking a product-related question may
have an underlying shopping need. Identifying a user's shopping need allows
voice assistants to enhance shopping experience by determining when to provide
recommendations, such as product or deal recommendations, or proactive shopping
actions recommendation. Identifying SPQs is a challenging problem and cannot be
done from question text alone, and thus requires to infer latent user behavior
patterns inferred from user's past shopping history. We propose features that
capture the user's latent shopping behavior from their purchase history, and
combine them using a novel Mixture-of-Experts (MoE) model. Our evaluation shows
that the proposed approach is able to identify SPQs with a high score of
F1=0.91. Furthermore, based on an online evaluation with real voice assistant
users, we identify SPQs in real-time and recommend shopping actions to users to
add the queried product into their shopping list. We demonstrate that we are
able to accurately identify SPQs, as indicated by the significantly higher rate
of added products to users' shopping lists when being prompted after SPQs vs
random PQs.
\\ ( https://arxiv.org/abs/2404.06017 ,  1953kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06063
Date: Tue, 9 Apr 2024 07:02:14 GMT   (381kb,D)

Title: All in One: An Empirical Study of GPT for Few-Shot Aspect-Based
  Sentiment Anlaysis
Authors: Baoxing Jiang
Categories: cs.CL cs.AI cs.LG
Comments: 9 pages, 5 figures
\\
  Aspect-Based Sentiment Analysis (ABSA) is an indispensable and highly
challenging task in natural language processing. Current efforts have focused
on specific sub-tasks, making it difficult to comprehensively cover all
sub-tasks within the ABSA domain. With the development of Generative
Pre-trained Transformers (GPTs), there came inspiration for a one-stop solution
to sentiment analysis. In this study, we used GPTs for all sub-tasks of
few-shot ABSA while defining a general learning paradigm for this application.
We propose the All in One (AiO) model, a simple yet effective two-stage model
for all ABSA sub-tasks. In the first stage, a specific backbone network learns
the semantic information of the review and generates heuristically enhanced
candidates. In the second stage, AiO leverages GPT contextual learning
capabilities to generate predictions. The study conducted comprehensive
comparative and ablation experiments on five benchmark datasets, and the
results show that AiO can effectively handle all ABSA sub-tasks, even with
few-shot data.
\\ ( https://arxiv.org/abs/2404.06063 ,  381kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06101
Date: Tue, 9 Apr 2024 08:08:03 GMT   (18549kb,D)

Title: Making Old Kurdish Publications Processable by Augmenting Available
  Optical Character Recognition Engines
Authors: Blnd Yaseen and Hossein Hassani
Categories: cs.CL
Comments: 30 pages, 21 figures, 2 tables
\\
  Kurdish libraries have many historical publications that were printed back in
the early days when printing devices were brought to Kurdistan. Having a good
Optical Character Recognition (OCR) to help process these publications and
contribute to the Kurdish languages resources which is crucial as Kurdish is
considered a low-resource language. Current OCR systems are unable to extract
text from historical documents as they have many issues, including being
damaged, very fragile, having many marks left on them, and often written in
non-standard fonts and more. This is a massive obstacle in processing these
documents as currently processing them requires manual typing which is very
time-consuming. In this study, we adopt an open-source OCR framework by Google,
Tesseract version 5.0, that has been used to extract text for various
languages. Currently, there is no public dataset, and we developed our own by
collecting historical documents from Zheen Center for Documentation and
Research, which were printed before 1950 and resulted in a dataset of 1233
images of lines with transcription of each. Then we used the Arabic model as
our base model and trained the model using the dataset. We used different
methods to evaluate our model, Tesseracts built-in evaluator lstmeval indicated
a Character Error Rate (CER) of 0.755%. Additionally, Ocreval demonstrated an
average character accuracy of 84.02%. Finally, we developed a web application
to provide an easy- to-use interface for end-users, allowing them to interact
with the model by inputting an image of a page and extracting the text. Having
an extensive dataset is crucial to develop OCR systems with reasonable
accuracy, as currently, no public datasets are available for historical Kurdish
documents; this posed a significant challenge in our work. Additionally, the
unaligned spaces between characters and words proved another challenge with our
work.
\\ ( https://arxiv.org/abs/2404.06101 ,  18549kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06107
Date: Tue, 9 Apr 2024 08:19:10 GMT   (20049kb,D)

Title: Exploring the Necessity of Visual Modality in Multimodal Machine
  Translation using Authentic Datasets
Authors: Zi Long, Zhenhao Tang, Xianghua Fu, Jian Chen, Shilong Hou, Jinze Lyu
Categories: cs.CL
Comments: bucc 2024 accepted
\\
  Recent research in the field of multimodal machine translation (MMT) has
indicated that the visual modality is either dispensable or offers only
marginal advantages. However, most of these conclusions are drawn from the
analysis of experimental results based on a limited set of bilingual
sentence-image pairs, such as Multi30k. In these kinds of datasets, the content
of one bilingual parallel sentence pair must be well represented by a manually
annotated image, which is different from the real-world translation scenario.
In this work, we adhere to the universal multimodal machine translation
framework proposed by Tang et al. (2022). This approach allows us to delve into
the impact of the visual modality on translation efficacy by leveraging
real-world translation datasets. Through a comprehensive exploration via
probing tasks, we find that the visual modality proves advantageous for the
majority of authentic translation datasets. Notably, the translation
performance primarily hinges on the alignment and coherence between textual and
visual contents. Furthermore, our results suggest that visual information
serves a supplementary role in multimodal translation and can be substituted.
\\ ( https://arxiv.org/abs/2404.06107 ,  20049kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06121
Date: Tue, 9 Apr 2024 08:44:02 GMT   (44kb,D)

Title: Detection of fields of applications in biomedical abstracts with the
  support of argumentation elements
Authors: Mariana Neves
Categories: cs.CL
\\
  Focusing on particular facts, instead of the complete text, can potentially
improve searching for specific information in the scientific literature. In
particular, argumentative elements allow focusing on specific parts of a
publication, e.g., the background section or the claims from the authors. We
evaluated some tools for the extraction of argumentation elements for a
specific task in biomedicine, namely, for detecting the fields of the
application in a biomedical publication, e.g, whether it addresses the problem
of disease diagnosis or drug development. We performed experiments with the
PubMedBERT pre-trained model, which was fine-tuned on a specific corpus for the
task. We compared the use of title and abstract to restricting to only some
argumentative elements. The top F1 scores ranged from 0.22 to 0.84, depending
on the field of application. The best argumentative labels were the ones
related the conclusion and background sections of an abstract.
\\ ( https://arxiv.org/abs/2404.06121 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06137
Date: Tue, 9 Apr 2024 09:03:44 GMT   (299kb,D)

Title: SmurfCat at SemEval-2024 Task 6: Leveraging Synthetic Data for
  Hallucination Detection
Authors: Elisei Rykov, Yana Shishkina, Kseniia Petrushina, Kseniia Titova,
  Sergey Petrakov and Alexander Panchenko
Categories: cs.CL cs.AI
Comments: 12 pages, 10 tables, 3 figures
\\
  In this paper, we present our novel systems developed for the SemEval-2024
hallucination detection task. Our investigation spans a range of strategies to
compare model predictions with reference standards, encompassing diverse
baselines, the refinement of pre-trained encoders through supervised learning,
and an ensemble approaches utilizing several high-performing models. Through
these explorations, we introduce three distinct methods that exhibit strong
performance metrics. To amplify our training data, we generate additional
training samples from unlabelled training subset. Furthermore, we provide a
detailed comparative analysis of our approaches. Notably, our premier method
achieved a commendable 9th place in the competition's model-agnostic track and
17th place in model-aware track, highlighting its effectiveness and potential.
\\ ( https://arxiv.org/abs/2404.06137 ,  299kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06138
Date: Tue, 9 Apr 2024 09:04:30 GMT   (1678kb,D)

Title: Cendol: Open Instruction-tuned Generative Large Language Models for
  Indonesian Languages
Authors: Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Rifki Afina Putri,
  Emmanuel Dave, Jhonson Lee, Nuur Shadieq, Wawan Cenggoro, Salsabil Maulana
  Akbar, Muhammad Ihza Mahendra, Dea Annisayanti Putri, Bryan Wilie, Genta
  Indra Winata, Alham Fikri Aji, Ayu Purwarianti, Pascale Fung
Categories: cs.CL
Comments: Cendol models are released under Apache 2.0 license and will be made
  publicly available soon
\\
  Large language models (LLMs) show remarkable human-like capability in various
domains and languages. However, a notable quality gap arises in low-resource
languages, e.g., Indonesian indigenous languages, rendering them ineffective
and inefficient in such linguistic contexts. To bridge this quality gap, we
introduce Cendol, a collection of Indonesian LLMs encompassing both
decoder-only and encoder-decoder architectures across a range of model sizes.
We highlight Cendol's effectiveness across a diverse array of tasks, attaining
20% improvement, and demonstrate its capability to generalize to unseen tasks
and indigenous languages of Indonesia. Furthermore, Cendol models showcase
improved human favorability despite their limitations in capturing indigenous
knowledge and cultural values in Indonesia. In addition, we discuss the
shortcomings of parameter-efficient tunings, such as LoRA, for language
adaptation. Alternatively, we propose the usage of vocabulary adaptation to
enhance efficiency. Lastly, we evaluate the safety of Cendol and showcase that
safety in pre-training in one language such as English is transferable to
low-resource languages, such as Indonesian, even without RLHF and safety
fine-tuning.
\\ ( https://arxiv.org/abs/2404.06138 ,  1678kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06150
Date: Tue, 9 Apr 2024 09:21:56 GMT   (5544kb,D)

Title: (Not) Understanding Latin Poetic Style with Deep Learning
Authors: Ben Nagy
Categories: cs.CL
\\
  This article summarizes some mostly unsuccessful attempts to understand
authorial style by examining the attention of various neural networks (LSTMs
and CNNs) trained on a corpus of classical Latin verse that has been encoded to
include sonic and metrical features. Carefully configured neural networks are
shown to be extremely strong authorship classifiers, so it is hoped that they
might therefore teach `traditional' readers something about how the authors
differ in style. Sadly their reasoning is, so far, inscrutable. While the
overall goal has not yet been reached, this work reports some useful findings
in terms of effective ways to encode and embed verse, the relative strengths
and weaknesses of the neural network families, and useful (and not so useful)
techniques for designing and inspecting NN models in this domain. This article
suggests that, for poetry, CNNs are better choices than LSTMs -- they train
more quickly, have equivalent accuracy, and (potentially) offer better
interpretability. Based on a great deal of experimentation, it also suggests
that simple, trainable embeddings are more effective than domain-specific
schemes, and stresses the importance of techniques to reduce overfitting, like
dropout and batch normalization.
\\ ( https://arxiv.org/abs/2404.06150 ,  5544kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06162
Date: Tue, 9 Apr 2024 09:34:25 GMT   (1404kb,D)

Title: Characterizing Multimodal Long-form Summarization: A Case Study on
  Financial Reports
Authors: Tianyu Cao, Natraj Raman, Danial Dervovic and Chenhao Tan
Categories: cs.CL cs.AI cs.LG
\\
  As large language models (LLMs) expand the power of natural language
processing to handle long inputs, rigorous and systematic analyses are
necessary to understand their abilities and behavior. A salient application is
summarization, due to its ubiquity and controversy (e.g., researchers have
declared the death of summarization). In this paper, we use financial report
summarization as a case study because financial reports not only are long but
also use numbers and tables extensively. We propose a computational framework
for characterizing multimodal long-form summarization and investigate the
behavior of Claude 2.0/2.1, GPT-4/3.5, and Command. We find that GPT-3.5 and
Command fail to perform this summarization task meaningfully. For Claude 2 and
GPT-4, we analyze the extractiveness of the summary and identify a position
bias in LLMs. This position bias disappears after shuffling the input for
Claude, which suggests that Claude has the ability to recognize important
information. We also conduct a comprehensive investigation on the use of
numeric data in LLM-generated summaries and offer a taxonomy of numeric
hallucination. We employ prompt engineering to improve GPT-4's use of numbers
with limited success. Overall, our analyses highlight the strong capability of
Claude 2 in handling long multimodal inputs compared to GPT-4.
\\ ( https://arxiv.org/abs/2404.06162 ,  1404kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06186
Date: Tue, 9 Apr 2024 10:12:34 GMT   (696kb,D)

Title: Clue-Instruct: Text-Based Clue Generation for Educational Crossword
  Puzzles
Authors: Andrea Zugarini, Kamyar Zeinalipour, Surya Sai Kadali, Marco Maggini,
  Marco Gori, Leonardo Rigutini
Categories: cs.CL cs.AI
\\
  Crossword puzzles are popular linguistic games often used as tools to engage
students in learning. Educational crosswords are characterized by less cryptic
and more factual clues that distinguish them from traditional crossword
puzzles. Despite there exist several publicly available clue-answer pair
databases for traditional crosswords, educational clue-answer pairs datasets
are missing. In this article, we propose a methodology to build educational
clue generation datasets that can be used to instruct Large Language Models
(LLMs). By gathering from Wikipedia pages informative content associated with
relevant keywords, we use Large Language Models to automatically generate
pedagogical clues related to the given input keyword and its context. With such
an approach, we created clue-instruct, a dataset containing 44,075 unique
examples with text-keyword pairs associated with three distinct crossword
clues. We used clue-instruct to instruct different LLMs to generate educational
clues from a given input content and keyword. Both human and automatic
evaluations confirmed the quality of the generated clues, thus validating the
effectiveness of our approach.
\\ ( https://arxiv.org/abs/2404.06186 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06214
Date: Tue, 9 Apr 2024 11:04:50 GMT   (157kb,D)

Title: [Call for Papers] The 2nd BabyLM Challenge: Sample-efficient pretraining
  on a developmentally plausible corpus
Authors: Leshem Choshen, Ryan Cotterell, Michael Y. Hu, Tal Linzen, Aaron
  Mueller, Candace Ross, Alex Warstadt, Ethan Wilcox, Adina Williams, Chengxu
  Zhuang
Categories: cs.CL
\\
  After last year's successful BabyLM Challenge, the competition will be hosted
again in 2024/2025. The overarching goals of the challenge remain the same;
however, some of the competition rules will be different. The big changes for
this year's competition are as follows: First, we replace the loose track with
a paper track, which allows (for example) non-model-based submissions, novel
cognitively-inspired benchmarks, or analysis techniques. Second, we are
relaxing the rules around pretraining data, and will now allow participants to
construct their own datasets provided they stay within the 100M-word or
10M-word budget. Third, we introduce a multimodal vision-and-language track,
and will release a corpus of 50% text-only and 50% image-text multimodal data
as a starting point for LM model training. The purpose of this CfP is to
provide rules for this year's challenge, explain these rule changes and their
rationale in greater detail, give a timeline of this year's competition, and
provide answers to frequently asked questions from last year's challenge.
\\ ( https://arxiv.org/abs/2404.06214 ,  157kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06217
Date: Tue, 9 Apr 2024 11:10:00 GMT   (4808kb,D)

Title: VI-OOD: A Unified Representation Learning Framework for Textual
  Out-of-distribution Detection
Authors: Li-Ming Zhan, Bo Liu, Xiao-Ming Wu
Categories: cs.CL
Comments: COLING 2024
\\
  Out-of-distribution (OOD) detection plays a crucial role in ensuring the
safety and reliability of deep neural networks in various applications. While
there has been a growing focus on OOD detection in visual data, the field of
textual OOD detection has received less attention. Only a few attempts have
been made to directly apply general OOD detection methods to natural language
processing (NLP) tasks, without adequately considering the characteristics of
textual data. In this paper, we delve into textual OOD detection with
Transformers. We first identify a key problem prevalent in existing OOD
detection methods: the biased representation learned through the maximization
of the conditional likelihood $p(y\mid x)$ can potentially result in subpar
performance. We then propose a novel variational inference framework for OOD
detection (VI-OOD), which maximizes the likelihood of the joint distribution
$p(x, y)$ instead of $p(y\mid x)$. VI-OOD is tailored for textual OOD detection
by efficiently exploiting the representations of pre-trained Transformers.
Through comprehensive experiments on various text classification tasks, VI-OOD
demonstrates its effectiveness and wide applicability. Our code has been
released at \url{https://github.com/liam0949/LLM-OOD}.
\\ ( https://arxiv.org/abs/2404.06217 ,  4808kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06224
Date: Tue, 9 Apr 2024 11:26:59 GMT   (7905kb,D)

Title: Low-Cost Generation and Evaluation of Dictionary Example Sentences
Authors: Bill Cai, Clarence Boon Liang Ng, Daniel Tan, Shelvia Hotama
Categories: cs.CL cs.AI cs.LG
\\
  Dictionary example sentences play an important role in illustrating word
definitions and usage, but manually creating quality sentences is challenging.
Prior works have demonstrated that language models can be trained to generate
example sentences. However, they relied on costly customized models and word
sense datasets for generation and evaluation of their work. Rapid advancements
in foundational models present the opportunity to create low-cost, zero-shot
methods for the generation and evaluation of dictionary example sentences. We
introduce a new automatic evaluation metric called OxfordEval that measures the
win-rate of generated sentences against existing Oxford Dictionary sentences.
OxfordEval shows high alignment with human judgments, enabling large-scale
automated quality evaluation. We experiment with various LLMs and
configurations to generate dictionary sentences across word classes. We
complement this with a novel approach of using masked language models to
identify and select sentences that best exemplify word meaning. The eventual
model, FM-MLM, achieves over 85.1% win rate against Oxford baseline sentences
according to OxfordEval, compared to 39.8% win rate for prior model-generated
sentences.
\\ ( https://arxiv.org/abs/2404.06224 ,  7905kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06228
Date: Tue, 9 Apr 2024 11:39:53 GMT   (39kb,D)

Title: Understanding Cross-Lingual Alignment -- A Survey
Authors: Katharina H\"ammerl, Jind\v{r}ich Libovick\'y, Alexander Fraser
Categories: cs.CL
\\
  Cross-lingual alignment, the meaningful similarity of representations across
languages in multilingual language models, has been an active field of research
in recent years. We survey the literature of techniques to improve
cross-lingual alignment, providing a taxonomy of methods and summarising
insights from throughout the field. We present different understandings of
cross-lingual alignment and their limitations. We provide a qualitative summary
of results from a large number of surveyed papers. Finally, we discuss how
these insights may be applied not only to encoder models, where this topic has
been heavily studied, but also to encoder-decoder or even decoder-only models,
and argue that an effective trade-off between language-neutral and
language-specific information is key.
\\ ( https://arxiv.org/abs/2404.06228 ,  39kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06283
Date: Tue, 9 Apr 2024 13:08:56 GMT   (338kb,D)

Title: LLMs' Reading Comprehension Is Affected by Parametric Knowledge and
  Struggles with Hypothetical Statements
Authors: Victoria Basmov, Yoav Goldberg, Reut Tsarfaty
Categories: cs.CL
\\
  The task of reading comprehension (RC), often implemented as context-based
question answering (QA), provides a primary means to assess language models'
natural language understanding (NLU) capabilities. Yet, when applied to large
language models (LLMs) with extensive built-in world knowledge, this method can
be deceptive. If the context aligns with the LLMs' internal knowledge, it is
hard to discern whether the models' answers stem from context comprehension or
from LLMs' internal information. Conversely, using data that conflicts with the
models' knowledge creates erroneous trends which distort the results. To
address this issue, we suggest to use RC on imaginary data, based on fictitious
facts and entities. This task is entirely independent of the models' world
knowledge, enabling us to evaluate LLMs' linguistic abilities without the
interference of parametric knowledge. Testing ChatGPT, GPT-4, LLaMA 2 and
Mixtral on such imaginary data, we uncover a class of linguistic phenomena
posing a challenge to current LLMs, involving thinking in terms of alternative,
hypothetical scenarios. While all the models handle simple affirmative and
negative contexts with high accuracy, they are much more prone to error when
dealing with modal and conditional contexts. Crucially, these phenomena also
trigger the LLMs' vulnerability to knowledge-conflicts again. In particular,
while some models prove virtually unaffected by knowledge conflicts in
affirmative and negative contexts, when faced with more semantically involved
modal and conditional environments, they often fail to separate the text from
their internal knowledge.
\\ ( https://arxiv.org/abs/2404.06283 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06292
Date: Tue, 9 Apr 2024 13:18:52 GMT   (41kb,D)

Title: nEMO: Dataset of Emotional Speech in Polish
Authors: Iwona Christop
Categories: cs.CL cs.SD eess.AS
Comments: Accepted for LREC-Coling 2024
\\
  Speech emotion recognition has become increasingly important in recent years
due to its potential applications in healthcare, customer service, and
personalization of dialogue systems. However, a major issue in this field is
the lack of datasets that adequately represent basic emotional states across
various language families. As datasets covering Slavic languages are rare,
there is a need to address this research gap. This paper presents the
development of nEMO, a novel corpus of emotional speech in Polish. The dataset
comprises over 3 hours of samples recorded with the participation of nine
actors portraying six emotional states: anger, fear, happiness, sadness,
surprise, and a neutral state. The text material used was carefully selected to
represent the phonetics of the Polish language adequately. The corpus is freely
available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).
\\ ( https://arxiv.org/abs/2404.06292 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06339
Date: Tue, 9 Apr 2024 14:25:27 GMT   (248kb)

Title: Finding fake reviews in e-commerce platforms by using hybrid algorithms
Authors: Mathivanan Periasamy, Rohith Mahadevan, Bagiya Lakshmi S, Raja CSP
  Raman, Hasan Kumar S, Jasper Jessiman
Categories: cs.CL cs.LG
\\
  Sentiment analysis, a vital component in natural language processing, plays a
crucial role in understanding the underlying emotions and opinions expressed in
textual data. In this paper, we propose an innovative ensemble approach for
sentiment analysis for finding fake reviews that amalgamate the predictive
capabilities of Support Vector Machine (SVM), K-Nearest Neighbors (KNN), and
Decision Tree classifiers. Our ensemble architecture strategically combines
these diverse models to capitalize on their strengths while mitigating inherent
weaknesses, thereby achieving superior accuracy and robustness in fake review
prediction. By combining all the models of our classifiers, the predictive
performance is boosted and it also fosters adaptability to varied linguistic
patterns and nuances present in real-world datasets. The metrics accounted for
on fake reviews demonstrate the efficacy and competitiveness of the proposed
ensemble method against traditional single-model approaches. Our findings
underscore the potential of ensemble techniques in advancing the
state-of-the-art in finding fake reviews using hybrid algorithms, with
implications for various applications in different social media and e-platforms
to find the best reviews and neglect the fake ones, eliminating puffery and
bluffs.
\\ ( https://arxiv.org/abs/2404.06339 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06347
Date: Tue, 9 Apr 2024 14:34:48 GMT   (300kb,D)

Title: RAR-b: Reasoning as Retrieval Benchmark
Authors: Chenghao Xiao, G Thomas Hudson, Noura Al Moubayed
Categories: cs.CL cs.IR
\\
  Semantic textual similartiy (STS) and information retrieval tasks (IR) tasks
have been the two major avenues to record the progress of embedding models in
the past few years. Under the emerging Retrieval-augmented Generation (RAG)
paradigm, we envision the need to evaluate next-level language understanding
abilities of embedding models, and take a conscious look at the reasoning
abilities stored in them. Addressing this, we pose the question: Can retrievers
solve reasoning problems? By transforming reasoning tasks into retrieval tasks,
we find that without specifically trained for reasoning-level language
understanding, current state-of-the-art retriever models may still be far from
being competent for playing the role of assisting LLMs, especially in
reasoning-intensive tasks. Moreover, albeit trained to be aware of
instructions, instruction-aware IR models are often better off without
instructions in inference time for reasoning tasks, posing an overlooked
retriever-LLM behavioral gap for the research community to align. However,
recent decoder-based embedding models show great promise in narrowing the gap,
highlighting the pathway for embedding models to achieve reasoning-level
language understanding. We also show that, although current off-the-shelf
re-ranker models fail on these tasks, injecting reasoning abilities into them
through fine-tuning still appears easier than doing so to bi-encoders, and we
are able to achieve state-of-the-art performance across all tasks by
fine-tuning a reranking model. We release Reasoning as Retrieval Benchmark
(RAR-b), a holistic suite of tasks and settings to evaluate the reasoning
abilities stored in retriever models. RAR-b is available at
https://github.com/gowitheflow-1998/RAR-b.
\\ ( https://arxiv.org/abs/2404.06347 ,  300kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06357
Date: Tue, 9 Apr 2024 14:48:32 GMT   (7356kb,D)

Title: Generalizable Sarcasm Detection Is Just Around The Corner, Of Course!
Authors: Hyewon Jang, Diego Frassinelli
Categories: cs.CL
\\
  We tested the robustness of sarcasm detection models by examining their
behavior when fine-tuned on four sarcasm datasets containing varying
characteristics of sarcasm: label source (authors vs. third-party), domain
(social media/online vs. offline conversations/dialogues), style (aggressive
vs. humorous mocking). We tested their prediction performance on the same
dataset (intra-dataset) and across different datasets (cross-dataset). For
intra-dataset predictions, models consistently performed better when fine-tuned
with third-party labels rather than with author labels. For cross-dataset
predictions, most models failed to generalize well to the other datasets,
implying that one type of dataset cannot represent all sorts of sarcasm with
different styles and domains. Compared to the existing datasets, models
fine-tuned on the new dataset we release in this work showed the highest
generalizability to other datasets. With a manual inspection of the datasets
and post-hoc analysis, we attributed the difficulty in generalization to the
fact that sarcasm actually comes in different domains and styles. We argue that
future sarcasm research should take the broad scope of sarcasm into account.
\\ ( https://arxiv.org/abs/2404.06357 ,  7356kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06364
Date: Tue, 9 Apr 2024 15:01:51 GMT   (538kb,D)

Title: SurveyAgent: A Conversational System for Personalized and Efficient
  Research Survey
Authors: Xintao Wang, Jiangjie Chen, Nianqi Li, Lida Chen, Xinfeng Yuan, Wei
  Shi, Xuyang Ge, Rui Xu, Yanghua Xiao
Categories: cs.CL
Comments: 6 pages
\\
  In the rapidly advancing research fields such as AI, managing and staying
abreast of the latest scientific literature has become a significant challenge
for researchers. Although previous efforts have leveraged AI to assist with
literature searches, paper recommendations, and question-answering, a
comprehensive support system that addresses the holistic needs of researchers
has been lacking. This paper introduces SurveyAgent, a novel conversational
system designed to provide personalized and efficient research survey
assistance to researchers. SurveyAgent integrates three key modules: Knowledge
Management for organizing papers, Recommendation for discovering relevant
literature, and Query Answering for engaging with content on a deeper level.
This system stands out by offering a unified platform that supports researchers
through various stages of their literature review process, facilitated by a
conversational interface that prioritizes user interaction and personalization.
Our evaluation demonstrates SurveyAgent's effectiveness in streamlining
research activities, showcasing its capability to facilitate how researchers
interact with scientific literature.
\\ ( https://arxiv.org/abs/2404.06364 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06367
Date: Tue, 9 Apr 2024 15:04:27 GMT   (451kb,D)

Title: ClinLinker: Medical Entity Linking of Clinical Concept Mentions in
  Spanish
Authors: Fernando Gallego, Guillermo L\'opez-Garc\'ia, Luis Gasco-S\'anchez,
  Martin Krallinger, and Francisco J. Veredas
Categories: cs.CL
\\
  Advances in natural language processing techniques, such as named entity
recognition and normalization to widely used standardized terminologies like
UMLS or SNOMED-CT, along with the digitalization of electronic health records,
have significantly advanced clinical text analysis. This study presents
ClinLinker, a novel approach employing a two-phase pipeline for medical entity
linking that leverages the potential of in-domain adapted language models for
biomedical text mining: initial candidate retrieval using a SapBERT-based
bi-encoder and subsequent re-ranking with a cross-encoder, trained by following
a contrastive-learning strategy to be tailored to medical concepts in Spanish.
This methodology, focused initially on content in Spanish, substantially
outperforming multilingual language models designed for the same purpose. This
is true even for complex scenarios involving heterogeneous medical
terminologies and being trained on a subset of the original data. Our results,
evaluated using top-k accuracy at 25 and other top-k metrics, demonstrate our
approach's performance on two distinct clinical entity linking Gold Standard
corpora, DisTEMIST (diseases) and MedProcNER (clinical procedures),
outperforming previous benchmarks by 40 points in DisTEMIST and 43 points in
MedProcNER, both normalized to SNOMED-CT codes. These findings highlight our
approach's ability to address language-specific nuances and set a new benchmark
in entity linking, offering a potent tool for enhancing the utility of digital
medical records. The resulting system is of practical value, both for large
scale automatic generation of structured data derived from clinical records, as
well as for exhaustive extraction and harmonization of predefined clinical
variables of interest.
\\ ( https://arxiv.org/abs/2404.06367 ,  451kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06390
Date: Tue, 9 Apr 2024 15:33:09 GMT   (6878kb,D)

Title: Latent Distance Guided Alignment Training for Large Language Models
Authors: Haotian Luo, Wenhao Zheng, Huaxiu Yao
Categories: cs.CL
\\
  Ensuring alignment with human preferences is a crucial characteristic of
large language models (LLMs). Presently, the primary alignment methods, RLHF
and DPO, require extensive human annotation, which is expensive despite their
efficacy. The significant expenses associated with current alignment techniques
motivate researchers to investigate the development of annotation-free
alignment training methods. In pursuit of improved alignment without relying on
external annotation, we introduce Latent Distance Guided Alignment Training
(LD-Align). This approach seeks to align the model with a high-quality
supervised fine-tune dataset using guidance from a latent space. The latent
space is generated through sample reconstruction, akin to auto-encoding.
Consequently, we utilize the distance between sample pairs in the latent space
to guide DPO-based alignment training. Extensive experimentation and evaluation
show the efficacy of our proposed method in achieving notable alignment.
\\ ( https://arxiv.org/abs/2404.06390 ,  6878kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06392
Date: Tue, 9 Apr 2024 15:35:41 GMT   (8548kb,D)

Title: Event Extraction in Basque: Typologically motivated Cross-Lingual
  Transfer-Learning Analysis
Authors: Mikel Zubillaga, Oscar Sainz, Ainara Estarrona, Oier Lopez de Lacalle,
  Eneko Agirre
Categories: cs.CL cs.AI
Comments: Accepted at LREC-Coling 2024
\\
  Cross-lingual transfer-learning is widely used in Event Extraction for
low-resource languages and involves a Multilingual Language Model that is
trained in a source language and applied to the target language. This paper
studies whether the typological similarity between source and target languages
impacts the performance of cross-lingual transfer, an under-explored topic. We
first focus on Basque as the target language, which is an ideal target language
because it is typologically different from surrounding languages. Our
experiments on three Event Extraction tasks show that the shared linguistic
characteristic between source and target languages does have an impact on
transfer quality. Further analysis of 72 language pairs reveals that for tasks
that involve token classification such as entity and event trigger
identification, common writing script and morphological features produce higher
quality cross-lingual transfer. In contrast, for tasks involving structural
prediction like argument extraction, common word order is the most relevant
feature. In addition, we show that when increasing the training size, not all
the languages scale in the same way in the cross-lingual setting. To perform
the experiments we introduce EusIE, an event extraction dataset for Basque,
which follows the Multilingual Event Extraction dataset (MEE). The dataset and
code are publicly available.
\\ ( https://arxiv.org/abs/2404.06392 ,  8548kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06395
Date: Tue, 9 Apr 2024 15:36:50 GMT   (17017kb,D)

Title: MiniCPM: Unveiling the Potential of Small Language Models with Scalable
  Training Strategies
Authors: Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi
  Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, Xinrong Zhang, Zheng Leng
  Thai, Kaihuo Zhang, Chongyi Wang, Yuan Yao, Chenyang Zhao, Jie Zhou, Jie Cai,
  Zhongwu Zhai, Ning Ding, Chao Jia, Guoyang Zeng, Dahai Li, Zhiyuan Liu,
  Maosong Sun
Categories: cs.CL cs.LG
Comments: 17 pages paper, 7 pages Appendix
\\
  The burgeoning interest in developing Large Language Models (LLMs) with up to
trillion parameters has been met with concerns regarding resource efficiency
and practical expense, particularly given the immense cost of experimentation.
This scenario underscores the importance of exploring the potential of Small
Language Models (SLMs) as a resource-efficient alternative. In this context, we
introduce MiniCPM, specifically the 1.2B and 2.4B non-embedding parameter
variants, not only excel in their respective categories but also demonstrate
capabilities on par with 7B-13B LLMs. While focusing on SLMs, our approach
exhibits scalability in both model and data dimensions for future LLM research.
Regarding model scaling, we employ extensive model wind tunnel experiments for
stable and optimal scaling. For data scaling, we introduce a
Warmup-Stable-Decay (WSD) learning rate scheduler (LRS), conducive to
continuous training and domain adaptation. We present an in-depth analysis of
the intriguing training dynamics that occurred in the WSD LRS. With WSD LRS, we
are now able to efficiently study data-model scaling law without extensive
retraining experiments on both axes of model and data, from which we derive the
much higher compute optimal data-model ratio than Chinchilla Optimal.
Additionally, we introduce MiniCPM family, including MiniCPM-DPO, MiniCPM-MoE
and MiniCPM-128K, whose excellent performance further cementing MiniCPM's
foundation in diverse SLM applications. MiniCPM models are available publicly
at https://github.com/OpenBMB/MiniCPM .
\\ ( https://arxiv.org/abs/2404.06395 ,  17017kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06407
Date: Tue, 9 Apr 2024 15:54:16 GMT   (40495kb,D)

Title: Take a Look at it! Rethinking How to Evaluate Language Model Jailbreak
Authors: Hongyu Cai, Arjun Arunasalam, Leo Y. Lin, Antonio Bianchi, and Z.
  Berkay Celik
Categories: cs.CL cs.AI cs.CR cs.LG
\\
  Large language models (LLMs) have become increasingly integrated with various
applications. To ensure that LLMs do not generate unsafe responses, they are
aligned with safeguards that specify what content is restricted. However, such
alignment can be bypassed to produce prohibited content using a technique
commonly referred to as jailbreak. Different systems have been proposed to
perform the jailbreak automatically. These systems rely on evaluation methods
to determine whether a jailbreak attempt is successful. However, our analysis
reveals that current jailbreak evaluation methods have two limitations. (1)
Their objectives lack clarity and do not align with the goal of identifying
unsafe responses. (2) They oversimplify the jailbreak result as a binary
outcome, successful or not.
  In this paper, we propose three metrics, safeguard violation,
informativeness, and relative truthfulness, to evaluate language model
jailbreak. Additionally, we demonstrate how these metrics correlate with the
goal of different malicious actors. To compute these metrics, we introduce a
multifaceted approach that extends the natural language generation evaluation
method after preprocessing the response. We evaluate our metrics on a benchmark
dataset produced from three malicious intent datasets and three jailbreak
systems. The benchmark dataset is labeled by three annotators. We compare our
multifaceted approach with three existing jailbreak evaluation methods.
Experiments demonstrate that our multifaceted evaluation outperforms existing
methods, with F1 scores improving on average by 17% compared to existing
baselines. Our findings motivate the need to move away from the binary view of
the jailbreak problem and incorporate a more comprehensive evaluation to ensure
the safety of the language model.
\\ ( https://arxiv.org/abs/2404.06407 ,  40495kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06479
Date: Tue, 9 Apr 2024 17:30:18 GMT   (2755kb,D)

Title: Text-Based Reasoning About Vector Graphics
Authors: Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li,
  Jiajun Wu, Heng Ji
Categories: cs.CL cs.AI cs.CV
Comments: Project page: https://mikewangwzhl.github.io/VDLM/
\\
  While large multimodal models excel in broad vision-language benchmarks, they
often struggle with tasks requiring precise perception of low-level visual
details, such as comparing line lengths or solving simple mazes. In particular,
this failure mode persists in question-answering tasks about vector graphics --
images composed purely of 2D objects and shapes. To address this challenge, we
propose the Visually Descriptive Language Model (VDLM), which performs
text-based reasoning about vector graphics. VDLM leverages Scalable Vector
Graphics (SVG) for a more precise visual description and first uses an
off-the-shelf raster-to-SVG algorithm for encoding. Since existing language
models cannot understand raw SVGs in a zero-shot setting, VDLM then bridges SVG
with pretrained language models through a newly introduced intermediate
symbolic representation, Primal Visual Description (PVD), comprising primitive
attributes (e.g., shape, position, measurement) with their corresponding
predicted values. PVD is task-agnostic and represents visual primitives that
are universal across all vector graphics. It can be learned with procedurally
generated (SVG, PVD) pairs and also enables the direct use of LLMs for
generalization to complex reasoning tasks. By casting an image to a text-based
representation, we can leverage the power of language models to learn alignment
from SVG to visual primitives and generalize to unseen question-answering
tasks. Empirical results show that VDLM achieves stronger zero-shot performance
compared to state-of-the-art LMMs, such as GPT-4V, in various low-level
multimodal perception and reasoning tasks on vector graphics. We additionally
present extensive analyses on VDLM's performance, demonstrating that our
framework offers better interpretability due to its disentangled perception and
reasoning processes. Project page: https://mikewangwzhl.github.io/VDLM/
\\ ( https://arxiv.org/abs/2404.06479 ,  2755kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06480
Date: Tue, 9 Apr 2024 17:30:48 GMT   (702kb,D)

Title: Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks
Authors: Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, Kai Chen
Categories: cs.CL cs.AI
Comments: NAACL 2023
\\
  Recently, the large language model (LLM) community has shown increasing
interest in enhancing LLMs' capability to handle extremely long documents. As
various long-text techniques and model architectures emerge, the precise and
detailed evaluation of models' long-text capabilities has become increasingly
important. Existing long-text evaluation benchmarks, such as L-Eval and
LongBench, construct long-text test sets based on open-source datasets,
focusing mainly on QA and summarization tasks. These datasets include test
samples of varying lengths (from 2k to 32k+) entangled together, making it
challenging to assess model capabilities across different length ranges.
Moreover, they do not cover the ultralong settings (100k+ tokens) that the
latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a
length-adaptable benchmark for evaluating the long-context understanding of
LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which
enable a more reliable evaluation of LLMs' long context capabilities. These
benchmarks support intricate manipulation of the length of test cases, and can
easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art
closed-source API models and 6 open-source models with Ada-LEval. The
evaluation results demonstrate the limitations of current LLMs, especially in
ultra-long-context settings. Our code is available at
https://github.com/open-compass/Ada-LEval.
\\ ( https://arxiv.org/abs/2404.06480 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06488
Date: Tue, 9 Apr 2024 17:42:59 GMT   (830kb)

Title: Pitfalls of Conversational LLMs on News Debiasing
Authors: Ipek Baris Schlicht and Defne Altiok and Maryanne Taouk and Lucie Flek
Categories: cs.CL cs.AI
Comments: The paper is accepted at the DELITE workshop which is co-located at
  COLING/LREC
\\
  This paper addresses debiasing in news editing and evaluates the
effectiveness of conversational Large Language Models in this task. We designed
an evaluation checklist tailored to news editors' perspectives, obtained
generated texts from three popular conversational models using a subset of a
publicly available dataset in media bias, and evaluated the texts according to
the designed checklist. Furthermore, we examined the models as evaluator for
checking the quality of debiased model outputs. Our findings indicate that none
of the LLMs are perfect in debiasing. Notably, some models, including ChatGPT,
introduced unnecessary changes that may impact the author's style and create
misinformation. Lastly, we show that the models do not perform as proficiently
as domain experts in evaluating the quality of debiased outputs.
\\ ( https://arxiv.org/abs/2404.06488 ,  830kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06503
Date: Tue, 9 Apr 2024 17:54:10 GMT   (7742kb,D)

Title: Comparing Two Model Designs for Clinical Note Generation; Is an LLM a
  Useful Evaluator of Consistency?
Authors: Nathan Brake, Thomas Schaaf
Categories: cs.CL
Comments: Accepted to NAACL 2024 Findings
\\
  Following an interaction with a patient, physicians are responsible for the
submission of clinical documentation, often organized as a SOAP note. A
clinical note is not simply a summary of the conversation but requires the use
of appropriate medical terminology. The relevant information can then be
extracted and organized according to the structure of the SOAP note. In this
paper we analyze two different approaches to generate the different sections of
a SOAP note based on the audio recording of the conversation, and specifically
examine them in terms of note consistency. The first approach generates the
sections independently, while the second method generates them all together. In
this work we make use of PEGASUS-X Transformer models and observe that both
methods lead to similar ROUGE values (less than 1% difference) and have no
difference in terms of the Factuality metric. We perform a human evaluation to
measure aspects of consistency and demonstrate that LLMs like Llama2 can be
used to perform the same tasks with roughly the same agreement as the human
annotators. Between the Llama2 analysis and the human reviewers we observe a
Cohen Kappa inter-rater reliability of 0.79, 1.00, and 0.32 for consistency of
age, gender, and body part injury, respectively. With this we demonstrate the
usefulness of leveraging an LLM to measure quality indicators that can be
identified by humans but are not currently captured by automatic metrics. This
allows scaling evaluation to larger data sets, and we find that clinical note
consistency improves by generating each new section conditioned on the output
of all previously generated sections.
\\ ( https://arxiv.org/abs/2404.06503 ,  7742kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06508
Date: Tue, 9 Apr 2024 17:57:29 GMT   (1729kb,D)

Title: On the Effect of (Near) Duplicate Subwords in Language Modelling
Authors: Anton Sch\"afer, Thomas Hofmann, Imanol Schlag, Tiago Pimentel
Categories: cs.CL cs.LG
ACM-class: I.2.7
\\
  Tokenisation is a core part of language models (LMs). It involves splitting a
character sequence into subwords which are assigned arbitrary indices before
being served to the LM. While typically lossless, however, this process may
lead to less sample efficient LM training: as it removes character-level
information, it could make it harder for LMs to generalise across similar
subwords, such as now and Now. We refer to such subwords as near duplicates. In
this paper, we study the impact of near duplicate subwords on LM training
efficiency. First, we design an experiment that gives us an upper bound to how
much we should expect a model to improve if we could perfectly generalise
across near duplicates. We do this by duplicating each subword in our LM's
vocabulary, creating perfectly equivalent classes of subwords. Experimentally,
we find that LMs need roughly 17% more data when trained in a fully duplicated
setting. Second, we investigate the impact of naturally occurring near
duplicates on LMs. Here, we see that merging them considerably hurts LM
performance. Therefore, although subword duplication negatively impacts LM
training efficiency, naturally occurring near duplicates may not be as similar
as anticipated, limiting the potential for performance improvements.
\\ ( https://arxiv.org/abs/2404.06508 ,  1729kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05737
Date: Thu, 28 Mar 2024 18:06:00 GMT   (2420kb)

Title: Soil respiration signals in response to sustainable soil management
  practices enhance soil organic carbon stocks
Authors: Mario Guevara
Categories: cs.LG
Comments: 13 pages, 3 figures
\\
  Development of a spatial-temporal and data-driven model of soil respiration
at the global scale based on soil temperature, yearly soil moisture, and soil
organic carbon (C) estimates. Prediction of soil respiration on an annual basis
(1991-2018) with relatively high accuracy (NSE 0.69, CCC 0.82). Lower soil
respiration trends, higher soil respiration magnitudes, and higher soil organic
C stocks across areas experiencing the presence of sustainable soil management
practices.
\\ ( https://arxiv.org/abs/2404.05737 ,  2420kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05741
Date: Tue, 2 Apr 2024 19:53:54 GMT   (1247kb,D)

Title: Enhancing Inference Efficiency of Large Language Models: Investigating
  Optimization Strategies and Architectural Innovations
Authors: Georgy Tyukin
Categories: cs.LG cs.AI cs.CL cs.PF
\\
  Large Language Models are growing in size, and we expect them to continue to
do so, as larger models train quicker. However, this increase in size will
severely impact inference costs. Therefore model compression is important, to
retain the performance of larger models, but with a reduced cost of running
them. In this thesis we explore the methods of model compression, and we
empirically demonstrate that the simple method of skipping latter attention
sublayers in Transformer LLMs is an effective method of model compression, as
these layers prove to be redundant, whilst also being incredibly
computationally expensive. We observed a 21% speed increase in one-token
generation for Llama 2 7B, whilst surprisingly and unexpectedly improving
performance over several common benchmarks.
\\ ( https://arxiv.org/abs/2404.05741 ,  1247kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05768
Date: Sun, 7 Apr 2024 14:29:23 GMT   (1492kb,D)

Title: Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A
  Multiobjective Hyperparameter and Architecture Optimization Approach
Authors: Yixuan Sun, Ololade Sowunmi, Romain Egele, Sri Hari Krishna Narayanan,
  Luke Van Roekel, Prasanna Balaprakash
Categories: cs.LG physics.ao-ph stat.ML
\\
  Training an effective deep learning model to learn ocean processes involves
careful choices of various hyperparameters. We leverage DeepHyper's advanced
search algorithms for multiobjective optimization, streamlining the development
of neural networks tailored for ocean modeling. The focus is on optimizing
Fourier neural operators (FNOs), a data-driven model capable of simulating
complex ocean behaviors. Selecting the correct model and tuning the
hyperparameters are challenging tasks, requiring much effort to ensure model
accuracy. DeepHyper allows efficient exploration of hyperparameters associated
with data preprocessing, FNO architecture-related hyperparameters, and various
model training strategies. We aim to obtain an optimal set of hyperparameters
leading to the most performant model. Moreover, on top of the commonly used
mean squared error for model training, we propose adopting the negative anomaly
correlation coefficient as the additional loss term to improve model
performance and investigate the potential trade-off between the two terms. The
experimental results show that the optimal set of hyperparameters enhanced
model performance in single timestepping forecasting and greatly exceeded the
baseline configuration in the autoregressive rollout for long-horizon
forecasting up to 30 days. Utilizing DeepHyper, we demonstrate an approach to
enhance the use of FNOs in ocean dynamics forecasting, offering a scalable
solution with improved precision.
\\ ( https://arxiv.org/abs/2404.05768 ,  1492kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05774
Date: Mon, 8 Apr 2024 03:38:52 GMT   (10721kb,D)

Title: STMGF: An Effective Spatial-Temporal Multi-Granularity Framework for
  Traffic Forecasting
Authors: Zhengyang Zhao, Haitao Yuan, Nan Jiang, Minxiao Chen, Ning Liu,
  Zengxiang Li
Categories: cs.LG cs.AI
\\
  Accurate Traffic Prediction is a challenging task in intelligent
transportation due to the spatial-temporal aspects of road networks. The
traffic of a road network can be affected by long-distance or long-term
dependencies where existing methods fall short in modeling them. In this paper,
we introduce a novel framework known as Spatial-Temporal Multi-Granularity
Framework (STMGF) to enhance the capture of long-distance and long-term
information of the road networks. STMGF makes full use of different granularity
information of road networks and models the long-distance and long-term
information by gathering information in a hierarchical interactive way.
Further, it leverages the inherent periodicity in traffic sequences to refine
prediction results by matching with recent traffic data. We conduct experiments
on two real-world datasets, and the results demonstrate that STMGF outperforms
all baseline models and achieves state-of-the-art performance.
\\ ( https://arxiv.org/abs/2404.05774 ,  10721kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05779
Date: Mon, 8 Apr 2024 15:19:57 GMT   (384kb,D)

Title: Data Readiness for AI: A 360-Degree Survey
Authors: Kaveen Hiniduma, Suren Byna and Jean Luca Bez
Categories: cs.LG cs.AI
Comments: 35 pages, 3 figures, 3 tables, submitted to ACM Computing Surveys
ACM-class: I.2.0; E.m
\\
  Data are the critical fuel for Artificial Intelligence (AI) models. Poor
quality data produces inaccurate and ineffective AI models that may lead to
incorrect or unsafe use. Checking for data readiness is a crucial step in
improving data quality. Numerous R&D efforts have been spent on improving data
quality. However, standardized metrics for evaluating data readiness for use in
AI training are still evolving. In this study, we perform a comprehensive
survey of metrics used for verifying AI's data readiness. This survey examines
more than 120 papers that are published by ACM Digital Library, IEEE Xplore,
other reputable journals, and articles published on the web by prominent AI
experts. This survey aims to propose a taxonomy of data readiness for AI (DRAI)
metrics for structured and unstructured datasets. We anticipate that this
taxonomy can lead to new standards for DRAI metrics that would be used for
enhancing the quality and accuracy of AI training and inference.
\\ ( https://arxiv.org/abs/2404.05779 ,  384kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05782
Date: Mon, 8 Apr 2024 17:33:11 GMT   (4890kb,D)

Title: Dynamical stability and chaos in artificial neural network trajectories
  along training
Authors: Kaloyan Danovski, Miguel C. Soriano, Lucas Lacasa
Categories: cs.LG cond-mat.dis-nn nlin.CD physics.data-an
Comments: 29 pages, 18 figures
\\
  The process of training an artificial neural network involves iteratively
adapting its parameters so as to minimize the error of the network's
prediction, when confronted with a learning task. This iterative change can be
naturally interpreted as a trajectory in network space -- a time series of
networks -- and thus the training algorithm (e.g. gradient descent optimization
of a suitable loss function) can be interpreted as a dynamical system in graph
space. In order to illustrate this interpretation, here we study the dynamical
properties of this process by analyzing through this lens the network
trajectories of a shallow neural network, and its evolution through learning a
simple classification task. We systematically consider different ranges of the
learning rate and explore both the dynamical and orbital stability of the
resulting network trajectories, finding hints of regular and chaotic behavior
depending on the learning rate regime. Our findings are put in contrast to
common wisdom on convergence properties of neural networks and dynamical
systems theory. This work also contributes to the cross-fertilization of ideas
between dynamical systems theory, network theory and machine learning
\\ ( https://arxiv.org/abs/2404.05782 ,  4890kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05809
Date: Mon, 8 Apr 2024 18:16:22 GMT   (975kb,D)

Title: Self-Labeling in Multivariate Causality and Quantification for Adaptive
  Machine Learning
Authors: Yutian Ren, Aaron Haohua Yen, G. P. Li
Categories: cs.LG cs.AI stat.ME
\\
  Adaptive machine learning (ML) aims to allow ML models to adapt to
ever-changing environments with potential concept drift after model deployment.
Traditionally, adaptive ML requires a new dataset to be manually labeled to
tailor deployed models to altered data distributions. Recently, an interactive
causality based self-labeling method was proposed to autonomously associate
causally related data streams for domain adaptation, showing promising results
compared to traditional feature similarity-based semi-supervised learning.
Several unanswered research questions remain, including self-labeling's
compatibility with multivariate causality and the quantitative analysis of the
auxiliary models used in the self-labeling. The auxiliary models, the
interaction time model (ITM) and the effect state detector (ESD), are vital to
the success of self-labeling. This paper further develops the self-labeling
framework and its theoretical foundations to address these research questions.
A framework for the application of self-labeling to multivariate causal graphs
is proposed using four basic causal relationships, and the impact of non-ideal
ITM and ESD performance is analyzed. A simulated experiment is conducted based
on a multivariate causal graph, validating the proposed theory.
\\ ( https://arxiv.org/abs/2404.05809 ,  975kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05817
Date: Mon, 8 Apr 2024 18:41:55 GMT   (1475kb,D)

Title: Label Propagation Training Schemes for Physics-Informed Neural Networks
  and Gaussian Processes
Authors: Ming Zhong, Dehao Liu, Raymundo Arroyave, Ulisses Braga-Neto
Categories: cs.LG
\\
  This paper proposes a semi-supervised methodology for training
physics-informed machine learning methods. This includes self-training of
physics-informed neural networks and physics-informed Gaussian processes in
isolation, and the integration of the two via co-training. We demonstrate via
extensive numerical experiments how these methods can ameliorate the issue of
propagating information forward in time, which is a common failure mode of
physics-informed machine learning.
\\ ( https://arxiv.org/abs/2404.05817 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05840
Date: Mon, 8 Apr 2024 20:06:33 GMT   (706kb,D)

Title: Attention-Driven Multi-Agent Reinforcement Learning: Enhancing Decisions
  with Expertise-Informed Tasks
Authors: Andre R Kuroswiski, Annie S Wu, Angelo Passaro
Categories: cs.LG cs.AI cs.MA
\\
  In this paper, we introduce an alternative approach to enhancing Multi-Agent
Reinforcement Learning (MARL) through the integration of domain knowledge and
attention-based policy mechanisms. Our methodology focuses on the incorporation
of domain-specific expertise into the learning process, which simplifies the
development of collaborative behaviors. This approach aims to reduce the
complexity and learning overhead typically associated with MARL by enabling
agents to concentrate on essential aspects of complex tasks, thus optimizing
the learning curve. The utilization of attention mechanisms plays a key role in
our model. It allows for the effective processing of dynamic context data and
nuanced agent interactions, leading to more refined decision-making. Applied in
standard MARL scenarios, such as the Stanford Intelligent Systems Laboratory
(SISL) Pursuit and Multi-Particle Environments (MPE) Simple Spread, our method
has been shown to improve both learning efficiency and the effectiveness of
collaborative behaviors. The results indicate that our attention-based approach
can be a viable approach for improving the efficiency of MARL training process,
integrating domain-specific knowledge at the action level.
\\ ( https://arxiv.org/abs/2404.05840 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05843
Date: Mon, 8 Apr 2024 20:14:10 GMT   (22kb)

Title: Softmax Attention with Constant Cost per Token
Authors: Franz A. Heinsen
Categories: cs.LG cs.CL
Comments: Source code and instructions for replicating our results are online
  at https://github.com/glassroom/heinsen_attention
\\
  We propose a simple modification to the conventional attention mechanism
applied by Transformers: Instead of quantifying pairwise query-key similarity
with scaled dot-products, we quantify it with the logarithms of scaled
dot-products of exponentials. Attention becomes expressible as a composition of
log-sums of exponentials that is linearizable, with a latent space of constant
size, enabling sequential application with constant time and space complexity
per token. We implement our modification, verify that it works in practice, and
conclude that it is a promising alternative to conventional attention.
\\ ( https://arxiv.org/abs/2404.05843 ,  22kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05868
Date: Mon, 8 Apr 2024 21:05:42 GMT   (19440kb,D)

Title: Negative Preference Optimization: From Catastrophic Collapse to
  Effective Unlearning
Authors: Ruiqi Zhang, Licong Lin, Yu Bai, Song Mei
Categories: cs.LG cs.AI cs.CL stat.ML
\\
  Large Language Models (LLMs) often memorize sensitive, private, or
copyrighted data during pre-training. LLM unlearning aims to eliminate the
influence of undesirable data from the pre-trained model while preserving the
model's utilities on other tasks. Several practical methods have recently been
proposed for LLM unlearning, mostly based on gradient ascent (GA) on the loss
of undesirable data. However, on certain unlearning tasks, these methods either
fail to effectively unlearn the target data or suffer from catastrophic
collapse -- a drastic degradation of the model's utilities.
  In this paper, we propose Negative Preference Optimization (NPO), a simple
alignment-inspired method that could efficiently and effectively unlearn a
target dataset. We theoretically show that the progression toward catastrophic
collapse by minimizing the NPO loss is exponentially slower than GA. Through
experiments on synthetic data and the benchmark TOFU dataset, we demonstrate
that NPO-based methods achieve a better balance between unlearning the
undesirable data and maintaining the model's utilities. We also observe that
NPO-based methods generate more sensible outputs than GA-based methods, whose
outputs are often gibberish. Remarkably, on TOFU, NPO-based methods are the
first to achieve reasonable unlearning results in forgetting 50% (or more) of
the training data, whereas existing methods already struggle with forgetting
10% of training data.
\\ ( https://arxiv.org/abs/2404.05868 ,  19440kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05879
Date: Mon, 8 Apr 2024 21:26:04 GMT   (9375kb,D)

Title: Rapid and Precise Topological Comparison with Merge Tree Neural Networks
Authors: Yu Qin, Brittany Terese Fasy, Carola Wenk, Brian Summa
Categories: cs.LG cs.CG
Comments: under review
\\
  Merge trees are a valuable tool in scientific visualization of scalar fields;
however, current methods for merge tree comparisons are computationally
expensive, primarily due to the exhaustive matching between tree nodes. To
address this challenge, we introduce the merge tree neural networks (MTNN), a
learned neural network model designed for merge tree comparison. The MTNN
enables rapid and high-quality similarity computation. We first demonstrate how
graph neural networks (GNNs), which emerged as an effective encoder for graphs,
can be trained to produce embeddings of merge trees in vector spaces that
enable efficient similarity comparison. Next, we formulate the novel MTNN model
that further improves the similarity comparisons by integrating the tree and
node embeddings with a new topological attention mechanism. We demonstrate the
effectiveness of our model on real-world data in different domains and examine
our model's generalizability across various datasets. Our experimental analysis
demonstrates our approach's superiority in accuracy and efficiency. In
particular, we speed up the prior state-of-the-art by more than 100x on the
benchmark datasets while maintaining an error rate below 0.1%.
\\ ( https://arxiv.org/abs/2404.05879 ,  9375kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05894
Date: Mon, 8 Apr 2024 22:40:57 GMT   (3858kb,D)

Title: Learning Heuristics for Transit Network Design and Improvement with Deep
  Reinforcement Learning
Authors: Andrew Holliday, Ahmed El-Geneidy, Gregory Dudek
Categories: cs.LG cs.AI cs.NE
Comments: In preparation for submission to the journal "Transportation Research
  Part C"
\\
  Transit agencies world-wide face tightening budgets. To maintain quality of
service while cutting costs, efficient transit network design is essential. But
planning a network of public transit routes is a challenging optimization
problem. The most successful approaches to date use metaheuristic algorithms to
search through the space of solutions by applying low-level heuristics that
randomly alter routes in a network. The design of these low-level heuristics
has a major impact on the quality of the result. In this paper we use deep
reinforcement learning with graph neural nets to learn low-level heuristics for
an evolutionary algorithm, instead of designing them manually. These learned
heuristics improve the algorithm's results on benchmark synthetic cities with
70 nodes or more, and obtain state-of-the-art results when optimizing operating
costs. They also improve upon a simulation of the real transit network in the
city of Laval, Canada, by as much as 54% and 18% on two key metrics, and offer
cost savings of up to 12% over the city's existing transit network.
\\ ( https://arxiv.org/abs/2404.05894 ,  3858kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05903
Date: Mon, 8 Apr 2024 23:15:41 GMT   (1373kb,D)

Title: Natural Learning
Authors: Hadi Fanaee-T
Categories: cs.LG cs.AI
Comments: 10 pages, 5 figures
\\
  We introduce Natural Learning (NL), a novel algorithm that elevates the
explainability and interpretability of machine learning to an extreme level. NL
simplifies decisions into intuitive rules, like "We rejected your loan because
your income, employment status, and age collectively resemble a rejected
prototype more than an accepted prototype." When applied to real-life datasets,
NL produces impressive results. For example, in a colon cancer dataset with
1545 patients and 10935 genes, NL achieves 98.1% accuracy, comparable to DNNs
and RF, by analyzing just 3 genes of test samples against 2 discovered
prototypes. Similarly, in the UCI's WDBC dataset, NL achieves 98.3% accuracy
using only 7 features and 2 prototypes. Even on the MNIST dataset (0 vs. 1), NL
achieves 99.5% accuracy with only 3 pixels from 2 prototype images. NL is
inspired by prototype theory, an old concept in cognitive psychology suggesting
that people learn single sparse prototypes to categorize objects. Leveraging
this relaxed assumption, we redesign Support Vector Machines (SVM), replacing
its mathematical formulation with a fully nearest-neighbor-based solution, and
to address the curse of dimensionality, we utilize locality-sensitive hashing.
Following theory's generalizability principle, we propose a recursive method to
prune non-core features. As a result, NL efficiently discovers the sparsest
prototypes in O(n^2pL) with high parallelization capacity in terms of n.
Evaluation of NL with 17 benchmark datasets shows its significant
outperformance compared to decision trees and logistic regression, two methods
widely favored in healthcare for their interpretability. Moreover, NL achieves
performance comparable to finetuned black-box models such as deep neural
networks and random forests in 40% of cases, with only a 1-2% lower average
accuracy. The code is available via http://natural-learning.cc.
\\ ( https://arxiv.org/abs/2404.05903 ,  1373kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05908
Date: Mon, 8 Apr 2024 23:46:59 GMT   (4447kb,D)

Title: Interpretability in Symbolic Regression: a benchmark of Explanatory
  Methods using the Feynman data set
Authors: Guilherme Seidyo Imai Aldeia and Fabricio Olivetti de Franca (Federal
  University of ABC)
Categories: cs.LG cs.AI
Comments: 47 pages, 10 figures. This is a post peer-review, pre-copyedit
  version of an article published in Genetic Programming and Evolvable Machines
  Volume 23, pages 309-349, (2022). The final version is available on
  https://link.springer.com/article/10.1007/s10710-022-09435-x
Journal-ref: Aldeia, G.S.I., de Franca, F.O. Interpretability in symbolic
  regression: a benchmark of explanatory methods using the Feynman data set.
  Genet Program Evolvable Mach 23, 309-349 (2022)
DOI: 10.1007/s10710-022-09435-x
\\
  In some situations, the interpretability of the machine learning models plays
a role as important as the model accuracy. Interpretability comes from the need
to trust the prediction model, verify some of its properties, or even enforce
them to improve fairness. Many model-agnostic explanatory methods exists to
provide explanations for black-box models. In the regression task, the
practitioner can use white-boxes or gray-boxes models to achieve more
interpretable results, which is the case of symbolic regression. When using an
explanatory method, and since interpretability lacks a rigorous definition,
there is a need to evaluate and compare the quality and different explainers.
This paper proposes a benchmark scheme to evaluate explanatory methods to
explain regression models, mainly symbolic regression models. Experiments were
performed using 100 physics equations with different interpretable and
non-interpretable regression methods and popular explanation methods,
evaluating the performance of the explainers performance with several
explanation measures. In addition, we further analyzed four benchmarks from the
GP community. The results have shown that Symbolic Regression models can be an
interesting alternative to white-box and black-box models that is capable of
returning accurate models with appropriate explanations. Regarding the
explainers, we observed that Partial Effects and SHAP were the most robust
explanation models, with Integrated Gradients being unstable only with
tree-based models. This benchmark is publicly available for further
experiments.
\\ ( https://arxiv.org/abs/2404.05908 ,  4447kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05913
Date: Tue, 9 Apr 2024 00:07:16 GMT   (1803kb,D)

Title: Deep Reinforcement Learning for Personalized Diagnostic Decision
  Pathways Using Electronic Health Records: A Comparative Study on Anemia and
  Systemic Lupus Erythematosus
Authors: Lillian Muyama, Antoine Neuraz, Adrien Coulet
Categories: cs.LG cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2305.06295
\\
  Background: Clinical diagnosis is typically reached by following a series of
steps recommended by guidelines authored by colleges of experts. Accordingly,
guidelines play a crucial role in rationalizing clinical decisions but suffer
from limitations as they are built to cover the majority of the population and
fail at covering patients with uncommon conditions. Moreover, their updates are
long and expensive, making them unsuitable for emerging diseases and practices.
  Methods: Inspired by guidelines, we formulate the task of diagnosis as a
sequential decision-making problem and study the use of Deep Reinforcement
Learning (DRL) algorithms to learn the optimal sequence of actions to perform
in order to obtain a correct diagnosis from Electronic Health Records (EHRs).
We apply DRL on synthetic, but realistic EHRs and develop two clinical use
cases: Anemia diagnosis, where the decision pathways follow the schema of a
decision tree; and Systemic Lupus Erythematosus (SLE) diagnosis, which follows
a weighted criteria score. We particularly evaluate the robustness of our
approaches to noisy and missing data since these frequently occur in EHRs.
  Results: In both use cases, and in the presence of imperfect data, our best
DRL algorithms exhibit competitive performance when compared to the traditional
classifiers, with the added advantage that they enable the progressive
generation of a pathway to the suggested diagnosis which can both guide and
explain the decision-making process.
  Conclusion: DRL offers the opportunity to learn personalized decision
pathways to diagnosis. We illustrate with our two use cases their advantages:
they generate step-by-step pathways that are self-explanatory; and their
correctness is competitive when compared to state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2404.05913 ,  1803kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05919
Date: Tue, 9 Apr 2024 00:43:45 GMT   (47kb,D)

Title: AdaGossip: Adaptive Consensus Step-size for Decentralized Deep Learning
  with Communication Compression
Authors: Sai Aparna Aketi, Abolfazl Hashemi, Kaushik Roy
Categories: cs.LG
Comments: 11 pages, 3 figures, 8 tables
\\
  Decentralized learning is crucial in supporting on-device learning over large
distributed datasets, eliminating the need for a central server. However, the
communication overhead remains a major bottleneck for the practical realization
of such decentralized setups. To tackle this issue, several algorithms for
decentralized training with compressed communication have been proposed in the
literature. Most of these algorithms introduce an additional hyper-parameter
referred to as consensus step-size which is tuned based on the compression
ratio at the beginning of the training. In this work, we propose AdaGossip, a
novel technique that adaptively adjusts the consensus step-size based on the
compressed model differences between neighboring agents. We demonstrate the
effectiveness of the proposed method through an exhaustive set of experiments
on various Computer Vision datasets (CIFAR-10, CIFAR-100, Fashion MNIST,
Imagenette, and ImageNet), model architectures, and network topologies. Our
experiments show that the proposed method achieves superior performance
($0-2\%$ improvement in test accuracy) compared to the current state-of-the-art
method for decentralized learning with communication compression.
\\ ( https://arxiv.org/abs/2404.05919 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05938
Date: Tue, 9 Apr 2024 01:43:02 GMT   (836kb,D)

Title: Neural networks can be FLOP-efficient integrators of 1D oscillatory
  integrands
Authors: Anshuman Sinha and Spencer H. Bryngelson
Categories: cs.LG cs.CE cs.NA math.NA
Comments: 11 pages, 7 figures, 3 tables. Published in TMLR 03/2024. Code at
  https://github.com/comp-physics/deepOscillations
Journal-ref: Transactions on Machine Learning Research; ISSN 2835-8856 (2024)
\\
  We demonstrate that neural networks can be FLOP-efficient integrators of
one-dimensional oscillatory integrands. We train a feed-forward neural network
to compute integrals of highly oscillatory 1D functions. The training set is a
parametric combination of functions with varying characters and oscillatory
behavior degrees. Numerical examples show that these networks are
FLOP-efficient for sufficiently oscillatory integrands with an average FLOP
gain of 1000 FLOPs. The network calculates oscillatory integrals better than
traditional quadrature methods under the same computational budget or number of
floating point operations. We find that feed-forward networks of 5 hidden
layers are satisfactory for a relative accuracy of 0.001. The computational
burden of inference of the neural network is relatively small, even compared to
inner-product pattern quadrature rules. We postulate that our result follows
from learning latent patterns in the oscillatory integrands that are otherwise
opaque to traditional numerical integrators.
\\ ( https://arxiv.org/abs/2404.05938 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05950
Date: Tue, 9 Apr 2024 02:11:35 GMT   (1381kb,D)

Title: Efficient Multi-Task Reinforcement Learning via Task-Specific Action
  Correction
Authors: Jinyuan Feng, Min Chen, Zhiqiang Pu, Tenghai Qiu, Jianqiang Yi
Categories: cs.LG cs.AI cs.RO
\\
  Multi-task reinforcement learning (MTRL) demonstrate potential for enhancing
the generalization of a robot, enabling it to perform multiple tasks
concurrently. However, the performance of MTRL may still be susceptible to
conflicts between tasks and negative interference. To facilitate efficient
MTRL, we propose Task-Specific Action Correction (TSAC), a general and
complementary approach designed for simultaneous learning of multiple tasks.
TSAC decomposes policy learning into two separate policies: a shared policy
(SP) and an action correction policy (ACP). To alleviate conflicts resulting
from excessive focus on specific tasks' details in SP, ACP incorporates
goal-oriented sparse rewards, enabling an agent to adopt a long-term
perspective and achieve generalization across tasks. Additional rewards
transform the original problem into a multi-objective MTRL problem.
Furthermore, to convert the multi-objective MTRL into a single-objective
formulation, TSAC assigns a virtual expected budget to the sparse rewards and
employs Lagrangian method to transform a constrained single-objective
optimization into an unconstrained one. Experimental evaluations conducted on
Meta-World's MT10 and MT50 benchmarks demonstrate that TSAC outperforms
existing state-of-the-art methods, achieving significant improvements in both
sample efficiency and effective action execution.
\\ ( https://arxiv.org/abs/2404.05950 ,  1381kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05971
Date: Tue, 9 Apr 2024 02:59:17 GMT   (4990kb,D)

Title: Does Transformer Interpretability Transfer to RNNs?
Authors: Gon\c{c}alo Paulo, Thomas Marshall, Nora Belrose
Categories: cs.LG cs.AI cs.CL
\\
  Recent advances in recurrent neural network architectures, such as Mamba and
RWKV, have enabled RNNs to match or exceed the performance of equal-size
transformers in terms of language modeling perplexity and downstream
evaluations, suggesting that future systems may be built on completely new
architectures. In this paper, we examine if selected interpretability methods
originally designed for transformer language models will transfer to these
up-and-coming recurrent architectures. Specifically, we focus on steering model
outputs via contrastive activation addition, on eliciting latent predictions
via the tuned lens, and eliciting latent knowledge from models fine-tuned to
produce false outputs under certain conditions. Our results show that most of
these techniques are effective when applied to RNNs, and we show that it is
possible to improve some of them by taking advantage of RNNs' compressed state.
\\ ( https://arxiv.org/abs/2404.05971 ,  4990kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05976
Date: Tue, 9 Apr 2024 03:10:45 GMT   (2334kb,D)

Title: A Cyber Manufacturing IoT System for Adaptive Machine Learning Model
  Deployment by Interactive Causality Enabled Self-Labeling
Authors: Yutian Ren, Yuqi He, Xuyin Zhang, Aaron Yen, G. P. Li
Categories: cs.LG cs.SY eess.SY stat.ME
\\
  Machine Learning (ML) has been demonstrated to improve productivity in many
manufacturing applications. To host these ML applications, several software and
Industrial Internet of Things (IIoT) systems have been proposed for
manufacturing applications to deploy ML applications and provide real-time
intelligence. Recently, an interactive causality enabled self-labeling method
has been proposed to advance adaptive ML applications in cyber-physical
systems, especially manufacturing, by automatically adapting and personalizing
ML models after deployment to counter data distribution shifts. The unique
features of the self-labeling method require a novel software system to support
dynamism at various levels.
  This paper proposes the AdaptIoT system, comprised of an end-to-end data
streaming pipeline, ML service integration, and an automated self-labeling
service. The self-labeling service consists of causal knowledge bases and
automated full-cycle self-labeling workflows to adapt multiple ML models
simultaneously. AdaptIoT employs a containerized microservice architecture to
deliver a scalable and portable solution for small and medium-sized
manufacturers. A field demonstration of a self-labeling adaptive ML application
is conducted with a makerspace and shows reliable performance.
\\ ( https://arxiv.org/abs/2404.05976 ,  2334kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05981
Date: Tue, 9 Apr 2024 03:27:09 GMT   (170kb,D)

Title: A Lightweight Measure of Classification Difficulty from Application
  Dataset Characteristics
Authors: Bryan Bo Cao, Abhinav Sharma, Lawrence O'Gorman, Michael Coss, Shubham
  Jain
Categories: cs.LG cs.CV
Comments: 13 pages, 3 figures
MSC-class: 65D19
\\
  Despite accuracy and computation benchmarks being widely available to help
choose among neural network models, these are usually trained on datasets with
many classes, and do not give a precise idea of performance for applications of
few (< 10) classes. The conventional procedure to predict performance is to
train and test repeatedly on the different models and dataset variations of
interest. However, this is computationally expensive. We propose an efficient
classification difficulty measure that is calculated from the number of classes
and intra- and inter-class similarity metrics of the dataset. After a single
stage of training and testing per model family, relative performance for
different datasets and models of the same family can be predicted by comparing
difficulty measures - without further training and testing. We show how this
measure can help a practitioner select a computationally efficient model for a
small dataset 6 to 29x faster than through repeated training and testing. We
give an example of use of the measure for an industrial application in which
options are identified to select a model 42% smaller than the baseline
YOLOv5-nano model, and if class merging from 3 to 2 classes meets requirements,
85% smaller.
\\ ( https://arxiv.org/abs/2404.05981 ,  170kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05993
Date: Tue, 9 Apr 2024 03:54:28 GMT   (5712kb,D)

Title: AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM
  Experts
Authors: Shaona Ghosh, Prasoon Varshney, Erick Galinkin, Christopher Parisien
Categories: cs.LG cs.CL cs.CY
\\
  As Large Language Models (LLMs) and generative AI become more widespread, the
content safety risks associated with their use also increase. We find a notable
deficiency in high-quality content safety datasets and benchmarks that
comprehensively cover a wide range of critical safety areas. To address this,
we define a broad content safety risk taxonomy, comprising 13 critical risk and
9 sparse risk categories. Additionally, we curate AEGISSAFETYDATASET, a new
dataset of approximately 26, 000 human-LLM interaction instances, complete with
human annotations adhering to the taxonomy. We plan to release this dataset to
the community to further research and to help benchmark LLM models for safety.
To demonstrate the effectiveness of the dataset, we instruction-tune multiple
LLM-based safety models. We show that our models (named AEGISSAFETYEXPERTS),
not only surpass or perform competitively with the state-of-the-art LLM-based
safety models and general purpose LLMs, but also exhibit robustness across
multiple jail-break attack categories. We also show how using
AEGISSAFETYDATASET during the LLM alignment phase does not negatively impact
the performance of the aligned models on MT Bench scores. Furthermore, we
propose AEGIS, a novel application of a no-regret online adaptation framework
with strong theoretical guarantees, to perform content moderation with an
ensemble of LLM content safety experts in deployment
\\ ( https://arxiv.org/abs/2404.05993 ,  5712kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06013
Date: Tue, 9 Apr 2024 04:45:18 GMT   (1277kb,D)

Title: Feel-Good Thompson Sampling for Contextual Dueling Bandits
Authors: Xuheng Li, Heyang Zhao, Quanquan Gu
Categories: cs.LG math.OC stat.ML
Comments: 30 pages, 6 figures
\\
  Contextual dueling bandits, where a learner compares two options based on
context and receives feedback indicating which was preferred, extends classic
dueling bandits by incorporating contextual information for decision-making and
preference learning. Several algorithms based on the upper confidence bound
(UCB) have been proposed for linear contextual dueling bandits. However, no
algorithm based on posterior sampling has been developed in this setting,
despite the empirical success observed in traditional contextual bandits. In
this paper, we propose a Thompson sampling algorithm, named FGTS.CDB, for
linear contextual dueling bandits. At the core of our algorithm is a new
Feel-Good exploration term specifically tailored for dueling bandits. This term
leverages the independence of the two selected arms, thereby avoiding a cross
term in the analysis. We show that our algorithm achieves nearly
minimax-optimal regret, i.e., $\tilde{\mathcal{O}}(d\sqrt T)$, where $d$ is the
model dimension and $T$ is the time horizon. Finally, we evaluate our algorithm
on synthetic data and observe that FGTS.CDB outperforms existing algorithms by
a large margin.
\\ ( https://arxiv.org/abs/2404.06013 ,  1277kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06090
Date: Tue, 9 Apr 2024 07:49:05 GMT   (1220kb,D)

Title: Fair Graph Neural Network with Supervised Contrastive Regularization
Authors: Mahdi Tavassoli Kejani (UT3), Fadi Dornaika, Jean-Michel Loubes (IMT)
Categories: cs.LG cs.AI
\\
  In recent years, Graph Neural Networks (GNNs) have made significant
advancements, particularly in tasks such as node classification, link
prediction, and graph representation. However, challenges arise from biases
that can be hidden not only in the node attributes but also in the connections
between entities. Therefore, ensuring fairness in graph neural network learning
has become a critical problem. To address this issue, we propose a novel model
for training fairness-aware GNN, which enhances the Counterfactual Augmented
Fair Graph Neural Network Framework (CAF). Our approach integrates Supervised
Contrastive Loss and Environmental Loss to enhance both accuracy and fairness.
Experimental validation on three real datasets demonstrates the superiority of
our proposed model over CAF and several other existing graph-based learning
methods.
\\ ( https://arxiv.org/abs/2404.06090 ,  1220kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06106
Date: Tue, 9 Apr 2024 08:17:32 GMT   (1512kb,D)

Title: Unifying Low Dimensional Observations in Deep Learning Through the Deep
  Linear Unconstrained Feature Model
Authors: Connall Garrod and Jonathan P. Keating
Categories: cs.LG
Comments: 35 pages, 14 figures
\\
  Modern deep neural networks have achieved high performance across various
tasks. Recently, researchers have noted occurrences of low-dimensional
structure in the weights, Hessian's, gradients, and feature vectors of these
networks, spanning different datasets and architectures when trained to
convergence. In this analysis, we theoretically demonstrate these observations
arising, and show how they can be unified within a generalized unconstrained
feature model that can be considered analytically. Specifically, we consider a
previously described structure called Neural Collapse, and its multi-layer
counterpart, Deep Neural Collapse, which emerges when the network approaches
global optima. This phenomenon explains the other observed low-dimensional
behaviours on a layer-wise level, such as the bulk and outlier structure seen
in Hessian spectra, and the alignment of gradient descent with the outlier
eigenspace of the Hessian. Empirical results in both the deep linear
unconstrained feature model and its non-linear equivalent support these
predicted observations.
\\ ( https://arxiv.org/abs/2404.06106 ,  1512kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06144
Date: Tue, 9 Apr 2024 09:09:36 GMT   (2310kb,D)

Title: Differential Privacy for Anomaly Detection: Analyzing the Trade-off
  Between Privacy and Explainability
Authors: Fatima Ezzeddine, Mirna Saad, Omran Ayoub, Davide Andreoletti, Martin
  Gjoreski, Ihab Sbeity, Marc Langheinrich, Silvia Giordano
Categories: cs.LG cs.AI
\\
  Anomaly detection (AD), also referred to as outlier detection, is a
statistical process aimed at identifying observations within a dataset that
significantly deviate from the expected pattern of the majority of the data.
Such a process finds wide application in various fields, such as finance and
healthcare. While the primary objective of AD is to yield high detection
accuracy, the requirements of explainability and privacy are also paramount.
The first ensures the transparency of the AD process, while the second
guarantees that no sensitive information is leaked to untrusted parties. In
this work, we exploit the trade-off of applying Explainable AI (XAI) through
SHapley Additive exPlanations (SHAP) and differential privacy (DP). We perform
AD with different models and on various datasets, and we thoroughly evaluate
the cost of privacy in terms of decreased accuracy and explainability. Our
results show that the enforcement of privacy through DP has a significant
impact on detection accuracy and explainability, which depends on both the
dataset and the considered AD model. We further show that the visual
interpretation of explanations is also influenced by the choice of the AD
algorithm.
\\ ( https://arxiv.org/abs/2404.06144 ,  2310kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06153
Date: Tue, 9 Apr 2024 09:25:16 GMT   (1788kb,D)

Title: scRDiT: Generating single-cell RNA-seq data by diffusion transformers
  and accelerating sampling
Authors: Shengze Dong, Zhuorui Cui, Ding Liu, Jinzhi Lei
Categories: cs.LG q-bio.GN
Comments: 11 pages, 4 figures,
\\
  Motivation: Single-cell RNA sequencing (scRNA-seq) is a groundbreaking
technology extensively utilized in biological research, facilitating the
examination of gene expression at the individual cell level within a given
tissue sample. While numerous tools have been developed for scRNA-seq data
analysis, the challenge persists in capturing the distinct features of such
data and replicating virtual datasets that share analogous statistical
properties. Results: Our study introduces a generative approach termed
scRNA-seq Diffusion Transformer (scRDiT). This method generates virtual
scRNA-seq data by leveraging a real dataset. The method is a neural network
constructed based on Denoising Diffusion Probabilistic Models (DDPMs) and
Diffusion Transformers (DiTs). This involves subjecting Gaussian noises to the
real dataset through iterative noise-adding steps and ultimately restoring the
noises to form scRNA-seq samples. This scheme allows us to learn data features
from actual scRNA-seq samples during model training. Our experiments, conducted
on two distinct scRNA-seq datasets, demonstrate superior performance.
Additionally, the model sampling process is expedited by incorporating
Denoising Diffusion Implicit Models (DDIM). scRDiT presents a unified
methodology empowering users to train neural network models with their unique
scRNA-seq datasets, enabling the generation of numerous high-quality scRNA-seq
samples. Availability and implementation: https://github.com/DongShengze/scRDiT
\\ ( https://arxiv.org/abs/2404.06153 ,  1788kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06167
Date: Tue, 9 Apr 2024 09:46:17 GMT   (3328kb,D)

Title: scCDCG: Efficient Deep Structural Clustering for single-cell RNA-seq via
  Deep Cut-informed Graph Embedding
Authors: Ping Xu, Zhiyuan Ning, Meng Xiao, Guihai Feng, Xin Li, Yuanchun Zhou,
  Pengfei Wang
Categories: cs.LG cs.AI q-bio.GN
Comments: Accepted as a long paper for the research track at DASFAA 2024
\\
  Single-cell RNA sequencing (scRNA-seq) is essential for unraveling cellular
heterogeneity and diversity, offering invaluable insights for bioinformatics
advancements. Despite its potential, traditional clustering methods in
scRNA-seq data analysis often neglect the structural information embedded in
gene expression profiles, crucial for understanding cellular correlations and
dependencies. Existing strategies, including graph neural networks, face
challenges in handling the inefficiency due to scRNA-seq data's intrinsic
high-dimension and high-sparsity. Addressing these limitations, we introduce
scCDCG (single-cell RNA-seq Clustering via Deep Cut-informed Graph), a novel
framework designed for efficient and accurate clustering of scRNA-seq data that
simultaneously utilizes intercellular high-order structural information. scCDCG
comprises three main components: (i) A graph embedding module utilizing deep
cut-informed techniques, which effectively captures intercellular high-order
structural information, overcoming the over-smoothing and inefficiency issues
prevalent in prior graph neural network methods. (ii) A self-supervised
learning module guided by optimal transport, tailored to accommodate the unique
complexities of scRNA-seq data, specifically its high-dimension and
high-sparsity. (iii) An autoencoder-based feature learning module that
simplifies model complexity through effective dimension reduction and feature
extraction. Our extensive experiments on 6 datasets demonstrate scCDCG's
superior performance and efficiency compared to 7 established models,
underscoring scCDCG's potential as a transformative tool in scRNA-seq data
analysis. Our code is available at: https://github.com/XPgogogo/scCDCG.
\\ ( https://arxiv.org/abs/2404.06167 ,  3328kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06170
Date: Tue, 9 Apr 2024 09:49:57 GMT   (1553kb,D)

Title: CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using
  Embeddings as Teachers
Authors: Lakshmi Nair
Categories: cs.LG cs.AI
Comments: Short paper - 5 pages; 5 figures
\\
  Contrastive Language-Image Pre-training (CLIP) has been shown to improve
zero-shot generalization capabilities of language and vision models. In this
paper, we extend CLIP for efficient knowledge distillation, by utilizing
embeddings as teachers. Typical knowledge distillation frameworks require
running forward passes through a teacher model, which is often prohibitive in
the case of billion or trillion parameter teachers. In these cases, using only
the embeddings of the teacher models to guide the distillation can yield
significant computational savings. Our preliminary findings show that
CLIP-based knowledge distillation with embeddings can outperform full scale
knowledge distillation using $9\times$ less memory and $8\times$ less training
time. Code available at: https://github.com/lnairGT/CLIP-Distillation/
\\ ( https://arxiv.org/abs/2404.06170 ,  1553kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06188
Date: Tue, 9 Apr 2024 10:15:18 GMT   (3747kb,D)

Title: Diverse Randomized Value Functions: A Provably Pessimistic Approach for
  Offline Reinforcement Learning
Authors: Xudong Yu, Chenjia Bai, Hongyi Guo, Changhong Wang, Zhen Wang
Categories: cs.LG cs.AI
\\
  Offline Reinforcement Learning (RL) faces distributional shift and unreliable
value estimation, especially for out-of-distribution (OOD) actions. To address
this, existing uncertainty-based methods penalize the value function with
uncertainty quantification and demand numerous ensemble networks, posing
computational challenges and suboptimal outcomes. In this paper, we introduce a
novel strategy employing diverse randomized value functions to estimate the
posterior distribution of $Q$-values. It provides robust uncertainty
quantification and estimates lower confidence bounds (LCB) of $Q$-values. By
applying moderate value penalties for OOD actions, our method fosters a
provably pessimistic approach. We also emphasize on diversity within randomized
value functions and enhance efficiency by introducing a diversity
regularization method, reducing the requisite number of networks. These modules
lead to reliable value estimation and efficient policy learning from offline
data. Theoretical analysis shows that our method recovers the provably
efficient LCB-penalty under linear MDP assumptions. Extensive empirical results
also demonstrate that our proposed method significantly outperforms baseline
methods in terms of performance and parametric efficiency.
\\ ( https://arxiv.org/abs/2404.06188 ,  3747kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06198
Date: Tue, 9 Apr 2024 10:41:59 GMT   (383kb,D)

Title: The impact of data set similarity and diversity on transfer learning
  success in time series forecasting
Authors: Claudia Ehrig, Catherine Cleophas, Germain Forestier
Categories: cs.LG
\\
  Models, pre-trained on a similar or diverse source data set, have become
pivotal in enhancing the efficiency and accuracy of time series forecasting on
target data sets by leveraging transfer learning. While benchmarks validate the
performance of model generalization on various target data sets, there is no
structured research providing similarity and diversity measures explaining
which characteristics of source and target data lead to transfer learning
success. Our study pioneers in systematically evaluating the impact of
source-target similarity and source diversity on zero-shot and fine-tuned
forecasting outcomes in terms of accuracy, bias, and uncertainty estimation. We
investigate these dynamics using pre-trained neural networks across five public
source datasets, applied in forecasting five target data sets, including
real-world wholesales data. We identify two feature-based similarity and
diversity measures showing: Source-target similarity enhances forecasting
accuracy and reduces bias, while source diversity enhances forecasting accuracy
and uncertainty estimation and increases the bias.
\\ ( https://arxiv.org/abs/2404.06198 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06209
Date: Tue, 9 Apr 2024 10:58:21 GMT   (775kb,D)

Title: Elephants Never Forget: Memorization and Learning of Tabular Data in
  Large Language Models
Authors: Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, Rich
  Caruana
Categories: cs.LG cs.AI cs.CL
\\
  While many have shown how Large Language Models (LLMs) can be applied to a
diverse set of tasks, the critical issues of data contamination and
memorization are often glossed over. In this work, we address this concern for
tabular data. Specifically, we introduce a variety of different techniques to
assess whether a language model has seen a tabular dataset during training.
This investigation reveals that LLMs have memorized many popular tabular
datasets verbatim. We then compare the few-shot learning performance of LLMs on
datasets that were seen during training to the performance on datasets released
after training. We find that LLMs perform better on datasets seen during
training, indicating that memorization leads to overfitting. At the same time,
LLMs show non-trivial performance on novel datasets and are surprisingly robust
to data transformations. We then investigate the in-context statistical
learning abilities of LLMs. Without fine-tuning, we find them to be limited.
This suggests that much of the few-shot performance on novel datasets is due to
the LLM's world knowledge. Overall, our results highlight the importance of
testing whether an LLM has seen an evaluation dataset during pre-training. We
make the exposure tests we developed available as the tabmemcheck Python
package at https://github.com/interpretml/LLM-Tabular-Memorization-Checker
\\ ( https://arxiv.org/abs/2404.06209 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06218
Date: Tue, 9 Apr 2024 11:12:39 GMT   (440kb,D)

Title: Quantum Circuit $C^*$-algebra Net
Authors: Yuka Hashimoto and Ryuichiro Hataya
Categories: cs.LG math.OA quant-ph
\\
  This paper introduces quantum circuit $C^*$-algebra net, which provides a
connection between $C^*$-algebra nets proposed in classical machine learning
and quantum circuits. Using $C^*$-algebra, a generalization of the space of
complex numbers, we can represent quantum gates as weight parameters of a
neural network. By introducing additional parameters, we can induce interaction
among multiple circuits constructed by quantum gates. This interaction enables
the circuits to share information among them, which contributes to improved
generalization performance in machine learning tasks. As an application, we
propose to use the quantum circuit $C^*$-algebra net to encode classical data
into quantum states, which enables us to integrate classical data into quantum
algorithms. Numerical results demonstrate that the interaction among circuits
improves performance significantly in image classification, and encoded data by
the quantum circuit $C^*$-algebra net are useful for downstream quantum machine
learning tasks.
\\ ( https://arxiv.org/abs/2404.06218 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06220
Date: Tue, 9 Apr 2024 11:14:45 GMT   (1620kb,D)

Title: Zero-Shot Relational Learning for Multimodal Knowledge Graphs
Authors: Rui Cai, Shichao Pei, Xiangliang Zhang
Categories: cs.LG cs.MM
\\
  Relational learning is an essential task in the domain of knowledge
representation, particularly in knowledge graph completion (KGC).While
relational learning in traditional single-modal settings has been extensively
studied, exploring it within a multimodal KGC context presents distinct
challenges and opportunities. One of the major challenges is inference on newly
discovered relations without any associated training data. This zero-shot
relational learning scenario poses unique requirements for multimodal KGC,
i.e., utilizing multimodality to facilitate relational learning. However,
existing works fail to support the leverage of multimodal information and leave
the problem unexplored. In this paper, we propose a novel end-to-end framework,
consisting of three components, i.e., multimodal learner, structure
consolidator, and relation embedding generator, to integrate diverse multimodal
information and knowledge graph structures to facilitate the zero-shot
relational learning. Evaluation results on two multimodal knowledge graphs
demonstrate the superior performance of our proposed method.
\\ ( https://arxiv.org/abs/2404.06220 ,  1620kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06230
Date: Tue, 9 Apr 2024 11:42:32 GMT   (873kb)

Title: Aggressive or Imperceptible, or Both: Network Pruning Assisted Hybrid
  Byzantines in Federated Learning
Authors: Emre Ozfatura and Kerem Ozfatura and Alptekin Kupcu and Deniz Gunduz
Categories: cs.LG cs.CR cs.DC
\\
  Federated learning (FL) has been introduced to enable a large number of
clients, possibly mobile devices, to collaborate on generating a generalized
machine learning model thanks to utilizing a larger number of local samples
without sharing to offer certain privacy to collaborating clients. However, due
to the participation of a large number of clients, it is often difficult to
profile and verify each client, which leads to a security threat that malicious
participants may hamper the accuracy of the trained model by conveying poisoned
models during the training. Hence, the aggregation framework at the parameter
server also needs to minimize the detrimental effects of these malicious
clients. A plethora of attack and defence strategies have been analyzed in the
literature. However, often the Byzantine problem is analyzed solely from the
outlier detection perspective, being oblivious to the topology of neural
networks (NNs).
  In the scope of this work, we argue that by extracting certain side
information specific to the NN topology, one can design stronger attacks.
Hence, inspired by the sparse neural networks, we introduce a hybrid sparse
Byzantine attack that is composed of two parts: one exhibiting a sparse nature
and attacking only certain NN locations with higher sensitivity, and the other
being more silent but accumulating over time, where each ideally targets a
different type of defence mechanism, and together they form a strong but
imperceptible attack. Finally, we show through extensive simulations that the
proposed hybrid Byzantine attack is effective against 8 different defence
methods.
\\ ( https://arxiv.org/abs/2404.06230 ,  873kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06267
Date: Tue, 9 Apr 2024 12:45:17 GMT   (291kb,D)

Title: PGTNet: A Process Graph Transformer Network for Remaining Time
  Prediction of Business Process Instances
Authors: Keyvan Amiri Elyasi, Han van der Aa and Heiner Stuckenschmidt
Categories: cs.LG cs.AI
Comments: 16 pages, 4 figures, To be published in: Advanced Information Systems
  Engineering - 36th International Conference, CAiSE 2024, Limassol, Cyprus,
  June 03-07, 2024, Proceedings
\\
  We present PGTNet, an approach that transforms event logs into graph datasets
and leverages graph-oriented data for training Process Graph Transformer
Networks to predict the remaining time of business process instances. PGTNet
consistently outperforms state-of-the-art deep learning approaches across a
diverse range of 20 publicly available real-world event logs. Notably, our
approach is most promising for highly complex processes, where existing deep
learning approaches encounter difficulties stemming from their limited ability
to learn control-flow relationships among process activities and capture
long-range dependencies. PGTNet addresses these challenges, while also being
able to consider multiple process perspectives during the learning process.
\\ ( https://arxiv.org/abs/2404.06267 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06280
Date: Tue, 9 Apr 2024 13:02:40 GMT   (68kb)

Title: Algorithms for Caching and MTS with reduced number of predictions
Authors: Karim Abdel Sadek and Marek Elias
Categories: cs.LG cs.DS
\\
  ML-augmented algorithms utilize predictions to achieve performance beyond
their worst-case bounds. Producing these predictions might be a costly
operation -- this motivated Im et al. '22 to introduce the study of algorithms
which use predictions parsimoniously. We design parsimonious algorithms for
caching and MTS with action predictions, proposed by Antoniadis et al. '20,
focusing on the parameters of consistency (performance with perfect
predictions) and smoothness (dependence of their performance on the prediction
error). Our algorithm for caching is 1-consistent, robust, and its smoothness
deteriorates with the decreasing number of available predictions. We propose an
algorithm for general MTS whose consistency and smoothness both scale linearly
with the decreasing number of predictions. Without the restriction on the
number of available predictions, both algorithms match the earlier guarantees
achieved by Antoniadis et al. '20.
\\ ( https://arxiv.org/abs/2404.06280 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06313
Date: Tue, 9 Apr 2024 13:47:37 GMT   (1036kb,D)

Title: On adversarial training and the 1 Nearest Neighbor classifier
Authors: Amir Hagai, Yair Weiss
Categories: cs.LG
\\
  The ability to fool deep learning classifiers with tiny perturbations of the
input has lead to the development of adversarial training in which the loss
with respect to adversarial examples is minimized in addition to the training
examples. While adversarial training improves the robustness of the learned
classifiers, the procedure is computationally expensive, sensitive to
hyperparameters and may still leave the classifier vulnerable to other types of
small perturbations. In this paper we analyze the adversarial robustness of the
1 Nearest Neighbor (1NN) classifier and compare its performance to adversarial
training. We prove that under reasonable assumptions, the 1 NN classifier will
be robust to {\em any} small image perturbation of the training images and will
give high adversarial accuracy on test images as the number of training
examples goes to infinity. In experiments with 45 different binary image
classification problems taken from CIFAR10, we find that 1NN outperform TRADES
(a powerful adversarial training algorithm) in terms of average adversarial
accuracy. In additional experiments with 69 pretrained robust models for
CIFAR10, we find that 1NN outperforms almost all of them in terms of robustness
to perturbations that are only slightly different from those seen during
training. Taken together, our results suggest that modern adversarial training
methods still fall short of the robustness of the simple 1NN classifier. our
code can be found at
https://github.com/amirhagai/On-Adversarial-Training-And-The-1-Nearest-Neighbor-Classifier
\\ ( https://arxiv.org/abs/2404.06313 ,  1036kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06326
Date: Tue, 9 Apr 2024 14:04:26 GMT   (10411kb,D)

Title: What is the $\textit{intrinsic}$ dimension of your binary data? -- and
  how to compute it quickly
Authors: Tom Hanika and Tobias Hille
Categories: cs.LG cs.AI
MSC-class: 68T05 06-08 68T01 68T09
\\
  Dimensionality is an important aspect for analyzing and understanding
(high-dimensional) data. In their 2006 ICDM paper Tatti et al. answered the
question for a (interpretable) dimension of binary data tables by introducing a
normalized correlation dimension. In the present work we revisit their results
and contrast them with a concept based notion of intrinsic dimension (ID)
recently introduced for geometric data sets. To do this, we present a novel
approximation for this ID that is based on computing concepts only up to a
certain support value. We demonstrate and evaluate our approximation using all
available datasets from Tatti et al., which have between 469 and 41271
extrinsic dimensions.
\\ ( https://arxiv.org/abs/2404.06326 ,  10411kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06330
Date: Tue, 9 Apr 2024 14:08:47 GMT   (598kb,D)

Title: Generative Pre-Trained Transformer for Symbolic Regression Base
  In-Context Reinforcement Learning
Authors: Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jingyi Liu, Wenqiang Li, Meilan
  Hao, Shu Wei, Yusong Deng
Categories: cs.LG cs.AI
Comments: 21 pages
\\
  The mathematical formula is the human language to describe nature and is the
essence of scientific research. Finding mathematical formulas from
observational data is a major demand of scientific research and a major
challenge of artificial intelligence. This area is called symbolic regression.
Originally symbolic regression was often formulated as a combinatorial
optimization problem and solved using GP or reinforcement learning algorithms.
These two kinds of algorithms have strong noise robustness ability and good
Versatility. However, inference time usually takes a long time, so the search
efficiency is relatively low. Later, based on large-scale pre-training data
proposed, such methods use a large number of synthetic data points and
expression pairs to train a Generative Pre-Trained Transformer(GPT). Then this
GPT can only need to perform one forward propagation to obtain the results, the
advantage is that the inference speed is very fast. However, its performance is
very dependent on the training data and performs poorly on data outside the
training set, which leads to poor noise robustness and Versatility of such
methods. So, can we combine the advantages of the above two categories of SR
algorithms? In this paper, we propose \textbf{FormulaGPT}, which trains a GPT
using massive sparse reward learning histories of reinforcement learning-based
SR algorithms as training data. After training, the SR algorithm based on
reinforcement learning is distilled into a Transformer. When new test data
comes, FormulaGPT can directly generate a "reinforcement learning process" and
automatically update the learning policy in context. Tested on more than ten
datasets including SRBench, formulaGPT achieves the state-of-the-art
performance in fitting ability compared with four baselines. In addition, it
achieves satisfactory results in noise robustness, versatility, and inference
efficiency.
\\ ( https://arxiv.org/abs/2404.06330 ,  598kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06349
Date: Tue, 9 Apr 2024 14:40:08 GMT   (1281kb,D)

Title: CausalBench: A Comprehensive Benchmark for Causal Learning Capability of
  Large Language Models
Authors: Yu Zhou, Xingyu Wu, Beicheng Huang, Jibin Wu, Liang Feng, Kay Chen Tan
Categories: cs.LG
\\
  Causality reveals fundamental principles behind data distributions in
real-world scenarios, and the capability of large language models (LLMs) to
understand causality directly impacts their efficacy across explaining outputs,
adapting to new evidence, and generating counterfactuals. With the
proliferation of LLMs, the evaluation of this capacity is increasingly
garnering attention. However, the absence of a comprehensive benchmark has
rendered existing evaluation studies being straightforward, undiversified, and
homogeneous. To address these challenges, this paper proposes a comprehensive
benchmark, namely CausalBench, to evaluate the causality understanding
capabilities of LLMs. Originating from the causal research community,
CausalBench encompasses three causal learning-related tasks, which facilitate a
convenient comparison of LLMs' performance with classic causal learning
algorithms. Meanwhile, causal networks of varying scales and densities are
integrated in CausalBench, to explore the upper limits of LLMs' capabilities
across task scenarios of varying difficulty. Notably, background knowledge and
structured data are also incorporated into CausalBench to thoroughly unlock the
underlying potential of LLMs for long-text comprehension and prior information
utilization. Based on CausalBench, this paper evaluates nineteen leading LLMs
and unveils insightful conclusions in diverse aspects. Firstly, we present the
strengths and weaknesses of LLMs and quantitatively explore the upper limits of
their capabilities across various scenarios. Meanwhile, we further discern the
adaptability and abilities of LLMs to specific structural networks and complex
chain of thought structures. Moreover, this paper quantitatively presents the
differences across diverse information sources and uncovers the gap between
LLMs' capabilities in causal understanding within textual contexts and
numerical domains.
\\ ( https://arxiv.org/abs/2404.06349 ,  1281kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06353
Date: Tue, 9 Apr 2024 14:44:12 GMT   (1249kb,D)

Title: High Noise Scheduling is a Must
Authors: Mahmut S. Gokmen, Cody Bumgardner, Jie Zhang, Ge Wang, Jin Chen
Categories: cs.LG cs.AI cs.CV
\\
  Consistency models possess high capabilities for image generation, advancing
sampling steps to a single step through their advanced techniques. Current
advancements move one step forward consistency training techniques and
eliminates the limitation of distillation training. Even though the proposed
curriculum and noise scheduling in improved training techniques yield better
results than basic consistency models, it lacks well balanced noise
distribution and its consistency between curriculum. In this study, it is
investigated the balance between high and low noise levels in noise
distribution and offered polynomial noise distribution to maintain the
stability. This proposed polynomial noise distribution is also supported with a
predefined Karras noises to prevent unique noise levels arises with Karras
noise generation algorithm. Furthermore, by elimination of learned noisy steps
with a curriculum based on sinusoidal function increase the performance of the
model in denoising. To make a fair comparison with the latest released
consistency model training techniques, experiments are conducted with same
hyper-parameters except curriculum and noise distribution. The models utilized
during experiments are determined with low depth to prove the robustness of our
proposed technique. The results show that the polynomial noise distribution
outperforms the model trained with log-normal noise distribution, yielding a
33.54 FID score after 100,000 training steps with constant discretization
steps. Additionally, the implementation of a sinusoidal-based curriculum
enhances denoising performance, resulting in a FID score of 30.48.
\\ ( https://arxiv.org/abs/2404.06353 ,  1249kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06356
Date: Tue, 9 Apr 2024 14:46:48 GMT   (301kb,D)

Title: Policy-Guided Diffusion
Authors: Matthew Thomas Jackson, Michael Tryfan Matthews, Cong Lu, Benjamin
  Ellis, Shimon Whiteson, Jakob Foerster
Categories: cs.LG cs.AI cs.RO
Comments: Previously at the NeurIPS 2023 Workshop on Robot Learning
\\
  In many real-world settings, agents must learn from an offline dataset
gathered by some prior behavior policy. Such a setting naturally leads to
distribution shift between the behavior policy and the target policy being
trained - requiring policy conservatism to avoid instability and overestimation
bias. Autoregressive world models offer a different solution to this by
generating synthetic, on-policy experience. However, in practice, model
rollouts must be severely truncated to avoid compounding error. As an
alternative, we propose policy-guided diffusion. Our method uses diffusion
models to generate entire trajectories under the behavior distribution,
applying guidance from the target policy to move synthetic experience further
on-policy. We show that policy-guided diffusion models a regularized form of
the target distribution that balances action likelihood under both the target
and behavior policies, leading to plausible trajectories with high target
policy probability, while retaining a lower dynamics error than an offline
world model baseline. Using synthetic experience from policy-guided diffusion
as a drop-in substitute for real data, we demonstrate significant improvements
in performance across a range of standard offline reinforcement learning
algorithms and environments. Our approach provides an effective alternative to
autoregressive offline world models, opening the door to the controllable
generation of synthetic training data.
\\ ( https://arxiv.org/abs/2404.06356 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06391
Date: Tue, 9 Apr 2024 15:35:02 GMT   (1357kb,D)

Title: Exploring Neural Network Landscapes: Star-Shaped and Geodesic
  Connectivity
Authors: Zhanran Lin, Puheng Li, Lei Wu
Categories: cs.LG stat.ML
Comments: The first two authors contributed equally
\\
  One of the most intriguing findings in the structure of neural network
landscape is the phenomenon of mode connectivity: For two typical global
minima, there exists a path connecting them without barrier. This concept of
mode connectivity has played a crucial role in understanding important
phenomena in deep learning.
  In this paper, we conduct a fine-grained analysis of this connectivity
phenomenon. First, we demonstrate that in the overparameterized case, the
connecting path can be as simple as a two-piece linear path, and the path
length can be nearly equal to the Euclidean distance. This finding suggests
that the landscape should be nearly convex in a certain sense. Second, we
uncover a surprising star-shaped connectivity: For a finite number of typical
minima, there exists a center on minima manifold that connects all of them
simultaneously via linear paths. These results are provably valid for linear
networks and two-layer ReLU networks under a teacher-student setup, and are
empirically supported by models trained on MNIST and CIFAR-10.
\\ ( https://arxiv.org/abs/2404.06391 ,  1357kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06400
Date: Tue, 9 Apr 2024 15:46:00 GMT   (8724kb,D)

Title: Dynamic Deep Learning Based Super-Resolution For The Shallow Water
  Equations
Authors: Maximilian Witte, Fabricio Rodrigues Lapolli, Philip Freese, Sebastian
  G\"otschel, Daniel Ruprecht, Peter Korn, Christopher Kadow
Categories: cs.LG physics.comp-ph physics.flu-dyn
Comments: 17 pages, 12 figures
MSC-class: 65M99, 68T07, 86-08, 35-11
\\
  Using the nonlinear shallow water equations as benchmark, we demonstrate that
a simulation with the ICON-O ocean model with a 20km resolution that is
frequently corrected by a U-net-type neural network can achieve discretization
errors of a simulation with 10km resolution. The network, originally developed
for image-based super-resolution in post-processing, is trained to compute the
difference between solutions on both meshes and is used to correct the coarse
mesh every 12h. Our setup is the Galewsky test case, modeling transition of a
barotropic instability into turbulent flow. We show that the ML-corrected
coarse resolution run correctly maintains a balance flow and captures the
transition to turbulence in line with the higher resolution simulation. After 8
day of simulation, the $L_2$-error of the corrected run is similar to a
simulation run on the finer mesh. While mass is conserved in the corrected
runs, we observe some spurious generation of kinetic energy.
\\ ( https://arxiv.org/abs/2404.06400 ,  8724kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06403
Date: Tue, 9 Apr 2024 15:53:02 GMT   (2780kb,D)

Title: Online Learning of Decision Trees with Thompson Sampling
Authors: Ayman Chaouki, Jesse Read, Albert Bifet
Categories: cs.LG
Comments: To be published in the Proceedings of the 27th International
  Conference on Artificial Intelligence and Statistics (AISTATS) 2024,
  Valencia, Spain. PMLR: Volume 238
\\
  Decision Trees are prominent prediction models for interpretable Machine
Learning. They have been thoroughly researched, mostly in the batch setting
with a fixed labelled dataset, leading to popular algorithms such as C4.5, ID3
and CART. Unfortunately, these methods are of heuristic nature, they rely on
greedy splits offering no guarantees of global optimality and often leading to
unnecessarily complex and hard-to-interpret Decision Trees. Recent
breakthroughs addressed this suboptimality issue in the batch setting, but no
such work has considered the online setting with data arriving in a stream. To
this end, we devise a new Monte Carlo Tree Search algorithm, Thompson Sampling
Decision Trees (TSDT), able to produce optimal Decision Trees in an online
setting. We analyse our algorithm and prove its almost sure convergence to the
optimal tree. Furthermore, we conduct extensive experiments to validate our
findings empirically. The proposed TSDT outperforms existing algorithms on
several benchmarks, all while presenting the practical advantage of being
tailored to the online setting.
\\ ( https://arxiv.org/abs/2404.06403 ,  2780kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06418
Date: Tue, 9 Apr 2024 16:07:35 GMT   (2024kb,D)

Title: Studying the Impact of Latent Representations in Implicit Neural
  Networks for Scientific Continuous Field Reconstruction
Authors: Wei Xu, Derek Freeman DeSantis, Xihaier Luo, Avish Parmar, Klaus Tan,
  Balu Nadiga, Yihui Ren, Shinjae Yoo
Categories: cs.LG cs.AI
\\
  Learning a continuous and reliable representation of physical fields from
sparse sampling is challenging and it affects diverse scientific disciplines.
In a recent work, we present a novel model called MMGN (Multiplicative and
Modulated Gabor Network) with implicit neural networks. In this work, we design
additional studies leveraging explainability methods to complement the previous
experiments and further enhance the understanding of latent representations
generated by the model. The adopted methods are general enough to be leveraged
for any latent space inspection. Preliminary results demonstrate the contextual
information incorporated in the latent representations and their impact on the
model performance. As a work in progress, we will continue to verify our
findings and develop novel explainability approaches.
\\ ( https://arxiv.org/abs/2404.06418 ,  2024kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06421
Date: Tue, 9 Apr 2024 16:10:39 GMT   (3271kb,D)

Title: Bayesian Survival Analysis by Approximate Inference of Neural Networks
Authors: Christian Marius Lillelund, Martin Magris and Christian Fischer
  Pedersen
Categories: cs.LG
\\
  Predicting future events always comes with uncertainty, but traditional
non-Bayesian methods cannot distinguish certain from uncertain predictions or
explain the confidence in their predictions. In survival analysis, Bayesian
methods applied to state-of-the-art solutions in the healthcare and biomedical
field are still novel, and their implications have not been fully evaluated. In
this paper, we study the benefits of modeling uncertainty in deep neural
networks for survival analysis with a focus on prediction and calibration
performance. For this, we present a Bayesian deep learning framework that
consists of three Bayesian network architectures, which we train by optimizing
the Cox partial likelihood and combining input-dependent aleatoric uncertainty
with model-specific epistemic uncertainty. This enables us to provide
uncertainty estimates as credible intervals when predicting the survival curve
or as a probability density function over the predicted median survival times.
For our empirical analyses, we evaluated our proposed method on four benchmark
datasets and found that our method demonstrates prediction performance
comparable to the state-of-the-art based on the concordance index and
outperforms all other Cox-based approaches in terms of the mean absolute error.
Our work explicitly compares the extent to which different Bayesian
approximation techniques differ from each other and improves the prediction
over traditional non-Bayesian alternatives.
\\ ( https://arxiv.org/abs/2404.06421 ,  3271kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06430
Date: Tue, 9 Apr 2024 16:23:01 GMT   (526kb,D)

Title: pfl-research: simulation framework for accelerating research in Private
  Federated Learning
Authors: Filip Granqvist, Congzheng Song, \'Aine Cahill, Rogier van Dalen,
  Martin Pelikan, Yi Sheng Chan, Xiaojun Feng, Natarajan Krishnaswami, Vojta
  Jina, Mona Chitnis
Categories: cs.LG cs.AI cs.CR cs.CV
\\
  Federated learning (FL) is an emerging machine learning (ML) training
paradigm where clients own their data and collaborate to train a global model,
without revealing any data to the server and other participants. Researchers
commonly perform experiments in a simulation environment to quickly iterate on
ideas. However, existing open-source tools do not offer the efficiency required
to simulate FL on larger and more realistic FL datasets. We introduce
pfl-research, a fast, modular, and easy-to-use Python framework for simulating
FL. It supports TensorFlow, PyTorch, and non-neural network models, and is
tightly integrated with state-of-the-art privacy algorithms. We study the speed
of open-source FL frameworks and show that pfl-research is 7-72$\times$ faster
than alternative open-source frameworks on common cross-device setups. Such
speedup will significantly boost the productivity of the FL research community
and enable testing hypotheses on realistic FL datasets that were previously too
resource intensive. We release a suite of benchmarks that evaluates an
algorithm's overall performance on a diverse set of realistic scenarios. The
code is available on GitHub at https://github.com/apple/pfl-research.
\\ ( https://arxiv.org/abs/2404.06430 ,  526kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06448
Date: Tue, 9 Apr 2024 16:50:30 GMT   (4634kb,D)

Title: Automated Federated Pipeline for Parameter-Efficient Fine-Tuning of
  Large Language Models
Authors: Zihan Fang, Zheng Lin, Zhe Chen, Xianhao Chen, Yue Gao, Yuguang Fang
Categories: cs.LG cs.AI
Comments: 15 pages, 16 figures
\\
  Recently, there has been a surge in the development of advanced intelligent
generative content (AIGC), especially large language models (LLMs). However,
for many downstream tasks, it is necessary to fine-tune LLMs using private
data. While federated learning offers a promising privacy-preserving solution
to LLM fine-tuning, the substantial size of an LLM, combined with high
computational and communication demands, makes it hard to apply to downstream
tasks. More importantly, private edge servers often possess varying computing
and network resources in real-world scenarios, introducing additional
complexities to LLM fine-tuning. To tackle these problems, we design and
implement an automated federated pipeline, named FedPipe, to fine-tune LLMs
with minimal training cost but without adding any inference latency. FedPipe
firstly identifies the weights to be fine-tuned based on their contributions to
the LLM training. It then configures a low-rank adapter for each selected
weight to train local low-rank adapters on an edge server, and aggregate local
adapters of all edge servers to fine-tune the whole LLM. Finally, it
appropriately quantizes the parameters of LLM to reduce memory space according
to the requirements of edge servers. Extensive experiments demonstrate that
FedPipe expedites the model training and achieves higher accuracy than
state-of-the-art benchmarks.
\\ ( https://arxiv.org/abs/2404.06448 ,  4634kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06466
Date: Tue, 9 Apr 2024 17:14:41 GMT   (946kb,D)

Title: Hyperparameter Selection in Continual Learning
Authors: Thomas L. Lee, Sigrid Passano Hellan, Linus Ericsson, Elliot J.
  Crowley and Amos Storkey
Categories: cs.LG stat.ML
Comments: Preprint, 9 pages
\\
  In continual learning (CL) -- where a learner trains on a stream of data --
standard hyperparameter optimisation (HPO) cannot be applied, as a learner does
not have access to all of the data at the same time. This has prompted the
development of CL-specific HPO frameworks. The most popular way to tune
hyperparameters in CL is to repeatedly train over the whole data stream with
different hyperparameter settings. However, this end-of-training HPO is
unrealistic as in practice a learner can only see the stream once. Hence, there
is an open question: what HPO framework should a practitioner use for a CL
problem in reality? This paper answers this question by evaluating several
realistic HPO frameworks. We find that all the HPO frameworks considered,
including end-of-training HPO, perform similarly. We therefore advocate using
the realistic and most computationally efficient method: fitting the
hyperparameters on the first task and then fixing them throughout training.
\\ ( https://arxiv.org/abs/2404.06466 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06486
Date: Tue, 9 Apr 2024 17:37:08 GMT   (4330kb,D)

Title: GO4Align: Group Optimization for Multi-Task Alignment
Authors: Jiayi Shen, Cheems Wang, Zehao Xiao, Nanne Van Noord, Marcel Worring
Categories: cs.LG
\\
  This paper proposes \textit{GO4Align}, a multi-task optimization approach
that tackles task imbalance by explicitly aligning the optimization across
tasks. To achieve this, we design an adaptive group risk minimization strategy,
compromising two crucial techniques in implementation: (i) dynamical group
assignment, which clusters similar tasks based on task interactions; (ii)
risk-guided group indicators, which exploit consistent task correlations with
risk information from previous iterations. Comprehensive experimental results
on diverse typical benchmarks demonstrate our method's performance superiority
with even lower computational costs.
\\ ( https://arxiv.org/abs/2404.06486 ,  4330kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06492
Date: Tue, 9 Apr 2024 17:45:25 GMT   (600kb,D)

Title: Graph Reinforcement Learning for Combinatorial Optimization: A Survey
  and Unifying Perspective
Authors: Victor-Alexandru Darvariu, Stephen Hailes, Mirco Musolesi
Categories: cs.LG cs.AI
\\
  Graphs are a natural representation for systems based on relations between
connected entities. Combinatorial optimization problems, which arise when
considering an objective function related to a process of interest on discrete
structures, are often challenging due to the rapid growth of the solution
space. The trial-and-error paradigm of Reinforcement Learning has recently
emerged as a promising alternative to traditional methods, such as exact
algorithms and (meta)heuristics, for discovering better decision-making
strategies in a variety of disciplines including chemistry, computer science,
and statistics. Despite the fact that they arose in markedly different fields,
these techniques share significant commonalities. Therefore, we set out to
synthesize this work in a unifying perspective that we term Graph Reinforcement
Learning, interpreting it as a constructive decision-making method for graph
problems. After covering the relevant technical background, we review works
along the dividing line of whether the goal is to optimize graph structure
given a process of interest, or to optimize the outcome of the process itself
under fixed graph structure. Finally, we discuss the common challenges facing
the field and open research questions. In contrast with other surveys, the
present work focuses on non-canonical graph problems for which performant
algorithms are typically not known and Reinforcement Learning is able to
provide efficient and effective solutions.
\\ ( https://arxiv.org/abs/2404.06492 ,  600kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06498
Date: Tue, 9 Apr 2024 17:50:38 GMT   (1217kb,D)

Title: Simultaneous linear connectivity of neural networks modulo permutation
Authors: Ekansh Sharma, Devin Kwok, Tom Denton, Daniel M. Roy, David Rolnick,
  Gintare Karolina Dziugaite
Categories: cs.LG stat.ML
Comments: 11 pages, 6 figures
\\
  Neural networks typically exhibit permutation symmetries which contribute to
the non-convexity of the networks' loss landscapes, since linearly
interpolating between two permuted versions of a trained network tends to
encounter a high loss barrier. Recent work has argued that permutation
symmetries are the only sources of non-convexity, meaning there are essentially
no such barriers between trained networks if they are permuted appropriately.
In this work, we refine these arguments into three distinct claims of
increasing strength. We show that existing evidence only supports "weak linear
connectivity"-that for each pair of networks belonging to a set of SGD
solutions, there exist (multiple) permutations that linearly connect it with
the other networks. In contrast, the claim "strong linear connectivity"-that
for each network, there exists one permutation that simultaneously connects it
with the other networks-is both intuitively and practically more desirable.
This stronger claim would imply that the loss landscape is convex after
accounting for permutation, and enable linear interpolation between three or
more independently trained models without increased loss. In this work, we
introduce an intermediate claim-that for certain sequences of networks, there
exists one permutation that simultaneously aligns matching pairs of networks
from these sequences. Specifically, we discover that a single permutation
aligns sequences of iteratively trained as well as iteratively pruned networks,
meaning that two networks exhibit low loss barriers at each step of their
optimization and sparsification trajectories respectively. Finally, we provide
the first evidence that strong linear connectivity may be possible under
certain conditions, by showing that barriers decrease with increasing network
width when interpolating among three networks.
\\ ( https://arxiv.org/abs/2404.06498 ,  1217kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.05746 (*cross-listing*)
Date: Wed, 3 Apr 2024 14:33:23 GMT   (6489kb,D)

Title: Causality for Earth Science -- A Review on Time-series and
  Spatiotemporal Causality Methods
Authors: Sahara Ali, Uzma Hasan, Xingyan Li, Omar Faruque, Akila Sampath, Yiyi
  Huang, Md Osman Gani and Jianwu Wang
Categories: physics.data-an cs.AI physics.ao-ph physics.geo-ph
\\
  This survey paper covers the breadth and depth of time-series and
spatiotemporal causality methods, and their applications in Earth Science. More
specifically, the paper presents an overview of causal discovery and causal
inference, explains the underlying causal assumptions, and enlists evaluation
techniques and key terminologies of the domain area. The paper elicits the
various state-of-the-art methods introduced for time-series and spatiotemporal
causal analysis along with their strengths and limitations. The paper further
describes the existing applications of several methods for answering specific
Earth Science questions such as extreme weather events, sea level rise,
teleconnections etc. This survey paper can serve as a primer for Data Science
researchers interested in data-driven causal study as we share a list of
resources, such as Earth Science datasets (synthetic, simulated and
observational data) and open source tools for causal analysis. It will equally
benefit the Earth Science community interested in taking an AI-driven approach
to study the causality of different dynamic and thermodynamic processes as we
present the open challenges and opportunities in performing causality-based
Earth Science study.
\\ ( https://arxiv.org/abs/2404.05746 ,  6489kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05758 (*cross-listing*)
Date: Fri, 5 Apr 2024 21:28:56 GMT   (37943kb,D)

Title: Implicit Assimilation of Sparse In Situ Data for Dense & Global Storm
  Surge Forecasting
Authors: Patrick Ebel, Brandon Victor, Peter Naylor, Gabriele Meoni, Federico
  Serva, Rochelle Schneider
Categories: physics.data-an cs.AI cs.CV cs.LG physics.ao-ph stat.AP
Comments: Accepted at CVPR EarthVision 2024
\\
  Hurricanes and coastal floods are among the most disastrous natural hazards.
Both are intimately related to storm surges, as their causes and effects,
respectively. However, the short-term forecasting of storm surges has proven
challenging, especially when targeting previously unseen locations or sites
without tidal gauges. Furthermore, recent work improved short and medium-term
weather forecasting but the handling of raw unassimilated data remains
non-trivial. In this paper, we tackle both challenges and demonstrate that
neural networks can implicitly assimilate sparse in situ tide gauge data with
coarse ocean state reanalysis in order to forecast storm surges. We curate a
global dataset to learn and validate the dense prediction of storm surges,
building on preceding efforts. Other than prior work limited to known gauges,
our approach extends to ungauged sites, paving the way for global storm surge
forecasting.
\\ ( https://arxiv.org/abs/2404.05758 ,  37943kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05762 (*cross-listing*)
Date: Sat, 6 Apr 2024 11:20:28 GMT   (199kb)

Title: Evaluating the Effectiveness of Artificial Intelligence in Predicting
  Adverse Drug Reactions among Cancer Patients: A Systematic Review and
  Meta-Analysis
Authors: Fatma Zahra Abdeldjouad, Menaouer Brahami, Mohammed Sabri
Categories: q-bio.QM cs.AI cs.LG
Comments: Paper has been accepted at the IEEE Challenges and Innovations on TIC
  (IEEE I2CIT) International Conference
\\
  Adverse drug reactions considerably impact patient outcomes and healthcare
costs in cancer therapy. Using artificial intelligence to predict adverse drug
reactions in real time could revolutionize oncology treatment. This study aims
to assess the performance of artificial intelligence models in predicting
adverse drug reactions in patients with cancer. This is the first systematic
review and meta-analysis. Scopus, PubMed, IEEE Xplore, and ACM Digital Library
databases were searched for studies in English, French, and Arabic from January
1, 2018, to August 20, 2023. The inclusion criteria were: (1) peer-reviewed
research articles; (2) use of artificial intelligence algorithms (machine
learning, deep learning, knowledge graphs); (3) study aimed to predict adverse
drug reactions (cardiotoxicity, neutropenia, nephrotoxicity, hepatotoxicity);
(4) study was on cancer patients. The data were extracted and evaluated by
three reviewers for study quality. Of the 332 screened articles, 17 studies
(5%) involving 93,248 oncology patients from 17 countries were included in the
systematic review, of which ten studies synthesized the meta-analysis. A
random-effects model was created to pool the sensitivity, specificity, and AUC
of the included studies. The pooled results were 0.82 (95% CI:0.69, 0.9), 0.84
(95% CI:0.75, 0.9), and 0.83 (95% CI:0.77, 0.87) for sensitivity, specificity,
and AUC, respectively, of ADR predictive models. Biomarkers proved their
effectiveness in predicting ADRs, yet they were adopted by only half of the
reviewed studies. The use of AI in cancer treatment shows great potential, with
models demonstrating high specificity and sensitivity in predicting ADRs.
However, standardized research and multicenter studies are needed to improve
the quality of evidence. AI can enhance cancer patient care by bridging the gap
between data-driven insights and clinical expertise.
\\ ( https://arxiv.org/abs/2404.05762 ,  199kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05765 (*cross-listing*)
Date: Sat, 6 Apr 2024 16:15:02 GMT   (920kb)

Title: A Novel Bi-LSTM And Transformer Architecture For Generating Tabla Music
Authors: Roopa Mayya, Vivekanand Venkataraman, Anwesh P R and Narayana
  Darapaneni
Categories: cs.SD cs.AI eess.AS
\\
  Introduction: Music generation is a complex task that has received
significant attention in recent years, and deep learning techniques have shown
promising results in this field. Objectives: While extensive work has been
carried out on generating Piano and other Western music, there is limited
research on generating classical Indian music due to the scarcity of Indian
music in machine-encoded formats. In this technical paper, methods for
generating classical Indian music, specifically tabla music, is proposed.
Initially, this paper explores piano music generation using deep learning
architectures. Then the fundamentals are extended to generating tabla music.
Methods: Tabla music in waveform (.wav) files are pre-processed using the
librosa library in Python. A novel Bi-LSTM with an Attention approach and a
transformer model are trained on the extracted features and labels. Results:
The models are then used to predict the next sequences of tabla music. A loss
of 4.042 and MAE of 1.0814 are achieved with the Bi-LSTM model. With the
transformer model, a loss of 55.9278 and MAE of 3.5173 are obtained for tabla
music generation. Conclusion: The resulting music embodies a harmonious fusion
of novelty and familiarity, pushing the limits of music composition to new
horizons.
\\ ( https://arxiv.org/abs/2404.05765 ,  920kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05767 (*cross-listing*)
Date: Sun, 7 Apr 2024 10:59:35 GMT   (1516kb,D)

Title: CSA-Trans: Code Structure Aware Transformer for AST
Authors: Saeyoon Oh, Shin Yoo
Categories: cs.SE cs.AI
\\
  When applying the Transformer architecture to source code, designing a good
self-attention mechanism is critical as it affects how node relationship is
extracted from the Abstract Syntax Trees (ASTs) of the source code. We present
Code Structure Aware Transformer (CSA-Trans), which uses Code Structure
Embedder (CSE) to generate specific PE for each node in AST. CSE generates node
Positional Encoding (PE) using disentangled attention. To further extend the
self-attention capability, we adopt Stochastic Block Model (SBM) attention. Our
evaluation shows that our PE captures the relationships between AST nodes
better than other graph-related PE techniques. We also show through
quantitative and qualitative analysis that SBM attention is able to generate
more node specific attention coefficients. We demonstrate that CSA-Trans
outperforms 14 baselines in code summarization tasks for both Python and Java,
while being 41.92% faster and 25.31% memory efficient in Java dataset compared
to AST-Trans and SG-Trans respectively.
\\ ( https://arxiv.org/abs/2404.05767 ,  1516kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05769 (*cross-listing*)
Date: Sun, 7 Apr 2024 19:00:15 GMT   (122kb,D)

Title: Dynamic Quality-Diversity Search
Authors: Roberto Gallotta, Antonios Liapis, Georgios N. Yannakakis
Categories: cs.NE cs.AI
Comments: Parts of this manuscripts are published at The Genetic and
  Evolutionary Computation Conference (GECCO) 2024
\\
  Evolutionary search via the quality-diversity (QD) paradigm can discover
highly performing solutions in different behavioural niches, showing
considerable potential in complex real-world scenarios such as evolutionary
robotics. Yet most QD methods only tackle static tasks that are fixed over
time, which is rarely the case in the real world. Unlike noisy environments,
where the fitness of an individual changes slightly at every evaluation,
dynamic environments simulate tasks where external factors at unknown and
irregular intervals alter the performance of the individual with a severity
that is unknown a priori. Literature on optimisation in dynamic environments is
extensive, yet such environments have not been explored in the context of QD
search. This paper introduces a novel and generalisable Dynamic QD methodology
that aims to keep the archive of past solutions updated in the case of
environment changes. Secondly, we present a novel characterisation of dynamic
environments that can be easily applied to well-known benchmarks, with minor
interventions to move them from a static task to a dynamic one. Our Dynamic QD
intervention is applied on MAP-Elites and CMA-ME, two powerful QD algorithms,
and we test the dynamic variants on different dynamic tasks.
\\ ( https://arxiv.org/abs/2404.05769 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05776 (*cross-listing*)
Date: Mon, 8 Apr 2024 06:47:03 GMT   (243kb)

Title: Forecasting Electric Vehicle Battery Output Voltage: A Predictive
  Modeling Approach
Authors: Narayana Darapaneni, Ashish K, Ullas M S, Anwesh Reddy Paduri
Categories: cs.CV cs.AI cs.SY eess.SY
\\
  The battery management system plays a vital role in ensuring the safety and
dependability of electric and hybrid vehicles. It is responsible for various
functions, including state evaluation, monitoring, charge control, and cell
balancing, all integrated within the BMS. Nonetheless, due to the uncertainties
surrounding battery performance, implementing these functionalities poses
significant challenges. In this study, we explore the latest approaches for
assessing battery states, highlight notable advancements in battery management
systems (BMS), address existing issues with current BMS technology, and put
forth possible solutions for predicting battery charging voltage.
\\ ( https://arxiv.org/abs/2404.05776 ,  243kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05777 (*cross-listing*)
Date: Mon, 8 Apr 2024 13:40:26 GMT   (15868kb,D)

Title: IA2: Leveraging Instance-Aware Index Advisor with Reinforcement Learning
  for Diverse Workloads
Authors: Taiyi Wang, Eiko Yoneki
Categories: cs.DB cs.AI
Comments: EuroMLSys 24, April 22, 2024, Athens, Greece
DOI: 10.1145/3642970.3655839
\\
  This study introduces the Instance-A}ware Index A}dvisor (IA2), a novel deep
reinforcement learning (DRL)-based approach for optimizing index selection in
databases facing large action spaces of potential candidates. IA2 introduces
the Twin Delayed Deep Deterministic Policy Gradient - Temporal Difference
State-Wise Action Refinery (TD3-TD-SWAR) model, enabling efficient index
selection by understanding workload-index dependencies and employing adaptive
action masking. This method includes a comprehensive workload model, enhancing
its ability to adapt to unseen workloads and ensuring robust performance across
diverse database environments. Evaluation on benchmarks such as TPC-H reveals
IA2's suggested indexes' performance in enhancing runtime, securing a 40%
reduction in runtime for complex TPC-H workloads compared to scenarios without
indexes, and delivering a 20% improvement over existing state-of-the-art
DRL-based index advisors.
\\ ( https://arxiv.org/abs/2404.05777 ,  15868kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05783 (*cross-listing*)
Date: Mon, 8 Apr 2024 17:53:21 GMT   (5696kb,D)

Title: Responsible Generative AI: What to Generate and What Not
Authors: Jindong Gu
Categories: cs.CY cs.AI cs.CL cs.CV
Comments: 74 pages, 10 figures
\\
  In recent years, generative AI (GenAI), like large language models and
text-to-image models, has received significant attention across various
domains. However, ensuring the responsible generation of content by these
models is crucial for their real-world applicability. This raises an
interesting question: \textit{What should responsible GenAI generate, and what
should it not?} To answer the question, this paper investigates the practical
responsible requirements of both textual and visual generative models,
outlining five key considerations: generating truthful content, avoiding toxic
content, refusing harmful instruction, leaking no training data-related
content, and ensuring generated content identifiable. Specifically, we review
recent advancements and challenges in addressing these requirements. Besides,
we discuss and emphasize the importance of responsible GenAI across healthcare,
education, finance, and artificial general intelligence domains. Through a
unified perspective on both textual and visual generative models, this paper
aims to provide insights into practical safety-related issues and further
benefit the community in building responsible GenAI.
\\ ( https://arxiv.org/abs/2404.05783 ,  5696kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05825 (*cross-listing*)
Date: Mon, 8 Apr 2024 19:29:07 GMT   (398kb,D)

Title: LLM-Augmented Retrieval: Enhancing Retrieval Models Through Language
  Models and Doc-Level Embedding
Authors: Mingrui Wu, Sheng Cao
Categories: cs.IR cs.AI
\\
  Recently embedding-based retrieval or dense retrieval have shown state of the
art results, compared with traditional sparse or bag-of-words based approaches.
This paper introduces a model-agnostic doc-level embedding framework through
large language model (LLM) augmentation. In addition, it also improves some
important components in the retrieval model training process, such as negative
sampling, loss function, etc. By implementing this LLM-augmented retrieval
framework, we have been able to significantly improve the effectiveness of
widely-used retriever models such as Bi-encoders (Contriever, DRAGON) and
late-interaction models (ColBERTv2), thereby achieving state-of-the-art results
on LoTTE datasets and BEIR datasets.
\\ ( https://arxiv.org/abs/2404.05825 ,  398kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05891 (*cross-listing*)
Date: Mon, 8 Apr 2024 22:20:23 GMT   (1502kb,D)

Title: Condition Monitoring with Incomplete Data: An Integrated Variational
  Autoencoder and Distance Metric Framework
Authors: Maryam Ahang, Mostafa Abbasi, Todd Charter, Homayoun Najjaran
Categories: eess.SP cs.AI cs.LG
\\
  Condition monitoring of industrial systems is crucial for ensuring safety and
maintenance planning, yet notable challenges arise in real-world settings due
to the limited or non-existent availability of fault samples. This paper
introduces an innovative solution to this problem by proposing a new method for
fault detection and condition monitoring for unseen data. Adopting an approach
inspired by zero-shot learning, our method can identify faults and assign a
relative health index to various operational conditions. Typically, we have
plenty of data on normal operations, some data on compromised conditions, and
very few (if any) samples of severe faults. We use a variational autoencoder to
capture the probabilistic distribution of previously seen and new unseen
conditions. The health status is determined by comparing each sample's
deviation from a normal operation reference distribution in the latent space.
Faults are detected by establishing a threshold for the health indexes,
allowing the model to identify severe, unseen faults with high accuracy, even
amidst noise. We validate our approach using the run-to-failure IMS-bearing
dataset and compare it with other methods. The health indexes generated by our
model closely match the established descriptive model of bearing wear,
attesting to the robustness and reliability of our method. These findings
highlight the potential of our methodology in augmenting fault detection
capabilities within industrial domains, thereby contributing to heightened
safety protocols and optimized maintenance practices.
\\ ( https://arxiv.org/abs/2404.05891 ,  1502kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05920 (*cross-listing*)
Date: Tue, 9 Apr 2024 00:51:24 GMT   (13kb)

Title: Inclusive Practices for Child-Centered AI Design and Testing
Authors: Emani Dotch and Vitica Arnold
Categories: cs.HC cs.AI
Comments: CHI 2024 Workshop on Child-centred AI Design, May 11, 2024, Honolulu,
  HI, USA
\\
  We explore ideas and inclusive practices for designing and testing
child-centered artificially intelligent technologies for neurodivergent
children. AI is promising for supporting social communication, self-regulation,
and sensory processing challenges common for neurodivergent children. The
authors, both neurodivergent individuals and related to neurodivergent people,
draw from their professional and personal experiences to offer insights on
creating AI technologies that are accessible and include input from
neurodivergent children. We offer ideas for designing AI technologies for
neurodivergent children and considerations for including them in the design
process while accounting for their sensory sensitivities. We conclude by
emphasizing the importance of adaptable and supportive AI technologies and
design processes and call for further conversation to refine child-centered AI
design and testing methods.
\\ ( https://arxiv.org/abs/2404.05920 ,  13kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05959 (*cross-listing*)
Date: Tue, 9 Apr 2024 02:45:39 GMT   (2805kb)

Title: Map Optical Properties to Subwavelength Structures Directly via a
  Diffusion Model
Authors: Shijie Rao, Kaiyu Cui, Yidong Huang, Jiawei Yang, Yali Li, Shengjin
  Wang, Xue Feng, Fang Liu, and Wei Zhang
Categories: physics.optics cs.AI
\\
  Subwavelength photonic structures and metamaterials provide revolutionary
approaches for controlling light. The inverse design methods proposed for these
subwavelength structures are vital to the development of new photonic devices.
However, most of the existing inverse design methods cannot realize direct
mapping from optical properties to photonic structures but instead rely on
forward simulation methods to perform iterative optimization. In this work, we
exploit the powerful generative abilities of artificial intelligence (AI) and
propose a practical inverse design method based on latent diffusion models. Our
method maps directly the optical properties to structures without the
requirement of forward simulation and iterative optimization. Here, the given
optical properties can work as "prompts" and guide the constructed model to
correctly "draw" the required photonic structures. Experiments show that our
direct mapping-based inverse design method can generate subwavelength photonic
structures at high fidelity while following the given optical properties. This
may change the method used for optical design and greatly accelerate the
research on new photonic devices.
\\ ( https://arxiv.org/abs/2404.05959 ,  2805kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05967 (*cross-listing*)
Date: Tue, 9 Apr 2024 02:55:12 GMT   (400kb,D)

Title: JSTR: Judgment Improves Scene Text Recognition
Authors: Masato Fujitake
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: IntelliSys 2024
\\
  In this paper, we present a method for enhancing the accuracy of scene text
recognition tasks by judging whether the image and text match each other. While
previous studies focused on generating the recognition results from input
images, our approach also considers the model's misrecognition results to
understand its error tendencies, thus improving the text recognition pipeline.
This method boosts text recognition accuracy by providing explicit feedback on
the data that the model is likely to misrecognize by predicting correct or
incorrect between the image and text. The experimental results on publicly
available datasets demonstrate that our proposed method outperforms the
baseline and state-of-the-art methods in scene text recognition.
\\ ( https://arxiv.org/abs/2404.05967 ,  400kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05980 (*cross-listing*)
Date: Tue, 9 Apr 2024 03:24:10 GMT   (17278kb,D)

Title: Tackling Structural Hallucination in Image Translation with Local
  Diffusion
Authors: Seunghoi Kim, Chen Jin, Tom Diethe, Matteo Figini, Henry F. J.
  Tregidgo, Asher Mullokandov, Philip Teare, Daniel C. Alexander
Categories: cs.CV cs.AI cs.LG
\\
  Recent developments in diffusion models have advanced conditioned image
generation, yet they struggle with reconstructing out-of-distribution (OOD)
images, such as unseen tumors in medical images, causing ``image
hallucination'' and risking misdiagnosis. We hypothesize such hallucinations
result from local OOD regions in the conditional images. We verify that
partitioning the OOD region and conducting separate image generations
alleviates hallucinations in several applications. From this, we propose a
training-free diffusion framework that reduces hallucination with multiple
Local Diffusion processes. Our approach involves OOD estimation followed by two
modules: a ``branching'' module generates locally both within and outside OOD
regions, and a ``fusion'' module integrates these predictions into one. Our
evaluation shows our method mitigates hallucination over baseline models
quantitatively and qualitatively, reducing misdiagnosis by 40% and 25% in the
real-world medical and natural image datasets, respectively. It also
demonstrates compatibility with various pre-trained diffusion models.
\\ ( https://arxiv.org/abs/2404.05980 ,  17278kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05990 (*cross-listing*)
Date: Tue, 9 Apr 2024 03:48:42 GMT   (234kb)

Title: Automatic Authorities: Power and AI
Authors: Seth Lazar
Categories: cs.CY cs.AI
\\
  As rapid advances in Artificial Intelligence and the rise of some of
history's most potent corporations meet the diminished neoliberal state, people
are increasingly subject to power exercised by means of automated systems.
Machine learning and related computational technologies now underpin vital
government services. They connect consumers and producers in new algorithmic
markets. They determine how we find out about everything from how to vote to
where to get vaccinated, and whose speech is amplified, reduced, or restricted.
And a new wave of products based on Large Language Models (LLMs) will further
transform our economic and political lives. Automatic Authorities are automated
computational systems used to exercise power over us by determining what we may
know, what we may have, and what our options will be. In response to their
rise, scholars working on the societal impacts of AI and related technologies
have advocated shifting attention from how to make AI systems beneficial or
fair towards a critical analysis of these new power relations. But power is
everywhere, and is not necessarily bad. On what basis should we object to new
or intensified power relations, and what can be done to justify them? This
paper introduces the philosophical materials with which to formulate these
questions, and offers preliminary answers. It starts by pinning down the
concept of power, focusing on the ability that some agents have to shape
others' lives. It then explores how AI enables and intensifies the exercise of
power so understood, and sketches three problems with power and three ways to
solve those problems. It emphasises, in particular, that justifying power
requires more than satisfying substantive justificatory criteria; standards of
proper authority and procedural legitimacy must also be met. We need to know
not only what power may be used for, but how it may be used, and by whom.
\\ ( https://arxiv.org/abs/2404.05990 ,  234kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06007 (*cross-listing*)
Date: Tue, 9 Apr 2024 04:26:16 GMT   (1330kb,D)

Title: Collaborative Edge AI Inference over Cloud-RAN
Authors: Pengfei Zhang, Dingzhu Wen, Guangxu Zhu, Qimei Chen, Kaifeng Han,
  Yuanming Shi
Categories: cs.IT cs.AI cs.LG eess.SP math.IT
Comments: This paper is accepted by IEEE Transactions on Communications on
  08-Apr-2024
\\
  In this paper, a cloud radio access network (Cloud-RAN) based collaborative
edge AI inference architecture is proposed. Specifically, geographically
distributed devices capture real-time noise-corrupted sensory data samples and
extract the noisy local feature vectors, which are then aggregated at each
remote radio head (RRH) to suppress sensing noise. To realize efficient uplink
feature aggregation, we allow each RRH receives local feature vectors from all
devices over the same resource blocks simultaneously by leveraging an
over-the-air computation (AirComp) technique. Thereafter, these aggregated
feature vectors are quantized and transmitted to a central processor (CP) for
further aggregation and downstream inference tasks. Our aim in this work is to
maximize the inference accuracy via a surrogate accuracy metric called
discriminant gain, which measures the discernibility of different classes in
the feature space. The key challenges lie on simultaneously suppressing the
coupled sensing noise, AirComp distortion caused by hostile wireless channels,
and the quantization error resulting from the limited capacity of fronthaul
links. To address these challenges, this work proposes a joint transmit
precoding, receive beamforming, and quantization error control scheme to
enhance the inference accuracy. Extensive numerical experiments demonstrate the
effectiveness and superiority of our proposed optimization algorithm compared
to various baselines.
\\ ( https://arxiv.org/abs/2404.06007 ,  1330kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06014 (*cross-listing*)
Date: Tue, 9 Apr 2024 04:47:01 GMT   (2798kb,D)

Title: Using 3-Objective Evolutionary Algorithms for the Dynamic Chance
  Constrained Knapsack Problem
Authors: Ishara Hewa Pathiranage and Frank Neumann and Denis Antipov and Aneta
  Neumann
Categories: cs.NE cs.AI
\\
  Real-world optimization problems often involve stochastic and dynamic
components. Evolutionary algorithms are particularly effective in these
scenarios, as they can easily adapt to uncertain and changing environments but
often uncertainty and dynamic changes are studied in isolation. In this paper,
we explore the use of 3-objective evolutionary algorithms for the chance
constrained knapsack problem with dynamic constraints. In our setting, the
weights of the items are stochastic and the knapsack's capacity changes over
time. We introduce a 3-objective formulation that is able to deal with the
stochastic and dynamic components at the same time and is independent of the
confidence level required for the constraint. This new approach is then
compared to the 2-objective formulation which is limited to a single confidence
level. We evaluate the approach using two different multi-objective
evolutionary algorithms (MOEAs), namely the global simple evolutionary
multi-objective optimizer (GSEMO) and the multi-objective evolutionary
algorithm based on decomposition (MOEA/D), across various benchmark scenarios.
Our analysis highlights the advantages of the 3-objective formulation over the
2-objective formulation in addressing the dynamic chance constrained knapsack
problem.
\\ ( https://arxiv.org/abs/2404.06014 ,  2798kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06022 (*cross-listing*)
Date: Tue, 9 Apr 2024 05:11:28 GMT   (1106kb,D)

Title: Band-Attention Modulated RetNet for Face Forgery Detection
Authors: Zhida Zhang, Jie Cao, Wenkui Yang, Qihang Fan, Kai Zhou, Ran He
Categories: cs.CV cs.AI cs.MM
\\
  The transformer networks are extensively utilized in face forgery detection
due to their scalability across large datasets.Despite their success,
transformers face challenges in balancing the capture of global context, which
is crucial for unveiling forgery clues, with computational complexity.To
mitigate this issue, we introduce Band-Attention modulated RetNet (BAR-Net), a
lightweight network designed to efficiently process extensive visual contexts
while avoiding catastrophic forgetting.Our approach empowers the target token
to perceive global information by assigning differential attention levels to
tokens at varying distances. We implement self-attention along both spatial
axes, thereby maintaining spatial priors and easing the computational
burden.Moreover, we present the adaptive frequency Band-Attention Modulation
mechanism, which treats the entire Discrete Cosine Transform spectrogram as a
series of frequency bands with learnable weights.Together, BAR-Net achieves
favorable performance on several face forgery datasets, outperforming current
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2404.06022 ,  1106kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06025 (*cross-listing*)
Date: Tue, 9 Apr 2024 05:21:32 GMT   (3322kb,D)

Title: Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs
Authors: Zander W. Blasingame, Chen Liu
Categories: cs.CV cs.AI
Comments: Initial preprint. Under review
\\
  Morphing attacks are an emerging threat to state-of-the-art Face Recognition
(FR) systems, which aim to create a single image that contains the biometric
information of multiple identities. Diffusion Morphs (DiM) are a recently
proposed morphing attack that has achieved state-of-the-art performance for
representation-based morphing attacks. However, none of the existing research
on DiMs have leveraged the iterative nature of DiMs and left the DiM model as a
black box, treating it no differently than one would a Generative Adversarial
Network (GAN) or Varational AutoEncoder (VAE). We propose a greedy strategy on
the iterative sampling process of DiM models which searches for an optimal step
guided by an identity-based heuristic function. We compare our proposed
algorithm against ten other state-of-the-art morphing algorithms using the
open-source SYN-MAD 2022 competition dataset. We find that our proposed
algorithm is unreasonably effective, fooling all of the tested FR systems with
an MMPMR of 100%, outperforming all other morphing algorithms compared.
\\ ( https://arxiv.org/abs/2404.06025 ,  3322kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06059 (*cross-listing*)
Date: Tue, 9 Apr 2024 06:53:12 GMT   (1361kb,D)

Title: Efficient Quantum Circuits for Machine Learning Activation Functions
  including Constant T-depth ReLU
Authors: Wei Zi, Siyi Wang, Hyunji Kim, Xiaoming Sun, Anupam Chattopadhyay,
  Patrick Rebentrost
Categories: quant-ph cs.AI
Comments: 13 pages
\\
  In recent years, Quantum Machine Learning (QML) has increasingly captured the
interest of researchers. Among the components in this domain, activation
functions hold a fundamental and indispensable role. Our research focuses on
the development of activation functions quantum circuits for integration into
fault-tolerant quantum computing architectures, with an emphasis on minimizing
$T$-depth. Specifically, we present novel implementations of ReLU and leaky
ReLU activation functions, achieving constant $T$-depths of 4 and 8,
respectively. Leveraging quantum lookup tables, we extend our exploration to
other activation functions such as the sigmoid. This approach enables us to
customize precision and $T$-depth by adjusting the number of qubits, making our
results more adaptable to various application scenarios. This study represents
a significant advancement towards enhancing the practicality and application of
quantum machine learning.
\\ ( https://arxiv.org/abs/2404.06059 ,  1361kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06077 (*cross-listing*)
Date: Tue, 9 Apr 2024 07:32:30 GMT   (7995kb,D)

Title: Is Your AI Truly Yours? Leveraging Blockchain for Copyrights,
  Provenance, and Lineage
Authors: Yilin Sai, Qin Wang, Guangsheng Yu, H.M.N. Dilum Bandara, Shiping Chen
Categories: cs.CR cs.AI cs.CY
\\
  As Artificial Intelligence (AI) integrates into diverse areas, particularly
in content generation, ensuring rightful ownership and ethical use becomes
paramount. AI service providers are expected to prioritize responsibly sourcing
training data and obtaining licenses from data owners. However, existing
studies primarily center on safeguarding static copyrights, which simply treats
metadata/datasets as non-fungible items with transferable/trading capabilities,
neglecting the dynamic nature of training procedures that can shape an ongoing
trajectory.
  In this paper, we present \textsc{IBis}, a blockchain-based framework
tailored for AI model training workflows. \textsc{IBis} integrates on-chain
registries for datasets, licenses and models, alongside off-chain signing
services to facilitate collaboration among multiple participants. Our framework
addresses concerns regarding data and model provenance and copyright
compliance. \textsc{IBis} enables iterative model retraining and fine-tuning,
and offers flexible license checks and renewals. Further, \textsc{IBis}
provides APIs designed for seamless integration with existing contract
management software, minimizing disruptions to established model training
processes. We implement \textsc{IBis} using Daml on the Canton blockchain.
Evaluation results showcase the feasibility and scalability of \textsc{IBis}
across varying numbers of users, datasets, models, and licenses.
\\ ( https://arxiv.org/abs/2404.06077 ,  7995kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06079 (*cross-listing*)
Date: Tue, 9 Apr 2024 07:37:41 GMT   (1015kb,D)

Title: The X-LANCE Technical Report for Interspeech 2024 Speech Processing
  Using Discrete Speech Unit Challenge
Authors: Yiwei Guo, Chenrun Wang, Yifan Yang, Hankun Wang, Ziyang Ma, Chenpeng
  Du, Shuai Wang, Hanzheng Li, Shuai Fan, Hui Zhang, Xie Chen, Kai Yu
Categories: eess.AS cs.AI
Comments: 5 pages, 3 figures. Report of a challenge
\\
  Discrete speech tokens have been more and more popular in multiple speech
processing fields, including automatic speech recognition (ASR), text-to-speech
(TTS) and singing voice synthesis (SVS). In this paper, we describe the systems
developed by the SJTU X-LANCE group for the TTS (acoustic + vocoder), SVS, and
ASR tracks in the Interspeech 2024 Speech Processing Using Discrete Speech Unit
Challenge. Notably, we achieved 1st rank on the leaderboard in the TTS track
both with the whole training set and only 1h training data, along with the
lowest bitrate among all submissions.
\\ ( https://arxiv.org/abs/2404.06079 ,  1015kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06114 (*cross-listing*)
Date: Tue, 9 Apr 2024 08:35:04 GMT   (1953kb,D)

Title: Communication-Efficient Large-Scale Distributed Deep Learning: A
  Comprehensive Survey
Authors: Feng Liang, Zhen Zhang, Haifeng Lu, Victor C. M. Leung, Yanyi Guo,
  Xiping Hu
Categories: cs.DC cs.AI
\\
  With the rapid growth in the volume of data sets, models, and devices in the
domain of deep learning, there is increasing attention on large-scale
distributed deep learning. In contrast to traditional distributed deep
learning, the large-scale scenario poses new challenges that include fault
tolerance, scalability of algorithms and infrastructures, and heterogeneity in
data sets, models, and resources. Due to intensive synchronization of models
and sharing of data across GPUs and computing nodes during distributed training
and inference processes, communication efficiency becomes the bottleneck for
achieving high performance at a large scale. This article surveys the
literature over the period of 2018-2023 on algorithms and technologies aimed at
achieving efficient communication in large-scale distributed deep learning at
various levels, including algorithms, frameworks, and infrastructures.
Specifically, we first introduce efficient algorithms for model synchronization
and communication data compression in the context of large-scale distributed
training. Next, we introduce efficient strategies related to resource
allocation and task scheduling for use in distributed training and inference.
After that, we present the latest technologies pertaining to modern
communication infrastructures used in distributed deep learning with a focus on
examining the impact of the communication overhead in a large-scale and
heterogeneous setting. Finally, we conduct a case study on the distributed
training of large language models at a large scale to illustrate how to apply
these technologies in real cases. This article aims to offer researchers a
comprehensive understanding of the current landscape of large-scale distributed
deep learning and to reveal promising future research directions toward
communication-efficient solutions in this scope.
\\ ( https://arxiv.org/abs/2404.06114 ,  1953kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06124 (*cross-listing*)
Date: Tue, 9 Apr 2024 08:49:01 GMT   (9098kb,D)

Title: Hierarchical Insights: Exploiting Structural Similarities for Reliable
  3D Semantic Segmentation
Authors: Mariella Dreissig, Florian Piewak, Joschka Boedecker
Categories: cs.CV cs.AI cs.RO
Comments: submitted to IROS 2024
\\
  Safety-critical applications like autonomous driving call for robust 3D
environment perception algorithms which can withstand highly diverse and
ambiguous surroundings. The predictive performance of any classification model
strongly depends on the underlying dataset and the prior knowledge conveyed by
the annotated labels. While the labels provide a basis for the learning
process, they usually fail to represent inherent relations between the classes
- representations, which are a natural element of the human perception system.
We propose a training strategy which enables a 3D LiDAR semantic segmentation
model to learn structural relationships between the different classes through
abstraction. We achieve this by implicitly modeling those relationships through
a learning rule for hierarchical multi-label classification (HMC). With a
detailed analysis we show, how this training strategy not only improves the
model's confidence calibration, but also preserves additional information for
downstream tasks like fusion, prediction and planning.
\\ ( https://arxiv.org/abs/2404.06124 ,  9098kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06127 (*cross-listing*)
Date: Tue, 9 Apr 2024 08:51:05 GMT   (1305kb)

Title: FLEX: FLEXible Federated Learning Framework
Authors: Francisco Herrera, Daniel Jim\'enez-L\'opez, Alberto Argente-Garrido,
  Nuria Rodr\'iguez-Barroso, Cristina Zuheros, Ignacio Aguilera-Martos, Beatriz
  Bello, Mario Garc\'ia-M\'arquez, M. Victoria Luz\'on
Categories: cs.CR cs.AI
Comments: Submitted to Information Fusion
\\
  In the realm of Artificial Intelligence (AI), the need for privacy and
security in data processing has become paramount. As AI applications continue
to expand, the collection and handling of sensitive data raise concerns about
individual privacy protection. Federated Learning (FL) emerges as a promising
solution to address these challenges by enabling decentralized model training
on local devices, thus preserving data privacy. This paper introduces FLEX: a
FLEXible Federated Learning Framework designed to provide maximum flexibility
in FL research experiments. By offering customizable features for data
distribution, privacy parameters, and communication strategies, FLEX empowers
researchers to innovate and develop novel FL techniques. The framework also
includes libraries for specific FL implementations including: (1) anomalies,
(2) blockchain, (3) adversarial attacks and defences, (4) natural language
processing and (5) decision trees, enhancing its versatility and applicability
in various domains. Overall, FLEX represents a significant advancement in FL
research, facilitating the development of robust and efficient FL applications.
\\ ( https://arxiv.org/abs/2404.06127 ,  1305kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06152 (*cross-listing*)
Date: Tue, 9 Apr 2024 09:23:04 GMT   (703kb,D)

Title: HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields
Authors: Arnab Dey, Di Yang, Antitza Dantcheva, Jean Martinet
Categories: cs.CV cs.AI
\\
  In recent advancements in novel view synthesis, generalizable Neural Radiance
Fields (NeRF) based methods applied to human subjects have shown remarkable
results in generating novel views from few images. However, this generalization
ability cannot capture the underlying structural features of the skeleton
shared across all instances. Building upon this, we introduce HFNeRF: a novel
generalizable human feature NeRF aimed at generating human biomechanic features
using a pre-trained image encoder. While previous human NeRF methods have shown
promising results in the generation of photorealistic virtual avatars, such
methods lack underlying human structure or biomechanic features such as
skeleton or joint information that are crucial for downstream applications
including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D
pre-trained foundation models toward learning human features in 3D using neural
rendering, and then volume rendering towards generating 2D feature maps. We
evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as
features. The proposed method is fully differentiable, allowing to successfully
learn color, geometry, and human skeleton in a simultaneous manner. This paper
presents preliminary results of HFNeRF, illustrating its potential in
generating realistic virtual avatars with biomechanic features using NeRF.
\\ ( https://arxiv.org/abs/2404.06152 ,  703kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06177 (*cross-listing*)
Date: Tue, 9 Apr 2024 09:58:10 GMT   (1985kb,D)

Title: Uncertainty-aware Evidential Fusion-based Learning for Semi-supervised
  Medical Image Segmentation
Authors: Yuanpeng He, Lijian Li
Categories: cs.CV cs.AI
\\
  Although the existing uncertainty-based semi-supervised medical segmentation
methods have achieved excellent performance, they usually only consider a
single uncertainty evaluation, which often fails to solve the problem related
to credibility completely. Therefore, based on the framework of evidential deep
learning, this paper integrates the evidential predictive results in the
cross-region of mixed and original samples to reallocate the confidence degree
and uncertainty measure of each voxel, which is realized by emphasizing
uncertain information of probability assignments fusion rule of traditional
evidence theory. Furthermore, we design a voxel-level asymptotic learning
strategy by introducing information entropy to combine with the fused
uncertainty measure to estimate voxel prediction more precisely. The model will
gradually pay attention to the prediction results with high uncertainty in the
learning process, to learn the features that are difficult to master. The
experimental results on LA, Pancreas-CT, ACDC and TBAD datasets demonstrate the
superior performance of our proposed method in comparison with the existing
state of the arts.
\\ ( https://arxiv.org/abs/2404.06177 ,  1985kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06181 (*cross-listing*)
Date: Tue, 9 Apr 2024 10:04:06 GMT   (1502kb,D)

Title: EPL: Evidential Prototype Learning for Semi-supervised Medical Image
  Segmentation
Authors: Yuanpeng He
Categories: cs.CV cs.AI
\\
  Although current semi-supervised medical segmentation methods can achieve
decent performance, they are still affected by the uncertainty in unlabeled
data and model predictions, and there is currently a lack of effective
strategies that can explore the uncertain aspects of both simultaneously. To
address the aforementioned issues, we propose Evidential Prototype Learning
(EPL), which utilizes an extended probabilistic framework to effectively fuse
voxel probability predictions from different sources and achieves prototype
fusion utilization of labeled and unlabeled data under a generalized evidential
framework, leveraging voxel-level dual uncertainty masking. The uncertainty not
only enables the model to self-correct predictions but also improves the guided
learning process with pseudo-labels and is able to feed back into the
construction of hidden features. The method proposed in this paper has been
experimented on LA, Pancreas-CT and TBAD datasets, achieving the
state-of-the-art performance in three different labeled ratios, which strongly
demonstrates the effectiveness of our strategy.
\\ ( https://arxiv.org/abs/2404.06181 ,  1502kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06201 (*cross-listing*)
Date: Tue, 9 Apr 2024 10:47:02 GMT   (386kb,D)

Title: Open-Source AI-based SE Tools: Opportunities and Challenges of
  Collaborative Software Learning
Authors: Zhihao Lin and Wei Ma and Tao Lin and Yaowen Zheng and Jingquan Ge and
  Jun Wang and Jacques Klein and Tegawende Bissyande and Yang Liu and Li Li
Categories: cs.SE cs.AI
\\
  Large Language Models (LLMs) have become instrumental in advancing software
engineering (SE) tasks, showcasing their efficacy in code understanding and
beyond. Like traditional SE tools, open-source collaboration is key in
realising the excellent products. However, with AI models, the essential need
is in data. The collaboration of these AI-based SE models hinges on maximising
the sources of high-quality data. However, data especially of high quality,
often holds commercial or sensitive value, making it less accessible for
open-source AI-based SE projects. This reality presents a significant barrier
to the development and enhancement of AI-based SE tools within the software
engineering community. Therefore, researchers need to find solutions for
enabling open-source AI-based SE models to tap into resources by different
organisations. Addressing this challenge, our position paper investigates one
solution to facilitate access to diverse organizational resources for
open-source AI models, ensuring privacy and commercial sensitivities are
respected. We introduce a governance framework centered on federated learning
(FL), designed to foster the joint development and maintenance of open-source
AI code models while safeguarding data privacy and security. Additionally, we
present guidelines for developers on AI-based SE tool collaboration, covering
data requirements, model architecture, updating strategies, and version
control. Given the significant influence of data characteristics on FL, our
research examines the effect of code data heterogeneity on FL performance.
\\ ( https://arxiv.org/abs/2404.06201 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06212 (*cross-listing*)
Date: Tue, 9 Apr 2024 11:00:19 GMT   (6646kb,D)

Title: OmniFusion Technical Report
Authors: Elizaveta Goncharova, Anton Razzhigaev, Matvey Mikhalchuk, Maxim
  Kurkin, Irina Abdullaeva, Matvey Skripkin, Ivan Oseledets, Denis Dimitrov and
  Andrey Kuznetsov
Categories: cs.CV cs.AI cs.LG
Comments: 17 pages, 4 figures, 9 tables, 2 appendices
MSC-class: 6804, 68T50 (Primary)
ACM-class: I.2.7; I.2.10; I.4.9
\\
  Last year, multimodal architectures served up a revolution in AI-based
approaches and solutions, extending the capabilities of large language models
(LLM). We propose an \textit{OmniFusion} model based on a pretrained LLM and
adapters for visual modality. We evaluated and compared several architecture
design principles for better text and visual data coupling: MLP and transformer
adapters, various CLIP ViT-based encoders (SigLIP, InternVIT, etc.), and their
fusing approach, image encoding method (whole image or tiles encoding) and two
7B LLMs (the proprietary one and open-source Mistral). Experiments on 8
visual-language benchmarks show the top score for the best OmniFusion setup in
terms of different VQA tasks in comparison with open-source LLaVA-like
solutions: VizWiz, Pope, MM-Vet, ScienceQA, MMBench, TextVQA, VQAv2, MMMU. We
also propose a variety of situations, where OmniFusion provides highly-detailed
answers in different domains: housekeeping, sightseeing, culture, medicine,
handwritten and scanned equations recognition, etc. Mistral-based OmniFusion
model is an open-source solution with weights, training and inference scripts
available at https://github.com/AIRI-Institute/OmniFusion.
\\ ( https://arxiv.org/abs/2404.06212 ,  6646kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06229 (*cross-listing*)
Date: Tue, 9 Apr 2024 11:40:37 GMT   (12776kb,D)

Title: Towards Autonomous Driving with Small-Scale Cars: A Survey of Recent
  Development
Authors: Dianzhao Li, Paul Auerbach, and Ostap Okhrin
Categories: cs.RO cs.AI
\\
  While engaging with the unfolding revolution in autonomous driving, a
challenge presents itself, how can we effectively raise awareness within
society about this transformative trend? While full-scale autonomous driving
vehicles often come with a hefty price tag, the emergence of small-scale car
platforms offers a compelling alternative. These platforms not only serve as
valuable educational tools for the broader public and young generations but
also function as robust research platforms, contributing significantly to the
ongoing advancements in autonomous driving technology. This survey outlines
various small-scale car platforms, categorizing them and detailing the research
advancements accomplished through their usage. The conclusion provides
proposals for promising future directions in the field.
\\ ( https://arxiv.org/abs/2404.06229 ,  12776kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06243 (*cross-listing*)
Date: Tue, 9 Apr 2024 12:09:56 GMT   (612kb,D)

Title: ActNetFormer: Transformer-ResNet Hybrid Method for Semi-Supervised
  Action Recognition in Videos
Authors: Sharana Dharshikgan Suresh Dass and Hrishav Bakul Barua and Ganesh
  Krishnasamy and Raveendran Paramesran and Raphael C.-W. Phan
Categories: cs.CV cs.AI cs.HC cs.LG cs.MM
Comments: Submitted for peer review
MSC-class: Artificial intelligence, Computer vision, Machine learning, Deep
  learning, Human-computer Interaction
ACM-class: I.2; I.2.9; I.2.10; I.3.3; I.4.5
\\
  Human action or activity recognition in videos is a fundamental task in
computer vision with applications in surveillance and monitoring, self-driving
cars, sports analytics, human-robot interaction and many more. Traditional
supervised methods require large annotated datasets for training, which are
expensive and time-consuming to acquire. This work proposes a novel approach
using Cross-Architecture Pseudo-Labeling with contrastive learning for
semi-supervised action recognition. Our framework leverages both labeled and
unlabelled data to robustly learn action representations in videos, combining
pseudo-labeling with contrastive learning for effective learning from both
types of samples. We introduce a novel cross-architecture approach where 3D
Convolutional Neural Networks (3D CNNs) and video transformers (VIT) are
utilised to capture different aspects of action representations; hence we call
it ActNetFormer. The 3D CNNs excel at capturing spatial features and local
dependencies in the temporal domain, while VIT excels at capturing long-range
dependencies across frames. By integrating these complementary architectures
within the ActNetFormer framework, our approach can effectively capture both
local and global contextual information of an action. This comprehensive
representation learning enables the model to achieve better performance in
semi-supervised action recognition tasks by leveraging the strengths of each of
these architectures. Experimental results on standard action recognition
datasets demonstrate that our approach performs better than the existing
methods, achieving state-of-the-art performance with only a fraction of labeled
data. The official website of this work is available at:
https://github.com/rana2149/ActNetFormer.
\\ ( https://arxiv.org/abs/2404.06243 ,  612kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06246 (*cross-listing*)
Date: Tue, 9 Apr 2024 12:11:25 GMT   (1487kb,D)

Title: GHNeRF: Learning Generalizable Human Features with Efficient Neural
  Radiance Fields
Authors: Arnab Dey, Di Yang, Rohith Agaram, Antitza Dantcheva, Andrew I.
  Comport, Srinath Sridhar, Jean Martinet
Categories: cs.CV cs.AI
\\
  Recent advances in Neural Radiance Fields (NeRF) have demonstrated promising
results in 3D scene representations, including 3D human representations.
However, these representations often lack crucial information on the underlying
human pose and structure, which is crucial for AR/VR applications and games. In
this paper, we introduce a novel approach, termed GHNeRF, designed to address
these limitations by learning 2D/3D joint locations of human subjects with NeRF
representation. GHNeRF uses a pre-trained 2D encoder streamlined to extract
essential human features from 2D images, which are then incorporated into the
NeRF framework in order to encode human biomechanic features. This allows our
network to simultaneously learn biomechanic features, such as joint locations,
along with human geometry and texture. To assess the effectiveness of our
method, we conduct a comprehensive comparison with state-of-the-art human NeRF
techniques and joint estimation algorithms. Our results show that GHNeRF can
achieve state-of-the-art results in near real-time.
\\ ( https://arxiv.org/abs/2404.06246 ,  1487kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06261 (*cross-listing*)
Date: Tue, 9 Apr 2024 12:34:28 GMT   (9580kb,D)

Title: Playing to Vision Foundation Model's Strengths in Stereo Matching
Authors: Chuang-Wei Liu, Qijun Chen, Rui Fan
Categories: cs.CV cs.AI cs.RO
\\
  Stereo matching has become a key technique for 3D environment perception in
intelligent vehicles. For a considerable time, convolutional neural networks
(CNNs) have remained the mainstream choice for feature extraction in this
domain. Nonetheless, there is a growing consensus that the existing paradigm
should evolve towards vision foundation models (VFM), particularly those
developed based on vision Transformers (ViTs) and pre-trained through
self-supervision on extensive, unlabeled datasets. While VFMs are adept at
extracting informative, general-purpose visual features, specifically for dense
prediction tasks, their performance often lacks in geometric vision tasks. This
study serves as the first exploration of a viable approach for adapting VFMs to
stereo matching. Our ViT adapter, referred to as ViTAS, is constructed upon
three types of modules: spatial differentiation, patch attention fusion, and
cross-attention. The first module initializes feature pyramids, while the
latter two aggregate stereo and multi-scale contextual information into
fine-grained features, respectively. ViTAStereo, which combines ViTAS with cost
volume-based stereo matching back-end processes, achieves the top rank on the
KITTI Stereo 2012 dataset and outperforms the second-best network StereoBase by
approximately 7.9% in terms of the percentage of error pixels, with a tolerance
of 3 pixels. Additional experiments across diverse scenarios further
demonstrate its superior generalizability compared to all other
state-of-the-art approaches. We believe this new paradigm will pave the way for
the next generation of stereo matching networks.
\\ ( https://arxiv.org/abs/2404.06261 ,  9580kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06278 (*cross-listing*)
Date: Tue, 9 Apr 2024 13:02:22 GMT   (72kb,D)

Title: Dimensionality Reduction in Sentence Transformer Vector Databases with
  Fast Fourier Transform
Authors: Vitaly Bulgakov, Alec Segal
Categories: cs.DB cs.AI cs.CL cs.LG
Comments: 13 pages, 5 figures
\\
  Dimensionality reduction in vector databases is pivotal for streamlining AI
data management, enabling efficient storage, faster computation, and improved
model performance. This paper explores the benefits of reducing vector database
dimensions, with a focus on computational efficiency and overcoming the curse
of dimensionality. We introduce a novel application of Fast Fourier Transform
(FFT) to dimensionality reduction, a method previously underexploited in this
context. By demonstrating its utility across various AI domains, including
Retrieval-Augmented Generation (RAG) models and image processing, this
FFT-based approach promises to improve data retrieval processes and enhance the
efficiency and scalability of AI solutions. The incorporation of FFT may not
only optimize operations in real-time processing and recommendation systems but
also extend to advanced image processing techniques, where dimensionality
reduction can significantly improve performance and analysis efficiency. This
paper advocates for the broader adoption of FFT in vector database management,
marking a significant stride towards addressing the challenges of data volume
and complexity in AI research and applications. Unlike many existing
approaches, we directly handle the embedding vectors produced by the model
after processing a test input.
\\ ( https://arxiv.org/abs/2404.06278 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06279 (*cross-listing*)
Date: Tue, 9 Apr 2024 13:02:33 GMT   (8437kb,D)

Title: NoiseNCA: Noisy Seed Improves Spatio-Temporal Continuity of Neural
  Cellular Automata
Authors: Ehsan Pajouheshgar, Yitao Xu, Sabine S\"usstrunk
Categories: cs.CV cs.AI cs.GR cs.MA
Comments: 9 pages, 12 figures
\\
  Neural Cellular Automata (NCA) is a class of Cellular Automata where the
update rule is parameterized by a neural network that can be trained using
gradient descent. In this paper, we focus on NCA models used for texture
synthesis, where the update rule is inspired by partial differential equations
(PDEs) describing reaction-diffusion systems. To train the NCA model, the
spatio-termporal domain is discretized, and Euler integration is used to
numerically simulate the PDE. However, whether a trained NCA truly learns the
continuous dynamic described by the corresponding PDE or merely overfits the
discretization used in training remains an open question. We study NCA models
at the limit where space-time discretization approaches continuity. We find
that existing NCA models tend to overfit the training discretization,
especially in the proximity of the initial condition, also called "seed". To
address this, we propose a solution that utilizes uniform noise as the initial
condition. We demonstrate the effectiveness of our approach in preserving the
consistency of NCA dynamics across a wide range of spatio-temporal
granularities. Our improved NCA model enables two new test-time interactions by
allowing continuous control over the speed of pattern formation and the scale
of the synthesized patterns. We demonstrate this new NCA feature in our
interactive online demo. Our work reveals that NCA models can learn continuous
dynamics and opens new venues for NCA research from a dynamical systems'
perspective.
\\ ( https://arxiv.org/abs/2404.06279 ,  8437kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06324 (*cross-listing*)
Date: Tue, 9 Apr 2024 14:03:04 GMT   (2763kb,D)

Title: Dynamic D2D-Assisted Federated Learning over O-RAN: Performance
  Analysis, MAC Scheduler, and Asymmetric User Selection
Authors: Payam Abdisarabshali, Kwang Taik Kim, Michael Langberg, Weifeng Su,
  Seyyedali Hosseinalipour
Categories: cs.NI cs.AI cs.LG
Comments: 120 pages, 13 figures
\\
  Existing studies on federated learning (FL) are mostly focused on system
orchestration for static snapshots of the network and making static control
decisions (e.g., spectrum allocation). However, real-world wireless networks
are susceptible to temporal variations of wireless channel capacity and users'
datasets. In this paper, we incorporate multi-granular system dynamics (MSDs)
into FL, including (M1) dynamic wireless channel capacity, captured by a set of
discrete-time events, called $\mathscr{D}$-Events, and (M2) dynamic datasets of
users. The latter is characterized by (M2-a) modeling the dynamics of user's
dataset size via an ordinary differential equation and (M2-b) introducing
dynamic model drift}, formulated via a partial differential inequality} drawing
concrete analytical connections between the dynamics of users' datasets and FL
accuracy. We then conduct FL orchestration under MSDs by introducing dynamic
cooperative FL with dedicated MAC schedulers (DCLM), exploiting the unique
features of open radio access network (O-RAN). DCLM proposes (i) a hierarchical
device-to-device (D2D)-assisted model training, (ii) dynamic control decisions
through dedicated O-RAN MAC schedulers, and (iii) asymmetric user selection. We
provide extensive theoretical analysis to study the convergence of DCLM. We
then optimize the degrees of freedom (e.g., user selection and spectrum
allocation) in DCLM through a highly non-convex optimization problem. We
develop a systematic approach to obtain the solution for this problem, opening
the door to solving a broad variety of network-aware FL optimization problems.
We show the efficiency of DCLM via numerical simulations and provide a series
of future directions.
\\ ( https://arxiv.org/abs/2404.06324 ,  2763kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06362 (*cross-listing*)
Date: Tue, 9 Apr 2024 14:56:34 GMT   (3255kb,D)

Title: Test-Time Adaptation with SaLIP: A Cascade of SAM and CLIP for Zero shot
  Medical Image Segmentation
Authors: Sidra Aleem, Fangyijie Wang, Mayug Maniparambil, Eric Arazo, Julia
  Dietlmeier, Kathleen Curran, Noel E. O'Connor, Suzanne Little
Categories: cs.CV cs.AI
\\
  The Segment Anything Model (SAM) and CLIP are remarkable vision foundation
models (VFMs). SAM, a prompt driven segmentation model, excels in segmentation
tasks across diverse domains, while CLIP is renowned for its zero shot
recognition capabilities. However, their unified potential has not yet been
explored in medical image segmentation. To adapt SAM to medical imaging,
existing methods primarily rely on tuning strategies that require extensive
data or prior prompts tailored to the specific task, making it particularly
challenging when only a limited number of data samples are available. This work
presents an in depth exploration of integrating SAM and CLIP into a unified
framework for medical image segmentation. Specifically, we propose a simple
unified framework, SaLIP, for organ segmentation. Initially, SAM is used for
part based segmentation within the image, followed by CLIP to retrieve the mask
corresponding to the region of interest (ROI) from the pool of SAM generated
masks. Finally, SAM is prompted by the retrieved ROI to segment a specific
organ. Thus, SaLIP is training and fine tuning free and does not rely on domain
expertise or labeled data for prompt engineering. Our method shows substantial
enhancements in zero shot segmentation, showcasing notable improvements in DICE
scores across diverse segmentation tasks like brain (63.46%), lung (50.11%),
and fetal head (30.82%), when compared to un prompted SAM. Code and text
prompts will be available online.
\\ ( https://arxiv.org/abs/2404.06362 ,  3255kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06369 (*cross-listing*)
Date: Tue, 9 Apr 2024 15:05:48 GMT   (672kb,D)

Title: VISION2UI: A Real-World Dataset with Layout for Code Generation from UI
  Designs
Authors: Yi Gui, Zhen Li, Yao Wan, Yemin Shi, Hongyu Zhang, Yi Su, Shaoling
  Dong, Xing Zhou, Wenbin Jiang
Categories: cs.CV cs.AI cs.SE
\\
  Automatically generating UI code from webpage design visions can
significantly alleviate the burden of developers, enabling beginner developers
or designers to directly generate Web pages from design diagrams. Currently,
prior research has accomplished the objective of generating UI code from
rudimentary design visions or sketches through designing deep neural networks.
Inspired by the groundbreaking advancements achieved by Multimodal Large
Language Models (MLLMs), the automatic generation of UI code from high-fidelity
design images is now emerging as a viable possibility. Nevertheless, our
investigation reveals that existing MLLMs are hampered by the scarcity of
authentic, high-quality, and large-scale datasets, leading to unsatisfactory
performance in automated UI code generation. To mitigate this gap, we present a
novel dataset, termed VISION2UI, extracted from real-world scenarios, augmented
with comprehensive layout information, tailored specifically for finetuning
MLLMs in UI code generation. Specifically, this dataset is derived through a
series of operations, encompassing collecting, cleaning, and filtering of the
open-source Common Crawl dataset. In order to uphold its quality, a neural
scorer trained on labeled samples is utilized to refine the data, retaining
higher-quality instances. Ultimately, this process yields a dataset comprising
2,000 (Much more is coming soon) parallel samples encompassing design visions
and UI code. The dataset is available at
https://huggingface.co/datasets/xcodemind/vision2ui.
\\ ( https://arxiv.org/abs/2404.06369 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06393 (*cross-listing*)
Date: Tue, 9 Apr 2024 15:35:52 GMT   (1547kb,D)

Title: MuPT: A Generative Symbolic Music Pretrained Transformer
Authors: Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu,
  Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo,
  Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma,
  Fengze Han, Wei Xue, Gus Xia, Emmanouil Benetos, Xiang Yue, Chenghua Lin, Xu
  Tan, Stephen W. Huang, Wenhu Chen, Jie Fu, and Ge Zhang
Categories: cs.SD cs.AI eess.AS
\\
  In this paper, we explore the application of Large Language Models (LLMs) to
the pre-training of music. While the prevalent use of MIDI in music modeling is
well-established, our findings suggest that LLMs are inherently more compatible
with ABC Notation, which aligns more closely with their design and strengths,
thereby enhancing the model's performance in musical composition. To address
the challenges associated with misaligned measures from different tracks during
generation, we propose the development of a \underline{S}ynchronized
\underline{M}ulti-\underline{T}rack ABC Notation (\textbf{SMT-ABC Notation}),
which aims to preserve coherence across multiple musical tracks. Our
contributions include a series of models capable of handling up to 8192 tokens,
covering 90\% of the symbolic music data in our training set. Furthermore, we
explore the implications of the \underline{S}ymbolic \underline{M}usic
\underline{S}caling Law (\textbf{SMS Law}) on model performance. The results
indicate a promising direction for future research in music generation,
offering extensive resources for community-led research through our open-source
contributions.
\\ ( https://arxiv.org/abs/2404.06393 ,  1547kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06404 (*cross-listing*)
Date: Tue, 9 Apr 2024 15:53:06 GMT   (510kb)

Title: Apprentices to Research Assistants: Advancing Research with Large
  Language Models
Authors: M. Namvarpour and A. Razi
Categories: cs.HC cs.AI cs.LG
ACM-class: I.2; H.5; H.3; K.4; I.7
\\
  Large Language Models (LLMs) have emerged as powerful tools in various
research domains. This article examines their potential through a literature
review and firsthand experimentation. While LLMs offer benefits like
cost-effectiveness and efficiency, challenges such as prompt tuning, biases,
and subjectivity must be addressed. The study presents insights from
experiments utilizing LLMs for qualitative analysis, highlighting successes and
limitations. Additionally, it discusses strategies for mitigating challenges,
such as prompt optimization techniques and leveraging human expertise. This
study aligns with the 'LLMs as Research Tools' workshop's focus on integrating
LLMs into HCI data work critically and ethically. By addressing both
opportunities and challenges, our work contributes to the ongoing dialogue on
their responsible application in research.
\\ ( https://arxiv.org/abs/2404.06404 ,  510kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06423 (*cross-listing*)
Date: Tue, 9 Apr 2024 16:14:03 GMT   (1084kb,D)

Title: Deep Reinforcement Learning-Based Approach for a Single Vehicle
  Persistent Surveillance Problem with Fuel Constraints
Authors: Hritik Bana and Manav Mishra and Saswata Sarkar and Sujeevraja
  Sanjeevi and Sujit PB and Kaarthik Sundar
Categories: cs.RO cs.AI cs.LG
Comments: 6 pages
Report-no: LA-UR-24-23186
\\
  This article presents a deep reinforcement learning-based approach to tackle
a persistent surveillance mission requiring a single unmanned aerial vehicle
initially stationed at a depot with fuel or time-of-flight constraints to
repeatedly visit a set of targets with equal priority. Owing to the vehicle's
fuel or time-of-flight constraints, the vehicle must be regularly refueled, or
its battery must be recharged at the depot. The objective of the problem is to
determine an optimal sequence of visits to the targets that minimizes the
maximum time elapsed between successive visits to any target while ensuring
that the vehicle never runs out of fuel or charge. We present a deep
reinforcement learning algorithm to solve this problem and present the results
of numerical experiments that corroborate the effectiveness of this approach in
comparison with common-sense greedy heuristics.
\\ ( https://arxiv.org/abs/2404.06423 ,  1084kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06429 (*cross-listing*)
Date: Tue, 9 Apr 2024 16:20:03 GMT   (36298kb,D)

Title: Magic-Boost: Boost 3D Generation with Mutli-View Conditioned Diffusion
Authors: Fan Yang, Jianfeng Zhang, Yichun Shi, Bowen Chen, Chenxu Zhang,
  Huichao Zhang, Xiaofeng Yang, Jiashi Feng, Guosheng Lin
Categories: cs.CV cs.AI
\\
  Benefiting from the rapid development of 2D diffusion models, 3D content
creation has made significant progress recently. One promising solution
involves the fine-tuning of pre-trained 2D diffusion models to harness their
capacity for producing multi-view images, which are then lifted into accurate
3D models via methods like fast-NeRFs or large reconstruction models. However,
as inconsistency still exists and limited generated resolution, the generation
results of such methods still lack intricate textures and complex geometries.
To solve this problem, we propose Magic-Boost, a multi-view conditioned
diffusion model that significantly refines coarse generative results through a
brief period of SDS optimization ($\sim15$min). Compared to the previous text
or single image based diffusion models, Magic-Boost exhibits a robust
capability to generate images with high consistency from pseudo synthesized
multi-view images. It provides precise SDS guidance that well aligns with the
identity of the input images, enriching the local detail in both geometry and
texture of the initial generative results. Extensive experiments show
Magic-Boost greatly enhances the coarse inputs and generates high-quality 3D
assets with rich geometric and textural details. (Project Page:
https://magic-research.github.io/magic-boost/)
\\ ( https://arxiv.org/abs/2404.06429 ,  36298kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06453 (*cross-listing*)
Date: Tue, 9 Apr 2024 16:54:19 GMT   (9771kb,D)

Title: PURE: Turning Polysemantic Neurons Into Pure Features by Identifying
  Relevant Circuits
Authors: Maximilian Dreyer, Erblina Purelku, Johanna Vielhaben, Wojciech Samek,
  Sebastian Lapuschkin
Categories: cs.CV cs.AI cs.LG
Comments: 14 pages (4 pages manuscript, 2 pages references, 8 pages appendix)
\\
  The field of mechanistic interpretability aims to study the role of
individual neurons in Deep Neural Networks. Single neurons, however, have the
capability to act polysemantically and encode for multiple (unrelated)
features, which renders their interpretation difficult. We present a method for
disentangling polysemanticity of any Deep Neural Network by decomposing a
polysemantic neuron into multiple monosemantic "virtual" neurons. This is
achieved by identifying the relevant sub-graph ("circuit") for each "pure"
feature. We demonstrate how our approach allows us to find and disentangle
various polysemantic units of ResNet models trained on ImageNet. While
evaluating feature visualizations using CLIP, our method effectively
disentangles representations, improving upon methods based on neuron
activations. Our code is available at https://github.com/maxdreyer/PURE.
\\ ( https://arxiv.org/abs/2404.06453 ,  9771kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06484 (*cross-listing*)
Date: Tue, 9 Apr 2024 17:35:11 GMT   (34kb,D)

Title: Public-private funding models in open source software development: A
  case study on scikit-learn
Authors: Cailean Osborne
Categories: cs.SE cs.AI cs.CY cs.LG
Comments: 17 pages
ACM-class: K.4.1
\\
  Governments are increasingly allocating funding for open source software
(OSS) development in order to address concerns related to software security,
digital sovereignty, and the competitiveness of domestic software markets,
amongst others. While such funding is generally welcomed by OSS practitioners,
how OSS developers perceive the relative benefits and drawbacks of governmental
funding remains an open question. This paper explores this question through a
case study on scikit-learn, a Python library for machine learning, whose
funding model combines research grants, commercial sponsorship, community
donations, and a 32 million EUR grant from the French government's artificial
intelligence strategy. Through 25 interviews with scikit-learn maintainers and
funders, this study makes two key contributions with implications for research
and practice. First, it provides novel insights into the role of a
public-private funding model in a successful, community-led OSS project and how
maintainers evaluate their funding model. Furthermore, it highlights the
governance mechanisms employed by maintainers to safeguard the community ethos
of the project. Second, it offers practical implications for OSS developer
communities, companies, and governments. For OSS communities, the study
illustrates the benefits of a diversified funding model in balancing the merits
and drawbacks of different funding sources. For companies, it serves as a
reminder that sponsoring developers or directly funding OSS projects can
significantly support OSS maintainers, who often struggle with limited
resources and towering workloads. For governments, the findings emphasise the
importance of funding the maintenance of existing OSS projects in addition to
or exclusively funding new innovations. The paper concludes with suggestions
for future research on OSS funding models.
\\ ( https://arxiv.org/abs/2404.06484 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06511 (*cross-listing*)
Date: Tue, 9 Apr 2024 17:59:31 GMT   (10281kb,D)

Title: MoReVQA: Exploring Modular Reasoning Models for Video Question Answering
Authors: Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024
\\
  This paper addresses the task of video question answering (videoQA) via a
decomposed multi-stage, modular reasoning framework. Previous modular methods
have shown promise with a single planning stage ungrounded in visual content.
However, through a simple and effective baseline, we find that such systems can
lead to brittle behavior in practice for challenging videoQA settings. Thus,
unlike traditional single-stage planning methods, we propose a multi-stage
system consisting of an event parser, a grounding stage, and a final reasoning
stage in conjunction with an external memory. All stages are training-free, and
performed using few-shot prompting of large models, creating interpretable
intermediate outputs at each stage. By decomposing the underlying planning and
task complexity, our method, MoReVQA, improves over prior work on standard
videoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) with
state-of-the-art results, and extensions to related tasks (grounded videoQA,
paragraph captioning).
\\ ( https://arxiv.org/abs/2404.06511 ,  10281kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06004 (*cross-listing*)
Date: Tue, 9 Apr 2024 04:20:27 GMT   (386kb,D)

Title: AiSAQ: All-in-Storage ANNS with Product Quantization for DRAM-free
  Information Retrieval
Authors: Kento Tatsuno, Daisuke Miyashita, Taiga Ikeda, Kiyoshi Ishiyama,
  Kazunari Sumiyoshi and Jun Deguchi
Categories: cs.IR cs.CL cs.DS
Comments: 5 pages, 6 figures and 4 tables
\\
  In approximate nearest neighbor search (ANNS) methods based on approximate
proximity graphs, DiskANN achieves good recall-speed balance for large-scale
datasets using both of RAM and storage. Despite it claims to save memory usage
by loading compressed vectors by product quantization (PQ), its memory usage
increases in proportion to the scale of datasets. In this paper, we propose
All-in-Storage ANNS with Product Quantization (AiSAQ), which offloads the
compressed vectors to storage. Our method achieves $\sim$10 MB memory usage in
query search even with billion-scale datasets with minor performance
degradation. AiSAQ also reduces the index load time before query search, which
enables the index switch between muitiple billion-scale datasets and
significantly enhances the flexibility of retrieval-augmented generation (RAG).
This method is applicable to all graph-based ANNS algorithms and can be
combined with higher-spec ANNS methods in the future.
\\ ( https://arxiv.org/abs/2404.06004 ,  386kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06371 (*cross-listing*)
Date: Tue, 9 Apr 2024 15:07:25 GMT   (1818kb,D)

Title: Model Generation from Requirements with LLMs: an Exploratory Study
Authors: Alessio Ferrari, Sallam Abualhaija, Chetan Arora
Categories: cs.SE cs.CL cs.LG
ACM-class: D.2; K.6.3; D.2.1; D.3.1; D.2.2; D.2.10; D.2.2; I.2; I.2.7
\\
  Complementing natural language (NL) requirements with graphical models can
improve stakeholders' communication and provide directions for system design.
However, creating models from requirements involves manual effort. The advent
of generative large language models (LLMs), ChatGPT being a notable example,
offers promising avenues for automated assistance in model generation. This
paper investigates the capability of ChatGPT to generate a specific type of
model, i.e., UML sequence diagrams, from NL requirements. We conduct a
qualitative study in which we examine the sequence diagrams generated by
ChatGPT for 28 requirements documents of various types and from different
domains. Observations from the analysis of the generated diagrams have
systematically been captured through evaluation logs, and categorized through
thematic analysis. Our results indicate that, although the models generally
conform to the standard and exhibit a reasonable level of understandability,
their completeness and correctness with respect to the specified requirements
often present challenges. This issue is particularly pronounced in the presence
of requirements smells, such as ambiguity and inconsistency. The insights
derived from this study can influence the practical utilization of LLMs in the
RE process, and open the door to novel RE-specific prompting strategies
targeting effective model generation.
\\ ( https://arxiv.org/abs/2404.06371 ,  1818kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06413 (*cross-listing*)
Date: Tue, 9 Apr 2024 16:03:26 GMT   (5481kb,D)

Title: Large Language Models to the Rescue: Deadlock Resolution in Multi-Robot
  Systems
Authors: Kunal Garg, Jacob Arkin, Songyuan Zhang, Nicholas Roy, Chuchu Fan
Categories: cs.RO cs.CL math.OC
\\
  Multi-agent robotic systems are prone to deadlocks in an obstacle environment
where the system can get stuck away from its desired location under a smooth
low-level control policy. Without an external intervention, often in terms of a
high-level command, it is not possible to guarantee that just a low-level
control policy can resolve such deadlocks. Utilizing the generalizability and
low data requirements of large language models (LLMs), this paper explores the
possibility of using LLMs for deadlock resolution. We propose a hierarchical
control framework where an LLM resolves deadlocks by assigning a leader and
direction for the leader to move along. A graph neural network (GNN) based
low-level distributed control policy executes the assigned plan. We
systematically study various prompting techniques to improve LLM's performance
in resolving deadlocks. In particular, as part of prompt engineering, we
provide in-context examples for LLMs. We conducted extensive experiments on
various multi-robot environments with up to 15 agents and 40 obstacles. Our
results demonstrate that LLM-based high-level planners are effective in
resolving deadlocks in MRS.
\\ ( https://arxiv.org/abs/2404.06413 ,  5481kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06512 (*cross-listing*)
Date: Tue, 9 Apr 2024 17:59:32 GMT   (4942kb,D)

Title: InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model
  Handling Resolutions from 336 Pixels to 4K HD
Authors: Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Bin Wang, Linke
  Ouyang, Songyang Zhang, Haodong Duan, Wenwei Zhang, Yining Li, Hang Yan, Yang
  Gao, Zhe Chen, Xinyue Zhang, Wei Li, Jingwen Li, Wenhai Wang, Kai Chen,
  Conghui He, Xingcheng Zhang, Jifeng Dai, Yu Qiao, Dahua Lin, Jiaqi Wang
Categories: cs.CV cs.CL
Comments: Code and models are publicly available at
  https://github.com/InternLM/InternLM-XComposer
\\
  The Large Vision-Language Model (LVLM) field has seen significant
advancements, yet its progression has been hindered by challenges in
comprehending fine-grained visual content due to limited resolution. Recent
efforts have aimed to enhance the high-resolution understanding capabilities of
LVLMs, yet they remain capped at approximately 1500 x 1500 pixels and
constrained to a relatively narrow resolution range. This paper represents
InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM
resolution capabilities up to 4K HD (3840 x 1600) and beyond. Concurrently,
considering the ultra-high resolution may not be necessary in all scenarios, it
supports a wide range of diverse resolutions from 336 pixels to 4K standard,
significantly broadening its scope of applicability. Specifically, this
research advances the patch division paradigm by introducing a novel extension:
dynamic resolution with automatic patch configuration. It maintains the
training image aspect ratios while automatically varying patch counts and
configuring layouts based on a pre-trained Vision Transformer (ViT) (336 x
336), leading to dynamic training resolution from 336 pixels to 4K standard.
Our research demonstrates that scaling training resolution up to 4K HD leads to
consistent performance enhancements without hitting the ceiling of potential
improvements. InternLM-XComposer2-4KHD shows superb capability that matches or
even surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks. The
InternLM-XComposer2-4KHD model series with 7B parameters are publicly available
at https://github.com/InternLM/InternLM-XComposer.
\\ ( https://arxiv.org/abs/2404.06512 ,  4942kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05748 (*cross-listing*)
Date: Thu, 4 Apr 2024 06:06:26 GMT   (2690kb)

Title: Analysing heterogeneity in Alzheimer Disease using multimodal normative
  modelling on ATN biomarkers
Authors: Sayantan Kumara, Thomas Earnest, Braden Yang, Deydeep Kothapalli,
  Tammie L. S. Benzinger, Brian A. Gordon, Philip Payne, Aristeidis Sotiras
Categories: q-bio.NC cs.LG
Comments: Submitted to Neurology, 5 figures, 2 tables. 5 figures and 3 tables
  in Supplementary material
\\
  Alzheimer Disease (AD) is a multi-faceted disorder, with each modality
providing unique and complementary info about AD. In this study, we used a
deep-learning based multimodal normative model to assess the heterogeneity in
regional brain patterns for ATN (amyloid-tau-neurodegeneration) biomarkers. We
selected discovery (n = 665) and replication (n = 430) cohorts with
simultaneous availability of ATN biomarkers: Florbetapir amyloid, Flortaucipir
tau and T1-weighted MRI (magnetic resonance imaging) imaging. A multimodal
variational autoencoder (conditioned on age and sex) was used as a normative
model to learn the multimodal regional brain patterns of a cognitively
unimpaired (CU) control group. The trained model was applied on individuals on
the ADS (AD Spectrum) to estimate their deviations (Z-scores) from the
normative distribution, resulting in a Z-score regional deviation map per ADS
individual per modality. ADS individuals with moderate or severe dementia
showed higher proportion of regional outliers for each modality as well as more
dissimilarity in modality-specific regional outlier patterns compared to ADS
individuals with early or mild dementia. DSI was associated with the
progressive stages of dementia, (ii) showed significant associations with
neuropsychological composite scores and (iii) related to the longitudinal risk
of CDR progression. Findings were reproducible in both discovery and
replication cohorts. Our is the first study to examine the heterogeneity in AD
through the lens of multiple neuroimaging modalities (ATN), based on distinct
or overlapping patterns of regional outlier deviations. Regional MRI and tau
outliers were more heterogenous than regional amyloid outliers. DSI has the
potential to be an individual patient metric of neurodegeneration that can help
in clinical decision making and monitoring patient response for anti-amyloid
treatments.
\\ ( https://arxiv.org/abs/2404.05748 ,  2690kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05752 (*cross-listing*)
Date: Fri, 5 Apr 2024 03:52:27 GMT   (1539kb,D)

Title: Physics Event Classification Using Large Language Models
Authors: Cristiano Fanelli and James Giroux and Patrick Moran and Hemalata
  Nayak and Karthik Suresh and Eric Walter
Categories: physics.data-an cs.LG hep-ex
Comments: 10 pages, 4 figures
\\
  The 2023 AI4EIC hackathon was the culmination of the third annual AI4EIC
workshop at The Catholic University of America. This workshop brought together
researchers from physics, data science and computer science to discuss the
latest developments in Artificial Intelligence (AI) and Machine Learning (ML)
for the Electron Ion Collider (EIC), including applications for detectors,
accelerators, and experimental control. The hackathon, held on the final day of
the workshop, involved using a chatbot powered by a Large Language Model,
ChatGPT-3.5, to train a binary classifier neutrons and photons in simulated
data from the \textsc{GlueX} Barrel Calorimeter. In total, six teams of up to
four participants from all over the world took part in this intense educational
and research event. This article highlights the hackathon challenge, the
resources and methodology used, and the results and insights gained from
analyzing physics data using the most cutting-edge tools in AI/ML.
\\ ( https://arxiv.org/abs/2404.05752 ,  1539kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05781 (*cross-listing*)
Date: Mon, 8 Apr 2024 16:36:07 GMT   (3644kb,D)

Title: Group-specific discriminant analysis reveals statistically validated sex
  differences in lateralization of brain functional network
Authors: Shuo Zhou, Junhao Luo, Yaya Jiang, Haolin Wang, Haiping Lu, Gaolang
  Gong
Categories: q-bio.NC cs.LG
\\
  Lateralization is a fundamental feature of the human brain, where sex
differences have been observed. Conventional studies in neuroscience on
sex-specific lateralization are typically conducted on univariate statistical
comparisons between male and female groups. However, these analyses often lack
effective validation of group specificity. Here, we formulate modeling sex
differences in lateralization of functional networks as a dual-classification
problem, consisting of first-order classification for left vs. right functional
networks and second-order classification for male vs. female models. To capture
sex-specific patterns, we develop the Group-Specific Discriminant Analysis
(GSDA) for first-order classification. The evaluation on two public
neuroimaging datasets demonstrates the efficacy of GSDA in learning
sex-specific models from functional networks, achieving a significant
improvement in group specificity over baseline methods. The major sex
differences are in the strength of lateralization and the interactions within
and between lobes. The GSDA-based method is generic in nature and can be
adapted to other group-specific analyses such as handedness-specific or
disease-specific analyses.
\\ ( https://arxiv.org/abs/2404.05781 ,  3644kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05816 (*cross-listing*)
Date: Mon, 8 Apr 2024 18:40:25 GMT   (333kb,D)

Title: Centrality Estimators for Probability Density Functions
Authors: Djemel Ziou
Categories: math.ST cs.LG stat.TH
\\
  In this report, we explore the data selection leading to a family of
estimators maximizing a centrality. The family allows a nice properties leading
to accurate and robust probability density function fitting according to some
criteria we define. We establish a link between the centrality estimator and
the maximum likelihood, showing that the latter is a particular case.
Therefore, a new probability interpretation of Fisher maximum likelihood is
provided. We will introduce and study two specific centralities that we have
named H\"older and Lehmer estimators. A numerical simulation is provided
showing the effectiveness of the proposed families of estimators opening the
door to development of new concepts and algorithms in machine learning, data
mining, statistics, and data analysis.
\\ ( https://arxiv.org/abs/2404.05816 ,  333kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05819 (*cross-listing*)
Date: Mon, 8 Apr 2024 18:55:07 GMT   (86kb,D)

Title: Just Wing It: Optimal Estimation of Missing Mass in a Markovian Sequence
Authors: Ashwin Pananjady, Vidya Muthukumar, Andrew Thangaraj
Categories: stat.ML cs.IT cs.LG math.IT math.PR math.ST stat.TH
Comments: 40 pages, 5 figures
\\
  We study the problem of estimating the stationary mass -- also called the
unigram mass -- that is missing from a single trajectory of a discrete-time,
ergodic Markov chain. This problem has several applications -- for example,
estimating the stationary missing mass is critical for accurately smoothing
probability estimates in sequence models. While the classical Good--Turing
estimator from the 1950s has appealing properties for i.i.d. data, it is known
to be biased in the Markov setting, and other heuristic estimators do not come
equipped with guarantees. Operating in the general setting in which the size of
the state space may be much larger than the length $n$ of the trajectory, we
develop a linear-runtime estimator called \emph{Windowed Good--Turing}
(\textsc{WingIt}) and show that its risk decays as
$\widetilde{\mathcal{O}}(\mathsf{T_{mix}}/n)$, where $\mathsf{T_{mix}}$ denotes
the mixing time of the chain in total variation distance. Notably, this rate is
independent of the size of the state space and minimax-optimal up to a
logarithmic factor in $n / \mathsf{T_{mix}}$. We also present a bound on the
variance of the missing mass random variable, which may be of independent
interest. We extend our estimator to approximate the stationary mass placed on
elements occurring with small frequency in $X^n$. Finally, we demonstrate the
efficacy of our estimators both in simulations on canonical chains and on
sequences constructed from a popular natural language corpus.
\\ ( https://arxiv.org/abs/2404.05819 ,  86kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05824 (*cross-listing*)
Date: Mon, 8 Apr 2024 19:23:17 GMT   (522kb,D)

Title: Quantum Adversarial Learning for Kernel Methods
Authors: Giuseppe Montalbano, Leonardo Banchi
Categories: quant-ph cs.CR cs.LG
\\
  We show that hybrid quantum classifiers based on quantum kernel methods and
support vector machines are vulnerable against adversarial attacks, namely
small engineered perturbations of the input data can deceive the classifier
into predicting the wrong result. Nonetheless, we also show that simple defence
strategies based on data augmentation with a few crafted perturbations can make
the classifier robust against new attacks. Our results find applications in
security-critical learning problems and in mitigating the effect of some forms
of quantum noise, since the attacker can also be understood as part of the
surrounding environment.
\\ ( https://arxiv.org/abs/2404.05824 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05835 (*cross-listing*)
Date: Mon, 8 Apr 2024 20:02:19 GMT   (323kb,D)

Title: Parameter-Adaptive Approximate MPC: Tuning Neural-Network Controllers
  without Re-Training
Authors: Henrik Hose and Alexander Gr\"afe and Sebastian Trimpe
Categories: eess.SY cs.LG cs.SY math.OC
Comments: Accepted to L4DC 2024
\\
  Model Predictive Control (MPC) is a method to control nonlinear systems with
guaranteed stability and constraint satisfaction but suffers from high
computation times. Approximate MPC (AMPC) with neural networks (NNs) has
emerged to address this limitation, enabling deployment on resource-constrained
embedded systems. However, when tuning AMPCs for real-world systems, large
datasets need to be regenerated and the NN needs to be retrained at every
tuning step. This work introduces a novel, parameter-adaptive AMPC architecture
capable of online tuning without recomputing large datasets and retraining. By
incorporating local sensitivities of nonlinear programs, the proposed method
not only mimics optimal MPC inputs but also adjusts to changes in physical
parameters of the model using linear predictions while still guaranteeing
stability. We showcase the effectiveness of parameter-adaptive AMPC by
controlling the swing-ups of two different real cartpole systems with a
severely resource-constrained microcontroller (MCU). We use the same NN across
both system instances that have different parameters. This work not only
represents the first experimental demonstration of AMPC for fast-moving systems
on low-cost MCUs to the best of our knowledge, but also showcases
generalization across system instances and variations through our
parameter-adaptation method. Taken together, these contributions represent a
marked step toward the practical application of AMPC in real-world systems.
\\ ( https://arxiv.org/abs/2404.05835 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05849 (*cross-listing*)
Date: Mon, 8 Apr 2024 20:31:27 GMT   (4346kb,D)

Title: Localizing Moments of Actions in Untrimmed Videos of Infants with Autism
  Spectrum Disorder
Authors: Halil Ismail Helvaci, Sen-ching Samson Cheung, Chen-Nee Chuah, Sally
  Ozonoff
Categories: cs.CV cs.LG
Comments: 7 pages, 2 figures, 3 tables
\\
  Autism Spectrum Disorder (ASD) presents significant challenges in early
diagnosis and intervention, impacting children and their families. With
prevalence rates rising, there is a critical need for accessible and efficient
screening tools. Leveraging machine learning (ML) techniques, in particular
Temporal Action Localization (TAL), holds promise for automating ASD screening.
This paper introduces a self-attention based TAL model designed to identify
ASD-related behaviors in infant videos. Unlike existing methods, our approach
simplifies complex modeling and emphasizes efficiency, which is essential for
practical deployment in real-world scenarios. Importantly, this work
underscores the importance of developing computer vision methods capable of
operating in naturilistic environments with little equipment control,
addressing key challenges in ASD screening. This study is the first to conduct
end-to-end temporal action localization in untrimmed videos of infants with
ASD, offering promising avenues for early intervention and support. We report
baseline results of behavior detection using our TAL model. We achieve 70%
accuracy for look face, 79% accuracy for look object, 72% for smile and 65% for
vocalization.
\\ ( https://arxiv.org/abs/2404.05849 ,  4346kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05858 (*cross-listing*)
Date: Mon, 8 Apr 2024 20:42:10 GMT   (8243kb,D)

Title: A Neuromorphic Approach to Obstacle Avoidance in Robot Manipulation
Authors: Ahmed Faisal Abdelrahman and Matias Valdenegro-Toro and Maren
  Bennewitz and Paul G. Pl\"oger
Categories: cs.RO cs.LG cs.NE
Comments: 35 pages, accepted at IJRR, authors' version
\\
  Neuromorphic computing mimics computational principles of the brain in
$\textit{silico}$ and motivates research into event-based vision and spiking
neural networks (SNNs). Event cameras (ECs) exclusively capture local intensity
changes and offer superior power consumption, response latencies, and dynamic
ranges. SNNs replicate biological neuronal dynamics and have demonstrated
potential as alternatives to conventional artificial neural networks (ANNs),
such as in reducing energy expenditure and inference time in visual
classification. Nevertheless, these novel paradigms remain scarcely explored
outside the domain of aerial robots.
  To investigate the utility of brain-inspired sensing and data processing, we
developed a neuromorphic approach to obstacle avoidance on a camera-equipped
manipulator. Our approach adapts high-level trajectory plans with reactive
maneuvers by processing emulated event data in a convolutional SNN, decoding
neural activations into avoidance motions, and adjusting plans using a dynamic
motion primitive. We conducted experiments with a Kinova Gen3 arm performing
simple reaching tasks that involve obstacles in sets of distinct task scenarios
and in comparison to a non-adaptive baseline.
  Our neuromorphic approach facilitated reliable avoidance of imminent
collisions in simulated and real-world experiments, where the baseline
consistently failed. Trajectory adaptations had low impacts on safety and
predictability criteria. Among the notable SNN properties were the correlation
of computations with the magnitude of perceived motions and a robustness to
different event emulation methods. Tests with a DAVIS346 EC showed similar
performance, validating our experimental event emulation. Our results motivate
incorporating SNN learning, utilizing neuromorphic processors, and further
exploring the potential of neuromorphic methods.
\\ ( https://arxiv.org/abs/2404.05858 ,  8243kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05872 (*cross-listing*)
Date: Mon, 8 Apr 2024 21:09:59 GMT   (2380kb,D)

Title: TabConv: Low-Computation CNN Inference via Table Lookups
Authors: Neelesh Gupta, Narayanan Kannan, Pengmiao Zhang, Viktor Prasanna
Categories: cs.CV cs.LG cs.NE
Comments: 8 pages, Accepted at CF '24
ACM-class: I.5.1
DOI: 10.1145/3649153.3649212
\\
  Convolutional Neural Networks (CNNs) have demonstrated remarkable ability
throughout the field of computer vision. However, CNN inference requires a
large number of arithmetic operations, making them expensive to deploy in
hardware. Current approaches alleviate this issue by developing
hardware-supported, algorithmic processes to simplify spatial convolution
functions. However, these methods still heavily rely on matrix multiplication,
leading to significant computational overhead. To bridge the gap between
hardware, algorithmic acceleration, and approximate matrix multiplication, we
propose TabConv, a novel, table-based approximation for convolution to
significantly reduce arithmetic operations during inference. Additionally, we
introduce a priority masking technique based on cosine similarity to select
layers for table-based approximation, thereby maintaining the model
performance. We evaluate our approach on popular CNNs: ResNet-18, ResNet-34,
and NetworkInNetwork (NIN). TabConv preserves over 93% of the original model's
performance while reducing arithmetic operations by 36.5%, 25.8%, and 99.4% for
ResNet-18 on CIFAR-10, CIFAR-100, and MNIST, respectively, 35.6% and 99.3% for
ResNet-34 on CIFAR-10 and MNIST, and 98.9% for NIN on MNIST, achieving
low-computation inference.
\\ ( https://arxiv.org/abs/2404.05872 ,  2380kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05898 (*cross-listing*)
Date: Mon, 8 Apr 2024 22:54:14 GMT   (522kb,D)

Title: Inexact Simplification of Symbolic Regression Expressions with
  Locality-sensitive Hashing
Authors: Guilherme Seidyo Imai Aldeia (1), Fabricio Olivetti de Franca (1) and
  William G. La Cava (2 and 3) ((1) Federal University of ABC, (2) Boston
  Children's Hospital, (3) Harvard Medical School)
Categories: cs.NE cs.LG
Comments: 9 pages, 10 figures, accepted to GECCO-24
Journal-ref: GSI Aldeia, FO de Fran\c{c}a, WG La Cava. 2024. Inexact
  Simplification of Symbolic Regression Expressions with Locality-sensitive
  Hashing. In Genetic and Evolutionary Computation Conference (GECCO '24)
DOI: 10.1145/3638529.3654147
\\
  Symbolic regression (SR) searches for parametric models that accurately fit a
dataset, prioritizing simplicity and interpretability. Despite this secondary
objective, studies point out that the models are often overly complex due to
redundant operations, introns, and bloat that arise during the iterative
process, and can hinder the search with repeated exploration of bloated
segments. Applying a fast heuristic algebraic simplification may not fully
simplify the expression and exact methods can be infeasible depending on size
or complexity of the expressions. We propose a novel agnostic simplification
and bloat control for SR employing an efficient memoization with
locality-sensitive hashing (LHS). The idea is that expressions and their
sub-expressions traversed during the iterative simplification process are
stored in a dictionary using LHS, enabling efficient retrieval of similar
structures. We iterate through the expression, replacing subtrees with others
of same hash if they result in a smaller expression. Empirical results shows
that applying this simplification during evolution performs equal or better
than without simplification in minimization of error, significantly reducing
the number of nonlinear functions. This technique can learn simplification
rules that work in general or for a specific problem, and improves convergence
while reducing model complexity.
\\ ( https://arxiv.org/abs/2404.05898 ,  522kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05905 (*cross-listing*)
Date: Mon, 8 Apr 2024 23:30:15 GMT   (2737kb,D)

Title: Computing Transition Pathways for the Study of Rare Events Using Deep
  Reinforcement Learning
Authors: Bo Lin, Yangzheng Zhong and Weiqing Ren
Categories: physics.comp-ph cs.LG cs.NA math.NA stat.ML
\\
  Understanding the transition events between metastable states in complex
systems is an important subject in the fields of computational physics,
chemistry and biology. The transition pathway plays an important role in
characterizing the mechanism underlying the transition, for example, in the
study of conformational changes of bio-molecules. In fact, computing the
transition pathway is a challenging task for complex and high-dimensional
systems. In this work, we formulate the path-finding task as a cost
minimization problem over a particular path space. The cost function is adapted
from the Freidlin-Wentzell action functional so that it is able to deal with
rough potential landscapes. The path-finding problem is then solved using a
actor-critic method based on the deep deterministic policy gradient algorithm
(DDPG). The method incorporates the potential force of the system in the policy
for generating episodes and combines physical properties of the system with the
learning process for molecular systems. The exploitation and exploration nature
of reinforcement learning enables the method to efficiently sample the
transition events and compute the globally optimal transition pathway. We
illustrate the effectiveness of the proposed method using three benchmark
systems including an extended Mueller system and the Lennard-Jones system of
seven particles.
\\ ( https://arxiv.org/abs/2404.05905 ,  2737kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05985 (*cross-listing*)
Date: Tue, 9 Apr 2024 03:36:39 GMT   (846kb)

Title: Boosting Digital Safeguards: Blending Cryptography and Steganography
Authors: Anamitra Maiti, Subham Laha, Rishav Upadhaya, Soumyajit Biswas, Vikas
  Choudhary, Biplab Kar, Nikhil Kumar, Jaydip Sen
Categories: cs.CR cs.LG
Comments: This report pertains to the Capstone Project done by Group 3 of the
  Fall batch of 2023 students at Praxis Tech School, Kolkata, India. The
  reports consists of 36 pages and it includes 11 figures and 5 tables
\\
  In today's digital age, the internet is essential for communication and the
sharing of information, creating a critical need for sophisticated data
security measures to prevent unauthorized access and exploitation. Cryptography
encrypts messages into a cipher text that is incomprehensible to unauthorized
readers, thus safeguarding data during its transmission. Steganography, on the
other hand, originates from the Greek term for "covered writing" and involves
the art of hiding data within another medium, thereby facilitating covert
communication by making the message invisible. This proposed approach takes
advantage of the latest advancements in Artificial Intelligence (AI) and Deep
Learning (DL), especially through the application of Generative Adversarial
Networks (GANs), to improve upon traditional steganographic methods. By
embedding encrypted data within another medium, our method ensures that the
communication remains hidden from prying eyes. The application of GANs enables
a smart, secure system that utilizes the inherent sensitivity of neural
networks to slight alterations in data, enhancing the protection against
detection. By merging the encryption techniques of cryptography with the hiding
capabilities of steganography, and augmenting these with the strengths of AI,
we introduce a comprehensive security system designed to maintain both the
privacy and integrity of information. This system is crafted not just to
prevent unauthorized access or modification of data, but also to keep the
existence of the data hidden. This fusion of technologies tackles the core
challenges of data security in the current era of open digital communication,
presenting an advanced solution with the potential to transform the landscape
of information security.
\\ ( https://arxiv.org/abs/2404.05985 ,  846kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06023 (*cross-listing*)
Date: Tue, 9 Apr 2024 05:12:44 GMT   (221kb,D)

Title: Prelimit Coupling and Steady-State Convergence of Constant-stepsize
  Nonsmooth Contractive SA
Authors: Yixuan Zhang, Dongyan Huo, Yudong Chen, Qiaomin Xie
Categories: stat.ML cs.LG math.OC math.PR
Comments: 71 pages, 3 figures
\\
  Motivated by Q-learning, we study nonsmooth contractive stochastic
approximation (SA) with constant stepsize. We focus on two important classes of
dynamics: 1) nonsmooth contractive SA with additive noise, and 2) synchronous
and asynchronous Q-learning, which features both additive and multiplicative
noise. For both dynamics, we establish weak convergence of the iterates to a
stationary limit distribution in Wasserstein distance. Furthermore, we propose
a prelimit coupling technique for establishing steady-state convergence and
characterize the limit of the stationary distribution as the stepsize goes to
zero. Using this result, we derive that the asymptotic bias of nonsmooth SA is
proportional to the square root of the stepsize, which stands in sharp contrast
to smooth SA. This bias characterization allows for the use of
Richardson-Romberg extrapolation for bias reduction in nonsmooth SA.
\\ ( https://arxiv.org/abs/2404.06023 ,  221kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06104 (*cross-listing*)
Date: Tue, 9 Apr 2024 08:11:46 GMT   (1640kb,D)

Title: A singular Riemannian Geometry Approach to Deep Neural Networks III.
  Piecewise Differentiable Layers and Random Walks on $n$-dimensional Classes
Authors: Alessandro Benfenati, Alessio Marta
Categories: math.DG cs.LG
\\
  Neural networks are playing a crucial role in everyday life, with the most
modern generative models able to achieve impressive results. Nonetheless, their
functioning is still not very clear, and several strategies have been adopted
to study how and why these model reach their outputs. A common approach is to
consider the data in an Euclidean settings: recent years has witnessed instead
a shift from this paradigm, moving thus to more general framework, namely
Riemannian Geometry. Two recent works introduced a geometric framework to study
neural networks making use of singular Riemannian metrics. In this paper we
extend these results to convolutional, residual and recursive neural networks,
studying also the case of non-differentiable activation functions, such as
ReLU. We illustrate our findings with some numerical experiments on
classification of images and thermodynamic problems.
\\ ( https://arxiv.org/abs/2404.06104 ,  1640kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06125 (*cross-listing*)
Date: Tue, 9 Apr 2024 08:49:41 GMT   (1610kb,D)

Title: Learning Model Predictive Control Parameters via Bayesian Optimization
  for Battery Fast Charging
Authors: Sebastian Hirt, Andreas H\"ohl, Joachim Schaeffer, Johannes Pohlodek,
  Richard D. Braatz, Rolf Findeisen
Categories: eess.SY cs.LG cs.SY
Comments: 6 pages, 5 figures, accepted for ADCHEM 2024
\\
  Tuning parameters in model predictive control (MPC) presents significant
challenges, particularly when there is a notable discrepancy between the
controller's predictions and the actual behavior of the closed-loop plant. This
mismatch may stem from factors like substantial model-plant differences,
limited prediction horizons that do not cover the entire time of interest, or
unforeseen system disturbances. Such mismatches can jeopardize both performance
and safety, including constraint satisfaction. Traditional methods address this
issue by modifying the finite horizon cost function to better reflect the
overall operational cost, learning parts of the prediction model from data, or
implementing robust MPC strategies, which might be either computationally
intensive or overly cautious. As an alternative, directly optimizing or
learning the controller parameters to enhance closed-loop performance has been
proposed. We apply Bayesian optimization for efficient learning of unknown
model parameters and parameterized constraint backoff terms, aiming to improve
closed-loop performance of battery fast charging. This approach establishes a
hierarchical control framework where Bayesian optimization directly fine-tunes
closed-loop behavior towards a global and long-term objective, while MPC
handles lower-level, short-term control tasks. For lithium-ion battery fast
charging, we show that the learning approach not only ensures safe operation
but also maximizes closed-loop performance. This includes maintaining the
battery's operation below its maximum terminal voltage and reducing charging
times, all achieved using a standard nominal MPC model with a short horizon and
notable initial model-plant mismatch.
\\ ( https://arxiv.org/abs/2404.06125 ,  1610kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06129 (*cross-listing*)
Date: Tue, 9 Apr 2024 08:56:43 GMT   (10717kb,D)

Title: Adaptable Recovery Behaviors in Robotics: A Behavior Trees and Motion
  Generators(BTMG) Approach for Failure Management
Authors: Faseeh Ahmad, Matthias Mayr, Sulthan Suresh-Fazeela and Volker Kreuger
Categories: cs.RO cs.LG
\\
  In dynamic operational environments, particularly in collaborative robotics,
the inevitability of failures necessitates robust and adaptable recovery
strategies. Traditional automated recovery strategies, while effective for
predefined scenarios, often lack the flexibility required for on-the-fly task
management and adaptation to expected failures. Addressing this gap, we propose
a novel approach that models recovery behaviors as adaptable robotic skills,
leveraging the Behavior Trees and Motion Generators~(BTMG) framework for policy
representation. This approach distinguishes itself by employing reinforcement
learning~(RL) to dynamically refine recovery behavior parameters, enabling a
tailored response to a wide array of failure scenarios with minimal human
intervention. We assess our methodology through a series of progressively
challenging scenarios within a peg-in-a-hole task, demonstrating the approach's
effectiveness in enhancing operational efficiency and task success rates in
collaborative robotics settings. We validate our approach using a dual-arm KUKA
robot.
\\ ( https://arxiv.org/abs/2404.06129 ,  10717kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06206 (*cross-listing*)
Date: Tue, 9 Apr 2024 10:53:29 GMT   (6976kb,D)

Title: Deep Learning Method for Computing Committor Functions with Adaptive
  Sampling
Authors: Bo Lin and Weiqing Ren
Categories: physics.comp-ph cs.LG cs.NA math.NA
\\
  The committor function is a central object for quantifying the transitions
between metastable states of dynamical systems. Recently, a number of
computational methods based on deep neural networks have been developed for
computing the high-dimensional committor function. The success of the methods
relies on sampling adequate data for the transition, which still is a
challenging task for complex systems at low temperatures. In this work, we
propose a deep learning method with two novel adaptive sampling schemes (I and
II). In the two schemes, the data are generated actively with a modified
potential where the bias potential is constructed from the learned committor
function. We theoretically demonstrate the advantages of the sampling schemes
and show that the data in sampling scheme II are uniformly distributed along
the transition tube. This makes a promising method for studying the transition
of complex systems. The efficiency of the method is illustrated in
high-dimensional systems including the alanine dipeptide and a solvated dimer
system.
\\ ( https://arxiv.org/abs/2404.06206 ,  6976kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06225 (*cross-listing*)
Date: Tue, 9 Apr 2024 11:27:07 GMT   (7783kb,D)

Title: Message Passing Variational Autoregressive Network for Solving
  Intractable Ising Models
Authors: Qunlong Ma, Zhi Ma, Jinlong Xu, Hairui Zhang, Ming Gao
Categories: cond-mat.stat-mech cond-mat.dis-nn cs.LG
Comments: 18 pages, 14 figures
\\
  Many deep neural networks have been used to solve Ising models, including
autoregressive neural networks, convolutional neural networks, recurrent neural
networks, and graph neural networks. Learning a probability distribution of
energy configuration or finding the ground states of a disordered, fully
connected Ising model is essential for statistical mechanics and NP-hard
problems. Despite tremendous efforts, a neural network architecture with the
ability to high-accurately solve these fully connected and extremely
intractable problems on larger systems is still lacking. Here we propose a
variational autoregressive architecture with a message passing mechanism, which
can effectively utilize the interactions between spin variables. The new
network trained under an annealing framework outperforms existing methods in
solving several prototypical Ising spin Hamiltonians, especially for larger
spin systems at low temperatures. The advantages also come from the great
mitigation of mode collapse during the training process of deep neural
networks. Considering these extremely difficult problems to be solved, our
method extends the current computational limits of unsupervised neural networks
to solve combinatorial optimization problems.
\\ ( https://arxiv.org/abs/2404.06225 ,  7783kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06236 (*cross-listing*)
Date: Tue, 9 Apr 2024 11:56:29 GMT   (321kb,D)

Title: Towards Robust Domain Generation Algorithm Classification
Authors: Arthur Drichel, Marc Meyer, Ulrike Meyer
Categories: cs.CR cs.LG
Comments: Accepted at ACM Asia Conference on Computer and Communications
  Security (ASIA CCS 2024)
Journal-ref: ACM Asia Conference on Computer and Communications Security (ASIA
  CCS 2024)
DOI: 10.1145/3634737.3656287
\\
  In this work, we conduct a comprehensive study on the robustness of domain
generation algorithm (DGA) classifiers. We implement 32 white-box attacks, 19
of which are very effective and induce a false-negative rate (FNR) of $\approx$
100\% on unhardened classifiers. To defend the classifiers, we evaluate
different hardening approaches and propose a novel training scheme that
leverages adversarial latent space vectors and discretized adversarial domains
to significantly improve robustness. In our study, we highlight a pitfall to
avoid when hardening classifiers and uncover training biases that can be easily
exploited by attackers to bypass detection, but which can be mitigated by
adversarial training (AT). In our study, we do not observe any trade-off
between robustness and performance, on the contrary, hardening improves a
classifier's detection performance for known and unknown DGAs. We implement all
attacks and defenses discussed in this paper as a standalone library, which we
make publicly available to facilitate hardening of DGA classifiers:
https://gitlab.com/rwth-itsec/robust-dga-detection
\\ ( https://arxiv.org/abs/2404.06236 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06282 (*cross-listing*)
Date: Tue, 9 Apr 2024 13:08:28 GMT   (44kb)

Title: Simple algorithms to test and learn local Hamiltonians
Authors: Francisco Escudero Guti\'errez
Categories: quant-ph cs.CC cs.DS cs.IT cs.LG math.IT
Comments: 7 pages
\\
  We consider the problems of testing and learning an $n$-qubit $k$-local
Hamiltonian from queries to its evolution operator with respect the 2-norm of
the Pauli spectrum, or equivalently, the normalized Frobenius norm. For testing
whether a Hamiltonian is $\epsilon_1$-close to $k$-local or $\epsilon_2$-far
from $k$-local, we show that $O(1/(\epsilon_2-\epsilon_1)^{8})$ queries
suffice. This solves two questions posed in a recent work by Bluhm, Caro and
Oufkir. For learning up to error $\epsilon$, we show that
$\exp(O(k^2+k\log(1/\epsilon)))$ queries suffice. Our proofs are simple,
concise and based on Pauli-analytic techniques.
\\ ( https://arxiv.org/abs/2404.06282 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06287 (*cross-listing*)
Date: Tue, 9 Apr 2024 13:13:24 GMT   (657kb,D)

Title: Counterfactual Reasoning for Multi-Label Image Classification via
  Patching-Based Training
Authors: Ming-Kun Xie, Jia-Hao Xiao, Pei Peng, Gang Niu, Masashi Sugiyama,
  Sheng-Jun Huang
Categories: cs.CV cs.LG
\\
  The key to multi-label image classification (MLC) is to improve model
performance by leveraging label correlations. Unfortunately, it has been shown
that overemphasizing co-occurrence relationships can cause the overfitting
issue of the model, ultimately leading to performance degradation. In this
paper, we provide a causal inference framework to show that the correlative
features caused by the target object and its co-occurring objects can be
regarded as a mediator, which has both positive and negative impacts on model
predictions. On the positive side, the mediator enhances the recognition
performance of the model by capturing co-occurrence relationships; on the
negative side, it has the harmful causal effect that causes the model to make
an incorrect prediction for the target object, even when only co-occurring
objects are present in an image. To address this problem, we propose a
counterfactual reasoning method to measure the total direct effect, achieved by
enhancing the direct effect caused only by the target object. Due to the
unknown location of the target object, we propose patching-based training and
inference to accomplish this goal, which divides an image into multiple patches
and identifies the pivot patch that contains the target object. Experimental
results on multiple benchmark datasets with diverse configurations validate
that the proposed method can achieve state-of-the-art performance.
\\ ( https://arxiv.org/abs/2404.06287 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06294 (*cross-listing*)
Date: Tue, 9 Apr 2024 13:19:43 GMT   (14198kb,D)

Title: Fortifying Fully Convolutional Generative Adversarial Networks for Image
  Super-Resolution Using Divergence Measures
Authors: Arkaprabha Basu, Kushal Bose, Sankha Subhra Mullick, Anish
  Chakrabarty, and Swagatam Das
Categories: eess.IV cs.CV cs.LG
\\
  Super-Resolution (SR) is a time-hallowed image processing problem that aims
to improve the quality of a Low-Resolution (LR) sample up to the standard of
its High-Resolution (HR) counterpart. We aim to address this by introducing
Super-Resolution Generator (SuRGe), a fully-convolutional Generative
Adversarial Network (GAN)-based architecture for SR. We show that distinct
convolutional features obtained at increasing depths of a GAN generator can be
optimally combined by a set of learnable convex weights to improve the quality
of generated SR samples. In the process, we employ the Jensen-Shannon and the
Gromov-Wasserstein losses respectively between the SR-HR and LR-SR pairs of
distributions to further aid the generator of SuRGe to better exploit the
available information in an attempt to improve SR. Moreover, we train the
discriminator of SuRGe with the Wasserstein loss with gradient penalty, to
primarily prevent mode collapse. The proposed SuRGe, as an end-to-end GAN
workflow tailor-made for super-resolution, offers improved performance while
maintaining low inference time. The efficacy of SuRGe is substantiated by its
superior performance compared to 18 state-of-the-art contenders on 10 benchmark
datasets.
\\ ( https://arxiv.org/abs/2404.06294 ,  14198kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06314 (*cross-listing*)
Date: Tue, 9 Apr 2024 13:48:53 GMT   (94kb)

Title: Qiskit-Torch-Module: Fast Prototyping of Quantum Neural Networks
Authors: Nico Meyer, Christian Ufrecht, Maniraman Periyasamy, Axel Plinge,
  Christopher Mutschler, Daniel D. Scherer, and Andreas Maier
Categories: quant-ph cs.LG cs.SE
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible. 7 pages, 4 figures, 3 tables
\\
  Quantum computer simulation software is an integral tool for the research
efforts in the quantum computing community. An important aspect is the
efficiency of respective frameworks, especially for training variational
quantum algorithms. Focusing on the widely used Qiskit software environment, we
develop the qiskit-torch-module. It improves runtime performance by two orders
of magnitude over comparable libraries, while facilitating low-overhead
integration with existing codebases. Moreover, the framework provides advanced
tools for integrating quantum neural networks with PyTorch. The pipeline is
tailored for single-machine compute systems, which constitute a widely employed
setup in day-to-day research efforts.
\\ ( https://arxiv.org/abs/2404.06314 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06336 (*cross-listing*)
Date: Tue, 9 Apr 2024 14:21:51 GMT   (2930kb,D)

Title: Quantum State Generation with Structure-Preserving Diffusion Model
Authors: Yuchen Zhu, Tianrong Chen, Evangelos A. Theodorou, Xie Chen, Molei Tao
Categories: quant-ph cs.LG stat.ML
Comments: 15 pages, 6 figures
\\
  This article considers the generative modeling of the states of quantum
systems, and an approach based on denoising diffusion model is proposed. The
key contribution is an algorithmic innovation that respects the physical nature
of quantum states. More precisely, the commonly used density matrix
representation of mixed-state has to be complex-valued Hermitian, positive
semi-definite, and trace one. Generic diffusion models, or other generative
methods, may not be able to generate data that strictly satisfy these
structural constraints, even if all training data do. To develop a machine
learning algorithm that has physics hard-wired in, we leverage the recent
development of Mirror Diffusion Model and design a previously unconsidered
mirror map, to enable strict structure-preserving generation. Both
unconditional generation and conditional generation via classifier-free
guidance are experimentally demonstrated efficacious, the latter even enabling
the design of new quantum states when generated on unseen labels.
\\ ( https://arxiv.org/abs/2404.06336 ,  2930kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06437 (*cross-listing*)
Date: Tue, 9 Apr 2024 16:28:54 GMT   (316kb,D)

Title: Seasonal Fire Prediction using Spatio-Temporal Deep Neural Networks
Authors: Dimitrios Michail and Lefki-Ioanna Panagiotou and Charalampos Davalas
  and Ioannis Prapas and Spyros Kondylatos and Nikolaos Ioannis Bountos and
  Ioannis Papoutsis
Categories: cs.CV cs.LG
\\
  With climate change expected to exacerbate fire weather conditions, the
accurate anticipation of wildfires on a global scale becomes increasingly
crucial for disaster mitigation. In this study, we utilize SeasFire, a
comprehensive global wildfire dataset with climate, vegetation, oceanic
indices, and human-related variables, to enable seasonal wildfire forecasting
with machine learning. For the predictive analysis, we train deep learning
models with different architectures that capture the spatio-temporal context
leading to wildfires. Our investigation focuses on assessing the effectiveness
of these models in predicting the presence of burned areas at varying
forecasting time horizons globally, extending up to six months into the future,
and on how different spatial or/and temporal context affects the performance of
the models. Our findings demonstrate the great potential of deep learning
models in seasonal fire forecasting; longer input time-series leads to more
robust predictions across varying forecasting horizons, while integrating
spatial information to capture wildfire spatio-temporal dynamics boosts
performance. Finally, our results hint that in order to enhance performance at
longer forecasting horizons, a larger receptive field spatially needs to be
considered.
\\ ( https://arxiv.org/abs/2404.06437 ,  316kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06455 (*cross-listing*)
Date: Tue, 9 Apr 2024 16:55:23 GMT   (1785kb,D)

Title: A comparative analysis of deep learning models for lung segmentation on
  X-ray images
Authors: Weronika Hryniewska-Guzik, Jakub Bilski, Bartosz Chrostowski, Jakub
  Drak Sbahi, Przemys{\l}aw Biecek
Categories: eess.IV cs.CV cs.LG
Comments: published at the Polish Conference on Artificial Intelligence
  (PP-RAI), 2024
\\
  Robust and highly accurate lung segmentation in X-rays is crucial in medical
imaging. This study evaluates deep learning solutions for this task, ranking
existing methods and analyzing their performance under diverse image
modifications. Out of 61 analyzed papers, only nine offered implementation or
pre-trained models, enabling assessment of three prominent methods: Lung VAE,
TransResUNet, and CE-Net. The analysis revealed that CE-Net performs best,
demonstrating the highest values in dice similarity coefficient and
intersection over union metric.
\\ ( https://arxiv.org/abs/2404.06455 ,  1785kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06460 (*cross-listing*)
Date: Tue, 9 Apr 2024 17:00:43 GMT   (18507kb,D)

Title: Learning Locally Interacting Discrete Dynamical Systems: Towards
  Data-Efficient and Scalable Prediction
Authors: Beomseok Kang, Harshit Kumar, Minah Lee, Biswadeep Chakraborty, and
  Saibal Mukhopadhyay
Categories: eess.SY cs.LG cs.SY
Comments: Accepted in Learning for Dynamics and Control Conference (L4DC) 2024
\\
  Locally interacting dynamical systems, such as epidemic spread, rumor
propagation through crowd, and forest fire, exhibit complex global dynamics
originated from local, relatively simple, and often stochastic interactions
between dynamic elements. Their temporal evolution is often driven by
transitions between a finite number of discrete states. Despite significant
advancements in predictive modeling through deep learning, such interactions
among many elements have rarely explored as a specific domain for predictive
modeling. We present Attentive Recurrent Neural Cellular Automata (AR-NCA), to
effectively discover unknown local state transition rules by associating the
temporal information between neighboring cells in a permutation-invariant
manner. AR-NCA exhibits the superior generalizability across various system
configurations (i.e., spatial distribution of states), data efficiency and
robustness in extremely data-limited scenarios even in the presence of
stochastic interactions, and scalability through spatial dimension-independent
prediction.
\\ ( https://arxiv.org/abs/2404.06460 ,  18507kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06470 (*cross-listing*)
Date: Tue, 9 Apr 2024 17:17:48 GMT   (19412kb,D)

Title: Learning State-Invariant Representations of Objects from Image
  Collections with State, Pose, and Viewpoint Changes
Authors: Rohan Sarkar, Avinash Kak
Categories: cs.CV cs.IR cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\
  We add one more invariance - state invariance - to the more commonly used
other invariances for learning object representations for recognition and
retrieval. By state invariance, we mean robust with respect to changes in the
structural form of the object, such as when an umbrella is folded, or when an
item of clothing is tossed on the floor. Since humans generally have no
difficulty in recognizing objects despite such state changes, we are naturally
faced with the question of whether it is possible to devise a neural
architecture with similar abilities. To that end, we present a novel dataset,
ObjectsWithStateChange, that captures state and pose variations in the object
images recorded from arbitrary viewpoints. We believe that this dataset will
facilitate research in fine-grained object recognition and retrieval of objects
that are capable of state changes. The goal of such research would be to train
models capable of generating object embeddings that remain invariant to state
changes while also staying invariant to transformations induced by changes in
viewpoint, pose, illumination, etc. To demonstrate the usefulness of the
ObjectsWithStateChange dataset, we also propose a curriculum learning strategy
that uses the similarity relationships in the learned embedding space after
each epoch to guide the training process. The model learns discriminative
features by comparing visually similar objects within and across different
categories, encouraging it to differentiate between objects that may be
challenging to distinguish due to changes in their state. We believe that this
strategy enhances the model's ability to capture discriminative features for
fine-grained tasks that may involve objects with state changes, leading to
performance improvements on object-level tasks not only on our new dataset, but
also on two other challenging multi-view datasets such as ModelNet40 and
ObjectPI.
\\ ( https://arxiv.org/abs/2404.06470 ,  19412kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06481 (*cross-listing*)
Date: Tue, 9 Apr 2024 17:31:18 GMT   (9244kb,D)

Title: GeoDirDock: Guiding Docking Along Geodesic Paths
Authors: Ra\'ul Mi\~n\'an, Javier Gallardo, \'Alvaro Ciudad, Alexis Molina
Categories: q-bio.BM cs.LG
Comments: Generative and Experimental Perspectives for Biomolecular Design
  Workshop at ICLR 2024
\\
  This work introduces GeoDirDock (GDD), a novel approach to molecular docking
that enhances the accuracy and physical plausibility of ligand docking
predictions. GDD guides the denoising process of a diffusion model along
geodesic paths within multiple spaces representing translational, rotational,
and torsional degrees of freedom. Our method leverages expert knowledge to
direct the generative modeling process, specifically targeting desired
protein-ligand interaction regions. We demonstrate that GDD significantly
outperforms existing blind docking methods in terms of RMSD accuracy and
physicochemical pose realism. Our results indicate that incorporating domain
expertise into the diffusion process leads to more biologically relevant
docking predictions. Additionally, we explore the potential of GDD for lead
optimization in drug discovery through angle transfer in maximal common
substructure (MCS) docking, showcasing its capability to predict ligand
orientations for chemically similar compounds accurately.
\\ ( https://arxiv.org/abs/2404.06481 ,  9244kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2307.12289
replaced with revised version Tue, 9 Apr 2024 07:19:29 GMT   (75kb)

Title: Controller Synthesis for Timeline-based Games
Authors: Renato Acampora and Luca Geatti and Nicola Gigante and Angelo
  Montanari and Valentino Picotti
Categories: cs.AI
Comments: arXiv admin note: substantial text overlap with arXiv:2209.10319 This
  is a submission to the LMCS journal of the journal version of 2209.10319
\\ ( https://arxiv.org/abs/2307.12289 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20327
replaced with revised version Tue, 9 Apr 2024 13:22:43 GMT   (431kb,D)

Title: Improving Entropy-Based Test-Time Adaptation from a Clustering View
Authors: Guoliang Lin, Hanjiang Lai, Yan Pan, Jian Yin
Categories: cs.AI
\\ ( https://arxiv.org/abs/2310.20327 ,  431kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02996
replaced with revised version Tue, 9 Apr 2024 09:22:30 GMT   (5405kb,D)

Title: Visual-information-driven model for crowd simulation using temporal
  convolutional network
Authors: Xuanwen Liang and Eric Wai Ming Lee
Categories: cs.AI cs.LG
Journal-ref: IEEE Transactions on Intelligent Transportation Systems
DOI: 10.1109/TITS.2024.3375528
\\ ( https://arxiv.org/abs/2311.02996 ,  5405kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05772
replaced with revised version Mon, 8 Apr 2024 20:42:17 GMT   (8556kb,D)

Title: ADaPT: As-Needed Decomposition and Planning with Language Models
Authors: Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark,
  Ashish Sabharwal, Mohit Bansal, Tushar Khot
Categories: cs.AI cs.CL cs.LG
Comments: NAACL 2024 (findings) camera-ready. Project Page:
  https://allenai.github.io/adaptllm
\\ ( https://arxiv.org/abs/2311.05772 ,  8556kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09919
replaced with revised version Tue, 9 Apr 2024 11:41:21 GMT   (2585kb,D)

Title: Road Graph Generator: Mapping roads at construction sites from GPS data
Authors: Katarzyna Micha{\l}owska, Helga Margrete Bodahl Holmestad, Signe
  Riemer-S{\o}rensen
Categories: cs.AI
Comments: 18 pages, 4 figures, 3 tables
\\ ( https://arxiv.org/abs/2402.09919 ,  2585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10877
replaced with revised version Tue, 9 Apr 2024 12:33:53 GMT   (2637kb,D)

Title: Robust agents learn causal world models
Authors: Jonathan Richens, Tom Everitt
Categories: cs.AI cs.LG
Comments: ICLR 2024 (oral). Proofs in appendix simplified
\\ ( https://arxiv.org/abs/2402.10877 ,  2637kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04471
replaced with revised version Tue, 9 Apr 2024 15:09:35 GMT   (734kb)

Title: The Shutdown Problem: An AI Engineering Puzzle for Decision Theorists
Authors: Elliott Thornley
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.04471 ,  734kb)
------------------------------------------------------------------------------
\\
arXiv:2210.13034
replaced with revised version Tue, 9 Apr 2024 09:33:31 GMT   (4977kb,D)

Title: Subspace Representations for Soft Set Operations and Sentence
  Similarities
Authors: Yoichi Ishibashi, Sho Yokoi, Katsuhito Sudoh, Satoshi Nakamura
Categories: cs.CL cs.LG
Comments: Accepted at NAACL 2024
\\ ( https://arxiv.org/abs/2210.13034 ,  4977kb)
------------------------------------------------------------------------------
\\
arXiv:2304.01982
replaced with revised version Mon, 8 Apr 2024 18:31:32 GMT   (1030kb,D)

Title: Rethinking the Role of Token Retrieval in Multi-Vector Retrieval
Authors: Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar
  Naim, Ming-Wei Chang, Vincent Y. Zhao
Categories: cs.CL cs.IR
Comments: NeurIPS 2023. Code available at
  https://github.com/google-deepmind/xtr
\\ ( https://arxiv.org/abs/2304.01982 ,  1030kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07412
replaced with revised version Tue, 9 Apr 2024 05:12:30 GMT   (57kb,D)

Title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks
Authors: Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky
Categories: cs.CL cs.LG
Comments: Accepted at the 2024 Annual Conference of the North American Chapter
  of the Association for Computational Linguistics (NAACL 2024). The first two
  authors contributed equally to this work
\\ ( https://arxiv.org/abs/2309.07412 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10931
replaced with revised version Tue, 9 Apr 2024 07:54:56 GMT   (870kb)

Title: A Family of Pretrained Transformer Language Models for Russian
Authors: Dmitry Zmitrovich, Alexander Abramov, Andrey Kalmykov, Maria
  Tikhonova, Ekaterina Taktasheva, Danil Astafurov, Mark Baushenko, Artem
  Snegirev, Vitalii Kadulin, Sergey Markov, Tatiana Shavrina, Vladislav
  Mikhailov, Alena Fenogenova
Categories: cs.CL
Comments: to appear in LREC-COLING-2024
\\ ( https://arxiv.org/abs/2309.10931 ,  870kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12934
replaced with revised version Tue, 9 Apr 2024 11:27:48 GMT   (2261kb,D)

Title: TOPFORMER: Topology-Aware Authorship Attribution of Deepfake Texts with
  Diverse Writing Styles
Authors: Adaku Uchendu, Thai Le, Dongwon Lee
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.12934 ,  2261kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01605
replaced with revised version Tue, 9 Apr 2024 10:52:13 GMT   (50kb,D)

Title: Faithful and Robust Local Interpretability for Textual Predictions
Authors: Gianluigi Lopardo, Frederic Precioso, Damien Garreau
Categories: cs.CL cs.LG stat.ML
\\ ( https://arxiv.org/abs/2311.01605 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04902
replaced with revised version Mon, 8 Apr 2024 22:42:49 GMT   (3949kb,D)

Title: Beyond Size: How Gradients Shape Pruning Decisions in Large Language
  Models
Authors: Rocktim Jyoti Das and Mingjie Sun and Liqun Ma and Zhiqiang Shen
Categories: cs.CL cs.AI cs.LG
Comments: Code and models at https://github.com/VILA-Lab/GBLM-Pruner
\\ ( https://arxiv.org/abs/2311.04902 ,  3949kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09480
replaced with revised version Mon, 8 Apr 2024 18:21:53 GMT   (1101kb,D)

Title: Show Your Work with Confidence: Confidence Bands for Tuning Curves
Authors: Nicholas Lourie, Kyunghyun Cho, He He
Categories: cs.CL cs.LG stat.ML
Comments: Accepted to NAACL 2024. 18 pages, 20 figures
\\ ( https://arxiv.org/abs/2311.09480 ,  1101kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09502
replaced with revised version Mon, 8 Apr 2024 21:31:57 GMT   (854kb,D)

Title: SQATIN: Supervised Instruction Tuning Meets Question Answering for
  Improved Dialogue NLU
Authors: Evgeniia Razumovskaia, Goran Glava\v{s}, Anna Korhonen, Ivan Vuli\'c
Categories: cs.CL
Comments: Accepted to NAACL Main 2024
\\ ( https://arxiv.org/abs/2311.09502 ,  854kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09696
replaced with revised version Mon, 8 Apr 2024 20:57:40 GMT   (5235kb,D)

Title: Fumbling in Babel: An Investigation into ChatGPT's Language
  Identification Ability
Authors: Wei-Rui Chen, Ife Adebara, Khai Duy Doan, Qisheng Liao, Muhammad
  Abdul-Mageed
Categories: cs.CL
Comments: Accepted to NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2311.09696 ,  5235kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17213
replaced with revised version Tue, 9 Apr 2024 13:46:46 GMT   (1021kb)

Title: General-Purpose vs. Domain-Adapted Large Language Models for Extraction
  of Structured Data from Chest Radiology Reports
Authors: Ali H. Dhanaliwala, Rikhiya Ghosh, Sanjeev Kumar Karn, Poikavila
  Ullaskrishnan, Oladimeji Farri, Dorin Comaniciu and Charles E. Kahn
Categories: cs.CL eess.IV
\\ ( https://arxiv.org/abs/2311.17213 ,  1021kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11052
replaced with revised version Tue, 9 Apr 2024 07:32:37 GMT   (449kb,D)

Title: Mining experimental data from Materials Science literature with Large
  Language Models: an evaluation study
Authors: Luca Foppiano, Guillaume Lambard, Toshiyuki Amagasa, Masashi Ishii
Categories: cs.CL
Comments: 5 figures, 1 tables, 40 pages. 32 Tables in the Appendix /
  Supplementary materials
\\ ( https://arxiv.org/abs/2401.11052 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16640
replaced with revised version Tue, 9 Apr 2024 14:35:02 GMT   (2408kb,D)

Title: TeenyTinyLlama: open-source tiny language models trained in Brazilian
  Portuguese
Authors: Nicholas Kluge Corr\^ea, Sophia Falk, Shiza Fatimah, Aniket Sen,
  Nythamar de Oliveira
Categories: cs.CL cs.LG
Comments: 21 pages, 5 figures
\\ ( https://arxiv.org/abs/2401.16640 ,  2408kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11541
replaced with revised version Tue, 9 Apr 2024 07:39:47 GMT   (811kb,D)

Title: Counter-intuitive: Large Language Models Can Better Understand Knowledge
  Graphs Than We Thought
Authors: Xinbang Dai, Yuncheng Hua, Tongtong Wu, Yang Sheng, Qiu Ji, Guilin Qi
Categories: cs.CL cs.AI
Comments: 13 pages
ACM-class: I.2.4; I.2.7
\\ ( https://arxiv.org/abs/2402.11541 ,  811kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12869
replaced with revised version Tue, 9 Apr 2024 09:00:57 GMT   (1905kb,D)

Title: Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based
  Question Answering with Domain Hybrid Data
Authors: Dehai Min, Nan Hu, Rihui Jin, Nuo Lin, Jiaoyan Chen, Yongrui Chen, Yu
  Li, Guilin Qi, Yun Li, Nijun Li, Qianren Wang
Categories: cs.CL
Comments: Accepted to NAACL 2024 Industry Track Paper
\\ ( https://arxiv.org/abs/2402.12869 ,  1905kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15873
replaced with revised version Tue, 9 Apr 2024 10:19:48 GMT   (17kb)

Title: SemEval-2024 Task 8: Weighted Layer Averaging RoBERTa for Black-Box
  Machine-Generated Text Detection
Authors: Ayan Datta, Aryan Chandramania, Radhika Mamidi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.15873 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06108
replaced with revised version Tue, 9 Apr 2024 16:38:01 GMT   (1832kb,D)

Title: Large Language Models on Fine-grained Emotion Detection Dataset with
  Data Augmentation and Transfer Learning
Authors: Kaipeng Wang, Zhi Jing, Yongye Su, Yikun Han
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.06108 ,  1832kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16432
replaced with revised version Tue, 9 Apr 2024 13:05:49 GMT   (7544kb,D)

Title: $\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on
  Prompt-based Language Models
Authors: Yue Xu, Wenjie Wang
Categories: cs.CL cs.AI
Comments: Accepted to the main conference of NAACL2024
\\ ( https://arxiv.org/abs/2403.16432 ,  7544kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18249
replaced with revised version Mon, 8 Apr 2024 19:55:37 GMT   (5468kb,D)

Title: Exploring the Deceptive Power of LLM-Generated Fake News: A Study of
  Real-World Detection Challenges
Authors: Yanshen Sun, Jianfeng He, Limeng Cui, Shuo Lei, Chang-Tien Lu
Categories: cs.CL cs.SI
\\ ( https://arxiv.org/abs/2403.18249 ,  5468kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18253
replaced with revised version Tue, 9 Apr 2024 03:47:29 GMT   (7826kb,D)

Title: Enhancing Metaphor Detection through Soft Labels and Target Word
  Prediction
Authors: Kaidi Jia and Rongsheng Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.18253 ,  7826kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19521
replaced with revised version Tue, 9 Apr 2024 17:06:56 GMT   (7064kb,D)

Title: Interpreting Key Mechanisms of Factual Recall in Transformer-Based
  Language Models
Authors: Ang Lv, Kaiyi Zhang, Yuhan Chen, Yulong Wang, Lifeng Liu, Ji-Rong Wen,
  Jian Xie, Rui Yan
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2403.19521 ,  7064kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04167
replaced with revised version Tue, 9 Apr 2024 10:48:19 GMT   (2385kb,D)

Title: Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model
Authors: Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang
  Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui
  Zhou, Binhang Yuan, Wenhu Chen, Jie Fu, and Ge Zhang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2404.04167 ,  2385kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04845
replaced with revised version Tue, 9 Apr 2024 07:21:37 GMT   (7950kb,D)

Title: SLPL SHROOM at SemEval2024 Task 06: A comprehensive study on models
  ability to detect hallucination
Authors: Pouya Fallah, Soroush Gooran, Mohammad Jafarinasab, Pouya Sadeghi,
  Reza Farnia, Amirreza Tarabkhah, Zainab Sadat Taghavi, Hossein Sameti
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2404.04845 ,  7950kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05333
replaced with revised version Tue, 9 Apr 2024 12:30:49 GMT   (40kb)

Title: PORTULAN ExtraGLUE Datasets and Models: Kick-starting a Benchmark for
  the Neural Processing of Portuguese
Authors: Tom\'as Os\'orio, Bernardo Leite, Henrique Lopes Cardoso, Lu\'is
  Gomes, Jo\~ao Rodrigues, Rodrigo Santos, Ant\'onio Branco
Categories: cs.CL
Comments: Preprint - Paper accepted for BUCC 2024
\\ ( https://arxiv.org/abs/2404.05333 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05632
replaced with revised version Tue, 9 Apr 2024 09:30:46 GMT   (7743kb,D)

Title: Fighting crime with Transformers: Empirical analysis of address parsing
  methods in payment data
Authors: Haitham Hammami, Louis Baligand, Bojan Petrovski
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.05632 ,  7743kb)
------------------------------------------------------------------------------
\\
arXiv:1901.08930
replaced with revised version Mon, 8 Apr 2024 19:23:50 GMT   (31859kb,D)

Title: Effectiveness of Tree-based Ensembles for Anomaly Discovery: Insights,
  Batch and Streaming Active Learning
Authors: Shubhomoy Das, Md Rakibul Islam, Nitthilan Kannappan Jayakodi,
  Janardhan Rao Doppa
Categories: cs.LG stat.ML
Comments: Accepted for Publication in Journal of Artificial Intelligence
  Research. 46 pages; code is available at
  https://github.com/shubhomoydas/ad_examples. arXiv admin note: substantial
  text overlap with arXiv:1809.06477
\\ ( https://arxiv.org/abs/1901.08930 ,  31859kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12554
replaced with revised version Tue, 9 Apr 2024 02:42:28 GMT   (3058kb,D)

Title: Improving the Accuracy-Robustness Trade-Off of Classifiers via Adaptive
  Smoothing
Authors: Yatong Bai, Brendon G. Anderson, Aerin Kim, Somayeh Sojoudi
Categories: cs.LG cs.CR cs.CV
MSC-class: 68T07
\\ ( https://arxiv.org/abs/2301.12554 ,  3058kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13821
replaced with revised version Tue, 9 Apr 2024 11:57:04 GMT   (3423kb,D)

Title: Complete Neural Networks for Complete Euclidean Graphs
Authors: Snir Hordan, Tal Amir, Steven J. Gortler, Nadav Dym
Categories: cs.LG
Comments: The 38th AAAI Conference on Artificial Intelligence
Journal-ref: Proceedings of the AAAI Conference on Artificial Intelligence,
  38(11), 12482-12490 (2024)
DOI: 10.1609/aaai.v38i11.29141
\\ ( https://arxiv.org/abs/2301.13821 ,  3423kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00709
replaced with revised version Tue, 9 Apr 2024 11:24:34 GMT   (41087kb,D)

Title: MSS-PAE: Saving Autoencoder-based Outlier Detection from Unexpected
  Reconstruction
Authors: Xu Tan, Jiawei Yang, Junqi Chen, Sylwan Rahardja, Susanto Rahardja
Categories: cs.LG
Comments: 14 pages, 11 figures. Submitted to IEEE Transactions on Knowledge and
  Data Engineering (TKDE)
\\ ( https://arxiv.org/abs/2304.00709 ,  41087kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15215
replaced with revised version Mon, 8 Apr 2024 22:03:54 GMT   (2858kb,D)

Title: Shadow Cones: A Generalized Framework for Partial Order Embeddings
Authors: Tao Yu, Toni J.B. Liu, Albert Tseng, Christopher De Sa
Categories: cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2305.15215 ,  2858kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03305
replaced with revised version Tue, 9 Apr 2024 03:41:38 GMT   (2059kb,D)

Title: A Vulnerability of Attribution Methods Using Pre-Softmax Scores
Authors: Miguel Lerma, Mirtha Lucas
Categories: cs.LG cs.AI
Comments: 7 pages, 5 figures
MSC-class: 68T07
ACM-class: I.2.m
\\ ( https://arxiv.org/abs/2307.03305 ,  2059kb)
------------------------------------------------------------------------------
\\
arXiv:2307.06104
replaced with revised version Tue, 9 Apr 2024 07:41:38 GMT   (736kb,D)

Title: Deep learning for dynamic graphs: models and benchmarks
Authors: Alessio Gravina and Davide Bacciu
Categories: cs.LG cs.SI
Comments: Preprint version. Accepted at IEEE TNNLS
  (https://ieeexplore.ieee.org/document/10490120)
DOI: 10.1109/TNNLS.2024.3379735
\\ ( https://arxiv.org/abs/2307.06104 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12062
replaced with revised version Tue, 9 Apr 2024 07:18:56 GMT   (6330kb,D)

Title: Game-Theoretic Robust Reinforcement Learning Handles Temporally-Coupled
  Perturbations
Authors: Yongyuan Liang, Yanchao Sun, Ruijie Zheng, Xiangyu Liu, Benjamin
  Eysenbach, Tuomas Sandholm, Furong Huang and Stephen McAleer
Categories: cs.LG cs.AI
Comments: Accepted at The Twelfth International Conference on Learning
  Representations (ICLR 2024)
\\ ( https://arxiv.org/abs/2307.12062 ,  6330kb)
------------------------------------------------------------------------------
\\
arXiv:2308.13453
replaced with revised version Tue, 9 Apr 2024 11:17:56 GMT   (186kb,D)

Title: Learning to Intervene on Concept Bottlenecks
Authors: David Steinmann, Wolfgang Stammer, Felix Friedrich, Kristian Kersting
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.13453 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06717
replaced with revised version Tue, 9 Apr 2024 16:05:23 GMT   (4459kb,D)

Title: Bias Amplification Enhances Minority Group Performance
Authors: Gaotang Li, Jiarui Liu, Wei Hu
Categories: cs.LG cs.CY
Comments: To appear in TMLR
\\ ( https://arxiv.org/abs/2309.06717 ,  4459kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12488
replaced with revised version Mon, 8 Apr 2024 18:14:43 GMT   (296kb,D)

Title: Sharpness-Aware Minimization and the Edge of Stability
Authors: Philip M. Long and Peter L. Bartlett
Categories: cs.LG cs.NE stat.ML
\\ ( https://arxiv.org/abs/2309.12488 ,  296kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10060
replaced with revised version Tue, 9 Apr 2024 08:54:14 GMT   (2270kb,D)

Title: Data Augmentation for Time-Series Classification: An Extensive Empirical
  Study and Comprehensive Survey
Authors: Zijun Gao and Lingbo Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.10060 ,  2270kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11211
replaced with revised version Tue, 9 Apr 2024 02:43:58 GMT   (1150kb,D)

Title: Understanding Fairness Surrogate Functions in Algorithmic Fairness
Authors: Wei Yao, Zhanke Zhou, Zhicong Li, Bo Han, Yong Liu
Categories: cs.LG cs.AI
Comments: Accepted by TMLR 2024
\\ ( https://arxiv.org/abs/2310.11211 ,  1150kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19698
replaced with revised version Tue, 9 Apr 2024 10:30:14 GMT   (562kb,D)

Title: When Do Prompting and Prefix-Tuning Work? A Theory of Capabilities and
  Limitations
Authors: Aleksandar Petrov, Philip H.S. Torr, Adel Bibi
Categories: cs.LG cs.CL
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2310.19698 ,  562kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04818
replaced with revised version Tue, 9 Apr 2024 14:15:32 GMT   (3500kb,D)

Title: Cross-Silo Federated Learning Across Divergent Domains with Iterative
  Parameter Alignment
Authors: Matt Gorbett, Hossein Shirazi, Indrakshi Ray
Categories: cs.LG cs.CV cs.DC
Comments: Published at IEEE Big Data 2023
\\ ( https://arxiv.org/abs/2311.04818 ,  3500kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04951
replaced with revised version Tue, 9 Apr 2024 11:21:14 GMT   (820kb,D)

Title: Leveraging Speculative Sampling and KV-Cache Optimizations Together for
  Generative AI using OpenVINO
Authors: Haim Barad, Ekaterina Aidova, Yury Gorbachev
Categories: cs.LG cs.AI cs.PF
Comments: Code available at
  https://github.com/openvinotoolkit/openvino_notebooks/tree/latest/notebooks/speculative-sampling
\\ ( https://arxiv.org/abs/2311.04951 ,  820kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06798
replaced with revised version Tue, 9 Apr 2024 15:07:02 GMT   (1625kb,D)

Title: MetaMix: Meta-state Precision Searcher for Mixed-precision Activation
  Quantization
Authors: Han-Byul Kim, Joo Hyung Lee, Sungjoo Yoo, Hong-Seok Kim
Categories: cs.LG cs.CV
Comments: Proc. The 38th Annual AAAI Conference on Artificial Intelligence
  (AAAI)
DOI: 10.1609/aaai.v38i12.29212
\\ ( https://arxiv.org/abs/2311.06798 ,  1625kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13166
replaced with revised version Tue, 9 Apr 2024 05:43:38 GMT   (8353kb,D)

Title: AdaptiveFL: Adaptive Heterogeneous Federated Learning for
  Resource-Constrained AIoT Systems
Authors: Chentao Jia and Ming Hu and Zekai Chen and Yanxin Yang and Xiaofei Xie
  and Yang Liu and Mingsong Chen
Categories: cs.LG cs.DC
Comments: This paper has been accepted by DAC2024
DOI: 10.1145/3649329.3655917
\\ ( https://arxiv.org/abs/2311.13166 ,  8353kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13580
replaced with revised version Tue, 9 Apr 2024 08:38:48 GMT   (24538kb,D)

Title: $\sigma$-PCA: a unified neural model for linear and nonlinear principal
  component analysis
Authors: Fahdi Kanavati, Lucy Katsnith, Masayuki Tsuneki
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2311.13580 ,  24538kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16026
replaced with revised version Tue, 9 Apr 2024 17:08:55 GMT   (4117kb,D)

Title: A Neural Framework for Generalized Causal Sensitivity Analysis
Authors: Dennis Frauen, Fergus Imrie, Alicia Curth, Valentyn Melnychuk, Stefan
  Feuerriegel, Mihaela van der Schaar
Categories: cs.LG stat.ML
Comments: Accepted at ICLR 2024
\\ ( https://arxiv.org/abs/2311.16026 ,  4117kb)
------------------------------------------------------------------------------
\\
arXiv:2401.03955
replaced with revised version Tue, 9 Apr 2024 07:19:41 GMT   (2250kb,D)

Title: Tiny Time Mixers (TTMs): Fast Pre-trained Models for Enhanced
  Zero/Few-Shot Forecasting of Multivariate Time Series
Authors: Vijay Ekambaram, Arindam Jati, Nam H. Nguyen, Pankaj Dayama, Chandra
  Reddy, Wesley M. Gifford, Jayant Kalagnanam
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2401.03955 ,  2250kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01348
replaced with revised version Tue, 9 Apr 2024 08:33:22 GMT   (3142kb,D)

Title: CORE: Mitigating Catastrophic Forgetting in Continual Learning through
  Cognitive Replay
Authors: Jianshu Zhang, Yankai Fu, Ziheng Peng, Dongyu Yao, Kun He
Categories: cs.LG cs.AI
Comments: Accepted by CogSci24 as oral presentation
\\ ( https://arxiv.org/abs/2402.01348 ,  3142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06445
replaced with revised version Tue, 9 Apr 2024 07:26:38 GMT   (121kb,D)

Title: The Deep Equilibrium Algorithmic Reasoner
Authors: Dobrik Georgiev, Pietro Li\`o, Davide Buffelli
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.06445 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15731
replaced with revised version Tue, 9 Apr 2024 09:40:22 GMT   (1462kb)

Title: Clustering in Dynamic Environments: A Framework for Benchmark Dataset
  Generation With Heterogeneous Changes
Authors: Danial Yazdani, Juergen Branke, Mohammad Sadegh Khorshidi, Mohammad
  Nabi Omidvar, Xiaodong Li, Amir H. Gandomi and Xin Yao
Categories: cs.LG cs.NE
\\ ( https://arxiv.org/abs/2402.15731 ,  1462kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04764
replaced with revised version Tue, 9 Apr 2024 01:28:43 GMT   (886kb,D)

Title: TS-RSR: A provably efficient approach for batch bayesian optimization
Authors: Zhaolin Ren and Na Li
Categories: cs.LG math.OC stat.ML
Comments: Added more numerical results
\\ ( https://arxiv.org/abs/2403.04764 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2403.06668
replaced with revised version Tue, 9 Apr 2024 05:47:39 GMT   (3762kb,D)

Title: PeerAiD: Improving Adversarial Distillation from a Specialized Peer
  Tutor
Authors: Jaewon Jung, Hongsun Jang, Jaeyong Song, Jinho Lee
Categories: cs.LG cs.CV
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2403.06668 ,  3762kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11343
replaced with revised version Tue, 9 Apr 2024 10:34:40 GMT   (547kb,D)

Title: Federated Transfer Learning with Differential Privacy
Authors: Mengchu Li, Ye Tian, Yang Feng, Yi Yu
Categories: cs.LG cs.CR math.ST stat.ME stat.ML stat.TH
Comments: 78 pages, 3 figures
\\ ( https://arxiv.org/abs/2403.11343 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16459
replaced with revised version Tue, 9 Apr 2024 03:06:10 GMT   (60kb,D)

Title: On the rates of convergence for learning with convolutional neural
  networks
Authors: Yunfei Yang, Han Feng, Ding-Xuan Zhou
Categories: cs.LG math.ST stat.ML stat.TH
\\ ( https://arxiv.org/abs/2403.16459 ,  60kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19871
replaced with revised version Mon, 8 Apr 2024 21:52:11 GMT   (3484kb,D)

Title: Towards Stable Machine Learning Model Retraining via Slowly Varying
  Sequences
Authors: Dimitris Bertsimas, Vassilis Digalakis Jr, Yu Ma, Phevos Paschalidis
Categories: cs.LG cs.AI math.OC
\\ ( https://arxiv.org/abs/2403.19871 ,  3484kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01306
replaced with revised version Tue, 9 Apr 2024 14:59:10 GMT   (8124kb,D)

Title: NeuroPrune: A Neuro-inspired Topological Sparse Training Algorithm for
  Large Language Models
Authors: Amit Dhurandhar, Tejaswini Pedapati, Ronny Luss, Soham Dan, Aurelie
  Lozano, Payel Das and Georgios Kollias
Categories: cs.LG cs.CL
\\ ( https://arxiv.org/abs/2404.01306 ,  8124kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04002
replaced with revised version Tue, 9 Apr 2024 09:35:24 GMT   (1745kb,D)

Title: Continual Learning with Weight Interpolation
Authors: J\k{e}drzej Kozal and Jan Wasilewski and Bartosz Krawczyk and
  Micha{\l} Wo\'zniak
Categories: cs.LG
\\ ( https://arxiv.org/abs/2404.04002 ,  1745kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04234
replaced with revised version Mon, 8 Apr 2024 18:11:10 GMT   (1218kb,D)

Title: player2vec: A Language Modeling Approach to Understand Player Behavior
  in Games
Authors: Tianze Wang, Maryam Honari-Jahromi, Styliani Katsarou, Olga Mikheeva,
  Theodoros Panagiotakopoulos, Sahar Asadi, Oleg Smirnov
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2404.04234 ,  1218kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05241
replaced with revised version Tue, 9 Apr 2024 06:16:58 GMT   (317kb,D)

Title: Lightweight Inference for Forward-Forward Algorithm
Authors: Amin Aminifar, Baichuan Huang, Azra Abtahi, Amir Aminifar
Categories: cs.LG
\\ ( https://arxiv.org/abs/2404.05241 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05576
replaced with revised version Tue, 9 Apr 2024 15:04:24 GMT   (1652kb,D)

Title: Dynamic Backtracking in GFlowNets: Enhancing Decision Steps with
  Reward-Dependent Adjustment Mechanisms
Authors: Shuai Guo, Jielei Chu, Lei Zhu, Tianrui Li
Categories: cs.LG
\\ ( https://arxiv.org/abs/2404.05576 ,  1652kb)
------------------------------------------------------------------------------
\\
arXiv:2204.03330
replaced with revised version Tue, 9 Apr 2024 15:44:05 GMT   (39092kb,D)

Title: Learning Local and Global Temporal Contexts for Video Semantic
  Segmentation
Authors: Guolei Sun, Yun Liu, Henghui Ding, Min Wu, Luc Van Gool
Categories: cs.CV cs.AI
Comments: Accepted to TPAMI, an extended version of a paper published in CVPR
  2022
\\ ( https://arxiv.org/abs/2204.03330 ,  39092kb)
------------------------------------------------------------------------------
\\
arXiv:2210.11470
replaced with revised version Mon, 8 Apr 2024 22:01:32 GMT   (17638kb,D)

Title: i-MAE: Are Latent Representations in Masked Autoencoders Linearly
  Separable?
Authors: Kevin Zhang and Zhiqiang Shen
Categories: cs.CV cs.AI cs.LG
Comments: Project page: https://zhiqiangshen.com/projects/i-mae/
\\ ( https://arxiv.org/abs/2210.11470 ,  17638kb)
------------------------------------------------------------------------------
\\
arXiv:2211.15930 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 17:45:59 GMT   (4580kb,D)

Title: Offline Supervised Learning V.S. Online Direct Policy Optimization: A
  Comparative Study and A Unified Training Paradigm for Neural Network-Based
  Optimal Feedback Control
Authors: Yue Zhao, Jiequn Han
Categories: math.OC cs.AI
Comments: Accepted for publication in Physica D
DOI: 10.1016/j.physd.2024.134130
\\ ( https://arxiv.org/abs/2211.15930 ,  4580kb)
------------------------------------------------------------------------------
\\
arXiv:2212.08731
replaced with revised version Tue, 9 Apr 2024 17:52:49 GMT   (3514kb,D)

Title: Multi-person 3D pose estimation from unlabelled data
Authors: Daniel Rodriguez-Criado, Pilar Bachiller, George Vogiatzis and Luis J.
  Manso
Categories: cs.CV cs.AI
Journal-ref: Machine Vision and Applications 35, 46 (2024)
DOI: 10.1007/s00138-024-01530-6
\\ ( https://arxiv.org/abs/2212.08731 ,  3514kb)
------------------------------------------------------------------------------
\\
arXiv:2301.01946 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 16:34:02 GMT   (8042kb,D)

Title: EPR-Net: Constructing non-equilibrium potential landscape via a
  variational force projection formulation
Authors: Yue Zhao, Wei Zhang, Tiejun Li
Categories: physics.bio-ph cond-mat.stat-mech cs.AI physics.comp-ph
Comments: Accepted for publication in National Science Review
DOI: 10.1093/nsr/nwae052
\\ ( https://arxiv.org/abs/2301.01946 ,  8042kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17546
replaced with revised version Mon, 8 Apr 2024 16:49:16 GMT   (65704kb,D)

Title: PAIR-Diffusion: A Comprehensive Multimodal Object-Level Image Editor
Authors: Vidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Xingqian Xu, Nicu
  Sebe, Trevor Darrell, Zhangyang Wang, Humphrey Shi
Categories: cs.CV cs.AI cs.LG
Comments: Accepted in CVPR 2024, Project page
  https://vidit98.github.io/publication/conference-paper/pair_diff.html
\\ ( https://arxiv.org/abs/2303.17546 ,  65704kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02730
replaced with revised version Tue, 9 Apr 2024 13:18:22 GMT   (6998kb,D)

Title: Fine-grained Action Analysis: A Multi-modality and Multi-task Dataset of
  Figure Skating
Authors: Sheng-Lan Liu, Yu-Ning Ding, Gang Yan, Si-Fan Zhang, Jin-Rong Zhang,
  Wen-Yue Chen, Xue-Hai Xu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2307.02730 ,  6998kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03366
replaced with revised version Mon, 8 Apr 2024 21:54:14 GMT   (705kb,D)

Title: Neural Code Generation Enhancement via Functional Overlap Reranking
Authors: Hung Quoc To, Minh Huynh Nguyen, Nghi D. Q. Bui
Categories: cs.SE cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.03366 ,  705kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13081
replaced with revised version Mon, 8 Apr 2024 21:04:51 GMT   (4731kb,D)

Title: Learning to Fly in Seconds
Authors: Jonas Eschmann, Dario Albani, Giuseppe Loianno
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
Comments: Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L)
\\ ( https://arxiv.org/abs/2311.13081 ,  4731kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02813
replaced with revised version Tue, 9 Apr 2024 09:12:58 GMT   (32872kb,D)

Title: BIVDiff: A Training-Free Framework for General-Purpose Video Synthesis
  via Bridging Image and Video Diffusion Models
Authors: Fengyuan Shi, Jiaxi Gu, Hang Xu, Songcen Xu, Wei Zhang, Limin Wang
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024. Project page: https://bivdiff.github.io;
  GitHub repository: https://github.com/MCG-NJU/BIVDiff
\\ ( https://arxiv.org/abs/2312.02813 ,  32872kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17053
replaced with revised version Tue, 9 Apr 2024 13:47:18 GMT   (7794kb,D)

Title: BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane
  Extrapolation
Authors: Zhennan Wu, Yang Li, Han Yan, Taizhang Shang, Weixuan Sun, Senbo Wang,
  Ruikai Cui, Weizhe Liu, Hiroyuki Sato, Hongdong Li, and Pan Ji
Categories: cs.CV cs.AI cs.GR
Comments: Video: https://www.youtube.com/watch?v=PxIBtd6G0mA
\\ ( https://arxiv.org/abs/2401.17053 ,  7794kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03130
replaced with revised version Tue, 9 Apr 2024 12:37:56 GMT   (4614kb,D)

Title: Evaluation of ChatGPT Usability as A Code Generation Tool
Authors: Tanha Miah and Hong Zhu
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2402.03130 ,  4614kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11363 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 04:50:29 GMT   (8047kb,D)

Title: Transformer-based de novo peptide sequencing for data-independent
  acquisition mass spectrometry
Authors: Shiva Ebrahimi and Xuan Guo
Categories: q-bio.QM cs.AI
Comments: Ebrahimi S., Guo X. Transformer-based de novo peptide sequencing for
  data-independent acquisition mass spectrometry. In 2023 IEEE 23rd
  International Conference on Bioinformatics and Bioengineering (BIBE) 2022 Dec
  6 (pp. 17-22). IEEE
\\ ( https://arxiv.org/abs/2402.11363 ,  8047kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10380
replaced with revised version Mon, 8 Apr 2024 20:58:09 GMT   (4097kb,D)

Title: BirdSet: A Multi-Task Benchmark for Classification in Computational
  Avian Bioacoustics
Authors: Lukas Rauch, Raphael Schwinger, Moritz Wirth, Ren\'e Heinrich, Jonas
  Lange, Stefan Kahl, Bernhard Sick, Sven Tomforde, Christoph Scholz
Categories: cs.SD cs.AI eess.AS
Comments: Work in progress, to be submitted @DMLR next month
\\ ( https://arxiv.org/abs/2403.10380 ,  4097kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02361
replaced with revised version Tue, 9 Apr 2024 16:32:22 GMT   (702kb)

Title: EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to
  Grid Energy Management
Authors: Tiago Fonseca, Luis Ferreira, Bernardo Cabral, Ricardo Severino,
  Isabel Praca
Categories: cs.MA cs.AI
Comments: 6 pages, 6 figures, 2 tables
\\ ( https://arxiv.org/abs/2404.02361 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03664
replaced with revised version Tue, 9 Apr 2024 08:08:52 GMT   (8250kb,D)

Title: LLMs in the Heart of Differential Testing: A Case Study on a Medical
  Rule Engine
Authors: Erblin Isaku, Christoph Laaber, Hassan Sartaj, Shaukat Ali, Thomas
  Schwitalla, Jan F. Nyg{\aa}rd
Categories: cs.SE cs.AI
Comments: 12 pages, 6 figures, 4 tables, 1 listing
\\ ( https://arxiv.org/abs/2404.03664 ,  8250kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03892
replaced with revised version Tue, 9 Apr 2024 07:21:32 GMT   (2914kb,D)

Title: Enhancing Breast Cancer Diagnosis in Mammography: Evaluation and
  Integration of Convolutional Neural Networks and Explainable AI
Authors: Maryam Ahmed, Tooba Bibi, Rizwan Ahmed Khan, Sidra Nasir
Categories: cs.CV cs.AI cs.LG eess.IV
\\ ( https://arxiv.org/abs/2404.03892 ,  2914kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05393
replaced with revised version Tue, 9 Apr 2024 09:52:32 GMT   (15911kb,D)

Title: PAT: Pixel-wise Adaptive Training for Long-tailed Segmentation
Authors: Khoi Do, Duong Nguyen, Nguyen H. Tran, Viet Dung Nguyen
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2404.05393 ,  15911kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05499
replaced with revised version Tue, 9 Apr 2024 11:29:47 GMT   (1723kb)

Title: Guiding Large Language Models to Generate Computer-Parsable Content
Authors: Jiaye Wang
Categories: cs.SE cs.AI
Comments: 44 pages, 39 figures, 8 tables, Chinese version:
  https://chinaxiv.org/abs/202403.00340
\\ ( https://arxiv.org/abs/2404.05499 ,  1723kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08030 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 22:23:21 GMT   (2009kb,D)

Title: AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised
  Features for Audio-Visual Speech Enhancement
Authors: Ju-Chieh Chou, Chung-Ming Chien, Karen Livescu
Categories: eess.AS cs.CL cs.SD
Comments: extended version for the accepted paper at ICASSP 2024
\\ ( https://arxiv.org/abs/2309.08030 ,  2009kb)
------------------------------------------------------------------------------
\\
arXiv:1709.04402
replaced with revised version Tue, 9 Apr 2024 12:31:11 GMT   (5709kb,D)

Title: On Early-stage Debunking Rumors on Twitter: Leveraging the Wisdom of
  Weak Learners
Authors: Tu Nguyen, Cheng Li, Claudia Nieder\'ee
Categories: cs.SI cs.LG
Comments: The 9th International Conference on Social Informatics
\\ ( https://arxiv.org/abs/1709.04402 ,  5709kb)
------------------------------------------------------------------------------
\\
arXiv:1711.00726
replaced with revised version Tue, 9 Apr 2024 12:24:14 GMT   (7770kb,D)

Title: A Comprehensive Low and High-level Feature Analysis for Early Rumor
  Detection on Twitter
Authors: Tu Nguyen
Categories: cs.SI cs.LG
Comments: CIKM 2017 Workshop on Interpretable Data Mining - Bridging the Gap
  between Shallow and Deep Models (IDM 2017). arXiv admin note: substantial
  text overlap with arXiv:1709.04402
\\ ( https://arxiv.org/abs/1711.00726 ,  7770kb)
------------------------------------------------------------------------------
\\
arXiv:1803.07890
replaced with revised version Tue, 9 Apr 2024 12:35:48 GMT   (1516kb,D)

Title: Multiple Models for Recommending Temporal Aspects of Entities
Authors: Tu Nguyen, Nattiya Kanhabua, Wolfgang Nejdl
Categories: cs.IR cs.LG
Comments: In proceedings of the 15th Extended Semantic Web Conference (ESWC
  2018)
\\ ( https://arxiv.org/abs/1803.07890 ,  1516kb)
------------------------------------------------------------------------------
\\
arXiv:1808.07380
replaced with revised version Tue, 9 Apr 2024 14:40:13 GMT   (5677kb,D)

Title: On the Predictability of non-CGM Diabetes Data for Personalized
  Recommendation
Authors: Tu Nguyen and Markus Rokicki
Categories: cs.CY cs.LG stat.ML
Comments: In Proceedings of ACM CIKM 2018 Workshops
\\ ( https://arxiv.org/abs/1808.07380 ,  5677kb)
------------------------------------------------------------------------------
\\
arXiv:2206.13475 (*cross-listing*)
replaced with revised version Mon, 8 Apr 2024 23:41:21 GMT   (36503kb,D)

Title: Thermodynamics-inspired Explanations of Artificial Intelligence
Authors: Shams Mehdi and Pratyush Tiwary
Categories: cond-mat.stat-mech cond-mat.dis-nn cs.LG physics.comp-ph
Comments: revised theory and examples
\\ ( https://arxiv.org/abs/2206.13475 ,  36503kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04227
replaced with revised version Tue, 9 Apr 2024 13:30:15 GMT   (1750kb,D)

Title: Self-training via Metric Learning for Source-Free Domain Adaptation of
  Semantic Segmentation
Authors: Ibrahim Batuhan Akkaya and Ugur Halici
Categories: cs.CV cs.LG
Comments: This paper is under consideration at Computer Vision and Image
  Understanding
\\ ( https://arxiv.org/abs/2212.04227 ,  1750kb)
------------------------------------------------------------------------------
\\
arXiv:2301.07876
replaced with revised version Tue, 9 Apr 2024 03:11:26 GMT   (326kb,D)

Title: Suboptimality analysis of receding horizon quadratic control with
  unknown linear systems and its applications in learning-based control
Authors: Shengling Shi, Anastasios Tsiamis, Bart De Schutter
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2301.07876 ,  326kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06140
replaced with revised version Tue, 9 Apr 2024 17:09:03 GMT   (30042kb,D)

Title: An Edit Friendly DDPM Noise Space: Inversion and Manipulations
Authors: Inbar Huberman-Spiegelglas, Vladimir Kulikov and Tomer Michaeli
Categories: cs.CV cs.LG
Comments: CVPR 2024. Code and examples are available at
  https://github.com/inbarhub/DDPM_inversion
\\ ( https://arxiv.org/abs/2304.06140 ,  30042kb)
------------------------------------------------------------------------------
\\
arXiv:2305.13033
replaced with revised version Tue, 9 Apr 2024 16:22:13 GMT   (20947kb,D)

Title: Towards generalizing deep-audio fake detection networks
Authors: Konstantin Gasenzer (1) and Moritz Wolter (1) ((1) High Performance
  Computing and Analytics Lab, Universit\"at Bonn, Germany)
Categories: cs.SD cs.LG eess.AS
Comments: Code available at:
  https://github.com/gan-police/audiodeepfake-detection
Journal-ref: Published in Transactions on Machine Learning Research (04/2024)
\\ ( https://arxiv.org/abs/2305.13033 ,  20947kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18171
replaced with revised version Tue, 9 Apr 2024 13:42:07 GMT   (1618kb,D)

Title: Improved Probabilistic Image-Text Representations
Authors: Sanghyuk Chun
Categories: cs.CV cs.LG
Comments: ICLR 2024 camera-ready; Code: https://github.com/naver-ai/pcmepp.
  Project page: https://naver-ai.github.io/pcmepp/. 30 pages, 2.2 MB
\\ ( https://arxiv.org/abs/2305.18171 ,  1618kb)
------------------------------------------------------------------------------
\\
arXiv:2306.09694 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 00:43:58 GMT   (1740kb)

Title: Linear convergence of forward-backward accelerated algorithms without
  knowledge of the modulus of strong convexity
Authors: Bowen Li, Bin Shi, Ya-xiang Yuan
Categories: math.OC cs.LG cs.NA math.NA stat.ML
Comments: 17 pages, 3 figures; To appear in SIAM Journal on Optimization
\\ ( https://arxiv.org/abs/2306.09694 ,  1740kb)
------------------------------------------------------------------------------
\\
arXiv:2307.03108
replaced with revised version Tue, 9 Apr 2024 16:31:33 GMT   (8946kb,D)

Title: DIAGNOSIS: Detecting Unauthorized Data Usages in Text-to-image Diffusion
  Models
Authors: Zhenting Wang, Chen Chen, Lingjuan Lyu, Dimitris N. Metaxas, Shiqing
  Ma
Categories: cs.CV cs.CR cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2307.03108 ,  8946kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08165 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 06:47:55 GMT   (1313kb,D)

Title: Stochastic Controlled Averaging for Federated Learning with
  Communication Compression
Authors: Xinmeng Huang, Ping Li, Xiaoyun Li
Categories: math.OC cs.DC cs.LG stat.ML
Comments: 45 pages, 4 figures
\\ ( https://arxiv.org/abs/2308.08165 ,  1313kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09552
replaced with revised version Tue, 9 Apr 2024 11:41:25 GMT   (3096kb,D)

Title: Attesting Distributional Properties of Training Data for Machine
  Learning
Authors: Vasisht Duddu, Anudeep Das, Nora Khayata, Hossein Yalame, Thomas
  Schneider, N. Asokan
Categories: cs.CR cs.LG
Comments: European Symposium on Research in Computer Security (ESORICS), 2024
\\ ( https://arxiv.org/abs/2308.09552 ,  3096kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15640 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 04:28:46 GMT   (945kb,D)

Title: Identifying Constitutive Parameters for Complex Hyperelastic Materials
  using Physics-Informed Neural Networks
Authors: Siyuan Song, Hanxun Jin
Categories: cond-mat.mtrl-sci cs.LG
Comments: 28 pages, 5 figures, 1 table
\\ ( https://arxiv.org/abs/2308.15640 ,  945kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08630 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 01:18:31 GMT   (1080kb,D)

Title: PCN: A Deep Learning Approach to Jet Tagging Utilizing Novel Graph
  Construction Methods and Chebyshev Graph Convolutions
Authors: Yash Semlani, Mihir Relan, Krithik Ramesh
Categories: hep-ph cs.LG hep-ex
Comments: 14 pages, 2 figures, and 6 tables
\\ ( https://arxiv.org/abs/2309.08630 ,  1080kb)
------------------------------------------------------------------------------
\\
arXiv:2309.09743 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 13:14:45 GMT   (20408kb,D)

Title: The NFLikelihood: an unsupervised DNNLikelihood from Normalizing Flows
Authors: Humberto Reyes-Gonzalez, Riccardo Torre
Categories: hep-ph cs.LG hep-ex
Comments: 16 pages, 5 figures, 11 tables. Minor corrections and changes with
  respect to v1
\\ ( https://arxiv.org/abs/2309.09743 ,  20408kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13475
replaced with revised version Tue, 9 Apr 2024 01:26:58 GMT   (4724kb,D)

Title: Detecting and Mitigating System-Level Anomalies of Vision-Based
  Controllers
Authors: Aryaman Gupta, Kaustav Chakraborty, Somil Bansal
Categories: cs.RO cs.CV cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2309.13475 ,  4724kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14265
replaced with revised version Tue, 9 Apr 2024 13:33:30 GMT   (2082kb,D)

Title: Industrial Application of 6D Pose Estimation for Robotic Manipulation in
  Automotive Internal Logistics
Authors: Philipp Quentin, Dino Knoll, Daniel Goehring
Categories: cs.RO cs.CV cs.LG
Comments: Accepted for publication at IEEE International Conference on
  Automation Science and Engineering (CASE 2023)
\\ ( https://arxiv.org/abs/2309.14265 ,  2082kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14419 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 09:45:34 GMT   (271kb,D)

Title: On the expressivity of embedding quantum kernels
Authors: Elies Gil-Fuster, Jens Eisert, Vedran Dunjko
Categories: quant-ph cs.LG stat.ML
Comments: 17+12 pages, 4 figures
Journal-ref: Machine Learning: Science and Technology 5, 025003 (2024)
DOI: 10.1088/2632-2153/ad2f51
\\ ( https://arxiv.org/abs/2309.14419 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05468 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 14:41:39 GMT   (20359kb,D)

Title: ExIFFI and EIF+: Interpretability and Enhanced Generalizability to
  Extend the Extended Isolation Forest
Authors: Alessio Arcudi, Davide Frizzo, Chiara Masiero, Gian Antonio Susto
Categories: stat.ML cs.LG stat.AP
\\ ( https://arxiv.org/abs/2310.05468 ,  20359kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07166
replaced with revised version Tue, 9 Apr 2024 12:40:18 GMT   (3679kb,D)

Title: Anchor-based Multi-view Subspace Clustering with Hierarchical Feature
  Descent
Authors: Qiyuan Ou, Siwei Wang, Pei Zhang, Sihang Zhou, En Zhu
Categories: cs.CV cs.LG
DOI: 10.1016/j.inffus.2024.102225
\\ ( https://arxiv.org/abs/2310.07166 ,  3679kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10655
replaced with revised version Tue, 9 Apr 2024 07:50:13 GMT   (2813kb)

Title: Enhancing Trustworthiness in ML-Based Network Intrusion Detection with
  Uncertainty Quantification
Authors: Jacopo Talpini, Fabio Sartori, Marco Savi
Categories: cs.CR cs.LG
Comments: Preprint submitted to a journal
\\ ( https://arxiv.org/abs/2310.10655 ,  2813kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17032 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 10:34:38 GMT   (3978kb,D)

Title: Quantum Long Short-Term Memory (QLSTM) vs Classical LSTM in Time Series
  Forecasting: A Comparative Study in Solar Power Forecasting
Authors: Saad Zafar Khan, Nazeefa Muzammil, Salman Ghafoor, Haibat Khan, Syed
  Mohammad Hasan Zaidi, Abdulah Jeza Aljohani, Imran Aziz
Categories: quant-ph cs.LG
Comments: 33 pages, 9 figures
\\ ( https://arxiv.org/abs/2310.17032 ,  3978kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04806
replaced with revised version Mon, 8 Apr 2024 18:34:42 GMT   (6037kb,D)

Title: The PetShop Dataset -- Finding Causes of Performance Issues across
  Microservices
Authors: Michaela Hardt, William R. Orchard, Patrick Bl\"obaum, Shiva
  Kasiviswanathan, and Elke Kirschbaum
Categories: cs.DC cs.LG
Comments: 22 pages, 6 figures, 10 tables, for associated git repo see
  https://github.com/amazon-science/petshop-root-cause-analysis/, to be
  published in Proceedings of Machine Learning Research vol 236, 2024, 3rd
  Conference on Causal Learning and Reasoning
ACM-class: E.0
\\ ( https://arxiv.org/abs/2311.04806 ,  6037kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12570 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 09:35:08 GMT   (959kb,D)

Title: BEND: Benchmarking DNA Language Models on biologically meaningful tasks
Authors: Frederikke Isa Marin, Felix Teufel, Marc Horlacher, Dennis Madsen,
  Dennis Pultz, Ole Winther, Wouter Boomsma
Categories: q-bio.GN cs.LG
Comments: 9 pages, 1 figure, 3 tables, code available at
  https://github.com/frederikkemarin/BEND, to be published in ICLR 2024
\\ ( https://arxiv.org/abs/2311.12570 ,  959kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05332
replaced with revised version Tue, 9 Apr 2024 04:38:33 GMT   (186kb,D)

Title: MPC-Inspired Reinforcement Learning for Verifiable Model-Free Control
Authors: Yiwen Lu, Zishuo Li, Yihan Zhou, Na Li, Yilin Mo
Categories: eess.SY cs.LG cs.RO cs.SY math.OC
\\ ( https://arxiv.org/abs/2312.05332 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09168
replaced with revised version Tue, 9 Apr 2024 15:47:56 GMT   (39769kb,D)

Title: DiffusionLight: Light Probes for Free by Painting a Chrome Ball
Authors: Pakkapon Phongthawee, Worameth Chinchuthakun, Nontaphat Sinsunthithet,
  Amit Raj, Varun Jampani, Pramook Khungurn, Supasorn Suwajanakorn
Categories: cs.CV cs.GR cs.LG
Comments: CVPR 2024 Oral. For more information and code, please visit our
  website https://diffusionlight.github.io/
ACM-class: I.3.3; I.4.8
\\ ( https://arxiv.org/abs/2312.09168 ,  39769kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09250
replaced with revised version Mon, 8 Apr 2024 22:40:01 GMT   (49209kb,D)

Title: Single Mesh Diffusion Models with Field Latents for Texture Generation
Authors: Thomas W. Mitchel, Carlos Esteves, Ameesh Makadia
Categories: cs.CV cs.GR cs.LG
Comments: CVPR 2024. Code and additional visualizations available:
  https://single-mesh-diffusion.github.io/
\\ ( https://arxiv.org/abs/2312.09250 ,  49209kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10634
replaced with revised version Tue, 9 Apr 2024 09:18:26 GMT   (11104kb,D)

Title: Anomaly Score: Evaluating Generative Models and Individual Generated
  Images based on Complexity and Vulnerability
Authors: Jaehui Hwang, Junghyuk Lee, Jong-Seok Lee
Categories: cs.CV cs.LG
Comments: Accepted in CVPR 2024
\\ ( https://arxiv.org/abs/2312.10634 ,  11104kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01168
replaced with revised version Tue, 9 Apr 2024 07:46:50 GMT   (543kb,D)

Title: FedQV: Leveraging Quadratic Voting in Federated Learning
Authors: Tianyue Chu and Nikolaos Laoutaris
Categories: cs.CR cs.LG
Comments: Please cite the ACM SIGMETRICS'24 version of this paper
\\ ( https://arxiv.org/abs/2401.01168 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01810 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 16:11:31 GMT   (657kb,D)

Title: Misspecification uncertainties in near-deterministic regression
Authors: Thomas D Swinburne and Danny Perez
Categories: stat.ML cs.LG physics.data-an
\\ ( https://arxiv.org/abs/2402.01810 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10069
replaced with revised version Tue, 9 Apr 2024 15:47:15 GMT   (2307kb,D)

Title: Learning fast changing slow in spiking neural networks
Authors: Cristiano Capone and Paolo Muratore
Categories: cs.NE cs.LG
Comments: 14 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.10069 ,  2307kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02968 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 06:07:17 GMT   (282kb,D)

Title: Hamiltonian Property Testing
Authors: Andreas Bluhm, Matthias C. Caro, Aadil Oufkir
Categories: quant-ph cs.CC cs.DS cs.IT cs.LG math.IT
Comments: 39+21 pages, 3 figures; improved upper bounds, added tolerant tester,
  corrected some typos
\\ ( https://arxiv.org/abs/2403.02968 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13358
replaced with revised version Tue, 9 Apr 2024 07:55:41 GMT   (2181kb,D)

Title: GeRM: A Generalist Robotic Model with Mixture-of-experts for Quadruped
  Robot
Authors: Wenxuan Song, Han Zhao, Pengxiang Ding, Can Cui, Shangke Lyu, Yaning
  Fan, Donglin Wang
Categories: cs.RO cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.13358 ,  2181kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01448 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 03:47:27 GMT   (15616kb)

Title: Prior Frequency Guided Diffusion Model for Limited Angle (LA)-CBCT
  Reconstruction
Authors: Jiacheng Xie, Hua-Chieh Shao, Yunxiang Li, You Zhang
Categories: physics.med-ph cs.LG
Comments: 20 pages, 8 figures, submitted to Physics in Medicine & Biology
\\ ( https://arxiv.org/abs/2404.01448 ,  15616kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05678 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 16:17:14 GMT   (557kb,D)

Title: Flexible Fairness Learning via Inverse Conditional Permutation
Authors: Yuheng Lai, Leying Guan
Categories: stat.ML cs.CY cs.LG
\\ ( https://arxiv.org/abs/2404.05678 ,  557kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
