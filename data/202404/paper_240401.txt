Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 80021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年4月1日 12:46
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Thu 28 Mar 24 18:00:00 GMT  to  Fri 29 Mar 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2403.19760
Date: Thu, 28 Mar 2024 18:19:38 GMT   (661kb,D)

Title: Leveraging Counterfactual Paths for Contrastive Explanations of POMDP
  Policies
Authors: Benjamin Kraske, Zakariya Laouar, and Zachary Sunberg
Categories: cs.AI cs.HC
Comments: 5 pages, 1 figure
\\
  As humans come to rely on autonomous systems more, ensuring the transparency
of such systems is important to their continued adoption. Explainable
Artificial Intelligence (XAI) aims to reduce confusion and foster trust in
systems by providing explanations of agent behavior. Partially observable
Markov decision processes (POMDPs) provide a flexible framework capable of
reasoning over transition and state uncertainty, while also being amenable to
explanation. This work investigates the use of user-provided counterfactuals to
generate contrastive explanations of POMDP policies. Feature expectations are
used as a means of contrasting the performance of these policies. We
demonstrate our approach in a Search and Rescue (SAR) setting. We analyze and
discuss the associated challenges through two case studies.
\\ ( https://arxiv.org/abs/2403.19760 ,  661kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19790
Date: Thu, 28 Mar 2024 19:17:07 GMT   (3576kb,D)

Title: Bespoke Large Language Models for Digital Triage Assistance in Mental
  Health Care
Authors: Niall Taylor, Andrey Kormilitzin, Isabelle Lorge, Alejo
  Nevado-Holgado, Dan W Joyce
Categories: cs.AI
\\
  Contemporary large language models (LLMs) may have utility for processing
unstructured, narrative free-text clinical data contained in electronic health
records (EHRs) -- a particularly important use-case for mental health where a
majority of routinely-collected patient data lacks structured, machine-readable
content.
  A significant problem for the the United Kingdom's National Health Service
(NHS) are the long waiting lists for specialist mental healthcare. According to
NHS data, in each month of 2023, there were between 370,000 and 470,000
individual new referrals into secondary mental healthcare services. Referrals
must be triaged by clinicians, using clinical information contained in the
patient's EHR to arrive at a decision about the most appropriate mental
healthcare team to assess and potentially treat these patients.
  The ability to efficiently recommend a relevant team by ingesting potentially
voluminous clinical notes could help services both reduce referral waiting
times and with the right technology, improve the evidence available to justify
triage decisions.
  We present and evaluate three different approaches for LLM-based, end-to-end
ingestion of variable-length clinical EHR data to assist clinicians when
triaging referrals. Our model is able to deliver triage recommendations
consistent with existing clinical practices and it's architecture was
implemented on a single GPU, making it practical for implementation in
resource-limited NHS environments where private implementations of LLM
technology will be necessary to ensure confidential clinical data is
appropriately controlled and governed.
\\ ( https://arxiv.org/abs/2403.19790 ,  3576kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19826
Date: Thu, 28 Mar 2024 20:34:02 GMT   (6197kb,D)

Title: Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic
  Segmentation
Authors: Qitian Ma and Shyam Nanda Rai and Carlo Masone and Tatiana Tommasi
Categories: cs.AI
\\
  In the domain of computer vision, semantic segmentation emerges as a
fundamental application within machine learning, wherein individual pixels of
an image are classified into distinct semantic categories. This task transcends
traditional accuracy metrics by incorporating uncertainty quantification, a
critical measure for assessing the reliability of each segmentation prediction.
Such quantification is instrumental in facilitating informed decision-making,
particularly in applications where precision is paramount. Within this nuanced
framework, the metric known as PAvPU (Patch Accuracy versus Patch Uncertainty)
has been developed as a specialized tool for evaluating entropy-based
uncertainty in image segmentation tasks. However, our investigation identifies
three core deficiencies within the PAvPU framework and proposes robust
solutions aimed at refining the metric. By addressing these issues, we aim to
enhance the reliability and applicability of uncertainty quantification,
especially in scenarios that demand high levels of safety and accuracy, thus
contributing to the advancement of semantic segmentation methodologies in
critical applications.
\\ ( https://arxiv.org/abs/2403.19826 ,  6197kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19856
Date: Thu, 28 Mar 2024 22:05:32 GMT   (28kb,D)

Title: Towards a Brazilian History Knowledge Graph
Authors: Valeria de Paiva, Alexandre Rademaker
Categories: cs.AI cs.DL
\\
  This short paper describes the first steps in a project to construct a
knowledge graph for Brazilian history based on the Brazilian Dictionary of
Historical Biographies (DHBB) and Wikipedia/Wikidata. We contend that large
repositories of Brazilian-named entities (people, places, organizations, and
political events and movements) would be beneficial for extracting information
from Portuguese texts. We show that many of the terms/entities described in the
DHBB do not have corresponding concepts (or Q items) in Wikidata, the largest
structured database of entities associated with Wikipedia. We describe previous
work on extracting information from the DHBB and outline the steps to construct
a Wikidata-based historical knowledge graph.
\\ ( https://arxiv.org/abs/2403.19856 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19857
Date: Thu, 28 Mar 2024 22:06:04 GMT   (6080kb,D)

Title: LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal
  Sensor Traces
Authors: Xiaomin Ouyang and Mani Srivastava
Categories: cs.AI
Comments: 6 pages
\\
  Most studies on machine learning in sensing systems focus on low-level
perception tasks that process raw sensory data within a short time window.
However, many practical applications, such as human routine modeling and
occupancy tracking, require high-level reasoning abilities to comprehend
concepts and make inferences based on long-term sensor traces. Existing machine
learning-based approaches for handling such complex tasks struggle to
generalize due to the limited training samples and the high dimensionality of
sensor traces, necessitating the integration of human knowledge for designing
first-principle models or logic reasoning methods. We pose a fundamental
question: Can we harness the reasoning capabilities and world knowledge of
Large Language Models (LLMs) to recognize complex events from long-term
spatiotemporal sensor traces? To answer this question, we design an effective
prompting framework for LLMs on high-level reasoning tasks, which can handle
traces from the raw sensor data as well as the low-level perception results. We
also design two strategies to enhance performance with long sensor traces,
including summarization before reasoning and selective inclusion of historical
traces. Our framework can be implemented in an edge-cloud setup, running small
LLMs on the edge for data summarization and performing high-level reasoning on
the cloud for privacy preservation. The results show that LLMSense can achieve
over 80\% accuracy on two high-level reasoning tasks such as dementia diagnosis
with behavior traces and occupancy tracking with environmental sensor traces.
This paper provides a few insights and guidelines for leveraging LLM for
high-level reasoning on sensor traces and highlights several directions for
future work.
\\ ( https://arxiv.org/abs/2403.19857 ,  6080kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19881
Date: Thu, 28 Mar 2024 23:31:25 GMT   (1187kb,D)

Title: IME: Integrating Multi-curvature Shared and Specific Embedding for
  Temporal Knowledge Graph Completion
Authors: Jiapu Wang, Zheng Cui, Boyue Wang, Shirui Pan, Junbin Gao, Baocai Yin,
  Wen Gao
Categories: cs.AI
\\
  Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing
for a precise capture of the evolution of knowledge and reflecting the dynamic
nature of the real world. Typically, TKGs contain complex geometric structures,
with various geometric structures interwoven. However, existing Temporal
Knowledge Graph Completion (TKGC) methods either model TKGs in a single space
or neglect the heterogeneity of different curvature spaces, thus constraining
their capacity to capture these intricate geometric structures. In this paper,
we propose a novel Integrating Multi-curvature shared and specific Embedding
(IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature
spaces, including hyperspherical, hyperbolic, and Euclidean spaces.
Subsequently, IME incorporates two key properties, namely space-shared property
and space-specific property. The space-shared property facilitates the learning
of commonalities across different curvature spaces and alleviates the spatial
gap caused by the heterogeneous nature of multi-curvature spaces, while the
space-specific property captures characteristic features. Meanwhile, IME
proposes an Adjustable Multi-curvature Pooling (AMP) approach to effectively
retain important information. Furthermore, IME innovatively designs similarity,
difference, and structure loss functions to attain the stated objective.
Experimental results clearly demonstrate the superior performance of IME over
existing state-of-the-art TKGC models.
\\ ( https://arxiv.org/abs/2403.19881 ,  1187kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19883
Date: Thu, 28 Mar 2024 23:40:20 GMT   (124kb,D)

Title: Policy-Space Search: Equivalences, Improvements, and Compression
Authors: Frederico Messa, Andr\'e Grahl Pereira
Categories: cs.AI
\\
  Fully-observable non-deterministic (FOND) planning is at the core of
artificial intelligence planning with uncertainty. It models uncertainty
through actions with non-deterministic effects. A* with Non-Determinism (AND*)
(Messa and Pereira, 2023) is a FOND planner that generalizes A* (Hart et al.,
1968) for FOND planning. It searches for a solution policy by performing an
explicit heuristic search on the policy space of the FOND task. In this paper,
we study and improve the performance of the policy-space search performed by
AND*. We present a polynomial-time procedure that constructs a solution policy
given just the set of states that should be mapped. This procedure, together
with a better understanding of the structure of FOND policies, allows us to
present three concepts of equivalences between policies. We use policy
equivalences to prune part of the policy search space, making AND*
substantially more effective in solving FOND tasks. We also study the impact of
taking into account structural state-space symmetries to strengthen the
detection of equivalence policies and the impact of performing the search with
satisficing techniques. We apply a recent technique from the group theory
literature to better compute structural state-space symmetries. Finally, we
present a solution compressor that, given a policy defined over complete
states, finds a policy that unambiguously represents it using the minimum
number of partial states. AND* with the introduced techniques generates, on
average, two orders of magnitude fewer policies to solve FOND tasks. These
techniques allow explicit policy-space search to be competitive in terms of
both coverage and solution compactness with other state-of-the-art FOND
planners.
\\ ( https://arxiv.org/abs/2403.19883 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19941
Date: Fri, 29 Mar 2024 02:49:15 GMT   (308kb,D)

Title: Diverse Feature Learning by Self-distillation and Reset
Authors: Sejik Park
Categories: cs.AI
Comments: 15 pages, 6 Figures
\\
  Our paper addresses the problem of models struggling to learn diverse
features, due to either forgetting previously learned features or failing to
learn new ones. To overcome this problem, we introduce Diverse Feature Learning
(DFL), a method that combines an important feature preservation algorithm with
a new feature learning algorithm. Specifically, for preserving important
features, we utilize self-distillation in ensemble models by selecting the
meaningful model weights observed during training. For learning new features,
we employ reset that involves periodically re-initializing part of the model.
As a result, through experiments with various models on the image
classification, we have identified the potential for synergistic effects
between self-distillation and reset.
\\ ( https://arxiv.org/abs/2403.19941 ,  308kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19992
Date: Fri, 29 Mar 2024 06:09:24 GMT   (8638kb,D)

Title: MindArm: Mechanized Intelligent Non-Invasive Neuro-Driven Prosthetic Arm
  System
Authors: Maha Nawaz, Abdul Basit, Muhammad Shafique
Categories: cs.AI cs.HC cs.RO
Comments: 8 pages, 21 figures, paper submitted to IROS 24, authors affiliated
  to NYUAD
ACM-class: I.2.9
\\
  Currently, people with disability or difficulty to move their arms (referred
to as "patients") have very limited technological solutions to efficiently
address their physiological limitations. It is mainly due to two reasons: (1)
the non-invasive solutions like mind-controlled prosthetic devices are
typically very costly and require expensive maintenance; and (2) other
solutions require costly invasive brain surgery, which is high risk to perform,
expensive, and difficult to maintain. Therefore, current technological
solutions are not accessible for all patients with different financial
backgrounds. Toward this, we propose a low-cost technological solution called
MindArm, a mechanized intelligent non-invasive neuro-driven prosthetic arm
system. Our MindArm system employs a deep neural network (DNN) engine to
translate brain signals into the intended prosthetic arm motion, thereby
helping patients to perform many activities despite their physiological
limitations. Here, our MindArm system utilizes widely accessible and low-cost
surface electroencephalogram (EEG) electrodes coupled with an Open Brain
Computer Interface and UDP networking for acquiring brain signals and
transmitting them to the compute module for signal processing. In the compute
module, we run a trained DNN model to interpret normalized micro-voltage of the
brain signals, and then translate them into a prosthetic arm action via serial
communication seamlessly. The experimental results on a fully working prototype
demonstrate that, from the three defined actions, our MindArm system achieves
positive success rates, i.e., 91\% for idle/stationary, 85\% for shake hand,
and 84\% for pick-up cup. This demonstrates that our MindArm provides a novel
approach for an alternate low-cost mind-controlled prosthetic devices for all
patients.
\\ ( https://arxiv.org/abs/2403.19992 ,  8638kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19995
Date: Fri, 29 Mar 2024 06:22:37 GMT   (6449kb,D)

Title: Development of Compositionality and Generalization through Interactive
  Learning of Language and Action of Robots
Authors: Prasanna Vijayaraghavan, Jeffrey Frederic Queisser, Sergio Verduzco
  Flores, Jun Tani
Categories: cs.AI cs.CL cs.RO
Comments: 59 pages, 6 figures, 10 supplementary figures
MSC-class: 68T35, 68T40
ACM-class: I.2.9
\\
  Humans excel at applying learned behavior to unlearned situations. A crucial
component of this generalization behavior is our ability to compose/decompose a
whole into reusable parts, an attribute known as compositionality. One of the
fundamental questions in robotics concerns this characteristic. "How can
linguistic compositionality be developed concomitantly with sensorimotor skills
through associative learning, particularly when individuals only learn partial
linguistic compositions and their corresponding sensorimotor patterns?" To
address this question, we propose a brain-inspired neural network model that
integrates vision, proprioception, and language into a framework of predictive
coding and active inference, based on the free-energy principle. The
effectiveness and capabilities of this model were assessed through various
simulation experiments conducted with a robot arm. Our results show that
generalization in learning to unlearned verb-noun compositions, is
significantly enhanced when training variations of task composition are
increased. We attribute this to self-organized compositional structures in
linguistic latent state space being influenced significantly by sensorimotor
learning. Ablation studies show that visual attention and working memory are
essential to accurately generate visuo-motor sequences to achieve
linguistically represented goals. These insights advance our understanding of
mechanisms underlying development of compositionality through interactions of
linguistic and sensorimotor experience.
\\ ( https://arxiv.org/abs/2403.19995 ,  6449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20089
Date: Fri, 29 Mar 2024 09:54:09 GMT   (127kb,D)

Title: Implications of the AI Act for Non-Discrimination Law and Algorithmic
  Fairness
Authors: Luca Deck, Jan-Laurin M\"uller, Conradin Braun, Domenique Zipperling,
  Niklas K\"uhl
Categories: cs.AI
\\
  The topic of fairness in AI, as debated in the FATE (Fairness,
Accountability, Transparency, and Ethics in AI) communities, has sparked
meaningful discussions in the past years. However, from a legal perspective,
particularly from European Union law, many open questions remain. Whereas
algorithmic fairness aims to mitigate structural inequalities at the design
level, European non-discrimination law is tailored to individual cases of
discrimination after an AI model has been deployed. The AI Act might present a
tremendous step towards bridging these two concepts by shifting
non-discrimination responsibilities into the design stage of AI models. Based
on an integrative reading of the AI Act, we comment on legal as well as
technical enforcement problems and propose practical implications on bias
detection and bias correction in order to specify and comply with specific
technical requirements.
\\ ( https://arxiv.org/abs/2403.20089 ,  127kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20097
Date: Fri, 29 Mar 2024 10:23:18 GMT   (2981kb,D)

Title: ITCMA: A Generative Agent Based on a Computational Consciousness
  Structure
Authors: Hanzhong Zhang, Jibin Yin, Haoyang Wang, Ziwei Xiang
Categories: cs.AI cs.HC q-bio.NC
Comments: 20 pages, 11 figures
ACM-class: I.2; J.4
\\
  Large Language Models (LLMs) still face challenges in tasks requiring
understanding implicit instructions and applying common-sense knowledge. In
such scenarios, LLMs may require multiple attempts to achieve human-level
performance, potentially leading to inaccurate responses or inferences in
practical environments, affecting their long-term consistency and behavior.
This paper introduces the Internal Time-Consciousness Machine (ITCM), a
computational consciousness structure. We further propose the ITCM-based Agent
(ITCMA), which supports behavior generation and reasoning in open-world
settings. ITCMA enhances LLMs' ability to understand implicit instructions and
apply common-sense knowledge by considering agents' interaction and reasoning
with the environment. Evaluations in the Alfworld environment show that trained
ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even
untrained ITCMA achieves a 96% task completion rate on the seen set, 5% higher
than SOTA, indicating its superiority over traditional intelligent agents in
utility and generalization. In real-world tasks with quadruped robots, the
untrained ITCMA achieves an 85% task completion rate, which is close to its
performance in the unseen set, demonstrating its comparable utility in
real-world settings.
\\ ( https://arxiv.org/abs/2403.20097 ,  2981kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20127
Date: Fri, 29 Mar 2024 11:33:34 GMT   (485kb,D)

Title: The Impact of Prompts on Zero-Shot Detection of AI-Generated Text
Authors: Kaito Taguchi, Yujie Gu, and Kouichi Sakurai
Categories: cs.AI
\\
  In recent years, there have been significant advancements in the development
of Large Language Models (LLMs). While their practical applications are now
widespread, their potential for misuse, such as generating fake news and
committing plagiarism, has posed significant concerns. To address this issue,
detectors have been developed to evaluate whether a given text is
human-generated or AI-generated. Among others, zero-shot detectors stand out as
effective approaches that do not require additional training data and are often
likelihood-based. In chat-based applications, users commonly input prompts and
utilize the AI-generated texts. However, zero-shot detectors typically analyze
these texts in isolation, neglecting the impact of the original prompts. It is
conceivable that this approach may lead to a discrepancy in likelihood
assessments between the text generation phase and the detection phase. So far,
there remains an unverified gap concerning how the presence or absence of
prompts impacts detection accuracy for zero-shot detectors. In this paper, we
introduce an evaluative framework to empirically analyze the impact of prompts
on the detection accuracy of AI-generated text. We assess various zero-shot
detectors using both white-box detection, which leverages the prompt, and
black-box detection, which operates without prompt information. Our experiments
reveal the significant influence of prompts on detection accuracy. Remarkably,
compared with black-box detection without prompts, the white-box methods using
prompts demonstrate an increase in AUC of at least $0.1$ across all zero-shot
detectors tested. Code is available:
\url{https://github.com/kaito25atugich/Detector}.
\\ ( https://arxiv.org/abs/2403.20127 ,  485kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20137
Date: Fri, 29 Mar 2024 12:15:06 GMT   (124kb,D)

Title: Accurate Block Quantization in LLMs with Outliers
Authors: Nikita Trukhanov and Ilya Soloveychik
Categories: cs.AI cs.AR cs.NA math.NA
\\
  The demand for inference on extremely large scale LLMs has seen enormous
growth in the recent months. It made evident the colossal shortage of dedicated
hardware capable of efficient and fast processing of the involved compute and
memory movement. The problem is aggravated by the exploding raise in the
lengths of the sequences being processed, since those require efficient on-chip
storage of the KV-cache of size proportional to the sequence length. To make
the required compute feasible and fit the involved data into available memory,
numerous quantization techniques have been proposed that allow accurate
quantization for both weights and activations. One of the main recent
breakthroughs in this direction was introduction of the family of Block
Floating Point (BFP) formats characterized by a block of mantissas with a
shared scale factor. These enable memory- power-, and compute- efficient
hardware support of the tensor operations and provide extremely good
quantization accuracy. The main issues preventing widespread application of
block formats is caused by the presence of outliers in weights and activations
since those affect the accuracy of the other values in the same block. In this
paper, we focus on the most critical problem of limited KV-cache storage. We
propose a novel approach enabling usage of low precision BFP formats without
compromising the resulting model accuracy. We exploit the common channel-wise
patterns exhibited by the outliers to rearrange them in such a way, that their
quantization quality is significantly improved. The methodology yields 2x
savings in the memory footprint without significant degradation of the model's
accuracy. Importantly, the rearrangement of channels happens at the compile
time and thus has no impact on the inference latency.
\\ ( https://arxiv.org/abs/2403.20137 ,  124kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20151
Date: Fri, 29 Mar 2024 12:46:07 GMT   (5459kb)

Title: A Learning-based Incentive Mechanism for Mobile AIGC Service in
  Decentralized Internet of Vehicles
Authors: Jiani Fan, Minrui Xu, Ziyao Liu, Huanyi Ye, Chaojie Gu, Dusit Niyato,
  Kwok-Yan Lam
Categories: cs.AI
Comments: 2023 IEEE 98th Vehicular Technology Conference (VTC2023-Fall)
DOI: 10.1109/VTC2023-Fall60731.2023.10333689
\\
  Artificial Intelligence-Generated Content (AIGC) refers to the paradigm of
automated content generation utilizing AI models. Mobile AIGC services in the
Internet of Vehicles (IoV) network have numerous advantages over traditional
cloud-based AIGC services, including enhanced network efficiency, better
reconfigurability, and stronger data security and privacy. Nonetheless, AIGC
service provisioning frequently demands significant resources. Consequently,
resource-constrained roadside units (RSUs) face challenges in maintaining a
heterogeneous pool of AIGC services and addressing all user service requests
without degrading overall performance. Therefore, in this paper, we propose a
decentralized incentive mechanism for mobile AIGC service allocation, employing
multi-agent deep reinforcement learning to find the balance between the supply
of AIGC services on RSUs and user demand for services within the IoV context,
optimizing user experience and minimizing transmission latency. Experimental
results demonstrate that our approach achieves superior performance compared to
other baseline models.
\\ ( https://arxiv.org/abs/2403.20151 ,  5459kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20177
Date: Fri, 29 Mar 2024 13:47:47 GMT   (1202kb)

Title: Artificial consciousness. Some logical and conceptual preliminaries
Authors: K. Evers, M. Farisco, R. Chatila, B. D. Earp, I. T. Freire, F. Hamker,
  E. Nemeth, P. F. M. J. Verschure, M. Khamassi
Categories: cs.AI cs.RO q-bio.NC
\\
  Is artificial consciousness theoretically possible? Is it plausible? If so,
is it technically feasible? To make progress on these questions, it is
necessary to lay some groundwork clarifying the logical and empirical
conditions for artificial consciousness to arise and the meaning of relevant
terms involved. Consciousness is a polysemic word: researchers from different
fields, including neuroscience, Artificial Intelligence, robotics, and
philosophy, among others, sometimes use different terms in order to refer to
the same phenomena or the same terms to refer to different phenomena. In fact,
if we want to pursue artificial consciousness, a proper definition of the key
concepts is required. Here, after some logical and conceptual preliminaries, we
argue for the necessity of using dimensions and profiles of consciousness for a
balanced discussion about their possible instantiation or realisation in
artificial systems. Our primary goal in this paper is to review the main
theoretical questions that arise in the domain of artificial consciousness. On
the basis of this review, we propose to assess the issue of artificial
consciousness within a multidimensional account. The theoretical possibility of
artificial consciousness is already presumed within some theoretical
frameworks; however, empirical possibility cannot simply be deduced from these
frameworks but needs independent empirical validation. We break down the
complexity of consciousness by identifying constituents, components, and
dimensions, and reflect pragmatically about the general challenges confronting
the creation of artificial consciousness. Despite these challenges, we outline
a research strategy for showing how "awareness" as we propose to understand it
could plausibly be realised in artificial systems.
\\ ( https://arxiv.org/abs/2403.20177 ,  1202kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20204
Date: Fri, 29 Mar 2024 14:32:41 GMT   (1428kb,D)

Title: The Future of Combating Rumors? Retrieval, Discrimination, and
  Generation
Authors: Junhao Xu, Longdi Xian, Zening Liu, Mingliang Chen, Qiuyang Yin,
  Fenghua Song
Categories: cs.AI
Comments: 8 pages
MSC-class: 68T99
\\
  Artificial Intelligence Generated Content (AIGC) technology development has
facilitated the creation of rumors with misinformation, impacting societal,
economic, and political ecosystems, challenging democracy. Current rumor
detection efforts fall short by merely labeling potentially misinformation
(classification task), inadequately addressing the issue, and it is unrealistic
to have authoritative institutions debunk every piece of information on social
media. Our proposed comprehensive debunking process not only detects rumors but
also provides explanatory generated content to refute the authenticity of the
information. The Expert-Citizen Collective Wisdom (ECCW) module we designed
aensures high-precision assessment of the credibility of information and the
retrieval module is responsible for retrieving relevant knowledge from a
Real-time updated debunking database based on information keywords. By using
prompt engineering techniques, we feed results and knowledge into a LLM (Large
Language Model), achieving satisfactory discrimination and explanatory effects
while eliminating the need for fine-tuning, saving computational costs, and
contributing to debunking efforts.
\\ ( https://arxiv.org/abs/2403.20204 ,  1428kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20212
Date: Fri, 29 Mar 2024 14:47:54 GMT   (2495kb,D)

Title: On Size and Hardness Generalization in Unsupervised Learning for the
  Travelling Salesman Problem
Authors: Yimeng Min, Carla P. Gomes
Categories: cs.AI cs.LG
\\
  We study the generalization capability of Unsupervised Learning in solving
the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN)
trained with a surrogate loss function to generate an embedding for each node.
We use these embeddings to construct a heat map that indicates the likelihood
of each edge being part of the optimal route. We then apply local search to
generate our final predictions. Our investigation explores how different
training instance sizes, embedding dimensions, and distributions influence the
outcomes of Unsupervised Learning methods. Our results show that training with
larger instance sizes and increasing embedding dimensions can build a more
effective representation, enhancing the model's ability to solve TSP.
Furthermore, in evaluating generalization across different distributions, we
first determine the hardness of various distributions and explore how different
hardnesses affect the final results. Our findings suggest that models trained
on harder instances exhibit better generalization capabilities, highlighting
the importance of selecting appropriate training instances in solving TSP using
Unsupervised Learning.
\\ ( https://arxiv.org/abs/2403.20212 ,  2495kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20234
Date: Fri, 29 Mar 2024 15:23:30 GMT   (11713kb,D)

Title: Artificial Neural Networks-based Real-time Classification of ENG Signals
  for Implanted Nerve Interfaces
Authors: ntonio Coviello, Francesco Linsalata, Umberto Spagnolini, Maurizio
  Magarini
Categories: cs.AI
\\
  Neuropathies are gaining higher relevance in clinical settings, as they risk
permanently jeopardizing a person's life. To support the recovery of patients,
the use of fully implanted devices is emerging as one of the most promising
solutions. However, these devices, even if becoming an integral part of a fully
complex neural nanonetwork system, pose numerous challenges. In this article,
we address one of them, which consists of the classification of motor/sensory
stimuli. The task is performed by exploring four different types of artificial
neural networks (ANNs) to extract various sensory stimuli from the
electroneurographic (ENG) signal measured in the sciatic nerve of rats.
Different sizes of the data sets are considered to analyze the feasibility of
the investigated ANNs for real-time classification through a comparison of
their performance in terms of accuracy, F1-score, and prediction time. The
design of the ANNs takes advantage of the modelling of the ENG signal as a
multiple-input multiple-output (MIMO) system to describe the measures taken by
state-of-the-art implanted nerve interfaces. These are based on the use of
multi-contact cuff electrodes to achieve nanoscale spatial discrimination of
the nerve activity. The MIMO ENG signal model is another contribution of this
paper. Our results show that some ANNs are more suitable for real-time
applications, being capable of achieving accuracies over $90\%$ for signal
windows of $100$ and $200\,$ms with a low enough processing time to be
effective for pathology recovery.
\\ ( https://arxiv.org/abs/2403.20234 ,  11713kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20306
Date: Fri, 29 Mar 2024 17:22:48 GMT   (282kb,D)

Title: Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM
  Inference
Authors: Jovan Stojkovic, Esha Choukse, Chaojie Zhang, Inigo Goiri, Josep
  Torrellas
Categories: cs.AI cs.AR cs.DC
Comments: 6 pages, 15 figures
ACM-class: C.0; I.2
\\
  With the ubiquitous use of modern large language models (LLMs) across
industries, the inference serving for these models is ever expanding. Given the
high compute and memory requirements of modern LLMs, more and more
top-of-the-line GPUs are being deployed to serve these models. Energy
availability has come to the forefront as the biggest challenge for data center
expansion to serve these models. In this paper, we present the trade-offs
brought up by making energy efficiency the primary goal of LLM serving under
performance SLOs. We show that depending on the inputs, the model, and the
service-level agreements, there are several knobs available to the LLM
inference provider to use for being energy efficient. We characterize the
impact of these knobs on the latency, throughput, as well as the energy. By
exploring these trade-offs, we offer valuable insights into optimizing energy
usage without compromising on performance, thereby paving the way for
sustainable and cost-effective LLM deployment in data center environments.
\\ ( https://arxiv.org/abs/2403.20306 ,  282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19708
Date: Sat, 23 Mar 2024 10:42:49 GMT   (635kb,D)

Title: AttentionStore: Cost-effective Attention Reuse across Multi-turn
  Conversations in Large Language Model Serving
Authors: Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic,
  Junbo Deng, Xingkun Yang, Zhou Yu, Pengfei Zuo
Categories: cs.CL cs.LG
\\
  Interacting with humans through multi-turn conversations is a fundamental
feature of large language models (LLMs). However, existing LLM serving engines
for executing multi-turn conversations are inefficient due to the need to
repeatedly compute the key-value (KV) caches of historical tokens, incurring
high serving costs. To address the problem, this paper proposes AttentionStore,
a new attention mechanism that enables the reuse of KV caches (i.e., attention
reuse) across multi-turn conversations, significantly reducing the repetitive
computation overheads. AttentionStore maintains a hierarchical KV caching
system that leverages cost-effective memory/storage mediums to save KV caches
for all requests. To reduce KV cache access overheads from slow mediums,
AttentionStore employs layer-wise pre-loading and asynchronous saving schemes
to overlap the KV cache access with the GPU computation. To ensure that the KV
caches to be accessed are placed in the fastest hierarchy, AttentionStore
employs scheduler-aware fetching and eviction schemes to consciously place the
KV caches in different layers based on the hints from the inference job
scheduler. To avoid the invalidation of the saved KV caches incurred by context
window overflow, AttentionStore enables the saved KV caches to remain valid via
decoupling the positional encoding and effectively truncating the KV caches.
Extensive experimental results demonstrate that AttentionStore significantly
decreases the time to the first token (TTFT) by up to 88%, improves the prompt
prefilling throughput by 8.2$\times$ for multi-turn conversations, and reduces
the end-to-end inference cost by up to 56%. For long sequence inference,
AttentionStore reduces the TTFT by up to 95% and improves the prompt prefilling
throughput by 22$\times$.
\\ ( https://arxiv.org/abs/2403.19708 ,  635kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19710
Date: Mon, 25 Mar 2024 18:32:44 GMT   (2185kb,D)

Title: STRUM-LLM: Attributed and Structured Contrastive Summarization
Authors: Beliz Gunel, James B. Wendt, Jing Xie, Yichao Zhou, Nguyen Vo, Zachary
  Fisher, Sandeep Tata
Categories: cs.CL cs.AI cs.IR cs.LG
\\
  Users often struggle with decision-making between two options (A vs B), as it
usually requires time-consuming research across multiple web pages. We propose
STRUM-LLM that addresses this challenge by generating attributed, structured,
and helpful contrastive summaries that highlight key differences between the
two options. STRUM-LLM identifies helpful contrast: the specific attributes
along which the two options differ significantly and which are most likely to
influence the user's decision. Our technique is domain-agnostic, and does not
require any human-labeled data or fixed attribute list as supervision.
STRUM-LLM attributes all extractions back to the input sources along with
textual evidence, and it does not have a limit on the length of input sources
that it can process. STRUM-LLM Distilled has 100x more throughput than the
models with comparable performance while being 10x smaller. In this paper, we
provide extensive evaluations for our method and lay out future directions for
our currently deployed system.
\\ ( https://arxiv.org/abs/2403.19710 ,  2185kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19713
Date: Tue, 26 Mar 2024 14:09:49 GMT   (822kb)

Title: NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential
  Identification
Authors: Jingyuan Wang, Shengdong Xu, Yang Yang
Categories: cs.CL cs.LG
\\
  This report provide a detailed description of the method that we proposed in
the TRAC-2024 Offline Harm Potential dentification which encloses two
sub-tasks. The investigation utilized a rich dataset comprised of social media
comments in several Indian languages, annotated with precision by expert judges
to capture the nuanced implications for offline context harm. The objective
assigned to the participants was to design algorithms capable of accurately
assessing the likelihood of harm in given situations and identifying the most
likely target(s) of offline harm. Our approach ranked second in two separate
tracks, with F1 values of 0.73 and 0.96 respectively. Our method principally
involved selecting pretrained models for finetuning, incorporating contrastive
learning techniques, and culminating in an ensemble approach for the test set.
\\ ( https://arxiv.org/abs/2403.19713 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19716
Date: Wed, 27 Mar 2024 17:41:16 GMT   (16942kb,D)

Title: Capability-aware Prompt Reformulation Learning for Text-to-Image
  Generation
Authors: Jingtao Zhan, Qingyao Ai, Yiqun Liu, Jia Chen, Shaoping Ma
Categories: cs.CL cs.AI cs.CV cs.IR
Comments: Accepted at SIGIR 2024
\\
  Text-to-image generation systems have emerged as revolutionary tools in the
realm of artistic creation, offering unprecedented ease in transforming textual
prompts into visual art. However, the efficacy of these systems is intricately
linked to the quality of user-provided prompts, which often poses a challenge
to users unfamiliar with prompt crafting. This paper addresses this challenge
by leveraging user reformulation data from interaction logs to develop an
automatic prompt reformulation model. Our in-depth analysis of these logs
reveals that user prompt reformulation is heavily dependent on the individual
user's capability, resulting in significant variance in the quality of
reformulation pairs. To effectively use this data for training, we introduce
the Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively
integrates user capability into the reformulation process through two key
components: the Conditional Reformulation Model (CRM) and Configurable
Capability Features (CCF). CRM reformulates prompts according to a specified
user capability, as represented by CCF. The CCF, in turn, offers the
flexibility to tune and guide the CRM's behavior. This enables CAPR to
effectively learn diverse reformulation strategies across various user
capacities and to simulate high-capability user reformulation during inference.
Extensive experiments on standard text-to-image generation benchmarks showcase
CAPR's superior performance over existing baselines and its remarkable
robustness on unseen systems. Furthermore, comprehensive analyses validate the
effectiveness of different components. CAPR can facilitate user-friendly
interaction with text-to-image systems and make advanced artistic creation more
achievable for a broader range of users.
\\ ( https://arxiv.org/abs/2403.19716 ,  16942kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19723
Date: Thu, 28 Mar 2024 03:20:54 GMT   (9702kb,D)

Title: HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for
  Few-shot Complex Table Understanding
Authors: Rihui Jin, Yu Li, Guilin Qi, Nan Hu, Yuan-Fang Li, Jiaoyan Chen,
  Jianan Wang, Yongrui Chen, Dehai Min
Categories: cs.CL cs.AI cs.DB cs.MM
\\
  Table understanding (TU) has achieved promising advancements, but it faces
the challenges of the scarcity of manually labeled tables and the presence of
complex table structures.To address these challenges, we propose HGT, a
framework with a heterogeneous graph (HG)-enhanced large language model (LLM)
to tackle few-shot TU tasks.It leverages the LLM by aligning the table
semantics with the LLM's parametric knowledge through soft prompts and
instruction turning and deals with complex tables by a multi-task pre-training
scheme involving three novel multi-granularity self-supervised HG pre-training
objectives.We empirically demonstrate the effectiveness of HGT, showing that it
outperforms the SOTA for few-shot complex TU on several benchmarks.
\\ ( https://arxiv.org/abs/2403.19723 ,  9702kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19725
Date: Thu, 28 Mar 2024 07:33:53 GMT   (843kb,D)

Title: MUGC: Machine Generated versus User Generated Content Detection
Authors: Yaqi Xie, Anjali Rawal, Yujing Cen, Dixuan Zhao, Sunil K Narang, Shanu
  Sushmita
Categories: cs.CL cs.AI cs.LG
Comments: 11 pages, 16 figures
\\
  As advanced modern systems like deep neural networks (DNNs) and generative AI
continue to enhance their capabilities in producing convincing and realistic
content, the need to distinguish between user-generated and machine generated
content is becoming increasingly evident. In this research, we undertake a
comparative evaluation of eight traditional machine-learning algorithms to
distinguish between machine-generated and human-generated data across three
diverse datasets: Poems, Abstracts, and Essays. Our results indicate that
traditional methods demonstrate a high level of accuracy in identifying
machine-generated data, reflecting the documented effectiveness of popular
pre-trained models like RoBERT. We note that machine-generated texts tend to be
shorter and exhibit less word variety compared to human-generated content.
While specific domain-related keywords commonly utilized by humans, albeit
disregarded by current LLMs (Large Language Models), may contribute to this
high detection accuracy, we show that deeper word representations like word2vec
can capture subtle semantic variances. Furthermore, readability, bias, moral,
and affect comparisons reveal a discernible contrast between machine-generated
and human generated content. There are variations in expression styles and
potentially underlying biases in the data sources (human and
machine-generated). This study provides valuable insights into the advancing
capacities and challenges associated with machine-generated content across
various domains.
\\ ( https://arxiv.org/abs/2403.19725 ,  843kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19726
Date: Thu, 28 Mar 2024 07:59:58 GMT   (40kb,D)

Title: A Benchmark Evaluation of Clinical Named Entity Recognition in French
Authors: Nesrine Bannour (STL), Christophe Servan (STL), Aur\'elie N\'ev\'eol
  (STL), Xavier Tannier (LIMICS)
Categories: cs.CL cs.AI q-bio.QM
Journal-ref: The 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024), May 2024,
  Torino, Italy
\\
  Background: Transformer-based language models have shown strong performance
on many Natural LanguageProcessing (NLP) tasks. Masked Language Models (MLMs)
attract sustained interest because they can be adaptedto different languages
and sub-domains through training or fine-tuning on specific corpora while
remaining lighterthan modern Large Language Models (LLMs). Recently, several
MLMs have been released for the biomedicaldomain in French, and experiments
suggest that they outperform standard French counterparts. However,
nosystematic evaluation comparing all models on the same corpora is available.
Objective: This paper presentsan evaluation of masked language models for
biomedical French on the task of clinical named entity recognition.Material and
methods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare
them tostandard French models CamemBERT, FlauBERT and FrALBERT as well as
multilingual mBERT using three publicallyavailable corpora for clinical named
entity recognition in French. The evaluation set-up relies on
gold-standardcorpora as released by the corpus developers. Results: Results
suggest that CamemBERT-bio outperformsDrBERT consistently while FlauBERT offers
competitive performance and FrAlBERT achieves the lowest carbonfootprint.
Conclusion: This is the first benchmark evaluation of biomedical masked
language models for Frenchclinical entity recognition that compares model
performance consistently on nested entity recognition using metricscovering
performance and environmental impact.
\\ ( https://arxiv.org/abs/2403.19726 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19727
Date: Thu, 28 Mar 2024 08:40:02 GMT   (47kb)

Title: New Semantic Task for the French Spoken Language Understanding MEDIA
  Benchmark
Authors: Nad\`ege Alavoine (STL), Ga\"elle Laperriere (LIA), Christophe Servan
  (STL), Sahar Ghannay (STL), Sophie Rosset (STL)
Categories: cs.CL cs.AI
Journal-ref: The 2024 Joint International Conference on Computational
  Linguistics, Language Resources and Evaluation (LREC-COLING 2024), May 2024,
  Torino, Italy
\\
  Intent classification and slot-filling are essential tasks of Spoken Language
Understanding (SLU). In most SLUsystems, those tasks are realized by
independent modules. For about fifteen years, models achieving both of
themjointly and exploiting their mutual enhancement have been proposed. A
multilingual module using a joint modelwas envisioned to create a touristic
dialogue system for a European project, HumanE-AI-Net. A combination ofmultiple
datasets, including the MEDIA dataset, was suggested for training this joint
model. The MEDIA SLU datasetis a French dataset distributed since 2005 by ELRA,
mainly used by the French research community and free foracademic research
since 2020. Unfortunately, it is annotated only in slots but not intents. An
enhanced version ofMEDIA annotated with intents has been built to extend its
use to more tasks and use cases. This paper presents thesemi-automatic
methodology used to obtain this enhanced version. In addition, we present the
first results of SLUexperiments on this enhanced dataset using joint models for
intent classification and slot-filling.
\\ ( https://arxiv.org/abs/2403.19727 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19728
Date: Thu, 28 Mar 2024 10:31:09 GMT   (352kb)

Title: EmoScan: Automatic Screening of Depression Symptoms in Romanized Sinhala
  Tweets
Authors: Jayathi Hewapathirana and Deshan Sumanathilaka
Categories: cs.CL cs.CY cs.LG
Comments: 4 pages, 2 tables, 1 Figure , Preprint
\\
  This work explores the utilization of Romanized Sinhala social media data to
identify individuals at risk of depression. A machine learning-based framework
is presented for the automatic screening of depression symptoms by analyzing
language patterns, sentiment, and behavioural cues within a comprehensive
dataset of social media posts. The research has been carried out to compare the
suitability of Neural Networks over the classical machine learning techniques.
The proposed Neural Network with an attention layer which is capable of
handling long sequence data, attains a remarkable accuracy of 93.25% in
detecting depression symptoms, surpassing current state-of-the-art methods.
These findings underscore the efficacy of this approach in pinpointing
individuals in need of proactive interventions and support. Mental health
professionals, policymakers, and social media companies can gain valuable
insights through the proposed model. Leveraging natural language processing
techniques and machine learning algorithms, this work offers a promising
pathway for mental health screening in the digital era. By harnessing the
potential of social media data, the framework introduces a proactive method for
recognizing and assisting individuals at risk of depression. In conclusion,
this research contributes to the advancement of proactive interventions and
support systems for mental health, thereby influencing both research and
practical applications in the field.
\\ ( https://arxiv.org/abs/2403.19728 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19754
Date: Thu, 28 Mar 2024 18:08:22 GMT   (5282kb,D)

Title: GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided
  Language Data Generation
Authors: Mohsen Gholami, Mohammad Akbari, Cindy Hu, Vaden Masrani, Z. Jane
  Wang, and Yong Zhang
Categories: cs.CL
\\
  Knowledge distillation from LLMs is essential for the efficient deployment of
language models. Prior works have proposed data generation using LLMs for
preparing distilled models. We argue that generating data with LLMs is prone to
sampling mainly from the center of original content distribution. This
limitation hinders the distilled model from learning the true underlying data
distribution and to forget the tails of the distributions (samples with lower
probability). To this end, we propose GOLD, a task-agnostic data generation and
knowledge distillation framework, which employs an iterative
out-of-distribution-guided feedback mechanism for the LLM. As a result, the
generated data improves the generalizability of distilled models. An
energy-based OOD evaluation approach is also introduced to deal with noisy
generated data. Our extensive experiments on 10 different classification and
sequence-to-sequence tasks in NLP show that GOLD respectively outperforms prior
arts and the LLM with an average improvement of 5% and 14%. We will also show
that the proposed method is applicable to less explored and novel tasks. The
code is available.
\\ ( https://arxiv.org/abs/2403.19754 ,  5282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19802
Date: Thu, 28 Mar 2024 19:31:32 GMT   (4910kb,D)

Title: Developing Healthcare Language Model Embedding Spaces
Authors: Niall Taylor, Dan Schofield, Andrey Kormilitzin, Dan W Joyce, Alejo
  Nevado-Holgado
Categories: cs.CL cs.AI
\\
  Pre-trained Large Language Models (LLMs) often struggle on out-of-domain
datasets like healthcare focused text. We explore specialized pre-training to
adapt smaller LLMs to different healthcare datasets. Three methods are
assessed: traditional masked language modeling, Deep Contrastive Learning for
Unsupervised Textual Representations (DeCLUTR), and a novel pre-training
objective utilizing metadata categories from the healthcare settings. These
schemes are evaluated on downstream document classification tasks for each
dataset, with additional analysis of the resultant embedding spaces.
Contrastively trained models outperform other approaches on the classification
tasks, delivering strong performance from limited labeled data and with fewer
model parameter updates required. While metadata-based pre-training does not
further improve classifications across the datasets, it yields interesting
embedding cluster separability. All domain adapted LLMs outperform their
publicly available general base LLM, validating the importance of
domain-specialization. This research illustrates efficient approaches to
instill healthcare competency in compact LLMs even under tight computational
budgets, an essential capability for responsible and sustainable deployment in
local healthcare settings. We provide pre-training guidelines for specialized
healthcare LLMs, motivate continued inquiry into contrastive objectives, and
demonstrates adaptation techniques to align small LLMs with privacy-sensitive
medical tasks.
\\ ( https://arxiv.org/abs/2403.19802 ,  4910kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19822
Date: Thu, 28 Mar 2024 20:23:39 GMT   (1354kb,D)

Title: Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition
Authors: Yash Jain, David Chan, Pranav Dheram, Aparna Khare, Olabanji
  Shonibare, Venkatesh Ravichandran, Shalini Ghosh
Categories: cs.CL cs.AI
Comments: Accepted in LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
\\
  Recent advances in machine learning have demonstrated that multi-modal
pre-training can improve automatic speech recognition (ASR) performance
compared to randomly initialized models, even when models are fine-tuned on
uni-modal tasks. Existing multi-modal pre-training methods for the ASR task
have primarily focused on single-stage pre-training where a single unsupervised
task is used for pre-training followed by fine-tuning on the downstream task.
In this work, we introduce a novel method combining multi-modal and multi-task
unsupervised pre-training with a translation-based supervised mid-training
approach. We empirically demonstrate that such a multi-stage approach leads to
relative word error rate (WER) improvements of up to 38.45% over baselines on
both Librispeech and SUPERB. Additionally, we share several important findings
for choosing pre-training methods and datasets.
\\ ( https://arxiv.org/abs/2403.19822 ,  1354kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19827
Date: Thu, 28 Mar 2024 20:35:10 GMT   (935kb,D)

Title: Language Models Learn Rare Phenomena from Less Rare Phenomena: The Case
  of the Missing AANNs
Authors: Kanishka Misra, Kyle Mahowald
Categories: cs.CL
\\
  Language models learn rare syntactic phenomena, but it has been argued that
they rely on rote memorization, as opposed to grammatical generalization.
Training on a corpus of human-scale in size (100M words), we iteratively
trained transformer language models on systematically manipulated corpora and
then evaluated their learning of a particular rare grammatical phenomenon: the
English Article+Adjective+Numeral+Noun (AANN) construction (``a beautiful five
days''). We first compared how well this construction was learned on the
default corpus relative to a counterfactual corpus in which the AANN sentences
were removed. AANNs were still learned better than systematically perturbed
variants of the construction. Using additional counterfactual corpora, we
suggest that this learning occurs through generalization from related
constructions (e.g., ``a few days''). An additional experiment showed that this
learning is enhanced when there is more variability in the input. Taken
together, our results provide an existence proof that models learn rare
grammatical phenomena by generalization from less rare phenomena. Code
available at https://github.com/kanishkamisra/aannalysis
\\ ( https://arxiv.org/abs/2403.19827 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19836
Date: Thu, 28 Mar 2024 21:15:15 GMT   (1219kb,D)

Title: Target Span Detection for Implicit Harmful Content
Authors: Nazanin Jafari, James Allan
Categories: cs.CL
\\
  Identifying the targets of hate speech is a crucial step in grasping the
nature of such speech and, ultimately, in improving the detection of offensive
posts on online forums. Much harmful content on online platforms uses implicit
language especially when targeting vulnerable and protected groups such as
using stereotypical characteristics instead of explicit target names, making it
harder to detect and mitigate the language. In this study, we focus on
identifying implied targets of hate speech, essential for recognizing subtler
hate speech and enhancing the detection of harmful content on digital
platforms. We define a new task aimed at identifying the targets even when they
are not explicitly stated. To address that task, we collect and annotate target
spans in three prominent implicit hate speech datasets: SBIC, DynaHate, and
IHC. We call the resulting merged collection Implicit-Target-Span. The
collection is achieved using an innovative pooling method with matching scores
based on human annotations and Large Language Models (LLMs). Our experiments
indicate that Implicit-Target-Span provides a challenging test bed for target
span detection methods.
\\ ( https://arxiv.org/abs/2403.19836 ,  1219kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19851
Date: Thu, 28 Mar 2024 21:53:24 GMT   (2227kb,D)

Title: Localizing Paragraph Memorization in Language Models
Authors: Niklas Stoehr, Mitchell Gordon, Chiyuan Zhang, Owen Lewis
Categories: cs.CL cs.CR cs.LG stat.ML
\\
  Can we localize the weights and mechanisms used by a language model to
memorize and recite entire paragraphs of its training data? In this paper, we
show that while memorization is spread across multiple layers and model
components, gradients of memorized paragraphs have a distinguishable spatial
pattern, being larger in lower model layers than gradients of non-memorized
examples. Moreover, the memorized examples can be unlearned by fine-tuning only
the high-gradient weights. We localize a low-layer attention head that appears
to be especially involved in paragraph memorization. This head is predominantly
focusing its attention on distinctive, rare tokens that are least frequent in a
corpus-level unigram distribution. Next, we study how localized memorization is
across the tokens in the prefix by perturbing tokens and measuring the caused
change in the decoding. A few distinctive tokens early in a prefix can often
corrupt the entire continuation. Overall, memorized continuations are not only
harder to unlearn, but also to corrupt than non-memorized ones.
\\ ( https://arxiv.org/abs/2403.19851 ,  2227kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19887
Date: Thu, 28 Mar 2024 23:55:06 GMT   (941kb,D)

Title: Jamba: A Hybrid Transformer-Mamba Language Model
Authors: Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay
  Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai
  Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman
  Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez
  Shwartz, Mor Zusman, Yoav Shoham
Categories: cs.CL cs.LG
Comments: Webpage: https://www.ai21.com/jamba
\\
  We present Jamba, a new base large language model based on a novel hybrid
Transformer-Mamba mixture-of-experts (MoE) architecture. Specifically, Jamba
interleaves blocks of Transformer and Mamba layers, enjoying the benefits of
both model families. MoE is added in some of these layers to increase model
capacity while keeping active parameter usage manageable. This flexible
architecture allows resource- and objective-specific configurations. In the
particular configuration we have implemented, we end up with a powerful model
that fits in a single 80GB GPU. Built at large scale, Jamba provides high
throughput and small memory footprint compared to vanilla Transformers, and at
the same time state-of-the-art performance on standard language model
benchmarks and long-context evaluations. Remarkably, the model presents strong
results for up to 256K tokens context length. We study various architectural
decisions, such as how to combine Transformer and Mamba layers, and how to mix
experts, and show that some of them are crucial in large scale modeling. We
also describe several interesting properties of these architectures which the
training and evaluation of Jamba have revealed, and plan to release checkpoints
from various ablation runs, to encourage further exploration of this novel
architecture. We make the weights of our implementation of Jamba publicly
available under a permissive license.
\\ ( https://arxiv.org/abs/2403.19887 ,  941kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19889
Date: Fri, 29 Mar 2024 00:14:46 GMT   (2805kb,D)

Title: Towards a Robust Retrieval-Based Summarization System
Authors: Shengjie Liu, Jing Wu, Jingyuan Bao, Wenyi Wang, Naira Hovakimyan,
  Christopher G Healey
Categories: cs.CL cs.AI cs.IR cs.LG
\\
  This paper describes an investigation of the robustness of large language
models (LLMs) for retrieval augmented generation (RAG)-based summarization
tasks. While LLMs provide summarization capabilities, their performance in
complex, real-world scenarios remains under-explored. Our first contribution is
LogicSumm, an innovative evaluation framework incorporating realistic scenarios
to assess LLM robustness during RAG-based summarization. Based on limitations
identified by LogiSumm, we then developed SummRAG, a comprehensive system to
create training dialogues and fine-tune a model to enhance robustness within
LogicSumm's scenarios. SummRAG is an example of our goal of defining structured
methods to test the capabilities of an LLM, rather than addressing issues in a
one-off fashion. Experimental results confirm the power of SummRAG, showcasing
improved logical coherence and summarization quality. Data, corresponding model
weights, and Python code are available online.
\\ ( https://arxiv.org/abs/2403.19889 ,  2805kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19913
Date: Fri, 29 Mar 2024 01:53:24 GMT   (8281kb,D)

Title: MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of
  Large Language Models
Authors: Peng Ding and Jiading Fang and Peng Li and Kangrui Wang and Xiaochen
  Zhou and Mo Yu and Jing Li and Matthew R. Walter and Hongyuan Mei
Categories: cs.CL cs.AI cs.LG cs.RO
\\
  Large language models such as ChatGPT and GPT-4 have recently achieved
astonishing performance on a variety of natural language processing tasks. In
this paper, we propose MANGO, a benchmark to evaluate their capabilities to
perform text-based mapping and navigation. Our benchmark includes 53 mazes
taken from a suite of textgames: each maze is paired with a walkthrough that
visits every location but does not cover all possible paths. The task is
question-answering: for each maze, a large language model reads the walkthrough
and answers hundreds of mapping and navigation questions such as "How should
you go to Attic from West of House?" and "Where are we if we go north and east
from Cellar?". Although these questions are easy to humans, it turns out that
even GPT-4, the best-to-date language model, performs poorly at answering them.
Further, our experiments suggest that a strong mapping and navigation ability
would benefit large language models in performing relevant downstream tasks,
such as playing textgames. Our MANGO benchmark will facilitate future research
on methods that improve the mapping and navigation capabilities of language
models. We host our leaderboard, data, code, and evaluation program at
https://mango.ttic.edu and https://github.com/oaklight/mango/.
\\ ( https://arxiv.org/abs/2403.19913 ,  8281kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19928
Date: Fri, 29 Mar 2024 02:32:15 GMT   (252kb,D)

Title: DiJiang: Efficient Large Language Models through Compact Kernelization
Authors: Hanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe Wang
Categories: cs.CL cs.LG
\\
  In an effort to reduce the computational load of Transformers, research on
linear attention has gained significant momentum. However, the improvement
strategies for attention mechanisms typically necessitate extensive retraining,
which is impractical for large language models with a vast array of parameters.
In this paper, we present DiJiang, a novel Frequency Domain Kernelization
approach that enables the transformation of a pre-trained vanilla Transformer
into a linear complexity model with little training costs. By employing a
weighted Quasi-Monte Carlo method for sampling, the proposed approach
theoretically offers superior approximation efficiency. To further reduce the
training computational complexity, our kernelization is based on Discrete
Cosine Transform (DCT) operations. Extensive experiments demonstrate that the
proposed method achieves comparable performance to the original Transformer,
but with significantly reduced training costs and much faster inference speeds.
Our DiJiang-7B achieves comparable performance with LLaMA2-7B on various
benchmark while requires only about 1/50 training cost. Code is available at
https://github.com/YuchuanTian/DiJiang.
\\ ( https://arxiv.org/abs/2403.19928 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19930
Date: Fri, 29 Mar 2024 02:36:54 GMT   (853kb,D)

Title: Are LLMs Effective Backbones for Fine-tuning? An Experimental
  Investigation of Supervised LLMs on Chinese Short Text Matching
Authors: Shulin Liu, Chengcheng Xu, Hao Liu, Tinghao Yu, Tao Yang
Categories: cs.CL
\\
  The recent success of Large Language Models (LLMs) has garnered significant
attention in both academia and industry. Prior research on LLMs has primarily
focused on enhancing or leveraging their generalization capabilities in zero-
and few-shot settings. However, there has been limited investigation into
effectively fine-tuning LLMs for a specific natural language understanding task
in supervised settings. In this study, we conduct an experimental analysis by
fine-tuning LLMs for the task of Chinese short text matching. We explore
various factors that influence performance when fine-tuning LLMs, including
task modeling methods, prompt formats, and output formats.
\\ ( https://arxiv.org/abs/2403.19930 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19936
Date: Fri, 29 Mar 2024 02:42:39 GMT   (1109kb,D)

Title: SLFNet: Generating Semantic Logic Forms from Natural Language Using
  Semantic Probability Graphs
Authors: Hao Wu, Fan Xu
Categories: cs.CL
\\
  Building natural language interfaces typically uses a semantic parser to
parse the user's natural language and convert it into structured
\textbf{S}emantic \textbf{L}ogic \textbf{F}orms (SLFs). The mainstream approach
is to adopt a sequence-to-sequence framework, which requires that natural
language commands and SLFs must be represented serially. Since a single natural
language may have multiple SLFs or multiple natural language commands may have
the same SLF, training a sequence-to-sequence model is sensitive to the choice
among them, a phenomenon recorded as "order matters". To solve this problem, we
propose a novel neural network, SLFNet, which firstly incorporates dependent
syntactic information as prior knowledge and can capture the long-range
interactions between contextual information and words. Secondly construct
semantic probability graphs to obtain local dependencies between predictor
variables. Finally we propose the Multi-Head SLF Attention mechanism to
synthesize SLFs from natural language commands based on Sequence-to-Slots.
Experiments show that SLFNet achieves state-of-the-art performance on the
ChineseQCI-TS and Okapi datasets, and competitive performance on the ATIS
dataset.
\\ ( https://arxiv.org/abs/2403.19936 ,  1109kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19962
Date: Fri, 29 Mar 2024 03:48:12 GMT   (619kb,D)

Title: Enhancing the General Agent Capabilities of Low-Parameter LLMs through
  Tuning and Multi-Branch Reasoning
Authors: Qinhao Zhou, Zihan Zhang, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li
Categories: cs.CL cs.AI cs.LG
Comments: To appear at NAACL 2024
\\
  Open-source pre-trained Large Language Models (LLMs) exhibit strong language
understanding and generation capabilities, making them highly successful in a
variety of tasks. However, when used as agents for dealing with complex
problems in the real world, their performance is far inferior to large
commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need
to have the capabilities of task planning, long-term memory, and the ability to
leverage external tools to achieve satisfactory performance. Various methods
have been proposed to enhance the agent capabilities of LLMs. On the one hand,
methods involve constructing agent-specific data and fine-tuning the models. On
the other hand, some methods focus on designing prompts that effectively
activate the reasoning abilities of the LLMs. We explore both strategies on the
7B and 13B models. We propose a comprehensive method for constructing
agent-specific data using GPT-4. Through supervised fine-tuning with
constructed data, we find that for these models with a relatively small number
of parameters, supervised fine-tuning can significantly reduce hallucination
outputs and formatting errors in agent tasks. Furthermore, techniques such as
multi-path reasoning and task decomposition can effectively decrease problem
complexity and enhance the performance of LLMs as agents. We evaluate our
method on five agent tasks of AgentBench and achieve satisfactory results.
\\ ( https://arxiv.org/abs/2403.19962 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20005
Date: Fri, 29 Mar 2024 06:43:55 GMT   (7740kb,D)

Title: Large Language Model based Situational Dialogues for Second Language
  Learning
Authors: Shuyao Xu, Long Qin, Tianyang Chen, Zhenzhou Zha, and Bingxue Qiu,
  Weizhi Wang
Categories: cs.CL
Comments: 14 pages, 6 figures
\\
  In second language learning, scenario-based conversation practice is
important for language learners to achieve fluency in speaking, but students
often lack sufficient opportunities to practice their conversational skills
with qualified instructors or native speakers. To bridge this gap, we propose
situational dialogue models for students to engage in conversational practice.
Our situational dialogue models are fine-tuned on large language models (LLMs),
with the aim of combining the engaging nature of an open-ended conversation
with the focused practice of scenario-based tasks. Leveraging the
generalization capabilities of LLMs, we demonstrate that our situational
dialogue models perform effectively not only on training topics but also on
topics not encountered during training. This offers a promising solution to
support a wide range of conversational topics without extensive manual work.
Additionally, research in the field of dialogue systems still lacks reliable
automatic evaluation metrics, leading to human evaluation as the gold standard
(Smith et al., 2022), which is typically expensive. To address the limitations
of existing evaluation methods, we present a novel automatic evaluation method
that employs fine-tuned LLMs to efficiently and effectively assess the
performance of situational dialogue models.
\\ ( https://arxiv.org/abs/2403.20005 ,  7740kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20009
Date: Fri, 29 Mar 2024 06:48:30 GMT   (4060kb,D)

Title: On Large Language Models' Hallucination with Regard to Known Facts
Authors: Che Jiang, Biqing Qi, Xiangyu Hong, Dayuan Fu, Yang Cheng, Fandong
  Meng, Mo Yu, Bowen Zhou, Jie Zhou
Categories: cs.CL cs.LG
Comments: Accepted by NAACL 2024 MainConference
\\
  Large language models are successful in answering factoid questions but are
also prone to hallucination.We investigate the phenomenon of LLMs possessing
correct answer knowledge yet still hallucinating from the perspective of
inference dynamics, an area not previously covered in studies on
hallucinations.We are able to conduct this analysis via two key ideas.First, we
identify the factual questions that query the same triplet knowledge but result
in different answers. The difference between the model behaviors on the correct
and incorrect outputs hence suggests the patterns when hallucinations happen.
Second, to measure the pattern, we utilize mappings from the residual streams
to vocabulary space. We reveal the different dynamics of the output token
probabilities along the depths of layers between the correct and hallucinated
cases. In hallucinated cases, the output token's information rarely
demonstrates abrupt increases and consistent superiority in the later stages of
the model. Leveraging the dynamic curve as a feature, we build a classifier
capable of accurately detecting hallucinatory predictions with an 88\% success
rate. Our study shed light on understanding the reasons for LLMs'
hallucinations on their known facts, and more importantly, on accurately
predicting when they are hallucinating.
\\ ( https://arxiv.org/abs/2403.20009 ,  4060kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20015
Date: Fri, 29 Mar 2024 07:01:39 GMT   (41kb)

Title: Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion
Authors: Juhwan Choi, YoungBin Kim
Categories: cs.CL cs.AI
Comments: ICLR 2024 Tiny Papers
\\
  In the field of text data augmentation, rule-based methods are widely adopted
for real-world applications owing to their cost-efficiency. However,
conventional rule-based approaches suffer from the possibility of losing the
original semantics of the given text. We propose a novel text data augmentation
strategy that avoids such phenomena through a straightforward deletion of
adverbs, which play a subsidiary role in the sentence. Our comprehensive
experiments demonstrate the efficiency and effectiveness of our proposed
approach for not just single text classification, but also natural language
inference that requires semantic preservation. We publicly released our source
code for reproducibility.
\\ ( https://arxiv.org/abs/2403.20015 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20041
Date: Fri, 29 Mar 2024 08:26:53 GMT   (654kb)

Title: Transformer-Lite: High-efficiency Deployment of Large Language Models on
  Mobile Phone GPUs
Authors: Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, Qin Xie
Categories: cs.CL
Comments: 21 pages, 6 figures
\\
  The Large Language Model (LLM) is widely employed for tasks such as
intelligent assistants, text summarization, translation, and multi-modality on
mobile phones. However, the current methods for on-device LLM deployment
maintain slow inference speed, which causes poor user experience. To facilitate
high-efficiency LLM deployment on device GPUs, we propose four optimization
techniques: (a) a symbolic expression-based approach to support dynamic shape
model inference; (b) operator optimizations and execution priority setting to
enhance inference speed and reduce phone lagging; (c) an FP4 quantization
method termed M0E4 to reduce dequantization overhead; (d) a sub-tensor-based
technique to eliminate the need for copying KV cache after LLM inference.
Furthermore, we implement these methods in our mobile inference engine,
Transformer-Lite, which is compatible with both Qualcomm and MTK processors. We
evaluated Transformer-Lite's performance using LLMs with varied architectures
and parameters ranging from 2B to 14B. Specifically, we achieved prefill and
decoding speeds of 121 token/s and 14 token/s for ChatGLM2 6B, and 330 token/s
and 30 token/s for smaller Gemma 2B, respectively. Compared with CPU-based
FastLLM and GPU-based MLC-LLM, our engine attains over 10x speedup for the
prefill speed and 2~3x speedup for the decoding speed.
\\ ( https://arxiv.org/abs/2403.20041 ,  654kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20046
Date: Fri, 29 Mar 2024 08:30:34 GMT   (8378kb,D)

Title: Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to
  Boost for Reasoning
Authors: Yongqi Tong, Dawei Li, Sizhe Wang, Yujia Wang, Fei Teng, Jingbo Shang
Categories: cs.CL
\\
  Recent works have shown the benefits to LLMs from fine-tuning golden-standard
Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot
prompting. While humans can indeed imitate correct examples, learning from our
mistakes is another vital aspect of human cognition. Hence, a question
naturally arises: \textit{can LLMs learn and benefit from their mistakes,
especially for their reasoning? } This study investigates this problem from
both the prompting and model-tuning perspectives. We begin by introducing
\textsc{CoTErrorSet}, a new benchmark with 609,432 questions, each designed
with both correct and error references, and demonstrating the types and reasons
for making such mistakes. To explore the effectiveness of those mistakes, we
design two methods: (1) \textbf{Self-rethinking} prompting guides LLMs to
rethink whether they have made similar previous mistakes; and (2)
\textbf{Mistake tuning} involves finetuning models in both correct and
incorrect reasoning domains, rather than only tuning models to learn ground
truth in traditional methodology. We conduct a series of experiments to prove
LLMs can obtain benefits from mistakes in both directions. Our two methods
offer potentially cost-effective strategies by leveraging errors to enhance
reasoning capabilities, which costs significantly less than creating
meticulously hand-crafted golden references. We ultimately make a thorough
analysis of the reasons behind LLMs' errors, which provides directions that
future research needs to overcome. \textsc{CoTErrorSet} will be published soon
on \texttt{Anonymity Link}.
\\ ( https://arxiv.org/abs/2403.20046 ,  8378kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20056
Date: Fri, 29 Mar 2024 08:47:15 GMT   (999kb,D)

Title: Cross-Lingual Transfer Robustness to Lower-Resource Languages on
  Adversarial Datasets
Authors: Shadi Manafi and Nikhil Krishnaswamy
Categories: cs.CL
Comments: accepted in LREC-COLING 2024
\\
  Multilingual Language Models (MLLMs) exhibit robust cross-lingual transfer
capabilities, or the ability to leverage information acquired in a source
language and apply it to a target language. These capabilities find practical
applications in well-established Natural Language Processing (NLP) tasks such
as Named Entity Recognition (NER). This study aims to investigate the
effectiveness of a source language when applied to a target language,
particularly in the context of perturbing the input test set. We evaluate on 13
pairs of languages, each including one high-resource language (HRL) and one
low-resource language (LRL) with a geographic, genetic, or borrowing
relationship. We evaluate two well-known MLLMs--MBERT and XLM-R--on these
pairs, in native LRL and cross-lingual transfer settings, in two tasks, under a
set of different perturbations. Our findings indicate that NER cross-lingual
transfer depends largely on the overlap of entity chunks. If a source and
target language have more entities in common, the transfer ability is stronger.
Models using cross-lingual transfer also appear to be somewhat more robust to
certain perturbations of the input, perhaps indicating an ability to leverage
stronger representations derived from the HRL. Our research provides valuable
insights into cross-lingual transfer and its implications for NLP applications,
and underscores the need to consider linguistic nuances and potential
limitations when employing MLLMs across distinct languages.
\\ ( https://arxiv.org/abs/2403.20056 ,  999kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20084
Date: Fri, 29 Mar 2024 09:33:34 GMT   (186kb)

Title: IPA Transcription of Bengali Texts
Authors: Kanij Fatema, Fazle Dawood Haider, Nirzona Ferdousi Turpa, Tanveer
  Azmal, Sourav Ahmed, Navid Hasan, Mohammad Akhlaqur Rahman, Biplab Kumar
  Sarkar, Afrar Jahin, Md. Rezuwan Hassan, Md Foriduzzaman Zihad, Rubayet
  Sabbir Faruque, Asif Sushmit, Mashrur Imtiaz, Farig Sadeque, Syed Shahrier
  Rahman
Categories: cs.CL
\\
  The International Phonetic Alphabet (IPA) serves to systematize phonemes in
language, enabling precise textual representation of pronunciation. In Bengali
phonology and phonetics, ongoing scholarly deliberations persist concerning the
IPA standard and core Bengali phonemes. This work examines prior research,
identifies current and potential issues, and suggests a framework for a Bengali
IPA standard, facilitating linguistic analysis and NLP resource creation and
downstream technology development. In this work, we present a comprehensive
study of Bengali IPA transcription and introduce a novel IPA transcription
framework incorporating a novel dataset with DL-based benchmarks.
\\ ( https://arxiv.org/abs/2403.20084 ,  186kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20088
Date: Fri, 29 Mar 2024 09:52:18 GMT   (12117kb,D)

Title: An Efficient Approach for Studying Cross-Lingual Transfer in
  Multilingual Language Models
Authors: Fahim Faisal, Antonios Anastasopoulos
Categories: cs.CL
\\
  The capacity and effectiveness of pre-trained multilingual models (MLMs) for
zero-shot cross-lingual transfer is well established. However, phenomena of
positive or negative transfer, and the effect of language choice still need to
be fully understood, especially in the complex setting of massively
multilingual LMs. We propose an \textit{efficient} method to study transfer
language influence in zero-shot performance on another target language. Unlike
previous work, our approach disentangles downstream tasks from language, using
dedicated adapter units. Our findings suggest that some languages do not
largely affect others, while some languages, especially ones unseen during
pre-training, can be extremely beneficial or detrimental for different target
languages. We find that no transfer language is beneficial for all target
languages. We do, curiously, observe languages previously unseen by MLMs
consistently benefit from transfer from almost any language. We additionally
use our modular approach to quantify negative interference efficiently and
categorize languages accordingly. Furthermore, we provide a list of promising
transfer-target language configurations that consistently lead to target
language performance improvements. Code and data are publicly available:
https://github.com/ffaisal93/neg_inf
\\ ( https://arxiv.org/abs/2403.20088 ,  12117kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20101
Date: Fri, 29 Mar 2024 10:31:32 GMT   (388kb,D)

Title: RealKIE: Five Novel Datasets for Enterprise Key Information Extraction
Authors: Benjamin Townsend, Madison May and Christopher Wells
Categories: cs.CL cs.CV cs.LG
\\
  We introduce RealKIE, a benchmark of five challenging datasets aimed at
advancing key information extraction methods, with an emphasis on enterprise
applications. The datasets include a diverse range of documents including SEC
S1 Filings, US Non-disclosure Agreements, UK Charity Reports, FCC Invoices, and
Resource Contracts. Each presents unique challenges: poor text serialization,
sparse annotations in long documents, and complex tabular layouts. These
datasets provide a realistic testing ground for key information extraction
tasks like investment analysis and legal data processing.
  In addition to presenting these datasets, we offer an in-depth description of
the annotation process, document processing techniques, and baseline modeling
approaches. This contribution facilitates the development of NLP models capable
of handling practical challenges and supports further research into information
extraction technologies applicable to industry-specific problems.
  The annotated data and OCR outputs are available to download at
https://indicodatasolutions.github.io/RealKIE/ code to reproduce the baselines
will be available shortly.
\\ ( https://arxiv.org/abs/2403.20101 ,  388kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20103
Date: Fri, 29 Mar 2024 10:32:44 GMT   (7758kb,D)

Title: NLP for Counterspeech against Hate: A Survey and How-To Guide
Authors: Helena Bonaldi, Yi-Ling Chung, Gavin Abercrombie, Marco Guerini
Categories: cs.CL
Comments: To appear in Proceedings of the 2024 Conference of the North American
  Chapter of the Association for Computational Linguistics (findings)
\\
  In recent years, counterspeech has emerged as one of the most promising
strategies to fight online hate. These non-escalatory responses tackle online
abuse while preserving the freedom of speech of the users, and can have a
tangible impact in reducing online and offline violence. Recently, there has
been growing interest from the Natural Language Processing (NLP) community in
addressing the challenges of analysing, collecting, classifying, and
automatically generating counterspeech, to reduce the huge burden of manually
producing it. In particular, researchers have taken different directions in
addressing these challenges, thus providing a variety of related tasks and
resources. In this paper, we provide a guide for doing research on
counterspeech, by describing - with detailed examples - the steps to undertake,
and providing best practices that can be learnt from the NLP studies on this
topic. Finally, we discuss open challenges and future directions of
counterspeech research in NLP.
\\ ( https://arxiv.org/abs/2403.20103 ,  7758kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20134
Date: Fri, 29 Mar 2024 11:54:13 GMT   (97kb,D)

Title: User Modeling Challenges in Interactive AI Assistant Systems
Authors: Megan Su and Yuwei Bao
Categories: cs.CL
\\
  Interactive Artificial Intelligent(AI) assistant systems are designed to
offer timely guidance to help human users to complete a variety tasks. One of
the remaining challenges is to understand user's mental states during the task
for more personalized guidance. In this work, we analyze users' mental states
during task executions and investigate the capabilities and challenges for
large language models to interpret user profiles for more personalized user
guidance.
\\ ( https://arxiv.org/abs/2403.20134 ,  97kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20145
Date: Fri, 29 Mar 2024 12:25:37 GMT   (7068kb,D)

Title: Fine-tuning Large Language Models for Automated Diagnostic Screening
  Summaries
Authors: Manjeet Yadav, Nilesh Kumar Sahu, Mudita Chaturvedi, Snehil Gupta,
  Haroon R Lone
Categories: cs.CL
\\
  Improving mental health support in developing countries is a pressing need.
One potential solution is the development of scalable, automated systems to
conduct diagnostic screenings, which could help alleviate the burden on mental
health professionals. In this work, we evaluate several state-of-the-art Large
Language Models (LLMs), with and without fine-tuning, on our custom dataset for
generating concise summaries from mental state examinations. We rigorously
evaluate four different models for summary generation using established ROUGE
metrics and input from human evaluators. The results highlight that our
top-performing fine-tuned model outperforms existing models, achieving ROUGE-1
and ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed
the fine-tuned model's generalizability on a publicly available D4 dataset, and
the outcomes were promising, indicating its potential applicability beyond our
custom dataset.
\\ ( https://arxiv.org/abs/2403.20145 ,  7068kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20147
Date: Fri, 29 Mar 2024 12:32:06 GMT   (7403kb,D)

Title: IndiBias: A Benchmark Dataset to Measure Social Biases in Language
  Models for Indian Context
Authors: Nihar Ranjan Sahoo, Pranamya Prashant Kulkarni, Narjis Asad, Arif
  Ahmad, Tanu Goyal, Aparna Garimella, Pushpak Bhattacharyya
Categories: cs.CL
\\
  The pervasive influence of social biases in language data has sparked the
need for benchmark datasets that capture and evaluate these biases in Large
Language Models (LLMs). Existing efforts predominantly focus on English
language and the Western context, leaving a void for a reliable dataset that
encapsulates India's unique socio-cultural nuances. To bridge this gap, we
introduce IndiBias, a comprehensive benchmarking dataset designed specifically
for evaluating social biases in the Indian context. We filter and translate the
existing CrowS-Pairs dataset to create a benchmark dataset suited to the Indian
context in Hindi language. Additionally, we leverage LLMs including ChatGPT and
InstructGPT to augment our dataset with diverse societal biases and stereotypes
prevalent in India. The included bias dimensions encompass gender, religion,
caste, age, region, physical appearance, and occupation. We also build a
resource to address intersectional biases along three intersectional
dimensions. Our dataset contains 800 filtered sentences from the CrowS-Pairs
dataset and tuples for bias measurement across different demographics. It is
made available in English and Hindi languages, providing a size comparable to
existing benchmark datasets. Furthermore, using IndiBias we compare ten
different language models on multiple bias measurement metrics. We observed
that the language models exhibit more bias across a majority of the
intersectional groups.
\\ ( https://arxiv.org/abs/2403.20147 ,  7403kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20157
Date: Fri, 29 Mar 2024 13:09:23 GMT   (41kb,D)

Title: A Systematic Analysis of Subwords and Cross-Lingual Transfer in
  Multilingual Translation
Authors: Francois Meyer and Jan Buys
Categories: cs.CL
\\
  Multilingual modelling can improve machine translation for low-resource
languages, partly through shared subword representations. This paper studies
the role of subword segmentation in cross-lingual transfer. We systematically
compare the efficacy of several subword methods in promoting synergy and
preventing interference across different linguistic typologies. Our findings
show that subword regularisation boosts synergy in multilingual modelling,
whereas BPE more effectively facilitates transfer during cross-lingual
fine-tuning. Notably, our results suggest that differences in orthographic word
boundary conventions (the morphological granularity of written words) may
impede cross-lingual transfer more significantly than linguistic unrelatedness.
Our study confirms that decisions around subword modelling can be key to
optimising the benefits of multilingual modelling.
\\ ( https://arxiv.org/abs/2403.20157 ,  41kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20158
Date: Fri, 29 Mar 2024 13:12:09 GMT   (382kb)

Title: ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned
  Language Models
Authors: Zehao Wen and Rabih Younes
Categories: cs.CL cs.AI
Comments: 9 pages, 1 figure, published on Applied and Computational Engineering
Journal-ref: ACE (2023) Vol. 21: 249-257.
DOI: 10.54254/2755-2721/21/20231153
\\
  In our rapidly evolving digital sphere, the ability to discern media bias
becomes crucial as it can shape public sentiment and influence pivotal
decisions. The advent of large language models (LLMs), such as ChatGPT, noted
for their broad utility in various natural language processing (NLP) tasks,
invites exploration of their efficacy in media bias detection. Can ChatGPT
detect media bias? This study seeks to answer this question by leveraging the
Media Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in
distinguishing six categories of media bias, juxtaposed against fine-tuned
models such as BART, ConvBERT, and GPT-2. The findings present a dichotomy:
ChatGPT performs at par with fine-tuned models in detecting hate speech and
text-level context bias, yet faces difficulties with subtler elements of other
bias detections, namely, fake news, racial, gender, and cognitive biases.
\\ ( https://arxiv.org/abs/2403.20158 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20180
Date: Fri, 29 Mar 2024 13:56:21 GMT   (934kb,D)

Title: Measuring Taiwanese Mandarin Language Understanding
Authors: Po-Heng Chen, Sijia Cheng, Wei-Lin Chen, Yen-Ting Lin, Yun-Nung Chen
Categories: cs.CL
Comments: Preprint. Under review
\\
  The evaluation of large language models (LLMs) has drawn substantial
attention in the field recently. This work focuses on evaluating LLMs in a
Chinese context, specifically, for Traditional Chinese which has been largely
underrepresented in existing benchmarks. We present TMLU, a holistic evaluation
suit tailored for assessing the advanced knowledge and reasoning capability in
LLMs, under the context of Taiwanese Mandarin. TMLU consists of an array of 37
subjects across social science, STEM, humanities, Taiwan-specific content, and
others, ranging from middle school to professional levels. In addition, we
curate chain-of-thought-like few-shot explanations for each subject to
facilitate the evaluation of complex reasoning skills. To establish a
comprehensive baseline, we conduct extensive experiments and analysis on 24
advanced LLMs. The results suggest that Chinese open-weight models demonstrate
inferior performance comparing to multilingual proprietary ones, and
open-weight models tailored for Taiwanese Mandarin lag behind the
Simplified-Chinese counterparts. The findings indicate great headrooms for
improvement, and emphasize the goal of TMLU to foster the development of
localized Taiwanese-Mandarin LLMs. We release the benchmark and evaluation
scripts for the community to promote future research.
\\ ( https://arxiv.org/abs/2403.20180 ,  934kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20196
Date: Fri, 29 Mar 2024 14:18:26 GMT   (8384kb,D)

Title: Automatic Alignment of Discourse Relations of Different Discourse
  Annotation Frameworks
Authors: Yingxue Fu
Categories: cs.CL
\\
  Existing discourse corpora are annotated based on different frameworks, which
show significant dissimilarities in definitions of arguments and relations and
structural constraints. Despite surface differences, these frameworks share
basic understandings of discourse relations. The relationship between these
frameworks has been an open research question, especially the correlation
between relation inventories utilized in different frameworks. Better
understanding of this question is helpful for integrating discourse theories
and enabling interoperability of discourse corpora annotated under different
frameworks. However, studies that explore correlations between discourse
relation inventories are hindered by different criteria of discourse
segmentation, and expert knowledge and manual examination are typically needed.
Some semi-automatic methods have been proposed, but they rely on corpora
annotated in multiple frameworks in parallel. In this paper, we introduce a
fully automatic approach to address the challenges. Specifically, we extend the
label-anchored contrastive learning method introduced by Zhang et al. (2022b)
to learn label embeddings during a classification task. These embeddings are
then utilized to map discourse relations from different frameworks. We show
experimental results on RST-DT (Carlson et al., 2001) and PDTB 3.0 (Prasad et
al., 2018).
\\ ( https://arxiv.org/abs/2403.20196 ,  8384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20215
Date: Fri, 29 Mar 2024 14:54:19 GMT   (946kb,D)

Title: Advancing the Arabic WordNet: Elevating Content Quality
Authors: Abed Alhakim Freihat, Hadi Khalilia, G\'abor Bella, Fausto Giunchiglia
Categories: cs.CL
\\
  High-quality WordNets are crucial for achieving high-quality results in NLP
applications that rely on such resources. However, the wordnets of most
languages suffer from serious issues of correctness and completeness with
respect to the words and word meanings they define, such as incorrect lemmas,
missing glosses and example sentences, or an inadequate, Western-centric
representation of the morphology and the semantics of the language. Previous
efforts have largely focused on increasing lexical coverage while ignoring
other qualitative aspects. In this paper, we focus on the Arabic language and
introduce a major revision of the Arabic WordNet that addresses multiple
dimensions of lexico-semantic resource quality. As a result, we updated more
than 58% of the synsets of the existing Arabic WordNet by adding missing
information and correcting errors. In order to address issues of language
diversity and untranslatability, we also extended the wordnet structure by new
elements: phrasets and lexical gaps.
\\ ( https://arxiv.org/abs/2403.20215 ,  946kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20252
Date: Fri, 29 Mar 2024 15:58:46 GMT   (2451kb,D)

Title: Using LLMs to Model the Beliefs and Preferences of Targeted Populations
Authors: Keiichi Namikoshi, Alex Filipowicz, David A. Shamma, Rumen Iliev,
  Candice L. Hogan, Nikos Arechiga
Categories: cs.CL cs.AI cs.LG
\\
  We consider the problem of aligning a large language model (LLM) to model the
preferences of a human population. Modeling the beliefs, preferences, and
behaviors of a specific population can be useful for a variety of different
applications, such as conducting simulated focus groups for new products,
conducting virtual surveys, and testing behavioral interventions, especially
for interventions that are expensive, impractical, or unethical. Existing work
has had mixed success using LLMs to accurately model human behavior in
different contexts. We benchmark and evaluate two well-known fine-tuning
approaches and evaluate the resulting populations on their ability to match the
preferences of real human respondents on a survey of preferences for battery
electric vehicles (BEVs). We evaluate our models against their ability to match
population-wide statistics as well as their ability to match individual
responses, and we investigate the role of temperature in controlling the
trade-offs between these two. Additionally, we propose and evaluate a novel
loss term to improve model performance on responses that require a numeric
response.
\\ ( https://arxiv.org/abs/2403.20252 ,  2451kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20262
Date: Fri, 29 Mar 2024 16:13:31 GMT   (276kb,D)

Title: ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language
  Models
Authors: Thibaut Thonet, Jos Rozen, Laurent Besacier
Categories: cs.CL cs.AI cs.LG
\\
  Research on Large Language Models (LLMs) has recently witnessed an increasing
interest in extending models' context size to better capture dependencies
within long documents. While benchmarks have been proposed to assess long-range
abilities, existing efforts primarily considered generic tasks that are not
necessarily aligned with real-world applications. In contrast, our work
proposes a new benchmark for long-context LLMs focused on a practical meeting
assistant scenario. In this scenario, the long contexts consist of transcripts
obtained by automatic speech recognition, presenting unique challenges for LLMs
due to the inherent noisiness and oral nature of such data. Our benchmark,
named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271
manually crafted questions and their ground-truth answers. Our experiments with
recent long-context LLMs on ELITR-Bench highlight a gap between open-source and
proprietary models, especially when questions are asked sequentially within a
conversation. We also provide a thorough analysis of our GPT-4-based evaluation
method, encompassing insights from a crowdsourcing study. Our findings suggest
that while GPT-4's evaluation scores are correlated with human judges', its
ability to differentiate among more than three score levels may be limited.
\\ ( https://arxiv.org/abs/2403.20262 ,  276kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20266
Date: Fri, 29 Mar 2024 16:16:48 GMT   (317kb,D)

Title: Latxa: An Open Language Model and Evaluation Suite for Basque
Authors: Julen Etxaniz, Oscar Sainz, Naiara Perez, Itziar Aldabe, German Rigau,
  Eneko Agirre, Aitor Ormazabal, Mikel Artetxe, Aitor Soroa
Categories: cs.CL cs.AI cs.LG
\\
  We introduce Latxa, a family of large language models for Basque ranging from
7 to 70 billion parameters. Latxa is based on Llama 2, which we continue
pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens.
Addressing the scarcity of high-quality benchmarks for Basque, we further
introduce 4 multiple choice evaluation datasets: EusProficiency, comprising
5,169 questions from official language proficiency exams; EusReading,
comprising 352 reading comprehension questions; EusTrivia, comprising 1,715
trivia questions from 5 knowledge areas; and EusExams, comprising 16,774
questions from public examinations. In our extensive evaluation, Latxa
outperforms all previous open models we compare to by a large margin. In
addition, it is competitive with GPT-4 Turbo in language proficiency and
understanding, despite lagging behind in reading comprehension and
knowledge-intensive tasks. Both the Latxa family of models, as well as our new
pretraining corpora and evaluation datasets, are publicly available under open
licenses at https://github.com/hitz-zentroa/latxa. Our suite enables
reproducible research on methods to build LLMs for low-resource languages.
\\ ( https://arxiv.org/abs/2403.20266 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20279
Date: Fri, 29 Mar 2024 16:49:24 GMT   (8902kb,D)

Title: LUQ: Long-text Uncertainty Quantification for LLMs
Authors: Caiqi Zhang, Fangyu Liu, Marco Basaldella, Nigel Collier
Categories: cs.CL
\\
  Large Language Models (LLMs) have demonstrated remarkable capability in a
variety of NLP tasks. Despite their effectiveness, these models are prone to
generate nonfactual content. Uncertainty Quantification (UQ) is pivotal in
enhancing our understanding of a model's confidence in its generated content,
thereby aiding in the mitigation of nonfactual outputs. Existing research on UQ
predominantly targets short text generation, typically yielding brief,
word-limited responses. However, real-world applications frequently necessitate
much longer responses. Our study first highlights the limitations of current UQ
methods in handling long text generation. We then introduce \textsc{Luq}, a
novel sampling-based UQ approach specifically designed for long text. Our
findings reveal that \textsc{Luq} outperforms existing baseline methods in
correlating with the model's factuality scores (negative coefficient of -0.85
observed for Gemini Pro). With \textsc{Luq} as the tool for UQ, we investigate
behavior patterns of several popular LLMs' response confidence spectrum and how
that interplays with the response' factuality. We identify that LLMs lack
confidence in generating long text for rare facts and a factually strong model
(i.e. GPT-4) tends to reject questions it is not sure about. To further improve
the factual accuracy of LLM responses, we propose a method called
\textsc{Luq-Ensemble} that ensembles responses from multiple models and selects
the response with the least uncertainty. The ensembling method greatly improves
the response factuality upon the best standalone LLM.
\\ ( https://arxiv.org/abs/2403.20279 ,  8902kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20284
Date: Fri, 29 Mar 2024 16:53:11 GMT   (6227kb,D)

Title: LayerNorm: A key component in parameter-efficient fine-tuning
Authors: Taha ValizadehAslani, Hualou Liang
Categories: cs.CL cs.LG
\\
  Fine-tuning a pre-trained model, such as Bidirectional Encoder
Representations from Transformers (BERT), has been proven to be an effective
method for solving many natural language processing (NLP) tasks. However, due
to the large number of parameters in many state-of-the-art NLP models,
including BERT, the process of fine-tuning is computationally expensive. One
attractive solution to this issue is parameter-efficient fine-tuning, which
involves modifying only a minimal segment of the model while keeping the
remainder unchanged. Yet, it remains unclear which segment of the BERT model is
crucial for fine-tuning. In this paper, we first analyze different components
in the BERT model to pinpoint which one undergoes the most significant changes
after fine-tuning. We find that output LayerNorm changes more than any other
components when fine-tuned for different General Language Understanding
Evaluation (GLUE) tasks. Then we show that only fine-tuning the LayerNorm can
reach comparable, or in some cases better, performance to full fine-tuning and
other parameter-efficient fine-tuning methods. Moreover, we use Fisher
information to determine the most critical subset of LayerNorm and demonstrate
that many NLP tasks in the GLUE benchmark can be solved by fine-tuning only a
small portion of LayerNorm with negligible performance degradation.
\\ ( https://arxiv.org/abs/2403.20284 ,  6227kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20288
Date: Fri, 29 Mar 2024 16:59:13 GMT   (3551kb,D)

Title: Can LLMs Correct Physicians, Yet? Investigating Effective Interaction
  Methods in the Medical Domain
Authors: Burcu Sayin, Pasquale Minervini, Jacopo Staiano, Andrea Passerini
Categories: cs.CL cs.AI
\\
  We explore the potential of Large Language Models (LLMs) to assist and
potentially correct physicians in medical decision-making tasks. We evaluate
several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability
of these models to interact effectively with physicians across different
scenarios. We consider questions from PubMedQA and several tasks, ranging from
binary (yes/no) responses to long answer generation, where the answer of the
model is produced after an interaction with a physician. Our findings suggest
that prompt design significantly influences the downstream accuracy of LLMs and
that LLMs can provide valuable feedback to physicians, challenging incorrect
diagnoses and contributing to more accurate decision-making. For example, when
the physician is accurate 38% of the time, Mistral can produce the correct
answer, improving accuracy up to 74% depending on the prompt being used, while
Llama2 and Meditron models exhibit greater sensitivity to prompt choice. Our
analysis also uncovers the challenges of ensuring that LLM-generated
suggestions are pertinent and useful, emphasizing the need for further research
in this area.
\\ ( https://arxiv.org/abs/2403.20288 ,  3551kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20289
Date: Fri, 29 Mar 2024 17:00:55 GMT   (1894kb,D)

Title: Emotion-Anchored Contrastive Learning Framework for Emotion Recognition
  in Conversation
Authors: Fangxu Yu, Junjie Guo, Zhen Wu, Xinyu Dai
Categories: cs.CL cs.SD eess.AS
Comments: Accepted by Findings of NAACL 2024
\\
  Emotion Recognition in Conversation (ERC) involves detecting the underlying
emotion behind each utterance within a conversation. Effectively generating
representations for utterances remains a significant challenge in this task.
Recent works propose various models to address this issue, but they still
struggle with differentiating similar emotions such as excitement and
happiness. To alleviate this problem, We propose an Emotion-Anchored
Contrastive Learning (EACL) framework that can generate more distinguishable
utterance representations for similar emotions. To achieve this, we utilize
label encodings as anchors to guide the learning of utterance representations
and design an auxiliary loss to ensure the effective separation of anchors for
similar emotions. Moreover, an additional adaptation process is proposed to
adapt anchors to serve as effective classifiers to improve classification
performance. Across extensive experiments, our proposed EACL achieves
state-of-the-art emotion recognition performance and exhibits superior
performance on similar emotions. Our code is available at
https://github.com/Yu-Fangxu/EACL.
\\ ( https://arxiv.org/abs/2403.20289 ,  1894kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20308
Date: Fri, 29 Mar 2024 17:22:53 GMT   (8580kb,D)

Title: ChainNet: Structured Metaphor and Metonymy in WordNet
Authors: Rowan Hall Maudslay, Simone Teufel, Francis Bond, James Pustejovsky
Categories: cs.CL cs.AI
\\
  The senses of a word exhibit rich internal structure. In a typical lexicon,
this structure is overlooked: a word's senses are encoded as a list without
inter-sense relations. We present ChainNet, a lexical resource which for the
first time explicitly identifies these structures. ChainNet expresses how
senses in the Open English Wordnet are derived from one another: every nominal
sense of a word is either connected to another sense by metaphor or metonymy,
or is disconnected in the case of homonymy. Because WordNet senses are linked
to resources which capture information about their meaning, ChainNet represents
the first dataset of grounded metaphor and metonymy.
\\ ( https://arxiv.org/abs/2403.20308 ,  8580kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20322
Date: Fri, 29 Mar 2024 17:50:28 GMT   (8522kb,D)

Title: Towards a Framework for Evaluating Explanations in Automated Fact
  Verification
Authors: Neema Kotonya and Francesca Toni
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\
  As deep neural models in NLP become more complex, and as a consequence
opaque, the necessity to interpret them becomes greater. A burgeoning interest
has emerged in rationalizing explanations to provide short and coherent
justifications for predictions. In this position paper, we advocate for a
formal framework for key concepts and properties about rationalizing
explanations to support their evaluation systematically. We also outline one
such formal framework, tailored to rationalizing explanations of increasingly
complex structures, from free-form explanations to deductive explanations, to
argumentative explanations (with the richest structure). Focusing on the
automated fact verification task, we provide illustrations of the use and
usefulness of our formalization for evaluating explanations, tailored to their
varying structures.
\\ ( https://arxiv.org/abs/2403.20322 ,  8522kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20327
Date: Fri, 29 Mar 2024 17:56:40 GMT   (1502kb,D)

Title: Gecko: Versatile Text Embeddings Distilled from Large Language Models
Authors: Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy R.
  Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher
  Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya
  Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang,
  Iftekhar Naim
Categories: cs.CL cs.AI
Comments: 18 pages
\\
  We present Gecko, a compact and versatile text embedding model. Gecko
achieves strong retrieval performance by leveraging a key idea: distilling
knowledge from large language models (LLMs) into a retriever. Our two-step
distillation process begins with generating diverse, synthetic paired data
using an LLM. Next, we further refine the data quality by retrieving a set of
candidate passages for each query, and relabeling the positive and hard
negative passages using the same LLM. The effectiveness of our approach is
demonstrated by the compactness of the Gecko. On the Massive Text Embedding
Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing
entries with 768 embedding size. Gecko with 768 embedding dimensions achieves
an average score of 66.31, competing with 7x larger models and 5x higher
dimensional embeddings.
\\ ( https://arxiv.org/abs/2403.20327 ,  1502kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20329
Date: Fri, 29 Mar 2024 17:59:06 GMT   (7019kb,D)

Title: ReALM: Reference Resolution As Language Modeling
Authors: Joel Ruben Antony Moniz, Soundarya Krishnan, Melis Ozyildirim,
  Prathamesh Saraf, Halim Cagri Ates, Yuan Zhang, Hong Yu, Nidhi Rajshree
Categories: cs.CL cs.AI cs.LG
\\
  Reference resolution is an important problem, one that is essential to
understand and successfully handle context of different kinds. This context
includes both previous turns and context that pertains to non-conversational
entities, such as entities on the user's screen or those running in the
background. While LLMs have been shown to be extremely powerful for a variety
of tasks, their use in reference resolution, particularly for
non-conversational entities, remains underutilized. This paper demonstrates how
LLMs can be used to create an extremely effective system to resolve references
of various types, by showing how reference resolution can be converted into a
language modeling problem, despite involving forms of entities like those on
screen that are not traditionally conducive to being reduced to a text-only
modality. We demonstrate large improvements over an existing system with
similar functionality across different types of references, with our smallest
model obtaining absolute gains of over 5% for on-screen references. We also
benchmark against GPT-3.5 and GPT-4, with our smallest model achieving
performance comparable to that of GPT-4, and our larger models substantially
outperforming it.
\\ ( https://arxiv.org/abs/2403.20329 ,  7019kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19669
Date: Thu, 15 Feb 2024 22:19:41 GMT   (338kb,D)

Title: Analyzing the Roles of Language and Vision in Learning from Limited Data
Authors: Allison Chen, Ilia Sucholutsky, Olga Russakovsky, Thomas L. Griffiths
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: 8 pages, 4 figures
\\
  Does language help make sense of the visual world? How important is it to
actually see the world rather than having it described with words? These basic
questions about the nature of intelligence have been difficult to answer
because we only had one example of an intelligent system -- humans -- and
limited access to cases that isolated language or vision. However, the
development of sophisticated Vision-Language Models (VLMs) by artificial
intelligence researchers offers us new opportunities to explore the
contributions that language and vision make to learning about the world. We
ablate components from the cognitive architecture of these models to identify
their contributions to learning new tasks from limited data. We find that a
language model leveraging all components recovers a majority of a VLM's
performance, despite its lack of visual input, and that language seems to allow
this by providing access to prior knowledge and reasoning.
\\ ( https://arxiv.org/abs/2403.19669 ,  338kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19717
Date: Wed, 27 Mar 2024 17:46:14 GMT   (1545kb,D)

Title: A Picture is Worth 500 Labels: A Case Study of Demographic Disparities
  in Local Machine Learning Models for Instagram and TikTok
Authors: Jack West, Lea Thiemt, Shimaa Ahmed, Maggie Bartig, Kassem Fawaz,
  Suman Banerjee
Categories: cs.LG cs.CR cs.CY
Comments: 18 pages, 13 figures, to appear at IEEE Symposium on Security and
  Privacy 2024
ACM-class: K.4.2; C.4; D.2.2
\\
  Mobile apps have embraced user privacy by moving their data processing to the
user's smartphone. Advanced machine learning (ML) models, such as vision
models, can now locally analyze user images to extract insights that drive
several functionalities. Capitalizing on this new processing model of locally
analyzing user images, we analyze two popular social media apps, TikTok and
Instagram, to reveal (1) what insights vision models in both apps infer about
users from their image and video data and (2) whether these models exhibit
performance disparities with respect to demographics. As vision models provide
signals for sensitive technologies like age verification and facial
recognition, understanding potential biases in these models is crucial for
ensuring that users receive equitable and accurate services.
  We develop a novel method for capturing and evaluating ML tasks in mobile
apps, overcoming challenges like code obfuscation, native code execution, and
scalability. Our method comprises ML task detection, ML pipeline
reconstruction, and ML performance assessment, specifically focusing on
demographic disparities. We apply our methodology to TikTok and Instagram,
revealing significant insights. For TikTok, we find issues in age and gender
prediction accuracy, particularly for minors and Black individuals. In
Instagram, our analysis uncovers demographic disparities in the extraction of
over 500 visual concepts from images, with evidence of spurious correlations
between demographic features and certain concepts.
\\ ( https://arxiv.org/abs/2403.19717 ,  1545kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19721
Date: Wed, 27 Mar 2024 22:39:08 GMT   (2884kb,D)

Title: Computationally and Memory-Efficient Robust Predictive Analytics Using
  Big Data
Authors: Daniel Menges, Adil Rasheed
Categories: cs.LG cs.AI eess.IV
\\
  In the current data-intensive era, big data has become a significant asset
for Artificial Intelligence (AI), serving as a foundation for developing
data-driven models and providing insight into various unknown fields. This
study navigates through the challenges of data uncertainties, storage
limitations, and predictive data-driven modeling using big data. We utilize
Robust Principal Component Analysis (RPCA) for effective noise reduction and
outlier elimination, and Optimal Sensor Placement (OSP) for efficient data
compression and storage. The proposed OSP technique enables data compression
without substantial information loss while simultaneously reducing storage
needs. While RPCA offers an enhanced alternative to traditional Principal
Component Analysis (PCA) for high-dimensional data management, the scope of
this work extends its utilization, focusing on robust, data-driven modeling
applicable to huge data sets in real-time. For that purpose, Long Short-Term
Memory (LSTM) networks, a type of recurrent neural network, are applied to
model and predict data based on a low-dimensional subset obtained from OSP,
leading to a crucial acceleration of the training phase. LSTMs are feasible for
capturing long-term dependencies in time series data, making them particularly
suited for predicting the future states of physical systems on historical data.
All the presented algorithms are not only theorized but also simulated and
validated using real thermal imaging data mapping a ship's engine.
\\ ( https://arxiv.org/abs/2403.19721 ,  2884kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19792
Date: Thu, 28 Mar 2024 19:17:54 GMT   (13192kb,D)

Title: MAPL: Model Agnostic Peer-to-peer Learning
Authors: Sayak Mukherjee, Andrea Simonetto, Hadi Jamali-Rad
Categories: cs.LG cs.AI cs.CR cs.DC
Comments: Our code is available and can be accessed here:
  https://github.com/SayakMukherjee/MAPL
\\
  Effective collaboration among heterogeneous clients in a decentralized
setting is a rather unexplored avenue in the literature. To structurally
address this, we introduce Model Agnostic Peer-to-peer Learning (coined as
MAPL) a novel approach to simultaneously learn heterogeneous personalized
models as well as a collaboration graph through peer-to-peer communication
among neighboring clients. MAPL is comprised of two main modules: (i)
local-level Personalized Model Learning (PML), leveraging a combination of
intra- and inter-client contrastive losses; (ii) network-wide decentralized
Collaborative Graph Learning (CGL) dynamically refining collaboration weights
in a privacy-preserving manner based on local task similarities. Our extensive
experimentation demonstrates the efficacy of MAPL and its competitive (or, in
most cases, superior) performance compared to its centralized model-agnostic
counterparts, without relying on any central server. Our code is available and
can be accessed here: https://github.com/SayakMukherjee/MAPL
\\ ( https://arxiv.org/abs/2403.19792 ,  13192kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19800
Date: Thu, 28 Mar 2024 19:29:17 GMT   (7880kb,D)

Title: Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction
Authors: Jhon A. Castro-Correa, Jhony H. Giraldo, Mohsen Badiey, Fragkiskos D.
  Malliaros
Categories: cs.LG cs.AI eess.SP
Comments: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
DOI: 10.1109/TNNLS.2024.3381069
\\
  Reconstructing time-varying graph signals (or graph time-series imputation)
is a critical problem in machine learning and signal processing with broad
applications, ranging from missing data imputation in sensor networks to
time-series forecasting. Accurately capturing the spatio-temporal information
inherent in these signals is crucial for effectively addressing these tasks.
However, existing approaches relying on smoothness assumptions of temporal
differences and simple convex optimization techniques have inherent
limitations. To address these challenges, we propose a novel approach that
incorporates a learning module to enhance the accuracy of the downstream task.
To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv)
operator, which is a generalization of the conventional Chebyshev graph
convolution by leveraging the theory of Gegenbauer polynomials. By deviating
from traditional convex problems, we expand the complexity of the model and
offer a more accurate solution for recovering time-varying graph signals.
Building upon GegenConv, we design the Gegenbauer-based time Graph Neural
Network (GegenGNN) architecture, which adopts an encoder-decoder structure.
Likewise, our approach also utilizes a dedicated loss function that
incorporates a mean squared error component alongside Sobolev smoothness
regularization. This combination enables GegenGNN to capture both the fidelity
to ground truth and the underlying smoothness properties of the signals,
enhancing the reconstruction performance. We conduct extensive experiments on
real datasets to evaluate the effectiveness of our proposed approach. The
experimental results demonstrate that GegenGNN outperforms state-of-the-art
methods, showcasing its superior capability in recovering time-varying graph
signals.
\\ ( https://arxiv.org/abs/2403.19800 ,  7880kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19806
Date: Thu, 28 Mar 2024 19:41:17 GMT   (1429kb,D)

Title: Feature-Based Echo-State Networks: A Step Towards Interpretability and
  Minimalism in Reservoir Computer
Authors: Debdipta Goswami
Categories: cs.LG cs.SY eess.SY
Comments: 6 pages, 12 figures, 1 table. arXiv admin note: substantial text
  overlap with arXiv:2304.00198, arXiv:2211.05992
\\
  This paper proposes a novel and interpretable recurrent neural-network
structure using the echo-state network (ESN) paradigm for time-series
prediction. While the traditional ESNs perform well for dynamical systems
prediction, it needs a large dynamic reservoir with increased computational
complexity. It also lacks interpretability to discern contributions from
different input combinations to the output. Here, a systematic reservoir
architecture is developed using smaller parallel reservoirs driven by different
input combinations, known as features, and then they are nonlinearly combined
to produce the output. The resultant feature-based ESN (Feat-ESN) outperforms
the traditional single-reservoir ESN with less reservoir nodes. The predictive
capability of the proposed architecture is demonstrated on three systems: two
synthetic datasets from chaotic dynamical systems and a set of real-time
traffic data.
\\ ( https://arxiv.org/abs/2403.19806 ,  1429kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19816
Date: Thu, 28 Mar 2024 20:01:35 GMT   (10340kb,D)

Title: The State of Lithium-Ion Battery Health Prognostics in the CPS Era
Authors: Gaurav Shinde, Rohan Mohapatra, Pooja Krishan, Harish Garg, Srikanth
  Prabhu, Sanchari Das, Mohammad Masum and Saptarshi Sengupta
Categories: cs.LG eess.SP
Comments: 18 pages, 12 figures, 6 tables. arXiv admin note: text overlap with
  arXiv:2310.00023
MSC-class: 68
ACM-class: B.8.1
\\
  Lithium-ion batteries (Li-ion) have revolutionized energy storage technology,
becoming integral to our daily lives by powering a diverse range of devices and
applications. Their high energy density, fast power response, recyclability,
and mobility advantages have made them the preferred choice for numerous
sectors. This paper explores the seamless integration of Prognostics and Health
Management within batteries, presenting a multidisciplinary approach that
enhances the reliability, safety, and performance of these powerhouses.
Remaining useful life (RUL), a critical concept in prognostics, is examined in
depth, emphasizing its role in predicting component failure before it occurs.
The paper reviews various RUL prediction methods, from traditional models to
cutting-edge data-driven techniques. Furthermore, it highlights the paradigm
shift toward deep learning architectures within the field of Li-ion battery
health prognostics, elucidating the pivotal role of deep learning in addressing
battery system complexities. Practical applications of PHM across industries
are also explored, offering readers insights into real-world
implementations.This paper serves as a comprehensive guide, catering to both
researchers and practitioners in the field of Li-ion battery PHM.
\\ ( https://arxiv.org/abs/2403.19816 ,  10340kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19820
Date: Thu, 28 Mar 2024 20:11:34 GMT   (374kb,D)

Title: Evaluating Explanatory Capabilities of Machine Learning Models in
  Medical Diagnostics: A Human-in-the-Loop Approach
Authors: Jos\'e Bobes-Bascar\'an (1), Eduardo Mosqueira-Rey (1), \'Angel
  Fern\'andez-Leal (1), Elena Hern\'andez-Pereira (1), David Alonso-R\'ios (1),
  Vicente Moret-Bonillo (1), Israel Figueirido-Arnoso (1), Yolanda
  Vidal-\'Insua (2) ((1) University of Coru\~na (CITIC), (2) Complejo
  Hospitalario (CHUS))
Categories: cs.LG cs.AI
ACM-class: I.2
\\
  This paper presents a comprehensive study on the evaluation of explanatory
capabilities of machine learning models, with a focus on Decision Trees, Random
Forest and XGBoost models using a pancreatic cancer dataset. We use
Human-in-the-Loop related techniques and medical guidelines as a source of
domain knowledge to establish the importance of the different features that are
relevant to establish a pancreatic cancer treatment. These features are not
only used as a dimensionality reduction approach for the machine learning
models, but also as way to evaluate the explainability capabilities of the
different models using agnostic and non-agnostic explainability techniques. To
facilitate interpretation of explanatory results, we propose the use of
similarity measures such as the Weighted Jaccard Similarity coefficient. The
goal is to not only select the best performing model but also the one that can
best explain its conclusions and aligns with human domain knowledge.
\\ ( https://arxiv.org/abs/2403.19820 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19837
Date: Thu, 28 Mar 2024 21:15:38 GMT   (1929kb,D)

Title: Concept-based Analysis of Neural Networks via Vision-Language Models
Authors: Ravi Mangal, Nina Narodytska, Divya Gopinath, Boyue Caroline Hu,
  Anirban Roy, Susmit Jha, Corina Pasareanu
Categories: cs.LG cs.AI cs.CL cs.CV cs.LO
\\
  Formal analysis of vision-based deep neural networks (DNNs) is highly
desirable but it is very challenging due to the difficulty of expressing formal
specifications for vision tasks and the lack of efficient verification
procedures. In this paper, we propose to leverage emerging multimodal,
vision-language, foundation models (VLMs) as a lens through which we can reason
about vision models. VLMs have been trained on a large body of images
accompanied by their textual description, and are thus implicitly aware of
high-level, human-understandable concepts describing the images. We describe a
logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to
facilitate writing specifications in terms of these concepts. To define and
formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a
VLM, which provides a means to encode and efficiently check natural-language
properties of vision models. We demonstrate our techniques on a ResNet-based
classifier trained on the RIVAL-10 dataset leveraging CLIP as the multimodal
model.
\\ ( https://arxiv.org/abs/2403.19837 ,  1929kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19839
Date: Thu, 28 Mar 2024 21:20:27 GMT   (325kb,D)

Title: The New Agronomists: Language Models are Experts in Crop Management
Authors: Jing Wu, Zhixin Lai, Suiyao Chen, Ran Tao, Pan Zhao, Naira Hovakimyan
Categories: cs.LG cs.AI cs.CL
\\
  Crop management plays a crucial role in determining crop yield, economic
profitability, and environmental sustainability. Despite the availability of
management guidelines, optimizing these practices remains a complex and
multifaceted challenge. In response, previous studies have explored using
reinforcement learning with crop simulators, typically employing simple
neural-network-based reinforcement learning (RL) agents. Building on this
foundation, this paper introduces a more advanced intelligent crop management
system. This system uniquely combines RL, a language model (LM), and crop
simulations facilitated by the Decision Support System for Agrotechnology
Transfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train
management policies that process numerous state variables from the simulator as
observations. A novel aspect of our approach is the conversion of these state
variables into more informative language, facilitating the language model's
capacity to understand states and explore optimal management practices. The
empirical results reveal that the LM exhibits superior learning capabilities.
Through simulation experiments with maize crops in Florida (US) and Zaragoza
(Spain), the LM not only achieves state-of-the-art performance under various
evaluation metrics but also demonstrates a remarkable improvement of over 49\%
in economic profit, coupled with reduced environmental impact when compared to
baseline methods. Our code is available at
\url{https://github.com/jingwu6/LM_AG}.
\\ ( https://arxiv.org/abs/2403.19839 ,  325kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19849
Date: Thu, 28 Mar 2024 21:52:15 GMT   (430kb)

Title: Biased Over-the-Air Federated Learning under Wireless Heterogeneity
Authors: Muhammad Faraz Ul Abrar and Nicol\`o Michelusi
Categories: cs.LG eess.SP
Comments: Accepted at IEEE International Conference on Communications (ICC),
  2024
\\
  Recently, Over-the-Air (OTA) computation has emerged as a promising federated
learning (FL) paradigm that leverages the waveform superposition properties of
the wireless channel to realize fast model updates. Prior work focused on the
OTA device ``pre-scaler" design under \emph{homogeneous} wireless conditions,
in which devices experience the same average path loss, resulting in zero-bias
solutions. Yet, zero-bias designs are limited by the device with the worst
average path loss and hence may perform poorly in \emph{heterogeneous} wireless
settings. In this scenario, there may be a benefit in designing \emph{biased}
solutions, in exchange for a lower variance in the model updates. To optimize
this trade-off, we study the design of OTA device pre-scalers by focusing on
the OTA-FL convergence. We derive an upper bound on the model ``optimality
error", which explicitly captures the effect of bias and variance in terms of
the choice of the pre-scalers. Based on this bound, we identify two solutions
of interest: minimum noise variance, and minimum noise variance zero-bias
solutions. Numerical evaluations show that using OTA device pre-scalers that
minimize the variance of FL updates, while allowing a small bias, can provide
high gains over existing schemes.
\\ ( https://arxiv.org/abs/2403.19849 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19852
Date: Thu, 28 Mar 2024 21:54:48 GMT   (1693kb,D)

Title: A Review of Graph Neural Networks in Epidemic Modeling
Authors: Zewen Liu, Guancheng Wan, B. Aditya Prakash, Max S. Y. Lau, Wei Jin
Categories: cs.LG cs.SI physics.soc-ph q-bio.PE
\\
  Since the onset of the COVID-19 pandemic, there has been a growing interest
in studying epidemiological models. Traditional mechanistic models
mathematically describe the transmission mechanisms of infectious diseases.
However, they often fall short when confronted with the growing challenges of
today. Consequently, Graph Neural Networks (GNNs) have emerged as a
progressively popular tool in epidemic research. In this paper, we endeavor to
furnish a comprehensive review of GNNs in epidemic tasks and highlight
potential future directions. To accomplish this objective, we introduce
hierarchical taxonomies for both epidemic tasks and methodologies, offering a
trajectory of development within this domain. For epidemic tasks, we establish
a taxonomy akin to those typically employed within the epidemic domain. For
methodology, we categorize existing work into \textit{Neural Models} and
\textit{Hybrid Models}. Following this, we perform an exhaustive and systematic
examination of the methodologies, encompassing both the tasks and their
technical details. Furthermore, we discuss the limitations of existing methods
from diverse perspectives and systematically propose future research
directions. This survey aims to bridge literature gaps and promote the
progression of this promising field. We hope that it will facilitate synergies
between the communities of GNNs and epidemiology, and contribute to their
collective progress.
\\ ( https://arxiv.org/abs/2403.19852 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19863
Date: Thu, 28 Mar 2024 22:17:19 GMT   (11860kb,D)

Title: DeNetDM: Debiasing by Network Depth Modulation
Authors: Silpa Vadakkeeveetil Sreelatha, Adarsh Kappiyath, Anjan Dutta
Categories: cs.LG cs.CV
Comments: 23 pages including supplementary
\\
  When neural networks are trained on biased datasets, they tend to
inadvertently learn spurious correlations, leading to challenges in achieving
strong generalization and robustness. Current approaches to address such biases
typically involve utilizing bias annotations, reweighting based on pseudo-bias
labels, or enhancing diversity within bias-conflicting data points through
augmentation techniques. We introduce DeNetDM, a novel debiasing method based
on the observation that shallow neural networks prioritize learning core
attributes, while deeper ones emphasize biases when tasked with acquiring
distinct information. Using a training paradigm derived from Product of
Experts, we create both biased and debiased branches with deep and shallow
architectures and then distill knowledge to produce the target debiased model.
Extensive experiments and analyses demonstrate that our approach outperforms
current debiasing techniques, achieving a notable improvement of around 5% in
three datasets, encompassing both synthetic and real-world data. Remarkably,
DeNetDM accomplishes this without requiring annotations pertaining to bias
labels or bias types, while still delivering performance on par with supervised
counterparts. Furthermore, our approach effectively harnesses the diversity of
bias-conflicting points within the data, surpassing previous methods and
obviating the need for explicit augmentation-based methods to enhance the
diversity of such bias-conflicting points. The source code will be available
upon acceptance.
\\ ( https://arxiv.org/abs/2403.19863 ,  11860kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19871
Date: Thu, 28 Mar 2024 22:45:38 GMT   (3484kb,D)

Title: Towards Stable Machine Learning Model Retraining via Slowly Varying
  Sequences
Authors: Vassilis Digalakis Jr, Yu Ma, Phevos Paschalidis, Dimitris Bertsimas
Categories: cs.LG cs.AI math.OC
Comments: For correspondence, contact Yu Ma, midsumer@mit.edu
\\
  Retraining machine learning models remains an important task for real-world
machine learning model deployment. Existing methods focus largely on greedy
approaches to find the best-performing model without considering the stability
of trained model structures across different retraining evolutions. In this
study, we develop a mixed integer optimization algorithm that holistically
considers the problem of retraining machine learning models across different
data batch updates. Our method focuses on retaining consistent analytical
insights - which is important to model interpretability, ease of
implementation, and fostering trust with users - by using custom-defined
distance metrics that can be directly incorporated into the optimization
problem. Importantly, our method shows stronger stability than greedily trained
models with a small, controllable sacrifice in model performance in a
real-world production case study. Finally, important analytical insights, as
demonstrated using SHAP feature importance, are shown to be consistent across
retraining iterations.
\\ ( https://arxiv.org/abs/2403.19871 ,  3484kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19888
Date: Fri, 29 Mar 2024 00:05:13 GMT   (7814kb,D)

Title: MambaMixer: Efficient Selective State Space Models with Dual Token and
  Channel Selection
Authors: Ali Behrouz and Michele Santacatterina and Ramin Zabih
Categories: cs.LG cs.AI cs.CV
Comments: Work in progress
\\
  Recent advances in deep learning have mainly relied on Transformers due to
their data dependency and ability to learn at scale. The attention module in
these architectures, however, exhibits quadratic time and space in input size,
limiting their scalability for long-sequence modeling. Despite recent attempts
to design efficient and effective architecture backbone for multi-dimensional
data, such as images and multivariate time series, existing models are either
data independent, or fail to allow inter- and intra-dimension communication.
Recently, State Space Models (SSMs), and more specifically Selective State
Space Models, with efficient hardware-aware implementation, have shown
promising potential for long sequence modeling. Motivated by the success of
SSMs, we present MambaMixer, a new architecture with data-dependent weights
that uses a dual selection mechanism across tokens and channels, called
Selective Token and Channel Mixer. MambaMixer connects selective mixers using a
weighted averaging mechanism, allowing layers to have direct access to early
features. As a proof of concept, we design Vision MambaMixer (ViM2) and Time
Series MambaMixer (TSM2) architectures based on the MambaMixer block and
explore their performance in various vision and time series forecasting tasks.
Our results underline the importance of selective mixing across both tokens and
channels. In ImageNet classification, object detection, and semantic
segmentation tasks, ViM2 achieves competitive performance with well-established
vision models and outperforms SSM-based vision models. In time series
forecasting, TSM2 achieves outstanding performance compared to state-of-the-art
methods while demonstrating significantly improved computational cost. These
results show that while Transformers, cross-channel attention, and MLPs are
sufficient for good performance in time series forecasting, neither is
necessary.
\\ ( https://arxiv.org/abs/2403.19888 ,  7814kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19896
Date: Fri, 29 Mar 2024 00:33:37 GMT   (611kb)

Title: Nonlinearity Enhanced Adaptive Activation Function
Authors: David Yevick
Categories: cs.LG cs.CV cs.NE
\\
  A simply implemented activation function with even cubic nonlinearity is
introduced that increases the accuracy of neural networks without substantial
additional computational resources. This is partially enabled through an
apparent tradeoff between convergence and accuracy. The activation function
generalizes the standard RELU function by introducing additional degrees of
freedom through optimizable parameters that enable the degree of nonlinearity
to be adjusted. The associated accuracy enhancement is quantified in the
context of the MNIST digit data set through a comparison with standard
techniques.
\\ ( https://arxiv.org/abs/2403.19896 ,  611kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19907
Date: Fri, 29 Mar 2024 01:25:05 GMT   (2839kb)

Title: Beyond the Known: Novel Class Discovery for Open-world Graph Learning
Authors: Yucheng Jin and Yun Xiong and Juncheng Fang and Xixi Wu and Dongxiao
  He and Xing Jia and Bingchen Zhao and Philip Yu
Categories: cs.LG cs.AI
\\
  Node classification on graphs is of great importance in many applications.
Due to the limited labeling capability and evolution in real-world open
scenarios, novel classes can emerge on unlabeled testing nodes. However, little
attention has been paid to novel class discovery on graphs. Discovering novel
classes is challenging as novel and known class nodes are correlated by edges,
which makes their representations indistinguishable when applying message
passing GNNs. Furthermore, the novel classes lack labeling information to guide
the learning process. In this paper, we propose a novel method Open-world gRAph
neuraL network (ORAL) to tackle these challenges. ORAL first detects
correlations between classes through semi-supervised prototypical learning.
Inter-class correlations are subsequently eliminated by the prototypical
attention network, leading to distinctive representations for different
classes. Furthermore, to fully explore multi-scale graph features for
alleviating label deficiencies, ORAL generates pseudo-labels by aligning and
ensembling label estimations from multiple stacked prototypical attention
networks. Extensive experiments on several benchmark datasets show the
effectiveness of our proposed method.
\\ ( https://arxiv.org/abs/2403.19907 ,  2839kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19925
Date: Fri, 29 Mar 2024 02:25:55 GMT   (134kb,D)

Title: Decision Mamba: Reinforcement Learning via Sequence Modeling with
  Selective State Spaces
Authors: Toshihiro Ota
Categories: cs.LG cs.AI
Comments: 8 pages, 1 figure
Report-no: RIKEN-iTHEMS-Report-24
\\
  Decision Transformer, a promising approach that applies Transformer
architectures to reinforcement learning, relies on causal self-attention to
model sequences of states, actions, and rewards. While this method has shown
competitive results, this paper investigates the integration of the Mamba
framework, known for its advanced capabilities in efficient and effective
sequence modeling, into the Decision Transformer architecture, focusing on the
potential performance enhancements in sequential decision-making tasks. Our
study systematically evaluates this integration by conducting a series of
experiments across various decision-making environments, comparing the modified
Decision Transformer, Decision Mamba, with its traditional counterpart. This
work contributes to the advancement of sequential decision-making models,
suggesting that the architecture and training methodology of neural networks
can significantly impact their performance in complex tasks, and highlighting
the potential of Mamba as a valuable tool for improving the efficacy of
Transformer-based models in reinforcement learning scenarios.
\\ ( https://arxiv.org/abs/2403.19925 ,  134kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19943
Date: Fri, 29 Mar 2024 02:54:41 GMT   (44037kb,D)

Title: TDANet: A Novel Temporal Denoise Convolutional Neural Network With
  Attention for Fault Diagnosis
Authors: Zhongzhi Li, Rong Fan, Jingqi Tu, Jinyi Ma, Jianliang Ai and Yiqun
  Dong
Categories: cs.LG cs.AI eess.SP
\\
  Fault diagnosis plays a crucial role in maintaining the operational integrity
of mechanical systems, preventing significant losses due to unexpected
failures. As intelligent manufacturing and data-driven approaches evolve, Deep
Learning (DL) has emerged as a pivotal technique in fault diagnosis research,
recognized for its ability to autonomously extract complex features. However,
the practical application of current fault diagnosis methods is challenged by
the complexity of industrial environments. This paper proposed the Temporal
Denoise Convolutional Neural Network With Attention (TDANet), designed to
improve fault diagnosis performance in noise environments. This model
transforms one-dimensional signals into two-dimensional tensors based on their
periodic properties, employing multi-scale 2D convolution kernels to extract
signal information both within and across periods. This method enables
effective identification of signal characteristics that vary over multiple time
scales. The TDANet incorporates a Temporal Variable Denoise (TVD) module with
residual connections and a Multi-head Attention Fusion (MAF) module, enhancing
the saliency of information within noisy data and maintaining effective fault
diagnosis performance. Evaluation on two datasets, CWRU (single sensor) and
Real aircraft sensor fault (multiple sensors), demonstrates that the TDANet
model significantly outperforms existing deep learning approaches in terms of
diagnostic accuracy under noisy environments.
\\ ( https://arxiv.org/abs/2403.19943 ,  44037kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19950
Date: Fri, 29 Mar 2024 03:16:29 GMT   (471kb,D)

Title: Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data
Authors: Xin Zou, Weiwei Liu
Categories: cs.LG
Journal-ref: AAAI (2024) Vol. 38, No. 15, pages 17263-17270
DOI: 10.1609/aaai.v38i15.29673
\\
  Out-of-distribution (OOD) generalization has attracted increasing research
attention in recent years, due to its promising experimental results in
real-world applications. In this paper,we study the confidence set prediction
problem in the OOD generalization setting. Split conformal prediction (SCP) is
an efficient framework for handling the confidence set prediction problem.
However, the validity of SCP requires the examples to be exchangeable, which is
violated in the OOD setting. Empirically, we show that trivially applying SCP
results in a failure to maintain the marginal coverage when the unseen target
domain is different from the source domain. To address this issue, we develop a
method for forming confident prediction sets in the OOD setting and
theoretically prove the validity of our method. Finally, we conduct experiments
on simulated data to empirically verify the correctness of our theory and the
validity of our proposed method.
\\ ( https://arxiv.org/abs/2403.19950 ,  471kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19996
Date: Fri, 29 Mar 2024 06:24:07 GMT   (1071kb,D)

Title: DeepHeteroIoT: Deep Local and Global Learning over Heterogeneous IoT
  Sensor Data
Authors: Muhammad Sakib Khan Inan, Kewen Liao, Haifeng Shen, Prem Prakash
  Jayaraman, Dimitrios Georgakopoulos, Ming Jian Tang
Categories: cs.LG eess.SP
Comments: Accepted for Publication and Presented in EAI MobiQuitous 2023 - 20th
  EAI International Conference on Mobile and Ubiquitous Systems: Computing,
  Networking and Services
\\
  Internet of Things (IoT) sensor data or readings evince variations in
timestamp range, sampling frequency, geographical location, unit of
measurement, etc. Such presented sequence data heterogeneity makes it difficult
for traditional time series classification algorithms to perform well.
Therefore, addressing the heterogeneity challenge demands learning not only the
sub-patterns (local features) but also the overall pattern (global feature). To
address the challenge of classifying heterogeneous IoT sensor data (e.g.,
categorizing sensor data types like temperature and humidity), we propose a
novel deep learning model that incorporates both Convolutional Neural Network
and Bi-directional Gated Recurrent Unit to learn local and global features
respectively, in an end-to-end manner. Through rigorous experimentation on
heterogeneous IoT sensor datasets, we validate the effectiveness of our
proposed model, which outperforms recent state-of-the-art classification
methods as well as several machine learning and deep learning baselines. In
particular, the model achieves an average absolute improvement of 3.37% in
Accuracy and 2.85% in F1-Score across datasets
\\ ( https://arxiv.org/abs/2403.19996 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20047
Date: Fri, 29 Mar 2024 08:33:05 GMT   (696kb,D)

Title: Embracing Unknown Step by Step: Towards Reliable Sparse Training in Real
  World
Authors: Bowen Lei, Dongkuan Xu, Ruqi Zhang, Bani Mallick
Categories: cs.LG cs.CV
\\
  Sparse training has emerged as a promising method for resource-efficient deep
neural networks (DNNs) in real-world applications. However, the reliability of
sparse models remains a crucial concern, particularly in detecting unknown
out-of-distribution (OOD) data. This study addresses the knowledge gap by
investigating the reliability of sparse training from an OOD perspective and
reveals that sparse training exacerbates OOD unreliability. The lack of unknown
information and the sparse constraints hinder the effective exploration of
weight space and accurate differentiation between known and unknown knowledge.
To tackle these challenges, we propose a new unknown-aware sparse training
method, which incorporates a loss modification, auto-tuning strategy, and a
voting scheme to guide weight space exploration and mitigate confusion between
known and unknown information without incurring significant additional costs or
requiring access to additional OOD data. Theoretical insights demonstrate how
our method reduces model confidence when faced with OOD samples. Empirical
experiments across multiple datasets, model architectures, and sparsity levels
validate the effectiveness of our method, with improvements of up to
\textbf{8.4\%} in AUROC while maintaining comparable or higher accuracy and
calibration. This research enhances the understanding and readiness of sparse
DNNs for deployment in resource-limited applications. Our code is available on:
\url{https://github.com/StevenBoys/MOON}.
\\ ( https://arxiv.org/abs/2403.20047 ,  696kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20075
Date: Fri, 29 Mar 2024 09:17:40 GMT   (1535kb)

Title: Adaptive Decentralized Federated Learning in Energy and Latency
  Constrained Wireless Networks
Authors: Zhigang Yan and Dong Li
Categories: cs.LG cs.SY eess.SY
\\
  In Federated Learning (FL), with parameter aggregated by a central node, the
communication overhead is a substantial concern. To circumvent this limitation
and alleviate the single point of failure within the FL framework, recent
studies have introduced Decentralized Federated Learning (DFL) as a viable
alternative. Considering the device heterogeneity, and energy cost associated
with parameter aggregation, in this paper, the problem on how to efficiently
leverage the limited resources available to enhance the model performance is
investigated. Specifically, we formulate a problem that minimizes the loss
function of DFL while considering energy and latency constraints. The proposed
solution involves optimizing the number of local training rounds across diverse
devices with varying resource budgets. To make this problem tractable, we first
analyze the convergence of DFL with edge devices with different rounds of local
training. The derived convergence bound reveals the impact of the rounds of
local training on the model performance. Then, based on the derived bound, the
closed-form solutions of rounds of local training in different devices are
obtained. Meanwhile, since the solutions require the energy cost of aggregation
as low as possible, we modify different graph-based aggregation schemes to
solve this energy consumption minimization problem, which can be applied to
different communication scenarios. Finally, a DFL framework which jointly
considers the optimized rounds of local training and the energy-saving
aggregation scheme is proposed. Simulation results show that, the proposed
algorithm achieves a better performance than the conventional schemes with
fixed rounds of local training, and consumes less energy than other traditional
aggregation schemes.
\\ ( https://arxiv.org/abs/2403.20075 ,  1535kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20109
Date: Fri, 29 Mar 2024 10:44:51 GMT   (6608kb)

Title: Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic
  Rewards for Goal-directed Molecular Generation
Authors: Jinyeong Park, Jaegyoon Ahn, Jonghwan Choi, Jibum Kim
Categories: cs.LG cs.AI q-bio.BM
\\
  Optimizing techniques for discovering molecular structures with desired
properties is crucial in artificial intelligence(AI)-based drug discovery.
Combining deep generative models with reinforcement learning has emerged as an
effective strategy for generating molecules with specific properties. Despite
its potential, this approach is ineffective in exploring the vast chemical
space and optimizing particular chemical properties. To overcome these
limitations, we present Mol-AIR, a reinforcement learning-based framework using
adaptive intrinsic rewards for effective goal-directed molecular generation.
Mol-AIR leverages the strengths of both history-based and learning-based
intrinsic rewards by exploiting random distillation network and counting-based
strategies. In benchmark tests, Mol-AIR demonstrates superior performance over
existing approaches in generating molecules with desired properties without any
prior knowledge, including penalized LogP, QED, and celecoxib similarity. We
believe that Mol-AIR represents a significant advancement in drug discovery,
offering a more efficient path to discovering novel therapeutics.
\\ ( https://arxiv.org/abs/2403.20109 ,  6608kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20122
Date: Fri, 29 Mar 2024 11:23:10 GMT   (606kb)

Title: Learning using granularity statistical invariants for classification
Authors: Ting-Ting Zhu, Yuan-Hai Shao, Chun-Na Li, Tian Liu
Categories: cs.LG
\\
  Learning using statistical invariants (LUSI) is a new learning paradigm,
which adopts weak convergence mechanism, and can be applied to a wider range of
classification problems. However, the computation cost of invariant matrices in
LUSI is high for large-scale datasets during training. To settle this issue,
this paper introduces a granularity statistical invariant for LUSI, and
develops a new learning paradigm called learning using granularity statistical
invariants (LUGSI). LUGSI employs both strong and weak convergence mechanisms,
taking a perspective of minimizing expected risk. As far as we know, it is the
first time to construct granularity statistical invariants. Compared to LUSI,
the introduction of this new statistical invariant brings two advantages.
Firstly, it enhances the structural information of the data. Secondly, LUGSI
transforms a large invariant matrix into a smaller one by maximizing the
distance between classes, achieving feasibility for large-scale datasets
classification problems and significantly enhancing the training speed of model
operations. Experimental results indicate that LUGSI not only exhibits improved
generalization capabilities but also demonstrates faster training speed,
particularly for large-scale datasets.
\\ ( https://arxiv.org/abs/2403.20122 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20124
Date: Fri, 29 Mar 2024 11:27:37 GMT   (1341kb)

Title: Application of Machine Learning Algorithms in Classifying Postoperative
  Success in Metabolic Bariatric Surgery: A Comprehensive Study
Authors: Jos\'e Alberto Ben\'itez-Andrades, Camino Prada-Garc\'ia, Rub\'en
  Garc\'ia-Fern\'andez, Mar\'ia D. Ballesteros-Pomar, Mar\'ia-Inmaculada
  Gonz\'alez-Alonso and Antonio Serrano-Garc\'ia
Categories: cs.LG q-bio.QM
Journal-ref: DIGITAL HEALTH (2024), 10, 20552076241239274
DOI: 10.1177/20552076241239274
\\
  Objectives: Metabolic Bariatric Surgery (MBS) is a critical intervention for
patients living with obesity and related health issues. Accurate classification
and prediction of patient outcomes are vital for optimizing treatment
strategies. This study presents a novel machine learning approach to classify
patients in the context of metabolic bariatric surgery, providing insights into
the efficacy of different models and variable types. Methods: Various machine
learning models, including GaussianNB, ComplementNB, KNN, Decision Tree, KNN
with RandomOverSampler, and KNN with SMOTE, were applied to a dataset of 73
patients. The dataset, comprising psychometric, socioeconomic, and analytical
variables, was analyzed to determine the most efficient predictive model. The
study also explored the impact of different variable groupings and oversampling
techniques. Results: Experimental results indicate average accuracy values as
high as 66.7% for the best model. Enhanced versions of KNN and Decision Tree,
along with variations of KNN such as RandomOverSampler and SMOTE, yielded the
best results. Conclusions: The study unveils a promising avenue for classifying
patients in the realm of metabolic bariatric surgery. The results underscore
the importance of selecting appropriate variables and employing diverse
approaches to achieve optimal performance. The developed system holds potential
as a tool to assist healthcare professionals in decision-making, thereby
enhancing metabolic bariatric surgery outcomes. These findings lay the
groundwork for future collaboration between hospitals and healthcare entities
to improve patient care through the utilization of machine learning algorithms.
Moreover, the findings suggest room for improvement, potentially achievable
with a larger dataset and careful parameter tuning.
\\ ( https://arxiv.org/abs/2403.20124 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20149
Date: Fri, 29 Mar 2024 12:34:57 GMT   (3174kb,D)

Title: Conformal Prediction for Stochastic Decision-Making of PV Power in
  Electricity Markets
Authors: Yvet Renkema, Nico Brinkel and Tarek Alskaif
Categories: cs.LG cs.SY eess.SY stat.ML
\\
  This paper studies the use of conformal prediction (CP), an emerging
probabilistic forecasting method, for day-ahead photovoltaic power predictions
to enhance participation in electricity markets. First, machine learning models
are used to construct point predictions. Thereafter, several variants of CP are
implemented to quantify the uncertainty of those predictions by creating CP
intervals and cumulative distribution functions. Optimal quantity bids for the
electricity market are estimated using several bidding strategies under
uncertainty, namely: trust-the-forecast, worst-case, Newsvendor and expected
utility maximization (EUM). Results show that CP in combination with k-nearest
neighbors and/or Mondrian binning outperforms its corresponding linear quantile
regressors. Using CP in combination with certain bidding strategies can yield
high profit with minimal energy imbalance. In concrete, using conformal
predictive systems with k-nearest neighbors and Mondrian binning after random
forest regression yields the best profit and imbalance regardless of the
decision-making strategy. Combining this uncertainty quantification method with
the EUM strategy with conditional value at risk (CVaR) can yield up to 93\% of
the potential profit with minimal energy imbalance.
\\ ( https://arxiv.org/abs/2403.20149 ,  3174kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20150
Date: Fri, 29 Mar 2024 12:37:57 GMT   (11173kb,D)

Title: TFB: Towards Comprehensive and Fair Benchmarking of Time Series
  Forecasting Methods
Authors: Xiangfei Qiu, Jilin Hu, Lekui Zhou, Xingjian Wu, Junyang Du, Buang
  Zhang, Chenjuan Guo, Aoying Zhou, Christian S. Jensen, Zhenli Sheng and Bin
  Yang
Categories: cs.LG cs.AI cs.CY
Comments: Accepted by PVLDB 2024
\\
  Time series are generated in diverse domains such as economic, traffic,
health, and energy, where forecasting of future values has numerous important
applications. Not surprisingly, many forecasting methods are being proposed. To
ensure progress, it is essential to be able to study and compare such methods
empirically in a comprehensive and reliable manner. To achieve this, we propose
TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB
advances the state-of-the-art by addressing shortcomings related to datasets,
comparison methods, and evaluation pipelines: 1) insufficient coverage of data
domains, 2) stereotype bias against traditional methods, and 3) inconsistent
and inflexible pipelines. To achieve better domain coverage, we include
datasets from 10 different domains: traffic, electricity, energy, the
environment, nature, economic, stock markets, banking, health, and the web. We
also provide a time series characterization to ensure that the selected
datasets are comprehensive. To remove biases against some methods, we include a
diverse range of methods, including statistical learning, machine learning, and
deep learning methods, and we also support a variety of evaluation strategies
and metrics to ensure a more comprehensive evaluations of different methods. To
support the integration of different methods into the benchmark and enable fair
comparisons, TFB features a flexible and scalable pipeline that eliminates
biases. Next, we employ TFB to perform a thorough evaluation of 21 Univariate
Time Series Forecasting (UTSF) methods on 8,068 univariate time series and 14
Multivariate Time Series Forecasting (MTSF) methods on 25 datasets. The
benchmark code and data are available at
https://github.com/decisionintelligence/TFB.
\\ ( https://arxiv.org/abs/2403.20150 ,  11173kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20156
Date: Fri, 29 Mar 2024 13:05:59 GMT   (5292kb,D)

Title: CAESAR: Enhancing Federated RL in Heterogeneous MDPs through
  Convergence-Aware Sampling with Screening
Authors: Hei Yi Mak, Flint Xiaofeng Fan, Luca A. Lanzend\"orfer, Cheston Tan,
  Wei Tsang Ooi, Roger Wattenhofer
Categories: cs.LG cs.AI
\\
  In this study, we delve into Federated Reinforcement Learning (FedRL) in the
context of value-based agents operating across diverse Markov Decision
Processes (MDPs). Existing FedRL methods typically aggregate agents' learning
by averaging the value functions across them to improve their performance.
However, this aggregation strategy is suboptimal in heterogeneous environments
where agents converge to diverse optimal value functions. To address this
problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR)
aggregation scheme designed to enhance the learning of individual agents across
varied MDPs. CAESAR is an aggregation strategy used by the server that combines
convergence-aware sampling with a screening mechanism. By exploiting the fact
that agents learning in identical MDPs are converging to the same optimal value
function, CAESAR enables the selective assimilation of knowledge from more
proficient counterparts, thereby significantly enhancing the overall learning
efficiency. We empirically validate our hypothesis and demonstrate the
effectiveness of CAESAR in enhancing the learning efficiency of agents, using
both a custom-built GridWorld environment and the classical FrozenLake-v1 task,
each presenting varying levels of environmental heterogeneity.
\\ ( https://arxiv.org/abs/2403.20156 ,  5292kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20208
Date: Fri, 29 Mar 2024 14:41:21 GMT   (712kb,D)

Title: Unleashing the Potential of Large Language Models for Predictive Tabular
  Tasks in Data Science
Authors: Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu
Categories: cs.LG cs.AI
Comments: 10 pages
\\
  In the domain of data science, the predictive tasks of classification,
regression, and imputation of missing values are commonly encountered
challenges associated with tabular data. This research endeavors to apply Large
Language Models (LLMs) towards addressing these predictive tasks. Despite their
proficiency in comprehending natural language, LLMs fall short in dealing with
structured tabular data. This limitation stems from their lacking exposure to
the intricacies of tabular data during their foundational training. Our
research aims to mitigate this gap by compiling a comprehensive corpus of
tables annotated with instructions and executing large-scale training of
Llama-2 on this enriched dataset. Furthermore, we investigate the practical
application of applying the trained model to zero-shot prediction, few-shot
prediction, and in-context learning scenarios. Through extensive experiments,
our methodology has shown significant improvements over existing benchmarks.
These advancements highlight the efficacy of tailoring LLM training to solve
table-related problems in data science, thereby establishing a new benchmark in
the utilization of LLMs for enhancing tabular intelligence.
\\ ( https://arxiv.org/abs/2403.20208 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20221
Date: Fri, 29 Mar 2024 15:05:57 GMT   (288kb,D)

Title: Graph Neural Aggregation-diffusion with Metastability
Authors: Kaiyuan Cui, Xinyan Wang, Zicheng Zhang, Weichen Zhao
Categories: cs.LG cs.AI
Comments: 10 pages, 2 figures
\\
  Continuous graph neural models based on differential equations have expanded
the architecture of graph neural networks (GNNs). Due to the connection between
graph diffusion and message passing, diffusion-based models have been widely
studied. However, diffusion naturally drives the system towards an equilibrium
state, leading to issues like over-smoothing. To this end, we propose GRADE
inspired by graph aggregation-diffusion equations, which includes the delicate
balance between nonlinear diffusion and aggregation induced by interaction
potentials. The node representations obtained through aggregation-diffusion
equations exhibit metastability, indicating that features can aggregate into
multiple clusters. In addition, the dynamics within these clusters can persist
for long time periods, offering the potential to alleviate over-smoothing
effects. This nonlinear diffusion in our model generalizes existing
diffusion-based models and establishes a connection with classical GNNs. We
prove that GRADE achieves competitive performance across various benchmarks and
alleviates the over-smoothing issue in GNNs evidenced by the enhanced Dirichlet
energy.
\\ ( https://arxiv.org/abs/2403.20221 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20246
Date: Fri, 29 Mar 2024 15:45:25 GMT   (2145kb,D)

Title: Enhancing Dimension-Reduced Scatter Plots with Class and Feature
  Centroids
Authors: Daniel B. Hier, Tayo Obafemi-Ajayi, Gayla R. Olbricht, Devin M. Burns,
  Sasha Petrenko, Donald C. Wunsch II
Categories: cs.LG cs.HC
Comments: Submitted to 46th Annual International Conference of the IEEE
  Engineering in Medicine and Biology Society
ACM-class: J.3
\\
  Dimension reduction is increasingly applied to high-dimensional biomedical
data to improve its interpretability. When datasets are reduced to two
dimensions, each observation is assigned an x and y coordinates and is
represented as a point on a scatter plot. A significant challenge lies in
interpreting the meaning of the x and y axes due to the complexities inherent
in dimension reduction. This study addresses this challenge by using the x and
y coordinates derived from dimension reduction to calculate class and feature
centroids, which can be overlaid onto the scatter plots. This method connects
the low-dimension space to the original high-dimensional space. We illustrate
the utility of this approach with data derived from the phenotypes of three
neurogenetic diseases and demonstrate how the addition of class and feature
centroids increases the interpretability of scatter plots.
\\ ( https://arxiv.org/abs/2403.20246 ,  2145kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20280
Date: Fri, 29 Mar 2024 16:49:40 GMT   (648kb,D)

Title: Sparse multimodal fusion with modal channel attention
Authors: Josiah Bjorgaard
Categories: cs.LG cs.AI
\\
  The ability of masked multimodal transformer architectures to learn a robust
embedding space when modality samples are sparsely aligned is studied by
measuring the quality of generated embedding spaces as a function of modal
sparsity. An extension to the masked multimodal transformer model is proposed
which incorporates modal-incomplete channels in the multihead attention
mechanism called modal channel attention (MCA). Two datasets with 4 modalities
are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for
multiomics. Models are shown to learn uniform and aligned embedding spaces with
only two out of four modalities in most samples. It was found that, even with
no modal sparsity, the proposed MCA mechanism improves the quality of generated
embedding spaces, recall metrics, and subsequent performance on downstream
tasks.
\\ ( https://arxiv.org/abs/2403.20280 ,  648kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20324
Date: Fri, 29 Mar 2024 17:51:50 GMT   (1335kb,D)

Title: Localising the Seizure Onset Zone from Single-Pulse Electrical
  Stimulation Responses with a Transformer
Authors: Jamie Norris, Aswin Chari, Gerald Cooray, Martin Tisdall, Karl
  Friston, Richard Rosch
Categories: cs.LG eess.SP q-bio.NC
Comments: 15 pages, 7 figures, submitted to CHIL 2024
\\
  Epilepsy is one of the most common neurological disorders, and many patients
require surgical intervention when medication fails to control seizures. For
effective surgical outcomes, precise localisation of the epileptogenic focus -
often approximated through the Seizure Onset Zone (SOZ) - is critical yet
remains a challenge. Active probing through electrical stimulation is already
standard clinical practice for identifying epileptogenic areas. This paper
advances the application of deep learning for SOZ localisation using Single
Pulse Electrical Stimulation (SPES) responses. We achieve this by introducing
Transformer models that incorporate cross-channel attention. We evaluate these
models on held-out patient test sets to assess their generalisability to unseen
patients and electrode placements.
  Our study makes three key contributions: Firstly, we implement an existing
deep learning model to compare two SPES analysis paradigms - namely, divergent
and convergent. These paradigms evaluate outward and inward effective
connections, respectively. Our findings reveal a notable improvement in moving
from a divergent (AUROC: 0.574) to a convergent approach (AUROC: 0.666),
marking the first application of the latter in this context. Secondly, we
demonstrate the efficacy of the Transformer models in handling heterogeneous
electrode placements, increasing the AUROC to 0.730. Lastly, by incorporating
inter-trial variability, we further refine the Transformer models, with an
AUROC of 0.745, yielding more consistent predictions across patients. These
advancements provide a deeper insight into SOZ localisation and represent a
significant step in modelling patient-specific intracranial EEG electrode
placements in SPES. Future work will explore integrating these models into
clinical decision-making processes to bridge the gap between deep learning
research and practical healthcare applications.
\\ ( https://arxiv.org/abs/2403.20324 ,  1335kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2403.19709 (*cross-listing*)
Date: Mon, 25 Mar 2024 17:21:56 GMT   (128kb,D)

Title: Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of
  Large Speech Models
Authors: Tsendsuren Munkhdalai, Youzheng Chen, Khe Chai Sim, Fadi Biadsy, Tara
  Sainath and Pedro Moreno Mengibar
Categories: eess.AS cs.AI cs.CL cs.LG cs.NE
Comments: 5 pages, 3 figures, 5 tables
\\
  Parameter efficient adaptation methods have become a key mechanism to train
large pre-trained models for downstream tasks. However, their per-task
parameter overhead is considered still high when the number of downstream tasks
to adapt for is large. We introduce an adapter module that has a better
efficiency in large scale multi-task adaptation scenario. Our adapter is
hierarchical in terms of how the adapter parameters are allocated. The adapter
consists of a single shared controller network and multiple task-level adapter
heads to reduce the per-task parameter overhead without performance regression
on downstream tasks. The adapter is also recurrent so the entire adapter
parameters are reused across different layers of the pre-trained model. Our
Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based
approaches as well as full model fine-tuning baseline in both single and
multi-task adaptation settings when evaluated on automatic speech recognition
tasks.
\\ ( https://arxiv.org/abs/2403.19709 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19736 (*cross-listing*)
Date: Thu, 28 Mar 2024 14:54:57 GMT   (7328kb,D)

Title: Physics-Informed Neural Networks for Satellite State Estimation
Authors: Jacob Varey, Jessica D. Ruprecht, Michael Tierney, Ryan Sullenberger
Categories: astro-ph.IM cs.AI cs.LG
\\
  The Space Domain Awareness (SDA) community routinely tracks satellites in
orbit by fitting an orbital state to observations made by the Space
Surveillance Network (SSN). In order to fit such orbits, an accurate model of
the forces that are acting on the satellite is required. Over the past several
decades, high-quality, physics-based models have been developed for satellite
state estimation and propagation. These models are exceedingly good at
estimating and propagating orbital states for non-maneuvering satellites;
however, there are several classes of anomalous accelerations that a satellite
might experience which are not well-modeled, such as satellites that use
low-thrust electric propulsion to modify their orbit. Physics-Informed Neural
Networks (PINNs) are a valuable tool for these classes of satellites as they
combine physics models with Deep Neural Networks (DNNs), which are highly
expressive and versatile function approximators. By combining a physics model
with a DNN, the machine learning model need not learn astrodynamics, which
results in more efficient and effective utilization of machine learning
resources. This paper details the application of PINNs to estimate the orbital
state and a continuous, low-amplitude anomalous acceleration profile for
satellites. The PINN is trained to learn the unknown acceleration by minimizing
the mean square error of observations. We evaluate the performance of pure
physics models with PINNs in terms of their observation residuals and their
propagation accuracy beyond the fit span of the observations. For a two-day
simulation of a GEO satellite using an unmodeled acceleration profile on the
order of $10^{-8} \text{ km/s}^2$, the PINN outperformed the best-fit physics
model by orders of magnitude for both observation residuals (123 arcsec vs 1.00
arcsec) as well as propagation accuracy (3860 km vs 164 km after five days).
\\ ( https://arxiv.org/abs/2403.19736 ,  7328kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19758 (*cross-listing*)
Date: Thu, 28 Mar 2024 18:15:07 GMT   (2504kb,D)

Title: Natural Language, AI, and Quantum Computing in 2024: Research
  Ingredients and Directions in QNLP
Authors: Dominic Widdows, Willie Aboumrad, Dohun Kim, Sayonee Ray, Jonathan Mei
Categories: quant-ph cs.AI cs.CL
\\
  Language processing is at the heart of current developments in artificial
intelligence, and quantum computers are becoming available at the same time.
This has led to great interest in quantum natural language processing, and
several early proposals and experiments. This paper surveys the state of this
area, showing how NLP-related techniques including word embeddings, sequential
models, attention, and grammatical parsing have been used in quantum language
processing. We introduce a new quantum design for the basic task of text
encoding (representing a string of characters in memory), which has not been
addressed in detail before.
  As well as motivating new technologies, quantum theory has made key
contributions to the challenging questions of 'What is uncertainty?' and 'What
is intelligence?' As these questions are taking on fresh urgency with
artificial systems, the paper also considers some of the ways facts are
conceptualized and presented in language. In particular, we argue that the
problem of 'hallucinations' arises through a basic misunderstanding: language
expresses any number of plausible hypotheses, only a few of which become
actual, a distinction that is ignored in classical mechanics, but present
(albeit confusing) in quantum mechanics.
\\ ( https://arxiv.org/abs/2403.19758 ,  2504kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19770 (*cross-listing*)
Date: Thu, 28 Mar 2024 18:45:43 GMT   (32381kb,D)

Title: Hierarchical Deep Learning for Intention Estimation of Teleoperation
  Manipulation in Assembly Tasks
Authors: Mingyu Cai, Karankumar Patel, Soshi Iba, Songpo Li
Categories: cs.RO cs.AI cs.LG
Comments: ICRA 2024
\\
  In human-robot collaboration, shared control presents an opportunity to
teleoperate robotic manipulation to improve the efficiency of manufacturing and
assembly processes. Robots are expected to assist in executing the user's
intentions. To this end, robust and prompt intention estimation is needed,
relying on behavioral observations. The framework presents an intention
estimation technique at hierarchical levels i.e., low-level actions and
high-level tasks, by incorporating multi-scale hierarchical information in
neural networks. Technically, we employ hierarchical dependency loss to boost
overall accuracy. Furthermore, we propose a multi-window method that assigns
proper hierarchical prediction windows of input data. An analysis of the
predictive power with various inputs demonstrates the predominance of the deep
hierarchical model in the sense of prediction accuracy and early intention
identification. We implement the algorithm on a virtual reality (VR) setup to
teleoperate robotic hands in a simulation with various assembly tasks to show
the effectiveness of online estimation.
\\ ( https://arxiv.org/abs/2403.19770 ,  32381kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19833 (*cross-listing*)
Date: Thu, 28 Mar 2024 21:04:11 GMT   (13886kb,D)

Title: ChatTracer: Large Language Model Powered Real-time Bluetooth Device
  Tracking System
Authors: Qijun Wang, Shichen Zhang, Kunzhe Song, Huacheng Zeng
Categories: cs.NI cs.AI
\\
  Large language models (LLMs), exemplified by OpenAI ChatGPT and Google Bard,
have transformed the way we interact with cyber technologies. In this paper, we
study the possibility of connecting LLM with wireless sensor networks (WSN). A
successful design will not only extend LLM's knowledge landscape to the
physical world but also revolutionize human interaction with WSN. To the end,
we present ChatTracer, an LLM-powered real-time Bluetooth device tracking
system. ChatTracer comprises three key components: an array of Bluetooth
sniffing nodes, a database, and a fine-tuned LLM. ChatTracer was designed based
on our experimental observation that commercial Apple/Android devices always
broadcast hundreds of BLE packets per minute even in their idle status. Its
novelties lie in two aspects: i) a reliable and efficient BLE packet grouping
algorithm; and ii) an LLM fine-tuning strategy that combines both supervised
fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). We
have built a prototype of ChatTracer with four sniffing nodes. Experimental
results show that ChatTracer not only outperforms existing localization
approaches, but also provides an intelligent interface for user interaction.
\\ ( https://arxiv.org/abs/2403.19833 ,  13886kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19838 (*cross-listing*)
Date: Thu, 28 Mar 2024 21:18:33 GMT   (482kb,D)

Title: Multi-Frame, Lightweight & Efficient Vision-Language Models for Question
  Answering in Autonomous Driving
Authors: Akshay Gopalkrishnan, Ross Greer, Mohan Trivedi
Categories: cs.CV cs.AI
Comments: 9 pages, 3 figures
\\
  Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have
become prominent in autonomous driving research, as these models can provide
interpretable textual reasoning and responses for end-to-end autonomous driving
safety tasks using traffic scene images and other data modalities. However,
current approaches to these systems use expensive large language model (LLM)
backbones and image encoders, making such systems unsuitable for real-time
autonomous driving systems where tight memory constraints exist and fast
inference time is necessary. To address these previous issues, we develop
EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which
performs Visual Question Answering for autonomous driving. In comparison to
previous approaches, EM-VLM4AD requires at least 10 times less memory and
floating point operations, while also achieving higher BLEU-4, METEOR, CIDEr,
and ROGUE scores than the existing baseline on the DriveLM dataset. EM-VLM4AD
also exhibits the ability to extract relevant information from traffic views
related to prompts and can answer questions for various autonomous driving
subtasks. We release our code to train and evaluate our model at
https://github.com/akshaygopalkr/EM-VLM4AD.
\\ ( https://arxiv.org/abs/2403.19838 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19866 (*cross-listing*)
Date: Thu, 28 Mar 2024 22:25:05 GMT   (3720kb,D)

Title: Is Synthetic Image Useful for Transfer Learning? An Investigation into
  Data Generation, Volume, and Utilization
Authors: Yuhang Li, Xin Dong, Chen Chen, Jingtao Li, Yuxin Wen, Michael
  Spranger, Lingjuan Lyu
Categories: cs.CV cs.AI
Comments: ICLR24 Score 6865
  https://openreview.net/forum?id=CjPt1AC6w0&referrer=%5Bthe%20profile%20of%20Chen%20Chen%5D(%2Fprofile%3Fid%3D~Chen_Chen20)
\\
  Synthetic image data generation represents a promising avenue for training
deep learning models, particularly in the realm of transfer learning, where
obtaining real images within a specific domain can be prohibitively expensive
due to privacy and intellectual property considerations. This work delves into
the generation and utilization of synthetic images derived from text-to-image
generative models in facilitating transfer learning paradigms. Despite the high
visual fidelity of the generated images, we observe that their naive
incorporation into existing real-image datasets does not consistently enhance
model performance due to the inherent distribution gap between synthetic and
real images. To address this issue, we introduce a novel two-stage framework
called bridged transfer, which initially employs synthetic images for
fine-tuning a pre-trained model to improve its transferability and subsequently
uses real data for rapid adaptation. Alongside, We propose dataset style
inversion strategy to improve the stylistic alignment between synthetic and
real images. Our proposed methods are evaluated across 10 different datasets
and 5 distinct models, demonstrating consistent improvements, with up to 30%
accuracy increase on classification tasks. Intriguingly, we note that the
enhancements were not yet saturated, indicating that the benefits may further
increase with an expanded volume of synthetic data.
\\ ( https://arxiv.org/abs/2403.19866 ,  3720kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19867 (*cross-listing*)
Date: Thu, 28 Mar 2024 22:26:38 GMT   (21kb)

Title: Finding Decision Tree Splits in Streaming and Massively Parallel Models
Authors: Huy Pham, Hoang Ta, Hoa T. Vu
Categories: cs.DS cs.AI cs.LG
\\
  In this work, we provide data stream algorithms that compute optimal splits
in decision tree learning. In particular, given a data stream of observations
$x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$
that divides the data into two sets such that the mean squared error (for
regression) or misclassification rate (for classification) is minimized. We
provide various fast streaming algorithms that use sublinear space and a small
number of passes for these problems. These algorithms can also be extended to
the massively parallel computation model. Our work, while not directly
comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
\\ ( https://arxiv.org/abs/2403.19867 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19905 (*cross-listing*)
Date: Fri, 29 Mar 2024 01:11:56 GMT   (775kb,D)

Title: Classification of Diabetic Retinopathy using Pre-Trained Deep Learning
  Models
Authors: Inas Al-Kamachy (Karlstad University, Sweden), Prof. Dr. Reza
  Hassanpour (Rotterdam University, Netherlands), Prof. Roya Choupani (Angelo
  State University, USA)
Categories: cs.CV cs.AI
Comments: 3 pages, 1 figure, 1 table
MSC-class: 68T07
\\
  Diabetic Retinopathy (DR) stands as the leading cause of blindness globally,
particularly affecting individuals between the ages of 20 and 70. This paper
presents a Computer-Aided Diagnosis (CAD) system designed for the automatic
classification of retinal images into five distinct classes: Normal, Mild,
Moderate, Severe, and Proliferative Diabetic Retinopathy (PDR). The proposed
system leverages Convolutional Neural Networks (CNNs) employing pre-trained
deep learning models. Through the application of fine-tuning techniques, our
model is trained on fundus images of diabetic retinopathy with resolutions of
350x350x3 and 224x224x3. Experimental results obtained on the Kaggle platform,
utilizing resources comprising 4 CPUs, 17 GB RAM, and 1 GB Disk, demonstrate
the efficacy of our approach. The achieved Area Under the Curve (AUC) values
for CNN, MobileNet, VGG-16, InceptionV3, and InceptionResNetV2 models are 0.50,
0.70, 0.53, 0.63, and 0.69, respectively.
\\ ( https://arxiv.org/abs/2403.19905 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19918 (*cross-listing*)
Date: Fri, 29 Mar 2024 02:10:19 GMT   (6205kb,D)

Title: CtRL-Sim: Reactive and Controllable Driving Agents with Offline
  Reinforcement Learning
Authors: Luke Rowe, Roger Girgis, Anthony Gosselin, Bruno Carrez, Florian
  Golemo, Felix Heide, Liam Paull, Christopher Pal
Categories: cs.RO cs.AI cs.LG
Comments: 20 pages, 8 figures, 4 tables
\\
  Evaluating autonomous vehicle stacks (AVs) in simulation typically involves
replaying driving logs from real-world recorded traffic. However, agents
replayed from offline data do not react to the actions of the AV, and their
behaviour cannot be easily controlled to simulate counterfactual scenarios.
Existing approaches have attempted to address these shortcomings by proposing
methods that rely on heuristics or learned generative models of real-world data
but these approaches either lack realism or necessitate costly iterative
sampling procedures to control the generated behaviours. In this work, we take
an alternative approach and propose CtRL-Sim, a method that leverages
return-conditioned offline reinforcement learning within a physics-enhanced
Nocturne simulator to efficiently generate reactive and controllable traffic
agents. Specifically, we process real-world driving data through the Nocturne
simulator to generate a diverse offline reinforcement learning dataset,
annotated with various reward terms. With this dataset, we train a
return-conditioned multi-agent behaviour model that allows for fine-grained
manipulation of agent behaviours by modifying the desired returns for the
various reward components. This capability enables the generation of a wide
range of driving behaviours beyond the scope of the initial dataset, including
those representing adversarial behaviours. We demonstrate that CtRL-Sim can
efficiently generate diverse and realistic safety-critical scenarios while
providing fine-grained control over agent behaviours. Further, we show that
fine-tuning our model on simulated safety-critical scenarios generated by our
model enhances this controllability.
\\ ( https://arxiv.org/abs/2403.19918 ,  6205kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19946 (*cross-listing*)
Date: Fri, 29 Mar 2024 03:00:54 GMT   (13303kb,AD)

Title: A Peg-in-hole Task Strategy for Holes in Concrete
Authors: Andr\'e Yuji Yasutomi, Hiroki Mori, Tetsuya Ogata
Categories: cs.RO cs.AI
Comments: Published in 2021 IEEE International Conference on Robotics and
  Automation (ICRA) on 30 May 2021
Journal-ref: 2021 IEEE International Conference on Robotics and Automation
  (ICRA), Xi'an, China, 2021, pp. 2205-2211
DOI: 10.1109/ICRA48506.2021.9561370
\\
  A method that enables an industrial robot to accomplish the peg-in-hole task
for holes in concrete is proposed. The proposed method involves slightly
detaching the peg from the wall, when moving between search positions, to avoid
the negative influence of the concrete's high friction coefficient. It uses a
deep neural network (DNN), trained via reinforcement learning, to effectively
find holes with variable shape and surface finish (due to the brittle nature of
concrete) without analytical modeling or control parameter tuning. The method
uses displacement of the peg toward the wall surface, in addition to force and
torque, as one of the inputs of the DNN. Since the displacement increases as
the peg gets closer to the hole (due to the chamfered shape of holes in
concrete), it is a useful parameter for inputting in the DNN. The proposed
method was evaluated by training the DNN on a hole 500 times and attempting to
find 12 unknown holes. The results of the evaluation show the DNN enabled a
robot to find the unknown holes with average success rate of 96.1% and average
execution time of 12.5 seconds. Additional evaluations with random initial
positions and a different type of peg demonstrate the trained DNN can
generalize well to different conditions. Analyses of the influence of the peg
displacement input showed the success rate of the DNN is increased by utilizing
this parameter. These results validate the proposed method in terms of its
effectiveness and applicability to the construction industry.
\\ ( https://arxiv.org/abs/2403.19946 ,  13303kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19979 (*cross-listing*)
Date: Fri, 29 Mar 2024 05:23:12 GMT   (7875kb,D)

Title: Semantically-Shifted Incremental Adapter-Tuning is A Continual
  ViTransformer
Authors: Yuwen Tan, Qinhao Zhou, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li
Categories: cs.CV cs.AI cs.LG
Comments: To appear at CVPR 2024
\\
  Class-incremental learning (CIL) aims to enable models to continuously learn
new classes while overcoming catastrophic forgetting. The introduction of
pre-trained models has brought new tuning paradigms to CIL. In this paper, we
revisit different parameter-efficient tuning (PET) methods within the context
of continual learning. We observe that adapter tuning demonstrates superiority
over prompt-based methods, even without parameter expansion in each learning
session. Motivated by this, we propose incrementally tuning the shared adapter
without imposing parameter update constraints, enhancing the learning capacity
of the backbone. Additionally, we employ feature sampling from stored
prototypes to retrain a unified classifier, further improving its performance.
We estimate the semantic shift of old prototypes without access to past samples
and update stored prototypes session by session. Our proposed method eliminates
model expansion and avoids retaining any image samples. It surpasses previous
pre-trained model-based CIL methods and demonstrates remarkable continual
learning capabilities. Experimental results on five CIL benchmarks validate the
effectiveness of our approach, achieving state-of-the-art (SOTA) performance.
\\ ( https://arxiv.org/abs/2403.19979 ,  7875kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20012 (*cross-listing*)
Date: Fri, 29 Mar 2024 06:53:52 GMT   (3082kb,D)

Title: Colorful Cutout: Enhancing Image Data Augmentation with Curriculum
  Learning
Authors: Juhwan Choi, YoungBin Kim
Categories: cs.CV cs.AI
Comments: ICLR 2024 Tiny Papers
\\
  Data augmentation is one of the regularization strategies for the training of
deep learning models, which enhances generalizability and prevents overfitting,
leading to performance improvement. Although researchers have proposed various
data augmentation techniques, they often lack consideration for the difficulty
of augmented data. Recently, another line of research suggests incorporating
the concept of curriculum learning with data augmentation in the field of
natural language processing. In this study, we adopt curriculum data
augmentation for image data augmentation and propose colorful cutout, which
gradually increases the noise and difficulty introduced in the augmented image.
Our experimental results highlight the possibility of curriculum data
augmentation for image data. We publicly released our source code to improve
the reproducibility of our study.
\\ ( https://arxiv.org/abs/2403.20012 ,  3082kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20014 (*cross-listing*)
Date: Fri, 29 Mar 2024 07:01:29 GMT   (2785kb,D)

Title: PURPLE: Making a Large Language Model a Better SQL Writer
Authors: Tonghui Ren, Yuankai Fan, Zhenying He, Ren Huang, Jiaqi Dai, Can
  Huang, Yinan Jing, Kai Zhang, Yifan Yang, X.Sean Wang
Categories: cs.DB cs.AI cs.CL
Comments: 12 pages, accepted by ICDE 2024 (40th IEEE International Conference
  on Data Engineering)
\\
  Large Language Model (LLM) techniques play an increasingly important role in
Natural Language to SQL (NL2SQL) translation. LLMs trained by extensive corpora
have strong natural language understanding and basic SQL generation abilities
without additional tuning specific to NL2SQL tasks. Existing LLMs-based NL2SQL
approaches try to improve the translation by enhancing the LLMs with an
emphasis on user intention understanding. However, LLMs sometimes fail to
generate appropriate SQL due to their lack of knowledge in organizing complex
logical operator composition. A promising method is to input the LLMs with
demonstrations, which include known NL2SQL translations from various databases.
LLMs can learn to organize operator compositions from the input demonstrations
for the given task. In this paper, we propose PURPLE (Pre-trained models
Utilized to Retrieve Prompts for Logical Enhancement), which improves accuracy
by retrieving demonstrations containing the requisite logical operator
composition for the NL2SQL task on hand, thereby guiding LLMs to produce better
SQL translation. PURPLE achieves a new state-of-the-art performance of 80.5%
exact-set match accuracy and 87.8% execution match accuracy on the validation
set of the popular NL2SQL benchmark Spider. PURPLE maintains high accuracy
across diverse benchmarks, budgetary constraints, and various LLMs, showing
robustness and cost-effectiveness.
\\ ( https://arxiv.org/abs/2403.20014 ,  2785kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20058 (*cross-listing*)
Date: Fri, 29 Mar 2024 08:47:49 GMT   (10905kb,D)

Title: Revolutionizing Disease Diagnosis with simultaneous functional PET/MR
  and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks
Authors: Luoyu Wang, Yitian Tao, Qing Yang, Yan Liang, Siwei Liu, Hongcheng
  Shi, Dinggang Shen and Han Zhang
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 11 pages
\\
  Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal
neuroimaging technique. It provides an unprecedented opportunity for
concurrently monitoring and integrating multifaceted brain networks built by
spatiotemporally covaried metabolic activity, neural activity, and cerebral
blood flow (perfusion). Albeit high scientific/clinical values, short in
hardware accessibility of PET/MR hinders its applications, let alone modern
AI-based PET/MR fusion models. Our objective is to develop a clinically
feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR
data with the power of, during inferencing, allowing single modality input
(e.g., PET only) as well as enforcing multimodal-based accuracy. To this end,
we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction
Model. It is modality detachable and exchangeable, allocating different
multi-layer perceptrons dynamically ("mixture of experts") through learnable
weights to learn respective representations from different modalities. Such
design will not sacrifice model performance in uni-modal situation. To fully
exploit the inherent complex and nonlinear relation among modalities while
producing fine-grained representations for uni-modal inference, we subsequently
add a modal alignment module to line up a dominant modality (e.g., PET) with
representations of auxiliary modalities (MR). We further adopt multimodal
reconstruction to promote the quality of learned features. Experiments on
precious multimodal sf-PET/MR data for Mild Cognitive Impairment diagnosis
showcase the efficacy of our model toward clinically feasible precision
medicine.
\\ ( https://arxiv.org/abs/2403.20058 ,  10905kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20183 (*cross-listing*)
Date: Fri, 29 Mar 2024 13:57:46 GMT   (126kb,D)

Title: HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on
  Bidirectional Selective SSM
Authors: Shuangjian Li, Tao Zhu, Furong Duan, Liming Chen, Huansheng Ning and
  Yaping Wan
Categories: cs.CV cs.AI
\\
  Wearable sensor human activity recognition (HAR) is a crucial area of
research in activity sensing. While transformer-based temporal deep learning
models have been extensively studied and implemented, their large number of
parameters present significant challenges in terms of system computing load and
memory usage, rendering them unsuitable for real-time mobile activity
recognition applications. Recently, an efficient hardware-aware state space
model (SSM) called Mamba has emerged as a promising alternative. Mamba
demonstrates strong potential in long sequence modeling, boasts a simpler
network architecture, and offers an efficient hardware-aware design. Leveraging
SSM for activity recognition represents an appealing avenue for exploration. In
this study, we introduce HARMamba, which employs a more lightweight selective
SSM as the foundational model architecture for activity recognition. The goal
is to address the computational resource constraints encountered in real-time
activity recognition scenarios. Our approach involves processing sensor data
flow by independently learning each channel and segmenting the data into
"patches". The marked sensor sequence's position embedding serves as the input
token for the bidirectional state space model, ultimately leading to activity
categorization through the classification head. Compared to established
activity recognition frameworks like Transformer-based models, HARMamba
achieves superior performance while also reducing computational and memory
overhead. Furthermore, our proposed method has been extensively tested on four
public activity datasets: PAMAP2, WISDM, UNIMIB, and UCI, demonstrating
impressive performance in activity recognition tasks.
\\ ( https://arxiv.org/abs/2403.20183 ,  126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20188 (*cross-listing*)
Date: Fri, 29 Mar 2024 14:05:40 GMT   (889kb,D)

Title: Distributed Swarm Learning for Edge Internet of Things
Authors: Yue Wang, Zhi Tian, FXin Fan, Zhipeng Cai, Cameron Nowzari, Kai Zeng
Categories: cs.NI cs.AI cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2210.16705
\\
  The rapid growth of Internet of Things (IoT) has led to the widespread
deployment of smart IoT devices at wireless edge for collaborative machine
learning tasks, ushering in a new era of edge learning. With a huge number of
hardware-constrained IoT devices operating in resource-limited wireless
networks, edge learning encounters substantial challenges, including
communication and computation bottlenecks, device and data heterogeneity,
security risks, privacy leakages, non-convex optimization, and complex wireless
environments. To address these issues, this article explores a novel framework
known as distributed swarm learning (DSL), which combines artificial
intelligence and biological swarm intelligence in a holistic manner. By
harnessing advanced signal processing and communications, DSL provides
efficient solutions and robust tools for large-scale IoT at the edge of
wireless networks.
\\ ( https://arxiv.org/abs/2403.20188 ,  889kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20199 (*cross-listing*)
Date: Fri, 29 Mar 2024 14:24:15 GMT   (725kb)

Title: NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for
  Delay-Tolerant Lunar Communication Networks
Authors: Parth Patel, Milena Radenkovic
Categories: cs.NI cs.AI
\\
  Space Communication poses challenges such as severe delays, hard-to-predict
routes and communication disruptions. The Delay Tolerant Network architecture,
having been specifically designed keeping such scenarios in mind, is suitable
to address some challenges. The traditional DTN routing protocols fall short of
delivering optimal performance, due to the inherent complexities of space
communication. Researchers have aimed at using recent advancements in AI to
mitigate some routing challenges [9]. We propose utilising a feedforward neural
network to develop a novel protocol NeuraLunaDTNet, which enhances the
efficiency of the PRoPHET routing protocol for lunar communication, by learning
contact plans in dynamically changing spatio-temporal graph.
\\ ( https://arxiv.org/abs/2403.20199 ,  725kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20216 (*cross-listing*)
Date: Fri, 29 Mar 2024 14:55:40 GMT   (371kb)

Title: Distributed agency in second language learning and teaching through
  generative AI
Authors: Robert Godwin-Jones
Categories: cs.CY cs.AI
Comments: 26 pages. To be published in Language Learning & Technology, volume
  28, issue 2
\\
  Generative AI offers significant opportunities for language learning. Tools
like ChatGPT can provide informal second language practice through chats in
written or voice forms, with the learner specifying through prompts
conversational parameters such as proficiency level, language register, and
discussion topics. AI can be instructed to give corrective feedback, create
practice exercises, or develop an extended study plan. Instructors can use AI
to build learning and assessment materials in a variety of media. AI is likely
to make immersive technologies more powerful and versatile, moving away from
scripted interactions. For both learners and teachers, it is important to
understand the limitations of AI systems that arise from their purely
statistical model of human language, which limits their ability to deal with
nuanced social and cultural aspects of language use. Additionally, there are
ethical concerns over how AI systems are created as well as practical
constraints in their use, especially for less privileged populations. The power
and versatility of AI tools are likely to turn them into valuable and constant
companions in many peoples lives (akin to smartphones), creating a close
connection that goes beyond simple tool use. Ecological theories such as
sociomaterialism are helpful in examining the shared agency that develops
through close user-AI interactions, as are the perspectives on human-object
relations from Indigenous cultures.
\\ ( https://arxiv.org/abs/2403.20216 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20250 (*cross-listing*)
Date: Fri, 29 Mar 2024 15:55:06 GMT   (3429kb,D)

Title: Optimal Policy Learning with Observational Data in Multi-Action
  Scenarios: Estimation, Risk Preference, and Potential Failures
Authors: Giovanni Cerulli
Categories: stat.ML cs.AI cs.LG
\\
  This paper deals with optimal policy learning (OPL) with observational data,
i.e. data-driven optimal decision-making, in multi-action (or multi-arm)
settings, where a finite set of decision options is available. It is organized
in three parts, where I discuss respectively: estimation, risk preference, and
potential failures. The first part provides a brief review of the key
approaches to estimating the reward (or value) function and optimal policy
within this context of analysis. Here, I delineate the identification
assumptions and statistical properties related to offline optimal policy
learning estimators. In the second part, I delve into the analysis of decision
risk. This analysis reveals that the optimal choice can be influenced by the
decision maker's attitude towards risks, specifically in terms of the trade-off
between reward conditional mean and conditional variance. Here, I present an
application of the proposed model to real data, illustrating that the average
regret of a policy with multi-valued treatment is contingent on the
decision-maker's attitude towards risk. The third part of the paper discusses
the limitations of optimal data-driven decision-making by highlighting
conditions under which decision-making can falter. This aspect is linked to the
failure of the two fundamental assumptions essential for identifying the
optimal choice: (i) overlapping, and (ii) unconfoundedness. Some conclusions
end the paper.
\\ ( https://arxiv.org/abs/2403.20250 ,  3429kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20261 (*cross-listing*)
Date: Fri, 29 Mar 2024 16:10:34 GMT   (1465kb,D)

Title: FABind+: Enhancing Molecular Docking through Improved Pocket Prediction
  and Pose Generation
Authors: Kaiyuan Gao, Qizhi Pei, Jinhua Zhu, Tao Qin, Kun He, Tie-Yan Liu,
  Lijun Wu
Categories: q-bio.BM cs.AI cs.LG
Comments: 17 pages, 14 figures, 5 tables
\\
  Molecular docking is a pivotal process in drug discovery. While traditional
techniques rely on extensive sampling and simulation governed by physical
principles, these methods are often slow and costly. The advent of deep
learning-based approaches has shown significant promise, offering increases in
both accuracy and efficiency. Building upon the foundational work of FABind, a
model designed with a focus on speed and accuracy, we present FABind+, an
enhanced iteration that largely boosts the performance of its predecessor. We
identify pocket prediction as a critical bottleneck in molecular docking and
propose a novel methodology that significantly refines pocket prediction,
thereby streamlining the docking process. Furthermore, we introduce
modifications to the docking module to enhance its pose generation
capabilities. In an effort to bridge the gap with conventional
sampling/generative methods, we incorporate a simple yet effective sampling
technique coupled with a confidence model, requiring only minor adjustments to
the regression framework of FABind. Experimental results and analysis reveal
that FABind+ remarkably outperforms the original FABind, achieves competitive
state-of-the-art performance, and delivers insightful modeling strategies. This
demonstrates FABind+ represents a substantial step forward in molecular docking
and drug discovery. Our code is in https://github.com/QizhiPei/FABind.
\\ ( https://arxiv.org/abs/2403.20261 ,  1465kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20300 (*cross-listing*)
Date: Fri, 29 Mar 2024 17:16:20 GMT   (791kb,D)

Title: Improving Learnt Local MAPF Policies with Heuristic Search
Authors: Rishi Veerapaneni, Qian Wang, Kevin Ren, Arthur Jakobsson, Jiaoyang
  Li, Maxim Likhachev
Categories: cs.MA cs.AI cs.RO
Comments: Accepted in ICAPS 2024
\\
  Multi-agent path finding (MAPF) is the problem of finding collision-free
paths for a team of agents to reach their goal locations. State-of-the-art
classical MAPF solvers typically employ heuristic search to find solutions for
hundreds of agents but are typically centralized and can struggle to scale when
run with short timeouts. Machine learning (ML) approaches that learn policies
for each agent are appealing as these could enable decentralized systems and
scale well while maintaining good solution quality. Current ML approaches to
MAPF have proposed methods that have started to scratch the surface of this
potential. However, state-of-the-art ML approaches produce "local" policies
that only plan for a single timestep and have poor success rates and
scalability. Our main idea is that we can improve a ML local policy by using
heuristic search methods on the output probability distribution to resolve
deadlocks and enable full horizon planning. We show several model-agnostic ways
to use heuristic search with learnt policies that significantly improve the
policies' success rates and scalability. To our best knowledge, we demonstrate
the first time ML-based MAPF approaches have scaled to high congestion
scenarios (e.g. 20% agent density).
\\ ( https://arxiv.org/abs/2403.20300 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20318 (*cross-listing*)
Date: Fri, 29 Mar 2024 17:41:57 GMT   (7942kb,D)

Title: SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular
  3D Detection of Large Objects
Authors: Abhinav Kumar, Yuliang Guo, Xinyu Huang, Liu Ren, Xiaoming Liu
Categories: cs.CV cs.AI
Comments: CVPR 2024
\\
  Monocular 3D detectors achieve remarkable performance on cars and smaller
objects. However, their performance drops on larger objects, leading to fatal
accidents. Some attribute the failures to training data scarcity or their
receptive field requirements of large objects. In this paper, we highlight this
understudied problem of generalization to large objects. We find that modern
frontal detectors struggle to generalize to large objects even on nearly
balanced datasets. We argue that the cause of failure is the sensitivity of
depth regression losses to noise of larger objects. To bridge this gap, we
comprehensively investigate regression and dice losses, examining their
robustness under varying error levels and object sizes. We mathematically prove
that the dice loss leads to superior noise-robustness and model convergence for
large objects compared to regression losses for a simplified case. Leveraging
our theoretical insights, we propose SeaBird (Segmentation in Bird's View) as
the first step towards generalizing to large objects. SeaBird effectively
integrates BEV segmentation on foreground objects for 3D detection, with the
segmentation head trained with the dice loss. SeaBird achieves SoTA results on
the KITTI-360 leaderboard and improves existing detectors on the nuScenes
leaderboard, particularly for large objects. Code and models at
https://github.com/abhi1kumar/SeaBird
\\ ( https://arxiv.org/abs/2403.20318 ,  7942kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20320 (*cross-listing*)
Date: Fri, 29 Mar 2024 17:43:58 GMT   (1008kb,D)

Title: MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning
Authors: Ahmed Agiza, Marina Neseem, Sherief Reda
Categories: cs.CV cs.AI cs.LG
Comments: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR), 2024
\\
  Adapting models pre-trained on large-scale datasets to a variety of
downstream tasks is a common strategy in deep learning. Consequently,
parameter-efficient fine-tuning methods have emerged as a promising way to
adapt pre-trained models to different tasks while training only a minimal
number of parameters. While most of these methods are designed for single-task
adaptation, parameter-efficient training in Multi-Task Learning (MTL)
architectures is still unexplored. In this paper, we introduce MTLoRA, a novel
framework for parameter-efficient training of MTL models. MTLoRA employs
Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively
disentangle the parameter space in MTL fine-tuning, thereby enabling the model
to adeptly handle both task specialization and interaction within MTL contexts.
We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting
them to multiple downstream dense prediction tasks. Our extensive experiments
on the PASCAL dataset show that MTLoRA achieves higher accuracy on downstream
tasks compared to fully fine-tuning the MTL model while reducing the number of
trainable parameters by 3.6x. Furthermore, MTLoRA establishes a Pareto-optimal
trade-off between the number of trainable parameters and the accuracy of the
downstream tasks, outperforming current state-of-the-art parameter-efficient
training methods in both accuracy and efficiency. Our code is publicly
available.
\\ ( https://arxiv.org/abs/2403.20320 ,  1008kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20331 (*cross-listing*)
Date: Fri, 29 Mar 2024 17:59:53 GMT   (5256kb,D)

Title: Unsolvable Problem Detection: Evaluating Trustworthiness of Vision
  Language Models
Authors: Atsuyuki Miyai, Jingkang Yang, Jingyang Zhang, Yifei Ming, Qing Yu, Go
  Irie, Yixuan Li, Hai Li, Ziwei Liu, Kiyoharu Aizawa
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Code: https://github.com/AtsuMiyai/UPD
\\
  This paper introduces a novel and significant challenge for Vision Language
Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the
VLM's ability to withhold answers when faced with unsolvable problems in the
context of Visual Question Answering (VQA) tasks. UPD encompasses three
distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set
Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply
investigate the UPD problem, extensive experiments indicate that most VLMs,
including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying
extents, highlighting significant room for the improvements. To address UPD, we
explore both training-free and training-based solutions, offering new insights
into their effectiveness and limitations. We hope our insights, together with
future efforts within the proposed UPD settings, will enhance the broader
understanding and development of more practical and reliable VLMs.
\\ ( https://arxiv.org/abs/2403.20331 ,  5256kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20026 (*cross-listing*)
Date: Fri, 29 Mar 2024 07:28:50 GMT   (8562kb,D)

Title: FSMR: A Feature Swapping Multi-modal Reasoning Approach with Joint
  Textual and Visual Clues
Authors: Shuang Li, Jiahua Wang, Lijie Wen
Categories: cs.CV cs.CL
\\
  Multi-modal reasoning plays a vital role in bridging the gap between textual
and visual information, enabling a deeper understanding of the context. This
paper presents the Feature Swapping Multi-modal Reasoning (FSMR) model,
designed to enhance multi-modal reasoning through feature swapping. FSMR
leverages a pre-trained visual-language model as an encoder, accommodating both
text and image inputs for effective feature representation from both
modalities. It introduces a unique feature swapping module, enabling the
exchange of features between identified objects in images and corresponding
vocabulary words in text, thereby enhancing the model's comprehension of the
interplay between images and text. To further bolster its multi-modal alignment
capabilities, FSMR incorporates a multi-modal cross-attention mechanism,
facilitating the joint modeling of textual and visual information. During
training, we employ image-text matching and cross-entropy losses to ensure
semantic consistency between visual and language elements. Extensive
experiments on the PMR dataset demonstrate FSMR's superiority over
state-of-the-art baseline models across various performance metrics.
\\ ( https://arxiv.org/abs/2403.20026 ,  8562kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20184 (*cross-listing*)
Date: Fri, 29 Mar 2024 13:59:34 GMT   (1314kb,D)

Title: Exploring Pathological Speech Quality Assessment with ASR-Powered
  Wav2Vec2 in Data-Scarce Context
Authors: Tuan Nguyen, Corinne Fredouille, Alain Ghio, Mathieu Balaguer,
  Virginie Woisard
Categories: eess.AS cs.CL cs.LG cs.SD
Comments: Accepted at LREC-COLING 2024
\\
  Automatic speech quality assessment has raised more attention as an
alternative or support to traditional perceptual clinical evaluation. However,
most research so far only gains good results on simple tasks such as binary
classification, largely due to data scarcity. To deal with this challenge,
current works tend to segment patients' audio files into many samples to
augment the datasets. Nevertheless, this approach has limitations, as it
indirectly relates overall audio scores to individual segments. This paper
introduces a novel approach where the system learns at the audio level instead
of segments despite data scarcity. This paper proposes to use the pre-trained
Wav2Vec2 architecture for both SSL, and ASR as feature extractor in speech
assessment. Carried out on the HNC dataset, our ASR-driven approach established
a new baseline compared with other approaches, obtaining average $MSE=0.73$ and
$MSE=1.15$ for the prediction of intelligibility and severity scores
respectively, using only 95 training samples. It shows that the ASR based
Wav2Vec2 model brings the best results and may indicate a strong correlation
between ASR and speech quality assessment. We also measure its ability on
variable segment durations and speech content, exploring factors influencing
its decision.
\\ ( https://arxiv.org/abs/2403.20184 ,  1314kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20222 (*cross-listing*)
Date: Fri, 29 Mar 2024 15:07:21 GMT   (178kb,D)

Title: Shallow Cross-Encoders for Low-Latency Retrieval
Authors: Aleksandr V. Petrov, Sean MacAvaney, Craig Macdonald
Categories: cs.IR cs.CL
Comments: Accepted by ECIR2024
DOI: 10.1007/978-3-031-56063-7_10
\\
  Transformer-based Cross-Encoders achieve state-of-the-art effectiveness in
text retrieval. However, Cross-Encoders based on large transformer models (such
as BERT or T5) are computationally expensive and allow for scoring only a small
number of documents within a reasonably small latency window. However, keeping
search latencies low is important for user satisfaction and energy usage. In
this paper, we show that weaker shallow transformer models (i.e., transformers
with a limited number of layers) actually perform better than full-scale models
when constrained to these practical low-latency settings since they can
estimate the relevance of more documents in the same time budget. We further
show that shallow transformers may benefit from the generalized Binary
Cross-Entropy (gBCE) training scheme, which has recently demonstrated success
for recommendation tasks. Our experiments with TREC Deep Learning passage
ranking query sets demonstrate significant improvements in shallow and
full-scale models in low-latency scenarios. For example, when the latency limit
is 25ms per query, MonoBERT-Large (a cross-encoder based on a full-scale BERT
model) is only able to achieve NDCG@10 of 0.431 on TREC DL 2019, while
TinyBERT-gBCE (a cross-encoder based on TinyBERT trained with gBCE) reaches
NDCG@10 of 0.652, a +51% gain over MonoBERT-Large. We also show that shallow
Cross-Encoders are effective even when used without a GPU (e.g., with CPU
inference, NDCG@10 decreases only by 3% compared to GPU inference with 50ms
latency), which makes Cross-Encoders practical to run even without specialized
hardware acceleration.
\\ ( https://arxiv.org/abs/2403.20222 ,  178kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19706 (*cross-listing*)
Date: Fri, 22 Mar 2024 22:19:08 GMT   (1695kb)

Title: First path component power based NLOS mitigation in UWB positioning
  system
Authors: Marcin Kolakowski, Jozef Modelski
Categories: eess.SP cs.LG
Comments: Originally presented at 2017 25th Telecommunication Forum (TELFOR),
  Belgrade, Serbia
Journal-ref: 2017 25th Telecommunication Forum (TELFOR), 2017, pp. 1-4
DOI: 10.1109/TELFOR.2017.8249313
\\
  The paper describes an NLOS (Non-Line-of-Sight) mitigation method intended
for use in a UWB positioning system. In the proposed method propagation
conditions between the localized objects and the anchors forming system
infrastructure are classified into one of three categories: LOS
(Line-of-Sight), NLOS and severe NLOS. Non-Line-of-Sight detection is conducted
based on first path signal component power measurements. For each of the
categories, average NLOS inducted time of arrival bias and bias standard
deviation have been estimated based on results gathered during a measurement
campaign conducted in a fully furnished apartment. To locate a tag, an EKF
(Extended Kalman Filter) based algorithm is used. The proposed method of NLOS
mitigation consists in correcting measurement results obtained in NLOS
conditions and lowering their significance in a tag position estimation
process. The paper includes the description of the method and the results of
the conducted experiments.
\\ ( https://arxiv.org/abs/2403.19706 ,  1695kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19718 (*cross-listing*)
Date: Wed, 27 Mar 2024 19:02:09 GMT   (3297kb,D)

Title: A Python library for efficient computation of molecular fingerprints
Authors: Micha{\l} Szafarczyk, Piotr Ludynia, Przemys{\l}aw Kukla
Categories: q-bio.QM cs.LG
Comments: 56 pages
\\
  Machine learning solutions are very popular in the field of chemoinformatics,
where they have numerous applications, such as novel drug discovery or
molecular property prediction. Molecular fingerprints are algorithms commonly
used for vectorizing chemical molecules as a part of preprocessing in this kind
of solution. However, despite their popularity, there are no libraries that
implement them efficiently for large datasets, utilizing modern, multicore
architectures. On top of that, most of them do not provide the user with an
intuitive interface, or one that would be compatible with other machine
learning tools.
  In this project, we created a Python library that computes molecular
fingerprints efficiently and delivers an interface that is comprehensive and
enables the user to easily incorporate the library into their existing machine
learning workflow. The library enables the user to perform computation on large
datasets using parallelism. Because of that, it is possible to perform such
tasks as hyperparameter tuning in a reasonable time. We describe tools used in
implementation of the library and asses its time performance on example
benchmark datasets. Additionally, we show that using molecular fingerprints we
can achieve results comparable to state-of-the-art ML solutions even with very
simple models.
\\ ( https://arxiv.org/abs/2403.19718 ,  3297kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19720 (*cross-listing*)
Date: Wed, 27 Mar 2024 21:18:43 GMT   (228kb,D)

Title: Meta-Learning with Generalized Ridge Regression: High-dimensional
  Asymptotics, Optimality and Hyper-covariance Estimation
Authors: Yanhao Jin, Krishnakumar Balasubramanian, Debashis Paul
Categories: math.ST cs.LG stat.ML stat.TH
\\
  Meta-learning involves training models on a variety of training tasks in a
way that enables them to generalize well on new, unseen test tasks. In this
work, we consider meta-learning within the framework of high-dimensional
multivariate random-effects linear models and study generalized
ridge-regression based predictions. The statistical intuition of using
generalized ridge regression in this setting is that the covariance structure
of the random regression coefficients could be leveraged to make better
predictions on new tasks. Accordingly, we first characterize the precise
asymptotic behavior of the predictive risk for a new test task when the data
dimension grows proportionally to the number of samples per task. We next show
that this predictive risk is optimal when the weight matrix in generalized
ridge regression is chosen to be the inverse of the covariance matrix of random
coefficients. Finally, we propose and analyze an estimator of the inverse
covariance matrix of random regression coefficients based on data from the
training tasks. As opposed to intractable MLE-type estimators, the proposed
estimators could be computed efficiently as they could be obtained by solving
(global) geodesically-convex optimization problems. Our analysis and
methodology use tools from random matrix theory and Riemannian optimization.
Simulation results demonstrate the improved generalization performance of the
proposed method on new unseen test tasks within the considered framework.
\\ ( https://arxiv.org/abs/2403.19720 ,  228kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19776 (*cross-listing*)
Date: Thu, 28 Mar 2024 18:58:43 GMT   (40181kb,D)

Title: CLoRA: A Contrastive Approach to Compose Multiple LoRA Models
Authors: Tuna Han Salih Meral, Enis Simsar, Federico Tombari, Pinar Yanardag
Categories: cs.CV cs.LG
\\
  Low-Rank Adaptations (LoRAs) have emerged as a powerful and popular technique
in the field of image generation, offering a highly effective way to adapt and
refine pre-trained deep learning models for specific tasks without the need for
comprehensive retraining. By employing pre-trained LoRA models, such as those
representing a specific cat and a particular dog, the objective is to generate
an image that faithfully embodies both animals as defined by the LoRAs.
However, the task of seamlessly blending multiple concept LoRAs to capture a
variety of concepts in one image proves to be a significant challenge. Common
approaches often fall short, primarily because the attention mechanisms within
different LoRA models overlap, leading to scenarios where one concept may be
completely ignored (e.g., omitting the dog) or where concepts are incorrectly
combined (e.g., producing an image of two cats instead of one cat and one dog).
To overcome these issues, CLoRA addresses them by updating the attention maps
of multiple LoRA models and leveraging them to create semantic masks that
facilitate the fusion of latent representations. Our method enables the
creation of composite images that truly reflect the characteristics of each
LoRA, successfully merging multiple concepts or styles. Our comprehensive
evaluations, both qualitative and quantitative, demonstrate that our approach
outperforms existing methodologies, marking a significant advancement in the
field of image generation with LoRAs. Furthermore, we share our source code,
benchmark dataset, and trained LoRA models to promote further research on this
topic.
\\ ( https://arxiv.org/abs/2403.19776 ,  40181kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19781 (*cross-listing*)
Date: Thu, 28 Mar 2024 19:06:50 GMT   (2743kb,D)

Title: Reinforcement Learning in Agent-Based Market Simulation: Unveiling
  Realistic Stylized Facts and Behavior
Authors: Zhiyuan Yao, Zheng Li, Matthew Thomas, Ionut Florescu
Categories: q-fin.TR cs.LG cs.MA
Comments: Accpeted in IJCNN 2024
\\
  Investors and regulators can greatly benefit from a realistic market
simulator that enables them to anticipate the consequences of their decisions
in real markets. However, traditional rule-based market simulators often fall
short in accurately capturing the dynamic behavior of market participants,
particularly in response to external market impact events or changes in the
behavior of other participants. In this study, we explore an agent-based
simulation framework employing reinforcement learning (RL) agents. We present
the implementation details of these RL agents and demonstrate that the
simulated market exhibits realistic stylized facts observed in real-world
markets. Furthermore, we investigate the behavior of RL agents when confronted
with external market impacts, such as a flash crash. Our findings shed light on
the effectiveness and adaptability of RL-based agents within the simulation,
offering insights into their response to significant market events.
\\ ( https://arxiv.org/abs/2403.19781 ,  2743kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19783 (*cross-listing*)
Date: Thu, 28 Mar 2024 19:09:46 GMT   (802kb,D)

Title: AlloyBERT: Alloy Property Prediction with Large Language Models
Authors: Akshat Chaudhari, Chakradhar Guntuboina, Hongshuo Huang, and Amir
  Barati Farimani
Categories: cond-mat.mtrl-sci cs.LG
Comments: 20 pages, 3 figures
\\
  The pursuit of novel alloys tailored to specific requirements poses
significant challenges for researchers in the field. This underscores the
importance of developing predictive techniques for essential physical
properties of alloys based on their chemical composition and processing
parameters. This study introduces AlloyBERT, a transformer encoder-based model
designed to predict properties such as elastic modulus and yield strength of
alloys using textual inputs. Leveraging the pre-trained RoBERTa encoder model
as its foundation, AlloyBERT employs self-attention mechanisms to establish
meaningful relationships between words, enabling it to interpret human-readable
input and predict target alloy properties. By combining a tokenizer trained on
our textual data and a RoBERTa encoder pre-trained and fine-tuned for this
specific task, we achieved a mean squared error (MSE) of 0.00015 on the Multi
Principal Elemental Alloys (MPEA) data set and 0.00611 on the Refractory Alloy
Yield Strength (RAYS) dataset. This surpasses the performance of shallow
models, which achieved a best-case MSE of 0.00025 and 0.0076 on the MPEA and
RAYS datasets respectively. Our results highlight the potential of language
models in material science and establish a foundational framework for
text-based prediction of alloy properties that does not rely on complex
underlying representations, calculations, or simulations.
\\ ( https://arxiv.org/abs/2403.19783 ,  802kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19844 (*cross-listing*)
Date: Thu, 28 Mar 2024 21:36:07 GMT   (5126kb,D)

Title: Expanding Chemical Representation with k-mers and Fragment-based
  Fingerprints for Molecular Fingerprinting
Authors: Sarwan Ali, Prakash Chourasia, Murray Patterson
Categories: q-bio.BM cs.LG physics.chem-ph
Comments: 12 Pages, 3 tables, Accepted at SimBig2023
Journal-ref: SimBig2023
\\
  This study introduces a novel approach, combining substruct counting,
$k$-mers, and Daylight-like fingerprints, to expand the representation of
chemical structures in SMILES strings. The integrated method generates
comprehensive molecular embeddings that enhance discriminative power and
information content. Experimental evaluations demonstrate its superiority over
traditional Morgan fingerprinting, MACCS, and Daylight fingerprint alone,
improving chemoinformatics tasks such as drug classification. The proposed
method offers a more informative representation of chemical structures,
advancing molecular similarity analysis and facilitating applications in
molecular design and drug discovery. It presents a promising avenue for
molecular structure analysis and design, with significant potential for
practical implementation.
\\ ( https://arxiv.org/abs/2403.19844 ,  5126kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19845 (*cross-listing*)
Date: Thu, 28 Mar 2024 21:37:57 GMT   (89kb,D)

Title: Generalized Gradient Descent is a Hypergraph Functor
Authors: Tyler Hanks, Matthew Klawonn, James Fairbanks
Categories: math.CT cs.LG
\\
  Cartesian reverse derivative categories (CRDCs) provide an axiomatic
generalization of the reverse derivative, which allows generalized analogues of
classic optimization algorithms such as gradient descent to be applied to a
broad class of problems. In this paper, we show that generalized gradient
descent with respect to a given CRDC induces a hypergraph functor from a
hypergraph category of optimization problems to a hypergraph category of
dynamical systems. The domain of this functor consists of objective functions
that are 1) general in the sense that they are defined with respect to an
arbitrary CRDC, and 2) open in that they are decorated spans that can be
composed with other such objective functions via variable sharing. The codomain
is specified analogously as a category of general and open dynamical systems
for the underlying CRDC. We describe how the hypergraph functor induces a
distributed optimization algorithm for arbitrary composite problems specified
in the domain. To illustrate the kinds of problems our framework can model, we
show that parameter sharing models in multitask learning, a prevalent machine
learning paradigm, yield a composite optimization problem for a given choice of
CRDC. We then apply the gradient descent functor to this composite problem and
describe the resulting distributed gradient descent algorithm for training
parameter sharing models.
\\ ( https://arxiv.org/abs/2403.19845 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19882 (*cross-listing*)
Date: Thu, 28 Mar 2024 23:31:59 GMT   (3253kb,D)

Title: Enhancing Efficiency in Vision Transformer Networks: Design Techniques
  and Insights
Authors: Moein Heidari, Reza Azad, Sina Ghorbani Kolahi, Ren\'e Arimond, Leon
  Niggemeier, Alaa Sulaiman, Afshin Bozorgpour, Ehsan Khodapanah Aghdam,
  Amirhossein Kazerouni, Ilker Hacihaliloglu, Dorit Merhof
Categories: eess.IV cs.CV cs.LG
Comments: Submitted to Computational Visual Media Journal
\\
  Intrigued by the inherent ability of the human visual system to identify
salient regions in complex scenes, attention mechanisms have been seamlessly
integrated into various Computer Vision (CV) tasks. Building upon this
paradigm, Vision Transformer (ViT) networks exploit attention mechanisms for
improved efficiency. This review navigates the landscape of redesigned
attention mechanisms within ViTs, aiming to enhance their performance. This
paper provides a comprehensive exploration of techniques and insights for
designing attention mechanisms, systematically reviewing recent literature in
the field of CV. This survey begins with an introduction to the theoretical
foundations and fundamental concepts underlying attention mechanisms. We then
present a systematic taxonomy of various attention mechanisms within ViTs,
employing redesigned approaches. A multi-perspective categorization is proposed
based on their application, objectives, and the type of attention applied. The
analysis includes an exploration of the novelty, strengths, weaknesses, and an
in-depth evaluation of the different proposed strategies. This culminates in
the development of taxonomies that highlight key properties and contributions.
Finally, we gather the reviewed studies along with their available open-source
implementations at our
\href{https://github.com/mindflow-institue/Awesome-Attention-Mechanism-in-Medical-Imaging}{GitHub}\footnote{\url{https://github.com/xmindflow/Awesome-Attention-Mechanism-in-Medical-Imaging}}.
We aim to regularly update it with the most recent relevant papers.
\\ ( https://arxiv.org/abs/2403.19882 ,  3253kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19895 (*cross-listing*)
Date: Fri, 29 Mar 2024 00:29:57 GMT   (585kb)

Title: An Information-Theoretic Framework for Out-of-Distribution
  Generalization
Authors: Wenliang Liu, Guanding Yu, Lele Wang, and Renjie Liao
Categories: cs.IT cs.LG math.IT
\\
  We study the Out-of-Distribution (OOD) generalization in machine learning and
propose a general framework that provides information-theoretic generalization
bounds. Our framework interpolates freely between Integral Probability Metric
(IPM) and $f$-divergence, which naturally recovers some known results
(including Wasserstein- and KL-bounds), as well as yields new generalization
bounds. Moreover, we show that our framework admits an optimal transport
interpretation. When evaluated in two concrete examples, the proposed bounds
either strictly improve upon existing bounds in some cases or recover the best
among existing OOD generalization bounds.
\\ ( https://arxiv.org/abs/2403.19895 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19897 (*cross-listing*)
Date: Fri, 29 Mar 2024 00:36:38 GMT   (11223kb,D)

Title: Disentangling Racial Phenotypes: Fine-Grained Control of Race-related
  Facial Phenotype Characteristics
Authors: Seyma Yucer, Amir Atapour Abarghouei, Noura Al Moubayed, Toby P.
  Breckon
Categories: cs.CV cs.LG
\\
  Achieving an effective fine-grained appearance variation over 2D facial
images, whilst preserving facial identity, is a challenging task due to the
high complexity and entanglement of common 2D facial feature encoding spaces.
Despite these challenges, such fine-grained control, by way of disentanglement
is a crucial enabler for data-driven racial bias mitigation strategies across
multiple automated facial analysis tasks, as it allows to analyse, characterise
and synthesise human facial diversity. In this paper, we propose a novel GAN
framework to enable fine-grained control over individual race-related phenotype
attributes of the facial images. Our framework factors the latent (feature)
space into elements that correspond to race-related facial phenotype
representations, thereby separating phenotype aspects (e.g. skin, hair colour,
nose, eye, mouth shapes), which are notoriously difficult to annotate robustly
in real-world facial data. Concurrently, we also introduce a high quality
augmented, diverse 2D face image dataset drawn from CelebA-HQ for GAN training.
Unlike prior work, our framework only relies upon 2D imagery and related
parameters to achieve state-of-the-art individual control over race-related
phenotype attributes with improved photo-realistic output.
\\ ( https://arxiv.org/abs/2403.19897 ,  11223kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19964 (*cross-listing*)
Date: Fri, 29 Mar 2024 03:56:19 GMT   (10605kb,D)

Title: FairRAG: Fair Human Generation via Fair Retrieval Augmentation
Authors: Robik Shrestha, Yang Zou, Qiuyu Chen, Zhiheng Li, Yusheng Xie, Siqi
  Deng
Categories: cs.CV cs.CY cs.LG
\\
  Existing text-to-image generative models reflect or even amplify societal
biases ingrained in their training data. This is especially concerning for
human image generation where models are biased against certain demographic
groups. Existing attempts to rectify this issue are hindered by the inherent
limitations of the pre-trained models and fail to substantially improve
demographic diversity. In this work, we introduce Fair Retrieval Augmented
Generation (FairRAG), a novel framework that conditions pre-trained generative
models on reference images retrieved from an external image database to improve
fairness in human generation. FairRAG enables conditioning through a
lightweight linear module that projects reference images into the textual
space. To enhance fairness, FairRAG applies simple-yet-effective debiasing
strategies, providing images from diverse demographic groups during the
generative process. Extensive experiments demonstrate that FairRAG outperforms
existing methods in terms of demographic diversity, image-text alignment, and
image fidelity while incurring minimal computational overhead during inference.
\\ ( https://arxiv.org/abs/2403.19964 ,  10605kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19969 (*cross-listing*)
Date: Fri, 29 Mar 2024 04:28:06 GMT   (1852kb,D)

Title: Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output
  Channel Pruning on Computer Vision Tasks
Authors: Guanhua Ding, Zexi Ye, Zhen Zhong, Gang Li, David Shao
Categories: cs.CV cs.LG
\\
  Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce
model size, improve inference latency, and lower power consumption on DNN
accelerators. Among various pruning techniques, block and output channel
pruning have shown significant potential in accelerating hardware performance.
However, their accuracy often requires further improvement. In response to this
challenge, we introduce a separate, dynamic and differentiable (SMART) pruner.
This pruner stands out by utilizing a separate, learnable probability mask for
weight importance ranking, employing a differentiable Top k operator to achieve
target sparsity, and leveraging a dynamic temperature parameter trick to escape
from non-sparse local minima. In our experiments, the SMART pruner consistently
demonstrated its superiority over existing pruning methods across a wide range
of tasks and models on block and output channel pruning. Additionally, we
extend our testing to Transformer-based models in N:M pruning scenarios, where
SMART pruner also yields state-of-the-art results, demonstrating its
adaptability and robustness across various neural network architectures, and
pruning types.
\\ ( https://arxiv.org/abs/2403.19969 ,  1852kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20016 (*cross-listing*)
Date: Fri, 29 Mar 2024 07:03:10 GMT   (13730kb,D)

Title: EnCoMP: Enhanced Covert Maneuver Planning using Offline Reinforcement
  Learning
Authors: Jumman Hossain, Abu-Zaher Faridee, Nirmalya Roy
Categories: cs.RO cs.LG
Comments: Paper under review for CVPR Workshop
\\
  Cover navigation in complex environments is a critical challenge for
autonomous robots, requiring the identification and utilization of
environmental cover while maintaining efficient navigation. We propose an
enhanced navigation system that enables robots to identify and utilize natural
and artificial environmental features as cover, thereby minimizing exposure to
potential threats. Our perception pipeline leverages LiDAR data to generate
high-fidelity cover maps and potential threat maps, providing a comprehensive
understanding of the surrounding environment. We train an offline reinforcement
learning model using a diverse dataset collected from real-world environments,
learning a robust policy that evaluates the quality of candidate actions based
on their ability to maximize cover utilization, minimize exposure to threats,
and reach the goal efficiently. Extensive real-world experiments demonstrate
the superiority of our approach in terms of success rate, cover utilization,
exposure minimization, and navigation efficiency compared to state-of-the-art
methods.
\\ ( https://arxiv.org/abs/2403.20016 ,  13730kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20020 (*cross-listing*)
Date: Fri, 29 Mar 2024 07:15:30 GMT   (1376kb,D)

Title: Nonparametric Bellman Mappings for Reinforcement Learning: Application
  to Robust Adaptive Filtering
Authors: Yuki Akiyama, Minh Vu, and Konstantinos Slavakis
Categories: eess.SP cs.LG
Comments: 22 pages
\\
  This paper designs novel nonparametric Bellman mappings in reproducing kernel
Hilbert spaces (RKHSs) for reinforcement learning (RL). The proposed mappings
benefit from the rich approximating properties of RKHSs, adopt no assumptions
on the statistics of the data owing to their nonparametric nature, require no
knowledge on transition probabilities of Markov decision processes, and may
operate without any training data. Moreover, they allow for sampling on-the-fly
via the design of trajectory samples, re-use past test data via experience
replay, effect dimensionality reduction by random Fourier features, and enable
computationally lightweight operations to fit into efficient online or
time-adaptive learning. The paper offers also a variational framework to design
the free parameters of the proposed Bellman mappings, and shows that
appropriate choices of those parameters yield several popular Bellman-mapping
designs. As an application, the proposed mappings are employed to offer a novel
solution to the problem of countering outliers in adaptive filtering. More
specifically, with no prior information on the statistics of the outliers and
no training data, a policy-iteration algorithm is introduced to select online,
per time instance, the ``optimal'' coefficient p in the
least-mean-p-power-error method. Numerical tests on synthetic data showcase, in
most of the cases, the superior performance of the proposed solution over
several RL and non-RL schemes.
\\ ( https://arxiv.org/abs/2403.20020 ,  1376kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20033 (*cross-listing*)
Date: Fri, 29 Mar 2024 07:59:33 GMT   (245kb,D)

Title: A novel decision fusion approach for sale price prediction using Elastic
  Net and MOPSO
Authors: Amir Eshaghi Chaleshtori
Categories: stat.ML cs.LG cs.NE
\\
  Price prediction algorithms propose prices for every product or service
according to market trends, projected demand, and other characteristics,
including government rules, international transactions, and speculation and
expectation. As the dependent variable in price prediction, it is affected by
several independent and correlated variables which may challenge the price
prediction. To overcome this challenge, machine learning algorithms allow more
accurate price prediction without explicitly modeling the relatedness between
variables. However, as inputs increase, it challenges the existing machine
learning approaches regarding computing efficiency and prediction
effectiveness. Hence, this study introduces a novel decision level fusion
approach to select informative variables in price prediction. The suggested
metaheuristic algorithm balances two competitive objective functions, which are
defined to improve the prediction utilized variables and reduce the error rate
simultaneously. To generate Pareto optimal solutions, an Elastic net approach
is employed to eliminate unrelated and redundant variables to increase the
accuracy. Afterward, we propose a novel method for combining solutions and
ensuring that a subset of features is optimal. Two various real datasets
evaluate the proposed price prediction method. The results support the
suggested superiority of the model concerning its relative root mean square
error and adjusted correlation coefficient.
\\ ( https://arxiv.org/abs/2403.20033 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20078 (*cross-listing*)
Date: Fri, 29 Mar 2024 09:19:52 GMT   (6429kb,D)

Title: Negative Label Guided OOD Detection with Pretrained Vision-Language
  Models
Authors: Xue Jiang, Feng Liu, Zhen Fang, Hong Chen, Tongliang Liu, Feng Zheng,
  Bo Han
Categories: cs.CV cs.LG
Comments: ICLR 2024 Spotlight
\\
  Out-of-distribution (OOD) detection aims at identifying samples from unknown
classes, playing a crucial role in trustworthy models against errors on
unexpected inputs. Extensive research has been dedicated to exploring OOD
detection in the vision modality. Vision-language models (VLMs) can leverage
both textual and visual information for various multi-modal applications,
whereas few OOD detection methods take into account information from the text
modality. In this paper, we propose a novel post hoc OOD detection method,
called NegLabel, which takes a vast number of negative labels from extensive
corpus databases. We design a novel scheme for the OOD score collaborated with
negative labels. Theoretical analysis helps to understand the mechanism of
negative labels. Extensive experiments demonstrate that our method NegLabel
achieves state-of-the-art performance on various OOD detection benchmarks and
generalizes well on multiple VLM architectures. Furthermore, our method
NegLabel exhibits remarkable robustness against diverse domain shifts. The
codes are available at https://github.com/tmlr-group/NegLabel.
\\ ( https://arxiv.org/abs/2403.20078 ,  6429kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20106 (*cross-listing*)
Date: Fri, 29 Mar 2024 10:40:41 GMT   (4917kb,D)

Title: Aggregating Local and Global Features via Selective State Spaces Model
  for Efficient Image Deblurring
Authors: Hu Gao, Depeng Dang
Categories: cs.CV cs.LG
\\
  Image deblurring is a process of restoring a high quality image from the
corresponding blurred image. Significant progress in this field has been made
possible by the emergence of various effective deep learning models, including
CNNs and Transformers. However, these methods often face the dilemma between
eliminating long-range blur degradation perturbations and maintaining
computational efficiency, which hinders their practical application. To address
this issue, we propose an efficient image deblurring network that leverages
selective structured state spaces model to aggregate enriched and accurate
features. Specifically, we design an aggregate local and global block
(ALGBlock) to capture and fuse both local invariant properties and non-local
information. The ALGBlock consists of two blocks: (1) The local block models
local connectivity using simplified channel attention. (2) The global block
captures long-range dependency features with linear complexity through
selective structured state spaces. Nevertheless, we note that the image details
are local features of images, we accentuate the local part for restoration by
recalibrating the weight when aggregating the two branches for recovery.
Experimental results demonstrate that the proposed method outperforms
state-of-the-art approaches on widely used benchmarks, highlighting its
superior performance.
\\ ( https://arxiv.org/abs/2403.20106 ,  4917kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20112 (*cross-listing*)
Date: Fri, 29 Mar 2024 10:49:02 GMT   (14930kb,D)

Title: Segmentation, Classification and Interpretation of Breast Cancer Medical
  Images using Human-in-the-Loop Machine Learning
Authors: David V\'azquez-Lema (1), Eduardo Mosqueira-Rey (1), Elena
  Hern\'andez-Pereira (1), Carlos Fern\'andez-Lozano (1), Fernando Seara-Romera
  (1), Jorge Pombo-Otero (2) ((1) University of Coru\~na (CITIC), (2) Complejo
  Hospitalario Universitario de A Coru\~na (CHUAC))
Categories: cs.CV cs.LG
ACM-class: I.2
\\
  This paper explores the application of Human-in-the-Loop (HITL) strategies in
training machine learning models in the medical domain. In this case a
doctor-in-the-loop approach is proposed to leverage human expertise in dealing
with large and complex data. Specifically, the paper deals with the integration
of genomic data and Whole Slide Imaging (WSI) analysis of breast cancer. Three
different tasks were developed: segmentation of histopathological images,
classification of this images regarding the genomic subtype of the cancer and,
finally, interpretation of the machine learning results. The involvement of a
pathologist helped us to develop a better segmentation model and to enhance the
explainatory capabilities of the models, but the classification results were
suboptimal, highlighting the limitations of this approach: despite involving
human experts, complex domains can still pose challenges, and a HITL approach
may not always be effective.
\\ ( https://arxiv.org/abs/2403.20112 ,  14930kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20130 (*cross-listing*)
Date: Fri, 29 Mar 2024 11:44:14 GMT   (2215kb,D)

Title: Sound event localization and classification using WASN in Outdoor
  Environment
Authors: Dongzhe Zhang, Jianfeng Chen, Jisheng Bai, Mou Wang
Categories: cs.SD cs.LG eess.AS
\\
  Deep learning-based sound event localization and classification is an
emerging research area within wireless acoustic sensor networks. However,
current methods for sound event localization and classification typically rely
on a single microphone array, making them susceptible to signal attenuation and
environmental noise, which limits their monitoring range. Moreover, methods
using multiple microphone arrays often focus solely on source localization,
neglecting the aspect of sound event classification. In this paper, we propose
a deep learning-based method that employs multiple features and attention
mechanisms to estimate the location and class of sound source. We introduce a
Soundmap feature to capture spatial information across multiple frequency
bands. We also use the Gammatone filter to generate acoustic features more
suitable for outdoor environments. Furthermore, we integrate attention
mechanisms to learn channel-wise relationships and temporal dependencies within
the acoustic features. To evaluate our proposed method, we conduct experiments
using simulated datasets with different levels of noise and size of monitoring
areas, as well as different arrays and source positions. The experimental
results demonstrate the superiority of our proposed method over
state-of-the-art methods in both sound event classification and sound source
localization tasks. And we provide further analysis to explain the reasons for
the observed errors.
\\ ( https://arxiv.org/abs/2403.20130 ,  2215kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20139 (*cross-listing*)
Date: Fri, 29 Mar 2024 12:16:01 GMT   (1405kb,D)

Title: Designing Poisson Integrators Through Machine Learning
Authors: Miguel Vaquero, David Mart\'in de Diego, Jorge Cort\'es
Categories: math-ph cs.LG cs.NA math.DG math.DS math.MP math.NA
Comments: 5 pages, 5 figures
MSC-class: 37J06, 70H15, 70H20, 70G45, 65L05, 68T07
ACM-class: G.1.8; J.2
\\
  This paper presents a general method to construct Poisson integrators, i.e.,
integrators that preserve the underlying Poisson geometry. We assume the
Poisson manifold is integrable, meaning there is a known local symplectic
groupoid for which the Poisson manifold serves as the set of units. Our
constructions build upon the correspondence between Poisson diffeomorphisms and
Lagrangian bisections, which allows us to reformulate the design of Poisson
integrators as solutions to a certain PDE (Hamilton-Jacobi). The main novelty
of this work is to understand the Hamilton-Jacobi PDE as an optimization
problem, whose solution can be easily approximated using machine learning
related techniques. This research direction aligns with the current trend in
the PDE and machine learning communities, as initiated by Physics- Informed
Neural Networks, advocating for designs that combine both physical modeling
(the Hamilton-Jacobi PDE) and data.
\\ ( https://arxiv.org/abs/2403.20139 ,  1405kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20190 (*cross-listing*)
Date: Fri, 29 Mar 2024 14:09:59 GMT   (154kb,D)

Title: Homomorphic WiSARDs: Efficient Weightless Neural Network training over
  encrypted data
Authors: Leonardo Neumann, Antonio Guimar\~aes, Diego F. Aranha and Edson Borin
Categories: cs.CR cs.LG
\\
  The widespread application of machine learning algorithms is a matter of
increasing concern for the data privacy research community, and many have
sought to develop privacy-preserving techniques for it. Among existing
approaches, the homomorphic evaluation of ML algorithms stands out by
performing operations directly over encrypted data, enabling strong guarantees
of confidentiality. The homomorphic evaluation of inference algorithms is
practical even for relatively deep Convolution Neural Networks (CNNs). However,
training is still a major challenge, with current solutions often resorting to
lightweight algorithms that can be unfit for solving more complex problems,
such as image recognition. This work introduces the homomorphic evaluation of
Wilkie, Stonham, and Aleksander's Recognition Device (WiSARD) and subsequent
Weightless Neural Networks (WNNs) for training and inference on encrypted data.
Compared to CNNs, WNNs offer better performance with a relatively small
accuracy drop. We develop a complete framework for it, including several
building blocks that can be of independent interest. Our framework achieves
91.7% accuracy on the MNIST dataset after only 3.5 minutes of encrypted
training (multi-threaded), going up to 93.8% in 3.5 hours. For the HAM10000
dataset, we achieve 67.9% accuracy in just 1.5 minutes, going up to 69.9% after
1 hour. Compared to the state of the art on the HE evaluation of CNN training,
Glyph (Lou et al., NeurIPS 2020), these results represent a speedup of up to
1200 times with an accuracy loss of at most 5.4%. For HAM10000, we even
achieved a 0.65% accuracy improvement while being 60 times faster than Glyph.
We also provide solutions for small-scale encrypted training. In a single
thread on a desktop machine using less than 200MB of memory, we train over 1000
MNIST images in 12 minutes or over the entire Wisconsin Breast Cancer dataset
in just 11 seconds.
\\ ( https://arxiv.org/abs/2403.20190 ,  154kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20195 (*cross-listing*)
Date: Fri, 29 Mar 2024 14:17:30 GMT   (47870kb,D)

Title: Enhancing Lithological Mapping with Spatially Constrained Bayesian
  Network (SCB-Net): An Approach for Field Data-Constrained Predictions with
  Uncertainty Evaluation
Authors: Victor Silva dos Santos, Erwan Gloaguen, Shiva Tirdad
Categories: cs.CV cs.LG eess.IV
Comments: 17 pages, 3559 words, 14 figures
ACM-class: F.2.2, I.2.7
\\
  Geological maps are an extremely valuable source of information for the Earth
sciences. They provide insights into mineral exploration, vulnerability to
natural hazards, and many other applications. These maps are created using
numerical or conceptual models that use geological observations to extrapolate
data. Geostatistical techniques have traditionally been used to generate
reliable predictions that take into account the spatial patterns inherent in
the data. However, as the number of auxiliary variables increases, these
methods become more labor-intensive. Additionally, traditional machine learning
methods often struggle with spatially correlated data and extracting valuable
non-linear information from geoscientific datasets. To address these
limitations, a new architecture called the Spatially Constrained Bayesian
Network (SCB-Net) has been developed. The SCB-Net aims to effectively exploit
the information from auxiliary variables while producing spatially constrained
predictions. It is made up of two parts, the first part focuses on learning
underlying patterns in the auxiliary variables while the second part integrates
ground-truth data and the learned embeddings from the first part. Moreover, to
assess model uncertainty, a technique called Monte Carlo dropout is used as a
Bayesian approximation. The SCB-Net has been applied to two selected areas in
northern Quebec, Canada, and has demonstrated its potential in generating
field-data-constrained lithological maps while allowing assessment of
prediction uncertainty for decision-making. This study highlights the promising
advancements of deep neural networks in geostatistics, particularly in handling
complex spatial feature learning tasks, leading to improved spatial information
techniques.
\\ ( https://arxiv.org/abs/2403.20195 ,  47870kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20197 (*cross-listing*)
Date: Fri, 29 Mar 2024 14:19:26 GMT   (2644kb,D)

Title: Dual Simplex Volume Maximization for Simplex-Structured Matrix
  Factorization
Authors: Maryam Abdolali, Giovanni Barbarino, Nicolas Gillis
Categories: math.NA cs.IR cs.LG cs.NA eess.SP stat.ML
Comments: 31 pages, 10 figures
\\
  Simplex-structured matrix factorization (SSMF) is a generalization of
nonnegative matrix factorization, a fundamental interpretable data analysis
model, and has applications in hyperspectral unmixing and topic modeling. To
obtain identifiable solutions, a standard approach is to find minimum-volume
solutions. By taking advantage of the duality/polarity concept for polytopes,
we convert minimum-volume SSMF in the primal space to a maximum-volume problem
in the dual space. We first prove the identifiability of this maximum-volume
dual problem. Then, we use this dual formulation to provide a novel
optimization approach which bridges the gap between two existing families of
algorithms for SSMF, namely volume minimization and facet identification.
Numerical experiments show that the proposed approach performs favorably
compared to the state-of-the-art SSMF algorithms.
\\ ( https://arxiv.org/abs/2403.20197 ,  2644kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20202 (*cross-listing*)
Date: Fri, 29 Mar 2024 14:31:36 GMT   (3951kb)

Title: Voice Signal Processing for Machine Learning. The Case of Speaker
  Isolation
Authors: Radan Ganchev
Categories: cs.SD cs.LG eess.AS
Comments: MSc. thesis. for associated source code, see
  https://github.com/rganchev/speech-signal-processing-for-ml
\\
  The widespread use of automated voice assistants along with other recent
technological developments have increased the demand for applications that
process audio signals and human voice in particular. Voice recognition tasks
are typically performed using artificial intelligence and machine learning
models. Even though end-to-end models exist, properly pre-processing the signal
can greatly reduce the complexity of the task and allow it to be solved with a
simpler ML model and fewer computational resources. However, ML engineers who
work on such tasks might not have a background in signal processing which is an
entirely different area of expertise.
  The objective of this work is to provide a concise comparative analysis of
Fourier and Wavelet transforms that are most commonly used as signal
decomposition methods for audio processing tasks. Metrics for evaluating speech
intelligibility are also discussed, namely Scale-Invariant Signal-to-Distortion
Ratio (SI-SDR), Perceptual Evaluation of Speech Quality (PESQ), and Short-Time
Objective Intelligibility (STOI). The level of detail in the exposition is
meant to be sufficient for an ML engineer to make informed decisions when
choosing, fine-tuning, and evaluating a decomposition method for a specific ML
model. The exposition contains mathematical definitions of the relevant
concepts accompanied with intuitive non-mathematical explanations in order to
make the text more accessible to engineers without deep expertise in signal
processing. Formal mathematical definitions and proofs of theorems are
intentionally omitted in order to keep the text concise.
\\ ( https://arxiv.org/abs/2403.20202 ,  3951kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20230 (*cross-listing*)
Date: Fri, 29 Mar 2024 15:20:33 GMT   (3709kb,D)

Title: An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer
  Hybrid EfficientViT
Authors: Haikuo Shao, Huihong Shi, Wendong Mao, Zhongfeng Wang
Categories: cs.AR cs.LG
Comments: To appear in the 2024 IEEE International Symposium on Circuits and
  Systems (ISCAS 2024)
\\
  Vision Transformers (ViTs) have achieved significant success in computer
vision. However, their intensive computations and massive memory footprint
challenge ViTs' deployment on embedded devices, calling for efficient ViTs.
Among them, EfficientViT, the state-of-the-art one, features a
Convolution-Transformer hybrid architecture, enhancing both accuracy and
hardware efficiency. Unfortunately, existing accelerators cannot fully exploit
the hardware benefits of EfficientViT due to its unique architecture. In this
paper, we propose an FPGA-based accelerator for EfficientViT to advance the
hardware efficiency frontier of ViTs. Specifically, we design a reconfigurable
architecture to efficiently support various operation types, including
lightweight convolutions and attention, boosting hardware utilization.
Additionally, we present a time-multiplexed and pipelined dataflow to
facilitate both intra- and inter-layer fusions, reducing off-chip data access
costs. Experimental results show that our accelerator achieves up to 780.2 GOPS
in throughput and 105.1 GOPS/W in energy efficiency at 200MHz on the Xilinx
ZCU102 FPGA, which significantly outperforms prior works.
\\ ( https://arxiv.org/abs/2403.20230 ,  3709kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20233 (*cross-listing*)
Date: Fri, 29 Mar 2024 15:22:03 GMT   (3177kb,D)

Title: Functional Bilevel Optimization for Machine Learning
Authors: Ieva Petrulionyte, Julien Mairal, Michael Arbel
Categories: stat.ML cs.LG
\\
  In this paper, we introduce a new functional point of view on bilevel
optimization problems for machine learning, where the inner objective is
minimized over a function space. These types of problems are most often solved
by using methods developed in the parametric setting, where the inner objective
is strongly convex with respect to the parameters of the prediction function.
The functional point of view does not rely on this assumption and notably
allows using over-parameterized neural networks as the inner prediction
function. We propose scalable and efficient algorithms for the functional
bilevel optimization problem and illustrate the benefits of our approach on
instrumental regression and reinforcement learning tasks, which admit natural
functional bilevel structures.
\\ ( https://arxiv.org/abs/2403.20233 ,  3177kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20253 (*cross-listing*)
Date: Fri, 29 Mar 2024 15:59:11 GMT   (5507kb,D)

Title: MedCLIP-SAM: Bridging Text and Image Towards Universal Medical Image
  Segmentation
Authors: Taha Koleilat, Hojat Asgariandehkordi, Hassan Rivaz, Yiming Xiao
Categories: cs.CV cs.LG
Comments: 10 pages, 2 figures
\\
  Medical image segmentation of anatomical structures and pathology is crucial
in modern clinical diagnosis, disease study, and treatment planning. To date,
great progress has been made in deep learning-based segmentation techniques,
but most methods still lack data efficiency, generalizability, and
interactability. Consequently, the development of new, precise segmentation
methods that demand fewer labeled datasets is of utmost importance in medical
image analysis. Recently, the emergence of foundation models, such as CLIP and
Segment-Anything-Model (SAM), with comprehensive cross-domain representation
opened the door for interactive and universal image segmentation. However,
exploration of these models for data-efficient medical image segmentation is
still limited, but is highly necessary. In this paper, we propose a novel
framework, called MedCLIP-SAM that combines CLIP and SAM models to generate
segmentation of clinical scans using text prompts in both zero-shot and weakly
supervised settings. To achieve this, we employed a new Decoupled Hard Negative
Noise Contrastive Estimation (DHN-NCE) loss to fine-tune the BiomedCLIP model
and the recent gScoreCAM to generate prompts to obtain segmentation masks from
SAM in a zero-shot setting. Additionally, we explored the use of zero-shot
segmentation labels in a weakly supervised paradigm to improve the segmentation
quality further. By extensively testing three diverse segmentation tasks and
medical image modalities (breast tumor ultrasound, brain tumor MRI, and lung
X-ray), our proposed framework has demonstrated excellent accuracy.
\\ ( https://arxiv.org/abs/2403.20253 ,  5507kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20287 (*cross-listing*)
Date: Fri, 29 Mar 2024 16:58:13 GMT   (5529kb,D)

Title: Benchmarking Counterfactual Image Generation
Authors: Thomas Melistas, Nikos Spyrou, Nefeli Gkouti, Pedro Sanchez,
  Athanasios Vlontzos, Giorgos Papanastasiou, Sotirios A. Tsaftaris
Categories: cs.CV cs.LG
\\
  Counterfactual image generation is pivotal for understanding the causal
relations of variables, with applications in interpretability and generation of
unbiased synthetic data. However, evaluating image generation is a
long-standing challenge in itself. The need to evaluate counterfactual
generation compounds on this challenge, precisely because counterfactuals, by
definition, are hypothetical scenarios without observable ground truths. In
this paper, we present a novel comprehensive framework aimed at benchmarking
counterfactual image generation methods. We incorporate metrics that focus on
evaluating diverse aspects of counterfactuals, such as composition,
effectiveness, minimality of interventions, and image realism. We assess the
performance of three distinct conditional image generation model types, based
on the Structural Causal Model paradigm. Our work is accompanied by a
user-friendly Python package which allows to further evaluate and benchmark
existing and future counterfactual image generation methods. Our framework is
extendable to additional SCM and other causal methods, generative models, and
datasets.
\\ ( https://arxiv.org/abs/2403.20287 ,  5529kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20298 (*cross-listing*)
Date: Fri, 29 Mar 2024 17:15:21 GMT   (3152kb,D)

Title: Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and
  Hierarchy-Aware Domain Disentanglement
Authors: Yoonhyuk Choi
Categories: cs.IR cs.LG
\\
  The issue of data sparsity poses a significant challenge to recommender
systems. In response to this, algorithms that leverage side information such as
review texts have been proposed. Furthermore, Cross-Domain Recommendation
(CDR), which captures domain-shareable knowledge and transfers it from a richer
domain (source) to a sparser one (target), has received notable attention.
Nevertheless, the majority of existing methodologies assume a Euclidean
embedding space, encountering difficulties in accurately representing richer
text information and managing complex interactions between users and items.
This paper advocates a hyperbolic CDR approach based on review texts for
modeling user-item relationships. We first emphasize that conventional
distance-based domain alignment techniques may cause problems because small
modifications in hyperbolic geometry result in magnified perturbations,
ultimately leading to the collapse of hierarchical structures. To address this
challenge, we propose hierarchy-aware embedding and domain alignment schemes
that adjust the scale to extract domain-shareable information without
disrupting structural forms. The process involves the initial embedding of
review texts in hyperbolic space, followed by feature extraction incorporating
degree-based normalization and structure alignment. We conducted extensive
experiments to substantiate the efficiency, robustness, and scalability of our
proposed model in comparison to state-of-the-art baselines.
\\ ( https://arxiv.org/abs/2403.20298 ,  3152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20328 (*cross-listing*)
Date: Fri, 29 Mar 2024 17:59:05 GMT   (5166kb,D)

Title: Learning Visual Quadrupedal Loco-Manipulation from Demonstrations
Authors: Zhengmao He, Kun Lei, Yanjie Ze, Koushil Sreenath, Zhongyu Li, Huazhe
  Xu
Categories: cs.RO cs.LG
Comments: Project website: https://zhengmaohe.github.io/leg-manip
\\
  Quadruped robots are progressively being integrated into human environments.
Despite the growing locomotion capabilities of quadrupedal robots, their
interaction with objects in realistic scenes is still limited. While additional
robotic arms on quadrupedal robots enable manipulating objects, they are
sometimes redundant given that a quadruped robot is essentially a mobile unit
equipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,
we aim to empower a quadruped robot to execute real-world manipulation tasks
using only its legs. We decompose the loco-manipulation process into a
low-level reinforcement learning (RL)-based controller and a high-level
Behavior Cloning (BC)-based planner. By parameterizing the manipulation
trajectory, we synchronize the efforts of the upper and lower layers, thereby
leveraging the advantages of both RL and BC. Our approach is validated through
simulations and real-world experiments, demonstrating the robot's ability to
perform tasks that demand mobility and high precision, such as lifting a basket
from the ground while moving, closing a dishwasher, pressing a button, and
pushing a door. Project website: https://zhengmaohe.github.io/leg-manip
\\ ( https://arxiv.org/abs/2403.20328 ,  5166kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2206.01818
replaced with revised version Thu, 28 Mar 2024 20:38:01 GMT   (250kb,D)

Title: QAGCN: Answering Multi-Relation Questions via Single-Step Implicit
  Reasoning over Knowledge Graphs
Authors: Ruijie Wang, Luca Rossetto, Michael Cochez, Abraham Bernstein
Categories: cs.AI cs.CL cs.IR cs.LG
\\ ( https://arxiv.org/abs/2206.01818 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09591
replaced with revised version Fri, 29 Mar 2024 13:04:03 GMT   (14729kb,D)

Title: Gradient strikes back: How filtering out high frequencies improves
  explanations
Authors: Sabine Muzellec, Thomas Fel, Victor Boutin, L\'eo and\'eol, Rufin
  VanRullen, Thomas Serre
Categories: cs.AI cs.CV cs.LG
\\ ( https://arxiv.org/abs/2307.09591 ,  14729kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06629
replaced with revised version Thu, 28 Mar 2024 18:18:08 GMT   (2096kb,D)

Title: The Relational Bottleneck as an Inductive Bias for Efficient Abstraction
Authors: Taylor W. Webb, Steven M. Frankland, Awni Altabaa, Simon Segert,
  Kamesh Krishnamurthy, Declan Campbell, Jacob Russin, Tyler Giallanza, Randall
  O'Reilly, John Lafferty, Jonathan D. Cohen
Categories: cs.AI cs.NE
\\ ( https://arxiv.org/abs/2309.06629 ,  2096kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02330
replaced with revised version Fri, 29 Mar 2024 09:01:56 GMT   (1285kb,D)

Title: Enhance Reasoning for Large Language Models in the Game Werewolf
Authors: Shuang Wu, Liwen Zhu, Tao Yang, Shiwei Xu, Qiang Fu, Yang Wei, Haobo
  Fu
Categories: cs.AI cs.CL
\\ ( https://arxiv.org/abs/2402.02330 ,  1285kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15456
replaced with revised version Fri, 29 Mar 2024 04:38:51 GMT   (1308kb,D)

Title: WoLF: Wide-scope Large Language Model Framework for CXR Understanding
Authors: Seil Kang, Donghyun Kim, Junhyeok Kim, Hyo Kyung Lee, Seong Jae Hwang
Categories: cs.AI cs.CL
Comments: 11 pages main paper, 2 pages supplementary
\\ ( https://arxiv.org/abs/2403.15456 ,  1308kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16501
replaced with revised version Thu, 28 Mar 2024 21:46:45 GMT   (10321kb,D)

Title: Learning To Guide Human Decision Makers With Vision-Language Models
Authors: Debodeep Banerjee, Stefano Teso, Burcu Sayin, Andrea Passerini
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.16501 ,  10321kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17247
replaced with revised version Thu, 28 Mar 2024 22:47:15 GMT   (1071kb,D)

Title: DASA: Delay-Adaptive Multi-Agent Stochastic Approximation
Authors: Nicolo Dal Fabbro, Arman Adibi, H. Vincent Poor, Sanjeev R. Kulkarni,
  Aritra Mitra and George J. Pappas
Categories: cs.AI cs.RO cs.SY eess.SY math.OC stat.ML
\\ ( https://arxiv.org/abs/2403.17247 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2304.14178
replaced with revised version Fri, 29 Mar 2024 08:13:38 GMT   (32001kb,D)

Title: mPLUG-Owl: Modularization Empowers Large Language Models with
  Multimodality
Authors: Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou,
  Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu,
  Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou
Categories: cs.CL cs.CV cs.LG
Comments: Working in Process
\\ ( https://arxiv.org/abs/2304.14178 ,  32001kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14004
replaced with revised version Fri, 29 Mar 2024 16:42:53 GMT   (79kb)

Title: S\={a}mayik: A Benchmark and Dataset for English-Sanskrit Translation
Authors: Ayush Maheshwari, Ashim Gupta, Amrith Krishna, Atul Kumar Singh,
  Ganesh Ramakrishnan, G. Anil Kumar, Jitin Singla
Categories: cs.CL
Comments: LREC-COLING, 2024
\\ ( https://arxiv.org/abs/2305.14004 ,  79kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02477
replaced with revised version Thu, 28 Mar 2024 23:37:24 GMT   (924kb,D)

Title: Reasoning or Reciting? Exploring the Capabilities and Limitations of
  Language Models Through Counterfactual Tasks
Authors: Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky\"urek, Boyuan Chen,
  Bailin Wang, Najoung Kim, Jacob Andreas, Yoon Kim
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2307.02477 ,  924kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10814
replaced with revised version Fri, 29 Mar 2024 01:18:18 GMT   (8372kb,D)

Title: Natural Language Embedded Programs for Hybrid Language Symbolic
  Reasoning
Authors: Tianhua Zhang, Jiaxin Ge, Hongyin Luo, Yung-Sung Chuang, Mingye Gao,
  Yuan Gong, Xixin Wu, Yoon Kim, Helen Meng, James Glass
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2309.10814 ,  8372kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00648
replaced with revised version Fri, 29 Mar 2024 05:22:15 GMT   (249kb,D)

Title: PETA: Parameter-Efficient Trojan Attacks
Authors: Lauren Hong, Ting Wang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.00648 ,  249kb)
------------------------------------------------------------------------------
\\
arXiv:2310.20689
replaced with revised version Fri, 29 Mar 2024 07:17:39 GMT   (1760kb,D)

Title: Learning From Mistakes Makes LLM Better Reasoner
Authors: Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou,
  Weizhu Chen
Categories: cs.CL cs.AI
Comments: 23 pages, 13 figures, 6 tables
\\ ( https://arxiv.org/abs/2310.20689 ,  1760kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01677
replaced with revised version Fri, 29 Mar 2024 11:35:30 GMT   (8611kb,D)

Title: DialogBench: Evaluating LLMs as Human-like Dialogue Systems
Authors: Jiao Ou, Junda Lu, Che Liu, Yihong Tang, Fuzheng Zhang, Di Zhang, Kun
  Gai
Categories: cs.CL cs.AI
Comments: Accepted at NAACL 2024 (main conference)
\\ ( https://arxiv.org/abs/2311.01677 ,  8611kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08590
replaced with revised version Fri, 29 Mar 2024 11:24:46 GMT   (7497kb,D)

Title: PEMA: An Offsite-Tunable Plug-in External Memory Adaptation for Language
  Models
Authors: HyunJin Kim, Young Jin Kim, JinYeong Bak
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.08590 ,  7497kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17269
replaced with revised version Fri, 29 Mar 2024 06:32:18 GMT   (707kb,D)

Title: Conversational Question Answering with Reformulations over Knowledge
  Graph
Authors: Lihui Liu, Blaine Hill, Boxin Du, Fei Wang, Hanghang Tong
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2312.17269 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05632
replaced with revised version Thu, 28 Mar 2024 21:47:46 GMT   (7369kb,D)

Title: Natural Language Processing for Dialects of a Language: A Survey
Authors: Aditya Joshi, Raj Dabre, Diptesh Kanojia, Zhuang Li, Haolan Zhan,
  Gholamreza Haffari, Doris Dippold
Categories: cs.CL
Comments: The paper is under review at ACM Computing Surveys. Please reach out
  to the authors in the case of feedback
\\ ( https://arxiv.org/abs/2401.05632 ,  7369kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00786
replaced with revised version Fri, 29 Mar 2024 14:56:42 GMT   (4612kb,D)

Title: CroissantLLM: A Truly Bilingual French-English Language Model
Authors: Manuel Faysse, Patrick Fernandes, Nuno M. Guerreiro, Ant\'onio Loison,
  Duarte M. Alves, Caio Corro, Nicolas Boizard, Jo\~ao Alves, Ricardo Rei,
  Pedro H. Martins, Antoni Bigata Casademunt, Fran\c{c}ois Yvon, Andr\'e F.T.
  Martins, Gautier Viaud, C\'eline Hudelot, Pierre Colombo
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2402.00786 ,  4612kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10189
replaced with revised version Thu, 28 Mar 2024 19:41:34 GMT   (1453kb,D)

Title: Uncertainty Quantification for In-Context Learning of Large Language
  Models
Authors: Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou
  Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang
  Zhao, Haifeng Chen
Categories: cs.CL cs.LG
Comments: Accepted to the main conference of NAACL 2024
\\ ( https://arxiv.org/abs/2402.10189 ,  1453kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11676
replaced with revised version Fri, 29 Mar 2024 15:01:38 GMT   (7068kb,D)

Title: A Multi-Aspect Framework for Counter Narrative Evaluation using Large
  Language Models
Authors: Jaylen Jones, Lingbo Mo, Eric Fosler-Lussier, Huan Sun
Categories: cs.CL cs.AI
Comments: 22 pages, camera-ready version; references added, typos corrected,
  methodology section expanded, additional table
\\ ( https://arxiv.org/abs/2402.11676 ,  7068kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16139
replaced with revised version Fri, 29 Mar 2024 17:51:32 GMT   (245kb,D)

Title: What Generative Artificial Intelligence Means for Terminological
  Definitions
Authors: Antonio San Mart\'in
Categories: cs.CL cs.AI
Comments: 37 pages, 1 figure
\\ ( https://arxiv.org/abs/2402.16139 ,  245kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18825
replaced with revised version Fri, 29 Mar 2024 08:08:41 GMT   (152kb,D)

Title: Utilizing Local Hierarchy with Adversarial Training for Hierarchical
  Text Classification
Authors: Zihan Wang, Peiyi Wang, Houfeng Wang
Categories: cs.CL
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2402.18825 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07726
replaced with revised version Fri, 29 Mar 2024 17:59:07 GMT   (8830kb,D)

Title: SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and
  Related Observable Overgeneration Mistakes
Authors: Timothee Mickus, Elaine Zosa, Ra\'ul V\'azquez, Teemu Vahtola, J\"org
  Tiedemann, Vincent Segonne, Alessandro Raganato, Marianna Apidianaki
Categories: cs.CL
Comments: SemEval 2024 shared task. Pre-review version
\\ ( https://arxiv.org/abs/2403.07726 ,  8830kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09488
replaced with revised version Fri, 29 Mar 2024 05:51:11 GMT   (2854kb,D)

Title: Rectifying Demonstration Shortcut in In-Context Learning
Authors: Joonwon Jang, Sanghwan Jang, Wonbin Kweon, Minjin Jeon and Hwanjo Yu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.09488 ,  2854kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09732
replaced with revised version Fri, 29 Mar 2024 03:21:01 GMT   (159kb,D)

Title: PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with
  Cross-consistency
Authors: Zhishuai Li, Xiang Wang, Jingjing Zhao, Sun Yang, Guoqing Du, Xiaoru
  Hu, Bin Zhang, Yuxiao Ye, Ziyue Li, Rui Zhao, Hangyu Mao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.09732 ,  159kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15454
replaced with revised version Thu, 28 Mar 2024 21:26:39 GMT   (564kb)

Title: Emotion Detection with Transformers: A Comparative Study
Authors: Mahdi Rezapour
Categories: cs.CL stat.AP
\\ ( https://arxiv.org/abs/2403.15454 ,  564kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19154
replaced with revised version Fri, 29 Mar 2024 05:15:12 GMT   (717kb,D)

Title: STaR-GATE: Teaching Language Models to Ask Clarifying Questions
Authors: Chinmaya Andukuri, Jan-Philipp Fr\"anken, Tobias Gerstenberg, Noah D.
  Goodman
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.19154 ,  717kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19432
replaced with revised version Fri, 29 Mar 2024 17:21:02 GMT   (554kb,D)

Title: Uncovering Misattributed Suicide Causes through Annotation Inconsistency
  Detection in Death Investigation Notes
Authors: Song Wang, Yiliang Zhou, Ziqiang Han, Cui Tao, Yunyu Xiao, Ying Ding,
  Joydeep Ghosh, Yifan Peng
Categories: cs.CL cs.AI
Comments: 19 pages, 6 figures
\\ ( https://arxiv.org/abs/2403.19432 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2112.02856
replaced with revised version Fri, 29 Mar 2024 04:18:14 GMT   (444kb)

Title: Doubly Optimal No-Regret Online Learning in Strongly Monotone Games with
  Bandit Feedback
Authors: Wenjia Ba, Tianyi Lin, Jiawei Zhang, Zhengyuan Zhou
Categories: cs.LG cs.GT math.OC
Comments: 43 pages, 4 figures
\\ ( https://arxiv.org/abs/2112.02856 ,  444kb)
------------------------------------------------------------------------------
\\
arXiv:2203.13534
replaced with revised version Thu, 28 Mar 2024 18:38:18 GMT   (47kb)

Title: Generalization bounds for learning under graph-dependence: A survey
Authors: Rui-Ray Zhang, Massih-Reza Amini
Categories: cs.LG stat.ML
Comments: To appear in Machine Learning Journal
\\ ( https://arxiv.org/abs/2203.13534 ,  47kb)
------------------------------------------------------------------------------
\\
arXiv:2206.01206
replaced with revised version Thu, 28 Mar 2024 23:25:14 GMT   (2299kb,D)

Title: Positive Unlabeled Contrastive Learning
Authors: Anish Acharya and Sujay Sanghavi and Li Jing and Bhargav Bhushanam and
  Dhruv Choudhary and Michael Rabbat and Inderjit Dhillon
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2206.01206 ,  2299kb)
------------------------------------------------------------------------------
\\
arXiv:2206.10716
replaced with revised version Thu, 28 Mar 2024 20:48:00 GMT   (2305kb,D)

Title: Meta Reinforcement Learning with Finite Training Tasks -- a Density
  Estimation Approach
Authors: Zohar Rimon, Aviv Tamar, Gilad Adler
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2206.10716 ,  2305kb)
------------------------------------------------------------------------------
\\
arXiv:2209.11964
replaced with revised version Fri, 29 Mar 2024 08:46:46 GMT   (11544kb,D)

Title: Strong Transferable Adversarial Attacks via Ensembled Asymptotically
  Normal Distribution Learning
Authors: Zhengwei Fang, Rui Wang, Tao Huang, Liping Jing
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2209.11964 ,  11544kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13821
replaced with revised version Thu, 28 Mar 2024 18:45:43 GMT   (3423kb,D)

Title: Complete Neural Networks for Complete Euclidean Graphs
Authors: Snir Hordan, Tal Amir, Steven J. Gortler, Nadav Dym
Categories: cs.LG
Comments: The 38th AAAI Conference on Artificial Intelligence
Journal-ref: Proceedings of the AAAI Conference on Artificial Intelligence,
  38(11), 12482-12490 (2024)
DOI: 10.1609/aaai.v38i11.29141
\\ ( https://arxiv.org/abs/2301.13821 ,  3423kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04181
replaced with revised version Thu, 28 Mar 2024 19:35:01 GMT   (133kb,D)

Title: Attending to Graph Transformers
Authors: Luis M\"uller, Mikhail Galkin, Christopher Morris, Ladislav
  Ramp\'a\v{s}ek
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2302.04181 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2303.06471
replaced with revised version Thu, 28 Mar 2024 21:36:56 GMT   (5738kb,D)

Title: Multimodal Data Integration for Oncology in the Era of Deep Neural
  Networks: A Review
Authors: Asim Waqas, Aakash Tripathi, Ravi P. Ramachandran, Paul Stewart,
  Ghulam Rasool
Categories: cs.LG
\\ ( https://arxiv.org/abs/2303.06471 ,  5738kb)
------------------------------------------------------------------------------
\\
arXiv:2303.06530
replaced with revised version Fri, 29 Mar 2024 03:37:04 GMT   (9185kb,D)

Title: Making Batch Normalization Great in Federated Deep Learning
Authors: Jike Zhong, Hong-You Chen, Wei-Lun Chao
Categories: cs.LG cs.AI
Comments: An extended version of the workshop paper in NeurIPS 2023
  (https://federated-learning.org/fl@fm-neurips-2023/)
\\ ( https://arxiv.org/abs/2303.06530 ,  9185kb)
------------------------------------------------------------------------------
\\
arXiv:2304.12420
replaced with revised version Fri, 29 Mar 2024 04:13:25 GMT   (2407kb,D)

Title: Sample-Efficient and Surrogate-Based Design Optimization of Underwater
  Vehicle Hulls
Authors: Harsh Vardhan, David Hyde, Umesh Timalsina, Peter Volgyesi, Janos
  Sztipanovits
Categories: cs.LG cs.AI physics.app-ph physics.flu-dyn stat.AP stat.ML
\\ ( https://arxiv.org/abs/2304.12420 ,  2407kb)
------------------------------------------------------------------------------
\\
arXiv:2307.09244
replaced with revised version Fri, 29 Mar 2024 15:54:02 GMT   (762kb,D)

Title: Energy Efficient Deep Multi-Label ON/OFF Classification of Low Frequency
  Metered Home Appliances
Authors: An\v{z}e Pirnat, Bla\v{z} Bertalani\v{c}, Gregor Cerar, Mihael
  Mohor\v{c}i\v{c} and Carolina Fortuna
Categories: cs.LG cs.AI
DOI: 10.1109/ACCESS.2024.3382830
\\ ( https://arxiv.org/abs/2307.09244 ,  762kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10806
replaced with revised version Fri, 29 Mar 2024 06:15:41 GMT   (133kb,D)

Title: DFWLayer: Differentiable Frank-Wolfe Optimization Layer
Authors: Zixuan Liu, Liu Liu, Xueqian Wang, Peilin Zhao
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2308.10806 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2308.12044
replaced with revised version Fri, 29 Mar 2024 14:25:29 GMT   (671kb,D)

Title: A multiobjective continuation method to compute the regularization path
  of deep neural networks
Authors: Augustina C. Amakor, Konstantin Sonntag and Sebastian Peitz
Categories: cs.LG cs.AI math.OC stat.ML
Comments: 7 pages, 6 figures
\\ ( https://arxiv.org/abs/2308.12044 ,  671kb)
------------------------------------------------------------------------------
\\
arXiv:2309.02671
replaced with revised version Fri, 29 Mar 2024 14:36:03 GMT   (281kb,D)

Title: RLSynC: Offline-Online Reinforcement Learning for Synthon Completion
Authors: Frazier N. Baker, Ziqi Chen, Daniel Adu-Ampratwum, and Xia Ning
Categories: cs.LG cs.AI
Comments: 32 pages, 5 figures, 4 tables
\\ ( https://arxiv.org/abs/2309.02671 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16971
replaced with revised version Fri, 29 Mar 2024 03:44:48 GMT   (5311kb,D)

Title: Multi-Resolution Active Learning of Fourier Neural Operators
Authors: Shibo Li, Xin Yu, Wei Xing, Mike Kirby, Akil Narayan, Shandian Zhe
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.16971 ,  5311kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03223
replaced with revised version Fri, 29 Mar 2024 02:47:29 GMT   (7717kb,D)

Title: TacoGFN: Target-conditioned GFlowNet for Structure-based Drug Design
Authors: Tony Shen, Seonghwan Seo, Grayson Lee, Mohit Pandey, Jason R Smith,
  Artem Cherkasov, Woo Youn Kim, Martin Ester
Categories: cs.LG
Comments: Accepted at NeurIPS 2023 AID3 and at NeurIPS 2023 GenBio as Spotlight
Journal-ref: NeurIPS 2023 Generative AI and Biology (GenBio) Workshop
\\ ( https://arxiv.org/abs/2310.03223 ,  7717kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05764
replaced with revised version Thu, 28 Mar 2024 19:53:36 GMT   (14300kb,D)

Title: Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and
  Binding Site Design
Authors: Hannes St\"ark, Bowen Jing, Regina Barzilay, Tommi Jaakkola
Categories: cs.LG cs.AI
Comments: Under review
\\ ( https://arxiv.org/abs/2310.05764 ,  14300kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13240
replaced with revised version Fri, 29 Mar 2024 09:49:02 GMT   (16179kb,D)

Title: Transparency challenges in policy evaluation with causal machine
  learning -- improving usability and accountability
Authors: Patrick Rehill and Nicholas Biddle
Categories: cs.LG econ.EM
Comments: 31 pages, 8 figures
\\ ( https://arxiv.org/abs/2310.13240 ,  16179kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02402
replaced with revised version Thu, 28 Mar 2024 20:01:02 GMT   (12019kb,D)

Title: Hybrid quantum image classification and federated learning for hepatic
  steatosis diagnosis
Authors: Luca Lusnig, Asel Sagingalieva, Mikhail Surmach, Tatjana Protasevich,
  Ovidiu Michiu, Joseph McLoughlin, Christopher Mansell, Graziano de' Petris,
  Deborah Bonazza, Fabrizio Zanconati, Alexey Melnikov, Fabio Cavalli
Categories: cs.LG cs.CV quant-ph
Comments: 13 pages, 3 figures, 2 tables
Journal-ref: Diagnostics 14(5), 558 (2024)
DOI: 10.3390/diagnostics14050558
\\ ( https://arxiv.org/abs/2311.02402 ,  12019kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10144
replaced with revised version Thu, 28 Mar 2024 21:32:10 GMT   (917kb,D)

Title: Data-Efficient Multimodal Fusion on a Single GPU
Authors: No\"el Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin
  Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims
  Volkovs
Categories: cs.LG cs.AI cs.CV
Comments: Accepted at CVPR 2024
\\ ( https://arxiv.org/abs/2312.10144 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10359
replaced with revised version Fri, 29 Mar 2024 04:44:44 GMT   (3238kb,D)

Title: Conformer-Based Speech Recognition On Extreme Edge-Computing Devices
Authors: Mingbin Xu, Alex Jin, Sicheng Wang, Mu Su, Tim Ng, Henry Mason, Shiyi
  Han, Zhihong Lei Yaqiao Deng, Zhen Huang, Mahesh Krishnamoorthy
Categories: cs.LG cs.PF
\\ ( https://arxiv.org/abs/2312.10359 ,  3238kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12400
replaced with revised version Fri, 29 Mar 2024 02:58:19 GMT   (228kb,D)

Title: New Classes of the Greedy-Applicable Arm Feature Distributions in the
  Sparse Linear Bandit Problem
Authors: Koji Ichikawa, Shinji Ito, Daisuke Hatano, Hanna Sumita, Takuro
  Fukunaga, Naonori Kakimura, Ken-ichi Kawarabayashi
Categories: cs.LG stat.ML
Comments: Accepted by AAAI 2024
Journal-ref: Proceedings of the AAAI Conference on Artificial Intelligence 38
  (2024) 12708-12716
DOI: 10.1609/aaai.v38i11.29166
\\ ( https://arxiv.org/abs/2312.12400 ,  228kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13460
replaced with revised version Thu, 28 Mar 2024 22:24:30 GMT   (3085kb,D)

Title: Multi-Agent Diagnostics for Robustness via Illuminated Diversity
Authors: Mikayel Samvelyan, Davide Paglieri, Minqi Jiang, Jack Parker-Holder,
  Tim Rockt\"aschel
Categories: cs.LG cs.AI cs.MA
\\ ( https://arxiv.org/abs/2401.13460 ,  3085kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02977
replaced with revised version Fri, 29 Mar 2024 12:28:46 GMT   (49684kb,D)

Title: Variational Flow Models: Flowing in Your Style
Authors: Kien Do, Duc Kieu, Toan Nguyen, Dang Nguyen, Hung Le, Dung Nguyen,
  Thin Nguyen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2402.02977 ,  49684kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03646
replaced with revised version Fri, 29 Mar 2024 02:01:11 GMT   (3302kb,D)

Title: Lens: A Foundation Model for Network Traffic in Cybersecurity
Authors: Qineng Wang, Chen Qian, Xiaochang Li, Ziyu Yao, Huajie Shao
Categories: cs.LG cs.NI
\\ ( https://arxiv.org/abs/2402.03646 ,  3302kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08585
replaced with revised version Fri, 29 Mar 2024 10:15:47 GMT   (190kb,D)

Title: Improving Implicit Regularization of SGD with Preconditioning for Least
  Square Problems
Authors: Junwei Su, Difan Zou, Chuan Wu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.08585 ,  190kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10424
replaced with revised version Fri, 29 Mar 2024 13:48:44 GMT   (112kb,D)

Title: Structured Evaluation of Synthetic Tabular Data
Authors: Scott Cheng-Hsin Yang, Baxter Eaves, Michael Schmidt, Ken Swanson, and
  Patrick Shafto
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.10424 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12236
replaced with revised version Fri, 29 Mar 2024 06:41:07 GMT   (11237kb,D)

Title: Improving Generalization via Meta-Learning on Hard Samples
Authors: Nishant Jain, Arun S. Suggala and Pradeep Shenoy
Categories: cs.LG cs.CV
Comments: Accepted at CVPR 2024
\\ ( https://arxiv.org/abs/2403.12236 ,  11237kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15905
replaced with revised version Fri, 29 Mar 2024 16:53:58 GMT   (5298kb,D)

Title: Towards Low-Energy Adaptive Personalization for Resource-Constrained
  Devices
Authors: Yushan Huang, Josh Millar, Yuxuan Long, Yuchen Zhao, Hamed Haddadi
Categories: cs.LG cs.CV
Comments: Accepetd to The 4th Workshop on Machine Learning and Systems
  (EuroMLSys '24)
\\ ( https://arxiv.org/abs/2403.15905 ,  5298kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16460
replaced with revised version Fri, 29 Mar 2024 08:46:16 GMT   (1699kb,D)

Title: FedAC: An Adaptive Clustered Federated Learning Framework for
  Heterogeneous Data
Authors: Yuxin Zhang, Haoyu Chen, Zheng Lin, Zhe Chen, Jin Zhao
Categories: cs.LG cs.AI cs.DC
Comments: 14 pages, 4 figures
\\ ( https://arxiv.org/abs/2403.16460 ,  1699kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17410
replaced with revised version Thu, 28 Mar 2024 22:28:02 GMT   (2209kb,D)

Title: On permutation-invariant neural networks
Authors: Masanari Kimura, Ryotaro Shimizu, Yuki Hirakawa, Ryosuke Goto, Yuki
  Saito
Categories: cs.LG cs.AI stat.ML
\\ ( https://arxiv.org/abs/2403.17410 ,  2209kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19591
replaced with revised version Fri, 29 Mar 2024 14:13:11 GMT   (858kb,D)

Title: Genetic Quantization-Aware Approximation for Non-Linear Operations in
  Transformers
Authors: Pingcheng Dong, Yonghao Tan, Dong Zhang, Tianwei Ni, Xuejiao Liu, Yu
  Liu, Peng Luo, Luhong Liang, Shih-Yang Liu, Xijie Huang, Huaiyu Zhu, Yun Pan,
  Fengwei An, Kwang-Ting Cheng
Categories: cs.LG cs.AR cs.NE
Comments: 61st ACM/IEEE Design Automation Conference (DAC) 2024
\\ ( https://arxiv.org/abs/2403.19591 ,  858kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06186
replaced with revised version Thu, 28 Mar 2024 18:49:33 GMT   (14975kb,D)

Title: GOTCHA: Real-Time Video Deepfake Detection via Challenge-Response
Authors: Govind Mittal, Chinmay Hegde, Nasir Memon
Categories: cs.CR cs.AI cs.CV
Comments: 20 pages, 19 figures, Code and data released
\\ ( https://arxiv.org/abs/2210.06186 ,  14975kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14426 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 14:45:11 GMT   (623kb,D)

Title: Restricting to the chip architecture maintains the quantum neural
  network accuracy
Authors: Lucas Friedrich, Jonas Maziero
Categories: quant-ph cs.AI cs.LG
Journal-ref: Quantum Inf. Process. 23, 131 (2024)
DOI: 10.1007/s11128-024-04336-7
\\ ( https://arxiv.org/abs/2212.14426 ,  623kb)
------------------------------------------------------------------------------
\\
arXiv:2303.00510
replaced with revised version Fri, 29 Mar 2024 08:01:32 GMT   (493kb,D)

Title: A Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit
Authors: Mina Huh, Ruchira Ray, Corey Karnei
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2303.00510 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00746
replaced with revised version Fri, 29 Mar 2024 13:32:53 GMT   (34463kb,D)

Title: VGTS: Visually Guided Text Spotting for Novel Categories in Historical
  Manuscripts
Authors: Wenbo Hu, Hongjian Zhan, Xinchen Ma, Cong Liu, Bing Yin, Yue Lu
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2304.00746 ,  34463kb)
------------------------------------------------------------------------------
\\
arXiv:2304.03773
replaced with revised version Fri, 29 Mar 2024 17:52:11 GMT   (14595kb,D)

Title: Safe Explicable Planning
Authors: Akkamahadevi Hanni, Andrew Boateng and Yu Zhang
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2304.03773 ,  14595kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13604
replaced with revised version Fri, 29 Mar 2024 09:59:34 GMT   (34063kb,D)

Title: Distribution-Aware Continual Test-Time Adaptation for Semantic
  Segmentation
Authors: Jiayi Ni, Senqiao Yang, Ran Xu, Jiaming Liu, Xiaoqi Li, Wenyu Jiao,
  Zehui Chen, Yi Liu, Shanghang Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2309.13604 ,  34063kb)
------------------------------------------------------------------------------
\\
arXiv:2309.17166
replaced with revised version Thu, 28 Mar 2024 20:49:55 GMT   (15100kb,D)

Title: Advances in Kidney Biopsy Lesion Assessment through Dense Instance
  Segmentation
Authors: Zhan Xiong, Junling He, Pieter Valkema, Tri Q. Nguyen, Maarten
  Naesens, Jesper Kers, and Fons J. Verbeek
Categories: cs.CV cs.AI
Comments: 16 pages, 15 figures, 6 tables, Journal
\\ ( https://arxiv.org/abs/2309.17166 ,  15100kb)
------------------------------------------------------------------------------
\\
arXiv:2310.01642
replaced with revised version Thu, 28 Mar 2024 20:12:16 GMT   (19469kb,D)

Title: Naming Practices of Pre-Trained Models in Hugging Face
Authors: Wenxin Jiang, Chingwo Cheung, Mingyu Kim, Heesoo Kim, George K.
  Thiruvathukal, James C. Davis
Categories: cs.SE cs.AI
Comments: 21 pages
\\ ( https://arxiv.org/abs/2310.01642 ,  19469kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05737
replaced with revised version Fri, 29 Mar 2024 17:44:41 GMT   (7016kb,D)

Title: Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation
Authors: Lijun Yu, Jos\'e Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk
  Sohn, David Minnen, Yong Cheng, Vighnesh Birodkar, Agrim Gupta, Xiuye Gu,
  Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A.
  Ross, Lu Jiang
Categories: cs.CV cs.AI cs.MM
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.05737 ,  7016kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05916
replaced with revised version Fri, 29 Mar 2024 03:40:47 GMT   (35915kb,D)

Title: Interpreting CLIP's Image Representation via Text-Based Decomposition
Authors: Yossi Gandelsman, Alexei A. Efros, Jacob Steinhardt
Categories: cs.CV cs.AI
Comments: Project page and code:
  https://yossigandelsman.github.io/clip_decomposition/
\\ ( https://arxiv.org/abs/2310.05916 ,  35915kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10375
replaced with revised version Fri, 29 Mar 2024 01:08:12 GMT   (11671kb,D)

Title: GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers
Authors: Takeru Miyato, Bernhard Jaeger, Max Welling, Andreas Geiger
Categories: cs.CV cs.AI cs.LG stat.ML
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.10375 ,  11671kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13001
replaced with revised version Fri, 29 Mar 2024 05:51:53 GMT   (1006kb)

Title: Conversational Financial Information Retrieval Model (ConFIRM)
Authors: Stephen Choi, William Gazeley, Siu Ho Wong, Tingting Li
Categories: cs.IR cs.AI cs.CE cs.CL cs.LG
Comments: 10 pages, 2 figures, 2 tables, 2 appendices
\\ ( https://arxiv.org/abs/2310.13001 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17076
replaced with revised version Thu, 28 Mar 2024 23:02:27 GMT   (6252kb,D)

Title: Compositional Chain-of-Thought Prompting for Large Multimodal Models
Authors: Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig
Categories: cs.CV cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.17076 ,  6252kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17095
replaced with revised version Fri, 29 Mar 2024 02:18:40 GMT   (30101kb,D)

Title: Emergent Open-Vocabulary Semantic Segmentation from Off-the-shelf
  Vision-Language Models
Authors: Jiayun Luo, Siddhesh Khandelwal, Leonid Sigal, Boyang Li
Categories: cs.CV cs.AI
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2311.17095 ,  30101kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17132
replaced with revised version Fri, 29 Mar 2024 04:55:51 GMT   (14664kb,D)

Title: TransNeXt: Robust Foveal Visual Perception for Vision Transformers
Authors: Dai Shi
Categories: cs.CV cs.AI
Comments: CVPR 2024 Camera-ready Version. Project Page:
  https://github.com/DaiShiResearch/TransNeXt
\\ ( https://arxiv.org/abs/2311.17132 ,  14664kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00598
replaced with revised version Thu, 28 Mar 2024 21:29:55 GMT   (16787kb,D)

Title: Learning from One Continuous Video Stream
Authors: Jo\~ao Carreira, Michael King, Viorica P\u{a}tr\u{a}ucean, Dilara
  Gokay, C\u{a}t\u{a}lin Ionescu, Yi Yang, Daniel Zoran, Joseph Heyward, Carl
  Doersch, Yusuf Aytar, Dima Damen, Andrew Zisserman
Categories: cs.CV cs.AI
Comments: CVPR camera ready version
\\ ( https://arxiv.org/abs/2312.00598 ,  16787kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04670
replaced with revised version Fri, 29 Mar 2024 16:39:28 GMT   (4131kb,D)

Title: Rapid Motor Adaptation for Robotic Manipulator Arms
Authors: Yichao Liang, Kevin Ellis, Jo\~ao Henriques
Categories: cs.RO cs.AI cs.CV cs.LG
Comments: Accepted at CVPR 2024. 12 pages
\\ ( https://arxiv.org/abs/2312.04670 ,  4131kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05291
replaced with revised version Fri, 29 Mar 2024 16:49:59 GMT   (7359kb,D)

Title: GlitchBench: Can large multimodal models detect video game glitches?
Authors: Mohammad Reza Taesiri, Tianjun Feng, Anh Nguyen, Cor-Paul Bezemer
Categories: cs.CV cs.AI cs.CL
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2312.05291 ,  7359kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16438
replaced with revised version Fri, 29 Mar 2024 03:04:02 GMT   (17097kb,AD)

Title: Visual Spatial Attention and Proprioceptive Data-Driven Reinforcement
  Learning for Robust Peg-in-Hole Task Under Variable Conditions
Authors: Andr\'e Yuji Yasutomi, Hideyuki Ichiwara, Hiroshi Ito, Hiroki Mori,
  Tetsuya Ogata
Categories: cs.RO cs.AI
Comments: Published in IEEE Robotics and Automation Letters on 08 February 2023
Journal-ref: IEEE Robotics and Automation Letters, vol. 8, issue 3, pp.
  1834-1841, 2023
DOI: 10.1109/LRA.2023.3243526
\\ ( https://arxiv.org/abs/2312.16438 ,  17097kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15741
replaced with revised version Fri, 29 Mar 2024 17:42:21 GMT   (1014kb)

Title: SERNet-Former: Semantic Segmentation by Efficient Residual Network with
  Attention-Boosting Gates and Attention-Fusion Networks
Authors: Serdar Erisen
Categories: cs.CV cs.AI
DOI: 10.48550/arXiv.2401.15741
\\ ( https://arxiv.org/abs/2401.15741 ,  1014kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11677
replaced with revised version Fri, 29 Mar 2024 12:34:34 GMT   (1460kb,D)

Title: MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of
  LiDAR-Camera Fusion for 3D Object Detection
Authors: Till Beemelmanns, Quan Zhang, and Lutz Eckstein
Categories: cs.CV cs.AI
Comments: Code: https://github.com/ika-rwth-aachen/MultiCorrupt
\\ ( https://arxiv.org/abs/2402.11677 ,  1460kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12168
replaced with revised version Fri, 29 Mar 2024 12:12:30 GMT   (5925kb,D)

Title: Defending Against Weight-Poisoning Backdoor Attacks for
  Parameter-Efficient Fine-Tuning
Authors: Shuai Zhao, Leilei Gan, Luu Anh Tuan, Jie Fu, Lingjuan Lyu, Meihuizi
  Jia, Jinming Wen
Categories: cs.CR cs.AI cs.CL
Comments: NAACL Findings 2024
\\ ( https://arxiv.org/abs/2402.12168 ,  5925kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09669
replaced with revised version Thu, 28 Mar 2024 04:45:23 GMT   (4568kb,D)

Title: STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video
  Generative Models
Authors: Pum Jun Kim, Seojun Kim, Jaejun Yoo
Categories: cs.CV cs.AI
Comments: Our work is accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2403.09669 ,  4568kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15585
replaced with revised version Fri, 29 Mar 2024 00:44:18 GMT   (8600kb,D)

Title: MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis
Authors: Mai A. Shaaban, Adnan Khan, Mohammad Yaqub
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.15585 ,  8600kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18795
replaced with revised version Fri, 29 Mar 2024 08:02:14 GMT   (4538kb,D)

Title: Gamba: Marry Gaussian Splatting with Mamba for single view 3D
  reconstruction
Authors: Qiuhong Shen, Xuanyu Yi, Zike Wu, Pan Zhou, Hanwang Zhang, Shuicheng
  Yan, Xinchao Wang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.18795 ,  4538kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19060
replaced with revised version Fri, 29 Mar 2024 00:25:03 GMT   (42777kb,D)

Title: Towards Human-Centered Construction Robotics: An RL-Driven Companion
  Robot For Contextually Assisting Carpentry Workers
Authors: Yuning Wu, Jiaying Wei, Jean Oh, Daniel Cardoso Llach
Categories: cs.RO cs.AI cs.HC cs.LG
Comments: 8 pages, 9 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\ ( https://arxiv.org/abs/2403.19060 ,  42777kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04119
replaced with revised version Fri, 29 Mar 2024 15:27:47 GMT   (3996kb,D)

Title: DialogCC: An Automated Pipeline for Creating High-Quality Multi-Modal
  Dialogue Dataset
Authors: Young-Jun Lee, Byungsoo Ko, Han-Gyu Kim, Jonghwan Hyeon, Ho-Jin Choi
Categories: cs.CV cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2212.04119 ,  3996kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17343
replaced with revised version Thu, 28 Mar 2024 21:28:00 GMT   (2180kb,D)

Title: Residual-based Language Models are Free Boosters for Biomedical Imaging
Authors: Zhixin Lai, Jing Wu, Suiyao Chen, Yucheng Zhou, Naira Hovakimyan
Categories: cs.CV cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.17343 ,  2180kb)
------------------------------------------------------------------------------
\\
arXiv:2010.11970 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 14:40:04 GMT   (268kb,D)

Title: Two-sample Test using Projected Wasserstein Distance
Authors: Jie Wang, Rui Gao, Yao Xie
Categories: stat.ML cs.LG
Comments: 10 pages, 3 figures. Accepted in ISIT-21, typo in Proposition 3 has
  been corrected
DOI: 10.1109/ISIT45174.2021.9518186
\\ ( https://arxiv.org/abs/2010.11970 ,  268kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13398
replaced with revised version Fri, 29 Mar 2024 04:13:49 GMT   (8203kb,D)

Title: CPPF++: Uncertainty-Aware Sim2Real Object Pose Estimation by Vote
  Aggregation
Authors: Yang You, Wenhao He, Jin Liu, Hongkai Xiong, Weiming Wang, Cewu Lu
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2211.13398 ,  8203kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06089
replaced with revised version Thu, 28 Mar 2024 18:31:28 GMT   (9021kb,D)

Title: Federated attention consistent learning models for prostate cancer
  diagnosis and Gleason grading
Authors: Fei Kong, Xiyue Wang, Jinxi Xiang, Sen Yang, Xinran Wang, Meng Yue,
  Jun Zhang, Junhan Zhao, Xiao Han, Yuhan Dong, Biyue Zhu, Fang Wang, Yueping
  Liu
Categories: cs.CV cs.LG q-bio.QM
Comments: 14 pages
\\ ( https://arxiv.org/abs/2302.06089 ,  9021kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13417 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 09:57:24 GMT   (1967kb,D)

Title: Training neural networks with structured noise improves classification
  and generalization
Authors: Marco Benedetti and Enrico Ventura
Categories: cond-mat.dis-nn cs.LG
Comments: 17 pages, 10 figures
\\ ( https://arxiv.org/abs/2302.13417 ,  1967kb)
------------------------------------------------------------------------------
\\
arXiv:2303.01563
replaced with revised version Thu, 28 Mar 2024 21:16:15 GMT   (4479kb,D)

Title: Data-efficient, Explainable and Safe Box Manipulation: Illustrating the
  Advantages of Physical Priors in Model-Predictive Control
Authors: Achkan Salehi, Stephane Doncieux
Categories: cs.RO cs.LG
Comments: accepted for publication by l4dc 2024, 12 pages (with references), 4
  figures, 2 tables
\\ ( https://arxiv.org/abs/2303.01563 ,  4479kb)
------------------------------------------------------------------------------
\\
arXiv:2304.05301
replaced with revised version Fri, 29 Mar 2024 17:34:59 GMT   (718kb,D)

Title: TACOS: Topology-Aware Collective Algorithm Synthesizer for Distributed
  Machine Learning
Authors: William Won, Midhilesh Elavazhagan, Sudarshan Srinivasan, Ajaya Durg,
  Samvit Kaul, Swati Gupta, Tushar Krishna
Categories: cs.DC cs.LG
\\ ( https://arxiv.org/abs/2304.05301 ,  718kb)
------------------------------------------------------------------------------
\\
arXiv:2305.03039
replaced with revised version Thu, 28 Mar 2024 19:51:55 GMT   (8604kb,D)

Title: SuperNOVA: Design Strategies and Opportunities for Interactive
  Visualization in Computational Notebooks
Authors: Zijie J. Wang, David Munechika, Seongmin Lee, Duen Horng Chau
Categories: cs.HC cs.LG
Comments: Accepted at CHI 2024 (Late-Breaking Work). 17 pages, 11 figures, 1
  table. SuperNOVA is available at: http://poloclub.github.io/supernova/. The
  code is available at: https://github.com/poloclub/supernova
DOI: 10.1145/3613905.3650848
\\ ( https://arxiv.org/abs/2305.03039 ,  8604kb)
------------------------------------------------------------------------------
\\
arXiv:2305.08396 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 12:50:38 GMT   (1682kb,D)

Title: MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation
Authors: Abdul Rehman Khan, Asifullah Khan
Categories: eess.IV cs.CV cs.LG
Comments: 19 pages, 6 figures, 5 tables
\\ ( https://arxiv.org/abs/2305.08396 ,  1682kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18334
replaced with revised version Thu, 28 Mar 2024 21:27:18 GMT   (579kb,D)

Title: PQA: Exploring the Potential of Product Quantization in DNN Hardware
  Acceleration
Authors: Ahmed F. AbouElhamayed, Angela Cui, Javier Fernandez-Marques, Nicholas
  D. Lane, Mohamed S. Abdelfattah
Categories: cs.AR cs.LG
Comments: ACM Transactions on Reconfigurable Technology and Systems (TRETS) -
  FCCM 2024 Journal Track
\\ ( https://arxiv.org/abs/2305.18334 ,  579kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02766
replaced with revised version Fri, 29 Mar 2024 02:55:22 GMT   (3892kb,D)

Title: Temporal Difference Learning for High-Dimensional PIDEs with Jumps
Authors: Liwei Lu, Hailong Guo, Xu Yang, Yi Zhu
Categories: math.NA cs.LG cs.NA
\\ ( https://arxiv.org/abs/2307.02766 ,  3892kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11256 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 17:50:17 GMT   (8592kb,D)

Title: Gromov-Wassertein-like Distances in the Gaussian Mixture Models Space
Authors: Antoine Salmona, Julie Delon, Agn\`es Desolneux
Categories: stat.ML cs.CV cs.LG
Comments: preprint
\\ ( https://arxiv.org/abs/2310.11256 ,  8592kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14085
replaced with revised version Thu, 28 Mar 2024 19:37:02 GMT   (44kb)

Title: Adaptive, Doubly Optimal No-Regret Learning in Strongly Monotone and
  Exp-Concave Games with Gradient Feedback
Authors: Michael I. Jordan, Tianyi Lin and Zhengyuan Zhou
Categories: cs.GT cs.LG math.OC
Comments: Accepted by Operations Research; 47 pages
\\ ( https://arxiv.org/abs/2310.14085 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18274
replaced with revised version Fri, 29 Mar 2024 17:34:40 GMT   (47400kb,D)

Title: LipSim: A Provably Robust Perceptual Similarity Metric
Authors: Sara Ghazanfari, Alexandre Araujo, Prashanth Krishnamurthy, Farshad
  Khorrami, Siddharth Garg
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2310.18274 ,  47400kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04855 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 17:10:55 GMT   (4072kb,D)

Title: Algorithms for Non-Negative Matrix Factorization on Noisy Data With
  Negative Values
Authors: Dylan Green, Stephen Bailey
Categories: astro-ph.IM cs.LG eess.SP stat.ME
Comments: 12 pages, 8 figures. Submitted to IEEE Transactions on Signal
  Processing. Updated version after reviewer comments, expanding paper with a
  new section and a new appendix as well as more equations. Algorithm
  derivation flow was significantly altered to be more tractable
\\ ( https://arxiv.org/abs/2311.04855 ,  4072kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17693
replaced with revised version Thu, 28 Mar 2024 18:24:46 GMT   (8257kb,D)

Title: Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using
  Reinforcement and Imitation Learning
Authors: Amr Gomaa and Bilal Mahdy and Niko Kleer and Antonio Kr\"uger
Categories: cs.RO cs.CV cs.HC cs.LG
\\ ( https://arxiv.org/abs/2311.17693 ,  8257kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05269
replaced with revised version Fri, 29 Mar 2024 15:44:05 GMT   (5517kb,D)

Title: LifelongMemory: Leveraging LLMs for Answering Queries in Long-form
  Egocentric Videos
Authors: Ying Wang, Yanlai Yang, Mengye Ren
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.05269 ,  5517kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11461
replaced with revised version Fri, 29 Mar 2024 04:32:57 GMT   (30235kb,D)

Title: GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning
Authors: Ye Yuan, Xueting Li, Yangyi Huang, Shalini De Mello, Koki Nagano, Jan
  Kautz, Umar Iqbal
Categories: cs.CV cs.GR cs.LG
Comments: CVPR 2024. Project website: https://nvlabs.github.io/GAvatar
\\ ( https://arxiv.org/abs/2312.11461 ,  30235kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00320
replaced with revised version Fri, 29 Mar 2024 12:04:52 GMT   (33454kb,D)

Title: DXAI: Explaining Classification by Image Decomposition
Authors: Elnatan Kadar, Guy Gilboa
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.00320 ,  33454kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11624
replaced with revised version Fri, 29 Mar 2024 14:20:17 GMT   (1945kb,D)

Title: Dual-Channel Multiplex Graph Neural Networks for Recommendation
Authors: Xiang Li, Chaofan Fu, Zhongying Zhao, Guanjie Zheng, Chao Huang, Junyu
  Dong, Yanwei Yu
Categories: cs.IR cs.LG
\\ ( https://arxiv.org/abs/2403.11624 ,  1945kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12687
replaced with revised version Fri, 29 Mar 2024 12:45:27 GMT   (597kb,D)

Title: Audio-Visual Compound Expression Recognition Method based on Late
  Modality Fusion and Rule-based Decision
Authors: Elena Ryumina, Maxim Markitantov, Dmitry Ryumin, Heysem Kaya, Alexey
  Karpov
Categories: cs.CV cs.LG
Comments: 7 pages, 3 figures
\\ ( https://arxiv.org/abs/2403.12687 ,  597kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16970 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 16:14:41 GMT   (2071kb,D)

Title: Joint chest X-ray diagnosis and clinical visual attention prediction
  with multi-stage cooperative learning: enhancing interpretability
Authors: Zirui Qiu, Hassan Rivaz, Yiming Xiao
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.16970 ,  2071kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
