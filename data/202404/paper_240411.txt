Gmail jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 100021 1
send mail ONLY to cs <no-reply@arxiv.org> 2024年4月11日 11:21
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue  9 Apr 24 18:00:00 GMT  to  Wed 10 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.06571
Date: Tue, 9 Apr 2024 18:46:46 GMT   (2156kb)

Title: Building A Knowledge Graph to Enrich ChatGPT Responses in Manufacturing
  Service Discovery
Authors: Yunqing Li, Binil Starly
Categories: cs.AI
\\
  Sourcing and identification of new manufacturing partners is crucial for
manufacturing system integrators to enhance agility and reduce risk through
supply chain diversification in the global economy. The advent of advanced
large language models has captured significant interest, due to their ability
to generate comprehensive and articulate responses across a wide range of
knowledge domains. However, the system often falls short in accuracy and
completeness when responding to domain-specific inquiries, particularly in
areas like manufacturing service discovery. This research explores the
potential of leveraging Knowledge Graphs in conjunction with ChatGPT to
streamline the process for prospective clients in identifying small
manufacturing enterprises. In this study, we propose a method that integrates
bottom-up ontology with advanced machine learning models to develop a
Manufacturing Service Knowledge Graph from an array of structured and
unstructured data sources, including the digital footprints of small-scale
manufacturers throughout North America. The Knowledge Graph and the learned
graph embedding vectors are leveraged to tackle intricate queries within the
digital supply chain network, responding with enhanced reliability and greater
interpretability. The approach highlighted is scalable to millions of entities
that can be distributed to form a global Manufacturing Service Knowledge
Network Graph that can potentially interconnect multiple types of Knowledge
Graphs that span industry sectors, geopolitical boundaries, and business
domains. The dataset developed for this study, now publicly accessible,
encompasses more than 13,000 manufacturers' weblinks, manufacturing services,
certifications, and location entity types.
\\ ( https://arxiv.org/abs/2404.06571 ,  2156kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06609
Date: Tue, 9 Apr 2024 20:40:00 GMT   (42652kb,D)

Title: GOAT-Bench: A Benchmark for Multi-Modal Lifelong Navigation
Authors: Mukul Khanna, Ram Ramrakhya, Gunjan Chhablani, Sriram Yenamandra,
  Theophile Gervet, Matthew Chang, Zsolt Kira, Devendra Singh Chaplot, Dhruv
  Batra, Roozbeh Mottaghi
Categories: cs.AI cs.RO
\\
  The Embodied AI community has made significant strides in visual navigation
tasks, exploring targets from 3D coordinates, objects, language descriptions,
and images. However, these navigation models often handle only a single input
modality as the target. With the progress achieved so far, it is time to move
towards universal navigation models capable of handling various goal types,
enabling more effective user interaction with robots. To facilitate this goal,
we propose GOAT-Bench, a benchmark for the universal navigation task referred
to as GO to AnyThing (GOAT). In this task, the agent is directed to navigate to
a sequence of targets specified by the category name, language description, or
image in an open-vocabulary fashion. We benchmark monolithic RL and modular
methods on the GOAT task, analyzing their performance across modalities, the
role of explicit and implicit scene memories, their robustness to noise in goal
specifications, and the impact of memory in lifelong scenarios.
\\ ( https://arxiv.org/abs/2404.06609 ,  42652kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06681
Date: Wed, 10 Apr 2024 02:02:34 GMT   (251kb,D)

Title: Causal Unit Selection using Tractable Arithmetic Circuits
Authors: Haiying Huang, Adnan Darwiche
Categories: cs.AI cs.LG stat.ME
\\
  The unit selection problem aims to find objects, called units, that optimize
a causal objective function which describes the objects' behavior in a causal
context (e.g., selecting customers who are about to churn but would most likely
change their mind if encouraged). While early studies focused mainly on
bounding a specific class of counterfactual objective functions using data,
more recent work allows one to find optimal units exactly by reducing the
causal objective to a classical objective on a meta-model, and then applying a
variant of the classical Variable Elimination (VE) algorithm to the meta-model
-- assuming a fully specified causal model is available. In practice, however,
finding optimal units using this approach can be very expensive because the
used VE algorithm must be exponential in the constrained treewidth of the
meta-model, which is larger and denser than the original model. We address this
computational challenge by introducing a new approach for unit selection that
is not necessarily limited by the constrained treewidth. This is done through
compiling the meta-model into a special class of tractable arithmetic circuits
that allows the computation of optimal units in time linear in the circuit
size. We finally present empirical results on random causal models that show
order-of-magnitude speedups based on the proposed method for solving unit
selection.
\\ ( https://arxiv.org/abs/2404.06681 ,  251kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06946
Date: Wed, 10 Apr 2024 11:55:33 GMT   (1565kb,D)

Title: A Survey on the Integration of Generative AI for Critical Thinking in
  Mobile Networks
Authors: Athanasios Karapantelakis, Alexandros Nikou, Ajay Kattepur, Jean
  Martins, Leonid Mokrushin, Swarup Kumar Mohalik, Marin Orlic, Aneta
  Vulgarakis Feljan
Categories: cs.AI
Comments: 14 pages, 3 figures, 4 tables
\\
  In the near future, mobile networks are expected to broaden their services
and coverage to accommodate a larger user base and diverse user needs. Thus,
they will increasingly rely on artificial intelligence (AI) to manage network
operation and control costs, undertaking complex decision-making roles. This
shift will necessitate the application of techniques that incorporate critical
thinking abilities, including reasoning and planning. Symbolic AI techniques
already facilitate critical thinking based on existing knowledge. Yet, their
use in telecommunications is hindered by the high cost of mostly manual
curation of this knowledge and high computational complexity of reasoning
tasks. At the same time, there is a spurt of innovations in industries such as
telecommunications due to Generative AI (GenAI) technologies, operating
independently of human-curated knowledge. However, their capacity for critical
thinking remains uncertain. This paper aims to address this gap by examining
the current status of GenAI algorithms with critical thinking capabilities and
investigating their potential applications in telecom networks. Specifically,
the aim of this study is to offer an introduction to the potential utilization
of GenAI for critical thinking techniques in mobile networks, while also
establishing a foundation for future research.
\\ ( https://arxiv.org/abs/2404.06946 ,  1565kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07139
Date: Wed, 10 Apr 2024 16:14:05 GMT   (10523kb,D)

Title: Towards a Game-theoretic Understanding of Explanation-based Membership
  Inference Attacks
Authors: Kavita Kumari, Murtuza Jadliwala, Sumit Kumar Jha, Anindya Maiti
Categories: cs.AI cs.GT
Comments: arXiv admin note: text overlap with arXiv:2202.02659
\\
  Model explanations improve the transparency of black-box machine learning
(ML) models and their decisions; however, they can also be exploited to carry
out privacy threats such as membership inference attacks (MIA). Existing works
have only analyzed MIA in a single "what if" interaction scenario between an
adversary and the target ML model; thus, it does not discern the factors
impacting the capabilities of an adversary in launching MIA in repeated
interaction settings. Additionally, these works rely on assumptions about the
adversary's knowledge of the target model's structure and, thus, do not
guarantee the optimality of the predefined threshold required to distinguish
the members from non-members. In this paper, we delve into the domain of
explanation-based threshold attacks, where the adversary endeavors to carry out
MIA attacks by leveraging the variance of explanations through iterative
interactions with the system comprising of the target ML model and its
corresponding explanation method. We model such interactions by employing a
continuous-time stochastic signaling game framework. In our framework, an
adversary plays a stopping game, interacting with the system (having imperfect
information about the type of an adversary, i.e., honest or malicious) to
obtain explanation variance information and computing an optimal threshold to
determine the membership of a datapoint accurately. First, we propose a sound
mathematical formulation to prove that such an optimal threshold exists, which
can be used to launch MIA. Then, we characterize the conditions under which a
unique Markov perfect equilibrium (or steady state) exists in this dynamic
system. By means of a comprehensive set of simulations of the proposed game
model, we assess different factors that can impact the capability of an
adversary to launch MIA in such repeated interaction settings.
\\ ( https://arxiv.org/abs/2404.07139 ,  10523kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07198
Date: Wed, 10 Apr 2024 17:56:07 GMT   (1058kb,D)

Title: Zero-shot Logical Query Reasoning on any Knowledge Graph
Authors: Mikhail Galkin, Jincheng Zhou, Bruno Ribeiro, Jian Tang, Zhaocheng Zhu
Categories: cs.AI cs.LG
\\
  Complex logical query answering (CLQA) in knowledge graphs (KGs) goes beyond
simple KG completion and aims at answering compositional queries comprised of
multiple projections and logical operations. Existing CLQA methods that learn
parameters bound to certain entity or relation vocabularies can only be applied
to the graph they are trained on which requires substantial training time
before being deployed on a new graph. Here we present UltraQuery, an inductive
reasoning model that can zero-shot answer logical queries on any KG. The core
idea of UltraQuery is to derive both projections and logical operations as
vocabulary-independent functions which generalize to new entities and relations
in any KG. With the projection operation initialized from a pre-trained
inductive KG reasoning model, UltraQuery can solve CLQA on any KG even if it is
only finetuned on a single dataset. Experimenting on 23 datasets, UltraQuery in
the zero-shot inference mode shows competitive or better query answering
performance than best available baselines and sets a new state of the art on 14
of them.
\\ ( https://arxiv.org/abs/2404.07198 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06579
Date: Tue, 9 Apr 2024 19:02:12 GMT   (262kb,D)

Title: Less is More for Improving Automatic Evaluation of Factual Consistency
Authors: Tong Wang, Ninad Kulkarni, Yanjun Qi
Categories: cs.CL cs.AI cs.LG
Comments: Accepted in NAACL24 Industry; 7 pages
\\
  Assessing the factual consistency of automatically generated texts in
relation to source context is crucial for developing reliable natural language
generation applications. Recent literature proposes AlignScore which uses a
unified alignment model to evaluate factual consistency and substantially
outperforms previous methods across many benchmark tasks. In this paper, we
take a closer look of datasets used in AlignScore and uncover an unexpected
finding: utilizing a smaller number of data points can actually improve
performance. We process the original AlignScore training dataset to remove
noise, augment with robustness-enhanced samples, and utilize a subset
comprising 10\% of the data to train an improved factual consistency evaluation
model, we call LIM-RA (Less Is More for Robust AlignScore). LIM-RA demonstrates
superior performance, consistently outperforming AlignScore and other strong
baselines like ChatGPT across four benchmarks (two utilizing traditional
natural language generation datasets and two focused on large language model
outputs). Our experiments show that LIM-RA achieves the highest score on 24 of
the 33 test datasets, while staying competitive on the rest, establishing the
new state-of-the-art benchmarks.
\\ ( https://arxiv.org/abs/2404.06579 ,  262kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06619
Date: Tue, 9 Apr 2024 21:09:22 GMT   (3497kb,D)

Title: FairPair: A Robust Evaluation of Biases in Language Models through
  Paired Perturbations
Authors: Jane Dwivedi-Yu and Raaz Dwivedi and Timo Schick
Categories: cs.CL cs.CY cs.LG
\\
  The accurate evaluation of differential treatment in language models to
specific groups is critical to ensuring a positive and safe user experience. An
ideal evaluation should have the properties of being robust, extendable to new
groups or attributes, and being able to capture biases that appear in typical
usage (rather than just extreme, rare cases). Relatedly, bias evaluation should
surface not only egregious biases but also ones that are subtle and
commonplace, such as a likelihood for talking about appearances with regard to
women. We present FairPair, an evaluation framework for assessing differential
treatment that occurs during ordinary usage. FairPair operates through
counterfactual pairs, but crucially, the paired continuations are grounded in
the same demographic group, which ensures equivalent comparison. Additionally,
unlike prior work, our method factors in the inherent variability that comes
from the generation process itself by measuring the sampling variability. We
present an evaluation of several commonly used generative models and a
qualitative analysis that indicates a preference for discussing family and
hobbies with regard to women.
\\ ( https://arxiv.org/abs/2404.06619 ,  3497kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06621
Date: Tue, 9 Apr 2024 21:12:08 GMT   (371kb,D)

Title: What is Your Favorite Gender, MLM? Gender Bias Evaluation in
  Multilingual Masked Language Models
Authors: Jeongrok Yu and Seong Ug Kim and Jacob Choi and Jinho D. Choi
Categories: cs.CL
\\
  Bias is a disproportionate prejudice in favor of one side against another.
Due to the success of transformer-based Masked Language Models (MLMs) and their
impact on many NLP tasks, a systematic evaluation of bias in these models is
needed more than ever. While many studies have evaluated gender bias in English
MLMs, only a few works have been conducted for the task in other languages.
This paper proposes a multilingual approach to estimate gender bias in MLMs
from 5 languages: Chinese, English, German, Portuguese, and Spanish. Unlike
previous work, our approach does not depend on parallel corpora coupled with
English to detect gender bias in other languages using multilingual lexicons.
Moreover, a novel model-based method is presented to generate sentence pairs
for a more robust analysis of gender bias, compared to the traditional
lexicon-based method. For each language, both the lexicon-based and model-based
methods are applied to create two datasets respectively, which are used to
evaluate gender bias in an MLM specifically trained for that language using one
existing and 3 new scoring metrics. Our results show that the previous approach
is data-sensitive and not stable as it does not remove contextual dependencies
irrelevant to gender. In fact, the results often flip when different scoring
metrics are used on the same dataset, suggesting that gender bias should be
studied on a large dataset using multiple evaluation metrics for best practice.
\\ ( https://arxiv.org/abs/2404.06621 ,  371kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06644
Date: Tue, 9 Apr 2024 22:38:13 GMT   (6357kb,D)

Title: Khayyam Challenge (PersianMMLU): Is Your LLM Truly Wise to The Persian
  Language?
Authors: Omid Ghahroodi, Marzia Nouri, Mohammad Vali Sanian, Alireza Sahebi,
  Doratossadat Dastgheib, Ehsaneddin Asgari, Mahdieh Soleymani Baghshah, and
  Mohammad Hossein Rohban
Categories: cs.CL cs.AI
\\
  Evaluating Large Language Models (LLMs) is challenging due to their
generative nature, necessitating precise evaluation methodologies.
Additionally, non-English LLM evaluation lags behind English, resulting in the
absence or weakness of LLMs for many languages. In response to this necessity,
we introduce Khayyam Challenge (also known as PersianMMLU), a meticulously
curated collection comprising 20,192 four-choice questions sourced from 38
diverse tasks extracted from Persian examinations, spanning a wide spectrum of
subjects, complexities, and ages. The primary objective of the Khayyam
Challenge is to facilitate the rigorous evaluation of LLMs that support the
Persian language. Distinctive features of the Khayyam Challenge are (i) its
comprehensive coverage of various topics, including literary comprehension,
mathematics, sciences, logic, intelligence testing, etc., aimed at assessing
different facets of LLMs such as language comprehension, reasoning, and
information retrieval across various educational stages, from lower primary
school to upper secondary school (ii) its inclusion of rich metadata such as
human response rates, difficulty levels, and descriptive answers (iii) its
utilization of new data to avoid data contamination issues prevalent in
existing frameworks (iv) its use of original, non-translated data tailored for
Persian speakers, ensuring the framework is free from translation challenges
and errors while encompassing cultural nuances (v) its inherent scalability for
future data updates and evaluations without requiring special human effort.
Previous works lacked an evaluation framework that combined all of these
features into a single comprehensive benchmark. Furthermore, we evaluate a wide
range of existing LLMs that support the Persian language, with statistical
analyses and interpretations of their outputs.
\\ ( https://arxiv.org/abs/2404.06644 ,  6357kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06654
Date: Tue, 9 Apr 2024 23:41:27 GMT   (164kb,D)

Title: RULER: What's the Real Context Size of Your Long-Context Language
  Models?
Authors: Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima
  Rekesh, Fei Jia, Boris Ginsburg
Categories: cs.CL
\\
  The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve
a piece of information (the "needle") from long distractor texts (the
"haystack"), has been widely adopted to evaluate long-context language models
(LMs). However, this simple retrieval-based test is indicative of only a
superficial form of long-context understanding. To provide a more comprehensive
evaluation of long-context LMs, we create a new synthetic benchmark RULER with
flexible configurations for customized sequence length and task complexity.
RULER expands upon the vanilla NIAH test to encompass variations with diverse
types and quantities of needles. Moreover, RULER introduces new task categories
multi-hop tracing and aggregation to test behaviors beyond searching from
context. We evaluate ten long-context LMs with 13 representative tasks in
RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, all
models exhibit large performance drops as the context length increases. While
these models all claim context sizes of 32K tokens or greater, only four models
(GPT-4, Command-R, Yi-34B, and Mixtral) can maintain satisfactory performance
at the length of 32K. Our analysis of Yi-34B, which supports context length of
200K, reveals large room for improvement as we increase input length and task
complexity. We open source RULER to spur comprehensive evaluation of
long-context LMs.
\\ ( https://arxiv.org/abs/2404.06654 ,  164kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06659
Date: Tue, 9 Apr 2024 23:51:29 GMT   (111kb,D)

Title: Leveraging Interesting Facts to Enhance User Engagement with
  Conversational Interfaces
Authors: Nikhita Vedula, Giuseppe Castellucci, Eugene Agichtein, Oleg
  Rokhlenko, Shervin Malmasi
Categories: cs.CL
Comments: 10 pages, 1 figure
\\
  Conversational Task Assistants (CTAs) guide users in performing a multitude
of activities, such as making recipes. However, ensuring that interactions
remain engaging, interesting, and enjoyable for CTA users is not trivial,
especially for time-consuming or challenging tasks. Grounded in psychological
theories of human interest, we propose to engage users with contextual and
interesting statements or facts during interactions with a multi-modal CTA, to
reduce fatigue and task abandonment before a task is complete. To
operationalize this idea, we train a high-performing classifier (82% F1-score)
to automatically identify relevant and interesting facts for users. We use it
to create an annotated dataset of task-specific interesting facts for the
domain of cooking. Finally, we design and validate a dialogue policy to
incorporate the identified relevant and interesting facts into a conversation,
to improve user engagement and task completion. Live testing on a leading
multi-modal voice assistant shows that 66% of the presented facts were received
positively, leading to a 40% gain in the user satisfaction rating, and a 37%
increase in conversation length. These findings emphasize that strategically
incorporating interesting facts into the CTA experience can promote real-world
user participation for guided task interactions.
\\ ( https://arxiv.org/abs/2404.06659 ,  111kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06664
Date: Wed, 10 Apr 2024 00:25:09 GMT   (7317kb,D)

Title: CulturalTeaming: AI-Assisted Interactive Red-Teaming for Challenging
  LLMs' (Lack of) Multicultural Knowledge
Authors: Yu Ying Chiu, Liwei Jiang, Maria Antoniak, Chan Young Park, Shuyue
  Stella Li, Mehar Bhatia, Sahithya Ravi, Yulia Tsvetkov, Vered Shwartz, Yejin
  Choi
Categories: cs.CL cs.AI cs.HC
Comments: Preprint (under review)
\\
  Frontier large language models (LLMs) are developed by researchers and
practitioners with skewed cultural backgrounds and on datasets with skewed
sources. However, LLMs' (lack of) multicultural knowledge cannot be effectively
assessed with current methods for developing benchmarks. Existing multicultural
evaluations primarily rely on expensive and restricted human annotations or
potentially outdated internet resources. Thus, they struggle to capture the
intricacy, dynamics, and diversity of cultural norms. LLM-generated benchmarks
are promising, yet risk propagating the same biases they are meant to measure.
To synergize the creativity and expert cultural knowledge of human annotators
and the scalability and standardizability of LLM-based automation, we introduce
CulturalTeaming, an interactive red-teaming system that leverages human-AI
collaboration to build truly challenging evaluation dataset for assessing the
multicultural knowledge of LLMs, while improving annotators' capabilities and
experiences. Our study reveals that CulturalTeaming's various modes of AI
assistance support annotators in creating cultural questions, that modern LLMs
fail at, in a gamified manner. Importantly, the increased level of AI
assistance (e.g., LLM-generated revision hints) empowers users to create more
difficult questions with enhanced perceived creativity of themselves, shedding
light on the promises of involving heavier AI assistance in modern evaluation
dataset creation procedures. Through a series of 1-hour workshop sessions, we
gather CULTURALBENCH-V0.1, a compact yet high-quality evaluation dataset with
users' red-teaming attempts, that different families of modern LLMs perform
with accuracy ranging from 37.7% to 72.2%, revealing a notable gap in LLMs'
multicultural proficiency.
\\ ( https://arxiv.org/abs/2404.06664 ,  7317kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06670
Date: Wed, 10 Apr 2024 01:14:12 GMT   (1704kb,D)

Title: What's Mine becomes Yours: Defining, Annotating and Detecting
  Context-Dependent Paraphrases in News Interview Dialogs
Authors: Anna Wegmann, Tijs van den Broek, Dong Nguyen
Categories: cs.CL
\\
  Best practices for high conflict conversations like counseling or customer
support almost always include recommendations to paraphrase the previous
speaker. Although paraphrase classification has received widespread attention
in NLP, paraphrases are usually considered independent from context, and common
models and datasets are not applicable to dialog settings. In this work, we
investigate paraphrases in dialog (e.g., Speaker 1: "That book is mine."
becomes Speaker 2: "That book is yours."). We provide an operationalization of
context-dependent paraphrases, and develop a training for crowd-workers to
classify paraphrases in dialog. We introduce a dataset with utterance pairs
from NPR and CNN news interviews annotated for context-dependent paraphrases.
To enable analyses on label variation, the dataset contains 5,581 annotations
on 600 utterance pairs. We present promising results with in-context learning
and with token classification models for automatic paraphrase detection in
dialog.
\\ ( https://arxiv.org/abs/2404.06670 ,  1704kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06680
Date: Wed, 10 Apr 2024 02:02:34 GMT   (847kb,D)

Title: Onco-Retriever: Generative Classifier for Retrieval of EHR Records in
  Oncology
Authors: Shashi Kant Gupta, Aditya Basu, Bradley Taylor, Anai Kothari, Hrituraj
  Singh
Categories: cs.CL
Comments: 18 pages
\\
  Retrieving information from EHR systems is essential for answering specific
questions about patient journeys and improving the delivery of clinical care.
Despite this fact, most EHR systems still rely on keyword-based searches. With
the advent of generative large language models (LLMs), retrieving information
can lead to better search and summarization capabilities. Such retrievers can
also feed Retrieval-augmented generation (RAG) pipelines to answer any query.
However, the task of retrieving information from EHR real-world clinical data
contained within EHR systems in order to solve several downstream use cases is
challenging due to the difficulty in creating query-document support pairs. We
provide a blueprint for creating such datasets in an affordable manner using
large language models. Our method results in a retriever that is 30-50 F-1
points better than propriety counterparts such as Ada and Mistral for oncology
data elements. We further compare our model, called Onco-Retriever, against
fine-tuned PubMedBERT model as well. We conduct an extensive manual evaluation
on real-world EHR data along with latency analysis of the different models and
provide a path forward for healthcare organizations to build domain-specific
retrievers.
\\ ( https://arxiv.org/abs/2404.06680 ,  847kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06709
Date: Wed, 10 Apr 2024 03:30:01 GMT   (173kb,D)

Title: CQIL: Inference Latency Optimization with Concurrent Computation of
  Quasi-Independent Layers
Authors: Longwei Zou, Qingyang Wang, Han Zhao, Jiangang Kong, Yi Yang, Yangdong
  Deng
Categories: cs.CL
Comments: ARR Under Review
\\
  The fast-growing large scale language models are delivering unprecedented
performance on almost all natural language processing tasks. However, the
effectiveness of large language models are reliant on an exponentially
increasing number of parameters. The overwhelming computation complexity incurs
a high inference latency that negatively affects user experience. Existing
methods to improve inference efficiency, such as tensor parallelism and
quantization, target to reduce per-layer computing latency, yet overlook the
cumulative latency due to the number of layers. Recent works on reducing the
cumulative latency through layer removing, however, lead to significant
performance drop. Motivated by the similarity of inputs among adjacent layers,
we propose to identify quasi-independent layers, which can be concurrently
computed to significantly decrease inference latency. We also introduce a
bypassing technique to mitigate the effect of information loss. Empirical
experiments of the proposed approach on the LLaMA models confirm that
Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by
up to 48.3% on the LLaMA-33B model, while maintaining a close level of
performance.
\\ ( https://arxiv.org/abs/2404.06709 ,  173kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06711
Date: Wed, 10 Apr 2024 03:35:51 GMT   (2196kb,D)

Title: MathVC: An LLM-Simulated Multi-Character Virtual Classroom for
  Mathematics Education
Authors: Murong Yue, Wijdane Mifdal, Yixuan Zhang, Jennifer Suh, Ziyu Yao
Categories: cs.CL cs.HC
Comments: Work in progress
\\
  Mathematical modeling (MM) is considered a fundamental skill for students in
STEM disciplines. Practicing the MM skill is often the most effective when
students can engage in group discussion and collaborative problem-solving.
However, due to unevenly distributed teachers and educational resources needed
to monitor such group activities, students do not always receive equal
opportunities for this practice. Excitingly, large language models (LLMs) have
recently demonstrated strong capability in both modeling mathematical problems
and simulating characters with different traits and properties. Drawing
inspiration from the advancement of LLMs, in this work, we present MATHVC, the
very first LLM-powered virtual classroom containing multiple LLM-simulated
student characters, with whom a human student can practice their MM skill. To
encourage each LLM character's behaviors to be aligned with their specified
math-relevant properties (termed "characteristics alignment") and the overall
conversational procedure to be close to an authentic student MM discussion
(termed "conversational procedural alignment"), we proposed three innovations:
integrating MM domain knowledge into the simulation, defining a symbolic schema
as the ground for character simulation, and designing a meta planner at the
platform level to drive the conversational procedure. Through experiments and
ablation studies, we confirmed the effectiveness of our simulation approach and
showed the promise for MATHVC to benefit real-life students in the future.
\\ ( https://arxiv.org/abs/2404.06711 ,  2196kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06714
Date: Wed, 10 Apr 2024 03:46:03 GMT   (919kb,D)

Title: Llama-VITS: Enhancing TTS Synthesis with Semantic Awareness
Authors: Xincan Feng, Akifumi Yoshimoto
Categories: cs.CL cs.SD eess.AS
Comments: 9 pages, 2 figures, 4 tables
\\
  Recent advancements in Natural Language Processing (NLP) have seen
Large-scale Language Models (LLMs) excel at producing high-quality text for
various purposes. Notably, in Text-To-Speech (TTS) systems, the integration of
BERT for semantic token generation has underscored the importance of semantic
content in producing coherent speech outputs. Despite this, the specific
utility of LLMs in enhancing TTS synthesis remains considerably limited. This
research introduces an innovative approach, Llama-VITS, which enhances TTS
synthesis by enriching the semantic content of text using LLM. Llama-VITS
integrates semantic embeddings from Llama2 with the VITS model, a leading
end-to-end TTS framework. By leveraging Llama2 for the primary speech synthesis
process, our experiments demonstrate that Llama-VITS matches the naturalness of
the original VITS (ORI-VITS) and those incorporate BERT (BERT-VITS), on the
LJSpeech dataset, a substantial collection of neutral, clear speech. Moreover,
our method significantly enhances emotive expressiveness on the EmoV_DB_bea_sem
dataset, a curated selection of emotionally consistent speech from the EmoV_DB
dataset, highlighting its potential to generate emotive speech.
\\ ( https://arxiv.org/abs/2404.06714 ,  919kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06742
Date: Wed, 10 Apr 2024 05:00:35 GMT   (9094kb,D)

Title: Transferable and Efficient Non-Factual Content Detection via Probe
  Training with Offline Consistency Checking
Authors: Xiaokang Zhang, Zijun Yao, Jing Zhang, Kaifeng Yun, Jifan Yu, Juanzi
  Li, Jie Tang
Categories: cs.CL
\\
  Detecting non-factual content is a longstanding goal to increase the
trustworthiness of large language models (LLMs) generations. Current factuality
probes, trained using humanannotated labels, exhibit limited transferability to
out-of-distribution content, while online selfconsistency checking imposes
extensive computation burden due to the necessity of generating multiple
outputs. This paper proposes PINOSE, which trains a probing model on offline
self-consistency checking results, thereby circumventing the need for
human-annotated data and achieving transferability across diverse data
distributions. As the consistency check process is offline, PINOSE reduces the
computational burden of generating multiple responses by online consistency
verification. Additionally, it examines various aspects of internal states
prior to response decoding, contributing to more effective detection of factual
inaccuracies. Experiment results on both factuality detection and question
answering benchmarks show that PINOSE achieves surpassing results than existing
factuality detection methods. Our code and datasets are publicly available on
this anonymized repository.
\\ ( https://arxiv.org/abs/2404.06742 ,  9094kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06760
Date: Wed, 10 Apr 2024 05:56:46 GMT   (148kb,D)

Title: DiffusionDialog: A Diffusion Model for Diverse Dialog Generation with
  Latent Space
Authors: Jianxiang Xiang, Zhenhua Liu, Haodong Liu, Yin Bai, Jia Cheng,
  Wenliang Chen
Categories: cs.CL cs.AI
Comments: LREC-COLING 2024 camera ready
\\
  In real-life conversations, the content is diverse, and there exists the
one-to-many problem that requires diverse generation. Previous studies
attempted to introduce discrete or Gaussian-based continuous latent variables
to address the one-to-many problem, but the diversity is limited. Recently,
diffusion models have made breakthroughs in computer vision, and some attempts
have been made in natural language processing. In this paper, we propose
DiffusionDialog, a novel approach to enhance the diversity of dialogue
generation with the help of diffusion model. In our approach, we introduce
continuous latent variables into the diffusion model. The problem of using
latent variables in the dialog task is how to build both an effective prior of
the latent space and an inferring process to obtain the proper latent given the
context. By combining the encoder and latent-based diffusion model, we encode
the response's latent representation in a continuous space as the prior,
instead of fixed Gaussian distribution or simply discrete ones. We then infer
the latent by denoising step by step with the diffusion model. The experimental
results show that our model greatly enhances the diversity of dialog responses
while maintaining coherence. Furthermore, in further analysis, we find that our
diffusion model achieves high inference efficiency, which is the main challenge
of applying diffusion models in natural language processing.
\\ ( https://arxiv.org/abs/2404.06760 ,  148kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06762
Date: Wed, 10 Apr 2024 06:03:13 GMT   (2653kb,D)

Title: Personality-aware Student Simulation for Conversational Intelligent
  Tutoring Systems
Authors: Zhengyuan Liu, Stella Xin Yin, Geyu Lin, Nancy F. Chen
Categories: cs.CL cs.HC
\\
  Intelligent Tutoring Systems (ITSs) can provide personalized and self-paced
learning experience. The emergence of large language models (LLMs) further
enables better human-machine interaction, and facilitates the development of
conversational ITSs in various disciplines such as math and language learning.
In dialogic teaching, recognizing and adapting to individual characteristics
can significantly enhance student engagement and learning efficiency. However,
characterizing and simulating student's persona remain challenging in training
and evaluating conversational ITSs. In this work, we propose a framework to
construct profiles of different student groups by refining and integrating both
cognitive and noncognitive aspects, and leverage LLMs for personality-aware
student simulation in a language learning scenario. We further enhance the
framework with multi-aspect validation, and conduct extensive analysis from
both teacher and student perspectives. Our experimental results show that
state-of-the-art LLMs can produce diverse student responses according to the
given language ability and personality traits, and trigger teacher's adaptive
scaffolding strategies.
\\ ( https://arxiv.org/abs/2404.06762 ,  2653kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06809
Date: Wed, 10 Apr 2024 07:56:26 GMT   (6149kb,D)

Title: Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation
Authors: Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang,
  Xunliang Cai, Le Sun
Categories: cs.CL
Comments: Our code, benchmark, and models are available at
  https://github.com/panruotong/CAG
\\
  The rapid development of large language models has led to the widespread
adoption of Retrieval-Augmented Generation (RAG), which integrates external
knowledge to alleviate knowledge bottlenecks and mitigate hallucinations.
However, the existing RAG paradigm inevitably suffers from the impact of flawed
information introduced during the retrieval phrase, thereby diminishing the
reliability and correctness of the generated outcomes. In this paper, we
propose Credibility-aware Generation (CAG), a universally applicable framework
designed to mitigate the impact of flawed information in RAG. At its core, CAG
aims to equip models with the ability to discern and process information based
on its credibility. To this end, we propose an innovative data transformation
framework that generates data based on credibility, thereby effectively
endowing models with the capability of CAG. Furthermore, to accurately evaluate
the models' capabilities of CAG, we construct a comprehensive benchmark
covering three critical real-world scenarios. Experimental results demonstrate
that our model can effectively understand and utilize credibility for
generation, significantly outperform other models with retrieval augmentation,
and exhibit resilience against the disruption caused by noisy documents,
thereby maintaining robust performance. Moreover, our model supports customized
credibility, offering a wide range of potential applications.
\\ ( https://arxiv.org/abs/2404.06809 ,  6149kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06812
Date: Wed, 10 Apr 2024 08:00:26 GMT   (965kb,D)

Title: Emotion-cause pair extraction method based on multi-granularity
  information and multi-module interaction
Authors: Mingrui Fu and Weijiang Li
Categories: cs.CL
\\
  The purpose of emotion-cause pair extraction is to extract the pair of
emotion clauses and cause clauses. On the one hand, the existing methods do not
take fully into account the relationship between the emotion extraction of two
auxiliary tasks. On the other hand, the existing two-stage model has the
problem of error propagation. In addition, existing models do not adequately
address the emotion and cause-induced locational imbalance of samples. To solve
these problems, an end-to-end multitasking model (MM-ECPE) based on shared
interaction between GRU, knowledge graph and transformer modules is proposed.
Furthermore, based on MM-ECPE, in order to use the encoder layer to better
solve the problem of imbalanced distribution of clause distances between
clauses and emotion clauses, we propose a novel encoding based on BERT,
sentiment lexicon, and position-aware interaction module layer of emotion motif
pair retrieval model (MM-ECPE(BERT)). The model first fully models the
interaction between different tasks through the multi-level sharing module, and
mines the shared information between emotion-cause pair extraction and the
emotion extraction and cause extraction. Second, to solve the imbalanced
distribution of emotion clauses and cause clauses problem, suitable labels are
screened out according to the knowledge graph path length and task-specific
features are constructed so that the model can focus on extracting pairs with
corresponding emotion-cause relationships. Experimental results on the ECPE
benchmark dataset show that the proposed model achieves good performance,
especially on position-imbalanced samples.
\\ ( https://arxiv.org/abs/2404.06812 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06833
Date: Wed, 10 Apr 2024 08:49:27 GMT   (10627kb,D)

Title: Does Mapo Tofu Contain Coffee? Probing LLMs for Food-related Cultural
  Knowledge
Authors: Li Zhou, Taelin Karidi, Nicolas Garneau, Yong Cao, Wanlong Liu, Wenyu
  Chen, Daniel Hershcovich
Categories: cs.CL
Comments: 20 pages,8 figures
\\
  Recent studies have highlighted the presence of cultural biases in Large
Language Models (LLMs), yet often lack a robust methodology to dissect these
phenomena comprehensively. Our work aims to bridge this gap by delving into the
Food domain, a universally relevant yet culturally diverse aspect of human
life. We introduce FmLAMA, a multilingual dataset centered on food-related
cultural facts and variations in food practices. We analyze LLMs across various
architectures and configurations, evaluating their performance in both
monolingual and multilingual settings. By leveraging templates in six different
languages, we investigate how LLMs interact with language-specific and cultural
knowledge. Our findings reveal that (1) LLMs demonstrate a pronounced bias
towards food knowledge prevalent in the United States; (2) Incorporating
relevant cultural context significantly improves LLMs' ability to access
cultural knowledge; (3) The efficacy of LLMs in capturing cultural nuances is
highly dependent on the interplay between the probing language, the specific
model architecture, and the cultural context in question. This research
underscores the complexity of integrating cultural understanding into LLMs and
emphasizes the importance of culturally diverse datasets to mitigate biases and
enhance model performance across different cultural domains.
\\ ( https://arxiv.org/abs/2404.06833 ,  10627kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06838
Date: Wed, 10 Apr 2024 09:02:33 GMT   (1231kb,D)

Title: Simpler becomes Harder: Do LLMs Exhibit a Coherent Behavior on
  Simplified Corpora?
Authors: Miriam Ansch\"utz, Edoardo Mosca, Georg Groh
Categories: cs.CL
Comments: Published at DeTermIt! Workshop at LREC-COLING 2024
\\
  Text simplification seeks to improve readability while retaining the original
content and meaning. Our study investigates whether pre-trained classifiers
also maintain such coherence by comparing their predictions on both original
and simplified inputs. We conduct experiments using 11 pre-trained models,
including BERT and OpenAI's GPT 3.5, across six datasets spanning three
languages. Additionally, we conduct a detailed analysis of the correlation
between prediction change rates and simplification types/strengths. Our
findings reveal alarming inconsistencies across all languages and models. If
not promptly addressed, simplified inputs can be easily exploited to craft
zero-iteration model-agnostic adversarial attacks with success rates of up to
50%
\\ ( https://arxiv.org/abs/2404.06838 ,  1231kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06854
Date: Wed, 10 Apr 2024 09:28:14 GMT   (8156kb,D)

Title: Control-DAG: Constrained Decoding for Non-Autoregressive Directed
  Acyclic T5 using Weighted Finite State Automata
Authors: Jinghong Chen, Weizhe Lin, Jingbiao Mei, Bill Byrne
Categories: cs.CL
Comments: 11 pages. NAACL 2024
ACM-class: I.2
\\
  The Directed Acyclic Transformer is a fast non-autoregressive (NAR) model
that performs well in Neural Machine Translation. Two issues prevent its
application to general Natural Language Generation (NLG) tasks: frequent
Out-Of-Vocabulary (OOV) errors and the inability to faithfully generate entity
names. We introduce Control-DAG, a constrained decoding algorithm for our
Directed Acyclic T5 (DA-T5) model which offers lexical, vocabulary and length
control. We show that Control-DAG significantly enhances DA-T5 on the Schema
Guided Dialogue and the DART datasets, establishing strong NAR results for
Task-Oriented Dialogue and Data-to-Text NLG.
\\ ( https://arxiv.org/abs/2404.06854 ,  8156kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06910
Date: Wed, 10 Apr 2024 11:03:17 GMT   (605kb,D)

Title: Superposition Prompting: Improving and Accelerating Retrieval-Augmented
  Generation
Authors: Thomas Merth, Qichen Fu, Mohammad Rastegari, Mahyar Najibi
Categories: cs.CL cs.AI cs.LG
\\
  Despite the successes of large language models (LLMs), they exhibit
significant drawbacks, particularly when processing long contexts. Their
inference cost scales quadratically with respect to sequence length, making it
expensive for deployment in some real-world text processing applications, such
as retrieval-augmented generation (RAG). Additionally, LLMs also exhibit the
"distraction phenomenon," where irrelevant context in the prompt degrades
output quality. To address these drawbacks, we propose a novel RAG prompting
methodology, superposition prompting, which can be directly applied to
pre-trained transformer-based LLMs without the need for fine-tuning. At a high
level, superposition prompting allows the LLM to process input documents in
parallel prompt paths, discarding paths once they are deemed irrelevant. We
demonstrate the capability of our method to simultaneously enhance time
efficiency across a variety of question-answering benchmarks using multiple
pre-trained LLMs. Furthermore, our technique significantly improves accuracy
when the retrieved context is large relative the context the model was trained
on. For example, our approach facilitates an 93x reduction in compute time
while improving accuracy by 43\% on the NaturalQuestions-Open dataset with the
MPT-7B instruction-tuned model over naive RAG.
\\ ( https://arxiv.org/abs/2404.06910 ,  605kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06911
Date: Wed, 10 Apr 2024 11:03:57 GMT   (9559kb,D)

Title: GraSAME: Injecting Token-Level Structural Information to Pretrained
  Language Models via Graph-guided Self-Attention Mechanism
Authors: Shuzhou Yuan, Michael F\"arber
Categories: cs.CL
Comments: NAACL 2024 Findings
\\
  Pretrained Language Models (PLMs) benefit from external knowledge stored in
graph structures for various downstream tasks. However, bridging the modality
gap between graph structures and text remains a significant challenge.
Traditional methods like linearizing graphs for PLMs lose vital graph
connectivity, whereas Graph Neural Networks (GNNs) require cumbersome processes
for integration into PLMs. In this work, we propose a novel graph-guided
self-attention mechanism, GraSAME. GraSAME seamlessly incorporates token-level
structural information into PLMs without necessitating additional alignment or
concatenation efforts. As an end-to-end, lightweight multimodal module, GraSAME
follows a multi-task learning strategy and effectively bridges the gap between
graph and textual modalities, facilitating dynamic interactions between GNNs
and PLMs. Our experiments on the graph-to-text generation task demonstrate that
GraSAME outperforms baseline models and achieves results comparable to
state-of-the-art (SOTA) models on WebNLG datasets. Furthermore, compared to
SOTA models, GraSAME eliminates the need for extra pre-training tasks to adjust
graph inputs and reduces the number of trainable parameters by over 100
million.
\\ ( https://arxiv.org/abs/2404.06911 ,  9559kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06921
Date: Wed, 10 Apr 2024 11:17:33 GMT   (785kb,D)

Title: GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM
  Applications
Authors: Shishir G. Patil and Tianjun Zhang and Vivian Fang and Noppapon C. and
  Roy Huang and Aaron Hao and Martin Casado and Joseph E. Gonzalez and Raluca
  Ada Popa and Ion Stoica
Categories: cs.CL cs.AI
\\
  Large Language Models (LLMs) are evolving beyond their classical role of
providing information within dialogue systems to actively engaging with tools
and performing actions on real-world applications and services. Today, humans
verify the correctness and appropriateness of the LLM-generated outputs (e.g.,
code, functions, or actions) before putting them into real-world execution.
This poses significant challenges as code comprehension is well known to be
notoriously difficult. In this paper, we study how humans can efficiently
collaborate with, delegate to, and supervise autonomous LLMs in the future. We
argue that in many cases, "post-facto validation" - verifying the correctness
of a proposed action after seeing the output - is much easier than the
aforementioned "pre-facto validation" setting. The core concept behind enabling
a post-facto validation system is the integration of an intuitive undo feature,
and establishing a damage confinement for the LLM-generated actions as
effective strategies to mitigate the associated risks. Using this, a human can
now either revert the effect of an LLM-generated output or be confident that
the potential risk is bounded. We believe this is critical to unlock the
potential for LLM agents to interact with applications and services with
limited (post-facto) human involvement. We describe the design and
implementation of our open-source runtime for executing LLM actions, Gorilla
Execution Engine (GoEX), and present open research questions towards realizing
the goal of LLMs and applications interacting with each other with minimal
human supervision. We release GoEX at https://github.com/ShishirPatil/gorilla/.
\\ ( https://arxiv.org/abs/2404.06921 ,  785kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06948
Date: Wed, 10 Apr 2024 11:56:01 GMT   (8235kb,D)

Title: MetaCheckGPT -- A Multi-task Hallucination Detection Using LLM
  Uncertainty and Meta-models
Authors: Rahul Mehta, Andrew Hoblitzell, Jack O'Keefe, Hyeju Jang, Vasudeva
  Varma
Categories: cs.CL cs.AI
Comments: Entry for SemEval-2024 Shared Task 6: SHROOM, a Shared-task on
  Hallucinations and Related Observable Overgeneration Mistakes
MSC-class: 68T07, 68T50
ACM-class: I.2.7
\\
  This paper presents our winning solution for the SemEval-2024 Task 6
competition. We propose a meta-regressor framework of large language models
(LLMs) for model evaluation and integration that achieves the highest scores on
the leader board. Our approach leverages uncertainty signals present in a
diverse basket of LLMs to detect hallucinations more robustly.
\\ ( https://arxiv.org/abs/2404.06948 ,  8235kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06954
Date: Wed, 10 Apr 2024 12:12:07 GMT   (8569kb,D)

Title: Accelerating Inference in Large Language Models with a Unified Layer
  Skipping Strategy
Authors: Yijin Liu, Fandong Meng and Jie Zhou
Categories: cs.CL
Comments: 12 pages, codes at https://github.com/Adaxry/Unified_Layer_Skipping
\\
  Recently, dynamic computation methods have shown notable acceleration for
Large Language Models (LLMs) by skipping several layers of computations through
elaborate heuristics or additional predictors. However, in the decoding process
of existing approaches, different samples are assigned different computational
budgets, which cannot guarantee a stable and precise acceleration effect.
Furthermore, existing approaches generally skip multiple contiguous layers at
the bottom or top of the layers, leading to a drastic change in the model's
layer-wise representations, and thus a consequent performance degeneration.
Therefore, we propose a Unified Layer Skipping strategy, which selects the
number of layers to skip computation based solely on the target speedup ratio,
and then skips the corresponding number of intermediate layer computations in a
balanced manner. Since the Unified Layer Skipping strategy is independent of
input samples, it naturally supports popular acceleration techniques such as
batch decoding and KV caching, thus demonstrating more practicality for
real-world applications. Experimental results on two common tasks, i.e.,
machine translation and text summarization, indicate that given a target
speedup ratio, the Unified Layer Skipping strategy significantly enhances both
the inference performance and the actual model throughput over existing dynamic
approaches.
\\ ( https://arxiv.org/abs/2404.06954 ,  8569kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06964
Date: Wed, 10 Apr 2024 12:22:32 GMT   (1822kb,D)

Title: Charles Translator: A Machine Translation System between Ukrainian and
  Czech
Authors: Martin Popel, Lucie Pol\'akov\'a, Michal Nov\'ak, Jind\v{r}ich Helcl,
  Jind\v{r}ich Libovick\'y, Pavel Stra\v{n}\'ak, Tom\'a\v{s} Kraba\v{c},
  Jaroslava Hlav\'a\v{c}ov\'a, Mariia Anisimova, Tereza Chla\v{n}ov\'a
Categories: cs.CL
\\
  We present Charles Translator, a machine translation system between Ukrainian
and Czech, developed as part of a society-wide effort to mitigate the impact of
the Russian-Ukrainian war on individuals and society. The system was developed
in the spring of 2022 with the help of many language data providers in order to
quickly meet the demand for such a service, which was not available at the time
in the required quality. The translator was later implemented as an online web
interface and as an Android app with speech input, both featuring
Cyrillic-Latin script transliteration. The system translates directly, compared
to other available systems that use English as a pivot, and thus take advantage
of the typological similarity of the two languages. It uses the block
back-translation method, which allows for efficient use of monolingual training
data. The paper describes the development process, including data collection
and implementation, evaluation, mentions several use cases, and outlines
possibilities for the further development of the system for educational
purposes.
\\ ( https://arxiv.org/abs/2404.06964 ,  1822kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06970
Date: Wed, 10 Apr 2024 12:31:09 GMT   (533kb,D)

Title: Hybrid Multi-stage Decoding for Few-shot NER with Entity-aware
  Contrastive Learning
Authors: Peipei Liu, Gaosheng Wang, Ying Tong, Jian Liang, Zhenquan Ding,
  Hongsong Zhu
Categories: cs.CL
\\
  Few-shot named entity recognition can identify new types of named entities
based on a few labeled examples. Previous methods employing token-level or
span-level metric learning suffer from the computational burden and a large
number of negative sample spans. In this paper, we propose the Hybrid
Multi-stage Decoding for Few-shot NER with Entity-aware Contrastive Learning
(MsFNER), which splits the general NER into two stages: entity-span detection
and entity classification. There are 3 processes for introducing MsFNER:
training, finetuning, and inference. In the training process, we train and get
the best entity-span detection model and the entity classification model
separately on the source domain using meta-learning, where we create a
contrastive learning module to enhance entity representations for entity
classification. During finetuning, we finetune the both models on the support
dataset of target domain. In the inference process, for the unlabeled data, we
first detect the entity-spans, then the entity-spans are jointly determined by
the entity classification model and the KNN. We conduct experiments on the open
FewNERD dataset and the results demonstrate the advance of MsFNER.
\\ ( https://arxiv.org/abs/2404.06970 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06996
Date: Wed, 10 Apr 2024 13:19:56 GMT   (55kb,D)

Title: XNLIeu: a dataset for cross-lingual NLI in Basque
Authors: Maite Heredia, Julen Etxaniz, Muitze Zulaika, Xabier Saralegi, Jeremy
  Barnes, Aitor Soroa
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024
\\
  XNLI is a popular Natural Language Inference (NLI) benchmark widely used to
evaluate cross-lingual Natural Language Understanding (NLU) capabilities across
languages. In this paper, we expand XNLI to include Basque, a low-resource
language that can greatly benefit from transfer-learning approaches. The new
dataset, dubbed XNLIeu, has been developed by first machine-translating the
English XNLI corpus into Basque, followed by a manual post-edition step. We
have conducted a series of experiments using mono- and multilingual LLMs to
assess a) the effect of professional post-edition on the MT system; b) the best
cross-lingual strategy for NLI in Basque; and c) whether the choice of the best
cross-lingual strategy is influenced by the fact that the dataset is built by
translation. The results show that post-edition is necessary and that the
translate-train cross-lingual strategy obtains better results overall, although
the gain is lower when tested in a dataset that has been built natively from
scratch. Our code and datasets are publicly available under open licenses.
\\ ( https://arxiv.org/abs/2404.06996 ,  55kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07001
Date: Wed, 10 Apr 2024 13:31:07 GMT   (929kb,D)

Title: Event Grounded Criminal Court View Generation withCooperative (Large)
  Language Models
Authors: Linan Yue, Qi Liu, Lili Zhao, Li Wang, Weibo Gao and Yanqing An
Categories: cs.CL cs.AI
DOI: 10.1145/3626772.3657698
\\
  With the development of legal intelligence, Criminal Court View Generation
has attracted much attention as a crucial task of legal intelligence, which
aims to generate concise and coherent texts that summarize case facts and
provide explanations for verdicts. Existing researches explore the key
information in case facts to yield the court views. Most of them employ a
coarse-grained approach that partitions the facts into broad segments (e.g.,
verdict-related sentences) to make predictions. However, this approach fails to
capture the complex details present in the case facts, such as various criminal
elements and legal events. To this end, in this paper, we propose an Event
Grounded Generation (EGG) method for criminal court view generation with
cooperative (Large) Language Models, which introduces the fine-grained event
information into the generation. Specifically, we first design a LLMs-based
extraction method that can extract events in case facts without massive
annotated events. Then, we incorporate the extracted events into court view
generation by merging case facts and events. Besides, considering the
computational burden posed by the use of LLMs in the extraction phase of EGG,
we propose a LLMs-free EGG method that can eliminate the requirement for event
extraction using LLMs in the inference phase. Extensive experimental results on
a real-world dataset clearly validate the effectiveness of our proposed method.
\\ ( https://arxiv.org/abs/2404.07001 ,  929kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07004
Date: Wed, 10 Apr 2024 13:39:11 GMT   (538kb,D)

Title: LM Transparency Tool: Interactive Tool for Analyzing Transformer
  Language Models
Authors: Igor Tufanov, Karen Hambardzumyan, Javier Ferrando, Elena Voita
Categories: cs.CL
\\
  We present the LM Transparency Tool (LM-TT), an open-source interactive
toolkit for analyzing the internal workings of Transformer-based language
models. Differently from previously existing tools that focus on isolated parts
of the decision-making process, our framework is designed to make the entire
prediction process transparent, and allows tracing back model behavior from the
top-layer representation to very fine-grained parts of the model. Specifically,
it (1) shows the important part of the whole input-to-output information flow,
(2) allows attributing any changes done by a model block to individual
attention heads and feed-forward neurons, (3) allows interpreting the functions
of those heads or neurons. A crucial part of this pipeline is showing the
importance of specific model components at each step. As a result, we are able
to look at the roles of model components only in cases where they are important
for a prediction. Since knowing which components should be inspected is key for
analyzing large models where the number of these components is extremely high,
we believe our tool will greatly support the interpretability community both in
research settings and in practical applications.
\\ ( https://arxiv.org/abs/2404.07004 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07009
Date: Wed, 10 Apr 2024 13:50:46 GMT   (133kb,D)

Title: A Mathematical Theory for Learning Semantic Languages by Abstract
  Learners
Authors: Kuo-Yu Liao, Cheng-Shang Chang, Y.-W. Peter Hong
Categories: cs.CL cs.IT cs.LG math.IT
Comments: Submitted to ISIT 2024 on Jan. 28, 2024
\\
  Recent advances in Large Language Models (LLMs) have demonstrated the
emergence of capabilities (learned skills) when the number of system parameters
and the size of training data surpass certain thresholds. The exact mechanisms
behind such phenomena are not fully understood and remain a topic of active
research. Inspired by the skill-text bipartite graph model presented in [1] for
modeling semantic language, we develop a mathematical theory to explain the
emergence of learned skills, taking the learning (or training) process into
account. Our approach models the learning process for skills in the skill-text
bipartite graph as an iterative decoding process in Low-Density Parity Check
(LDPC) codes and Irregular Repetition Slotted ALOHA (IRSA). Using density
evolution analysis, we demonstrate the emergence of learned skills when the
ratio of the size of training texts to the number of skills exceeds a certain
threshold. Our analysis also yields a scaling law for testing errors relative
to the size of training texts. Upon completion of the training, we propose a
method for semantic compression and discuss its application in semantic
communication.
\\ ( https://arxiv.org/abs/2404.07009 ,  133kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07017
Date: Wed, 10 Apr 2024 14:05:44 GMT   (7969kb,D)

Title: Improving Language Model Reasoning with Self-motivated Learning
Authors: Yunlong Feng, Yang Xu, Libo Qin, Yasheng Wang, Wanxiang Che
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\
  Large-scale high-quality training data is important for improving the
performance of models. After trained with data that has rationales (reasoning
steps), models gain reasoning capability. However, the dataset with
high-quality rationales is relatively scarce due to the high annotation cost.
To address this issue, we propose \textit{Self-motivated Learning} framework.
The framework motivates the model itself to automatically generate rationales
on existing datasets. Based on the inherent rank from correctness across
multiple rationales, the model learns to generate better rationales, leading to
higher reasoning capability. Specifically, we train a reward model with the
rank to evaluate the quality of rationales, and improve the performance of
reasoning through reinforcement learning. Experiment results of Llama2 7B on
multiple reasoning datasets show that our method significantly improves the
reasoning ability of models, even outperforming text-davinci-002 in some
datasets.
\\ ( https://arxiv.org/abs/2404.07017 ,  7969kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07036
Date: Wed, 10 Apr 2024 14:28:09 GMT   (1322kb,D)

Title: A Computational Analysis of the Dehumanisation of Migrants from Syria
  and Ukraine in Slovene News Media
Authors: Jaya Caporusso and Damar Hoogland and Mojca Brglez and Boshko Koloski
  and Matthew Purver and Senja Pollak
Categories: cs.CL
Comments: The first authors have contributted equally. Accepted at LREC-COLING
\\
  Dehumanisation involves the perception and or treatment of a social group's
members as less than human. This phenomenon is rarely addressed with
computational linguistic techniques. We adapt a recently proposed approach for
English, making it easier to transfer to other languages and to evaluate,
introducing a new sentiment resource, the use of zero-shot cross-lingual
valence and arousal detection, and a new method for statistical significance
testing. We then apply it to study attitudes to migration expressed in Slovene
newspapers, to examine changes in the Slovene discourse on migration between
the 2015-16 migration crisis following the war in Syria and the 2022-23 period
following the war in Ukraine. We find that while this discourse became more
negative and more intense over time, it is less dehumanising when specifically
addressing Ukrainian migrants compared to others.
\\ ( https://arxiv.org/abs/2404.07036 ,  1322kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07053
Date: Wed, 10 Apr 2024 14:44:48 GMT   (151kb,D)

Title: Meta4XNLI: A Crosslingual Parallel Corpus for Metaphor Detection and
  Interpretation
Authors: Elisa Sanchez-Bayona, Rodrigo Agerri
Categories: cs.CL cs.AI cs.LG
\\
  Metaphors, although occasionally unperceived, are ubiquitous in our everyday
language. Thus, it is crucial for Language Models to be able to grasp the
underlying meaning of this kind of figurative language. In this work, we
present Meta4XNLI, a novel parallel dataset for the tasks of metaphor detection
and interpretation that contains metaphor annotations in both Spanish and
English. We investigate language models' metaphor identification and
understanding abilities through a series of monolingual and cross-lingual
experiments by leveraging our proposed corpus. In order to comprehend how these
non-literal expressions affect models' performance, we look over the results
and perform an error analysis. Additionally, parallel data offers many
potential opportunities to investigate metaphor transferability between these
languages and the impact of translation on the development of multilingual
annotated resources.
\\ ( https://arxiv.org/abs/2404.07053 ,  151kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07060
Date: Wed, 10 Apr 2024 14:50:10 GMT   (538kb,D)

Title: Groundedness in Retrieval-augmented Long-form Generation: An Empirical
  Study
Authors: Alessandro Stolfo
Categories: cs.CL cs.LG
Comments: NAACL 2024 (Findings)
\\
  We present an empirical study of groundedness in long-form question answering
(LFQA) by retrieval-augmented large language models (LLMs). In particular, we
evaluate whether every generated sentence is grounded in the retrieved
documents or the model's pre-training data. Across 3 datasets and 4 model
families, our findings reveal that a significant fraction of generated
sentences are consistently ungrounded, even when those sentences contain
correct ground-truth answers. Additionally, we examine the impacts of factors
such as model size, decoding strategy, and instruction tuning on groundedness.
Our results show that while larger models tend to ground their outputs more
effectively, a significant portion of correct answers remains compromised by
hallucinations. This study provides novel insights into the groundedness
challenges in LFQA and underscores the necessity for more robust mechanisms in
LLMs to mitigate the generation of ungrounded content.
\\ ( https://arxiv.org/abs/2404.07060 ,  538kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07066
Date: Wed, 10 Apr 2024 14:56:40 GMT   (1381kb,D)

Title: Exploring Concept Depth: How Large Language Models Acquire Knowledge at
  Different Layers?
Authors: Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang,
  Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan
  Du, Yongfeng Zhang
Categories: cs.CL cs.AI cs.LG
Comments: 12 pages
\\
  This paper studies the phenomenon that different concepts are learned in
different layers of large language models, i.e. more difficult concepts are
fully acquired with deeper layers. We define the difficulty of concepts by the
level of abstraction, and here it is crudely categorized by factual, emotional,
and inferential. Each category contains a spectrum of tasks, arranged from
simple to complex. For example, within the factual dimension, tasks range from
lie detection to categorizing mathematical problems. We employ a probing
technique to extract representations from different layers of the model and
apply these to classification tasks. Our findings reveal that models tend to
efficiently classify simpler tasks, indicating that these concepts are learned
in shallower layers. Conversely, more complex tasks may only be discernible at
deeper layers, if at all. This paper explores the implications of these
findings for our understanding of model learning processes and internal
representations. Our implementation is available at
\url{https://github.com/Luckfort/CD}.
\\ ( https://arxiv.org/abs/2404.07066 ,  1381kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07084
Date: Wed, 10 Apr 2024 15:17:17 GMT   (943kb,D)

Title: Dynamic Generation of Personalities with Large Language Models
Authors: Jianzhi Liu, Hexiang Gu, Tianyu Zheng, Liuyu Xiang, Huijia Wu, Jie Fu
  and Zhaofeng He
Categories: cs.CL cs.AI
\\
  In the realm of mimicking human deliberation, large language models (LLMs)
show promising performance, thereby amplifying the importance of this research
area. Deliberation is influenced by both logic and personality. However,
previous studies predominantly focused on the logic of LLMs, neglecting the
exploration of personality aspects. In this work, we introduce Dynamic
Personality Generation (DPG), a dynamic personality generation method based on
Hypernetworks. Initially, we embed the Big Five personality theory into GPT-4
to form a personality assessment machine, enabling it to evaluate characters'
personality traits from dialogues automatically. We propose a new metric to
assess personality generation capability based on this evaluation method. Then,
we use this personality assessment machine to evaluate dialogues in script
data, resulting in a personality-dialogue dataset. Finally, we fine-tune DPG on
the personality-dialogue dataset. Experiments prove that DPG's personality
generation capability is stronger after fine-tuning on this dataset than
traditional fine-tuning methods, surpassing prompt-based GPT-4.
\\ ( https://arxiv.org/abs/2404.07084 ,  943kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07103
Date: Wed, 10 Apr 2024 15:41:53 GMT   (3256kb,D)

Title: Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on
  Graphs
Authors: Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang,
  Suhang Wang, Yu Meng, Jiawei Han
Categories: cs.CL cs.IR cs.LG
Comments: 21 pages. Code: https://github.com/PeterGriffinJin/Graph-CoT
\\
  Large language models (LLMs), while exhibiting exceptional performance,
suffer from hallucinations, especially on knowledge-intensive tasks. Existing
works propose to augment LLMs with individual text units retrieved from
external knowledge corpora to alleviate the issue. However, in many domains,
texts are interconnected (e.g., academic papers in a bibliographic graph are
linked by citations and co-authorships) which form a (text-attributed) graph.
The knowledge in such graphs is encoded not only in single texts/nodes but also
in their associated connections. To facilitate the research of augmenting LLMs
with graphs, we manually construct a Graph Reasoning Benchmark dataset called
GRBench, containing 1,740 questions that can be answered with the knowledge
from 10 domain graphs. Then, we propose a simple and effective framework called
Graph Chain-of-thought (Graph-CoT) to augment LLMs with graphs by encouraging
LLMs to reason on the graph iteratively. Each Graph-CoT iteration consists of
three sub-steps: LLM reasoning, LLM-graph interaction, and graph execution. We
conduct systematic experiments with three LLM backbones on GRBench, where
Graph-CoT outperforms the baselines consistently. The code is available at
https://github.com/PeterGriffinJin/Graph-CoT.
\\ ( https://arxiv.org/abs/2404.07103 ,  3256kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07108
Date: Wed, 10 Apr 2024 15:46:08 GMT   (392kb,D)

Title: From Model-centered to Human-Centered: Revision Distance as a Metric for
  Text Evaluation in LLMs-based Applications
Authors: Yongqiang Ma, Lizhi Qin, Jiawei Liu, Yangyang Kang, Yue Zhang, Wei Lu,
  Xiaozhong Liu, Qikai Cheng
Categories: cs.CL cs.IR
Comments: 9 pages, 2 figures, under review
\\
  Evaluating large language models (LLMs) is fundamental, particularly in the
context of practical applications. Conventional evaluation methods, typically
designed primarily for LLM development, yield numerical scores that ignore the
user experience. Therefore, our study shifts the focus from model-centered to
human-centered evaluation in the context of AI-powered writing assistance
applications. Our proposed metric, termed ``Revision Distance,'' utilizes LLMs
to suggest revision edits that mimic the human writing process. It is
determined by counting the revision edits generated by LLMs. Benefiting from
the generated revision edit details, our metric can provide a self-explained
text evaluation result in a human-understandable manner beyond the
context-independent score. Our results show that for the easy-writing task,
``Revision Distance'' is consistent with established metrics (ROUGE,
Bert-score, and GPT-score), but offers more insightful, detailed feedback and
better distinguishes between texts. Moreover, in the context of challenging
academic writing tasks, our metric still delivers reliable evaluations where
other metrics tend to struggle. Furthermore, our metric also holds significant
potential for scenarios lacking reference texts.
\\ ( https://arxiv.org/abs/2404.07108 ,  392kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07117
Date: Wed, 10 Apr 2024 15:55:07 GMT   (8809kb,D)

Title: Continuous Language Model Interpolation for Dynamic and Controllable
  Text Generation
Authors: Sara Kangaslahti and David Alvarez-Melis
Categories: cs.CL cs.LG
Comments: 20 pages, 22 figures
\\
  As large language models (LLMs) have gained popularity for a variety of use
cases, making them adaptable and controllable has become increasingly
important, especially for user-facing applications. While the existing
literature on LLM adaptation primarily focuses on finding a model (or models)
that optimizes a single predefined objective, here we focus on the challenging
case where the model must dynamically adapt to diverse -- and often changing --
user preferences. For this, we leverage adaptation methods based on linear
weight interpolation, casting them as continuous multi-domain interpolators
that produce models with specific prescribed generation characteristics
on-the-fly. Specifically, we use low-rank updates to fine-tune a base model to
various different domains, yielding a set of anchor models with distinct
generation profiles. Then, we use the weight updates of these anchor models to
parametrize the entire (infinite) class of models contained within their convex
hull. We empirically show that varying the interpolation weights yields
predictable and consistent change in the model outputs with respect to all of
the controlled attributes. We find that there is little entanglement between
most attributes and identify and discuss the pairs of attributes for which this
is not the case. Our results suggest that linearly interpolating between the
weights of fine-tuned models facilitates predictable, fine-grained control of
model outputs with respect to multiple stylistic characteristics
simultaneously.
\\ ( https://arxiv.org/abs/2404.07117 ,  8809kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07135
Date: Wed, 10 Apr 2024 16:12:50 GMT   (1104kb,D)

Title: Towards Robustness of Text-to-Visualization Translation against Lexical
  and Phrasal Variability
Authors: Jinwei Lu, Yuanfeng Song, Haodi Zhang, Chen Zhang, Raymond Chi-Wing
  Wong
Categories: cs.CL cs.AI
\\
  Text-to-Vis is an emerging task in the natural language processing (NLP) area
that aims to automatically generate data visualizations from natural language
questions (NLQs). Despite their progress, existing text-to-vis models often
heavily rely on lexical matching between words in the questions and tokens in
data schemas. This overreliance on lexical matching may lead to a diminished
level of model robustness against input variations. In this study, we
thoroughly examine the robustness of current text-to-vis models, an area that
has not previously been explored. In particular, we construct the first
robustness dataset nvBench-Rob, which contains diverse lexical and phrasal
variations based on the original text-to-vis benchmark nvBench. Then, we found
that the performance of existing text-to-vis models on this new dataset
dramatically drops, implying that these methods exhibit inadequate robustness
overall. Finally, we propose a novel framework based on Retrieval-Augmented
Generation (RAG) technique, named GRED, specifically designed to address input
perturbations in these two variants. The framework consists of three parts:
NLQ-Retrieval Generator, Visualization Query-Retrieval Retuner and
Annotation-based Debugger, which are used to tackle the challenges posed by
natural language variants, programming style differences and data schema
variants, respectively. Extensive experimental evaluations show that, compared
to the state-of-the-art model RGVisNet in the Text-to-Vis field, RGDR performs
better in terms of model robustness, with a 32% increase in accuracy on the
proposed nvBench-Rob dataset.
\\ ( https://arxiv.org/abs/2404.07135 ,  1104kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07143
Date: Wed, 10 Apr 2024 16:18:42 GMT   (248kb,D)

Title: Leave No Context Behind: Efficient Infinite Context Transformers with
  Infini-attention
Authors: Tsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal
Categories: cs.CL cs.AI cs.LG cs.NE
Comments: 9 pages, 4 figures, 4 tables
\\
  This work introduces an efficient method to scale Transformer-based Large
Language Models (LLMs) to infinitely long inputs with bounded memory and
computation. A key component in our proposed approach is a new attention
technique dubbed Infini-attention. The Infini-attention incorporates a
compressive memory into the vanilla attention mechanism and builds in both
masked local attention and long-term linear attention mechanisms in a single
Transformer block. We demonstrate the effectiveness of our approach on
long-context language modeling benchmarks, 1M sequence length passkey context
block retrieval and 500K length book summarization tasks with 1B and 8B LLMs.
Our approach introduces minimal bounded memory parameters and enables fast
streaming inference for LLMs.
\\ ( https://arxiv.org/abs/2404.07143 ,  248kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06549
Date: Tue, 9 Apr 2024 18:02:01 GMT   (10275kb,D)

Title: Variational Stochastic Gradient Descent for Deep Neural Networks
Authors: Haotian Chen, Anna Kuzina, Babak Esmaeili, Jakub M Tomczak
Categories: cs.LG stat.ML
\\
  Optimizing deep neural networks is one of the main tasks in successful deep
learning. Current state-of-the-art optimizers are adaptive gradient-based
optimization methods such as Adam. Recently, there has been an increasing
interest in formulating gradient-based optimizers in a probabilistic framework
for better estimation of gradients and modeling uncertainties. Here, we propose
to combine both approaches, resulting in the Variational Stochastic Gradient
Descent (VSGD) optimizer. We model gradient updates as a probabilistic model
and utilize stochastic variational inference (SVI) to derive an efficient and
effective update rule. Further, we show how our VSGD method relates to other
adaptive gradient-based optimizers like Adam. Lastly, we carry out experiments
on two image classification datasets and four deep neural network
architectures, where we show that VSGD outperforms Adam and SGD.
\\ ( https://arxiv.org/abs/2404.06549 ,  10275kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06583
Date: Tue, 9 Apr 2024 19:25:16 GMT   (2266kb,D)

Title: Lecture notes on rough paths and applications to machine learning
Authors: Thomas Cass and Cristopher Salvi
Categories: cs.LG math.PR math.ST stat.TH
MSC-class: 60L10, 60L20
\\
  These notes expound the recent use of the signature transform and rough path
theory in data science and machine learning. We develop the core theory of the
signature from first principles and then survey some recent popular
applications of this approach, including signature-based kernel methods and
neural rough differential equations. The notes are based on a course given by
the two authors at Imperial College London.
\\ ( https://arxiv.org/abs/2404.06583 ,  2266kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06599
Date: Tue, 9 Apr 2024 20:06:25 GMT   (792kb,D)

Title: FMDA-OT: Federated Multi-source Domain Adaptation Through Optimal
  Transport
Authors: Omar Ghannou and Youn\`es Bennani
Categories: cs.LG cs.AI
\\
  Multi-source Domain Adaptation (MDA) aims to adapt models trained on multiple
labeled source domains to an unlabeled target domain. In this paper, we
introduce our approach as a collaborative MDA framework, which comprises two
adaptation phases. Firstly, we conduct domain adaptation for each source
individually with the target, utilizing optimal transport. Then, in the second
phase, which constitutes the final part of the framework, we design the
architecture of centralized federated learning to collaborate the N models
representing the N sources. This architecture offers the advantage of using the
sources without accessing their data, thus resolving data privacy issues
inherent in domain adaptation. Additionally, during this phase, the server
guides and fine-tunes the adaptation using a small number of pseudo-labeled
samples available in the target domain, referred to as the target validation
subset of the dataset.
\\ ( https://arxiv.org/abs/2404.06599 ,  792kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06641
Date: Tue, 9 Apr 2024 22:31:10 GMT   (2059kb)

Title: Federated learning model for predicting major postoperative
  complications
Authors: Yonggi Park, Yuanfang Ren, Benjamin Shickel, Ziyuan Guan, Ayush
  Patela, Yingbo Ma, Zhenhong Hu, Tyler J. Loftus, Parisa Rashidi, Tezcan
  Ozrazgat-Baslanti, Azra Bihorac
Categories: cs.LG cs.AI cs.CY
Comments: 57 pages. 2 figures, 3 tables, 2 supplemental figures, 8 supplemental
  tables
\\
  Background: The accurate prediction of postoperative complication risk using
Electronic Health Records (EHR) and artificial intelligence shows great
potential. Training a robust artificial intelligence model typically requires
large-scale and diverse datasets. In reality, collecting medical data often
encounters challenges surrounding privacy protection. Methods: This
retrospective cohort study includes adult patients who were admitted to UFH
Gainesville (GNV) (n = 79,850) and Jacksonville (JAX) (n = 28,636) for any type
of inpatient surgical procedure. Using perioperative and intraoperative
features, we developed federated learning models to predict nine major
postoperative complications (i.e., prolonged intensive care unit stay and
mechanical ventilation). We compared federated learning models with local
learning models trained on a single site and central learning models trained on
pooled dataset from two centers. Results: Our federated learning models
achieved the area under the receiver operating characteristics curve (AUROC)
values ranged from 0.81 for wound complications to 0.92 for prolonged ICU stay
at UFH GNV center. At UFH JAX center, these values ranged from 0.73-0.74 for
wound complications to 0.92-0.93 for hospital mortality. Federated learning
models achieved comparable AUROC performance to central learning models, except
for prolonged ICU stay, where the performance of federated learning models was
slightly higher than central learning models at UFH GNV center, but slightly
lower at UFH JAX center. In addition, our federated learning model obtained
comparable performance to the best local learning model at each center,
demonstrating strong generalizability. Conclusion: Federated learning is shown
to be a useful tool to train robust and generalizable models from large scale
data across multiple institutions where data protection barriers are high.
\\ ( https://arxiv.org/abs/2404.06641 ,  2059kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06668
Date: Wed, 10 Apr 2024 00:52:54 GMT   (342kb)

Title: Forecasting the Future with Future Technologies: Advancements in Large
  Meteorological Models
Authors: Hailong Shu, Yue Wang, Weiwei Song, Huichuang Guo, Zhen Song
Categories: cs.LG cs.AI physics.ao-ph
Comments: 5 pages
\\
  The field of meteorological forecasting has undergone a significant
transformation with the integration of large models, especially those employing
deep learning techniques. This paper reviews the advancements and applications
of these models in weather prediction, emphasizing their role in transforming
traditional forecasting methods. Models like FourCastNet, Pangu-Weather,
GraphCast, ClimaX, and FengWu have made notable contributions by providing
accurate, high-resolution forecasts, surpassing the capabilities of traditional
Numerical Weather Prediction (NWP) models. These models utilize advanced neural
network architectures, such as Convolutional Neural Networks (CNNs), Graph
Neural Networks (GNNs), and Transformers, to process diverse meteorological
data, enhancing predictive accuracy across various time scales and spatial
resolutions. The paper addresses challenges in this domain, including data
acquisition and computational demands, and explores future opportunities for
model optimization and hardware advancements. It underscores the integration of
artificial intelligence with conventional meteorological techniques, promising
improved weather prediction accuracy and a significant contribution to
addressing climate-related challenges. This synergy positions large models as
pivotal in the evolving landscape of meteorological forecasting.
\\ ( https://arxiv.org/abs/2404.06668 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06675
Date: Wed, 10 Apr 2024 01:35:17 GMT   (34kb)

Title: Toward Cross-Layer Energy Optimizations in Machine Learning Systems
Authors: Jae-Won Chung and Mosharaf Chowdhury
Categories: cs.LG cs.AR cs.DC
\\
  The enormous energy consumption of machine learning (ML) and generative AI
workloads shows no sign of waning, taking a toll on operating costs, power
delivery, and environmental sustainability. Despite a long line of research on
energy-efficient hardware, we found that software plays a critical role in ML
energy optimization through two recent works: Zeus and Perseus. This is
especially true for large language models (LLMs) because their model sizes and,
therefore, energy demands are growing faster than hardware efficiency
improvements. Therefore, we advocate for a cross-layer approach for energy
optimizations in ML systems, where hardware provides architectural support that
pushes energy-efficient software further, while software leverages and
abstracts the hardware to develop techniques that bring hardware-agnostic
energy-efficiency gains.
\\ ( https://arxiv.org/abs/2404.06675 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06676
Date: Wed, 10 Apr 2024 01:37:41 GMT   (3239kb)

Title: Topological Feature Search Method for Multichannel EEG: Application in
  ADHD classification
Authors: Tianming Cai, Guoying Zhao, Junbin Zang, Chen Zong, Zhidong Zhang,
  Chenyang Xue
Categories: cs.LG eess.SP stat.AP
\\
  In recent years, the preliminary diagnosis of Attention Deficit Hyperactivity
Disorder (ADHD) using electroencephalography (EEG) has garnered attention from
researchers. EEG, known for its expediency and efficiency, plays a pivotal role
in the diagnosis and treatment of ADHD. However, the non-stationarity of EEG
signals and inter-subject variability pose challenges to the diagnostic and
classification processes. Topological Data Analysis (TDA) offers a novel
perspective for ADHD classification, diverging from traditional time-frequency
domain features. Yet, conventional TDA models are restricted to single-channel
time series and are susceptible to noise, leading to the loss of topological
features in persistence diagrams.This paper presents an enhanced TDA approach
applicable to multi-channel EEG in ADHD. Initially, optimal input parameters
for multi-channel EEG are determined. Subsequently, each channel's EEG
undergoes phase space reconstruction (PSR) followed by the utilization of
k-Power Distance to Measure (k-PDTM) for approximating ideal point clouds.
Then, multi-dimensional time series are re-embedded, and TDA is applied to
obtain topological feature information. Gaussian function-based Multivariate
Kernel Density Estimation (MKDE) is employed in the merger persistence diagram
to filter out desired topological feature mappings. Finally, persistence image
(PI) method is utilized to extract topological features, and the influence of
various weighting functions on the results is discussed.The effectiveness of
our method is evaluated using the IEEE ADHD dataset. Results demonstrate that
the accuracy, sensitivity, and specificity reach 85.60%, 83.61%, and 88.33%,
respectively. Compared to traditional TDA methods, our method was effectively
improved and outperforms typical nonlinear descriptors. These findings indicate
that our method exhibits higher precision and robustness.
\\ ( https://arxiv.org/abs/2404.06676 ,  3239kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06694
Date: Wed, 10 Apr 2024 02:54:18 GMT   (14432kb,D)

Title: How to Craft Backdoors with Unlabeled Data Alone?
Authors: Yifei Wang, Wenhan Ma, Yisen Wang
Categories: cs.LG cs.AI cs.CR
Comments: Accepted at ICLR 2024 Workshop on Data Problems for Foundation Models
  (DPFM)
\\
  Relying only on unlabeled data, Self-supervised learning (SSL) can learn rich
features in an economical and scalable way. As the drive-horse for building
foundation models, SSL has received a lot of attention recently with wide
applications, which also raises security concerns where backdoor attack is a
major type of threat: if the released dataset is maliciously poisoned,
backdoored SSL models can behave badly when triggers are injected to test
samples. The goal of this work is to investigate this potential risk. We notice
that existing backdoors all require a considerable amount of \emph{labeled}
data that may not be available for SSL. To circumvent this limitation, we
explore a more restrictive setting called no-label backdoors, where we only
have access to the unlabeled data alone, where the key challenge is how to
select the proper poison set without using label information. We propose two
strategies for poison selection: clustering-based selection using pseudolabels,
and contrastive selection derived from the mutual information principle.
Experiments on CIFAR-10 and ImageNet-100 show that both no-label backdoors are
effective on many SSL methods and outperform random poisoning by a large
margin. Code will be available at https://github.com/PKU-ML/nlb.
\\ ( https://arxiv.org/abs/2404.06694 ,  14432kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06723
Date: Wed, 10 Apr 2024 04:19:59 GMT   (452kb,D)

Title: Global Contrastive Training for Multimodal Electronic Health Records
  with Language Supervision
Authors: Yingbo Ma, Suraj Kolla, Zhenhong Hu, Dhruv Kaliraman, Victoria Nolan,
  Ziyuan Guan, Yuanfang Ren, Brooke Armfield, Tezcan Ozrazgat-Baslanti, Jeremy
  A. Balch, Tyler J. Loftus, Parisa Rashidi, Azra Bihorac, Benjamin Shickel
Categories: cs.LG cs.CL
Comments: 12 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:2403.04012
\\
  Modern electronic health records (EHRs) hold immense promise in tracking
personalized patient health trajectories through sequential deep learning,
owing to their extensive breadth, scale, and temporal granularity. Nonetheless,
how to effectively leverage multiple modalities from EHRs poses significant
challenges, given its complex characteristics such as high dimensionality,
multimodality, sparsity, varied recording frequencies, and temporal
irregularities. To this end, this paper introduces a novel multimodal
contrastive learning framework, specifically focusing on medical time series
and clinical notes. To tackle the challenge of sparsity and irregular time
intervals in medical time series, the framework integrates temporal
cross-attention transformers with a dynamic embedding and tokenization scheme
for learning multimodal feature representations. To harness the interconnected
relationships between medical time series and clinical notes, the framework
equips a global contrastive loss, aligning a patient's multimodal feature
representations with the corresponding discharge summaries. Since discharge
summaries uniquely pertain to individual patients and represent a holistic view
of the patient's hospital stay, machine learning models are led to learn
discriminative multimodal features via global contrasting. Extensive
experiments with a real-world EHR dataset demonstrated that our framework
outperformed state-of-the-art approaches on the exemplar task of predicting the
occurrence of nine postoperative complications for more than 120,000 major
inpatient surgeries using multimodal data from UF health system split among
three hospitals (UF Health Gainesville, UF Health Jacksonville, and UF Health
Jacksonville-North).
\\ ( https://arxiv.org/abs/2404.06723 ,  452kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06737
Date: Wed, 10 Apr 2024 04:55:57 GMT   (28577kb,D)

Title: Disguised Copyright Infringement of Latent Diffusion Model
Authors: Yiwei Lu, Matthew Y.R. Yang, Zuoqiu Liu, Gautam Kamath, Yaoliang Yu
Categories: cs.LG cs.CR
\\
  Copyright infringement may occur when a generative model produces samples
substantially similar to some copyrighted data that it had access to during the
training phase. The notion of access usually refers to including copyrighted
samples directly in the training dataset, which one may inspect to identify an
infringement. We argue that such visual auditing largely overlooks a concealed
copyright infringement, where one constructs a disguise that looks drastically
different from the copyrighted sample yet still induces the effect of training
Latent Diffusion Models on it. Such disguises only require indirect access to
the copyrighted material and cannot be visually distinguished, thus easily
circumventing the current auditing tools. In this paper, we provide a better
understanding of such disguised copyright infringement by uncovering the
disguises generation algorithm, the revelation of the disguises, and
importantly, how to detect them to augment the existing toolbox. Additionally,
we introduce a broader notion of acknowledgment for comprehending such indirect
access.
\\ ( https://arxiv.org/abs/2404.06737 ,  28577kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06749
Date: Wed, 10 Apr 2024 05:32:03 GMT   (38194kb,D)

Title: CGNSDE: Conditional Gaussian Neural Stochastic Differential Equation for
  Modeling Complex Systems and Data Assimilation
Authors: Chuanqi Chen, Nan Chen, Jin-Long Wu
Categories: cs.LG
\\
  A new knowledge-based and machine learning hybrid modeling approach, called
conditional Gaussian neural stochastic differential equation (CGNSDE), is
developed to facilitate modeling complex dynamical systems and implementing
analytic formulae of the associated data assimilation (DA). In contrast to the
standard neural network predictive models, the CGNSDE is designed to
effectively tackle both forward prediction tasks and inverse state estimation
problems. The CGNSDE starts by exploiting a systematic causal inference via
information theory to build a simple knowledge-based nonlinear model that
nevertheless captures as much explainable physics as possible. Then, neural
networks are supplemented to the knowledge-based model in a specific way, which
not only characterizes the remaining features that are challenging to model
with simple forms but also advances the use of analytic formulae to efficiently
compute the nonlinear DA solution. These analytic formulae are used as an
additional computationally affordable loss to train the neural networks that
directly improve the DA accuracy. This DA loss function promotes the CGNSDE to
capture the interactions between state variables and thus advances its modeling
skills. With the DA loss, the CGNSDE is more capable of estimating extreme
events and quantifying the associated uncertainty. Furthermore, crucial
physical properties in many complex systems, such as the translate-invariant
local dependence of state variables, can significantly simplify the neural
network structures and facilitate the CGNSDE to be applied to high-dimensional
systems. Numerical experiments based on chaotic systems with intermittency and
strong non-Gaussian features indicate that the CGNSDE outperforms
knowledge-based regression models, and the DA loss further enhances the
modeling skills of the CGNSDE.
\\ ( https://arxiv.org/abs/2404.06749 ,  38194kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06756
Date: Wed, 10 Apr 2024 05:44:28 GMT   (482kb,D)

Title: CrimeAlarm: Towards Intensive Intent Dynamics in Fine-grained Crime
  Prediction
Authors: Kaixi Hu, Lin Li, Qing Xie, Xiaohui Tao and Guandong Xu
Categories: cs.LG cs.AI
Comments: Accepted by DASFAA 2024
\\
  Granularity and accuracy are two crucial factors for crime event prediction.
Within fine-grained event classification, multiple criminal intents may
alternately exhibit in preceding sequential events, and progress differently in
next. Such intensive intent dynamics makes training models hard to capture
unobserved intents, and thus leads to sub-optimal generalization performance,
especially in the intertwining of numerous potential events. To capture
comprehensive criminal intents, this paper proposes a fine-grained sequential
crime prediction framework, CrimeAlarm, that equips with a novel mutual
distillation strategy inspired by curriculum learning. During the early
training phase, spot-shared criminal intents are captured through
high-confidence sequence samples. In the later phase, spot-specific intents are
gradually learned by increasing the contribution of low-confidence sequences.
Meanwhile, the output probability distributions are reciprocally learned
between prediction networks to model unobserved criminal intents. Extensive
experiments show that CrimeAlarm outperforms state-of-the-art methods in terms
of NDCG@5, with improvements of 4.51% for the NYC16 and 7.73% for the CHI18 in
accuracy measures.
\\ ( https://arxiv.org/abs/2404.06756 ,  482kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06776
Date: Wed, 10 Apr 2024 06:35:25 GMT   (402kb,D)

Title: Logit Calibration and Feature Contrast for Robust Federated Learning on
  Non-IID Data
Authors: Yu Qiao, Chaoning Zhang, Apurba Adhikary, Choong Seon Hong
Categories: cs.LG cs.AI cs.CV
\\
  Federated learning (FL) is a privacy-preserving distributed framework for
collaborative model training on devices in edge networks. However, challenges
arise due to vulnerability to adversarial examples (AEs) and the
non-independent and identically distributed (non-IID) nature of data
distribution among devices, hindering the deployment of adversarially robust
and accurate learning models at the edge. While adversarial training (AT) is
commonly acknowledged as an effective defense strategy against adversarial
attacks in centralized training, we shed light on the adverse effects of
directly applying AT in FL that can severely compromise accuracy, especially in
non-IID challenges. Given this limitation, this paper proposes FatCC, which
incorporates local logit \underline{C}alibration and global feature
\underline{C}ontrast into the vanilla federated adversarial training
(\underline{FAT}) process from both logit and feature perspectives. This
approach can effectively enhance the federated system's robust accuracy (RA)
and clean accuracy (CA). First, we propose logit calibration, where the logits
are calibrated during local adversarial updates, thereby improving adversarial
robustness. Second, FatCC introduces feature contrast, which involves a global
alignment term that aligns each local representation with unbiased global
features, thus further enhancing robustness and accuracy in federated
adversarial environments. Extensive experiments across multiple datasets
demonstrate that FatCC achieves comparable or superior performance gains in
both CA and RA compared to other baselines.
\\ ( https://arxiv.org/abs/2404.06776 ,  402kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06787
Date: Wed, 10 Apr 2024 06:58:58 GMT   (6533kb,D)

Title: Private Wasserstein Distance with Random Noises
Authors: Wenqian Li, Haozhi Wang, Zhe Huang, Yan Pang
Categories: cs.LG cs.AI
\\
  Wasserstein distance is a principle measure of data divergence from a
distributional standpoint. However, its application becomes challenging in the
context of data privacy, where sharing raw data is restricted. Prior attempts
have employed techniques like Differential Privacy or Federated optimization to
approximate Wasserstein distance. Nevertheless, these approaches often lack
accuracy and robustness against potential attack. In this study, we investigate
the underlying triangular properties within the Wasserstein space, leading to a
straightforward solution named TriangleWad. This approach enables the
computation of Wasserstein distance between datasets stored across different
entities. Notably, TriangleWad is 20 times faster, making raw data information
truly invisible, enhancing resilience against attacks, and without sacrificing
estimation accuracy. Through comprehensive experimentation across various tasks
involving both image and text data, we demonstrate its superior performance and
generalizations.
\\ ( https://arxiv.org/abs/2404.06787 ,  6533kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06795
Date: Wed, 10 Apr 2024 07:34:37 GMT   (3292kb,D)

Title: Extracting Clean and Balanced Subset for Noisy Long-tailed
  Classification
Authors: Zhuo Li, He Zhao, Zhen Li, Tongliang Liu, Dandan Guo and Xiang Wan
Categories: cs.LG
\\
  Real-world datasets usually are class-imbalanced and corrupted by label
noise. To solve the joint issue of long-tailed distribution and label noise,
most previous works usually aim to design a noise detector to distinguish the
noisy and clean samples. Despite their effectiveness, they may be limited in
handling the joint issue effectively in a unified way. In this work, we develop
a novel pseudo labeling method using class prototypes from the perspective of
distribution matching, which can be solved with optimal transport (OT). By
setting a manually-specific probability measure and using a learned transport
plan to pseudo-label the training samples, the proposed method can reduce the
side-effects of noisy and long-tailed data simultaneously. Then we introduce a
simple yet effective filter criteria by combining the observed labels and
pseudo labels to obtain a more balanced and less noisy subset for a robust
model training. Extensive experiments demonstrate that our method can extract
this class-balanced subset with clean labels, which brings effective
performance gains for long-tailed classification with label noise.
\\ ( https://arxiv.org/abs/2404.06795 ,  3292kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06808
Date: Wed, 10 Apr 2024 07:55:10 GMT   (1482kb,D)

Title: Formation-Controlled Dimensionality Reduction
Authors: Taeuk Jeong, Yoon Mo Jung
Categories: cs.LG
\\
  Dimensionality reduction represents the process of generating a low
dimensional representation of high dimensional data. Motivated by the formation
control of mobile agents, we propose a nonlinear dynamical system for
dimensionality reduction. The system consists of two parts; the control of
neighbor points, addressing local structures, and the control of remote points,
accounting for global structures. We also include a brief mathematical
observation of the model and its numerical procedure. Numerical experiments are
performed on both synthetic and real datasets and comparisons with existing
models demonstrate the soundness and effectiveness of the proposed model.
\\ ( https://arxiv.org/abs/2404.06808 ,  1482kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06824
Date: Wed, 10 Apr 2024 08:23:05 GMT   (3047kb,D)

Title: Error Mitigation for TDoA UWB Indoor Localization using Unsupervised
  Machine Learning
Authors: Phuong Bich Duong, Ben Van Herbruggen, Arne Broering, Adnan Shahid and
  Eli De Poorter
Categories: cs.LG
Comments: 5 pages, 3 figures, 3 tables, 14 references
ACM-class: I.2.1
\\
  Indoor positioning systems based on Ultra-wideband (UWB) technology are
gaining recognition for their ability to provide cm-level localization
accuracy. However, these systems often encounter challenges caused by dense
multi-path fading, leading to positioning errors. To address this issue, in
this letter, we propose a novel methodology for unsupervised anchor node
selection using deep embedded clustering (DEC). Our approach uses an Auto
Encoder (AE) before clustering, thereby better separating UWB features into
separable clusters of UWB input signals. We furthermore investigate how to rank
these clusters based on their cluster quality, allowing us to remove
untrustworthy signals. Experimental results show the efficiency of our proposed
method, demonstrating a significant 23.1% reduction in mean absolute error
(MAE) compared to without anchor exclusion. Especially in the dense multi-path
area, our algorithm achieves even more significant enhancements, reducing the
MAE by 26.6% and the 95th percentile error by 49.3% compared to without anchor
exclusion.
\\ ( https://arxiv.org/abs/2404.06824 ,  3047kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06831
Date: Wed, 10 Apr 2024 08:47:57 GMT   (510kb,D)

Title: Optimal Regret with Limited Adaptivity for Generalized Linear Contextual
  Bandits
Authors: Ayush Sawarni, Nirjhar Das, Gaurav Sinha, Siddharth Barman
Categories: cs.LG
Comments: 31 pages
\\
  We study the generalized linear contextual bandit problem within the
requirements of limited adaptivity. In this paper, we present two algorithms,
\texttt{B-GLinCB} and \texttt{RS-GLinCB}, that address, respectively, two
prevalent limited adaptivity models: batch learning with stochastic contexts
and rare policy switches with adversarial contexts. For both these models, we
establish essentially tight regret bounds. Notably, in the obtained bounds, we
manage to eliminate a dependence on a key parameter $\kappa$, which captures
the non-linearity of the underlying reward model. For our batch learning
algorithm \texttt{B-GLinCB}, with $\Omega\left( \log{\log T} \right)$ batches,
the regret scales as $\tilde{O}(\sqrt{T})$. Further, we establish that our
rarely switching algorithm \texttt{RS-GLinCB} updates its policy at most
$\tilde{O}(\log^2 T)$ times and achieves a regret of $\tilde{O}(\sqrt{T})$. Our
approach for removing the dependence on $\kappa$ for generalized linear
contextual bandits might be of independent interest.
\\ ( https://arxiv.org/abs/2404.06831 ,  510kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06846
Date: Wed, 10 Apr 2024 09:17:22 GMT   (354kb,D)

Title: Register Your Forests: Decision Tree Ensemble Optimization by Explicit
  CPU Register Allocation
Authors: Daniel Biebert, Christian Hakert, Kuan-Hsun Chen, Jian-Jia Chen
Categories: cs.LG
\\
  Bringing high-level machine learning models to efficient and well-suited
machine implementations often invokes a bunch of tools, e.g.~code generators,
compilers, and optimizers. Along such tool chains, abstractions have to be
applied. This leads to not optimally used CPU registers. This is a shortcoming,
especially in resource constrained embedded setups. In this work, we present a
code generation approach for decision tree ensembles, which produces machine
assembly code within a single conversion step directly from the high-level
model representation. Specifically, we develop various approaches to
effectively allocate registers for the inference of decision tree ensembles.
Extensive evaluations of the proposed method are conducted in comparison to the
basic realization of C code from the high-level machine learning model and
succeeding compilation. The results show that the performance of decision tree
ensemble inference can be significantly improved (by up to $\approx1.6\times$),
if the methods are applied carefully to the appropriate scenario.
\\ ( https://arxiv.org/abs/2404.06846 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06869
Date: Wed, 10 Apr 2024 09:47:34 GMT   (1899kb,D)

Title: SleepPPG-Net2: Deep learning generalization for sleep staging from
  photoplethysmography
Authors: Shirel Attia, Revital Shani Hershkovich, Alissa Tabakhov, Angeleene
  Ang, Sharon Haimov, Riva Tauman, Joachim A. Behar
Categories: cs.LG cs.AI eess.SP
\\
  Background: Sleep staging is a fundamental component in the diagnosis of
sleep disorders and the management of sleep health. Traditionally, this
analysis is conducted in clinical settings and involves a time-consuming
scoring procedure. Recent data-driven algorithms for sleep staging, using the
photoplethysmogram (PPG) time series, have shown high performance on local test
sets but lower performance on external datasets due to data drift. Methods:
This study aimed to develop a generalizable deep learning model for the task of
four class (wake, light, deep, and rapid eye movement (REM)) sleep staging from
raw PPG physiological time-series. Six sleep datasets, totaling 2,574 patients
recordings, were used. In order to create a more generalizable representation,
we developed and evaluated a deep learning model called SleepPPG-Net2, which
employs a multi-source domain training approach.SleepPPG-Net2 was benchmarked
against two state-of-the-art models. Results: SleepPPG-Net2 showed consistently
higher performance over benchmark approaches, with generalization performance
(Cohen's kappa) improving by up to 19%. Performance disparities were observed
in relation to age, sex, and sleep apnea severity. Conclusion: SleepPPG-Net2
sets a new standard for staging sleep from raw PPG time-series.
\\ ( https://arxiv.org/abs/2404.06869 ,  1899kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06962
Date: Wed, 10 Apr 2024 12:22:03 GMT   (30076kb,D)

Title: Advancing Real-time Pandemic Forecasting Using Large Language Models: A
  COVID-19 Case Study
Authors: Hongru Du, Jianan Zhao, Yang Zhao, Shaochong Xu, Xihong Lin, Yiran
  Chen, Lauren M. Gardner, Hao (Frank) Yang
Categories: cs.LG cs.AI
Comments: 35 pages, 10 figures
\\
  Forecasting the short-term spread of an ongoing disease outbreak is a
formidable challenge due to the complexity of contributing factors, some of
which can be characterized through interlinked, multi-modality variables such
as epidemiological time series data, viral biology, population demographics,
and the intersection of public policy and human behavior. Existing forecasting
model frameworks struggle with the multifaceted nature of relevant data and
robust results translation, which hinders their performances and the provision
of actionable insights for public health decision-makers. Our work introduces
PandemicLLM, a novel framework with multi-modal Large Language Models (LLMs)
that reformulates real-time forecasting of disease spread as a text reasoning
problem, with the ability to incorporate real-time, complex, non-numerical
information that previously unattainable in traditional forecasting models.
This approach, through a unique AI-human cooperative prompt design and time
series representation learning, encodes multi-modal data for LLMs. The model is
applied to the COVID-19 pandemic, and trained to utilize textual public health
policies, genomic surveillance, spatial, and epidemiological time series data,
and is subsequently tested across all 50 states of the U.S. Empirically,
PandemicLLM is shown to be a high-performing pandemic forecasting framework
that effectively captures the impact of emerging variants and can provide
timely and accurate predictions. The proposed PandemicLLM opens avenues for
incorporating various pandemic-related data in heterogeneous formats and
exhibits performance benefits over existing models. This study illuminates the
potential of adapting LLMs and representation learning to enhance pandemic
forecasting, illustrating how AI innovations can strengthen pandemic responses
and crisis management in the future.
\\ ( https://arxiv.org/abs/2404.06962 ,  30076kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06966
Date: Wed, 10 Apr 2024 12:24:05 GMT   (201kb,D)

Title: Are EEG Sequences Time Series? EEG Classification with Time Series
  Models and Joint Subject Training
Authors: Johannes Burchert, Thorben Werner, Vijaya Krishna Yalavarthi, Diego
  Coello de Portugal, Maximilian Stubbemann, and Lars Schmidt-Thieme
Categories: cs.LG eess.SP
\\
  As with most other data domains, EEG data analysis relies on rich
domain-specific preprocessing. Beyond such preprocessing, machine learners
would hope to deal with such data as with any other time series data. For EEG
classification many models have been developed with layer types and
architectures we typically do not see in time series classification.
Furthermore, typically separate models for each individual subject are learned,
not one model for all of them. In this paper, we systematically study the
differences between EEG classification models and generic time series
classification models. We describe three different model setups to deal with
EEG data from different subjects, subject-specific models (most EEG
literature), subject-agnostic models and subject-conditional models. In
experiments on three datasets, we demonstrate that off-the-shelf time series
classification models trained per subject perform close to EEG classification
models, but that do not quite reach the performance of domain-specific
modeling. Additionally, we combine time-series models with subject embeddings
to train one joint subject-conditional classifier on all subjects. The
resulting models are competitive with dedicated EEG models in 2 out of 3
datasets, even outperforming all EEG methods on one of them.
\\ ( https://arxiv.org/abs/2404.06966 ,  201kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06969
Date: Wed, 10 Apr 2024 12:29:05 GMT   (1203kb,D)

Title: FiP: a Fixed-Point Approach for Causal Generative Modeling
Authors: Meyer Scetbon, Joel Jennings, Agrin Hilmkil, Cheng Zhang, Chao Ma
Categories: cs.LG stat.ML
\\
  Modeling true world data-generating processes lies at the heart of empirical
science. Structural Causal Models (SCMs) and their associated Directed Acyclic
Graphs (DAGs) provide an increasingly popular answer to such problems by
defining the causal generative process that transforms random noise into
observations. However, learning them from observational data poses an ill-posed
and NP-hard inverse problem in general. In this work, we propose a new and
equivalent formalism that do not require DAGs to describe them, viewed as
fixed-point problems on the causally ordered variables, and show three
important cases where they can be uniquely recovered given the topological
ordering (TO). To the best of our knowledge, we obtain the most general
recovery results when the TO is known. Based on our theoretical findings, we
design a two-stage causal generative model that first infers the causal order
from observations in a zero-shot manner, thus by-passing the search, and then
learns the generative fixed-point SCM on the ordered variables. To infer TOs
from observations, we propose to amortize the learning of TOs on generated
datasets by sequentially predicting the leaves of graphs seen during training.
To learn fixed-point SCMs, we design a transformer-based architecture that
exploits a new attention mechanism enabling the modeling of causal structures,
and show that this parameterization is consistent with our formalism. Finally,
we conduct an extensive evaluation of each method individually, and show that
when combined, our model outperforms various baselines on generated
out-of-distribution problems.
\\ ( https://arxiv.org/abs/2404.06969 ,  1203kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06972
Date: Wed, 10 Apr 2024 12:32:18 GMT   (4656kb,D)

Title: Toward industrial use of continual learning : new metrics proposal for
  class incremental learning
Authors: Konat\'e Mohamed Abbas and Anne-Fran\c{c}oise Yao and Thierry Chateau
  and Pierre Bouges
Categories: cs.LG cs.AI
Comments: 7 pages, Accepted at IJCNN 2023
DOI: 10.1109/IJCNN54540.2023.10191657
\\
  In this paper, we investigate continual learning performance metrics used in
class incremental learning strategies for continual learning (CL) using some
high performing methods. We investigate especially mean task accuracy. First,
we show that it lacks of expressiveness through some simple experiments to
capture performance. We show that monitoring average tasks performance is over
optimistic and can lead to misleading conclusions for future real life
industrial uses. Then, we propose first a simple metric, Minimal Incremental
Class Accuracy (MICA) which gives a fair and more useful evaluation of
different continual learning methods. Moreover, in order to provide a simple
way to easily compare different methods performance in continual learning, we
derive another single scalar metric that take into account the learning
performance variation as well as our newly introduced metric.
\\ ( https://arxiv.org/abs/2404.06972 ,  4656kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06989
Date: Wed, 10 Apr 2024 13:08:07 GMT   (1590kb)

Title: On Fixing the Right Problems in Predictive Analytics: AUC Is Not the
  Problem
Authors: Ryan S. Baker, Nigel Bosch, Stephen Hutt, Andres F. Zambrano, Alex J.
  Bowers
Categories: cs.LG
\\
  Recently, ACM FAccT published an article by Kwegyir-Aggrey and colleagues
(2023), critiquing the use of AUC ROC in predictive analytics in several
domains. In this article, we offer a critique of that article. Specifically, we
highlight technical inaccuracies in that paper's comparison of metrics,
mis-specification of the interpretation and goals of AUC ROC, the article's use
of the accuracy metric as a gold standard for comparison to AUC ROC, and the
article's application of critiques solely to AUC ROC for concerns that would
apply to the use of any metric. We conclude with a re-framing of the very valid
concerns raised in that article, and discuss how the use of AUC ROC can remain
a valid and appropriate practice in a well-informed predictive analytics
approach taking those concerns into account. We conclude by discussing the
combined use of multiple metrics, including machine learning bias metrics, and
AUC ROC's place in such an approach. Like broccoli, AUC ROC is healthy, but
also like broccoli, researchers and practitioners in our field shouldn't eat a
diet of only AUC ROC.
\\ ( https://arxiv.org/abs/2404.06989 ,  1590kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07008
Date: Wed, 10 Apr 2024 13:47:22 GMT   (7549kb,D)

Title: Knowledge graphs for empirical concept retrieval
Authors: Lenka T\v{e}tkov\'a, Teresa Karen Scheidt, Maria Mandrup Fogh, Ellen
  Marie Gaunby J{\o}rgensen, Finn {\AA}rup Nielsen and Lars Kai Hansen
Categories: cs.LG cs.AI
Comments: Preprint. Accepted to The 2nd World Conference on eXplainable
  Artificial Intelligence
\\
  Concept-based explainable AI is promising as a tool to improve the
understanding of complex models at the premises of a given user, viz.\ as a
tool for personalized explainability. An important class of concept-based
explainability methods is constructed with empirically defined concepts,
indirectly defined through a set of positive and negative examples, as in the
TCAV approach (Kim et al., 2018). While it is appealing to the user to avoid
formal definitions of concepts and their operationalization, it can be
challenging to establish relevant concept datasets. Here, we address this
challenge using general knowledge graphs (such as, e.g., Wikidata or WordNet)
for comprehensive concept definition and present a workflow for user-driven
data collection in both text and image domains. The concepts derived from
knowledge graphs are defined interactively, providing an opportunity for
personalization and ensuring that the concepts reflect the user's intentions.
We test the retrieved concept datasets on two concept-based explainability
methods, namely concept activation vectors (CAVs) and concept activation
regions (CARs) (Crabbe and van der Schaar, 2022). We show that CAVs and CARs
based on these empirical concept datasets provide robust and accurate
explanations. Importantly, we also find good alignment between the models'
representations of concepts and the structure of knowledge graphs, i.e., human
representations. This supports our conclusion that knowledge graph-based
concepts are relevant for XAI.
\\ ( https://arxiv.org/abs/2404.07008 ,  7549kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07046
Date: Wed, 10 Apr 2024 14:36:35 GMT   (196kb)

Title: Comparison of decision trees with Local Interpretable Model-Agnostic
  Explanations (LIME) technique and multi-linear regression for explaining
  support vector regression model in terms of root mean square error (RMSE)
  values
Authors: Amit Thombre
Categories: cs.LG cs.AI
\\
  In this work the decision trees are used for explanation of support vector
regression model. The decision trees act as a global technique as well as a
local technique. They are compared against the popular technique of LIME which
is a local explanatory technique and with multi linear regression. It is
observed that decision trees give a lower RMSE value when fitted to support
vector regression as compared to LIME in 87% of the runs over 5 datasets. The
comparison of results is statistically significant. Multi linear regression
also gives a lower RMSE value when fitted to support vector regression model as
compared to LIME in 73% of the runs over 5 datasets but the comparison of
results is not statistically significant. Also, when used as a local
explanatory technique, decision trees give better performance than LIME and the
comparison of results is statistically significant.
\\ ( https://arxiv.org/abs/2404.07046 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07049
Date: Wed, 10 Apr 2024 14:38:58 GMT   (536kb,D)

Title: Towards Learning Stochastic Population Models by Gradient Descent
Authors: Justin N. Kreikemeyer, Philipp Andelfinger, Adelinde M. Uhrmacher
Categories: cs.LG
Comments: 5 pages, 2 figures, to appear in Proceedings of the ACM
  SIGSIM-PADS'24
\\
  Increasing effort is put into the development of methods for learning
mechanistic models from data. This task entails not only the accurate
estimation of parameters, but also a suitable model structure. Recent work on
the discovery of dynamical systems formulates this problem as a linear equation
system. Here, we explore several simulation-based optimization approaches,
which allow much greater freedom in the objective formulation and weaker
conditions on the available data. We show that even for relatively small
stochastic population models, simultaneous estimation of parameters and
structure poses major challenges for optimization procedures. Particularly, we
investigate the application of the local stochastic gradient descent method,
commonly used for training machine learning models. We demonstrate accurate
estimation of models but find that enforcing the inference of parsimonious,
interpretable models drastically increases the difficulty. We give an outlook
on how this challenge can be overcome.
\\ ( https://arxiv.org/abs/2404.07049 ,  536kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07083
Date: Wed, 10 Apr 2024 15:16:04 GMT   (113kb,D)

Title: Minimizing Chebyshev Prototype Risk Magically Mitigates the Perils of
  Overfitting
Authors: Nathaniel Dean, Dilip Sarkar
Categories: cs.LG
Comments: 17 pages, 2 figures
ACM-class: I.5.1
\\
  Overparameterized deep neural networks (DNNs), if not sufficiently
regularized, are susceptible to overfitting their training examples and not
generalizing well to test data. To discourage overfitting, researchers have
developed multicomponent loss functions that reduce intra-class feature
correlation and maximize inter-class feature distance in one or more layers of
the network. By analyzing the penultimate feature layer activations output by a
DNN's feature extraction section prior to the linear classifier, we find that
modified forms of the intra-class feature covariance and inter-class prototype
separation are key components of a fundamental Chebyshev upper bound on the
probability of misclassification, which we designate the Chebyshev Prototype
Risk (CPR). While previous approaches' covariance loss terms scale
quadratically with the number of network features, our CPR bound indicates that
an approximate covariance loss in log-linear time is sufficient to reduce the
bound and is scalable to large architectures. We implement the terms of the CPR
bound into our Explicit CPR (exCPR) loss function and observe from empirical
results on multiple datasets and network architectures that our training
algorithm reduces overfitting and improves upon previous approaches in many
settings. Our code is available
$\href{https://github.com/Deano1718/Regularization_exCPR}{here}$.
\\ ( https://arxiv.org/abs/2404.07083 ,  113kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07091
Date: Wed, 10 Apr 2024 15:29:29 GMT   (3880kb,D)

Title: LaTiM: Longitudinal representation learning in continuous-time models to
  predict disease progression
Authors: Rachid Zeghlache, Pierre-Henri Conze, Mostafa El Habib Daho, Yihao Li,
  Hugo Le Boit\'e, Ramin Tadayoni, Pascal Massin, B\'eatrice Cochener, Alireza
  Rezaei, Ikram Brahim, Gwenol\'e Quellec, Mathieu Lamard
Categories: cs.LG cs.AI
Comments: Submitted to MICCAI 2024
\\
  This work proposes a novel framework for analyzing disease progression using
time-aware neural ordinary differential equations (NODE). We introduce a
"time-aware head" in a framework trained through self-supervised learning (SSL)
to leverage temporal information in latent space for data augmentation. This
approach effectively integrates NODEs with SSL, offering significant
performance improvements compared to traditional methods that lack explicit
temporal integration. We demonstrate the effectiveness of our strategy for
diabetic retinopathy progression prediction using the OPHDIAT database.
Compared to the baseline, all NODE architectures achieve statistically
significant improvements in area under the ROC curve (AUC) and Kappa metrics,
highlighting the efficacy of pre-training with SSL-inspired approaches.
Additionally, our framework promotes stable training for NODEs, a commonly
encountered challenge in time-aware modeling.
\\ ( https://arxiv.org/abs/2404.07091 ,  3880kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07099
Date: Wed, 10 Apr 2024 15:39:49 GMT   (2238kb,D)

Title: Rethinking Out-of-Distribution Detection for Reinforcement Learning:
  Advancing Methods for Evaluation and Detection
Authors: Linas Nasvytis, Kai Sandbrink, Jakob Foerster, Tim Franzmeyer,
  Christian Schroeder de Witt
Categories: cs.LG cs.AI
Comments: Accepted as a full paper to the 23rd International Conference on
  Autonomous Agents and Multiagent Systems (AAMAS 2024)
\\
  While reinforcement learning (RL) algorithms have been successfully applied
across numerous sequential decision-making problems, their generalization to
unforeseen testing environments remains a significant concern. In this paper,
we study the problem of out-of-distribution (OOD) detection in RL, which
focuses on identifying situations at test time that RL agents have not
encountered in their training environments. We first propose a clarification of
terminology for OOD detection in RL, which aligns it with the literature from
other machine learning domains. We then present new benchmark scenarios for OOD
detection, which introduce anomalies with temporal autocorrelation into
different components of the agent-environment loop. We argue that such
scenarios have been understudied in the current literature, despite their
relevance to real-world situations. Confirming our theoretical predictions, our
experimental results suggest that state-of-the-art OOD detectors are not able
to identify such anomalies. To address this problem, we propose a novel method
for OOD detection, which we call DEXTER (Detection via Extraction of Time
Series Representations). By treating environment observations as time series
data, DEXTER extracts salient time series features, and then leverages an
ensemble of isolation forest algorithms to detect anomalies. We find that
DEXTER can reliably identify anomalies across benchmark scenarios, exhibiting
superior performance compared to both state-of-the-art OOD detectors and
high-dimensional changepoint detectors adopted from statistics.
\\ ( https://arxiv.org/abs/2404.07099 ,  2238kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07129
Date: Wed, 10 Apr 2024 16:07:38 GMT   (1682kb,D)

Title: What needs to go right for an induction head? A mechanistic study of
  in-context learning circuits and their formation
Authors: Aaditya K. Singh, Ted Moskovitz, Felix Hill, Stephanie C.Y. Chan,
  Andrew M. Saxe
Categories: cs.LG
Comments: 26 pages, 18 figures
\\
  In-context learning is a powerful emergent ability in transformer models.
Prior work in mechanistic interpretability has identified a circuit element
that may be critical for in-context learning -- the induction head (IH), which
performs a match-and-copy operation. During training of large transformers on
natural language data, IHs emerge around the same time as a notable phase
change in the loss. Despite the robust evidence for IHs and this interesting
coincidence with the phase change, relatively little is known about the
diversity and emergence dynamics of IHs. Why is there more than one IH, and how
are they dependent on each other? Why do IHs appear all of a sudden, and what
are the subcircuits that enable them to emerge? We answer these questions by
studying IH emergence dynamics in a controlled setting by training on synthetic
data. In doing so, we develop and share a novel optogenetics-inspired causal
framework for modifying activations throughout training. Using this framework,
we delineate the diverse and additive nature of IHs. By clamping subsets of
activations throughout training, we then identify three underlying subcircuits
that interact to drive IH formation, yielding the phase change. Furthermore,
these subcircuits shed light on data-dependent properties of formation, such as
phase change timing, already showing the promise of this more in-depth
understanding of subcircuits that need to "go right" for an induction head.
\\ ( https://arxiv.org/abs/2404.07129 ,  1682kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07148
Date: Wed, 10 Apr 2024 16:29:21 GMT   (547kb,D)

Title: How Consistent are Clinicians? Evaluating the Predictability of Sepsis
  Disease Progression with Dynamics Models
Authors: Unnseo Park, Venkatesh Sivaraman, Adam Perer
Categories: cs.LG cs.HC
Comments: 6 pages, 3 figures; accepted workshop paper at Time Series for Health
  @ ICLR 2024
\\
  Reinforcement learning (RL) is a promising approach to generate treatment
policies for sepsis patients in intensive care. While retrospective evaluation
metrics show decreased mortality when these policies are followed, studies with
clinicians suggest their recommendations are often spurious. We propose that
these shortcomings may be due to lack of diversity in observed actions and
outcomes in the training data, and we construct experiments to investigate the
feasibility of predicting sepsis disease severity changes due to clinician
actions. Preliminary results suggest incorporating action information does not
significantly improve model performance, indicating that clinician actions may
not be sufficiently variable to yield measurable effects on disease
progression. We discuss the implications of these findings for optimizing
sepsis treatment.
\\ ( https://arxiv.org/abs/2404.07148 ,  547kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07172
Date: Wed, 10 Apr 2024 17:08:46 GMT   (5659kb,D)

Title: A Gauss-Newton Approach for Min-Max Optimization in Generative
  Adversarial Networks
Authors: Neel Mishra, Bamdev Mishra, Pratik Jawanpuria, Pawan Kumar
Categories: cs.LG cs.NA math.NA math.OC
Comments: accepted in IJCNN 2023, 9 pages
\\
  A novel first-order method is proposed for training generative adversarial
networks (GANs). It modifies the Gauss-Newton method to approximate the min-max
Hessian and uses the Sherman-Morrison inversion formula to calculate the
inverse. The method corresponds to a fixed-point method that ensures necessary
contraction. To evaluate its effectiveness, numerical experiments are conducted
on various datasets commonly used in image generation tasks, such as MNIST,
Fashion MNIST, CIFAR10, FFHQ, and LSUN. Our method is capable of generating
high-fidelity images with greater diversity across multiple datasets. It also
achieves the highest inception score for CIFAR10 among all compared methods,
including state-of-the-art second-order methods. Additionally, its execution
time is comparable to that of first-order min-max methods.
\\ ( https://arxiv.org/abs/2404.07172 ,  5659kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07177
Date: Wed, 10 Apr 2024 17:27:54 GMT   (342kb,D)

Title: Scaling Laws for Data Filtering -- Data Curation cannot be Compute
  Agnostic
Authors: Sachin Goyal, Pratyush Maini, Zachary C. Lipton, Aditi Raghunathan, J.
  Zico Kolter
Categories: cs.LG
Comments: Published at CVPR 2024
\\
  Vision-language models (VLMs) are trained for thousands of GPU hours on
carefully curated web datasets. In recent times, data curation has gained
prominence with several works developing strategies to retain 'high-quality'
subsets of 'raw' scraped data. For instance, the LAION public dataset retained
only 10% of the total crawled data. However, these strategies are typically
developed agnostic of the available compute for training. In this paper, we
first demonstrate that making filtering decisions independent of training
compute is often suboptimal: the limited high-quality data rapidly loses its
utility when repeated, eventually requiring the inclusion of 'unseen' but
'lower-quality' data. To address this quality-quantity tradeoff
($\texttt{QQT}$), we introduce neural scaling laws that account for the
non-homogeneous nature of web data, an angle ignored in existing literature.
Our scaling laws (i) characterize the $\textit{differing}$ 'utility' of various
quality subsets of web data; (ii) account for how utility diminishes for a data
point at its 'nth' repetition; and (iii) formulate the mutual interaction of
various data pools when combined, enabling the estimation of model performance
on a combination of multiple data pools without ever jointly training on them.
Our key message is that data curation $\textit{cannot}$ be agnostic of the
total compute that a model will be trained for. Our scaling laws allow us to
curate the best possible pool for achieving top performance on Datacomp at
various compute budgets, carving out a pareto-frontier for data curation. Code
is available at https://github.com/locuslab/scaling_laws_data_filtering.
\\ ( https://arxiv.org/abs/2404.07177 ,  342kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07194
Date: Wed, 10 Apr 2024 17:50:29 GMT   (10173kb,D)

Title: VN-EGNN: E(3)-Equivariant Graph Neural Networks with Virtual Nodes
  Enhance Protein Binding Site Identification
Authors: Florian Sestak, Lisa Schneckenreiter, Johannes Brandstetter, Sepp
  Hochreiter, Andreas Mayr, G\"unter Klambauer
Categories: cs.LG cs.AI q-bio.BM
\\
  Being able to identify regions within or around proteins, to which ligands
can potentially bind, is an essential step to develop new drugs. Binding site
identification methods can now profit from the availability of large amounts of
3D structures in protein structure databases or from AlphaFold predictions.
Current binding site identification methods heavily rely on graph neural
networks (GNNs), usually designed to output E(3)-equivariant predictions. Such
methods turned out to be very beneficial for physics-related tasks like binding
energy or motion trajectory prediction. However, the performance of GNNs at
binding site identification is still limited potentially due to the lack of
dedicated nodes that model hidden geometric entities, such as binding pockets.
In this work, we extend E(n)-Equivariant Graph Neural Networks (EGNNs) by
adding virtual nodes and applying an extended message passing scheme. The
virtual nodes in these graphs are dedicated quantities to learn representations
of binding sites, which leads to improved predictive performance. In our
experiments, we show that our proposed method VN-EGNN sets a new
state-of-the-art at locating binding site centers on COACH420, HOLO4K and
PDBbind2020.
\\ ( https://arxiv.org/abs/2404.07194 ,  10173kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07200
Date: Wed, 10 Apr 2024 17:58:04 GMT   (6739kb,D)

Title: Toward a Better Understanding of Fourier Neural Operators: Analysis and
  Improvement from a Spectral Perspective
Authors: Shaoxiang Qin, Fuyuan Lyu, Wenhui Peng, Dingyang Geng, Ju Wang,
  Naiping Gao, Xue Liu, Liangzhu Leon Wang
Categories: cs.LG
\\
  In solving partial differential equations (PDEs), Fourier Neural Operators
(FNOs) have exhibited notable effectiveness compared to Convolutional Neural
Networks (CNNs). This paper presents clear empirical evidence through spectral
analysis to elucidate the superiority of FNO over CNNs: FNO is significantly
more capable of learning low-frequencies. This empirical evidence also unveils
FNO's distinct low-frequency bias, which limits FNO's effectiveness in learning
high-frequency information from PDE data. To tackle this challenge, we
introduce SpecBoost, an ensemble learning framework that employs multiple FNOs
to better capture high-frequency information. Specifically, a secondary FNO is
utilized to learn the overlooked high-frequency information from the prediction
residual of the initial FNO. Experiments demonstrate that SpecBoost noticeably
enhances FNO's prediction accuracy on diverse PDE applications, achieving an up
to 71% improvement.
\\ ( https://arxiv.org/abs/2404.07200 ,  6739kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.06519 (*cross-listing*)
Date: Fri, 5 Apr 2024 22:03:35 GMT   (1387kb,D)

Title: Best Response Shaping
Authors: Milad Aghajohari, Tim Cooijmans, Juan Agustin Duque, Shunichi
  Akatsuka, Aaron Courville
Categories: cs.GT cs.AI cs.LG cs.MA
\\
  We investigate the challenge of multi-agent deep reinforcement learning in
partially competitive environments, where traditional methods struggle to
foster reciprocity-based cooperation. LOLA and POLA agents learn
reciprocity-based cooperative policies by differentiation through a few
look-ahead optimization steps of their opponent. However, there is a key
limitation in these techniques. Because they consider a few optimization steps,
a learning opponent that takes many steps to optimize its return may exploit
them. In response, we introduce a novel approach, Best Response Shaping (BRS),
which differentiates through an opponent approximating the best response,
termed the "detective." To condition the detective on the agent's policy for
complex games we propose a state-aware differentiable conditioning mechanism,
facilitated by a question answering (QA) method that extracts a representation
of the agent based on its behaviour on specific environment states. To
empirically validate our method, we showcase its enhanced performance against a
Monte Carlo Tree Search (MCTS) opponent, which serves as an approximation to
the best response in the Coin Game. This work expands the applicability of
multi-agent RL in partially competitive environments and provides a new pathway
towards achieving improved social welfare in general sum games.
\\ ( https://arxiv.org/abs/2404.06519 ,  1387kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06524 (*cross-listing*)
Date: Tue, 9 Apr 2024 03:28:00 GMT   (9770kb,D)

Title: An Enhanced Grey Wolf Optimizer with Elite Inheritance and Balance
  Search Mechanisms
Authors: Jianhua Jiang, Ziying Zhao, Weihua Li, Keqin Li
Categories: cs.NE cs.AI
Comments: 51 pages, 21 tables, 16 figures, journal
\\
  The Grey Wolf Optimizer (GWO) is recognized as a novel meta-heuristic
algorithm inspired by the social leadership hierarchy and hunting mechanism of
grey wolves. It is well-known for its simple parameter setting, fast
convergence speed, and strong optimization capability. In the original GWO,
there are two significant design flaws in its fundamental optimization
mechanisms. Problem (1): the algorithm fails to inherit from elite positions
from the last iteration when generating the next positions of the wolf
population, potentially leading to suboptimal solutions. Problem (2): the
positions of the population are updated based on the central position of the
three leading wolves (alpha, beta, delta), without a balanced mechanism between
local and global search. To tackle these problems, an enhanced Grey Wolf
Optimizer with Elite Inheritance Mechanism and Balance Search Mechanism, named
as EBGWO, is proposed to improve the effectiveness of the position updating and
the quality of the convergence solutions. The IEEE CEC 2014 benchmark functions
suite and a series of simulation tests are employed to evaluate the performance
of the proposed algorithm. The simulation tests involve a comparative study
between EBGWO, three GWO variants, GWO and two well-known meta-heuristic
algorithms. The experimental results demonstrate that the proposed EBGWO
algorithm outperforms other meta-heuristic algorithms in both accuracy and
convergence speed. Three engineering optimization problems are adopted to prove
its capability in processing real-world problems. The results indicate that the
proposed EBGWO outperforms several popular algorithms.
\\ ( https://arxiv.org/abs/2404.06524 ,  9770kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06529 (*cross-listing*)
Date: Tue, 9 Apr 2024 17:12:16 GMT   (9324kb,D)

Title: Emergent Braitenberg-style Behaviours for Navigating the ViZDoom `My Way
  Home' Labyrinth
Authors: Caleidgh Bayer and Robert J. Smith and Malcolm I. Heywood
Categories: cs.NE cs.AI
\\
  The navigation of complex labyrinths with tens of rooms under visual
partially observable state is typically addressed using recurrent deep
reinforcement learning architectures. In this work, we show that navigation can
be achieved through the emergent evolution of a simple Braitentberg-style
heuristic that structures the interaction between agent and labyrinth, i.e.
complex behaviour from simple heuristics. To do so, the approach of tangled
program graphs is assumed in which programs cooperatively coevolve to develop a
modular indexing scheme that only employs 0.8\% of the state space. We
attribute this simplicity to several biases implicit in the representation,
such as the use of pixel indexing as opposed to deploying a convolutional
kernel or image processing operators.
\\ ( https://arxiv.org/abs/2404.06529 ,  9324kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06561 (*cross-listing*)
Date: Tue, 9 Apr 2024 18:25:21 GMT   (9228kb,D)

Title: Learning Strategies For Successful Crowd Navigation
Authors: Rajshree Daulatabad, Serena Nath
Categories: cs.RO cs.AI cs.SY eess.SY
Comments: 8 pages
\\
  Teaching autonomous mobile robots to successfully navigate human crowds is a
challenging task. Not only does it require planning, but it requires
maintaining social norms which may differ from one context to another. Here we
focus on crowd navigation, using a neural network to learn specific strategies
in-situ with a robot. This allows us to take into account human behavior and
reactions toward a real robot as well as learn strategies that are specific to
various scenarios in that context. A CNN takes a top-down image of the scene as
input and outputs the next action for the robot to take in terms of speed and
angle. Here we present the method, experimental results, and quantitatively
evaluate our approach.
\\ ( https://arxiv.org/abs/2404.06561 ,  9228kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06593 (*cross-listing*)
Date: Tue, 9 Apr 2024 19:49:01 GMT   (4864kb,D)

Title: Spatially Optimized Compact Deep Metric Learning Model for Similarity
  Search
Authors: Md. Farhadul Islam, Md. Tanzim Reza, Meem Arafat Manab, Mohammad
  Rakibul Hasan Mahin, Sarah Zabeen, Jannatun Noor
Categories: cs.CV cs.AI cs.LG
Comments: 5 pages, 3 figures,
MSC-class: 68
ACM-class: I.4.7; I.2.6; I.2.10
\\
  Spatial optimization is often overlooked in many computer vision tasks.
Filters should be able to recognize the features of an object regardless of
where it is in the image. Similarity search is a crucial task where spatial
features decide an important output. The capacity of convolution to capture
visual patterns across various locations is limited. In contrast to
convolution, the involution kernel is dynamically created at each pixel based
on the pixel value and parameters that have been learned. This study
demonstrates that utilizing a single layer of involution feature extractor
alongside a compact convolution model significantly enhances the performance of
similarity search. Additionally, we improve predictions by using the GELU
activation function rather than the ReLU. The negligible amount of weight
parameters in involution with a compact model with better performance makes the
model very useful in real-world implementations. Our proposed model is below 1
megabyte in size. We have experimented with our proposed methodology and other
models on CIFAR-10, FashionMNIST, and MNIST datasets. Our proposed method
outperforms across all three datasets.
\\ ( https://arxiv.org/abs/2404.06593 ,  4864kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06631 (*cross-listing*)
Date: Tue, 9 Apr 2024 21:46:14 GMT   (11584kb,D)

Title: Counting Objects in a Robotic Hand
Authors: Francis Tsow, Tianze Chen, and Yu Sun
Categories: cs.RO cs.AI
\\
  A robot performing multi-object grasping needs to sense the number of objects
in the hand after grasping. The count plays an important role in determining
the robot's next move and the outcome and efficiency of the whole pick-place
process. This paper presents a data-driven contrastive learning-based counting
classifier with a modified loss function as a simple and effective approach for
object counting despite significant occlusion challenges caused by robotic
fingers and objects. The model was validated against other models with three
different common shapes (spheres, cylinders, and cubes) in simulation and in a
real setup. The proposed contrastive learning-based counting approach achieved
above 96\% accuracy for all three objects in the real setup.
\\ ( https://arxiv.org/abs/2404.06631 ,  11584kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06633 (*cross-listing*)
Date: Tue, 9 Apr 2024 21:53:53 GMT   (2498kb,D)

Title: Evolving Loss Functions for Specific Image Augmentation Techniques
Authors: Brandon Morgan and Dean Hougen
Categories: cs.NE cs.AI
\\
  Previous work in Neural Loss Function Search (NLFS) has shown a lack of
correlation between smaller surrogate functions and large convolutional neural
networks with massive regularization. We expand upon this research by revealing
another disparity that exists, correlation between different types of image
augmentation techniques. We show that different loss functions can perform well
on certain image augmentation techniques, while performing poorly on others. We
exploit this disparity by performing an evolutionary search on five types of
image augmentation techniques in the hopes of finding image augmentation
specific loss functions. The best loss functions from each evolution were then
taken and transferred to WideResNet-28-10 on CIFAR-10 and CIFAR-100 across each
of the five image augmentation techniques. The best from that were then taken
and evaluated by fine-tuning EfficientNetV2Small on the CARS, Oxford-Flowers,
and Caltech datasets across each of the five image augmentation techniques.
Multiple loss functions were found that outperformed cross-entropy across
multiple experiments. In the end, we found a single loss function, which we
called the inverse bessel logarithm loss, that was able to outperform
cross-entropy across the majority of experiments.
\\ ( https://arxiv.org/abs/2404.06633 ,  2498kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06645 (*cross-listing*)
Date: Tue, 9 Apr 2024 22:47:25 GMT   (4829kb,D)

Title: GenCHiP: Generating Robot Policy Code for High-Precision and
  Contact-Rich Manipulation Tasks
Authors: Kaylee Burns, Ajinkya Jain, Keegan Go, Fei Xia, Michael Stark, Stefan
  Schaal, Karol Hausman
Categories: cs.RO cs.AI
Comments: 14 pages, 12 figures
ACM-class: I.2.9
\\
  Large Language Models (LLMs) have been successful at generating robot policy
code, but so far these results have been limited to high-level tasks that do
not require precise movement. It is an open question how well such approaches
work for tasks that require reasoning over contact forces and working within
tight success tolerances. We find that, with the right action space, LLMs are
capable of successfully generating policies for a variety of contact-rich and
high-precision manipulation tasks, even under noisy conditions, such as
perceptual errors or grasping inaccuracies. Specifically, we reparameterize the
action space to include compliance with constraints on the interaction forces
and stiffnesses involved in reaching a target pose. We validate this approach
on subtasks derived from the Functional Manipulation Benchmark (FMB) and NIST
Task Board Benchmarks. Exposing this action space alongside methods for
estimating object poses improves policy generation with an LLM by greater than
3x and 4x when compared to non-compliant action spaces
\\ ( https://arxiv.org/abs/2404.06645 ,  4829kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06647 (*cross-listing*)
Date: Tue, 9 Apr 2024 22:55:06 GMT   (1372kb,D)

Title: From Protoscience to Epistemic Monoculture: How Benchmarking Set the
  Stage for the Deep Learning Revolution
Authors: Bernard J. Koch, David Peterson
Categories: cs.CY cs.AI cs.LG
\\
  Over the past decade, AI research has focused heavily on building ever-larger
deep learning models. This approach has simultaneously unlocked incredible
achievements in science and technology, and hindered AI from overcoming
long-standing limitations with respect to explainability, ethical harms, and
environmental efficiency. Drawing on qualitative interviews and computational
analyses, our three-part history of AI research traces the creation of this
"epistemic monoculture" back to a radical reconceptualization of scientific
progress that occurred in the 1990s. In the first era of AI research
(1950s-late 1980s), researchers and patrons approached AI as a "basic" science
that would advance through autonomous exploration and organic assessments of
progress (e.g., peer-review, theoretical consensus). The failure of this
approach led to a retrenchment of funding in the 1980s. Amid this "AI Winter,"
an intervention by the U.S. government reoriented the field towards measurable
progress on tasks of military and commercial interest. A new evaluation system
called "benchmarking" provided an objective way to quantify progress on tasks
by focusing exclusively on increasing predictive accuracy on example datasets.
Distilling science down to verifiable metrics clarified the roles of
scientists, allowed the field to rapidly integrate talent, and provided clear
signals of significance and progress. But history has also revealed a tradeoff
to this streamlined approach to science: the consolidation around external
interests and inherent conservatism of benchmarking has disincentivized
exploration beyond scaling monoculture. In the discussion, we explain how AI's
monoculture offers a compelling challenge to the belief that basic,
exploration-driven research is needed for scientific progress. Implications for
the spread of AI monoculture to other sciences in the era of generative AI are
also discussed.
\\ ( https://arxiv.org/abs/2404.06647 ,  1372kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06666 (*cross-listing*)
Date: Wed, 10 Apr 2024 00:26:08 GMT   (26176kb,D)

Title: SafeGen: Mitigating Unsafe Content Generation in Text-to-Image Models
Authors: Xinfeng Li, Yuchen Yang, Jiangyi Deng, Chen Yan, Yanjiao Chen, Xiaoyu
  Ji, Wenyuan Xu
Categories: cs.CV cs.AI cs.CL cs.CR
\\
  Text-to-image (T2I) models, such as Stable Diffusion, have exhibited
remarkable performance in generating high-quality images from text descriptions
in recent years. However, text-to-image models may be tricked into generating
not-safe-for-work (NSFW) content, particularly in sexual scenarios. Existing
countermeasures mostly focus on filtering inappropriate inputs and outputs, or
suppressing improper text embeddings, which can block explicit NSFW-related
content (e.g., naked or sexy) but may still be vulnerable to adversarial
prompts inputs that appear innocent but are ill-intended. In this paper, we
present SafeGen, a framework to mitigate unsafe content generation by
text-to-image models in a text-agnostic manner. The key idea is to eliminate
unsafe visual representations from the model regardless of the text input. In
this way, the text-to-image model is resistant to adversarial prompts since
unsafe visual representations are obstructed from within. Extensive experiments
conducted on four datasets demonstrate SafeGen's effectiveness in mitigating
unsafe content generation while preserving the high-fidelity of benign images.
SafeGen outperforms eight state-of-the-art baseline methods and achieves 99.1%
sexual content removal performance. Furthermore, our constructed benchmark of
adversarial prompts provides a basis for future development and evaluation of
anti-NSFW-generation methods.
\\ ( https://arxiv.org/abs/2404.06666 ,  26176kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06674 (*cross-listing*)
Date: Wed, 10 Apr 2024 01:33:08 GMT   (7188kb,D)

Title: VoiceShop: A Unified Speech-to-Speech Framework for Identity-Preserving
  Zero-Shot Voice Editing
Authors: Philip Anastassiou, Zhenyu Tang, Kainan Peng, Dongya Jia, Jiaxin Li,
  Ming Tu, Yuping Wang, Yuxuan Wang, Mingbo Ma
Categories: cs.SD cs.AI eess.AS
\\
  We present VoiceShop, a novel speech-to-speech framework that can modify
multiple attributes of speech, such as age, gender, accent, and speech style,
in a single forward pass while preserving the input speaker's timbre. Previous
works have been constrained to specialized models that can only edit these
attributes individually and suffer from the following pitfalls: the magnitude
of the conversion effect is weak, there is no zero-shot capability for
out-of-distribution speakers, or the synthesized outputs exhibit timbre leakage
which changes the speaker's perceived identity. Our work proposes solutions for
each of these issues in a simple modular framework based on a conditional
diffusion backbone model with optional normalizing flow-based and
sequence-to-sequence speaker attribute-editing modules, whose components can be
combined or removed during inference to meet a wide array of tasks without
additional model finetuning. Audio samples are available at
https://voiceshopai.github.io
\\ ( https://arxiv.org/abs/2404.06674 ,  7188kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06679 (*cross-listing*)
Date: Wed, 10 Apr 2024 02:00:24 GMT   (12056kb,D)

Title: Neural Optimizer Equation, Decay Function, and Learning Rate Schedule
  Joint Evolution
Authors: Brandon Morgan and Dean Hougen
Categories: cs.NE cs.AI
\\
  A major contributor to the quality of a deep learning model is the selection
of the optimizer. We propose a new dual-joint search space in the realm of
neural optimizer search (NOS), along with an integrity check, to automate the
process of finding deep learning optimizers. Our dual-joint search space
simultaneously allows for the optimization of not only the update equation, but
also internal decay functions and learning rate schedules for optimizers. We
search the space using our proposed mutation-only, particle-based genetic
algorithm able to be massively parallelized for our domain-specific problem. We
evaluate our candidate optimizers on the CIFAR-10 dataset using a small
ConvNet. To assess generalization, the final optimizers were then transferred
to large-scale image classification on CIFAR- 100 and TinyImageNet, while also
being fine-tuned on Flowers102, Cars196, and Caltech101 using
EfficientNetV2Small. We found multiple optimizers, learning rate schedules, and
Adam variants that outperformed Adam, as well as other standard deep learning
optimizers, across the image classification tasks.
\\ ( https://arxiv.org/abs/2404.06679 ,  12056kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06690 (*cross-listing*)
Date: Wed, 10 Apr 2024 02:32:58 GMT   (1724kb,D)

Title: CoVoMix: Advancing Zero-Shot Speech Generation for Human-like
  Multi-talker Conversations
Authors: Leying Zhang, Yao Qian, Long Zhou, Shujie Liu, Dongmei Wang, Xiaofei
  Wang, Midia Yousefi, Yanmin Qian, Jinyu Li, Lei He, Sheng Zhao, Michael Zeng
Categories: eess.AS cs.AI cs.CL cs.LG cs.SD
\\
  Recent advancements in zero-shot text-to-speech (TTS) modeling have led to
significant strides in generating high-fidelity and diverse speech. However,
dialogue generation, along with achieving human-like naturalness in speech,
continues to be a challenge in the field. In this paper, we introduce CoVoMix:
Conversational Voice Mixture Generation, a novel model for zero-shot,
human-like, multi-speaker, multi-round dialogue speech generation. CoVoMix is
capable of first converting dialogue text into multiple streams of discrete
tokens, with each token stream representing semantic information for individual
talkers. These token streams are then fed into a flow-matching based acoustic
model to generate mixed mel-spectrograms. Finally, the speech waveforms are
produced using a HiFi-GAN model. Furthermore, we devise a comprehensive set of
metrics for measuring the effectiveness of dialogue modeling and generation.
Our experimental results show that CoVoMix can generate dialogues that are not
only human-like in their naturalness and coherence but also involve multiple
talkers engaging in multiple rounds of conversation. These dialogues, generated
within a single channel, are characterized by seamless speech transitions,
including overlapping speech, and appropriate paralinguistic behaviors such as
laughter. Audio samples are available at https://aka.ms/covomix.
\\ ( https://arxiv.org/abs/2404.06690 ,  1724kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06704 (*cross-listing*)
Date: Wed, 10 Apr 2024 03:20:33 GMT   (15891kb,D)

Title: Convolution-based Probability Gradient Loss for Semantic Segmentation
Authors: Guohang Shan and Shuangcheng Jia
Categories: cs.CV cs.AI
Comments: 12 pages, 7 figures
\\
  In this paper, we introduce a novel Convolution-based Probability Gradient
(CPG) loss for semantic segmentation. It employs convolution kernels similar to
the Sobel operator, capable of computing the gradient of pixel intensity in an
image. This enables the computation of gradients for both ground-truth and
predicted category-wise probabilities. It enhances network performance by
maximizing the similarity between these two probability gradients. Moreover, to
specifically enhance accuracy near the object's boundary, we extract the object
boundary based on the ground-truth probability gradient and exclusively apply
the CPG loss to pixels belonging to boundaries. CPG loss proves to be highly
convenient and effective. It establishes pixel relationships through
convolution, calculating errors from a distinct dimension compared to
pixel-wise loss functions such as cross-entropy loss. We conduct qualitative
and quantitative analyses to evaluate the impact of the CPG loss on three
well-established networks (DeepLabv3-Resnet50, HRNetV2-OCR, and
LRASPP_MobileNet_V3_Large) across three standard segmentation datasets
(Cityscapes, COCO-Stuff, ADE20K). Our extensive experimental results
consistently and significantly demonstrate that the CPG loss enhances the mean
Intersection over Union.
\\ ( https://arxiv.org/abs/2404.06704 ,  15891kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06710 (*cross-listing*)
Date: Wed, 10 Apr 2024 03:31:32 GMT   (23331kb,D)

Title: SpikeNVS: Enhancing Novel View Synthesis from Blurry Images via Spike
  Camera
Authors: Gaole Dai and Zhenyu Wang and Qinwen Xu and Wen Cheng and Ming Lu and
  Boxing Shi and Shanghang Zhang and Tiejun Huang
Categories: cs.CV cs.AI
\\
  One of the most critical factors in achieving sharp Novel View Synthesis
(NVS) using neural field methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) is the quality of the training images. However,
Conventional RGB cameras are susceptible to motion blur. In contrast,
neuromorphic cameras like event and spike cameras inherently capture more
comprehensive temporal information, which can provide a sharp representation of
the scene as additional training data. Recent methods have explored the
integration of event cameras to improve the quality of NVS. The event-RGB
approaches have some limitations, such as high training costs and the inability
to work effectively in the background. Instead, our study introduces a new
method that uses the spike camera to overcome these limitations. By considering
texture reconstruction from spike streams as ground truth, we design the
Texture from Spike (TfS) loss. Since the spike camera relies on temporal
integration instead of temporal differentiation used by event cameras, our
proposed TfS loss maintains manageable training costs. It handles foreground
objects with backgrounds simultaneously. We also provide a real-world dataset
captured with our spike-RGB camera system to facilitate future research
endeavors. We conduct extensive experiments using synthetic and real-world
datasets to demonstrate that our design can enhance novel view synthesis across
NeRF and 3DGS. The code and dataset will be made available for public access.
\\ ( https://arxiv.org/abs/2404.06710 ,  23331kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06717 (*cross-listing*)
Date: Wed, 10 Apr 2024 04:04:05 GMT   (44kb)

Title: Racial/Ethnic Categories in AI and Algorithmic Fairness: Why They Matter
  and What They Represent
Authors: Jennifer Mickel
Categories: cs.CY cs.AI
\\
  Racial diversity has become increasingly discussed within the AI and
algorithmic fairness literature, yet little attention is focused on justifying
the choices of racial categories and understanding how people are racialized
into these chosen racial categories. Even less attention is given to how racial
categories shift and how the racialization process changes depending on the
context of a dataset or model. An unclear understanding of \textit{who}
comprises the racial categories chosen and \textit{how} people are racialized
into these categories can lead to varying interpretations of these categories.
These varying interpretations can lead to harm when the understanding of racial
categories and the racialization process is misaligned from the actual
racialization process and racial categories used. Harm can also arise if the
racialization process and racial categories used are irrelevant or do not exist
in the context they are applied.
  In this paper, we make two contributions. First, we demonstrate how racial
categories with unclear assumptions and little justification can lead to
varying datasets that poorly represent groups obfuscated or unrepresented by
the given racial categories and models that perform poorly on these groups.
Second, we develop a framework, CIRCSheets, for documenting the choices and
assumptions in choosing racial categories and the process of racialization into
these categories to facilitate transparency in understanding the processes and
assumptions made by dataset or model developers when selecting or using these
racial categories.
\\ ( https://arxiv.org/abs/2404.06717 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06731 (*cross-listing*)
Date: Wed, 10 Apr 2024 04:35:54 GMT   (330kb)

Title: Accuracy of a Large Language Model in Distinguishing Anti- And
  Pro-vaccination Messages on Social Media: The Case of Human Papillomavirus
  Vaccination
Authors: Soojong Kim, Kwanho Kim, Claire Wonjeong Jo
Categories: cs.CY cs.AI
Comments: Forthcoming in Preventive Medicine Reports
\\
  Objective. Vaccination has engendered a spectrum of public opinions, with
social media acting as a crucial platform for health-related discussions. The
emergence of artificial intelligence technologies, such as large language
models (LLMs), offers a novel opportunity to efficiently investigate public
discourses. This research assesses the accuracy of ChatGPT, a widely used and
freely available service built upon an LLM, for sentiment analysis to discern
different stances toward Human Papillomavirus (HPV) vaccination. Methods.
Messages related to HPV vaccination were collected from social media supporting
different message formats: Facebook (long format) and Twitter (short format). A
selection of 1,000 human-evaluated messages was input into the LLM, which
generated multiple response instances containing its classification results.
Accuracy was measured for each message as the level of concurrence between
human and machine decisions, ranging between 0 and 1. Results. Average accuracy
was notably high when 20 response instances were used to determine the machine
decision of each message: .882 (SE = .021) and .750 (SE = .029) for anti- and
pro-vaccination long-form; .773 (SE = .027) and .723 (SE = .029) for anti- and
pro-vaccination short-form, respectively. Using only three or even one instance
did not lead to a severe decrease in accuracy. However, for long-form messages,
the language model exhibited significantly lower accuracy in categorizing
pro-vaccination messages than anti-vaccination ones. Conclusions. ChatGPT shows
potential in analyzing public opinions on HPV vaccination using social media
content. However, understanding the characteristics and limitations of a
language model within specific public health contexts remains imperative.
\\ ( https://arxiv.org/abs/2404.06731 ,  330kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06733 (*cross-listing*)
Date: Wed, 10 Apr 2024 04:38:17 GMT   (4965kb,D)

Title: Incremental XAI: Memorable Understanding of AI with Incremental
  Explanations
Authors: Jessica Y. Bo, Pan Hao, Brian Y. Lim
Categories: cs.HC cs.AI
Comments: CHI 2024
\\
  Many explainable AI (XAI) techniques strive for interpretability by providing
concise salient information, such as sparse linear factors. However, users
either only see inaccurate global explanations, or highly-varying local
explanations. We propose to provide more detailed explanations by leveraging
the human cognitive capacity to accumulate knowledge by incrementally receiving
more details. Focusing on linear factor explanations (factors $\times$ values =
outcome), we introduce Incremental XAI to automatically partition explanations
for general and atypical instances by providing Base + Incremental factors to
help users read and remember more faithful explanations. Memorability is
improved by reusing base factors and reducing the number of factors shown in
atypical cases. In modeling, formative, and summative user studies, we
evaluated the faithfulness, memorability and understandability of Incremental
XAI against baseline explanation methods. This work contributes towards more
usable explanation that users can better ingrain to facilitate intuitive
engagement with AI.
\\ ( https://arxiv.org/abs/2404.06733 ,  4965kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06750 (*cross-listing*)
Date: Wed, 10 Apr 2024 05:34:07 GMT   (200kb)

Title: Frontier AI Ethics: Anticipating and Evaluating the Societal Impacts of
  Generative Agents
Authors: Seth Lazar
Categories: cs.CY cs.AI
\\
  Some have criticised Generative AI Systems for replicating the familiar
pathologies of already widely-deployed AI systems. Other critics highlight how
they foreshadow vastly more powerful future systems, which might threaten
humanity's survival. The first group says there is nothing new here; the other
looks through the present to a perhaps distant horizon. In this paper, I
instead pay attention to what makes these particular systems distinctive: both
their remarkable scientific achievement, and the most likely and consequential
ways in which they will change society over the next five to ten years. In
particular, I explore the potential societal impacts and normative questions
raised by the looming prospect of 'Generative Agents', in which multimodal
large language models (LLMs) form the executive centre of complex, tool-using
AI systems that can take unsupervised sequences of actions towards some goal.
\\ ( https://arxiv.org/abs/2404.06750 ,  200kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06757 (*cross-listing*)
Date: Wed, 10 Apr 2024 05:53:25 GMT   (51kb,D)

Title: Language Generation in the Limit
Authors: Jon Kleinberg and Sendhil Mullainathan
Categories: cs.DS cs.AI cs.CL cs.LG
Comments: 24 pages, 2 figures
\\
  Although current large language models are complex, the most basic
specifications of the underlying language generation problem itself are simple
to state: given a finite set of training samples from an unknown language,
produce valid new strings from the language that don't already appear in the
training data. Here we ask what we can conclude about language generation using
only this specification, without further assumptions. In particular, suppose
that an adversary enumerates the strings of an unknown target language L that
is known only to come from one of a possibly infinite list of candidates. A
computational agent is trying to learn to generate from this language; we say
that the agent generates from L in the limit if after some finite point in the
enumeration of L, the agent is able to produce new elements that come
exclusively from L and that have not yet been presented by the adversary. Our
main result is that there is an agent that is able to generate in the limit for
every countable list of candidate languages. This contrasts dramatically with
negative results due to Gold and Angluin in a well-studied model of language
learning where the goal is to identify an unknown language from samples; the
difference between these results suggests that identifying a language is a
fundamentally different problem than generating from it.
\\ ( https://arxiv.org/abs/2404.06757 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06828 (*cross-listing*)
Date: Wed, 10 Apr 2024 08:32:29 GMT   (836kb,D)

Title: Proposed modified computational model for the amoeba-inspired
  combinatorial optimization machine
Authors: Yusuke Miyajima and Masahito Mochizuki
Categories: cs.NE cond-mat.dis-nn cs.AI nlin.CD stat.CO
Comments: 13 pages, 2 figures
\\
  A single-celled amoeba can solve the traveling salesman problem through its
shape-changing dynamics. In this paper, we examine roles of several elements in
a previously proposed computational model of the solution-search process of
amoeba and three modifications towards enhancing the solution-search
preformance. We find that appropriate modifications can indeed significantly
improve the quality of solutions. It is also found that a condition associated
with the volume conservation can also be modified in contrast to the naive
belief that it is indispensable for the solution-search ability of amoeba. A
proposed modified model shows much better performance.
\\ ( https://arxiv.org/abs/2404.06828 ,  836kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06859 (*cross-listing*)
Date: Wed, 10 Apr 2024 09:35:36 GMT   (993kb,D)

Title: Multi-Label Continual Learning for the Medical Domain: A Novel Benchmark
Authors: Marina Ceccon, Davide Dalle Pezze, Alessandro Fabris, Gian Antonio
  Susto
Categories: cs.CV cs.AI
\\
  Multi-label image classification in dynamic environments is a problem that
poses significant challenges. Previous studies have primarily focused on
scenarios such as Domain Incremental Learning and Class Incremental Learning,
which do not fully capture the complexity of real-world applications. In this
paper, we study the problem of classification of medical imaging in the
scenario termed New Instances \& New Classes, which combines the challenges of
both new class arrivals and domain shifts in a single framework. Unlike
traditional scenarios, it reflects the realistic nature of CL in domains such
as medical imaging, where updates may introduce both new classes and changes in
domain characteristics. To address the unique challenges posed by this complex
scenario, we introduce a novel approach called Pseudo-Label Replay. This method
aims to mitigate forgetting while adapting to new classes and domain shifts by
combining the advantages of the Replay and Pseudo-Label methods and solving
their limitations in the proposed scenario. % part3 We evaluate our proposed
approach on a challenging benchmark consisting of two datasets, seven tasks,
and nineteen classes, modeling a realistic Continual Learning scenario. Our
experimental findings demonstrate the effectiveness of Pseudo-Label Replay in
addressing the challenges posed by the complex scenario proposed. Our method
surpasses existing approaches, exhibiting superior performance while showing
minimal forgetting.
\\ ( https://arxiv.org/abs/2404.06859 ,  993kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06883 (*cross-listing*)
Date: Wed, 10 Apr 2024 10:13:37 GMT   (775kb)

Title: Research on Detection of Floating Objects in River and Lake Based on AI
  Intelligent Image Recognition
Authors: Jingyu Zhang, Ao Xiang, Yu Cheng, Qin Yang, Liyang Wang
Categories: cs.CV cs.AI
\\
  With the rapid advancement of artificial intelligence technology, AI-enabled
image recognition has emerged as a potent tool for addressing challenges in
traditional environmental monitoring. This study focuses on the detection of
floating objects in river and lake environments, exploring an innovative
approach based on deep learning. By intricately analyzing the technical
pathways for detecting static and dynamic features and considering the
characteristics of river and lake debris, a comprehensive image acquisition and
processing workflow has been developed. The study highlights the application
and performance comparison of three mainstream deep learning models -SSD,
Faster-RCNN, and YOLOv5- in debris identification. Additionally, a detection
system for floating objects has been designed and implemented, encompassing
both hardware platform construction and software framework development. Through
rigorous experimental validation, the proposed system has demonstrated its
ability to significantly enhance the accuracy and efficiency of debris
detection, thus offering a new technological avenue for water quality
monitoring in rivers and lakes
\\ ( https://arxiv.org/abs/2404.06883 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06903 (*cross-listing*)
Date: Wed, 10 Apr 2024 10:46:59 GMT   (43040kb,D)

Title: DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic
  Gaussian Splatting
Authors: Shijie Zhou, Zhiwen Fan, Dejia Xu, Haoran Chang, Pradyumna Chari,
  Tejas Bharadwaj, Suya You, Zhangyang Wang, Achuta Kadambi
Categories: cs.CV cs.AI
\\
  The increasing demand for virtual reality applications has highlighted the
significance of crafting immersive 3D assets. We present a text-to-3D
360$^{\circ}$ scene generation pipeline that facilitates the creation of
comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of
minutes. Our approach utilizes the generative power of a 2D diffusion model and
prompt self-refinement to create a high-quality and globally coherent panoramic
image. This image acts as a preliminary "flat" (2D) scene representation.
Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to
enable real-time exploration. To produce consistent 3D geometry, our pipeline
constructs a spatially coherent structure by aligning the 2D monocular depth
into a globally optimized point cloud. This point cloud serves as the initial
state for the centroids of 3D Gaussians. In order to address invisible issues
inherent in single-view inputs, we impose semantic and geometric constraints on
both synthesized and input camera views as regularizations. These guide the
optimization of Gaussians, aiding in the reconstruction of unseen regions. In
summary, our method offers a globally consistent 3D scene within a
360$^{\circ}$ perspective, providing an enhanced immersive experience over
existing techniques. Project website at: http://dreamscene360.github.io/
\\ ( https://arxiv.org/abs/2404.06903 ,  43040kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06939 (*cross-listing*)
Date: Wed, 10 Apr 2024 11:43:26 GMT   (791kb,D)

Title: Fast System Technology Co-Optimization Framework for Emerging Technology
  Based on Graph Neural Networks
Authors: Tianliang Ma, Guangxi Fan, Xuguang Sun, Zhihui Deng, Kainlu Low,
  Leilai Shao
Categories: cs.ET cs.AI
Comments: Accepted by the 61th Design Automation Conference (DAC)
\\
  This paper proposes a fast system technology co-optimization (STCO) framework
that optimizes power, performance, and area (PPA) for next-generation IC
design, addressing the challenges and opportunities presented by novel
materials and device architectures. We focus on accelerating the technology
level of STCO using AI techniques, by employing graph neural network
(GNN)-based approaches for both TCAD simulation and cell library
characterization, which are interconnected through a unified compact model,
collectively achieving over a 100X speedup over traditional methods. These
advancements enable comprehensive STCO iterations with runtime speedups ranging
from 1.9X to 14.1X and supports both emerging and traditional technologies.
\\ ( https://arxiv.org/abs/2404.06939 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06955 (*cross-listing*)
Date: Wed, 10 Apr 2024 12:12:50 GMT   (551kb,D)

Title: Untangling Critical Interaction with AI in Students Written Assessment
Authors: Antonette Shibani, Simon Knight, Kirsty Kitto, Ajanie Karunanayake,
  Simon Buckingham Shum
Categories: cs.HC cs.AI
ACM-class: I.2; K.3.1
DOI: 10.1145/3613905.3651083
\\
  Artificial Intelligence (AI) has become a ubiquitous part of society, but a
key challenge exists in ensuring that humans are equipped with the required
critical thinking and AI literacy skills to interact with machines effectively
by understanding their capabilities and limitations. These skills are
particularly important for learners to develop in the age of generative AI
where AI tools can demonstrate complex knowledge and ability previously thought
to be uniquely human. To activate effective human-AI partnerships in writing,
this paper provides a first step toward conceptualizing the notion of critical
learner interaction with AI. Using both theoretical models and empirical data,
our preliminary findings suggest a general lack of Deep interaction with AI
during the writing process. We believe that the outcomes can lead to better
task and tool design in the future for learners to develop deep, critical
thinking when interacting with AI.
\\ ( https://arxiv.org/abs/2404.06955 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06957 (*cross-listing*)
Date: Wed, 10 Apr 2024 12:17:25 GMT   (13481kb,D)

Title: Adversarial purification for no-reference image-quality metrics:
  applicability study and new methods
Authors: Aleksandr Gushchin, Anna Chistyakova, Vladislav Minashkin, Anastasia
  Antsiferova and Dmitriy Vatolin
Categories: cs.CV cs.AI
\\
  Recently, the area of adversarial attacks on image quality metrics has begun
to be explored, whereas the area of defences remains under-researched. In this
study, we aim to cover that case and check the transferability of adversarial
purification defences from image classifiers to IQA methods. In this paper, we
apply several widespread attacks on IQA models and examine the success of the
defences against them. The purification methodologies covered different
preprocessing techniques, including geometrical transformations, compression,
denoising, and modern neural network-based methods. Also, we address the
challenge of assessing the efficacy of a defensive methodology by proposing
ways to estimate output visual quality and the success of neutralizing attacks.
Defences were tested against attack on three IQA metrics -- Linearity, MetaIQA
and SPAQ. The code for attacks and defences is available at: (link is hidden
for a blind review).
\\ ( https://arxiv.org/abs/2404.06957 ,  13481kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06971 (*cross-listing*)
Date: Wed, 10 Apr 2024 12:31:43 GMT   (3044kb,D)

Title: TrajPRed: Trajectory Prediction with Region-based Relation Learning
Authors: Chen Zhou, Ghassan AlRegib, Armin Parchami, Kunjan Singh
Categories: cs.CV cs.AI cs.LG
DOI: 10.1109/TITS.2024.3381843
\\
  Forecasting human trajectories in traffic scenes is critical for safety
within mixed or fully autonomous systems. Human future trajectories are driven
by two major stimuli, social interactions, and stochastic goals. Thus, reliable
forecasting needs to capture these two stimuli. Edge-based relation modeling
represents social interactions using pairwise correlations from precise
individual states. Nevertheless, edge-based relations can be vulnerable under
perturbations. To alleviate these issues, we propose a region-based relation
learning paradigm that models social interactions via region-wise dynamics of
joint states, i.e., the changes in the density of crowds. In particular,
region-wise agent joint information is encoded within convolutional feature
grids. Social relations are modeled by relating the temporal changes of local
joint information from a global perspective. We show that region-based
relations are less susceptible to perturbations. In order to account for the
stochastic individual goals, we exploit a conditional variational autoencoder
to realize multi-goal estimation and diverse future prediction. Specifically,
we perform variational inference via the latent distribution, which is
conditioned on the correlation between input states and associated target
goals. Sampling from the latent distribution enables the framework to reliably
capture the stochastic behavior in test data. We integrate multi-goal
estimation and region-based relation learning to model the two stimuli, social
interactions, and stochastic goals, in a prediction framework. We evaluate our
framework on the ETH-UCY dataset and Stanford Drone Dataset (SDD). We show that
the diverse prediction better fits the ground truth when incorporating the
relation module. Our framework outperforms the state-of-the-art models on SDD
by $27.61\%$/$18.20\%$ of ADE/FDE metrics.
\\ ( https://arxiv.org/abs/2404.06971 ,  3044kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07005 (*cross-listing*)
Date: Wed, 10 Apr 2024 13:40:29 GMT   (281kb,D)

Title: WordDecipher: Enhancing Digital Workspace Communication with Explainable
  AI for Non-native English Speakers
Authors: Yuexi Chen, Zhicheng Liu
Categories: cs.HC cs.AI cs.CL
Comments: The Third Workshop on Intelligent and Interactive Writing Assistants
  (In2Writing) at CHI 2024
\\
  Non-native English speakers (NNES) face challenges in digital workspace
communication (e.g., emails, Slack messages), often inadvertently translating
expressions from their native languages, which can lead to awkward or incorrect
usage. Current AI-assisted writing tools are equipped with fluency enhancement
and rewriting suggestions; however, NNES may struggle to grasp the subtleties
among various expressions, making it challenging to choose the one that
accurately reflects their intent. Such challenges are exacerbated in high-stake
text-based communications, where the absence of non-verbal cues heightens the
risk of misinterpretation. By leveraging the latest advancements in large
language models (LLM) and word embeddings, we propose WordDecipher, an
explainable AI-assisted writing tool to enhance digital workspace communication
for NNES. WordDecipher not only identifies the perceived social intentions
detected in users' writing, but also generates rewriting suggestions aligned
with users' intended messages, either numerically or by inferring from users'
writing in their native language. Then, WordDecipher provides an overview of
nuances to help NNES make selections. Through a usage scenario, we demonstrate
how WordDecipher can significantly enhance an NNES's ability to communicate her
request, showcasing its potential to transform workspace communication for
NNES.
\\ ( https://arxiv.org/abs/2404.07005 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07063 (*cross-listing*)
Date: Wed, 10 Apr 2024 14:52:35 GMT   (9483kb,D)

Title: LaPlaSS: Latent Space Planning for Stochastic Systems
Authors: Marlyse Reeves and Brian C. Williams
Categories: cs.RO cs.AI
\\
  Autonomous mobile agents often operate in hazardous environments,
necessitating an awareness of safety. These agents can have non-linear,
stochastic dynamics that must be considered during planning to guarantee
bounded risk. Most state of the art methods require closed-form dynamics to
verify plan correctness and safety however modern robotic systems often have
dynamics that are learned from data. Thus, there is a need to perform efficient
trajectory planning with guarantees on risk for agents without known dynamics
models. We propose a "generate-and-test" approach to risk-bounded planning in
which a planner generates a candidate trajectory using an approximate linear
dynamics model and a validator assesses the risk of the trajectory, computing
additional safety constraints for the planner if the candidate does not satisfy
the desired risk bound. To acquire the approximate model, we use a variational
autoencoder to learn a latent linear dynamics model and encode the planning
problem into the latent space to generate the candidate trajectory. The VAE
also serves to sample trajectories around the candidate to use in the
validator. We demonstrate that our algorithm, LaPlaSS, is able to generate
trajectory plans with bounded risk for a real-world agent with learned dynamics
and is an order of magnitude more efficient than the state of the art.
\\ ( https://arxiv.org/abs/2404.07063 ,  9483kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07096 (*cross-listing*)
Date: Wed, 10 Apr 2024 15:36:59 GMT   (1102kb,D)

Title: TransTARec: Time-Adaptive Translating Embedding Model for Next POI
  Recommendation
Authors: Yiping Sun
Categories: cs.IR cs.AI
Comments: This paper has been accepted by the 2024 5th International Conference
  on Computer Engineering and Application (ICCEA 2024)
\\
  The rapid growth of location acquisition technologies makes
Point-of-Interest(POI) recommendation possible due to redundant user check-in
records. In this paper, we focus on next POI recommendation in which next POI
is based on previous POI. We observe that time plays an important role in next
POI recommendation but is neglected in the recent proposed translating
embedding methods. To tackle this shortage, we propose a time-adaptive
translating embedding model (TransTARec) for next POI recommendation that
naturally incorporates temporal influence, sequential dynamics, and user
preference within a single component. Methodologically, we treat a (previous
timestamp, user, next timestamp) triplet as a union translation vector and
develop a neural-based fusion operation to fuse user preference and temporal
influence. The superiority of TransTARec, which is confirmed by extensive
experiments on real-world datasets, comes from not only the introduction of
temporal influence but also the direct unification with user preference and
sequential dynamics.
\\ ( https://arxiv.org/abs/2404.07096 ,  1102kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07123 (*cross-listing*)
Date: Wed, 10 Apr 2024 16:04:07 GMT   (2028kb,D)

Title: Semantically-correlated memories in a dense associative model
Authors: Thomas F Burns
Categories: cs.NE cs.AI cs.LG q-bio.NC
Comments: 35 pages, 32 figures
MSC-class: 68T07, 92B20, 68T01, 00A69
ACM-class: I.2; I.5; I.4; J.2; J.3
\\
  I introduce a novel associative memory model named Correlated Dense
Associative Memory (CDAM), which integrates both auto- and hetero-association
in a unified framework for continuous-valued memory patterns. Employing an
arbitrary graph structure to semantically link memory patterns, CDAM is
theoretically and numerically analysed, revealing four distinct dynamical
modes: auto-association, narrow hetero-association, wide hetero-association,
and neutral quiescence. Drawing inspiration from inhibitory modulation studies,
I employ anti-Hebbian learning rules to control the range of
hetero-association, extract multi-scale representations of community structures
in graphs, and stabilise the recall of temporal sequences. Experimental
demonstrations showcase CDAM's efficacy in handling real-world data,
replicating a classical neuroscience experiment, performing image retrieval,
and simulating arbitrary finite automata.
\\ ( https://arxiv.org/abs/2404.07123 ,  2028kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07124 (*cross-listing*)
Date: Wed, 10 Apr 2024 16:04:21 GMT   (4270kb,D)

Title: Measuring proximity to standard planes during fetal brain ultrasound
  scanning
Authors: Chiara Di Vece, Antonio Cirigliano, Meala Le Lous, Raffaele
  Napolitano, Anna L. David, Donald Peebles, Pierre Jannin, Francisco
  Vasconcelos, Danail Stoyanov
Categories: cs.CV cs.AI
Comments: 11 pages, 5 figures
ACM-class: I.2.0; I.4.0; J.2.0; J.3.0
\\
  This paper introduces a novel pipeline designed to bring ultrasound (US)
plane pose estimation closer to clinical use for more effective navigation to
the standard planes (SPs) in the fetal brain. We propose a semi-supervised
segmentation model utilizing both labeled SPs and unlabeled 3D US volume
slices. Our model enables reliable segmentation across a diverse set of fetal
brain images. Furthermore, the model incorporates a classification mechanism to
identify the fetal brain precisely. Our model not only filters out frames
lacking the brain but also generates masks for those containing it, enhancing
the relevance of plane pose regression in clinical settings. We focus on fetal
brain navigation from 2D ultrasound (US) video analysis and combine this model
with a US plane pose regression network to provide sensorless proximity
detection to SPs and non-SPs planes; we emphasize the importance of proximity
detection to SPs for guiding sonographers, offering a substantial advantage
over traditional methods by allowing earlier and more precise adjustments
during scanning. We demonstrate the practical applicability of our approach
through validation on real fetal scan videos obtained from sonographers of
varying expertise levels. Our findings demonstrate the potential of our
approach to complement existing fetal US technologies and advance prenatal
diagnostic practices.
\\ ( https://arxiv.org/abs/2404.07124 ,  4270kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07164 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:00:04 GMT   (456kb,D)

Title: Analysis of Distributed Optimization Algorithms on a Real
  Processing-In-Memory System
Authors: Steve Rhyner, Haocong Luo, Juan G\'omez-Luna, Mohammad Sadrosadati,
  Jiawei Jiang, Ataberk Olgun, Harshita Gupta, Ce Zhang, Onur Mutlu
Categories: cs.AR cs.AI cs.DC cs.LG
\\
  Machine Learning (ML) training on large-scale datasets is a very expensive
and time-consuming workload. Processor-centric architectures (e.g., CPU, GPU)
commonly used for modern ML training workloads are limited by the data movement
bottleneck, i.e., due to repeatedly accessing the training dataset. As a
result, processor-centric systems suffer from performance degradation and high
energy consumption. Processing-In-Memory (PIM) is a promising solution to
alleviate the data movement bottleneck by placing the computation mechanisms
inside or near memory.
  Our goal is to understand the capabilities and characteristics of popular
distributed optimization algorithms on real-world PIM architectures to
accelerate data-intensive ML training workloads. To this end, we 1) implement
several representative centralized distributed optimization algorithms on
UPMEM's real-world general-purpose PIM system, 2) rigorously evaluate these
algorithms for ML training on large-scale datasets in terms of performance,
accuracy, and scalability, 3) compare to conventional CPU and GPU baselines,
and 4) discuss implications for future PIM hardware and the need to shift to an
algorithm-hardware codesign perspective to accommodate decentralized
distributed optimization algorithms.
  Our results demonstrate three major findings: 1) Modern general-purpose PIM
architectures can be a viable alternative to state-of-the-art CPUs and GPUs for
many memory-bound ML training workloads, when operations and datatypes are
natively supported by PIM hardware, 2) the importance of carefully choosing the
optimization algorithm that best fit PIM, and 3) contrary to popular belief,
contemporary PIM architectures do not scale approximately linearly with the
number of nodes for many data-intensive ML training workloads. To facilitate
future research, we aim to open-source our complete codebase.
\\ ( https://arxiv.org/abs/2404.07164 ,  456kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07168 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:04:06 GMT   (7440kb,D)

Title: Using Neural Networks to Model Hysteretic Kinematics in Tendon-Actuated
  Continuum Robots
Authors: Yuan Wang, Max McCandless, Abdulhamit Donder, Giovanni Pittiglio,
  Behnam Moradkhani, Yash Chitalia and Pierre E. Dupont
Categories: cs.RO cs.AI
Comments: 7 pages, 8 figures, conference
\\
  The ability to accurately model mechanical hysteretic behavior in
tendon-actuated continuum robots using deep learning approaches is a growing
area of interest. In this paper, we investigate the hysteretic response of two
types of tendon-actuated continuum robots and, ultimately, compare three types
of neural network modeling approaches with both forward and inverse kinematic
mappings: feedforward neural network (FNN), FNN with a history input buffer,
and long short-term memory (LSTM) network. We seek to determine which model
best captures temporal dependent behavior. We find that, depending on the
robot's design, choosing different kinematic inputs can alter whether
hysteresis is exhibited by the system. Furthermore, we present the results of
the model fittings, revealing that, in contrast to the standard FNN, both FNN
with a history input buffer and the LSTM model exhibit the capacity to model
historical dependence with comparable performance in capturing rate-dependent
hysteresis.
\\ ( https://arxiv.org/abs/2404.07168 ,  7440kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07170 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:05:12 GMT   (14746kb,D)

Title: Worst-Case Convergence Time of ML Algorithms via Extreme Value Theory
Authors: Saeid Tizpaz-Niari and Sriram Sankaranarayanan
Categories: cs.SE cs.AI cs.LG cs.PF cs.PL
Comments: In 3rd International Conference on AI Engineering: Software
  Engineering for AI (CAIN 2024)
DOI: 10.1145/3644815.3644989
\\
  This paper leverages the statistics of extreme values to predict the
worst-case convergence times of machine learning algorithms. Timing is a
critical non-functional property of ML systems, and providing the worst-case
converge times is essential to guarantee the availability of ML and its
services. However, timing properties such as worst-case convergence times
(WCCT) are difficult to verify since (1) they are not encoded in the syntax or
semantics of underlying programming languages of AI, (2) their evaluations
depend on both algorithmic implementations and underlying systems, and (3)
their measurements involve uncertainty and noise. Therefore, prevalent formal
methods and statistical models fail to provide rich information on the amounts
and likelihood of WCCT.
  Our key observation is that the timing information we seek represents the
extreme tail of execution times. Therefore, extreme value theory (EVT), a
statistical discipline that focuses on understanding and predicting the
distribution of extreme values in the tail of outcomes, provides an ideal
framework to model and analyze WCCT in the training and inference phases of ML
paradigm. Building upon the mathematical tools from EVT, we propose a practical
framework to predict the worst-case timing properties of ML. Over a set of
linear ML training algorithms, we show that EVT achieves a better accuracy for
predicting WCCTs than relevant statistical methods such as the Bayesian factor.
On the set of larger machine learning training algorithms and deep neural
network inference, we show the feasibility and usefulness of EVT models to
accurately predict WCCTs, their expected return periods, and their likelihood.
\\ ( https://arxiv.org/abs/2404.07170 ,  14746kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07185 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:40:27 GMT   (9632kb,D)

Title: Reward Learning from Suboptimal Demonstrations with Applications in
  Surgical Electrocautery
Authors: Zohre Karimi, Shing-Hei Ho, Bao Thach, Alan Kuntz, Daniel S. Brown
Categories: cs.RO cs.AI cs.LG
Comments: In proceedings of the International Symposium on Medical Robotics
  (ISMR) 2024. Equal contribution from two first authors
\\
  Automating robotic surgery via learning from demonstration (LfD) techniques
is extremely challenging. This is because surgical tasks often involve
sequential decision-making processes with complex interactions of physical
objects and have low tolerance for mistakes. Prior works assume that all
demonstrations are fully observable and optimal, which might not be practical
in the real world. This paper introduces a sample-efficient method that learns
a robust reward function from a limited amount of ranked suboptimal
demonstrations consisting of partial-view point cloud observations. The method
then learns a policy by optimizing the learned reward function using
reinforcement learning (RL). We show that using a learned reward function to
obtain a policy is more robust than pure imitation learning. We apply our
approach on a physical surgical electrocautery task and demonstrate that our
method can perform well even when the provided demonstrations are suboptimal
and the observations are high-dimensional point clouds.
\\ ( https://arxiv.org/abs/2404.07185 ,  9632kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07199 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:57:41 GMT   (29478kb,D)

Title: RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth
  Diffusion
Authors: Jaidev Shriram, Alex Trevithick, Lingjie Liu, Ravi Ramamoorthi
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Project Page: https://realmdreamer.github.io/
\\
  We introduce RealmDreamer, a technique for generation of general
forward-facing 3D scenes from text descriptions. Our technique optimizes a 3D
Gaussian Splatting representation to match complex text prompts. We initialize
these splats by utilizing the state-of-the-art text-to-image generators,
lifting their samples into 3D, and computing the occlusion volume. We then
optimize this representation across multiple views as a 3D inpainting task with
image-conditional diffusion models. To learn correct geometric structure, we
incorporate a depth diffusion model by conditioning on the samples from the
inpainting model, giving rich geometric structure. Finally, we finetune the
model using sharpened samples from image generators. Notably, our technique
does not require video or multi-view data and can synthesize a variety of
high-quality 3D scenes in different styles, consisting of multiple objects. Its
generality additionally allows 3D synthesis from a single image.
\\ ( https://arxiv.org/abs/2404.07199 ,  29478kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07202 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:59:20 GMT   (11163kb,D)

Title: UMBRAE: Unified Multimodal Decoding of Brain Signals
Authors: Weihao Xia, Raoul de Charette, Cengiz \"Oztireli, Jing-Hao Xue
Categories: cs.CV cs.AI cs.CL
Comments: Project Page: https://weihaox.github.io/UMBRAE
\\
  We address prevailing challenges of the brain-powered research, departing
from the observation that the literature hardly recover accurate spatial
information and require subject-specific models. To address these challenges,
we propose UMBRAE, a unified multimodal decoding of brain signals. First, to
extract instance-level conceptual and spatial details from neural signals, we
introduce an efficient universal brain encoder for multimodal-brain alignment
and recover object descriptions at multiple levels of granularity from
subsequent multimodal large language model (MLLM). Second, we introduce a
cross-subject training strategy mapping subject-specific features to a common
feature space. This allows a model to be trained on multiple subjects without
extra resources, even yielding superior results compared to subject-specific
models. Further, we demonstrate this supports weakly-supervised adaptation to
new subjects, with only a fraction of the total training data. Experiments
demonstrate that UMBRAE not only achieves superior results in the newly
introduced tasks but also outperforms methods in well established tasks. To
assess our method, we construct and share with the community a comprehensive
brain understanding benchmark BrainHub. Our code and benchmark are available at
https://weihaox.github.io/UMBRAE.
\\ ( https://arxiv.org/abs/2404.07202 ,  11163kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07204 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:59:45 GMT   (9368kb,D)

Title: BRAVE: Broadening the visual encoding of vision-language models
Authors: O\u{g}uzhan Fatih Kar, Alessio Tonioni, Petra Poklukar, Achin
  Kulshrestha, Amir Zamir, Federico Tombari
Categories: cs.CV cs.AI cs.LG
Comments: Project page at https://brave-vlms.epfl.ch/
\\
  Vision-language models (VLMs) are typically composed of a vision encoder,
e.g. CLIP, and a language model (LM) that interprets the encoded features to
solve downstream tasks. Despite remarkable progress, VLMs are subject to
several shortcomings due to the limited capabilities of vision encoders, e.g.
"blindness" to certain image features, visual hallucination, etc. To address
these issues, we study broadening the visual encoding capabilities of VLMs. We
first comprehensively benchmark several vision encoders with different
inductive biases for solving VLM tasks. We observe that there is no single
encoding configuration that consistently achieves top performance across
different tasks, and encoders with different biases can perform surprisingly
similarly. Motivated by this, we introduce a method, named BRAVE, that
consolidates features from multiple frozen encoders into a more versatile
representation that can be directly fed as the input to a frozen LM. BRAVE
achieves state-of-the-art performance on a broad range of captioning and VQA
benchmarks and significantly reduces the aforementioned issues of VLMs, while
requiring a smaller number of trainable parameters than existing methods and
having a more compressed representation. Our results highlight the potential of
incorporating different visual biases for a more broad and contextualized
visual understanding of VLMs.
\\ ( https://arxiv.org/abs/2404.07204 ,  9368kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07206 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:59:59 GMT   (31868kb,D)

Title: GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models
Authors: Zewei Zhang, Huan Liu, Jun Chen, Xiangyu Xu
Categories: cs.CV cs.AI cs.GR cs.LG cs.MM
\\
  In this paper, we introduce GoodDrag, a novel approach to improve the
stability and image quality of drag editing. Unlike existing methods that
struggle with accumulated perturbations and often result in distortions,
GoodDrag introduces an AlDD framework that alternates between drag and
denoising operations within the diffusion process, effectively improving the
fidelity of the result. We also propose an information-preserving motion
supervision operation that maintains the original features of the starting
point for precise manipulation and artifact reduction. In addition, we
contribute to the benchmarking of drag editing by introducing a new dataset,
Drag100, and developing dedicated quality assessment metrics, Dragging Accuracy
Index and Gemini Score, utilizing Large Multimodal Models. Extensive
experiments demonstrate that the proposed GoodDrag compares favorably against
the state-of-the-art approaches both qualitatively and quantitatively. The
project page is https://gooddrag.github.io.
\\ ( https://arxiv.org/abs/2404.07206 ,  31868kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13980 (*cross-listing*)
Date: Thu, 21 Dec 2023 16:10:33 GMT   (9455kb,D)
Date (revised v2): Tue, 9 Apr 2024 04:41:53 GMT   (9933kb,D)

Title: Carve3D: Improving Multi-view Reconstruction Consistency for Diffusion
  Models with RL Finetuning
Authors: Desai Xie, Jiahao Li, Hao Tan, Xin Sun, Zhixin Shu, Yi Zhou, Sai Bi,
  S\"oren Pirk, Arie E. Kaufman
Categories: cs.CV cs.LG
Comments: 22 pages, 16 figures. Our code, training and testing data, and video
  results are available at: https://desaixie.github.io/carve-3d. This paper has
  been accepted to CVPR 2024. v2: incorporated changes from the CVPR 2024
  camera-ready version
\\
  Multi-view diffusion models, obtained by applying Supervised Finetuning (SFT)
to text-to-image diffusion models, have driven recent breakthroughs in
text-to-3D research. However, due to the limited size and quality of existing
3D datasets, they still suffer from multi-view inconsistencies and Neural
Radiance Field (NeRF) reconstruction artifacts. We argue that multi-view
diffusion models can benefit from further Reinforcement Learning Finetuning
(RLFT), which allows models to learn from the data generated by themselves and
improve beyond their dataset limitations during SFT. To this end, we introduce
Carve3D, an improved RLFT algorithm coupled with a novel Multi-view
Reconstruction Consistency (MRC) metric, to enhance the consistency of
multi-view diffusion models. To measure the MRC metric on a set of multi-view
images, we compare them with their corresponding NeRF renderings at the same
camera viewpoints. The resulting model, which we denote as Carve3DM,
demonstrates superior multi-view consistency and NeRF reconstruction quality
than existing models. Our results suggest that pairing SFT with Carve3D's RLFT
is essential for developing multi-view-consistent diffusion models, mirroring
the standard Large Language Model (LLM) alignment pipeline. Our code, training
and testing data, and video results are available at:
https://desaixie.github.io/carve-3d.
\\ ( https://arxiv.org/abs/2312.13980 ,  9933kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05746 (*cross-listing*)
Date: Wed, 3 Apr 2024 14:33:23 GMT   (6489kb,D)

Title: Causality for Earth Science -- A Review on Time-series and
  Spatiotemporal Causality Methods
Authors: Sahara Ali, Uzma Hasan, Xingyan Li, Omar Faruque, Akila Sampath, Yiyi
  Huang, Md Osman Gani and Jianwu Wang
Categories: physics.data-an cs.AI cs.LG physics.ao-ph physics.geo-ph
\\
  This survey paper covers the breadth and depth of time-series and
spatiotemporal causality methods, and their applications in Earth Science. More
specifically, the paper presents an overview of causal discovery and causal
inference, explains the underlying causal assumptions, and enlists evaluation
techniques and key terminologies of the domain area. The paper elicits the
various state-of-the-art methods introduced for time-series and spatiotemporal
causal analysis along with their strengths and limitations. The paper further
describes the existing applications of several methods for answering specific
Earth Science questions such as extreme weather events, sea level rise,
teleconnections etc. This survey paper can serve as a primer for Data Science
researchers interested in data-driven causal study as we share a list of
resources, such as Earth Science datasets (synthetic, simulated and
observational data) and open source tools for causal analysis. It will equally
benefit the Earth Science community interested in taking an AI-driven approach
to study the causality of different dynamic and thermodynamic processes as we
present the open challenges and opportunities in performing causality-based
Earth Science study.
\\ ( https://arxiv.org/abs/2404.05746 ,  6489kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06516 (*cross-listing*)
Date: Thu, 4 Apr 2024 01:02:03 GMT   (153kb,D)

Title: Convergence to Nash Equilibrium and No-regret Guarantee in (Markov)
  Potential Games
Authors: Jing Dong, Baoxiang Wang, Yaoliang Yu
Categories: cs.GT cs.LG
\\
  In this work, we study potential games and Markov potential games under
stochastic cost and bandit feedback. We propose a variant of the Frank-Wolfe
algorithm with sufficient exploration and recursive gradient estimation, which
provably converges to the Nash equilibrium while attaining sublinear regret for
each individual player. Our algorithm simultaneously achieves a Nash regret and
a regret bound of $O(T^{4/5})$ for potential games, which matches the best
available result, without using additional projection steps. Through carefully
balancing the reuse of past samples and exploration of new samples, we then
extend the results to Markov potential games and improve the best available
Nash regret from $O(T^{5/6})$ to $O(T^{4/5})$. Moreover, our algorithm requires
no knowledge of the game, such as the distribution mismatch coefficient, which
provides more flexibility in its practical implementation. Experimental results
corroborate our theoretical findings and underscore the practical effectiveness
of our method.
\\ ( https://arxiv.org/abs/2404.06516 ,  153kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06517 (*cross-listing*)
Date: Thu, 4 Apr 2024 05:24:22 GMT   (35505kb,D)

Title: DiffObs: Generative Diffusion for Global Forecasting of Satellite
  Observations
Authors: Jason Stock, Jaideep Pathak, Yair Cohen, Mike Pritchard, Piyush Garg,
  Dale Durran, Morteza Mardani, Noah Brenowitz
Categories: physics.comp-ph cs.CV cs.LG physics.ao-ph stat.ML
Comments: Published as a workshop paper at "Tackling Climate Change with
  Machine Learning", ICLR 2024
\\
  This work presents an autoregressive generative diffusion model (DiffObs) to
predict the global evolution of daily precipitation, trained on a satellite
observational product, and assessed with domain-specific diagnostics. The model
is trained to probabilistically forecast day-ahead precipitation. Nonetheless,
it is stable for multi-month rollouts, which reveal a qualitatively realistic
superposition of convectively coupled wave modes in the tropics. Cross-spectral
analysis confirms successful generation of low frequency variations associated
with the Madden--Julian oscillation, which regulates most subseasonal to
seasonal predictability in the observed atmosphere, and convectively coupled
moist Kelvin waves with approximately correct dispersion relationships. Despite
secondary issues and biases, the results affirm the potential for a next
generation of global diffusion models trained on increasingly sparse, and
increasingly direct and differentiated observations of the world, for practical
applications in subseasonal and climate prediction.
\\ ( https://arxiv.org/abs/2404.06517 ,  35505kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06535 (*cross-listing*)
Date: Tue, 9 Apr 2024 18:00:01 GMT   (2314kb,D)

Title: Learning to rank quantum circuits for hardware-optimized performance
  enhancement
Authors: Gavin S. Hartnett, Aaron Barbosa, Pranav S. Mundada, Michael Hush,
  Michael J. Biercuk, Yuval Baum
Categories: quant-ph cs.LG
Comments: 14 pages, 5 figures
\\
  We introduce and experimentally test a machine-learning-based method for
ranking logically equivalent quantum circuits based on expected performance
estimates derived from a training procedure conducted on real hardware. We
apply our method to the problem of layout selection, in which abstracted qubits
are assigned to physical qubits on a given device. Circuit measurements
performed on IBM hardware indicate that the maximum and median fidelities of
logically equivalent layouts can differ by an order of magnitude. We introduce
a circuit score used for ranking that is parameterized in terms of a
physics-based, phenomenological error model whose parameters are fit by
training a ranking-loss function over a measured dataset. The dataset consists
of quantum circuits exhibiting a diversity of structures and executed on IBM
hardware, allowing the model to incorporate the contextual nature of real
device noise and errors without the need to perform an exponentially costly
tomographic protocol. We perform model training and execution on the 16-qubit
ibmq_guadalupe device and compare our method to two common approaches: random
layout selection and a publicly available baseline called Mapomatic. Our model
consistently outperforms both approaches, predicting layouts that exhibit lower
noise and higher performance. In particular, we find that our best model leads
to a $1.8\times$ reduction in selection error when compared to the baseline
approach and a $3.2\times$ reduction when compared to random selection. Beyond
delivering a new form of predictive quantum characterization, verification, and
validation, our results reveal the specific way in which context-dependent and
coherent gate errors appear to dominate the divergence from performance
estimates extrapolated from simple proxy measures.
\\ ( https://arxiv.org/abs/2404.06535 ,  2314kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06563 (*cross-listing*)
Date: Tue, 9 Apr 2024 18:27:59 GMT   (3844kb,D)

Title: Demonstration of MaskSearch: Efficiently Querying Image Masks for
  Machine Learning Workflows
Authors: Lindsey Linxi Wei, Chung Yik Edward Yeung, Hongjian Yu, Jingchuan
  Zhou, Dong He, Magdalena Balazinska
Categories: cs.DB cs.LG cs.MM
\\
  We demonstrate MaskSearch, a system designed to accelerate queries over
databases of image masks generated by machine learning models. MaskSearch
formalizes and accelerates a new category of queries for retrieving images and
their corresponding masks based on mask properties, which support various
applications, from identifying spurious correlations learned by models to
exploring discrepancies between model saliency and human attention. This
demonstration makes the following contributions:(1) the introduction of
MaskSearch's graphical user interface (GUI), which enables interactive
exploration of image databases through mask properties, (2) hands-on
opportunities for users to explore MaskSearch's capabilities and constraints
within machine learning workflows, and (3) an opportunity for conference
attendees to understand how MaskSearch accelerates queries over image masks.
\\ ( https://arxiv.org/abs/2404.06563 ,  3844kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06589 (*cross-listing*)
Date: Tue, 9 Apr 2024 19:33:05 GMT   (540kb,D)

Title: Leveraging Latents for Efficient Thermography Classification and
  Segmentation
Authors: Tamir Shor, Chaim Baskin, Alex Bronstein
Categories: eess.IV cs.CV cs.LG
\\
  Breast cancer is a prominent health concern worldwide, currently being the
secondmost common and second-deadliest type of cancer in women. While current
breast cancer diagnosis mainly relies on mammography imaging, in recent years
the use of thermography for breast cancer imaging has been garnering growing
popularity. Thermographic imaging relies on infrared cameras to capture
body-emitted heat distributions. While these heat signatures have proven useful
for computer-vision systems for accurate breast cancer segmentation and
classification, prior work often relies on handcrafted feature engineering or
complex architectures, potentially limiting the comparability and applicability
of these methods. In this work, we present a novel algorithm for both breast
cancer classification and segmentation. Rather than focusing efforts on manual
feature and architecture engineering, our algorithm focuses on leveraging an
informative, learned feature space, thus making our solution simpler to use and
extend to other frameworks and downstream tasks, as well as more applicable to
data-scarce settings. Our classification produces SOTA results, while we are
the first work to produce segmentation regions studied in this paper.
\\ ( https://arxiv.org/abs/2404.06589 ,  540kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06691 (*cross-listing*)
Date: Wed, 10 Apr 2024 02:37:24 GMT   (9316kb)

Title: Latent Chemical Space Searching for Plug-in Multi-objective Molecule
  Generation
Authors: Ningfeng Liu (1 and 2), Jie Yu (1), Siyu Xiu (1), Xinfang Zhao (1),
  Siyu Lin (1), Bo Qiang (1), Ruqiu Zheng (1), Hongwei Jin (1), Liangren Zhang
  (1), Zhenming Liu (1 and 3) ((1) State Key Laboratory of Natural and
  Biomimetic Drugs, School of Pharmaceutical Sciences, Peking University, (2)
  Peking-Tsinghua Center for Life Science (CLS), Peking University, (3) State
  Key Laboratory of Pharmaceutical Biotechnology, Nanjing University)
Categories: q-bio.BM cs.LG cs.NE
\\
  Molecular generation, an essential method for identifying new drug
structures, has been supported by advancements in machine learning and
computational technology. However, challenges remain in multi-objective
generation, model adaptability, and practical application in drug discovery. In
this study, we developed a versatile 'plug-in' molecular generation model that
incorporates multiple objectives related to target affinity, drug-likeness, and
synthesizability, facilitating its application in various drug development
contexts. We improved the Particle Swarm Optimization (PSO) in the context of
drug discoveries, and identified PSO-ENP as the optimal variant for
multi-objective molecular generation and optimization through comparative
experiments. The model also incorporates a novel target-ligand affinity
predictor, enhancing the model's utility by supporting three-dimensional
information and improving synthetic feasibility. Case studies focused on
generating and optimizing drug-like big marine natural products were performed,
underscoring PSO-ENP's effectiveness and demonstrating its considerable
potential for practical drug discovery applications.
\\ ( https://arxiv.org/abs/2404.06691 ,  9316kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06720 (*cross-listing*)
Date: Wed, 10 Apr 2024 04:15:50 GMT   (77kb)

Title: Gradient Descent is Pareto-Optimal in the Oracle Complexity and Memory
  Tradeoff for Feasibility Problems
Authors: Moise Blanchard
Categories: math.OC cs.CC cs.DS cs.LG stat.ML
\\
  In this paper we provide oracle complexity lower bounds for finding a point
in a given set using a memory-constrained algorithm that has access to a
separation oracle. We assume that the set is contained within the unit
$d$-dimensional ball and contains a ball of known radius $\epsilon>0$. This
setup is commonly referred to as the feasibility problem. We show that to solve
feasibility problems with accuracy $\epsilon \geq e^{-d^{o(1)}}$, any
deterministic algorithm either uses $d^{1+\delta}$ bits of memory or must make
at least $1/(d^{0.01\delta }\epsilon^{2\frac{1-\delta}{1+1.01 \delta}-o(1)})$
oracle queries, for any $\delta\in[0,1]$. Additionally, we show that randomized
algorithms either use $d^{1+\delta}$ memory or make at least $1/(d^{2\delta}
\epsilon^{2(1-4\delta)-o(1)})$ queries for any $\delta\in[0,\frac{1}{4}]$.
Because gradient descent only uses linear memory $\mathcal O(d\ln 1/\epsilon)$
but makes $\Omega(1/\epsilon^2)$ queries, our results imply that it is
Pareto-optimal in the oracle complexity/memory tradeoff. Further, our results
show that the oracle complexity for deterministic algorithms is always
polynomial in $1/\epsilon$ if the algorithm has less than quadratic memory in
$d$. This reveals a sharp phase transition since with quadratic $\mathcal O(d^2
\ln1/\epsilon)$ memory, cutting plane methods only require $\mathcal O(d\ln
1/\epsilon)$ queries.
\\ ( https://arxiv.org/abs/2404.06720 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06732 (*cross-listing*)
Date: Wed, 10 Apr 2024 04:36:24 GMT   (27109kb,D)

Title: Enhancing Safety in Mixed Traffic: Learning-Based Modeling and Efficient
  Control of Autonomous and Human-Driven Vehicles
Authors: Jie Wang, Yash Vardhan Pant, Lei Zhao, Micha{\l} Antkiewicz, Krzysztof
  Czarnecki
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: in IEEE Transactions on Intelligent Transportation Systems, 2024
DOI: 10.1109/TITS.2024.3384050
\\
  With the increasing presence of autonomous vehicles (AVs) on public roads,
developing robust control strategies to navigate the uncertainty of
human-driven vehicles (HVs) is crucial. This paper introduces an advanced
method for modeling HV behavior, combining a first-principles model with
Gaussian process (GP) learning to enhance velocity prediction accuracy and
provide a measurable uncertainty. We validated this innovative HV model using
real-world data from field experiments and applied it to develop a GP-enhanced
model predictive control (GP-MPC) strategy. This strategy aims to improve
safety in mixed vehicle platoons by integrating uncertainty assessment into
distance constraints. Comparative simulation studies with a conventional model
predictive control (MPC) approach demonstrated that our GP-MPC strategy ensures
more reliable safe distancing and fosters efficient vehicular dynamics,
achieving notably higher speeds within the platoon. By incorporating a sparse
GP technique in HV modeling and adopting a dynamic GP prediction within the MPC
framework, we significantly reduced the computation time of GP-MPC, marking it
only 4.6% higher than that of the conventional MPC. This represents a
substantial improvement, making the process about 100 times faster than our
preliminary work without these approximations. Our findings underscore the
effectiveness of learning-based HV modeling in enhancing both safety and
operational efficiency in mixed-traffic environments, paving the way for more
harmonious AV-HV interactions.
\\ ( https://arxiv.org/abs/2404.06732 ,  27109kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06735 (*cross-listing*)
Date: Wed, 10 Apr 2024 04:49:00 GMT   (1350kb,D)

Title: A Copula Graphical Model for Multi-Attribute Data using Optimal
  Transport
Authors: Qi Zhang, Bing Li, Lingzhou Xue
Categories: stat.ML cs.LG math.ST stat.AP stat.ME stat.TH
Comments: 37 pages
\\
  Motivated by modern data forms such as images and multi-view data, the
multi-attribute graphical model aims to explore the conditional independence
structure among vectors. Under the Gaussian assumption, the conditional
independence between vectors is characterized by blockwise zeros in the
precision matrix. To relax the restrictive Gaussian assumption, in this paper,
we introduce a novel semiparametric multi-attribute graphical model based on a
new copula named Cyclically Monotone Copula. This new copula treats the
distribution of the node vectors as multivariate marginals and transforms them
into Gaussian distributions based on the optimal transport theory. Since the
model allows the node vectors to have arbitrary continuous distributions, it is
more flexible than the classical Gaussian copula method that performs
coordinatewise Gaussianization. We establish the concentration inequalities of
the estimated covariance matrices and provide sufficient conditions for
selection consistency of the group graphical lasso estimator. For the setting
with high-dimensional attributes, a {Projected Cyclically Monotone Copula}
model is proposed to address the curse of dimensionality issue that arises from
solving high-dimensional optimal transport problems. Numerical results based on
synthetic and real data show the efficiency and flexibility of our methods.
\\ ( https://arxiv.org/abs/2404.06735 ,  1350kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06818 (*cross-listing*)
Date: Wed, 10 Apr 2024 08:06:15 GMT   (21845kb,D)

Title: Towards Efficient and Real-Time Piano Transcription Using Neural
  Autoregressive Models
Authors: Taegyun Kwon, Dasaem Jeong and Juhan Nam
Categories: eess.AS cs.LG cs.SD
Comments: 11 pages, 8 figures, preprint
\\
  In recent years, advancements in neural network designs and the availability
of large-scale labeled datasets have led to significant improvements in the
accuracy of piano transcription models. However, most previous work focused on
high-performance offline transcription, neglecting deliberate consideration of
model size. The goal of this work is to implement real-time inference for piano
transcription while ensuring both high performance and lightweight. To this
end, we propose novel architectures for convolutional recurrent neural
networks, redesigning an existing autoregressive piano transcription model.
First, we extend the acoustic module by adding a frequency-conditioned FiLM
layer to the CNN module to adapt the convolutional filters on the frequency
axis. Second, we improve note-state sequence modeling by using a pitchwise LSTM
that focuses on note-state transitions within a note. In addition, we augment
the autoregressive connection with an enhanced recursive context. Using these
components, we propose two types of models; one for high performance and the
other for high compactness. Through extensive experiments, we show that the
proposed models are comparable to state-of-the-art models in terms of note
accuracy on the MAESTRO dataset. We also investigate the effective model size
and real-time inference latency by gradually streamlining the architecture.
Finally, we conduct cross-data evaluation on unseen piano datasets and in-depth
analysis to elucidate the effect of the proposed components in the view of note
length and pitch range.
\\ ( https://arxiv.org/abs/2404.06818 ,  21845kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06832 (*cross-listing*)
Date: Wed, 10 Apr 2024 08:48:09 GMT   (7769kb,D)

Title: SplatPose & Detect: Pose-Agnostic 3D Anomaly Detection
Authors: Mathis Kruse, Marco Rudolph, Dominik Woiwode, Bodo Rosenhahn
Categories: cs.CV cs.LG
Comments: Visual Anomaly and Novelty Detection 2.0 Workshop at CVPR 2024
\\
  Detecting anomalies in images has become a well-explored problem in both
academia and industry. State-of-the-art algorithms are able to detect defects
in increasingly difficult settings and data modalities. However, most current
methods are not suited to address 3D objects captured from differing poses.
While solutions using Neural Radiance Fields (NeRFs) have been proposed, they
suffer from excessive computation requirements, which hinder real-world
usability. For this reason, we propose the novel 3D Gaussian splatting-based
framework SplatPose which, given multi-view images of a 3D object, accurately
estimates the pose of unseen views in a differentiable manner, and detects
anomalies in them. We achieve state-of-the-art results in both training and
inference speed, and detection performance, even when using less training data
than competing methods. We thoroughly evaluate our framework using the recently
proposed Pose-agnostic Anomaly Detection benchmark and its multi-pose anomaly
detection (MAD) data set.
\\ ( https://arxiv.org/abs/2404.06832 ,  7769kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06834 (*cross-listing*)
Date: Wed, 10 Apr 2024 08:52:12 GMT   (602kb,D)

Title: Solving Parametric PDEs with Radial Basis Functions and Deep Neural
  Networks
Authors: Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng
Categories: math.NA cs.LG cs.NA
\\
  We propose the POD-DNN, a novel algorithm leveraging deep neural networks
(DNNs) along with radial basis functions (RBFs) in the context of the proper
orthogonal decomposition (POD) reduced basis method (RBM), aimed at
approximating the parametric mapping of parametric partial differential
equations on irregular domains. The POD-DNN algorithm capitalizes on the
low-dimensional characteristics of the solution manifold for parametric
equations, alongside the inherent offline-online computational strategy of RBM
and DNNs. In numerical experiments, POD-DNN demonstrates significantly
accelerated computation speeds during the online phase. Compared to other
algorithms that utilize RBF without integrating DNNs, POD-DNN substantially
improves the computational speed in the online inference process. Furthermore,
under reasonable assumptions, we have rigorously derived upper bounds on the
complexity of approximating parametric mappings with POD-DNN, thereby providing
a theoretical analysis of the algorithm's empirical performance.
\\ ( https://arxiv.org/abs/2404.06834 ,  602kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06856 (*cross-listing*)
Date: Wed, 10 Apr 2024 09:28:54 GMT   (596kb,D)

Title: Beyond Random Inputs: A Novel ML-Based Hardware Fuzzing
Authors: Mohamadreza Rostami, Marco Chilese, Shaza Zeitouni, Rahul Kande,
  Jeyavijayan Rajendran, Ahmad-Reza Sadeghi
Categories: cs.SE cs.AR cs.CR cs.LG
\\
  Modern computing systems heavily rely on hardware as the root of trust.
However, their increasing complexity has given rise to security-critical
vulnerabilities that cross-layer at-tacks can exploit. Traditional hardware
vulnerability detection methods, such as random regression and formal
verification, have limitations. Random regression, while scalable, is slow in
exploring hardware, and formal verification techniques are often concerned with
manual effort and state explosions. Hardware fuzzing has emerged as an
effective approach to exploring and detecting security vulnerabilities in
large-scale designs like modern processors. They outperform traditional methods
regarding coverage, scalability, and efficiency. However, state-of-the-art
fuzzers struggle to achieve comprehensive coverage of intricate hardware
designs within a practical timeframe, often falling short of a 70% coverage
threshold. We propose a novel ML-based hardware fuzzer, ChatFuzz, to address
this challenge. Ourapproach leverages LLMs like ChatGPT to understand processor
language, focusing on machine codes and generating assembly code sequences. RL
is integrated to guide the input generation process by rewarding the inputs
using code coverage metrics. We use the open-source RISCV-based RocketCore
processor as our testbed. ChatFuzz achieves condition coverage rate of 75% in
just 52 minutes compared to a state-of-the-art fuzzer, which requires a lengthy
30-hour window to reach a similar condition coverage. Furthermore, our fuzzer
can attain 80% coverage when provided with a limited pool of 10 simulation
instances/licenses within a 130-hour window. During this time, it conducted a
total of 199K test cases, of which 6K produced discrepancies with the
processor's golden model. Our analysis identified more than 10 unique
mismatches, including two new bugs in the RocketCore and discrepancies from the
RISC-V ISA Simulator.
\\ ( https://arxiv.org/abs/2404.06856 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06975 (*cross-listing*)
Date: Wed, 10 Apr 2024 13:49:20 GMT   (680kb,D)

Title: Multi-Agent Soft Actor-Critic with Global Loss for Autonomous
  Mobility-on-Demand Fleet Control
Authors: Zeno Woywood, Jasper I. Wiltfang, Julius Luy, Tobias Enders,
  Maximilian Schiffer
Categories: eess.SY cs.LG cs.MA cs.SY
\\
  We study a sequential decision-making problem for a profit-maximizing
operator of an Autonomous Mobility-on-Demand system. Optimizing a central
operator's vehicle-to-request dispatching policy requires efficient and
effective fleet control strategies. To this end, we employ a multi-agent Soft
Actor-Critic algorithm combined with weighted bipartite matching. We propose a
novel vehicle-based algorithm architecture and adapt the critic's loss function
to appropriately consider global actions. Furthermore, we extend our algorithm
to incorporate rebalancing capabilities. Through numerical experiments, we show
that our approach outperforms state-of-the-art benchmarks by up to 12.9% for
dispatching and up to 38.9% with integrated rebalancing.
\\ ( https://arxiv.org/abs/2404.06975 ,  680kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06978 (*cross-listing*)
Date: Wed, 10 Apr 2024 12:48:10 GMT   (3007kb,D)

Title: The CAST package for training and assessment of spatial prediction
  models in R
Authors: Hanna Meyer, Marvin Ludwig, Carles Mil\`a, Jan Linnenbrink, Fabian
  Schumacher
Categories: stat.ML cs.LG q-bio.QM
Comments: 16 pages,9 figures
\\
  One key task in environmental science is to map environmental variables
continuously in space or even in space and time. Machine learning algorithms
are frequently used to learn from local field observations to make spatial
predictions by estimating the value of the variable of interest in places where
it has not been measured. However, the application of machine learning
strategies for spatial mapping involves additional challenges compared to
"non-spatial" prediction tasks that often originate from spatial
autocorrelation and from training data that are not independent and identically
distributed.
  In the past few years, we developed a number of methods to support the
application of machine learning for spatial data which involves the development
of suitable cross-validation strategies for performance assessment and model
selection, spatial feature selection, and methods to assess the area of
applicability of the trained models. The intention of the CAST package is to
support the application of machine learning strategies for predictive mapping
by implementing such methods and making them available for easy integration
into modelling workflows.
  Here we introduce the CAST package and its core functionalities. At the case
study of mapping plant species richness, we will go through the different steps
of the modelling workflow and show how CAST can be used to support more
reliable spatial predictions.
\\ ( https://arxiv.org/abs/2404.06978 ,  3007kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06993 (*cross-listing*)
Date: Wed, 10 Apr 2024 13:12:07 GMT   (1529kb,D)

Title: Quiver Laplacians and Feature Selection
Authors: Otto Sumray, Heather A. Harrington, Vidit Nanda
Categories: stat.ML cs.LG math.CO math.RT math.ST q-bio.QM stat.TH
Comments: 40 pages, 7 figures
MSC-class: 16G20, 05C50, 62P05, 62H25
\\
  The challenge of selecting the most relevant features of a given dataset
arises ubiquitously in data analysis and dimensionality reduction. However,
features found to be of high importance for the entire dataset may not be
relevant to subsets of interest, and vice versa. Given a feature selector and a
fixed decomposition of the data into subsets, we describe a method for
identifying selected features which are compatible with the decomposition into
subsets. We achieve this by re-framing the problem of finding compatible
features to one of finding sections of a suitable quiver representation. In
order to approximate such sections, we then introduce a Laplacian operator for
quiver representations valued in Hilbert spaces. We provide explicit bounds on
how the spectrum of a quiver Laplacian changes when the representation and the
underlying quiver are modified in certain natural ways. Finally, we apply this
machinery to the study of peak-calling algorithms which measure chromatin
accessibility in single-cell data. We demonstrate that eigenvectors of the
associated quiver Laplacian yield locally and globally compatible features.
\\ ( https://arxiv.org/abs/2404.06993 ,  1529kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06997 (*cross-listing*)
Date: Wed, 10 Apr 2024 13:24:27 GMT   (5764kb,D)

Title: Agent-driven Generative Semantic Communication for Remote Surveillance
Authors: Wanting Yang, Zehui Xiong, Yanli Yuan, Wenchao Jiang, Tony Q.S. Quek,
  Merouane Debbah
Categories: cs.NI cs.LG
Comments: Under review with IEEE Transactions on Wireless Communications
\\
  In the era of 6G, featuring compelling visions of intelligent transportation
system, digital twins, remote surveillance is poised to become a ubiquitous
practice. The substantial data volume and frequent updates present challenges
in wireless networks. To address this, we propose a novel agent-driven
generative semantic communication (A-GSC) framework based on reinforcement
learning. In contrast to the existing research on semantic communication
(SemCom), which mainly focuses on semantic compression or semantic sampling, we
seamlessly cascade both together by jointly considering the intrinsic
attributes of source information and the contextual information regarding the
task. Notably, the introduction of the generative artificial intelligence (GAI)
enables the independent design of semantic encoders and decoders. In this work,
we develop an agent-assisted semantic encoder leveraging the knowledge based
soft actor-critic algorithm, which can track the semantic changes, channel
condition, and sampling intervals, so as to perform adaptive semantic sampling.
Accordingly, we design a semantic decoder with both predictive and generative
capabilities, which consists of two tailored modules. Moreover, the
effectiveness of the designed models has been verified based on the dataset
generated from CDNet2014, and the performance gain of the overall A-GSC
framework in both energy saving and reconstruction accuracy have been
demonstrated.
\\ ( https://arxiv.org/abs/2404.06997 ,  5764kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07110 (*cross-listing*)
Date: Wed, 10 Apr 2024 15:47:35 GMT   (11419kb,D)

Title: Wild Visual Navigation: Fast Traversability Learning via Pre-Trained
  Models and Online Self-Supervision
Authors: Mat\'ias Mattamala and Jonas Frey and Piotr Libera and Nived Chebrolu
  and Georg Martius and Cesar Cadena and Marco Hutter and Maurice Fallon
Categories: cs.RO cs.CV cs.LG
Comments: Extended version of arXiv:2305.08510
\\
  Natural environments such as forests and grasslands are challenging for
robotic navigation because of the false perception of rigid obstacles from high
grass, twigs, or bushes. In this work, we present Wild Visual Navigation (WVN),
an online self-supervised learning system for visual traversability estimation.
The system is able to continuously adapt from a short human demonstration in
the field, only using onboard sensing and computing. One of the key ideas to
achieve this is the use of high-dimensional features from pre-trained
self-supervised models, which implicitly encode semantic information that
massively simplifies the learning task. Further, the development of an online
scheme for supervision generator enables concurrent training and inference of
the learned model in the wild. We demonstrate our approach through diverse
real-world deployments in forests, parks, and grasslands. Our system is able to
bootstrap the traversable terrain segmentation in less than 5 min of in-field
training time, enabling the robot to navigate in complex, previously unseen
outdoor terrains. Code: https://bit.ly/498b0CV - Project
page:https://bit.ly/3M6nMHH
\\ ( https://arxiv.org/abs/2404.07110 ,  11419kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07159 (*cross-listing*)
Date: Wed, 10 Apr 2024 16:50:07 GMT   (3643kb,D)

Title: Exploring Physiological Responses in Virtual Reality-based Interventions
  for Autism Spectrum Disorder: A Data-Driven Investigation
Authors: Gianpaolo Alvari, Ersilia Vallefuoco, Melanie Cristofolini, Elio
  Salvadori, Marco Dianti, Alessia Moltani, Davide Dal Castello, Paola Venuti
  and Cesare Furlanello
Categories: cs.HC cs.LG
Comments: 19 pages, 6 figures
MSC-class: 92C30 (Primary) 92C55, 68T99 (Secondary)
\\
  Virtual Reality (VR) has emerged as a promising tool for enhancing social
skills and emotional well-being in individuals with Autism Spectrum Disorder
(ASD). Through a technical exploration, this study employs a multiplayer
serious gaming environment within VR, engaging 34 individuals diagnosed with
ASD and employing high-precision biosensors for a comprehensive view of the
participants' arousal and responses during the VR sessions. Participants were
subjected to a series of 3 virtual scenarios designed in collaboration with
stakeholders and clinical experts to promote socio-cognitive skills and
emotional regulation in a controlled and structured virtual environment. We
combined the framework with wearable non-invasive sensors for bio-signal
acquisition, focusing on the collection of heart rate variability, and
respiratory patterns to monitor participants behaviors. Further, behavioral
assessments were conducted using observation and semi-structured interviews,
with the data analyzed in conjunction with physiological measures to identify
correlations and explore digital-intervention efficacy. Preliminary analysis
revealed significant correlations between physiological responses and
behavioral outcomes, indicating the potential of physiological feedback to
enhance VR-based interventions for ASD. The study demonstrated the feasibility
of using real-time data to adapt virtual scenarios, suggesting a promising
avenue to support personalized therapy. The integration of quantitative
physiological feedback into digital platforms represents a forward step in the
personalized intervention for ASD. By leveraging real-time data to adjust
therapeutic content, this approach promises to enhance the efficacy and
engagement of digital-based therapies.
\\ ( https://arxiv.org/abs/2404.07159 ,  3643kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07181 (*cross-listing*)
Date: Wed, 10 Apr 2024 17:31:49 GMT   (4412kb,D)

Title: BAMBOO: a predictive and transferable machine learning force field
  framework for liquid electrolyte development
Authors: Sheng Gong, Yumin Zhang, Zhenliang Mu, Zhichen Pu, Hongyi Wang, Zhiao
  Yu, Mengyi Chen, Tianze Zheng, Zhi Wang, Lifei Chen, Xiaojie Wu, Shaochen
  Shi, Weihao Gao, Wen Yan, and Liang Xiang
Categories: cond-mat.mtrl-sci cs.LG physics.comp-ph
\\
  Despite the widespread applications of machine learning force field (MLFF) on
solids and small molecules, there is a notable gap in applying MLFF to complex
liquid electrolytes. In this work, we introduce BAMBOO (ByteDance AI Molecular
Simulation Booster), a novel framework for molecular dynamics (MD) simulations,
with a demonstration of its capabilities in the context of liquid electrolytes
for lithium batteries. We design a physics-inspired graph equivariant
transformer architecture as the backbone of BAMBOO to learn from quantum
mechanical simulations. Additionally, we pioneer an ensemble knowledge
distillation approach and apply it on MLFFs to improve the stability of MD
simulations. Finally, we propose the density alignment algorithm to align
BAMBOO with experimental measurements. BAMBOO demonstrates state-of-the-art
accuracy in predicting key electrolyte properties such as density, viscosity,
and ionic conductivity across various solvents and salt combinations. Our
current model, trained on more than 15 chemical species, achieves the average
density error of 0.01 g/cm^3 on various compositions compared with experimental
data. Moreover, our model demonstrates transferability to molecules not
included in the quantum mechanical dataset. We envision this work as paving the
way to a ''universal MLFF'' capable of simulating properties of common organic
liquids.
\\ ( https://arxiv.org/abs/2404.07181 ,  4412kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2106.15802
replaced with revised version Wed, 10 Apr 2024 14:11:50 GMT   (1981kb,D)

Title: CityNet: A Comprehensive Multi-Modal Urban Dataset for Advanced Research
  in Urban Computing
Authors: Zhengfei Zheng, Xu Geng, and Hai Yang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2106.15802 ,  1981kb)
------------------------------------------------------------------------------
\\
arXiv:2303.10538
replaced with revised version Wed, 10 Apr 2024 05:59:10 GMT   (1889kb,D)

Title: Unsupervised Learning for Solving the Travelling Salesman Problem
Authors: Yimeng Min, Yiwei Bai, Carla P. Gomes
Categories: cs.AI cs.LG
Comments: NeurIPS 2023 Camera-ready version fix typos in appendix
\\ ( https://arxiv.org/abs/2303.10538 ,  1889kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11884
replaced with revised version Wed, 10 Apr 2024 07:45:22 GMT   (82kb)

Title: YAGO 4.5: A Large and Clean Knowledge Base with a Rich Taxonomy
Authors: Fabian Suchanek, Mehwish Alam, Thomas Bonald, Lihu Chen, Pierre-Henri
  Paris, Jules Soria
Categories: cs.AI cs.IR
Comments: Published at SIGIR 2024, cite that paper in scientific articles
\\ ( https://arxiv.org/abs/2308.11884 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11055
replaced with revised version Wed, 10 Apr 2024 16:46:59 GMT   (1453kb)

Title: Designing Interpretable ML System to Enhance Trust in Healthcare: A
  Systematic Review to Proposed Responsible Clinician-AI-Collaboration
  Framework
Authors: Elham Nasarian, Roohallah Alizadehsani, U. Rajendra Acharya,
  Kwok-Leung Tsui
Categories: cs.AI cs.HC cs.LG
Comments: 42 pages (without appendixes and references) + 16 figures + 5 tables
DOI: 10.1016/j.inffus.2024.102412
\\ ( https://arxiv.org/abs/2311.11055 ,  1453kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04267
replaced with revised version Tue, 9 Apr 2024 20:06:17 GMT   (549kb)

Title: What AIs are not Learning (and Why): Bio-Inspired Foundation Models for
  Robots
Authors: Mark Stefik
Categories: cs.AI cs.HC
Comments: 13 pages
\\ ( https://arxiv.org/abs/2404.04267 ,  549kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06474
replaced with revised version Wed, 10 Apr 2024 04:55:54 GMT   (18451kb,D)

Title: Autonomous Evaluation and Refinement of Digital Agents
Authors: Jiayi Pan, Yichi Zhang, Nicholas Tomlin, Yifei Zhou, Sergey Levine,
  and Alane Suhr
Categories: cs.AI
Comments: Code at https://github.com/Berkeley-NLP/Agent-Eval-Refine
\\ ( https://arxiv.org/abs/2404.06474 ,  18451kb)
------------------------------------------------------------------------------
\\
arXiv:2210.13034
replaced with revised version Wed, 10 Apr 2024 02:16:55 GMT   (4977kb,D)

Title: Subspace Representations for Soft Set Operations and Sentence
  Similarities
Authors: Yoichi Ishibashi, Sho Yokoi, Katsuhito Sudoh, Satoshi Nakamura
Categories: cs.CL cs.LG
Comments: Accepted at NAACL 2024
\\ ( https://arxiv.org/abs/2210.13034 ,  4977kb)
------------------------------------------------------------------------------
\\
arXiv:2211.05985
replaced with revised version Wed, 10 Apr 2024 14:13:29 GMT   (145kb,D)

Title: Using Persuasive Writing Strategies to Explain and Detect Health
  Misinformation
Authors: Danial Kamali, Joseph Romain, Huiyi Liu, Wei Peng, Jingbo Meng, Parisa
  Kordjamshidi
Categories: cs.CL cs.AI cs.LG
Comments: Accepted at LREC-CoLING-2024
\\ ( https://arxiv.org/abs/2211.05985 ,  145kb)
------------------------------------------------------------------------------
\\
arXiv:2303.17513
replaced with revised version Wed, 10 Apr 2024 14:19:42 GMT   (38kb,D)

Title: Improving the Diproche CNL through Autoformalization via Large Language
  Models
Authors: Merlin Carl (Europa-Universit\"at Flensburg)
Categories: cs.CL cs.LO
Comments: In Proceedings ThEdu'23, arXiv:2404.03709
Journal-ref: EPTCS 400, 2024, pp. 44-58
DOI: 10.4204/EPTCS.400.4
\\ ( https://arxiv.org/abs/2303.17513 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2304.06186
replaced with revised version Wed, 10 Apr 2024 14:19:26 GMT   (38kb,D)

Title: Using Large Language Models for (De-)Formalization and Natural
  Argumentation Exercises for Beginner's Students
Authors: Merlin Carl (Europa-Universit\"at Flensburg)
Categories: cs.CL math.LO
Comments: In Proceedings ThEdu'23, arXiv:2404.03709
Journal-ref: EPTCS 400, 2024, pp. 28-43
DOI: 10.4204/EPTCS.400.3
\\ ( https://arxiv.org/abs/2304.06186 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2304.09820
replaced with revised version Wed, 10 Apr 2024 14:03:01 GMT   (7991kb,D)

Title: A Two-Stage Framework with Self-Supervised Distillation For Cross-Domain
  Text Classification
Authors: Yunlong Feng, Bohan Li, Libo Qin, Xiao Xu, Wanxiang Che
Categories: cs.CL cs.AI
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2304.09820 ,  7991kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08543
replaced with revised version Wed, 10 Apr 2024 02:30:19 GMT   (318kb,D)

Title: MiniLLM: Knowledge Distillation of Large Language Models
Authors: Yuxian Gu, Li Dong, Furu Wei, Minlie Huang
Categories: cs.CL cs.AI
Comments: Published as a conference paper in ICLR 2024
\\ ( https://arxiv.org/abs/2306.08543 ,  318kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08173
replaced with revised version Wed, 10 Apr 2024 13:24:55 GMT   (4725kb,D)

Title: FedJudge: Federated Legal Large Language Model
Authors: Linan Yue, Qi Liu, Yichao Du, Weibo Gao, Ye Liu, Fangzhou Yao
Categories: cs.CL
Comments: Accepted to DASFAA 2024
\\ ( https://arxiv.org/abs/2309.08173 ,  4725kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04562
replaced with revised version Tue, 9 Apr 2024 19:48:22 GMT   (1299kb,D)

Title: Towards Foundation Models for Knowledge Graph Reasoning
Authors: Mikhail Galkin, Xinyu Yuan, Hesham Mostafa, Jian Tang, Zhaocheng Zhu
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2310.04562 ,  1299kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05910
replaced with revised version Tue, 9 Apr 2024 23:21:45 GMT   (343kb,D)

Title: SALMON: Self-Alignment with Instructable Reward Models
Authors: Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen,
  David Cox, Yiming Yang, Chuang Gan
Categories: cs.CL cs.AI cs.LG
Comments: Previous Title: SALMON: Self-Alignment with Principle-Following
  Reward Models. Accepted to ICLR 2024. Project page:
  https://github.com/IBM/SALMON
\\ ( https://arxiv.org/abs/2310.05910 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07811
replaced with revised version Wed, 10 Apr 2024 15:38:33 GMT   (1017kb,D)

Title: In-context Learning Generalizes, But Not Always Robustly: The Case of
  Syntax
Authors: Aaron Mueller, Albert Webson, Jackson Petty, Tal Linzen
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.07811 ,  1017kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09761
replaced with revised version Tue, 9 Apr 2024 20:49:26 GMT   (1456kb,D)

Title: MAFALDA: A Benchmark and Comprehensive Study of Fallacy Detection and
  Classification
Authors: Chadi Helwe, Tom Calamai, Pierre-Henri Paris, Chlo\'e Clavel, Fabian
  Suchanek
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.09761 ,  1456kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01283
replaced with revised version Wed, 10 Apr 2024 13:05:42 GMT   (117kb,D)

Title: Quality and Quantity of Machine Translation References for Automatic
  Metrics
Authors: Vil\'em Zouhar, Ond\v{r}ej Bojar
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.01283 ,  117kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05125
replaced with revised version Wed, 10 Apr 2024 05:37:26 GMT   (1531kb,D)

Title: Zero-Shot Clinical Trial Patient Matching with LLMs
Authors: Michael Wornow, Alejandro Lozano, Dev Dash, Jenelle Jindal, Kenneth W.
  Mahaffey, Nigam H. Shah
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2402.05125 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00226
replaced with revised version Wed, 10 Apr 2024 07:15:51 GMT   (672kb,D)

Title: A Semantic Distance Metric Learning approach for Lexical Semantic Change
  Detection
Authors: Taichi Aida, Danushka Bollegala
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.00226 ,  672kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05326
replaced with revised version Wed, 10 Apr 2024 13:08:07 GMT   (2938kb,D)

Title: ChatASU: Evoking LLM's Reflexion to Truly Understand Aspect Sentiment in
  Dialogues
Authors: Yiding Liu and Jingjing Wang and Jiamin Luo and Tao Zeng and Guodong
  Zhou
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.05326 ,  2938kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08295
replaced with revised version Wed, 10 Apr 2024 12:32:33 GMT   (125kb,D)

Title: Gemma: Open Models Based on Gemini Research and Technology
Authors: Gemma Team: Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya
  Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi\`ere, Mihir Sanjay
  Kale, Juliette Love, Pouya Tafti, L\'eonard Hussenot, Pier Giuseppe Sessa,
  Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros,
  Ambrose Slone, Am\'elie H\'eliou, Andrea Tacchetti, Anna Bulanova, Antonia
  Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A.
  Choquette-Choo, Cl\'ement Crepy, Daniel Cer, Daphne Ippolito, David Reid,
  Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker,
  George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian
  Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski,
  Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan
  Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie
  Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej
  Miku{\l}a, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain,
  Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel,
  Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross
  McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan
  Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko,
  Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu-hui Chen, Zafarali Ahmed,
  Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl\'ement Farabet,
  Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin
  Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand
  Joulin, Noah Fiedel, Evan Senter, Alek Andreev, Kathleen Kenealy
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.08295 ,  125kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19335
replaced with revised version Tue, 9 Apr 2024 21:06:32 GMT   (795kb,D)

Title: KazSAnDRA: Kazakh Sentiment Analysis Dataset of Reviews and Attitudes
Authors: Rustem Yeshpanov, Huseyin Atakan Varol
Categories: cs.CL
Comments: To appear in Proceedings of the 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING
  2024)
\\ ( https://arxiv.org/abs/2403.19335 ,  795kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19399
replaced with revised version Tue, 9 Apr 2024 20:58:41 GMT   (54kb,D)

Title: KazParC: Kazakh Parallel Corpus for Machine Translation
Authors: Rustem Yeshpanov, Alina Polonskaya, Huseyin Atakan Varol
Categories: cs.CL
Comments: To appear in Proceedings of the 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING
  2024)
\\ ( https://arxiv.org/abs/2403.19399 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02772
replaced with revised version Wed, 10 Apr 2024 04:25:09 GMT   (624kb,D)

Title: FPT: Feature Prompt Tuning for Few-shot Readability Assessment
Authors: Ziyang Wang and Sanwoo Lee and Hsiu-Yuan Huang and Yunfang Wu
Categories: cs.CL
Comments: NAACL-2024 main conference
\\ ( https://arxiv.org/abs/2404.02772 ,  624kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06357
replaced with revised version Wed, 10 Apr 2024 07:48:08 GMT   (7349kb,D)

Title: Generalizable Sarcasm Detection Is Just Around The Corner, Of Course!
Authors: Hyewon Jang, Diego Frassinelli
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.06357 ,  7349kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06479
replaced with revised version Wed, 10 Apr 2024 02:12:27 GMT   (2755kb,D)

Title: Text-Based Reasoning About Vector Graphics
Authors: Zhenhailong Wang, Joy Hsu, Xingyao Wang, Kuan-Hao Huang, Manling Li,
  Jiajun Wu, Heng Ji
Categories: cs.CL cs.AI cs.CV
Comments: Project page: https://mikewangwzhl.github.io/VDLM/
\\ ( https://arxiv.org/abs/2404.06479 ,  2755kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06480
replaced with revised version Wed, 10 Apr 2024 07:40:56 GMT   (702kb,D)

Title: Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks
Authors: Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, Kai Chen
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2404.06480 ,  702kb)
------------------------------------------------------------------------------
\\
arXiv:2209.00905
replaced with revised version Tue, 9 Apr 2024 18:34:52 GMT   (9112kb,AD)

Title: From latent dynamics to meaningful representations
Authors: Dedi Wang, Yihang Wang, Luke Evans and Pratyush Tiwary
Categories: cs.LG cond-mat.dis-nn cond-mat.stat-mech physics.chem-ph physics.comp-ph
\\ ( https://arxiv.org/abs/2209.00905 ,  9112kb)
------------------------------------------------------------------------------
\\
arXiv:2209.15240
replaced with revised version Wed, 10 Apr 2024 09:04:26 GMT   (352kb,D)

Title: Universal Prompt Tuning for Graph Neural Networks
Authors: Taoran Fang, Yunchao Zhang, Yang Yang, Chunping Wang, Lei Chen
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2209.15240 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2210.12124
replaced with revised version Wed, 10 Apr 2024 11:45:10 GMT   (949kb,D)

Title: Equivariant Networks for Zero-Shot Coordination
Authors: Darius Muglich, Christian Schroeder de Witt, Elise van der Pol, Shimon
  Whiteson, Jakob Foerster
Categories: cs.LG
\\ ( https://arxiv.org/abs/2210.12124 ,  949kb)
------------------------------------------------------------------------------
\\
arXiv:2211.04175
replaced with revised version Wed, 10 Apr 2024 12:01:20 GMT   (5981kb,D)

Title: Enhancing Efficiency in Multidevice Federated Learning through Data
  Selection
Authors: Fan Mo, Mohammad Malekzadeh, Soumyajit Chatterjee, Fahim Kawsar, Akhil
  Mathur
Categories: cs.LG
Comments: Previous version (v3) was presented at ICLR 2023 Workshop on Machine
  Learning for IoT: Datasets, Perception, and Understanding
\\ ( https://arxiv.org/abs/2211.04175 ,  5981kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14855
replaced with revised version Wed, 10 Apr 2024 17:35:16 GMT   (21779kb,D)

Title: Disentangled Explanations of Neural Network Predictions by Finding
  Relevant Subspaces
Authors: Pattarawat Chormai, Jan Herrmann, Klaus-Robert M\"uller, Gr\'egoire
  Montavon
Categories: cs.LG cs.AI cs.CV
Comments: 17 pages + supplement
\\ ( https://arxiv.org/abs/2212.14855 ,  21779kb)
------------------------------------------------------------------------------
\\
arXiv:2301.12811
replaced with revised version Wed, 10 Apr 2024 04:03:06 GMT   (16545kb,D)

Title: SAN: Inducing Metrizability of GAN with Discriminative Normalized Linear
  Layer
Authors: Yuhta Takida, Masaaki Imaizumi, Takashi Shibuya, Chieh-Hsin Lai,
  Toshimitsu Uesaka, Naoki Murata, Yuki Mitsufuji
Categories: cs.LG
Comments: 34 pages with 17 figures, accepted for publication in ICLR 2024
\\ ( https://arxiv.org/abs/2301.12811 ,  16545kb)
------------------------------------------------------------------------------
\\
arXiv:2302.04406
replaced with revised version Wed, 10 Apr 2024 07:12:31 GMT   (16065kb,D)

Title: Neural Architecture Search via Two Constant Shared Weights
  Initialisations
Authors: Ekaterina Gracheva
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2302.04406 ,  16065kb)
------------------------------------------------------------------------------
\\
arXiv:2302.06670
replaced with revised version Tue, 9 Apr 2024 21:27:20 GMT   (6094kb,D)

Title: Explainable Anomaly Detection in Images and Videos: A Survey
Authors: Yizhou Wang, Dongliang Guo, Sheng Li, Octavia Camps, Yun Fu
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2302.06670 ,  6094kb)
------------------------------------------------------------------------------
\\
arXiv:2302.08070
replaced with revised version Wed, 10 Apr 2024 16:22:16 GMT   (336kb,D)

Title: Local Causal Discovery for Estimating Causal Effects
Authors: Shantanu Gupta, David Childers, Zachary C. Lipton
Categories: cs.LG stat.ME
Comments: Accepted at CLeaR 2023
\\ ( https://arxiv.org/abs/2302.08070 ,  336kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13425
replaced with revised version Wed, 10 Apr 2024 00:19:54 GMT   (9206kb,D)

Title: A Comprehensive Survey on Uncertainty Quantification for Deep Learning
Authors: Wenchong He and Zhe Jiang
Categories: cs.LG stat.ML
Comments: 39 pages, 14 figures
\\ ( https://arxiv.org/abs/2302.13425 ,  9206kb)
------------------------------------------------------------------------------
\\
arXiv:2303.12306
replaced with revised version Wed, 10 Apr 2024 02:32:52 GMT   (337kb,D)

Title: Understanding Expressivity of GNN in Rule Learning
Authors: Haiquan Qiu, Yongqi Zhang, Yong Li, Quanming Yao
Categories: cs.LG cs.AI
Comments: 24 pages, 6 figures
\\ ( https://arxiv.org/abs/2303.12306 ,  337kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16351
replaced with revised version Tue, 9 Apr 2024 23:55:28 GMT   (1567kb,D)

Title: Federated Learning Model Aggregation in Heterogenous Aerial and Space
  Networks
Authors: Fan Dong, Ali Abbasi, Steve Drew, Henry Leung, Xin Wang, Jiayu Zhou
Categories: cs.LG cs.AI cs.DC
Comments: 6 pages, 7 figures
ACM-class: I.2.11; C.2.4
\\ ( https://arxiv.org/abs/2305.16351 ,  1567kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13812
replaced with revised version Tue, 9 Apr 2024 21:01:56 GMT   (3103kb,D)

Title: Maintaining Plasticity in Deep Continual Learning
Authors: Shibhansh Dohare, J. Fernando Hernandez-Garcia, Parash Rahman, A.
  Rupam Mahmood, Richard S. Sutton
Categories: cs.LG
\\ ( https://arxiv.org/abs/2306.13812 ,  3103kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11732
replaced with revised version Tue, 9 Apr 2024 20:28:37 GMT   (1447kb,D)

Title: Advancing Ad Auction Realism: Practical Insights & Modeling Implications
Authors: Ming Chen, Sareh Nabi, Marciano Siniscalchi
Categories: cs.LG cs.GT econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2307.11732 ,  1447kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03103
replaced with revised version Wed, 10 Apr 2024 15:44:27 GMT   (498kb,D)

Title: Dual Prompt Tuning for Domain-Aware Federated Learning
Authors: Guoyizhe Wei, Feng Wang, Anshul Shah, Rama Chellappa
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.03103 ,  498kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17256
replaced with revised version Wed, 10 Apr 2024 11:22:51 GMT   (4553kb,D)

Title: fairret: a Framework for Differentiable Fairness Regularization Terms
Authors: Maarten Buyl, MaryBeth Defrance, Tijl De Bie
Categories: cs.LG
Comments: Presented at ICLR 2024
\\ ( https://arxiv.org/abs/2310.17256 ,  4553kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12624
replaced with revised version Wed, 10 Apr 2024 11:35:14 GMT   (17kb)

Title: Bridging Algorithmic Information Theory and Machine Learning: A New
  Approach to Kernel Learning
Authors: Boumediene Hamzi, Marcus Hutter, Houman Owhadi
Categories: cs.LG cs.IT math.IT stat.ML
Comments: An earlier version of this paper appeared at
  https://www.researchgate.net/publication/371875631_A_note_on_learning_kernels_from_data_from_an_Algorithmic_Information_Theoretic_point_of_view.
  arXiv admin note: text overlap with arXiv:2111.13037, arXiv:2007.05074
Journal-ref: Physica D: Nonlinear Phenomena, 2024, 134153
DOI: 10.13140/RG.2.2.36344.01285 10.1016/j.physd.2024.134153
\\ ( https://arxiv.org/abs/2311.12624 ,  17kb)
------------------------------------------------------------------------------
\\
arXiv:2311.15487
replaced with revised version Wed, 10 Apr 2024 16:55:52 GMT   (19kb)

Title: Global $\mathcal{L}^2$ minimization at uniform exponential rate via
  geometrically adapted gradient descent in Deep Learning
Authors: Thomas Chen
Categories: cs.LG cs.AI math-ph math.MP math.OC stat.ML
Comments: AMS Latex, 20 pages. Significantly edited and extended, abstract
  changed
MSC-class: 57R70, 62M45
\\ ( https://arxiv.org/abs/2311.15487 ,  19kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04610
replaced with revised version Tue, 9 Apr 2024 18:14:35 GMT   (1104kb)

Title: Data-driven Semi-supervised Machine Learning with Surrogate Safety
  Measures for Abnormal Driving Behavior Detection
Authors: Lanxin Zhang, Yongqi Dong, Haneen Farah, Arkady Zgonnikov, Bart van
  Arem
Categories: cs.LG cs.AI eess.SP stat.OT
Comments: 20 pages, 10 figures, accepted by the 103rd Transportation Research
  Board (TRB) Annual Meeting, under review by Transportation Research Record:
  Journal of the Transportation Research Board
\\ ( https://arxiv.org/abs/2312.04610 ,  1104kb)
------------------------------------------------------------------------------
\\
arXiv:2312.10144
replaced with revised version Wed, 10 Apr 2024 13:58:08 GMT   (917kb,D)

Title: Data-Efficient Multimodal Fusion on a Single GPU
Authors: No\"el Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin
  Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims
  Volkovs
Categories: cs.LG cs.AI cs.CV
Comments: CVPR 2024 (Highlight)
\\ ( https://arxiv.org/abs/2312.10144 ,  917kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00162
replaced with revised version Wed, 10 Apr 2024 13:32:06 GMT   (4098kb,D)

Title: Policy Optimization with Smooth Guidance Learned from State-Only
  Demonstrations
Authors: Guojian Wang, Faguo Wu, Xiao Zhang, Tianyuan Chen, Zhiming Zheng
Categories: cs.LG
Comments: 31 pages, 23 figures
\\ ( https://arxiv.org/abs/2401.00162 ,  4098kb)
------------------------------------------------------------------------------
\\
arXiv:2401.02225
replaced with revised version Wed, 10 Apr 2024 14:05:38 GMT   (1680kb,D)

Title: Trajectory-Oriented Policy Optimization with Sparse Rewards
Authors: Guojian Wang, Faguo Wu, Xiao Zhang
Categories: cs.LG
Comments: 6 pages, 7 figures
\\ ( https://arxiv.org/abs/2401.02225 ,  1680kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05737
replaced with revised version Wed, 10 Apr 2024 09:06:41 GMT   (3791kb,D)

Title: An experimental evaluation of Deep Reinforcement Learning algorithms for
  HVAC control
Authors: Antonio Manjavacas, Alejandro Campoy-Nieves, Javier Jim\'enez-Raboso,
  Miguel Molina-Solana, Juan G\'omez-Romero
Categories: cs.LG cs.SY eess.SY
Comments: Submitted to Artificial Intelligence Review. Under review
ACM-class: I.2.8; J.2
\\ ( https://arxiv.org/abs/2401.05737 ,  3791kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11563
replaced with revised version Tue, 9 Apr 2024 20:36:05 GMT   (1109kb,D)

Title: Distributed Multi-Task Learning for Stochastic Bandits with Context
  Distribution and Stage-wise Constraints
Authors: Jiabin Lin and Shana Moothedath
Categories: cs.LG cs.MA
\\ ( https://arxiv.org/abs/2401.11563 ,  1109kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02263
replaced with revised version Wed, 10 Apr 2024 09:00:44 GMT   (1738kb,D)

Title: MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly
  Mixed Classifiers
Authors: Yatong Bai, Mo Zhou, Vishal M. Patel, Somayeh Sojoudi
Categories: cs.LG cs.AI cs.CV
MSC-class: 68T07
\\ ( https://arxiv.org/abs/2402.02263 ,  1738kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05052
replaced with revised version Wed, 10 Apr 2024 02:08:29 GMT   (1105kb,D)

Title: Causal Representation Learning from Multiple Distributions: A General
  Setting
Authors: Kun Zhang, Shaoan Xie, Ignavier Ng, Yujia Zheng
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.05052 ,  1105kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05106
replaced with revised version Wed, 10 Apr 2024 17:39:53 GMT   (4286kb,D)

Title: Simulating Battery-Powered TinyML Systems Optimised using Reinforcement
  Learning in Image-Based Anomaly Detection
Authors: Jared M. Ping and Ken J. Nixon
Categories: cs.LG
Comments: Accepted as a full paper by the tinyML Research Symposium 2024
\\ ( https://arxiv.org/abs/2403.05106 ,  4286kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11782
replaced with revised version Wed, 10 Apr 2024 09:44:31 GMT   (2669kb,D)

Title: A tutorial on learning from preferences and choices with Gaussian
  Processes
Authors: Alessio Benavoli and Dario Azzimonti
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2403.11782 ,  2669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13728
replaced with revised version Wed, 10 Apr 2024 15:25:00 GMT   (12538kb)

Title: M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via
  Multiplier Induced Loss Landscape Scheduling
Authors: Xudong Sun, Nutan Chen, Alexej Gossmann, Yu Xing, Carla Feistner,
  Emilio Dorigatt, Felix Drost, Daniele Scarcella, Lisa Beer, Carsten Marr
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.13728 ,  12538kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18415
replaced with revised version Wed, 10 Apr 2024 09:24:16 GMT   (59kb)

Title: The Topos of Transformer Networks
Authors: Mattia Jacopo Villani and Peter McBurney
Categories: cs.LG math.CT
\\ ( https://arxiv.org/abs/2403.18415 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05728
replaced with revised version Wed, 10 Apr 2024 16:55:37 GMT   (66kb)

Title: A Large-Scale Exploration of $\mu$-Transfer
Authors: Lucas Lingle
Categories: cs.LG
Comments: Improved formatting and added comparison with SP
\\ ( https://arxiv.org/abs/2404.05728 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05768
replaced with revised version Wed, 10 Apr 2024 16:41:49 GMT   (1492kb,D)

Title: Streamlining Ocean Dynamics Modeling with Fourier Neural Operators: A
  Multiobjective Hyperparameter and Architecture Optimization Approach
Authors: Yixuan Sun, Ololade Sowunmi, Romain Egele, Sri Hari Krishna Narayanan,
  Luke Van Roekel, Prasanna Balaprakash
Categories: cs.LG physics.ao-ph stat.ML
\\ ( https://arxiv.org/abs/2404.05768 ,  1492kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06280
replaced with revised version Wed, 10 Apr 2024 16:30:07 GMT   (68kb)

Title: Algorithms for Caching and MTS with reduced number of predictions
Authors: Karim Abdel Sadek and Marek Elias
Categories: cs.LG cs.DS
\\ ( https://arxiv.org/abs/2404.06280 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2210.16101
replaced with revised version Wed, 10 Apr 2024 02:33:57 GMT   (10852kb,D)

Title: A Generic Shared Attention Mechanism for Various Backbone Neural
  Networks
Authors: Zhongzhan Huang, Senwei Liang, Mingfu Liang, Liang Lin
Categories: cs.CV cs.AI
Comments: Work in progress. arXiv admin note: text overlap with
  arXiv:1905.10671
\\ ( https://arxiv.org/abs/2210.16101 ,  10852kb)
------------------------------------------------------------------------------
\\
arXiv:2211.02736
replaced with revised version Wed, 10 Apr 2024 04:51:33 GMT   (2196kb,D)

Title: Discovering Closed-Loop Failures of Vision-Based Controllers via
  Reachability Analysis
Authors: Kaustav Chakraborty and Somil Bansal
Categories: cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY
Journal-ref: IEEE Robotics and Automation Letters 8.5 (2023): 2692-2699
DOI: 10.1109/LRA.2023.3258719
\\ ( https://arxiv.org/abs/2211.02736 ,  2196kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10035
replaced with revised version Wed, 10 Apr 2024 09:34:03 GMT   (6659kb,D)

Title: Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey
Authors: Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng Gao, Xiao-Yong Wei,
  Yaowei Wang, Yonghong Tian, Wen Gao
Categories: cs.CV cs.AI cs.MM
Comments: Accepted by Machine Intelligence Research (MIR)
\\ ( https://arxiv.org/abs/2302.10035 ,  6659kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01147
replaced with revised version Wed, 10 Apr 2024 04:09:44 GMT   (16575kb,D)

Title: Ripple Knowledge Graph Convolutional Networks For Recommendation Systems
Authors: Chen Li, Yang Cao, Ye Zhu, Debo Cheng, Chengyuan Li, Yasuhiko Morimoto
Categories: cs.IR cs.AI cs.LG
Journal-ref: Machine Intelligence Research, 2024
  (https://link.springer.com/article/10.1007/s11633-023-1440-x)
DOI: 10.1007/s11633-023-1440-x
\\ ( https://arxiv.org/abs/2305.01147 ,  16575kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10054
replaced with revised version Wed, 10 Apr 2024 12:08:54 GMT   (346kb)

Title: A Shift In Artistic Practices through Artificial Intelligence
Authors: K{\i}van\c{c} Tatar, Petter Ericson, Kelsey Cotton, Paola Torres
  N\'u\~nez del Prado, Roser Batlle-Roca, Beatriz Cabrero-Daniel, Sara
  Ljungblad, Georgios Diapoulis and Jabbar Hussain
Categories: cs.CY cs.AI cs.MM
Journal-ref: Leonardo, 2024, 293-97
DOI: 10.1162/leon_a_02523
\\ ( https://arxiv.org/abs/2306.10054 ,  346kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12220
replaced with revised version Wed, 10 Apr 2024 13:15:41 GMT   (1824kb)

Title: Expediting Building Footprint Extraction from High-resolution Remote
  Sensing Images via progressive lenient supervision
Authors: Haonan Guo, Bo Du, Chen Wu, Xin Su, Liangpei Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2307.12220 ,  1824kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12256
replaced with revised version Wed, 10 Apr 2024 13:43:54 GMT   (2617kb)

Title: Building-road Collaborative Extraction from Remotely Sensed Images via
  Cross-Interaction
Authors: Haonan Guo, Xin Su, Chen Wu, Bo Du, Liangpei Zhang
Categories: cs.CV cs.AI
Comments: IEEE Transactions on Geoscience and Remote Sensing
Journal-ref: IEEE Transactions on Geoscience and Remote Sensing
DOI: 10.1109/TGRS.2024.3383057
\\ ( https://arxiv.org/abs/2307.12256 ,  2617kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02003
replaced with revised version Wed, 10 Apr 2024 13:38:30 GMT   (7916kb,D)

Title: L2MAC: Large Language Model Automatic Computer for Extensive Code
  Generation
Authors: Samuel Holt, Max Ruiz Luyten, Mihaela van der Schaar
Categories: cs.SE cs.AI cs.LG cs.PL
Comments: Published in The Twelfth International Conference on Learning
  Representations (ICLR), 2024. Copyright 2023 by the author(s)
ACM-class: I.2.7; I.2.6; I.2.5; D.2.2; D.2.3; D.3.4
\\ ( https://arxiv.org/abs/2310.02003 ,  7916kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00825
replaced with revised version Tue, 9 Apr 2024 23:28:49 GMT   (41148kb,D)

Title: SocialCounterfactuals: Probing and Mitigating Intersectional Social
  Biases in Vision-Language Models with Counterfactual Examples
Authors: Phillip Howard, Avinash Madasu, Tiep Le, Gustavo Lujan Moreno, Anahita
  Bhiwandiwalla, and Vasudev Lal
Categories: cs.CV cs.AI
Comments: Accepted to CVPR 2024. arXiv admin note: text overlap with
  arXiv:2310.02988
\\ ( https://arxiv.org/abs/2312.00825 ,  41148kb)
------------------------------------------------------------------------------
\\
arXiv:2312.04746
replaced with revised version Tue, 9 Apr 2024 21:48:42 GMT   (21487kb,D)

Title: Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized
  Narratives from Open-Source Histopathology Videos
Authors: Mehmet Saygin Seyfioglu, Wisdom O. Ikezogwo, Fatemeh Ghezloo, Ranjay
  Krishna, Linda Shapiro
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2312.04746 ,  21487kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08604
replaced with revised version Wed, 10 Apr 2024 03:29:32 GMT   (903kb,D)

Title: Verification of Neural Reachable Tubes via Scenario Optimization and
  Conformal Prediction
Authors: Albert Lin and Somil Bansal
Categories: cs.RO cs.AI cs.LG cs.SY eess.SY
Comments: Accepted to 6th Annual Learning for Dynamics & Control Conference.
  arXiv admin note: text overlap with arXiv:2209.12336
\\ ( https://arxiv.org/abs/2312.08604 ,  903kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10831
replaced with revised version Wed, 10 Apr 2024 15:19:07 GMT   (23960kb,D)

Title: Understanding Video Transformers via Universal Concept Discovery
Authors: Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos
  G. Derpanis, Pavel Tokmakov
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: CVPR 2024 (Highlight)
\\ ( https://arxiv.org/abs/2401.10831 ,  23960kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13138
replaced with revised version Wed, 10 Apr 2024 13:57:06 GMT   (432kb,D)

Title: Visibility into AI Agents
Authors: Alan Chan, Carson Ezell, Max Kaufmann, Kevin Wei, Lewis Hammond,
  Herbie Bradley, Emma Bluemke, Nitarshan Rajkumar, David Krueger, Noam Kolt,
  Lennart Heim, Markus Anderljung
Categories: cs.CY cs.AI
Comments: Accepted to ACM Conference on Fairness, Accountability, and
  Transparency (ACM FAccT 2024)
\\ ( https://arxiv.org/abs/2401.13138 ,  432kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11319
replaced with revised version Wed, 10 Apr 2024 08:31:08 GMT   (13118kb,D)

Title: Hysteresis Compensation of Flexible Continuum Manipulator using RGBD
  Sensing and Temporal Convolutional Network
Authors: Junhyun Park, Seonghyeok Jang, Hyojae Park, Seongjun Bae, Minho Hwang
Categories: cs.RO cs.AI
Comments: 8 pages, 11 figures, 5 tables
\\ ( https://arxiv.org/abs/2402.11319 ,  13118kb)
------------------------------------------------------------------------------
\\
arXiv:2402.18320
replaced with revised version Wed, 10 Apr 2024 15:09:22 GMT   (14613kb,D)

Title: Location-guided Head Pose Estimation for Fisheye Image
Authors: Bing Li, Dong Zhang, Cheng Huang, Yun Xian, Ming Li, and Dah-Jye Lee
Categories: cs.CV cs.AI
Comments: Revised Introduction and Related Work; Submitted to lEEE Transactions
  on Cognitive and Developmental Systems for review
\\ ( https://arxiv.org/abs/2402.18320 ,  14613kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05916
replaced with revised version Wed, 10 Apr 2024 07:58:44 GMT   (4941kb,D)

Title: GPT as Psychologist? Preliminary Evaluations for GPT-4V on Visual
  Affective Computing
Authors: Hao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingyong Hu, Jiaqi Tang,
  Yuting Zhang, Kaishen Yuan, Bin Huang, Zitong Yu, Dengbo He, Shuiguang Deng,
  Hao Chen, Yingcong Chen, Shiguang Shan
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.05916 ,  4941kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08551 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 07:58:04 GMT   (1412kb,D)

Title: GaussianImage: 1000 FPS Image Representation and Compression by 2D
  Gaussian Splatting
Authors: Xinjie Zhang, Xingtong Ge, Tongda Xu, Dailan He, Yan Wang, Hongwei
  Qin, Guo Lu, Jing Geng, Jun Zhang
Categories: eess.IV cs.AI cs.CV cs.MM
\\ ( https://arxiv.org/abs/2403.08551 ,  1412kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14274
replaced with revised version Wed, 10 Apr 2024 08:53:13 GMT   (632kb,D)

Title: Multi-role Consensus through LLMs Discussions for Vulnerability
  Detection
Authors: Zhenyu Mao, Jialong Li, Munan Li, Kenji Tei
Categories: cs.SE cs.AI
\\ ( https://arxiv.org/abs/2403.14274 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05101 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 21:01:59 GMT   (1062kb,D)

Title: StockGPT: A GenAI Model for Stock Prediction and Trading
Authors: Dat Mai
Categories: q-fin.CP cs.AI q-fin.PM q-fin.PR q-fin.ST
Comments: 19 pages, 3 figures, 7 tables
\\ ( https://arxiv.org/abs/2404.05101 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05777
replaced with revised version Wed, 10 Apr 2024 08:23:48 GMT   (15868kb,D)

Title: IA2: Leveraging Instance-Aware Index Advisor with Reinforcement Learning
  for Diverse Workloads
Authors: Taiyi Wang, Eiko Yoneki
Categories: cs.DB cs.AI
Comments: EuroMLSys 24, April 22, 2024, Athens, Greece
DOI: 10.1145/3642970.3655839
\\ ( https://arxiv.org/abs/2404.05777 ,  15868kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06079 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 00:33:25 GMT   (1015kb,D)

Title: The X-LANCE Technical Report for Interspeech 2024 Speech Processing
  Using Discrete Speech Unit Challenge
Authors: Yiwei Guo, Chenrun Wang, Yifan Yang, Hankun Wang, Ziyang Ma, Chenpeng
  Du, Shuai Wang, Hanzheng Li, Shuai Fan, Hui Zhang, Xie Chen, Kai Yu
Categories: eess.AS cs.AI
Comments: 5 pages, 3 figures. Report of a challenge
\\ ( https://arxiv.org/abs/2404.06079 ,  1015kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06393
replaced with revised version Wed, 10 Apr 2024 15:09:52 GMT   (1547kb,D)

Title: MuPT: A Generative Symbolic Music Pretrained Transformer
Authors: Xingwei Qu, Yuelin Bai, Yinghao Ma, Ziya Zhou, Ka Man Lo, Jiaheng Liu,
  Ruibin Yuan, Lejun Min, Xueling Liu, Tianyu Zhang, Xinrun Du, Shuyue Guo,
  Yiming Liang, Yizhi Li, Shangda Wu, Junting Zhou, Tianyu Zheng, Ziyang Ma,
  Fengze Han, Wei Xue, Gus Xia, Emmanouil Benetos, Xiang Yue, Chenghua Lin, Xu
  Tan, Stephen W. Huang, Wenhu Chen, Jie Fu, and Ge Zhang
Categories: cs.SD cs.AI eess.AS
\\ ( https://arxiv.org/abs/2404.06393 ,  1547kb)
------------------------------------------------------------------------------
\\
arXiv:2404.06484
replaced with revised version Wed, 10 Apr 2024 15:12:32 GMT   (34kb,D)

Title: Public-private funding models in open source software development: A
  case study on scikit-learn
Authors: Cailean Osborne
Categories: cs.SE cs.AI cs.CY cs.LG
Comments: 18 pages
ACM-class: K.4.1
\\ ( https://arxiv.org/abs/2404.06484 ,  34kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14279
replaced with revised version Wed, 10 Apr 2024 14:25:30 GMT   (415kb,D)

Title: Characterizing and Classifying Developer Forum Posts with their
  Intentions
Authors: Xingfang Wu, Eric Laufer, Heng Li, Foutse Khomh, Santhosh Srinivasan,
  Jayden Luo
Categories: cs.SE cs.CL cs.LG
Comments: Journal of Empirical Software Engineering, 40 pages
\\ ( https://arxiv.org/abs/2312.14279 ,  415kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05195
replaced with revised version Tue, 9 Apr 2024 22:14:37 GMT   (36356kb,D)

Title: $\lambda$-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion
  Models by Leveraging CLIP Latent Space
Authors: Maitreya Patel, Sangmin Jung, Chitta Baral, Yezhou Yang
Categories: cs.CV cs.CL
Comments: Project page: https://eclipse-t2i.github.io/Lambda-ECLIPSE/
\\ ( https://arxiv.org/abs/2402.05195 ,  36356kb)
------------------------------------------------------------------------------
\\
arXiv:2404.04125
replaced with revised version Mon, 8 Apr 2024 21:14:43 GMT   (37863kb,D)

Title: No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency
  Determines Multimodal Model Performance
Authors: Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip
  H.S. Torr, Adel Bibi, Samuel Albanie, Matthias Bethge
Categories: cs.CV cs.CL cs.LG
Comments: Extended version of the short paper accepted at DPFM, ICLR'24
\\ ( https://arxiv.org/abs/2404.04125 ,  37863kb)
------------------------------------------------------------------------------
\\
arXiv:2106.07243 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 03:50:54 GMT   (1313kb)

Title: Compressed Gradient Tracking for Decentralized Optimization Over General
  Directed Networks
Authors: Zhuoqing Song, Lei Shi, Shi Pu, Ming Yan
Categories: math.OC cs.DC cs.LG cs.MA eess.SP
Journal-ref: IEEE Transactions on Signal Processing, 70(2022), 1775-1787
DOI: 10.1109/TSP.2022.3160238
\\ ( https://arxiv.org/abs/2106.07243 ,  1313kb)
------------------------------------------------------------------------------
\\
arXiv:2208.12553
replaced with revised version Wed, 10 Apr 2024 14:16:11 GMT   (69kb,D)

Title: I still know it's you! On Challenges in Anonymizing Source Code
Authors: Micha Horlboge, Erwin Quiring, Roland Meyer, Konrad Rieck
Categories: cs.CR cs.LG cs.PL cs.SE
\\ ( https://arxiv.org/abs/2208.12553 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2212.11120
replaced with revised version Wed, 10 Apr 2024 17:15:23 GMT   (7776kb,D)

Title: Deep Learning for Inertial Sensor Alignment
Authors: Maxim Freydin, Niv Sfaradi, Nimrod Segol, Areej Eweida, and Barak Or
Categories: cs.CV cs.LG cs.RO
Comments: 9 Pages, Preprint. Accepted IEEE
DOI: 10.1109/JSEN.2024.3384302
\\ ( https://arxiv.org/abs/2212.11120 ,  7776kb)
------------------------------------------------------------------------------
\\
arXiv:2301.04218
replaced with revised version Wed, 10 Apr 2024 01:11:15 GMT   (13309kb,D)

Title: Leveraging Diffusion For Strong and High Quality Face Morphing Attacks
Authors: Zander W. Blasingame and Chen Liu
Categories: cs.CV cs.CR cs.LG
Comments: Diffusion Morphs (DiM) paper. Accepted in IEEE TBIOM
Journal-ref: IEEE Transactions on Biometrics, Behavior, and Identity Science (
  Volume: 6, Issue: 1, January 2024)
DOI: 10.1109/TBIOM.2024.3349857
\\ ( https://arxiv.org/abs/2301.04218 ,  13309kb)
------------------------------------------------------------------------------
\\
arXiv:2303.03084 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 14:52:19 GMT   (572kb,D)

Title: On Regression in Extreme Regions
Authors: Nathan Huet, Stephan Cl\'emen\c{c}on, Anne Sabourin
Categories: stat.ML cs.LG math.ST stat.TH
Comments: 16 pages (main paper), 13 pages (appendix)
\\ ( https://arxiv.org/abs/2303.03084 ,  572kb)
------------------------------------------------------------------------------
\\
arXiv:2306.08745
replaced with revised version Wed, 10 Apr 2024 14:30:58 GMT   (2569kb,D)

Title: PLAN: Variance-Aware Private Mean Estimation
Authors: Martin Aum\"uller, Christian Janos Lebeda, Boel Nelson, Rasmus Pagh
Categories: cs.CR cs.DS cs.LG
\\ ( https://arxiv.org/abs/2306.08745 ,  2569kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10798
replaced with revised version Wed, 10 Apr 2024 11:42:22 GMT   (15765kb,D)

Title: ExpPoint-MAE: Better interpretability and performance for
  self-supervised point cloud transformers
Authors: Ioannis Romanelis and Vlassis Fotis and Konstantinos Moustakas and
  Adrian Munteanu
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2306.10798 ,  15765kb)
------------------------------------------------------------------------------
\\
arXiv:2309.05102 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 15:02:35 GMT   (11kb)

Title: Is Learning in Biological Neural Networks based on Stochastic Gradient
  Descent? An analysis using stochastic processes
Authors: S\"oren Christensen and Jan Kallsen
Categories: q-bio.NC cs.LG cs.NE math.PR
MSC-class: 92C20, 68T07
\\ ( https://arxiv.org/abs/2309.05102 ,  11kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02265
replaced with revised version Wed, 10 Apr 2024 12:54:12 GMT   (26280kb,D)

Title: DREAM: Visual Decoding from Reversing Human Visual System
Authors: Weihao Xia, Raoul de Charette, Cengiz \"Oztireli, Jing-Hao Xue
Categories: cs.CV cs.LG eess.IV q-bio.NC
Comments: Project Page: https://weihaox.github.io/DREAM
\\ ( https://arxiv.org/abs/2310.02265 ,  26280kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07672 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 00:35:36 GMT   (590kb,D)

Title: Stabilizing Estimates of Shapley Values with Control Variates
Authors: Jeremy Goldwasser and Giles Hooker
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2310.07672 ,  590kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09920
replaced with revised version Wed, 10 Apr 2024 05:39:23 GMT   (12245kb,D)

Title: BONES: Near-Optimal Neural-Enhanced Video Streaming
Authors: Lingdong Wang, Simran Singh, Jacob Chakareski, Mohammad Hajiesmaili,
  Ramesh K. Sitaraman
Categories: eess.SY cs.LG cs.NI cs.SY
\\ ( https://arxiv.org/abs/2310.09920 ,  12245kb)
------------------------------------------------------------------------------
\\
arXiv:2310.11230 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 02:35:38 GMT   (317kb,D)

Title: Zipformer: A faster and better encoder for automatic speech recognition
Authors: Zengwei Yao, Liyong Guo, Xiaoyu Yang, Wei Kang, Fangjun Kuang, Yifan
  Yang, Zengrui Jin, Long Lin, Daniel Povey
Categories: eess.AS cs.LG cs.SD
Comments: Published as a conference paper at ICLR 2024
\\ ( https://arxiv.org/abs/2310.11230 ,  317kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17540
replaced with revised version Tue, 9 Apr 2024 23:39:23 GMT   (3382kb,D)

Title: EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality
  for Autonomous Driving
Authors: Yuping Wang, Jier Chen
Categories: cs.RO cs.LG
Comments: 6 pages, 7 figures, Accepted 2024 International Conference on
  Robotics and Automation
\\ ( https://arxiv.org/abs/2310.17540 ,  3382kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01888 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 18:23:58 GMT   (1995kb,D)

Title: Learning Sparse Codes with Entropy-Based ELBOs
Authors: Dmytro Velychko, Simon Damm, Asja Fischer and J\"org L\"ucke
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2311.01888 ,  1995kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07474 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 21:13:46 GMT   (716kb,D)

Title: A Federated Data Fusion-Based Prognostic Model for Applications with
  Multi-Stream Incomplete Signals
Authors: Madi Arabi and Xiaolei Fang
Categories: stat.ML cs.LG eess.SP stat.ME
\\ ( https://arxiv.org/abs/2311.07474 ,  716kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12796
replaced with revised version Wed, 10 Apr 2024 10:37:22 GMT   (9264kb,D)

Title: Physics-guided Shape-from-Template: Monocular Video Perception through
  Neural Surrogate Models
Authors: David Stotko, Nils Wandel, Reinhard Klein
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.12796 ,  9264kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06275
replaced with revised version Wed, 10 Apr 2024 11:49:05 GMT   (551kb,D)

Title: DG-TTA: Out-of-domain medical image segmentation through Domain
  Generalization and Test-Time Adaptation
Authors: Christian Weihsbach, Christian N. Kruse, Alexander Bigalke, Mattias P.
  Heinrich
Categories: cs.CV cs.LG
MSC-class: 92C55 (Primary), 68T07 (Secondary)
ACM-class: I.2.6; I.4.6
\\ ( https://arxiv.org/abs/2312.06275 ,  551kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12610 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 20:42:32 GMT   (12577kb,D)

Title: Enhancing predictive capabilities in fusion burning plasmas through
  surrogate-based optimization in core transport solvers
Authors: P. Rodriguez-Fernandez, N.T. Howard, A. Saltzman, S. Kantamneni, J.
  Candy, C. Holland, M. Balandat, S. Ament and A.E. White
Categories: physics.plasm-ph cs.LG physics.comp-ph
\\ ( https://arxiv.org/abs/2312.12610 ,  12577kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03893
replaced with revised version Wed, 10 Apr 2024 13:34:24 GMT   (15228kb,D)

Title: Prediction Horizon Requirements for Automated Driving: Optimizing
  Safety, Comfort, and Efficiency
Authors: Manuel Mu\~noz S\'anchez, Chris van der Ploeg, Robin Smit, Jos
  Elfring, Emilia Silvas, Ren\'e van de Molengraft
Categories: cs.RO cs.LG
Comments: Submitted to IEEE Intelligent Vehicles Symposium. 9 pages. 10
  figures. 6 tables
\\ ( https://arxiv.org/abs/2402.03893 ,  15228kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04022 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 07:24:59 GMT   (2326kb,D)

Title: A General Theory for Kernel Packets: from state space model to compactly
  supported basis
Authors: Liang Ding and Rui Tuo
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2402.04022 ,  2326kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16543 (*cross-listing*)
replaced with revised version Wed, 10 Apr 2024 12:01:43 GMT   (4017kb,D)

Title: Model-based deep reinforcement learning for accelerated learning from
  flow simulations
Authors: Andre Weiner, Janis Geise
Categories: physics.flu-dyn cs.CE cs.LG
\\ ( https://arxiv.org/abs/2402.16543 ,  4017kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01102 (*cross-listing*)
replaced with revised version Tue, 9 Apr 2024 19:26:36 GMT   (1226kb,D)

Title: Diffusion based Zero-shot Medical Image-to-Image Translation for Cross
  Modality Segmentation
Authors: Zihao Wang, Yingyu Yang, Yuzhou Chen, Tingting Yuan, Maxime Sermesant,
  Herve Delingette, Ona Wu
Categories: eess.IV cs.CV cs.LG
Comments: Neurips 2023 Diffusion Workshop
\\ ( https://arxiv.org/abs/2404.01102 ,  1226kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03729
replaced with revised version Tue, 9 Apr 2024 22:53:57 GMT   (17894kb,D)

Title: JUICER: Data-Efficient Imitation Learning for Robotic Assembly
Authors: Lars Ankile and Anthony Simeonov and Idan Shenfeld and Pulkit Agrawal
Categories: cs.RO cs.LG
Comments: Project website: https://imitation-juicer.github.io/
\\ ( https://arxiv.org/abs/2404.03729 ,  17894kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
