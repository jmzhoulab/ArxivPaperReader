Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年4月4日 12:22
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Tue  2 Apr 24 18:00:00 GMT  to  Wed  3 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.02454
Date: Wed, 3 Apr 2024 04:50:43 GMT   (28kb)

Title: Techniques for Measuring the Inferential Strength of Forgetting Policies
Authors: Patrick Doherty and Andrzej Szalas
Categories: cs.AI cs.LO
\\
  The technique of forgetting in knowledge representation has been shown to be
a powerful and useful knowledge engineering tool with widespread application.
Yet, very little research has been done on how different policies of
forgetting, or use of different forgetting operators, affects the inferential
strength of the original theory. The goal of this paper is to define loss
functions for measuring changes in inferential strength based on intuitions
from model counting and probability theory. Properties of such loss measures
are studied and a pragmatic knowledge engineering tool is proposed for
computing loss measures using Problog. The paper includes a working methodology
for studying and determining the strength of different forgetting policies, in
addition to concrete examples showing how to apply the theoretical results
using Problog. Although the focus is on forgetting, the results are much more
general and should have wider application to other areas.
\\ ( https://arxiv.org/abs/2404.02454 ,  28kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02499
Date: Wed, 3 Apr 2024 06:25:42 GMT   (301kb,D)

Title: Learning Generalized Policies for Fully Observable Non-Deterministic
  Planning Domains
Authors: Till Hofmann, Hector Geffner
Categories: cs.AI cs.LG
\\
  General policies represent reactive strategies for solving large families of
planning problems like the infinite collection of solvable instances from a
given domain. Methods for learning such policies from a collection of small
training instances have been developed successfully for classical domains. In
this work, we extend the formulations and the resulting combinatorial methods
for learning general policies over fully observable, non-deterministic (FOND)
domains. We also evaluate the resulting approach experimentally over a number
of benchmark domains in FOND planning, present the general policies that result
in some of these domains, and prove their correctness. The method for learning
general policies for FOND planning can actually be seen as an alternative FOND
planning method that searches for solutions, not in the given state space but
in an abstract space defined by features that must be learned as well.
\\ ( https://arxiv.org/abs/2404.02499 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02532
Date: Wed, 3 Apr 2024 07:43:11 GMT   (798kb,D)

Title: Learn to Disguise: Avoid Refusal Responses in LLM's Defense via a
  Multi-agent Attacker-Disguiser Game
Authors: Qianqiao Xu, Zhiliang Tian, Hongyan Wu, Zhen Huang, Yiping Song, Feng
  Liu, Dongsheng Li
Categories: cs.AI cs.CL
Comments: 13 pages, 2 figures
\\
  With the enhanced performance of large models on natural language processing
tasks, potential moral and ethical issues of large models arise. There exist
malicious attackers who induce large models to jailbreak and generate
information containing illegal, privacy-invasive information through techniques
such as prompt engineering. As a result, large models counter malicious
attackers' attacks using techniques such as safety alignment. However, the
strong defense mechanism of the large model through rejection replies is easily
identified by attackers and used to strengthen attackers' capabilities. In this
paper, we propose a multi-agent attacker-disguiser game approach to achieve a
weak defense mechanism that allows the large model to both safely reply to the
attacker and hide the defense intent. First, we construct a multi-agent
framework to simulate attack and defense scenarios, playing different roles to
be responsible for attack, disguise, safety evaluation, and disguise evaluation
tasks. After that, we design attack and disguise game algorithms to optimize
the game strategies of the attacker and the disguiser and use the curriculum
learning process to strengthen the capabilities of the agents. The experiments
verify that the method in this paper is more effective in strengthening the
model's ability to disguise the defense intent compared with other methods.
Moreover, our approach can adapt any black-box large model to assist the model
in defense and does not suffer from model version iterations.
\\ ( https://arxiv.org/abs/2404.02532 ,  798kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02579
Date: Wed, 3 Apr 2024 08:54:58 GMT   (7375kb,D)

Title: Learning Alternative Ways of Performing a Task
Authors: David Nieves, Mar\'ia Jos\'e Ram\'irez-Quintana, Carlos Monserrat,
  C\'esar Ferri, Jos\'e Hern\'andez-Orallo
Categories: cs.AI
Comments: 32 pages, Github repository, published paper, authors' version
ACM-class: I.2.6; I.5.4
Journal-ref: Expert Systems With Applications, volume 148, 2020, 113263
DOI: 10.1016/j.eswa.2020.113263
\\
  A common way of learning to perform a task is to observe how it is carried
out by experts. However, it is well known that for most tasks there is no
unique way to perform them. This is especially noticeable the more complex the
task is because factors such as the skill or the know-how of the expert may
well affect the way she solves the task. In addition, learning from experts
also suffers of having a small set of training examples generally coming from
several experts (since experts are usually a limited and expensive resource),
being all of them positive examples (i.e. examples that represent successful
executions of the task). Traditional machine learning techniques are not useful
in such scenarios, as they require extensive training data. Starting from very
few executions of the task presented as activity sequences, we introduce a
novel inductive approach for learning multiple models, with each one
representing an alternative strategy of performing a task. By an iterative
process based on generalisation and specialisation, we learn the underlying
patterns that capture the different styles of performing a task exhibited by
the examples. We illustrate our approach on two common activity recognition
tasks: a surgical skills training task and a cooking domain. We evaluate the
inferred models with respect to two metrics that measure how well the models
represent the examples and capture the different forms of executing a task
showed by the examples. We compare our results with the traditional process
mining approach and show that a small set of meaningful examples is enough to
obtain patterns that capture the different strategies that are followed to
solve the tasks.
\\ ( https://arxiv.org/abs/2404.02579 ,  7375kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02611
Date: Wed, 3 Apr 2024 09:56:38 GMT   (533kb,D)

Title: SHIELD: A regularization technique for eXplainable Artificial
  Intelligence
Authors: Iv\'an Sevillano-Garc\'ia, Juli\'an Luengo and Francisco Herrera
Categories: cs.AI
Comments: 18 pages, 8 figures
ACM-class: I.2.6
\\
  As Artificial Intelligence systems become integral across domains, the demand
for explainability grows. While the effort by the scientific community is
focused on obtaining a better explanation for the model, it is important not to
ignore the potential of this explanation process to improve training as well.
While existing efforts primarily focus on generating and evaluating
explanations for black-box models, there remains a critical gap in directly
enhancing models through these evaluations. This paper introduces SHIELD
(Selective Hidden Input Evaluation for Learning Dynamics), a regularization
technique for explainable artificial intelligence designed to improve model
quality by concealing portions of input data and assessing the resulting
discrepancy in predictions. In contrast to conventional approaches, SHIELD
regularization seamlessly integrates into the objective function, enhancing
model explainability while also improving performance. Experimental validation
on benchmark datasets underscores SHIELD's effectiveness in improving
Artificial Intelligence model explainability and overall performance. This
establishes SHIELD regularization as a promising pathway for developing
transparent and reliable Artificial Intelligence regularization techniques.
\\ ( https://arxiv.org/abs/2404.02611 ,  533kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02831
Date: Wed, 3 Apr 2024 16:08:01 GMT   (3289kb,D)

Title: Empowering Biomedical Discovery with AI Agents
Authors: Shanghua Gao, Ada Fang, Yepeng Huang, Valentina Giunchiglia, Ayush
  Noori, Jonathan Richard Schwarz, Yasha Ektefaie, Jovana Kondic, Marinka
  Zitnik
Categories: cs.AI
\\
  We envision 'AI scientists' as systems capable of skeptical learning and
reasoning that empower biomedical research through collaborative agents that
integrate machine learning tools with experimental platforms. Rather than
taking humans out of the discovery process, biomedical AI agents combine human
creativity and expertise with AI's ability to analyze large datasets, navigate
hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a
variety of tasks, including self-assessment and planning of discovery
workflows. These agents use large language models and generative models to
feature structured memory for continual learning and use machine learning tools
to incorporate scientific knowledge, biological principles, and theories. AI
agents can impact areas ranging from hybrid cell simulation, programmable
control of phenotypes, and the design of cellular circuits to the development
of new therapies.
\\ ( https://arxiv.org/abs/2404.02831 ,  3289kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02838
Date: Wed, 3 Apr 2024 16:17:53 GMT   (14087kb,D)

Title: I-Design: Personalized LLM Interior Designer
Authors: Ata \c{C}elen, Guo Han, Konrad Schindler, Luc Van Gool, Iro Armeni,
  Anton Obukhov, Xi Wang
Categories: cs.AI
\\
  Interior design allows us to be who we are and live how we want - each design
is as unique as our distinct personality. However, it is not trivial for
non-professionals to express and materialize this since it requires aligning
functional and visual expectations with the constraints of physical space; this
renders interior design a luxury. To make it more accessible, we present
I-Design, a personalized interior designer that allows users to generate and
visualize their design goals through natural language communication. I-Design
starts with a team of large language model agents that engage in dialogues and
logical reasoning with one another, transforming textual user input into
feasible scene graph designs with relative object relationships. Subsequently,
an effective placement algorithm determines optimal locations for each object
within the scene. The final design is then constructed in 3D by retrieving and
integrating assets from an existing object database. Additionally, we propose a
new evaluation protocol that utilizes a vision-language model and complements
the design pipeline. Extensive quantitative and qualitative experiments show
that I-Design outperforms existing methods in delivering high-quality 3D design
solutions and aligning with abstract concepts that match user input, showcasing
its advantages across detailed 3D arrangement and conceptual fidelity.
\\ ( https://arxiv.org/abs/2404.02838 ,  14087kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02872
Date: Wed, 3 Apr 2024 17:09:00 GMT   (321kb,D)

Title: Integrating Explanations in Learning LTL Specifications from
  Demonstrations
Authors: Ashutosh Gupta, John Komp, Abhay Singh Rajput, Krishna
  Shankaranarayanan, Ashutosh Trivedi, Namrita Varshney
Categories: cs.AI
Comments: 21 Pages, 13 Page Appendix
ACM-class: I.2.8
\\
  This paper investigates whether recent advances in Large Language Models
(LLMs) can assist in translating human explanations into a format that can
robustly support learning Linear Temporal Logic (LTL) from demonstrations. Both
LLMs and optimization-based methods can extract LTL specifications from
demonstrations; however, they have distinct limitations. LLMs can quickly
generate solutions and incorporate human explanations, but their lack of
consistency and reliability hampers their applicability in safety-critical
domains. On the other hand, optimization-based methods do provide formal
guarantees but cannot process natural language explanations and face
scalability challenges. We present a principled approach to combining LLMs and
optimization-based methods to faithfully translate human explanations and
demonstrations into LTL specifications. We have implemented a tool called
Janaka based on our approach. Our experiments demonstrate the effectiveness of
combining explanations with demonstrations in learning LTL specifications
through several case studies.
\\ ( https://arxiv.org/abs/2404.02872 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02204
Date: Tue, 2 Apr 2024 18:00:28 GMT   (9533kb,D)

Title: Emergent Abilities in Reduced-Scale Generative Language Models
Authors: Sherin Muckatira, Vijeta Deshpande, Vladislav Lialin, Anna Rumshisky
Categories: cs.CL cs.LG
Comments: 16 pages, 4 figures. Accepted to NAACL 2024 Findings
\\
  Large language models can solve new tasks without task-specific fine-tuning.
This ability, also known as in-context learning (ICL), is considered an
emergent ability and is primarily seen in large language models with billions
of parameters. This study investigates if such emergent properties are strictly
tied to model size or can be demonstrated by smaller models trained on
reduced-scale data. To explore this, we simplify pre-training data and
pre-train 36 causal language models with parameters varying from 1 million to
165 million parameters. We show that models trained on this simplified
pre-training data demonstrate enhanced zero-shot capabilities across various
tasks in simplified language, achieving performance comparable to that of
pre-trained models six times larger on unrestricted language. This suggests
that downscaling the language allows zero-shot learning capabilities to emerge
in models with limited size. Additionally, we find that these smaller models
pre-trained on simplified data demonstrate a power law relationship between the
evaluation loss and the three scaling factors: compute, dataset size, and model
size.
\\ ( https://arxiv.org/abs/2404.02204 ,  9533kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02255
Date: Tue, 2 Apr 2024 19:23:10 GMT   (2592kb,D)

Title: $\texttt{LM}^\texttt{2}$: A Simple Society of Language Models Solves
  Complex Reasoning
Authors: Gurusha Juneja, Subhabrata Dutta, Tanmoy Chakraborty
Categories: cs.CL cs.AI
\\
  Despite demonstrating emergent reasoning abilities, Large Language Models
(LLMS) often lose track of complex, multi-step reasoning. Existing studies show
that providing guidance via decomposing the original question into multiple
subproblems elicits more robustness in LLM reasoning -- a decomposer generates
the subproblems, and a solver solves each of these subproblems. However, these
techniques fail to accommodate coordination between the decomposer and the
solver modules (either in a single model or different specialized ones) -- the
decomposer does not keep track of the ability of the solver to follow the
decomposed reasoning. In this paper, we propose LM2 to address these
challenges. LM2 modularizes the decomposition, solution, and verification into
three different language models. The decomposer module identifies the key
concepts necessary to solve the problem and generates step-by-step subquestions
according to the reasoning requirement. The solver model generates the solution
to the subproblems that are then checked by the verifier module; depending upon
the feedback from the verifier, the reasoning context is constructed using the
subproblems and the solutions. These models are trained to coordinate using
policy learning. Exhaustive experimentation suggests the superiority of LM2
over existing methods on in- and out-domain reasoning problems, outperforming
the best baselines by $8.1\%$ on MATH, $7.71\%$ on JEEBench, and $9.7\%$ on
MedQA problems (code available at
https://github.com/LCS2-IIITD/Language_Model_Multiplex).
\\ ( https://arxiv.org/abs/2404.02255 ,  2592kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02261
Date: Tue, 2 Apr 2024 19:34:22 GMT   (357kb,D)

Title: LLMs in the Loop: Leveraging Large Language Model Annotations for Active
  Learning in Low-Resource Languages
Authors: Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah
  Gumus, Michael Granitzer
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: 19 pages, 6 tables. The source code related to this paper is
  available at https://anonymous.4open.science/r/llms-in-the-loop-4E32
ACM-class: I.2.7; I.2.6
\\
  Low-resource languages face significant barriers in AI development due to
limited linguistic resources and expertise for data labeling, rendering them
rare and costly. The scarcity of data and the absence of preexisting tools
exacerbate these challenges, especially since these languages may not be
adequately represented in various NLP datasets. To address this gap, we propose
leveraging the potential of LLMs in the active learning loop for data
annotation. Initially, we conduct evaluations to assess inter-annotator
agreement and consistency, facilitating the selection of a suitable LLM
annotator. The chosen annotator is then integrated into a training loop for a
classifier using an active learning paradigm, minimizing the amount of queried
data required. Empirical evaluations, notably employing GPT-4-Turbo,
demonstrate near-state-of-the-art performance with significantly reduced data
requirements, as indicated by estimated potential cost savings of at least
42.45 times compared to human annotation. Our proposed solution shows promising
potential to substantially reduce both the monetary and computational costs
associated with automation in low-resource settings. By bridging the gap
between low-resource languages and AI, this approach fosters broader inclusion
and shows the potential to enable automation across diverse linguistic
landscapes.
\\ ( https://arxiv.org/abs/2404.02261 ,  357kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02269
Date: Tue, 2 Apr 2024 19:49:34 GMT   (508kb,D)

Title: Extracting Norms from Contracts Via ChatGPT: Opportunities and
  Challenges
Authors: Amanul Haque and Munindar P. Singh
Categories: cs.CL cs.AI
Comments: Accepted at COINE-AAMAS 2024
\\
  We investigate the effectiveness of ChatGPT in extracting norms from
contracts. Norms provide a natural way to engineer multiagent systems by
capturing how to govern the interactions between two or more autonomous
parties. We extract norms of commitment, prohibition, authorization, and power,
along with associated norm elements (the parties involved, antecedents, and
consequents) from contracts. Our investigation reveals ChatGPT's effectiveness
and limitations in norm extraction from contracts. ChatGPT demonstrates
promising performance in norm extraction without requiring training or
fine-tuning, thus obviating the need for annotated data, which is not generally
available in this domain. However, we found some limitations of ChatGPT in
extracting these norms that lead to incorrect norm extractions. The limitations
include oversight of crucial details, hallucination, incorrect parsing of
conjunctions, and empty norm elements. Enhanced norm extraction from contracts
can foster the development of more transparent and trustworthy formal agent
interaction specifications, thereby contributing to the improvement of
multiagent systems.
\\ ( https://arxiv.org/abs/2404.02269 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02305
Date: Tue, 2 Apr 2024 21:03:37 GMT   (6066kb,D)

Title: Collapse of Self-trained Language Models
Authors: David Herel and Tomas Mikolov
Categories: cs.CL cs.AI
Comments: ICLR 2024
\\
  In various fields of knowledge creation, including science, new ideas often
build on pre-existing information. In this work, we explore this concept within
the context of language models. Specifically, we explore the potential of
self-training models on their own outputs, akin to how humans learn and build
on their previous thoughts and actions. While this approach is intuitively
appealing, our research reveals its practical limitations. We find that
extended self-training of the GPT-2 model leads to a significant degradation in
performance, resulting in repetitive and collapsed token output.
\\ ( https://arxiv.org/abs/2404.02305 ,  6066kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02319
Date: Tue, 2 Apr 2024 21:35:54 GMT   (506kb,D)

Title: Prompts As Programs: A Structure-Aware Approach to Efficient
  Compile-Time Prompt Optimization
Authors: Tobias Schnabel, Jennifer Neville
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) can now handle longer and more complex inputs,
which facilitate the use of more elaborate prompts. However, prompts often
require some tuning to improve performance for deployment. Recent work has
proposed automatic prompt optimization methods, but as prompt complexity and
LLM strength increase, many prompt optimization techniques are no longer
sufficient and a new approach is needed to optimize {\em meta prompt programs}.
To address this, we introduce SAMMO, a framework for {\em compile-time}
optimizations of metaprompt programs, which represent prompts as structured
objects that allows for a rich set of transformations that can be searched over
during optimization. We show that SAMMO generalizes previous methods and
improves the performance of complex prompts on (1) instruction tuning, (2) RAG
pipeline tuning, and (3) prompt compression, across several different LLMs.
  We make all code available open-source at https://github.com/microsoft/sammo .
\\ ( https://arxiv.org/abs/2404.02319 ,  506kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02323
Date: Tue, 2 Apr 2024 21:50:18 GMT   (1135kb,D)

Title: Toward Informal Language Processing: Knowledge of Slang in Large
  Language Models
Authors: Zhewei Sun, Qian Hu, Rahul Gupta, Richard Zemel, Yang Xu
Categories: cs.CL
Comments: Accepted to NAACL 2024 main conference
\\
  Recent advancement in large language models (LLMs) has offered a strong
potential for natural language systems to process informal language. A
representative form of informal language is slang, used commonly in daily
conversations and online social media. To date, slang has not been
comprehensively evaluated in LLMs due partly to the absence of a carefully
designed and publicly accessible benchmark. Using movie subtitles, we construct
a dataset that supports evaluation on a diverse set of tasks pertaining to
automatic processing of slang. For both evaluation and finetuning, we show the
effectiveness of our dataset on two core applications: 1) slang detection, and
2) identification of regional and historical sources of slang from natural
sentences. We also show how our dataset can be used to probe the output
distributions of LLMs for interpretive insights. We find that while LLMs such
as GPT-4 achieve good performance in a zero-shot setting, smaller BERT-like
models finetuned on our dataset achieve comparable performance. Furthermore, we
show that our dataset enables finetuning of LLMs such as GPT-3.5 that achieve
substantially better performance than strong zero-shot baselines. Our work
offers a comprehensive evaluation and a high-quality benchmark on English slang
based on the OpenSubtitles corpus, serving both as a publicly accessible
resource and a platform for applying tools for informal language processing.
\\ ( https://arxiv.org/abs/2404.02323 ,  1135kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02330
Date: Tue, 2 Apr 2024 22:04:51 GMT   (463kb,D)

Title: Comparative Study of Domain Driven Terms Extraction Using Large Language
  Models
Authors: Sandeep Chataut, Tuyen Do, Bichar Dip Shrestha Gurung, Shiva Aryal,
  Anup Khanal, Carol Lushbough, Etienne Gnimpieba
Categories: cs.CL cs.AI
\\
  Keywords play a crucial role in bridging the gap between human understanding
and machine processing of textual data. They are essential to data enrichment
because they form the basis for detailed annotations that provide a more
insightful and in-depth view of the underlying data. Keyword/domain driven term
extraction is a pivotal task in natural language processing, facilitating
information retrieval, document summarization, and content categorization. This
review focuses on keyword extraction methods, emphasizing the use of three
major Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We
employed a custom Python package to interface with these LLMs, simplifying
keyword extraction. Our study, utilizing the Inspec and PubMed datasets,
evaluates the performance of these models. The Jaccard similarity index was
used for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for
GPT-3.5, 0.40 and 0.17 for Llama2-7B, and 0.23 and 0.12 for Falcon-7B. This
paper underlines the role of prompt engineering in LLMs for better keyword
extraction and discusses the impact of hallucination in LLMs on result
evaluation. It also sheds light on the challenges in using LLMs for keyword
extraction, including model complexity, resource demands, and optimization
techniques.
\\ ( https://arxiv.org/abs/2404.02330 ,  463kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02335
Date: Tue, 2 Apr 2024 22:15:48 GMT   (586kb,D)

Title: Multi-BERT: Leveraging Adapters and Prompt Tuning for Low-Resource
  Multi-Domain Adaptation
Authors: Parham Abed Azad and Hamid Beigy
Categories: cs.CL cs.AI
\\
  The rapid expansion of texts' volume and diversity presents formidable
challenges in multi-domain settings. These challenges are also visible in the
Persian name entity recognition (NER) settings. Traditional approaches, either
employing a unified model for multiple domains or individual models for each
domain, frequently pose significant limitations. Single models often struggle
to capture the nuances of diverse domains, while utilizing multiple large
models can lead to resource constraints, rendering the training of a model for
each domain virtually impractical. Therefore, this paper introduces a novel
approach composed of one core model with multiple sets of domain-specific
parameters. We utilize techniques such as prompt tuning and adapters, combined
with the incorporation of additional layers, to add parameters that we can
train for the specific domains. This enables the model to perform comparably to
individual models for each domain. Experimental results on different formal and
informal datasets show that by employing these added parameters, the proposed
model significantly surpasses existing practical models in performance.
Remarkably, the proposed model requires only one instance for training and
storage, yet achieves outstanding results across all domains, even surpassing
the state-of-the-art in some. Moreover, we analyze each adaptation strategy,
delineating its strengths, weaknesses, and optimal hyper-parameters for the
Persian NER settings. Finally, we introduce a document-based domain detection
pipeline tailored for scenarios with unknown text domains, enhancing the
adaptability and practicality of this paper in real-world applications.
\\ ( https://arxiv.org/abs/2404.02335 ,  586kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02340
Date: Tue, 2 Apr 2024 22:27:24 GMT   (271kb,D)

Title: Corpus Considerations for Annotator Modeling and Scaling
Authors: Olufunke O. Sarumi and B\'ela Neuendorf and Joan Plepi and Lucie Flek
  and J\"org Schl\"otterer and Charles Welch
Categories: cs.CL
Comments: Accepted at NAACL 2024
ACM-class: F.2.2; I.2.7
\\
  Recent trends in natural language processing research and annotation tasks
affirm a paradigm shift from the traditional reliance on a single ground truth
to a focus on individual perspectives, particularly in subjective tasks. In
scenarios where annotation tasks are meant to encompass diversity, models that
solely rely on the majority class labels may inadvertently disregard valuable
minority perspectives. This oversight could result in the omission of crucial
information and, in a broader context, risk disrupting the balance within
larger ecosystems. As the landscape of annotator modeling unfolds with diverse
representation techniques, it becomes imperative to investigate their
effectiveness with the fine-grained features of the datasets in view. This
study systematically explores various annotator modeling techniques and
compares their performance across seven corpora.
  From our findings, we show that the commonly used user token model
consistently outperforms more complex models. We introduce a composite
embedding approach and show distinct differences in which model performs best
as a function of the agreement with a given dataset. Our findings shed light on
the relationship between corpus statistics and annotator modeling performance,
which informs future work on corpus construction and perspectivist NLP.
\\ ( https://arxiv.org/abs/2404.02340 ,  271kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02342
Date: Tue, 2 Apr 2024 22:31:38 GMT   (413kb,D)

Title: A Computational Analysis of Lyric Similarity Perception
Authors: Haven Kim and Taketo Akama
Categories: cs.CL cs.SD eess.AS
\\
  In musical compositions that include vocals, lyrics significantly contribute
to artistic expression. Consequently, previous studies have introduced the
concept of a recommendation system that suggests lyrics similar to a user's
favorites or personalized preferences, aiding in the discovery of lyrics among
millions of tracks. However, many of these systems do not fully consider human
perceptions of lyric similarity, primarily due to limited research in this
area. To bridge this gap, we conducted a comparative analysis of computational
methods for modeling lyric similarity with human perception. Results indicated
that computational models based on similarities between embeddings from
pre-trained BERT-based models, the audio from which the lyrics are derived, and
phonetic components are indicative of perceptual lyric similarity. This finding
underscores the importance of semantic, stylistic, and phonetic similarities in
human perception about lyric similarity. We anticipate that our findings will
enhance the development of similarity-based lyric recommendation systems by
offering pseudo-labels for neural network development and introducing objective
evaluation metrics.
\\ ( https://arxiv.org/abs/2404.02342 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02356
Date: Tue, 2 Apr 2024 22:58:38 GMT   (8231kb,D)

Title: Two Heads are Better than One: Nested PoE for Robust Defense Against
  Multi-Backdoors
Authors: Victoria Graf, Qin Liu, Muhao Chen
Categories: cs.CL
Comments: Accepted by NAACL 2024 Main Conference
\\
  Data poisoning backdoor attacks can cause undesirable behaviors in large
language models (LLMs), and defending against them is of increasing importance.
Existing defense mechanisms often assume that only one type of trigger is
adopted by the attacker, while defending against multiple simultaneous and
independent trigger types necessitates general defense frameworks and is
relatively unexplored. In this paper, we propose Nested Product of
Experts(NPoE) defense framework, which involves a mixture of experts (MoE) as a
trigger-only ensemble within the PoE defense framework to simultaneously defend
against multiple trigger types. During NPoE training, the main model is trained
in an ensemble with a mixture of smaller expert models that learn the features
of backdoor triggers. At inference time, only the main model is used.
Experimental results on sentiment analysis, hate speech detection, and question
classification tasks demonstrate that NPoE effectively defends against a
variety of triggers both separately and in trigger mixtures. Due to the
versatility of the MoE structure in NPoE, this framework can be further
expanded to defend against other attack settings
\\ ( https://arxiv.org/abs/2404.02356 ,  8231kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02375
Date: Wed, 3 Apr 2024 00:21:14 GMT   (283kb,D)

Title: Optical Text Recognition in Nepali and Bengali: A Transformer-based
  Approach
Authors: S M Rakib Hasan, Aakar Dhakal, Md Humaion Kabir Mehedi, Annajiat Alim
  Rasel
Categories: cs.CL
Comments: Accepted and Presented at ICAECC 2023, Bengaluru, India
\\
  Efforts on the research and development of OCR systems for Low-Resource
Languages are relatively new. Low-resource languages have little training data
available for training Machine Translation systems or other systems. Even
though a vast amount of text has been digitized and made available on the
internet the text is still in PDF and Image format, which are not instantly
accessible. This paper discusses text recognition for two scripts: Bengali and
Nepali; there are about 300 and 40 million Bengali and Nepali speakers
respectively. In this study, using encoder-decoder transformers, a model was
developed, and its efficacy was assessed using a collection of optical text
images, both handwritten and printed. The results signify that the suggested
technique corresponds with current approaches and achieves high precision in
recognizing text in Bengali and Nepali. This study can pave the way for the
advanced and accessible study of linguistics in South East Asia.
\\ ( https://arxiv.org/abs/2404.02375 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02389
Date: Wed, 3 Apr 2024 01:16:20 GMT   (4858kb,D)

Title: On Linearizing Structured Data in Encoder-Decoder Language Models:
  Insights from Text-to-SQL
Authors: Yutong Shao and Ndapa Nakashole
Categories: cs.CL cs.AI
Comments: to appear at NAACL 2024
\\
  Structured data, prevalent in tables, databases, and knowledge graphs, poses
a significant challenge in its representation. With the advent of large
language models (LLMs), there has been a shift towards linearization-based
methods, which process structured data as sequential token streams, diverging
from approaches that explicitly model structure, often as a graph. Crucially,
there remains a gap in our understanding of how these linearization-based
methods handle structured data, which is inherently non-linear. This work
investigates the linear handling of structured data in encoder-decoder language
models, specifically T5. Our findings reveal the model's ability to mimic
human-designed processes such as schema linking and syntax prediction,
indicating a deep, meaningful learning of structure beyond simple token
sequencing. We also uncover insights into the model's internal mechanisms,
including the ego-centric nature of structure node encodings and the potential
for model compression due to modality fusion redundancy. Overall, this work
sheds light on the inner workings of linearization-based methods and could
potentially provide guidance for future research.
\\ ( https://arxiv.org/abs/2404.02389 ,  4858kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02392
Date: Wed, 3 Apr 2024 01:31:41 GMT   (7888kb,D)

Title: Low-resource neural machine translation with morphological modeling
Authors: Antoine Nzeyimana
Categories: cs.CL
Comments: NAACL Findings 2024
ACM-class: I.2.7; I.2
\\
  Morphological modeling in neural machine translation (NMT) is a promising
approach to achieving open-vocabulary machine translation for
morphologically-rich languages. However, existing methods such as sub-word
tokenization and character-based models are limited to the surface forms of the
words. In this work, we propose a framework-solution for modeling complex
morphology in low-resource settings. A two-tier transformer architecture is
chosen to encode morphological information at the inputs. At the target-side
output, a multi-task multi-label training scheme coupled with a beam
search-based decoder are found to improve machine translation performance. An
attention augmentation scheme to the transformer model is proposed in a generic
form to allow integration of pre-trained language models and also facilitate
modeling of word order relationships between the source and target languages.
Several data augmentation techniques are evaluated and shown to increase
translation performance in low-resource settings. We evaluate our proposed
solution on Kinyarwanda - English translation using public-domain parallel
text. Our final models achieve competitive performance in relation to large
multi-lingual models. We hope that our results will motivate more use of
explicit morphological information and the proposed model and data
augmentations in low-resource NMT.
\\ ( https://arxiv.org/abs/2404.02392 ,  7888kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02393
Date: Wed, 3 Apr 2024 01:32:31 GMT   (7965kb,D)

Title: Backdoor Attack on Multilingual Machine Translation
Authors: Jun Wang, Qiongkai Xu, Xuanli He, Benjamin I. P. Rubinstein, Trevor
  Cohn
Categories: cs.CL
Comments: NAACL main long paper
\\
  While multilingual machine translation (MNMT) systems hold substantial
promise, they also have security vulnerabilities. Our research highlights that
MNMT systems can be susceptible to a particularly devious style of backdoor
attack, whereby an attacker injects poisoned data into a low-resource language
pair to cause malicious translations in other languages, including
high-resource languages. Our experimental results reveal that injecting less
than 0.01% poisoned data into a low-resource language pair can achieve an
average 20% attack success rate in attacking high-resource language pairs. This
type of attack is of particular concern, given the larger attack surface of
languages inherent to low-resource settings. Our aim is to bring attention to
these vulnerabilities within MNMT systems with the hope of encouraging the
community to address security concerns in machine translation, especially in
the context of low-resource languages.
\\ ( https://arxiv.org/abs/2404.02393 ,  7965kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02402
Date: Wed, 3 Apr 2024 02:11:39 GMT   (348kb,D)

Title: Token Trails: Navigating Contextual Depths in Conversational AI with
  ChatLLM
Authors: Md. Kowsher, Ritesh Panditi, Nusrat Jahan Prottasha, Prakash Bhat,
  Anupam Kumar Bairagi, Mohammad Shamsul Arefin
Categories: cs.CL cs.AI cs.IR cs.LG
\\
  Conversational modeling using Large Language Models (LLMs) requires a nuanced
understanding of context to generate coherent and contextually relevant
responses. In this paper, we present Token Trails, a novel approach that
leverages token-type embeddings to navigate the intricate contextual nuances
within conversations. Our framework utilizes token-type embeddings to
distinguish between user utterances and bot responses, facilitating the
generation of context-aware replies. Through comprehensive experimentation and
evaluation, we demonstrate the effectiveness of Token Trails in improving
conversational understanding and response generation, achieving
state-of-the-art performance. Our results highlight the significance of
contextual modeling in conversational AI and underscore the promising potential
of Token Trails to advance the field, paving the way for more sophisticated and
contextually aware chatbot interactions.
\\ ( https://arxiv.org/abs/2404.02402 ,  348kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02403
Date: Wed, 3 Apr 2024 02:12:29 GMT   (1007kb,D)

Title: Benchmarking Large Language Models for Persian: A Preliminary Study
  Focusing on ChatGPT
Authors: Amirhossein Abaskohi, Sara Baruni, Mostafa Masoudi, Nesa Abbasi,
  Mohammad Hadi Babalou, Ali Edalat, Sepehr Kamahi, Samin Mahdizadeh Sani,
  Nikoo Naghavian, Danial Namazifard, Pouya Sadeghi and Yadollah Yaghoobzadeh
Categories: cs.CL cs.LG
Comments: 14 pages, 1 figure, 6 tables, Proceeding of the 2024 Joint
  International Conference on Computational Linguistics, Language Resources and
  Evaluation (LREC-COLING)
\\
  This paper explores the efficacy of large language models (LLMs) for Persian.
While ChatGPT and consequent LLMs have shown remarkable performance in English,
their efficiency for more low-resource languages remains an open question. We
present the first comprehensive benchmarking study of LLMs across diverse
Persian language tasks. Our primary focus is on GPT-3.5-turbo, but we also
include GPT-4 and OpenChat-3.5 to provide a more holistic evaluation. Our
assessment encompasses a diverse set of tasks categorized into classic,
reasoning, and knowledge-based domains. To enable a thorough comparison, we
evaluate LLMs against existing task-specific fine-tuned models. Given the
limited availability of Persian datasets for reasoning tasks, we introduce two
new benchmarks: one based on elementary school math questions and another
derived from the entrance exams for 7th and 10th grades. Our findings reveal
that while LLMs, especially GPT-4, excel in tasks requiring reasoning abilities
and a broad understanding of general knowledge, they often lag behind smaller
pre-trained models fine-tuned specifically for particular tasks. Additionally,
we observe improved performance when test sets are translated to English before
inputting them into GPT-3.5. These results highlight the significant potential
for enhancing LLM performance in the Persian language. This is particularly
noteworthy due to the unique attributes of Persian, including its distinct
alphabet and writing styles.
\\ ( https://arxiv.org/abs/2404.02403 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02408
Date: Wed, 3 Apr 2024 02:21:46 GMT   (8815kb,D)

Title: CMULAB: An Open-Source Framework for Training and Deployment of Natural
  Language Processing Models
Authors: Zaid Sheikh, Antonios Anastasopoulos, Shruti Rijhwani, Lindia Tjuatja,
  Robbie Jimerson, Graham Neubig
Categories: cs.CL
Comments: Live demo at https://cmulab.dev
\\
  Effectively using Natural Language Processing (NLP) tools in under-resourced
languages requires a thorough understanding of the language itself, familiarity
with the latest models and training methodologies, and technical expertise to
deploy these models. This could present a significant obstacle for language
community members and linguists to use NLP tools. This paper introduces the CMU
Linguistic Annotation Backend, an open-source framework that simplifies model
deployment and continuous human-in-the-loop fine-tuning of NLP models. CMULAB
enables users to leverage the power of multilingual models to quickly adapt and
extend existing tools for speech recognition, OCR, translation, and syntactic
analysis to new languages, even with limited training data. We describe various
tools and APIs that are currently available and how developers can easily add
new models/functionality to the framework. Code is available at
https://github.com/neulab/cmulab along with a live demo at https://cmulab.dev
\\ ( https://arxiv.org/abs/2404.02408 ,  8815kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02418
Date: Wed, 3 Apr 2024 02:56:52 GMT   (241kb,D)

Title: Auxiliary task demands mask the capabilities of smaller language models
Authors: Jennifer Hu, Michael C. Frank
Categories: cs.CL cs.AI
\\
  Developmental psychologists have argued about when cognitive capacities such
as language understanding or theory of mind emerge. These debates often hinge
on the concept of "task demands" -- the auxiliary challenges associated with
performing a particular evaluation -- that may mask the child's underlying
ability. The same issues arise when measuring the capacities of language models
(LMs): performance on a task is a function of the model's underlying
competence, combined with the model's ability to interpret and perform the task
given its available resources. Here, we show that for analogical reasoning,
reflective reasoning, word prediction, and grammaticality judgments, evaluation
methods with greater task demands yield lower performance than evaluations with
reduced demands. This "demand gap" is most pronounced for models with fewer
parameters and less training data. Our results illustrate that LM performance
should not be interpreted as a direct indication of intelligence (or lack
thereof), but as a reflection of capacities seen through the lens of
researchers' design choices.
\\ ( https://arxiv.org/abs/2404.02418 ,  241kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02421
Date: Wed, 3 Apr 2024 03:14:27 GMT   (9171kb,D)

Title: Revisiting subword tokenization: A case study on affixal negation in
  large language models
Authors: Thinh Hung Truong, Yulia Otmakhova, Karin Verspoor, Trevor Cohn,
  Timothy Baldwin
Categories: cs.CL
Comments: NAACL 2024
\\
  In this work, we measure the impact of affixal negation on modern English
large language models (LLMs). In affixal negation, the negated meaning is
expressed through a negative morpheme, which is potentially challenging for
LLMs as their tokenizers are often not morphologically plausible. We conduct
extensive experiments using LLMs with different subword tokenization methods,
which lead to several insights on the interaction between tokenization
performance and negation sensitivity. Despite some interesting mismatches
between tokenization accuracy and negation detection performance, we show that
models can, on the whole, reliably recognize the meaning of affixal negation.
\\ ( https://arxiv.org/abs/2404.02421 ,  9171kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02422
Date: Wed, 3 Apr 2024 03:24:19 GMT   (637kb,D)

Title: Enhancing Low-Resource LLMs Classification with PEFT and Synthetic Data
Authors: Parth Patwa, Simone Filice, Zhiyu Chen, Giuseppe Castellucci, Oleg
  Rokhlenko, Shervin Malmasi
Categories: cs.CL cs.LG
Comments: Accepted at LREC-COLING 2024
\\
  Large Language Models (LLMs) operating in 0-shot or few-shot settings achieve
competitive results in Text Classification tasks. In-Context Learning (ICL)
typically achieves better accuracy than the 0-shot setting, but it pays in
terms of efficiency, due to the longer input prompt. In this paper, we propose
a strategy to make LLMs as efficient as 0-shot text classifiers, while getting
comparable or better accuracy than ICL. Our solution targets the low resource
setting, i.e., when only 4 examples per class are available. Using a single LLM
and few-shot real data we perform a sequence of generation, filtering and
Parameter-Efficient Fine-Tuning steps to create a robust and efficient
classifier. Experimental results show that our approach leads to competitive
results on multiple text classification datasets.
\\ ( https://arxiv.org/abs/2404.02422 ,  637kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02431
Date: Wed, 3 Apr 2024 03:37:22 GMT   (13389kb,D)

Title: On the Multilingual Ability of Decoder-based Pre-trained Language
  Models: Finding and Controlling Language-Specific Neurons
Authors: Takeshi Kojima, Itsuki Okimura, Yusuke Iwasawa, Hitomi Yanaka, Yutaka
  Matsuo
Categories: cs.CL
Comments: Accepted to NAACL2024. Our code is available at
  https://github.com/kojima-takeshi188/lang_neuron
\\
  Current decoder-based pre-trained language models (PLMs) successfully
demonstrate multilingual capabilities. However, it is unclear how these models
handle multilingualism. We analyze the neuron-level internal behavior of
multilingual decoder-based PLMs, Specifically examining the existence of
neurons that fire ``uniquely for each language'' within decoder-only
multilingual PLMs. We analyze six languages: English, German, French, Spanish,
Chinese, and Japanese, and show that language-specific neurons are unique, with
a slight overlap (< 5%) between languages. These neurons are mainly distributed
in the models' first and last few layers. This trend remains consistent across
languages and models. Additionally, we tamper with less than 1% of the total
neurons in each model during inference and demonstrate that tampering with a
few language-specific neurons drastically changes the probability of target
language occurrence in text generation.
\\ ( https://arxiv.org/abs/2404.02431 ,  13389kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02438
Date: Wed, 3 Apr 2024 03:53:37 GMT   (6130kb,D)

Title: From Narratives to Numbers: Valid Inference Using Language Model
  Predictions from Verbal Autopsy Narratives
Authors: Shuxian Fan, Adam Visokay, Kentaro Hoffman, Stephen Salerno, Li Liu,
  Jeffrey T. Leek, Tyler H. McCormick
Categories: cs.CL cs.LG stat.ML
Comments: 12 pages, 7 figures
\\
  In settings where most deaths occur outside the healthcare system, verbal
autopsies (VAs) are a common tool to monitor trends in causes of death (COD).
VAs are interviews with a surviving caregiver or relative that are used to
predict the decedent's COD. Turning VAs into actionable insights for
researchers and policymakers requires two steps (i) predicting likely COD using
the VA interview and (ii) performing inference with predicted CODs (e.g.
modeling the breakdown of causes by demographic factors using a sample of
deaths). In this paper, we develop a method for valid inference using outcomes
(in our case COD) predicted from free-form text using state-of-the-art NLP
techniques. This method, which we call multiPPI++, extends recent work in
"prediction-powered inference" to multinomial classification. We leverage a
suite of NLP techniques for COD prediction and, through empirical analysis of
VA data, demonstrate the effectiveness of our approach in handling
transportability issues. multiPPI++ recovers ground truth estimates, regardless
of which NLP model produced predictions and regardless of whether they were
produced by a more accurate predictor like GPT-4-32k or a less accurate
predictor like KNN. Our findings demonstrate the practical importance of
inference correction for public health decision-making and suggests that if
inference tasks are the end goal, having a small amount of contextually
relevant, high quality labeled data is essential regardless of the NLP
algorithm.
\\ ( https://arxiv.org/abs/2404.02438 ,  6130kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02444
Date: Wed, 3 Apr 2024 04:15:29 GMT   (8523kb,D)

Title: The Promises and Pitfalls of Using Language Models to Measure
  Instruction Quality in Education
Authors: Paiheng Xu, Jing Liu, Nathan Jones, Julie Cohen, Wei Ai
Categories: cs.CL cs.AI
Comments: NAACL 2024
\\
  Assessing instruction quality is a fundamental component of any improvement
efforts in the education system. However, traditional manual assessments are
expensive, subjective, and heavily dependent on observers' expertise and
idiosyncratic factors, preventing teachers from getting timely and frequent
feedback. Different from prior research that mostly focuses on low-inference
instructional practices on a singular basis, this paper presents the first
study that leverages Natural Language Processing (NLP) techniques to assess
multiple high-inference instructional practices in two distinct educational
settings: in-person K-12 classrooms and simulated performance tasks for
pre-service teachers. This is also the first study that applies NLP to measure
a teaching practice that is widely acknowledged to be particularly effective
for students with special needs. We confront two challenges inherent in
NLP-based instructional analysis, including noisy and long input data and
highly skewed distributions of human ratings. Our results suggest that
pretrained Language Models (PLMs) demonstrate performances comparable to the
agreement level of human raters for variables that are more discrete and
require lower inference, but their efficacy diminishes with more complex
teaching practices. Interestingly, using only teachers' utterances as input
yields strong results for student-centered variables, alleviating common
concerns over the difficulty of collecting and transcribing high-quality
student speech data in in-person teaching settings. Our findings highlight both
the potential and the limitations of current NLP techniques in the education
domain, opening avenues for further exploration.
\\ ( https://arxiv.org/abs/2404.02444 ,  8523kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02452
Date: Wed, 3 Apr 2024 04:40:57 GMT   (9931kb,D)

Title: Adaptive Cross-lingual Text Classification through In-Context One-Shot
  Demonstrations
Authors: Emilio Villa-Cueva, A. Pastor L\'opez-Monroy, Fernando S\'anchez-Vega,
  Thamar Solorio
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\
  Zero-Shot Cross-lingual Transfer (ZS-XLT) utilizes a model trained in a
source language to make predictions in another language, often with a
performance loss. To alleviate this, additional improvements can be achieved
through subsequent adaptation using examples in the target language. In this
paper, we exploit In-Context Tuning (ICT) for One-Shot Cross-lingual transfer
in the classification task by introducing In-Context Cross-lingual Transfer
(IC-XLT). The novel concept involves training a model to learn from context
examples and subsequently adapting it during inference to a target language by
prepending a One-Shot context demonstration in that language. Our results show
that IC-XLT successfully leverages target-language examples to improve the
cross-lingual capabilities of the evaluated mT5 model, outperforming
prompt-based models in the Zero and Few-shot scenarios adapted through
fine-tuning. Moreover, we show that when source-language data is limited, the
fine-tuning framework employed for IC-XLT performs comparably to prompt-based
fine-tuning with significantly more training data in the source language.
\\ ( https://arxiv.org/abs/2404.02452 ,  9931kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02456
Date: Wed, 3 Apr 2024 04:53:14 GMT   (421kb,D)

Title: PhonologyBench: Evaluating Phonological Skills of Large Language Models
Authors: Ashima Suvarna, Harshita Khandelwal, Nanyun Peng
Categories: cs.CL cs.AI cs.LG cs.SD eess.AS
Comments: 17 pages, 7 figures, 6 tables
\\
  Phonology, the study of speech's structure and pronunciation rules, is a
critical yet often overlooked component in Large Language Model (LLM) research.
LLMs are widely used in various downstream applications that leverage phonology
such as educational tools and poetry generation. Moreover, LLMs can potentially
learn imperfect associations between orthographic and phonological forms from
the training data. Thus, it is imperative to benchmark the phonological skills
of LLMs. To this end, we present PhonologyBench, a novel benchmark consisting
of three diagnostic tasks designed to explicitly test the phonological skills
of LLMs in English: grapheme-to-phoneme conversion, syllable counting, and
rhyme word generation. Despite having no access to speech data, LLMs showcased
notable performance on the PhonologyBench tasks. However, we observe a
significant gap of 17% and 45% on Rhyme Word Generation and Syllable counting,
respectively, when compared to humans. Our findings underscore the importance
of studying LLM performance on phonological tasks that inadvertently impact
real-world applications. Furthermore, we encourage researchers to choose LLMs
that perform well on the phonological task that is closely related to the
downstream application since we find that no single model consistently
outperforms the others on all the tasks.
\\ ( https://arxiv.org/abs/2404.02456 ,  421kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02466
Date: Wed, 3 Apr 2024 05:10:11 GMT   (601kb,D)

Title: Prompting for Numerical Sequences: A Case Study on Market Comment
  Generation
Authors: Masayuki Kawarada, Tatsuya Ishigaki, Hiroya Takamura
Categories: cs.CL cs.AI cs.CE
Comments: Accepted to LREC-COLING2024 long paper
\\
  Large language models (LLMs) have been applied to a wide range of
data-to-text generation tasks, including tables, graphs, and time-series
numerical data-to-text settings. While research on generating prompts for
structured data such as tables and graphs is gaining momentum, in-depth
investigations into prompting for time-series numerical data are lacking.
Therefore, this study explores various input representations, including
sequences of tokens and structured formats such as HTML, LaTeX, and
Python-style codes. In our experiments, we focus on the task of Market Comment
Generation, which involves taking a numerical sequence of stock prices as input
and generating a corresponding market comment. Contrary to our expectations,
the results show that prompts resembling programming languages yield better
outcomes, whereas those similar to natural languages and longer formats, such
as HTML and LaTeX, are less effective. Our findings offer insights into
creating effective prompts for tasks that generate text from numerical
sequences.
\\ ( https://arxiv.org/abs/2404.02466 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02474
Date: Wed, 3 Apr 2024 05:31:59 GMT   (667kb,D)

Title: uTeBC-NLP at SemEval-2024 Task 9: Can LLMs be Lateral Thinkers?
Authors: Pouya Sadeghi and Amirhossein Abaskohi and Yadollah Yaghoobzadeh
Categories: cs.CL cs.AI cs.IR cs.LG
Comments: 12 pages, 5 figures, 6 tables, Proceedings of the 18th International
  Workshop on Semantic Evaluation (SemEval-2024) @ NAACL 2024
\\
  Inspired by human cognition, Jiang et al.(2023c) create a benchmark for
assessing LLMs' lateral thinking-thinking outside the box. Building upon this
benchmark, we investigate how different prompting methods enhance LLMs'
performance on this task to reveal their inherent power for outside-the-box
thinking ability. Through participating in SemEval-2024, task 9, Sentence
Puzzle sub-task, we explore prompt engineering methods: chain of thoughts (CoT)
and direct prompting, enhancing with informative descriptions, and employing
contextualizing prompts using a retrieval augmented generation (RAG) pipeline.
Our experiments involve three LLMs including GPT-3.5, GPT-4, and
Zephyr-7B-beta. We generate a dataset of thinking paths between riddles and
options using GPT-4, validated by humans for quality. Findings indicate that
compressed informative prompts enhance performance. Dynamic in-context learning
enhances model performance significantly. Furthermore, fine-tuning Zephyr on
our dataset enhances performance across other commonsense datasets,
underscoring the value of innovative thinking.
\\ ( https://arxiv.org/abs/2404.02474 ,  667kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02490
Date: Wed, 3 Apr 2024 05:58:53 GMT   (8862kb,D)

Title: Enhancing Cross-lingual Sentence Embedding for Low-resource Languages
  with Word Alignment
Authors: Zhongtao Miao, Qiyu Wu, Kaiyan Zhao, Zilong Wu, Yoshimasa Tsuruoka
Categories: cs.CL
Comments: NAACL 2024 findings
\\
  The field of cross-lingual sentence embeddings has recently experienced
significant advancements, but research concerning low-resource languages has
lagged due to the scarcity of parallel corpora. This paper shows that
cross-lingual word representation in low-resource languages is notably
under-aligned with that in high-resource languages in current models. To
address this, we introduce a novel framework that explicitly aligns words
between English and eight low-resource languages, utilizing off-the-shelf word
alignment models. This framework incorporates three primary training
objectives: aligned word prediction and word translation ranking, along with
the widely used translation ranking. We evaluate our approach through
experiments on the bitext retrieval task, which demonstrate substantial
improvements on sentence embeddings in low-resource languages. In addition, the
competitive performance of the proposed model across a broader range of tasks
in high-resource languages underscores its practicality.
\\ ( https://arxiv.org/abs/2404.02490 ,  8862kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02491
Date: Wed, 3 Apr 2024 05:58:57 GMT   (9299kb,D)

Title: Measuring Social Norms of Large Language Models
Authors: Ye Yuan, Kexin Tang, Jianhao Shen, Ming Zhang, Chenguang Wang
Categories: cs.CL cs.AI cs.LG
\\
  We present a new challenge to examine whether large language models
understand social norms. In contrast to existing datasets, our dataset requires
a fundamental understanding of social norms to solve. Our dataset features the
largest set of social norm skills, consisting of 402 skills and 12,383
questions covering a wide set of social norms ranging from opinions and
arguments to culture and laws. We design our dataset according to the K-12
curriculum. This enables the direct comparison of the social understanding of
large language models to humans, more specifically, elementary students. While
prior work generates nearly random accuracy on our benchmark, recent large
language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the
performance significantly, only slightly below human performance. We then
propose a multi-agent framework based on large language models to improve the
models' ability to understand social norms. This method further improves large
language models to be on par with humans. Given the increasing adoption of
large language models in real-world applications, our finding is particularly
important and presents a unique direction for future improvements.
\\ ( https://arxiv.org/abs/2404.02491 ,  9299kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02505
Date: Wed, 3 Apr 2024 06:47:15 GMT   (417kb,D)

Title: Dynamic Demonstration Retrieval and Cognitive Understanding for
  Emotional Support Conversation
Authors: Zhe Xu, Daoyuan Chen, Jiayi Kuang, Zihao Yi, Yaliang Li, Ying Shen
Categories: cs.CL cs.AI
Comments: Accpeted by SIGIR 2024
MSC-class: 68T50
ACM-class: I.2.7
\\
  Emotional Support Conversation (ESC) systems are pivotal in providing
empathetic interactions, aiding users through negative emotional states by
understanding and addressing their unique experiences. In this paper, we tackle
two key challenges in ESC: enhancing contextually relevant and empathetic
response generation through dynamic demonstration retrieval, and advancing
cognitive understanding to grasp implicit mental states comprehensively. We
introduce Dynamic Demonstration Retrieval and Cognitive-Aspect Situation
Understanding (\ourwork), a novel approach that synergizes these elements to
improve the quality of support provided in ESCs. By leveraging in-context
learning and persona information, we introduce an innovative retrieval
mechanism that selects informative and personalized demonstration pairs. We
also propose a cognitive understanding module that utilizes four cognitive
relationships from the ATOMIC knowledge source to deepen situational awareness
of help-seekers' mental states. Our supportive decoder integrates information
from diverse knowledge sources, underpinning response generation that is both
empathetic and cognitively aware. The effectiveness of \ourwork is demonstrated
through extensive automatic and human evaluations, revealing substantial
improvements over numerous state-of-the-art models, with up to 13.79\%
enhancement in overall performance of ten metrics. Our codes are available for
public access to facilitate further research and development.
\\ ( https://arxiv.org/abs/2404.02505 ,  417kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02507
Date: Wed, 3 Apr 2024 06:51:49 GMT   (8034kb,D)

Title: Lifelong Event Detection with Embedding Space Separation and Compaction
Authors: Chengwei Qin, Ruirui Chen, Ruochen Zhao, Wenhan Xia, Shafiq Joty
Categories: cs.CL
Comments: NAACL 2024 main conference
\\
  To mitigate forgetting, existing lifelong event detection methods typically
maintain a memory module and replay the stored memory data during the learning
of a new task. However, the simple combination of memory data and new-task
samples can still result in substantial forgetting of previously acquired
knowledge, which may occur due to the potential overlap between the feature
distribution of new data and the previously learned embedding space. Moreover,
the model suffers from overfitting on the few memory samples rather than
effectively remembering learned patterns. To address the challenges of
forgetting and overfitting, we propose a novel method based on embedding space
separation and compaction. Our method alleviates forgetting of previously
learned tasks by forcing the feature distribution of new data away from the
previous embedding space. It also mitigates overfitting by a memory calibration
mechanism that encourages memory data to be close to its prototype to enhance
intra-class compactness. In addition, the learnable parameters of the new task
are initialized by drawing upon acquired knowledge from the previously learned
task to facilitate forward knowledge transfer. With extensive experiments, we
demonstrate that our method can significantly outperform previous
state-of-the-art approaches.
\\ ( https://arxiv.org/abs/2404.02507 ,  8034kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02512
Date: Wed, 3 Apr 2024 06:57:45 GMT   (8145kb,D)

Title: Towards Large Language Model driven Reference-less Translation
  Evaluation for English and Indian Languages
Authors: Vandan Mujadia, Pruthwik Mishra, Arafat Ahsan, Dipti Misra Sharma
Categories: cs.CL
Comments: arXiv admin note: text overlap with arXiv:2311.09216
\\
  With the primary focus on evaluating the effectiveness of large language
models for automatic reference-less translation assessment, this work presents
our experiments on mimicking human direct assessment to evaluate the quality of
translations in English and Indian languages. We constructed a translation
evaluation task where we performed zero-shot learning, in-context
example-driven learning, and fine-tuning of large language models to provide a
score out of 100, where 100 represents a perfect translation and 1 represents a
poor translation. We compared the performance of our trained systems with
existing methods such as COMET, BERT-Scorer, and LABSE, and found that the
LLM-based evaluator (LLaMA-2-13B) achieves a comparable or higher overall
correlation with human judgments for the considered Indian language pairs.
\\ ( https://arxiv.org/abs/2404.02512 ,  8145kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02529
Date: Wed, 3 Apr 2024 07:31:53 GMT   (1153kb,D)

Title: A School Student Essay Corpus for Analyzing Interactions of
  Argumentative Structure and Quality
Authors: Maja Stahl, Nadine Michel, Sebastian Kilsbach, Julian Schmidtke, Sara
  Rezat, Henning Wachsmuth
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\
  Learning argumentative writing is challenging. Besides writing fundamentals
such as syntax and grammar, learners must select and arrange argument
components meaningfully to create high-quality essays. To support argumentative
writing computationally, one step is to mine the argumentative structure. When
combined with automatic essay scoring, interactions of the argumentative
structure and quality scores can be exploited for comprehensive writing
support. Although studies have shown the usefulness of using information about
the argumentative structure for essay scoring, no argument mining corpus with
ground-truth essay quality annotations has been published yet. Moreover, none
of the existing corpora contain essays written by school students specifically.
To fill this research gap, we present a German corpus of 1,320 essays from
school students of two age groups. Each essay has been manually annotated for
argumentative structure and quality on multiple levels of granularity. We
propose baseline approaches to argument mining and essay scoring, and we
analyze interactions between both tasks, thereby laying the ground for
quality-oriented argumentative writing support.
\\ ( https://arxiv.org/abs/2404.02529 ,  1153kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02534
Date: Wed, 3 Apr 2024 07:44:38 GMT   (67kb)

Title: ANGOFA: Leveraging OFA Embedding Initialization and Synthetic Data for
  Angolan Language Model
Authors: Osvaldo Luamba Quinjica, David Ifeoluwa Adelani
Categories: cs.CL cs.AI
\\
  In recent years, the development of pre-trained language models (PLMs) has
gained momentum, showcasing their capacity to transcend linguistic barriers and
facilitate knowledge transfer across diverse languages. However, this progress
has predominantly bypassed the inclusion of very-low resource languages,
creating a notable void in the multilingual landscape. This paper addresses
this gap by introducing four tailored PLMs specifically finetuned for Angolan
languages, employing a Multilingual Adaptive Fine-tuning (MAFT) approach. In
this paper, we survey the role of informed embedding initialization and
synthetic data in enhancing the performance of MAFT models in downstream tasks.
We improve baseline over SOTA AfroXLMR-base (developed through MAFT) and OFA
(an effective embedding initialization) by 12.3 and 3.8 points respectively.
\\ ( https://arxiv.org/abs/2404.02534 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02540
Date: Wed, 3 Apr 2024 07:55:57 GMT   (316kb)

Title: CSEPrompts: A Benchmark of Introductory Computer Science Prompts
Authors: Nishat Raihan, Dhiman Goswami, Sadiya Sayara Chowdhury Puspo,
  Christian Newman, Tharindu Ranasinghe, Marcos Zampieri
Categories: cs.CL
\\
  Recent advances in AI, machine learning, and NLP have led to the development
of a new generation of Large Language Models (LLMs) that are trained on massive
amounts of data and often have trillions of parameters. Commercial applications
(e.g., ChatGPT) have made this technology available to the general public, thus
making it possible to use LLMs to produce high-quality texts for academic and
professional purposes. Schools and universities are aware of the increasing use
of AI-generated content by students and they have been researching the impact
of this new technology and its potential misuse. Educational programs in
Computer Science (CS) and related fields are particularly affected because LLMs
are also capable of generating programming code in various programming
languages. To help understand the potential impact of publicly available LLMs
in CS education, we introduce CSEPrompts, a framework with hundreds of
programming exercise prompts and multiple-choice questions retrieved from
introductory CS and programming courses. We also provide experimental results
on CSEPrompts to evaluate the performance of several LLMs with respect to
generating Python code and answering basic computer science and programming
questions.
\\ ( https://arxiv.org/abs/2404.02540 ,  316kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02570
Date: Wed, 3 Apr 2024 08:44:51 GMT   (35kb,D)

Title: MaiNLP at SemEval-2024 Task 1: Analyzing Source Language Selection in
  Cross-Lingual Textual Relatedness
Authors: Shijia Zhou, Huangyan Shan, Barbara Plank, Robert Litschko
Categories: cs.CL
\\
  This paper presents our system developed for the SemEval-2024 Task 1:
Semantic Textual Relatedness (STR), on Track C: Cross-lingual. The task aims to
detect semantic relatedness of two sentences in a given target language without
access to direct supervision (i.e. zero-shot cross-lingual transfer). To this
end, we focus on different source language selection strategies on two
different pre-trained languages models: XLM-R and Furina. We experiment with 1)
single-source transfer and select source languages based on typological
similarity, 2) augmenting English training data with the two nearest-neighbor
source languages, and 3) multi-source transfer where we compare selecting on
all training languages against languages from the same family. We further study
machine translation-based data augmentation and the impact of script
differences. Our submission achieved the first place in the C8 (Kinyarwanda)
test set.
\\ ( https://arxiv.org/abs/2404.02570 ,  35kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02575
Date: Wed, 3 Apr 2024 08:49:11 GMT   (1323kb,D)

Title: Language Models as Compilers: Simulating Pseudocode Execution Improves
  Algorithmic Reasoning in Language Models
Authors: Hyungjoo Chae, Yeonghyeon Kim, Seungone Kim, Kai Tzu-iunn Ong,
  Beong-woo Kwak, Moohyeon Kim, Seonghwan Kim, Taeyoon Kwon, Jiwan Chung,
  Youngjae Yu, Jinyoung Yeo
Categories: cs.CL
Comments: 38 pages, 4 figures
\\
  Algorithmic reasoning refers to the ability to understand the complex
patterns behind the problem and decompose them into a sequence of reasoning
steps towards the solution. Such nature of algorithmic reasoning makes it a
challenge for large language models (LLMs), even though they have demonstrated
promising performance in other reasoning tasks. Within this context, some
recent studies use programming languages (e.g., Python) to express the
necessary logic for solving a given instance/question (e.g.,
Program-of-Thought) as inspired by their strict and precise syntaxes. However,
it is non-trivial to write an executable code that expresses the correct logic
on the fly within a single inference call. Also, the code generated
specifically for an instance cannot be reused for others, even if they are from
the same task and might require identical logic to solve. This paper presents
Think-and-Execute, a novel framework that decomposes the reasoning process of
language models into two steps. (1) In Think, we discover a task-level logic
that is shared across all instances for solving a given task and then express
the logic with pseudocode; (2) In Execute, we further tailor the generated
pseudocode to each instance and simulate the execution of the code. With
extensive experiments on seven algorithmic reasoning tasks, we demonstrate the
effectiveness of Think-and-Execute. Our approach better improves LMs' reasoning
compared to several strong baselines performing instance-specific reasoning
(e.g., CoT and PoT), suggesting the helpfulness of discovering task-level
logic. Also, we show that compared to natural language, pseudocode can better
guide the reasoning of LMs, even though they are trained to follow natural
language instructions.
\\ ( https://arxiv.org/abs/2404.02575 ,  1323kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02581
Date: Wed, 3 Apr 2024 08:56:00 GMT   (3317kb,D)

Title: Multi-Granularity Guided Fusion-in-Decoder
Authors: Eunseong Choi, Hyeri Lee, Jongwuk Lee
Categories: cs.CL cs.IR
Comments: Findings of the Association for Computational Linguistics: NAACL
  2024; 12 pages; 8 figures and 5 tables. Code and data available at
  http://github.com/eunseongc/MGFiD
\\
  In Open-domain Question Answering (ODQA), it is essential to discern relevant
contexts as evidence and avoid spurious ones among retrieved results. The model
architecture that uses concatenated multiple contexts in the decoding phase,
i.e., Fusion-in-Decoder, demonstrates promising performance but generates
incorrect outputs from seemingly plausible contexts. To address this problem,
we propose the Multi-Granularity guided Fusion-in-Decoder (MGFiD), discerning
evidence across multiple levels of granularity. Based on multi-task learning,
MGFiD harmonizes passage re-ranking with sentence classification. It aggregates
evident sentences into an anchor vector that instructs the decoder.
Additionally, it improves decoding efficiency by reusing the results of passage
re-ranking for passage pruning. Through our experiments, MGFiD outperforms
existing models on the Natural Questions (NQ) and TriviaQA (TQA) datasets,
highlighting the benefits of its multi-granularity solution.
\\ ( https://arxiv.org/abs/2404.02581 ,  3317kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02588
Date: Wed, 3 Apr 2024 09:13:26 GMT   (94kb)

Title: Large Language Models for Expansion of Spoken Language Understanding
  Systems to New Languages
Authors: Jakub Hoscilowicz, Pawel Pawlowski, Marcin Skorupa, Marcin Sowa\'nski,
  Artur Janicki
Categories: cs.CL
Comments: Code and info on model checkpoint are available at
  https://github.com/Samsung/MT-LLM-NLU
\\
  Spoken Language Understanding (SLU) models are a core component of voice
assistants (VA), such as Alexa, Bixby, and Google Assistant. In this paper, we
introduce a pipeline designed to extend SLU systems to new languages, utilizing
Large Language Models (LLMs) that we fine-tune for machine translation of
slot-annotated SLU training data. Our approach improved on the MultiATIS++
benchmark, a primary multi-language SLU dataset, in the cloud scenario using an
mBERT model. Specifically, we saw an improvement in the Overall Accuracy
metric: from 53% to 62.18%, compared to the existing state-of-the-art method,
Fine and Coarse-grained Multi-Task Learning Framework (FC-MTLF). In the
on-device scenario (tiny and not pretrained SLU), our method improved the
Overall Accuracy from 5.31% to 22.06% over the baseline Global-Local
Contrastive Learning Framework (GL-CLeF) method. Contrary to both FC-MTLF and
GL-CLeF, our LLM-based machine translation does not require changes in the
production architecture of SLU. Additionally, our pipeline is slot-type
independent: it does not require any slot definitions or examples.
\\ ( https://arxiv.org/abs/2404.02588 ,  94kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02589
Date: Wed, 3 Apr 2024 09:14:24 GMT   (12224kb,D)

Title: Affective-NLI: Towards Accurate and Interpretable Personality
  Recognition in Conversation
Authors: Zhiyuan Wen, Jiannong Cao, Yu Yang, Ruosong Yang, Shuaiqi Liu
Categories: cs.CL cs.AI
Comments: Accepted by IEEE PerCom 2024
\\
  Personality Recognition in Conversation (PRC) aims to identify the
personality traits of speakers through textual dialogue content. It is
essential for providing personalized services in various applications of
Human-Computer Interaction (HCI), such as AI-based mental therapy and companion
robots for the elderly. Most recent studies analyze the dialog content for
personality classification yet overlook two major concerns that hinder their
performance. First, crucial implicit factors contained in conversation, such as
emotions that reflect the speakers' personalities are ignored. Second, only
focusing on the input dialog content disregards the semantic understanding of
personality itself, which reduces the interpretability of the results. In this
paper, we propose Affective Natural Language Inference (Affective-NLI) for
accurate and interpretable PRC. To utilize affectivity within dialog content
for accurate personality recognition, we fine-tuned a pre-trained language
model specifically for emotion recognition in conversations, facilitating
real-time affective annotations for utterances. For interpretability of
recognition results, we formulate personality recognition as an NLI problem by
determining whether the textual description of personality labels is entailed
by the dialog content. Extensive experiments on two daily conversation datasets
suggest that Affective-NLI significantly outperforms (by 6%-7%)
state-of-the-art approaches. Additionally, our Flow experiment demonstrates
that Affective-NLI can accurately recognize the speaker's personality in the
early stages of conversations by surpassing state-of-the-art methods with
22%-34%.
\\ ( https://arxiv.org/abs/2404.02589 ,  12224kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02592
Date: Wed, 3 Apr 2024 09:17:38 GMT   (467kb)

Title: Leveraging the Interplay Between Syntactic and Acoustic Cues for
  Optimizing Korean TTS Pause Formation
Authors: Yejin Jeon, Yunsu Kim and Gary Geunbae Lee
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to LREC-COLING 2024
\\
  Contemporary neural speech synthesis models have indeed demonstrated
remarkable proficiency in synthetic speech generation as they have attained a
level of quality comparable to that of human-produced speech. Nevertheless, it
is important to note that these achievements have predominantly been verified
within the context of high-resource languages such as English. Furthermore, the
Tacotron and FastSpeech variants show substantial pausing errors when applied
to the Korean language, which affects speech perception and naturalness. In
order to address the aforementioned issues, we propose a novel framework that
incorporates comprehensive modeling of both syntactic and acoustic cues that
are associated with pausing patterns. Remarkably, our framework possesses the
capability to consistently generate natural speech even for considerably more
extended and intricate out-of-domain (OOD) sentences, despite its training on
short audio clips. Architectural design choices are validated through
comparisons with baseline models and ablation studies using subjective and
objective metrics, thus confirming model performance.
\\ ( https://arxiv.org/abs/2404.02592 ,  467kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02619
Date: Wed, 3 Apr 2024 10:13:18 GMT   (8311kb,D)

Title: Adjusting Interpretable Dimensions in Embedding Space with Human
  Judgments
Authors: Katrin Erk and Marianna Apidianaki
Categories: cs.CL
Comments: NAACL 2024
\\
  Embedding spaces contain interpretable dimensions indicating gender,
formality in style, or even object properties. This has been observed multiple
times. Such interpretable dimensions are becoming valuable tools in different
areas of study, from social science to neuroscience. The standard way to
compute these dimensions uses contrasting seed words and computes difference
vectors over them. This is simple but does not always work well. We combine
seed-based vectors with guidance from human ratings of where words fall along a
specific dimension, and evaluate on predicting both object properties like size
and danger, and the stylistic properties of formality and complexity. We obtain
interpretable dimensions with markedly better performance especially in cases
where seed-based dimensions do not work well.
\\ ( https://arxiv.org/abs/2404.02619 ,  8311kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02622
Date: Wed, 3 Apr 2024 10:22:35 GMT   (1805kb,D)

Title: Estimating the Causal Effects of Natural Logic Features in
  Transformer-Based NLI Models
Authors: Julia Rozanova, Marco Valentino, Andr\'e Freitas
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024 - Camera Ready. arXiv admin note:
  substantial text overlap with arXiv:2305.08572
\\
  Rigorous evaluation of the causal effects of semantic features on language
model predictions can be hard to achieve for natural language reasoning
problems. However, this is such a desirable form of analysis from both an
interpretability and model evaluation perspective, that it is valuable to
investigate specific patterns of reasoning with enough structure and regularity
to identify and quantify systematic reasoning failures in widely-used models.
In this vein, we pick a portion of the NLI task for which an explicit causal
diagram can be systematically constructed: the case where across two sentences
(the premise and hypothesis), two related words/terms occur in a shared
context. In this work, we apply causal effect estimation strategies to measure
the effect of context interventions (whose effect on the entailment label is
mediated by the semantic monotonicity characteristic) and interventions on the
inserted word-pair (whose effect on the entailment label is mediated by the
relation between these words). Extending related work on causal analysis of NLP
models in different settings, we perform an extensive interventional study on
the NLI task to investigate robustness to irrelevant changes and sensitivity to
impactful changes of Transformers. The results strongly bolster the fact that
similar benchmark accuracy scores may be observed for models that exhibit very
different behaviour. Moreover, our methodology reinforces previously suspected
biases from a causal perspective, including biases in favour of upward-monotone
contexts and ignoring the effects of negation markers.
\\ ( https://arxiv.org/abs/2404.02622 ,  1805kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02625
Date: Wed, 3 Apr 2024 10:29:06 GMT   (1372kb,D)

Title: A Differentiable Integer Linear Programming Solver for Explanation-Based
  Natural Language Inference
Authors: Mokanarangan Thayaparan, Marco Valentino, Andr\'e Freitas
Categories: cs.CL cs.AI cs.LG
Comments: Accepted to LREC-COLING 2024 - Camera Ready. arXiv admin note:
  substantial text overlap with arXiv:2208.03339
\\
  Integer Linear Programming (ILP) has been proposed as a formalism for
encoding precise structural and semantic constraints for Natural Language
Inference (NLI). However, traditional ILP frameworks are non-differentiable,
posing critical challenges for the integration of continuous language
representations based on deep learning. In this paper, we introduce a novel
approach, named Diff-Comb Explainer, a neuro-symbolic architecture for
explanation-based NLI based on Differentiable BlackBox Combinatorial Solvers
(DBCS). Differently from existing neuro-symbolic solvers, Diff-Comb Explainer
does not necessitate a continuous relaxation of the semantic constraints,
enabling a direct, more precise, and efficient incorporation of neural
representations into the ILP formulation. Our experiments demonstrate that
Diff-Comb Explainer achieves superior performance when compared to conventional
ILP solvers, neuro-symbolic black-box solvers, and Transformer-based encoders.
Moreover, a deeper analysis reveals that Diff-Comb Explainer can significantly
improve the precision, consistency, and faithfulness of the constructed
explanations, opening new opportunities for research on neuro-symbolic
architectures for explainable and transparent NLI in complex domains.
\\ ( https://arxiv.org/abs/2404.02625 ,  1372kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02655
Date: Wed, 3 Apr 2024 11:36:12 GMT   (3780kb,D)

Title: Calibrating the Confidence of Large Language Models by Eliciting
  Fidelity
Authors: Mozhi Zhang, Mianqiu Huang, Rundong Shi, Linsen Guo, Chong Peng, Peng
  Yan, Yaqian Zhou, Xipeng Qiu
Categories: cs.CL
Comments: 17 pages, 13 figures
\\
  Large language models optimized with techniques like RLHF have achieved good
alignment in being helpful and harmless. However, post-alignment, these
language models often exhibit overconfidence, where the expressed confidence
does not accurately calibrate with their correctness rate. In this paper, we
decompose the language model confidence into the \textit{Uncertainty} about the
question and the \textit{Fidelity} to the answer generated by language models.
Then, we propose a plug-and-play method to estimate the confidence of language
models. Our method has shown good calibration performance by conducting
experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two
novel metrics, IPR and CE, to evaluate the calibration of the model, and we
have conducted a detailed discussion on \textit{Truly Well-Calibrated
Confidence}. Our method could serve as a strong baseline, and we hope that this
work will provide some insights into the model confidence calibration.
\\ ( https://arxiv.org/abs/2404.02655 ,  3780kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02657
Date: Wed, 3 Apr 2024 11:40:17 GMT   (301kb,D)

Title: Rethinking Kullback-Leibler Divergence in Knowledge Distillation for
  Large Language Models
Authors: Taiqiang Wu, Chaofan Tao, Jiahao Wang, Zhe Zhao, Ngai Wong
Categories: cs.CL cs.AI
Comments: Under review as a conference paper at COLM 2024
\\
  Kullback-Leiber divergence has been widely used in Knowledge Distillation
(KD) to compress Large Language Models (LLMs). Contrary to prior assertions
that reverse Kullback-Leibler (RKL) divergence is mode-seeking and thus
preferable over the mean-seeking forward Kullback-Leibler (FKL) divergence,
this study empirically and theoretically demonstrates that neither mode-seeking
nor mean-seeking properties manifest in KD for LLMs. Instead, RKL and FKL are
found to share the same optimization objective and both converge after a
sufficient number of epochs. However, due to practical constraints, LLMs are
seldom trained for such an extensive number of epochs. Meanwhile, we further
find that RKL focuses on the tail part of the distributions, while FKL focuses
on the head part at the beginning epochs. Consequently, we propose a simple yet
effective Adaptive Kullback-Leiber (AKL) divergence method, which adaptively
allocates weights to combine FKL and RKL. Metric-based and GPT-4-based
evaluations demonstrate that the proposed AKL outperforms the baselines across
various tasks and improves the diversity and quality of generated responses.
\\ ( https://arxiv.org/abs/2404.02657 ,  301kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02681
Date: Wed, 3 Apr 2024 12:24:48 GMT   (148kb,D)

Title: PejorativITy: Disambiguating Pejorative Epithets to Improve Misogyny
  Detection in Italian Tweets
Authors: Arianna Muti, Federico Ruggeri, Cagri Toraman, Lorenzo Musetti, Samuel
  Algherini, Silvia Ronchi, Gianmarco Saretto, Caterina Zapparoli, Alberto
  Barr\'on-Cede\~no
Categories: cs.CL cs.AI
\\
  Misogyny is often expressed through figurative language. Some neutral words
can assume a negative connotation when functioning as pejorative epithets.
Disambiguating the meaning of such terms might help the detection of misogyny.
In order to address such task, we present PejorativITy, a novel corpus of 1,200
manually annotated Italian tweets for pejorative language at the word level and
misogyny at the sentence level. We evaluate the impact of injecting information
about disambiguated words into a model targeting misogyny detection. In
particular, we explore two different approaches for injection: concatenation of
pejorative information and substitution of ambiguous words with univocal terms.
Our experimental results, both on our corpus and on two popular benchmarks on
Italian tweets, show that both approaches lead to a major classification
improvement, indicating that word sense disambiguation is a promising
preliminary step for misogyny detection. Furthermore, we investigate LLMs'
understanding of pejorative epithets by means of contextual word embeddings
analysis and prompting.
\\ ( https://arxiv.org/abs/2404.02681 ,  148kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02684
Date: Wed, 3 Apr 2024 12:27:36 GMT   (111kb,D)

Title: Cross-Architecture Transfer Learning for Linear-Cost Inference
  Transformers
Authors: Sehyun Choi
Categories: cs.CL cs.AI cs.LG
Comments: Preprint
\\
  Recently, multiple architectures has been proposed to improve the efficiency
of the Transformer Language Models through changing the design of the
self-attention block to have a linear-cost inference (LCI). A notable approach
in this realm is the State-Space Machines (SSMs) architecture, which showed
on-par performance on language modeling tasks with the self-attention
transformers. However, such an architectural change requires a full pretraining
of the weights from scratch, which incurs a huge cost to researchers and
practitioners who want to use the new architectures. In the more traditional
linear attention works, it has been proposed to approximate full attention with
linear attention by swap-and-finetune framework. Motivated by this approach, we
propose Cross-Architecture Transfer Learning (XATL), in which the weights of
the shared components between LCI and self-attention-based transformers, such
as layernorms, MLPs, input/output embeddings, are directly transferred to the
new architecture from already pre-trained model parameters. We experimented the
efficacy of the method on varying sizes and alternative attention architectures
and show that \methodabbr significantly reduces the training time up to 2.5x
times and converges to a better minimum with up to 2.6% stronger model on the
LM benchmarks within the same compute budget.
\\ ( https://arxiv.org/abs/2404.02684 ,  111kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02699
Date: Wed, 3 Apr 2024 12:57:19 GMT   (3986kb,D)

Title: Scalable Model Editing via Customized Expert Networks
Authors: Zihan Yao, Yu He, Tianyu Qi and Ming Li
Categories: cs.CL
\\
  Addressing the issue of hallucinations and outdated knowledge in large
language models is critical for their reliable application. Model Editing
presents a promising avenue for mitigating these challenges in a cost-effective
manner. However, existing methods often suffer from unsatisfactory
generalization and unintended effects on unrelated samples. To overcome these
limitations, we introduce a novel approach: Scalable Model Editing via
Customized Expert Networks (SCEN), which is a two-stage continuous training
paradigm. Specifically, in the first stage, we train lightweight expert
networks individually for each piece of knowledge that needs to be updated.
Subsequently, we train a corresponding neuron for each expert to control the
activation state of that expert. Our experiments on two different sizes of
open-source large language models, the Llama2 7B and 13B, achieve
state-of-the-art results compared to existing mainstream Model Editing methods.
Our code is available at https: //github.com/TAL-auroraX/SCEN
\\ ( https://arxiv.org/abs/2404.02699 ,  3986kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02710
Date: Wed, 3 Apr 2024 13:08:26 GMT   (2436kb,D)

Title: ART: The Alternating Reading Task Corpus for Speech Entrainment and
  Imitation
Authors: Zheng Yuan, Dorina de Jong, \v{S}tefan Be\v{n}u\v{s}, No\"el Nguyen,
  Ruitao Feng, R\'obert Sabo, Luciano Fadiga, Alessandro D`Ausilio
Categories: cs.CL eess.AS
Comments: 15 pages, 2 figures, 7 tables, accepted at LREC-COLING 2024
  conference
\\
  We introduce the Alternating Reading Task (ART) Corpus, a collection of
dyadic sentence reading for studying the entrainment and imitation behaviour in
speech communication. The ART corpus features three experimental conditions -
solo reading, alternating reading, and deliberate imitation - as well as three
sub-corpora encompassing French-, Italian-, and Slovak-accented English. This
design allows systematic investigation of speech entrainment in a controlled
and less-spontaneous setting. Alongside detailed transcriptions, it includes
English proficiency scores, demographics, and in-experiment questionnaires for
probing linguistic, personal and interpersonal influences on entrainment. Our
presentation covers its design, collection, annotation processes, initial
analysis, and future research prospects.
\\ ( https://arxiv.org/abs/2404.02710 ,  2436kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02717
Date: Wed, 3 Apr 2024 13:20:24 GMT   (244kb,D)

Title: Automatic Prompt Selection for Large Language Models
Authors: Viet-Tung Do, Van-Khanh Hoang, Duy-Hung Nguyen, Shahab Sabahi, Jeff
  Yang, Hajime Hotta, Minh-Tien Nguyen, Hung Le
Categories: cs.CL cs.LG
Comments: preprint
\\
  Large Language Models (LLMs) can perform various natural language processing
tasks with suitable instruction prompts. However, designing effective prompts
manually is challenging and time-consuming. Existing methods for automatic
prompt optimization either lack flexibility or efficiency. In this paper, we
propose an effective approach to automatically select the optimal prompt for a
given input from a finite set of synthetic candidate prompts. Our approach
consists of three steps: (1) clustering the training data and generating
candidate prompts for each cluster using an LLM-based prompt generator; (2)
synthesizing a dataset of input-prompt-output tuples for training a prompt
evaluator to rank the prompts based on their relevance to the input; (3) using
the prompt evaluator to select the best prompt for a new input at test time.
Our approach balances prompt generality-specificity and eliminates the need for
resource-intensive training and inference. It demonstrates competitive
performance on zero-shot question-answering datasets: GSM8K, MultiArith, and
AQuA.
\\ ( https://arxiv.org/abs/2404.02717 ,  244kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02761
Date: Wed, 3 Apr 2024 14:07:02 GMT   (102kb,D)

Title: AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation
  Quality in Online Discussions Using LLMs
Authors: Maike Behrendt, Stefan Sylvius Wagner, Marc Ziegele, Lena Wilms, Anke
  Stoll, Dominique Heinbach and Stefan Harmeling
Categories: cs.CL cs.AI cs.LG
\\
  Measuring the quality of contributions in political online discussions is
crucial in deliberation research and computer science. Research has identified
various indicators to assess online discussion quality, and with deep learning
advancements, automating these measures has become feasible. While some studies
focus on analyzing specific quality indicators, a comprehensive quality score
incorporating various deliberative aspects is often preferred. In this work, we
introduce AQuA, an additive score that calculates a unified deliberative
quality score from multiple indices for each discussion post. Unlike other
singular scores, AQuA preserves information on the deliberative aspects present
in comments, enhancing model transparency. We develop adapter models for 20
deliberative indices, and calculate correlation coefficients between experts'
annotations and the perceived deliberativeness by non-experts to weigh the
individual indices into a single deliberative score. We demonstrate that the
AQuA score can be computed easily from pre-trained adapters and aligns well
with annotations on other datasets that have not be seen during training. The
analysis of experts' vs. non-experts' annotations confirms theoretical findings
in the social science literature.
\\ ( https://arxiv.org/abs/2404.02761 ,  102kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02772
Date: Wed, 3 Apr 2024 14:39:47 GMT   (624kb,D)

Title: FPT: Feature Prompt Tuning for Few-shot Readability Assessment
Authors: Ziyang Wang and Sanwoo Lee and Hsiu-Yuan Huang and Yunfang Wu
Categories: cs.CL
\\
  Prompt-based methods have achieved promising results in most few-shot text
classification tasks. However, for readability assessment tasks, traditional
prompt methods lackcrucial linguistic knowledge, which has already been proven
to be essential. Moreover, previous studies on utilizing linguistic features
have shown non-robust performance in few-shot settings and may even impair
model performance.To address these issues, we propose a novel prompt-based
tuning framework that incorporates rich linguistic knowledge, called Feature
Prompt Tuning (FPT). Specifically, we extract linguistic features from the text
and embed them into trainable soft prompts. Further, we devise a new loss
function to calibrate the similarity ranking order between categories.
Experimental results demonstrate that our proposed method FTP not only exhibits
a significant performance improvement over the prior best prompt-based tuning
approaches, but also surpasses the previous leading methods that incorporate
linguistic features. Also, our proposed model significantly outperforms the
large language model gpt-3.5-turbo-16k in most cases. Our proposed method
establishes a new architecture for prompt tuning that sheds light on how
linguistic features can be easily adapted to linguistic-related tasks.
\\ ( https://arxiv.org/abs/2404.02772 ,  624kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02800
Date: Wed, 3 Apr 2024 15:17:21 GMT   (1340kb,D)

Title: On Few-Shot Prompting for Controllable Question-Answer Generation in
  Narrative Comprehension
Authors: Bernardo Leite and Henrique Lopes Cardoso
Categories: cs.CL cs.AI
Comments: Preprint - Accepted for publication at CSEDU 2024
\\
  Question Generation aims to automatically generate questions based on a given
input provided as context. A controllable question generation scheme focuses on
generating questions with specific attributes, allowing better control. In this
study, we propose a few-shot prompting strategy for controlling the generation
of question-answer pairs from children's narrative texts. We aim to control two
attributes: the question's explicitness and underlying narrative elements. With
empirical evaluation, we show the effectiveness of controlling the generation
process by employing few-shot prompting side by side with a reference model.
Our experiments highlight instances where the few-shot strategy surpasses the
reference model, particularly in scenarios such as semantic closeness
evaluation and the diversity and coherency of question-answer pairs. However,
these improvements are not always statistically significant. The code is
publicly available at github.com/bernardoleite/few-shot-prompting-qg-control.
\\ ( https://arxiv.org/abs/2404.02800 ,  1340kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02823
Date: Wed, 3 Apr 2024 15:55:39 GMT   (2903kb,D)

Title: Conifer: Improving Complex Constrained Instruction-Following Ability of
  Large Language Models
Authors: Haoran Sun and Lixin Liu and Junjie Li and Fengyu Wang and Baohua Dong
  and Ran Lin and Ruohui Huang
Categories: cs.CL cs.AI cs.LG
\\
  The ability of large language models (LLMs) to follow instructions is crucial
to real-world applications. Despite recent advances, several studies have
highlighted that LLMs struggle when faced with challenging instructions,
especially those that include complex constraints, hindering their
effectiveness in various tasks. To address this challenge, we introduce
Conifer, a novel instruction tuning dataset, designed to enhance LLMs to follow
multi-level instructions with complex constraints. Utilizing GPT-4, we curate
the dataset by a series of LLM-driven refinement processes to ensure high
quality. We also propose a progressive learning scheme that emphasizes an
easy-to-hard progression, and learning from process feedback. Models trained
with Conifer exhibit remarkable improvements in instruction-following
abilities, especially for instructions with complex constraints. On several
instruction-following benchmarks, our 7B model outperforms the state-of-the-art
open-source 7B models, even exceeds the performance of models 10 times larger
on certain metrics. All the code and Conifer dataset are available at
https://www.github.com/ConiferLM/Conifer.
\\ ( https://arxiv.org/abs/2404.02823 ,  2903kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02835
Date: Wed, 3 Apr 2024 16:13:29 GMT   (81kb,D)

Title: Retrieving Examples from Memory for Retrieval Augmented Neural Machine
  Translation: A Systematic Comparison
Authors: Maxime Bouthors, Josep Crego, Francois Yvon
Categories: cs.CL
\\
  Retrieval-Augmented Neural Machine Translation (RAMT) architectures retrieve
examples from memory to guide the generation process. While most works in this
trend explore new ways to exploit the retrieved examples, the upstream
retrieval step is mostly unexplored. In this paper, we study the effect of
varying retrieval methods for several translation architectures, to better
understand the interplay between these two processes. We conduct experiments in
two language pairs in a multi-domain setting and consider several downstream
architectures based on a standard autoregressive model, an edit-based model,
and a large language model with in-context learning. Our experiments show that
the choice of the retrieval technique impacts the translation scores, with
variance across architectures. We also discuss the effects of increasing the
number and diversity of examples, which are mostly positive across the board.
\\ ( https://arxiv.org/abs/2404.02835 ,  81kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02837
Date: Wed, 3 Apr 2024 16:16:31 GMT   (466kb,D)

Title: Cherry on Top: Parameter Heterogeneity and Quantization in Large
  Language Models
Authors: Wanyun Cui, Qianle Wang
Categories: cs.CL
\\
  This paper reveals the phenomenon of parameter heterogeneity in large
language models (LLMs). We find that a small subset of ``cherry'' parameters
exhibit a disproportionately large influence on model performance, while the
vast majority of parameters have minimal impact. This heterogeneity is found to
be prevalent across different model families, scales, and types. Motivated by
this observation, we propose CherryQ, a novel quantization method that unifies
the optimization of mixed-precision parameters. CherryQ identifies and
preserves the critical cherry parameters in high precision while aggressively
quantizing the remaining parameters to low precision. Extensive experiments
demonstrate the effectiveness of CherryQ. CherryQ outperforms existing
quantization approaches in terms of perplexity and downstream task performance.
Notably, our 3-bit quantized Vicuna-1.5 exhibits competitive performance
compared to their 16-bit counterparts. These findings highlight the potential
of CherryQ for enabling efficient deployment of LLMs by taking advantage of
parameter heterogeneity.
\\ ( https://arxiv.org/abs/2404.02837 ,  466kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02893
Date: Wed, 3 Apr 2024 17:51:18 GMT   (713kb,D)

Title: ChatGLM-Math: Improving Math Problem-Solving in Large Language Models
  with a Self-Critique Pipeline
Authors: Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang,
  Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, Yuxiao Dong
Categories: cs.CL
\\
  Large language models (LLMs) have shown excellent mastering of human
language, but still struggle in real-world applications that require
mathematical problem-solving. While many strategies and datasets to enhance
LLMs' mathematics are developed, it remains a challenge to simultaneously
maintain and improve both language and mathematical capabilities in deployed
LLM systems.In this work, we tailor the Self-Critique pipeline, which addresses
the challenge in the feedback learning stage of LLM alignment. We first train a
general Math-Critique model from the LLM itself to provide feedback signals.
Then, we sequentially employ rejective fine-tuning and direct preference
optimization over the LLM's own generations for data collection. Based on
ChatGLM3-32B, we conduct a series of experiments on both academic and our newly
created challenging dataset, MathUserEval. Results show that our pipeline
significantly enhances the LLM's mathematical problem-solving while still
improving its language ability, outperforming LLMs that could be two times
larger. Related techniques have been deployed to
ChatGLM\footnote{\url{https://chatglm.cn}}, an online serving LLM. Related
evaluation dataset and scripts are released at
\url{https://github.com/THUDM/ChatGLM-Math}.
\\ ( https://arxiv.org/abs/2404.02893 ,  713kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02180
Date: Tue, 2 Apr 2024 09:15:32 GMT   (7149kb,D)

Title: Remote sensing framework for geological mapping via stacked autoencoders
  and clustering
Authors: Sandeep Nagar, Ehsan Farahbakhsh, Joseph Awange, Rohitash Chandra
Categories: cs.LG cs.AI
\\
  Supervised learning methods for geological mapping via remote sensing face
limitations due to the scarcity of accurately labelled training data. In
contrast, unsupervised learning methods, such as dimensionality reduction and
clustering have the ability to uncover patterns and structures in remote
sensing data without relying on predefined labels. Dimensionality reduction
methods have the potential to play a crucial role in improving the accuracy of
geological maps. Although conventional dimensionality reduction methods may
struggle with nonlinear data, unsupervised deep learning models such as
autoencoders have the ability to model nonlinear relationship in data. Stacked
autoencoders feature multiple interconnected layers to capture hierarchical
data representations that can be useful for remote sensing data. In this study,
we present an unsupervised machine learning framework for processing remote
sensing data by utilizing stacked autoencoders for dimensionality reduction and
k-means clustering for mapping geological units. We use the Landsat-8, ASTER,
and Sentinel-2 datasets of the Mutawintji region in Western New South Wales,
Australia to evaluate the framework for geological mapping. We also provide a
comparison of stacked autoencoders with principal component analysis and
canonical autoencoders. Our results reveal that the framework produces accurate
and interpretable geological maps, efficiently discriminating rock units. We
find that the stacked autoencoders provide better accuracy when compared to the
counterparts. We also find that the generated maps align with prior geological
knowledge of the study area while providing novel insights into geological
structures.
\\ ( https://arxiv.org/abs/2404.02180 ,  7149kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02181
Date: Tue, 2 Apr 2024 12:44:51 GMT   (1987kb,D)

Title: Leveraging Machine Learning for Early Autism Detection via INDT-ASD
  Indian Database
Authors: Trapti Shrivastava, Harshal Chaudhari and Vrijendra Singh
Categories: cs.LG cs.AI
\\
  Machine learning (ML) has advanced quickly, particularly throughout the area
of health care. The diagnosis of neurodevelopment problems using ML is a very
important area of healthcare. Autism spectrum disorder (ASD) is one of the
developmental disorders that is growing the fastest globally. The clinical
screening tests used to identify autistic symptoms are expensive and
time-consuming. But now that ML has been advanced, it's feasible to identify
autism early on. Previously, many different techniques have been used in
investigations. Still, none of them have produced the anticipated outcomes when
it comes to the capacity to predict autistic features utilizing a clinically
validated Indian ASD database. Therefore, this study aimed to develop a simple,
quick, and inexpensive technique for identifying ASD by using ML. Various
machine learning classifiers, including Adaboost (AB), Gradient Boost (GB),
Decision Tree (DT), Logistic Regression (LR), Random Forest (RF), Gaussian
Naive Bayes (GNB), Linear Discriminant Analysis (LDA), Quadratic Discriminant
Analysis (QDA), K-Nearest Neighbors (KNN), and Support Vector Machine (SVM),
were used to develop the autism prediction model. The proposed method was
tested with records from the AIIMS Modified INDT-ASD (AMI) database, which were
collected through an application developed by AIIMS in Delhi, India. Feature
engineering has been applied to make the proposed solution easier than already
available solutions. Using the proposed model, we succeeded in predicting ASD
using a minimized set of 20 questions rather than the 28 questions presented in
AMI with promising accuracy. In a comparative evaluation, SVM emerged as the
superior model among others, with 100 $\pm$ 0.05\% accuracy, higher recall by
5.34\%, and improved accuracy by 2.22\%-6.67\% over RF. We have also introduced
a web-based solution supporting both Hindi and English.
\\ ( https://arxiv.org/abs/2404.02181 ,  1987kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02184
Date: Tue, 2 Apr 2024 15:28:59 GMT   (1731kb,D)

Title: What is to be gained by ensemble models in analysis of spectroscopic
  data?
Authors: Katarina Domijan
Categories: cs.LG stat.ME
Comments: 14 pages, 8 figures, 1 algorithm
Journal-ref: Chemometrics and Intelligent Laboratory Systems, Volume 241, 2023,
  104936, ISSN 0169-7439
DOI: 10.1016/j.chemolab.2023.104936
\\
  An empirical study was carried out to compare different implementations of
ensemble models aimed at improving prediction in spectroscopic data. A wide
range of candidate models were fitted to benchmark datasets from regression and
classification settings. A statistical analysis using linear mixed model was
carried out on prediction performance criteria resulting from model fits over
random splits of the data. The results showed that the ensemble classifiers
were able to consistently outperform candidate models in our application
\\ ( https://arxiv.org/abs/2404.02184 ,  1731kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02187
Date: Tue, 2 Apr 2024 16:07:27 GMT   (2499kb)

Title: A Generative Deep Learning Approach for Crash Severity Modeling with
  Imbalanced Data
Authors: Junlan Chen, Ziyuan Pu, Nan Zheng, Xiao Wen, Hongliang Ding, Xiucheng
  Guo
Categories: cs.LG cs.AI
\\
  Crash data is often greatly imbalanced, with the majority of crashes being
non-fatal crashes, and only a small number being fatal crashes due to their
rarity. Such data imbalance issue poses a challenge for crash severity modeling
since it struggles to fit and interpret fatal crash outcomes with very limited
samples. Usually, such data imbalance issues are addressed by data resampling
methods, such as under-sampling and over-sampling techniques. However, most
traditional and deep learning-based data resampling methods, such as synthetic
minority oversampling technique (SMOTE) and generative Adversarial Networks
(GAN) are designed dedicated to processing continuous variables. Though some
resampling methods have improved to handle both continuous and discrete
variables, they may have difficulties in dealing with the collapse issue
associated with sparse discrete risk factors. Moreover, there is a lack of
comprehensive studies that compare the performance of various resampling
methods in crash severity modeling. To address the aforementioned issues, the
current study proposes a crash data generation method based on the Conditional
Tabular GAN. After data balancing, a crash severity model is employed to
estimate the performance of classification and interpretation. A comparative
study is conducted to assess classification accuracy and distribution
consistency of the proposed generation method using a 4-year imbalanced crash
dataset collected in Washington State, U.S. Additionally, Monte Carlo
simulation is employed to estimate the performance of parameter and probability
estimation in both two- and three-class imbalance scenarios. The results
indicate that using synthetic data generated by CTGAN-RU for crash severity
modeling outperforms using original data or synthetic data generated by other
resampling methods.
\\ ( https://arxiv.org/abs/2404.02187 ,  2499kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02189
Date: Tue, 2 Apr 2024 16:48:34 GMT   (2294kb,D)

Title: Insights from the Use of Previously Unseen Neural Architecture Search
  Datasets
Authors: Rob Geada, David Towers, Matthew Forshaw, Amir Atapour-Abarghouei, A.
  Stephen McGough
Categories: cs.LG cs.AI cs.CV
\\
  The boundless possibility of neural networks which can be used to solve a
problem -- each with different performance -- leads to a situation where a Deep
Learning expert is required to identify the best neural network. This goes
against the hope of removing the need for experts. Neural Architecture Search
(NAS) offers a solution to this by automatically identifying the best
architecture. However, to date, NAS work has focused on a small set of datasets
which we argue are not representative of real-world problems. We introduce
eight new datasets created for a series of NAS Challenges: AddNIST, Language,
MultNIST, CIFARTile, Gutenberg, Isabella, GeoClassing, and Chesseract. These
datasets and challenges are developed to direct attention to issues in NAS
development and to encourage authors to consider how their models will perform
on datasets unknown to them at development time. We present experimentation
using standard Deep Learning methods as well as the best results from challenge
participants.
\\ ( https://arxiv.org/abs/2404.02189 ,  2294kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02234
Date: Tue, 2 Apr 2024 18:44:53 GMT   (31326kb,D)

Title: Deep Neural Networks with 3D Point Clouds for Empirical Friction
  Measurements in Hydrodynamic Flood Models
Authors: Francisco Haces-Garcia, Vasileios Kotzamanis, Craig Glennie, Hanadi
  Rifai
Categories: cs.LG physics.flu-dyn
Comments: 25 pages, 15 figures, 2 tables. Implementation code available at
  https://github.com/f-haces/LidarManning
\\
  Friction is one of the cruxes of hydrodynamic modeling; flood conditions are
highly sensitive to the Friction Factors (FFs) used to calculate momentum
losses. However, empirical FFs are challenging to measure because they require
laboratory experiments. Flood models often rely on surrogate observations (such
as land use) to estimate FFs, introducing uncertainty. This research presents a
laboratory-trained Deep Neural Network (DNN), trained using flume experiments
with data augmentation techniques, to measure Manning's n based on Point Cloud
data. The DNN was deployed on real-world lidar Point Clouds to directly measure
Manning's n under regulatory and extreme storm events, showing improved
prediction capabilities in both 1D and 2D hydrodynamic models. For 1D models,
the lidar values decreased differences with regulatory models for in-channel
water depth when compared to land cover values. For 1D/2D coupled models, the
lidar values produced better agreement with flood extents measured from
airborne imagery, while better matching flood insurance claim data for
Hurricane Harvey. In both 1D and 1D/2D coupled models, lidar resulted in better
agreement with validation gauges. For these reasons, the lidar measurements of
Manning's n were found to improve both regulatory models and forecasts for
extreme storm events, while simultaneously providing a pathway to standardize
the measurement of FFs. Changing FFs significantly affected fluvial and pluvial
flood models, while surge flooding was generally unaffected. Downstream flow
conditions were found to change the importance of FFs to fluvial models,
advancing the literature of friction in flood models. This research introduces
a reliable, repeatable, and readily-accessible avenue to measure
high-resolution FFs based on 3D point clouds, improving flood prediction, and
removing uncertainty from hydrodynamic modeling.
\\ ( https://arxiv.org/abs/2404.02234 ,  31326kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02235
Date: Tue, 2 Apr 2024 18:45:01 GMT   (9139kb,D)

Title: Is Exploration All You Need? Effective Exploration Characteristics for
  Transfer in Reinforcement Learning
Authors: Jonathan C. Balloch, Rishav Bhagat, Geigh Zollicoffer, Ruoran Jia,
  Julia Kim, Mark O. Riedl
Categories: cs.LG cs.AI
\\
  In deep reinforcement learning (RL) research, there has been a concerted
effort to design more efficient and productive exploration methods while
solving sparse-reward problems. These exploration methods often share common
principles (e.g., improving diversity) and implementation details (e.g.,
intrinsic reward). Prior work found that non-stationary Markov decision
processes (MDPs) require exploration to efficiently adapt to changes in the
environment with online transfer learning. However, the relationship between
specific exploration characteristics and effective transfer learning in deep RL
has not been characterized. In this work, we seek to understand the
relationships between salient exploration characteristics and improved
performance and efficiency in transfer learning. We test eleven popular
exploration algorithms on a variety of transfer types -- or ``novelties'' -- to
identify the characteristics that positively affect online transfer learning.
Our analysis shows that some characteristics correlate with improved
performance and efficiency across a wide range of transfer tasks, while others
only improve transfer performance with respect to specific environment changes.
From our analysis, make recommendations about which exploration algorithm
characteristics are best suited to specific transfer situations.
\\ ( https://arxiv.org/abs/2404.02235 ,  9139kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02258
Date: Tue, 2 Apr 2024 19:28:11 GMT   (1763kb,D)

Title: Mixture-of-Depths: Dynamically allocating compute in transformer-based
  language models
Authors: David Raposo, Sam Ritter, Blake Richards, Timothy Lillicrap, Peter
  Conway Humphreys, Adam Santoro
Categories: cs.LG cs.CL
\\
  Transformer-based language models spread FLOPs uniformly across input
sequences. In this work we demonstrate that transformers can instead learn to
dynamically allocate FLOPs (or compute) to specific positions in a sequence,
optimising the allocation along the sequence for different layers across the
model depth. Our method enforces a total compute budget by capping the number
of tokens ($k$) that can participate in the self-attention and MLP computations
at a given layer. The tokens to be processed are determined by the network
using a top-$k$ routing mechanism. Since $k$ is defined a priori, this simple
procedure uses a static computation graph with known tensor sizes, unlike other
conditional computation techniques. Nevertheless, since the identities of the
$k$ tokens are fluid, this method can expend FLOPs non-uniformly across the
time and model depth dimensions. Thus, compute expenditure is entirely
predictable in sum total, but dynamic and context-sensitive at the token-level.
Not only do models trained in this way learn to dynamically allocate compute,
they do so efficiently. These models match baseline performance for equivalent
FLOPS and wall-clock times to train, but require a fraction of the FLOPs per
forward pass, and can be upwards of 50\% faster to step during post-training
sampling.
\\ ( https://arxiv.org/abs/2404.02258 ,  1763kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02300
Date: Tue, 2 Apr 2024 20:55:39 GMT   (682kb,D)

Title: CATGNN: Cost-Efficient and Scalable Distributed Training for Graph
  Neural Networks
Authors: Xin Huang, Weipeng Zhuo, Minh Phu Vuong, Shiju Li, Jongryool Kim,
  Bradley Rees, Chul-Ho Lee
Categories: cs.LG cs.DC
\\
  Graph neural networks have been shown successful in recent years. While
different GNN architectures and training systems have been developed, GNN
training on large-scale real-world graphs still remains challenging. Existing
distributed systems load the entire graph in memory for graph partitioning,
requiring a huge memory space to process large graphs and thus hindering GNN
training on such large graphs using commodity workstations. In this paper, we
propose CATGNN, a cost-efficient and scalable distributed GNN training system
which focuses on scaling GNN training to billion-scale or larger graphs under
limited computational resources. Among other features, it takes a stream of
edges as input, instead of loading the entire graph in memory, for
partitioning. We also propose a novel streaming partitioning algorithm named
SPRING for distributed GNN training. We verify the correctness and
effectiveness of CATGNN with SPRING on 16 open datasets. In particular, we
demonstrate that CATGNN can handle the largest publicly available dataset with
limited memory, which would have been infeasible without increasing the memory
space. SPRING also outperforms state-of-the-art partitioning algorithms
significantly, with a 50% reduction in replication factor on average.
\\ ( https://arxiv.org/abs/2404.02300 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02304
Date: Tue, 2 Apr 2024 21:03:17 GMT   (698kb,D)

Title: Virtual Sensor for Real-Time Bearing Load Prediction Using Heterogeneous
  Temporal Graph Neural Networks
Authors: Mengjie Zhao, Cees Taal, Stephan Baggerohr, and Olga Fink
Categories: cs.LG cs.AI cs.ET
Comments: 8 pages, 6 figures
\\
  Accurate bearing load monitoring is essential for their Prognostics and
Health Management (PHM), enabling damage assessment, wear prediction, and
proactive maintenance. While bearing sensors are typically placed on the
bearing housing, direct load monitoring requires sensors inside the bearing
itself. Recently introduced sensor rollers enable direct bearing load
monitoring but are constrained by their battery life. Data-driven virtual
sensors can learn from sensor roller data collected during a batterys lifetime
to map operating conditions to bearing loads. Although spatially distributed
bearing sensors offer insights into load distribution (e.g., correlating
temperature with load), traditional machine learning algorithms struggle to
fully exploit these spatial-temporal dependencies. To address this gap, we
introduce a graph-based virtual sensor that leverages Graph Neural Networks
(GNNs) to analyze spatial-temporal dependencies among sensor signals, mapping
existing measurements (temperature, vibration) to bearing loads. Since
temperature and vibration signals exhibit vastly different dynamics, we propose
Heterogeneous Temporal Graph Neural Networks (HTGNN), which explicitly models
these signal types and their interactions for effective load prediction. Our
results demonstrate that HTGNN outperforms Convolutional Neural Networks
(CNNs), which struggle to capture both spatial and heterogeneous signal
characteristics. These findings highlight the importance of capturing the
complex spatial interactions between temperature, vibration, and load.
\\ ( https://arxiv.org/abs/2404.02304 ,  698kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02314
Date: Tue, 2 Apr 2024 21:20:51 GMT   (1269kb,D)

Title: Is Meta-training Really Necessary for Molecular Few-Shot Learning ?
Authors: Philippe Formont, Hugo Jeannin, Pablo Piantanida, Ismail Ben Ayed
Categories: cs.LG cs.AI
\\
  Few-shot learning has recently attracted significant interest in drug
discovery, with a recent, fast-growing literature mostly involving convoluted
meta-learning strategies. We revisit the more straightforward fine-tuning
approach for molecular data, and propose a regularized quadratic-probe loss
based on the the Mahalanobis distance. We design a dedicated block-coordinate
descent optimizer, which avoid the degenerate solutions of our loss.
Interestingly, our simple fine-tuning approach achieves highly competitive
performances in comparison to state-of-the-art methods, while being applicable
to black-box settings and removing the need for specific episodic pre-training
strategies. Furthermore, we introduce a new benchmark to assess the robustness
of the competing methods to domain shifts. In this setting, our fine-tuning
baseline obtains consistently better results than meta-learning methods.
\\ ( https://arxiv.org/abs/2404.02314 ,  1269kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02325
Date: Tue, 2 Apr 2024 21:51:39 GMT   (21kb)

Title: Heat Death of Generative Models in Closed-Loop Learning
Authors: Matteo Marchi, Stefano Soatto, Pratik Chaudhari, Paulo Tabuada
Categories: cs.LG
\\
  Improvement and adoption of generative machine learning models is rapidly
accelerating, as exemplified by the popularity of LLMs (Large Language Models)
for text, and diffusion models for image generation.As generative models become
widespread, data they generate is incorporated into shared content through the
public web. This opens the question of what happens when data generated by a
model is fed back to the model in subsequent training campaigns. This is a
question about the stability of the training process, whether the distribution
of publicly accessible content, which we refer to as "knowledge", remains
stable or collapses.
  Small scale empirical experiments reported in the literature show that this
closed-loop training process is prone to degenerating. Models may start
producing gibberish data, or sample from only a small subset of the desired
data distribution (a phenomenon referred to as mode collapse). So far there has
been only limited theoretical understanding of this process, in part due to the
complexity of the deep networks underlying these generative models.
  The aim of this paper is to provide insights into this process (that we refer
to as "generative closed-loop learning") by studying the learning dynamics of
generative models that are fed back their own produced content in addition to
their original training dataset. The sampling of many of these models can be
controlled via a "temperature" parameter. Using dynamical systems tools, we
show that, unless a sufficient amount of external data is introduced at each
iteration, any non-trivial temperature leads the model to asymptotically
degenerate. In fact, either the generative distribution collapses to a small
set of outputs, or becomes uniform over a large set of outputs.
\\ ( https://arxiv.org/abs/2404.02325 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02359
Date: Tue, 2 Apr 2024 23:05:56 GMT   (15kb)

Title: Attribution Regularization for Multimodal Paradigms
Authors: Sahiti Yerramilli, Jayant Sravan Tamarapalli, Jonathan Francis, Eric
  Nyberg
Categories: cs.LG
\\
  Multimodal machine learning has gained significant attention in recent years
due to its potential for integrating information from multiple modalities to
enhance learning and decision-making processes. However, it is commonly
observed that unimodal models outperform multimodal models, despite the latter
having access to richer information. Additionally, the influence of a single
modality often dominates the decision-making process, resulting in suboptimal
performance. This research project aims to address these challenges by
proposing a novel regularization term that encourages multimodal models to
effectively utilize information from all modalities when making decisions. The
focus of this project lies in the video-audio domain, although the proposed
regularization technique holds promise for broader applications in embodied AI
research, where multiple modalities are involved. By leveraging this
regularization term, the proposed approach aims to mitigate the issue of
unimodal dominance and improve the performance of multimodal machine learning
systems. Through extensive experimentation and evaluation, the effectiveness
and generalizability of the proposed technique will be assessed. The findings
of this research project have the potential to significantly contribute to the
advancement of multimodal machine learning and facilitate its application in
various domains, including multimedia analysis, human-computer interaction, and
embodied AI research.
\\ ( https://arxiv.org/abs/2404.02359 ,  15kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02360
Date: Tue, 2 Apr 2024 23:16:15 GMT   (507kb,D)

Title: FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction
Authors: Adamo Young, Fei Wang, David Wishart, Bo Wang, Hannes R\"ost, Russ
  Greiner
Categories: cs.LG q-bio.BM
Comments: 21 pages, 4 figures, 9 tables
\\
  The process of identifying a compound from its mass spectrum is a critical
step in the analysis of complex mixtures. Typical solutions for the mass
spectrum to compound (MS2C) problem involve matching the unknown spectrum
against a library of known spectrum-molecule pairs, an approach that is limited
by incomplete library coverage. Compound to mass spectrum (C2MS) models can
improve retrieval rates by augmenting real libraries with predicted spectra.
Unfortunately, many existing C2MS models suffer from problems with prediction
resolution, scalability, or interpretability. We develop a new probabilistic
method for C2MS prediction, FraGNNet, that can efficiently and accurately
predict high-resolution spectra. FraGNNet uses a structured latent space to
provide insight into the underlying processes that define the spectrum. Our
model achieves state-of-the-art performance in terms of prediction error, and
surpasses existing C2MS models as a tool for retrieval-based MS2C.
\\ ( https://arxiv.org/abs/2404.02360 ,  507kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02395
Date: Wed, 3 Apr 2024 01:42:30 GMT   (1341kb,D)

Title: Optimal Batch Allocation for Wireless Federated Learning
Authors: Jaeyoung Song, Sang-Woon Jeon
Categories: cs.LG cs.DC
\\
  Federated learning aims to construct a global model that fits the dataset
distributed across local devices without direct access to private data,
leveraging communication between a server and the local devices. In the context
of a practical communication scheme, we study the completion time required to
achieve a target performance. Specifically, we analyze the number of iterations
required for federated learning to reach a specific optimality gap from a
minimum global loss. Subsequently, we characterize the time required for each
iteration under two fundamental multiple access schemes: time-division multiple
access (TDMA) and random access (RA). We propose a step-wise batch allocation,
demonstrated to be optimal for TDMA-based federated learning systems.
Additionally, we show that the non-zero batch gap between devices provided by
the proposed step-wise batch allocation significantly reduces the completion
time for RA-based learning systems. Numerical evaluations validate these
analytical results through real-data experiments, highlighting the remarkable
potential for substantial completion time reduction.
\\ ( https://arxiv.org/abs/2404.02395 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02424
Date: Wed, 3 Apr 2024 03:27:01 GMT   (453kb,D)

Title: RESSA: Repair Sparse Vision-Language Models via Sparse Cross-Modality
  Adaptation
Authors: Shwai He, Tianlong Chen
Categories: cs.LG cs.CV
\\
  Vision-Language Models (VLMs), integrating diverse information from multiple
modalities, have shown remarkable success across various tasks. However,
deploying VLMs, comprising large-scale vision and language models poses
challenges in resource-constrained scenarios. While pruning followed by
finetuning offers a potential solution to maintain performance with smaller
model sizes, its application to VLMs remains relatively unexplored, presenting
two main questions: how to distribute sparsity across different
modality-specific models, and how to repair the performance of pruned sparse
VLMs. To answer the first question, we conducted preliminary studies on VLM
pruning and found that pruning vision models and language models with the same
sparsity ratios contribute to nearly optimal performance. For the second
question, unlike finetuning unimodal sparse models, sparse VLMs involve
cross-modality interactions, requiring specialized techniques for post-pruning
performance repair. Moreover, while parameter-efficient LoRA finetuning has
been proposed to repair the performance of sparse models, a significant
challenge of weights merging arises due to the incompatibility of dense LoRA
modules with sparse models that destroy the sparsity of pruned models. To
tackle these challenges, we propose to Repair Sparse Vision-Language Models via
Sparse Cross-modality Adaptation (RESSA). RESSA utilizes cross-modality
finetuning to enhance task-specific performance and facilitate knowledge
distillation from original dense models. Additionally, we introduce SparseLoRA,
which applies sparsity directly to LoRA weights, enabling seamless integration
with sparse models. Our experimental results validate the effectiveness of
RESSA, showcasing significant enhancements, such as an 11.3\% improvement under
2:4 sparsity and a remarkable 47.6\% enhancement under unstructured 70\%
sparsity.
\\ ( https://arxiv.org/abs/2404.02424 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02429
Date: Wed, 3 Apr 2024 03:36:35 GMT   (1257kb,D)

Title: AD4RL: Autonomous Driving Benchmarks for Offline Reinforcement Learning
  with Value-based Dataset
Authors: Dongsu Lee, Chanin Eom, Minhae Kwon
Categories: cs.LG cs.AI
Comments: ICRA 2024 Website at: https://sites.google.com/view/ad4rl
\\
  Offline reinforcement learning has emerged as a promising technology by
enhancing its practicality through the use of pre-collected large datasets.
Despite its practical benefits, most algorithm development research in offline
reinforcement learning still relies on game tasks with synthetic datasets. To
address such limitations, this paper provides autonomous driving datasets and
benchmarks for offline reinforcement learning research. We provide 19 datasets,
including real-world human driver's datasets, and seven popular offline
reinforcement learning algorithms in three realistic driving scenarios. We also
provide a unified decision-making process model that can operate effectively
across different scenarios, serving as a reference framework in algorithm
design. Our research lays the groundwork for further collaborations in the
community to explore practical aspects of existing reinforcement learning
methods. Dataset and codes can be found in https://sites.google.com/view/ad4rl.
\\ ( https://arxiv.org/abs/2404.02429 ,  1257kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02446
Date: Wed, 3 Apr 2024 04:23:01 GMT   (40875kb,D)

Title: Masked Completion via Structured Diffusion with White-Box Transformers
Authors: Druv Pai, Ziyang Wu, Sam Buchanan, Yaodong Yu, Yi Ma
Categories: cs.LG stat.ML
Comments: To be published at ICLR 2024; 44 pages. arXiv admin note: substantial
  text overlap with arXiv:2311.13110
\\
  Modern learning frameworks often train deep neural networks with massive
amounts of unlabeled data to learn representations by solving simple pretext
tasks, then use the representations as foundations for downstream tasks. These
networks are empirically designed; as such, they are usually not interpretable,
their representations are not structured, and their designs are potentially
redundant. White-box deep networks, in which each layer explicitly identifies
and transforms structures in the data, present a promising alternative.
However, existing white-box architectures have only been shown to work at scale
in supervised settings with labeled data, such as classification. In this work,
we provide the first instantiation of the white-box design paradigm that can be
applied to large-scale unsupervised representation learning. We do this by
exploiting a fundamental connection between diffusion, compression, and
(masked) completion, deriving a deep transformer-like masked autoencoder
architecture, called CRATE-MAE, in which the role of each layer is
mathematically fully interpretable: they transform the data distribution to and
from a structured representation. Extensive empirical evaluations confirm our
analytical insights. CRATE-MAE demonstrates highly promising performance on
large-scale imagery datasets while using only ~30% of the parameters compared
to the standard masked autoencoder with the same model configuration. The
representations learned by CRATE-MAE have explicit structure and also contain
semantic meaning. Code is available at https://github.com/Ma-Lab-Berkeley/CRATE .
\\ ( https://arxiv.org/abs/2404.02446 ,  40875kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02450
Date: Wed, 3 Apr 2024 04:31:09 GMT   (438kb,D)

Title: Task Agnostic Architecture for Algorithm Induction via Implicit
  Composition
Authors: Sahil J. Sindhi, Ignas Budvytis
Categories: cs.LG cs.AI
Comments: 12 pages, 2 figures, 2024 ICLR Generative Models for Decision Making
  Workshop
\\
  Different fields in applied machine learning such as computer vision, speech
or natural language processing have been building domain-specialised solutions.
Currently, we are witnessing an opposing trend towards developing more
generalist architectures, driven by Large Language Models and multi-modal
foundational models. These architectures are designed to tackle a variety of
tasks, including those previously unseen and using inputs across multiple
modalities. Taking this trend of generalization to the extreme suggests the
possibility of a single deep network architecture capable of solving all tasks.
This position paper aims to explore developing such a unified architecture and
proposes a theoretical framework of how it could be constructed. Our proposal
is based on the following assumptions. Firstly, tasks are solved by following a
sequence of instructions, typically implemented in code for conventional
computing hardware, which inherently operates sequentially. Second, recent
Generative AI, especially Transformer-based models, demonstrate potential as an
architecture capable of constructing algorithms for a wide range of domains.
For example, GPT-4 shows exceptional capability at in-context learning of novel
tasks which is hard to explain in any other way than the ability to compose
novel solutions from fragments on previously learnt algorithms. Third, the
observation that the main missing component in developing a truly generalised
network is an efficient approach for self-consistent input of previously learnt
sub-steps of an algorithm and their (implicit) composition during the network's
internal forward pass. Our exploration delves into current capabilities and
limitations of Transformer-based and other methods in efficient and correct
algorithm composition and proposes a Transformer-like architecture as well as a
discrete learning framework to overcome these limitations.
\\ ( https://arxiv.org/abs/2404.02450 ,  438kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02461
Date: Wed, 3 Apr 2024 05:04:06 GMT   (730kb,D)

Title: On the Efficiency and Robustness of Vibration-based Foundation Models
  for IoT Sensing: A Case Study
Authors: Tomoyoshi Kimura, Jinyang Li, Tianshi Wang, Denizhan Kara, Yizhuo
  Chen, Yigong Hu, Ruijie Wang, Maggie Wigness, Shengzhong Liu, Mani
  Srivastava, Suhas Diggavi, Tarek Abdelzaher
Categories: cs.LG eess.SP
\\
  This paper demonstrates the potential of vibration-based Foundation Models
(FMs), pre-trained with unlabeled sensing data, to improve the robustness of
run-time inference in (a class of) IoT applications. A case study is presented
featuring a vehicle classification application using acoustic and seismic
sensing. The work is motivated by the success of foundation models in the areas
of natural language processing and computer vision, leading to generalizations
of the FM concept to other domains as well, where significant amounts of
unlabeled data exist that can be used for self-supervised pre-training. One
such domain is IoT applications. Foundation models for selected sensing
modalities in the IoT domain can be pre-trained in an environment-agnostic
fashion using available unlabeled sensor data and then fine-tuned to the
deployment at hand using a small amount of labeled data. The paper shows that
the pre-training/fine-tuning approach improves the robustness of downstream
inference and facilitates adaptation to different environmental conditions.
More specifically, we present a case study in a real-world setting to evaluate
a simple (vibration-based) FM-like model, called FOCAL, demonstrating its
superior robustness and adaptation, compared to conventional supervised deep
neural networks (DNNs). We also demonstrate its superior convergence over
supervised solutions. Our findings highlight the advantages of vibration-based
FMs (and FM-inspired selfsupervised models in general) in terms of inference
robustness, runtime efficiency, and model adaptation (via fine-tuning) in
resource-limited IoT settings.
\\ ( https://arxiv.org/abs/2404.02461 ,  730kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02478
Date: Wed, 3 Apr 2024 05:36:21 GMT   (5528kb,D)

Title: FedSelect: Personalized Federated Learning with Customized Selection of
  Parameters for Fine-Tuning
Authors: Rishub Tamirisa, Chulin Xie, Wenxuan Bao, Andy Zhou, Ron Arel, Aviv
  Shamsian
Categories: cs.LG cs.AI
Comments: Published in CVPR 2024
\\
  Standard federated learning approaches suffer when client data distributions
have sufficient heterogeneity. Recent methods addressed the client data
heterogeneity issue via personalized federated learning (PFL) - a class of FL
algorithms aiming to personalize learned global knowledge to better suit the
clients' local data distributions. Existing PFL methods usually decouple global
updates in deep neural networks by performing personalization on particular
layers (i.e. classifier heads) and global aggregation for the rest of the
network. However, preselecting network layers for personalization may result in
suboptimal storage of global knowledge. In this work, we propose FedSelect, a
novel PFL algorithm inspired by the iterative subnetwork discovery procedure
used for the Lottery Ticket Hypothesis. FedSelect incrementally expands
subnetworks to personalize client parameters, concurrently conducting global
aggregations on the remaining parameters. This approach enables the
personalization of both client parameters and subnetwork structure during the
training process. Finally, we show that FedSelect outperforms recent
state-of-the-art PFL algorithms under challenging client data heterogeneity
settings and demonstrates robustness to various real-world distributional
shifts. Our code is available at https://github.com/lapisrocks/fedselect.
\\ ( https://arxiv.org/abs/2404.02478 ,  5528kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02484
Date: Wed, 3 Apr 2024 05:44:03 GMT   (447kb,D)

Title: New methods for drug synergy prediction
Authors: Fatemeh Abbasi and Juho Rousu
Categories: cs.LG cs.AI q-bio.QM
ACM-class: I.2.6; J.3
\\
  In this mini-review, we explore the new prediction methods for drug
combination synergy relying on high-throughput combinatorial screens. The fast
progress of the field is witnessed in the more than thirty original machine
learning methods published since 2021, a clear majority of them based on deep
learning techniques. We aim to put these papers under a unifying lens by
highlighting the core technologies, the data sources, the input data types and
synergy scores used in the methods, as well as the prediction scenarios and
evaluation protocols that the papers deal with. Our finding is that the best
methods accurately solve the synergy prediction scenarios involving known drugs
or cell lines while the scenarios involving new drugs or cell lines still fall
short of an accurate prediction level.
\\ ( https://arxiv.org/abs/2404.02484 ,  447kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02510
Date: Wed, 3 Apr 2024 06:53:56 GMT   (120kb,D)

Title: An Interpretable Client Decision Tree Aggregation process for Federated
  Learning
Authors: Alberto Argente-Garrido and Cristina Zuheros and M. Victoria Luz\'on
  and Francisco Herrera
Categories: cs.LG cs.AI
Comments: Submitted to Information Science Journal
\\
  Trustworthy Artificial Intelligence solutions are essential in today's
data-driven applications, prioritizing principles such as robustness, safety,
transparency, explainability, and privacy among others. This has led to the
emergence of Federated Learning as a solution for privacy and distributed
machine learning. While decision trees, as self-explanatory models, are ideal
for collaborative model training across multiple devices in
resource-constrained environments such as federated learning environments for
injecting interpretability in these models. Decision tree structure makes the
aggregation in a federated learning environment not trivial. They require
techniques that can merge their decision paths without introducing bias or
overfitting while keeping the aggregated decision trees robust and
generalizable. In this paper, we propose an Interpretable Client Decision Tree
Aggregation process for Federated Learning scenarios that keeps the
interpretability and the precision of the base decision trees used for the
aggregation. This model is based on aggregating multiple decision paths of the
decision trees and can be used on different decision tree types, such as ID3
and CART. We carry out the experiments within four datasets, and the analysis
shows that the tree built with the model improves the local models, and
outperforms the state-of-the-art.
\\ ( https://arxiv.org/abs/2404.02510 ,  120kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02545
Date: Wed, 3 Apr 2024 08:03:27 GMT   (1749kb,D)

Title: Grid-Mapping Pseudo-Count Constraint for Offline Reinforcement Learning
Authors: Yi Shen, Hanyan Huang and Shan Xie
Categories: cs.LG cs.AI
\\
  Offline reinforcement learning learns from a static dataset without
interacting with the environment, which ensures security and thus owns a good
prospect of application. However, directly applying naive reinforcement
learning methods usually fails in an offline environment due to function
approximation errors caused by out-of-distribution(OOD) actions. To solve this
problem, existing algorithms mainly penalize the Q-value of OOD actions, the
quality of whose constraints also matter. Imprecise constraints may lead to
suboptimal solutions, while precise constraints require significant
computational costs. In this paper, we propose a novel count-based method for
continuous domains, called Grid-Mapping Pseudo-Count method(GPC), to penalize
the Q-value appropriately and reduce the computational cost. The proposed
method maps the state and action space to discrete space and constrains their
Q-values through the pseudo-count. It is theoretically proved that only a few
conditions are needed to obtain accurate uncertainty constraints in the
proposed method. Moreover, we develop a Grid-Mapping Pseudo-Count Soft
Actor-Critic(GPC-SAC) algorithm using GPC under the Soft Actor-Critic(SAC)
framework to demonstrate the effectiveness of GPC. The experimental results on
D4RL benchmark datasets show that GPC-SAC has better performance and less
computational cost compared to other algorithms.
\\ ( https://arxiv.org/abs/2404.02545 ,  1749kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02572
Date: Wed, 3 Apr 2024 08:47:32 GMT   (2055kb,D)

Title: Incremental Learning with Concept Drift Detection and Prototype-based
  Embeddings for Graph Stream Classification
Authors: Kleanthis Malialis and Jin Li and Christos G. Panayiotou and Marios M.
  Polycarpou
Categories: cs.LG
Comments: IEEE World Congress on Computational Intelligence (WCCI) 2024;
  Keywords: graph streams, concept drift, incremental learning, graph
  prototypes, nonstationary environments
\\
  Data stream mining aims at extracting meaningful knowledge from continually
evolving data streams, addressing the challenges posed by nonstationary
environments, particularly, concept drift which refers to a change in the
underlying data distribution over time. Graph structures offer a powerful
modelling tool to represent complex systems, such as, critical infrastructure
systems and social networks. Learning from graph streams becomes a necessity to
understand the dynamics of graph structures and to facilitate informed
decision-making. This work introduces a novel method for graph stream
classification which operates under the general setting where a data generating
process produces graphs with varying nodes and edges over time. The method uses
incremental learning for continual model adaptation, selecting representative
graphs (prototypes) for each class, and creating graph embeddings.
Additionally, it incorporates a loss-based concept drift detection mechanism to
recalculate graph prototypes when drift is detected.
\\ ( https://arxiv.org/abs/2404.02572 ,  2055kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02577
Date: Wed, 3 Apr 2024 08:53:42 GMT   (2292kb,D)

Title: Solving a Real-World Optimization Problem Using Proximal Policy
  Optimization with Curriculum Learning and Reward Engineering
Authors: Abhijeet Pendyala, Asma Atamna and Tobias Glasmachers
Categories: cs.LG
\\
  We present a proximal policy optimization (PPO) agent trained through
curriculum learning (CL) principles and meticulous reward engineering to
optimize a real-world high-throughput waste sorting facility. Our work
addresses the challenge of effectively balancing the competing objectives of
operational safety, volume optimization, and minimizing resource usage. A
vanilla agent trained from scratch on these multiple criteria fails to solve
the problem due to its inherent complexities. This problem is particularly
difficult due to the environment's extremely delayed rewards with long time
horizons and class (or action) imbalance, with important actions being
infrequent in the optimal policy. This forces the agent to anticipate long-term
action consequences and prioritize rare but rewarding behaviours, creating a
non-trivial reinforcement learning task. Our five-stage CL approach tackles
these challenges by gradually increasing the complexity of the environmental
dynamics during policy transfer while simultaneously refining the reward
mechanism. This iterative and adaptable process enables the agent to learn a
desired optimal policy. Results demonstrate that our approach significantly
improves inference-time safety, achieving near-zero safety violations in
addition to enhancing waste sorting plant efficiency.
\\ ( https://arxiv.org/abs/2404.02577 ,  2292kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02583
Date: Wed, 3 Apr 2024 09:08:15 GMT   (5445kb,D)

Title: Transformer-based Stagewise Decomposition for Large-Scale Multistage
  Stochastic Optimization
Authors: Chanyeong Kim, Jongwoong Park, Hyunglip Bae, Woo Chang Kim
Categories: cs.LG
Comments: Accepted at ICML 2023
\\
  Solving large-scale multistage stochastic programming (MSP) problems poses a
significant challenge as commonly used stagewise decomposition algorithms,
including stochastic dual dynamic programming (SDDP), face growing time
complexity as the subproblem size and problem count increase. Traditional
approaches approximate the value functions as piecewise linear convex functions
by incrementally accumulating subgradient cutting planes from the primal and
dual solutions of stagewise subproblems. Recognizing these limitations, we
introduce TranSDDP, a novel Transformer-based stagewise decomposition
algorithm. This innovative approach leverages the structural advantages of the
Transformer model, implementing a sequential method for integrating subgradient
cutting planes to approximate the value function. Through our numerical
experiments, we affirm TranSDDP's effectiveness in addressing MSP problems. It
efficiently generates a piecewise linear approximation for the value function,
significantly reducing computation time while preserving solution quality, thus
marking a promising progression in the treatment of large-scale multistage
stochastic programming problems.
\\ ( https://arxiv.org/abs/2404.02583 ,  5445kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02591
Date: Wed, 3 Apr 2024 09:15:38 GMT   (42kb,D)

Title: Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the
  Hot Stove Effect
Authors: Jerker Denrell
Categories: cs.LG stat.ML
\\
  The Hot Stove Effect is a negativity bias resulting from the adaptive
character of learning. The mechanism is that learning algorithms that pursue
alternatives with positive estimated values, but avoid alternatives with
negative estimated values, will correct errors of overestimation but fail to
correct errors of underestimation. Here, we generalize the theory behind the
Hot Stove Effect to settings in which negative estimates do not necessarily
lead to avoidance but to a smaller sample size (i.e., a learner selects fewer
of alternative B if B is believed to be inferior but does not entirely avoid
B). We formally demonstrate that the negativity bias remains in this set-up. We
also show there is a negativity bias for Bayesian learners in the sense that
most such learners underestimate the expected value of an alternative.
\\ ( https://arxiv.org/abs/2404.02591 ,  42kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02629
Date: Wed, 3 Apr 2024 10:36:08 GMT   (5323kb,D)

Title: Effector: A Python package for regional explanations
Authors: Vasilis Gkolemis, Christos Diou, Eirini Ntoutsi, Theodore Dalamagas,
  Bernd Bischl, Julia Herbinger, Giuseppe Casalicchio
Categories: cs.LG
Comments: 33 pages, 17 figures
\\
  Global feature effect methods explain a model outputting one plot per
feature. The plot shows the average effect of the feature on the output, like
the effect of age on the annual income. However, average effects may be
misleading when derived from local effects that are heterogeneous, i.e., they
significantly deviate from the average. To decrease the heterogeneity, regional
effects provide multiple plots per feature, each representing the average
effect within a specific subspace. For interpretability, subspaces are defined
as hyperrectangles defined by a chain of logical rules, like age's effect on
annual income separately for males and females and different levels of
professional experience. We introduce Effector, a Python library dedicated to
regional feature effects. Effector implements well-established global effect
methods, assesses the heterogeneity of each method and, based on that, provides
regional effects. Effector automatically detects subspaces where regional
effects have reduced heterogeneity. All global and regional effect methods
share a common API, facilitating comparisons between them. Moreover, the
library's interface is extensible so new methods can be easily added and
benchmarked. The library has been thoroughly tested, ships with many tutorials
(https://xai-effector.github.io/) and is available under an open-source license
at PyPi (https://pypi.org/project/effector/) and Github
(https://github.com/givasile/effector).
\\ ( https://arxiv.org/abs/2404.02629 ,  5323kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02649
Date: Wed, 3 Apr 2024 11:21:23 GMT   (272kb,D)

Title: On the Importance of Uncertainty in Decision-Making with Large Language
  Models
Authors: Nicol\`o Felicioni, Lucas Maystre, Sina Ghiassian, Kamil Ciosek
Categories: cs.LG
Comments: 12 pages of main content, 25 pages with references and appendix
\\
  We investigate the role of uncertainty in decision-making problems with
natural language as input. For such tasks, using Large Language Models as
agents has become the norm. However, none of the recent approaches employ any
additional phase for estimating the uncertainty the agent has about the world
during the decision-making task. We focus on a fundamental decision-making
framework with natural language as input, which is the one of contextual
bandits, where the context information consists of text. As a representative of
the approaches with no uncertainty estimation, we consider an LLM bandit with a
greedy policy, which picks the action corresponding to the largest predicted
reward. We compare this baseline to LLM bandits that make active use of
uncertainty estimation by integrating the uncertainty in a Thompson Sampling
policy. We employ different techniques for uncertainty estimation, such as
Laplace Approximation, Dropout, and Epinets. We empirically show on real-world
data that the greedy policy performs worse than the Thompson Sampling policies.
These findings suggest that, while overlooked in the LLM literature,
uncertainty plays a fundamental role in bandit tasks with LLMs.
\\ ( https://arxiv.org/abs/2404.02649 ,  272kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02650
Date: Wed, 3 Apr 2024 11:25:20 GMT   (3620kb,D)

Title: Towards detecting unanticipated bias in Large Language Models
Authors: Anna Kruspe
Categories: cs.LG cs.AI cs.CL
\\
  Over the last year, Large Language Models (LLMs) like ChatGPT have become
widely available and have exhibited fairness issues similar to those in
previous machine learning systems. Current research is primarily focused on
analyzing and quantifying these biases in training data and their impact on the
decisions of these models, alongside developing mitigation strategies. This
research largely targets well-known biases related to gender, race, ethnicity,
and language. However, it is clear that LLMs are also affected by other, less
obvious implicit biases. The complex and often opaque nature of these models
makes detecting such biases challenging, yet this is crucial due to their
potential negative impact in various applications. In this paper, we explore
new avenues for detecting these unanticipated biases in LLMs, focusing
specifically on Uncertainty Quantification and Explainable AI methods. These
approaches aim to assess the certainty of model decisions and to make the
internal decision-making processes of LLMs more transparent, thereby
identifying and understanding biases that are not immediately apparent. Through
this research, we aim to contribute to the development of fairer and more
transparent AI systems.
\\ ( https://arxiv.org/abs/2404.02650 ,  3620kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02660
Date: Wed, 3 Apr 2024 11:49:43 GMT   (4635kb,D)

Title: Adversarial Attacks and Dimensionality in Text Classifiers
Authors: Nandish Chattopadhyay, Atreya Goswami, Anupam Chattopadhyay
Categories: cs.LG
Comments: This paper is accepted for publication at EURASIP Journal on
  Information Security in 2024
\\
  Adversarial attacks on machine learning algorithms have been a key deterrent
to the adoption of AI in many real-world use cases. They significantly
undermine the ability of high-performance neural networks by forcing
misclassifications. These attacks introduce minute and structured perturbations
or alterations in the test samples, imperceptible to human annotators in
general, but trained neural networks and other models are sensitive to it.
Historically, adversarial attacks have been first identified and studied in the
domain of image processing. In this paper, we study adversarial examples in the
field of natural language processing, specifically text classification tasks.
We investigate the reasons for adversarial vulnerability, particularly in
relation to the inherent dimensionality of the model. Our key finding is that
there is a very strong correlation between the embedding dimensionality of the
adversarial samples and their effectiveness on models tuned with input samples
with same embedding dimension. We utilize this sensitivity to design an
adversarial defense mechanism. We use ensemble models of varying inherent
dimensionality to thwart the attacks. This is tested on multiple datasets for
its efficacy in providing robustness. We also study the problem of measuring
adversarial perturbation using different distance metrics. For all of the
aforementioned studies, we have run tests on multiple models with varying
dimensionality and used a word-vector level adversarial attack to substantiate
the findings.
\\ ( https://arxiv.org/abs/2404.02660 ,  4635kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02688
Date: Wed, 3 Apr 2024 12:36:25 GMT   (45kb)

Title: Reinforcement Learning in Categorical Cybernetics
Authors: Jules Hedges and Riu Rodr\'iguez Sakamoto
Categories: cs.LG math.CT
\\
  We show that several major algorithms of reinforcement learning (RL) fit into
the framework of categorical cybernetics, that is to say, parametrised
bidirectional processes. We build on our previous work in which we show that
value iteration can be represented by precomposition with a certain optic. The
outline of the main construction in this paper is: (1) We extend the Bellman
operators to parametrised optics that apply to action-value functions and
depend on a sample. (2) We apply a representable contravariant functor,
obtaining a parametrised function that applies the Bellman iteration. (3) This
parametrised function becomes the backward pass of another parametrised optic
that represents the model, which interacts with an environment via an agent.
Thus, parametrised optics appear in two different ways in our construction,
with one becoming part of the other. As we show, many of the major classes of
algorithms in RL can be seen as different extremal cases of this general setup:
dynamic programming, Monte Carlo methods, temporal difference learning, and
deep RL. We see this as strong evidence that this approach is a natural one and
believe that it will be a fruitful way to think about RL in the future.
\\ ( https://arxiv.org/abs/2404.02688 ,  45kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02690
Date: Wed, 3 Apr 2024 12:37:34 GMT   (84kb,D)

Title: Attention is Naturally Sparse with Gaussian Distributed Input
Authors: Yichuan Deng, Zhao Song, Chiwun Yang
Categories: cs.LG cs.AI cs.CL
\\
  The computational intensity of Large Language Models (LLMs) is a critical
bottleneck, primarily due to the $O(n^2)$ complexity of the attention mechanism
in transformer architectures. Addressing this, sparse attention emerges as a
key innovation, aiming to reduce computational load while maintaining model
performance. This study presents a rigorous theoretical analysis of the
sparsity in attention scores within LLMs, particularly under the framework of
Gaussian inputs. By establishing a set of foundational assumptions and
employing a methodical theoretical approach, we unravel the intrinsic
characteristics of attention score sparsity and its implications on
computational efficiency. Our main contribution lies in providing a detailed
theoretical examination of how sparsity manifests in attention mechanisms,
offering insights into the potential trade-offs between computational savings
and model effectiveness. This work not only advances our understanding of
sparse attention but also provides a scaffold for future research in optimizing
the computational frameworks of LLMs, paving the way for more scalable and
efficient AI systems.
\\ ( https://arxiv.org/abs/2404.02690 ,  84kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02696
Date: Wed, 3 Apr 2024 12:50:45 GMT   (20617kb,D)

Title: Deep Privacy Funnel Model: From a Discriminative to a Generative
  Approach with an Application to Face Recognition
Authors: Behrooz Razeghi, Parsa Rahimi, S\'ebastien Marcel
Categories: cs.LG
\\
  In this study, we apply the information-theoretic Privacy Funnel (PF) model
to the domain of face recognition, developing a novel method for
privacy-preserving representation learning within an end-to-end training
framework. Our approach addresses the trade-off between obfuscation and utility
in data protection, quantified through logarithmic loss, also known as
self-information loss. This research provides a foundational exploration into
the integration of information-theoretic privacy principles with representation
learning, focusing specifically on the face recognition systems. We
particularly highlight the adaptability of our framework with recent
advancements in face recognition networks, such as AdaFace and ArcFace. In
addition, we introduce the Generative Privacy Funnel ($\mathsf{GenPF}$) model,
a paradigm that extends beyond the traditional scope of the PF model, referred
to as the Discriminative Privacy Funnel ($\mathsf{DisPF}$). This
$\mathsf{GenPF}$ model brings new perspectives on data generation methods with
estimation-theoretic and information-theoretic privacy guarantees.
Complementing these developments, we also present the deep variational PF
(DVPF) model. This model proposes a tractable variational bound for measuring
information leakage, enhancing the understanding of privacy preservation
challenges in deep representation learning. The DVPF model, associated with
both $\mathsf{DisPF}$ and $\mathsf{GenPF}$ models, sheds light on connections
with various generative models such as Variational Autoencoders (VAEs),
Generative Adversarial Networks (GANs), and Diffusion models. Complementing our
theoretical contributions, we release a reproducible PyTorch package,
facilitating further exploration and application of these privacy-preserving
methodologies in face recognition systems.
\\ ( https://arxiv.org/abs/2404.02696 ,  20617kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02719
Date: Wed, 3 Apr 2024 13:21:58 GMT   (760kb,D)

Title: Can We Understand Plasticity Through Neural Collapse?
Authors: Guglielmo Bonifazi, Iason Chalas, Gian Hess, Jakub {\L}ucki
Categories: cs.LG cs.AI
\\
  This paper explores the connection between two recently identified phenomena
in deep learning: plasticity loss and neural collapse. We analyze their
correlation in different scenarios, revealing a significant association during
the initial training phase on the first task. Additionally, we introduce a
regularization approach to mitigate neural collapse, demonstrating its
effectiveness in alleviating plasticity loss in this specific setting.
\\ ( https://arxiv.org/abs/2404.02719 ,  760kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02722
Date: Wed, 3 Apr 2024 13:22:47 GMT   (7436kb,D)

Title: On-line conformalized neural networks ensembles for probabilistic
  forecasting of day-ahead electricity prices
Authors: Alessandro Brusaferri, Andrea Ballarino, Luigi Grossi, Fabrizio
  Laurini
Categories: cs.LG
Comments: 46 pages
\\
  Probabilistic electricity price forecasting (PEPF) is subject of increasing
interest, following the demand for proper quantification of prediction
uncertainty, to support the operation in complex power markets with increasing
share of renewable generation. Distributional neural networks ensembles have
been recently shown to outperform state of the art PEPF benchmarks. Still, they
require critical reliability enhancements, as fail to pass the coverage tests
at various steps on the prediction horizon. In this work, we propose a novel
approach to PEPF, extending the state of the art neural networks ensembles
based methods through conformal inference based techniques, deployed within an
on-line recalibration procedure. Experiments have been conducted on multiple
market regions, achieving day-ahead forecasts with improved hourly coverage and
stable probabilistic scores.
\\ ( https://arxiv.org/abs/2404.02722 ,  7436kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02754
Date: Wed, 3 Apr 2024 13:56:33 GMT   (1260kb)

Title: Continual Learning of Numerous Tasks from Long-tail Distributions
Authors: Liwei Kang, Wee Sun Lee
Categories: cs.LG
\\
  Continual learning, an important aspect of artificial intelligence and
machine learning research, focuses on developing models that learn and adapt to
new tasks while retaining previously acquired knowledge. Existing continual
learning algorithms usually involve a small number of tasks with uniform sizes
and may not accurately represent real-world learning scenarios. In this paper,
we investigate the performance of continual learning algorithms with a large
number of tasks drawn from a task distribution that is long-tail in terms of
task sizes. We design one synthetic dataset and two real-world continual
learning datasets to evaluate the performance of existing algorithms in such a
setting. Moreover, we study an overlooked factor in continual learning, the
optimizer states, e.g. first and second moments in the Adam optimizer, and
investigate how it can be used to improve continual learning performance. We
propose a method that reuses the optimizer states in Adam by maintaining a
weighted average of the second moments from previous tasks. We demonstrate that
our method, compatible with most existing continual learning algorithms,
effectively reduces forgetting with only a small amount of additional
computational or memory costs, and provides further improvements on existing
continual learning algorithms, particularly in a long-tail task sequence.
\\ ( https://arxiv.org/abs/2404.02754 ,  1260kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02779
Date: Wed, 3 Apr 2024 14:47:48 GMT   (1007kb,D)

Title: Federated Computing -- Survey on Building Blocks, Extensions and Systems
Authors: Ren\'e Schwermer, Ruben Mayer, Hans-Arno Jacobsen
Categories: cs.LG
\\
  In response to the increasing volume and sensitivity of data, traditional
centralized computing models face challenges, such as data security breaches
and regulatory hurdles. Federated Computing (FC) addresses these concerns by
enabling collaborative processing without compromising individual data privacy.
This is achieved through a decentralized network of devices, each retaining
control over its data, while participating in collective computations. The
motivation behind FC extends beyond technical considerations to encompass
societal implications. As the need for responsible AI and ethical data
practices intensifies, FC aligns with the principles of user empowerment and
data sovereignty. FC comprises of Federated Learning (FL) and Federated
Analytics (FA). FC systems became more complex over time and they currently
lack a clear definition and taxonomy describing its moving pieces. Current
surveys capture domain-specific FL use cases, describe individual components in
an FC pipeline individually or decoupled from each other, or provide a
quantitative overview of the number of published papers. This work surveys more
than 150 papers to distill the underlying structure of FC systems with their
basic building blocks, extensions, architecture, environment, and motivation.
We capture FL and FA systems individually and point out unique difference
between those two.
\\ ( https://arxiv.org/abs/2404.02779 ,  1007kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02785
Date: Wed, 3 Apr 2024 14:55:17 GMT   (2284kb,D)

Title: Domain Generalization through Meta-Learning: A Survey
Authors: Arsham Gholamzadeh Khoee, Yinan Yu, Robert Feldt
Categories: cs.LG cs.AI cs.CV cs.NE
\\
  Deep neural networks (DNNs) have revolutionized artificial intelligence but
often lack performance when faced with out-of-distribution (OOD) data, a common
scenario due to the inevitable domain shifts in real-world applications. This
limitation stems from the common assumption that training and testing data
share the same distribution-an assumption frequently violated in practice.
Despite their effectiveness with large amounts of data and computational power,
DNNs struggle with distributional shifts and limited labeled data, leading to
overfitting and poor generalization across various tasks and domains.
Meta-learning presents a promising approach by employing algorithms that
acquire transferable knowledge across various tasks for fast adaptation,
eliminating the need to learn each task from scratch. This survey paper delves
into the realm of meta-learning with a focus on its contribution to domain
generalization. We first clarify the concept of meta-learning for domain
generalization and introduce a novel taxonomy based on the feature extraction
strategy and the classifier learning methodology, offering a granular view of
methodologies. Through an exhaustive review of existing methods and underlying
theories, we map out the fundamentals of the field. Our survey provides
practical insights and an informed discussion on promising research directions,
paving the way for future innovation in meta-learning for domain
generalization.
\\ ( https://arxiv.org/abs/2404.02785 ,  2284kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02810
Date: Wed, 3 Apr 2024 15:31:18 GMT   (1347kb,D)

Title: Generative-Contrastive Heterogeneous Graph Neural Network
Authors: Yu Wang, Lei Sang, Yi Zhang and Yiwen Zhang
Categories: cs.LG cs.IR
Comments: 10 pages, 8figures
\\
  Heterogeneous Graphs (HGs) can effectively model complex relationships in the
real world by multi-type nodes and edges. In recent years, inspired by
self-supervised learning, contrastive Heterogeneous Graphs Neural Networks
(HGNNs) have shown great potential by utilizing data augmentation and
discriminators for downstream tasks. However, data augmentation is still
limited due to the discrete and abstract nature of graphs. To tackle the above
limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous
Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous
graph generative learning enhanced contrastive paradigm. This paradigm
includes: 1) A contrastive view augmentation strategy by using masked
autoencoder. 2) Position-aware and semantics-aware positive sample sampling
strategy for generate hard negative samples. 3) A hierarchical contrastive
learning strategy for capturing local and global information. Furthermore, the
hierarchical contrastive learning and sampling strategies aim to constitute an
enhanced discriminator under the generative-contrastive perspective. Finally,
we compare our model with seventeen baselines on eight real-world datasets. Our
model outperforms the latest contrastive and generative baselines on node
classification and link prediction tasks. To reproduce our work, we have
open-sourced our code at https://github.com/xxx.
\\ ( https://arxiv.org/abs/2404.02810 ,  1347kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02827
Date: Wed, 3 Apr 2024 15:59:42 GMT   (1137kb,D)

Title: BAdam: A Memory Efficient Full Parameter Training Method for Large
  Language Models
Authors: Qijun Luo, Hengxu Yu, Xiao Li
Categories: cs.LG
Comments: 11 pages
\\
  This work presents BAdam, an optimizer that leverages the block coordinate
optimization framework with Adam as the inner solver. BAdam offers a memory
efficient approach to the full parameter finetuning of large language models
and reduces running time of the backward process thanks to the chain rule
property. Experimentally, we apply BAdam to instruction-tune the Llama 2-7B
model on the Alpaca-GPT4 dataset using a single RTX3090-24GB GPU. The results
indicate that BAdam exhibits superior convergence behavior in comparison to
LoRA and LOMO. Furthermore, our downstream performance evaluation of the
instruction-tuned models using the MT-bench shows that BAdam modestly surpasses
LoRA and more substantially outperforms LOMO. Finally, we compare BAdam with
Adam on a medium-sized task, i.e., finetuning RoBERTa-large on the SuperGLUE
benchmark. The results demonstrate that BAdam is capable of narrowing the
performance gap with Adam. Our code is available at
https://github.com/Ledzy/BAdam.
\\ ( https://arxiv.org/abs/2404.02827 ,  1137kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02852
Date: Wed, 3 Apr 2024 16:33:42 GMT   (1107kb,D)

Title: Toward Inference-optimal Mixture-of-Expert Large Language Models
Authors: Longfei Yun, Yonghao Zhuang, Yao Fu, Eric P Xing, Hao Zhang
Categories: cs.LG
Comments: 15 pages, 8 figures
\\
  Mixture-of-Expert (MoE) based large language models (LLMs), such as the
recent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size
without suffering from the quadratic growth of training cost of dense
transformers. Like dense models, training MoEs requires answering the same
question: given a training budget, what is the optimal allocation on the model
size and number of tokens? We study the scaling law of MoE-based LLMs regarding
the relations between the model performance, model size, dataset size, and the
expert degree. Echoing previous research studying MoE in different contexts, we
observe the diminishing return of increasing the number of experts, but this
seems to suggest we should scale the number of experts until saturation, as the
training cost would remain constant, which is problematic during inference
time. We propose to amend the scaling law of MoE by introducing inference
efficiency as another metric besides the validation loss. We find that MoEs
with a few (4/8) experts are the most serving efficient solution under the same
performance, but costs 2.5-3.5x more in training. On the other hand, training a
(16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but
with a larger training dataset is a promising setup under a training budget.
\\ ( https://arxiv.org/abs/2404.02852 ,  1107kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02865
Date: Wed, 3 Apr 2024 16:57:26 GMT   (543kb,D)

Title: End-To-End Self-tuning Self-supervised Time Series Anomaly Detection
Authors: Boje Deforce, Meng-Chieh Lee, Bart Baesens, Estefan\'ia Serral
  Asensio, Jaemin Yoo, Leman Akoglu
Categories: cs.LG
\\
  Time series anomaly detection (TSAD) finds many applications such as
monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A
two-fold challenge for TSAD is a versatile and unsupervised model that can
detect various different types of time series anomalies (spikes,
discontinuities, trend shifts, etc.) without any labeled data. Modern neural
networks have outstanding ability in modeling complex time series.
Self-supervised models in particular tackle unsupervised TSAD by transforming
the input via various augmentations to create pseudo anomalies for training.
However, their performance is sensitive to the choice of augmentation, which is
hard to choose in practice, while there exists no effort in the literature on
data augmentation tuning for TSAD without labels. Our work aims to fill this
gap. We introduce TSAP for TSA "on autoPilot", which can (self-)tune
augmentation hyperparameters end-to-end. It stands on two key components: a
differentiable augmentation architecture and an unsupervised validation loss to
effectively assess the alignment between augmentation type and anomaly type.
Case studies show TSAP's ability to effectively select the (discrete)
augmentation type and associated (continuous) hyperparameters. In turn, it
outperforms established baselines, including SOTA self-supervised models, on
diverse TSAD tasks exhibiting different anomaly types.
\\ ( https://arxiv.org/abs/2404.02865 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02866
Date: Wed, 3 Apr 2024 16:58:03 GMT   (258kb,D)

Title: Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds
Authors: Kamalika Chaudhuri, Chuan Guo, Laurens van der Maaten, Saeed
  Mahloujifar, and Mark Tygert
Categories: cs.LG cs.CR cs.CY stat.ML
Comments: 18 pages, 6 figures
\\
  Protecting privacy during inference with deep neural networks is possible by
adding noise to the activations in the last layers prior to the final
classifiers or other task-specific layers. The activations in such layers are
known as "features" (or, less commonly, as "embeddings" or "feature
embeddings"). The added noise helps prevent reconstruction of the inputs from
the noisy features. Lower bounding the variance of every possible unbiased
estimator of the inputs quantifies the confidentiality arising from such added
noise. Convenient, computationally tractable bounds are available from classic
inequalities of Hammersley and of Chapman and Robbins -- the HCR bounds.
Numerical experiments indicate that the HCR bounds are on the precipice of
being effectual for small neural nets with the data sets, "MNIST" and
"CIFAR-10," which contain 10 classes each for image classification. The HCR
bounds appear to be insufficient on their own to guarantee confidentiality of
the inputs to inference with standard deep neural nets, "ResNet-18" and
"Swin-T," pre-trained on the data set, "ImageNet-1000," which contains 1000
classes. Supplementing the addition of noise to features with other methods for
providing confidentiality may be warranted in the case of ImageNet. In all
cases, the results reported here limit consideration to amounts of added noise
that incur little degradation in the accuracy of classification from the noisy
features. Thus, the added noise enhances confidentiality without much reduction
in the accuracy on the task of image classification.
\\ ( https://arxiv.org/abs/2404.02866 ,  258kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02869
Date: Wed, 3 Apr 2024 17:05:41 GMT   (345kb)

Title: Human Activity Recognition using Smartphones
Authors: Mayur Sonawane, Sahil Rajesh Dhayalkar, Siddesh Waje, Soyal
  Markhelkar, Akshay Wattamwar, Seema C. Shrawne
Categories: cs.LG cs.AI
Journal-ref: International Journal of Engineering Science and Computing,
  October 2018
\\
  Human Activity Recognition is a subject of great research today and has its
applications in remote healthcare, activity tracking of the elderly or the
disables, calories burnt tracking etc. In our project, we have created an
Android application that recognizes the daily human activities and calculate
the calories burnt in real time. We first captured labeled triaxial
acceleration readings for different daily human activities from the
smartphone's embedded accelerometer. These readings were preprocessed using a
median filter. 42 features were extracted using various methods. We then tested
various machine learning algorithms along with dimensionality reduction.
Finally, in our Android application, we used the machine learning algorithm and
a subset of features that provided maximum accuracy and minimum model building
time. This is used for real-time activity recognition and calculation of
calories burnt using a formula based on Metabolic Equivalent.
\\ ( https://arxiv.org/abs/2404.02869 ,  345kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02882
Date: Wed, 3 Apr 2024 17:33:21 GMT   (172kb,D)

Title: Linear Attention Sequence Parallelism
Authors: Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong
Categories: cs.LG cs.CL
Comments: Technical Report. Weigao Sun and Zhen Qin contribute equally to this
  paper. Yiran Zhong is the corresponding author. The code is available at
  https://github.com/OpenNLPLab/LASP
\\
  Sequence Parallel (SP) serves as a prevalent strategy to handle long
sequences that exceed the memory limit of a single GPU. However, existing SP
methods do not take advantage of linear attention features, resulting in
sub-optimal parallelism efficiency and usability for linear attention-based
language models. In this paper, we introduce Linear Attention Sequence Parallel
(LASP), an efficient SP method tailored to linear attention-based language
models. Specifically, we design an efficient point-to-point communication
mechanism to leverage the right-product kernel trick of linear attention, which
sharply decreases the communication overhead of SP. We also enhance the
practical efficiency of LASP by performing kernel fusion and intermediate state
caching, making the implementation of LASP hardware-friendly on GPU clusters.
Furthermore, we meticulously ensure the compatibility of sequence-level LASP
with all types of batch-level data parallel methods, which is vital for
distributed training on large clusters with long sequences and large batches.
We conduct extensive experiments on two linear attention-based models with
varying sequence lengths and GPU cluster sizes. LASP scales sequence length up
to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than
existing SP methods while being significantly faster. The code is available at
https://github.com/OpenNLPLab/LASP.
\\ ( https://arxiv.org/abs/2404.02882 ,  172kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02892
Date: Wed, 3 Apr 2024 17:49:41 GMT   (380kb,D)

Title: MODNO: Multi Operator Learning With Distributed Neural Operators
Authors: Zecheng Zhang
Categories: cs.LG cs.NA math.NA
\\
  The study of operator learning involves the utilization of neural networks to
approximate operators. Traditionally, the focus has been on single-operator
learning (SOL). However, recent advances have rapidly expanded this to include
the approximation of multiple operators using foundation models equipped with
millions or billions of trainable parameters, leading to the research of
multi-operator learning (MOL). In this paper, we present a novel distributed
training approach aimed at enabling a single neural operator with significantly
fewer parameters to effectively tackle multi-operator learning challenges, all
without incurring additional average costs. Our method is applicable to various
Chen-Chen-type neural operators, such as Deep Operator Neural Networks (DON).
The core idea is to independently learn the output basis functions for each
operator using its dedicated data, while simultaneously centralizing the
learning of the input function encoding shared by all operators using the
entire dataset. Through a systematic study of five numerical examples, we
compare the accuracy and cost of training a single neural operator for each
operator independently versus training a MOL model using our proposed method.
Our results demonstrate enhanced efficiency and satisfactory accuracy.
Moreover, our approach illustrates that some operators with limited data can be
more effectively constructed with the aid of data from analogous operators
through MOL learning. This highlights another MOL's potential to bolster
operator learning.
\\ ( https://arxiv.org/abs/2404.02892 ,  380kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02896
Date: Wed, 3 Apr 2024 17:53:32 GMT   (40kb,D)

Title: Comment on "Machine learning conservation laws from differential
  equations"
Authors: Michael F. Zimmer
Categories: cs.LG
Comments: 2 pages, 1 figure
\\
  In lieu of abstract, first paragraph reads: Six months after the author
derived a constant of motion for a 1D damped harmonic oscillator [1], a similar
result appeared by Liu, Madhavan, and Tegmark [2, 3], without citing the
author. However, their derivation contained six serious errors, causing both
their method and result to be incorrect. In this Comment, those errors are
reviewed.
\\ ( https://arxiv.org/abs/2404.02896 ,  40kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2404.02174 (*cross-listing*)
Date: Mon, 1 Apr 2024 01:25:40 GMT   (4505kb,D)

Title: Bounds of Block Rewards in Honest PinFi Systems
Authors: Qi He, Yunwei Mao, Ju Li
Categories: cs.GT cs.AI cs.CE
\\
  PinFi is a class of novel protocols for decentralized pricing of dissipative
assets, whose value naturally declines over time. Central to the protocol's
functionality and its market efficiency is the role of liquidity providers
(LPs). This study addresses critical stability and sustainability challenges
within the protocol, namely: the propensity of LPs to prefer selling in
external markets over participation in the protocol; a similar inclination
towards selling within the PinFi system rather than contributing as LPs; and a
scenario where LPs are disinclined to sell within the protocol. Employing a
game-theoretic approach, we explore PinFi's mechanisms and its broader
ramifications. Our findings reveal that, under a variety of common conditions
and with an assumption of participant integrity, PinFi is capable of fostering
a dynamic equilibrium among LPs, sellers, and buyers. This balance is
maintained through a carefully calibrated range of block rewards for LPs,
ensuring the protocol's long-term stability and utility.
\\ ( https://arxiv.org/abs/2404.02174 ,  4505kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02176 (*cross-listing*)
Date: Mon, 1 Apr 2024 19:52:08 GMT   (377kb,D)

Title: Versatile Navigation under Partial Observability via Value-guided
  Diffusion Policy
Authors: Gengyu Zhang, Hao Tang, Yan Yan
Categories: cs.RO cs.AI
Comments: 13 pages, 7 figures, CVPR 2024
\\
  Route planning for navigation under partial observability plays a crucial
role in modern robotics and autonomous driving. Existing route planning
approaches can be categorized into two main classes: traditional autoregressive
and diffusion-based methods. The former often fails due to its myopic nature,
while the latter either assumes full observability or struggles to adapt to
unfamiliar scenarios, due to strong couplings with behavior cloning from
experts. To address these deficiencies, we propose a versatile diffusion-based
approach for both 2D and 3D route planning under partial observability.
Specifically, our value-guided diffusion policy first generates plans to
predict actions across various timesteps, providing ample foresight to the
planning. It then employs a differentiable planner with state estimations to
derive a value function, directing the agent's exploration and goal-seeking
behaviors without seeking experts while explicitly addressing partial
observability. During inference, our policy is further enhanced by a
best-plan-selection strategy, substantially boosting the planning success rate.
Moreover, we propose projecting point clouds, derived from RGB-D inputs, onto
2D grid-based bird-eye-view maps via semantic segmentation, generalizing to 3D
environments. This simple yet effective adaption enables zero-shot transfer
from 2D-trained policy to 3D, cutting across the laborious training for 3D
policy, and thus certifying our versatility. Experimental results demonstrate
our superior performance, particularly in navigating situations beyond expert
demonstrations, surpassing state-of-the-art autoregressive and diffusion-based
baselines for both 2D and 3D scenarios.
\\ ( https://arxiv.org/abs/2404.02176 ,  377kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02179 (*cross-listing*)
Date: Tue, 2 Apr 2024 03:21:06 GMT   (1148kb,D)

Title: Distributed and Rate-Adaptive Feature Compression
Authors: Aditya Deshmukh, Venugopal V. Veeravalli, Gunjan Verma
Categories: cs.IT cs.AI math.IT stat.ML
\\
  We study the problem of distributed and rate-adaptive feature compression for
linear regression. A set of distributed sensors collect disjoint features of
regressor data. A fusion center is assumed to contain a pretrained linear
regression model, trained on a dataset of the entire uncompressed data. At
inference time, the sensors compress their observations and send them to the
fusion center through communication-constrained channels, whose rates can
change with time. Our goal is to design a feature compression {scheme} that can
adapt to the varying communication constraints, while maximizing the inference
performance at the fusion center. We first obtain the form of optimal
quantizers assuming knowledge of underlying regressor data distribution. Under
a practically reasonable approximation, we then propose a distributed
compression scheme which works by quantizing a one-dimensional projection of
the sensor data. We also propose a simple adaptive scheme for handling changes
in communication constraints. We demonstrate the effectiveness of the
distributed adaptive compression scheme through simulated experiments.
\\ ( https://arxiv.org/abs/2404.02179 ,  1148kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02183 (*cross-listing*)
Date: Tue, 2 Apr 2024 13:37:28 GMT   (10073kb,D)

Title: Self-Organized Agents: A LLM Multi-Agent Framework toward Ultra
  Large-Scale Code Generation and Optimization
Authors: Yoichi Ishibashi, Yoshimasa Nishimura
Categories: cs.SE cs.AI cs.CL cs.LG cs.MA
\\
  Recent advancements in automatic code generation using large language model
(LLM) agent have brought us closer to the future of automated software
development. However, existing single-agent approaches face limitations in
generating and improving large-scale, complex codebases due to constraints in
context length. To tackle this challenge, we propose Self-Organized multi-Agent
framework (SoA), a novel multi-agent framework that enables the scalable and
efficient generation and optimization of large-scale code. In SoA,
self-organized agents operate independently to generate and modify code
components while seamlessly collaborating to construct the overall codebase. A
key feature of our framework is the automatic multiplication of agents based on
problem complexity, allowing for dynamic scalability. This enables the overall
code volume to be increased indefinitely according to the number of agents,
while the amount of code managed by each agent remains constant. We evaluate
SoA on the HumanEval benchmark and demonstrate that, compared to a single-agent
system, each agent in SoA handles significantly less code, yet the overall
generated code is substantially greater. Moreover, SoA surpasses the powerful
single-agent baseline by 5% in terms of Pass@1 accuracy.
\\ ( https://arxiv.org/abs/2404.02183 ,  10073kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02205 (*cross-listing*)
Date: Tue, 2 Apr 2024 18:00:42 GMT   (8649kb,D)

Title: A Holistic Indicator of Polarization to Measure Online Sexism
Authors: Vahid Ghafouri, Jose Such, Guillermo Suarez-Tangil
Categories: cs.SI cs.AI
\\
  The online trend of the manosphere and feminist discourse on social networks
requires a holistic measure of the level of sexism in an online community. This
indicator is important for policymakers and moderators of online communities
(e.g., subreddits) and computational social scientists, either to revise
moderation strategies based on the degree of sexism or to match and compare the
temporal sexism across different platforms and communities with real-time
events and infer social scientific insights.
  In this paper, we build a model that can provide a comparable holistic
indicator of toxicity targeted toward male and female identity and male and
female individuals. Despite previous supervised NLP methods that require
annotation of toxic comments at the target level (e.g. annotating comments that
are specifically toxic toward women) to detect targeted toxic comments, our
indicator uses supervised NLP to detect the presence of toxicity and
unsupervised word embedding association test to detect the target
automatically.
  We apply our model to gender discourse communities (e.g., r/TheRedPill,
r/MGTOW, r/FemaleDatingStrategy) to detect the level of toxicity toward genders
(i.e., sexism). Our results show that our framework accurately and consistently
(93% correlation) measures the level of sexism in a community. We finally
discuss how our framework can be generalized in the future to measure qualities
other than toxicity (e.g. sentiment, humor) toward general-purpose targets and
turn into an indicator of different sorts of polarizations.
\\ ( https://arxiv.org/abs/2404.02205 ,  8649kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02213 (*cross-listing*)
Date: Tue, 2 Apr 2024 18:05:26 GMT   (1091kb,D)

Title: Exploring How Multiple Levels of GPT-Generated Programming Hints Support
  or Disappoint Novices
Authors: Ruiwei Xiao, Xinying Hou, John Stamper
Categories: cs.HC cs.AI cs.CY
Comments: Accepted CHI 2024 LBW - 10 pages
DOI: 10.1145/3613905.3650937
\\
  Recent studies have integrated large language models (LLMs) into diverse
educational contexts, including providing adaptive programming hints, a type of
feedback focuses on helping students move forward during problem-solving.
However, most existing LLM-based hint systems are limited to one single hint
type. To investigate whether and how different levels of hints can support
students' problem-solving and learning, we conducted a think-aloud study with
12 novices using the LLM Hint Factory, a system providing four levels of hints
from general natural language guidance to concrete code assistance, varying in
format and granularity. We discovered that high-level natural language hints
alone can be helpless or even misleading, especially when addressing next-step
or syntax-related help requests. Adding lower-level hints, like code examples
with in-line comments, can better support students. The findings open up future
work on customizing help responses from content, format, and granularity levels
to accurately identify and meet students' learning needs.
\\ ( https://arxiv.org/abs/2404.02213 ,  1091kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02225 (*cross-listing*)
Date: Tue, 2 Apr 2024 18:27:03 GMT   (19815kb,D)

Title: CHOSEN: Contrastive Hypothesis Selection for Multi-View Depth Refinement
Authors: Di Qiu, Yinda Zhang, Thabo Beeler, Vladimir Tankovich, Christian
  H\"ane, Sean Fanello, Christoph Rhemann, Sergio Orts Escolano
Categories: cs.CV cs.AI
\\
  We propose CHOSEN, a simple yet flexible, robust and effective multi-view
depth refinement framework. It can be employed in any existing multi-view
stereo pipeline, with straightforward generalization capability for different
multi-view capture systems such as camera relative positioning and lenses.
Given an initial depth estimation, CHOSEN iteratively re-samples and selects
the best hypotheses, and automatically adapts to different metric or intrinsic
scales determined by the capture system. The key to our approach is the
application of contrastive learning in an appropriate solution space and a
carefully designed hypothesis feature, based on which positive and negative
hypotheses can be effectively distinguished. Integrated in a simple baseline
multi-view stereo pipeline, CHOSEN delivers impressive quality in terms of
depth and normal accuracy compared to many current deep learning based
multi-view stereo pipelines.
\\ ( https://arxiv.org/abs/2404.02225 ,  19815kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02227 (*cross-listing*)
Date: Tue, 2 Apr 2024 18:30:29 GMT   (260kb,D)

Title: OOSTraj: Out-of-Sight Trajectory Prediction With Vision-Positioning
  Denoising
Authors: Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, Yun Fu
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: In Proceedings of IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2024 (CVPR)
\\
  Trajectory prediction is fundamental in computer vision and autonomous
driving, particularly for understanding pedestrian behavior and enabling
proactive decision-making. Existing approaches in this field often assume
precise and complete observational data, neglecting the challenges associated
with out-of-view objects and the noise inherent in sensor data due to limited
camera range, physical obstructions, and the absence of ground truth for
denoised sensor data. Such oversights are critical safety concerns, as they can
result in missing essential, non-visible objects. To bridge this gap, we
present a novel method for out-of-sight trajectory prediction that leverages a
vision-positioning technique. Our approach denoises noisy sensor observations
in an unsupervised manner and precisely maps sensor-based trajectories of
out-of-sight objects into visual trajectories. This method has demonstrated
state-of-the-art performance in out-of-sight noisy sensor trajectory denoising
and prediction on the Vi-Fi and JRDB datasets. By enhancing trajectory
prediction accuracy and addressing the challenges of out-of-sight objects, our
work significantly contributes to improving the safety and reliability of
autonomous driving in complex environments. Our work represents the first
initiative towards Out-Of-Sight Trajectory prediction (OOSTraj), setting a new
benchmark for future research. The code is available at
\url{https://github.com/Hai-chao-Zhang/OOSTraj}.
\\ ( https://arxiv.org/abs/2404.02227 ,  260kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02249 (*cross-listing*)
Date: Tue, 2 Apr 2024 19:14:23 GMT   (310kb,D)

Title: RAT: Retrieval-Augmented Transformer for Click-Through Rate Prediction
Authors: Yushen Li, Jinpeng Wang, Tao Dai, Jieming Zhu, Jun Yuan, Rui Zhang,
  Shu-Tao Xia
Categories: cs.IR cs.AI cs.LG cs.SI
Comments: Accepted to The ACM Web Conference 2024 (WWW'24, short paper). Data
  and code are available
DOI: 10.1145/3589335.3651550
\\
  Predicting click-through rates (CTR) is a fundamental task for Web
applications, where a key issue is to devise effective models for feature
interactions. Current methodologies predominantly concentrate on modeling
feature interactions within an individual sample, while overlooking the
potential cross-sample relationships that can serve as a reference context to
enhance the prediction. To make up for such deficiency, this paper develops a
Retrieval-Augmented Transformer (RAT), aiming to acquire fine-grained feature
interactions within and across samples. By retrieving similar samples, we
construct augmented input for each target sample. We then build Transformer
layers with cascaded attention to capture both intra- and cross-sample feature
interactions, facilitating comprehensive reasoning for improved CTR prediction
while retaining efficiency. Extensive experiments on real-world datasets
substantiate the effectiveness of RAT and suggest its advantage in long-tail
scenarios. The code has been open-sourced at
\url{https://github.com/YushenLi807/WWW24-RAT}.
\\ ( https://arxiv.org/abs/2404.02249 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02263 (*cross-listing*)
Date: Tue, 2 Apr 2024 19:37:58 GMT   (27801kb,D)

Title: OFMPNet: Deep End-to-End Model for Occupancy and Flow Prediction in
  Urban Environment
Authors: Youshaa Murhij and Dmitry Yudin
Categories: cs.CV cs.AI cs.RO
Comments: Accepted in Neurocomputing journal - 2024
\\
  The task of motion prediction is pivotal for autonomous driving systems,
providing crucial data to choose a vehicle behavior strategy within its
surroundings. Existing motion prediction techniques primarily focus on
predicting the future trajectory of each agent in the scene individually,
utilizing its past trajectory data. In this paper, we introduce an end-to-end
neural network methodology designed to predict the future behaviors of all
dynamic objects in the environment. This approach leverages the occupancy map
and the scene's motion flow. We are investigatin various alternatives for
constructing a deep encoder-decoder model called OFMPNet. This model uses a
sequence of bird's-eye-view road images, occupancy grid, and prior motion flow
as input data. The encoder of the model can incorporate transformer,
attention-based, or convolutional units. The decoder considers the use of both
convolutional modules and recurrent blocks. Additionally, we propose a novel
time-weighted motion flow loss, whose application has shown a substantial
decrease in end-point error. Our approach has achieved state-of-the-art results
on the Waymo Occupancy and Flow Prediction benchmark, with a Soft IoU of 52.1%
and an AUC of 76.75% on Flow-Grounded Occupancy.
\\ ( https://arxiv.org/abs/2404.02263 ,  27801kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02287 (*cross-listing*)
Date: Tue, 2 Apr 2024 20:29:59 GMT   (1127kb,D)

Title: One Noise to Rule Them All: Multi-View Adversarial Attacks with
  Universal Perturbation
Authors: Mehmet Ergezer and Phat Duong and Christian Green and Tommy Nguyen and
  Abdurrahman Zeybey
Categories: cs.CV cs.AI
Comments: 6 pages, 4 figures, presented at ICAIA, Springer to publish under
  Algorithms for Intelligent Systems
Journal-ref: 2nd International Conference on Artificial Intelligence and
  Applications (ICAIA 2024)
\\
  This paper presents a novel universal perturbation method for generating
robust multi-view adversarial examples in 3D object recognition. Unlike
conventional attacks limited to single views, our approach operates on multiple
2D images, offering a practical and scalable solution for enhancing model
scalability and robustness. This generalizable method bridges the gap between
2D perturbations and 3D-like attack capabilities, making it suitable for
real-world applications.
  Existing adversarial attacks may become ineffective when images undergo
transformations like changes in lighting, camera position, or natural
deformations. We address this challenge by crafting a single universal noise
perturbation applicable to various object views. Experiments on diverse
rendered 3D objects demonstrate the effectiveness of our approach. The
universal perturbation successfully identified a single adversarial noise for
each given set of 3D object renders from multiple poses and viewpoints.
Compared to single-view attacks, our universal attacks lower classification
confidence across multiple viewing angles, especially at low noise levels. A
sample implementation is made available at
https://github.com/memoatwit/UniversalPerturbation.
\\ ( https://arxiv.org/abs/2404.02287 ,  1127kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02353 (*cross-listing*)
Date: Tue, 2 Apr 2024 22:54:24 GMT   (676kb,D)

Title: Semantic Augmentation in Images using Language
Authors: Sahiti Yerramilli, Jayant Sravan Tamarapalli, Tanmay Girish Kulkarni,
  Jonathan Francis, Eric Nyberg
Categories: cs.CV cs.AI cs.LG
\\
  Deep Learning models are incredibly data-hungry and require very large
labeled datasets for supervised learning. As a consequence, these models often
suffer from overfitting, limiting their ability to generalize to real-world
examples. Recent advancements in diffusion models have enabled the generation
of photorealistic images based on textual inputs. Leveraging the substantial
datasets used to train these diffusion models, we propose a technique to
utilize generated images to augment existing datasets. This paper explores
various strategies for effective data augmentation to improve the out-of-domain
generalization capabilities of deep learning models.
\\ ( https://arxiv.org/abs/2404.02353 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02361 (*cross-listing*)
Date: Tue, 2 Apr 2024 23:16:17 GMT   (630kb)

Title: EnergAIze: Multi Agent Deep Deterministic Policy Gradient for Vehicle to
  Grid Energy Management
Authors: Tiago Fonseca, Luis Ferreira, Bernardo Cabral, Ricardo Severino,
  Isabel Praca
Categories: cs.MA cs.AI
Comments: 6 pages, 6 figures, 2 tables
\\
  This paper investigates the increasing roles of Renewable Energy Sources
(RES) and Electric Vehicles (EVs). While indicating a new era of sustainable
energy, these also introduce complex challenges, including the need to balance
supply and demand and smooth peak consumptions amidst rising EV adoption rates.
Addressing these challenges requires innovative solutions such as Demand
Response (DR), energy flexibility management, Renewable Energy Communities
(RECs), and more specifically for EVs, Vehicle-to-Grid (V2G). However, existing
V2G approaches often fall short in real-world adaptability, global REC
optimization with other flexible assets, scalability, and user engagement. To
bridge this gap, this paper introduces EnergAIze, a Multi-Agent Reinforcement
Learning (MARL) energy management framework, leveraging the Multi-Agent Deep
Deterministic Policy Gradient (MADDPG) algorithm. EnergAIze enables
user-centric and multi-objective energy management by allowing each prosumer to
select from a range of personal management objectives, thus encouraging
engagement. Additionally, it architects' data protection and ownership through
decentralized computing, where each prosumer can situate an energy management
optimization node directly at their own dwelling. The local node not only
manages local energy assets but also fosters REC wide optimization. The
efficacy of EnergAIze was evaluated through case studies employing the
CityLearn simulation framework. These simulations were instrumental in
demonstrating EnergAIze's adeptness at implementing V2G technology within a REC
and other energy assets. The results show reduction in peak loads, ramping,
carbon emissions, and electricity costs at the REC level while optimizing for
individual prosumers objectives.
\\ ( https://arxiv.org/abs/2404.02361 ,  630kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02370 (*cross-listing*)
Date: Wed, 3 Apr 2024 00:09:05 GMT   (434kb,D)

Title: Enhancing Human-Computer Interaction in Chest X-ray Analysis using
  Vision and Language Model with Eye Gaze Patterns
Authors: Yunsoo Kim, Jinge Wu, Yusuf Abdulle, Yue Gao, Honghan Wu
Categories: cs.CV cs.AI cs.CL
Comments: Under review
\\
  Recent advancements in Computer Assisted Diagnosis have shown promising
performance in medical imaging tasks, particularly in chest X-ray analysis.
However, the interaction between these models and radiologists has been
primarily limited to input images. This work proposes a novel approach to
enhance human-computer interaction in chest X-ray analysis using
Vision-Language Models (VLMs) enhanced with radiologists' attention by
incorporating eye gaze data alongside textual prompts. Our approach leverages
heatmaps generated from eye gaze data, overlaying them onto medical images to
highlight areas of intense radiologist's focus during chest X-ray evaluation.
We evaluate this methodology in tasks such as visual question answering, chest
X-ray report automation, error detection, and differential diagnosis. Our
results demonstrate the inclusion of eye gaze information significantly
enhances the accuracy of chest X-ray analysis. Also, the impact of eye gaze on
fine-tuning was confirmed as it outperformed other medical VLMs in all tasks
except visual question answering. This work marks the potential of leveraging
both the VLM's capabilities and the radiologist's domain knowledge to improve
the capabilities of AI models in medical imaging, paving a novel way for
Computer Assisted Diagnosis with a human-centred AI.
\\ ( https://arxiv.org/abs/2404.02370 ,  434kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02406 (*cross-listing*)
Date: Wed, 3 Apr 2024 02:16:53 GMT   (8101kb,D)

Title: Exploring Backdoor Vulnerabilities of Chat Models
Authors: Yunzhuo Hao, Wenkai Yang, Yankai Lin
Categories: cs.CR cs.AI cs.CL
Comments: Code and data are available at
  https://github.com/hychaochao/Chat-Models-Backdoor-Attacking
\\
  Recent researches have shown that Large Language Models (LLMs) are
susceptible to a security threat known as Backdoor Attack. The backdoored model
will behave well in normal cases but exhibit malicious behaviours on inputs
inserted with a specific backdoor trigger. Current backdoor studies on LLMs
predominantly focus on instruction-tuned LLMs, while neglecting another
realistic scenario where LLMs are fine-tuned on multi-turn conversational data
to be chat models. Chat models are extensively adopted across various
real-world scenarios, thus the security of chat models deserves increasing
attention. Unfortunately, we point out that the flexible multi-turn interaction
format instead increases the flexibility of trigger designs and amplifies the
vulnerability of chat models to backdoor attacks. In this work, we reveal and
achieve a novel backdoor attacking method on chat models by distributing
multiple trigger scenarios across user inputs in different rounds, and making
the backdoor be triggered only when all trigger scenarios have appeared in the
historical conversations. Experimental results demonstrate that our method can
achieve high attack success rates (e.g., over 90% ASR on Vicuna-7B) while
successfully maintaining the normal capabilities of chat models on providing
helpful responses to benign user requests. Also, the backdoor can not be easily
removed by the downstream re-alignment, highlighting the importance of
continued research and attention to the security concerns of chat models.
Warning: This paper may contain toxic content.
\\ ( https://arxiv.org/abs/2404.02406 ,  8101kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02407 (*cross-listing*)
Date: Wed, 3 Apr 2024 02:17:34 GMT   (493kb,D)

Title: Decision Transformer as a Foundation Model for Partially Observable
  Continuous Control
Authors: Xiangyuan Zhang, Weichao Mao, Haoran Qiu, Tamer Ba\c{s}ar
Categories: eess.SY cs.AI cs.LG cs.RO cs.SY
Comments: Submitted to CDC 2024
\\
  Closed-loop control of nonlinear dynamical systems with partial-state
observability demands expert knowledge of a diverse, less standardized set of
theoretical tools. Moreover, it requires a delicate integration of controller
and estimator designs to achieve the desired system behavior. To establish a
general controller synthesis framework, we explore the Decision Transformer
(DT) architecture. Specifically, we first frame the control task as predicting
the current optimal action based on past observations, actions, and rewards,
eliminating the need for a separate estimator design. Then, we leverage the
pre-trained language models, i.e., the Generative Pre-trained Transformer (GPT)
series, to initialize DT and subsequently train it for control tasks using
low-rank adaptation (LoRA). Our comprehensive experiments across five distinct
control tasks, ranging from maneuvering aerospace systems to controlling
partial differential equations (PDEs), demonstrate DT's capability to capture
the parameter-agnostic structures intrinsic to control tasks. DT exhibits
remarkable zero-shot generalization abilities for completely new tasks and
rapidly surpasses expert performance levels with a minimal amount of
demonstration data. These findings highlight the potential of DT as a
foundational controller for general control applications.
\\ ( https://arxiv.org/abs/2404.02407 ,  493kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02447 (*cross-listing*)
Date: Wed, 3 Apr 2024 04:26:50 GMT   (376kb)

Title: A Novel Approach to Breast Cancer Histopathological Image Classification
  Using Cross-Colour Space Feature Fusion and Quantum-Classical Stack Ensemble
  Method
Authors: Sambit Mallick, Snigdha Paul and Anindya Sen
Categories: cs.CV cs.AI
\\
  Breast cancer classification stands as a pivotal pillar in ensuring timely
diagnosis and effective treatment. This study with histopathological images
underscores the profound significance of harnessing the synergistic
capabilities of colour space ensembling and quantum-classical stacking to
elevate the precision of breast cancer classification. By delving into the
distinct colour spaces of RGB, HSV and CIE L*u*v, the authors initiated a
comprehensive investigation guided by advanced methodologies. Employing the
DenseNet121 architecture for feature extraction the authors have capitalized on
the robustness of Random Forest, SVM, QSVC, and VQC classifiers. This research
encompasses a unique feature fusion technique within the colour space ensemble.
This approach not only deepens our comprehension of breast cancer
classification but also marks a milestone in personalized medical assessment.
The amalgamation of quantum and classical classifiers through stacking emerges
as a potent catalyst, effectively mitigating the inherent constraints of
individual classifiers, paving a robust path towards more dependable and
refined breast cancer identification. Through rigorous experimentation and
meticulous analysis, fusion of colour spaces like RGB with HSV and RGB with CIE
L*u*v, presents an classification accuracy, nearing the value of unity. This
underscores the transformative potential of our approach, where the fusion of
diverse colour spaces and the synergy of quantum and classical realms converge
to establish a new horizon in medical diagnostics. Thus the implications of
this research extend across medical disciplines, offering promising avenues for
advancing diagnostic accuracy and treatment efficacy.
\\ ( https://arxiv.org/abs/2404.02447 ,  376kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02448 (*cross-listing*)
Date: Wed, 3 Apr 2024 04:27:07 GMT   (5395kb,D)

Title: Electric Vehicle Routing Problem for Emergency Power Supply: Towards
  Telecom Base Station Relief
Authors: Daisuke Kikuta and Hiroki Ikeuchi and Kengo Tajiri and Yuta Toyama and
  Yuusuke Nakano
Categories: math.OC cs.AI cs.LG cs.MA
Comments: Accepted at AAMAS 2024 (extended abstract). 10 pages, 5 figures. Work
  in progress
\\
  As a telecom provider, our company has a critical mission to maintain telecom
services even during power outages. To accomplish the mission, it is essential
to maintain the power of the telecom base stations. Here we consider a solution
where electric vehicles (EVs) directly supply power to base stations by
traveling to their locations. The goal is to find EV routes that minimize both
the total travel distance of all EVs and the number of downed base stations. In
this paper, we formulate this routing problem as a new variant of the Electric
Vehicle Routing Problem (EVRP) and propose a solver that combines a rule-based
vehicle selector and a reinforcement learning (RL)-based node selector. The
rule of the vehicle selector ensures the exact environmental states when the
selected EV starts to move. In addition, the node selection by the RL model
enables fast route generation, which is critical in emergencies. We evaluate
our solver on both synthetic datasets and real datasets. The results show that
our solver outperforms baselines in terms of the objective value and
computation time. Moreover, we analyze the generalization and scalability of
our solver, demonstrating the capability toward unseen settings and large-scale
problems. Check also our project page: https://ntt-dkiku.github.io/rl-evrpeps.
\\ ( https://arxiv.org/abs/2404.02448 ,  5395kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02460 (*cross-listing*)
Date: Wed, 3 Apr 2024 05:02:46 GMT   (8710kb,D)

Title: TSNet:A Two-stage Network for Image Dehazing with Multi-scale Fusion and
  Adaptive Learning
Authors: Xiaolin Gong and Zehan Zheng and Heyuan Du
Categories: cs.CV cs.AI
Comments: 12 pages, 10 figures, 7 tables
\\
  Image dehazing has been a popular topic of research for a long time. Previous
deep learning-based image dehazing methods have failed to achieve satisfactory
dehazing effects on both synthetic datasets and real-world datasets, exhibiting
poor generalization. Moreover, single-stage networks often result in many
regions with artifacts and color distortion in output images. To address these
issues, this paper proposes a two-stage image dehazing network called TSNet,
mainly consisting of the multi-scale fusion module (MSFM) and the adaptive
learning module (ALM). Specifically, MSFM and ALM enhance the generalization of
TSNet. The MSFM can obtain large receptive fields at multiple scales and
integrate features at different frequencies to reduce the differences between
inputs and learning objectives. The ALM can actively learn of regions of
interest in images and restore texture details more effectively. Additionally,
TSNet is designed as a two-stage network, where the first-stage network
performs image dehazing, and the second-stage network is employed to improve
issues such as artifacts and color distortion present in the results of the
first-stage network. We also change the learning objective from ground truth
images to opposite fog maps, which improves the learning efficiency of TSNet.
Extensive experiments demonstrate that TSNet exhibits superior dehazing
performance on both synthetic and real-world datasets compared to previous
state-of-the-art methods.
\\ ( https://arxiv.org/abs/2404.02460 ,  8710kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02476 (*cross-listing*)
Date: Wed, 3 Apr 2024 05:32:10 GMT   (3596kb,D)

Title: Deep Reinforcement Learning for Traveling Purchaser Problems
Authors: Haofeng Yuan, Rongping Zhu, Wanlu Yang, Shiji Song, Keyou You, Yuli
  Zhang
Categories: math.OC cs.AI cs.LG
\\
  The traveling purchaser problem (TPP) is an important combinatorial
optimization problem with broad applications. Due to the coupling between
routing and purchasing, existing works on TPPs commonly address route
construction and purchase planning simultaneously, which, however, leads to
exact methods with high computational cost and heuristics with sophisticated
design but limited performance. In sharp contrast, we propose a novel approach
based on deep reinforcement learning (DRL), which addresses route construction
and purchase planning separately, while evaluating and optimizing the solution
from a global perspective. The key components of our approach include a
bipartite graph representation for TPPs to capture the market-product
relations, and a policy network that extracts information from the bipartite
graph and uses it to sequentially construct the route. One significant benefit
of our framework is that we can efficiently construct the route using the
policy network, and once the route is determined, the associated purchasing
plan can be easily derived through linear programming, while, leveraging DRL,
we can train the policy network to optimize the global solution objective.
Furthermore, by introducing a meta-learning strategy, the policy network can be
trained stably on large-sized TPP instances, and generalize well across
instances of varying sizes and distributions, even to much larger instances
that are never seen during training. Experiments on various synthetic TPP
instances and the TPPLIB benchmark demonstrate that our DRL-based approach can
significantly outperform well-established TPP heuristics, reducing the
optimality gap by 40%-90%, and also showing an advantage in runtime, especially
on large-sized instances.
\\ ( https://arxiv.org/abs/2404.02476 ,  3596kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02477 (*cross-listing*)
Date: Wed, 3 Apr 2024 05:34:32 GMT   (3488kb)

Title: Enhancing Sum-Rate Performance in Constrained Multicell Networks: A
  Low-Information Exchange Approach
Authors: Youjin Kim, Jonggyu Jang, Hyun Jong Yang
Categories: eess.SP cs.AI
Comments: 5 pages, 12 figures
\\
  Despite the extensive research on massive MIMO systems for 5G
telecommunications and beyond, the reality is that many deployed base stations
are equipped with a limited number of antennas rather than supporting massive
MIMO configurations. Furthermore, while the cell-less network concept, which
eliminates cell boundaries, is under investigation, practical deployments often
grapple with significantly limited backhaul connection capacities between base
stations. This letter explores techniques to maximize the sum-rate performance
within the constraints of these more realistically equipped multicell networks.
We propose an innovative approach that dramatically reduces the need for
information exchange between base stations to a mere few bits, in stark
contrast to conventional methods that require the exchange of hundreds of bits.
Our proposed method not only addresses the limitations imposed by current
network infrastructure but also showcases significantly improved performance
under these constrained conditions.
\\ ( https://arxiv.org/abs/2404.02477 ,  3488kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02508 (*cross-listing*)
Date: Wed, 3 Apr 2024 06:53:27 GMT   (4943kb,D)

Title: VIAssist: Adapting Multi-modal Large Language Models for Users with
  Visual Impairments
Authors: Bufang Yang, Lixing He, Kaiwei Liu, Zhenyu Yan
Categories: cs.CV cs.AI cs.LG
Comments: Accepted to IEEE International Workshop on Foundation Models for
  Cyber-Physical Systems & Internet of Things (FMSys 2024)
\\
  Individuals with visual impairments, encompassing both partial and total
difficulties in visual perception, are referred to as visually impaired (VI)
people. An estimated 2.2 billion individuals worldwide are affected by visual
impairments. Recent advancements in multi-modal large language models (MLLMs)
have showcased their extraordinary capabilities across various domains. It is
desirable to help VI individuals with MLLMs' great capabilities of visual
understanding and reasoning. However, it is challenging for VI people to use
MLLMs due to the difficulties in capturing the desirable images to fulfill
their daily requests. For example, the target object is not fully or partially
placed in the image. This paper explores how to leverage MLLMs for VI
individuals to provide visual-question answers. VIAssist can identify undesired
images and provide detailed actions. Finally, VIAssist can provide reliable
answers to users' queries based on the images. Our results show that VIAssist
provides +0.21 and +0.31 higher BERTScore and ROUGE scores than the baseline,
respectively.
\\ ( https://arxiv.org/abs/2404.02508 ,  4943kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02515 (*cross-listing*)
Date: Wed, 3 Apr 2024 07:07:29 GMT   (9320kb,D)

Title: Tightly-Coupled LiDAR-IMU-Wheel Odometry with Online Calibration of a
  Kinematic Model for Skid-Steering Robots
Authors: Taku Okawara, Kenji Koide, Shuji Oishi, Masashi Yokozuka, Atsuhiko
  Banno, Kentaro Uno, Kazuya Yoshida
Categories: cs.RO cs.AI
\\
  Tunnels and long corridors are challenging environments for mobile robots
because a LiDAR point cloud should degenerate in these environments. To tackle
point cloud degeneration, this study presents a tightly-coupled LiDAR-IMU-wheel
odometry algorithm with an online calibration for skid-steering robots. We
propose a full linear wheel odometry factor, which not only serves as a motion
constraint but also performs the online calibration of kinematic models for
skid-steering robots. Despite the dynamically changing kinematic model (e.g.,
wheel radii changes caused by tire pressures) and terrain conditions, our
method can address the model error via online calibration. Moreover, our method
enables an accurate localization in cases of degenerated environments, such as
long and straight corridors, by calibration while the LiDAR-IMU fusion
sufficiently operates. Furthermore, we estimate the uncertainty (i.e.,
covariance matrix) of the wheel odometry online for creating a reasonable
constraint. The proposed method is validated through three experiments. The
first indoor experiment shows that the proposed method is robust in severe
degeneracy cases (long corridors) and changes in the wheel radii. The second
outdoor experiment demonstrates that our method accurately estimates the sensor
trajectory despite being in rough outdoor terrain owing to online uncertainty
estimation of wheel odometry. The third experiment shows the proposed online
calibration enables robust odometry estimation in changing terrains.
\\ ( https://arxiv.org/abs/2404.02515 ,  9320kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02523 (*cross-listing*)
Date: Wed, 3 Apr 2024 07:23:03 GMT   (2881kb,D)

Title: Text-driven Affordance Learning from Egocentric Vision
Authors: Tomoya Yoshida, Shuhei Kurita, Taichi Nishimura, Shinsuke Mori
Categories: cs.CV cs.AI
\\
  Visual affordance learning is a key component for robots to understand how to
interact with objects. Conventional approaches in this field rely on
pre-defined objects and actions, falling short of capturing diverse
interactions in realworld scenarios. The key idea of our approach is employing
textual instruction, targeting various affordances for a wide range of objects.
This approach covers both hand-object and tool-object interactions. We
introduce text-driven affordance learning, aiming to learn contact points and
manipulation trajectories from an egocentric view following textual
instruction. In our task, contact points are represented as heatmaps, and the
manipulation trajectory as sequences of coordinates that incorporate both
linear and rotational movements for various manipulations. However, when we
gather data for this task, manual annotations of these diverse interactions are
costly. To this end, we propose a pseudo dataset creation pipeline and build a
large pseudo-training dataset: TextAFF80K, consisting of over 80K instances of
the contact points, trajectories, images, and text tuples. We extend existing
referring expression comprehension models for our task, and experimental
results show that our approach robustly handles multiple affordances, serving
as a new standard for affordance learning in real-world scenarios.
\\ ( https://arxiv.org/abs/2404.02523 ,  2881kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02530 (*cross-listing*)
Date: Wed, 3 Apr 2024 07:33:30 GMT   (25298kb,D)

Title: Severity Controlled Text-to-Image Generative Model Bias Manipulation
Authors: Jordan Vice, Naveed Akhtar, Richard Hartley, and Ajmal Mian
Categories: cs.CV cs.AI
Comments: This research was supported by National Intelligence and Security
  Discovery Research Grants (project# NS220100007), funded by the Department of
  Defence Australia
\\
  Text-to-image (T2I) generative models are gaining wide popularity, especially
in public domains. However, their intrinsic bias and potential malicious
manipulations remain under-explored. Charting the susceptibility of T2I models
to such manipulation, we first expose the new possibility of a dynamic and
computationally efficient exploitation of model bias by targeting the embedded
language models. By leveraging mathematical foundations of vector algebra, our
technique enables a scalable and convenient control over the severity of output
manipulation through model bias. As a by-product, this control also allows a
form of precise prompt engineering to generate images which are generally
implausible with regular text prompts. We also demonstrate a constructive
application of our manipulation for balancing the frequency of generated
classes - as in model debiasing. Our technique does not require training and is
also framed as a backdoor attack with severity control using semantically-null
text triggers in the prompts. With extensive analysis, we present interesting
qualitative and quantitative results to expose potential manipulation
possibilities for T2I models.
  Key-words: Text-to-Image Models, Generative Models, Backdoor Attacks, Prompt
Engineering, Bias
\\ ( https://arxiv.org/abs/2404.02530 ,  25298kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02543 (*cross-listing*)
Date: Wed, 3 Apr 2024 08:00:46 GMT   (160kb,D)

Title: Unbiased Learning to Rank Meets Reality: Lessons from Baidu's
  Large-Scale Search Dataset
Authors: Philipp Hager, Romain Deffayet, Jean-Michel Renders, Onno Zoeter,
  Maarten de Rijke
Categories: cs.IR cs.AI
\\
  Unbiased learning-to-rank (ULTR) is a well-established framework for learning
from user clicks, which are often biased by the ranker collecting the data.
While theoretically justified and extensively tested in simulation, ULTR
techniques lack empirical validation, especially on modern search engines. The
dataset released for the WSDM Cup 2023, collected from Baidu's search engine,
offers a rare opportunity to assess the real-world performance of prominent
ULTR techniques. Despite multiple submissions during the WSDM Cup 2023 and the
subsequent NTCIR ULTRE-2 task, it remains unclear whether the observed
improvements stem from applying ULTR or other learning techniques. We revisit
and extend the available experiments. We find that unbiased learning-to-rank
techniques do not bring clear performance improvements, especially compared to
the stark differences brought by the choice of ranking loss and query-document
features. Our experiments reveal that ULTR robustly improves click prediction.
However, these gains in click prediction do not translate to enhanced ranking
performance on expert relevance annotations, implying that conclusions strongly
depend on how success is measured in this benchmark.
\\ ( https://arxiv.org/abs/2404.02543 ,  160kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02548 (*cross-listing*)
Date: Wed, 3 Apr 2024 08:15:08 GMT   (468kb,D)

Title: AI-Tutoring in Software Engineering Education
Authors: Eduard Frankford, Clemens Sauerwein, Patrick Bassner, Stephan Krusche
  and Ruth Breu
Categories: cs.SE cs.AI
Comments: 11 pages, 5 figures
DOI: 10.1145/3639474.3640061
\\
  With the rapid advancement of artificial intelligence (AI) in various
domains, the education sector is set for transformation. The potential of
AI-driven tools in enhancing the learning experience, especially in
programming, is immense. However, the scientific evaluation of Large Language
Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an
AI-Tutor remains largely unexplored. Therefore, there is a need to understand
how students interact with such AI-Tutors and to analyze their experiences. In
this paper, we conducted an exploratory case study by integrating the
GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a
combination of empirical data collection and an exploratory survey, we
identified different user types based on their interaction patterns with the
AI-Tutor. Additionally, the findings highlight advantages, such as timely
feedback and scalability. However, challenges like generic responses and
students' concerns about a learning progress inhibition when using the AI-Tutor
were also evident. This research adds to the discourse on AI's role in
education.
\\ ( https://arxiv.org/abs/2404.02548 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02552 (*cross-listing*)
Date: Wed, 3 Apr 2024 08:18:45 GMT   (6685kb,D)

Title: Solar synthetic imaging: Introducing denoising diffusion probabilistic
  models on SDO/AIA data
Authors: Francesco P. Ramunno, S. Hackstein, V. Kinakh, M. Drozdova, G.
  Quetant, A. Csillaghy and S. Voloshynovskiy
Categories: astro-ph.SR astro-ph.IM cs.AI
Comments: 16 pages, 10 figures. Accepted for publication in Astronomy and
  Astrophysics (A&A)
\\
  Given the rarity of significant solar flares compared to smaller ones,
training effective machine learning models for solar activity forecasting is
challenging due to insufficient data. This study proposes using generative deep
learning models, specifically a Denoising Diffusion Probabilistic Model (DDPM),
to create synthetic images of solar phenomena, including flares of varying
intensities. By employing a dataset from the AIA instrument aboard the SDO
spacecraft, focusing on the 171 {\AA} band that captures various solar
activities, and classifying images with GOES X-ray measurements based on flare
intensity, we aim to address the data scarcity issue. The DDPM's performance is
evaluated using cluster metrics, Frechet Inception Distance (FID), and
F1-score, showcasing promising results in generating realistic solar imagery.
We conduct two experiments: one to train a supervised classifier for event
identification and another for basic flare prediction, demonstrating the value
of synthetic data in managing imbalanced datasets. This research underscores
the potential of DDPMs in solar data analysis and forecasting, suggesting
further exploration into their capabilities for solar flare prediction and
application in other deep learning and physical tasks.
\\ ( https://arxiv.org/abs/2404.02552 ,  6685kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02569 (*cross-listing*)
Date: Wed, 3 Apr 2024 08:42:36 GMT   (3509kb,D)

Title: SliceIt! -- A Dual Simulator Framework for Learning Robot Food Slicing
Authors: Cristian C. Beltran-Hernandez, Nicolas Erbetti and Masashi Hamaya
Categories: cs.RO cs.AI
Comments: Accepted to ICRA 2024
\\
  Cooking robots can enhance the home experience by reducing the burden of
daily chores. However, these robots must perform their tasks dexterously and
safely in shared human environments, especially when handling dangerous tools
such as kitchen knives. This study focuses on enabling a robot to autonomously
and safely learn food-cutting tasks. More specifically, our goal is to enable a
collaborative robot or industrial robot arm to perform food-slicing tasks by
adapting to varying material properties using compliance control. Our approach
involves using Reinforcement Learning (RL) to train a robot to compliantly
manipulate a knife, by reducing the contact forces exerted by the food items
and by the cutting board. However, training the robot in the real world can be
inefficient, and dangerous, and result in a lot of food waste. Therefore, we
proposed SliceIt!, a framework for safely and efficiently learning robot
food-slicing tasks in simulation. Following a real2sim2real approach, our
framework consists of collecting a few real food slicing data, calibrating our
dual simulation environment (a high-fidelity cutting simulator and a robotic
simulator), learning compliant control policies on the calibrated simulation
environment, and finally, deploying the policies on the real robot.
\\ ( https://arxiv.org/abs/2404.02569 ,  3509kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02580 (*cross-listing*)
Date: Wed, 3 Apr 2024 08:55:44 GMT   (47699kb,D)

Title: Active learning for efficient annotation in precision agriculture: a
  use-case on crop-weed semantic segmentation
Authors: Bart M. van Marrewijk, Charbel Dandjinou, Dan Jeric Arcega Rustia,
  Nicolas Franco Gonzalez, Boubacar Diallo, J\'er\^ome Dias, Paul Melki, Pieter
  M. Blok
Categories: cs.CV cs.AI
\\
  Optimizing deep learning models requires large amounts of annotated images, a
process that is both time-intensive and costly. Especially for semantic
segmentation models in which every pixel must be annotated. A potential
strategy to mitigate annotation effort is active learning. Active learning
facilitates the identification and selection of the most informative images
from a large unlabelled pool. The underlying premise is that these selected
images can improve the model's performance faster than random selection to
reduce annotation effort. While active learning has demonstrated promising
results on benchmark datasets like Cityscapes, its performance in the
agricultural domain remains largely unexplored. This study addresses this
research gap by conducting a comparative study of three active learning-based
acquisition functions: Bayesian Active Learning by Disagreement (BALD),
stochastic-based BALD (PowerBALD), and Random. The acquisition functions were
tested on two agricultural datasets: Sugarbeet and Corn-Weed, both containing
three semantic classes: background, crop and weed. Our results indicated that
active learning, especially PowerBALD, yields a higher performance than Random
sampling on both datasets. But due to the relatively large standard deviations,
the differences observed were minimal; this was partly caused by high image
redundancy and imbalanced classes. Specifically, more than 89\% of the pixels
belonged to the background class on both datasets. The absence of significant
results on both datasets indicates that further research is required for
applying active learning on agricultural datasets, especially if they contain a
high-class imbalance and redundant images. Recommendations and insights are
provided in this paper to potentially resolve such issues.
\\ ( https://arxiv.org/abs/2404.02580 ,  47699kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02587 (*cross-listing*)
Date: Wed, 3 Apr 2024 09:12:22 GMT   (100kb)

Title: The Surprising Effectiveness of Rankers Trained on Expanded Queries
Authors: Abhijit Anand, Venktesh V, Vinay Setty, Avishek Anand
Categories: cs.IR cs.AI
\\
  An important problem in text-ranking systems is handling the hard queries
that form the tail end of the query distribution. The difficulty may arise due
to the presence of uncommon, underspecified, or incomplete queries. In this
work, we improve the ranking performance of hard or difficult queries without
compromising the performance of other queries. Firstly, we do LLM based query
enrichment for training queries using relevant documents. Next, a specialized
ranker is fine-tuned only on the enriched hard queries instead of the original
queries. We combine the relevance scores from the specialized ranker and the
base ranker, along with a query performance score estimated for each query. Our
approach departs from existing methods that usually employ a single ranker for
all queries, which is biased towards easy queries, which form the majority of
the query distribution. In our extensive experiments on the DL-Hard dataset, we
find that a principled query performance based scoring method using base and
specialized ranker offers a significant improvement of up to 25% on the passage
ranking task and up to 48.4% on the document ranking task when compared to the
baseline performance of using original queries, even outperforming SOTA model.
\\ ( https://arxiv.org/abs/2404.02587 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02618 (*cross-listing*)
Date: Wed, 3 Apr 2024 10:11:22 GMT   (23116kb,D)

Title: Diffexplainer: Towards Cross-modal Global Explanations with Diffusion
  Models
Authors: Matteo Pennisi, Giovanni Bellitto, Simone Palazzo, Mubarak Shah,
  Concetto Spampinato
Categories: cs.CV cs.AI
\\
  We present DiffExplainer, a novel framework that, leveraging language-vision
models, enables multimodal global explainability. DiffExplainer employs
diffusion models conditioned on optimized text prompts, synthesizing images
that maximize class outputs and hidden features of a classifier, thus providing
a visual tool for explaining decisions. Moreover, the analysis of generated
visual descriptions allows for automatic identification of biases and spurious
features, as opposed to traditional methods that often rely on manual
intervention. The cross-modal transferability of language-vision models also
enables the possibility to describe decisions in a more human-interpretable
way, i.e., through text. We conduct comprehensive experiments, which include an
extensive user study, demonstrating the effectiveness of DiffExplainer on 1)
the generation of high-quality images explaining model decisions, surpassing
existing activation maximization methods, and 2) the automated identification
of biases and spurious features.
\\ ( https://arxiv.org/abs/2404.02618 ,  23116kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02637 (*cross-listing*)
Date: Wed, 3 Apr 2024 10:54:07 GMT   (26kb,D)

Title: Vocabulary Attack to Hijack Large Language Model Applications
Authors: Patrick Levi and Christoph P. Neumann
Categories: cs.CR cs.AI cs.DC
Comments: To be published in: Proc of the 14th International Conference on
  Cloud Computing, GRIDs, and Virtualization (Cloud Computing 2024), Venice,
  Italy, April 2024
\\
  The fast advancements in Large Language Models (LLMs) are driving an
increasing number of applications. Together with the growing number of users,
we also see an increasing number of attackers who try to outsmart these
systems. They want the model to reveal confidential information, specific false
information, or offensive behavior. To this end, they manipulate their
instructions for the LLM by inserting separators or rephrasing them
systematically until they reach their goal. Our approach is different. It
inserts words from the model vocabulary. We find these words using an
optimization procedure and embeddings from another LLM (attacker LLM). We prove
our approach by goal hijacking two popular open-source LLMs from the Llama2 and
the Flan-T5 families, respectively. We present two main findings. First, our
approach creates inconspicuous instructions and therefore it is hard to detect.
For many attack cases, we find that even a single word insertion is sufficient.
Second, we demonstrate that we can conduct our attack using a different model
than the target model to conduct our attack with.
\\ ( https://arxiv.org/abs/2404.02637 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02648 (*cross-listing*)
Date: Wed, 3 Apr 2024 11:21:10 GMT   (19206kb,D)

Title: A Universal Deep Neural Network for Signal Detection in Wireless
  Communication Systems
Authors: Khalid Albagami, Nguyen Van Huynh, and Geoffrey Ye Li
Categories: cs.NI cs.AI cs.IT math.IT
\\
  Recently, deep learning (DL) has been emerging as a promising approach for
channel estimation and signal detection in wireless communications. The
majority of the existing studies investigating the use of DL techniques in this
domain focus on analysing channel impulse responses that are generated from
only one channel distribution such as additive white Gaussian channel noise and
Rayleigh channels. In practice, to cope with the dynamic nature of the wireless
channel, DL methods must be re-trained on newly non-aged collected data which
is costly, inefficient, and impractical. To tackle this challenge, this paper
proposes a novel universal deep neural network (Uni-DNN) that can achieve high
detection performance in various wireless environments without retraining the
model. In particular, our proposed Uni-DNN model consists of a wireless channel
classifier and a signal detector which are constructed by using DNNs. The
wireless channel classifier enables the signal detector to generalise and
perform optimally for multiple wireless channel distributions. In addition, to
further improve the signal detection performance of the proposed model,
convolutional neural network is employed. Extensive simulations using the
orthogonal frequency division multiplexing scheme demonstrate that the bit
error rate performance of our proposed solution can outperform conventional
DL-based approaches as well as least square and minimum mean square error
channel estimators in practical low pilot density scenarios.
\\ ( https://arxiv.org/abs/2404.02648 ,  19206kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02656 (*cross-listing*)
Date: Wed, 3 Apr 2024 11:37:03 GMT   (9781kb,D)

Title: Non-negative Subspace Feature Representation for Few-shot Learning in
  Medical Imaging
Authors: Keqiang Fan, Xiaohao Cai and Mahesan Niranjan
Categories: cs.CV cs.AI
\\
  Unlike typical visual scene recognition domains, in which massive datasets
are accessible to deep neural networks, medical image interpretations are often
obstructed by the paucity of data. In this paper, we investigate the
effectiveness of data-based few-shot learning in medical imaging by exploring
different data attribute representations in a low-dimensional space. We
introduce different types of non-negative matrix factorization (NMF) in
few-shot learning, addressing the data scarcity issue in medical image
classification. Extensive empirical studies are conducted in terms of
validating the effectiveness of NMF, especially its supervised variants (e.g.,
discriminative NMF, and supervised and constrained NMF with sparseness), and
the comparison with principal component analysis (PCA), i.e., the collaborative
representation-based dimensionality reduction technique derived from
eigenvectors. With 14 different datasets covering 11 distinct illness
categories, thorough experimental results and comparison with related
techniques demonstrate that NMF is a competitive alternative to PCA for
few-shot learning in medical imaging, and the supervised NMF algorithms are
more discriminative in the subspace with greater effectiveness. Furthermore, we
show that the part-based representation of NMF, especially its supervised
variants, is dramatically impactful in detecting lesion areas in medical
imaging with limited samples.
\\ ( https://arxiv.org/abs/2404.02656 ,  9781kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02675 (*cross-listing*)
Date: Wed, 3 Apr 2024 12:18:45 GMT   (359kb,D)

Title: Responsible Reporting for Frontier AI Development
Authors: Noam Kolt, Markus Anderljung, Joslyn Barnhart, Asher Brass, Kevin
  Esvelt, Gillian K. Hadfield, Lennart Heim, Mikel Rodriguez, Jonas B.
  Sandbrink, Thomas Woodside
Categories: cs.CY cs.AI
\\
  Mitigating the risks from frontier AI systems requires up-to-date and
reliable information about those systems. Organizations that develop and deploy
frontier systems have significant access to such information. By reporting
safety-critical information to actors in government, industry, and civil
society, these organizations could improve visibility into new and emerging
risks posed by frontier systems. Equipped with this information, developers
could make better informed decisions on risk management, while policymakers
could design more targeted and robust regulatory infrastructure. We outline the
key features of responsible reporting and propose mechanisms for implementing
them in practice.
\\ ( https://arxiv.org/abs/2404.02675 ,  359kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02702 (*cross-listing*)
Date: Wed, 3 Apr 2024 13:00:08 GMT   (277kb,D)

Title: PromptCodec: High-Fidelity Neural Speech Codec using Disentangled
  Representation Learning based Adaptive Feature-aware Prompt Encoders
Authors: Yu Pan, Lei Ma, Jianjun Zhao
Categories: cs.SD cs.AI
Comments: 7
\\
  Neural speech codec has recently gained widespread attention in generative
speech modeling domains, like voice conversion, text-to-speech synthesis, etc.
However, ensuring high-fidelity audio reconstruction of speech codecs under
high compression rates remains an open and challenging issue. In this paper, we
propose PromptCodec, a novel end-to-end neural speech codec model using
disentangled representation learning based feature-aware prompt encoders. By
incorporating additional feature representations from prompt encoders,
PromptCodec can distribute the speech information requiring processing and
enhance its capabilities. Moreover, a simple yet effective adaptive feature
weighted fusion approach is introduced to integrate features of different
encoders. Meanwhile, we propose a novel disentangled representation learning
strategy based on cosine distance to optimize PromptCodec's encoders to ensure
their efficiency, thereby further improving the performance of PromptCodec.
Experiments on LibriTTS demonstrate that our proposed PromptCodec consistently
outperforms state-of-the-art neural speech codec models under all different
bitrate conditions while achieving impressive performance with low bitrates.
\\ ( https://arxiv.org/abs/2404.02702 ,  277kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02728 (*cross-listing*)
Date: Wed, 3 Apr 2024 13:28:52 GMT   (385kb,D)

Title: Unsupervised Learning of Effective Actions in Robotics
Authors: Marko Zaric, Jakob Hollenstein, Justus Piater, Erwan Renaudo
Categories: cs.RO cs.AI cs.LG
Comments: Accepted at The First Austrian Symposium on AI, Robotics, and Vision
  (AIROV24)
\\
  Learning actions that are relevant to decision-making and can be executed
effectively is a key problem in autonomous robotics. Current state-of-the-art
action representations in robotics lack proper effect-driven learning of the
robot's actions. Although successful in solving manipulation tasks, deep
learning methods also lack this ability, in addition to their high cost in
terms of memory or training data. In this paper, we propose an unsupervised
algorithm to discretize a continuous motion space and generate "action
prototypes", each producing different effects in the environment. After an
exploration phase, the algorithm automatically builds a representation of the
effects and groups motions into action prototypes, where motions more likely to
produce an effect are represented more than those that lead to negligible
changes. We evaluate our method on a simulated stair-climbing reinforcement
learning task, and the preliminary results show that our effect driven
discretization outperforms uniformly and randomly sampled discretizations in
convergence speed and maximum reward.
\\ ( https://arxiv.org/abs/2404.02728 ,  385kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02729 (*cross-listing*)
Date: Wed, 3 Apr 2024 13:29:12 GMT   (1376kb,D)

Title: Learning Sequence Attractors in Recurrent Networks with Hidden Neurons
Authors: Yao Lu, Si Wu
Categories: cs.NE cs.AI cs.LG
\\
  The brain is targeted for processing temporal sequence information. It
remains largely unclear how the brain learns to store and retrieve sequence
memories. Here, we study how recurrent networks of binary neurons learn
sequence attractors to store predefined pattern sequences and retrieve them
robustly. We show that to store arbitrary pattern sequences, it is necessary
for the network to include hidden neurons even though their role in displaying
sequence memories is indirect. We develop a local learning algorithm to learn
sequence attractors in the networks with hidden neurons. The algorithm is
proven to converge and lead to sequence attractors. We demonstrate that the
network model can store and retrieve sequences robustly on synthetic and
real-world datasets. We hope that this study provides new insights in
understanding sequence memory and temporal information processing in the brain.
\\ ( https://arxiv.org/abs/2404.02729 ,  1376kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02755 (*cross-listing*)
Date: Wed, 3 Apr 2024 13:57:08 GMT   (3045kb,D)

Title: DIBS: Enhancing Dense Video Captioning with Unlabeled Videos via Pseudo
  Boundary Enrichment and Online Refinement
Authors: Hao Wu, Huabin Liu, Yu Qiao, Xiao Sun
Categories: cs.CV cs.AI cs.MM
Comments: Accepted by CVPR 2024
\\
  We present Dive Into the BoundarieS (DIBS), a novel pretraining framework for
dense video captioning (DVC), that elaborates on improving the quality of the
generated event captions and their associated pseudo event boundaries from
unlabeled videos. By leveraging the capabilities of diverse large language
models (LLMs), we generate rich DVC-oriented caption candidates and optimize
the corresponding pseudo boundaries under several meticulously designed
objectives, considering diversity, event-centricity, temporal ordering, and
coherence. Moreover, we further introduce a novel online boundary refinement
strategy that iteratively improves the quality of pseudo boundaries during
training. Comprehensive experiments have been conducted to examine the
effectiveness of the proposed technique components. By leveraging a substantial
amount of unlabeled video data, such as HowTo100M, we achieve a remarkable
advancement on standard DVC datasets like YouCook2 and ActivityNet. We
outperform the previous state-of-the-art Vid2Seq across a majority of metrics,
achieving this with just 0.4% of the unlabeled video data used for pre-training
by Vid2Seq.
\\ ( https://arxiv.org/abs/2404.02755 ,  3045kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02759 (*cross-listing*)
Date: Wed, 3 Apr 2024 14:05:39 GMT   (6529kb,D)

Title: Unsupervised Occupancy Learning from Sparse Point Cloud
Authors: Amine Ouasfi and Adnane Boukhayma
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: CVPR 2024
\\
  Implicit Neural Representations have gained prominence as a powerful
framework for capturing complex data modalities, encompassing a wide range from
3D shapes to images and audio. Within the realm of 3D shape representation,
Neural Signed Distance Functions (SDF) have demonstrated remarkable potential
in faithfully encoding intricate shape geometry. However, learning SDFs from 3D
point clouds in the absence of ground truth supervision remains a very
challenging task. In this paper, we propose a method to infer occupancy fields
instead of SDFs as they are easier to learn from sparse inputs. We leverage a
margin-based uncertainty measure to differentially sample from the decision
boundary of the occupancy function and supervise the sampled boundary points
using the input point cloud. We further stabilize the optimization process at
the early stages of the training by biasing the occupancy function towards
minimal entropy fields while maximizing its entropy at the input point cloud.
Through extensive experiments and evaluations, we illustrate the efficacy of
our proposed method, highlighting its capacity to improve implicit shape
inference with respect to baselines and the state-of-the-art using synthetic
and real data.
\\ ( https://arxiv.org/abs/2404.02759 ,  6529kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02806 (*cross-listing*)
Date: Wed, 3 Apr 2024 15:20:57 GMT   (3431kb,D)

Title: The RealHumanEval: Evaluating Large Language Models' Abilities to
  Support Programmers
Authors: Hussein Mozannar, Valerie Chen, Mohammed Alsobay, Subhro Das,
  Sebastian Zhao, Dennis Wei, Manish Nagireddy, Prasanna Sattigeri, Ameet
  Talwalkar, David Sontag
Categories: cs.SE cs.AI cs.HC
\\
  Evaluation of large language models (LLMs) for code has primarily relied on
static benchmarks, including HumanEval (Chen et al., 2021), which measure the
ability of LLMs to generate complete code that passes unit tests. As LLMs are
increasingly used as programmer assistants, we study whether gains on existing
benchmarks translate to gains in programmer productivity when coding with LLMs,
including time spent coding. In addition to static benchmarks, we investigate
the utility of preference metrics that might be used as proxies to measure LLM
helpfulness, such as code acceptance or copy rates. To do so, we introduce
RealHumanEval, a web interface to measure the ability of LLMs to assist
programmers, through either autocomplete or chat support. We conducted a user
study (N=213) using RealHumanEval in which users interacted with six LLMs of
varying base model performance. Despite static benchmarks not incorporating
humans-in-the-loop, we find that improvements in benchmark performance lead to
increased programmer productivity; however gaps in benchmark versus human
performance are not proportional -- a trend that holds across both forms of LLM
support. In contrast, we find that programmer preferences do not correlate with
their actual performance, motivating the need for better, human-centric proxy
signals. We also open-source RealHumanEval to enable human-centric evaluation
of new models and the study data to facilitate efforts to improve code models.
\\ ( https://arxiv.org/abs/2404.02806 ,  3431kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02807 (*cross-listing*)
Date: Wed, 3 Apr 2024 15:23:17 GMT   (7001kb,D)

Title: An Optimization Framework to Personalize Passive Cardiac Mechanics
Authors: Lei Shi, Ian Chen, Hiroo Takayama, Vijay Vedula
Categories: physics.med-ph cs.AI
\\
  Personalized cardiac mechanics modeling is a powerful tool for understanding
the biomechanics of cardiac function in health and disease and assisting in
treatment planning. However, current models are limited to using medical images
acquired at a single cardiac phase, often limiting their applicability for
processing dynamic image acquisitions. This study introduces an inverse finite
element analysis (iFEA) framework to estimate the passive mechanical properties
of cardiac tissue using time-dependent medical image data. The iFEA framework
relies on a novel nested optimization scheme, in which the outer iterations
utilize a traditional optimization method to best approximate material
parameters that fit image data, while the inner iterations employ an augmented
Sellier's algorithm to estimate the stress-free reference configuration. With a
focus on characterizing the passive mechanical behavior, the framework employs
structurally based anisotropic hyperelastic constitutive models and
physiologically relevant boundary conditions to simulate myocardial mechanics.
We use a stabilized variational multiscale formulation for solving the
governing nonlinear elastodynamics equations, verified for cardiac mechanics
applications. The framework is tested in myocardium models of biventricle and
left atrium derived from cardiac phase-resolved computed tomographic (CT)
images of a healthy subject and three patients with hypertrophic obstructive
cardiomyopathy (HOCM). The impact of the choice of optimization methods and
other numerical settings, including fiber direction parameters, mesh size,
initial parameters for optimization, and perturbations to optimal material
parameters, is assessed using a rigorous sensitivity analysis. The performance
of the current iFEA is compared against an assumed power-law-based
pressure-volume relation, typically used for single-phase image acquisition.
\\ ( https://arxiv.org/abs/2404.02807 ,  7001kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02817 (*cross-listing*)
Date: Wed, 3 Apr 2024 15:38:36 GMT   (18394kb,D)

Title: A Survey of Optimization-based Task and Motion Planning: From Classical
  To Learning Approaches
Authors: Zhigen Zhao, Shuo Chen, Yan Ding, Ziyi Zhou, Shiqi Zhang, Danfei Xu,
  Ye Zhao
Categories: cs.RO cs.AI
Comments: 24 pages, 12 figures, submitted for review
\\
  Task and Motion Planning (TAMP) integrates high-level task planning and
low-level motion planning to equip robots with the autonomy to effectively
reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on
hybrid optimization approaches that define goal conditions via objective
functions and are capable of handling open-ended goals, robotic dynamics, and
physical interaction between the robot and the environment. Therefore,
optimization-based TAMP is particularly suited to solve highly complex,
contact-rich locomotion and manipulation problems. This survey provides a
comprehensive review on optimization-based TAMP, covering (i) planning domain
representations, including action description languages and temporal logic,
(ii) individual solution strategies for components of TAMP, including AI
planning and trajectory optimization (TO), and (iii) the dynamic interplay
between logic-based task planning and model-based TO. A particular focus of
this survey is to highlight the algorithm structures to efficiently solve TAMP,
especially hierarchical and distributed approaches. Additionally, the survey
emphasizes the synergy between the classical methods and contemporary
learning-based innovations such as large language models. Furthermore, the
future research directions for TAMP is discussed in this survey, highlighting
both algorithmic and application-specific challenges.
\\ ( https://arxiv.org/abs/2404.02817 ,  18394kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02830 (*cross-listing*)
Date: Wed, 3 Apr 2024 16:04:59 GMT   (43253kb,D)

Title: Enhancing Interpretability of Vertebrae Fracture Grading using
  Human-interpretable Prototypes
Authors: Poulami Sinhamahapatra, Suprosanna Shit, Anjany Sekuboyina, Malek
  Husseini, David Schinz, Nicolas Lenhart, Joern Menze, Jan Kirschke, Karsten
  Roscher, Stephan Guennemann
Categories: cs.CV cs.AI
\\
  Vertebral fracture grading classifies the severity of vertebral fractures,
which is a challenging task in medical imaging and has recently attracted Deep
Learning (DL) models. Only a few works attempted to make such models
human-interpretable despite the need for transparency and trustworthiness in
critical use cases like DL-assisted medical diagnosis. Moreover, such models
either rely on post-hoc methods or additional annotations. In this work, we
propose a novel interpretable-by-design method, ProtoVerse, to find relevant
sub-parts of vertebral fractures (prototypes) that reliably explain the model's
decision in a human-understandable way. Specifically, we introduce a novel
diversity-promoting loss to mitigate prototype repetitions in small datasets
with intricate semantics. We have experimented with the VerSe'19 dataset and
outperformed the existing prototype-based method. Further, our model provides
superior interpretability against the post-hoc method. Importantly, expert
radiologists validated the visual interpretability of our results, showing
clinical applicability.
\\ ( https://arxiv.org/abs/2404.02830 ,  43253kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02877 (*cross-listing*)
Date: Wed, 3 Apr 2024 17:24:27 GMT   (12691kb,D)

Title: FlightScope: A Deep Comprehensive Assessment of Aircraft Detection
  Algorithms in Satellite Imagery
Authors: Safouane El Ghazouali, Arnaud Gucciardi, Nicola Venturi, Michael
  Rueegsegger, Umberto Michelucci
Categories: cs.CV cs.AI
Comments: 15 figures, 4 tables, comprehensive survey, comparative study
\\
  Object detection in remotely sensed satellite pictures is fundamental in many
fields such as biophysical, and environmental monitoring. While deep learning
algorithms are constantly evolving, they have been mostly implemented and
tested on popular ground-based taken photos. This paper critically evaluates
and compares a suite of advanced object detection algorithms customized for the
task of identifying aircraft within satellite imagery. Using the large
HRPlanesV2 dataset, together with a rigorous validation with the GDIT dataset,
this research encompasses an array of methodologies including YOLO versions 5
and 8, Faster RCNN, CenterNet, RetinaNet, RTMDet, and DETR, all trained from
scratch. This exhaustive training and validation study reveal YOLOv5 as the
preeminent model for the specific case of identifying airplanes from remote
sensing data, showcasing high precision and adaptability across diverse imaging
conditions. This research highlight the nuanced performance landscapes of these
algorithms, with YOLOv5 emerging as a robust solution for aerial object
detection, underlining its importance through superior mean average precision,
Recall, and Intersection over Union scores. The findings described here
underscore the fundamental role of algorithm selection aligned with the
specific demands of satellite imagery analysis and extend a comprehensive
framework to evaluate model efficacy. The benchmark toolkit and codes,
available via https://github.com/toelt-llc/FlightScope_Bench, aims to further
exploration and innovation in the realm of remote sensing object detection,
paving the way for improved analytical methodologies in satellite imagery
applications.
\\ ( https://arxiv.org/abs/2404.02877 ,  12691kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02883 (*cross-listing*)
Date: Wed, 3 Apr 2024 17:34:28 GMT   (3929kb,D)

Title: On the Scalability of Diffusion-based Text-to-Image Generation
Authors: Hao Li, Yang Zou, Ying Wang, Orchid Majumder, Yusheng Xie, R.
  Manmatha, Ashwin Swaminathan, Zhuowen Tu, Stefano Ermon, Stefano Soatto
Categories: cs.CV cs.AI cs.LG
Comments: CVPR2024
\\
  Scaling up model and data size has been quite successful for the evolution of
LLMs. However, the scaling law for the diffusion based text-to-image (T2I)
models is not fully explored. It is also unclear how to efficiently scale the
model for better performance at reduced cost. The different training settings
and expensive training cost make a fair model comparison extremely difficult.
In this work, we empirically study the scaling properties of diffusion based
T2I models by performing extensive and rigours ablations on scaling both
denoising backbones and training set, including training scaled UNet and
Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M
images. For model scaling, we find the location and amount of cross attention
distinguishes the performance of existing UNet designs. And increasing the
transformer blocks is more parameter-efficient for improving text-image
alignment than increasing channel numbers. We then identify an efficient UNet
variant, which is 45% smaller and 28% faster than SDXL's UNet. On the data
scaling side, we show the quality and diversity of the training set matters
more than simply dataset size. Increasing caption density and diversity
improves text-image alignment performance and the learning efficiency. Finally,
we provide scaling functions to predict the text-image alignment performance as
functions of the scale of model size, compute and dataset size.
\\ ( https://arxiv.org/abs/2404.02883 ,  3929kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02900 (*cross-listing*)
Date: Wed, 3 Apr 2024 17:58:21 GMT   (2570kb,D)

Title: DeiT-LT Distillation Strikes Back for Vision Transformer Training on
  Long-Tailed Datasets
Authors: Harsh Rangwani, Pradipto Mondal, Mayank Mishra, Ashish Ramayee Asokan,
  R. Venkatesh Babu
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024. Project Page: https://rangwani-harsh.github.io/DeiT-LT
\\
  Vision Transformer (ViT) has emerged as a prominent architecture for various
computer vision tasks. In ViT, we divide the input image into patch tokens and
process them through a stack of self attention blocks. However, unlike
Convolutional Neural Networks (CNN), ViTs simple architecture has no
informative inductive bias (e.g., locality,etc. ). Due to this, ViT requires a
large amount of data for pre-training. Various data efficient approaches (DeiT)
have been proposed to train ViT on balanced datasets effectively. However,
limited literature discusses the use of ViT for datasets with long-tailed
imbalances. In this work, we introduce DeiT-LT to tackle the problem of
training ViTs from scratch on long-tailed datasets. In DeiT-LT, we introduce an
efficient and effective way of distillation from CNN via distillation DIST
token by using out-of-distribution images and re-weighting the distillation
loss to enhance focus on tail classes. This leads to the learning of local
CNN-like features in early ViT blocks, improving generalization for tail
classes. Further, to mitigate overfitting, we propose distilling from a flat
CNN teacher, which leads to learning low-rank generalizable features for DIST
tokens across all ViT blocks. With the proposed DeiT-LT scheme, the
distillation DIST token becomes an expert on the tail classes, and the
classifier CLS token becomes an expert on the head classes. The experts help to
effectively learn features corresponding to both the majority and minority
classes using a distinct set of tokens within the same ViT architecture. We
show the effectiveness of DeiT-LT for training ViT from scratch on datasets
ranging from small-scale CIFAR-10 LT to large-scale iNaturalist-2018.
\\ ( https://arxiv.org/abs/2404.02900 ,  2570kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02904 (*cross-listing*)
Date: Wed, 3 Apr 2024 17:59:36 GMT   (31615kb,D)

Title: ALOHa: A New Measure for Hallucination in Captioning Models
Authors: Suzanne Petryk, David M. Chan, Anish Kachinthaya, Haodi Zou, John
  Canny, Joseph E. Gonzalez, Trevor Darrell
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: To appear at NAACL 2024
\\
  Despite recent advances in multimodal pre-training for visual description,
state-of-the-art models still produce captions containing errors, such as
hallucinating objects not present in a scene. The existing prominent metric for
object hallucination, CHAIR, is limited to a fixed set of MS COCO objects and
synonyms. In this work, we propose a modernized open-vocabulary metric, ALOHa,
which leverages large language models (LLMs) to measure object hallucinations.
Specifically, we use an LLM to extract groundable objects from a candidate
caption, measure their semantic similarity to reference objects from captions
and object detections, and use Hungarian matching to produce a final
hallucination score. We show that ALOHa correctly identifies 13.6% more
hallucinated objects than CHAIR on HAT, a new gold-standard subset of MS COCO
Captions annotated for hallucinations, and 30.8% more on nocaps, where objects
extend beyond MS COCO categories. Our code is available at
https://davidmchan.github.io/aloha/.
\\ ( https://arxiv.org/abs/2404.02904 ,  31615kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02905 (*cross-listing*)
Date: Wed, 3 Apr 2024 17:59:53 GMT   (5611kb,D)

Title: Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale
  Prediction
Authors: Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, Liwei Wang
Categories: cs.CV cs.AI
\\
  We present Visual AutoRegressive modeling (VAR), a new generation paradigm
that redefines the autoregressive learning on images as coarse-to-fine
"next-scale prediction" or "next-resolution prediction", diverging from the
standard raster-scan "next-token prediction". This simple, intuitive
methodology allows autoregressive (AR) transformers to learn visual
distributions fast and generalize well: VAR, for the first time, makes AR
models surpass diffusion transformers in image generation. On ImageNet 256x256
benchmark, VAR significantly improve AR baseline by improving Frechet inception
distance (FID) from 18.65 to 1.80, inception score (IS) from 80.4 to 356.4,
with around 20x faster inference speed. It is also empirically verified that
VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions
including image quality, inference speed, data efficiency, and scalability.
Scaling up VAR models exhibits clear power-law scaling laws similar to those
observed in LLMs, with linear correlation coefficients near -0.998 as solid
evidence. VAR further showcases zero-shot generalization ability in downstream
tasks including image in-painting, out-painting, and editing. These results
suggest VAR has initially emulated the two important properties of LLMs:
Scaling Laws and zero-shot task generalization. We have released all models and
codes to promote the exploration of AR/VAR models for visual generation and
unified learning.
\\ ( https://arxiv.org/abs/2404.02905 ,  5611kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02372 (*cross-listing*)
Date: Wed, 3 Apr 2024 00:13:23 GMT   (820kb,D)

Title: Obfuscated Malware Detection: Investigating Real-world Scenarios through
  Memory Analysis
Authors: S M Rakib Hasan and Aakar Dhakal
Categories: cs.CR cs.CL cs.LG
Comments: Accepted and Presented at IEEE-ICTP2023, Dhaka, Bangladesh
\\
  In the era of the internet and smart devices, the detection of malware has
become crucial for system security. Malware authors increasingly employ
obfuscation techniques to evade advanced security solutions, making it
challenging to detect and eliminate threats. Obfuscated malware, adept at
hiding itself, poses a significant risk to various platforms, including
computers, mobile devices, and IoT devices. Conventional methods like
heuristic-based or signature-based systems struggle against this type of
malware, as it leaves no discernible traces on the system. In this research, we
propose a simple and cost-effective obfuscated malware detection system through
memory dump analysis, utilizing diverse machine-learning algorithms. The study
focuses on the CIC-MalMem-2022 dataset, designed to simulate real-world
scenarios and assess memory-based obfuscated malware detection. We evaluate the
effectiveness of machine learning algorithms, such as decision trees, ensemble
methods, and neural networks, in detecting obfuscated malware within memory
dumps. Our analysis spans multiple malware categories, providing insights into
algorithmic strengths and limitations. By offering a comprehensive assessment
of machine learning algorithms for obfuscated malware detection through memory
analysis, this paper contributes to ongoing efforts to enhance cybersecurity
and fortify digital ecosystems against evolving and sophisticated malware
threats. The source code is made open-access for reproducibility and future
research endeavours. It can be accessed at https://bit.ly/MalMemCode.
\\ ( https://arxiv.org/abs/2404.02372 ,  820kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02489 (*cross-listing*)
Date: Wed, 3 Apr 2024 05:50:42 GMT   (1385kb,D)

Title: DUQGen: Effective Unsupervised Domain Adaptation of Neural Rankers by
  Diversifying Synthetic Query Generation
Authors: Ramraj Chandradevan, Kaustubh D. Dhole, Eugene Agichtein
Categories: cs.IR cs.CL
Comments: NAACL 2024 Main Conference
\\
  State-of-the-art neural rankers pre-trained on large task-specific training
data such as MS-MARCO, have been shown to exhibit strong performance on various
ranking tasks without domain adaptation, also called zero-shot. However,
zero-shot neural ranking may be sub-optimal, as it does not take advantage of
the target domain information. Unfortunately, acquiring sufficiently large and
high quality target training data to improve a modern neural ranker can be
costly and time-consuming. To address this problem, we propose a new approach
to unsupervised domain adaptation for ranking, DUQGen, which addresses a
critical gap in prior literature, namely how to automatically generate both
effective and diverse synthetic training data to fine tune a modern neural
ranker for a new domain. Specifically, DUQGen produces a more effective
representation of the target domain by identifying clusters of similar
documents; and generates a more diverse training dataset by probabilistic
sampling over the resulting document clusters. Our extensive experiments, over
the standard BEIR collection, demonstrate that DUQGen consistently outperforms
all zero-shot baselines and substantially outperforms the SOTA baselines on 16
out of 18 datasets, for an average of 4% relative improvement across all
datasets. We complement our results with a thorough analysis for more in-depth
understanding of the proposed method's performance and to identify promising
areas for further improvements.
\\ ( https://arxiv.org/abs/2404.02489 ,  1385kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02616 (*cross-listing*)
Date: Wed, 3 Apr 2024 10:05:47 GMT   (149kb,D)

Title: Improving Topic Relevance Model by Mix-structured Summarization and
  LLM-based Data Augmentation
Authors: Yizhu Liu, Ran Tao, Shengyu Guo and Yifan Yang
Categories: cs.IR cs.CL
\\
  Topic relevance between query and document is a very important part of social
search, which can evaluate the degree of matching between document and user's
requirement. In most social search scenarios such as Dianping, modeling search
relevance always faces two challenges. One is that many documents in social
search are very long and have much redundant information. The other is that the
training data for search relevance model is difficult to get, especially for
multi-classification relevance model. To tackle above two problems, we first
take query concatenated with the query-based summary and the document summary
without query as the input of topic relevance model, which can help model learn
the relevance degree between query and the core topic of document. Then, we
utilize the language understanding and generation abilities of large language
model (LLM) to rewrite and generate query from queries and documents in
existing training data, which can construct new query-document pairs as
training data. Extensive offline experiments and online A/B tests show that the
proposed approaches effectively improve the performance of relevance modeling.
\\ ( https://arxiv.org/abs/2404.02616 ,  149kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02677 (*cross-listing*)
Date: Wed, 3 Apr 2024 12:20:51 GMT   (1072kb,D)

Title: The VoicePrivacy 2024 Challenge Evaluation Plan
Authors: Natalia Tomashenko, Xiaoxiao Miao, Pierre Champion, Sarina Meyer, Xin
  Wang, Emmanuel Vincent, Michele Panariello, Nicholas Evans, Junichi
  Yamagishi, Massimiliano Todisco
Categories: eess.AS cs.CL cs.CR
Comments: arXiv admin note: substantial text overlap with arXiv:2203.12468
\\
  The task of the challenge is to develop a voice anonymization system for
speech data which conceals the speaker's voice identity while protecting
linguistic content and emotional states. The organizers provide development and
evaluation datasets and evaluation scripts, as well as baseline anonymization
systems and a list of training resources formed on the basis of the
participants' requests. Participants apply their developed anonymization
systems, run evaluation scripts and submit evaluation results and anonymized
speech data to the organizers. Results will be presented at a workshop held in
conjunction with Interspeech 2024 to which all participants are invited to
present their challenge systems and to submit additional workshop papers.
\\ ( https://arxiv.org/abs/2404.02677 ,  1072kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02822 (*cross-listing*)
Date: Wed, 3 Apr 2024 15:55:27 GMT   (57kb)

Title: Identifying Climate Targets in National Laws and Policies using Machine
  Learning
Authors: Matyas Juhasz, Tina Marchand, Roshan Melwani, Kalyan Dutia, Sarah
  Goodenough, Harrison Pim, Henry Franks
Categories: cs.CY cs.CL cs.LG
\\
  Quantified policy targets are a fundamental element of climate policy,
typically characterised by domain-specific and technical language. Current
methods for curating comprehensive views of global climate policy targets
entail significant manual effort. At present there are few scalable methods for
extracting climate targets from national laws or policies, which limits
policymakers' and researchers' ability to (1) assess private and public sector
alignment with global goals and (2) inform policy decisions. In this paper we
present an approach for extracting mentions of climate targets from national
laws and policies. We create an expert-annotated dataset identifying three
categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable
energy targets)) and train a classifier to reliably identify them in text. We
investigate bias and equity impacts related to our model and identify specific
years and country names as problematic features. Finally, we investigate the
characteristics of the dataset produced by running this classifier on the
Climate Policy Radar (CPR) dataset of global national climate laws and policies
and UNFCCC submissions, highlighting the potential of automated and scalable
data collection for existing climate policy databases and supporting further
research. Our work represents a significant upgrade in the accessibility of
these key climate policy elements for policymakers and researchers. We publish
our model at
\url{https://huggingface.co/ClimatePolicyRadar/national-climate-targets} and
related dataset at
\url{https://huggingface.co/datasets/ClimatePolicyRadar/national-climate-targets}.
\\ ( https://arxiv.org/abs/2404.02822 ,  57kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02171 (*cross-listing*)
Date: Fri, 29 Mar 2024 17:01:09 GMT   (10893kb,D)

Title: Path planning of magnetic microswimmers in high-fidelity simulations of
  capillaries with deep reinforcement learning
Authors: Lucas Amoudruz and Sergey Litvinov and Petros Koumoutsakos
Categories: physics.bio-ph cs.LG cs.RO
\\
  Biomedical applications such as targeted drug delivery, microsurgery or
sensing rely on reaching precise areas within the body in a minimally invasive
way. Artificial bacterial flagella (ABFs) have emerged as potential tools for
this task by navigating through the circulatory system. While the control and
swimming characteristics of ABFs is understood in simple scenarios, their
behavior within the bloodstream remains unclear. We conduct simulations of ABFs
evolving in the complex capillary networks found in the human retina. The ABF
is robustly guided to a prescribed target by a reinforcement learning agent
previously trained on a reduced order model.
\\ ( https://arxiv.org/abs/2404.02171 ,  10893kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02175 (*cross-listing*)
Date: Mon, 1 Apr 2024 11:23:31 GMT   (941kb)

Title: Social Dynamics of Consumer Response: A Unified Framework Integrating
  Statistical Physics and Marketing Dynamics
Authors: Javier Marin
Categories: physics.soc-ph cs.LG q-fin.GN
\\
  Comprehending how consumers react to advertising inputs is essential for
marketers aiming to optimize advertising strategies and improve campaign
effectiveness. This study examines the complex nature of consumer behaviour by
applying theoretical frameworks derived from physics and social psychology. We
present an innovative equation that captures the relation between spending on
advertising and consumer response, using concepts such as symmetries, scaling
laws, and phase transitions. By validating our equation against well-known
models such as the Michaelis-Menten and Hill equations, we prove its
effectiveness in accurately representing the complexity of consumer response
dynamics. The analysis emphasizes the importance of key model parameters, such
as marketing effectiveness, response sensitivity, and behavioural sensitivity,
in influencing consumer behaviour. The work explores the practical implications
for advertisers and marketers, as well as discussing the limitations and future
research directions. In summary, this study provides a thorough framework for
comprehending and forecasting consumer reactions to advertising, which has
implications for optimizing advertising strategies and allocating resources.
\\ ( https://arxiv.org/abs/2404.02175 ,  941kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02177 (*cross-listing*)
Date: Mon, 1 Apr 2024 20:55:03 GMT   (1187kb)

Title: Exploring Quantum-Enhanced Machine Learning for Computer Vision:
  Applications and Insights on Noisy Intermediate-Scale Quantum Devices
Authors: Purnachandra Mandadapu
Categories: quant-ph cs.LG
\\
  As medium-scale quantum computers progress, the application of quantum
algorithms across diverse fields like simulating physical systems, chemistry,
optimization, and cryptography becomes more prevalent. However, these quantum
computers, known as Noisy Intermediate Scale Quantum (NISQ), are susceptible to
noise, prompting the search for applications that can capitalize on quantum
advantage without extensive error correction procedures. Since, Machine
Learning (ML), particularly Deep Learning (DL), faces challenges due to
resource-intensive training and algorithmic opacity. Therefore, this study
explores the intersection of quantum computing and ML, focusing on computer
vision tasks. Specifically, it evaluates the effectiveness of hybrid
quantum-classical algorithms, such as the data re-uploading scheme and the
patch Generative Adversarial Networks (GAN) model, on small-scale quantum
devices. Through practical implementation and testing, the study reveals
comparable or superior performance of these algorithms compared to classical
counterparts, highlighting the potential of leveraging quantum algorithms in ML
tasks.
\\ ( https://arxiv.org/abs/2404.02177 ,  1187kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02239 (*cross-listing*)
Date: Tue, 2 Apr 2024 18:52:28 GMT   (43kb)

Title: Proximal Oracles for Optimization and Sampling
Authors: Jiaming Liang and Yongxin Chen
Categories: math.OC cs.LG
Comments: 25 pages. arXiv admin note: text overlap with arXiv:2202.13975
\\
  We consider convex optimization with non-smooth objective function and
log-concave sampling with non-smooth potential (negative log density). In
particular, we study two specific settings where the convex objective/potential
function is either semi-smooth or in composite form as the finite sum of
semi-smooth components. To overcome the challenges caused by non-smoothness,
our algorithms employ two powerful proximal frameworks in optimization and
sampling: the proximal point framework for optimization and the alternating
sampling framework (ASF) that uses Gibbs sampling on an augmented distribution.
A key component of both optimization and sampling algorithms is the efficient
implementation of the proximal map by the regularized cutting-plane method. We
establish the iteration-complexity of the proximal map in both semi-smooth and
composite settings. We further propose an adaptive proximal bundle method for
non-smooth optimization. The proposed method is universal since it does not
need any problem parameters as input. Additionally, we develop a proximal
sampling oracle that resembles the proximal map in optimization and establish
its complexity using a novel technique (a modified Gaussian integral). Finally,
we combine this proximal sampling oracle and ASF to obtain a Markov chain Monte
Carlo method with non-asymptotic complexity bounds for sampling in semi-smooth
and composite settings.
\\ ( https://arxiv.org/abs/2404.02239 ,  43kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02254 (*cross-listing*)
Date: Tue, 2 Apr 2024 19:21:28 GMT   (264kb)

Title: On Stronger Computational Separations Between Multimodal and Unimodal
  Machine Learning
Authors: Ari Karchmer
Categories: stat.ML cs.LG
\\
  In multimodal machine learning, multiple modalities of data (e.g., text and
images) are combined to facilitate the learning of a better machine learning
model, which remains applicable to a corresponding unimodal task (e.g., text
generation). Recently, multimodal machine learning has enjoyed huge empirical
success (e.g. GPT-4). Motivated to develop theoretical justification for this
empirical success, Lu (NeurIPS '23, ALT '24) introduces a theory of multimodal
learning, and considers possible separations between theoretical models of
multimodal and unimodal learning. In particular, Lu (ALT '24) shows a
computational separation, which is relevant to worst-case instances of the
learning task.
  In this paper, we give a stronger average-case computational separation,
where for "typical" instances of the learning task, unimodal learning is
computationally hard, but multimodal learning is easy. We then question how
"organic" the average-case separation is. Would it be encountered in practice?
To this end, we prove that under natural conditions, any given computational
separation between average-case unimodal and multimodal learning tasks implies
a corresponding cryptographic key agreement protocol. We suggest to interpret
this as evidence that very strong computational advantages of multimodal
learning may arise infrequently in practice, since they exist only for the
"pathological" case of inherently cryptographic distributions. However, this
does not apply to possible (super-polynomial) statistical advantages.
\\ ( https://arxiv.org/abs/2404.02254 ,  264kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02289 (*cross-listing*)
Date: Tue, 2 Apr 2024 20:32:32 GMT   (25432kb,D)

Title: Federated Multi-Agent Mapping for Planetary Exploration
Authors: Tiberiu-Ioan Szatmari and Abhishek Cauligi
Categories: cs.RO cs.LG cs.MA
Comments: 8 pages, 5 figures
ACM-class: I.2.11; I.2.9
\\
  In multi-agent robotic exploration, managing and effectively utilizing the
vast, heterogeneous data generated from dynamic environments poses a
significant challenge. Federated learning (FL) is a promising approach for
distributed mapping, addressing the challenges of decentralized data in
collaborative learning. FL enables joint model training across multiple agents
without requiring the centralization or sharing of raw data, overcoming
bandwidth and storage constraints. Our approach leverages implicit neural
mapping, representing maps as continuous functions learned by neural networks,
for compact and adaptable representations. We further enhance this approach
with meta-initialization on Earth datasets, pre-training the network to quickly
learn new map structures. This combination demonstrates strong generalization
to diverse domains like Martian terrain and glaciers. We rigorously evaluate
this approach, demonstrating its effectiveness for real-world deployment in
multi-agent exploration scenarios.
\\ ( https://arxiv.org/abs/2404.02289 ,  25432kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02294 (*cross-listing*)
Date: Tue, 2 Apr 2024 20:46:13 GMT   (6460kb,D)

Title: Constrained Robotic Navigation on Preferred Terrains Using LLMs and
  Speech Instruction: Exploiting the Power of Adverbs
Authors: Faraz Lotfi, Farnoosh Faraji, Nikhil Kakodkar, Travis Manderson, David
  Meger, and Gregory Dudek
Categories: cs.RO cs.LG
Comments: Presented at ISER 2023
\\
  This paper explores leveraging large language models for map-free off-road
navigation using generative AI, reducing the need for traditional data
collection and annotation. We propose a method where a robot receives verbal
instructions, converted to text through Whisper, and a large language model
(LLM) model extracts landmarks, preferred terrains, and crucial adverbs
translated into speed settings for constrained navigation. A language-driven
semantic segmentation model generates text-based masks for identifying
landmarks and terrain types in images. By translating 2D image points to the
vehicle's motion plane using camera parameters, an MPC controller can guides
the vehicle towards the desired terrain. This approach enhances adaptation to
diverse environments and facilitates the use of high-level instructions for
navigating complex and challenging terrains.
\\ ( https://arxiv.org/abs/2404.02294 ,  6460kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02343 (*cross-listing*)
Date: Tue, 2 Apr 2024 22:37:22 GMT   (205kb,D)

Title: Improved model-free bounds for multi-asset options using option-implied
  information and deep learning
Authors: Evangelia Dragazi, Shuaiqiang Liu, Antonis Papapantoleon
Categories: q-fin.PR cs.LG math.OC stat.ML
MSC-class: 91G20, 91G60, 68T07
\\
  We consider the computation of model-free bounds for multi-asset options in a
setting that combines dependence uncertainty with additional information on the
dependence structure. More specifically, we consider the setting where the
marginal distributions are known and partial information, in the form of known
prices for multi-asset options, is also available in the market. We provide a
fundamental theorem of asset pricing in this setting, as well as a superhedging
duality that allows to transform the maximization problem over probability
measures in a more tractable minimization problem over trading strategies. The
latter is solved using a penalization approach combined with a deep learning
approximation using artificial neural networks. The numerical method is fast
and the computational time scales linearly with respect to the number of traded
assets. We finally examine the significance of various pieces of additional
information. Empirical evidence suggests that "relevant" information, i.e.
prices of derivatives with the same payoff structure as the target payoff, are
more useful that other information, and should be prioritized in view of the
trade-off between accuracy and computational efficiency.
\\ ( https://arxiv.org/abs/2404.02343 ,  205kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02364 (*cross-listing*)
Date: Tue, 2 Apr 2024 23:34:39 GMT   (50kb)

Title: Learning Intersections of Halfspaces with Distribution Shift: Improved
  Algorithms and SQ Lower Bounds
Authors: Adam R. Klivans, Konstantinos Stavropoulos and Arsen Vasilyan
Categories: cs.DS cs.LG
\\
  Recent work of Klivans, Stavropoulos, and Vasilyan initiated the study of
testable learning with distribution shift (TDS learning), where a learner is
given labeled samples from training distribution $\mathcal{D}$, unlabeled
samples from test distribution $\mathcal{D}'$, and the goal is to output a
classifier with low error on $\mathcal{D}'$ whenever the training samples pass
a corresponding test. Their model deviates from all prior work in that no
assumptions are made on $\mathcal{D}'$. Instead, the test must accept (with
high probability) when the marginals of the training and test distributions are
equal.
  Here we focus on the fundamental case of intersections of halfspaces with
respect to Gaussian training distributions and prove a variety of new upper
bounds including a $2^{(k/\epsilon)^{O(1)}} \mathsf{poly}(d)$-time algorithm
for TDS learning intersections of $k$ homogeneous halfspaces to accuracy
$\epsilon$ (prior work achieved $d^{(k/\epsilon)^{O(1)}}$). We work under the
mild assumption that the Gaussian training distribution contains at least an
$\epsilon$ fraction of both positive and negative examples
($\epsilon$-balanced). We also prove the first set of SQ lower-bounds for any
TDS learning problem and show (1) the $\epsilon$-balanced assumption is
necessary for $\mathsf{poly}(d,1/\epsilon)$-time TDS learning for a single
halfspace and (2) a $d^{\tilde{\Omega}(\log 1/\epsilon)}$ lower bound for the
intersection of two general halfspaces, even with the $\epsilon$-balanced
assumption.
  Our techniques significantly expand the toolkit for TDS learning. We use
dimension reduction and coverings to give efficient algorithms for computing a
localized version of discrepancy distance, a key metric from the domain
adaptation literature.
\\ ( https://arxiv.org/abs/2404.02364 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02378 (*cross-listing*)
Date: Wed, 3 Apr 2024 00:41:19 GMT   (67kb)

Title: Faster Convergence of Stochastic Accelerated Gradient Descent under
  Interpolation
Authors: Aaron Mishkin, Mert Pilanci, Mark Schmidt
Categories: math.OC cs.LG
Comments: Results extend work from Aaron Mishkin's master's thesis
\\
  We prove new convergence rates for a generalized version of stochastic
Nesterov acceleration under interpolation conditions. Unlike previous analyses,
our approach accelerates any stochastic gradient method which makes sufficient
progress in expectation. The proof, which proceeds using the estimating
sequences framework, applies to both convex and strongly convex functions and
is easily specialized to accelerated SGD under the strong growth condition. In
this special case, our analysis reduces the dependence on the strong growth
constant from $\rho$ to $\sqrt{\rho}$ as compared to prior work. This
improvement is comparable to a square-root of the condition number in the worst
case and address criticism that guarantees for stochastic acceleration could be
worse than those for SGD.
\\ ( https://arxiv.org/abs/2404.02378 ,  67kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02387 (*cross-listing*)
Date: Wed, 3 Apr 2024 01:02:06 GMT   (585kb,D)

Title: An inversion problem for optical spectrum data via physics-guided
  machine learning
Authors: Hwiwoo Park, Jun H. Park, and Jungseek Hwang
Categories: physics.data-an cond-mat.str-el cs.LG physics.comp-ph
Comments: 19 pages, 4 figures
\\
  We propose the regularized recurrent inference machine (rRIM), a novel
machine-learning approach to solve the challenging problem of deriving the
pairing glue function from measured optical spectra. The rRIM incorporates
physical principles into both training and inference and affords noise
robustness, flexibility with out-of-distribution data, and reduced data
requirements. It effectively obtains reliable pairing glue functions from
experimental optical spectra and yields promising solutions for similar inverse
problems of the Fredholm integral equation of the first kind.
\\ ( https://arxiv.org/abs/2404.02387 ,  585kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02396 (*cross-listing*)
Date: Wed, 3 Apr 2024 01:55:15 GMT   (619kb,D)

Title: Enhancing Diffusion-based Point Cloud Generation with Smoothness
  Constraint
Authors: Yukun Li, Liping Liu
Categories: cs.CV cs.GR cs.LG
\\
  Diffusion models have been popular for point cloud generation tasks. Existing
works utilize the forward diffusion process to convert the original point
distribution into a noise distribution and then learn the reverse diffusion
process to recover the point distribution from the noise distribution. However,
the reverse diffusion process can produce samples with non-smooth points on the
surface because of the ignorance of the point cloud geometric properties. We
propose alleviating the problem by incorporating the local smoothness
constraint into the diffusion framework for point cloud generation. Experiments
demonstrate the proposed model can generate realistic shapes and smoother point
clouds, outperforming multiple state-of-the-art methods.
\\ ( https://arxiv.org/abs/2404.02396 ,  619kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02511 (*cross-listing*)
Date: Wed, 3 Apr 2024 06:55:59 GMT   (412kb,D)

Title: Stochastic Constrained Decentralized Optimization for Machine Learning
  with Fewer Data Oracles: a Gradient Sliding Approach
Authors: Hoang Huy Nguyen, Yan Li, Tuo Zhao
Categories: math.OC cs.LG
\\
  In modern decentralized applications, ensuring communication efficiency and
privacy for the users are the key challenges. In order to train
machine-learning models, the algorithm has to communicate to the data center
and sample data for its gradient computation, thus exposing the data and
increasing the communication cost. This gives rise to the need for a
decentralized optimization algorithm that is communication-efficient and
minimizes the number of gradient computations. To this end, we propose the
primal-dual sliding with conditional gradient sliding framework, which is
communication-efficient and achieves an $\varepsilon$-approximate solution with
the optimal gradient complexity of
$O(1/\sqrt{\varepsilon}+\sigma^2/{\varepsilon^2})$ and
$O(\log(1/\varepsilon)+\sigma^2/\varepsilon)$ for the convex and strongly
convex setting respectively and an LO (Linear Optimization) complexity of
$O(1/\varepsilon^2)$ for both settings given a stochastic gradient oracle with
variance $\sigma^2$. Compared with the prior work \cite{wai-fw-2017}, our
framework relaxes the assumption of the optimal solution being a strict
interior point of the feasible set and enjoys wider applicability for
large-scale training using a stochastic gradient oracle. We also demonstrate
the efficiency of our algorithms with various numerical experiments.
\\ ( https://arxiv.org/abs/2404.02511 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02538 (*cross-listing*)
Date: Wed, 3 Apr 2024 07:50:53 GMT   (121kb,D)

Title: Convergence Analysis of Flow Matching in Latent Space with Transformers
Authors: Yuling Jiao, Yanming Lai, Yang Wang and Bokai Yan
Categories: stat.ML cs.LG
\\
  We present theoretical convergence guarantees for ODE-based generative
models, specifically flow matching. We use a pre-trained autoencoder network to
map high-dimensional original inputs to a low-dimensional latent space, where a
transformer network is trained to predict the velocity field of the
transformation from a standard normal distribution to the target latent
distribution. Our error analysis demonstrates the effectiveness of this
approach, showing that the distribution of samples generated via estimated ODE
flow converges to the target distribution in the Wasserstein-2 distance under
mild and practical assumptions. Furthermore, we show that arbitrary smooth
functions can be effectively approximated by transformer networks with
Lipschitz continuity, which may be of independent interest.
\\ ( https://arxiv.org/abs/2404.02538 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02555 (*cross-listing*)
Date: Wed, 3 Apr 2024 08:22:41 GMT   (2086kb)

Title: An Interpretable Power System Transient Stability Assessment Method with
  Expert Guiding Neural-Regression-Tree
Authors: Hanxuan Wang, Na Lu, Zixuan Wang, Jiacheng Liu, Jun Liu
Categories: eess.SY cs.LG cs.SY
\\
  Deep learning based transient stability assessment (TSA) has achieved great
success, yet the lack of interpretability hinders its industrial application.
Although a great number of studies have tried to explore the interpretability
of network solutions, many problems still remain unsolved: (1) the difference
between the widely accepted power system knowledge and the generated
interpretive rules is large, (2) the probability characteristics of the neural
network have not been fully considered during generating the interpretive
rules, (3) the cost of the trade-off between accuracy and interpretability is
too heavy to take. To address these issues, an interpretable power system
Transient Stability Assessment method with Expert guiding
Neural-Regression-Tree (TSA-ENRT) is proposed. TSA-ENRT utilizes an expert
guiding nonlinear regression tree to approximate the neural network prediction
and the neural network can be explained by the interpretive rules generated by
the tree model. The nonlinearity of the expert guiding nonlinear regression
tree is endowed with the extracted knowledge from a simple two-machine
three-bus power system, which forms an expert knowledge base and thus the
generated interpretive rules are more consistent with human cognition. Besides,
the expert guiding tree model can build a bridge between the interpretive rules
and the probability prediction of neural network in a regression way. By
regularizing the neural network with the average decision length of ENRT, the
association of the neural network and tree model is constructed in the model
training level which provides a better trade-off between accuracy and
interpretability. Extensive experiments indicate the interpretive rules
generated by the proposed TSA-ENRT are highly consistent with the neural
network prediction and more agreed with human expert cognition.
\\ ( https://arxiv.org/abs/2404.02555 ,  2086kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02595 (*cross-listing*)
Date: Wed, 3 Apr 2024 09:19:46 GMT   (467kb,D)

Title: QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection
Authors: Nouhaila Innan, Alberto Marchisio, Muhammad Shafique, and Mohamed
  Bennai
Categories: quant-ph cs.LG q-fin.RM
Comments: 3 pages, 3 figures
\\
  This study introduces the Quantum Federated Neural Network for Financial
Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine
Learning (QML) and quantum computing with Federated Learning (FL) to innovate
financial fraud detection. Using quantum technologies' computational power and
FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying
fraudulent transactions. Implementing a dual-phase training model across
distributed clients surpasses existing methods in performance. QFNN-FFD
significantly improves fraud detection and ensures data confidentiality,
marking a significant advancement in fintech solutions and establishing a new
standard for privacy-focused fraud detection.
\\ ( https://arxiv.org/abs/2404.02595 ,  467kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02621 (*cross-listing*)
Date: Wed, 3 Apr 2024 10:19:53 GMT   (1532kb,D)

Title: Polynomial Graphical Lasso: Learning Edges from Gaussian
  Graph-Stationary Signals
Authors: Andrei Buciulea, Jiaxi Ying, Antonio G. Marques, and Daniel P. Palomar
Categories: eess.SP cs.LG
\\
  This paper introduces Polynomial Graphical Lasso (PGL), a new approach to
learning graph structures from nodal signals. Our key contribution lies in
modeling the signals as Gaussian and stationary on the graph, enabling the
development of a graph-learning formulation that combines the strengths of
graphical lasso with a more encompassing model. Specifically, we assume that
the precision matrix can take any polynomial form of the sought graph, allowing
for increased flexibility in modeling nodal relationships. Given the resulting
complexity and nonconvexity of the resulting optimization problem, we (i)
propose a low-complexity algorithm that alternates between estimating the graph
and precision matrices, and (ii) characterize its convergence. We evaluate the
performance of PGL through comprehensive numerical simulations using both
synthetic and real data, demonstrating its superiority over several
alternatives. Overall, this approach presents a significant advancement in
graph learning and holds promise for various applications in graph-aware signal
analysis and beyond.
\\ ( https://arxiv.org/abs/2404.02621 ,  1532kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02692 (*cross-listing*)
Date: Wed, 3 Apr 2024 12:39:37 GMT   (685kb,D)

Title: Automated Inference of Graph Transformation Rules
Authors: Jakob L. Andersen, Akbar Davoodi, Rolf Fagerberg, Christoph Flamm,
  Walter Fontana, Juri Kol\v{c}\'ak, Christophe V.F.P. Laurent, Daniel Merkle,
  Nikolai N{\o}jgaard
Categories: cs.DM cs.LG q-bio.MN
Comments: Preprint
\\
  The explosion of data available in life sciences is fueling an increasing
demand for expressive models and computational methods. Graph transformation is
a model for dynamic systems with a large variety of applications. We introduce
a novel method of the graph transformation model construction, combining
generative and dynamical viewpoints to give a fully automated data-driven model
inference method.
  The method takes the input dynamical properties, given as a "snapshot" of the
dynamics encoded by explicit transitions, and constructs a compatible model.
The obtained model is guaranteed to be minimal, thus framing the approach as
model compression (from a set of transitions into a set of rules). The
compression is permissive to a lossy case, where the constructed model is
allowed to exhibit behavior outside of the input transitions, thus suggesting a
completion of the input dynamics.
  The task of graph transformation model inference is naturally highly
challenging due to the combinatorics involved. We tackle the exponential
explosion by proposing a heuristically minimal translation of the task into a
well-established problem, set cover, for which highly optimized solutions
exist. We further showcase how our results relate to Kolmogorov complexity
expressed in terms of graph transformation.
\\ ( https://arxiv.org/abs/2404.02692 ,  685kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02726 (*cross-listing*)
Date: Wed, 3 Apr 2024 13:27:54 GMT   (6791kb,D)

Title: Harnessing the Power of Large Vision Language Models for Synthetic Image
  Detection
Authors: Mamadou Keita, Wassim Hamidouche, Hassen Bougueffa, Abdenour Hadid,
  Abdelmalik Taleb-Ahmed
Categories: cs.CV cs.CR cs.LG
Comments: arXiv admin note: substantial text overlap with arXiv:2404.01959
\\
  In recent years, the emergence of models capable of generating images from
text has attracted considerable interest, offering the possibility of creating
realistic images from text descriptions. Yet these advances have also raised
concerns about the potential misuse of these images, including the creation of
misleading content such as fake news and propaganda. This study investigates
the effectiveness of using advanced vision-language models (VLMs) for synthetic
image identification. Specifically, the focus is on tuning state-of-the-art
image captioning models for synthetic image detection. By harnessing the robust
understanding capabilities of large VLMs, the aim is to distinguish authentic
images from synthetic images produced by diffusion-based models. This study
contributes to the advancement of synthetic image detection by exploiting the
capabilities of visual language models such as BLIP-2 and ViTGPT2. By tailoring
image captioning models, we address the challenges associated with the
potential misuse of synthetic images in real-world applications. Results
described in this paper highlight the promising role of VLMs in the field of
synthetic image detection, outperforming conventional image-based detection
techniques. Code and models can be found at
https://github.com/Mamadou-Keita/VLM-DETECT.
\\ ( https://arxiv.org/abs/2404.02726 ,  6791kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02873 (*cross-listing*)
Date: Wed, 3 Apr 2024 17:09:25 GMT   (394kb)

Title: Gaussian Process Regression with Soft Inequality and Monotonicity
  Constraints
Authors: Didem Kochan and Xiu Yang
Categories: stat.ML cs.LG math.OC
Comments: 21 pages, 17 figures and 6 tables
\\
  Gaussian process (GP) regression is a non-parametric, Bayesian framework to
approximate complex models. Standard GP regression can lead to an unbounded
model in which some points can take infeasible values. We introduce a new GP
method that enforces the physical constraints in a probabilistic manner. This
GP model is trained by the quantum-inspired Hamiltonian Monte Carlo (QHMC).
QHMC is an efficient way to sample from a broad class of distributions. Unlike
the standard Hamiltonian Monte Carlo algorithm in which a particle has a fixed
mass, QHMC allows a particle to have a random mass matrix with a probability
distribution. Introducing the QHMC method to the inequality and monotonicity
constrained GP regression in the probabilistic sense, our approach improves the
accuracy and reduces the variance in the resulting GP model. According to our
experiments on several datasets, the proposed approach serves as an efficient
method as it accelerates the sampling process while maintaining the accuracy,
and it is applicable to high dimensional problems.
\\ ( https://arxiv.org/abs/2404.02873 ,  394kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2108.11204
replaced with revised version Wed, 3 Apr 2024 15:35:04 GMT   (1639kb,D)

Title: Subgoal Search For Complex Reasoning Tasks
Authors: Konrad Czechowski, Tomasz Odrzyg\'o\'zd\'z, Marek Zbysi\'nski,
  Micha{\l} Zawalski, Krzysztof Olejnik, Yuhuai Wu, {\L}ukasz Kuci\'nski, Piotr
  Mi{\l}o\'s
Categories: cs.AI cs.LG
Comments: NeurIPS 2021
\\ ( https://arxiv.org/abs/2108.11204 ,  1639kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15099
replaced with revised version Wed, 3 Apr 2024 08:15:14 GMT   (281kb,D)

Title: Probabilistic Dataset Reconstruction from Interpretable Models
Authors: Julien Ferry (LAAS-ROC), Ulrich A\"ivodji (ETS), S\'ebastien Gambs
  (UQAM), Marie-Jos\'e Huguet (LAAS-ROC), Mohamed Siala (LAAS-ROC)
Categories: cs.AI cs.IT math.IT
Report-no: Rapport LAAS n{\textdegree} 23244
Journal-ref: 2nd IEEE Conference on Secure and Trustworthy Machine Learning,
  Apr 2024, Toronto, Canada
\\ ( https://arxiv.org/abs/2308.15099 ,  281kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09270
replaced with revised version Wed, 3 Apr 2024 10:17:57 GMT   (912kb,D)

Title: Retro-fallback: retrosynthetic planning in an uncertain world
Authors: Austin Tripp, Krzysztof Maziarz, Sarah Lewis, Marwin Segler, Jos\'e
  Miguel Hern\'andez-Lobato
Categories: cs.AI cs.LG
Comments: ICLR 2024 camera ready version
  (https://openreview.net/forum?id=dl0u4ODCuW). This version is 58 pages (9
  pages main text, rest references and appendices). Code available at:
  https://github.com/AustinT/retro-fallback-iclr24. This version has 1) updated
  writing 2) updated figures 3) additional experimental results 4) more
  complete explanation of AND/OR graphs in the appendices
\\ ( https://arxiv.org/abs/2310.09270 ,  912kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18351
replaced with revised version Tue, 2 Apr 2024 20:48:39 GMT   (958kb)

Title: BioImage.IO Chatbot: A Community-Driven AI Assistant for Advanced
  Bioimage Analysis and Tool Integration
Authors: Wanlu Lei, Caterina Fuster-Barcel\'o, Gabriel Reder, Arrate
  Mu\~noz-Barrutia, Wei Ouyang
Categories: cs.AI q-bio.QM
Comments: 13 pages, 2 figures
DOI: 10.5281/zenodo.10032227
\\ ( https://arxiv.org/abs/2310.18351 ,  958kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13148
replaced with revised version Wed, 3 Apr 2024 03:13:38 GMT   (225kb,D)

Title: Towards Responsible Generative AI: A Reference Architecture for
  Designing Foundation Model based Agents
Authors: Qinghua Lu, Liming Zhu, Xiwei Xu, Zhenchang Xing, Stefan Harrer, Jon
  Whittle
Categories: cs.AI cs.SE
\\ ( https://arxiv.org/abs/2311.13148 ,  225kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00326
replaced with revised version Wed, 3 Apr 2024 10:10:44 GMT   (707kb,D)

Title: Agent-OM: Leveraging LLM Agents for Ontology Matching
Authors: Zhangcheng Qiang, Weiqing Wang, Kerry Taylor
Categories: cs.AI cs.CL cs.IR
Comments: 14 pages, 10 figures, 7 tables
\\ ( https://arxiv.org/abs/2312.00326 ,  707kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03678
replaced with revised version Wed, 3 Apr 2024 00:45:12 GMT   (849kb,D)

Title: Logical Specifications-guided Dynamic Task Sampling for Reinforcement
  Learning Agents
Authors: Yash Shukla, Tanushree Burman, Abhishek Kulkarni, Robert Wright,
  Alvaro Velasquez, Jivko Sinapov
Categories: cs.AI cs.LG cs.RO
\\ ( https://arxiv.org/abs/2402.03678 ,  849kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16424
replaced with revised version Wed, 3 Apr 2024 07:22:05 GMT   (194kb)

Title: An Experiment with the Use of ChatGPT for LCSH Subject Assignment on
  Electronic Theses and Dissertations
Authors: Eric H. C. Chow, TJ Kao and Xiaoli Li
Categories: cs.AI cs.DL cs.IR
Comments: 20 pages
\\ ( https://arxiv.org/abs/2403.16424 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00276
replaced with revised version Wed, 3 Apr 2024 05:47:00 GMT   (1887kb,D)

Title: Instruction-Driven Game Engines on Large Language Models
Authors: Hongqiu Wu, Y. Wang, Xingyuan Liu, Hai Zhao, Min Zhang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2404.00276 ,  1887kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01677
replaced with revised version Wed, 3 Apr 2024 09:28:31 GMT   (289kb,D)

Title: Towards Generalizable and Faithful Logic Reasoning over Natural Language
  via Resolution Refutation
Authors: Zhouhao Sun, Xiao Ding, Li Du, Bibo Cai, Jinglong Gao, Ting Liu, Qin
  Bing
Categories: cs.AI cs.CL
Comments: LREC-Coling 2024
\\ ( https://arxiv.org/abs/2404.01677 ,  289kb)
------------------------------------------------------------------------------
\\
arXiv:2110.00125
replaced with revised version Wed, 3 Apr 2024 12:01:46 GMT   (513kb,D)

Title: Combining Transformers with Natural Language Explanations
Authors: Federico Ruggeri, Marco Lippi, Paolo Torroni
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2110.00125 ,  513kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14710
replaced with revised version Wed, 3 Apr 2024 09:15:15 GMT   (4020kb,D)

Title: Instructions as Backdoors: Backdoor Vulnerabilities of Instruction
  Tuning for Large Language Models
Authors: Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2305.14710 ,  4020kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14910
replaced with revised version Tue, 2 Apr 2024 23:01:17 GMT   (7894kb,D)

Title: From Shortcuts to Triggers: Backdoor Defense with Denoised PoE
Authors: Qin Liu, Fei Wang, Chaowei Xiao, Muhao Chen
Categories: cs.CL cs.AI cs.CR cs.LG
Comments: Accepted by NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2305.14910 ,  7894kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15067
replaced with revised version Wed, 3 Apr 2024 15:52:28 GMT   (166kb,D)

Title: Not All Metrics Are Guilty: Improving NLG Evaluation by Diversifying
  References
Authors: Tianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang,
  Dongdong Zhang, Wayne Xin Zhao, Tom Kocmi, Furu Wei
Categories: cs.CL
Comments: Accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2305.15067 ,  166kb)
------------------------------------------------------------------------------
\\
arXiv:2305.16130
replaced with revised version Wed, 3 Apr 2024 16:27:31 GMT   (3884kb,D)

Title: Language Models Implement Simple Word2Vec-style Vector Arithmetic
Authors: Jack Merullo, Carsten Eickhoff, Ellie Pavlick
Categories: cs.CL cs.LG
Comments: NAACL
\\ ( https://arxiv.org/abs/2305.16130 ,  3884kb)
------------------------------------------------------------------------------
\\
arXiv:2306.15550
replaced with revised version Wed, 3 Apr 2024 14:48:57 GMT   (66kb,D)

Title: CamemBERT-bio: Leveraging Continual Pre-training for Cost-Effective
  Models on French Biomedical Data
Authors: Rian Touchent, Laurent Romary, Eric de la Clergerie
Categories: cs.CL cs.AI
Comments: Accepted to LREC-COLING 2024
\\ ( https://arxiv.org/abs/2306.15550 ,  66kb)
------------------------------------------------------------------------------
\\
arXiv:2307.15992
replaced with revised version Wed, 3 Apr 2024 04:36:40 GMT   (1856kb,D)

Title: Towards Codable Watermarking for Injecting Multi-bits Information to
  LLMs
Authors: Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng,
  Jie Zhou, Xu Sun
Categories: cs.CL
Comments: ICLR 2024 poster
\\ ( https://arxiv.org/abs/2307.15992 ,  1856kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16888
replaced with revised version Wed, 3 Apr 2024 05:53:20 GMT   (453kb,D)

Title: Backdooring Instruction-Tuned Large Language Models with Virtual Prompt
  Injection
Authors: Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang,
  Vijay Srinivasan, Xiang Ren, Hongxia Jin
Categories: cs.CL cs.CR cs.LG
Comments: Accepted to NAACL 2024. Project page: https://poison-llm.github.io
\\ ( https://arxiv.org/abs/2307.16888 ,  453kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10168
replaced with revised version Wed, 3 Apr 2024 00:25:39 GMT   (59kb,D)

Title: Head-to-Tail: How Knowledgeable are Large Language Models (LLMs)? A.K.A.
  Will LLMs Replace Knowledge Graphs?
Authors: Kai Sun, Yifan Ethan Xu, Hanwen Zha, Yue Liu, Xin Luna Dong
Categories: cs.CL
Comments: To appear in NAACL 2024
\\ ( https://arxiv.org/abs/2308.10168 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16705
replaced with revised version Wed, 3 Apr 2024 05:57:49 GMT   (8679kb,D)

Title: Exploring Cross-Cultural Differences in English Hate Speech Annotations:
  From Dataset Construction to Analysis
Authors: Nayeon Lee, Chani Jung, Junho Myung, Jiho Jin, Jose Camacho-Collados,
  Juho Kim, Alice Oh
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 Main Conference
\\ ( https://arxiv.org/abs/2308.16705 ,  8679kb)
------------------------------------------------------------------------------
\\
arXiv:2309.07990
replaced with revised version Tue, 2 Apr 2024 23:53:28 GMT   (8614kb,D)

Title: Leveraging Contextual Information for Effective Entity Salience
  Detection
Authors: Rajarshi Bhowmik, Marco Ponza, Atharva Tendle, Anant Gupta, Rebecca
  Jiang, Xingyu Lu, Qian Zhao, Daniel Preotiuc-Pietro
Categories: cs.CL
\\ ( https://arxiv.org/abs/2309.07990 ,  8614kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12960
replaced with revised version Wed, 3 Apr 2024 14:14:28 GMT   (1166kb,D)

Title: Nested Event Extraction upon Pivot Element Recogniton
Authors: Weicheng Ren, Zixuan Li, Xiaolong Jin, Long Bai, Miao Su, Yantao Liu,
  Saiping Guan, Jiafeng Guo, Xueqi Cheng
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2309.12960 ,  1166kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02263
replaced with revised version Wed, 3 Apr 2024 00:16:19 GMT   (100kb,D)

Title: Automatic Pair Construction for Contrastive Post-training
Authors: Canwen Xu, Corby Rosset, Ethan C. Chau, Luciano Del Corro, Shweti
  Mahajan, Julian McAuley, Jennifer Neville, Ahmed Hassan Awadallah, Nikhil Rao
Categories: cs.CL cs.AI cs.LG
Comments: NAACL 2024 (Findings)
\\ ( https://arxiv.org/abs/2310.02263 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02973
replaced with revised version Wed, 3 Apr 2024 14:12:36 GMT   (119kb,D)

Title: UniverSLU: Universal Spoken Language Understanding for Diverse Tasks
  with Natural Language Instructions
Authors: Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan
  Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Karen Livescu, Shinji Watanabe
Categories: cs.CL cs.SD eess.AS
Comments: Accepted at NAACL 2024
\\ ( https://arxiv.org/abs/2310.02973 ,  119kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03686
replaced with revised version Wed, 3 Apr 2024 12:09:26 GMT   (8032kb,D)

Title: DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers
Authors: Anna Langedijk, Hosein Mohebbi, Gabriele Sarti, Willem Zuidema, Jaap
  Jumelet
Categories: cs.CL
Comments: Accepted to Findings of NAACL 2024
\\ ( https://arxiv.org/abs/2310.03686 ,  8032kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03903
replaced with revised version Tue, 2 Apr 2024 22:35:39 GMT   (14955kb,D)

Title: LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination
  Abilities in Large Language Models
Authors: Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang
Categories: cs.CL cs.MA
\\ ( https://arxiv.org/abs/2310.03903 ,  14955kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05116
replaced with revised version Wed, 3 Apr 2024 05:41:05 GMT   (8789kb,D)

Title: Utilizing Contextual Clues and Role Correlations for Enhancing
  Document-level Event Argument Extraction
Authors: Wanlong Liu, Dingyi Zeng, Li Zhou, Yichen Xiao, Weishan Kong, Malu
  Zhang, Shaohuan Cheng, Hongyang Zhao, Wenyu Chen
Categories: cs.CL cs.IR
Comments: pre-submission
\\ ( https://arxiv.org/abs/2310.05116 ,  8789kb)
------------------------------------------------------------------------------
\\
arXiv:2310.05746
replaced with revised version Wed, 3 Apr 2024 03:37:56 GMT   (1097kb,D)

Title: Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and
  Execution of LLM Agents in an Auction Arena
Authors: Jiangjie Chen, Siyu Yuan, Rong Ye, Bodhisattwa Prasad Majumder, Kyle
  Richardson
Categories: cs.CL cs.AI
Comments: Preprint
\\ ( https://arxiv.org/abs/2310.05746 ,  1097kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10492
replaced with revised version Wed, 3 Apr 2024 06:05:56 GMT   (2006kb,D)

Title: UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking
Authors: Chuang Li, Yan Zhang, Min-Yen Kan, Haizhou Li
Categories: cs.CL
Comments: Accepted to Findings of NAACL 2024
\\ ( https://arxiv.org/abs/2310.10492 ,  2006kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14607
replaced with revised version Tue, 2 Apr 2024 21:29:20 GMT   (7423kb,D)

Title: Confronting LLMs with Traditional ML: Rethinking the Fairness of Large
  Language Models in Tabular Classifications
Authors: Yanchen Liu, Srishti Gautam, Jiaqi Ma, Himabindu Lakkaraju
Categories: cs.CL cs.LG
Comments: NAACL 2024 (Main Conference)
\\ ( https://arxiv.org/abs/2310.14607 ,  7423kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15398
replaced with revised version Wed, 3 Apr 2024 07:20:15 GMT   (6691kb,D)

Title: "One-Size-Fits-All"? Examining Expectations around What Constitute
  "Fair" or "Good" NLG System Behaviors
Authors: Li Lucy, Su Lin Blodgett, Milad Shokouhi, Hanna Wallach, Alexandra
  Olteanu
Categories: cs.CL cs.HC
Comments: 36 pages, 24 figures, NAACL 2024
\\ ( https://arxiv.org/abs/2310.15398 ,  6691kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19660
replaced with revised version Wed, 3 Apr 2024 14:29:03 GMT   (5468kb,D)

Title: Interpretable-by-Design Text Understanding with Iteratively Generated
  Concept Bottleneck
Authors: Josh Magnus Ludan, Qing Lyu, Yue Yang, Liam Dugan, Mark Yatskar, Chris
  Callison-Burch
Categories: cs.CL
\\ ( https://arxiv.org/abs/2310.19660 ,  5468kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01544
replaced with revised version Wed, 3 Apr 2024 11:49:53 GMT   (2776kb,D)

Title: Divergent Token Metrics: Measuring degradation to prune away LLM
  components -- and optimize quantization
Authors: Bj\"orn Deiseroth, Max Meuer, Nikolas Gritsch, Constantin Eichenberg,
  Patrick Schramowski, Matthias A{\ss}enmacher, Kristian Kersting
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.01544 ,  2776kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04978
replaced with revised version Tue, 2 Apr 2024 18:29:52 GMT   (7083kb,D)

Title: On the steerability of large language models toward data-driven personas
Authors: Junyi Li, Ninareh Mehrabi, Charith Peris, Palash Goyal, Kai-Wei Chang,
  Aram Galstyan, Richard Zemel, Rahul Gupta
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.04978 ,  7083kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07463
replaced with revised version Tue, 2 Apr 2024 21:18:00 GMT   (9717kb,D)

Title: MEGAVERSE: Benchmarking Large Language Models Across Languages,
  Modalities, Models and Tasks
Authors: Sanchit Ahuja, Divyanshu Aggarwal, Varun Gumma, Ishaan Watts, Ashutosh
  Sathe, Millicent Ochieng, Rishav Hada, Prachi Jain, Maxamed Axmed, Kalika
  Bali, Sunayana Sitaram
Categories: cs.CL
Comments: 40 pages, 35 figures and 34 tables
\\ ( https://arxiv.org/abs/2311.07463 ,  9717kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07484
replaced with revised version Wed, 3 Apr 2024 15:45:45 GMT   (657kb,D)

Title: Psychometric Predictive Power of Large Language Models
Authors: Tatsuki Kuribayashi, Yohei Oseki, Timothy Baldwin
Categories: cs.CL cs.AI
Comments: 23 pages; camera-ready version for NAACL 2024
\\ ( https://arxiv.org/abs/2311.07484 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08324
replaced with revised version Tue, 2 Apr 2024 19:03:15 GMT   (7911kb,D)

Title: Anti-LM Decoding for Zero-shot In-context Machine Translation
Authors: Suzanna Sia, Alexandra DeLucia, Kevin Duh
Categories: cs.CL cs.AI
Comments: Accepted to NAACL Findings 2024
\\ ( https://arxiv.org/abs/2311.08324 ,  7911kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08349
replaced with revised version Tue, 2 Apr 2024 18:28:04 GMT   (12285kb,D)

Title: AI-generated text boundary detection with RoFT
Authors: Laida Kushnareva, Tatiana Gaintseva, German Magai, Serguei Barannikov,
  Dmitry Abulkhanov, Kristian Kuznetsov, Eduard Tulchinskii, Irina
  Piontkovskaya, Sergey Nikolenko
Categories: cs.CL
\\ ( https://arxiv.org/abs/2311.08349 ,  12285kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09144
replaced with revised version Tue, 2 Apr 2024 19:34:15 GMT   (222kb,D)

Title: Grounding Gaps in Language Model Generations
Authors: Omar Shaikh, Kristina Gligori\'c, Ashna Khetan, Matthias Gerstgrasser,
  Diyi Yang, Dan Jurafsky
Categories: cs.CL cs.HC
Comments: NAACL 2024; 18 pages, 2 figures
\\ ( https://arxiv.org/abs/2311.09144 ,  222kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09467
replaced with revised version Wed, 3 Apr 2024 17:02:33 GMT   (7968kb,D)

Title: Think While You Write: Hypothesis Verification Promotes Faithful
  Knowledge-to-Text Generation
Authors: Yifu Qiu, Varun Embar, Shay B. Cohen, Benjamin Han
Categories: cs.CL cs.AI
Comments: NAACL 2024 (Findings)
\\ ( https://arxiv.org/abs/2311.09467 ,  7968kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09521
replaced with revised version Tue, 2 Apr 2024 20:46:37 GMT   (618kb,D)

Title: AMRFact: Enhancing Summarization Factuality Evaluation with AMR-Driven
  Negative Samples Generation
Authors: Haoyi Qiu, Kung-Hsiang Huang, Jingnong Qu, Nanyun Peng
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2311.09521 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09533
replaced with revised version Tue, 2 Apr 2024 20:04:01 GMT   (302kb,D)

Title: Effective Large Language Model Adaptation for Improved Grounding and
  Citation Generation
Authors: Xi Ye, Ruoxi Sun, Sercan \"O. Arik, Tomas Pfister
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2311.09533 ,  302kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09542
replaced with revised version Tue, 2 Apr 2024 19:42:21 GMT   (1282kb,D)

Title: Pregnant Questions: The Importance of Pragmatic Awareness in Maternal
  Health Question Answering
Authors: Neha Srikanth, Rupak Sarkar, Heran Mane, Elizabeth M. Aparicio, Quynh
  C. Nguyen, Rachel Rudinger, Jordan Boyd-Graber
Categories: cs.CL
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.09542 ,  1282kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09619
replaced with revised version Tue, 2 Apr 2024 21:05:40 GMT   (362kb,D)

Title: Take One Step at a Time to Know Incremental Utility of Demonstration: An
  Analysis on Reranking for Few-Shot In-Context Learning
Authors: Kazuma Hashimoto, Karthik Raman, Michael Bendersky
Categories: cs.CL
Comments: Accepted as a long paper at NAACL 2024
\\ ( https://arxiv.org/abs/2311.09619 ,  362kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09694
replaced with revised version Wed, 3 Apr 2024 15:07:45 GMT   (10055kb,D)

Title: Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness
Authors: Ashim Gupta, Rishanth Rajendhran, Nathan Stringham, Vivek Srikumar,
  Ana Marasovi\'c
Categories: cs.CL
Comments: To appear at NAACL 24 - main conference. The code is available at:
  https://github.com/utahnlp/scaling_robustness/
\\ ( https://arxiv.org/abs/2311.09694 ,  10055kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11805
replaced with revised version Tue, 2 Apr 2024 22:35:21 GMT   (26595kb,D)

Title: Gemini: A Family of Highly Capable Multimodal Models
Authors: Gemini Team Google: Rohan Anil, Sebastian Borgeaud, Jean-Baptiste
  Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth,
  Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian
  Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap,
  Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham,
  Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan
  Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem
  Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni,
  Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn,
  Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia
  Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, et al.
  (1290 additional authors not shown)
Categories: cs.CL cs.AI cs.CV
\\ ( https://arxiv.org/abs/2312.11805 ,  26595kb)
------------------------------------------------------------------------------
\\
arXiv:2312.14346
replaced with revised version Wed, 3 Apr 2024 02:49:25 GMT   (1733kb,D)

Title: Don't Believe Everything You Read: Enhancing Summarization
  Interpretability through Automatic Identification of Hallucinations in Large
  Language Models
Authors: Priyesh Vakharia, Devavrat Joshi, Meenal Chavan, Dhananjay Sonawane,
  Bhrigu Garg, Parsa Mazaheri
Categories: cs.CL cs.AI
Comments: All authors contributed equally to this work
\\ ( https://arxiv.org/abs/2312.14346 ,  1733kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17296
replaced with revised version Wed, 3 Apr 2024 17:35:11 GMT   (2769kb,D)

Title: Structured Packing in LLM Training Improves Long Context Utilization
Authors: Konrad Staniszewski, Szymon Tworkowski, Yu Zhao, Sebastian Jaszczur,
  Henryk Michalewski, {\L}ukasz Kuci\'nski, Piotr Mi{\l}o\'s
Categories: cs.CL
\\ ( https://arxiv.org/abs/2312.17296 ,  2769kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05190
replaced with revised version Tue, 2 Apr 2024 20:58:38 GMT   (3930kb,D)

Title: DCR: Divide-and-Conquer Reasoning for Multi-choice Question Answering
  with LLMs
Authors: Zijie Meng, Yan Zhang, Zhaopeng Feng, Zuozhu Liu
Categories: cs.CL
Comments: Technique Report
\\ ( https://arxiv.org/abs/2401.05190 ,  3930kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05827
replaced with revised version Wed, 3 Apr 2024 12:42:32 GMT   (40kb)

Title: Hallucination Benchmark in Medical Visual Question Answering
Authors: Jinge Wu, Yunsoo Kim, Honghan Wu
Categories: cs.CL cs.AI cs.CV
Comments: Accepted to ICLR 2024 Tiny Papers(Notable)
\\ ( https://arxiv.org/abs/2401.05827 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09407
replaced with revised version Wed, 3 Apr 2024 03:20:10 GMT   (18869kb,D)

Title: Deciphering Textual Authenticity: A Generalized Strategy through the
  Lens of Large Language Semantics for Detecting Human vs. Machine-Generated
  Text
Authors: Mazal Bethany, Brandon Wherry, Emet Bethany, Nishant Vishwamitra,
  Anthony Rios, Peyman Najafirad
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.09407 ,  18869kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11033
replaced with revised version Wed, 3 Apr 2024 10:34:10 GMT   (8217kb,D)

Title: FAIR Enough: How Can We Develop and Assess a FAIR-Compliant Dataset for
  Large Language Models' Training?
Authors: Shaina Raza, Shardul Ghuge, Chen Ding, Elham Dolatabadi, Deval Pandya
Categories: cs.CL
Comments: Accepted
\\ ( https://arxiv.org/abs/2401.11033 ,  8217kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13303
replaced with revised version Wed, 3 Apr 2024 08:23:06 GMT   (283kb,D)

Title: MaLA-500: Massive Language Adaptation of Large Language Models
Authors: Peiqin Lin, Shaoxiong Ji, J\"org Tiedemann, Andr\'e F. T. Martins,
  Hinrich Sch\"utze
Categories: cs.CL
\\ ( https://arxiv.org/abs/2401.13303 ,  283kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13660
replaced with revised version Wed, 3 Apr 2024 02:36:27 GMT   (1779kb,D)

Title: MambaByte: Token-free Selective State Space Model
Authors: Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2401.13660 ,  1779kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15269
replaced with revised version Wed, 3 Apr 2024 01:27:20 GMT   (487kb,D)

Title: Improving Medical Reasoning through Retrieval and Self-Reflection with
  Retrieval-Augmented Large Language Models
Authors: Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, Jaewoo Kang
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.15269 ,  487kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17377
replaced with revised version Tue, 2 Apr 2024 18:14:53 GMT   (7819kb,D)

Title: Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion
  Tokens
Authors: Jiacheng Liu, Sewon Min, Luke Zettlemoyer, Yejin Choi, Hannaneh
  Hajishirzi
Categories: cs.CL cs.AI cs.IR
\\ ( https://arxiv.org/abs/2401.17377 ,  7819kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10175
replaced with revised version Tue, 2 Apr 2024 21:51:36 GMT   (112kb,D)

Title: Unlocking Structure Measuring: Introducing PDD, an Automatic Metric for
  Positional Discourse Coherence
Authors: Yinhong Liu, Yixuan Su, Ehsan Shareghi and Nigel Collier
Categories: cs.CL
Comments: Accepted by NAACL 2024 main conference
\\ ( https://arxiv.org/abs/2402.10175 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2402.12343
replaced with revised version Wed, 3 Apr 2024 12:25:47 GMT   (1278kb,D)

Title: Emulated Disalignment: Safety Alignment for Large Language Models May
  Backfire!
Authors: Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli
  Ouyang, Yu Qiao
Categories: cs.CL cs.AI cs.LG
Comments: Code is available at https://github.com/ZHZisZZ/emulated-disalignment
\\ ( https://arxiv.org/abs/2402.12343 ,  1278kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13043
replaced with revised version Tue, 2 Apr 2024 20:06:52 GMT   (9029kb,D)

Title: Effective and Efficient Conversation Retrieval for Dialogue State
  Tracking with Implicit Text Summaries
Authors: Seanie Lee, Jianpeng Cheng, Joris Driesen, Alexandru Coca, Anders
  Johannsen
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2402.13043 ,  9029kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17231
replaced with revised version Wed, 3 Apr 2024 15:22:35 GMT   (6451kb,D)

Title: MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical
  Reasoning
Authors: Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.17231 ,  6451kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02712
replaced with revised version Wed, 3 Apr 2024 14:29:41 GMT   (112kb,D)

Title: Breeze-7B Technical Report
Authors: Chan-Jan Hsu, Chang-Le Liu, Feng-Ting Liao, Po-Chun Hsu, Yi-Chang
  Chen, Da-Shan Shiu
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.02712 ,  112kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08730
replaced with revised version Wed, 3 Apr 2024 15:22:23 GMT   (6933kb,D)

Title: Strengthening Multimodal Large Language Model with Bootstrapped
  Preference Optimization
Authors: Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan,
  Tong Zhang
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2403.08730 ,  6933kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11322
replaced with revised version Tue, 2 Apr 2024 18:57:49 GMT   (3630kb,D)

Title: StateFlow: Enhancing LLM Task-Solving through State-Driven Workflows
Authors: Yiran Wu, Tianwei Yue, Shaokun Zhang, Chi Wang, Qingyun Wu
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.11322 ,  3630kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14541
replaced with revised version Wed, 3 Apr 2024 16:09:22 GMT   (7827kb,D)

Title: EDT: Improving Large Language Models' Generation by Entropy-based
  Dynamic Temperature Sampling
Authors: Shimao Zhang, Yu Bao, Shujian Huang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.14541 ,  7827kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14982
replaced with revised version Wed, 3 Apr 2024 07:40:07 GMT   (7821kb,D)

Title: MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of
  Chain-of-Thoughts
Authors: Md Nishat Raihan, Dhiman Goswami, Al Nahian Bin Emran, Sadiya Sayara
  Chowdhury Puspo, Amrita Ganguly, Marcos Zampieri
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.14982 ,  7821kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14990
replaced with revised version Tue, 2 Apr 2024 23:42:13 GMT   (8501kb,D)

Title: MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic
  Textual Relatedness
Authors: Dhiman Goswami, Sadiya Sayara Chowdhury Puspo, Md Nishat Raihan, Al
  Nahian Bin Emran, Amrita Ganguly, Marcos Zampieri
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.14990 ,  8501kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16865
replaced with revised version Wed, 3 Apr 2024 12:59:20 GMT   (886kb,D)

Title: Encoding of lexical tone in self-supervised models of spoken language
Authors: Gaofei Shen, Michaela Watkins, Afra Alishahi, Arianna Bisazza,
  Grzegorz Chrupa{\l}a
Categories: cs.CL eess.AS
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2403.16865 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18346
replaced with revised version Wed, 3 Apr 2024 17:18:51 GMT   (5670kb,D)

Title: Quantifying and Mitigating Unimodal Biases in Multimodal Large Language
  Models: A Causal Perspective
Authors: Meiqi Chen, Yixin Cao, Yan Zhang, and Chaochao Lu
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2403.18346 ,  5670kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19183
replaced with revised version Wed, 3 Apr 2024 05:53:38 GMT   (6658kb,D)

Title: Empirical Analysis for Unsupervised Universal Dependency Parse Tree
  Aggregation
Authors: Adithya Kulkarni, Oliver Eulenstein, Qi Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.19183 ,  6658kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20147
replaced with revised version Wed, 3 Apr 2024 11:59:19 GMT   (7340kb,D)

Title: IndiBias: A Benchmark Dataset to Measure Social Biases in Language
  Models for Indian Context
Authors: Nihar Ranjan Sahoo, Pranamya Prashant Kulkarni, Narjis Asad, Arif
  Ahmad, Tanu Goyal, Aparna Garimella, Pushpak Bhattacharyya
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.20147 ,  7340kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00213
replaced with revised version Tue, 2 Apr 2024 20:09:45 GMT   (365kb,D)

Title: Injecting New Knowledge into Large Language Models via Supervised
  Fine-Tuning
Authors: Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel Holstein, Leonardo
  Nunes, Sara Malvar, Bruno Silva, Ranveer Chandra, Vijay Aski, Pavan Kumar
  Reddy Yannam, Tolga Aktas, Todd Hendry
Categories: cs.CL
Comments: 16 pages; 7 figures. updated authors list
\\ ( https://arxiv.org/abs/2404.00213 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00267
replaced with revised version Wed, 3 Apr 2024 17:29:12 GMT   (3869kb,D)

Title: Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal
  Traits
Authors: Zhivar Sourati, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Nuan
  Wen, Ala Tak, Fred Morstatter, Morteza Dehghani
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.00267 ,  3869kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00934
replaced with revised version Wed, 3 Apr 2024 17:04:06 GMT   (1243kb,D)

Title: ChatGLM-RLHF: Practices of Aligning Large Language Models with Human
  Feedback
Authors: Zhenyu Hou, Yilin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan
  Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.00934 ,  1243kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01744
replaced with revised version Wed, 3 Apr 2024 17:40:17 GMT   (1650kb,D)

Title: Octopus v2: On-device language model for super agent
Authors: Wei Chen, Zhiyuan Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.01744 ,  1650kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02013
replaced with revised version Wed, 3 Apr 2024 09:14:01 GMT   (543kb,D)

Title: Breaking the Silence Detecting and Mitigating Gendered Abuse in Hindi,
  Tamil, and Indian English Online Spaces
Authors: Advaitha Vetagiri, Gyandeep Kalita, Eisha Halder, Chetna Taparia,
  Partha Pakray, Riyanka Manna
Categories: cs.CL
\\ ( https://arxiv.org/abs/2404.02013 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:1903.00374
replaced with revised version Wed, 3 Apr 2024 14:26:32 GMT   (13826kb,D)

Title: Model-Based Reinforcement Learning for Atari
Authors: Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy
  H Campbell, Konrad Czechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski,
  Sergey Levine, Afroz Mohiuddin, Ryan Sepassi, George Tucker, Henryk
  Michalewski
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/1903.00374 ,  13826kb)
------------------------------------------------------------------------------
\\
arXiv:1911.12905
replaced with revised version Wed, 3 Apr 2024 14:35:49 GMT   (15108kb,D)

Title: Simulation-based reinforcement learning for real-world autonomous
  driving
Authors: B{\l}a\.zej Osi\'nski, Adam Jakubowski, Piotr Mi{\l}o\'s, Pawe{\l}
  Zi\k{e}cina, Christopher Galias, Silviu Homoceanu, and Henryk Michalewski
Categories: cs.LG cs.AI cs.RO
\\ ( https://arxiv.org/abs/1911.12905 ,  15108kb)
------------------------------------------------------------------------------
\\
arXiv:2107.07260
replaced with revised version Wed, 3 Apr 2024 10:56:36 GMT   (15775kb,D)

Title: MCL-GAN: Generative Adversarial Networks with Multiple Specialized
  Discriminators
Authors: Jinyoung Choi and Bohyung Han
Categories: cs.LG
Comments: NeurIPS 2022
\\ ( https://arxiv.org/abs/2107.07260 ,  15775kb)
------------------------------------------------------------------------------
\\
arXiv:2111.06464
replaced with revised version Wed, 3 Apr 2024 15:39:55 GMT   (4100kb,D)

Title: Catalytic Role Of Noise And Necessity Of Inductive Biases In The
  Emergence Of Compositional Communication
Authors: {\L}ukasz Kuci\'nski, Tomasz Korbak, Pawe{\l} Ko{\l}odziej, Piotr
  Mi{\l}o\'s
Categories: cs.LG cs.AI cs.CL
Comments: NeurIPS 2021
\\ ( https://arxiv.org/abs/2111.06464 ,  4100kb)
------------------------------------------------------------------------------
\\
arXiv:2111.11229
replaced with revised version Wed, 3 Apr 2024 17:13:05 GMT   (26224kb,D)

Title: Off-Policy Correction For Multi-Agent Reinforcement Learning
Authors: Micha{\l} Zawalski, B{\l}a\.zej Osi\'nski, Henryk Michalewski, Piotr
  Mi{\l}o\'s
Categories: cs.LG cs.AI cs.MA
ACM-class: I.2
\\ ( https://arxiv.org/abs/2111.11229 ,  26224kb)
------------------------------------------------------------------------------
\\
arXiv:2210.01708
replaced with revised version Wed, 3 Apr 2024 15:01:26 GMT   (5761kb,D)

Title: Conquering the Communication Constraints to Enable Large Pre-Trained
  Models in Federated Learning
Authors: Guangyu Sun, Umar Khalid, Matias Mendieta, Taojiannan Yang, Chen Chen
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2210.01708 ,  5761kb)
------------------------------------------------------------------------------
\\
arXiv:2210.12529
replaced with revised version Tue, 2 Apr 2024 22:48:13 GMT   (1195kb,D)

Title: On-Demand Sampling: Learning Optimally from Multiple Distributions
Authors: Nika Haghtalab and Michael I. Jordan and Eric Zhao
Categories: cs.LG cs.CY
Comments: 28 pages, 1 figure. Authors are ordered alphabetically. Outstanding
  paper award at the Thirty-sixth Conference on Neural Information Processing
  Systems (NeurIPS 2022). Version v2 updates a minor mistake in Lemma 3.1
\\ ( https://arxiv.org/abs/2210.12529 ,  1195kb)
------------------------------------------------------------------------------
\\
arXiv:2211.00759
replaced with revised version Wed, 3 Apr 2024 08:58:01 GMT   (365kb,D)

Title: Online Control of Adaptive Large Neighborhood Search using Deep
  Reinforcement Learning
Authors: Robbert Reijnen, Yingqian Zhang, Hoong Chuin Lau, Zaharah Bukhsh
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2211.00759 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2211.16462
replaced with revised version Tue, 2 Apr 2024 21:15:23 GMT   (88kb,D)

Title: Will My Robot Achieve My Goals? Predicting the Probability that an MDP
  Policy Reaches a User-Specified Behavior Target
Authors: Alexander Guyer, Thomas G. Dietterich
Categories: cs.LG stat.ME stat.ML
Comments: 12 pages, 4 figures. Appears in Proceedings of AAAI FSS-22 Symposium
  "Lessons Learned for Autonomous Assessment of Machine Abilities (LLAAMA)" The
  original submission had an error in theorem 2. Moreover, the stated
  guarantees for PCQR^{-1} were incorrect. This revision states the correct
  guarantees and corresponding theorem
\\ ( https://arxiv.org/abs/2211.16462 ,  88kb)
------------------------------------------------------------------------------
\\
arXiv:2304.10079
replaced with revised version Wed, 3 Apr 2024 15:46:30 GMT   (3311kb,D)

Title: Structure-reinforced Transformer for Dynamic Graph Representation
  Learning with Edge Temporal States
Authors: Shengxiang Hu, Guobing Zou, Song Yang, Shiyi Lin, Bofeng Zhang, Yixin
  Chen
Categories: cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
ACM-class: I.2.4
\\ ( https://arxiv.org/abs/2304.10079 ,  3311kb)
------------------------------------------------------------------------------
\\
arXiv:2306.05401
replaced with revised version Wed, 3 Apr 2024 10:16:22 GMT   (13951kb,D)

Title: RDumb: A simple approach that questions our progress in continual
  test-time adaptation
Authors: Ori Press, Steffen Schneider, Matthias K\"ummerer, Matthias Bethge
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2306.05401 ,  13951kb)
------------------------------------------------------------------------------
\\
arXiv:2306.10606
replaced with revised version Wed, 3 Apr 2024 12:22:29 GMT   (3571kb,D)

Title: Decongestion by Representation: Learning to Improve Economic Welfare in
  Marketplaces
Authors: Omer Nahum, Gali Noti, David Parkes, Nir Rosenfeld
Categories: cs.LG cs.GT
Comments: Accepted to ICLR 2024 poster
\\ ( https://arxiv.org/abs/2306.10606 ,  3571kb)
------------------------------------------------------------------------------
\\
arXiv:2306.17630
replaced with revised version Wed, 3 Apr 2024 08:34:26 GMT   (20856kb,D)

Title: Navigating Noise: A Study of How Noise Influences Generalisation and
  Calibration of Neural Networks
Authors: Martin Ferianc, Ondrej Bohdal, Timothy Hospedales, Miguel Rodrigues
Categories: cs.LG
Comments: Accepted at Transactions on Machine Learning Research (April 2024).
  Martin and Ondrej contributed equally
\\ ( https://arxiv.org/abs/2306.17630 ,  20856kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05862
replaced with revised version Wed, 3 Apr 2024 06:02:43 GMT   (5724kb,D)

Title: Ecosystem-level Analysis of Deployed Machine Learning Reveals
  Homogeneous Outcomes
Authors: Connor Toups, Rishi Bommasani, Kathleen A. Creel, Sarah H. Bana, Dan
  Jurafsky, Percy Liang
Categories: cs.LG cs.AI cs.CY
Comments: Published at NeurIPS 2023. Code is available at
  https://github.com/rishibommasani/EcosystemLevelAnalysis
\\ ( https://arxiv.org/abs/2307.05862 ,  5724kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13179
replaced with revised version Wed, 3 Apr 2024 13:21:45 GMT   (9059kb,D)

Title: Enhancing Multi-Objective Optimization through Machine
  Learning-Supported Multiphysics Simulation
Authors: Diego Botache, Jens Decke, Winfried Ripken, Abhinay Dornipati, Franz
  G\"otz-Hahn, Mohamed Ayeb, Bernhard Sick
Categories: cs.LG math.OC
\\ ( https://arxiv.org/abs/2309.13179 ,  9059kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00728
replaced with revised version Tue, 2 Apr 2024 19:44:34 GMT   (644kb,D)

Title: Physics-Informed Graph Neural Network for Dynamic Reconfiguration of
  Power Systems
Authors: Jules Authier and Rabab Haider and Anuradha Annaswamy and Florian
  Dorfler
Categories: cs.LG cs.SY eess.SY math.OC stat.ML
Comments: 8 pages, 5 figures, 2 tables. To appear at PSCC 2024
\\ ( https://arxiv.org/abs/2310.00728 ,  644kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10833
replaced with revised version Wed, 3 Apr 2024 16:31:00 GMT   (6085kb,D)

Title: Proper Laplacian Representation Learning
Authors: Diego Gomez, Michael Bowling, Marlos C. Machado
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2310.10833 ,  6085kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14714
replaced with revised version Wed, 3 Apr 2024 06:05:11 GMT   (1427kb,D)

Title: BatteryML:An Open-source platform for Machine Learning on Battery
  Degradation
Authors: Han Zhang, Xiaofan Gui, Shun Zheng, Ziheng Lu, Yuqi Li, Jiang Bian
Categories: cs.LG cs.AI
MSC-class: 68T05
\\ ( https://arxiv.org/abs/2310.14714 ,  1427kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14814
replaced with revised version Wed, 3 Apr 2024 09:32:09 GMT   (2531kb,D)

Title: Leveraging Ensemble Diversity for Robust Self-Training in the Presence
  of Sample Selection Bias
Authors: Ambroise Odonnat, Vasilii Feofanov, Ievgen Redko
Categories: cs.LG cs.AI stat.ML
Comments: Accepted at AISTATS 2024, Valencia, Spain
\\ ( https://arxiv.org/abs/2310.14814 ,  2531kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17463
replaced with revised version Wed, 3 Apr 2024 14:49:00 GMT   (4806kb,D)

Title: Bayesian Neural Controlled Differential Equations for Treatment Effect
  Estimation
Authors: Konstantin Hess, Valentyn Melnychuk, Dennis Frauen, Stefan Feuerriegel
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.17463 ,  4806kb)
------------------------------------------------------------------------------
\\
arXiv:2310.19558
replaced with revised version Wed, 3 Apr 2024 16:33:34 GMT   (4299kb,D)

Title: Privacy-preserving Federated Primal-dual Learning for Non-convex and
  Non-smooth Problems with Model Sparsification
Authors: Yiwei Li, Chien-Wei Huang, Shuai Wang, Chong-Yung Chi, Tony Q. S. Quek
Categories: cs.LG
Comments: 33 pages, 8 figures, 1 table. Accepted by IEEE Internet of Things
  Journal
DOI: 10.1109/JIOT.2024.3376552
\\ ( https://arxiv.org/abs/2310.19558 ,  4299kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01230
replaced with revised version Wed, 3 Apr 2024 10:15:00 GMT   (8365kb,D)

Title: Multi-Operational Mathematical Derivations in Latent Space
Authors: Marco Valentino, Jordan Meadows, Lan Zhang, Andr\'e Freitas
Categories: cs.LG cs.AI cs.SC
Comments: Accepted to NAACL 2024 - Camera Ready
\\ ( https://arxiv.org/abs/2311.01230 ,  8365kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13447
replaced with revised version Wed, 3 Apr 2024 14:23:20 GMT   (38kb)

Title: Differentially Private Non-Convex Optimization under the KL Condition
  with Optimal Rates
Authors: Michael Menart, Enayat Ullah, Raman Arora, Raef Bassily, Crist\'obal
  Guzm\'an
Categories: cs.LG cs.CR math.OC stat.ML
\\ ( https://arxiv.org/abs/2311.13447 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09056
replaced with revised version Wed, 3 Apr 2024 13:09:27 GMT   (8518kb,D)

Title: ReCoRe: Regularized Contrastive Representation Learning of World Model
Authors: Rudra P.K. Poudel, Harit Pandya, Stephan Liwicki, Roberto Cipolla
Categories: cs.LG cs.AI cs.CV cs.RO stat.ML
Comments: Accepted at CVPR 2024. arXiv admin note: text overlap with
  arXiv:2209.14932
\\ ( https://arxiv.org/abs/2312.09056 ,  8518kb)
------------------------------------------------------------------------------
\\
arXiv:2312.16335
replaced with revised version Wed, 3 Apr 2024 16:18:24 GMT   (1042kb,D)

Title: LeanVec: Searching vectors faster by making them fit
Authors: Mariano Tepper, Ishwar Singh Bhati, Cecilia Aguerrebere, Mark
  Hildebrand, Ted Willke
Categories: cs.LG cs.DB
\\ ( https://arxiv.org/abs/2312.16335 ,  1042kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05379
replaced with revised version Wed, 3 Apr 2024 00:16:25 GMT   (195kb,D)

Title: Tradeoffs of Diagonal Fisher Information Matrix Estimators
Authors: Alexander Soen and Ke Sun
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.05379 ,  195kb)
------------------------------------------------------------------------------
\\
arXiv:2402.06530
replaced with revised version Wed, 3 Apr 2024 13:18:53 GMT   (2585kb,D)

Title: Refining Myocardial Infarction Detection: A Novel Multi-Modal Composite
  Kernel Strategy in One-Class Classification
Authors: Muhammad Uzair Zahid, Aysen Degerli, Fahad Sohrab, Serkan Kiranyaz,
  Tahir Hamid, Rashid Mazhar, and Moncef Gabbouj
Categories: cs.LG cs.AI eess.SP
\\ ( https://arxiv.org/abs/2402.06530 ,  2585kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14227
replaced with revised version Wed, 3 Apr 2024 15:33:47 GMT   (7007kb,D)

Title: Quaternion recurrent neural network with real-time recurrent learning
  and maximum correntropy criterion
Authors: Pauline Bourigault, Dongpo Xu, Danilo P. Mandic
Categories: cs.LG
Comments: 2024 International Joint Conference on Neural Networks (IJCNN)
\\ ( https://arxiv.org/abs/2402.14227 ,  7007kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17802
replaced with revised version Tue, 2 Apr 2024 21:26:01 GMT   (226kb,D)

Title: Time Series Analysis in Compressor-Based Machines: A Survey
Authors: Francesca Forbicini, Nicol\`o Oreste Pinciroli Vago, Piero Fraternali
Categories: cs.LG
\\ ( https://arxiv.org/abs/2402.17802 ,  226kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00041
replaced with revised version Wed, 3 Apr 2024 15:16:58 GMT   (6857kb,D)

Title: Global and Local Prompts Cooperation via Optimal Transport for Federated
  Learning
Authors: Hongxia Li, Wei Huang, Jingya Wang and Ye Shi
Categories: cs.LG cs.AI cs.DC
\\ ( https://arxiv.org/abs/2403.00041 ,  6857kb)
------------------------------------------------------------------------------
\\
arXiv:2403.02694
replaced with revised version Wed, 3 Apr 2024 16:06:30 GMT   (1083kb,D)

Title: Privacy-Aware Semantic Cache for Large Language Models
Authors: Waris Gill (1), Mohamed Elidrisi (2), Pallavi Kalapatapu (2), Ali
  Anwar (3), Muhammad Ali Gulzar (1) ((1) Virginia Tech, USA, (2) Cisco, USA
  (3) University of Minnesota, Minneapolis, USA)
Categories: cs.LG cs.AI cs.CL cs.CR cs.DC
Comments: This study presents the first privacy aware semantic cache for LLMs
  based on Federated Learning. Total pages 13
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2403.02694 ,  1083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05713
replaced with revised version Wed, 3 Apr 2024 17:17:21 GMT   (1482kb,D)

Title: tsGT: Stochastic Time Series Modeling With Transformer
Authors: {\L}ukasz Kuci\'nski, Witold Drzewakowski, Mateusz Olko, Piotr
  Kozakowski, {\L}ukasz Maziarka, Marta Emilia Nowakowska, {\L}ukasz Kaiser,
  Piotr Mi{\l}o\'s
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.05713 ,  1482kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08167
replaced with revised version Wed, 3 Apr 2024 01:00:53 GMT   (6975kb,D)

Title: MolBind: Multimodal Alignment of Language, Molecules, and Proteins
Authors: Teng Xiao, Chao Cui, Huaisheng Zhu, and Vasant G. Honavar
Categories: cs.LG cs.CL q-bio.QM
\\ ( https://arxiv.org/abs/2403.08167 ,  6975kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15027
replaced with revised version Wed, 3 Apr 2024 09:51:29 GMT   (98kb,D)

Title: Grey-informed neural network for time-series forecasting
Authors: Wanli Xie and Ruibin Zhao and Zhenguo Xu and Tingting Liang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.15027 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18393
replaced with revised version Wed, 3 Apr 2024 06:26:05 GMT   (1466kb,D)

Title: Tensor-based Graph Learning with Consistency and Specificity for
  Multi-view Clustering
Authors: Long Shi and Lei Cao and Yunshan Ye and Yu Zhao and Badong Chen
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.18393 ,  1466kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18742
replaced with revised version Wed, 3 Apr 2024 15:30:03 GMT   (2293kb,D)

Title: Understanding the Learning Dynamics of Alignment with Human Feedback
Authors: Shawn Im, Yixuan Li
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.18742 ,  2293kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19800
replaced with revised version Wed, 3 Apr 2024 13:49:23 GMT   (8481kb,D)

Title: Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction
Authors: Jhon A. Castro-Correa, Jhony H. Giraldo, Mohsen Badiey, Fragkiskos D.
  Malliaros
Categories: cs.LG cs.AI eess.SP
Comments: Accepted by IEEE Transactions on Neural Networks and Learning Systems
  (TNNLS)
DOI: 10.1109/TNNLS.2024.3381069
\\ ( https://arxiv.org/abs/2403.19800 ,  8481kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20208
replaced with revised version Wed, 3 Apr 2024 12:23:27 GMT   (712kb,D)

Title: Unleashing the Potential of Large Language Models for Predictive Tabular
  Tasks in Data Science
Authors: Yazheng Yang, Yuqi Wang, Sankalok Sen, Lei Li, Qi Liu
Categories: cs.LG cs.AI
Comments: 10 pages
\\ ( https://arxiv.org/abs/2403.20208 ,  712kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00162
replaced with revised version Wed, 3 Apr 2024 03:28:48 GMT   (10814kb)

Title: Modeling Large-Scale Walking and Cycling Networks: A Machine Learning
  Approach Using Mobile Phone and Crowdsourced Data
Authors: Meead Saberi and Tanapon Lilasathapornkit
Categories: cs.LG cs.NA math.NA
Comments: 22 pages, 8 figures, 13 tables
\\ ( https://arxiv.org/abs/2404.00162 ,  10814kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00228
replaced with revised version Wed, 3 Apr 2024 07:15:05 GMT   (4324kb,D)

Title: InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
Authors: Yan-Shuo Liang, Wu-Jun Li
Categories: cs.LG cs.AI cs.CV
Comments: Accepted by the 2024 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR 2024)
\\ ( https://arxiv.org/abs/2404.00228 ,  4324kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00686
replaced with revised version Wed, 3 Apr 2024 14:32:17 GMT   (0kb,I)

Title: Utilizing Maximum Mean Discrepancy Barycenter for Propagating the
  Uncertainty of Value Functions in Reinforcement Learning
Authors: Srinjoy Roy, Swagatam Das
Categories: cs.LG
Comments: We found some flaws in our analysis and we are in the process of
  rectifying those
\\ ( https://arxiv.org/abs/2404.00686 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2404.02003
replaced with revised version Wed, 3 Apr 2024 12:05:27 GMT   (777kb,D)

Title: AUTODIFF: Autoregressive Diffusion Modeling for Structure-based Drug
  Design
Authors: Xinze Li, Penglei Wang, Tianfan Fu, Wenhao Gao, Chengtao Li, Leilei
  Shi, Junhong Liu
Categories: cs.LG
\\ ( https://arxiv.org/abs/2404.02003 ,  777kb)
------------------------------------------------------------------------------
\\
arXiv:2209.13628
replaced with revised version Tue, 2 Apr 2024 23:27:36 GMT   (35640kb,D)

Title: Unified Control Framework for Real-Time Interception and Obstacle
  Avoidance of Fast-Moving Objects with Diffusion Variational Autoencoder
Authors: Apan Dastider, Hao Fang and Mingjie Lin
Categories: cs.RO cs.AI
Comments: Submitted to IEEE International Conference on Intelligent Robots and
  Systems (IROS) 2024
\\ ( https://arxiv.org/abs/2209.13628 ,  35640kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13715 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 16:03:35 GMT   (2882kb,D)

Title: Trust Your $\nabla$: Gradient-based Intervention Targeting for Causal
  Discovery
Authors: Mateusz Olko, Micha{\l} Zaj\k{a}c, Aleksandra Nowak, Nino Scherrer,
  Yashas Annadani, Stefan Bauer, {\L}ukasz Kuci\'nski, Piotr Mi{\l}o\'s
Categories: stat.ML cs.AI cs.LG stat.ME
Comments: Accepted to 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2211.13715 ,  2882kb)
------------------------------------------------------------------------------
\\
arXiv:2303.04989
replaced with revised version Wed, 3 Apr 2024 06:51:21 GMT   (15052kb,D)

Title: ARS-DETR: Aspect Ratio Sensitive Oriented Object Detection with
  Transformer
Authors: Ying Zeng, Xue Yang, Qingyun Li, Yushi Chen, Junchi Yan
Categories: cs.CV cs.AI
Comments: 10 pages, 8 figures, 8 tables, the source code is available at
  https://github.com/httle/ARS-DETR
Journal-ref: IEEE Transactions on Geoscience and Remote Sensing, vol. 62, pp.
  1-15, 2024
DOI: 10.1109/TGRS.2024.3364713
\\ ( https://arxiv.org/abs/2303.04989 ,  15052kb)
------------------------------------------------------------------------------
\\
arXiv:2304.00553
replaced with revised version Wed, 3 Apr 2024 10:36:31 GMT   (5719kb,D)

Title: From Isolated Islands to Pangea: Unifying Semantic Space for Human
  Action Understanding
Authors: Yong-Lu Li, Xiaoqian Wu, Xinpeng Liu, Zehao Wang, Yiming Dou, Yikun
  Ji, Junyi Zhang, Yixing Li, Jingru Tan, Xudong Lu, Cewu Lu
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024, Project Webpage: https://mvig-rhos.com/pangea
\\ ( https://arxiv.org/abs/2304.00553 ,  5719kb)
------------------------------------------------------------------------------
\\
arXiv:2307.01449 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 02:26:24 GMT   (2751kb,D)

Title: A Double Machine Learning Approach to Combining Experimental and
  Observational Data
Authors: Harsh Parikh, Marco Morucci, Vittorio Orlandi, Sudeepa Roy, Cynthia
  Rudin, Alexander Volfovsky
Categories: stat.ME cs.AI cs.LG econ.EM
\\ ( https://arxiv.org/abs/2307.01449 ,  2751kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01235
replaced with revised version Tue, 2 Apr 2024 19:22:27 GMT   (428kb,D)

Title: Advancing the Search Frontier with AI Agents
Authors: Ryen W. White
Categories: cs.IR cs.AI
Comments: 11 pages, 6 figures, Accepted for publication in Communications of
  the ACM
\\ ( https://arxiv.org/abs/2311.01235 ,  428kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13127
replaced with revised version Tue, 2 Apr 2024 20:42:51 GMT   (39271kb,D)

Title: MetaCloak: Preventing Unauthorized Subject-driven Text-to-image
  Diffusion-based Synthesis via Meta-learning
Authors: Yixin Liu, Chenrui Fan, Yutong Dai, Xun Chen, Pan Zhou, and Lichao Sun
Categories: cs.CV cs.AI cs.CR
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2311.13127 ,  39271kb)
------------------------------------------------------------------------------
\\
arXiv:2311.16432
replaced with revised version Wed, 3 Apr 2024 15:05:28 GMT   (44666kb,D)

Title: Text-Driven Image Editing via Learnable Regions
Authors: Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu Jiang, Ming-Hsuan Yang
Categories: cs.CV cs.AI cs.LG
Comments: Accepted to CVPR 2024 Project webpage:
  https://yuanze-lin.me/LearnableRegions_page
\\ ( https://arxiv.org/abs/2311.16432 ,  44666kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12433
replaced with revised version Tue, 2 Apr 2024 18:09:22 GMT   (10240kb,D)

Title: TAO-Amodal: A Benchmark for Tracking Any Object Amodally
Authors: Cheng-Yen Hsieh, Kaihua Chen, Achal Dave, Tarasha Khurana, Deva
  Ramanan
Categories: cs.CV cs.AI cs.LG
Comments: Project Page: https://tao-amodal.github.io
\\ ( https://arxiv.org/abs/2312.12433 ,  10240kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12865
replaced with revised version Wed, 3 Apr 2024 14:39:32 GMT   (17377kb,D)

Title: RadEdit: stress-testing biomedical vision models via diffusion image
  editing
Authors: Fernando P\'erez-Garc\'ia, Sam Bond-Taylor, Pedro P. Sanchez, Boris
  van Breugel, Daniel C. Castro, Harshita Sharma, Valentina Salvatelli, Maria
  T. A. Wetscherek, Hannah Richardson, Matthew P. Lungren, Aditya Nori, Javier
  Alvarez-Valle, Ozan Oktay and Maximilian Ilse
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2312.12865 ,  17377kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04647
replaced with revised version Wed, 3 Apr 2024 09:25:08 GMT   (12275kb,D)

Title: Advancing Ante-Hoc Explainable Models through Generative Adversarial
  Networks
Authors: Tanmay Garg, Deepika Vemuri, Vineeth N Balasubramanian
Categories: cs.CV cs.AI cs.LG
Comments: Paper accepted in Human-Centric Representation Learning workshop at
  AAAI 2024 (https://hcrl-workshop.github.io/2024/). Paper accepted and
  presented at Deployable AI Workshop at AAAI-2024
  (https://sites.google.com/view/dai-2024/home)
\\ ( https://arxiv.org/abs/2401.04647 ,  12275kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10831
replaced with revised version Tue, 2 Apr 2024 18:54:50 GMT   (23960kb,D)

Title: Understanding Video Transformers via Universal Concept Discovery
Authors: Matthew Kowal, Achal Dave, Rares Ambrus, Adrien Gaidon, Konstantinos
  G. Derpanis, Pavel Tokmakov
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2401.10831 ,  23960kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12255
replaced with revised version Wed, 3 Apr 2024 06:23:34 GMT   (3108kb,D)

Title: Instructional Fingerprinting of Large Language Models
Authors: Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang Wei Koh, Chaowei Xiao,
  Muhao Chen
Categories: cs.CR cs.AI cs.CL cs.LG
Comments: Accepted at NAACL 2024; 30 pages
\\ ( https://arxiv.org/abs/2401.12255 ,  3108kb)
------------------------------------------------------------------------------
\\
arXiv:2401.13201
replaced with revised version Wed, 3 Apr 2024 03:52:44 GMT   (8513kb)

Title: MLLMReID: Multimodal Large Language Model-based Person Re-identification
Authors: Shan Yang, Yongfei Zhang
Categories: cs.CV cs.AI cs.CL
\\ ( https://arxiv.org/abs/2401.13201 ,  8513kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09664
replaced with revised version Wed, 3 Apr 2024 06:23:48 GMT   (9000kb,D)

Title: CodeMind: A Framework to Challenge Large Language Models for Code
  Reasoning
Authors: Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, Reyhaneh
  Jabbarvand
Categories: cs.SE cs.AI cs.CL cs.PL
\\ ( https://arxiv.org/abs/2402.09664 ,  9000kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14505
replaced with revised version Wed, 3 Apr 2024 14:59:08 GMT   (8453kb,D)

Title: Towards Seamless Adaptation of Pre-trained Models for Visual Place
  Recognition
Authors: Feng Lu, Lijun Zhang, Xiangyuan Lan, Shuting Dong, Yaowei Wang, Chun
  Yuan
Categories: cs.CV cs.AI
Comments: ICLR2024
\\ ( https://arxiv.org/abs/2402.14505 ,  8453kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15276
replaced with revised version Tue, 2 Apr 2024 20:54:46 GMT   (1791kb,D)

Title: CFIR: Fast and Effective Long-Text To Image Retrieval for Large Corpora
Authors: Zijun Long and Xuri Ge and Richard Mccreadie and Joemon Jose
Categories: cs.IR cs.AI cs.CV
\\ ( https://arxiv.org/abs/2402.15276 ,  1791kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17128
replaced with revised version Tue, 2 Apr 2024 23:14:42 GMT   (12024kb,D)

Title: OSCaR: Object State Captioning and State Change Representation
Authors: Nguyen Nguyen, Jing Bi, Ali Vosoughi, Yapeng Tian, Pooyan Fazli,
  Chenliang Xu
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2402.17128 ,  12024kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05839
replaced with revised version Wed, 3 Apr 2024 09:25:34 GMT   (29086kb,D)

Title: Long-term Frame-Event Visual Tracking: Benchmark Dataset and Baseline
Authors: Xiao Wang, Ju Huang, Shiao Wang, Chuanming Tang, Bo Jiang, Yonghong
  Tian, Jin Tang, Bin Luo
Categories: cs.CV cs.AI cs.NE
Comments: In Peer Review
\\ ( https://arxiv.org/abs/2403.05839 ,  29086kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15528 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 13:40:14 GMT   (449kb,D)

Title: Evaluating GPT-4 with Vision on Detection of Radiological Findings on
  Chest Radiographs
Authors: Yiliang Zhou, Hanley Ong, Patrick Kennedy, Carol Wu, Jacob Kazam,
  Keith Hentel, Adam Flanders, George Shih, Yifan Peng
Categories: eess.IV cs.AI cs.CV
\\ ( https://arxiv.org/abs/2403.15528 ,  449kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18305
replaced with revised version Wed, 3 Apr 2024 06:52:50 GMT   (4569kb,D)

Title: A Recommender System for NFT Collectibles with Item Feature
Authors: Minjoo Choi, Seonmi Kim, Yejin Kim, Youngbin Lee, Joohwan Hong,
  Yongjae Lee
Categories: cs.IR cs.AI
Comments: Presented at the AAAI 2023 Bridge on AI for Financial Services
  (https://sites.google.com/view/aaai-ai-fin/home)
\\ ( https://arxiv.org/abs/2403.18305 ,  4569kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19866
replaced with revised version Tue, 2 Apr 2024 22:41:53 GMT   (3720kb,D)

Title: Is Synthetic Image Useful for Transfer Learning? An Investigation into
  Data Generation, Volume, and Utilization
Authors: Yuhang Li, Xin Dong, Chen Chen, Jingtao Li, Yuxin Wen, Michael
  Spranger, Lingjuan Lyu
Categories: cs.CV cs.AI
Comments: ICLR24 Score 6865 https://openreview.net/forum?id=CjPt1AC6w0
\\ ( https://arxiv.org/abs/2403.19866 ,  3720kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01889
replaced with revised version Wed, 3 Apr 2024 09:18:09 GMT   (48263kb,D)

Title: RAVE: Residual Vector Embedding for CLIP-Guided Backlit Image
  Enhancement
Authors: Tatiana Gaintseva, Martin Benning, Gregory Slabaugh
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2404.01889 ,  48263kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00968
replaced with revised version Tue, 2 Apr 2024 19:57:32 GMT   (1693kb,D)

Title: Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of
  Low-rank Experts
Authors: Jialin Wu, Xia Hu, Yaqing Wang, Bo Pang, Radu Soricut
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2312.00968 ,  1693kb)
------------------------------------------------------------------------------
\\
arXiv:2312.01052
replaced with revised version Wed, 3 Apr 2024 06:15:16 GMT   (1316kb,D)

Title: SCTc-TE: A Comprehensive Formulation and Benchmark for Temporal Event
  Forecasting
Authors: Yunshan Ma, Chenchen Ye, Zijian Wu, Xiang Wang, Yixin Cao, Liang Pang,
  Tat-Seng Chua
Categories: cs.IR cs.CL
Comments: pre-print, 6 figures, 7 tables
ACM-class: H.3.0
\\ ( https://arxiv.org/abs/2312.01052 ,  1316kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16167
replaced with revised version Wed, 3 Apr 2024 09:02:52 GMT   (98kb,D)

Title: "You tell me": A Dataset of GPT-4-Based Behaviour Change Support
  Conversations
Authors: Selina Meyer and David Elsweiler
Categories: cs.HC cs.CL
Comments: Preprint as accepted at the 2024 ACM SIGIR Conference on Human
  Information Interaction and Retrieval (CHIIR '24)
\\ ( https://arxiv.org/abs/2401.16167 ,  98kb)
------------------------------------------------------------------------------
\\
arXiv:1905.12948 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 14:23:52 GMT   (479kb,D)

Title: Global Momentum Compression for Sparse Communication in Distributed
  Learning
Authors: Chang-Wei Shi, Shen-Yi Zhao, Yin-Peng Xie, Hao Gao, Wu-Jun Li
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/1905.12948 ,  479kb)
------------------------------------------------------------------------------
\\
arXiv:2110.09680 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 14:09:12 GMT   (4023kb,D)

Title: Multilevel Stochastic Optimization for Imputation in Massive Medical
  Data Records
Authors: Wenrui Li, Xiaoyu Wang, Yuetian Sun, Snezana Milanovic, Mark Kon,
  Julio Enrique Castrillon-Candas
Categories: stat.ML cs.LG stat.AP
Comments: 11 pages, 4 figures
Journal-ref: in IEEE Transactions on Big Data, vol. 10, no. 02, pp. 122-131,
  2024
DOI: 10.1109/TBDATA.2023.3328433
\\ ( https://arxiv.org/abs/2110.09680 ,  4023kb)
------------------------------------------------------------------------------
\\
arXiv:2111.09971
replaced with revised version Tue, 2 Apr 2024 20:54:46 GMT   (13008kb,D)

Title: Learning Robust Output Control Barrier Functions from Safe Expert
  Demonstrations
Authors: Lars Lindemann, Alexander Robey, Lejun Jiang, Satyajeet Das, Stephen
  Tu, and Nikolai Matni
Categories: eess.SY cs.LG cs.SY
Comments: Journal paper
\\ ( https://arxiv.org/abs/2111.09971 ,  13008kb)
------------------------------------------------------------------------------
\\
arXiv:2207.14554 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 10:39:21 GMT   (9810kb,AD)

Title: Reweighted Manifold Learning of Collective Variables from Enhanced
  Sampling Simulations
Authors: Jakub Rydzewski, Ming Chen, Tushar K. Ghosh, Omar Valsson
Categories: physics.chem-ph cs.LG physics.comp-ph
Comments: Published version
Journal-ref: J. Chem. Theory Comput. 2022, 18, 12, 7179-7192
DOI: 10.1021/acs.jctc.2c00873
\\ ( https://arxiv.org/abs/2207.14554 ,  9810kb)
------------------------------------------------------------------------------
\\
arXiv:2211.06757 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 14:31:43 GMT   (9823kb,D)

Title: DriftRec: Adapting diffusion models to blind JPEG restoration
Authors: Simon Welker, Henry N. Chapman, Timo Gerkmann
Categories: eess.IV cs.CV cs.LG
Comments: (C) 2024 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works
DOI: 10.1109/TIP.2024.3383776
\\ ( https://arxiv.org/abs/2211.06757 ,  9823kb)
------------------------------------------------------------------------------
\\
arXiv:2211.13289 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 12:04:18 GMT   (2990kb,D)

Title: Shapley Curves: A Smoothing Perspective
Authors: Ratmir Miftachov, Georg Keilbar, Wolfgang Karl H\"ardle
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2211.13289 ,  2990kb)
------------------------------------------------------------------------------
\\
arXiv:2212.03223 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 10:04:02 GMT   (1812kb,D)

Title: Financial Risk Management on a Neutral Atom Quantum Processor
Authors: Lucas Leclerc, Luis Ortiz-Guitierrez, Sebastian Grijalva, Boris
  Albrecht, Julia R. K. Cline, Vincent E. Elfving, Adrien Signoles, Lo\"ic
  Henriet, Gianni Del Bimbo, Usman Ayub Sheikh, Maitree Shah, Luc Andrea,
  Faysal Ishtiaq, Andoni Duarte, Samuel Mugel, Irene Caceres, Michel Kurek,
  Roman Orus, Achraf Seddik, Oumaima Hammammi, Hacene Isselnane, Didier M'tamon
Categories: quant-ph cond-mat.str-el cs.CE cs.LG
Comments: 17 pages, 11 figures, 2 tables, revised version
Journal-ref: Phys. Rev. Research 5, 043117 (2023)
\\ ( https://arxiv.org/abs/2212.03223 ,  1812kb)
------------------------------------------------------------------------------
\\
arXiv:2212.12561
replaced with revised version Wed, 3 Apr 2024 12:44:34 GMT   (1502kb,D)

Title: An active learning method for solving competitive multi-agent
  decision-making and control problems
Authors: Filippo Fabiani, Alberto Bemporad
Categories: eess.SY cs.LG cs.MA cs.SY math.OC
\\ ( https://arxiv.org/abs/2212.12561 ,  1502kb)
------------------------------------------------------------------------------
\\
arXiv:2303.11389
replaced with revised version Wed, 3 Apr 2024 11:33:14 GMT   (4006kb,D)

Title: Creating Ensembles of Classifiers through UMDA for Aerial Scene
  Classification
Authors: Fabio A. Faria, Luiz H. Buris, Luis A. M. Pereira, F\'abio A. M.
  Cappabianco
Categories: cs.CV cs.LG
Comments: 9 pages, 4 figures, accepted for presentation at the GECCO2024
\\ ( https://arxiv.org/abs/2303.11389 ,  4006kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15558 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 10:45:07 GMT   (321kb,D)

Title: Online Optimization for Randomized Network Resource Allocation with
  Long-Term Constraints
Authors: Ahmed Sid-Ali, Ioannis Lambadaris, Yiqiang Q. Zhao, Gennady Shaikhet,
  and Shima Kheradmand
Categories: math.OC cs.LG stat.ML
\\ ( https://arxiv.org/abs/2305.15558 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13674
replaced with revised version Wed, 3 Apr 2024 08:56:01 GMT   (7630kb,D)

Title: MeciFace: Mechanomyography and Inertial Fusion-based Glasses for Edge
  Real-Time Recognition of Facial and Eating Activities
Authors: Hymalai Bello, Sungho Suh, Bo Zhou and Paul Lukowicz
Categories: cs.CV cs.LG eess.IV eess.SP
Comments: Submitted to IEEE Transactions on Consumer Electronics
\\ ( https://arxiv.org/abs/2306.13674 ,  7630kb)
------------------------------------------------------------------------------
\\
arXiv:2307.11957 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 12:25:10 GMT   (1640kb,D)

Title: High-performance real-world optical computing trained by in situ
  model-free optimization
Authors: Guangyuan Zhao, Xin Shu, and Renjie Zhou
Categories: physics.optics cs.CV cs.ET cs.LG
\\ ( https://arxiv.org/abs/2307.11957 ,  1640kb)
------------------------------------------------------------------------------
\\
arXiv:2308.14740
replaced with revised version Wed, 3 Apr 2024 17:42:44 GMT   (23993kb,D)

Title: Total Selfie: Generating Full-Body Selfies
Authors: Bowei Chen, Brian Curless, Ira Kemelmacher-Shlizerman, Steven M. Seitz
Categories: cs.CV cs.GR cs.LG
Comments: Project page:
  https://homes.cs.washington.edu/~boweiche/project_page/totalselfie/
\\ ( https://arxiv.org/abs/2308.14740 ,  23993kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10224 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 10:57:10 GMT   (3657kb,D)

Title: CV-Attention UNet: Attention-based UNet for 3D Cerebrovascular
  Segmentation of Enhanced TOF-MRA Images
Authors: Syed Farhan Abbas, Nguyen Thanh Duc, Yoonguu Song, Kyungwon Kim, Ekta
  Srivastava, Boreom Lee
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.10224 ,  3657kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02184
replaced with revised version Wed, 3 Apr 2024 14:57:43 GMT   (0kb,I)

Title: Channel-Feedback-Free Transmission for Downlink FD-RAN: A Radio Map
  based Complex-valued Precoding Network Approach
Authors: Jiwei Zhao, Jiacheng Chen, Zeyu Sun, Yuhang Shi, Haibo Zhou, Xuemin
  (Sherman) Shen
Categories: cs.IT cs.LG math.IT
Comments: Content error, and I don't like to make it public now
\\ ( https://arxiv.org/abs/2312.02184 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11529
replaced with revised version Wed, 3 Apr 2024 13:52:07 GMT   (2269kb,D)

Title: Efficient and Scalable Graph Generation through Iterative Local
  Expansion
Authors: Andreas Bergmeister, Karolis Martinkus, Nathana\"el Perraudin, Roger
  Wattenhofer
Categories: cs.SI cs.LG
\\ ( https://arxiv.org/abs/2312.11529 ,  2269kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00035
replaced with revised version Wed, 3 Apr 2024 09:28:43 GMT   (1193kb,D)

Title: Robustness Assessment of a Runway Object Classifier for Safe Aircraft
  Taxiing
Authors: Yizhak Elboher, Raya Elsaleh, Omri Isac, M\'elanie Ducoffe, Audrey
  Galametz, Guillaume Pov\'eda, Ryma Boumazouza, No\'emie Cohen, Guy Katz
Categories: cs.CV cs.LG cs.LO
Comments: This is a preprint version of the paper in the proceedings of 43rd
  Digital Avionics Systems Conference (DASC)
\\ ( https://arxiv.org/abs/2402.00035 ,  1193kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00097
replaced with revised version Tue, 2 Apr 2024 21:23:03 GMT   (3975kb,D)

Title: Code-Aware Prompting: A study of Coverage Guided Test Generation in
  Regression Setting using LLM
Authors: Gabriel Ryan, Siddhartha Jain, Mingyue Shang, Shiqi Wang, Xiaofei Ma,
  Murali Krishna Ramanathan, Baishakhi Ray
Categories: cs.SE cs.LG
\\ ( https://arxiv.org/abs/2402.00097 ,  3975kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01687
replaced with revised version Wed, 3 Apr 2024 14:19:44 GMT   (521kb)

Title: "Which LLM should I use?": Evaluating LLMs for tasks performed by
  Undergraduate Computer Science Students
Authors: Vibhor Agarwal, Madhav Krishan Garg, Sahiti Dharmavaram, Dhruv Kumar
Categories: cs.CY cs.HC cs.LG
Comments: Under review
\\ ( https://arxiv.org/abs/2402.01687 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07739
replaced with revised version Wed, 3 Apr 2024 09:50:54 GMT   (14226kb,D)

Title: Task-conditioned adaptation of visual features in multi-task policy
  learning
Authors: Pierre Marza, Laetitia Matignon, Olivier Simonin, Christian Wolf
Categories: cs.CV cs.LG cs.RO
\\ ( https://arxiv.org/abs/2402.07739 ,  14226kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05743 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 00:44:52 GMT   (763kb)

Title: Forecasting Electricity Market Signals via Generative AI
Authors: Xinyi Wang, Qing Zhao, Lang Tong
Categories: eess.SP cs.LG econ.GN q-fin.EC
\\ ( https://arxiv.org/abs/2403.05743 ,  763kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15749 (*cross-listing*)
replaced with revised version Tue, 2 Apr 2024 18:11:09 GMT   (24kb,D)

Title: Horoballs and the subgradient method
Authors: Adrian S. Lewis and Genaro Lopez-Acedo and Adriana Nicolae
Categories: math.OC cs.CC cs.LG
MSC-class: 90C48, 65Y20, 49M29
ACM-class: G.1.6
\\ ( https://arxiv.org/abs/2403.15749 ,  24kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00015 (*cross-listing*)
replaced with revised version Wed, 3 Apr 2024 10:09:05 GMT   (554kb,D)

Title: Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning
Authors: Javier Mancilla, Andr\'e Sequeira, Tomas Tagliani, Francisco Llaneza,
  Claudio Beiza
Categories: q-fin.RM cs.LG q-fin.ST quant-ph stat.ML
Comments: Preprint
\\ ( https://arxiv.org/abs/2404.00015 ,  554kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
