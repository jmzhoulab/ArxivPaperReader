Gmail	jiamu zhou <zhoujm99@gmail.com>
cs daily Subj-class mailing 200021 1
send mail ONLY to cs <no-reply@arxiv.org>	2024年4月2日 15:38
回复：cs@arxiv.org
收件人：cs daily title/abstract distribution <rabble@arxiv.org>
------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Artificial Intelligence
Computation and Language
Machine Learning
 received from  Fri 29 Mar 24 18:00:00 GMT  to  Mon  1 Apr 24 18:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2404.00017
Date: Sun, 17 Mar 2024 13:08:11 GMT   (5369kb,D)

Title: Psittacines of Innovation? Assessing the True Novelty of AI Creations
Authors: Anirban Mukherjee
Categories: cs.AI cs.CL cs.HC
\\
  We examine whether Artificial Intelligence (AI) systems generate truly novel
ideas rather than merely regurgitating patterns learned during training.
Utilizing a novel experimental design, we task an AI with generating project
titles for hypothetical crowdfunding campaigns. We compare within AI-generated
project titles, measuring repetition and complexity. We compare between the
AI-generated titles and actual observed field data using an extension of
maximum mean discrepancy--a metric derived from the application of kernel mean
embeddings of statistical distributions to high-dimensional machine learning
(large language) embedding vectors--yielding a structured analysis of AI output
novelty. Results suggest that (1) the AI generates unique content even under
increasing task complexity, and at the limits of its computational
capabilities, (2) the generated content has face validity, being consistent
with both inputs to other generative AI and in qualitative comparison to field
data, and (3) exhibits divergence from field data, mitigating concerns relating
to intellectual property rights. We discuss implications for copyright and
trademark law.
\\ ( https://arxiv.org/abs/2404.00017 ,  5369kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00051
Date: Mon, 25 Mar 2024 17:25:40 GMT   (7168kb,D)

Title: Deja vu: Contrastive Historical Modeling with Prefix-tuning for Temporal
  Knowledge Graph Reasoning
Authors: Miao Peng, Ben Liu, Wenjie Xu, Zihao Jiang, Jiahui Zhu, Min Peng
Categories: cs.AI cs.CL cs.LG
Comments: Accepted to NAACL 2024 Findings
\\
  Temporal Knowledge Graph Reasoning (TKGR) is the task of inferring missing
facts for incomplete TKGs in complex scenarios (e.g., transductive and
inductive settings), which has been gaining increasing attention. Recently, to
mitigate dependence on structured connections in TKGs, text-based methods have
been developed to utilize rich linguistic information from entity descriptions.
However, suffering from the enormous parameters and inflexibility of
pre-trained language models, existing text-based methods struggle to balance
the textual knowledge and temporal information with computationally expensive
purpose-built training strategies. To tap the potential of text-based models
for TKGR in various complex scenarios, we propose ChapTER, a Contrastive
historical modeling framework with prefix-tuning for TEmporal Reasoning.
ChapTER feeds history-contextualized text into the pseudo-Siamese encoders to
strike a textual-temporal balance via contrastive estimation between queries
and candidates. By introducing virtual time prefix tokens, it applies a
prefix-based tuning method to facilitate the frozen PLM capable for TKGR tasks
under different settings. We evaluate ChapTER on four transductive and three
few-shot inductive TKGR benchmarks, and experimental results demonstrate that
ChapTER achieves superior performance compared to competitive baselines with
only 0.17% tuned parameters. We conduct thorough analysis to verify the
effectiveness, flexibility and efficiency of ChapTER.
\\ ( https://arxiv.org/abs/2404.00051 ,  7168kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00099
Date: Fri, 29 Mar 2024 18:11:49 GMT   (70kb,D)

Title: Efficient and Sharp Off-Policy Evaluation in Robust Markov Decision
  Processes
Authors: Andrew Bennett, Nathan Kallus, Miruna Oprescu, Wen Sun, Kaiwen Wang
Categories: cs.AI stat.ML
Comments: 40 pages, 1 figure
\\
  We study evaluating a policy under best- and worst-case perturbations to a
Markov decision process (MDP), given transition observations from the original
MDP, whether under the same or different policy. This is an important problem
when there is the possibility of a shift between historical and future
environments, due to e.g. unmeasured confounding, distributional shift, or an
adversarial environment. We propose a perturbation model that can modify
transition kernel densities up to a given multiplicative factor or its
reciprocal, which extends the classic marginal sensitivity model (MSM) for
single time step decision making to infinite-horizon RL. We characterize the
sharp bounds on policy value under this model, that is, the tightest possible
bounds given by the transition observations from the original MDP, and we study
the estimation of these bounds from such transition observations. We develop an
estimator with several appealing guarantees: it is semiparametrically
efficient, and remains so even when certain necessary nuisance functions such
as worst-case Q-functions are estimated at slow nonparametric rates. It is also
asymptotically normal, enabling easy statistical inference using Wald
confidence intervals. In addition, when certain nuisances are estimated
inconsistently we still estimate a valid, albeit possibly not sharp bounds on
the policy value. We validate these properties in numeric simulations. The
combination of accounting for environment shifts from train to test
(robustness), being insensitive to nuisance-function estimation
(orthogonality), and accounting for having only finite samples to learn from
(inference) together leads to credible and reliable policy evaluation.
\\ ( https://arxiv.org/abs/2404.00099 ,  70kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00140
Date: Fri, 29 Mar 2024 20:28:42 GMT   (2585kb,D)

Title: Does Faithfulness Conflict with Plausibility? An Empirical Study in
  Explainable AI across NLP Tasks
Authors: Xiaolei Lu, Jianghong Ma
Categories: cs.AI cs.LG
\\
  Explainability algorithms aimed at interpreting decision-making AI systems
usually consider balancing two critical dimensions: 1) \textit{faithfulness},
where explanations accurately reflect the model's inference process. 2)
\textit{plausibility}, where explanations are consistent with domain experts.
However, the question arises: do faithfulness and plausibility inherently
conflict? In this study, through a comprehensive quantitative comparison
between the explanations from the selected explainability methods and
expert-level interpretations across three NLP tasks: sentiment analysis, intent
detection, and topic labeling, we demonstrate that traditional
perturbation-based methods Shapley value and LIME could attain greater
faithfulness and plausibility. Our findings suggest that rather than optimizing
for one dimension at the expense of the other, we could seek to optimize
explainability algorithms with dual objectives to achieve high levels of
accuracy and user accessibility in their explanations.
\\ ( https://arxiv.org/abs/2404.00140 ,  2585kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00276
Date: Sat, 30 Mar 2024 08:02:16 GMT   (1902kb,D)

Title: Instruction-Driven Game Engines on Large Language Models
Authors: Hongqiu Wu, Yan Wang, Xingyuan Liu, Hai Zhao, Min Zhang
Categories: cs.AI
\\
  The Instruction-Driven Game Engine (IDGE) project aims to democratize game
development by enabling a large language model (LLM) to follow free-form game
rules and autonomously generate game-play processes. The IDGE allows users to
create games by issuing simple natural language instructions, which
significantly lowers the barrier for game development. We approach the learning
process for IDGEs as a Next State Prediction task, wherein the model
autoregressively predicts in-game states given player actions. It is a
challenging task because the computation of in-game states must be precise;
otherwise, slight errors could disrupt the game-play. To address this, we train
the IDGE in a curriculum manner that progressively increases the model's
exposure to complex scenarios.
  Our initial progress lies in developing an IDGE for Poker, a universally
cherished card game. The engine we've designed not only supports a wide range
of poker variants but also allows for high customization of rules through
natural language inputs. Furthermore, it also favors rapid prototyping of new
games from minimal samples, proposing an innovative paradigm in game
development that relies on minimal prompt and data engineering. This work lays
the groundwork for future advancements in instruction-driven game creation,
potentially transforming how games are designed and played.
\\ ( https://arxiv.org/abs/2404.00276 ,  1902kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00320
Date: Sat, 30 Mar 2024 11:13:18 GMT   (1232kb,D)

Title: Advancing Multimodal Data Fusion in Pain Recognition: A Strategy
  Leveraging Statistical Correlation and Human-Centered Perspectives
Authors: Xingrui Gu, Zhixuan Wang, Irisa Jin, Zekun Wu
Categories: cs.AI
Comments: Under reviewed by ACII 2024
\\
  This research tackles the challenge of integrating heterogeneous data for
specific behavior recognition within the domain of Pain Recognition, presenting
a novel methodology that harmonizes statistical correlations with a
human-centered approach. By leveraging a diverse range of deep learning
architectures, we highlight the adaptability and efficacy of our approach in
improving model performance across various complex scenarios. The novelty of
our methodology is the strategic incorporation of statistical relevance weights
and the segmentation of modalities from a human-centric perspective, enhancing
model precision and providing a explainable analysis of multimodal data. This
study surpasses traditional modality fusion techniques by underscoring the role
of data diversity and customized modality segmentation in enhancing pain
behavior analysis. Introducing a framework that matches each modality with an
suited classifier, based on the statistical significance, signals a move
towards customized and accurate multimodal fusion strategies. Our contributions
extend beyond the field of Pain Recognition by delivering new insights into
modality fusion and human-centered computing applications, contributing towards
explainable AI and bolstering patient-centric healthcare interventions. Thus,
we bridge a significant void in the effective and interpretable fusion of
multimodal data, establishing a novel standard for forthcoming inquiries in
pain behavior recognition and allied fields.
\\ ( https://arxiv.org/abs/2404.00320 ,  1232kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00341
Date: Sat, 30 Mar 2024 12:38:47 GMT   (1565kb)

Title: Ontology in Holonic Cooperative Manufacturing: A Solution to Share and
  Exchange the Knowledge
Authors: Ahmed R.Sadik, Bodo Urban
Categories: cs.AI
\\
  Cooperative manufacturing is a new trend in industry, which depends on the
existence of a collaborative robot. A collaborative robot is usually a
light-weight robot which is capable of operating safely with a human co-worker
in a shared work environment. During this cooperation, a vast amount of
information is exchanged between the collaborative robot and the worker. This
information constructs the cooperative manufacturing knowledge, which describes
the production components and environment. In this research, we propose a
holonic control solution, which uses the ontology concept to represent the
cooperative manufacturing knowledge. The holonic control solution is
implemented as an autonomous multi-agent system that exchanges the
manufacturing knowledge based on an ontology model. Ultimately, the research
illustrates and implements the proposed solution over a cooperative assembly
scenario, which involves two workers and one collaborative robot, whom
cooperate together to assemble a customized product.
\\ ( https://arxiv.org/abs/2404.00341 ,  1565kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00560
Date: Sun, 31 Mar 2024 04:44:22 GMT   (502kb,D)

Title: A Theory for Length Generalization in Learning to Reason
Authors: Changnan Xiao and Bing Liu
Categories: cs.AI
\\
  Length generalization (LG) is a challenging problem in learning to reason. It
refers to the phenomenon that when trained on reasoning problems of smaller
lengths or sizes, the resulting model struggles with problems of larger sizes
or lengths. Although LG has been studied by many researchers, the challenge
remains. This paper proposes a theoretical study of LG for problems whose
reasoning processes can be modeled as DAGs (directed acyclic graphs). The paper
first identifies and proves the conditions under which LG can be achieved in
learning to reason. It then designs problem representations based on the theory
to learn to solve challenging reasoning problems like parity, addition, and
multiplication, using a Transformer to achieve perfect LG.
\\ ( https://arxiv.org/abs/2404.00560 ,  502kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00586
Date: Sun, 31 Mar 2024 07:19:29 GMT   (2024kb,D)

Title: RLGNet: Repeating-Local-Global History Network for Temporal Knowledge
  Graph Reasoning
Authors: Ao Lv, Yongzhong Huang, Guige Ouyang, Yue Chen, Haoran Xie
Categories: cs.AI
\\
  Temporal Knowledge Graph (TKG) reasoning is based on historical information
to predict the future. Therefore, parsing and mining historical information is
key to predicting the future. Most existing methods fail to concurrently
address and comprehend historical information from both global and local
perspectives. Neglecting the global view might result in overlooking
macroscopic trends and patterns, while ignoring the local view can lead to
missing critical detailed information. Additionally, some methods do not focus
on learning from high-frequency repeating events, which means they may not
fully grasp frequently occurring historical events. To this end, we propose the
\textbf{R}epetitive-\textbf{L}ocal-\textbf{G}lobal History
\textbf{Net}work(RLGNet). We utilize a global history encoder to capture the
overarching nature of historical information. Subsequently, the local history
encoder provides information related to the query timestamp. Finally, we employ
the repeating history encoder to identify and learn from frequently occurring
historical events. In the evaluation on six benchmark datasets, our approach
generally outperforms existing TKG reasoning models in multi-step and
single-step reasoning tasks.
\\ ( https://arxiv.org/abs/2404.00586 ,  2024kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00756
Date: Sun, 31 Mar 2024 17:54:22 GMT   (4343kb,D)

Title: Recover: A Neuro-Symbolic Framework for Failure Detection and Recovery
Authors: Cristina Cornelio and Mohammed Diab
Categories: cs.AI cs.LG cs.LO cs.RO
\\
  Recognizing failures during task execution and implementing recovery
procedures is challenging in robotics. Traditional approaches rely on the
availability of extensive data or a tight set of constraints, while more recent
approaches leverage large language models (LLMs) to verify task steps and
replan accordingly. However, these methods often operate offline, necessitating
scene resets and incurring in high costs. This paper introduces Recover, a
neuro-symbolic framework for online failure identification and recovery. By
integrating ontologies, logical rules, and LLM-based planners, Recover exploits
symbolic information to enhance the ability of LLMs to generate recovery plans
and also to decrease the associated costs. In order to demonstrate the
capabilities of our method in a simulated kitchen environment, we introduce
OntoThor, an ontology describing the AI2Thor simulator setting. Empirical
evaluation shows that OntoThor's logical rules accurately detect all failures
in the analyzed tasks, and that Recover considerably outperforms, for both
failure detection and recovery, a baseline method reliant solely on LLMs.
\\ ( https://arxiv.org/abs/2404.00756 ,  4343kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00886
Date: Mon, 1 Apr 2024 03:27:46 GMT   (5777kb,D)

Title: MTLight: Efficient Multi-Task Reinforcement Learning for Traffic Signal
  Control
Authors: Liwen Zhu, Peixi Peng, Zongqing Lu, Yonghong Tian
Categories: cs.AI
\\
  Traffic signal control has a great impact on alleviating traffic congestion
in modern cities. Deep reinforcement learning (RL) has been widely used for
this task in recent years, demonstrating promising performance but also facing
many challenges such as limited performances and sample inefficiency. To handle
these challenges, MTLight is proposed to enhance the agent observation with a
latent state, which is learned from numerous traffic indicators. Meanwhile,
multiple auxiliary and supervisory tasks are constructed to learn the latent
state, and two types of embedding latent features, the task-specific feature
and task-shared feature, are used to make the latent state more abundant.
Extensive experiments conducted on CityFlow demonstrate that MTLight has
leading convergence speed and asymptotic performance. We further simulate under
peak-hour pattern in all scenarios with increasing control difficulty and the
results indicate that MTLight is highly adaptable.
\\ ( https://arxiv.org/abs/2404.00886 ,  5777kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01266
Date: Mon, 1 Apr 2024 17:43:27 GMT   (3038kb,D)

Title: IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic
  Representations
Authors: Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani
  Yogatama, Robin Jia, Willie Neiswanger
Categories: cs.AI cs.CL
\\
  Current foundation models exhibit impressive capabilities when prompted
either with text only or with both image and text inputs. But do their
capabilities change depending on the input modality? In this work, we propose
$\textbf{IsoBench}$, a benchmark dataset containing problems from four major
areas: math, science, algorithms, and games. Each example is presented with
multiple $\textbf{isomorphic representations}$ of inputs, such as visual,
textual, and mathematical presentations. IsoBench provides fine-grained
feedback to diagnose performance gaps caused by the form of the representation.
Across various foundation models, we observe that on the same problem, models
have a consistent preference towards textual representations. Most prominently,
when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points
worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7
points worse and Gemini Pro is 14.9 points worse. Finally, we present two
prompting techniques, $\textit{IsoCombination}$ and $\textit{IsoScratchPad}$,
which improve model performance by considering combinations of, and
translations between, different input representations.
\\ ( https://arxiv.org/abs/2404.01266 ,  3038kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00124
Date: Fri, 29 Mar 2024 19:27:04 GMT   (2774kb,D)

Title: Where Are You From? Let Me Guess! Subdialect Recognition of Speeches in
  Sorani Kurdish
Authors: Sana Isam and Hossein Hassani
Categories: cs.CL cs.SD eess.AS
Comments: 30 pages, 25 figures, 6 tables
\\
  Classifying Sorani Kurdish subdialects poses a challenge due to the need for
publicly available datasets or reliable resources like social media or websites
for data collection. We conducted field visits to various cities and villages
to address this issue, connecting with native speakers from different age
groups, genders, academic backgrounds, and professions. We recorded their
voices while engaging in conversations covering diverse topics such as
lifestyle, background history, hobbies, interests, vacations, and life lessons.
The target area of the research was the Kurdistan Region of Iraq. As a result,
we accumulated 29 hours, 16 minutes, and 40 seconds of audio recordings from
107 interviews, constituting an unbalanced dataset encompassing six
subdialects. Subsequently, we adapted three deep learning models: ANN, CNN, and
RNN-LSTM. We explored various configurations, including different track
durations, dataset splitting, and imbalanced dataset handling techniques such
as oversampling and undersampling. Two hundred and twenty-five(225) experiments
were conducted, and the outcomes were evaluated. The results indicated that the
RNN-LSTM outperforms the other methods by achieving an accuracy of 96%. CNN
achieved an accuracy of 93%, and ANN 75%. All three models demonstrated
improved performance when applied to balanced datasets, primarily when we
followed the oversampling approach. Future studies can explore additional
future research directions to include other Kurdish dialects.
\\ ( https://arxiv.org/abs/2404.00124 ,  2774kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00141
Date: Fri, 29 Mar 2024 20:29:12 GMT   (121kb,D)

Title: Classifying Conspiratorial Narratives At Scale: False Alarms and
  Erroneous Connections
Authors: Ahmad Diab, Rr. Nefriana, and Yu-Ru Lin
Categories: cs.CL cs.SI
Comments: 12 pages, 6 tables, 1 figure, conference ICWSM_24
\\
  Online discussions frequently involve conspiracy theories, which can
contribute to the proliferation of belief in them. However, not all discussions
surrounding conspiracy theories promote them, as some are intended to debunk
them. Existing research has relied on simple proxies or focused on a
constrained set of signals to identify conspiracy theories, which limits our
understanding of conspiratorial discussions across different topics and online
communities. This work establishes a general scheme for classifying discussions
related to conspiracy theories based on authors' perspectives on the conspiracy
belief, which can be expressed explicitly through narrative elements, such as
the agent, action, or objective, or implicitly through references to known
theories, such as chemtrails or the New World Order. We leverage human-labeled
ground truth to train a BERT-based model for classifying online CTs, which we
then compared to the Generative Pre-trained Transformer machine (GPT) for
detecting online conspiratorial content. Despite GPT's known strengths in its
expressiveness and contextual understanding, our study revealed significant
flaws in its logical reasoning, while also demonstrating comparable strengths
from our classifiers. We present the first large-scale classification study
using posts from the most active conspiracy-related Reddit forums and find that
only one-third of the posts are classified as positive. This research sheds
light on the potential applications of large language models in tasks demanding
nuanced contextual comprehension.
\\ ( https://arxiv.org/abs/2404.00141 ,  121kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00152
Date: Fri, 29 Mar 2024 20:59:27 GMT   (627kb,D)

Title: On-the-fly Definition Augmentation of LLMs for Biomedical NER
Authors: Monica Munnangi, Sergey Feldman, Byron C Wallace, Silvio Amir, Tom
  Hope, Aakanksha Naik
Categories: cs.CL
Comments: To appear at NAACL 2024
\\
  Despite their general capabilities, LLMs still struggle on biomedical NER
tasks, which are difficult due to the presence of specialized terminology and
lack of training data. In this work we set out to improve LLM performance on
biomedical NER in limited data settings via a new knowledge augmentation
approach which incorporates definitions of relevant concepts on-the-fly. During
this process, to provide a test bed for knowledge augmentation, we perform a
comprehensive exploration of prompting strategies. Our experiments show that
definition augmentation is useful for both open source and closed LLMs. For
example, it leads to a relative improvement of 15\% (on average) in GPT-4
performance (F1) across all (six) of our test datasets. We conduct extensive
ablations and analyses to demonstrate that our performance improvements stem
from adding relevant definitional knowledge. We find that careful prompting
strategies also improve LLM performance, allowing them to outperform fine-tuned
language models in few-shot settings. To facilitate future research in this
direction, we release our code at https://github.com/allenai/beacon.
\\ ( https://arxiv.org/abs/2404.00152 ,  627kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00165
Date: Fri, 29 Mar 2024 21:44:24 GMT   (747kb)

Title: Individual Text Corpora Predict Openness, Interests, Knowledge and Level
  of Education
Authors: Markus J. Hofmann, Markus T. Jansen, Christoph Wigbels, Benny
  Briesemeister, Arthur M. Jacobs
Categories: cs.CL cs.LG
Comments: Proceedings of the 8th workshop on Cognitive Aspects of the Lexicon
  (CogALex-VIII), LREC/Coling 2024
\\
  Here we examine whether the personality dimension of openness to experience
can be predicted from the individual google search history. By web scraping,
individual text corpora (ICs) were generated from 214 participants with a mean
number of 5 million word tokens. We trained word2vec models and used the
similarities of each IC to label words, which were derived from a lexical
approach of personality. These IC-label-word similarities were utilized as
predictive features in neural models. For training and validation, we relied on
179 participants and held out a test sample of 35 participants. A grid search
with varying number of predictive features, hidden units and boost factor was
performed. As model selection criterion, we used R2 in the validation samples
penalized by the absolute R2 difference between training and validation. The
selected neural model explained 35% of the openness variance in the test
sample, while an ensemble model with the same architecture often provided
slightly more stable predictions for intellectual interests, knowledge in
humanities and level of education. Finally, a learning curve analysis suggested
that around 500 training participants are required for generalizable
predictions. We discuss ICs as a complement or replacement of survey-based
psychodiagnostics.
\\ ( https://arxiv.org/abs/2404.00165 ,  747kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00176
Date: Fri, 29 Mar 2024 22:11:54 GMT   (1506kb,D)

Title: The LSCD Benchmark: a Testbed for Diachronic Word Meaning Tasks
Authors: Dominik Schlechtweg, Shafqat Mumtaz Virk, Nikolay Arefyev
Categories: cs.CL
\\
  Lexical Semantic Change Detection (LSCD) is a complex, lemma-level task,
which is usually operationalized based on two subsequently applied usage-level
tasks: First, Word-in-Context (WiC) labels are derived for pairs of usages.
Then, these labels are represented in a graph on which Word Sense Induction
(WSI) is applied to derive sense clusters. Finally, LSCD labels are derived by
comparing sense clusters over time. This modularity is reflected in most LSCD
datasets and models. It also leads to a large heterogeneity in modeling options
and task definitions, which is exacerbated by a variety of dataset versions,
preprocessing options and evaluation metrics. This heterogeneity makes it
difficult to evaluate models under comparable conditions, to choose optimal
model combinations or to reproduce results. Hence, we provide a benchmark
repository standardizing LSCD evaluation. Through transparent implementation
results become easily reproducible and by standardization different components
can be freely combined. The repository reflects the task's modularity by
allowing model evaluation for WiC, WSI and LSCD. This allows for careful
evaluation of increasingly complex model components providing new ways of model
optimization.
\\ ( https://arxiv.org/abs/2404.00176 ,  1506kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00184
Date: Fri, 29 Mar 2024 22:41:40 GMT   (1883kb,D)

Title: Word Ladders: A Mobile Application for Semantic Data Collection
Authors: Marianna Marcella Bolognesi, Claudia Collacciani, Andrea Ferrari,
  Francesca Genovese, Tommaso Lamarra, Adele Loia, Giulia Rambelli, Andrea
  Amelio Ravelli, Caterina Villani
Categories: cs.CL
\\
  Word Ladders is a free mobile application for Android and iOS, developed for
collecting linguistic data, specifically lists of words related to each other
through semantic relations of categorical inclusion, within the Abstraction
project (ERC-2021-STG-101039777). We hereby provide an overview of Word
Ladders, explaining its game logic, motivation and expected results and
applications to nlp tasks as well as to the investigation of cognitive
scientific open questions
\\ ( https://arxiv.org/abs/2404.00184 ,  1883kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00188
Date: Fri, 29 Mar 2024 22:59:34 GMT   (1192kb,D)

Title: DataAgent: Evaluating Large Language Models' Ability to Answer
  Zero-Shot, Natural Language Queries
Authors: Manit Mishra, Abderrahman Braham, Charles Marsom, Bryan Chung, Gavin
  Griffin, Dakshesh Sidnerlikar, Chatanya Sarin, Arjun Rajaram
Categories: cs.CL cs.AI
Comments: 5 pages, Submitted to International Conference on AI in Cybersecurity
DOI: 10.1109/ICAIC60265.2024.10433803
\\
  Conventional processes for analyzing datasets and extracting meaningful
information are often time-consuming and laborious. Previous work has
identified manual, repetitive coding and data collection as major obstacles
that hinder data scientists from undertaking more nuanced labor and high-level
projects. To combat this, we evaluated OpenAI's GPT-3.5 as a "Language Data
Scientist" (LDS) that can extrapolate key findings, including correlations and
basic information, from a given dataset. The model was tested on a diverse set
of benchmark datasets to evaluate its performance across multiple standards,
including data science code-generation based tasks involving libraries such as
NumPy, Pandas, Scikit-Learn, and TensorFlow, and was broadly successful in
correctly answering a given data science query related to the benchmark
dataset. The LDS used various novel prompt engineering techniques to
effectively answer a given question, including Chain-of-Thought reinforcement
and SayCan prompt engineering. Our findings demonstrate great potential for
leveraging Large Language Models for low-level, zero-shot data analysis.
\\ ( https://arxiv.org/abs/2404.00188 ,  1192kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00189
Date: Fri, 29 Mar 2024 23:04:04 GMT   (2795kb,D)

Title: GPTA: Generative Prompt Tuning Assistant for Synergistic Downstream
  Neural Network Enhancement with LLMs
Authors: Xiao Liu and Jiawei Zhang
Categories: cs.CL
Comments: Work in Progress
\\
  This study introduces GPTA, a Large Language Model assistance training
framework, that enhances the training of downstream task models via prefix
prompt. By minimizing data exposure to LLM, the framework addresses the
security and legal challenges of applying LLM in downstream task model
training. GPTA utilizes a new synergistic training approach, optimizing the
downstream models with parameter gradients and LLMs with the novel ``dialogue
gradient''. The framework not only demonstrates significant improvements in
model performance across six NLP benchmark datasets, but also reduces
overfitting in low-resource scenarios effectively. The detailed analyses
further validate that our pioneer framework provides a cost-efficient and
adaptive method for downstream task model training with LLM support.
\\ ( https://arxiv.org/abs/2404.00189 ,  2795kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00205
Date: Sat, 30 Mar 2024 00:53:53 GMT   (284kb,D)

Title: Conceptual and Unbiased Reasoning in Language Models
Authors: Ben Zhou, Hongming Zhang, Sihao Chen, Dian Yu, Hongwei Wang, Baolin
  Peng, Dan Roth, Dong Yu
Categories: cs.CL
Comments: Preprint under review
\\
  Conceptual reasoning, the ability to reason in abstract and high-level
perspectives, is key to generalization in human cognition. However, limited
study has been done on large language models' capability to perform conceptual
reasoning. In this work, we bridge this gap and propose a novel
conceptualization framework that forces models to perform conceptual reasoning
on abstract questions and generate solutions in a verifiable symbolic space.
Using this framework as an analytical tool, we show that existing large
language models fall short on conceptual reasoning, dropping 9% to 28% on
various benchmarks compared to direct inference methods. We then discuss how
models can improve since high-level abstract reasoning is key to unbiased and
generalizable decision-making. We propose two techniques to add trustworthy
induction signals by generating familiar questions with similar underlying
reasoning paths and asking models to perform self-refinement. Experiments show
that our proposed techniques improve models' conceptual reasoning performance
by 8% to 11%, achieving a more robust reasoning system that relies less on
inductive biases.
\\ ( https://arxiv.org/abs/2404.00205 ,  284kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00207
Date: Sat, 30 Mar 2024 01:08:25 GMT   (1341kb,D)

Title: Causal Inference for Human-Language Model Collaboration
Authors: Bohan Zhang, Yixin Wang, Paramveer S. Dhillon
Categories: cs.CL cs.AI cs.LG
Comments: 9 pages (Accepted for publication at NAACL 2024 (Main Conference))
\\
  In this paper, we examine the collaborative dynamics between humans and
language models (LMs), where the interactions typically involve LMs proposing
text segments and humans editing or responding to these proposals. Productive
engagement with LMs in such scenarios necessitates that humans discern
effective text-based interaction strategies, such as editing and response
styles, from historical human-LM interactions. This objective is inherently
causal, driven by the counterfactual `what-if' question: how would the outcome
of collaboration change if humans employed a different text editing/refinement
strategy? A key challenge in answering this causal inference question is
formulating an appropriate causal estimand: the conventional average treatment
effect (ATE) estimand is inapplicable to text-based treatments due to their
high dimensionality. To address this concern, we introduce a new causal
estimand -- Incremental Stylistic Effect (ISE) -- which characterizes the
average impact of infinitesimally shifting a text towards a specific style,
such as increasing formality. We establish the conditions for the
non-parametric identification of ISE. Building on this, we develop
CausalCollab, an algorithm designed to estimate the ISE of various interaction
strategies in dynamic human-LM collaborations. Our empirical investigations
across three distinct human-LM collaboration scenarios reveal that CausalCollab
effectively reduces confounding and significantly improves counterfactual
estimation over a set of competitive baselines.
\\ ( https://arxiv.org/abs/2404.00207 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00209
Date: Sat, 30 Mar 2024 01:16:37 GMT   (10642kb,D)

Title: EventGround: Narrative Reasoning by Grounding to Eventuality-centric
  Knowledge Graphs
Authors: Cheng Jiayang, Lin Qiu, Chunkit Chan, Xin Liu, Yangqiu Song, Zheng
  Zhang
Categories: cs.CL
\\
  Narrative reasoning relies on the understanding of eventualities in story
contexts, which requires a wealth of background world knowledge. To help
machines leverage such knowledge, existing solutions can be categorized into
two groups. Some focus on implicitly modeling eventuality knowledge by
pretraining language models (LMs) with eventuality-aware objectives. However,
this approach breaks down knowledge structures and lacks interpretability.
Others explicitly collect world knowledge of eventualities into structured
eventuality-centric knowledge graphs (KGs). However, existing research on
leveraging these knowledge sources for free-texts is limited. In this work, we
propose an initial comprehensive framework called EventGround, which aims to
tackle the problem of grounding free-texts to eventuality-centric KGs for
contextualized narrative reasoning. We identify two critical problems in this
direction: the event representation and sparsity problems. We provide simple
yet effective parsing and partial information extraction methods to tackle
these problems. Experimental results demonstrate that our approach consistently
outperforms baseline models when combined with graph neural network (GNN) or
large language model (LLM) based graph reasoning models. Our framework,
incorporating grounded knowledge, achieves state-of-the-art performance while
providing interpretable evidence.
\\ ( https://arxiv.org/abs/2404.00209 ,  10642kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00211
Date: Sat, 30 Mar 2024 01:26:05 GMT   (938kb,D)

Title: Multi-Conditional Ranking with Large Language Models
Authors: Pouya Pezeshkpour, Estevam Hruschka
Categories: cs.CL cs.LG
\\
  Utilizing large language models (LLMs) to rank a set of items has become a
common approach in recommendation and retrieval systems. Typically, these
systems focus on ordering a substantial number of documents in a monotonic
order based on a given query. However, real-world scenarios often present a
different challenge: ranking a comparatively smaller set of items, but
according to a variety of diverse and occasionally conflicting conditions. In
this paper, we define and explore the task of multi-conditional ranking by
introducing MCRank, a benchmark tailored for assessing multi-conditional
ranking across various item types and conditions. Our analysis of LLMs using
MCRank indicates a significant decrease in performance as the number and
complexity of items and conditions grow. To overcome this limitation, we
propose a novel decomposed reasoning method, consisting of EXtracting and
Sorting the conditions, and then Iterativly Ranking the items (EXSIR). Our
extensive experiments show that this decomposed reasoning method enhances LLMs'
performance significantly, achieving up to a 12% improvement over existing
LLMs. We also provide a detailed analysis of LLMs performance across various
condition categories, and examine the effectiveness of decomposition step.
Furthermore, we compare our method with existing approaches such as
Chain-of-Thought and an encoder-type ranking model, demonstrating the
superiority of our approach and complexity of MCR task. We released our dataset
and code.
\\ ( https://arxiv.org/abs/2404.00211 ,  938kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00213
Date: Sat, 30 Mar 2024 01:56:07 GMT   (365kb,D)

Title: Injecting New Knowledge into Large Language Models via Supervised
  Fine-Tuning
Authors: Nick Mecklenburg, Yiyou Lin, Xiaoxiao Li, Daniel Holstein, Leonardo
  Nunes, Sara Malvar, Bruno Silva, Ranveer Chandra, Vijay Aski, Pavan Kumar
  Reddy Yannam, Tolga Aktas
Categories: cs.CL
\\
  In recent years, Large Language Models (LLMs) have shown remarkable
performance in generating human-like text, proving to be a valuable asset
across various applications. However, adapting these models to incorporate new,
out-of-domain knowledge remains a challenge, particularly for facts and events
that occur after the model's knowledge cutoff date. This paper investigates the
effectiveness of Supervised Fine-Tuning (SFT) as a method for knowledge
injection in LLMs, specifically focusing on the domain of recent sporting
events. We compare different dataset generation strategies -- token-based and
fact-based scaling -- to create training data that helps the model learn new
information. Our experiments on GPT-4 demonstrate that while token-based
scaling can lead to improvements in Q&A accuracy, it may not provide uniform
coverage of new knowledge. Fact-based scaling, on the other hand, offers a more
systematic approach to ensure even coverage across all facts. We present a
novel dataset generation process that leads to more effective knowledge
ingestion through SFT, and our results show considerable performance
improvements in Q&A tasks related to out-of-domain knowledge. This study
contributes to the understanding of domain adaptation for LLMs and highlights
the potential of SFT in enhancing the factuality of LLM responses in specific
knowledge domains.
\\ ( https://arxiv.org/abs/2404.00213 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00216
Date: Sat, 30 Mar 2024 02:08:28 GMT   (706kb,D)

Title: Is Factuality Decoding a Free Lunch for LLMs? Evaluation on Knowledge
  Editing Benchmark
Authors: Baolong Bi, Shenghua Liu, Yiwei Wang, Lingrui Mei and Xueqi Cheng
Categories: cs.CL cs.AI
\\
  The rapid development of large language models (LLMs) enables them to convey
factual knowledge in a more human-like fashion. Extensive efforts have been
made to reduce factual hallucinations by modifying LLMs with factuality
decoding. However, they also pose risks of hindering knowledge updates, as they
make models overly confident in known facts. In this work, we first revisite
the current factuality decoding methods and verified their effectiveness in
enhancing factual accuracy. Subsequently, we conduct further evaluation of
several strong factuality decoding methods on the knowledge editing benchmark.
All these decoding methods significantly diminish the performance of llama2
models compared to their original decoding, with the largest decrease being a
staggering 81.3\%. This further indicates that the current existing decoding
methods still cannot perfectly address the factual hallucinations, as they
overlook the importance of preserving the flexibility for knowledge editing.
Therefore, our work suggests that research into factual alignment should
simultaneously focus on the effectiveness of knowledge editing.
\\ ( https://arxiv.org/abs/2404.00216 ,  706kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00217
Date: Sat, 30 Mar 2024 02:22:57 GMT   (1115kb,D)

Title: Rationale-based Opinion Summarization
Authors: Haoyuan Li, Snigdha Chaturvedi
Categories: cs.CL
\\
  Opinion summarization aims to generate concise summaries that present popular
opinions of a large group of reviews. However, these summaries can be too
generic and lack supporting details. To address these issues, we propose a new
paradigm for summarizing reviews, rationale-based opinion summarization.
Rationale-based opinion summaries output the representative opinions as well as
one or more corresponding rationales. To extract good rationales, we define
four desirable properties: relatedness, specificity, popularity, and diversity
and present a Gibbs-sampling-based method to extract rationales. Overall, we
propose RATION, an unsupervised extractive system that has two components: an
Opinion Extractor (to extract representative opinions) and Rationales Extractor
(to extract corresponding rationales). We conduct automatic and human
evaluations to show that rationales extracted by RATION have the proposed
properties and its summaries are more useful than conventional summaries. The
implementation of our work is available at
https://github.com/leehaoyuan/RATION.
\\ ( https://arxiv.org/abs/2404.00217 ,  1115kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00224
Date: Sat, 30 Mar 2024 02:52:14 GMT   (841kb)

Title: Classification and Clustering of Sentence-Level Embeddings of Scientific
  Articles Generated by Contrastive Learning
Authors: Gustavo Bartz Guedes, Ana Estela Antunes da Silva
Categories: cs.CL
Journal-ref: Computer Science & Information Technology (CS & IT), pp. 293-305,
  2023
DOI: 10.5121/csit.2023.131923
\\
  Scientific articles are long text documents organized into sections, each
describing aspects of the research. Analyzing scientific production has become
progressively challenging due to the increase in the number of available
articles. Within this scenario, our approach consisted of fine-tuning
transformer language models to generate sentence-level embeddings from
scientific articles, considering the following labels: background, objective,
methods, results, and conclusion. We trained our models on three datasets with
contrastive learning. Two datasets are from the article's abstracts in the
computer science and medical domains. Also, we introduce PMC-Sents-FULL, a
novel dataset of sentences extracted from the full texts of medical articles.
We compare the fine-tuned and baseline models in clustering and classification
tasks to evaluate our approach. On average, clustering agreement measures
values were five times higher. For the classification measures, in the
best-case scenario, we had an average improvement in F1-micro of 30.73\%.
Results show that fine-tuning sentence transformers with contrastive learning
and using the generated embeddings in downstream tasks is a feasible approach
to sentence classification in scientific articles. Our experiment codes are
available on GitHub.
\\ ( https://arxiv.org/abs/2404.00224 ,  841kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00242
Date: Sat, 30 Mar 2024 04:34:54 GMT   (5798kb,D)

Title: DeFT: Flash Tree-attention with IO-Awareness for Efficient
  Tree-search-based LLM Inference
Authors: Jinwei Yao, Kaiqi Chen, Kexun Zhang, Jiaxuan You, Binhang Yuan, Zeke
  Wang, Tao Lin
Categories: cs.CL cs.AI
\\
  Decoding using tree search can greatly enhance the inference quality for
transformer-based Large Language Models (LLMs). Depending on the guidance
signal, it searches for the best path from root to leaf in the tree by forming
LLM outputs to improve controllability, reasoning ability, alignment, et
cetera. However, current tree decoding strategies and their inference systems
do not suit each other well due to redundancy in computation, memory
footprints, and memory access, resulting in inefficient inference. To address
this issue, we propose DeFT, an IO-aware tree attention algorithm that
maintains memory-efficient attention calculation with low memory footprints in
two stages: (1) QKV Preparation: we propose a KV-Guided Tree Split strategy to
group QKV wisely for high utilization of GPUs and reduction of memory
reads/writes for the KV cache between GPU global memory and on-chip shared
memory as much as possible; (2) Attention Calculation: we calculate partial
attention of each QKV groups in a fused kernel then apply a Tree-topology-aware
Global Reduction strategy to get final attention. Thanks to a reduction in KV
cache IO by 3.6-4.5$\times$, along with an additional reduction in IO for
$\mathbf{Q} \mathbf{K}^\top$ and Softmax equivalent to 25% of the total KV
cache IO, DeFT can achieve a speedup of 1.7-2.4$\times$ in end-to-end latency
across two practical reasoning tasks over the SOTA attention algorithms.
\\ ( https://arxiv.org/abs/2404.00242 ,  5798kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00246
Date: Sat, 30 Mar 2024 04:48:38 GMT   (5996kb,D)

Title: Your Co-Workers Matter: Evaluating Collaborative Capabilities of
  Language Models in Blocks World
Authors: Guande Wu, Chen Zhao, Claudio Silva, He He
Categories: cs.CL cs.AI cs.HC
\\
  Language agents that interact with the world on their own have great
potential for automating digital tasks. While large language model (LLM) agents
have made progress in understanding and executing tasks such as textual games
and webpage control, many real-world tasks also require collaboration with
humans or other LLMs in equal roles, which involves intent understanding, task
coordination, and communication. To test LLM's ability to collaborate, we
design a blocks-world environment, where two agents, each having unique goals
and skills, build a target structure together. To complete the goals, they can
act in the world and communicate in natural language. Under this environment,
we design increasingly challenging settings to evaluate different collaboration
perspectives, from independent to more complex, dependent tasks. We further
adopt chain-of-thought prompts that include intermediate reasoning steps to
model the partner's state and identify and correct execution errors. Both
human-machine and machine-machine experiments show that LLM agents have strong
grounding capacities, and our approach significantly improves the evaluation
metric.
\\ ( https://arxiv.org/abs/2404.00246 ,  5996kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00264
Date: Sat, 30 Mar 2024 06:40:54 GMT   (7861kb,D)

Title: DiLM: Distilling Dataset into Language Model for Text-level Dataset
  Distillation
Authors: Aru Maekawa, Satoshi Kosugi, Kotaro Funakoshi, Manabu Okumura
Categories: cs.CL cs.LG
Comments: Accepted by Findings of NAACL 2024
\\
  Dataset distillation aims to compress a training dataset by creating a small
number of informative synthetic samples such that neural networks trained on
them perform as well as those trained on the original training dataset. Current
text dataset distillation methods create each synthetic sample as a sequence of
word embeddings instead of a text to apply gradient-based optimization;
however, such embedding-level distilled datasets cannot be used for training
other models whose word embedding weights are different from the model used for
distillation. To address this issue, we propose a novel text dataset
distillation approach, called Distilling dataset into Language Model (DiLM),
which trains a language model to generate informative synthetic training
samples as text data, instead of directly optimizing synthetic samples. We
evaluated DiLM on various text classification datasets and showed that
distilled synthetic datasets from DiLM outperform those from current coreset
selection methods. DiLM achieved remarkable generalization performance in
training different types of models and in-context learning of large language
models. Our code will be available at https://github.com/arumaekawa/DiLM.
\\ ( https://arxiv.org/abs/2404.00264 ,  7861kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00267
Date: Sat, 30 Mar 2024 06:49:17 GMT   (3869kb,D)

Title: Secret Keepers: The Impact of LLMs on Linguistic Markers of Personal
  Traits
Authors: Zhivar Sourati, Meltem Ozcan, Colin McDaniel, Alireza Ziabari, Nuan
  Wen, Ala Tak, Fred Morstatter, Morteza Dehghani
Categories: cs.CL
\\
  Prior research has established associations between individuals' language
usage and their personal traits; our linguistic patterns reveal information
about our personalities, emotional states, and beliefs. However, with the
increasing adoption of Large Language Models (LLMs) as writing assistants in
everyday writing, a critical question emerges: are authors' linguistic patterns
still predictive of their personal traits when LLMs are involved in the writing
process? We investigate the impact of LLMs on the linguistic markers of
demographic and psychological traits, specifically examining three LLMs -
GPT3.5, Llama 2, and Gemini - across six different traits: gender, age,
political affiliation, personality, empathy, and morality. Our findings
indicate that although the use of LLMs slightly reduces the predictive power of
linguistic patterns over authors' personal traits, the significant changes are
infrequent, and the use of LLMs does not fully diminish the predictive power of
authors' linguistic patterns over their personal traits. We also note that some
theoretically established lexical-based linguistic markers lose their
reliability as predictors when LLMs are used in the writing process. Our
findings have important implications for the study of linguistic markers of
personal traits in the age of LLMs.
\\ ( https://arxiv.org/abs/2404.00267 ,  3869kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00284
Date: Sat, 30 Mar 2024 08:35:08 GMT   (849kb,D)

Title: A Likelihood Ratio Test of Genetic Relationship among Languages
Authors: V.S.D.S.Mahesh Akavarapu and Arnab Bhattacharya
Categories: cs.CL
Comments: Accepted at NAACL-2024 (Main Conference)
ACM-class: I.2.7
\\
  Lexical resemblances among a group of languages indicate that the languages
could be genetically related, i.e., they could have descended from a common
ancestral language. However, such resemblances can arise by chance and, hence,
need not always imply an underlying genetic relationship. Many tests of
significance based on permutation of wordlists and word similarity measures
appeared in the past to determine the statistical significance of such
relationships. We demonstrate that although existing tests may work well for
bilateral comparisons, i.e., on pairs of languages, they are either infeasible
by design or are prone to yield false positives when applied to groups of
languages or language families. To this end, inspired by molecular
phylogenetics, we propose a likelihood ratio test to determine if given
languages are related based on the proportion of invariant character sites in
the aligned wordlists applied during tree inference. Further, we evaluate some
language families and show that the proposed test solves the problem of false
positives. Finally, we demonstrate that the test supports the existence of
macro language families such as Nostratic and Macro-Mayan.
\\ ( https://arxiv.org/abs/2404.00284 ,  849kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00297
Date: Sat, 30 Mar 2024 09:20:43 GMT   (4161kb,D)

Title: TRABSA: Interpretable Sentiment Analysis of Tweets using Attention-based
  BiLSTM and Twitter-RoBERTa
Authors: Md Abrar Jahin, Md Sakib Hossain Shovon, M. F. Mridha
Categories: cs.CL cs.LG
\\
  Sentiment analysis is crucial for understanding public opinion and consumer
behavior. Existing models face challenges with linguistic diversity,
generalizability, and explainability. We propose TRABSA, a hybrid framework
integrating transformer-based architectures, attention mechanisms, and BiLSTM
networks to address this. Leveraging RoBERTa-trained on 124M tweets, we bridge
gaps in sentiment analysis benchmarks, ensuring state-of-the-art accuracy.
Augmenting datasets with tweets from 32 countries and US states, we compare six
word-embedding techniques and three lexicon-based labeling techniques,
selecting the best for optimal sentiment analysis. TRABSA outperforms
traditional ML and deep learning models with 94% accuracy and significant
precision, recall, and F1-score gains. Evaluation across diverse datasets
demonstrates consistent superiority and generalizability. SHAP and LIME
analyses enhance interpretability, improving confidence in predictions. Our
study facilitates pandemic resource management, aiding resource planning,
policy formation, and vaccination tactics.
\\ ( https://arxiv.org/abs/2404.00297 ,  4161kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00303
Date: Sat, 30 Mar 2024 09:55:58 GMT   (2539kb)

Title: A Comprehensive Study on NLP Data Augmentation for Hate Speech
  Detection: Legacy Methods, BERT, and LLMs
Authors: Md Saroar Jahan, Mourad Oussalah, Djamila Romaissa Beddia, Jhuma kabir
  Mim and Nabil Arhab
Categories: cs.CL
Comments: 31 page, Table 18, Figure 14
\\
  The surge of interest in data augmentation within the realm of NLP has been
driven by the need to address challenges posed by hate speech domains, the
dynamic nature of social media vocabulary, and the demands for large-scale
neural networks requiring extensive training data. However, the prevalent use
of lexical substitution in data augmentation has raised concerns, as it may
inadvertently alter the intended meaning, thereby impacting the efficacy of
supervised machine learning models. In pursuit of suitable data augmentation
methods, this study explores both established legacy approaches and
contemporary practices such as Large Language Models (LLM), including GPT in
Hate Speech detection. Additionally, we propose an optimized utilization of
BERT-based encoder models with contextual cosine similarity filtration,
exposing significant limitations in prior synonym substitution methods. Our
comparative analysis encompasses five popular augmentation techniques: WordNet
and Fast-Text synonym replacement, Back-translation, BERT-mask contextual
augmentation, and LLM. Our analysis across five benchmarked datasets revealed
that while traditional methods like back-translation show low label alteration
rates (0.3-1.5%), and BERT-based contextual synonym replacement offers sentence
diversity but at the cost of higher label alteration rates (over 6%). Our
proposed BERT-based contextual cosine similarity filtration markedly reduced
label alteration to just 0.05%, demonstrating its efficacy in 0.7% higher F1
performance. However, augmenting data with GPT-3 not only avoided overfitting
with up to sevenfold data increase but also improved embedding space coverage
by 15% and classification F1 score by 1.4% over traditional methods, and by
0.8% over our method.
\\ ( https://arxiv.org/abs/2404.00303 ,  2539kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00344
Date: Sat, 30 Mar 2024 12:48:31 GMT   (44kb,D)

Title: Can LLMs Master Math? Investigating Large Language Models on Math Stack
  Exchange
Authors: Ankit Satpute and Noah Giessing and Andre Greiner-Petter and Moritz
  Schubotz and Olaf Teschke and Akiko Aizawa and Bela Gipp
Categories: cs.CL cs.AI cs.IR
Comments: Accepted for publication at the 47th International ACM SIGIR
  Conference on Research and Development in Information Retrieval (SIGIR) July
  14--18, 2024, Washington D.C.,USA
\\
  Large Language Models (LLMs) have demonstrated exceptional capabilities in
various natural language tasks, often achieving performances that surpass those
of humans. Despite these advancements, the domain of mathematics presents a
distinctive challenge, primarily due to its specialized structure and the
precision it demands. In this study, we adopted a two-step approach for
investigating the proficiency of LLMs in answering mathematical questions.
First, we employ the most effective LLMs, as identified by their performance on
math question-answer benchmarks, to generate answers to 78 questions from the
Math Stack Exchange (MSE). Second, a case analysis is conducted on the LLM that
showed the highest performance, focusing on the quality and accuracy of its
answers through manual evaluation. We found that GPT-4 performs best (nDCG of
0.48 and P@10 of 0.37) amongst existing LLMs fine-tuned for answering
mathematics questions and outperforms the current best approach on ArqMATH3
Task1, considering P@10. Our Case analysis indicates that while the GPT-4 can
generate relevant responses in certain instances, it does not consistently
answer all questions accurately. This paper explores the current limitations of
LLMs in navigating complex mathematical problem-solving. Through case analysis,
we shed light on the gaps in LLM capabilities within mathematics, thereby
setting the stage for future research and advancements in AI-driven
mathematical reasoning. We make our code and findings publicly available for
research: \url{https://github.com/gipplab/LLM-Investig-MathStackExchange}
\\ ( https://arxiv.org/abs/2404.00344 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00361
Date: Sat, 30 Mar 2024 13:28:51 GMT   (663kb,D)

Title: Controllable and Diverse Data Augmentation with Large Language Model for
  Low-Resource Open-Domain Dialogue Generation
Authors: Zhenhua Liu, Tong Zhu, Jianxiang Xiang, Wenliang Chen
Categories: cs.CL
Comments: 13 pages, 5 figures
\\
  Data augmentation (DA) is crucial to mitigate model training instability and
over-fitting problems in low-resource open-domain dialogue generation. However,
traditional DA methods often neglect semantic data diversity, restricting the
overall quality. Recently, large language models (LLM) have been used for DA to
generate diversified dialogues. However, they have limited controllability and
tend to generate dialogues with a distribution shift compared to the seed
dialogues. To maximize the augmentation diversity and address the
controllability problem, we propose \textbf{S}ummary-based \textbf{D}ialogue
\textbf{A}ugmentation with LLM (SDA). Our approach enhances the controllability
of LLM by using dialogue summaries as a planning tool. Based on summaries, SDA
can generate high-quality and diverse dialogue data even with a small seed
dataset. To evaluate the efficacy of data augmentation methods for open-domain
dialogue, we designed a clustering-based metric to characterize the semantic
diversity of the augmented dialogue data. The experimental results show that
SDA can augment high-quality and semantically diverse dialogues given a small
seed dataset and an LLM, and the augmented data can boost the performance of
open-domain dialogue models.
\\ ( https://arxiv.org/abs/2404.00361 ,  663kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00376
Date: Sat, 30 Mar 2024 14:09:00 GMT   (1250kb,D)

Title: Small Language Models Learn Enhanced Reasoning Skills from Medical
  Textbooks
Authors: Hyunjae Kim, Hyeon Hwang, Jiwoo Lee, Sihyeon Park, Dain Kim, Taewhoo
  Lee, Chanwoong Yoon, Jiwoong Sohn, Donghee Choi, Jaewoo Kang
Categories: cs.CL
\\
  While recent advancements in commercial large language models (LM) have shown
promising results in medical tasks, their closed-source nature poses
significant privacy and security concerns, hindering their widespread use in
the medical field. Despite efforts to create open-source models, their limited
parameters often result in insufficient multi-step reasoning capabilities
required for solving complex medical problems. To address this, we introduce
Meerkat-7B, a novel medical AI system with 7 billion parameters. Meerkat-7B was
trained using our new synthetic dataset consisting of high-quality
chain-of-thought reasoning paths sourced from 18 medical textbooks, along with
diverse instruction-following datasets. Our system achieved remarkable accuracy
across seven medical benchmarks, surpassing GPT-3.5 by 13.1%, as well as
outperforming the previous best 7B models such as MediTron-7B and BioMistral-7B
by 13.4% and 9.8%, respectively. Notably, it surpassed the passing threshold of
the United States Medical Licensing Examination (USMLE) for the first time for
a 7B-parameter model. Additionally, our system offered more detailed free-form
responses to clinical queries compared to existing 7B and 13B models,
approaching the performance level of GPT-3.5. This significantly narrows the
performance gap with large LMs, showcasing its effectiveness in addressing
complex medical challenges.
\\ ( https://arxiv.org/abs/2404.00376 ,  1250kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00386
Date: Sat, 30 Mar 2024 14:58:44 GMT   (1605kb,D)

Title: Jetsons at FinNLP 2024: Towards Understanding the ESG Impact of a News
  Article using Transformer-based Models
Authors: Parag Pravin Dakle, Alolika Gon, Sihan Zha, Liang Wang, SaiKrishna
  Rallabandi, and Preethi Raghavan
Categories: cs.CL
\\
  In this paper, we describe the different approaches explored by the Jetsons
team for the Multi-Lingual ESG Impact Duration Inference (ML-ESG-3) shared
task. The shared task focuses on predicting the duration and type of the ESG
impact of a news article. The shared task dataset consists of 2,059 news titles
and articles in English, French, Korean, and Japanese languages. For the impact
duration classification task, we fine-tuned XLM-RoBERTa with a custom
fine-tuning strategy and using self-training and DeBERTa-v3 using only English
translations. These models individually ranked first on the leaderboard for
Korean and Japanese and in an ensemble for the English language, respectively.
For the impact type classification task, our XLM-RoBERTa model fine-tuned using
a custom fine-tuning strategy ranked first for the English language.
\\ ( https://arxiv.org/abs/2404.00386 ,  1605kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00397
Date: Sat, 30 Mar 2024 15:29:49 GMT   (1303kb,D)

Title: An Analysis of BPE Vocabulary Trimming in Neural Machine Translation
Authors: Marco Cognetta, Tatsuya Hiraoka, Naoaki Okazaki, Rico Sennrich, Yuval
  Pinter
Categories: cs.CL
Comments: 15 pages
\\
  We explore threshold vocabulary trimming in Byte-Pair Encoding subword
tokenization, a postprocessing step that replaces rare subwords with their
component subwords. The technique is available in popular tokenization
libraries but has not been subjected to rigorous scientific scrutiny. While the
removal of rare subwords is suggested as best practice in machine translation
implementations, both as a means to reduce model size and for improving model
performance through robustness, our experiments indicate that, across a large
space of hyperparameter settings, vocabulary trimming fails to improve
performance, and is even prone to incurring heavy degradation.
\\ ( https://arxiv.org/abs/2404.00397 ,  1303kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00399
Date: Sat, 30 Mar 2024 15:38:54 GMT   (4284kb,D)

Title: Aurora-M: The First Open Source Multilingual Language Model Red-teamed
  according to the U.S. Executive Order
Authors: Taishi Nakamura, Mayank Mishra, Simone Tedeschi, Yekun Chai, Jason T
  Stillerman, Felix Friedrich, Prateek Yadav, Tanmay Laud, Vu Minh Chien, Terry
  Yue Zhuo, Diganta Misra, Ben Bogin, Xuan-Son Vu, Marzena Karpinska, Arnav
  Varma Dantuluri, Wojciech Kusa, Tommaso Furlanello, Rio Yokota, Niklas
  Muennighoff, Suhas Pai, Tosin Adewumi, Veronika Laippala, Xiaozhe Yao,
  Adalberto Junior, Alpay Ariyak, Aleksandr Drozd, Jordan Clive, Kshitij Gupta,
  Liangyu Chen, Qi Sun, Ken Tsui, Noah Persaud, Nour Fahmy, Tianlong Chen,
  Mohit Bansal, Nicolo Monti, Tai Dang, Ziyang Luo, Tien-Tung Bui, Roberto
  Navigli, Virendra Mehta, Matthew Blumberg, Victor May, Huu Nguyen, Sampo
  Pyysalo
Categories: cs.CL cs.AI cs.LG
Comments: Preprint
\\
  Pretrained language models underpin several AI applications, but their high
computational cost for training limits accessibility. Initiatives such as BLOOM
and StarCoder aim to democratize access to pretrained models for collaborative
community development. However, such existing models face challenges: limited
multilingual capabilities, continual pretraining causing catastrophic
forgetting, whereas pretraining from scratch is computationally expensive, and
compliance with AI safety and development laws. This paper presents Aurora-M, a
15B parameter multilingual open-source model trained on English, Finnish,
Hindi, Japanese, Vietnamese, and code. Continually pretrained from
StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion
tokens in total training token count. It is the first open-source multilingual
model fine-tuned on human-reviewed safety instructions, thus aligning its
development not only with conventional red-teaming considerations, but also
with the specific concerns articulated in the Biden-Harris Executive Order on
the Safe, Secure, and Trustworthy Development and Use of Artificial
Intelligence. Aurora-M is rigorously evaluated across various tasks and
languages, demonstrating robustness against catastrophic forgetting and
outperforming alternatives in multilingual settings, particularly in safety
evaluations. To promote responsible open-source LLM development, Aurora-M and
its variants are released at
https://huggingface.co/collections/aurora-m/aurora-m-models-65fdfdff62471e09812f5407 .
\\ ( https://arxiv.org/abs/2404.00399 ,  4284kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00401
Date: Sat, 30 Mar 2024 15:48:49 GMT   (304kb,D)

Title: How Robust are the Tabular QA Models for Scientific Tables? A Study
  using Customized Dataset
Authors: Akash Ghosh, B Venkata Sahith, Niloy Ganguly, Pawan Goyal, Mayank
  Singh
Categories: cs.CL
\\
  Question-answering (QA) on hybrid scientific tabular and textual data deals
with scientific information, and relies on complex numerical reasoning. In
recent years, while tabular QA has seen rapid progress, understanding their
robustness on scientific information is lacking due to absence of any benchmark
dataset. To investigate the robustness of the existing state-of-the-art QA
models on scientific hybrid tabular data, we propose a new dataset, "SciTabQA",
consisting of 822 question-answer pairs from scientific tables and their
descriptions. With the help of this dataset, we assess the state-of-the-art
Tabular QA models based on their ability (i) to use heterogeneous information
requiring both structured data (table) and unstructured data (text) and (ii) to
perform complex scientific reasoning tasks. In essence, we check the capability
of the models to interpret scientific tables and text. Our experiments show
that "SciTabQA" is an innovative dataset to study question-answering over
scientific heterogeneous data. We benchmark three state-of-the-art Tabular QA
models, and find that the best F1 score is only 0.462.
\\ ( https://arxiv.org/abs/2404.00401 ,  304kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00403
Date: Sat, 30 Mar 2024 15:59:17 GMT   (489kb,D)

Title: UniMEEC: Towards Unified Multimodal Emotion Recognition and Emotion
  Cause
Authors: Guimin Hu, Zhihong Zhu, Daniel Hershcovich, Hasti Seifi, Jiayuan Xie
Categories: cs.CL
\\
  Multimodal emotion recognition in conversation (MERC) and multimodal
emotion-cause pair extraction (MECPE) has recently garnered significant
attention. Emotions are the expression of affect or feelings; responses to
specific events, thoughts, or situations are known as emotion causes. Both are
like two sides of a coin, collectively describing human behaviors and intents.
However, most existing works treat MERC and MECPE as separate tasks, which may
result in potential challenges in integrating emotion and cause in real-world
applications. In this paper, we propose a Unified Multimodal Emotion
recognition and Emotion-Cause analysis framework (UniMEEC) to explore the
causality and complementarity between emotion and emotion cause. Concretely,
UniMEEC reformulates the MERC and MECPE tasks as two mask prediction problems,
enhancing the interaction between emotion and cause. Meanwhile, UniMEEC shares
the prompt learning among modalities for probing modality-specific knowledge
from the Pre-trained model. Furthermore, we propose a task-specific
hierarchical context aggregation to control the information flow to the task.
Experiment results on four public benchmark datasets verify the model
performance on MERC and MECPE tasks and achieve consistent improvements
compared with state-of-the-art methods.
\\ ( https://arxiv.org/abs/2404.00403 ,  489kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00406
Date: Sat, 30 Mar 2024 16:14:46 GMT   (397kb)

Title: TACO -- Twitter Arguments from COnversations
Authors: Marc Feger and Stefan Dietze
Categories: cs.CL cs.AI
\\
  Twitter has emerged as a global hub for engaging in online conversations and
as a research corpus for various disciplines that have recognized the
significance of its user-generated content. Argument mining is an important
analytical task for processing and understanding online discourse.
Specifically, it aims to identify the structural elements of arguments, denoted
as information and inference. These elements, however, are not static and may
require context within the conversation they are in, yet there is a lack of
data and annotation frameworks addressing this dynamic aspect on Twitter. We
contribute TACO, the first dataset of Twitter Arguments utilizing 1,814 tweets
covering 200 entire conversations spanning six heterogeneous topics annotated
with an agreement of 0.718 Krippendorff's alpha among six experts. Second, we
provide our annotation framework, incorporating definitions from the Cambridge
Dictionary, to define and identify argument components on Twitter. Our
transformer-based classifier achieves an 85.06\% macro F1 baseline score in
detecting arguments. Moreover, our data reveals that Twitter users tend to
engage in discussions involving informed inferences and information. TACO
serves multiple purposes, such as training tweet classifiers to manage tweets
based on inference and information elements, while also providing valuable
insights into the conversational reply patterns of tweets.
\\ ( https://arxiv.org/abs/2404.00406 ,  397kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00415
Date: Sat, 30 Mar 2024 16:47:06 GMT   (710kb,D)

Title: CoDa: Constrained Generation based Data Augmentation for Low-Resource
  NLP
Authors: Chandra Kiran Reddy Evuru and Sreyan Ghosh and Sonal Kumar and
  Ramaneswaran S and Utkarsh Tyagi and Dinesh Manocha
Categories: cs.CL
Comments: Accepted to NAACL 2024 Findings
\\
  We present CoDa (Constrained Generation based Data Augmentation), a
controllable, effective, and training-free data augmentation technique for
low-resource (data-scarce) NLP. Our approach is based on prompting
off-the-shelf instruction-following Large Language Models (LLMs) for generating
text that satisfies a set of constraints. Precisely, we extract a set of simple
constraints from every instance in the low-resource dataset and verbalize them
to prompt an LLM to generate novel and diverse training instances. Our findings
reveal that synthetic data that follows simple constraints in the downstream
dataset act as highly effective augmentations, and CoDa can achieve this
without intricate decoding-time constrained generation techniques or
fine-tuning with complex algorithms that eventually make the model biased
toward the small number of training instances. Additionally, CoDa is the first
framework that provides users explicit control over the augmentation generation
process, thereby also allowing easy adaptation to several domains. We
demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3
low-resource settings. CoDa outperforms all our baselines, qualitatively and
quantitatively, with improvements of 0.12%-7.19%. Code is available here:
https://github.com/Sreyan88/CoDa
\\ ( https://arxiv.org/abs/2404.00415 ,  710kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00437
Date: Sat, 30 Mar 2024 17:59:43 GMT   (632kb,D)

Title: Automatic explanation of the classification of Spanish legal judgments
  in jurisdiction-dependent law categories with tree estimators
Authors: Jaime Gonz\'alez-Gonz\'alez, Francisco de Arriba-P\'erez, Silvia
  Garc\'ia-M\'endez, Andrea Busto-Casti\~neira, Francisco J.
  Gonz\'alez-Casta\~no
Categories: cs.CL cs.AI cs.LG
DOI: 10.1016/j.jksuci.2023.101634
\\
  Automatic legal text classification systems have been proposed in the
literature to address knowledge extraction from judgments and detect their
aspects. However, most of these systems are black boxes even when their models
are interpretable. This may raise concerns about their trustworthiness.
Accordingly, this work contributes with a system combining Natural Language
Processing (NLP) with Machine Learning (ML) to classify legal texts in an
explainable manner. We analyze the features involved in the decision and the
threshold bifurcation values of the decision paths of tree structures and
present this information to the users in natural language. This is the first
work on automatic analysis of legal texts combining NLP and ML along with
Explainable Artificial Intelligence techniques to automatically make the
models' decisions understandable to end users. Furthermore, legal experts have
validated our solution, and this knowledge has also been incorporated into the
explanation process as "expert-in-the-loop" dictionaries. Experimental results
on an annotated data set in law categories by jurisdiction demonstrate that our
system yields competitive classification performance, with accuracy values well
above 90%, and that its automatic explanations are easily understandable even
to non-expert users.
\\ ( https://arxiv.org/abs/2404.00437 ,  632kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00439
Date: Sat, 30 Mar 2024 18:11:39 GMT   (6849kb,D)

Title: DOCMASTER: A Unified Platform for Annotation, Training, & Inference in
  Document Question-Answering
Authors: Alex Nguyen, Zilong Wang, Jingbo Shang, Dheeraj Mekala
Categories: cs.CL
\\
  The application of natural language processing models to PDF documents is
pivotal for various business applications yet the challenge of training models
for this purpose persists in businesses due to specific hurdles. These include
the complexity of working with PDF formats that necessitate parsing text and
layout information for curating training data and the lack of
privacy-preserving annotation tools. This paper introduces DOCMASTER, a unified
platform designed for annotating PDF documents, model training, and inference,
tailored to document question-answering. The annotation interface enables users
to input questions and highlight text spans within the PDF file as answers,
saving layout information and text spans accordingly. Furthermore, DOCMASTER
supports both state-of-the-art layout-aware and text models for comprehensive
training purposes. Importantly, as annotations, training, and inference occur
on-device, it also safeguards privacy. The platform has been instrumental in
driving several research prototypes concerning document analysis such as the AI
assistant utilized by University of California San Diego's (UCSD) International
Services and Engagement Office (ISEO) for processing a substantial volume of
PDF documents.
\\ ( https://arxiv.org/abs/2404.00439 ,  6849kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00450
Date: Sat, 30 Mar 2024 18:41:51 GMT   (9121kb,D)

Title: Planning and Editing What You Retrieve for Enhanced Tool Learning
Authors: Tenghao Huang, Dongwon Jung, Muhao Chen
Categories: cs.CL
Comments: This paper is accepted at NAACL-Findings 2024
\\
  Recent advancements in integrating external tools with Large Language Models
(LLMs) have opened new frontiers, with applications in mathematical reasoning,
code generators, and smart assistants. However, existing methods, relying on
simple one-time retrieval strategies, fall short on effectively and accurately
shortlisting relevant tools. This paper introduces a novel \modelname
(\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&R)'' and
``Edit-and-Ground (E\&G)'' paradigms. The P\&R paradigm consists of a neural
retrieval module for shortlisting relevant tools and an LLM-based query planner
that decomposes complex queries into actionable tasks, enhancing the
effectiveness of tool utilization. The E\&G paradigm utilizes LLMs to enrich
tool descriptions based on user scenarios, bridging the gap between user
queries and tool functionalities. Experiment results demonstrate that these
paradigms significantly improve the recall and NDCG in tool retrieval tasks,
significantly surpassing current state-of-the-art models.
\\ ( https://arxiv.org/abs/2404.00450 ,  9121kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00457
Date: Sat, 30 Mar 2024 19:43:45 GMT   (383kb,D)

Title: MetaIE: Distilling a Meta Model from LLM for All Kinds of Information
  Extraction Tasks
Authors: Letian Peng, Zilong Wang, Feng Yao, Zihan Wang, Jingbo Shang
Categories: cs.CL
\\
  Information extraction (IE) is a fundamental area in natural language
processing where prompting large language models (LLMs), even with in-context
examples, cannot defeat small LMs tuned on very small IE datasets. We observe
that IE tasks, such as named entity recognition and relation extraction, all
focus on extracting important information, which can be formalized as a
label-to-span matching. In this paper, we propose a novel framework MetaIE to
build a small LM as meta-model by learning to extract "important information",
i.e., the meta-understanding of IE, so that this meta-model can be adapted to
all kind of IE tasks effectively and efficiently. Specifically, MetaIE obtains
the small LM via a symbolic distillation from an LLM following the
label-to-span scheme. We construct the distillation dataset via sampling
sentences from language model pre-training datasets (e.g., OpenWebText in our
implementation) and prompting an LLM to identify the typed spans of "important
information". We evaluate the meta-model under the few-shot adaptation setting.
Extensive results on 13 datasets from 6 IE tasks confirm that MetaIE can offer
a better starting point for few-shot tuning on IE datasets and outperform other
meta-models from (1) vanilla language model pre-training, (2) multi-IE-task
pre-training with human annotations, and (3) single-IE-task symbolic
distillation from LLM. Moreover, we provide comprehensive analyses of MetaIE,
such as the size of the distillation dataset, the meta-model architecture, and
the size of the meta-model.
\\ ( https://arxiv.org/abs/2404.00457 ,  383kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00458
Date: Sat, 30 Mar 2024 19:45:04 GMT   (30kb,D)

Title: Beyond One-Size-Fits-All: Multi-Domain, Multi-Task Framework for
  Embedding Model Selection
Authors: Vivek Khetan
Categories: cs.CL cs.IR
\\
  This position paper proposes a systematic approach towards developing a
framework to help select the most effective embedding models for natural
language processing (NLP) tasks, addressing the challenge posed by the
proliferation of both proprietary and open-source encoder models.
\\ ( https://arxiv.org/abs/2404.00458 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00459
Date: Sat, 30 Mar 2024 19:46:59 GMT   (888kb,D)

Title: NumeroLogic: Number Encoding for Enhanced LLMs' Numerical Reasoning
Authors: Eli Schwartz, Leshem Choshen, Joseph Shtok, Sivan Doveh, Leonid
  Karlinsky, Assaf Arbelle
Categories: cs.CL
\\
  Language models struggle with handling numerical data and performing
arithmetic operations. We hypothesize that this limitation can be partially
attributed to non-intuitive textual numbers representation. When a digit is
read or generated by a causal language model it does not know its place value
(e.g. thousands vs. hundreds) until the entire number is processed. To address
this issue, we propose a simple adjustment to how numbers are represented by
including the count of digits before each number. For instance, instead of
"42", we suggest using "{2:42}" as the new format. This approach, which we term
NumeroLogic, offers an added advantage in number generation by serving as a
Chain of Thought (CoT). By requiring the model to consider the number of digits
first, it enhances the reasoning process before generating the actual number.
We use arithmetic tasks to demonstrate the effectiveness of the NumeroLogic
formatting. We further demonstrate NumeroLogic applicability to general natural
language modeling, improving language understanding performance in the MMLU
benchmark.
\\ ( https://arxiv.org/abs/2404.00459 ,  888kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00463
Date: Sat, 30 Mar 2024 20:05:41 GMT   (7893kb,D)

Title: Addressing Both Statistical and Causal Gender Fairness in NLP Models
Authors: Hannah Chen, Yangfeng Ji, David Evans
Categories: cs.CL cs.CY cs.LG
Comments: NAACL 2024 (Findings)
\\
  Statistical fairness stipulates equivalent outcomes for every protected
group, whereas causal fairness prescribes that a model makes the same
prediction for an individual regardless of their protected characteristics.
Counterfactual data augmentation (CDA) is effective for reducing bias in NLP
models, yet models trained with CDA are often evaluated only on metrics that
are closely tied to the causal fairness notion; similarly, sampling-based
methods designed to promote statistical fairness are rarely evaluated for
causal fairness. In this work, we evaluate both statistical and causal
debiasing methods for gender bias in NLP models, and find that while such
methods are effective at reducing bias as measured by the targeted metric, they
do not necessarily improve results on other bias metrics. We demonstrate that
combinations of statistical and causal debiasing techniques are able to reduce
bias measured through both types of metrics.
\\ ( https://arxiv.org/abs/2404.00463 ,  7893kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00482
Date: Sat, 30 Mar 2024 22:20:08 GMT   (1100kb,D)

Title: Cross-lingual Named Entity Corpus for Slavic Languages
Authors: Jakub Piskorski, Micha{\l} Marci\'nczuk, Roman Yangarber
Categories: cs.CL cs.AI cs.LG
Comments: Published in LREC-COLING 2024 - The 2024 Joint International
  Conference on Computational Linguistics, Language Resources and Evaluation
\\
  This paper presents a corpus manually annotated with named entities for six
Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian.
This work is the result of a series of shared tasks, conducted in 2017-2023 as
a part of the Workshops on Slavic Natural Language Processing. The corpus
consists of 5 017 documents on seven topics. The documents are annotated with
five classes of named entities. Each entity is described by a category, a
lemma, and a unique cross-lingual identifier. We provide two train-tune dataset
splits - single topic out and cross topics. For each split, we set benchmarks
using a transformer-based neural network architecture with the pre-trained
multilingual models - XLM-RoBERTa-large for named entity mention recognition
and categorization, and mT5-large for named entity lemmatization and linking.
\\ ( https://arxiv.org/abs/2404.00482 ,  1100kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00484
Date: Sat, 30 Mar 2024 22:27:21 GMT   (8626kb,D)

Title: Edinburgh Clinical NLP at SemEval-2024 Task 2: Fine-tune your model
  unless you have access to GPT-4
Authors: Aryo Pradipta Gema, Giwon Hong, Pasquale Minervini, Luke Daines,
  Beatrice Alex
Categories: cs.CL
\\
  The NLI4CT task assesses Natural Language Inference systems in predicting
whether hypotheses entail or contradict evidence from Clinical Trial Reports.
In this study, we evaluate various Large Language Models (LLMs) with multiple
strategies, including Chain-of-Thought, In-Context Learning, and
Parameter-Efficient Fine-Tuning (PEFT). We propose a PEFT method to improve the
consistency of LLMs by merging adapters that were fine-tuned separately using
triplet and language modelling objectives. We found that merging the two PEFT
adapters improves the F1 score (+0.0346) and consistency (+0.152) of the LLMs.
However, our novel methods did not produce more accurate results than GPT-4 in
terms of faithfulness and consistency. Averaging the three metrics, GPT-4 ranks
joint-first in the competition with 0.8328. Finally, our contamination analysis
with GPT-4 indicates that there was no test data leakage.
\\ ( https://arxiv.org/abs/2404.00484 ,  8626kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00486
Date: Sat, 30 Mar 2024 22:41:05 GMT   (859kb,D)

Title: Dialectical Alignment: Resolving the Tension of 3H and Security Threats
  of LLMs
Authors: Shu Yang, Jiayuan Su, Han Jiang, Mengdi Li, Keyuan Cheng, Muhammad
  Asif Ali, Lijie Hu, and Di Wang
Categories: cs.CL cs.AI
\\
  With the rise of large language models (LLMs), ensuring they embody the
principles of being helpful, honest, and harmless (3H), known as Human
Alignment, becomes crucial. While existing alignment methods like RLHF, DPO,
etc., effectively fine-tune LLMs to match preferences in the preference
dataset, they often lead LLMs to highly receptive human input and external
evidence, even when this information is poisoned. This leads to a tendency for
LLMs to be Adaptive Chameleons when external evidence conflicts with their
parametric memory. This exacerbates the risk of LLM being attacked by external
poisoned data, which poses a significant security risk to LLM system
applications such as Retrieval-augmented generation (RAG). To address the
challenge, we propose a novel framework: Dialectical Alignment (DA), which (1)
utilizes AI feedback to identify optimal strategies for LLMs to navigate
inter-context conflicts and context-memory conflicts with different external
evidence in context window (i.e., different ratios of poisoned factual
contexts); (2) constructs the SFT dataset as well as the preference dataset
based on the AI feedback and strategies above; (3) uses the above datasets for
LLM alignment to defense poisoned context attack while preserving the
effectiveness of in-context knowledge editing. Our experiments show that the
dialectical alignment model improves poisoned data attack defense by 20 and
does not require any additional prompt engineering or prior declaration of
``you may be attacked`` to the LLMs' context window.
\\ ( https://arxiv.org/abs/2404.00486 ,  859kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00488
Date: Sat, 30 Mar 2024 23:06:34 GMT   (736kb)

Title: Noise-Aware Training of Layout-Aware Language Models
Authors: Ritesh Sarkhel, Xiaoqi Ren, Lauro Beltrao Costa, Guolong Su, Vincent
  Perot, Yanan Xie, Emmanouil Koukoumidis, Arnab Nandi
Categories: cs.CL cs.AI cs.LG
\\
  A visually rich document (VRD) utilizes visual features along with linguistic
cues to disseminate information. Training a custom extractor that identifies
named entities from a document requires a large number of instances of the
target document type annotated at textual and visual modalities. This is an
expensive bottleneck in enterprise scenarios, where we want to train custom
extractors for thousands of different document types in a scalable way.
Pre-training an extractor model on unlabeled instances of the target document
type, followed by a fine-tuning step on human-labeled instances does not work
in these scenarios, as it surpasses the maximum allowable training time
allocated for the extractor. We address this scenario by proposing a
Noise-Aware Training method or NAT in this paper. Instead of acquiring
expensive human-labeled documents, NAT utilizes weakly labeled documents to
train an extractor in a scalable way. To avoid degradation in the model's
quality due to noisy, weakly labeled samples, NAT estimates the confidence of
each training sample and incorporates it as uncertainty measure during
training. We train multiple state-of-the-art extractor models using NAT.
Experiments on a number of publicly available and in-house datasets show that
NAT-trained models are not only robust in performance -- it outperforms a
transfer-learning baseline by up to 6% in terms of macro-F1 score, but it is
also more label-efficient -- it reduces the amount of human-effort required to
obtain comparable performance by up to 73%.
\\ ( https://arxiv.org/abs/2404.00488 ,  736kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00489
Date: Sat, 30 Mar 2024 23:07:58 GMT   (413kb,D)

Title: PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt
  Compression
Authors: Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao,
  Tianhao Huang, Lijie Hu, Lu Yu, and Di Wang
Categories: cs.CL cs.AI cs.LG
\\
  Large language models (LLMs) have shown exceptional abilities for multiple
different natural language processing tasks. While prompting is a crucial tool
for LLM inference, we observe that there is a significant cost associated with
exceedingly lengthy prompts. Existing attempts to compress lengthy prompts lead
to sub-standard results in terms of readability and interpretability of the
compressed prompt, with a detrimental impact on prompt utility. To address
this, we propose PROMPT-SAW: Prompt compresSion via Relation AWare graphs, an
effective strategy for prompt compression over task-agnostic and task-aware
prompts. PROMPT-SAW uses the prompt's textual information to build a graph,
later extracts key information elements in the graph to come up with the
compressed prompt. We also propose GSM8K-AUG, i.e., an extended version of the
existing GSM8k benchmark for task-agnostic prompts in order to provide a
comprehensive evaluation platform. Experimental evaluation using benchmark
datasets shows that prompts compressed by PROMPT-SAW are not only better in
terms of readability, but they also outperform the best-performing baseline
models by up to 14.3 and 13.7 respectively for task-aware and task-agnostic
settings while compressing the original prompt text by 33.0 and 56.7.
\\ ( https://arxiv.org/abs/2404.00489 ,  413kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00492
Date: Sat, 30 Mar 2024 23:22:51 GMT   (2810kb,D)

Title: Multi-hop Question Answering under Temporal Knowledge Editing
Authors: Keyuan Cheng, Gang Lin, Haoyang Fei, Yuxuan zhai, Lu Yu, Muhammad Asif
  Ali, Lijie Hu, and Di Wang
Categories: cs.CL cs.AI cs.LG
Comments: 23 pages
\\
  Multi-hop question answering (MQA) under knowledge editing (KE) has garnered
significant attention in the era of large language models. However, existing
models for MQA under KE exhibit poor performance when dealing with questions
containing explicit temporal contexts. To address this limitation, we propose a
novel framework, namely TEMPoral knowLEdge augmented Multi-hop Question
Answering (TEMPLE-MQA). Unlike previous methods, TEMPLE-MQA first constructs a
time-aware graph (TAG) to store edit knowledge in a structured manner. Then,
through our proposed inference path, structural retrieval, and joint reasoning
stages, TEMPLE-MQA effectively discerns temporal contexts within the question
query. Experiments on benchmark datasets demonstrate that TEMPLE-MQA
significantly outperforms baseline models. Additionally, we contribute a new
dataset, namely TKEMQA, which serves as the inaugural benchmark tailored
specifically for MQA with temporal scopes.
\\ ( https://arxiv.org/abs/2404.00492 ,  2810kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00495
Date: Sat, 30 Mar 2024 23:28:05 GMT   (726kb,D)

Title: Configurable Safety Tuning of Language Models with Synthetic Preference
  Data
Authors: Victor Gallego
Categories: cs.CL cs.AI
\\
  State-of-the-art language model fine-tuning techniques, such as Direct
Preference Optimization (DPO), restrict user control by hard-coding predefined
behaviors into the model. To address this, we propose a novel method,
Configurable Safety Tuning (CST), that augments DPO using synthetic preference
data to facilitate flexible safety configuration of LLMs at inference time. CST
overcomes the constraints of vanilla DPO by introducing a system prompt
specifying safety configurations, enabling LLM deployers to disable/enable
safety preferences based on their need, just changing the system prompt. Our
experimental evaluations indicate that CST successfully manages different
safety configurations and retains the original functionality of LLMs, showing
it is a robust method for configurable deployment. Data and models available at
https://github.com/vicgalle/configurable-safety-tuning
\\ ( https://arxiv.org/abs/2404.00495 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00500
Date: Sat, 30 Mar 2024 23:51:25 GMT   (8218kb,D)

Title: The Shape of Word Embeddings: Recognizing Language Phylogenies through
  Topological Data Analysis
Authors: Ond\v{r}ej Draganov and Steven Skiena
Categories: cs.CL math.AT
\\
  Word embeddings represent language vocabularies as clouds of $d$-dimensional
points. We investigate how information is conveyed by the general shape of
these clouds, outside of representing the semantic meaning of each token.
Specifically, we use the notion of persistent homology from topological data
analysis (TDA) to measure the distances between language pairs from the shape
of their unlabeled embeddings. We use these distance matrices to construct
language phylogenetic trees over 81 Indo-European languages. Careful evaluation
shows that our reconstructed trees exhibit strong similarities to the reference
tree.
\\ ( https://arxiv.org/abs/2404.00500 ,  8218kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00511
Date: Sun, 31 Mar 2024 01:16:02 GMT   (3056kb,D)

Title: MIPS at SemEval-2024 Task 3: Multimodal Emotion-Cause Pair Extraction in
  Conversations with Multimodal Language Models
Authors: Zebang Cheng, Fuqiang Niu, Yuxiang Lin, Zhi-Qi Cheng, Bowen Zhang,
  Xiaojiang Peng
Categories: cs.CL cs.CV cs.MM
Comments: Ranked 3rd in SemEval '24 Task 3 with F1 of 0.3435, close to 1st &
  2nd by 0.0339 & 0.0025
\\
  This paper presents our winning submission to Subtask 2 of SemEval 2024 Task
3 on multimodal emotion cause analysis in conversations. We propose a novel
Multimodal Emotion Recognition and Multimodal Emotion Cause Extraction
(MER-MCE) framework that integrates text, audio, and visual modalities using
specialized emotion encoders. Our approach sets itself apart from
top-performing teams by leveraging modality-specific features for enhanced
emotion understanding and causality inference. Experimental evaluation
demonstrates the advantages of our multimodal approach, with our submission
achieving a competitive weighted F1 score of 0.3435, ranking third with a
margin of only 0.0339 behind the 1st team and 0.0025 behind the 2nd team.
Project: https://github.com/MIPS-COLT/MER-MCE.git
\\ ( https://arxiv.org/abs/2404.00511 ,  3056kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00530
Date: Sun, 31 Mar 2024 02:05:40 GMT   (1898kb,D)

Title: Comparing Bad Apples to Good Oranges: Aligning Large Language Models via
  Joint Preference Optimization
Authors: Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei
  Chang, Aditya Grover
Categories: cs.CL cs.AI cs.LG
Comments: 25 pages, 14 figures, 5 tables
\\
  A common technique for aligning large language models (LLMs) relies on
acquiring human preferences by comparing multiple generations conditioned on a
fixed context. This only leverages the pairwise comparisons when the
generations are placed in an identical context. However, such conditional
rankings often fail to capture the complex and multidimensional aspects of
human preferences. In this work, we revisit the traditional paradigm of
preference acquisition and propose a new axis that is based on eliciting
preferences jointly over the instruction-response pairs. While prior preference
optimizations are designed for conditional ranking protocols (e.g., DPO), our
proposed preference acquisition protocol introduces DOVE, a new preference
optimization objective that upweights the joint probability of the chosen
instruction-response pair over the rejected instruction-response pair.
Interestingly, we find that the LLM trained with joint instruction-response
preference data using DOVE outperforms the LLM trained with DPO by 5.2% and
3.3% win-rate for the summarization and open-ended dialogue datasets,
respectively. Our findings reveal that joint preferences over instruction and
response pairs can significantly enhance the alignment of LLMs by tapping into
a broader spectrum of human preference elicitation. The data and code is
available at https://github.com/Hritikbansal/dove.
\\ ( https://arxiv.org/abs/2404.00530 ,  1898kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00557
Date: Sun, 31 Mar 2024 04:36:57 GMT   (1655kb,D)

Title: DivTOD: Unleashing the Power of LLMs for Diversifying Task-Oriented
  Dialogue Representations
Authors: Weihao Zeng, Dayuan Fu, Keqing He, Yejie Wang, Yukai Xu, Weiran Xu
Categories: cs.CL
Comments: NAACL 2024 (Findings)
\\
  Language models pre-trained on general text have achieved impressive results
in diverse fields. Yet, the distinct linguistic characteristics of
task-oriented dialogues (TOD) compared to general text limit the practical
utility of existing language models. Current task-oriented dialogue
pre-training methods overlook the one-to-many property of conversations, where
multiple responses can be appropriate given the same conversation context. In
this paper, we propose a novel dialogue pre-training model called DivTOD, which
collaborates with LLMs to learn diverse task-oriented dialogue representations.
DivTOD guides LLMs in transferring diverse knowledge to smaller models while
removing domain knowledge that contradicts task-oriented dialogues. Experiments
show that our model outperforms strong TOD baselines on various downstream
dialogue tasks and learns the intrinsic diversity of task-oriented dialogues.
\\ ( https://arxiv.org/abs/2404.00557 ,  1655kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00565
Date: Sun, 31 Mar 2024 05:14:38 GMT   (2822kb,D)

Title: Leveraging Corpus Metadata to Detect Template-based Translation: An
  Exploratory Case Study of the Egyptian Arabic Wikipedia Edition
Authors: Saied Alshahrani, Hesham Haroon, Ali Elfilali, Mariama Njie, Jeanna
  Matthews
Categories: cs.CL
Comments: This paper has been accepted at LREC-COLING 2024: The 6th Workshop on
  Open-Source Arabic Corpora and Processing Tools (OSACT6)
\\
  Wikipedia articles (content pages) are commonly used corpora in Natural
Language Processing (NLP) research, especially in low-resource languages other
than English. Yet, a few research studies have studied the three Arabic
Wikipedia editions, Arabic Wikipedia (AR), Egyptian Arabic Wikipedia (ARZ), and
Moroccan Arabic Wikipedia (ARY), and documented issues in the Egyptian Arabic
Wikipedia edition regarding the massive automatic creation of its articles
using template-based translation from English to Arabic without human
involvement, overwhelming the Egyptian Arabic Wikipedia with articles that do
not only have low-quality content but also with articles that do not represent
the Egyptian people, their culture, and their dialect. In this paper, we aim to
mitigate the problem of template translation that occurred in the Egyptian
Arabic Wikipedia by identifying these template-translated articles and their
characteristics through exploratory analysis and building automatic detection
systems. We first explore the content of the three Arabic Wikipedia editions in
terms of density, quality, and human contributions and utilize the resulting
insights to build multivariate machine learning classifiers leveraging
articles' metadata to detect the template-translated articles automatically. We
then publicly deploy and host the best-performing classifier, XGBoost, as an
online application called EGYPTIAN WIKIPEDIA SCANNER and release the extracted,
filtered, and labeled datasets to the research community to benefit from our
datasets and the online, web-based detection system.
\\ ( https://arxiv.org/abs/2404.00565 ,  2822kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00570
Date: Sun, 31 Mar 2024 05:56:15 GMT   (310kb,D)

Title: ParaICL: Towards Robust Parallel In-Context Learning
Authors: Xingxuan Li, Xuan-Phi Nguyen, Shafiq Joty, Lidong Bing
Categories: cs.CL
Comments: Work in progress
\\
  Large language models (LLMs) have become the norm in natural language
processing (NLP), excelling in few-shot in-context learning (ICL) with their
remarkable abilities. Nonetheless, the success of ICL largely hinges on the
choice of few-shot demonstration examples, making the selection process
increasingly crucial. Existing methods have delved into optimizing the quantity
and semantic similarity of these examples to improve ICL performances. However,
our preliminary experiments indicate that the effectiveness of ICL is limited
by the length of the input context. Moreover, varying combinations of few-shot
demonstration examples can significantly boost accuracy across different test
samples. To address this, we propose a novel method named parallel in-context
learning (ParaICL) that effectively utilizes all demonstration examples without
exceeding the manageable input context length. ParaICL employs parallel
batching to distribute demonstration examples into different batches according
to the semantic similarities of the questions in the demonstrations to the test
question. It then computes normalized batch semantic scores for each batch. A
weighted average semantic objective, constrained by adaptive plausibility, is
applied to select the most appropriate tokens. Through extensive experiments,
we validate the effectiveness of ParaICL and conduct ablation studies to
underscore its design rationale. We further demonstrate that ParaICL can
seamlessly integrate with existing methods.
\\ ( https://arxiv.org/abs/2404.00570 ,  310kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00571
Date: Sun, 31 Mar 2024 06:03:54 GMT   (309kb,D)

Title: Explainable Multi-hop Question Generation: An End-to-End Approach
  without Intermediate Question Labeling
Authors: Seonjeong Hwang, Yunsu Kim, Gary Geunbae Lee
Categories: cs.CL
Comments: LREC-Coling 2024
\\
  In response to the increasing use of interactive artificial intelligence, the
demand for the capacity to handle complex questions has increased. Multi-hop
question generation aims to generate complex questions that requires multi-step
reasoning over several documents. Previous studies have predominantly utilized
end-to-end models, wherein questions are decoded based on the representation of
context documents. However, these approaches lack the ability to explain the
reasoning process behind the generated multi-hop questions. Additionally, the
question rewriting approach, which incrementally increases the question
complexity, also has limitations due to the requirement of labeling data for
intermediate-stage questions. In this paper, we introduce an end-to-end
question rewriting model that increases question complexity through sequential
rewriting. The proposed model has the advantage of training with only the final
multi-hop questions, without intermediate questions. Experimental results
demonstrate the effectiveness of our model in generating complex questions,
particularly 3- and 4-hop questions, which are appropriately paired with input
answers. We also prove that our model logically and incrementally increases the
complexity of questions, and the generated multi-hop questions are also
beneficial for training question answering models.
\\ ( https://arxiv.org/abs/2404.00571 ,  309kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00594
Date: Sun, 31 Mar 2024 08:00:40 GMT   (1392kb,D)

Title: LexAbSumm: Aspect-based Summarization of Legal Decisions
Authors: T.Y.S.S Santosh, Mahmoud Aly, Matthias Grabmair
Categories: cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  Legal professionals frequently encounter long legal judgments that hold
critical insights for their work. While recent advances have led to automated
summarization solutions for legal documents, they typically provide generic
summaries, which may not meet the diverse information needs of users. To
address this gap, we introduce LexAbSumm, a novel dataset designed for
aspect-based summarization of legal case decisions, sourced from the European
Court of Human Rights jurisdiction. We evaluate several abstractive
summarization models tailored for longer documents on LexAbSumm, revealing a
challenge in conditioning these models to produce aspect-specific summaries. We
release LexAbSum to facilitate research in aspect-based summarization for legal
domain.
\\ ( https://arxiv.org/abs/2404.00594 ,  1392kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00595
Date: Sun, 31 Mar 2024 08:03:39 GMT   (1039kb,D)

Title: Query-driven Relevant Paragraph Extraction from Legal Judgments
Authors: T.Y.S.S Santosh, Elvin Quero Hernandez, Matthias Grabmair
Categories: cs.CL cs.IR
Comments: Accepted to LREC-COLING 2024
\\
  Legal professionals often grapple with navigating lengthy legal judgements to
pinpoint information that directly address their queries. This paper focus on
this task of extracting relevant paragraphs from legal judgements based on the
query. We construct a specialized dataset for this task from the European Court
of Human Rights (ECtHR) using the case law guides. We assess the performance of
current retrieval models in a zero-shot way and also establish fine-tuning
benchmarks using various models. The results highlight the significant gap
between fine-tuned and zero-shot performance, emphasizing the challenge of
handling distribution shift in the legal domain. We notice that the legal
pre-training handles distribution shift on the corpus side but still struggles
on query side distribution shift, with unseen legal queries. We also explore
various Parameter Efficient Fine-Tuning (PEFT) methods to evaluate their
practicality within the context of information retrieval, shedding light on the
effectiveness of different PEFT methods across diverse configurations with
pre-training and model architectures influencing the choice of PEFT method.
\\ ( https://arxiv.org/abs/2404.00595 ,  1039kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00596
Date: Sun, 31 Mar 2024 08:06:54 GMT   (916kb,D)

Title: ECtHR-PCR: A Dataset for Precedent Understanding and Prior Case
  Retrieval in the European Court of Human Rights
Authors: T.Y.S.S Santosh, Rashid Gustav Haddad, Matthias Grabmair
Categories: cs.CL cs.IR
Comments: Accepted to LREC-COLING 2024
\\
  In common law jurisdictions, legal practitioners rely on precedents to
construct arguments, in line with the doctrine of \emph{stare decisis}. As the
number of cases grow over the years, prior case retrieval (PCR) has garnered
significant attention. Besides lacking real-world scale, existing PCR datasets
do not simulate a realistic setting, because their queries use complete case
documents while only masking references to prior cases. The query is thereby
exposed to legal reasoning not yet available when constructing an argument for
an undecided case as well as spurious patterns left behind by citation masks,
potentially short-circuiting a comprehensive understanding of case facts and
legal principles. To address these limitations, we introduce a PCR dataset
based on judgements from the European Court of Human Rights (ECtHR), which
explicitly separate facts from arguments and exhibit precedential practices,
aiding us to develop this PCR dataset to foster systems' comprehensive
understanding. We benchmark different lexical and dense retrieval approaches
with various negative sampling strategies, adapting them to deal with long text
sequences using hierarchical variants. We found that difficulty-based negative
sampling strategies were not effective for the PCR task, highlighting the need
for investigation into domain-specific difficulty criteria. Furthermore, we
observe performance of the dense models degrade with time and calls for further
research into temporal adaptation of retrieval models. Additionally, we assess
the influence of different views , Halsbury's and Goodhart's, in practice in
ECtHR jurisdiction using PCR task.
\\ ( https://arxiv.org/abs/2404.00596 ,  916kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00599
Date: Sun, 31 Mar 2024 08:10:50 GMT   (804kb,D)

Title: EvoCodeBench: An Evolving Code Generation Benchmark Aligned with
  Real-World Code Repositories
Authors: Jia Li, Ge Li, Xuanming Zhang, Yihong Dong, Zhi Jin
Categories: cs.CL cs.AI cs.SE
Comments: Data: https://github.com/seketeam/EvoCodeBench
\\
  How to evaluate Large Language Models (LLMs) in code generation is an open
question. Existing benchmarks demonstrate poor alignment with real-world code
repositories and are insufficient to evaluate the coding abilities of LLMs.
This paper proposes a new benchmark - EvoCodeBench to address the preceding
problems, which has three primary advances. (1) EvoCodeBench aligns with
real-world repositories in multiple dimensions, e.g., code distributions and
dependency distributions. (2) EvoCodeBench offers comprehensive annotations
(e.g., requirements, reference code, and reference dependencies), and robust
evaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving
benchmark to avoid data leakage. We build an automatic pipeline to update
EvoCodeBench from the latest repositories. We release the first version -
EvoCodeBench-2403, containing 275 samples from 25 real-world repositories.
Based on EvoCodeBench, we propose repository-level code generation and evaluate
10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa,
Gemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs
in real-world repositories. For example, the highest Pass@1 of gpt-4 only is
20.73% in our experiments. We also analyze failed cases and summarize the
shortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all
prompts, and LLMs' completions for further community analysis.
\\ ( https://arxiv.org/abs/2404.00599 ,  804kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00604
Date: Sun, 31 Mar 2024 08:30:15 GMT   (439kb,D)

Title: Extensive Self-Contrast Enables Feedback-Free Language Model Alignment
Authors: Xiao Liu, Xixuan Song, Yuxiao Dong, Jie Tang
Categories: cs.CL cs.AI cs.LG
\\
  Reinforcement learning from human feedback (RLHF) has been a central
technique for recent large language model (LLM) alignment. However, its heavy
dependence on costly human or LLM-as-Judge preference feedback could stymie its
wider applications. In this work, we introduce Self-Contrast, a feedback-free
large language model alignment method via exploiting extensive self-generated
negatives. With only supervised fine-tuning (SFT) targets, Self-Contrast
leverages the LLM itself to generate massive diverse candidates, and harnesses
a pre-trained embedding model to filter multiple negatives according to text
similarity. Theoretically, we illustrate that in this setting, merely scaling
negative responses can still effectively approximate situations with more
balanced positive and negative preference annotations. Our experiments with
direct preference optimization (DPO) on three datasets show that, Self-Contrast
could consistently outperform SFT and standard DPO training by large margins.
And as the number of self-generated negatives increases, the performance of
Self-Contrast continues to grow. Code and data are available at
https://github.com/THUDM/Self-Contrast.
\\ ( https://arxiv.org/abs/2404.00604 ,  439kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00610
Date: Sun, 31 Mar 2024 08:58:54 GMT   (1067kb,D)

Title: RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation
Authors: Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo,
  Jie Fu
Categories: cs.CL
\\
  Large Language Models (LLMs) exhibit remarkable capabilities but are prone to
generating inaccurate or hallucinatory responses. This limitation stems from
their reliance on vast pretraining datasets, making them susceptible to errors
in unseen scenarios. To tackle these challenges, Retrieval-Augmented Generation
(RAG) addresses this by incorporating external, relevant documents into the
response generation process, thus leveraging non-parametric knowledge alongside
LLMs' in-context learning abilities. However, existing RAG implementations
primarily focus on initial input for context retrieval, overlooking the nuances
of ambiguous or complex queries that necessitate further clarification or
decomposition for accurate responses. To this end, we propose learning to
Refine Query for Retrieval Augmented Generation (RQ-RAG) in this paper,
endeavoring to enhance the model by equipping it with capabilities for explicit
rewriting, decomposition, and disambiguation. Our experimental results indicate
that our method, when applied to a 7B Llama2 model, surpasses the previous
state-of-the-art (SOTA) by an average of 1.9\% across three single-hop QA
datasets, and also demonstrates enhanced performance in handling complex,
multi-hop QA datasets. Our code is available at
https://github.com/chanchimin/RQ-RAG.
\\ ( https://arxiv.org/abs/2404.00610 ,  1067kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00614
Date: Sun, 31 Mar 2024 09:04:01 GMT   (1063kb,D)

Title: Learning to Plan for Language Modeling from Unlabeled Data
Authors: Nathan Cornille, Marie-Francine Moens and Florian Mai
Categories: cs.CL cs.AI
Comments: under review
\\
  By training to predict the next token in an unlabeled corpus, large language
models learn to perform many tasks without any labeled data. However, their
next-token-prediction objective arguably limits their performance in scenarios
that require planning, such as writing a coherent article. In this paper, we
train a module for planning the future writing process via a self-supervised
learning objective. By conditioning on generated latent plans, our model
extends the successful language model formula to more abstract planning in an
unsupervised way. Empirically, we demonstrate that our method improves language
modeling performance in general, particularly with respect to the text
structure. Because our framework uses a planner module that is unsupervised and
external to the language model, new planner modules can be trained at large
scale and easily be shared with the community.
\\ ( https://arxiv.org/abs/2404.00614 ,  1063kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00620
Date: Sun, 31 Mar 2024 09:17:34 GMT   (59kb,D)

Title: Reporting Eye-Tracking Data Quality: Towards a New Standard
Authors: Deborah N. Jakobi and Daniel G. Krakowczyk and Lena A. J\"ager
Categories: cs.CL
\\
  Eye-tracking datasets are often shared in the format used by their creators
for their original analyses, usually resulting in the exclusion of data
considered irrelevant to the primary purpose. In order to increase re-usability
of existing eye-tracking datasets for more diverse and initially not considered
use cases, this work advocates a new approach of sharing eye-tracking data.
Instead of publishing filtered and pre-processed datasets, the eye-tracking
data at all pre-processing stages should be published together with data
quality reports. In order to transparently report data quality and enable
cross-dataset comparisons, we develop data quality reporting standards and
metrics that can be automatically applied to a dataset, and integrate them into
the open-source Python package pymovements
(https://github.com/aeye-lab/pymovements).
\\ ( https://arxiv.org/abs/2404.00620 ,  59kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00629
Date: Sun, 31 Mar 2024 09:50:39 GMT   (2109kb,D)

Title: Against The Achilles' Heel: A Survey on Red Teaming for Generative
  Models
Authors: Lizhi Lin and Honglin Mu and Zenan Zhai and Minghan Wang and Yuxia
  Wang and Renxi Wang and Junjie Gao and Yixuan Zhang and Wanxiang Che and
  Timothy Baldwin and Xudong Han and Haonan Li
Categories: cs.CL
\\
  Generative models are rapidly gaining popularity and being integrated into
everyday applications, raising concerns over their safety issues as various
vulnerabilities are exposed. Faced with the problem, the field of red teaming
is experiencing fast-paced growth, which highlights the need for a
comprehensive organization covering the entire pipeline and addressing emerging
topics for the community. Our extensive survey, which examines over 120 papers,
introduces a taxonomy of fine-grained attack strategies grounded in the
inherent capabilities of language models. Additionally, we have developed the
searcher framework that unifies various automatic red teaming approaches.
Moreover, our survey covers novel areas including multimodal attacks and
defenses, risks around multilingual models, overkill of harmless queries, and
safety of downstream applications. We hope this survey can provide a systematic
perspective on the field and unlock new areas of research.
\\ ( https://arxiv.org/abs/2404.00629 ,  2109kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00656
Date: Sun, 31 Mar 2024 12:01:32 GMT   (868kb,D)

Title: WavLLM: Towards Robust and Adaptive Speech Large Language Model
Authors: Shujie Hu, Long Zhou, Shujie Liu, Sanyuan Chen, Hongkun Hao, Jing Pan,
  Xunying Liu, Jinyu Li, Sunit Sivasankaran, Linquan Liu, Furu Wei
Categories: cs.CL cs.AI cs.SD eess.AS
\\
  The recent advancements in large language models (LLMs) have revolutionized
the field of natural language processing, progressively broadening their scope
to multimodal perception and generation. However, effectively integrating
listening capabilities into LLMs poses significant challenges, particularly
with respect to generalizing across varied contexts and executing complex
auditory tasks. In this work, we introduce WavLLM, a robust and adaptive speech
large language model with dual encoders, and a prompt-aware LoRA weight
adapter, optimized by a two-stage curriculum learning approach. Leveraging dual
encoders, we decouple different types of speech information, utilizing a
Whisper encoder to process the semantic content of speech, and a WavLM encoder
to capture the unique characteristics of the speaker's identity. Within the
curriculum learning framework, WavLLM first builds its foundational
capabilities by optimizing on mixed elementary single tasks, followed by
advanced multi-task training on more complex tasks such as combinations of the
elementary tasks. To enhance the flexibility and adherence to different tasks
and instructions, a prompt-aware LoRA weight adapter is introduced in the
second advanced multi-task training stage. We validate the proposed model on
universal speech benchmarks including tasks such as ASR, ST, SV, ER, and also
apply it to specialized datasets like Gaokao English listening comprehension
set for SQA, and speech Chain-of-Thought (CoT) evaluation set. Experiments
demonstrate that the proposed model achieves state-of-the-art performance
across a range of speech tasks on the same model size, exhibiting robust
generalization capabilities in executing complex tasks using CoT approach.
Furthermore, our model successfully completes Gaokao tasks without specialized
training. The codes, models, audio, and Gaokao evaluation set can be accessed
at \url{aka.ms/wavllm}.
\\ ( https://arxiv.org/abs/2404.00656 ,  868kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00681
Date: Sun, 31 Mar 2024 13:19:36 GMT   (772kb,D)

Title: CoUDA: Coherence Evaluation via Unified Data Augmentation
Authors: Dawei Zhu, Wenhao Wu, Yifan Song, Fangwei Zhu, Ziqiang Cao, Sujian Li
Categories: cs.CL
Comments: NAACL 2024
\\
  Coherence evaluation aims to assess the organization and structure of a
discourse, which remains challenging even in the era of large language models.
Due to the scarcity of annotated data, data augmentation is commonly used for
training coherence evaluation models. However, previous augmentations for this
task primarily rely on heuristic rules, lacking designing criteria as guidance.
In this paper, we take inspiration from linguistic theory of discourse
structure, and propose a data augmentation framework named CoUDA. CoUDA breaks
down discourse coherence into global and local aspects, and designs
augmentation strategies for both aspects, respectively. Especially for local
coherence, we propose a novel generative strategy for constructing augmentation
samples, which involves post-pretraining a generative model and applying two
controlling mechanisms to control the difficulty of generated samples. During
inference, CoUDA also jointly evaluates both global and local aspects to
comprehensively assess the overall coherence of a discourse. Extensive
experiments in coherence evaluation show that, with only 233M parameters, CoUDA
achieves state-of-the-art performance in both pointwise scoring and pairwise
ranking tasks, even surpassing recent GPT-3.5 and GPT-4 based metrics.
\\ ( https://arxiv.org/abs/2404.00681 ,  772kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00699
Date: Sun, 31 Mar 2024 14:32:02 GMT   (74kb,D)

Title: How Much are LLMs Contaminated? A Comprehensive Survey and the
  LLMSanitize Library
Authors: Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li,
  Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty
Categories: cs.CL
Comments: 10 pages, 1 figure, 3 tables
\\
  With the rise of Large Language Models (LLMs) in recent years, new
opportunities are emerging, but also new challenges, and contamination is
quickly becoming critical. Business applications and fundraising in AI have
reached a scale at which a few percentage points gained on popular
question-answering benchmarks could translate into dozens of millions of
dollars, placing high pressure on model integrity. At the same time, it is
becoming harder and harder to keep track of the data that LLMs have seen; if
not impossible with closed-source models like GPT-4 and Claude-3 not divulging
any information on the training set. As a result, contamination becomes a
critical issue: LLMs' performance may not be reliable anymore, as the high
performance may be at least partly due to their previous exposure to the data.
This limitation jeopardizes the entire progress in the field of NLP, yet, there
remains a lack of methods on how to efficiently address contamination, or a
clear consensus on prevention, mitigation and classification of contamination.
In this paper, we survey all recent work on contamination with LLMs, and help
the community track contamination levels of LLMs by releasing an open-source
Python library named LLMSanitize implementing major contamination detection
algorithms, which link is: https://github.com/ntunlp/LLMSanitize.
\\ ( https://arxiv.org/abs/2404.00699 ,  74kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00727
Date: Sun, 31 Mar 2024 16:00:41 GMT   (7919kb,D)

Title: A Controlled Reevaluation of Coreference Resolution Models
Authors: Ian Porada, Xiyuan Zou, Jackie Chi Kit Cheung
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  All state-of-the-art coreference resolution (CR) models involve finetuning a
pretrained language model. Whether the superior performance of one CR model
over another is due to the choice of language model or other factors, such as
the task-specific architecture, is difficult or impossible to determine due to
lack of a standardized experimental setup. To resolve this ambiguity, we
systematically evaluate five CR models and control for certain design decisions
including the pretrained language model used by each. When controlling for
language model size, encoder-based CR models outperform more recent
decoder-based models in terms of both accuracy and inference speed.
Surprisingly, among encoder-based CR models, more recent models are not always
more accurate, and the oldest CR model that we test generalizes the best to
out-of-domain textual genres. We conclude that controlling for the choice of
language model reduces most, but not all, of the increase in F1 score reported
in the past five years.
\\ ( https://arxiv.org/abs/2404.00727 ,  7919kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00739
Date: Sun, 31 Mar 2024 16:54:29 GMT   (69kb,D)

Title: Opera Graeca Adnotata: Building a 34M+ Token Multilayer Corpus for
  Ancient Greek
Authors: Giuseppe G. A. Celano
Categories: cs.CL
\\
  In this article, the beta version 0.1.0 of Opera Graeca Adnotata (OGA), the
largest open-access multilayer corpus for Ancient Greek (AG) is presented. OGA
consists of 1,687 literary works and 34M+ tokens coming from the PerseusDL and
OpenGreekAndLatin GitHub repositories, which host AG texts ranging from about
800 BCE to about 250 CE. The texts have been enriched with seven annotation
layers: (i) tokenization layer; (ii) sentence segmentation layer; (iii)
lemmatization layer; (iv) morphological layer; (v) dependency layer; (vi)
dependency function layer; (vii) Canonical Text Services (CTS) citation layer.
The creation of each layer is described by highlighting the main technical and
annotation-related issues encountered. Tokenization, sentence segmentation, and
CTS citation are performed by rule-based algorithms, while morphosyntactic
annotation is the output of the COMBO parser trained on the data of the Ancient
Greek Dependency Treebank. For the sake of scalability and reusability, the
corpus is released in the standoff formats PAULA XML and its offspring LAULA
XML.
\\ ( https://arxiv.org/abs/2404.00739 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00748
Date: Sun, 31 Mar 2024 17:33:43 GMT   (545kb,D)

Title: Benchmark Transparency: Measuring the Impact of Data on Evaluation
Authors: Venelin Kovatchev and Matthew Lease
Categories: cs.CL cs.AI
Comments: Accepted at NAACL 2024
\\
  In this paper we present an exploratory research on quantifying the impact
that data distribution has on the performance and evaluation of NLP models. We
propose an automated framework that measures the data point distribution across
6 different dimensions: ambiguity, difficulty, discriminability, length, noise,
and perplexity.
  We use disproportional stratified sampling to measure how much the data
distribution affects absolute (Acc/F1) and relative (Rank) model performance.
We experiment on 2 different datasets (SQUAD and MNLI) and test a total of 135
different models (125 on SQUAD and 10 on MNLI). We demonstrate that without
explicit control of the data distribution, standard evaluation frameworks are
inconsistent and unreliable. We find that the impact of the data is
statistically significant and is often larger than the impact of changing the
metric.
  In a second set of experiments, we demonstrate that the impact of data on
evaluation is not just observable, but also predictable. We propose to use
benchmark transparency as a method for comparing datasets and quantifying the
similarity between them. We find that the ``dataset similarity vector'' can be
used to predict how well a model generalizes out of distribution.
\\ ( https://arxiv.org/abs/2404.00748 ,  545kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00750
Date: Sun, 31 Mar 2024 17:38:33 GMT   (904kb,D)

Title: Can Language Models Recognize Convincing Arguments?
Authors: Paula Rescala, Manoel Horta Ribeiro, Tiancheng Hu, Robert West
Categories: cs.CL cs.CY
\\
  The remarkable and ever-increasing capabilities of Large Language Models
(LLMs) have raised concerns about their potential misuse for creating
personalized, convincing misinformation and propaganda. To gain insights into
LLMs' persuasive capabilities without directly engaging in experimentation with
humans, we propose studying their performance on the related task of detecting
convincing arguments. We extend a dataset by Durmus & Cardie (2018) with
debates, votes, and user traits and propose tasks measuring LLMs' ability to
(1) distinguish between strong and weak arguments, (2) predict stances based on
beliefs and demographic characteristics, and (3) determine the appeal of an
argument to an individual based on their traits. We show that LLMs perform on
par with humans in these tasks and that combining predictions from different
LLMs yields significant performance gains, even surpassing human performance.
The data and code released with this paper contribute to the crucial ongoing
effort of continuously evaluating and monitoring the rapidly evolving
capabilities and potential impact of LLMs.
\\ ( https://arxiv.org/abs/2404.00750 ,  904kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00752
Date: Sun, 31 Mar 2024 17:47:22 GMT   (8038kb,D)

Title: On the True Distribution Approximation of Minimum Bayes-Risk Decoding
Authors: Atsumoto Ohashi, Ukyo Honda, Tetsuro Morimura, Yuu Jinnai
Categories: cs.CL cs.AI
Comments: NAACL 2024 (main conference)
\\
  Minimum Bayes-risk (MBR) decoding has recently gained renewed attention in
text generation. MBR decoding considers texts sampled from a model as
pseudo-references and selects the text with the highest similarity to the
others. Therefore, sampling is one of the key elements of MBR decoding, and
previous studies reported that the performance varies by sampling methods. From
a theoretical standpoint, this performance variation is likely tied to how
closely the samples approximate the true distribution of references. However,
this approximation has not been the subject of in-depth study. In this study,
we propose using anomaly detection to measure the degree of approximation. We
first closely examine the performance variation and then show that previous
hypotheses about samples do not correlate well with the variation, but our
introduced anomaly scores do. The results are the first to empirically support
the link between the performance and the core assumption of MBR decoding.
\\ ( https://arxiv.org/abs/2404.00752 ,  8038kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00758
Date: Sun, 31 Mar 2024 18:08:37 GMT   (128kb,D)

Title: From Robustness to Improved Generalization and Calibration in
  Pre-trained Language Models
Authors: Josip Juki\'c, Jan \v{S}najder
Categories: cs.CL
\\
  Enhancing generalization and uncertainty quantification in pre-trained
language models (PLMs) is crucial for their effectiveness and reliability.
Building on machine learning research that established the importance of
robustness for improving generalization, we investigate the role of
representation smoothness, achieved via Jacobian and Hessian regularization, in
enhancing PLM performance. Although such regularization methods have proven
effective in computer vision, their application in natural language processing
(NLP), where PLM inputs are derived from a discrete domain, poses unique
challenges. We introduce a novel two-phase regularization approach, JacHess,
which minimizes the norms of the Jacobian and Hessian matrices within PLM
intermediate representations relative to their inputs. Our evaluation using the
GLUE benchmark demonstrates that JacHess significantly improves in-domain
generalization and calibration in PLMs, outperforming unregularized fine-tuning
and other similar regularization methods.
\\ ( https://arxiv.org/abs/2404.00758 ,  128kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00826
Date: Sun, 31 Mar 2024 23:37:18 GMT   (7460kb,D)

Title: Extracting Social Determinants of Health from Pediatric Patient Notes
  Using Large Language Models: Novel Corpus and Methods
Authors: Yujuan Fu, Giridhar Kaushik Ramachandran, Nicholas J Dobbins, Namu
  Park, Michael Leu, Abby R. Rosenberg, Kevin Lybarger, Fei Xia, Ozlem Uzuner,
  Meliha Yetisgen
Categories: cs.CL
Comments: 12 pages, 2 figures and 3 tables. Accepted by LERC-COLING 2024
\\
  Social determinants of health (SDoH) play a critical role in shaping health
outcomes, particularly in pediatric populations where interventions can have
long-term implications. SDoH are frequently studied in the Electronic Health
Record (EHR), which provides a rich repository for diverse patient data. In
this work, we present a novel annotated corpus, the Pediatric Social History
Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed
SDoH representations using fine-tuned and in-context learning methods with
Large Language Models (LLMs). PedSHAC comprises annotated social history
sections from 1,260 clinical notes obtained from pediatric patients within the
University of Washington (UW) hospital system. Employing an event-based
annotation scheme, PedSHAC captures ten distinct health determinants to
encompass living and economic stability, prior trauma, education access,
substance use history, and mental health with an overall annotator agreement of
81.9 F1. Our proposed fine-tuning LLM-based extractors achieve high performance
at 78.4 F1 for event arguments. In-context learning approaches with GPT-4
demonstrate promise for reliable SDoH extraction with limited annotated
examples, with extraction performance at 82.3 F1 for event triggers.
\\ ( https://arxiv.org/abs/2404.00826 ,  7460kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00828
Date: Sun, 31 Mar 2024 23:46:51 GMT   (1081kb,D)

Title: PID Control-Based Self-Healing to Improve the Robustness of Large
  Language Models
Authors: Zhuotong Chen, Zihu Wang, Yifan Yang, Qianxiao Li, Zheng Zhang
Categories: cs.CL
Comments: Transactions on Machine Learning Research
\\
  Despite the effectiveness of deep neural networks in numerous natural
language processing applications, recent findings have exposed the
vulnerability of these language models when minor perturbations are introduced.
While appearing semantically indistinguishable to humans, these perturbations
can significantly reduce the performance of well-trained language models,
raising concerns about the reliability of deploying them in safe-critical
situations. In this work, we construct a computationally efficient self-healing
process to correct undesired model behavior during online inference when
perturbations are applied to input data. This is formulated as a trajectory
optimization problem in which the internal states of the neural network layers
are automatically corrected using a PID (Proportional-Integral-Derivative)
control mechanism. The P controller targets immediate state adjustments, while
the I and D controllers consider past states and future dynamical trends,
respectively. We leverage the geometrical properties of the training data to
design effective linear PID controllers. This approach reduces the
computational cost to that of using just the P controller, instead of the full
PID control. Further, we introduce an analytical method for approximating the
optimal control solutions, enhancing the real-time inference capabilities of
this controlled system. Moreover, we conduct a theoretical error analysis of
the analytic solution in a simplified setting. The proposed PID control-based
self-healing is a low cost framework that improves the robustness of
pre-trained large language models, whether standard or robustly trained,
against a wide range of perturbations. A detailed implementation can be found
in:https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models.
\\ ( https://arxiv.org/abs/2404.00828 ,  1081kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00829
Date: Sun, 31 Mar 2024 23:48:50 GMT   (374kb,D)

Title: Returning to the Start: Generating Narratives with Related Endpoints
Authors: Anneliese Brei, Chao Zhao, Snigdha Chaturvedi
Categories: cs.CL
\\
  Human writers often bookend their writing with ending sentences that relate
back to the beginning sentences in order to compose a satisfying narrative that
"closes the loop." Motivated by this observation, we propose RENarGen, a
controllable story-generation paradigm that generates narratives by ensuring
the first and last sentences are related and then infilling the middle
sentences. Our contributions include an initial exploration of how various
methods of bookending from Narratology affect language modeling for stories.
Automatic and human evaluations indicate RENarGen produces better stories with
more narrative closure than current autoregressive models.
\\ ( https://arxiv.org/abs/2404.00829 ,  374kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00862
Date: Mon, 1 Apr 2024 02:04:44 GMT   (905kb,D)

Title: Bailong: Bilingual Transfer Learning based on QLoRA and Zip-tie
  Embedding
Authors: Lung-Chuan Chen and Zong-Ru Li
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) have demonstrated exceptional performance in
various NLP applications. However, the majority of existing open-source LLMs
are pre-trained primarily on English data and little part of other languages.
This deficiency in multilingual training data results in suboptimal performance
when applied to languages with fewer available resources. Furthermore,
enhancing the performance of LLMs on low-resource languages by full-parameter
fine-tuning with additional data requires substantial computational resources,
posing computational barriers for research organizations and individual
researchers. Consequently, several techniques such as parameter-efficient
tuning and advanced embedding initialization have been proposed to address
these challenges. In this work, we combine them to facilitate cross-lingual
transfer on English-dominated open-source LLM. To effectively enhance the
model's proficiency in Traditional Chinese, we conduct secondary pre-training
on Llama 2 7B with Traditional Chinese data by leveraging QLoRA and our
proposed zip-tie embedding initialization. The resulting model called Bailong,
which stands for Bilingual trAnsfer learnIng based on qLOra and zip-tie
embeddiNG. We present Bailong-instruct 7B, a fine-tuned version of Bailong 7B
optimized for multi-turn dialogue scenarios. Recognizing the inadequacy of
benchmark datasets in Traditional Chinese, we further introduce Bailong-bench
to assess the alignment of models with human preferences and the capability to
follow instructions in both Traditional Chinese and English tasks. In our
evaluation, Bailong-instruct 7B exhibits competitive performance on
Bailong-bench and other benchmark datasets when compared to other open-source
models of similar or even larger parameter sizes. Bailong-instruct 7B and
Bailong-bench are publicly available with the aim of empowering the community
to build upon our efforts.
\\ ( https://arxiv.org/abs/2404.00862 ,  905kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00884
Date: Mon, 1 Apr 2024 03:25:06 GMT   (7851kb,D)

Title: Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large
  Language Models
Authors: Wei He, Shichun Liu, Jun Zhao, Yiwen Ding, Yi Lu, Zhiheng Xi, Tao Gui,
  Qi Zhang, Xuanjing Huang
Categories: cs.CL cs.AI
Comments: Accepted to NAACL 2024 Findings
\\
  Large language models (LLMs) have shown promising abilities of in-context
learning (ICL), adapting swiftly to new tasks with only few-shot
demonstrations. However, current few-shot methods heavily depend on
high-quality, query-specific demos, which are often lacking. When faced with
out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or
external retrievers might fail. To bridge the gap between limited demos and OOD
queries, we propose Self-Demos, a novel prompting method that elicits the
inherent generalizability in LLMs by query-aware demo generation. The generated
demos strategically interpolate between existing demos and the given query,
transforming the query from OOD to ID. To evaluate the effectiveness of our
approach, we manually constructed OOD-Toolset, a dataset in the tool-using
scenario with over 300 real-world APIs and 1000 instances, each consisting of
three tool-use cases as demos and an OOD query. Thorough experiments on our
dataset and two public math benchmarks have shown that our method can
outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct
a range of analyses to validate Self-Demos's generalization and provide more
insights.
\\ ( https://arxiv.org/abs/2404.00884 ,  7851kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00899
Date: Mon, 1 Apr 2024 03:54:42 GMT   (69kb,D)

Title: TM-TREK at SemEval-2024 Task 8: Towards LLM-Based Automatic Boundary
  Detection for Human-Machine Mixed Text
Authors: Xiaoyan Qu and Xiangfeng Meng
Categories: cs.CL
Comments: 1st place at SemEval-2024 Task 8, Subtask C, to appear in
  SemEval-2024 proceedings
\\
  With the increasing prevalence of text generated by large language models
(LLMs), there is a growing concern about distinguishing between LLM-generated
and human-written texts in order to prevent the misuse of LLMs, such as the
dissemination of misleading information and academic dishonesty. Previous
research has primarily focused on classifying text as either entirely
human-written or LLM-generated, neglecting the detection of mixed texts that
contain both types of content. This paper explores LLMs' ability to identify
boundaries in human-written and machine-generated mixed texts. We approach this
task by transforming it into a token classification problem and regard the
label turning point as the boundary. Notably, our ensemble model of LLMs
achieved first place in the 'Human-Machine Mixed Text Detection' sub-task of
the SemEval'24 Competition Task 8. Additionally, we investigate factors that
influence the capability of LLMs in detecting boundaries within mixed texts,
including the incorporation of extra layers on top of LLMs, combination of
segmentation loss, and the impact of pretraining. Our findings aim to provide
valuable insights for future research in this area.
\\ ( https://arxiv.org/abs/2404.00899 ,  69kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00914
Date: Mon, 1 Apr 2024 04:39:44 GMT   (3048kb,D)

Title: Token-Efficient Leverage Learning in Large Language Models
Authors: Yuanhao Zeng, Min Wang, Yihang Wang, Yingxia Shao
Categories: cs.CL cs.AI cs.LG
Comments: 15 pages, 16 figures
\\
  Large Language Models (LLMs) have excelled in various tasks but perform
better in high-resource scenarios, which presents challenges in low-resource
scenarios. Data scarcity and the inherent difficulty of adapting LLMs to
specific tasks compound the challenge. To address the twin hurdles, we
introduce \textbf{Leverage Learning}. We present a streamlined implement of
this methodology called Token-Efficient Leverage Learning (TELL). TELL
showcases the potential of Leverage Learning, demonstrating effectiveness
across various LLMs and low-resource tasks, ranging from $10^4$ to $10^6$
tokens. It reduces task data requirements by up to nearly an order of magnitude
compared to conventional Supervised Fine-Tuning (SFT) while delivering
competitive performance. With the same amount of task data, TELL leads in
improving task performance compared to SFT. We discuss the mechanism of
Leverage Learning, suggesting it aligns with quantization hypothesis and
explore its promising potential through empirical testing.
\\ ( https://arxiv.org/abs/2404.00914 ,  3048kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00929
Date: Mon, 1 Apr 2024 05:13:56 GMT   (3034kb,D)

Title: A Survey on Multilingual Large Language Models: Corpora, Alignment, and
  Bias
Authors: Yuemei Xu, Ling Hu, Jiayi Zhao, Zihan Qiu, Yuqi Ye, Hanwen Gu
Categories: cs.CL cs.AI
\\
  Based on the foundation of Large Language Models (LLMs), Multilingual Large
Language Models (MLLMs) have been developed to address the challenges of
multilingual natural language processing tasks, hoping to achieve knowledge
transfer from high-resource to low-resource languages. However, significant
limitations and challenges still exist, such as language imbalance,
multilingual alignment, and inherent bias. In this paper, we aim to provide a
comprehensive analysis of MLLMs, delving deeply into discussions surrounding
these critical issues. First of all, we start by presenting an overview of
MLLMs, covering their evolution, key techniques, and multilingual capacities.
Secondly, we explore widely utilized multilingual corpora for MLLMs' training
and multilingual datasets oriented for downstream tasks that are crucial for
enhancing the cross-lingual capability of MLLMs. Thirdly, we survey the
existing studies on multilingual representations and investigate whether the
current MLLMs can learn a universal language representation. Fourthly, we
discuss bias on MLLMs including its category and evaluation metrics, and
summarize the existing debiasing techniques. Finally, we discuss existing
challenges and point out promising research directions. By demonstrating these
aspects, this paper aims to facilitate a deeper understanding of MLLMs and
their potentiality in various domains.
\\ ( https://arxiv.org/abs/2404.00929 ,  3034kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00930
Date: Mon, 1 Apr 2024 05:19:34 GMT   (6687kb,D)

Title: PSYDIAL: Personality-based Synthetic Dialogue Generation using Large
  Language Models
Authors: Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang, Kyung-Ah Sohn
Categories: cs.CL
Comments: LREC-COLING 2024 Main
\\
  We present a novel end-to-end personality-based synthetic dialogue data
generation pipeline, specifically designed to elicit responses from large
language models via prompting. We design the prompts to generate more
human-like dialogues considering real-world scenarios when users engage with
chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on
personality-based dialogues, curated using our proposed pipeline. Notably, we
focus on the Extraversion dimension of the Big Five personality model in our
research. Experimental results indicate that while pre-trained models and those
fine-tuned with a chit-chat dataset struggle to generate responses reflecting
personality, models trained with PSYDIAL show significant improvements. The
versatility of our pipeline extends beyond dialogue tasks, offering potential
for other non-dialogue related applications. This research opens doors for more
nuanced, personality-driven conversational AI in Korean and potentially other
languages. Our code is publicly available at
https://github.com/jiSilverH/psydial.
\\ ( https://arxiv.org/abs/2404.00930 ,  6687kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00934
Date: Mon, 1 Apr 2024 05:39:36 GMT   (1243kb,D)

Title: ChatGLM-RLHF: Practices of Aligning Large Language Models with Human
  Feedback
Authors: Zhenyu Hou, Yiin Niu, Zhengxiao Du, Xiaohan Zhang, Xiao Liu, Aohan
  Zeng, Qinkai Zheng, Minlie Huang, Hongning Wang, Jie Tang, Yuxiao Dong
Categories: cs.CL
\\
  ChatGLM is a free-to-use AI service powered by the ChatGLM family of large
language models (LLMs). In this paper, we present the ChatGLM-RLHF pipeline --
a reinforcement learning from human feedback (RLHF) system -- designed to
enhance ChatGLM's alignment with human preferences. ChatGLM-RLHF encompasses
three major components: the collection of human preference data, the training
of the reward model, and the optimization of policies. Throughout the process
of integrating ChatGLM-RLHF into production, we encountered and addressed
several unprecedented challenges. We introduce the strategies to mitigate
reward variance for stabilized large-scale training, implement model
parallelism with fused gradient-descent, and design regularization constraints
to avoid catastrophic forgetting in LLMs. Experiments show that ChatGLM-RLHF
brings significant improvements in alignment tasks compared to the supervised
fine-tuned (SFT) version of ChatGLM. For instance, it achieves on average 15\%
more wins against ChatGLM-SFT in Chinese alignment tasks. The work presents our
practices of aligning LLMs with human preferences, offering insights into the
challenges and solutions in RLHF implementations.
\\ ( https://arxiv.org/abs/2404.00934 ,  1243kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00942
Date: Mon, 1 Apr 2024 06:01:17 GMT   (1608kb,D)

Title: Evaluating the Factuality of Large Language Models using Large-Scale
  Knowledge Graphs
Authors: Xiaoze Liu, Feijie Wu, Tianyang Xu, Zhuo Chen, Yichi Zhang, Xiaoqian
  Wang, Jing Gao
Categories: cs.CL cs.AI cs.LG
\\
  The advent of Large Language Models (LLMs) has significantly transformed the
AI landscape, enhancing machine learning and AI capabilities. Factuality issue
is a critical concern for LLMs, as they may generate factually incorrect
responses. In this paper, we propose GraphEval to evaluate an LLM's performance
using a substantially large test dataset. Specifically, the test dataset is
retrieved from a large knowledge graph with more than 10 million facts without
expensive human efforts. Unlike conventional methods that evaluate LLMs based
on generated responses, GraphEval streamlines the evaluation process by
creating a judge model to estimate the correctness of the answers given by the
LLM. Our experiments demonstrate that the judge model's factuality assessment
aligns closely with the correctness of the LLM's generated outputs, while also
substantially reducing evaluation costs. Besides, our findings offer valuable
insights into LLM performance across different metrics and highlight the
potential for future improvements in ensuring the factual integrity of LLM
outputs. The code is publicly available at https://github.com/xz-liu/GraphEval.
\\ ( https://arxiv.org/abs/2404.00942 ,  1608kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00943
Date: Mon, 1 Apr 2024 06:03:39 GMT   (2657kb,D)

Title: Evalverse: Unified and Accessible Library for Large Language Model
  Evaluation
Authors: Jihoo Kim, Wonho Song, Dahyun Kim, Yunsu Kim, Yungi Kim, Chanjun Park
Categories: cs.CL cs.AI
\\
  This paper introduces Evalverse, a novel library that streamlines the
evaluation of Large Language Models (LLMs) by unifying disparate evaluation
tools into a single, user-friendly framework. Evalverse enables individuals
with limited knowledge of artificial intelligence to easily request LLM
evaluations and receive detailed reports, facilitated by an integration with
communication platforms like Slack. Thus, Evalverse serves as a powerful tool
for the comprehensive assessment of LLMs, offering both researchers and
practitioners a centralized and easily accessible evaluation framework.
Finally, we also provide a demo video for Evalverse, showcasing its
capabilities and implementation in a two-minute format.
\\ ( https://arxiv.org/abs/2404.00943 ,  2657kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00950
Date: Mon, 1 Apr 2024 06:25:47 GMT   (64kb,D)

Title: AISPACE at SemEval-2024 task 8: A Class-balanced Soft-voting System for
  Detecting Multi-generator Machine-generated Text
Authors: Renhua Gu and Xiangfeng Meng
Categories: cs.CL
Comments: 1st place at SemEval-2024 Task 8, Subtask B, to appear in
  SemEval-2024 proceedings
\\
  SemEval-2024 Task 8 provides a challenge to detect human-written and
machine-generated text. There are 3 subtasks for different detection scenarios.
This paper proposes a system that mainly deals with Subtask B. It aims to
detect if given full text is written by human or is generated by a specific
Large Language Model (LLM), which is actually a multi-class text classification
task. Our team AISPACE conducted a systematic study of fine-tuning
transformer-based models, including encoderonly, decoder-only and
encoder-decoder models. We compared their performance on this task and
identified that encoder-only models performed exceptionally well. We also
applied a weighted Cross Entropy loss function to address the issue of data
imbalance of different class samples. Additionally, we employed softvoting
strategy over multi-models ensemble to enhance the reliability of our
predictions. Our system ranked top 1 in Subtask B, which sets a
state-of-the-art benchmark for this new challenge.
\\ ( https://arxiv.org/abs/2404.00950 ,  64kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00978
Date: Mon, 1 Apr 2024 07:49:11 GMT   (122kb,D)

Title: Prior Constraints-based Reward Model Training for Aligning Large
  Language Models
Authors: Hang Zhou, Chenglong Wang, Yimin Hu, Tong Xiao, Chunliang Zhang and
  Jingbo Zhu
Categories: cs.CL
\\
  Reinforcement learning with human feedback for aligning large language models
(LLMs) trains a reward model typically using ranking loss with comparison
pairs.However, the training procedure suffers from an inherent problem: the
uncontrolled scaling of reward scores during reinforcement learning due to the
lack of constraints while training the reward model.This paper proposes a Prior
Constraints-based Reward Model (namely PCRM) training method to mitigate this
problem. PCRM incorporates prior constraints, specifically, length ratio and
cosine similarity between outputs of each comparison pair, during reward model
training to regulate optimization magnitude and control score margins. We
comprehensively evaluate PCRM by examining its rank correlation with human
preferences and its effectiveness in aligning LLMs via RL. Experimental results
demonstrate that PCRM significantly improves alignment performance by
effectively constraining reward score scaling. As another bonus, our method is
easily integrated into arbitrary rank-based alignment methods, such as direct
preference optimization, and can yield consistent improvement.
\\ ( https://arxiv.org/abs/2404.00978 ,  122kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00990
Date: Mon, 1 Apr 2024 08:35:56 GMT   (622kb)

Title: Exploring the Nexus of Large Language Models and Legal Systems: A Short
  Survey
Authors: Weicong Qin, Zhongxiang Sun
Categories: cs.CL
\\
  With the advancement of Artificial Intelligence (AI) and Large Language
Models (LLMs), there is a profound transformation occurring in the realm of
natural language processing tasks within the legal domain. The capabilities of
LLMs are increasingly demonstrating unique roles in the legal sector, bringing
both distinctive benefits and various challenges. This survey delves into the
synergy between LLMs and the legal system, such as their applications in tasks
like legal text comprehension, case retrieval, and analysis. Furthermore, this
survey highlights key challenges faced by LLMs in the legal domain, including
bias, interpretability, and ethical considerations, as well as how researchers
are addressing these issues. The survey showcases the latest advancements in
fine-tuned legal LLMs tailored for various legal systems, along with legal
datasets available for fine-tuning LLMs in various languages. Additionally, it
proposes directions for future research and development.
\\ ( https://arxiv.org/abs/2404.00990 ,  622kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00998
Date: Mon, 1 Apr 2024 09:02:12 GMT   (3595kb,D)

Title: LLM-RadJudge: Achieving Radiologist-Level Evaluation for X-Ray Report
  Generation
Authors: Zilong Wang, Xufang Luo, Xinyang Jiang, Dongsheng Li, Lili Qiu
Categories: cs.CL cs.AI
Comments: 11 pages, 6 figures
\\
  Evaluating generated radiology reports is crucial for the development of
radiology AI, but existing metrics fail to reflect the task's clinical
requirements. This study proposes a novel evaluation framework using large
language models (LLMs) to compare radiology reports for assessment. We compare
the performance of various LLMs and demonstrate that, when using GPT-4, our
proposed metric achieves evaluation consistency close to that of radiologists.
Furthermore, to reduce costs and improve accessibility, making this method
practical, we construct a dataset using LLM evaluation results and perform
knowledge distillation to train a smaller model. The distilled model achieves
evaluation capabilities comparable to GPT-4. Our framework and distilled model
offer an accessible and efficient evaluation method for radiology report
generation, facilitating the development of more clinically relevant models.
The model will be further open-sourced and accessible.
\\ ( https://arxiv.org/abs/2404.00998 ,  3595kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00999
Date: Mon, 1 Apr 2024 09:08:53 GMT   (4015kb,D)

Title: What Causes the Failure of Explicit to Implicit Discourse Relation
  Recognition?
Authors: Wei Liu, Stephen Wan, and Michael Strube
Categories: cs.CL
Comments: Accepted by NAACL2024 (Long Paper)
\\
  We consider an unanswered question in the discourse processing community: why
do relation classifiers trained on explicit examples (with connectives removed)
perform poorly in real implicit scenarios? Prior work claimed this is due to
linguistic dissimilarity between explicit and implicit examples but provided no
empirical evidence. In this study, we show that one cause for such failure is a
label shift after connectives are eliminated. Specifically, we find that the
discourse relations expressed by some explicit instances will change when
connectives disappear. Unlike previous work manually analyzing a few examples,
we present empirical evidence at the corpus level to prove the existence of
such shift. Then, we analyze why label shift occurs by considering factors such
as the syntactic role played by connectives, ambiguity of connectives, and
more. Finally, we investigate two strategies to mitigate the label shift:
filtering out noisy data and joint learning with connectives. Experiments on
PDTB 2.0, PDTB 3.0, and the GUM dataset demonstrate that classifiers trained
with our strategies outperform strong baselines.
\\ ( https://arxiv.org/abs/2404.00999 ,  4015kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01009
Date: Mon, 1 Apr 2024 09:24:06 GMT   (445kb,D)

Title: Constructing and Expanding Low-Resource and Underrepresented Parallel
  Datasets for Indonesian Local Languages
Authors: Joanito Agili Lopo and Radius Tanone
Categories: cs.CL
Comments: Submitted for consideration at the EAMT, 2024. Results pending
\\
  In Indonesia, local languages play an integral role in the culture. However,
the available Indonesian language resources still fall into the category of
limited data in the Natural Language Processing (NLP) field. This is become
problematic when build NLP model for these languages. To address this gap, we
introduce Bhinneka Korpus, a multilingual parallel corpus featuring five
Indonesian local languages. Our goal is to enhance access and utilization of
these resources, extending their reach within the country. We explained in a
detail the dataset collection process and associated challenges. Additionally,
we experimented with translation task using the IBM Model 1 due to data
constraints. The result showed that the performance of each language already
shows good indications for further development. Challenges such as lexical
variation, smoothing effects, and cross-linguistic variability are discussed.
We intend to evaluate the corpus using advanced NLP techniques for low-resource
languages, paving the way for multilingual translation models.
\\ ( https://arxiv.org/abs/2404.01009 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01015
Date: Mon, 1 Apr 2024 09:35:06 GMT   (603kb,D)

Title: PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison
Authors: ChaeHun Park, Minseok Choi, Dohyun Lee, and Jaegul Choo
Categories: cs.CL
\\
  Building a reliable and automated evaluation metric is a necessary but
challenging problem for open-domain dialogue systems. Recent studies proposed
evaluation metrics that assess generated responses by considering their
relevance to previous dialogue histories. Although effective, these metrics
evaluate individual responses directly rather than considering their relative
quality compared to other responses. To handle this, we propose PairEval, a
novel dialogue evaluation metric for assessing responses by comparing their
quality against responses in different conversations. PairEval is built on top
of open-sourced and moderate-size language models, and we make them specialized
in pairwise comparison between dialogue responses. Extensive experiments on
multiple benchmarks demonstrate that our metric exhibits a higher correlation
with human judgments than baseline metrics. We also find that the proposed
comparative metric is more robust in detecting common failures from open-domain
dialogue systems, including repetition and speaker insensitivity.
\\ ( https://arxiv.org/abs/2404.01015 ,  603kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01019
Date: Mon, 1 Apr 2024 09:39:38 GMT   (1019kb,D)

Title: Source-Aware Training Enables Knowledge Attribution in Language Models
Authors: Muhammad Khalifa, David Wadden, Emma Strubell, Honglak Lee, Lu Wang,
  Iz Beltagy, Hao Peng
Categories: cs.CL cs.AI
\\
  Large language models (LLMs) learn a vast amount of knowledge during
pretraining, but they are often oblivious to the source(s) of such knowledge.
We investigate the problem of intrinsic source citation, where LLMs are
required to cite the pretraining source supporting a generated response.
Intrinsic source citation can enhance LLM transparency, interpretability, and
verifiability. To give LLMs such ability, we explore source-aware training -- a
post pretraining recipe that involves (i) training the LLM to associate unique
source document identifiers with the knowledge in each document, followed by
(ii) an instruction-tuning to teach the LLM to cite a supporting pretraining
source when prompted. Source-aware training can easily be applied to pretrained
LLMs off the shelf, and diverges minimally from existing
pretraining/fine-tuning frameworks. Through experiments on carefully curated
data, we demonstrate that our training recipe can enable faithful attribution
to the pretraining data without a substantial impact on the model's quality
compared to standard pretraining. Our results also highlight the importance of
data augmentation in achieving attribution.
\\ ( https://arxiv.org/abs/2404.01019 ,  1019kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01029
Date: Mon, 1 Apr 2024 10:17:45 GMT   (7703kb,D)

Title: Verifying Claims About Metaphors with Large-Scale Automatic Metaphor
  Identification
Authors: Kotaro Aono, Ryohei Sasano, Koichi Takeda
Categories: cs.CL
Comments: 9 pages, 0 figures, accepted in NAACL2024
\\
  There are several linguistic claims about situations where words are more
likely to be used as metaphors. However, few studies have sought to verify such
claims with large corpora. This study entails a large-scale, corpus-based
analysis of certain existing claims about verb metaphors, by applying metaphor
detection to sentences extracted from Common Crawl and using the statistics
obtained from the results. The verification results indicate that the direct
objects of verbs used as metaphors tend to have lower degrees of concreteness,
imageability, and familiarity, and that metaphors are more likely to be used in
emotional and subjective sentences.
\\ ( https://arxiv.org/abs/2404.01029 ,  7703kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01037
Date: Mon, 1 Apr 2024 10:43:52 GMT   (737kb,D)

Title: ARAGOG: Advanced RAG Output Grading
Authors: Matou\v{s} Eibich, Shivay Nagpal, Alexander Fred-Ojala
Categories: cs.CL cs.IR
Comments: 14 pages, 8 figures, associated Github repo:
  https://github.com/predlico/ARAGOG
ACM-class: I.2.7
\\
  Retrieval-Augmented Generation (RAG) is essential for integrating external
knowledge into Large Language Model (LLM) outputs. While the literature on RAG
is growing, it primarily focuses on systematic reviews and comparisons of new
state-of-the-art (SoTA) techniques against their predecessors, with a gap in
extensive experimental comparisons. This study begins to address this gap by
assessing various RAG methods' impacts on retrieval precision and answer
similarity. We found that Hypothetical Document Embedding (HyDE) and LLM
reranking significantly enhance retrieval precision. However, Maximal Marginal
Relevance (MMR) and Cohere rerank did not exhibit notable advantages over a
baseline Naive RAG system, and Multi-query approaches underperformed. Sentence
Window Retrieval emerged as the most effective for retrieval precision, despite
its variable performance on answer similarity. The study confirms the potential
of the Document Summary Index as a competent retrieval approach. All resources
related to this research are publicly accessible for further investigation
through our GitHub repository ARAGOG (https://github.com/predlico/ARAGOG). We
welcome the community to further this exploratory study in RAG systems.
\\ ( https://arxiv.org/abs/2404.01037 ,  737kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01054
Date: Mon, 1 Apr 2024 11:26:50 GMT   (457kb,D)

Title: Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language
  Model Alignment
Authors: Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, Kenshi Abe
Categories: cs.CL cs.AI
\\
  Best-of-N (BoN) sampling with a reward model has been shown to be an
effective strategy for aligning Large Language Models (LLMs) to human
preferences at the time of decoding. BoN sampling is susceptible to a problem
known as reward hacking. Because the reward model is an imperfect proxy for the
true objective, over-optimizing its value can compromise its performance on the
true objective. A common solution to prevent reward hacking in preference
learning techniques is to optimize a reward using proximity regularization
(e.g., KL regularization), which ensures that the language model remains close
to the reference model. In this research, we propose Regularized Best-of-N
(RBoN), a variant of BoN that aims to mitigate reward hacking by incorporating
a proximity term in response selection, similar to preference learning
techniques. We evaluate two variants of RBoN on the AlpacaFarm dataset and find
that they outperform BoN, especially when the proxy reward model has a low
correlation with the true objective.
\\ ( https://arxiv.org/abs/2404.01054 ,  457kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01067
Date: Mon, 1 Apr 2024 12:01:06 GMT   (3401kb,D)

Title: Exploring the Mystery of Influential Data for Mathematical Reasoning
Authors: Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan,
  Weizhu Chen
Categories: cs.CL
\\
  Selecting influential data for fine-tuning on downstream tasks is a key
factor for both performance and computation efficiency. Recent works have shown
that training with only limited data can show a superior performance on general
tasks. However, the feasibility on mathematical reasoning tasks has not been
validated. To go further, there exist two open questions for mathematical
reasoning: how to select influential data and what is an influential data
composition. For the former one, we propose a Quality-aware Diverse Selection
(QaDS) strategy adaptable for mathematical reasoning. A comparison with other
selection strategies validates the superiority of QaDS. For the latter one, we
first enlarge our setting and explore the influential data composition. We
conduct a series of experiments and highlight: scaling up reasoning data, and
training with general data selected by QaDS is helpful. Then, we define our
optimal mixture as OpenMathMix, an influential data mixture with open-source
data selected by QaDS. With OpenMathMix, we achieve a state-of-the-art 48.8%
accuracy on MATH with 7B base model. Additionally, we showcase the use of QaDS
in creating efficient fine-tuning mixtures with various selection ratios, and
analyze the quality of a wide range of open-source datasets, which can perform
as a reference for future works on mathematical reasoning tasks.
\\ ( https://arxiv.org/abs/2404.01067 ,  3401kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01070
Date: Mon, 1 Apr 2024 12:03:35 GMT   (100kb)

Title: Advancing AI with Integrity: Ethical Challenges and Solutions in Neural
  Machine Translation
Authors: Richard Kimera, Yun-Seon Kim, Heeyoul Choi
Categories: cs.CL cs.AI
Comments: 11 pages
DOI: 10.22724/LMR.2024.22.1.171
\\
  This paper addresses the ethical challenges of Artificial Intelligence in
Neural Machine Translation (NMT) systems, emphasizing the imperative for
developers to ensure fairness and cultural sensitivity. We investigate the
ethical competence of AI models in NMT, examining the Ethical considerations at
each stage of NMT development, including data handling, privacy, data
ownership, and consent. We identify and address ethical issues through
empirical studies. These include employing Transformer models for
Luganda-English translations and enhancing efficiency with sentence
mini-batching. And complementary studies that refine data labeling techniques
and fine-tune BERT and Longformer models for analyzing Luganda and English
social media content. Our second approach is a literature review from databases
such as Google Scholar and platforms like GitHub. Additionally, the paper
probes the distribution of responsibility between AI systems and humans,
underscoring the essential role of human oversight in upholding NMT ethical
standards. Incorporating a biblical perspective, we discuss the societal impact
of NMT and the broader ethical responsibilities of developers, positing them as
stewards accountable for the societal repercussions of their creations.
\\ ( https://arxiv.org/abs/2404.01070 ,  100kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01077
Date: Mon, 1 Apr 2024 12:19:08 GMT   (649kb,D)

Title: Efficient Prompting Methods for Large Language Models: A Survey
Authors: Kaiyan Chang, Songcheng Xu, Chenglong Wang, Yingfeng Luo, Tong Xiao
  and Jingbo Zhu
Categories: cs.CL
\\
  Prompting has become a mainstream paradigm for adapting large language models
(LLMs) to specific natural language processing tasks. While this approach opens
the door to in-context learning of LLMs, it brings the additional computational
burden of model inference and human effort of manual-designed prompts,
particularly when using lengthy and complex prompts to guide and control the
behavior of LLMs. As a result, the LLM field has seen a remarkable surge in
efficient prompting methods. In this paper, we present a comprehensive overview
of these methods. At a high level, efficient prompting methods can broadly be
categorized into two approaches: prompting with efficient computation and
prompting with efficient design. The former involves various ways of
compressing prompts, and the latter employs techniques for automatic prompt
optimization. We present the basic concepts of prompting, review the advances
for efficient prompting, and highlight future research directions.
\\ ( https://arxiv.org/abs/2404.01077 ,  649kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01084
Date: Mon, 1 Apr 2024 12:27:55 GMT   (7981kb,D)

Title: AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer
  Models for Lateral Thinking Puzzles
Authors: Ioannis Panagiotopoulos, Giorgos Filandrianos, Maria Lymperaiou,
  Giorgos Stamou
Categories: cs.CL cs.AI
Comments: SemEval-2024
\\
  In this paper, we outline our submission for the SemEval-2024 Task 9
competition: 'BRAINTEASER: A Novel Task Defying Common Sense'. We engage in
both sub-tasks: Sub-task A-Sentence Puzzle and Sub-task B-Word Puzzle. We
evaluate a plethora of pre-trained transformer-based language models of
different sizes through fine-tuning. Subsequently, we undertake an analysis of
their scores and responses to aid future researchers in understanding and
utilizing these models effectively. Our top-performing approaches secured
competitive positions on the competition leaderboard across both sub-tasks. In
the evaluation phase, our best submission attained an average accuracy score of
81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly
outperforming the best neural baseline (ChatGPT) by more than 20% and 30%
respectively.
\\ ( https://arxiv.org/abs/2404.01084 ,  7981kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01104
Date: Mon, 1 Apr 2024 13:24:20 GMT   (2139kb,D)

Title: SentiCSE: A Sentiment-aware Contrastive Sentence Embedding Framework
  with Sentiment-guided Textual Similarity
Authors: Jaemin Kim, Yohan Na, Kangmin Kim, Sang Rak Lee, Dong-Kyu Chae
Categories: cs.CL
Comments: 14 pages, 8 figures
MSC-class: 68T50
ACM-class: I.2.7
Journal-ref: LREC-COLING2024
\\
  Recently, sentiment-aware pre-trained language models (PLMs) demonstrate
impressive results in downstream sentiment analysis tasks. However, they
neglect to evaluate the quality of their constructed sentiment representations;
they just focus on improving the fine-tuning performance, which overshadows the
representation quality. We argue that without guaranteeing the representation
quality, their downstream performance can be highly dependent on the
supervision of the fine-tuning data rather than representation quality. This
problem would make them difficult to foray into other sentiment-related
domains, especially where labeled data is scarce. We first propose
Sentiment-guided Textual Similarity (SgTS), a novel metric for evaluating the
quality of sentiment representations, which is designed based on the degree of
equivalence in sentiment polarity between two sentences. We then propose
SentiCSE, a novel Sentiment-aware Contrastive Sentence Embedding framework for
constructing sentiment representations via combined word-level and
sentence-level objectives, whose quality is guaranteed by SgTS. Qualitative and
quantitative comparison with the previous sentiment-aware PLMs shows the
superiority of our work. Our code is available at:
https://github.com/nayohan/SentiCSE
\\ ( https://arxiv.org/abs/2404.01104 ,  2139kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01129
Date: Mon, 1 Apr 2024 14:11:45 GMT   (1112kb,D)

Title: Structured Information Matters: Incorporating Abstract Meaning
  Representation into LLMs for Improved Open-Domain Dialogue Evaluation
Authors: Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin
Categories: cs.CL
\\
  Automatic open-domain dialogue evaluation has attracted increasing attention.
Trainable evaluation metrics are commonly trained with true positive and
randomly selected negative responses, resulting in a tendency for them to
assign a higher score to the responses that share higher content similarity
with a given context. However, adversarial negative responses possess high
content similarity with the contexts whilst being semantically different.
Therefore, existing evaluation metrics are not robust enough to evaluate such
responses, resulting in low correlations with human judgments. While recent
studies have shown some efficacy in utilizing Large Language Models (LLMs) for
open-domain dialogue evaluation, they still encounter challenges in effectively
handling adversarial negative examples. In this paper, we propose a simple yet
effective framework for open-domain dialogue evaluation, which combines
domain-specific language models (SLMs) with LLMs. The SLMs can explicitly
incorporate Abstract Meaning Representation (AMR) graph information of the
dialogue through a gating mechanism for enhanced semantic representation
learning. The evaluation result of SLMs and AMR graph information are plugged
into the prompt of LLM, for the enhanced in-context learning performance.
Experimental results on open-domain dialogue evaluation tasks demonstrate the
superiority of our method compared to a wide range of state-of-the-art
baselines, especially in discriminating adversarial negative responses. Our
code is available at https://github.com/Bernard-Yang/SIMAMR.
\\ ( https://arxiv.org/abs/2404.01129 ,  1112kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01140
Date: Mon, 1 Apr 2024 14:36:35 GMT   (8411kb,D)

Title: KoCoNovel: Annotated Dataset of Character Coreference in Korean Novels
Authors: Kyuhee Kim and Surin Lee and Sangah Lee
Categories: cs.CL
Comments: 12 pages
\\
  We present KoCoNovel, an novel character coreference dataset derived from
Korean literary texts, complete with detailed annotation guidelines. Comprising
178K tokens from 50 modern and contemporary Korean novels, KoCoNovel stands as
the second-largest public coreference resolution corpus in Korean, after the
NIKL corpus, and the first to be based on literary texts. To broaden its
utility, we provide four distinct versions of KoCoNovel, offering options for
the perspectives of the omniscient author and readers, and for handling
multiple entities as either separate or overlapping. This approach integrates
existing discourse surrounding coreference resolution in literary texts,
providing a comprehensive dataset for exploration. One of KoCoNovel's
distinctive features is that 24% of all character mentions are single common
nouns, lacking possessive markers or articles. This feature is particularly
influenced by the nuances of Korean address term culture, which favors the use
of terms denoting social relationships and kinship over personal names. In
experiments with a BERT-based coreference model, we observed notable
performance enhancements with KoCoNovel in comparison to the NIKL corpus. Such
findings underscore KoCoNovel's potential to significantly enhance coreference
resolution models through the integration of Korean cultural and linguistic
dynamics.
\\ ( https://arxiv.org/abs/2404.01140 ,  8411kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01147
Date: Mon, 1 Apr 2024 14:46:20 GMT   (9034kb,D)

Title: Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case
  Study on Reddit
Authors: Parker Seegmiller, Joseph Gatto, Omar Sharif, Madhusudan Basak and
  Sarah Masud Preum
Categories: cs.CL cs.LG
Comments: 4 pages, 2 figures
\\
  Large language models (LLMs) have been shown to be proficient in correctly
answering questions in the context of online discourse. However, the study of
using LLMs to model human-like answers to fact-driven social media questions is
still under-explored. In this work, we investigate how LLMs model the wide
variety of human answers to fact-driven questions posed on several
topic-specific Reddit communities, or subreddits. We collect and release a
dataset of 409 fact-driven questions and 7,534 diverse, human-rated answers
from 15 r/Ask{Topic} communities across 3 categories: profession, social
identity, and geographic location. We find that LLMs are considerably better at
modeling highly-rated human answers to such questions, as opposed to
poorly-rated human answers. We present several directions for future research
based on our initial findings.
\\ ( https://arxiv.org/abs/2404.01147 ,  9034kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01157
Date: Mon, 1 Apr 2024 15:01:45 GMT   (194kb,D)

Title: Green AI: Exploring Carbon Footprints, Mitigation Strategies, and Trade
  Offs in Large Language Model Training
Authors: Vivian Liu, Yiqiao Yin
Categories: cs.CL cs.PF
\\
  Prominent works in the field of Natural Language Processing have long
attempted to create new innovative models by improving upon previous model
training approaches, altering model architecture, and developing more in-depth
datasets to better their performance. However, with the quickly advancing field
of NLP comes increased greenhouse gas emissions, posing concerns over the
environmental damage caused by training LLMs. Gaining a comprehensive
understanding of the various costs, particularly those pertaining to
environmental aspects, that are associated with artificial intelligence serves
as the foundational basis for ensuring safe AI models. Currently,
investigations into the CO2 emissions of AI models remain an emerging area of
research, and as such, in this paper, we evaluate the CO2 emissions of
well-known large language models, which have an especially high carbon
footprint due to their significant amount of model parameters. We argue for the
training of LLMs in a way that is responsible and sustainable by suggesting
measures for reducing carbon emissions. Furthermore, we discuss how the choice
of hardware affects CO2 emissions by contrasting the CO2 emissions during model
training for two widely used GPUs. Based on our results, we present the
benefits and drawbacks of our proposed solutions and make the argument for the
possibility of training more environmentally safe AI models without sacrificing
their robustness and performance.
\\ ( https://arxiv.org/abs/2404.01157 ,  194kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01158
Date: Mon, 1 Apr 2024 15:03:27 GMT   (5910kb,D)

Title: Dialogue with Robots: Proposals for Broadening Participation and
  Research in the SLIVAR Community
Authors: Casey Kennington, Malihe Alikhani, Heather Pon-Barry, Katherine
  Atwell, Yonatan Bisk, Daniel Fried, Felix Gervits, Zhao Han, Mert Inan,
  Michael Johnston, Raj Korpan, Diane Litman, Matthew Marge, Cynthia Matuszek,
  Ross Mead, Shiwali Mohan, Raymond Mooney, Natalie Parde, Jivko Sinapov,
  Angela Stewart, Matthew Stone, Stefanie Tellex, Tom Williams
Categories: cs.CL cs.RO
Comments: NSF Report on the "Dialogue with Robots" Workshop held in Pittsburg,
  PA, April 2023
\\
  The ability to interact with machines using natural human language is
becoming not just commonplace, but expected. The next step is not just text
interfaces, but speech interfaces and not just with computers, but with all
machines including robots. In this paper, we chronicle the recent history of
this growing field of spoken dialogue with robots and offer the community three
proposals, the first focused on education, the second on benchmarks, and the
third on the modeling of language when it comes to spoken interaction with
robots. The three proposals should act as white papers for any researcher to
take and build upon.
\\ ( https://arxiv.org/abs/2404.01158 ,  5910kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01165
Date: Mon, 1 Apr 2024 15:14:07 GMT   (973kb,D)

Title: LITE: Modeling Environmental Ecosystems with Multimodal Large Language
  Models
Authors: Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, Huaxiu
  Yao
Categories: cs.CL
\\
  The modeling of environmental ecosystems plays a pivotal role in the
sustainable management of our planet. Accurate prediction of key environmental
variables over space and time can aid in informed policy and decision-making,
thus improving people's livelihood. Recently, deep learning-based methods have
shown promise in modeling the spatial-temporal relationships for predicting
environmental variables. However, these approaches often fall short in handling
incomplete features and distribution shifts, which are commonly observed in
environmental data due to the substantial cost of data collection and
malfunctions in measuring instruments. To address these issues, we propose LITE
-- a multimodal large language model for environmental ecosystems modeling.
Specifically, LITE unifies different environmental variables by transforming
them into natural language descriptions and line graph images. Then, LITE
utilizes unified encoders to capture spatial-temporal dynamics and correlations
in different modalities. During this step, the incomplete features are imputed
by a sparse Mixture-of-Experts framework, and the distribution shift is handled
by incorporating multi-granularity information from past observations. Finally,
guided by domain instructions, a language model is employed to fuse the
multimodal representations for the prediction. Our experiments demonstrate that
LITE significantly enhances performance in environmental spatial-temporal
prediction across different domains compared to the best baseline, with a
41.25% reduction in prediction error. This justifies its effectiveness. Our
data and code are available at https://github.com/hrlics/LITE.
\\ ( https://arxiv.org/abs/2404.01165 ,  973kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01182
Date: Mon, 1 Apr 2024 15:34:24 GMT   (1001kb,D)

Title: A Neuro-Symbolic Approach to Monitoring Salt Content in Food
Authors: Anuja Tayal, Barbara Di Eugenio, Devika Salunke, Andrew D. Boyd,
  Carolyn A Dickens, Eulalia P Abril, Olga Garcia-Bedoya, Paula G Allen-Meares
Categories: cs.CL cs.SC
Comments: Accepted in CL4Health workshop in LREC-COLING'24
\\
  We propose a dialogue system that enables heart failure patients to inquire
about salt content in foods and help them monitor and reduce salt intake.
Addressing the lack of specific datasets for food-based salt content inquiries,
we develop a template-based conversational dataset. The dataset is structured
to ask clarification questions to identify food items and their salt content.
Our findings indicate that while fine-tuning transformer-based models on the
dataset yields limited performance, the integration of Neuro-Symbolic Rules
significantly enhances the system's performance. Our experiments show that by
integrating neuro-symbolic rules, our system achieves an improvement in joint
goal accuracy of over 20% across different data sizes compared to naively
fine-tuning transformer-based models.
\\ ( https://arxiv.org/abs/2404.01182 ,  1001kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01189
Date: Mon, 1 Apr 2024 15:47:21 GMT   (12839kb,D)

Title: Generating Faithful and Complete Hospital-Course Summaries from the
  Electronic Health Record
Authors: Griffin Adams
Categories: cs.CL
Comments: PhD thesis
\\
  The rapid adoption of Electronic Health Records (EHRs) has been instrumental
in streamlining administrative tasks, increasing transparency, and enabling
continuity of care across providers. An unintended consequence of the increased
documentation burden, however, has been reduced face-time with patients and,
concomitantly, a dramatic rise in clinician burnout. In this thesis, we
pinpoint a particularly time-intensive, yet critical, documentation task:
generating a summary of a patient's hospital admissions, and propose and
evaluate automated solutions. In Chapter 2, we construct a dataset based on
109,000 hospitalizations (2M source notes) and perform exploratory analyses to
motivate future work on modeling and evaluation [NAACL 2021]. In Chapter 3, we
address faithfulness from a modeling perspective by revising noisy references
[EMNLP 2022] and, to reduce the reliance on references, directly calibrating
model outputs to metrics [ACL 2023]. These works relied heavily on automatic
metrics as human annotations were limited. To fill this gap, in Chapter 4, we
conduct a fine-grained expert annotation of system errors in order to
meta-evaluate existing metrics and better understand task-specific issues of
domain adaptation and source-summary alignments. To learn a metric less
correlated to extractiveness (copy-and-paste), we derive noisy faithfulness
labels from an ensemble of existing metrics and train a faithfulness classifier
on these pseudo labels [MLHC 2023]. Finally, in Chapter 5, we demonstrate that
fine-tuned LLMs (Mistral and Zephyr) are highly prone to entity hallucinations
and cover fewer salient entities. We improve both coverage and faithfulness by
performing sentence-level entity planning based on a set of pre-computed
salient entities from the source text, which extends our work on entity-guided
news summarization [ACL, 2023], [EMNLP, 2023].
\\ ( https://arxiv.org/abs/2404.01189 ,  12839kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01196
Date: Mon, 1 Apr 2024 15:55:18 GMT   (3053kb,D)

Title: Estimating Lexical Complexity from Document-Level Distributions
Authors: Sondre Wold, Petter M{\ae}hlum, Oddbj{\o}rn Hove
Categories: cs.CL
Comments: LREC-COLING 2024
\\
  Existing methods for complexity estimation are typically developed for entire
documents. This limitation in scope makes them inapplicable for shorter pieces
of text, such as health assessment tools. These typically consist of lists of
independent sentences, all of which are too short for existing methods to
apply. The choice of wording in these assessment tools is crucial, as both the
cognitive capacity and the linguistic competency of the intended patient groups
could vary substantially. As a first step towards creating better tools for
supporting health practitioners, we develop a two-step approach for estimating
lexical complexity that does not rely on any pre-annotated data. We implement
our approach for the Norwegian language and verify its effectiveness using
statistical testing and a qualitative evaluation of samples from real
assessment tools. We also investigate the relationship between our complexity
measure and certain features typically associated with complexity in the
literature, such as word length, frequency, and the number of syllables.
\\ ( https://arxiv.org/abs/2404.01196 ,  3053kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01204
Date: Mon, 1 Apr 2024 16:00:01 GMT   (394kb,D)

Title: The Fine Line: Navigating Large Language Model Pretraining with
  Down-streaming Capability Analysis
Authors: Chen Yang, Junzhuo Li, Xinyao Niu, Xinrun Du, Songyang Gao, Haoran
  Zhang, Zhaoliang Chen, Xingwei Qu, Ruibin Yuan, Yizhi Li, Jiaheng Liu,
  Stephen W. Huang, Shawn Yue, Wenhu Chen, Jie Fu, Ge Zhang
Categories: cs.CL
\\
  Uncovering early-stage metrics that reflect final model performance is one
core principle for large-scale pretraining. The existing scaling law
demonstrates the power-law correlation between pretraining loss and training
flops, which serves as an important indicator of the current training state for
large language models. However, this principle only focuses on the model's
compression properties on the training data, resulting in an inconsistency with
the ability improvements on the downstream tasks. Some follow-up works
attempted to extend the scaling-law to more complex metrics (such as
hyperparameters), but still lacked a comprehensive analysis of the dynamic
differences among various capabilities during pretraining. To address the
aforementioned limitations, this paper undertakes a comprehensive comparison of
model capabilities at various pretraining intermediate checkpoints. Through
this analysis, we confirm that specific downstream metrics exhibit similar
training dynamics across models of different sizes, up to 67 billion
parameters. In addition to our core findings, we've reproduced Amber and
OpenLLaMA, releasing their intermediate checkpoints. This initiative offers
valuable resources to the research community and facilitates the verification
and exploration of LLM pretraining by open-source researchers. Besides, we
provide empirical summaries, including performance comparisons of different
models and capabilities, and tuition of key metrics for different training
phases. Based on these findings, we provide a more user-friendly strategy for
evaluating the optimization state, offering guidance for establishing a stable
pretraining process.
\\ ( https://arxiv.org/abs/2404.01204 ,  394kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01210
Date: Mon, 1 Apr 2024 16:10:15 GMT   (11482kb,D)

Title: AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for
  hallucination detection and analysis
Authors: Natalia Griogoriadou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos
  Stamou
Categories: cs.CL
Comments: SemEval-2024
\\
  In this paper, we present our team's submissions for SemEval-2024 Task-6 -
SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration
Mistakes. The participants were asked to perform binary classification to
identify cases of fluent overgeneration hallucinations. Our experimentation
included fine-tuning a pre-trained model on hallucination detection and a
Natural Language Inference (NLI) model. The most successful strategy involved
creating an ensemble of these models, resulting in accuracy rates of 77.8% and
79.9% on model-agnostic and model-aware datasets respectively, outperforming
the organizers' baseline and achieving notable results when contrasted with the
top-performing results in the competition, which reported accuracies of 84.7%
and 81.3% correspondingly.
\\ ( https://arxiv.org/abs/2404.01210 ,  11482kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01226
Date: Mon, 1 Apr 2024 16:39:36 GMT   (5875kb,D)

Title: Stable Code Technical Report
Authors: Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James
  Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente,
  Carlos Riquelme, Nathan Cooper
Categories: cs.CL
\\
  We introduce Stable Code, the first in our new-generation of code language
models series, which serves as a general-purpose base code language model
targeting code completion, reasoning, math, and other software
engineering-based tasks. Additionally, we introduce an instruction variant
named Stable Code Instruct that allows conversing with the model in a natural
chat interface for performing question-answering and instruction-based tasks.
In this technical report, we detail the data and training procedure leading to
both models. Their weights are available via Hugging Face for anyone to
download and use at https://huggingface.co/stabilityai/stable-code-3b and
https://huggingface.co/stabilityai/stable-code-instruct-3b. This report
contains thorough evaluations of the models, including multilingual programming
benchmarks, and the MT benchmark focusing on multi-turn dialogues. At the time
of its release, Stable Code is the state-of-the-art open model under 3B
parameters and even performs comparably to larger models of sizes 7 billion and
15 billion parameters on the popular Multi-PL benchmark. Stable Code Instruct
also exhibits state-of-the-art performance on the MT-Bench coding tasks and on
Multi-PL completion compared to other instruction tuned models. Given its
appealing small size, we also provide throughput measurements on a number of
edge devices. In addition, we open source several quantized checkpoints and
provide their performance metrics compared to the original model.
\\ ( https://arxiv.org/abs/2404.01226 ,  5875kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01230
Date: Mon, 1 Apr 2024 16:50:54 GMT   (733kb,D)

Title: LLM as a Mastermind: A Survey of Strategic Reasoning with Large Language
  Models
Authors: Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Adrian de Wynter, Yan
  Xia, Wenshan Wu, Ting Song, Man Lan, Furu Wei
Categories: cs.CL
Comments: 9 pages, 5 figures
\\
  This paper presents a comprehensive survey of the current status and
opportunities for Large Language Models (LLMs) in strategic reasoning, a
sophisticated form of reasoning that necessitates understanding and predicting
adversary actions in multi-agent settings while adjusting strategies
accordingly. Strategic reasoning is distinguished by its focus on the dynamic
and uncertain nature of interactions among multi-agents, where comprehending
the environment and anticipating the behavior of others is crucial. We explore
the scopes, applications, methodologies, and evaluation metrics related to
strategic reasoning with LLMs, highlighting the burgeoning development in this
area and the interdisciplinary approaches enhancing their decision-making
performance. It aims to systematize and clarify the scattered literature on
this subject, providing a systematic review that underscores the importance of
strategic reasoning as a critical cognitive capability and offers insights into
future research directions and potential improvements.
\\ ( https://arxiv.org/abs/2404.01230 ,  733kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01232
Date: Mon, 1 Apr 2024 16:51:13 GMT   (405kb,D)

Title: Open-Vocabulary Federated Learning with Multimodal Prototyping
Authors: Huimin Zeng, Zhenrui Yue, Dong Wang
Categories: cs.CL cs.CV
Comments: Accepted at NAACL 204
\\
  Existing federated learning (FL) studies usually assume the training label
space and test label space are identical. However, in real-world applications,
this assumption is too ideal to be true. A new user could come up with queries
that involve data from unseen classes, and such open-vocabulary queries would
directly defect such FL systems. Therefore, in this work, we explicitly focus
on the under-explored open-vocabulary challenge in FL. That is, for a new user,
the global server shall understand her/his query that involves arbitrary
unknown classes. To address this problem, we leverage the pre-trained
vision-language models (VLMs). In particular, we present a novel adaptation
framework tailored for VLMs in the context of FL, named as Federated Multimodal
Prototyping (Fed-MP). Fed-MP adaptively aggregates the local model weights
based on light-weight client residuals, and makes predictions based on a novel
multimodal prototyping mechanism. Fed-MP exploits the knowledge learned from
the seen classes, and robustifies the adapted VLM to unseen categories. Our
empirical evaluation on various datasets validates the effectiveness of Fed-MP.
\\ ( https://arxiv.org/abs/2404.01232 ,  405kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01234
Date: Mon, 1 Apr 2024 16:51:50 GMT   (147kb,D)

Title: GFLean: An Autoformalisation Framework for Lean via GF
Authors: Shashank Pathak
Categories: cs.CL math.LO
Comments: 19 Pages, 3 Figures
ACM-class: I.2.7
\\
  We present an autoformalisation framework for the Lean theorem prover, called
GFLean. GFLean uses a high-level grammar writing tool called Grammatical
Framework (GF) for parsing and linearisation. GFLean is implemented in Haskell.
We explain the functionalities of GFLean, its inner working and discuss its
limitations. We also discuss how we can use neural network based translation
programs and rule based translation programs together complimenting each other
to build robust autoformalisation frameworks.
\\ ( https://arxiv.org/abs/2404.01234 ,  147kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01242
Date: Mon, 1 Apr 2024 17:03:16 GMT   (409kb,D)

Title: Effectively Prompting Small-sized Language Models for Cross-lingual
  Tasks via Winning Tickets
Authors: Mingqi Li and Feng Luo
Categories: cs.CL
\\
  Current soft prompt methods yield limited performance when applied to
small-sized models (fewer than a billion parameters). Deep prompt-tuning, which
entails prepending parameters in each layer for enhanced efficacy, presents a
solution for prompting small-sized models, albeit requiring carefully designed
implementation. In this paper, we introduce the Lottery Ticket Prompt-learning
(LTP) framework that integrates winning tickets with soft prompts. The LTP
offers a simpler implementation and requires only a one-time execution. We
demonstrate LTP on cross-lingual tasks, where prior works rely on external
tools like human-designed multilingual templates and bilingual dictionaries,
which may not be feasible in a low-resource regime. Specifically, we select a
subset of parameters that have been changed the most during the fine-tuning
with the Masked Language Modeling objective. Then, we prepend soft prompts to
the original pre-trained language model and only update the selected parameters
together with prompt-related parameters when adapting to the downstream tasks.
We verify the effectiveness of our LTP framework on cross-lingual tasks,
specifically targeting low-resource languages. Our approach outperforms the
baselines by only updating 20\% of the original parameters.
\\ ( https://arxiv.org/abs/2404.01242 ,  409kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01247
Date: Mon, 1 Apr 2024 17:08:50 GMT   (30558kb,D)

Title: An image speaks a thousand words, but can everyone listen? On
  translating images for cultural relevance
Authors: Simran Khanuja, Sathyanarayanan Ramamoorthy, Yueqi Song, Graham Neubig
Categories: cs.CL cs.CV
\\
  Given the rise of multimedia content, human translators increasingly focus on
culturally adapting not only words but also other modalities such as images to
convey the same meaning. While several applications stand to benefit from this,
machine translation systems remain confined to dealing with language in speech
and text. In this work, we take a first step towards translating images to make
them culturally relevant. First, we build three pipelines comprising
state-of-the-art generative models to do the task. Next, we build a two-part
evaluation dataset: i) concept: comprising 600 images that are cross-culturally
coherent, focusing on a single concept per image, and ii) application:
comprising 100 images curated from real-world applications. We conduct a
multi-faceted human evaluation of translated images to assess for cultural
relevance and meaning preservation. We find that as of today, image-editing
models fail at this task, but can be improved by leveraging LLMs and retrievers
in the loop. Best pipelines can only translate 5% of images for some countries
in the easier concept dataset and no translation is successful for some
countries in the application dataset, highlighting the challenging nature of
the task. Our code and data is released here:
https://github.com/simran-khanuja/image-transcreation.
\\ ( https://arxiv.org/abs/2404.01247 ,  30558kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01253
Date: Mon, 1 Apr 2024 17:22:07 GMT   (8002kb,D)

Title: UniArk: Improving Generalisation and Consistency for Factual Knowledge
  Extraction through Debiasing
Authors: Yijun Yang, Jie He, Pinzhen Chen, V\'ictor Guti\'errez-Basulto, Jeff
  Z. Pan
Categories: cs.CL
Comments: NAACL 2024
\\
  Several recent papers have investigated the potential of language models as
knowledge bases as well as the existence of severe biases when extracting
factual knowledge. In this work, we focus on the factual probing performance
over unseen prompts from tuning, and using a probabilistic view we show the
inherent misalignment between pre-training and downstream tuning objectives in
language models for probing knowledge. We hypothesize that simultaneously
debiasing these objectives can be the key to generalisation over unseen
prompts. We propose an adapter-based framework, UniArk, for generalised and
consistent factual knowledge extraction through simple methods without
introducing extra parameters. Extensive experiments show that UniArk can
significantly improve the model's out-of-domain generalisation as well as
consistency under various prompts. Additionally, we construct ParaTrex, a
large-scale and diverse dataset for measuring the inconsistency and
out-of-domain generation of models. Further, ParaTrex offers a reference method
for constructing paraphrased datasets using large language models.
\\ ( https://arxiv.org/abs/2404.01253 ,  8002kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01261
Date: Mon, 1 Apr 2024 17:33:38 GMT   (20930kb,D)

Title: FABLES: Evaluating faithfulness and content selection in book-length
  summarization
Authors: Yekyung Kim, Yapei Chang, Marzena Karpinska, Aparna Garimella, Varun
  Manjunatha, Kyle Lo, Tanya Goyal, Mohit Iyyer
Categories: cs.CL cs.AI
Comments: preprint - 39 pages
\\
  While long-context large language models (LLMs) can technically summarize
book-length documents (>100K tokens), the length and complexity of the
documents have so far prohibited evaluations of input-dependent aspects like
faithfulness. In this paper, we conduct the first large-scale human evaluation
of faithfulness and content selection on LLM-generated summaries of fictional
books. Our study mitigates the issue of data contamination by focusing on
summaries of books published in 2023 or 2024, and we hire annotators who have
fully read each book prior to the annotation task to minimize cost and
cognitive burden. We collect FABLES, a dataset of annotations on 3,158 claims
made in LLM-generated summaries of 26 books, at a cost of $5.2K USD, which
allows us to rank LLM summarizers based on faithfulness: Claude-3-Opus
significantly outperforms all closed-source LLMs, while the open-source Mixtral
is on par with GPT-3.5-Turbo. An analysis of the annotations reveals that most
unfaithful claims relate to events and character states, and they generally
require indirect reasoning over the narrative to invalidate. While LLM-based
auto-raters have proven reliable for factuality and coherence in other
settings, we implement several LLM raters of faithfulness and find that none
correlates strongly with human annotations, especially with regard to detecting
unfaithful claims. Our experiments suggest that detecting unfaithful claims is
an important future direction not only for summarization evaluation but also as
a testbed for long-context understanding. Finally, we move beyond faithfulness
by exploring content selection errors in book-length summarization: we develop
a typology of omission errors related to crucial narrative elements and also
identify a systematic over-emphasis on events occurring towards the end of the
book.
\\ ( https://arxiv.org/abs/2404.01261 ,  20930kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01263
Date: Mon, 1 Apr 2024 17:35:57 GMT   (1508kb)

Title: Artificial Intelligence and the Spatial Documentation of Languages
Authors: Hakam Ghanim
Categories: cs.CL
Comments: 29 pages, 1 figure, 16 maps
\\
  The advancement in technology has made interdisciplinary research more
accessible. Particularly the breakthrough in Artificial Intelligence AI has
given huge advantages to researchers working in interdisciplinary and
multidisciplinary fields. This study investigates the ability of AI models,
particularly GPT4 and GPT Data Analyst in creating language maps for language
documentation. The study Integrates documentary linguistics linguistic
geography and AI by showcasing how AI models facilitate the spatial
documentation of languages through the creation of language maps with minimal
cartographic expertise. The study is conducted using a CSV file and a GeoJSON
file both obtained from HDX and from the researchers fieldwork. The study data
is then applied in realtime conversations with the AI models in order to
generate the language distribution maps. The study highlights the two AI models
capabilities in generating highquality static and interactive web maps and
streamlining the mapmaking process, despite facing challenges like
inconsistencies and difficulties in adding legends. The findings suggest a
promising future for AI in generating language maps and enhancing the work of
documentary linguists as they collect their data in the field pointing towards
the need for further development to fully harness AI potential in this field.
\\ ( https://arxiv.org/abs/2404.01263 ,  1508kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01268
Date: Mon, 1 Apr 2024 17:45:15 GMT   (3875kb,D)

Title: Mapping the Increasing Use of LLMs in Scientific Papers
Authors: Weixin Liang, Yaohui Zhang, Zhengxuan Wu, Haley Lepp, Wenlong Ji,
  Xuandong Zhao, Hancheng Cao, Sheng Liu, Siyu He, Zhi Huang, Diyi Yang,
  Christopher Potts, Christopher D Manning, James Y. Zou
Categories: cs.CL cs.AI cs.DL cs.LG cs.SI
\\
  Scientific publishing lays the foundation of science by disseminating
research findings, fostering collaboration, encouraging reproducibility, and
ensuring that scientific knowledge is accessible, verifiable, and built upon
over time. Recently, there has been immense speculation about how many people
are using large language models (LLMs) like ChatGPT in their academic writing,
and to what extent this tool might have an effect on global scientific
practices. However, we lack a precise measure of the proportion of academic
writing substantially modified or produced by LLMs. To address this gap, we
conduct the first systematic, large-scale analysis across 950,965 papers
published between January 2020 and February 2024 on the arXiv, bioRxiv, and
Nature portfolio journals, using a population-level statistical framework to
measure the prevalence of LLM-modified content over time. Our statistical
estimation operates on the corpus level and is more robust than inference on
individual instances. Our findings reveal a steady increase in LLM usage, with
the largest and fastest growth observed in Computer Science papers (up to
17.5%). In comparison, Mathematics papers and the Nature portfolio showed the
least LLM modification (up to 6.3%). Moreover, at an aggregate level, our
analysis reveals that higher levels of LLM-modification are associated with
papers whose first authors post preprints more frequently, papers in more
crowded research areas, and papers of shorter lengths. Our findings suggests
that LLMs are being broadly used in scientific writings.
\\ ( https://arxiv.org/abs/2404.01268 ,  3875kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01288
Date: Mon, 1 Apr 2024 17:56:30 GMT   (2483kb,D)

Title: Large Language Models are Capable of Offering Cognitive Reappraisal, if
  Guided
Authors: Hongli Zhan, Allen Zheng, Yoon Kyung Lee, Jina Suh, Junyi Jessy Li,
  Desmond C. Ong
Categories: cs.CL
\\
  Large language models (LLMs) have offered new opportunities for emotional
support, and recent work has shown that they can produce empathic responses to
people in distress. However, long-term mental well-being requires emotional
self-regulation, where a one-time empathic response falls short. This work
takes a first step by engaging with cognitive reappraisals, a strategy from
psychology practitioners that uses language to targetedly change negative
appraisals that an individual makes of the situation; such appraisals is known
to sit at the root of human emotional experience. We hypothesize that
psychologically grounded principles could enable such advanced psychology
capabilities in LLMs, and design RESORT which consists of a series of
reappraisal constitutions across multiple dimensions that can be used as LLM
instructions. We conduct a first-of-its-kind expert evaluation (by clinical
psychologists with M.S. or Ph.D. degrees) of an LLM's zero-shot ability to
generate cognitive reappraisal responses to medium-length social media messages
asking for support. This fine-grained evaluation showed that even LLMs at the
7B scale guided by RESORT are capable of generating empathic responses that can
help users reappraise their situations.
\\ ( https://arxiv.org/abs/2404.01288 ,  2483kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01295
Date: Mon, 1 Apr 2024 17:59:06 GMT   (2559kb,D)

Title: Towards Safety and Helpfulness Balanced Responses via Controllable Large
  Language Models
Authors: Yi-Lin Tuan, Xilun Chen, Eric Michael Smith, Louis Martin, Soumya
  Batra, Asli Celikyilmaz, William Yang Wang, Daniel M. Bikel
Categories: cs.CL cs.AI
\\
  As large language models (LLMs) become easily accessible nowadays, the
trade-off between safety and helpfulness can significantly impact user
experience. A model that prioritizes safety will cause users to feel less
engaged and assisted while prioritizing helpfulness will potentially cause
harm. Possible harms include teaching people how to build a bomb, exposing
youth to inappropriate content, and hurting users' mental health. In this work,
we propose to balance safety and helpfulness in diverse use cases by
controlling both attributes in LLM. We explore training-free and fine-tuning
methods that do not require extra human annotations and analyze the challenges
of controlling safety and helpfulness in LLMs. Our experiments demonstrate that
our method can rewind a learned model and unlock its controllability.
\\ ( https://arxiv.org/abs/2404.01295 ,  2559kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00013
Date: Fri, 15 Mar 2024 13:01:09 GMT   (2601kb,D)

Title: Missing Data Imputation With Granular Semantics and AI-driven Pipeline
  for Bankruptcy Prediction
Authors: Debarati Chakraborty and Ravi Ranjan
Categories: cs.LG cs.AI q-fin.ST stat.AP
Comments: 15 pages
\\
  This work focuses on designing a pipeline for the prediction of bankruptcy.
The presence of missing values, high dimensional data, and highly
class-imbalance databases are the major challenges in the said task. A new
method for missing data imputation with granular semantics has been introduced
here. The merits of granular computing have been explored here to define this
method. The missing values have been predicted using the feature semantics and
reliable observations in a low-dimensional space, in the granular space. The
granules are formed around every missing entry, considering a few of the highly
correlated features and most reliable closest observations to preserve the
relevance and reliability, the context, of the database against the missing
entries. An intergranular prediction is then carried out for the imputation
within those contextual granules. That is, the contextual granules enable a
small relevant fraction of the huge database to be used for imputation and
overcome the need to access the entire database repetitively for each missing
value. This method is then implemented and tested for the prediction of
bankruptcy with the Polish Bankruptcy dataset. It provides an efficient
solution for big and high-dimensional datasets even with large imputation
rates. Then an AI-driven pipeline for bankruptcy prediction has been designed
using the proposed granular semantic-based data filling method followed by the
solutions to the issues like high dimensional dataset and high class-imbalance
in the dataset. The rest of the pipeline consists of feature selection with the
random forest for reducing dimensionality, data balancing with SMOTE, and
prediction with six different popular classifiers including deep NN. All
methods defined here have been experimentally verified with suitable
comparative studies and proven to be effective on all the data sets captured
over the five years.
\\ ( https://arxiv.org/abs/2404.00013 ,  2601kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00069
Date: Thu, 28 Mar 2024 14:44:44 GMT   (2745kb,D)

Title: A Two-Phase Recall-and-Select Framework for Fast Model Selection
Authors: Jianwei Cui, Wenhang Shi, Honglin Tao, Wei Lu, Xiaoyong Du
Categories: cs.LG
\\
  As the ubiquity of deep learning in various machine learning applications has
amplified, a proliferation of neural network models has been trained and shared
on public model repositories. In the context of a targeted machine learning
assignment, utilizing an apt source model as a starting point typically
outperforms the strategy of training from scratch, particularly with limited
training data. Despite the investigation and development of numerous model
selection strategies in prior work, the process remains time-consuming,
especially given the ever-increasing scale of model repositories. In this
paper, we propose a two-phase (coarse-recall and fine-selection) model
selection framework, aiming to enhance the efficiency of selecting a robust
model by leveraging the models' training performances on benchmark datasets.
Specifically, the coarse-recall phase clusters models showcasing similar
training performances on benchmark datasets in an offline manner. A
light-weight proxy score is subsequently computed between this model cluster
and the target dataset, which serves to recall a significantly smaller subset
of potential candidate models in a swift manner. In the following
fine-selection phase, the final model is chosen by fine-tuning the recalled
models on the target dataset with successive halving. To accelerate the
process, the final fine-tuning performance of each potential model is predicted
by mining the model's convergence trend on the benchmark datasets, which aids
in filtering lower performance models more earlier during fine-tuning. Through
extensive experimentation on tasks covering natural language processing and
computer vision, it has been demonstrated that the proposed methodology
facilitates the selection of a high-performing model at a rate about 3x times
faster than conventional baseline methods. Our code is available at
https://github.com/plasware/two-phase-selection.
\\ ( https://arxiv.org/abs/2404.00069 ,  2745kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00074
Date: Thu, 28 Mar 2024 19:57:48 GMT   (18058kb,D)

Title: A finite operator learning technique for mapping the elastic properties
  of microstructures to their mechanical deformations
Authors: Shahed Rezaei, Shirko Faroughi, Mahdi Asgharzadeh, Ali Harandi,
  Gottfried Laschet, Stefanie Reese, Markus Apel
Categories: cs.LG cs.CE cs.NA math.NA
\\
  To develop faster solvers for governing physical equations in solid
mechanics, we introduce a method that parametrically learns the solution to
mechanical equilibrium. The introduced method outperforms traditional ones in
terms of computational cost while acceptably maintaining accuracy. Moreover, it
generalizes and enhances the standard physics-informed neural networks to learn
a parametric solution with rather sharp discontinuities. We focus on
micromechanics as an example, where the knowledge of the micro-mechanical
solution, i.e., deformation and stress fields for a given heterogeneous
microstructure, is crucial. The parameter under investigation is the Young
modulus distribution within the heterogeneous solid system. Our method,
inspired by operator learning and the finite element method, demonstrates the
ability to train without relying on data from other numerical solvers. Instead,
we leverage ideas from the finite element approach to efficiently set up loss
functions algebraically, particularly based on the discretized weak form of the
governing equations. Notably, our investigations reveal that physics-based
training yields higher accuracy compared to purely data-driven approaches for
unseen microstructures. In essence, this method achieves independence from data
and enhances accuracy for predictions beyond the training range. The
aforementioned observations apply here to heterogeneous elastic
microstructures. Comparisons are also made with other well-known operator
learning algorithms, such as DeepOnet, to further emphasize the advantages of
the newly proposed architecture.
\\ ( https://arxiv.org/abs/2404.00074 ,  18058kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00075
Date: Thu, 28 Mar 2024 20:17:58 GMT   (2877kb,D)

Title: BEACON: Bayesian Experimental design Acceleration with Conditional
  Normalizing flows $-$ a case study in optimal monitor well placement for
  CO$_2$ sequestration
Authors: Rafael Orozco, Abhinav Gahlot, Felix J. Herrmann
Categories: cs.LG math-ph math.MP
\\
  CO$_2$ sequestration is a crucial engineering solution for mitigating climate
change. However, the uncertain nature of reservoir properties, necessitates
rigorous monitoring of CO$_2$ plumes to prevent risks such as leakage, induced
seismicity, or breaching licensed boundaries. To address this, project managers
use borehole wells for direct CO$_2$ and pressure monitoring at specific
locations. Given the high costs associated with drilling, it is crucial to
strategically place a limited number of wells to ensure maximally effective
monitoring within budgetary constraints. Our approach for selecting well
locations integrates fluid-flow solvers for forecasting plume trajectories with
generative neural networks for plume inference uncertainty. Our methodology is
extensible to three-dimensional domains and is developed within a Bayesian
framework for optimal experimental design, ensuring scalability and
mathematical optimality. We use a realistic case study to verify these claims
by demonstrating our method's application in a large scale domain and optimal
performance as compared to baseline well placement.
\\ ( https://arxiv.org/abs/2404.00075 ,  2877kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00085
Date: Fri, 29 Mar 2024 17:32:42 GMT   (1058kb,D)

Title: Bayesian Nonparametrics: An Alternative to Deep Learning
Authors: Bahman Moraffah
Categories: cs.LG stat.ML
\\
  Bayesian nonparametric models offer a flexible and powerful framework for
statistical model selection, enabling the adaptation of model complexity to the
intricacies of diverse datasets. This survey intends to delve into the
significance of Bayesian nonparametrics, particularly in addressing complex
challenges across various domains such as statistics, computer science, and
electrical engineering. By elucidating the basic properties and theoretical
foundations of these nonparametric models, this survey aims to provide a
comprehensive understanding of Bayesian nonparametrics and their relevance in
addressing complex problems, particularly in the domain of multi-object
tracking. Through this exploration, we uncover the versatility and efficacy of
Bayesian nonparametric methodologies, paving the way for innovative solutions
to intricate challenges across diverse disciplines.
\\ ( https://arxiv.org/abs/2404.00085 ,  1058kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00103
Date: Fri, 29 Mar 2024 18:23:34 GMT   (1074kb,D)

Title: PikeLPN: Mitigating Overlooked Inefficiencies of Low-Precision Neural
  Networks
Authors: Marina Neseem, Conor McCullough, Randy Hsin, Chas Leichner, Shan Li,
  In Suk Chong, Andrew G. Howard, Lukasz Lew, Sherief Reda, Ville-Mikko Rautio,
  Daniele Moro
Categories: cs.LG cs.CV
Comments: Accepted in CVPR 2024. 10 Figures, 9 Tables
\\
  Low-precision quantization is recognized for its efficacy in neural network
optimization. Our analysis reveals that non-quantized elementwise operations
which are prevalent in layers such as parameterized activation functions, batch
normalization, and quantization scaling dominate the inference cost of
low-precision models. These non-quantized elementwise operations are commonly
overlooked in SOTA efficiency metrics such as Arithmetic Computation Effort
(ACE). In this paper, we propose ACEv2 - an extended version of ACE which
offers a better alignment with the inference cost of quantized models and their
energy consumption on ML hardware. Moreover, we introduce PikeLPN, a model that
addresses these efficiency issues by applying quantization to both elementwise
operations and multiply-accumulate operations. In particular, we present a
novel quantization technique for batch normalization layers named QuantNorm
which allows for quantizing the batch normalization parameters without
compromising the model performance. Additionally, we propose applying Double
Quantization where the quantization scaling parameters are quantized.
Furthermore, we recognize and resolve the issue of distribution mismatch in
Separable Convolution layers by introducing Distribution-Heterogeneous
Quantization which enables quantizing them to low-precision. PikeLPN achieves
Pareto-optimality in efficiency-accuracy trade-off with up to 3X efficiency
improvement compared to SOTA low-precision models.
\\ ( https://arxiv.org/abs/2404.00103 ,  1074kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00162
Date: Fri, 29 Mar 2024 21:37:23 GMT   (10696kb)

Title: Modeling Large-Scale Walking and Cycling Networks: A Machine Learning
  Approach Using Mobile Phone and Crowdsourced Data
Authors: Meead Saberi and Tanapon Lilasathapornkit
Categories: cs.LG cs.NA math.NA
Comments: 22 pages, 8 figures, 13 tables
\\
  Walking and cycling are known to bring substantial health, environmental, and
economic advantages. However, the development of evidence-based active
transportation planning and policies has been impeded by significant data
limitations, such as biases in crowdsourced data and representativeness issues
of mobile phone data. In this study, we develop and apply a machine learning
based modeling approach for estimating daily walking and cycling volumes across
a large-scale regional network in New South Wales, Australia that includes
188,999 walking links and 114,885 cycling links. The modeling methodology
leverages crowdsourced and mobile phone data as well as a range of other
datasets on population, land use, topography, climate, etc. The study discusses
the unique challenges and limitations related to all three aspects of model
training, testing, and inference given the large geographical extent of the
modeled networks and relative scarcity of observed walking and cycling count
data. The study also proposes a new technique to identify model estimate
outliers and to mitigate their impact. Overall, the study provides a valuable
resource for transportation modelers, policymakers and urban planners seeking
to enhance active transportation infrastructure planning and policies with
advanced emerging data-driven modeling methodologies.
\\ ( https://arxiv.org/abs/2404.00162 ,  10696kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00173
Date: Fri, 29 Mar 2024 22:05:26 GMT   (3429kb,D)

Title: Comparing Hyper-optimized Machine Learning Models for Predicting
  Efficiency Degradation in Organic Solar Cells
Authors: David Valientea, Fernando Rodr\'iguez-Mas, Juan V. Alegre-Requena,
  David Dalmau, Juan C. Ferrer
Categories: cs.LG
\\
  This work presents a set of optimal machine learning (ML) models to represent
the temporal degradation suffered by the power conversion efficiency (PCE) of
polymeric organic solar cells (OSCs) with a multilayer structure
ITO/PEDOT:PSS/P3HT:PCBM/Al. To that aim, we generated a database with 996
entries, which includes up to 7 variables regarding both the manufacturing
process and environmental conditions for more than 180 days. Then, we relied on
a software framework that brings together a conglomeration of automated ML
protocols that execute sequentially against our database by simply command-line
interface. This easily permits hyper-optimizing and randomizing seeds of the ML
models through exhaustive benchmarking so that optimal models are obtained. The
accuracy achieved reaches values of the coefficient determination (R2) widely
exceeding 0.90, whereas the root mean squared error (RMSE), sum of squared
error (SSE), and mean absolute error (MAE)>1% of the target value, the PCE.
Additionally, we contribute with validated models able to screen the behavior
of OSCs never seen in the database. In that case, R2~0.96-0.97 and RMSE~1%,
thus confirming the reliability of the proposal to predict. For comparative
purposes, classical Bayesian regression fitting based on non-linear mean
squares (LMS) are also presented, which only perform sufficiently for
univariate cases of single OSCs. Hence they fail to outperform the breadth of
the capabilities shown by the ML models. Finally, thanks to the standardized
results offered by the ML framework, we study the dependencies between the
variables of the dataset and their implications for the optimal performance and
stability of the OSCs. Reproducibility is ensured by a standardized report
altogether with the dataset, which are publicly available at Github.
\\ ( https://arxiv.org/abs/2404.00173 ,  3429kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00195
Date: Fri, 29 Mar 2024 23:55:25 GMT   (30kb)

Title: Multiple-policy Evaluation via Density Estimation
Authors: Yilei Chen, Aldo Pacchiano, Ioannis Ch. Paschalidis
Categories: cs.LG cs.AI
\\
  In this work, we focus on the multiple-policy evaluation problem where we are
given a set of $K$ target policies and the goal is to evaluate their
performance (the expected total rewards) to an accuracy $\epsilon$ with
probability at least $1-\delta$. We propose an algorithm named
$\mathrm{CAESAR}$ to address this problem. Our approach is based on computing
an approximate optimal offline sampling distribution and using the data sampled
from it to perform the simultaneous estimation of the policy values.
$\mathrm{CAESAR}$ consists of two phases. In the first one we produce coarse
estimates of the vistation distributions of the target policies at a low order
sample complexity rate that scales with $\tilde{O}(\frac{1}{\epsilon})$. In the
second phase, we approximate the optimal offline sampling distribution and
compute the importance weighting ratios for all target policies by minimizing a
step-wise quadratic loss function inspired by the objective in DualDICE. Up to
low order and logarithm terms $\mathrm{CAESAR}$ achieves a sample complexity
$\tilde{O}\left(\frac{H^4}{\epsilon^2}\sum_{h=1}^H\max_{k\in[K]}\sum_{s,a}\frac{(d_h^{\pi^k}(s,a))^2}{\mu^*_h(s,a)}\right)$,
where $d^{\pi}$ is the visitation distribution of policy $\pi$ and $\mu^*$ is
the optimal sampling distribution.
\\ ( https://arxiv.org/abs/2404.00195 ,  30kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00225
Date: Sat, 30 Mar 2024 02:55:49 GMT   (131kb)

Title: Heterogeneous Contrastive Learning for Foundation Models and Beyond
Authors: Lecheng Zheng, Baoyu Jing, Zihao Li, Hanghang Tong, Jingrui He
Categories: cs.LG
\\
  In the era of big data and Artificial Intelligence, an emerging paradigm is
to utilize contrastive self-supervised learning to model large-scale
heterogeneous data. Many existing foundation models benefit from the
generalization capability of contrastive self-supervised learning by learning
compact and high-quality representations without relying on any label
information. Amidst the explosive advancements in foundation models across
multiple domains, including natural language processing and computer vision, a
thorough survey on heterogeneous contrastive learning for the foundation model
is urgently needed. In response, this survey critically evaluates the current
landscape of heterogeneous contrastive learning for foundation models,
highlighting the open challenges and future trends of contrastive learning. In
particular, we first present how the recent advanced contrastive learning-based
methods deal with view heterogeneity and how contrastive learning is applied to
train and fine-tune the multi-view foundation models. Then, we move to
contrastive learning methods for task heterogeneity, including pretraining
tasks and downstream tasks, and show how different tasks are combined with
contrastive learning loss for different purposes. Finally, we conclude this
survey by discussing the open challenges and shedding light on the future
directions of contrastive learning.
\\ ( https://arxiv.org/abs/2404.00225 ,  131kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00228
Date: Sat, 30 Mar 2024 03:16:37 GMT   (4754kb,D)

Title: InfLoRA: Interference-Free Low-Rank Adaptation for Continual Learning
Authors: Yan-Shuo Liang, Wu-Jun Li
Categories: cs.LG cs.AI cs.CV
Comments: Accepted by the 2024 IEEE/CVF Conference on Computer Vision and
  Pattern Recognition (CVPR 2024)
\\
  Continual learning requires the model to learn multiple tasks sequentially.
In continual learning, the model should possess the ability to maintain its
performance on old tasks (stability) and the ability to adapt to new tasks
continuously (plasticity). Recently, parameter-efficient fine-tuning (PEFT),
which involves freezing a pre-trained model and injecting a small number of
learnable parameters to adapt to downstream tasks, has gained increasing
popularity in continual learning. Although existing continual learning methods
based on PEFT have demonstrated superior performance compared to those not
based on PEFT, most of them do not consider how to eliminate the interference
of the new task on the old tasks, which inhibits the model from making a good
trade-off between stability and plasticity. In this work, we propose a new PEFT
method, called interference-free low-rank adaptation (InfLoRA), for continual
learning. InfLoRA injects a small number of parameters to reparameterize the
pre-trained weights and shows that fine-tuning these injected parameters is
equivalent to fine-tuning the pre-trained weights within a subspace.
Furthermore, InfLoRA designs this subspace to eliminate the interference of the
new task on the old tasks, making a good trade-off between stability and
plasticity. Experimental results show that InfLoRA outperforms existing
state-of-the-art continual learning methods on multiple datasets.
\\ ( https://arxiv.org/abs/2404.00228 ,  4754kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00254
Date: Sat, 30 Mar 2024 05:51:09 GMT   (3257kb,D)

Title: Clustering for Protein Representation Learning
Authors: Ruijie Quan, Wenguan Wang, Fan Ma, Hehe Fan, Yi Yang
Categories: cs.LG cs.CE q-bio.BM q-bio.QM
Comments: Accepted to CVPR2024
\\
  Protein representation learning is a challenging task that aims to capture
the structure and function of proteins from their amino acid sequences.
Previous methods largely ignored the fact that not all amino acids are equally
important for protein folding and activity. In this article, we propose a
neural clustering framework that can automatically discover the critical
components of a protein by considering both its primary and tertiary structure
information. Our framework treats a protein as a graph, where each node
represents an amino acid and each edge represents a spatial or sequential
connection between amino acids. We then apply an iterative clustering strategy
to group the nodes into clusters based on their 1D and 3D positions and assign
scores to each cluster. We select the highest-scoring clusters and use their
medoid nodes for the next iteration of clustering, until we obtain a
hierarchical and informative representation of the protein. We evaluate on four
protein-related tasks: protein fold classification, enzyme reaction
classification, gene ontology term prediction, and enzyme commission number
prediction. Experimental results demonstrate that our method achieves
state-of-the-art performance.
\\ ( https://arxiv.org/abs/2404.00254 ,  3257kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00271
Date: Sat, 30 Mar 2024 07:25:30 GMT   (10378kb,D)

Title: TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph
  Convolution Networks for Efficient Neural Architecture Search
Authors: Ye Qiao, Haocheng Xu, and Sitao Huang
Categories: cs.LG cs.AI
\\
  Neural architecture search (NAS) is an effective method for discovering new
convolutional neural network (CNN) architectures. However, existing approaches
often require time-consuming training or intensive sampling and evaluations.
Zero-shot NAS aims to create training-free proxies for architecture performance
prediction. However, existing proxies have suboptimal performance, and are
often outperformed by simple metrics such as model parameter counts or the
number of floating-point operations. Besides, existing model-based proxies
cannot be generalized to new search spaces with unseen new types of operators
without golden accuracy truth. A universally optimal proxy remains elusive. We
introduce TG-NAS, a novel model-based universal proxy that leverages a
transformer-based operator embedding generator and a graph convolution network
(GCN) to predict architecture performance. This approach guides neural
architecture search across any given search space without the need of
retraining. Distinct from other model-based predictor subroutines, TG-NAS
itself acts as a zero-cost (ZC) proxy, guiding architecture search with
advantages in terms of data independence, cost-effectiveness, and consistency
across diverse search spaces. Our experiments showcase its advantages over
existing proxies across various NAS benchmarks, suggesting its potential as a
foundational element for efficient architecture search. TG-NAS achieves up to
300X improvements in search efficiency compared to previous SOTA ZC proxy
methods. Notably, it discovers competitive models with 93.75% CIFAR-10 accuracy
on the NAS-Bench-201 space and 74.5% ImageNet top-1 accuracy on the DARTS
space.
\\ ( https://arxiv.org/abs/2404.00271 ,  10378kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00282
Date: Sat, 30 Mar 2024 08:28:08 GMT   (679kb,D)

Title: Survey on Large Language Model-Enhanced Reinforcement Learning: Concept,
  Taxonomy, and Methods
Authors: Yuji Cao, Huan Zhao, Yuheng Cheng, Ting Shu, Guolong Liu, Gaoqi Liang,
  Junhua Zhao, Yun Li
Categories: cs.LG cs.AI cs.CL cs.RO
Comments: 16 pages (including bibliography), 6 figures
\\
  With extensive pre-trained knowledge and high-level general capabilities,
large language models (LLMs) emerge as a promising avenue to augment
reinforcement learning (RL) in aspects such as multi-task learning, sample
efficiency, and task planning. In this survey, we provide a comprehensive
review of the existing literature in $\textit{LLM-enhanced RL}$ and summarize
its characteristics compared to conventional RL methods, aiming to clarify the
research scope and directions for future studies. Utilizing the classical
agent-environment interaction paradigm, we propose a structured taxonomy to
systematically categorize LLMs' functionalities in RL, including four roles:
information processor, reward designer, decision-maker, and generator.
Additionally, for each role, we summarize the methodologies, analyze the
specific RL challenges that are mitigated, and provide insights into future
directions. Lastly, potential applications, prospective opportunities and
challenges of the $\textit{LLM-enhanced RL}$ are discussed.
\\ ( https://arxiv.org/abs/2404.00282 ,  679kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00357
Date: Sat, 30 Mar 2024 13:18:27 GMT   (791kb,D)

Title: Revisiting Random Weight Perturbation for Efficiently Improving
  Generalization
Authors: Tao Li, Qinghua Tao, Weihao Yan, Zehao Lei, Yingwen Wu, Kun Fang,
  Mingzhen He, Xiaolin Huang
Categories: cs.LG
Comments: Accepted to TMLR 2024
\\
  Improving the generalization ability of modern deep neural networks (DNNs) is
a fundamental challenge in machine learning. Two branches of methods have been
proposed to seek flat minima and improve generalization: one led by
sharpness-aware minimization (SAM) minimizes the worst-case neighborhood loss
through adversarial weight perturbation (AWP), and the other minimizes the
expected Bayes objective with random weight perturbation (RWP). While RWP
offers advantages in computation and is closely linked to AWP on a mathematical
basis, its empirical performance has consistently lagged behind that of AWP. In
this paper, we revisit the use of RWP for improving generalization and propose
improvements from two perspectives: i) the trade-off between generalization and
convergence and ii) the random perturbation generation. Through extensive
experimental evaluations, we demonstrate that our enhanced RWP methods achieve
greater efficiency in enhancing generalization, particularly in large-scale
problems, while also offering comparable or even superior performance to SAM.
The code is released at https://github.com/nblt/mARWP.
\\ ( https://arxiv.org/abs/2404.00357 ,  791kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00371
Date: Sat, 30 Mar 2024 13:49:59 GMT   (2608kb,D)

Title: From Learning to Analytics: Improving Model Efficacy with Goal-Directed
  Client Selection
Authors: Jingwen Tong, Zhenzhen Chen, Liqun Fu, Jun Zhang and Zhu Han
Categories: cs.LG eess.SP
Comments: This work was partly presented at IEEE ICC 2022
MSC-class: 14J60
ACM-class: I.2.7
DOI: 10.1109/TMC.2024.3383038
\\
  Federated learning (FL) is an appealing paradigm for learning a global model
among distributed clients while preserving data privacy. Driven by the demand
for high-quality user experiences, evaluating the well-trained global model
after the FL process is crucial. In this paper, we propose a closed-loop model
analytics framework that allows for effective evaluation of the trained global
model using clients' local data. To address the challenges posed by system and
data heterogeneities in the FL process, we study a goal-directed client
selection problem based on the model analytics framework by selecting a subset
of clients for the model training. This problem is formulated as a stochastic
multi-armed bandit (SMAB) problem. We first put forth a quick initial upper
confidence bound (Quick-Init UCB) algorithm to solve this SMAB problem under
the federated analytics (FA) framework. Then, we further propose a belief
propagation-based UCB (BP-UCB) algorithm under the democratized analytics (DA)
framework. Moreover, we derive two regret upper bounds for the proposed
algorithms, which increase logarithmically over the time horizon. The numerical
results demonstrate that the proposed algorithms achieve nearly optimal
performance, with a gap of less than 1.44% and 3.12% under the FA and DA
frameworks, respectively.
\\ ( https://arxiv.org/abs/2404.00371 ,  2608kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00408
Date: Sat, 30 Mar 2024 16:34:28 GMT   (12521kb,D)

Title: Deep Learning with Parametric Lenses
Authors: Geoffrey S. H. Cruttwell, Bruno Gavranovic, Neil Ghani, Paul Wilson,
  Fabio Zanasi
Categories: cs.LG cs.LO
Comments: arXiv admin note: text overlap with arXiv:2403.13001
\\
  We propose a categorical semantics for machine learning algorithms in terms
of lenses, parametric maps, and reverse derivative categories. This foundation
provides a powerful explanatory and unifying framework: it encompasses a
variety of gradient descent algorithms such as ADAM, AdaGrad, and Nesterov
momentum, as well as a variety of loss functions such as MSE and Softmax
cross-entropy, and different architectures, shedding new light on their
similarities and differences. Furthermore, our approach to learning has
examples generalising beyond the familiar continuous domains (modelled in
categories of smooth maps) and can be realised in the discrete setting of
Boolean and polynomial circuits. We demonstrate the practical significance of
our framework with an implementation in Python.
\\ ( https://arxiv.org/abs/2404.00408 ,  12521kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00417
Date: Sat, 30 Mar 2024 16:53:10 GMT   (4042kb,D)

Title: Orchestrate Latent Expertise: Advancing Online Continual Learning with
  Multi-Level Supervision and Reverse Self-Distillation
Authors: HongWei Yan, Liyuan Wang, Kaisheng Ma, Yi Zhong
Categories: cs.LG cs.AI cs.CV
Comments: CVPR 2024
\\
  To accommodate real-world dynamics, artificial intelligence systems need to
cope with sequentially arriving content in an online manner. Beyond regular
Continual Learning (CL) attempting to address catastrophic forgetting with
offline training of each task, Online Continual Learning (OCL) is a more
challenging yet realistic setting that performs CL in a one-pass data stream.
Current OCL methods primarily rely on memory replay of old training samples.
However, a notable gap from CL to OCL stems from the additional
overfitting-underfitting dilemma associated with the use of rehearsal buffers:
the inadequate learning of new training samples (underfitting) and the repeated
learning of a few old training samples (overfitting). To this end, we introduce
a novel approach, Multi-level Online Sequential Experts (MOSE), which
cultivates the model as stacked sub-experts, integrating multi-level
supervision and reverse self-distillation. Supervision signals across multiple
stages facilitate appropriate convergence of the new task while gathering
various strengths from experts by knowledge distillation mitigates the
performance decline of old tasks. MOSE demonstrates remarkable efficacy in
learning new samples and preserving past knowledge through multi-level experts,
thereby significantly advancing OCL performance over state-of-the-art baselines
(e.g., up to 7.3% on Split CIFAR-100 and 6.1% on Split Tiny-ImageNet).
\\ ( https://arxiv.org/abs/2404.00417 ,  4042kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00418
Date: Sat, 30 Mar 2024 16:54:35 GMT   (7631kb,D)

Title: Continual Learning for Autonomous Robots: A Prototype-based Approach
Authors: Elvin Hajizada, Balachandran Swaminathan, Yulia Sandamirskaya
Categories: cs.LG cs.CV cs.RO
Comments: Submitted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)
\\
  Humans and animals learn throughout their lives from limited amounts of
sensed data, both with and without supervision. Autonomous, intelligent robots
of the future are often expected to do the same. The existing continual
learning (CL) methods are usually not directly applicable to robotic settings:
they typically require buffering and a balanced replay of training data. A
few-shot online continual learning (FS-OCL) setting has been proposed to
address more realistic scenarios where robots must learn from a non-repeated
sparse data stream. To enable truly autonomous life-long learning, an
additional challenge of detecting novelties and learning new items without
supervision needs to be addressed. We address this challenge with our new
prototype-based approach called Continually Learning Prototypes (CLP). In
addition to being capable of FS-OCL learning, CLP also detects novel objects
and learns them without supervision. To mitigate forgetting, CLP utilizes a
novel metaplasticity mechanism that adapts the learning rate individually per
prototype. CLP is rehearsal-free, hence does not require a memory buffer, and
is compatible with neuromorphic hardware, characterized by ultra-low power
consumption, real-time processing abilities, and on-chip learning. Indeed, we
have open-sourced a simple version of CLP in the neuromorphic software
framework Lava, targetting Intel's neuromorphic chip Loihi 2. We evaluate CLP
on a robotic vision dataset, OpenLORIS. In a low-instance FS-OCL scenario, CLP
shows state-of-the-art results. In the open world, CLP detects novelties with
superior precision and recall and learns features of the detected novel classes
without supervision, achieving a strong baseline of 99% base class and 65%/76%
(5-shot/10-shot) novel class accuracy.
\\ ( https://arxiv.org/abs/2404.00418 ,  7631kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00456
Date: Sat, 30 Mar 2024 19:20:06 GMT   (880kb,D)

Title: QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs
Authors: Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L. Croci, Bo Li,
  Martin Jaggi, Dan Alistarh, Torsten Hoefler, James Hensman
Categories: cs.LG
Comments: 19 pages, 6 figures
\\
  We introduce QuaRot, a new Quantization scheme based on Rotations, which is
able to quantize LLMs end-to-end, including all weights, activations, and KV
cache in 4 bits. QuaRot rotates LLMs in a way that removes outliers from the
hidden state without changing the output, making quantization easier. This
computational invariance is applied to the hidden state (residual) of the LLM,
as well as to the activations of the feed-forward components, aspects of the
attention mechanism and to the KV cache. The result is a quantized model where
all matrix multiplications are performed in 4-bits, without any channels
identified for retention in higher precision. Our quantized LLaMa2-70B model
has losses of at most 0.29 WikiText-2 perplexity and retains 99% of the
zero-shot performance. Code is available at: https://github.com/spcl/QuaRot.
\\ ( https://arxiv.org/abs/2404.00456 ,  880kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00461
Date: Sat, 30 Mar 2024 20:02:36 GMT   (958kb,D)

Title: Shortcuts Arising from Contrast: Effective and Covert Clean-Label
  Attacks in Prompt-Based Learning
Authors: Xiaopeng Xie, Ming Yan, Xiwen Zhou, Chenlong Zhao, Suli Wang, Yong
  Zhang, Joey Tianyi Zhou
Categories: cs.LG cs.AI cs.CL cs.CR
Comments: 10 pages, 6 figures, conference
MSC-class: 68T50
ACM-class: I.2.7
\\
  Prompt-based learning paradigm has demonstrated remarkable efficacy in
enhancing the adaptability of pretrained language models (PLMs), particularly
in few-shot scenarios. However, this learning paradigm has been shown to be
vulnerable to backdoor attacks. The current clean-label attack, employing a
specific prompt as a trigger, can achieve success without the need for external
triggers and ensure correct labeling of poisoned samples, which is more
stealthy compared to the poisoned-label attack, but on the other hand, it faces
significant issues with false activations and poses greater challenges,
necessitating a higher rate of poisoning. Using conventional negative data
augmentation methods, we discovered that it is challenging to trade off between
effectiveness and stealthiness in a clean-label setting. In addressing this
issue, we are inspired by the notion that a backdoor acts as a shortcut and
posit that this shortcut stems from the contrast between the trigger and the
data utilized for poisoning. In this study, we propose a method named
Contrastive Shortcut Injection (CSI), by leveraging activation values,
integrates trigger design and data selection strategies to craft stronger
shortcut features. With extensive experiments on full-shot and few-shot text
classification tasks, we empirically validate CSI's high effectiveness and high
stealthiness at low poisoning rates. Notably, we found that the two approaches
play leading roles in full-shot and few-shot settings, respectively.
\\ ( https://arxiv.org/abs/2404.00461 ,  958kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00462
Date: Sat, 30 Mar 2024 20:03:49 GMT   (3175kb,D)

Title: Zero-shot Safety Prediction for Autonomous Robots with Foundation World
  Models
Authors: Zhenjiang Mao, Siqi Dai, Yuang Geng and Ivan Ruchkin
Categories: cs.LG cs.RO
\\
  A world model creates a surrogate world to train a controller and predict
safety violations by learning the internal dynamic model of systems. However,
the existing world models rely solely on statistical learning of how
observations change in response to actions, lacking precise quantification of
how accurate the surrogate dynamics are, which poses a significant challenge in
safety-critical systems. To address this challenge, we propose foundation world
models that embed observations into meaningful and causally latent
representations. This enables the surrogate dynamics to directly predict causal
future states by leveraging a training-free large language model. In two common
benchmarks, this novel model outperforms standard world models in the safety
prediction task and has a performance comparable to supervised learning despite
not using any data. We evaluate its performance with a more specialized and
system-relevant metric by comparing estimated states instead of aggregating
observation-wide error.
\\ ( https://arxiv.org/abs/2404.00462 ,  3175kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00464
Date: Sat, 30 Mar 2024 20:11:34 GMT   (1136kb,D)

Title: Leveraging Pre-trained and Transformer-derived Embeddings from EHRs to
  Characterize Heterogeneity Across Alzheimer's Disease and Related Dementias
Authors: Matthew West, Colin Magdamo, Lily Cheng, Yingnan He, Sudeshna Das
Categories: cs.LG
Comments: 14 pages, 5 figures in main text
\\
  Alzheimer's disease is a progressive, debilitating neurodegenerative disease
that affects 50 million people globally. Despite this substantial health
burden, available treatments for the disease are limited and its fundamental
causes remain poorly understood. Previous work has suggested the existence of
clinically-meaningful sub-types, which it is suggested may correspond to
distinct etiologies, disease courses, and ultimately appropriate treatments.
Here, we use unsupervised learning techniques on electronic health records
(EHRs) from a cohort of memory disorder patients to characterise heterogeneity
in this disease population. Pre-trained embeddings for medical codes as well as
transformer-derived Clinical BERT embeddings of free text are used to encode
patient EHRs. We identify the existence of sub-populations on the basis of
comorbidities and shared textual features, and discuss their clinical
significance.
\\ ( https://arxiv.org/abs/2404.00464 ,  1136kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00466
Date: Sat, 30 Mar 2024 20:19:28 GMT   (62kb,D)

Title: Computation and Communication Efficient Lightweighting Vertical
  Federated Learning
Authors: Heqiang Wang, Jieming Bian, Lei Wang
Categories: cs.LG cs.DC
\\
  The exploration of computational and communication efficiency within
Federated Learning (FL) has emerged as a prominent and crucial field of study.
While most existing efforts to enhance these efficiencies have focused on
Horizontal FL, the distinct processes and model structures of Vertical FL
preclude the direct application of Horizontal FL-based techniques. In response,
we introduce the concept of Lightweight Vertical Federated Learning (LVFL),
targeting both computational and communication efficiencies. This approach
involves separate lightweighting strategies for the feature model, to improve
computational efficiency, and for feature embedding, to enhance communication
efficiency. Moreover, we establish a convergence bound for our LVFL algorithm,
which accounts for both communication and computational lightweighting ratios.
Our evaluation of the algorithm on a image classification dataset reveals that
LVFL significantly alleviates computational and communication demands while
preserving robust learning performance. This work effectively addresses the
gaps in communication and computational efficiency within Vertical FL.
\\ ( https://arxiv.org/abs/2404.00466 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00474
Date: Sat, 30 Mar 2024 20:47:55 GMT   (510kb,D)

Title: Linguistic Calibration of Language Models
Authors: Neil Band and Xuechen Li and Tengyu Ma and Tatsunori Hashimoto
Categories: cs.LG cs.AI cs.CL stat.ML
\\
  Language models (LMs) may lead their users to make suboptimal downstream
decisions when they confidently hallucinate. This issue can be mitigated by
having the LM verbally convey the probability that its claims are correct, but
existing models cannot produce text with calibrated confidence statements.
Through the lens of decision-making, we formalize linguistic calibration for
long-form generations: an LM is linguistically calibrated if its generations
enable its users to make calibrated probabilistic predictions. This definition
enables a training framework where a supervised finetuning step bootstraps an
LM to emit long-form generations with confidence statements such as "I estimate
a 30% chance of..." or "I am certain that...", followed by a reinforcement
learning step which rewards generations that enable a user to provide
calibrated answers to related questions. We linguistically calibrate Llama 2 7B
and find in automated and human evaluations of long-form generations that it is
significantly more calibrated than strong finetuned factuality baselines with
comparable accuracy. These findings generalize under distribution shift on
question-answering and under a significant task shift to person biography
generation. Our results demonstrate that long-form generations may be
calibrated end-to-end by constructing an objective in the space of the
predictions that users make in downstream decision-making.
\\ ( https://arxiv.org/abs/2404.00474 ,  510kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00477
Date: Sat, 30 Mar 2024 21:54:01 GMT   (1017kb,D)

Title: DE-HNN: An effective neural model for Circuit Netlist representation
Authors: Zhishang Luo, Truong Son Hy, Puoya Tabaghi, Donghyeon Koh, Michael
  Defferrard, Elahe Rezaei, Ryan Carey, Rhett Davis, Rajeev Jain, Yusu Wang
Categories: cs.LG cs.AR
\\
  The run-time for optimization tools used in chip design has grown with the
complexity of designs to the point where it can take several days to go through
one design cycle which has become a bottleneck. Designers want fast tools that
can quickly give feedback on a design. Using the input and output data of the
tools from past designs, one can attempt to build a machine learning model that
predicts the outcome of a design in significantly shorter time than running the
tool. The accuracy of such models is affected by the representation of the
design data, which is usually a netlist that describes the elements of the
digital circuit and how they are connected. Graph representations for the
netlist together with graph neural networks have been investigated for such
models. However, the characteristics of netlists pose several challenges for
existing graph learning frameworks, due to the large number of nodes and the
importance of long-range interactions between nodes. To address these
challenges, we represent the netlist as a directed hypergraph and propose a
Directional Equivariant Hypergraph Neural Network (DE-HNN) for the effective
learning of (directed) hypergraphs. Theoretically, we show that our DE-HNN can
universally approximate any node or hyperedge based function that satisfies
certain permutation equivariant and invariant properties natural for directed
hypergraphs. We compare the proposed DE-HNN with several State-of-the-art
(SOTA) machine learning models for (hyper)graphs and netlists, and show that
the DE-HNN significantly outperforms them in predicting the outcome of
optimized place-and-route tools directly from the input netlists. Our source
code and the netlists data used are publicly available at
https://github.com/YusuLab/chips.git
\\ ( https://arxiv.org/abs/2404.00477 ,  1017kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00498
Date: Sat, 30 Mar 2024 23:42:23 GMT   (1006kb,D)

Title: 94% on CIFAR-10 in 3.29 Seconds on a Single GPU
Authors: Keller Jordan
Categories: cs.LG cs.CV
\\
  CIFAR-10 is among the most widely used datasets in machine learning,
facilitating thousands of research projects per year. To accelerate research
and reduce the cost of experiments, we introduce training methods for CIFAR-10
which reach 94% accuracy in 3.29 seconds, 95% in 10.4 seconds, and 96% in 46.3
seconds, when run on a single NVIDIA A100 GPU. As one factor contributing to
these training speeds, we propose a derandomized variant of horizontal flipping
augmentation, which we show improves over the standard method in every case
where flipping is beneficial over no flipping at all. Our code is released at
https://github.com/KellerJordan/cifar10-airbench.
\\ ( https://arxiv.org/abs/2404.00498 ,  1006kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00502
Date: Sun, 31 Mar 2024 00:09:58 GMT   (6999kb,D)

Title: Conditional Pseudo-Reversible Normalizing Flow for Surrogate Modeling in
  Quantifying Uncertainty Propagation
Authors: Minglei Yang, Pengjun Wang, Ming Fan, Dan Lu, Yanzhao Cao, Guannan
  Zhang
Categories: cs.LG cs.NA math.NA
\\
  We introduce a conditional pseudo-reversible normalizing flow for
constructing surrogate models of a physical model polluted by additive noise to
efficiently quantify forward and inverse uncertainty propagation. Existing
surrogate modeling approaches usually focus on approximating the deterministic
component of physical model. However, this strategy necessitates knowledge of
noise and resorts to auxiliary sampling methods for quantifying inverse
uncertainty propagation. In this work, we develop the conditional
pseudo-reversible normalizing flow model to directly learn and efficiently
generate samples from the conditional probability density functions. The
training process utilizes dataset consisting of input-output pairs without
requiring prior knowledge about the noise and the function. Our model, once
trained, can generate samples from any conditional probability density
functions whose high probability regions are covered by the training set.
Moreover, the pseudo-reversibility feature allows for the use of
fully-connected neural network architectures, which simplifies the
implementation and enables theoretical analysis. We provide a rigorous
convergence analysis of the conditional pseudo-reversible normalizing flow
model, showing its ability to converge to the target conditional probability
density function using the Kullback-Leibler divergence. To demonstrate the
effectiveness of our method, we apply it to several benchmark tests and a
real-world geologic carbon storage problem.
\\ ( https://arxiv.org/abs/2404.00502 ,  6999kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00505
Date: Sun, 31 Mar 2024 00:22:36 GMT   (1744kb,D)

Title: Transfer Learning with Reconstruction Loss
Authors: Wei Cui and Wei Yu
Categories: cs.LG cs.AI cs.NI stat.ML
Comments: 16 pages, 5 figures. To appear in IEEE Transactions on Machine
  Learning in Communications and Networking (TMLCN)
\\
  In most applications of utilizing neural networks for mathematical
optimization, a dedicated model is trained for each specific optimization
objective. However, in many scenarios, several distinct yet correlated
objectives or tasks often need to be optimized on the same set of problem
inputs. Instead of independently training a different neural network for each
problem separately, it would be more efficient to exploit the correlations
between these objectives and to train multiple neural network models with
shared model parameters and feature representations. To achieve this, this
paper first establishes the concept of common information: the shared knowledge
required for solving the correlated tasks, then proposes a novel approach for
model training by adding into the model an additional reconstruction stage
associated with a new reconstruction loss. This loss is for reconstructing the
common information starting from a selected hidden layer in the model. The
proposed approach encourages the learned features to be general and
transferable, and therefore can be readily used for efficient transfer
learning. For numerical simulations, three applications are studied: transfer
learning on classifying MNIST handwritten digits, the device-to-device wireless
network power allocation, and the multiple-input-single-output network downlink
beamforming and localization. Simulation results suggest that the proposed
approach is highly efficient in data and model complexity, is resilient to
over-fitting, and has competitive performances.
\\ ( https://arxiv.org/abs/2404.00505 ,  1744kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00506
Date: Sun, 31 Mar 2024 00:29:00 GMT   (2036kb,D)

Title: Label-Agnostic Forgetting: A Supervision-Free Unlearning in Deep Models
Authors: Shaofei Shen, Chenhao Zhang, Yawen Zhao, Alina Bialkowski, Weitong
  Chen, Miao Xu
Categories: cs.LG
\\
  Machine unlearning aims to remove information derived from forgotten data
while preserving that of the remaining dataset in a well-trained model. With
the increasing emphasis on data privacy, several approaches to machine
unlearning have emerged. However, these methods typically rely on complete
supervision throughout the unlearning process. Unfortunately, obtaining such
supervision, whether for the forgetting or remaining data, can be impractical
due to the substantial cost associated with annotating real-world datasets.
This challenge prompts us to propose a supervision-free unlearning approach
that operates without the need for labels during the unlearning process.
Specifically, we introduce a variational approach to approximate the
distribution of representations for the remaining data. Leveraging this
approximation, we adapt the original model to eliminate information from the
forgotten data at the representation level. To further address the issue of
lacking supervision information, which hinders alignment with ground truth, we
introduce a contrastive loss to facilitate the matching of representations
between the remaining data and those of the original model, thus preserving
predictive performance. Experimental results across various unlearning tasks
demonstrate the effectiveness of our proposed method, Label-Agnostic Forgetting
(LAF) without using any labels, which achieves comparable performance to
state-of-the-art methods that rely on full supervision information.
Furthermore, our approach excels in semi-supervised scenarios, leveraging
limited supervision information to outperform fully supervised baselines. This
work not only showcases the viability of supervision-free unlearning in deep
models but also opens up a new possibility for future research in unlearning at
the representation level.
\\ ( https://arxiv.org/abs/2404.00506 ,  2036kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00509
Date: Sun, 31 Mar 2024 00:59:10 GMT   (41952kb,D)

Title: DailyMAE: Towards Pretraining Masked Autoencoders in One Day
Authors: Jiantao Wu, Shentong Mo, Sara Atito, Zhenhua Feng, Josef Kittler,
  Muhammad Awais
Categories: cs.LG cs.CV
\\
  Recently, masked image modeling (MIM), an important self-supervised learning
(SSL) method, has drawn attention for its effectiveness in learning data
representation from unlabeled data. Numerous studies underscore the advantages
of MIM, highlighting how models pretrained on extensive datasets can enhance
the performance of downstream tasks. However, the high computational demands of
pretraining pose significant challenges, particularly within academic
environments, thereby impeding the SSL research progress. In this study, we
propose efficient training recipes for MIM based SSL that focuses on mitigating
data loading bottlenecks and employing progressive training techniques and
other tricks to closely maintain pretraining performance. Our library enables
the training of a MAE-Base/16 model on the ImageNet 1K dataset for 800 epochs
within just 18 hours, using a single machine equipped with 8 A100 GPUs. By
achieving speed gains of up to 5.8 times, this work not only demonstrates the
feasibility of conducting high-efficiency SSL training but also paves the way
for broader accessibility and promotes advancement in SSL research particularly
for prototyping and initial testing of SSL ideas. The code is available in
https://github.com/erow/FastSSL.
\\ ( https://arxiv.org/abs/2404.00509 ,  41952kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00521
Date: Sun, 31 Mar 2024 01:41:36 GMT   (13944kb,D)

Title: CHAIN: Enhancing Generalization in Data-Efficient GANs via lipsCHitz
  continuity constrAIned Normalization
Authors: Yao Ni, Piotr Koniusz
Categories: cs.LG cs.CV
Comments: Accepted by CVPR2024, 26 pages full version
\\
  Generative Adversarial Networks (GANs) significantly advanced image
generation but their performance heavily depends on abundant training data. In
scenarios with limited data, GANs often struggle with discriminator overfitting
and unstable training. Batch Normalization (BN), despite being known for
enhancing generalization and training stability, has rarely been used in the
discriminator of Data-Efficient GANs. Our work addresses this gap by
identifying a critical flaw in BN: the tendency for gradient explosion during
the centering and scaling steps. To tackle this issue, we present CHAIN
(lipsCHitz continuity constrAIned Normalization), which replaces the
conventional centering step with zero-mean regularization and integrates a
Lipschitz continuity constraint in the scaling step. CHAIN further enhances GAN
training by adaptively interpolating the normalized and unnormalized features,
effectively avoiding discriminator overfitting. Our theoretical analyses firmly
establishes CHAIN's effectiveness in reducing gradients in latent features and
weights, improving stability and generalization in GAN training. Empirical
evidence supports our theory. CHAIN achieves state-of-the-art results in
data-limited scenarios on CIFAR-10/100, ImageNet, five low-shot and seven
high-resolution few-shot image datasets.
\\ ( https://arxiv.org/abs/2404.00521 ,  13944kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00522
Date: Sun, 31 Mar 2024 01:41:57 GMT   (254kb,D)

Title: Minimum-Norm Interpolation Under Covariate Shift
Authors: Neil Mallinar, Austin Zane, Spencer Frei, Bin Yu
Categories: cs.LG stat.ML
\\
  Transfer learning is a critical part of real-world machine learning
deployments and has been extensively studied in experimental works with
overparameterized neural networks. However, even in the simplest setting of
linear regression a notable gap still exists in the theoretical understanding
of transfer learning. In-distribution research on high-dimensional linear
regression has led to the identification of a phenomenon known as
\textit{benign overfitting}, in which linear interpolators overfit to noisy
training labels and yet still generalize well. This behavior occurs under
specific conditions on the source covariance matrix and input data dimension.
Therefore, it is natural to wonder how such high-dimensional linear models
behave under transfer learning. We prove the first non-asymptotic excess risk
bounds for benignly-overfit linear interpolators in the transfer learning
setting. From our analysis, we propose a taxonomy of \textit{beneficial} and
\textit{malignant} covariate shifts based on the degree of
overparameterization. We follow our analysis with empirical studies that show
these beneficial and malignant covariate shifts for linear interpolators on
real image data, and for fully-connected neural networks in settings where the
input data dimension is larger than the training sample size.
\\ ( https://arxiv.org/abs/2404.00522 ,  254kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00525
Date: Sun, 31 Mar 2024 01:58:38 GMT   (19835kb,D)

Title: Creating synthetic energy meter data using conditional diffusion and
  building metadata
Authors: Chun Fu, Hussain Kazmi, Matias Quintana, Clayton Miller
Categories: cs.LG cs.SY eess.SY
Comments: 17 pages, 11 figures, submitted to journal "Energy and Buildings"
\\
  Advances in machine learning and increased computational power have driven
progress in energy-related research. However, limited access to private energy
data from buildings hinders traditional regression models relying on historical
data. While generative models offer a solution, previous studies have primarily
focused on short-term generation periods (e.g., daily profiles) and a limited
number of meters. Thus, the study proposes a conditional diffusion model for
generating high-quality synthetic energy data using relevant metadata. Using a
dataset comprising 1,828 power meters from various buildings and countries,
this model is compared with traditional methods like Conditional Generative
Adversarial Networks (CGAN) and Conditional Variational Auto-Encoders (CVAE).
It explicitly handles long-term annual consumption profiles, harnessing
metadata such as location, weather, building, and meter type to produce
coherent synthetic data that closely resembles real-world energy consumption
patterns. The results demonstrate the proposed diffusion model's superior
performance, with a 36% reduction in Frechet Inception Distance (FID) score and
a 13% decrease in Kullback-Leibler divergence (KL divergence) compared to the
following best method. The proposed method successfully generates high-quality
energy data through metadata, and its code will be open-sourced, establishing a
foundation for a broader array of energy data generation models in the future.
\\ ( https://arxiv.org/abs/2404.00525 ,  19835kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00528
Date: Sun, 31 Mar 2024 02:03:28 GMT   (161kb,D)

Title: Generative weather for improved crop model simulations
Authors: Yuji Saikai
Categories: cs.LG
\\
  Accurate and precise crop yield prediction is invaluable for decision making
at both farm levels and regional levels. To make yield prediction, crop models
are widely used for their capability to simulate hypothetical scenarios. While
accuracy and precision of yield prediction critically depend on weather inputs
to simulations, surprisingly little attention has been paid to preparing
weather inputs. We propose a new method to construct generative models for
long-term weather forecasts and ultimately improve crop yield prediction. We
demonstrate use of the method in two representative scenarios -- single-year
production of wheat, barley and canola and three-year production using
rotations of these crops. Results show significant improvement from the
conventional method, measured in terms of mean and standard deviation of
prediction errors. Our method outperformed the conventional method in every one
of 18 metrics for the first scenario and in 29 out of 36 metrics for the second
scenario. For individual crop modellers to start applying the method to their
problems, technical details are carefully explained, and all the code, trained
PyTorch models, APSIM simulation files and result data are made available.
\\ ( https://arxiv.org/abs/2404.00528 ,  161kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00539
Date: Sun, 31 Mar 2024 03:01:56 GMT   (508kb,D)

Title: Solving the QAP by Two-Stage Graph Pointer Networks and Reinforcement
  Learning
Authors: Satoko Iida and Ryota Yasudo
Categories: cs.LG
Comments: 7 pages, 7 figures
\\
  Quadratic Assignment Problem (QAP) is a practical combinatorial optimization
problems that has been studied for several years. Since it is NP-hard, solving
large problem instances of QAP is challenging. Although heuristics can find
semi-optimal solutions, the execution time significantly increases as the
problem size increases. Recently, solving combinatorial optimization problems
by deep learning has been attracting attention as a faster solver than
heuristics. Even with deep learning, however, solving large QAP is still
challenging. In this paper, we propose the deep reinforcement learning model
called the two-stage graph pointer network (GPN) for solving QAP. Two-stage GPN
relies on GPN, which has been proposed for Euclidean Traveling Salesman Problem
(TSP). First, we extend GPN for general TSP, and then we add new algorithms to
that model for solving QAP. Our experimental results show that our two-stage
GPN provides semi-optimal solutions for benchmark problem instances from TSPlib
and QAPLIB.
\\ ( https://arxiv.org/abs/2404.00539 ,  508kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00572
Date: Sun, 31 Mar 2024 06:08:01 GMT   (8261kb,D)

Title: ADs: Active Data-sharing for Data Quality Assurance in Advanced
  Manufacturing Systems
Authors: Yue Zhao, Yuxuan Li, Chenang Liu and Yinan Wang
Categories: cs.LG
\\
  Machine learning (ML) methods are widely used in industrial applications,
which usually require a large amount of training data. However, data collection
needs extensive time costs and investments in the manufacturing system, and
data scarcity commonly exists. Therefore, data-sharing is widely enabled among
multiple machines with similar functionality to augment the dataset for
building ML methods. However, distribution mismatch inevitably exists in their
data due to different working conditions, while the ML methods are assumed to
be built and tested on the dataset following the same distribution. Thus, an
Active Data-sharing (ADs) framework is proposed to ensure the quality of the
shared data among multiple machines. It is designed to simultaneously select
the most informative data points benefiting the downstream tasks and mitigate
the distribution mismatch among all selected data points. The proposed method
is validated on anomaly detection on in-situ monitoring data from three
additive manufacturing processes.
\\ ( https://arxiv.org/abs/2404.00572 ,  8261kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00576
Date: Sun, 31 Mar 2024 06:38:08 GMT   (726kb)

Title: Automated Bi-Fold Weighted Ensemble Algorithms and its Application to
  Brain Tumor Detection and Classification
Authors: PoTsang B. Huang, Muhammad Rizwan, and Mehboob Ali
Categories: cs.LG cs.AI cs.CV
\\
  The uncontrolled and unstructured growth of brain cells is known as brain
tumor, which has one of the highest mortality rates among diseases from all
types of cancers. Due to limited diagnostic and treatment capabilities, they
pose significant challenges, especially in third-world countries. Early
diagnosis plays a vital role in effectively managing brain tumors and reducing
mortality rates. However, the availability of diagnostic methods is hindered by
various limitations, including high costs and lengthy result acquisition times,
impeding early detection of the disease. In this study, we present two
cutting-edge bi-fold weighted voting ensemble models that aim to boost the
effectiveness of weighted ensemble methods. These two proposed methods combine
the classification outcomes from multiple classifiers and determine the optimal
result by selecting the one with the highest probability in the first approach,
and the highest weighted prediction in the second technique. These approaches
significantly improve the overall performance of weighted ensemble techniques.
In the first proposed method, we improve the soft voting technique (SVT) by
introducing a novel unsupervised weight calculating schema (UWCS) to enhance
its weight assigning capability, known as the extended soft voting technique
(ESVT). Secondly, we propose a novel weighted method (NWM) by using the
proposed UWCS. Both of our approaches incorporate three distinct models: a
custom-built CNN, VGG-16, and InceptionResNetV2 which has been trained on
publicly available datasets. The effectiveness of our proposed systems is
evaluated through blind testing, where exceptional results are achieved. We
then establish a comparative analysis of the performance of our proposed
methods with that of SVT to show their superiority and effectiveness.
\\ ( https://arxiv.org/abs/2404.00576 ,  726kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00589
Date: Sun, 31 Mar 2024 07:38:39 GMT   (1975kb,D)

Title: Harnessing the Power of Large Language Model for Uncertainty Aware Graph
  Processing
Authors: Zhenyu Qian, Yiming Qian, Yuting Song, Fei Gao, Hai Jin, Chen Yu, Xia
  Xie
Categories: cs.LG cs.CL
\\
  Handling graph data is one of the most difficult tasks. Traditional
techniques, such as those based on geometry and matrix factorization, rely on
assumptions about the data relations that become inadequate when handling large
and complex graph data. On the other hand, deep learning approaches demonstrate
promising results in handling large graph data, but they often fall short of
providing interpretable explanations. To equip the graph processing with both
high accuracy and explainability, we introduce a novel approach that harnesses
the power of a large language model (LLM), enhanced by an uncertainty-aware
module to provide a confidence score on the generated answer. We experiment
with our approach on two graph processing tasks: few-shot knowledge graph
completion and graph classification. Our results demonstrate that through
parameter efficient fine-tuning, the LLM surpasses state-of-the-art algorithms
by a substantial margin across ten diverse benchmark datasets. Moreover, to
address the challenge of explainability, we propose an uncertainty estimation
based on perturbation, along with a calibration scheme to quantify the
confidence scores of the generated answers. Our confidence measure achieves an
AUC of 0.8 or higher on seven out of the ten datasets in predicting the
correctness of the answer generated by LLM.
\\ ( https://arxiv.org/abs/2404.00589 ,  1975kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00618
Date: Sun, 31 Mar 2024 09:10:32 GMT   (196kb,D)

Title: A Multi-Branched Radial Basis Network Approach to Predicting Complex
  Chaotic Behaviours
Authors: Aarush Sinha
Categories: cs.LG cs.CV cs.NE
Comments: 7 pages, 4 figures
\\
  In this study, we propose a multi branched network approach to predict the
dynamics of a physics attractor characterized by intricate and chaotic
behavior. We introduce a unique neural network architecture comprised of Radial
Basis Function (RBF) layers combined with an attention mechanism designed to
effectively capture nonlinear inter-dependencies inherent in the attractor's
temporal evolution. Our results demonstrate successful prediction of the
attractor's trajectory across 100 predictions made using a real-world dataset
of 36,700 time-series observations encompassing approximately 28 minutes of
activity. To further illustrate the performance of our proposed technique, we
provide comprehensive visualizations depicting the attractor's original and
predicted behaviors alongside quantitative measures comparing observed versus
estimated outcomes. Overall, this work showcases the potential of advanced
machine learning algorithms in elucidating hidden structures in complex
physical systems while offering practical applications in various domains
requiring accurate short-term forecasting capabilities.
\\ ( https://arxiv.org/abs/2404.00618 ,  196kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00623
Date: Sun, 31 Mar 2024 09:25:28 GMT   (4509kb,D)

Title: Variational Autoencoders for exteroceptive perception in reinforcement
  learning-based collision avoidance
Authors: Thomas Nakken Larsen, Eirik Runde Barlaug, Adil Rasheed
Categories: cs.LG cs.RO
\\
  Modern control systems are increasingly turning to machine learning
algorithms to augment their performance and adaptability. Within this context,
Deep Reinforcement Learning (DRL) has emerged as a promising control framework,
particularly in the domain of marine transportation. Its potential for
autonomous marine applications lies in its ability to seamlessly combine
path-following and collision avoidance with an arbitrary number of obstacles.
However, current DRL algorithms require disproportionally large computational
resources to find near-optimal policies compared to the posed control problem
when the searchable parameter space becomes large. To combat this, our work
delves into the application of Variational AutoEncoders (VAEs) to acquire a
generalized, low-dimensional latent encoding of a high-fidelity range-finding
sensor, which serves as the exteroceptive input to a DRL agent. The agent's
performance, encompassing path-following and collision avoidance, is
systematically tested and evaluated within a stochastic simulation environment,
presenting a comprehensive exploration of our proposed approach in maritime
control systems.
\\ ( https://arxiv.org/abs/2404.00623 ,  4509kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00638
Date: Sun, 31 Mar 2024 10:30:03 GMT   (1425kb,D)

Title: HypeBoy: Generative Self-Supervised Representation Learning on
  Hypergraphs
Authors: Sunwoo Kim, Shinhwan Kang, Fanchen Bu, Soo Yong Lee, Jaemin Yoo,
  Kijung Shin
Categories: cs.LG
Comments: Published as a conference paper at ICLR 2024
\\
  Hypergraphs are marked by complex topology, expressing higher-order
interactions among multiple nodes with hyperedges, and better capturing the
topology is essential for effective representation learning. Recent advances in
generative self-supervised learning (SSL) suggest that hypergraph neural
networks learned from generative self supervision have the potential to
effectively encode the complex hypergraph topology. Designing a generative SSL
strategy for hypergraphs, however, is not straightforward. Questions remain
with regard to its generative SSL task, connection to downstream tasks, and
empirical properties of learned representations. In light of the promises and
challenges, we propose a novel generative SSL strategy for hypergraphs. We
first formulate a generative SSL task on hypergraphs, hyperedge filling, and
highlight its theoretical connection to node classification. Based on the
generative SSL task, we propose a hypergraph SSL method, HypeBoy. HypeBoy
learns effective general-purpose hypergraph representations, outperforming 16
baseline methods across 11 benchmark datasets.
\\ ( https://arxiv.org/abs/2404.00638 ,  1425kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00651
Date: Sun, 31 Mar 2024 11:39:11 GMT   (3041kb,D)

Title: Learning Off-policy with Model-based Intrinsic Motivation For Active
  Online Exploration
Authors: Yibo Wang, Jiang Zhao
Categories: cs.LG cs.AI
Comments: Preprint
\\
  Recent advancements in deep reinforcement learning (RL) have demonstrated
notable progress in sample efficiency, spanning both model-based and model-free
paradigms. Despite the identification and mitigation of specific bottlenecks in
prior works, the agent's exploration ability remains under-emphasized in the
realm of sample-efficient RL. This paper investigates how to achieve
sample-efficient exploration in continuous control tasks. We introduce an RL
algorithm that incorporates a predictive model and off-policy learning
elements, where an online planner enhanced by a novelty-aware terminal value
function is employed for sample collection. Leveraging the forward predictive
error within a latent state space, we derive an intrinsic reward without
incurring parameters overhead. This reward establishes a solid connection to
model uncertainty, allowing the agent to effectively overcome the asymptotic
performance gap. Through extensive experiments, our method shows competitive or
even superior performance compared to prior works, especially the sparse reward
cases.
\\ ( https://arxiv.org/abs/2404.00651 ,  3041kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00657
Date: Sun, 31 Mar 2024 12:01:34 GMT   (732kb,D)

Title: Observations on Building RAG Systems for Technical Documents
Authors: Sumit Soman, Sujoy Roychowdhury
Categories: cs.LG cs.AI cs.CL
Comments: Published as a Tiny Paper at ICLR 2024
ACM-class: I.2.7
\\
  Retrieval augmented generation (RAG) for technical documents creates
challenges as embeddings do not often capture domain information. We review
prior art for important factors affecting RAG and perform experiments to
highlight best practices and potential challenges to build RAG systems for
technical documents.
\\ ( https://arxiv.org/abs/2404.00657 ,  732kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00666
Date: Sun, 31 Mar 2024 12:21:57 GMT   (2226kb,D)

Title: Accelerated Parameter-Free Stochastic Optimization
Authors: Itai Kreisler and Maor Ivgi and Oliver Hinder and Yair Carmon
Categories: cs.LG math.OC
\\
  We propose a method that achieves near-optimal rates for smooth stochastic
convex optimization and requires essentially no prior knowledge of problem
parameters. This improves on prior work which requires knowing at least the
initial distance to optimality d0. Our method, U-DoG, combines UniXGrad (Kavis
et al., 2019) and DoG (Ivgi et al., 2023) with novel iterate stabilization
techniques. It requires only loose bounds on d0 and the noise magnitude,
provides high probability guarantees under sub-Gaussian noise, and is also
near-optimal in the non-smooth case. Our experiments show consistent, strong
performance on convex problems and mixed results on neural network training.
\\ ( https://arxiv.org/abs/2404.00666 ,  2226kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00672
Date: Sun, 31 Mar 2024 12:44:24 GMT   (23332kb,D)

Title: A General and Efficient Training for Transformer via Token Expansion
Authors: Wenxuan Huang, Yunhang Shen, Jiao Xie, Baochang Zhang, Gaoqi He, Ke
  Li, Xing Sun, Shaohui Lin
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: Accepted to CVPR 2024. Code is available at
  https://github.com/Osilly/TokenExpansion
\\
  The remarkable performance of Vision Transformers (ViTs) typically requires
an extremely large training cost. Existing methods have attempted to accelerate
the training of ViTs, yet typically disregard method universality with accuracy
dropping. Meanwhile, they break the training consistency of the original
transformers, including the consistency of hyper-parameters, architecture, and
strategy, which prevents them from being widely applied to different
Transformer networks. In this paper, we propose a novel token growth scheme
Token Expansion (termed ToE) to achieve consistent training acceleration for
ViTs. We introduce an "initialization-expansion-merging" pipeline to maintain
the integrity of the intermediate feature distribution of original
transformers, preventing the loss of crucial learnable information in the
training process. ToE can not only be seamlessly integrated into the training
and fine-tuning process of transformers (e.g., DeiT and LV-ViT), but also
effective for efficient training frameworks (e.g., EfficientTrain), without
twisting the original training hyper-parameters, architecture, and introducing
additional training strategies. Extensive experiments demonstrate that ToE
achieves about 1.3x faster for the training of ViTs in a lossless manner, or
even with performance gains over the full-token training baselines. Code is
available at https://github.com/Osilly/TokenExpansion .
\\ ( https://arxiv.org/abs/2404.00672 ,  23332kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00686
Date: Sun, 31 Mar 2024 13:41:56 GMT   (41726kb,D)

Title: Utilizing Maximum Mean Discrepancy Barycenter for Propagating the
  Uncertainty of Value Functions in Reinforcement Learning
Authors: Srinjoy Roy, Swagatam Das
Categories: cs.LG
\\
  Accounting for the uncertainty of value functions boosts exploration in
Reinforcement Learning (RL). Our work introduces Maximum Mean Discrepancy
Q-Learning (MMD-QL) to improve Wasserstein Q-Learning (WQL) for uncertainty
propagation during Temporal Difference (TD) updates. MMD-QL uses the MMD
barycenter for this purpose, as MMD provides a tighter estimate of closeness
between probability measures than the Wasserstein distance. Firstly, we
establish that MMD-QL is Probably Approximately Correct in MDP (PAC-MDP) under
the average loss metric. Concerning the accumulated rewards, experiments on
tabular environments show that MMD-QL outperforms WQL and other algorithms.
Secondly, we incorporate deep networks into MMD-QL to create MMD Q-Network
(MMD-QN). Making reasonable assumptions, we analyze the convergence rates of
MMD-QN using function approximation. Empirical results on challenging Atari
games demonstrate that MMD-QN performs well compared to benchmark deep RL
algorithms, highlighting its effectiveness in handling large state-action
spaces.
\\ ( https://arxiv.org/abs/2404.00686 ,  41726kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00688
Date: Sun, 31 Mar 2024 13:52:07 GMT   (773kb,D)

Title: Meta Learning in Bandits within Shared Affine Subspaces
Authors: Steven Bilaj, Sofien Dhouib, Setareh Maghsudi
Categories: cs.LG stat.ML
Comments: Accepted in AISTATS 2024
\\
  We study the problem of meta-learning several contextual stochastic bandits
tasks by leveraging their concentration around a low-dimensional affine
subspace, which we learn via online principal component analysis to reduce the
expected regret over the encountered bandits. We propose and theoretically
analyze two strategies that solve the problem: One based on the principle of
optimism in the face of uncertainty and the other via Thompson sampling. Our
framework is generic and includes previously proposed approaches as special
cases. Besides, the empirical results show that our methods significantly
reduce the regret on several bandit tasks.
\\ ( https://arxiv.org/abs/2404.00688 ,  773kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00712
Date: Sun, 31 Mar 2024 15:09:47 GMT   (2589kb,D)

Title: Survey of Computerized Adaptive Testing: A Machine Learning Perspective
Authors: Qi Liu, Yan Zhuang, Haoyang Bi, Zhenya Huang, Weizhe Huang, Jiatong
  Li, Junhao Yu, Zirui Liu, Zirui Hu, Yuting Hong, Zachary A. Pardos, Haiping
  Ma, Mengxiao Zhu, Shijin Wang, Enhong Chen
Categories: cs.LG cs.AI cs.CY cs.IR
\\
  Computerized Adaptive Testing (CAT) provides an efficient and tailored method
for assessing the proficiency of examinees, by dynamically adjusting test
questions based on their performance. Widely adopted across diverse fields like
education, healthcare, sports, and sociology, CAT has revolutionized testing
practices. While traditional methods rely on psychometrics and statistics, the
increasing complexity of large-scale testing has spurred the integration of
machine learning techniques. This paper aims to provide a machine
learning-focused survey on CAT, presenting a fresh perspective on this adaptive
testing method. By examining the test question selection algorithm at the heart
of CAT's adaptivity, we shed light on its functionality. Furthermore, we delve
into cognitive diagnosis models, question bank construction, and test control
within CAT, exploring how machine learning can optimize these components.
Through an analysis of current methods, strengths, limitations, and challenges,
we strive to develop robust, fair, and efficient CAT systems. By bridging
psychometric-driven CAT research with machine learning, this survey advocates
for a more inclusive and interdisciplinary approach to the future of adaptive
testing.
\\ ( https://arxiv.org/abs/2404.00712 ,  2589kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00774
Date: Sun, 31 Mar 2024 19:09:09 GMT   (6323kb,D)

Title: SOAR: Improved Indexing for Approximate Nearest Neighbor Search
Authors: Philip Sun, David Simcha, Dave Dopson, Ruiqi Guo, Sanjiv Kumar
Categories: cs.LG
Journal-ref: Advances in Neural Information Processing Systems 36 (2023)
  3189-3204
\\
  This paper introduces SOAR: Spilling with Orthogonality-Amplified Residuals,
a novel data indexing technique for approximate nearest neighbor (ANN) search.
SOAR extends upon previous approaches to ANN search, such as spill trees, that
utilize multiple redundant representations while partitioning the data to
reduce the probability of missing a nearest neighbor during search. Rather than
training and computing these redundant representations independently, however,
SOAR uses an orthogonality-amplified residual loss, which optimizes each
representation to compensate for cases where other representations perform
poorly. This drastically improves the overall index quality, resulting in
state-of-the-art ANN benchmark performance while maintaining fast indexing
times and low memory consumption.
\\ ( https://arxiv.org/abs/2404.00774 ,  6323kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00776
Date: Sun, 31 Mar 2024 19:15:09 GMT   (322kb,D)

Title: PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning
Authors: Weihua Hu, Yiwen Yuan, Zecheng Zhang, Akihiro Nitta, Kaidi Cao, Vid
  Kocijan, Jure Leskovec, Matthias Fey
Categories: cs.LG cs.DB stat.ML
Comments: https://github.com/pyg-team/pytorch-frame
\\
  We present PyTorch Frame, a PyTorch-based framework for deep learning over
multi-modal tabular data. PyTorch Frame makes tabular deep learning easy by
providing a PyTorch-based data structure to handle complex tabular data,
introducing a model abstraction to enable modular implementation of tabular
models, and allowing external foundation models to be incorporated to handle
complex columns (e.g., LLMs for text columns). We demonstrate the usefulness of
PyTorch Frame by implementing diverse tabular models in a modular way,
successfully applying these models to complex multi-modal tabular data, and
integrating our framework with PyTorch Geometric, a PyTorch library for Graph
Neural Networks (GNNs), to perform end-to-end learning over relational
databases.
\\ ( https://arxiv.org/abs/2404.00776 ,  322kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00781
Date: Sun, 31 Mar 2024 19:57:38 GMT   (13596kb,D)

Title: Addressing Loss of Plasticity and Catastrophic Forgetting in Continual
  Learning
Authors: Mohamed Elsayed, A. Rupam Mahmood
Categories: cs.LG cs.AI
Comments: Published in the Proceedings of the 12th International Conference on
  Learning Representations (ICLR 2024)
\\
  Deep representation learning methods struggle with continual learning,
suffering from both catastrophic forgetting of useful units and loss of
plasticity, often due to rigid and unuseful units. While many methods address
these two issues separately, only a few currently deal with both
simultaneously. In this paper, we introduce Utility-based Perturbed Gradient
Descent (UPGD) as a novel approach for the continual learning of
representations. UPGD combines gradient updates with perturbations, where it
applies smaller modifications to more useful units, protecting them from
forgetting, and larger modifications to less useful units, rejuvenating their
plasticity. We use a challenging streaming learning setup where continual
learning problems have hundreds of non-stationarities and unknown task
boundaries. We show that many existing methods suffer from at least one of the
issues, predominantly manifested by their decreasing accuracy over tasks. On
the other hand, UPGD continues to improve performance and surpasses or is
competitive with all methods in all problems. Finally, in extended
reinforcement learning experiments with PPO, we show that while Adam exhibits a
performance drop after initial learning, UPGD avoids it by addressing both
continual learning issues.
\\ ( https://arxiv.org/abs/2404.00781 ,  13596kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00790
Date: Sun, 31 Mar 2024 20:28:44 GMT   (9328kb,D)

Title: Rehearsal-Free Modular and Compositional Continual Learning for Language
  Models
Authors: Mingyang Wang, Heike Adel, Lukas Lange, Jannik Str\"otgen, Hinrich
  Sch\"utze
Categories: cs.LG cs.CL
\\
  Continual learning aims at incrementally acquiring new knowledge while not
forgetting existing knowledge. To overcome catastrophic forgetting, methods are
either rehearsal-based, i.e., store data examples from previous tasks for data
replay, or isolate parameters dedicated to each task. However, rehearsal-based
methods raise privacy and memory issues, and parameter-isolation continual
learning does not consider interaction between tasks, thus hindering knowledge
transfer. In this work, we propose MoCL, a rehearsal-free Modular and
Compositional Continual Learning framework which continually adds new modules
to language models and composes them with existing modules. Experiments on
various benchmarks show that MoCL outperforms state of the art and effectively
facilitates knowledge transfer.
\\ ( https://arxiv.org/abs/2404.00790 ,  9328kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00798
Date: Sun, 31 Mar 2024 21:02:50 GMT   (2047kb,D)

Title: On Difficulties of Attention Factorization through Shared Memory
Authors: Uladzislau Yorsh, Martin Hole\v{n}a, Ond\v{r}ej Bojar, David Herel
Categories: cs.LG
Comments: 2 pages of main content, 8 pages in total, published as a Tiny Paper
  at ICLR 2024
\\
  Transformers have revolutionized deep learning in numerous fields, including
natural language processing, computer vision, and audio processing. Their
strength lies in their attention mechanism, which allows for the discovering of
complex input relationships. However, this mechanism's quadratic time and
memory complexity pose challenges for larger inputs. Researchers are now
investigating models like Linear Unified Nested Attention (Luna) or Memory
Augmented Transformer, which leverage external learnable memory to either
reduce the attention computation complexity down to linear, or to propagate
information between chunks in chunk-wise processing. Our findings challenge the
conventional thinking on these models, revealing that interfacing with the
memory directly through an attention operation is suboptimal, and that the
performance may be considerably improved by filtering the input signal before
communicating with memory.
\\ ( https://arxiv.org/abs/2404.00798 ,  2047kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00816
Date: Sun, 31 Mar 2024 22:22:10 GMT   (1728kb,D)

Title: HeteroMILE: a Multi-Level Graph Representation Learning Framework for
  Heterogeneous Graphs
Authors: Yue Zhang, Yuntian He, Saket Gurukar, Srinivasan Parthasarathy
Categories: cs.LG cs.AI
\\
  Heterogeneous graphs are ubiquitous in real-world applications because they
can represent various relationships between different types of entities.
Therefore, learning embeddings in such graphs is a critical problem in graph
machine learning. However, existing solutions for this problem fail to scale to
large heterogeneous graphs due to their high computational complexity. To
address this issue, we propose a Multi-Level Embedding framework of nodes on a
heterogeneous graph (HeteroMILE) - a generic methodology that allows
contemporary graph embedding methods to scale to large graphs. HeteroMILE
repeatedly coarsens the large sized graph into a smaller size while preserving
the backbone structure of the graph before embedding it, effectively reducing
the computational cost by avoiding time-consuming processing operations. It
then refines the coarsened embedding to the original graph using a
heterogeneous graph convolution neural network. We evaluate our approach using
several popular heterogeneous graph datasets. The experimental results show
that HeteroMILE can substantially reduce computational time (approximately 20x
speedup) and generate an embedding of better quality for link prediction and
node classification.
\\ ( https://arxiv.org/abs/2404.00816 ,  1728kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00848
Date: Mon, 1 Apr 2024 01:27:07 GMT   (677kb,D)

Title: Predictive Performance Comparison of Decision Policies Under Confounding
Authors: Luke Guerdan, Amanda Coston, Kenneth Holstein, Zhiwei Steven Wu
Categories: cs.LG cs.CY stat.ME
\\
  Predictive models are often introduced to decision-making tasks under the
rationale that they improve performance over an existing decision-making
policy. However, it is challenging to compare predictive performance against an
existing decision-making policy that is generally under-specified and dependent
on unobservable factors. These sources of uncertainty are often addressed in
practice by making strong assumptions about the data-generating mechanism. In
this work, we propose a method to compare the predictive performance of
decision policies under a variety of modern identification approaches from the
causal inference and off-policy evaluation literatures (e.g., instrumental
variable, marginal sensitivity model, proximal variable). Key to our method is
the insight that there are regions of uncertainty that we can safely ignore in
the policy comparison. We develop a practical approach for finite-sample
estimation of regret intervals under no assumptions on the parametric form of
the status quo policy. We verify our framework theoretically and via synthetic
data experiments. We conclude with a real-world application using our framework
to support a pre-deployment evaluation of a proposed modification to a
healthcare enrollment policy.
\\ ( https://arxiv.org/abs/2404.00848 ,  677kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00859
Date: Mon, 1 Apr 2024 02:01:28 GMT   (859kb,D)

Title: Do language models plan ahead for future tokens?
Authors: Wilson Wu, John X. Morris, Lionel Levine
Categories: cs.LG cs.CL
\\
  Do transformers "think ahead" during inference at a given position? It is
known transformers prepare information in the hidden states of the forward pass
at $t$ that is then used in future forward passes $t+\tau$. We posit two
explanations for this phenomenon: pre-caching, in which off-diagonal gradient
terms present in training result in the model computing features at $t$
irrelevant to the present inference task but useful for the future, and
breadcrumbs, in which features most relevant to time step $t$ are already the
same as those that would most benefit inference at time $t+\tau$. We test these
hypotheses by training language models without propagating gradients to past
timesteps, a scheme we formalize as myopic training. In a synthetic data
setting, we find clear evidence for pre-caching. In the autoregressive language
modeling setting, our experiments are more suggestive of the breadcrumbs
hypothesis.
\\ ( https://arxiv.org/abs/2404.00859 ,  859kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00860
Date: Mon, 1 Apr 2024 02:01:33 GMT   (7155kb,D)

Title: Lipsum-FT: Robust Fine-Tuning of Zero-Shot Models Using Random Text
  Guidance
Authors: Giung Nam, Byeongho Heo, Juho Lee
Categories: cs.LG cs.CV
Comments: ICLR 2024
\\
  Large-scale contrastive vision-language pre-trained models provide the
zero-shot model achieving competitive performance across a range of image
classification tasks without requiring training on downstream data. Recent
works have confirmed that while additional fine-tuning of the zero-shot model
on the reference data results in enhanced downstream performance, it
compromises the model's robustness against distribution shifts. Our
investigation begins by examining the conditions required to achieve the goals
of robust fine-tuning, employing descriptions based on feature distortion
theory and joint energy-based models. Subsequently, we propose a novel robust
fine-tuning algorithm, Lipsum-FT, that effectively utilizes the language
modeling aspect of the vision-language pre-trained models. Extensive
experiments conducted on distribution shift scenarios in DomainNet and ImageNet
confirm the superiority of our proposed Lipsum-FT approach over existing robust
fine-tuning methods.
\\ ( https://arxiv.org/abs/2404.00860 ,  7155kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00880
Date: Mon, 1 Apr 2024 03:18:42 GMT   (15589kb,D)

Title: Rethinking the Relationship between Recurrent and Non-Recurrent Neural
  Networks: A Study in Sparsity
Authors: Quincy Hershey, Randy Paffenroth, Harsh Pathak, Simon Tavener
Categories: cs.LG
\\
  Neural networks (NN) can be divided into two broad categories, recurrent and
non-recurrent. Both types of neural networks are popular and extensively
studied, but they are often treated as distinct families of machine learning
algorithms. In this position paper, we argue that there is a closer
relationship between these two types of neural networks than is normally
appreciated. We show that many common neural network models, such as Recurrent
Neural Networks (RNN), Multi-Layer Perceptrons (MLP), and even deep multi-layer
transformers, can all be represented as iterative maps.
  The close relationship between RNNs and other types of NNs should not be
surprising. In particular, RNNs are known to be Turing complete, and therefore
capable of representing any computable function (such as any other types of
NNs), but herein we argue that the relationship runs deeper and is more
practical than this. For example, RNNs are often thought to be more difficult
to train than other types of NNs, with RNNs being plagued by issues such as
vanishing or exploding gradients. However, as we demonstrate in this paper,
MLPs, RNNs, and many other NNs lie on a continuum, and this perspective leads
to several insights that illuminate both theoretical and practical aspects of
NNs.
\\ ( https://arxiv.org/abs/2404.00880 ,  15589kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00882
Date: Mon, 1 Apr 2024 03:23:43 GMT   (1534kb,D)

Title: Metric Learning to Accelerate Convergence of Operator Splitting Methods
  for Differentiable Parametric Programming
Authors: Ethan King, James Kotary, Ferdinando Fioretto, Jan Drgona
Categories: cs.LG
\\
  Recent work has shown a variety of ways in which machine learning can be used
to accelerate the solution of constrained optimization problems. Increasing
demand for real-time decision-making capabilities in applications such as
artificial intelligence and optimal control has led to a variety of approaches,
based on distinct strategies. This work proposes a novel approach to learning
optimization, in which the underlying metric space of a proximal operator
splitting algorithm is learned so as to maximize its convergence rate. While
prior works in optimization theory have derived optimal metrics for limited
classes of problems, the results do not extend to many practical problem forms
including general Quadratic Programming (QP). This paper shows how
differentiable optimization can enable the end-to-end learning of proximal
metrics, enhancing the convergence of proximal algorithms for QP problems
beyond what is possible based on known theory. Additionally, the results
illustrate a strong connection between the learned proximal metrics and active
constraints at the optima, leading to an interpretation in which the learning
of proximal metrics can be viewed as a form of active set learning.
\\ ( https://arxiv.org/abs/2404.00882 ,  1534kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00883
Date: Mon, 1 Apr 2024 03:23:55 GMT   (2122kb,D)

Title: Interpretable Multi-View Clustering Based on Anchor Graph Tensor
  Factorization
Authors: Jing Li, Quanxue Gao, Cheng Deng, Qianqian Wang, Ming Yang
Categories: cs.LG
\\
  The clustering method based on the anchor graph has gained significant
attention due to its exceptional clustering performance and ability to process
large-scale data. One common approach is to learn bipartite graphs with
K-connected components, helping avoid the need for post-processing. However,
this method has strict parameter requirements and may not always get
K-connected components. To address this issue, an alternative approach is to
directly obtain the cluster label matrix by performing non-negative matrix
factorization (NMF) on the anchor graph. Nevertheless, existing multi-view
clustering methods based on anchor graph factorization lack adequate cluster
interpretability for the decomposed matrix and often overlook the inter-view
information. We address this limitation by using non-negative tensor
factorization to decompose an anchor graph tensor that combines anchor graphs
from multiple views. This approach allows us to consider inter-view information
comprehensively. The decomposed tensors, namely the sample indicator tensor and
the anchor indicator tensor, enhance the interpretability of the factorization.
Extensive experiments validate the effectiveness of this method.
\\ ( https://arxiv.org/abs/2404.00883 ,  2122kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00885
Date: Mon, 1 Apr 2024 03:27:34 GMT   (521kb,D)

Title: Modeling Output-Level Task Relatedness in Multi-Task Learning with
  Feedback Mechanism
Authors: Xiangming Xi, Feng Gao, Jun Xu, Fangtai Guo, and Tianlei Jin
Categories: cs.LG
Comments: submitted to CDC2024
\\
  Multi-task learning (MTL) is a paradigm that simultaneously learns multiple
tasks by sharing information at different levels, enhancing the performance of
each individual task. While previous research has primarily focused on
feature-level or parameter-level task relatedness, and proposed various model
architectures and learning algorithms to improve learning performance, we aim
to explore output-level task relatedness. This approach introduces a posteriori
information into the model, considering that different tasks may produce
correlated outputs with mutual influences. We achieve this by incorporating a
feedback mechanism into MTL models, where the output of one task serves as a
hidden feature for another task, thereby transforming a static MTL model into a
dynamic one. To ensure the training process converges, we introduce a
convergence loss that measures the trend of a task's outputs during each
iteration. Additionally, we propose a Gumbel gating mechanism to determine the
optimal projection of feedback signals. We validate the effectiveness of our
method and evaluate its performance through experiments conducted on several
baseline models in spoken language understanding.
\\ ( https://arxiv.org/abs/2404.00885 ,  521kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00897
Date: Mon, 1 Apr 2024 03:49:42 GMT   (93kb)

Title: Machine Learning Robustness: A Primer
Authors: Houssem Ben Braiek and Foutse Khomh
Categories: cs.LG cs.AI cs.SE
\\
  This chapter explores the foundational concept of robustness in Machine
Learning (ML) and its integral role in establishing trustworthiness in
Artificial Intelligence (AI) systems. The discussion begins with a detailed
definition of robustness, portraying it as the ability of ML models to maintain
stable performance across varied and unexpected environmental conditions. ML
robustness is dissected through several lenses: its complementarity with
generalizability; its status as a requirement for trustworthy AI; its
adversarial vs non-adversarial aspects; its quantitative metrics; and its
indicators such as reproducibility and explainability. The chapter delves into
the factors that impede robustness, such as data bias, model complexity, and
the pitfalls of underspecified ML pipelines. It surveys key techniques for
robustness assessment from a broad perspective, including adversarial attacks,
encompassing both digital and physical realms. It covers non-adversarial data
shifts and nuances of Deep Learning (DL) software testing methodologies. The
discussion progresses to explore amelioration strategies for bolstering
robustness, starting with data-centric approaches like debiasing and
augmentation. Further examination includes a variety of model-centric methods
such as transfer learning, adversarial training, and randomized smoothing.
Lastly, post-training methods are discussed, including ensemble techniques,
pruning, and model repairs, emerging as cost-effective strategies to make
models more resilient against the unpredictable. This chapter underscores the
ongoing challenges and limitations in estimating and achieving ML robustness by
existing approaches. It offers insights and directions for future research on
this crucial concept, as a prerequisite for trustworthy AI systems.
\\ ( https://arxiv.org/abs/2404.00897 ,  93kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00898
Date: Mon, 1 Apr 2024 03:51:38 GMT   (4223kb,D)

Title: CAAP: Class-Dependent Automatic Data Augmentation Based On Adaptive
  Policies For Time Series
Authors: Tien-Yu Chang, Hao Dai, Vincent S. Tseng
Categories: cs.LG
\\
  Data Augmentation is a common technique used to enhance the performance of
deep learning models by expanding the training dataset. Automatic Data
Augmentation (ADA) methods are getting popular because of their capacity to
generate policies for various datasets. However, existing ADA methods primarily
focused on overall performance improvement, neglecting the problem of
class-dependent bias that leads to performance reduction in specific classes.
This bias poses significant challenges when deploying models in real-world
applications. Furthermore, ADA for time series remains an underexplored domain,
highlighting the need for advancements in this field. In particular, applying
ADA techniques to vital signals like an electrocardiogram (ECG) is a compelling
example due to its potential in medical domains such as heart disease
diagnostics.
  We propose a novel deep learning-based approach called Class-dependent
Automatic Adaptive Policies (CAAP) framework to overcome the notable
class-dependent bias problem while maintaining the overall improvement in
time-series data augmentation. Specifically, we utilize the policy network to
generate effective sample-wise policies with balanced difficulty through class
and feature information extraction. Second, we design the augmentation
probability regulation method to minimize class-dependent bias. Third, we
introduce the information region concepts into the ADA framework to preserve
essential regions in the sample. Through a series of experiments on real-world
ECG datasets, we demonstrate that CAAP outperforms representative methods in
achieving lower class-dependent bias combined with superior overall
performance. These results highlight the reliability of CAAP as a promising ADA
method for time series modeling that fits for the demands of real-world
applications.
\\ ( https://arxiv.org/abs/2404.00898 ,  4223kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00962
Date: Mon, 1 Apr 2024 07:12:27 GMT   (1591kb,D)

Title: Diffusion-Driven Domain Adaptation for Generating 3D Molecules
Authors: Haokai Hong, Wanyu Lin, and Kay Chen Tan
Categories: cs.LG physics.chem-ph q-bio.BM
Comments: 11 pages, 3 figures, and 3 tables
\\
  Can we train a molecule generator that can generate 3D molecules from a new
domain, circumventing the need to collect data? This problem can be cast as the
problem of domain adaptive molecule generation. This work presents a novel and
principled diffusion-based approach, called GADM, that allows shifting a
generative model to desired new domains without the need to collect even a
single molecule. As the domain shift is typically caused by the structure
variations of molecules, e.g., scaffold variations, we leverage a designated
equivariant masked autoencoder (MAE) along with various masking strategies to
capture the structural-grained representations of the in-domain varieties. In
particular, with an asymmetric encoder-decoder module, the MAE can generalize
to unseen structure variations from the target domains. These structure
variations are encoded with an equivariant encoder and treated as domain
supervisors to control denoising. We show that, with these encoded
structural-grained domain supervisors, GADM can generate effective molecules
within the desired new domains. We conduct extensive experiments across various
domain adaptation tasks over benchmarking datasets. We show that our approach
can improve up to 65.6% in terms of success rate defined based on molecular
validity, uniqueness, and novelty compared to alternative baselines.
\\ ( https://arxiv.org/abs/2404.00962 ,  1591kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00983
Date: Mon, 1 Apr 2024 07:59:29 GMT   (1173kb,D)

Title: Continual Learning for Smart City: A Survey
Authors: Li Yang, Zhipeng Luo, Shiming Zhang, Fei Teng, and Tianrui Li
Categories: cs.LG cs.AI
Comments: Preprint. Work in Progress
\\
  With the digitization of modern cities, large data volumes and powerful
computational resources facilitate the rapid update of intelligent models
deployed in smart cities. Continual learning (CL) is a novel machine learning
paradigm that constantly updates models to adapt to changing environments,
where the learning tasks, data, and distributions can vary over time. Our
survey provides a comprehensive review of continual learning methods that are
widely used in smart city development. The content consists of three parts: 1)
Methodology-wise. We categorize a large number of basic CL methods and advanced
CL frameworks in combination with other learning paradigms including graph
learning, spatial-temporal learning, multi-modal learning, and federated
learning. 2) Application-wise. We present numerous CL applications covering
transportation, environment, public health, safety, networks, and associated
datasets related to urban computing. 3) Challenges. We discuss current problems
and challenges and envision several promising research directions. We believe
this survey can help relevant researchers quickly familiarize themselves with
the current state of continual learning research used in smart city development
and direct them to future research trends.
\\ ( https://arxiv.org/abs/2404.00983 ,  1173kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00986
Date: Mon, 1 Apr 2024 08:18:38 GMT   (1156kb,D)

Title: Make Continual Learning Stronger via C-Flat
Authors: Ang Bian, Wei Li, Hangjie Yuan, Chengrong Yu, Zixiang Zhao, Mang Wang,
  Aojun Lu, Tao Feng
Categories: cs.LG cs.CV
\\
  Model generalization ability upon incrementally acquiring dynamically
updating knowledge from sequentially arriving tasks is crucial to tackle the
sensitivity-stability dilemma in Continual Learning (CL). Weight loss landscape
sharpness minimization seeking for flat minima lying in neighborhoods with
uniform low loss or smooth gradient is proven to be a strong training regime
improving model generalization compared with loss minimization based optimizer
like SGD. Yet only a few works have discussed this training regime for CL,
proving that dedicated designed zeroth-order sharpness optimizer can improve CL
performance. In this work, we propose a Continual Flatness (C-Flat) method
featuring a flatter loss landscape tailored for CL. C-Flat could be easily
called with only one line of code and is plug-and-play to any CL methods. A
general framework of C-Flat applied to all CL categories and a thorough
comparison with loss minima optimizer and flat minima based CL approaches is
presented in this paper, showing that our method can boost CL performance in
almost all cases. Code will be publicly available upon publication.
\\ ( https://arxiv.org/abs/2404.00986 ,  1156kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01039
Date: Mon, 1 Apr 2024 10:50:34 GMT   (626kb,D)

Title: A Survey on Hypergraph Neural Networks: An In-Depth and Step-By-Step
  Guide
Authors: Sunwoo Kim, Soo Yong Lee, Yue Gao, Alessia Antelmi, Mirko Polato,
  Kijung Shin
Categories: cs.LG
\\
  Higher-order interactions (HOIs) are ubiquitous in real-world complex systems
and applications, and thus investigation of deep learning for HOIs has become a
valuable agenda for the data mining and machine learning communities. As
networks of HOIs are expressed mathematically as hypergraphs, hypergraph neural
networks (HNNs) have emerged as a powerful tool for representation learning on
hypergraphs. Given the emerging trend, we present the first survey dedicated to
HNNs, with an in-depth and step-by-step guide. Broadly, the present survey
overviews HNN architectures, training strategies, and applications. First, we
break existing HNNs down into four design components: (i) input features, (ii)
input structures, (iii) message-passing schemes, and (iv) training strategies.
Second, we examine how HNNs address and learn HOIs with each of their
components. Third, we overview the recent applications of HNNs in
recommendation, biological and medical science, time series analysis, and
computer vision. Lastly, we conclude with a discussion on limitations and
future directions.
\\ ( https://arxiv.org/abs/2404.01039 ,  626kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01041
Date: Mon, 1 Apr 2024 10:54:49 GMT   (1811kb,D)

Title: Can LLMs get help from other LLMs without revealing private information?
Authors: Florian Hartmann, Duc-Hieu Tran, Peter Kairouz, Victor C\u{a}rbune,
  Blaise Aguera y Arcas
Categories: cs.LG cs.AI cs.CR cs.MA
\\
  Cascades are a common type of machine learning systems in which a large,
remote model can be queried if a local model is not able to accurately label a
user's data by itself. Serving stacks for large language models (LLMs)
increasingly use cascades due to their ability to preserve task performance
while dramatically reducing inference costs. However, applying cascade systems
in situations where the local model has access to sensitive data constitutes a
significant privacy risk for users since such data could be forwarded to the
remote model. In this work, we show the feasibility of applying cascade systems
in such setups by equipping the local model with privacy-preserving techniques
that reduce the risk of leaking private information when querying the remote
model. To quantify information leakage in such setups, we introduce two privacy
measures. We then propose a system that leverages the recently introduced
social learning paradigm in which LLMs collaboratively learn from each other by
exchanging natural language. Using this paradigm, we demonstrate on several
datasets that our methods minimize the privacy loss while at the same time
improving task performance compared to a non-cascade baseline.
\\ ( https://arxiv.org/abs/2404.01041 ,  1811kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01060
Date: Mon, 1 Apr 2024 11:48:03 GMT   (2407kb,D)

Title: A comparison of Single- and Double-generator formalisms for
  Thermodynamics-Informed Neural Networks
Authors: Pau Urdeitx, Ic\'iar Alfaro, David Gonz\'alez, Francisco Chinesta,
  El\'ias Cueto
Categories: cs.LG
Comments: 22 pages, 17 figures
MSC-class: I.2.6, K.3.2
\\
  The development of inductive biases has been shown to be a very effective way
to increase the accuracy and robustness of neural networks, particularly when
they are used to predict physical phenomena. These biases significantly
increase the certainty of predictions, decrease the error made and allow
considerably smaller datasets to be used.
  There are a multitude of methods in the literature to develop these biases.
One of the most effective ways, when dealing with physical phenomena, is to
introduce physical principles of recognised validity into the network
architecture.
  The problem becomes more complex without knowledge of the physical principles
governing the phenomena under study. A very interesting possibility then is to
turn to the principles of thermodynamics, which are universally valid,
regardless of the level of abstraction of the description sought for the
phenomenon under study.
  To ensure compliance with the principles of thermodynamics, there are
formulations that have a long tradition in many branches of science. In the
field of rheology, for example, two main types of formalisms are used to ensure
compliance with these principles: one-generator and two-generator formalisms.
In this paper we study the advantages and disadvantages of each, using
classical problems with known solutions and synthetic data.
\\ ( https://arxiv.org/abs/2404.01060 ,  2407kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01078
Date: Mon, 1 Apr 2024 12:19:33 GMT   (4486kb,D)

Title: Energy Model-based Accurate Shapley Value Estimation for Interpretable
  Deep Learning Predictive Modelling
Authors: Cheng Lu, Jiusun Zeng, Yu Xia, Jinhui Cai, Shihua Luo
Categories: cs.LG
\\
  As a favorable tool for explainable artificial intelligence (XAI), Shapley
value has been widely used to interpret deep learning based predictive models.
However, accurate and efficient estimation of Shapley value is a difficult task
since the computation load grows exponentially with the increase of input
features. Most existing accelerated Shapley value estimation methods have to
compromise on estimation accuracy with efficiency. In this article, we present
EmSHAP(Energy model-based Shapley value estimation), which can effectively
approximate the expectation of Shapley contribution function/deep learning
model under arbitrary subset of features given the rest. In order to determine
the proposal conditional distribution in the energy model, a gated recurrent
unit(GRU) is introduced by mapping the input features onto a hidden space, so
that the impact of input feature orderings can be eliminated. In addition, a
dynamic masking scheme is proposed to improve the generalization ability. It is
proved in Theorems 1, 2 and 3 that EmSHAP achieves tighter error bound than
state-of-the-art methods like KernelSHAP and VAEAC, leading to higher
estimation accuracy. Finally, case studies on a medical application and an
industrial application show that the proposed Shapley value-based explainable
framework exhibits enhanced estimation accuracy without compromise on
efficiency.
\\ ( https://arxiv.org/abs/2404.01078 ,  4486kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01099
Date: Mon, 1 Apr 2024 13:12:30 GMT   (2401kb,D)

Title: What's in Your "Safe" Data?: Identifying Benign Data that Breaks Safety
Authors: Luxi He, Mengzhou Xia, Peter Henderson
Categories: cs.LG cs.AI cs.CL cs.CR
\\
  Current Large Language Models (LLMs), even those tuned for safety and
alignment, are susceptible to jailbreaking. Some have found that just further
fine-tuning an aligned model with benign data (i.e., data without harmful
content) surprisingly leads to substantial degradation in safety. We delve into
the data-centric aspects of why benign fine-tuning inadvertently contributes to
jailbreaking. First, we represent fine-tuning data through two lenses:
representation and gradient spaces. Furthermore, we propose a bi-directional
anchoring method that prioritizes data points that are close to harmful
examples and distant from benign ones. By doing so, our approach effectively
identifies subsets of benign data that are more likely to degrade the model's
safety after fine-tuning. Training on just 100 of these seemingly benign
datapoints can lead to the fine-tuned model affirmatively responding to > 70%
of tested harmful requests, compared to < 20% after fine-tuning on randomly
selected data. We further find that selected data are often in the form of
lists and bullet points, or math questions.
\\ ( https://arxiv.org/abs/2404.01099 ,  2401kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01122
Date: Mon, 1 Apr 2024 13:56:12 GMT   (1620kb)

Title: Enhanced Precision in Rainfall Forecasting for Mumbai: Utilizing Physics
  Informed ConvLSTM2D Models for Finer Spatial and Temporal Resolution
Authors: Ajay Devda, Akshay Sunil, Murthy R, B Deepthi
Categories: cs.LG
Comments: Submitted to Computer and Geosciences. arXiv admin note: substantial
  text overlap with arXiv:2310.09311
\\
  Forecasting rainfall in tropical areas is challenging due to complex
atmospheric behaviour, elevated humidity levels, and the common presence of
convective rain events. In the Indian context, the difficulty is further
exacerbated because of the monsoon intra seasonal oscillations, which introduce
significant variability in rainfall patterns over short periods. Earlier
investigations into rainfall prediction leveraged numerical weather prediction
methods, along with statistical and deep learning approaches. This study
introduces deep learning spatial model aimed at enhancing rainfall prediction
accuracy on a finer scale. In this study, we hypothesize that integrating
physical understanding improves the precipitation prediction skill of deep
learning models with high precision for finer spatial scales, such as cities.
To test this hypothesis, we introduce a physics informed ConvLSTM2D model to
predict precipitation 6hr and 12hr ahead for Mumbai, India. We utilize ERA5
reanalysis data select predictor variables, across various geopotential levels.
The ConvLSTM2D model was trained on the target variable precipitation for 4
different grids representing different spatial grid locations of Mumbai. Thus,
the use of the ConvLSTM2D model for rainfall prediction, utilizing physics
informed data from specific grids with limited spatial information, reflects
current advancements in meteorological research that emphasize both efficiency
and localized precision.
\\ ( https://arxiv.org/abs/2404.01122 ,  1620kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01141
Date: Mon, 1 Apr 2024 14:38:51 GMT   (373kb,D)

Title: SoK: A Review of Differentially Private Linear Models For
  High-Dimensional Data
Authors: Amol Khanna and Edward Raff and Nathan Inkawhich
Categories: cs.LG cs.CR stat.ML
Comments: 21 pages, 7 figures. To be published at the 2nd IEEE Conference on
  Secure and Trustworthy Machine Learning (SaTML)
ACM-class: I.2
\\
  Linear models are ubiquitous in data science, but are particularly prone to
overfitting and data memorization in high dimensions. To guarantee the privacy
of training data, differential privacy can be used. Many papers have proposed
optimization techniques for high-dimensional differentially private linear
models, but a systematic comparison between these methods does not exist. We
close this gap by providing a comprehensive review of optimization methods for
private high-dimensional linear models. Empirical tests on all methods
demonstrate robust and coordinate-optimized algorithms perform best, which can
inform future research. Code for implementing all methods is released online.
\\ ( https://arxiv.org/abs/2404.01141 ,  373kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01198
Date: Mon, 1 Apr 2024 15:55:45 GMT   (26kb)

Title: Nearly-tight Approximation Guarantees for the Improving Multi-Armed
  Bandits Problem
Authors: Avrim Blum and Kavya Ravichandran
Categories: cs.LG cs.DS stat.ML
Comments: 12 pages, 0 figures
\\
  We give nearly-tight upper and lower bounds for the improving multi-armed
bandits problem. An instance of this problem has $k$ arms, each of whose reward
function is a concave and increasing function of the number of times that arm
has been pulled so far. We show that for any randomized online algorithm, there
exists an instance on which it must suffer at least an $\Omega(\sqrt{k})$
approximation factor relative to the optimal reward. We then provide a
randomized online algorithm that guarantees an $O(\sqrt{k})$ approximation
factor, if it is told the maximum reward achievable by the optimal arm in
advance. We then show how to remove this assumption at the cost of an extra
$O(\log k)$ approximation factor, achieving an overall $O(\sqrt{k} \log k)$
approximation relative to optimal.
\\ ( https://arxiv.org/abs/2404.01198 ,  26kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01206
Date: Mon, 1 Apr 2024 16:08:18 GMT   (617kb,D)

Title: Machine Unlearning for Traditional Models and Large Language Models: A
  Short Survey
Authors: Yi Xu
Categories: cs.LG cs.CR
Comments: 16 pages
\\
  With the implementation of personal data privacy regulations, the field of
machine learning (ML) faces the challenge of the "right to be forgotten".
Machine unlearning has emerged to address this issue, aiming to delete data and
reduce its impact on models according to user requests. Despite the widespread
interest in machine unlearning, comprehensive surveys on its latest
advancements, especially in the field of Large Language Models (LLMs) is
lacking. This survey aims to fill this gap by providing an in-depth exploration
of machine unlearning, including the definition, classification and evaluation
criteria, as well as challenges in different environments and their solutions.
Specifically, this paper categorizes and investigates unlearning on both
traditional models and LLMs, and proposes methods for evaluating the
effectiveness and efficiency of unlearning, and standards for performance
measurement. This paper reveals the limitations of current unlearning
techniques and emphasizes the importance of a comprehensive unlearning
evaluation to avoid arbitrary forgetting. This survey not only summarizes the
key concepts of unlearning technology but also points out its prominent issues
and feasible directions for future research, providing valuable guidance for
scholars in the field.
\\ ( https://arxiv.org/abs/2404.01206 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01216
Date: Mon, 1 Apr 2024 16:16:19 GMT   (968kb,D)

Title: Novel Node Category Detection Under Subpopulation Shift
Authors: Hsing-Huan Chung, Shravan Chaudhari, Yoav Wald, Xing Han, Joydeep
  Ghosh
Categories: cs.LG cs.SI stat.ML
\\
  In real-world graph data, distribution shifts can manifest in various ways,
such as the emergence of new categories and changes in the relative proportions
of existing categories. It is often important to detect nodes of novel
categories under such distribution shifts for safety or insight discovery
purposes. We introduce a new approach, Recall-Constrained Optimization with
Selective Link Prediction (RECO-SLIP), to detect nodes belonging to novel
categories in attributed graphs under subpopulation shifts. By integrating a
recall-constrained learning framework with a sample-efficient link prediction
mechanism, RECO-SLIP addresses the dual challenges of resilience against
subpopulation shifts and the effective exploitation of graph structure. Our
extensive empirical evaluation across multiple graph datasets demonstrates the
superior performance of RECO-SLIP over existing methods.
\\ ( https://arxiv.org/abs/2404.01216 ,  968kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01217
Date: Mon, 1 Apr 2024 16:17:11 GMT   (5246kb,D)

Title: Incorporating Domain Differential Equations into Graph Convolutional
  Networks to Lower Generalization Discrepancy
Authors: Yue Sun, Chao Chen, Yuesheng Xu, Sihong Xie, Rick S. Blum, Parv
  Venkitasubramaniam
Categories: cs.LG cs.AI
\\
  Ensuring both accuracy and robustness in time series prediction is critical
to many applications, ranging from urban planning to pandemic management. With
sufficient training data where all spatiotemporal patterns are
well-represented, existing deep-learning models can make reasonably accurate
predictions. However, existing methods fail when the training data are drawn
from different circumstances (e.g., traffic patterns on regular days) compared
to test data (e.g., traffic patterns after a natural disaster). Such challenges
are usually classified under domain generalization. In this work, we show that
one way to address this challenge in the context of spatiotemporal prediction
is by incorporating domain differential equations into Graph Convolutional
Networks (GCNs). We theoretically derive conditions where GCNs incorporating
such domain differential equations are robust to mismatched training and
testing data compared to baseline domain agnostic models. To support our
theory, we propose two domain-differential-equation-informed networks called
Reaction-Diffusion Graph Convolutional Network (RDGCN), which incorporates
differential equations for traffic speed evolution, and
Susceptible-Infectious-Recovered Graph Convolutional Network (SIRGCN), which
incorporates a disease propagation model. Both RDGCN and SIRGCN are based on
reliable and interpretable domain differential equations that allow the models
to generalize to unseen patterns. We experimentally show that RDGCN and SIRGCN
are more robust with mismatched testing data than the state-of-the-art deep
learning methods.
\\ ( https://arxiv.org/abs/2404.01217 ,  5246kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01218
Date: Mon, 1 Apr 2024 16:18:40 GMT   (978kb)

Title: Towards System Modelling to Support Diseases Data Extraction from the
  Electronic Health Records for Physicians Research Activities
Authors: Bushra F. Alsaqer, Alaa F. Alsaqer, Amna Asif
Categories: cs.LG cs.IR
Comments: 15 pages, 18 figures and 12 tables
\\
  The use of Electronic Health Records (EHRs) has increased dramatically in the
past 15 years, as, it is considered an important source of managing data od
patients. The EHRs are primary sources of disease diagnosis and demographic
data of patients worldwide. Therefore, the data can be utilized for secondary
tasks such as research. This paper aims to make such data usable for research
activities such as monitoring disease statistics for a specific population. As
a result, the researchers can detect the disease causes for the behavior and
lifestyle of the target group. One of the limitations of EHRs systems is that
the data is not available in the standard format but in various forms.
Therefore, it is required to first convert the names of the diseases and
demographics data into one standardized form to make it usable for research
activities. There is a large amount of EHRs available, and solving the
standardizing issues requires some optimized techniques. We used a first-hand
EHR dataset extracted from EHR systems. Our application uploads the dataset
from the EHRs and converts it to the ICD-10 coding system to solve the
standardization problem. So, we first apply the steps of pre-processing,
annotation, and transforming the data to convert it into the standard form. The
data pre-processing is applied to normalize demographic formats. In the
annotation step, a machine learning model is used to recognize the diseases
from the text. Furthermore, the transforming step converts the disease name to
the ICD-10 coding format. The model was evaluated manually by comparing its
performance in terms of disease recognition with an available dictionary-based
system (MetaMap). The accuracy of the proposed machine learning model is 81%,
that outperformed MetaMap accuracy of 67%. This paper contributed to system
modelling for EHR data extraction to support research activities.
\\ ( https://arxiv.org/abs/2404.01218 ,  978kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01224
Date: Mon, 1 Apr 2024 16:31:06 GMT   (6146kb,D)

Title: Collaborative Pareto Set Learning in Multiple Multi-Objective
  Optimization Problems
Authors: Chikai Shang, Rongguang Ye, Jiaqi Jiang, Fangqing Gu
Categories: cs.LG math.OC
Comments: Accepted by IJCNN 2024
\\
  Pareto Set Learning (PSL) is an emerging research area in multi-objective
optimization, focusing on training neural networks to learn the mapping from
preference vectors to Pareto optimal solutions. However, existing PSL methods
are limited to addressing a single Multi-objective Optimization Problem (MOP)
at a time. When faced with multiple MOPs, this limitation not only leads to
significant inefficiencies but also fails to exploit the potential synergies
across varying MOPs. In this paper, we propose a Collaborative Pareto Set
Learning (CoPSL) framework, which simultaneously learns the Pareto sets of
multiple MOPs in a collaborative manner. CoPSL employs an architecture
consisting of shared and MOP-specific layers, where shared layers aim to
capture common relationships among MOPs collaboratively, and MOP-specific
layers process these relationships to generate solution sets for each MOP. This
collaborative approach enables CoPSL to efficiently learn the Pareto sets of
multiple MOPs in a single run while leveraging the relationships among various
MOPs. To further understand these relationships, we experimentally demonstrate
that there exist shareable representations among MOPs. Leveraging these
collaboratively shared representations can effectively improve the capability
to approximate Pareto sets. Extensive experiments underscore the superior
efficiency and robustness of CoPSL in approximating Pareto sets compared to
state-of-the-art approaches on a variety of synthetic and real-world MOPs. Code
is available at https://github.com/ckshang/CoPSL.
\\ ( https://arxiv.org/abs/2404.01224 ,  6146kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01257
Date: Mon, 1 Apr 2024 17:25:27 GMT   (1486kb,D)

Title: New logarithmic step size for stochastic gradient descent
Authors: M. Soheil Shamaee, S. Fathi Hafshejani, Z. Saeidian
Categories: cs.LG math.OC
Journal-ref: Frontiers of Computer Science, 2025
DOI: 10.1007/s11704-023-3245-z
\\
  In this paper, we propose a novel warm restart technique using a new
logarithmic step size for the stochastic gradient descent (SGD) approach. For
smooth and non-convex functions, we establish an $O(\frac{1}{\sqrt{T}})$
convergence rate for the SGD. We conduct a comprehensive implementation to
demonstrate the efficiency of the newly proposed step size on the
~FashionMinst,~ CIFAR10, and CIFAR100 datasets. Moreover, we compare our
results with nine other existing approaches and demonstrate that the new
logarithmic step size improves test accuracy by $0.9\%$ for the CIFAR100
dataset when we utilize a convolutional neural network (CNN) model.
\\ ( https://arxiv.org/abs/2404.01257 ,  1486kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01270
Date: Mon, 1 Apr 2024 17:46:17 GMT   (506kb,D)

Title: Decentralized Collaborative Learning Framework with External Privacy
  Leakage Analysis
Authors: Tsuyoshi Id\'e and Dzung T. Phan and Rudy Raymond
Categories: cs.LG cs.CR cs.DC
Comments: To appear in Proceeding of 2023 International workshop Blockchain
  Kaigi (BCK 23), JPS Conference Proceedings, 2024
\\
  This paper presents two methodological advancements in decentralized
multi-task learning under privacy constraints, aiming to pave the way for
future developments in next-generation Blockchain platforms. First, we expand
the existing framework for collaborative dictionary learning (CollabDict),
which has previously been limited to Gaussian mixture models, by incorporating
deep variational autoencoders (VAEs) into the framework, with a particular
focus on anomaly detection. We demonstrate that the VAE-based anomaly score
function shares the same mathematical structure as the non-deep model, and
provide comprehensive qualitative comparison. Second, considering the
widespread use of "pre-trained models," we provide a mathematical analysis on
data privacy leakage when models trained with CollabDict are shared externally.
We show that the CollabDict approach, when applied to Gaussian mixtures,
adheres to a Renyi differential privacy criterion. Additionally, we propose a
practical metric for monitoring internal privacy breaches during the learning
process.
\\ ( https://arxiv.org/abs/2404.01270 ,  506kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01273
Date: Mon, 1 Apr 2024 17:48:55 GMT   (198kb,D)

Title: TWIN-GPT: Digital Twins for Clinical Trials via Large Language Model
Authors: Yue Wang, Yingzhou Lu, Yinlong Xu, Zihan Ma, Hongxia Xu, Bang Du,
  Honghao Gao, Jian Wu
Categories: cs.LG cs.CL stat.ME
\\
  Recently, there has been a burgeoning interest in virtual clinical trials,
which simulate real-world scenarios and hold the potential to significantly
enhance patient safety, expedite development, reduce costs, and contribute to
the broader scientific knowledge in healthcare. Existing research often focuses
on leveraging electronic health records (EHRs) to support clinical trial
outcome prediction. Yet, trained with limited clinical trial outcome data,
existing approaches frequently struggle to perform accurate predictions. Some
research has attempted to generate EHRs to augment model development but has
fallen short in personalizing the generation for individual patient profiles.
Recently, the emergence of large language models has illuminated new
possibilities, as their embedded comprehensive clinical knowledge has proven
beneficial in addressing medical issues. In this paper, we propose a large
language model-based digital twin creation approach, called TWIN-GPT. TWIN-GPT
can establish cross-dataset associations of medical information given limited
data, generating unique personalized digital twins for different patients,
thereby preserving individual patient characteristics. Comprehensive
experiments show that using digital twins created by TWIN-GPT can boost
clinical trial outcome prediction, exceeding various previous prediction
approaches. Besides, we also demonstrate that TWIN-GPT can generate
high-fidelity trial data that closely approximate specific patients, aiding in
more accurate result predictions in data-scarce situations. Moreover, our study
provides practical evidence for the application of digital twins in healthcare,
highlighting its potential significance.
\\ ( https://arxiv.org/abs/2404.01273 ,  198kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2312.00938 (*cross-listing*)
Date: Fri, 1 Dec 2023 21:36:14 GMT   (27153kb,D)

Title: WATonoBus: An All Weather Autonomous Shuttle
Authors: Neel P. Bhatt, Ruihe Zhang, Minghao Ning, Ahmad Reza Alghooneh, Joseph
  Sun, Pouya Panahandeh, Ehsan Mohammadbagher, Ted Ecclestone, Ben MacCallum,
  Ehsan Hashemi, Amir Khajepour
Categories: cs.RO cs.AI cs.CV
Comments: 12 pages, 12 figures. This work has been submitted to the IEEE for
  possible publication. Copyright may be transferred without notice, after
  which this version may no longer be accessible
\\
  Autonomous vehicle all-weather operation poses significant challenges,
encompassing modules from perception and decision-making to path planning and
control. The complexity arises from the need to address adverse weather
conditions like rain, snow, and fog across the autonomy stack. Conventional
model-based and single-module approaches often lack holistic integration with
upstream or downstream tasks. We tackle this problem by proposing a
multi-module and modular system architecture with considerations for adverse
weather across the perception level, through features such as snow covered curb
detection, to decision-making and safety monitoring. Through daily weekday
service on the WATonoBus platform for almost a year, we demonstrate that our
proposed approach is capable of addressing adverse weather conditions and
provide valuable learning from edge cases observed during operation.
\\ ( https://arxiv.org/abs/2312.00938 ,  27153kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00012 (*cross-listing*)
Date: Tue, 12 Mar 2024 08:23:30 GMT   (2133kb,D)

Title: Stress index strategy enhanced with financial news sentiment analysis
  for the equity markets
Authors: Baptiste Lefort, Eric Benhamou, Jean-Jacques Ohana, David Saltiel,
  Beatrice Guez, Thomas Jacquot
Categories: q-fin.ST cs.AI cs.CL q-fin.RM
\\
  This paper introduces a new risk-on risk-off strategy for the stock market,
which combines a financial stress indicator with a sentiment analysis done by
ChatGPT reading and interpreting Bloomberg daily market summaries. Forecasts of
market stress derived from volatility and credit spreads are enhanced when
combined with the financial news sentiment derived from GPT-4. As a result, the
strategy shows improved performance, evidenced by higher Sharpe ratio and
reduced maximum drawdowns. The improved performance is consistent across the
NASDAQ, the S&P 500 and the six major equity markets, indicating that the
method generalises across equities markets.
\\ ( https://arxiv.org/abs/2404.00012 ,  2133kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00014 (*cross-listing*)
Date: Fri, 15 Mar 2024 14:45:41 GMT   (5694kb)

Title: Deep Geometry Handling and Fragment-wise Molecular 3D Graph Generation
Authors: Odin Zhang, Yufei Huang, Shichen Cheng, Mengyao Yu, Xujun Zhang,
  Haitao Lin, Yundian Zeng, Mingyang Wang, Zhenxing Wu, Huifeng Zhao, Zaixi
  Zhang, Chenqing Hua, Yu Kang, Sunliang Cui, Peichen Pan, Chang-Yu Hsieh,
  Tingjun Hou
Categories: physics.chem-ph cs.AI q-bio.BM
\\
  Most earlier 3D structure-based molecular generation approaches follow an
atom-wise paradigm, incrementally adding atoms to a partially built molecular
fragment within protein pockets. These methods, while effective in designing
tightly bound ligands, often overlook other essential properties such as
synthesizability. The fragment-wise generation paradigm offers a promising
solution. However, a common challenge across both atom-wise and fragment-wise
methods lies in their limited ability to co-design plausible chemical and
geometrical structures, resulting in distorted conformations. In response to
this challenge, we introduce the Deep Geometry Handling protocol, a more
abstract design that extends the design focus beyond the model architecture.
Through a comprehensive review of existing geometry-related models and their
protocols, we propose a novel hybrid strategy, culminating in the development
of FragGen - a geometry-reliable, fragment-wise molecular generation method.
FragGen marks a significant leap forward in the quality of generated geometry
and the synthesis accessibility of molecules. The efficacy of FragGen is
further validated by its successful application in designing type II kinase
inhibitors at the nanomolar level.
\\ ( https://arxiv.org/abs/2404.00014 ,  5694kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00018 (*cross-listing*)
Date: Tue, 19 Mar 2024 07:41:45 GMT   (26185kb,D)

Title: Can AI Outperform Human Experts in Creating Social Media Creatives?
Authors: Eunkyung Park, Raymond K. Wong, Junbum Kwon
Categories: cs.HC cs.AI cs.SI
Comments: 17 pages, 5 figures
MSC-class: Computation and Language (cs.CL), Artificial Intelligence (cs.AI)
\\
  Artificial Intelligence has outperformed human experts in functional tasks
such as chess and baduk. How about creative tasks? This paper evaluates AI's
capability in the creative domain compared to human experts, which little
research has been conducted so far. We propose a novel Prompt-for-Prompt to
generate social media creatives via prompt augmentation by Large Language
Models. We take the most popular Instagram posts (with the biggest number of
like clicks) in top brands' Instagram accounts to create social media
creatives. We give GPT 4 several prompt instructions with text descriptions to
generate the most effective prompts for cutting-edge text-to-image generators:
Midjourney, DALL E 3, and Stable Diffusion. LLM-augmented prompts can boost
AI's abilities by adding objectives, engagement strategy, lighting and brand
consistency for social media image creation. We conduct an extensive human
evaluation experiment, and find that AI excels human experts, and Midjourney is
better than the other text-to-image generators. Surprisingly, unlike
conventional wisdom in the social media industry, prompt instruction including
eye-catching shows much poorer performance than those including natural.
Regarding the type of creatives, AI improves creatives with animals or products
but less with real people. Also, AI improves creatives with short text
descriptions more than with long text descriptions, because there is more room
for AI to augment prompts with shorter descriptions.
\\ ( https://arxiv.org/abs/2404.00018 ,  26185kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00019 (*cross-listing*)
Date: Tue, 19 Mar 2024 11:43:41 GMT   (1433kb,D)

Title: Advancing Explainable Autonomous Vehicle Systems: A Comprehensive Review
  and Research Roadmap
Authors: Sule Tekkesinoglu, Azra Habibovic and Lars Kunze
Categories: cs.HC cs.AI cs.LG cs.RO
\\
  Given the uncertainty surrounding how existing explainability methods for
autonomous vehicles (AVs) meet the diverse needs of stakeholders, a thorough
investigation is imperative to determine the contexts requiring explanations
and suitable interaction strategies. A comprehensive review becomes crucial to
assess the alignment of current approaches with the varied interests and
expectations within the AV ecosystem. This study presents a review to discuss
the complexities associated with explanation generation and presentation to
facilitate the development of more effective and inclusive explainable AV
systems. Our investigation led to categorising existing literature into three
primary topics: explanatory tasks, explanatory information, and explanatory
information communication. Drawing upon our insights, we have proposed a
comprehensive roadmap for future research centred on (i) knowing the
interlocutor, (ii) generating timely explanations, (ii) communicating
human-friendly explanations, and (iv) continuous learning. Our roadmap is
underpinned by principles of responsible research and innovation, emphasising
the significance of diverse explanation requirements. To effectively tackle the
challenges associated with implementing explainable AV systems, we have
delineated various research directions, including the development of
privacy-preserving data integration, ethical frameworks, real-time analytics,
human-centric interaction design, and enhanced cross-disciplinary
collaborations. By exploring these research directions, the study aims to guide
the development and deployment of explainable AVs, informed by a holistic
understanding of user needs, technological advancements, regulatory compliance,
and ethical considerations, thereby ensuring safer and more trustworthy
autonomous driving experiences.
\\ ( https://arxiv.org/abs/2404.00019 ,  1433kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00022 (*cross-listing*)
Date: Wed, 20 Mar 2024 14:20:42 GMT   (1390kb,D)

Title: Analysing and Organising Human Communications for AI Fairness-Related
  Decisions: Use Cases from the Public Sector
Authors: Mirthe Dankloff, Vanja Skoric, Giovanni Sileno, Sennay Ghebreab, Jacco
  Van Ossenbruggen, Emma Beauxis-Aussalet
Categories: cs.HC cs.AI
\\
  AI algorithms used in the public sector, e.g., for allocating social benefits
or predicting fraud, often involve multiple public and private stakeholders at
various phases of the algorithm's life-cycle. Communication issues between
these diverse stakeholders can lead to misinterpretation and misuse of
algorithms. We investigate the communication processes for AI fairness-related
decisions by conducting interviews with practitioners working on algorithmic
systems in the public sector. By applying qualitative coding analysis, we
identify key elements of communication processes that underlie fairness-related
human decisions. We analyze the division of roles, tasks, skills, and
challenges perceived by stakeholders. We formalize the underlying communication
issues within a conceptual framework that i. represents the communication
patterns ii. outlines missing elements, such as actors who miss skills for
their tasks. The framework is used for describing and analyzing key
organizational issues for fairness-related decisions. Three general patterns
emerge from the analysis: 1. Policy-makers, civil servants, and domain experts
are less involved compared to developers throughout a system's life-cycle. This
leads to developers taking on extra roles such as advisor, while they
potentially miss the required skills and guidance from domain experts. 2.
End-users and policy-makers often lack the technical skills to interpret a
system's limitations, and rely on developer roles for making decisions
concerning fairness issues. 3. Citizens are structurally absent throughout a
system's life-cycle, which may lead to decisions that do not include relevant
considerations from impacted stakeholders.
\\ ( https://arxiv.org/abs/2404.00022 ,  1390kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00026 (*cross-listing*)
Date: Wed, 20 Mar 2024 21:02:16 GMT   (761kb,D)

Title: Ink and Individuality: Crafting a Personalised Narrative in the Age of
  LLMs
Authors: Azmine Toushik Wasi and Raima Islam and Rafia Islam
Categories: cs.HC cs.AI cs.CL cs.IR cs.LG
Comments: 4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive
  Writing Assistants, a hybrid event co-located with The ACM CHI Conference on
  Human Factors in Computing Systems (CHI 2024) Openreview:
  https://openreview.net/forum?id=Pwk5suNA6P
\\
  Individuality and personalization comprise the distinctive characteristics
that make each writer unique and influence their words in order to effectively
engage readers while conveying authenticity. However, our growing reliance on
LLM-based writing assistants risks compromising our creativity and
individuality over time. We often overlook the negative impacts of this trend
on our creativity and uniqueness, despite the possible consequences. This study
investigates these concerns by performing a brief survey to explore different
perspectives and concepts, as well as trying to understand people's viewpoints,
in conjunction with past studies in the area. Addressing these issues is
essential for improving human-computer interaction systems and enhancing
writing assistants for personalization and individuality.
\\ ( https://arxiv.org/abs/2404.00026 ,  761kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00027 (*cross-listing*)
Date: Wed, 20 Mar 2024 21:06:42 GMT   (629kb,D)

Title: LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership
  and Reasoning
Authors: Azmine Toushik Wasi and Rafia Islam and Raima Islam
Categories: cs.HC cs.AI cs.CL cs.CY cs.LG
Comments: 4 Pages, 2 Figures; The Third Workshop on Intelligent and Interactive
  Writing Assistants, a hybrid event co-located with The ACM CHI Conference on
  Human Factors in Computing Systems (CHI 2024) Openreview:
  https://openreview.net/forum?id=wS0Or6FOyz
\\
  Sense of ownership in writing confines our investment of thoughts, time, and
contribution, leading to attachment to the output. However, using writing
assistants introduces a mental dilemma, as some content isn't directly our
creation. For instance, we tend to credit Large Language Models (LLMs) more in
creative tasks, even though all tasks are equal for them. Additionally, while
we may not claim complete ownership of LLM-generated content, we freely claim
authorship. We conduct a short survey to examine these issues and understand
underlying cognitive processes in order to gain a better knowledge of
human-computer interaction in writing and improve writing aid systems.
\\ ( https://arxiv.org/abs/2404.00027 ,  629kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00029 (*cross-listing*)
Date: Thu, 21 Mar 2024 07:27:17 GMT   (792kb,D)

Title: Complementarity in Human-AI Collaboration: Concept, Sources, and
  Evidence
Authors: Patrick Hemmer, Max Schemmer, Niklas K\"uhl, Michael V\"ossing,
  Gerhard Satzger
Categories: cs.HC cs.AI
\\
  Artificial intelligence (AI) can improve human decision-making in various
application areas. Ideally, collaboration between humans and AI should lead to
complementary team performance (CTP) -- a level of performance that neither of
them can attain individually. So far, however, CTP has rarely been observed,
suggesting an insufficient understanding of the complementary constituents in
human-AI collaboration that can contribute to CTP in decision-making. This work
establishes a holistic theoretical foundation for understanding and developing
human-AI complementarity. We conceptualize complementarity by introducing and
formalizing the notion of complementarity potential and its realization.
Moreover, we identify and outline sources that explain CTP. We illustrate our
conceptualization by applying it in two empirical studies exploring two
different sources of complementarity potential. In the first study, we focus on
information asymmetry as a source and, in a real estate appraisal use case,
demonstrate that humans can leverage unique contextual information to achieve
CTP. In the second study, we focus on capability asymmetry as an alternative
source, demonstrating how heterogeneous capabilities can help achieve CTP. Our
work provides researchers with a theoretical foundation of complementarity in
human-AI decision-making and demonstrates that leveraging sources of
complementarity potential constitutes a viable pathway toward effective
human-AI collaboration.
\\ ( https://arxiv.org/abs/2404.00029 ,  792kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00039 (*cross-listing*)
Date: Sun, 24 Mar 2024 02:45:34 GMT   (520kb,D)

Title: MicroHD: An Accuracy-Driven Optimization of Hyperdimensional Computing
  Algorithms for TinyML systems
Authors: Flavio Ponzina and Tajana Rosing
Categories: cs.PF cs.AI cs.LG cs.NE math.OC
Comments: Accepted as a full paper by the tinyML Research Symposium 2024
\\
  Hyperdimensional computing (HDC) is emerging as a promising AI approach that
can effectively target TinyML applications thanks to its lightweight computing
and memory requirements. Previous works on HDC showed that limiting the
standard 10k dimensions of the hyperdimensional space to much lower values is
possible, reducing even more HDC resource requirements. Similarly, other
studies demonstrated that binary values can be used as elements of the
generated hypervectors, leading to significant efficiency gains at the cost of
some degree of accuracy degradation. Nevertheless, current optimization
attempts do not concurrently co-optimize HDC hyper-parameters, and accuracy
degradation is not directly controlled, resulting in sub-optimal HDC models
providing several applications with unacceptable output qualities. In this
work, we propose MicroHD, a novel accuracy-driven HDC optimization approach
that iteratively tunes HDC hyper-parameters, reducing memory and computing
requirements while ensuring user-defined accuracy levels. The proposed method
can be applied to HDC implementations using different encoding functions,
demonstrates good scalability for larger HDC workloads, and achieves
compression and efficiency gains up to 200x when compared to baseline
implementations for accuracy degradations lower than 1%.
\\ ( https://arxiv.org/abs/2404.00039 ,  520kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00042 (*cross-listing*)
Date: Sun, 24 Mar 2024 14:45:11 GMT   (40kb)

Title: Stochastic Optimization with Constraints: A Non-asymptotic
  Instance-Dependent Analysis
Authors: Koulik Khamaru
Categories: math.OC cs.AI cs.LG stat.ML
Comments: 18 pages
\\
  We consider the problem of stochastic convex optimization under convex
constraints. We analyze the behavior of a natural variance reduced proximal
gradient (VRPG) algorithm for this problem. Our main result is a non-asymptotic
guarantee for VRPG algorithm. Contrary to minimax worst case guarantees, our
result is instance-dependent in nature. This means that our guarantee captures
the complexity of the loss function, the variability of the noise, and the
geometry of the constraint set. We show that the non-asymptotic performance of
the VRPG algorithm is governed by the scaled distance (scaled by $\sqrt{N}$)
between the solutions of the given problem and that of a certain small
perturbation of the given problem -- both solved under the given convex
constraints; here, $N$ denotes the number of samples. Leveraging a
well-established connection between local minimax lower bounds and solutions to
perturbed problems, we show that as $N \rightarrow \infty$, the VRPG algorithm
achieves the renowned local minimax lower bound by H\`{a}jek and Le Cam up to
universal constants and a logarithmic factor of the sample size.
\\ ( https://arxiv.org/abs/2404.00042 ,  40kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00044 (*cross-listing*)
Date: Mon, 25 Mar 2024 03:23:03 GMT   (1440kb,D)

Title: UAlign: Pushing the Limit of Template-free Retrosynthesis Prediction
  with Unsupervised SMILES Alignment
Authors: Kaipeng Zeng, Xin Zhao, Yu Zhang, Fan Nie, Xiaokang Yang, Yaohui Jin,
  Yanyan Xu
Categories: physics.chem-ph cs.AI cs.LG q-bio.QM
\\
  Retrosynthesis planning poses a formidable challenge in the organic chemical
industry, particularly in pharmaceuticals. Single-step retrosynthesis
prediction, a crucial step in the planning process, has witnessed a surge in
interest in recent years due to advancements in AI for science. Various deep
learning-based methods have been proposed for this task in recent years,
incorporating diverse levels of additional chemical knowledge dependency. This
paper introduces UAlign, a template-free graph-to-sequence pipeline for
retrosynthesis prediction. By combining graph neural networks and Transformers,
our method can more effectively leverage the inherent graph structure of
molecules. Based on the fact that the majority of molecule structures remain
unchanged during a chemical reaction, we propose a simple yet effective SMILES
alignment technique to facilitate the reuse of unchanged structures for
reactant generation. Extensive experiments show that our method substantially
outperforms state-of-the-art template-free and semi-template-based approaches.
Importantly, Our template-free method achieves effectiveness comparable to, or
even surpasses, established powerful template-based methods. Scientific
contribution: We present a novel graph-to-sequence template-free retrosynthesis
prediction pipeline that overcomes the limitations of Transformer-based methods
in molecular representation learning and insufficient utilization of chemical
information. We propose an unsupervised learning mechanism for establishing
product-atom correspondence with reactant SMILES tokens, achieving even better
results than supervised SMILES alignment methods. Extensive experiments
demonstrate that UAlign significantly outperforms state-of-the-art
template-free methods and rivals or surpasses template-based approaches, with
up to 5\% (top-5) and 5.4\% (top-10) increased accuracy over the strongest
baseline.
\\ ( https://arxiv.org/abs/2404.00044 ,  1440kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00045 (*cross-listing*)
Date: Mon, 25 Mar 2024 04:45:28 GMT   (87kb)

Title: Policy Optimization finds Nash Equilibrium in Regularized General-Sum LQ
  Games
Authors: Muhammad Aneeq uz Zaman, Shubham Aggarwal, Melih Bastopcu, and Tamer
  Ba\c{s}ar
Categories: cs.GT cs.AI cs.LG cs.MA
\\
  In this paper, we investigate the impact of introducing relative entropy
regularization on the Nash Equilibria (NE) of General-Sum $N$-agent games,
revealing the fact that the NE of such games conform to linear Gaussian
policies. Moreover, it delineates sufficient conditions, contingent upon the
adequacy of entropy regularization, for the uniqueness of the NE within the
game. As Policy Optimization serves as a foundational approach for
Reinforcement Learning (RL) techniques aimed at finding the NE, in this work we
prove the linear convergence of a policy optimization algorithm which (subject
to the adequacy of entropy regularization) is capable of provably attaining the
NE. Furthermore, in scenarios where the entropy regularization proves
insufficient, we present a $\delta$-augmentation technique, which facilitates
the achievement of an $\epsilon$-NE within the game.
\\ ( https://arxiv.org/abs/2404.00045 ,  87kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00057 (*cross-listing*)
Date: Tue, 26 Mar 2024 20:10:31 GMT   (306kb,D)

Title: PerOS: Personalized Self-Adapting Operating Systems in the Cloud
Authors: Hongyu H\`e
Categories: cs.HC cs.AI cs.CR cs.OS
Comments: 29 pages, 3 figures
\\
  Operating systems (OSes) are foundational to computer systems, managing
hardware resources and ensuring secure environments for diverse applications.
However, despite their enduring importance, the fundamental design objectives
of OSes have seen minimal evolution over decades. Traditionally prioritizing
aspects like speed, memory efficiency, security, and scalability, these
objectives often overlook the crucial aspect of intelligence as well as
personalized user experience. The lack of intelligence becomes increasingly
critical amid technological revolutions, such as the remarkable advancements in
machine learning (ML).
  Today's personal devices, evolving into intimate companions for users, pose
unique challenges for traditional OSes like Linux and iOS, especially with the
emergence of specialized hardware featuring heterogeneous components.
Furthermore, the rise of large language models (LLMs) in ML has introduced
transformative capabilities, reshaping user interactions and software
development paradigms.
  While existing literature predominantly focuses on leveraging ML methods for
system optimization or accelerating ML workloads, there is a significant gap in
addressing personalized user experiences at the OS level. To tackle this
challenge, this work proposes PerOS, a personalized OS ingrained with LLM
capabilities. PerOS aims to provide tailored user experiences while
safeguarding privacy and personal data through declarative interfaces,
self-adaptive kernels, and secure data management in a scalable cloud-centric
architecture; therein lies the main research question of this work: How can we
develop intelligent, secure, and scalable OSes that deliver personalized
experiences to thousands of users?
\\ ( https://arxiv.org/abs/2404.00057 ,  306kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00060 (*cross-listing*)
Date: Wed, 27 Mar 2024 07:17:16 GMT   (2857kb,D)

Title: Temporal Graph Networks for Graph Anomaly Detection in Financial
  Networks
Authors: Yejin Kim, Youngbin Lee, Minyoung Choe, Sungju Oh, Yongjae Lee
Categories: q-fin.ST cs.AI cs.LG
Comments: Presented at the AAAI 2024 Workshop on AI in Finance for Social
  Impact (https://sites.google.com/view/aifin-aaai2024)
\\
  This paper explores the utilization of Temporal Graph Networks (TGN) for
financial anomaly detection, a pressing need in the era of fintech and
digitized financial transactions. We present a comprehensive framework that
leverages TGN, capable of capturing dynamic changes in edges within financial
networks, for fraud detection. Our study compares TGN's performance against
static Graph Neural Network (GNN) baselines, as well as cutting-edge hypergraph
neural network baselines using DGraph dataset for a realistic financial
context. Our results demonstrate that TGN significantly outperforms other
models in terms of AUC metrics. This superior performance underlines TGN's
potential as an effective tool for detecting financial fraud, showcasing its
ability to adapt to the dynamic and complex nature of modern financial systems.
We also experimented with various graph embedding modules within the TGN
framework and compared the effectiveness of each module. In conclusion, we
demonstrated that, even with variations within TGN, it is possible to achieve
good performance in the anomaly detection task.
\\ ( https://arxiv.org/abs/2404.00060 ,  2857kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00076 (*cross-listing*)
Date: Fri, 29 Mar 2024 00:09:48 GMT   (6332kb,D)

Title: A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping
  Attacks
Authors: Orson Mengara
Categories: cs.CR cs.AI cs.CL cs.LG eess.SP
Comments: Accept by "IEEE Access" let's take a look at our global approach to
  the DNN(s) model(s) deployment chain in production: Danger NLP-Speech(Trigger
  universal approach)
DOI: 10.1109/ACCESS.2024.3382839
\\
  Audio-based machine learning systems frequently use public or third-party
data, which might be inaccurate. This exposes deep neural network (DNN) models
trained on such data to potential data poisoning attacks. In this type of
assault, attackers can train the DNN model using poisoned data, potentially
degrading its performance. Another type of data poisoning attack that is
extremely relevant to our investigation is label flipping, in which the
attacker manipulates the labels for a subset of data. It has been demonstrated
that these assaults may drastically reduce system performance, even for
attackers with minimal abilities. In this study, we propose a backdoor attack
named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to
input triggers (clapping) in the selected data patterns associated with the
target class, thereby enabling a stealthy backdoor.
\\ ( https://arxiv.org/abs/2404.00076 ,  6332kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00081 (*cross-listing*)
Date: Fri, 29 Mar 2024 08:55:39 GMT   (1998kb)

Title: Molecular Generative Adversarial Network with Multi-Property
  Optimization
Authors: Huidong Tang, Chen Li, Sayaka Kamei, Yoshihiro Yamanishi, Yasuhiko
  Morimoto
Categories: q-bio.BM cs.AI cs.LG
\\
  Deep generative models, such as generative adversarial networks (GANs), have
been employed for $de~novo$ molecular generation in drug discovery. Most prior
studies have utilized reinforcement learning (RL) algorithms, particularly
Monte Carlo tree search (MCTS), to handle the discrete nature of molecular
representations in GANs. However, due to the inherent instability in training
GANs and RL models, along with the high computational cost associated with MCTS
sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To
tackle these challenges, this study introduces a novel GAN based on
actor-critic RL with instant and global rewards, called InstGAN, to generate
molecules at the token-level with multi-property optimization. Furthermore,
maximized information entropy is leveraged to alleviate the mode collapse. The
experimental results demonstrate that InstGAN outperforms other baselines,
achieves comparable performance to state-of-the-art models, and efficiently
generates molecules with multi-property optimization. The source code will be
released upon acceptance of the paper.
\\ ( https://arxiv.org/abs/2404.00081 ,  1998kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00107 (*cross-listing*)
Date: Fri, 29 Mar 2024 18:38:59 GMT   (2212kb)

Title: Robust Ensemble Person Re-Identification via Orthogonal Fusion with
  Occlusion Handling
Authors: Syeda Nyma Ferdous, Xin Li
Categories: cs.CV cs.AI
\\
  Occlusion remains one of the major challenges in person reidentification
(ReID) as a result of the diversity of poses and the variation of appearances.
Developing novel architectures to improve the robustness of occlusion-aware
person Re-ID requires new insights, especially on low-resolution edge cameras.
We propose a deep ensemble model that harnesses both CNN and Transformer
architectures to generate robust feature representations. To achieve robust
Re-ID without the need to manually label occluded regions, we propose to take
an ensemble learning-based approach derived from the analogy between
arbitrarily shaped occluded regions and robust feature representation. Using
the orthogonality principle, our developed deep CNN model makes use of masked
autoencoder (MAE) and global-local feature fusion for robust person
identification. Furthermore, we present a part occlusion-aware transformer
capable of learning feature space that is robust to occluded regions.
Experimental results are reported on several Re-ID datasets to show the
effectiveness of our developed ensemble model named orthogonal fusion with
occlusion handling (OFOH). Compared to competing methods, the proposed OFOH
approach has achieved competent rank-1 and mAP performance.
\\ ( https://arxiv.org/abs/2404.00107 ,  2212kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00137 (*cross-listing*)
Date: Fri, 29 Mar 2024 20:19:36 GMT   (132kb)

Title: Budget-aware Query Tuning: An AutoML Perspective
Authors: Wentao Wu and Chi Wang
Categories: cs.DB cs.AI cs.LG
\\
  Modern database systems rely on cost-based query optimizers to come up with
good execution plans for input queries. Such query optimizers rely on cost
models to estimate the costs of candidate query execution plans. A cost model
represents a function from a set of cost units to query execution cost, where
each cost unit specifies the unit cost of executing a certain type of query
processing operation (such as table scan or join). These cost units are
traditionally viewed as constants, whose values only depend on the platform
configuration where the database system runs on top of but are invariant for
queries processed by the database system. In this paper, we challenge this
classic view by thinking of these cost units as variables instead. We show
that, by varying the cost-unit values one can obtain query plans that
significantly outperform the default query plans returned by the query
optimizer when viewing the cost units as constants. We term this cost-unit
tuning process "query tuning" (QT) and show that it is similar to the
well-known hyper-parameter optimization (HPO) problem in AutoML. As a result,
any state-of-the-art HPO technologies can be applied to QT. We study the QT
problem in the context of anytime tuning, which is desirable in practice by
constraining the total time spent on QT within a given budget -- we call this
problem budget-aware query tuning. We further extend our study from tuning a
single query to tuning a workload with multiple queries, and we call this
generalized problem budget-aware workload tuning (WT), which aims for
minimizing the execution time of the entire workload. WT is more challenging as
one needs to further prioritize individual query tuning within the given time
budget. We propose solutions to both QT and WT and experimental evaluation
using both benchmark and real workloads demonstrates the efficacy of our
proposed solutions.
\\ ( https://arxiv.org/abs/2404.00137 ,  132kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00139 (*cross-listing*)
Date: Fri, 29 Mar 2024 20:28:30 GMT   (358kb)

Title: Security Risks Concerns of Generative AI in the IoT
Authors: Honghui Xu, Yingshu Li, Olusesi Balogun, Shaoen Wu, Yue Wang, Zhipeng
  Cai
Categories: cs.CR cs.AI
Comments: 6 pages, 2 figures
\\
  In an era where the Internet of Things (IoT) intersects increasingly with
generative Artificial Intelligence (AI), this article scrutinizes the emergent
security risks inherent in this integration. We explore how generative AI
drives innovation in IoT and we analyze the potential for data breaches when
using generative AI and the misuse of generative AI technologies in IoT
ecosystems. These risks not only threaten the privacy and efficiency of IoT
systems but also pose broader implications for trust and safety in AI-driven
environments. The discussion in this article extends to strategic approaches
for mitigating these risks, including the development of robust security
protocols, the multi-layered security approaches, and the adoption of AI
technological solutions. Through a comprehensive analysis, this article aims to
shed light on the critical balance between embracing AI advancements and
ensuring stringent security in IoT, providing insights into the future
direction of these intertwined technologies.
\\ ( https://arxiv.org/abs/2404.00139 ,  358kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00143 (*cross-listing*)
Date: Fri, 29 Mar 2024 20:31:07 GMT   (41007kb,D)

Title: Accelerating Search-Based Planning for Multi-Robot Manipulation by
  Leveraging Online-Generated Experiences
Authors: Yorai Shaoul, Itamar Mishani, Maxim Likhachev, Jiaoyang Li
Categories: cs.RO cs.AI cs.MA
Comments: The first two authors contributed equally. Accepted to ICAPS 2024
\\
  An exciting frontier in robotic manipulation is the use of multiple arms at
once. However, planning concurrent motions is a challenging task using current
methods. The high-dimensional composite state space renders many well-known
motion planning algorithms intractable. Recently, Multi-Agent Path-Finding
(MAPF) algorithms have shown promise in discrete 2D domains, providing rigorous
guarantees. However, widely used conflict-based methods in MAPF assume an
efficient single-agent motion planner. This poses challenges in adapting them
to manipulation cases where this assumption does not hold, due to the high
dimensionality of configuration spaces and the computational bottlenecks
associated with collision checking. To this end, we propose an approach for
accelerating conflict-based search algorithms by leveraging their repetitive
and incremental nature -- making them tractable for use in complex scenarios
involving multi-arm coordination in obstacle-laden environments. We show that
our method preserves completeness and bounded sub-optimality guarantees, and
demonstrate its practical efficacy through a set of experiments with up to 10
robotic arms.
\\ ( https://arxiv.org/abs/2404.00143 ,  41007kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00166 (*cross-listing*)
Date: Fri, 29 Mar 2024 21:45:53 GMT   (19855kb,D)

Title: Uncovering Bias in Large Vision-Language Models with Counterfactuals
Authors: Phillip Howard, Anahita Bhiwandiwalla, Kathleen C. Fraser, Svetlana
  Kiritchenko
Categories: cs.CV cs.AI
\\
  With the advent of Large Language Models (LLMs) possessing increasingly
impressive capabilities, a number of Large Vision-Language Models (LVLMs) have
been proposed to augment LLMs with visual inputs. Such models condition
generated text on both an input image and a text prompt, enabling a variety of
use cases such as visual question answering and multimodal chat. While prior
studies have examined the social biases contained in text generated by LLMs,
this topic has been relatively unexplored in LVLMs. Examining social biases in
LVLMs is particularly challenging due to the confounding contributions of bias
induced by information contained across the text and visual modalities. To
address this challenging problem, we conduct a large-scale study of text
generated by different LVLMs under counterfactual changes to input images.
Specifically, we present LVLMs with identical open-ended text prompts while
conditioning on images from different counterfactual sets, where each set
contains images which are largely identical in their depiction of a common
subject (e.g., a doctor), but vary only in terms of intersectional social
attributes (e.g., race and gender). We comprehensively evaluate the text
produced by different LVLMs under this counterfactual generation setting and
find that social attributes such as race, gender, and physical characteristics
depicted in input images can significantly influence toxicity and the
generation of competency-associated words.
\\ ( https://arxiv.org/abs/2404.00166 ,  19855kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00172 (*cross-listing*)
Date: Fri, 29 Mar 2024 22:03:53 GMT   (16959kb,D)

Title: Universal Bovine Identification via Depth Data and Deep Metric Learning
Authors: Asheesh Sharma, Lucy Randewich, William Andrew, Sion Hannuna, Neill
  Campbell, Siobhan Mullan, Andrew W. Dowsey, Melvyn Smith, Mark Hansen, Tilo
  Burghardt
Categories: cs.CV cs.AI cs.LG
Comments: LaTeX, 38 pages, 14 figures, 3 tables
\\
  This paper proposes and evaluates, for the first time, a top-down (dorsal
view), depth-only deep learning system for accurately identifying individual
cattle and provides associated code, datasets, and training weights for
immediate reproducibility. An increase in herd size skews the cow-to-human
ratio at the farm and makes the manual monitoring of individuals more
challenging. Therefore, real-time cattle identification is essential for the
farms and a crucial step towards precision livestock farming. Underpinned by
our previous work, this paper introduces a deep-metric learning method for
cattle identification using depth data from an off-the-shelf 3D camera. The
method relies on CNN and MLP backbones that learn well-generalised embedding
spaces from the body shape to differentiate individuals -- requiring neither
species-specific coat patterns nor close-up muzzle prints for operation. The
network embeddings are clustered using a simple algorithm such as $k$-NN for
highly accurate identification, thus eliminating the need to retrain the
network for enrolling new individuals. We evaluate two backbone architectures,
ResNet, as previously used to identify Holstein Friesians using RGB images, and
PointNet, which is specialised to operate on 3D point clouds. We also present
CowDepth2023, a new dataset containing 21,490 synchronised colour-depth image
pairs of 99 cows, to evaluate the backbones. Both ResNet and PointNet
architectures, which consume depth maps and point clouds, respectively, led to
high accuracy that is on par with the coat pattern-based backbone.
\\ ( https://arxiv.org/abs/2404.00172 ,  16959kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00178 (*cross-listing*)
Date: Fri, 29 Mar 2024 22:23:35 GMT   (3792kb,D)

Title: Beyond Suspension: A Two-phase Methodology for Concluding Sports Leagues
Authors: Ali Hassanzadeh, Mojtaba Hosseini, John G. Turner
Categories: math.OC cs.AI cs.LG
Comments: 32 pages, 9 figures
MSC-class: 90B50 (Primary) 90C06, 90C11, 90C90 (Secondary)
\\
  Problem definition: Professional sports leagues may be suspended due to
various reasons such as the recent COVID-19 pandemic. A critical question the
league must address when re-opening is how to appropriately select a subset of
the remaining games to conclude the season in a shortened time frame.
Academic/practical relevance: Despite the rich literature on scheduling an
entire season starting from a blank slate, concluding an existing season is
quite different. Our approach attempts to achieve team rankings similar to that
which would have resulted had the season been played out in full. Methodology:
We propose a data-driven model which exploits predictive and prescriptive
analytics to produce a schedule for the remainder of the season comprised of a
subset of originally-scheduled games. Our model introduces novel rankings-based
objectives within a stochastic optimization model, whose parameters are first
estimated using a predictive model. We introduce a deterministic equivalent
reformulation along with a tailored Frank-Wolfe algorithm to efficiently solve
our problem, as well as a robust counterpart based on min-max regret. Results:
We present simulation-based numerical experiments from previous National
Basketball Association (NBA) seasons 2004--2019, and show that our models are
computationally efficient, outperform a greedy benchmark that approximates a
non-rankings-based scheduling policy, and produce interpretable results.
Managerial implications: Our data-driven decision-making framework may be used
to produce a shortened season with 25-50\% fewer games while still producing an
end-of-season ranking similar to that of the full season, had it been played.
\\ ( https://arxiv.org/abs/2404.00178 ,  3792kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00185 (*cross-listing*)
Date: Fri, 29 Mar 2024 22:51:45 GMT   (43677kb,D)

Title: On Inherent Adversarial Robustness of Active Vision Systems
Authors: Amitangshu Mukherjee, Timur Ibrayev, and Kaushik Roy
Categories: cs.CV cs.AI
\\
  Current Deep Neural Networks are vulnerable to adversarial examples, which
alter their predictions by adding carefully crafted noise. Since human eyes are
robust to such inputs, it is possible that the vulnerability stems from the
standard way of processing inputs in one shot by processing every pixel with
the same importance. In contrast, neuroscience suggests that the human vision
system can differentiate salient features by (1) switching between multiple
fixation points (saccades) and (2) processing the surrounding with a
non-uniform external resolution (foveation). In this work, we advocate that the
integration of such active vision mechanisms into current deep learning systems
can offer robustness benefits. Specifically, we empirically demonstrate the
inherent robustness of two active vision methods - GFNet and FALcon - under a
black box threat model. By learning and inferencing based on downsampled
glimpses obtained from multiple distinct fixation points within an input, we
show that these active methods achieve (2-3) times greater robustness compared
to a standard passive convolutional network under state-of-the-art adversarial
attacks. More importantly, we provide illustrative and interpretable
visualization analysis that demonstrates how performing inference from distinct
fixation points makes active vision methods less vulnerable to malicious
inputs.
\\ ( https://arxiv.org/abs/2404.00185 ,  43677kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00231 (*cross-listing*)
Date: Sat, 30 Mar 2024 03:23:52 GMT   (28424kb)

Title: Attention-based Shape-Deformation Networks for Artifact-Free Geometry
  Reconstruction of Lumbar Spine from MR Images
Authors: Linchen Qian, Jiasong Chen, Linhai Ma, Timur Urakov, Weiyong Gu, Liang
  Liang
Categories: cs.CV cs.AI cs.LG
\\
  Lumbar disc degeneration, a progressive structural wear and tear of lumbar
intervertebral disc, is regarded as an essential role on low back pain, a
significant global health concern. Automated lumbar spine geometry
reconstruction from MR images will enable fast measurement of medical
parameters to evaluate the lumbar status, in order to determine a suitable
treatment. Existing image segmentation-based techniques often generate
erroneous segments or unstructured point clouds, unsuitable for medical
parameter measurement. In this work, we present TransDeformer: a novel
attention-based deep learning approach that reconstructs the contours of the
lumbar spine with high spatial accuracy and mesh correspondence across
patients, and we also present a variant of TransDeformer for error estimation.
Specially, we devise new attention modules with a new attention formula, which
integrates image features and tokenized contour features to predict the
displacements of the points on a shape template without the need for image
segmentation. The deformed template reveals the lumbar spine geometry in the
input image. We develop a multi-stage training strategy to enhance model
robustness with respect to template initialization. Experiment results show
that our TransDeformer generates artifact-free geometry outputs, and its
variant predicts the error of a reconstructed geometry. Our code is available
at https://github.com/linchenq/TransDeformer-Mesh.
\\ ( https://arxiv.org/abs/2404.00231 ,  28424kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00247 (*cross-listing*)
Date: Sat, 30 Mar 2024 04:58:59 GMT   (72kb)

Title: Facilitating Reinforcement Learning for Process Control Using Transfer
  Learning: Perspectives
Authors: Runze Lin, Junghui Chen, Lei Xie, Hongye Su, Biao Huang
Categories: eess.SY cs.AI cs.LG cs.SY
\\
  This paper provides insights into deep reinforcement learning (DRL) for
process control from the perspective of transfer learning. We analyze the
challenges of applying DRL in the field of process industries and the necessity
of introducing transfer learning. Furthermore, recommendations and prospects
are provided for future research directions on how transfer learning can be
integrated with DRL to empower process control.
\\ ( https://arxiv.org/abs/2404.00247 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00257 (*cross-listing*)
Date: Sat, 30 Mar 2024 06:17:39 GMT   (8804kb,D)

Title: YOLOOC: YOLO-based Open-Class Incremental Object Detection with Novel
  Class Discovery
Authors: Qian Wan, Xiang Xiang, Qinhao Zhou
Categories: cs.CV cs.AI cs.LG eess.IV
Comments: Initially submitted to ACCV 2022
\\
  Because of its use in practice, open-world object detection (OWOD) has gotten
a lot of attention recently. The challenge is how can a model detect novel
classes and then incrementally learn them without forgetting previously known
classes. Previous approaches hinge on strongly-supervised or weakly-supervised
novel-class data for novel-class detection, which may not apply to real
applications. We construct a new benchmark that novel classes are only
encountered at the inference stage. And we propose a new OWOD detector YOLOOC,
based on the YOLO architecture yet for the Open-Class setup. We introduce label
smoothing to prevent the detector from over-confidently mapping novel classes
to known classes and to discover novel classes. Extensive experiments conducted
on our more realistic setup demonstrate the effectiveness of our method for
discovering novel classes in our new benchmark.
\\ ( https://arxiv.org/abs/2404.00257 ,  8804kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00261 (*cross-listing*)
Date: Sat, 30 Mar 2024 06:21:56 GMT   (1186kb,D)

Title: A Simple Yet Effective Approach for Diversified Session-Based
  Recommendation
Authors: Qing Yin, Hui Fang, Zhu Sun, and Yew-Soon Ong
Categories: cs.IR cs.AI
\\
  Session-based recommender systems (SBRSs) have become extremely popular in
view of the core capability of capturing short-term and dynamic user
preferences. However, most SBRSs primarily maximize recommendation accuracy but
ignore user minor preferences, thus leading to filter bubbles in the long run.
Only a handful of works, being devoted to improving diversity, depend on unique
model designs and calibrated loss functions, which cannot be easily adapted to
existing accuracy-oriented SBRSs. It is thus worthwhile to come up with a
simple yet effective design that can be used as a plugin to facilitate existing
SBRSs on generating a more diversified list in the meantime preserving the
recommendation accuracy. In this case, we propose an end-to-end framework
applied for every existing representative (accuracy-oriented) SBRS, called
diversified category-aware attentive SBRS (DCA-SBRS), to boost the performance
on recommendation diversity. It consists of two novel designs: a model-agnostic
diversity-oriented loss function, and a non-invasive category-aware attention
mechanism. Extensive experiments on three datasets showcase that our framework
helps existing SBRSs achieve extraordinary performance in terms of
recommendation diversity and comprehensive performance, without significantly
deteriorating recommendation accuracy compared to state-of-the-art
accuracy-oriented SBRSs.
\\ ( https://arxiv.org/abs/2404.00261 ,  1186kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00285 (*cross-listing*)
Date: Sat, 30 Mar 2024 08:37:19 GMT   (4590kb,D)

Title: Long-Tailed Recognition on Binary Networks by Calibrating A Pre-trained
  Model
Authors: Jihun Kim, Dahyun Kim, Hyungrok Jung, Taeil Oh, Jonghyun Choi
Categories: cs.CV cs.AI
\\
  Deploying deep models in real-world scenarios entails a number of challenges,
including computational efficiency and real-world (e.g., long-tailed) data
distributions. We address the combined challenge of learning long-tailed
distributions using highly resource-efficient binary neural networks as
backbones. Specifically, we propose a calibrate-and-distill framework that uses
off-the-shelf pretrained full-precision models trained on balanced datasets to
use as teachers for distillation when learning binary networks on long-tailed
datasets. To better generalize to various datasets, we further propose a novel
adversarial balancing among the terms in the objective function and an
efficient multiresolution learning scheme. We conducted the largest empirical
study in the literature using 15 datasets, including newly derived long-tailed
datasets from existing balanced datasets, and show that our proposed method
outperforms prior art by large margins (>14.33% on average).
\\ ( https://arxiv.org/abs/2404.00285 ,  4590kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00306 (*cross-listing*)
Date: Sat, 30 Mar 2024 10:07:02 GMT   (430kb)

Title: Leveraging Intelligent Recommender system as a first step resilience
  measure -- A data-driven supply chain disruption response framework
Authors: Yang Hu
Categories: cs.CE cs.AI
\\
  Interests in the value of digital technologies for its potential uses to
increase supply chain resilience (SCRes) are increasing in light to the
industry 4.0 and the global pandemic. Utilization of Recommender systems (RS)
as a supply chain (SC) resilience measure is neglected although RS is a capable
tool to enhance SC resilience from a reactive aspect. To address this problem,
this research proposed a novel data-driven supply chain disruption response
framework based on the intelligent recommender system techniques and validated
the conceptual model through a practical use case. Results show that our
framework can be implemented as an effective SC disruption mitigation measure
in the very first response phrase and help SC participants get better reaction
performance after the SC disruption.
\\ ( https://arxiv.org/abs/2404.00306 ,  430kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00312 (*cross-listing*)
Date: Sat, 30 Mar 2024 10:25:28 GMT   (1341kb,D)

Title: Bayesian Exploration of Pre-trained Models for Low-shot Image
  Classification
Authors: Yibo Miao, Yu Lei, Feng Zhou, Zhijie Deng
Categories: cs.CV cs.AI
\\
  Low-shot image classification is a fundamental task in computer vision, and
the emergence of large-scale vision-language models such as CLIP has greatly
advanced the forefront of research in this field. However, most existing
CLIP-based methods lack the flexibility to effectively incorporate other
pre-trained models that encompass knowledge distinct from CLIP. To bridge the
gap, this work proposes a simple and effective probabilistic model ensemble
framework based on Gaussian processes, which have previously demonstrated
remarkable efficacy in processing small data. We achieve the integration of
prior knowledge by specifying the mean function with CLIP and the kernel
function with an ensemble of deep kernels built upon various pre-trained
models. By regressing the classification label directly, our framework enables
analytical inference, straightforward uncertainty quantification, and
principled hyper-parameter tuning. Through extensive experiments on standard
benchmarks, we demonstrate that our method consistently outperforms competitive
ensemble baselines regarding predictive performance. Additionally, we assess
the robustness of our method and the quality of the yielded uncertainty
estimates on out-of-distribution datasets. We also illustrate that our method,
despite relying on label regression, still enjoys superior model calibration
compared to most deterministic baselines.
\\ ( https://arxiv.org/abs/2404.00312 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00330 (*cross-listing*)
Date: Sat, 30 Mar 2024 12:01:04 GMT   (5396kb,D)

Title: Memory-Scalable and Simplified Functional Map Learning
Authors: Robin Magnet, Maks Ovsjanikov
Categories: cs.CV cs.AI
\\
  Deep functional maps have emerged in recent years as a prominent
learning-based framework for non-rigid shape matching problems. While early
methods in this domain only focused on learning in the functional domain, the
latest techniques have demonstrated that by promoting consistency between
functional and pointwise maps leads to significant improvements in accuracy.
Unfortunately, existing approaches rely heavily on the computation of large
dense matrices arising from soft pointwise maps, which compromises their
efficiency and scalability. To address this limitation, we introduce a novel
memory-scalable and efficient functional map learning pipeline. By leveraging
the specific structure of functional maps, we offer the possibility to achieve
identical results without ever storing the pointwise map in memory.
Furthermore, based on the same approach, we present a differentiable map
refinement layer adapted from an existing axiomatic refinement algorithm.
Unlike many functional map learning methods, which use this algorithm at a
post-processing step, ours can be easily used at train time, enabling to
enforce consistency between the refined and initial versions of the map. Our
resulting approach is both simpler, more efficient and more numerically stable,
by avoiding differentiation through a linear system, while achieving close to
state-of-the-art results in challenging scenarios.
\\ ( https://arxiv.org/abs/2404.00330 ,  5396kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00364 (*cross-listing*)
Date: Sat, 30 Mar 2024 13:34:54 GMT   (16638kb,D)

Title: Accurate Cutting-point Estimation for Robotic Lychee Harvesting through
  Geometry-aware Learning
Authors: Gengming Zhang, Hao Cao, Kewei Hu, Yaoqiang Pan, Yuqin Deng, Hongjun
  Wang, Hanwen Kang
Categories: cs.RO cs.AI
\\
  Accurately identifying lychee-picking points in unstructured orchard
environments and obtaining their coordinate locations is critical to the
success of lychee-picking robots. However, traditional two-dimensional (2D)
image-based object detection methods often struggle due to the complex
geometric structures of branches, leaves and fruits, leading to incorrect
determination of lychee picking points. In this study, we propose a
Fcaf3d-lychee network model specifically designed for the accurate localisation
of lychee picking points. Point cloud data of lychee picking points in natural
environments are acquired using Microsoft's Azure Kinect DK time-of-flight
(TOF) camera through multi-view stitching. We augment the Fully Convolutional
Anchor-Free 3D Object Detection (Fcaf3d) model with a
squeeze-and-excitation(SE) module, which exploits human visual attention
mechanisms for improved feature extraction of lychee picking points. The
trained network model is evaluated on a test set of lychee-picking locations
and achieves an impressive F1 score of 88.57%, significantly outperforming
existing models. Subsequent three-dimensional (3D) position detection of
picking points in real lychee orchard environments yields high accuracy, even
under varying degrees of occlusion. Localisation errors of lychee picking
points are within 1.5 cm in all directions, demonstrating the robustness and
generality of the model.
\\ ( https://arxiv.org/abs/2404.00364 ,  16638kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00369 (*cross-listing*)
Date: Sat, 30 Mar 2024 13:44:17 GMT   (4101kb)

Title: Worker Robot Cooperation and Integration into the Manufacturing Workcell
  via the Holonic Control Architecture
Authors: Ahmed R. Sadik, Bodo Urban, Omar Adel
Categories: cs.RO cs.AI cs.MA cs.SY eess.SY
\\
  Worker-Robot Cooperation is a new industrial trend, which aims to sum the
advantages of both the human and the industrial robot to afford a new
intelligent manufacturing techniques. The cooperative manufacturing between the
worker and the robot contains other elements such as the product parts and the
manufacturing tools. All these production elements must cooperate in one
manufacturing workcell to fulfill the production requirements. The
manufacturing control system is the mean to connect all these cooperative
elements together in one body. This manufacturing control system is distributed
and autonomous due to the nature of the cooperative workcell. Accordingly, this
article proposes the holonic control architecture as the manufacturing concept
of the cooperative workcell. Furthermore, the article focuses on the
feasibility of this manufacturing concept, by applying it over a case study
that involves the cooperation between a dual-arm robot and a worker. During
this case study, the worker uses a variety of hand gestures to cooperate with
the robot to achieve the highest production flexibility
\\ ( https://arxiv.org/abs/2404.00369 ,  4101kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00383 (*cross-listing*)
Date: Sat, 30 Mar 2024 14:51:01 GMT   (616kb,D)

Title: SpikingJET: Enhancing Fault Injection for Fully and Convolutional
  Spiking Neural Networks
Authors: Anil Bayram Gogebakan, Enrico Magliano, Alessio Carpegna, Annachiara
  Ruospo, Alessandro Savino, Stefano Di Carlo
Categories: cs.NE cs.AI
ACM-class: I.2
\\
  As artificial neural networks become increasingly integrated into
safety-critical systems such as autonomous vehicles, devices for medical
diagnosis, and industrial automation, ensuring their reliability in the face of
random hardware faults becomes paramount. This paper introduces SpikingJET, a
novel fault injector designed specifically for fully connected and
convolutional Spiking Neural Networks (SNNs). Our work underscores the critical
need to evaluate the resilience of SNNs to hardware faults, considering their
growing prominence in real-world applications. SpikingJET provides a
comprehensive platform for assessing the resilience of SNNs by inducing errors
and injecting faults into critical components such as synaptic weights, neuron
model parameters, internal states, and activation functions. This paper
demonstrates the effectiveness of Spiking-JET through extensive software-level
experiments on various SNN architectures, revealing insights into their
vulnerability and resilience to hardware faults. Moreover, highlighting the
importance of fault resilience in SNNs contributes to the ongoing effort to
enhance the reliability and safety of Neural Network (NN)-powered systems in
diverse domains.
\\ ( https://arxiv.org/abs/2404.00383 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00385 (*cross-listing*)
Date: Sat, 30 Mar 2024 14:58:40 GMT   (4744kb,D)

Title: Constrained Layout Generation with Factor Graphs
Authors: Mohammed Haroon Dupty, Yanfei Dong, Sicong Leng, Guoji Fu, Yong Liang
  Goh, Wei Lu, Wee Sun Lee
Categories: cs.CV cs.AI cs.LG
Comments: To be published at IEEE/CVF CVPR 2024
\\
  This paper addresses the challenge of object-centric layout generation under
spatial constraints, seen in multiple domains including floorplan design
process. The design process typically involves specifying a set of spatial
constraints that include object attributes like size and inter-object relations
such as relative positioning. Existing works, which typically represent objects
as single nodes, lack the granularity to accurately model complex interactions
between objects. For instance, often only certain parts of an object, like a
room's right wall, interact with adjacent objects. To address this gap, we
introduce a factor graph based approach with four latent variable nodes for
each room, and a factor node for each constraint. The factor nodes represent
dependencies among the variables to which they are connected, effectively
capturing constraints that are potentially of a higher order. We then develop
message-passing on the bipartite graph, forming a factor graph neural network
that is trained to produce a floorplan that aligns with the desired
requirements. Our approach is simple and generates layouts faithful to the user
requirements, demonstrated by a large improvement in IOU scores over existing
methods. Additionally, our approach, being inferential and accurate, is
well-suited to the practical human-in-the-loop design process where
specifications evolve iteratively, offering a practical and powerful tool for
AI-guided design.
\\ ( https://arxiv.org/abs/2404.00385 ,  4744kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00413 (*cross-listing*)
Date: Sat, 30 Mar 2024 16:43:59 GMT   (9772kb,D)

Title: Language Models are Spacecraft Operators
Authors: Victor Rodriguez-Fernandez, Alejandro Carrasco, Jason Cheng, Eli
  Scharf, Peng Mun Siew, Richard Linares
Categories: physics.space-ph cs.AI cs.LG
Comments: Source code available on Github at:
  https://github.com/ARCLab-MIT/kspdg
\\
  Recent trends are emerging in the use of Large Language Models (LLMs) as
autonomous agents that take actions based on the content of the user text
prompts. We intend to apply these concepts to the field of Guidance,
Navigation, and Control in space, enabling LLMs to have a significant role in
the decision-making process for autonomous satellite operations. As a first
step towards this goal, we have developed a pure LLM-based solution for the
Kerbal Space Program Differential Games (KSPDG) challenge, a public software
design competition where participants create autonomous agents for maneuvering
satellites involved in non-cooperative space operations, running on the KSP
game engine. Our approach leverages prompt engineering, few-shot prompting, and
fine-tuning techniques to create an effective LLM-based agent that ranked 2nd
in the competition. To the best of our knowledge, this work pioneers the
integration of LLM agents into space research. Code is available at
https://github.com/ARCLab-MIT/kspdg.
\\ ( https://arxiv.org/abs/2404.00413 ,  9772kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00424 (*cross-listing*)
Date: Sat, 30 Mar 2024 17:18:00 GMT   (1531kb,D)

Title: From attention to profit: quantitative trading strategy based on
  transformer
Authors: Zhaofeng Zhang, Banghao Chen, Shengxin Zhu, Nicolas Langren\'e
Categories: q-fin.MF cs.AI cs.CE
ACM-class: G.3; J.2
\\
  In traditional quantitative trading practice, navigating the complicated and
dynamic financial market presents a persistent challenge. Former machine
learning approaches have struggled to fully capture various market variables,
often ignore long-term information and fail to catch up with essential signals
that may lead the profit. This paper introduces an enhanced transformer
architecture and designs a novel factor based on the model. By transfer
learning from sentiment analysis, the proposed model not only exploits its
original inherent advantages in capturing long-range dependencies and modelling
complex data relationships but is also able to solve tasks with numerical
inputs and accurately forecast future returns over a period. This work collects
more than 5,000,000 rolling data of 4,601 stocks in the Chinese capital market
from 2010 to 2019. The results of this study demonstrated the model's superior
performance in predicting stock trends compared with other 100 factor-based
quantitative strategies with lower turnover rates and a more robust half-life
period. Notably, the model's innovative use transformer to establish factors,
in conjunction with market sentiment information, has been shown to enhance the
accuracy of trading signals significantly, thereby offering promising
implications for the future of quantitative trading strategies.
\\ ( https://arxiv.org/abs/2404.00424 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00438 (*cross-listing*)
Date: Sat, 30 Mar 2024 18:07:29 GMT   (803kb,D)

Title: Communication Efficient Distributed Training with Distributed Lion
Authors: Bo Liu, Lemeng Wu, Lizhang Chen, Kaizhao Liang, Jiaxu Zhu, Chen Liang,
  Raghuraman Krishnamoorthi, Qiang Liu
Categories: cs.DC cs.AI cs.LG math.OC stat.ML
Comments: 22 pages
\\
  The Lion optimizer has been a promising competitor with the AdamW for
training large AI models, with advantages on memory, computation, and sample
efficiency. In this paper, we introduce Distributed Lion, an innovative
adaptation of Lion for distributed training environments. Leveraging the sign
operator in Lion, our Distributed Lion only requires communicating binary or
lower-precision vectors between workers to the center server, significantly
reducing the communication cost. Our theoretical analysis confirms Distributed
Lion's convergence properties. Empirical results demonstrate its robustness
across a range of tasks, worker counts, and batch sizes, on both vision and
language problems. Notably, Distributed Lion attains comparable performance to
standard Lion or AdamW optimizers applied on aggregated gradients, but with
significantly reduced communication bandwidth. This feature is particularly
advantageous for training large models. In addition, we also demonstrate that
Distributed Lion presents a more favorable performance-bandwidth balance
compared to existing efficient distributed methods such as deep gradient
compression and ternary gradients.
\\ ( https://arxiv.org/abs/2404.00438 ,  803kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00442 (*cross-listing*)
Date: Sat, 30 Mar 2024 18:16:28 GMT   (9518kb,D)

Title: Interactive Multi-Robot Flocking with Gesture Responsiveness and Musical
  Accompaniment
Authors: Catie Cuan, Kyle Jeffrey, Kim Kleiven, Adrian Li, Emre Fisher, Matt
  Harrison, Benjie Holson, Allison Okamura, and Matt Bennice
Categories: cs.RO cs.AI cs.HC
\\
  For decades, robotics researchers have pursued various tasks for multi-robot
systems, from cooperative manipulation to search and rescue. These tasks are
multi-robot extensions of classical robotic tasks and often optimized on
dimensions such as speed or efficiency. As robots transition from commercial
and research settings into everyday environments, social task aims such as
engagement or entertainment become increasingly relevant. This work presents a
compelling multi-robot task, in which the main aim is to enthrall and interest.
In this task, the goal is for a human to be drawn to move alongside and
participate in a dynamic, expressive robot flock. Towards this aim, the
research team created algorithms for robot movements and engaging interaction
modes such as gestures and sound. The contributions are as follows: (1) a novel
group navigation algorithm involving human and robot agents, (2) a gesture
responsive algorithm for real-time, human-robot flocking interaction, (3) a
weight mode characterization system for modifying flocking behavior, and (4) a
method of encoding a choreographer's preferences inside a dynamic, adaptive,
learned system. An experiment was performed to understand individual human
behavior while interacting with the flock under three conditions: weight modes
selected by a human choreographer, a learned model, or subset list. Results
from the experiment showed that the perception of the experience was not
influenced by the weight mode selection. This work elucidates how differing
task aims such as engagement manifest in multi-robot system design and
execution, and broadens the domain of multi-robot tasks.
\\ ( https://arxiv.org/abs/2404.00442 ,  9518kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00487 (*cross-listing*)
Date: Sat, 30 Mar 2024 23:01:34 GMT   (12079kb,D)

Title: Contextual AI Journaling: Integrating LLM and Time Series Behavioral
  Sensing Technology to Promote Self-Reflection and Well-being using the
  MindScape App
Authors: Subigya Nepal, Arvind Pillai, William Campbell, Talie Massachi, Eunsol
  Soul Choi, Orson Xu, Joanna Kuc, Jeremy Huckins, Jason Holden, Colin Depp,
  Nicholas Jacobson, Mary Czerwinski, Eric Granholm, Andrew T. Campbell
Categories: cs.HC cs.AI
ACM-class: H.5.0; H.5.3; H.5.m; J.0
DOI: 10.1145/3613905.3650767
\\
  MindScape aims to study the benefits of integrating time series behavioral
patterns (e.g., conversational engagement, sleep, location) with Large Language
Models (LLMs) to create a new form of contextual AI journaling, promoting
self-reflection and well-being. We argue that integrating behavioral sensing in
LLMs will likely lead to a new frontier in AI. In this Late-Breaking Work
paper, we discuss the MindScape contextual journal App design that uses LLMs
and behavioral sensing to generate contextual and personalized journaling
prompts crafted to encourage self-reflection and emotional development. We also
discuss the MindScape study of college students based on a preliminary user
study and our upcoming study to assess the effectiveness of contextual AI
journaling in promoting better well-being on college campuses. MindScape
represents a new application class that embeds behavioral intelligence in AI.
\\ ( https://arxiv.org/abs/2404.00487 ,  12079kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00526 (*cross-listing*)
Date: Sun, 31 Mar 2024 02:01:05 GMT   (966kb)

Title: The Emotional Impact of Game Duration: A Framework for Understanding
  Player Emotions in Extended Gameplay Sessions
Authors: Anoop Kumar, Suresh Dodda, Navin Kamuni, Venkata Sai Mahesh
  Vuppalapati
Categories: cs.HC cs.AI
\\
  Video games have played a crucial role in entertainment since their
development in the 1970s, becoming even more prominent during the lockdown
period when people were looking for ways to entertain them. However, at that
time, players were unaware of the significant impact that playtime could have
on their feelings. This has made it challenging for designers and developers to
create new games since they have to control the emotional impact that these
games will take on players. Thus, the purpose of this study is to look at how a
player's emotions are affected by the duration of the game. In order to achieve
this goal, a framework for emotion detection is created. According to the
experiment's results, the volunteers' general ability to express emotions
increased from 20 to 60 minutes. In comparison to shorter gameplay sessions,
the experiment found that extended gameplay sessions did significantly affect
the player's emotions. According to the results, it was recommended that in
order to lessen the potential emotional impact that playing computer and video
games may have in the future, game producers should think about creating
shorter, entertaining games.
\\ ( https://arxiv.org/abs/2404.00526 ,  966kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00540 (*cross-listing*)
Date: Sun, 31 Mar 2024 03:02:35 GMT   (39636kb,D)

Title: Embodied Active Defense: Leveraging Recurrent Feedback to Counter
  Adversarial Patches
Authors: Lingxuan Wu, Xiao Yang, Yinpeng Dong, Liuwei Xie, Hang Su, Jun Zhu
Categories: cs.CV cs.AI
Comments: 27pages
\\
  The vulnerability of deep neural networks to adversarial patches has
motivated numerous defense strategies for boosting model robustness. However,
the prevailing defenses depend on single observation or pre-established
adversary information to counter adversarial patches, often failing to be
confronted with unseen or adaptive adversarial attacks and easily exhibiting
unsatisfying performance in dynamic 3D environments. Inspired by active human
perception and recurrent feedback mechanisms, we develop Embodied Active
Defense (EAD), a proactive defensive strategy that actively contextualizes
environmental information to address misaligned adversarial patches in 3D
real-world settings. To achieve this, EAD develops two central recurrent
sub-modules, i.e., a perception module and a policy module, to implement two
critical functions of active vision. These models recurrently process a series
of beliefs and observations, facilitating progressive refinement of their
comprehension of the target object and enabling the development of strategic
actions to counter adversarial patches in 3D environments. To optimize learning
efficiency, we incorporate a differentiable approximation of environmental
dynamics and deploy patches that are agnostic to the adversary strategies.
Extensive experiments demonstrate that EAD substantially enhances robustness
against a variety of patches within just a few steps through its action policy
in safety-critical tasks (e.g., face recognition and object detection), without
compromising standard accuracy. Furthermore, due to the attack-agnostic
characteristic, EAD facilitates excellent generalization to unseen attacks,
diminishing the averaged attack success rate by 95 percent across a range of
unseen adversarial attacks.
\\ ( https://arxiv.org/abs/2404.00540 ,  39636kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00544 (*cross-listing*)
Date: Sun, 31 Mar 2024 03:16:08 GMT   (2597kb,D)

Title: Deep Extrinsic Manifold Representation for Vision Tasks
Authors: Tongtong Zhang, Xian Wei, Yuanxiang Li
Categories: cs.CV cs.AI
\\
  Non-Euclidean data is frequently encountered across different fields, yet
there is limited literature that addresses the fundamental challenge of
training neural networks with manifold representations as outputs. We introduce
the trick named Deep Extrinsic Manifold Representation (DEMR) for visual tasks
in this context. DEMR incorporates extrinsic manifold embedding into deep
neural networks, which helps generate manifold representations. The DEMR
approach does not directly optimize the complex geodesic loss. Instead, it
focuses on optimizing the computation graph within the embedded Euclidean
space, allowing for adaptability to various architectural requirements. We
provide empirical evidence supporting the proposed concept on two types of
manifolds, $SE(3)$ and its associated quotient manifolds. This evidence offers
theoretical assurances regarding feasibility, asymptotic properties, and
generalization capability. The experimental results show that DEMR effectively
adapts to point cloud alignment, producing outputs in $ SE(3) $, as well as in
illumination subspace learning with outputs on the Grassmann manifold.
\\ ( https://arxiv.org/abs/2404.00544 ,  2597kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00579 (*cross-listing*)
Date: Sun, 31 Mar 2024 06:57:57 GMT   (1288kb,D)

Title: A Review of Modern Recommender Systems Using Generative Models
  (Gen-RecSys)
Authors: Yashar Deldjoo, Zhankui He, Julian McAuley, Anton Korikov, Scott
  Sanner, Arnau Ramisa, Ren\'e Vidal, Maheswaran Sathiamoorthy, Atoosa
  Kasirzadeh, Silvia Milano
Categories: cs.IR cs.AI
\\
  Traditional recommender systems (RS) have used user-item rating histories as
their primary data source, with collaborative filtering being one of the
principal methods. However, generative models have recently developed abilities
to model and sample from complex data distributions, including not only
user-item interaction histories but also text, images, and videos - unlocking
this rich data for novel recommendation tasks. Through this comprehensive and
multi-disciplinary survey, we aim to connect the key advancements in RS using
Generative Models (Gen-RecSys), encompassing: a foundational overview of
interaction-driven generative models; the application of large language models
(LLM) for generative recommendation, retrieval, and conversational
recommendation; and the integration of multimodal models for processing and
generating image and video content in RS. Our holistic perspective allows us to
highlight necessary paradigms for evaluating the impact and harm of Gen-RecSys
and identify open challenges. A more up-to-date version of the papers is
maintained at: https://github.com/yasdel/LLM-RecSys.
\\ ( https://arxiv.org/abs/2404.00579 ,  1288kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00588 (*cross-listing*)
Date: Sun, 31 Mar 2024 07:30:41 GMT   (21182kb,D)

Title: Memory-based Cross-modal Semantic Alignment Network for Radiology Report
  Generation
Authors: Yitian Tao, Liyan Ma, Jing Yu and Han Zhang
Categories: cs.CV cs.AI
Comments: 12 pages, 8 figures
\\
  Generating radiology reports automatically reduces the workload of
radiologists and helps the diagnoses of specific diseases. Many existing
methods take this task as modality transfer process. However, since the key
information related to disease accounts for a small proportion in both image
and report, it is hard for the model to learn the latent relation between the
radiology image and its report, thus failing to generate fluent and accurate
radiology reports. To tackle this problem, we propose a memory-based
cross-modal semantic alignment model (MCSAM) following an encoder-decoder
paradigm. MCSAM includes a well initialized long-term clinical memory bank to
learn disease-related representations as well as prior knowledge for different
modalities to retrieve and use the retrieved memory to perform feature
consolidation. To ensure the semantic consistency of the retrieved cross modal
prior knowledge, a cross-modal semantic alignment module (SAM) is proposed. SAM
is also able to generate semantic visual feature embeddings which can be added
to the decoder and benefits report generation. More importantly, to memorize
the state and additional information while generating reports with the decoder,
we use learnable memory tokens which can be seen as prompts. Extensive
experiments demonstrate the promising performance of our proposed method which
generates state-of-the-art performance on the MIMIC-CXR dataset.
\\ ( https://arxiv.org/abs/2404.00588 ,  21182kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00593 (*cross-listing*)
Date: Sun, 31 Mar 2024 07:56:07 GMT   (21300kb,D)

Title: LAESI: Leaf Area Estimation with Synthetic Imagery
Authors: Jacek Ka{\l}u\.zny and Yannik Schreckenberg and Karol Cyganik and
  Peter Annigh\"ofer and S\"oren Pirk and Dominik L. Michels and Mikolaj
  Cieslak and Farhah Assaad-Gerbert and Bedrich Benes and Wojciech Pa{\l}ubicki
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: 10 pages, 12 figures, 1 table
MSC-class: 68T07, 68T45
ACM-class: I.2.10; I.4.6
\\
  We introduce LAESI, a Synthetic Leaf Dataset of 100,000 synthetic leaf images
on millimeter paper, each with semantic masks and surface area labels. This
dataset provides a resource for leaf morphology analysis primarily aimed at
beech and oak leaves. We evaluate the applicability of the dataset by training
machine learning models for leaf surface area prediction and semantic
segmentation, using real images for validation. Our validation shows that these
models can be trained to predict leaf surface area with a relative error not
greater than an average human annotator. LAESI also provides an efficient
framework based on 3D procedural models and generative AI for the large-scale,
controllable generation of data with potential further applications in
agriculture and biology. We evaluate the inclusion of generative AI in our
procedural data generation pipeline and show how data filtering based on
annotation consistency results in datasets which allow training the highest
performing vision models.
\\ ( https://arxiv.org/abs/2404.00593 ,  21300kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00600 (*cross-listing*)
Date: Sun, 31 Mar 2024 08:14:25 GMT   (21kb,D)

Title: AI Act and Large Language Models (LLMs): When critical issues and
  privacy impact require human and ethical oversight
Authors: Nicola Fabiano
Categories: cs.CY cs.AI cs.CL
\\
  The imposing evolution of artificial intelligence systems and, specifically,
of Large Language Models (LLM) makes it necessary to carry out assessments of
their level of risk and the impact they may have in the area of privacy,
personal data protection and at an ethical level, especially on the weakest and
most vulnerable. This contribution addresses human oversight, ethical
oversight, and privacy impact assessment.
\\ ( https://arxiv.org/abs/2404.00600 ,  21kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00636 (*cross-listing*)
Date: Sun, 31 Mar 2024 10:13:55 GMT   (3860kb,D)

Title: Learning to Generate Conditional Tri-plane for 3D-aware Expression
  Controllable Portrait Animation
Authors: Taekyung Ki, Dongchan Min and Gyeongsu Chae
Categories: cs.CV cs.AI cs.MM
\\
  In this paper, we present Export3D, a one-shot 3D-aware portrait animation
method that is able to control the facial expression and camera view of a given
portrait image. To achieve this, we introduce a tri-plane generator that
directly generates a tri-plane of 3D prior by transferring the expression
parameter of 3DMM into the source image. The tri-plane is then decoded into the
image of different view through a differentiable volume rendering. Existing
portrait animation methods heavily rely on image warping to transfer the
expression in the motion space, challenging on disentanglement of appearance
and expression. In contrast, we propose a contrastive pre-training framework
for appearance-free expression parameter, eliminating undesirable appearance
swap when transferring a cross-identity expression. Extensive experiments show
that our pre-training framework can learn the appearance-free expression
representation hidden in 3DMM, and our model can generate 3D-aware expression
controllable portrait image without appearance swap in the cross-identity
manner.
\\ ( https://arxiv.org/abs/2404.00636 ,  3860kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00673 (*cross-listing*)
Date: Sun, 31 Mar 2024 12:44:48 GMT   (2659kb,D)

Title: A Survey of Privacy-Preserving Model Explanations: Privacy Risks,
  Attacks, and Countermeasures
Authors: Thanh Tam Nguyen, Thanh Trung Huynh, Zhao Ren, Thanh Toan Nguyen, Phi
  Le Nguyen, Hongzhi Yin and Quoc Viet Hung Nguyen
Categories: cs.CR cs.AI cs.CY cs.LG
\\
  As the adoption of explainable AI (XAI) continues to expand, the urgency to
address its privacy implications intensifies. Despite a growing corpus of
research in AI privacy and explainability, there is little attention on
privacy-preserving model explanations. This article presents the first thorough
survey about privacy attacks on model explanations and their countermeasures.
Our contribution to this field comprises a thorough analysis of research papers
with a connected taxonomy that facilitates the categorisation of privacy
attacks and countermeasures based on the targeted explanations. This work also
includes an initial investigation into the causes of privacy leaks. Finally, we
discuss unresolved issues and prospective research directions uncovered in our
analysis. This survey aims to be a valuable resource for the research community
and offers clear insights for those new to this domain. To support ongoing
research, we have established an online resource repository, which will be
continuously updated with new and relevant findings. Interested readers are
encouraged to access our repository at
https://github.com/tamlhp/awesome-privex.
\\ ( https://arxiv.org/abs/2404.00673 ,  2659kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00675 (*cross-listing*)
Date: Sun, 31 Mar 2024 12:48:07 GMT   (10979kb,D)

Title: LLM meets Vision-Language Models for Zero-Shot One-Class Classification
Authors: Yassir Bendou, Giulia Lioi, Bastien Pasdeloup, Lukas Mauch, Ghouthi
  Boukli Hacene, Fabien Cardinaux and Vincent Gripon
Categories: cs.CV cs.AI
\\
  We consider the problem of zero-shot one-class visual classification. In this
setting, only the label of the target class is available, and the goal is to
discriminate between positive and negative query samples without requiring any
validation example from the target task. We propose a two-step solution that
first queries large language models for visually confusing objects and then
relies on vision-language pre-trained models (e.g., CLIP) to perform
classification. By adapting large-scale vision benchmarks, we demonstrate the
ability of the proposed method to outperform adapted off-the-shelf alternatives
in this setting. Namely, we propose a realistic benchmark where negative query
samples are drawn from the same original dataset as positive ones, including a
granularity-controlled version of iNaturalist, where negative samples are at a
fixed distance in the taxonomy tree from the positive ones. Our work shows that
it is possible to discriminate between a single category and other semantically
related ones using only its label
\\ ( https://arxiv.org/abs/2404.00675 ,  10979kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00684 (*cross-listing*)
Date: Sun, 31 Mar 2024 13:29:43 GMT   (750kb,D)

Title: Generative Retrieval as Multi-Vector Dense Retrieval
Authors: Shiguang Wu, Wenda Wei, Mengqi Zhang, Zhumin Chen, Jun Ma, Zhaochun
  Ren, Maarten de Rijke and Pengjie Ren
Categories: cs.IR cs.AI
Comments: 12 pages, 5 figures, 8 tables, accepted at SIGIR 2024
\\
  Generative retrieval generates identifiers of relevant documents in an
end-to-end manner using a sequence-to-sequence architecture for a given query.
The relation between generative retrieval and other retrieval methods,
especially those based on matching within dense retrieval models, is not yet
fully comprehended. Prior work has demonstrated that generative retrieval with
atomic identifiers is equivalent to single-vector dense retrieval. Accordingly,
generative retrieval exhibits behavior analogous to hierarchical search within
a tree index in dense retrieval when using hierarchical semantic identifiers.
However, prior work focuses solely on the retrieval stage without considering
the deep interactions within the decoder of generative retrieval.
  In this paper, we fill this gap by demonstrating that generative retrieval
and multi-vector dense retrieval share the same framework for measuring the
relevance to a query of a document. Specifically, we examine the attention
layer and prediction head of generative retrieval, revealing that generative
retrieval can be understood as a special case of multi-vector dense retrieval.
Both methods compute relevance as a sum of products of query and document
vectors and an alignment matrix. We then explore how generative retrieval
applies this framework, employing distinct strategies for computing document
token vectors and the alignment matrix. We have conducted experiments to verify
our conclusions and show that both paradigms exhibit commonalities of term
matching in their alignment matrix.
\\ ( https://arxiv.org/abs/2404.00684 ,  750kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00685 (*cross-listing*)
Date: Sun, 31 Mar 2024 13:30:12 GMT   (574kb,D)

Title: Scaling Properties of Speech Language Models
Authors: Santiago Cuervo, Ricard Marxer
Categories: eess.AS cs.AI cs.CL cs.NE
\\
  Speech Language Models (SLMs) aim to learn language from raw audio, without
textual resources. Despite significant advances, our current models exhibit
weak syntax and semantic abilities. However, if the scaling properties of
neural language models hold for the speech modality, these abilities will
improve as the amount of compute used for training increases. In this paper, we
use models of this scaling behavior to estimate the scale at which our current
methods will yield a SLM with the English proficiency of text-based Large
Language Models (LLMs). We establish a strong correlation between pre-training
loss and downstream syntactic and semantic performance in SLMs and LLMs, which
results in predictable scaling of linguistic performance. We show that the
linguistic performance of SLMs scales up to three orders of magnitude more
slowly than that of text-based LLMs. Additionally, we study the benefits of
synthetic data designed to boost semantic understanding and the effects of
coarser speech tokenization.
\\ ( https://arxiv.org/abs/2404.00685 ,  574kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00722 (*cross-listing*)
Date: Sun, 31 Mar 2024 15:34:45 GMT   (7933kb,D)

Title: DRCT: Saving Image Super-resolution away from Information Bottleneck
Authors: Chih-Chung Hsu, Chia-Ming Lee, and Yi-Shiuan Chou
Categories: cs.CV cs.AI
Comments: Submitted to NTIRE 2024
\\
  In recent years, Vision Transformer-based applications to low-level vision
tasks have achieved widespread success. Unlike CNN-based models, Transformers
are more adept at capturing long-range dependencies, enabling the
reconstruction of images utilizing information from non-local areas. In the
domain of super-resolution, Swin-transformer-based approaches have become
mainstream due to their capacity to capture global spatial information and
their shifting-window attention mechanism that facilitates the interchange of
information between different windows. Many researchers have enhanced image
quality and network efficiency by expanding the receptive field or designing
complex networks, yielding commendable results. However, we observed that
spatial information tends to diminish during the forward propagation process
due to increased depth, leading to a loss of spatial information and,
consequently, limiting the model's potential. To address this, we propose the
Dense-residual-connected Transformer (DRCT), aimed at mitigating the loss of
spatial information through dense-residual connections between layers, thereby
unleashing the model's potential and enhancing performance. Experiment results
indicate that our approach is not only straightforward but also achieves
remarkable efficiency, surpassing state-of-the-art methods and performing
commendably at NTIRE2024.
\\ ( https://arxiv.org/abs/2404.00722 ,  7933kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00725 (*cross-listing*)
Date: Sun, 31 Mar 2024 15:55:49 GMT   (1030kb,D)

Title: The Larger the Better? Improved LLM Code-Generation via Budget
  Reallocation
Authors: Michael Hassid, Tal Remez, Jonas Gehring, Roy Schwartz, Yossi Adi
Categories: cs.SE cs.AI cs.CL cs.LG
\\
  It is a common belief that large language models (LLMs) are better than
smaller-sized ones. However, larger models also require significantly more time
and compute during inference. This begs the question: what happens when both
models operate under the same budget? (e.g., compute, run-time). To address
this question, we analyze code generation LLMs of various sizes and make
comparisons such as running a 70B model once vs. generating five outputs from a
13B model and selecting one. Our findings reveal that, in a standard unit-test
setup, the repeated use of smaller models can yield consistent improvements,
with gains of up to 15% across five tasks. On the other hand, in scenarios
where unit-tests are unavailable, a ranking-based selection of candidates from
the smaller model falls short of the performance of a single output from larger
ones. Our results highlight the potential of using smaller models instead of
larger ones, and the importance of studying approaches for ranking LLM outputs.
\\ ( https://arxiv.org/abs/2404.00725 ,  1030kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00746 (*cross-listing*)
Date: Sun, 31 Mar 2024 17:32:08 GMT   (22359kb,D)

Title: Mining Weighted Sequential Patterns in Incremental Uncertain Databases
Authors: Kashob Kumar Roy, Md Hasibul Haque Moon, Md Mahmudur Rahman, Chowdhury
  Farhan Ahmed, Carson Kai-Sang Leung
Categories: cs.DB cs.AI
Comments: Accepted to Information Science journal
Journal-ref: Information Sciences 582 (2022): 865-896
\\
  Due to the rapid development of science and technology, the importance of
imprecise, noisy, and uncertain data is increasing at an exponential rate.
Thus, mining patterns in uncertain databases have drawn the attention of
researchers. Moreover, frequent sequences of items from these databases need to
be discovered for meaningful knowledge with great impact. In many real cases,
weights of items and patterns are introduced to find interesting sequences as a
measure of importance. Hence, a constraint of weight needs to be handled while
mining sequential patterns. Besides, due to the dynamic nature of databases,
mining important information has become more challenging. Instead of mining
patterns from scratch after each increment, incremental mining algorithms
utilize previously mined information to update the result immediately. Several
algorithms exist to mine frequent patterns and weighted sequences from
incremental databases. However, these algorithms are confined to mine the
precise ones. Therefore, we have developed an algorithm to mine frequent
sequences in an uncertain database in this work. Furthermore, we have proposed
two new techniques for mining when the database is incremental. Extensive
experiments have been conducted for performance evaluation. The analysis showed
the efficiency of our proposed framework.
\\ ( https://arxiv.org/abs/2404.00746 ,  22359kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00777 (*cross-listing*)
Date: Sun, 31 Mar 2024 19:28:04 GMT   (28304kb,D)

Title: Privacy-preserving Optics for Enhancing Protection in Face
  De-identification
Authors: Jhon Lopez, Carlos Hinojosa, Henry Arguello, Bernard Ghanem
Categories: cs.CV cs.AI cs.CR cs.LG eess.IV
Comments: Accepted to CVPR 2024. Project Website and Code coming soon
\\
  The modern surge in camera usage alongside widespread computer vision
technology applications poses significant privacy and security concerns.
Current artificial intelligence (AI) technologies aid in recognizing relevant
events and assisting in daily tasks in homes, offices, hospitals, etc. The need
to access or process personal information for these purposes raises privacy
concerns. While software-level solutions like face de-identification provide a
good privacy/utility trade-off, they present vulnerabilities to sniffing
attacks. In this paper, we propose a hardware-level face de-identification
method to solve this vulnerability. Specifically, our approach first learns an
optical encoder along with a regression model to obtain a face heatmap while
hiding the face identity from the source image. We also propose an
anonymization framework that generates a new face using the privacy-preserving
image, face heatmap, and a reference face image from a public dataset as input.
We validate our approach with extensive simulations and hardware experiments.
\\ ( https://arxiv.org/abs/2404.00777 ,  28304kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00806 (*cross-listing*)
Date: Sun, 31 Mar 2024 21:43:05 GMT   (1215kb,D)

Title: Algorithmic Collusion by Large Language Models
Authors: Sara Fish, Yannai A. Gonczarowski, Ran I. Shorrer
Categories: econ.GN cs.AI cs.GT q-fin.EC
\\
  The rise of algorithmic pricing raises concerns of algorithmic collusion. We
conduct experiments with algorithmic pricing agents based on Large Language
Models (LLMs), and specifically GPT-4. We find that (1) LLM-based agents are
adept at pricing tasks, (2) LLM-based pricing agents autonomously collude in
oligopoly settings to the detriment of consumers, and (3) variation in
seemingly innocuous phrases in LLM instructions ("prompts") may increase
collusion. These results extend to auction settings. Our findings underscore
the need for antitrust regulation regarding algorithmic pricing, and uncover
regulatory challenges unique to LLM-based pricing agents.
\\ ( https://arxiv.org/abs/2404.00806 ,  1215kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00815 (*cross-listing*)
Date: Sun, 31 Mar 2024 22:18:56 GMT   (11553kb,D)

Title: Towards Realistic Scene Generation with LiDAR Diffusion Models
Authors: Haoxi Ran, Vitor Guizilini, Yue Wang
Categories: cs.CV cs.AI cs.RO
Comments: CVPR 2024. Code available at
  https://github.com/hancyran/LiDAR-Diffusion
\\
  Diffusion models (DMs) excel in photo-realistic image synthesis, but their
adaptation to LiDAR scene generation poses a substantial hurdle. This is
primarily because DMs operating in the point space struggle to preserve the
curve-like patterns and 3D geometry of LiDAR scenes, which consumes much of
their representation power. In this paper, we propose LiDAR Diffusion Models
(LiDMs) to generate LiDAR-realistic scenes from a latent space tailored to
capture the realism of LiDAR scenes by incorporating geometric priors into the
learning pipeline. Our method targets three major desiderata: pattern realism,
geometry realism, and object realism. Specifically, we introduce curve-wise
compression to simulate real-world LiDAR patterns, point-wise coordinate
supervision to learn scene geometry, and patch-wise encoding for a full 3D
object context. With these three core designs, our method achieves competitive
performance on unconditional LiDAR generation in 64-beam scenario and state of
the art on conditional LiDAR generation, while maintaining high efficiency
compared to point-based DMs (up to 107$\times$ faster). Furthermore, by
compressing LiDAR scenes into a latent space, we enable the controllability of
DMs with various conditions such as semantic maps, camera views, and text
prompts. Our code and pretrained weights are available at
https://github.com/hancyran/LiDAR-Diffusion.
\\ ( https://arxiv.org/abs/2404.00815 ,  11553kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00855 (*cross-listing*)
Date: Mon, 1 Apr 2024 01:49:08 GMT   (2846kb,D)

Title: TSOM: Small Object Motion Detection Neural Network Inspired by Avian
  Visual Circuit
Authors: Pignge Hu, Xiaoteng Zhang, Mengmeng Li, Yingjie Zhu and Li Shi
Categories: cs.CV cs.AI
\\
  Detecting small moving objects in complex backgrounds from an overhead
perspective is a highly challenging task for machine vision systems. As an
inspiration from nature, the avian visual system is capable of processing
motion information in various complex aerial scenes, and its Retina-OT-Rt
visual circuit is highly sensitive to capturing the motion information of small
objects from high altitudes. However, more needs to be done on small object
motion detection algorithms based on the avian visual system. In this paper, we
conducted mathematical modeling based on extensive studies of the biological
mechanisms of the Retina-OT-Rt visual circuit. Based on this, we proposed a
novel tectum small object motion detection neural network (TSOM). The neural
network includes the retina, SGC dendritic, SGC Soma, and Rt layers, each layer
corresponding to neurons in the visual pathway. The Retina layer is responsible
for accurately projecting input content, the SGC dendritic layer perceives and
encodes spatial-temporal information, the SGC Soma layer computes complex
motion information and extracts small objects, and the Rt layer integrates and
decodes motion information from multiple directions to determine the position
of small objects. Extensive experiments on pigeon neurophysiological
experiments and image sequence data showed that the TSOM is biologically
interpretable and effective in extracting reliable small object motion features
from complex high-altitude backgrounds.
\\ ( https://arxiv.org/abs/2404.00855 ,  2846kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00856 (*cross-listing*)
Date: Mon, 1 Apr 2024 01:49:09 GMT   (1382kb,D)

Title: Removing Speaker Information from Speech Representation using
  Variable-Length Soft Pooling
Authors: Injune Hwang, Kyogu Lee
Categories: cs.SD cs.AI eess.AS
\\
  Recently, there have been efforts to encode the linguistic information of
speech using a self-supervised framework for speech synthesis. However,
predicting representations from surrounding representations can inadvertently
entangle speaker information in the speech representation. This paper aims to
remove speaker information by exploiting the structured nature of speech,
composed of discrete units like phonemes with clear boundaries. A neural
network predicts these boundaries, enabling variable-length pooling for
event-based representation extraction instead of fixed-rate methods. The
boundary predictor outputs a probability for the boundary between 0 and 1,
making pooling soft. The model is trained to minimize the difference with the
pooled representation of the data augmented by time-stretch and pitch-shift. To
confirm that the learned representation includes contents information but is
independent of speaker information, the model was evaluated with libri-light's
phonetic ABX task and SUPERB's speaker identification task.
\\ ( https://arxiv.org/abs/2404.00856 ,  1382kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00903 (*cross-listing*)
Date: Mon, 1 Apr 2024 04:13:42 GMT   (250kb)

Title: Maximizing User Experience with LLMOps-Driven Personalized
  Recommendation Systems
Authors: Chenxi Shi, Penghao Liang, Yichao Wu, Tong Zhan, Zhengyu Jin
Categories: cs.IR cs.AI
\\
  The integration of LLMOps into personalized recommendation systems marks a
significant advancement in managing LLM-driven applications. This innovation
presents both opportunities and challenges for enterprises, requiring
specialized teams to navigate the complexity of engineering technology while
prioritizing data security and model interpretability. By leveraging LLMOps,
enterprises can enhance the efficiency and reliability of large-scale machine
learning models, driving personalized recommendations aligned with user
preferences. Despite ethical considerations, LLMOps is poised for widespread
adoption, promising more efficient and secure machine learning services that
elevate user experience and shape the future of personalized recommendation
systems.
\\ ( https://arxiv.org/abs/2404.00903 ,  250kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00913 (*cross-listing*)
Date: Mon, 1 Apr 2024 04:39:21 GMT   (3163kb,D)

Title: LLaMA-Excitor: General Instruction Tuning via Indirect Feature
  Interaction
Authors: Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao
Categories: cs.CV cs.AI cs.CL
Comments: This paper is accepted by CVPR 2024
\\
  Existing methods to fine-tune LLMs, like Adapter, Prefix-tuning, and LoRA,
which introduce extra modules or additional input sequences to inject new
skills or knowledge, may compromise the innate abilities of LLMs. In this
paper, we propose LLaMA-Excitor, a lightweight method that stimulates the LLMs'
potential to better follow instructions by gradually paying more attention to
worthwhile information. Specifically, the LLaMA-Excitor does not directly
change the intermediate hidden state during the self-attention calculation of
the transformer structure. We designed the Excitor block as a bypass module for
the similarity score computation in LLMs' self-attention to reconstruct keys
and change the importance of values by learnable prompts. LLaMA-Excitor ensures
a self-adaptive allocation of additional attention to input instructions, thus
effectively preserving LLMs' pre-trained knowledge when fine-tuning LLMs on
low-quality instruction-following datasets. Furthermore, we unify the modeling
of multi-modal tuning and language-only tuning, extending LLaMA-Excitor to a
powerful visual instruction follower without the need for complex multi-modal
alignment. Our proposed approach is evaluated in language-only and multi-modal
tuning experimental scenarios. Notably, LLaMA-Excitor is the only method that
maintains basic capabilities while achieving a significant improvement (+6%) on
the MMLU benchmark. In the visual instruction tuning, we achieve a new
state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO, and a
comparable performance (88.39%) on ScienceQA to cutting-edge models with more
parameters and extensive vision-language pertaining.
\\ ( https://arxiv.org/abs/2404.00913 ,  3163kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00923 (*cross-listing*)
Date: Mon, 1 Apr 2024 04:57:41 GMT   (13705kb,D)

Title: MM3DGS SLAM: Multi-modal 3D Gaussian Splatting for SLAM Using Vision,
  Depth, and Inertial Measurements
Authors: Lisong C. Sun, Neel P. Bhatt, Jonathan C. Liu, Zhiwen Fan, Zhangyang
  Wang, Todd E. Humphreys, and Ufuk Topcu
Categories: cs.CV cs.AI cs.RO
Comments: Project Webpage: https://vita-group.github.io/MM3DGS-SLAM
\\
  Simultaneous localization and mapping is essential for position tracking and
scene understanding. 3D Gaussian-based map representations enable
photorealistic reconstruction and real-time rendering of scenes using multiple
posed cameras. We show for the first time that using 3D Gaussians for map
representation with unposed camera images and inertial measurements can enable
accurate SLAM. Our method, MM3DGS, addresses the limitations of prior neural
radiance field-based representations by enabling faster rendering, scale
awareness, and improved trajectory tracking. Our framework enables
keyframe-based mapping and tracking utilizing loss functions that incorporate
relative pose transformations from pre-integrated inertial measurements, depth
estimates, and measures of photometric rendering quality. We also release a
multi-modal dataset, UT-MM, collected from a mobile robot equipped with a
camera and an inertial measurement unit. Experimental evaluation on several
scenes from the dataset shows that MM3DGS achieves 3x improvement in tracking
and 5% improvement in photometric rendering quality compared to the current
3DGS SLAM state-of-the-art, while allowing real-time rendering of a
high-resolution dense 3D map. Project Webpage:
https://vita-group.github.io/MM3DGS-SLAM
\\ ( https://arxiv.org/abs/2404.00923 ,  13705kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00971 (*cross-listing*)
Date: Mon, 1 Apr 2024 07:31:45 GMT   (584kb,D)

Title: Exploring and Evaluating Hallucinations in LLM-Powered Code Generation
Authors: Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, Li
  Zhang
Categories: cs.SE cs.AI
\\
  The rise of Large Language Models (LLMs) has significantly advanced many
applications on software engineering tasks, particularly in code generation.
Despite the promising performance, LLMs are prone to generate hallucinations,
which means LLMs might produce outputs that deviate from users' intent, exhibit
internal inconsistencies, or misalign with the factual knowledge, making the
deployment of LLMs potentially risky in a wide range of applications. Existing
work mainly focuses on investing the hallucination in the domain of natural
language generation (NLG), leaving a gap in understanding the types and extent
of hallucinations in the context of code generation. To bridge the gap, we
conducted a thematic analysis of the LLM-generated code to summarize and
categorize the hallucinations present in it. Our study established a
comprehensive taxonomy of hallucinations in LLM-generated code, encompassing 5
primary categories of hallucinations depending on the conflicting objectives
and varying degrees of deviation observed in code generation. Furthermore, we
systematically analyzed the distribution of hallucinations, exploring
variations among different LLMs and their correlation with code correctness.
Based on the results, we proposed HalluCode, a benchmark for evaluating the
performance of code LLMs in recognizing hallucinations. Hallucination
recognition and mitigation experiments with HalluCode and HumanEval show
existing LLMs face great challenges in recognizing hallucinations, particularly
in identifying their types, and are hardly able to mitigate hallucinations. We
believe our findings will shed light on future research about hallucination
evaluation, detection, and mitigation, ultimately paving the way for building
more effective and reliable code LLMs in the future.
\\ ( https://arxiv.org/abs/2404.00971 ,  584kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00977 (*cross-listing*)
Date: Mon, 1 Apr 2024 07:49:10 GMT   (3296kb,D)

Title: Nonlinear Impulse Pattern Formulation dynamical social and political
  prediction algorithm for city planning and public participation
Authors: Rolf Bader, Simon Linke, Stefanie Gernert
Categories: nlin.AO cs.AI math.DS
\\
  A nonlinear-dynamical algorithm for city planning is proposed as an Impulse
Pattern Formulation (IPF) for predicting relevant parameters like health,
artistic freedom, or financial developments of different social or political
stakeholders over the cause of a planning process. The IPF has already shown
high predictive precision at low computational cost in musical instrument
simulations, brain dynamics, and human-human interactions. The social and
political IPF consists of three basic equations of system state developments,
self-adaptation of stakeholders, two adaptive interactions, and external impact
terms suitable for respective planning situations. Typical scenarios of
stakeholder interactions and developments are modeled by adjusting a set of
system parameters. These include stakeholder reaction to external input,
enhanced system stability through self-adaptation, stakeholder convergence due
to mediative interaction adaptation, as well as complex dynamics in terms of
direct stakeholder impacts. A workflow for implementing the algorithm in real
city planning scenarios is outlined. This workflow includes machine learning of
a suitable set of parameters suggesting best-practice planning to aim at the
desired development of the planning process and its output.
\\ ( https://arxiv.org/abs/2404.00977 ,  3296kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00989 (*cross-listing*)
Date: Mon, 1 Apr 2024 08:34:42 GMT   (33248kb,D)

Title: 360+x: A Panoptic Multi-modal Scene Understanding Dataset
Authors: Hao Chen, Yuqi Hou, Chenyuan Qu, Irene Testini, Xiaohan Hong, Jianbo
  Jiao
Categories: cs.CV cs.AI cs.MM cs.SD eess.AS
Comments: To access the public dataset, please visit
  https://x360dataset.github.io
Journal-ref: The IEEE/CVF Computer Vision and Pattern Recognition Conference
  (CVPR) 2024
\\
  Human perception of the world is shaped by a multitude of viewpoints and
modalities. While many existing datasets focus on scene understanding from a
certain perspective (e.g. egocentric or third-person views), our dataset offers
a panoptic perspective (i.e. multiple viewpoints with multiple data
modalities). Specifically, we encapsulate third-person panoramic and front
views, as well as egocentric monocular/binocular views with rich modalities
including video, multi-channel audio, directional binaural delay, location data
and textual scene descriptions within each scene captured, presenting
comprehensive observation of the world. Figure 1 offers a glimpse of all 28
scene categories of our 360+x dataset. To the best of our knowledge, this is
the first database that covers multiple viewpoints with multiple data
modalities to mimic how daily information is accessed in the real world.
Through our benchmark analysis, we presented 5 different scene understanding
tasks on the proposed 360+x dataset to evaluate the impact and benefit of each
data modality and perspective in panoptic scene understanding. We hope this
unique dataset could broaden the scope of comprehensive scene understanding and
encourage the community to approach these problems from more diverse
perspectives.
\\ ( https://arxiv.org/abs/2404.00989 ,  33248kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01012 (*cross-listing*)
Date: Mon, 1 Apr 2024 09:33:05 GMT   (187kb,D)

Title: Query Performance Prediction using Relevance Judgments Generated by
  Large Language Models
Authors: Chuan Meng, Negar Arabzadeh, Arian Askari, Mohammad Aliannejadi,
  Maarten de Rijke
Categories: cs.IR cs.AI cs.CL cs.LG
ACM-class: H.3.3
\\
  Query performance prediction (QPP) aims to estimate the retrieval quality of
a search system for a query without human relevance judgments. Previous QPP
methods typically return a single scalar value and do not require the predicted
values to approximate a specific information retrieval (IR) evaluation measure,
leading to certain drawbacks: (i) a single scalar is insufficient to accurately
represent different IR evaluation measures, especially when metrics do not
highly correlate, and (ii) a single scalar limits the interpretability of QPP
methods because solely using a scalar is insufficient to explain QPP results.
To address these issues, we propose a QPP framework using automatically
generated relevance judgments (QPP-GenRE), which decomposes QPP into
independent subtasks of judging the relevance of each item in a ranked list to
a given query. This allows us to predict any IR evaluation measure using the
generated relevance judgments as pseudo-labels; Also, this allows us to
interpret predicted IR evaluation measures, and identify, track and rectify
errors in generated relevance judgments to improve QPP quality. We judge
relevance by leveraging a leading open-source large language model (LLM),
LLaMA, to ensure scientific reproducibility. In doing so, we address two main
challenges: (i) excessive computational costs of judging the entire corpus for
predicting a recall-based metric, and (ii) poor performance in prompting LLaMA
in a zero-/few-shot manner. We devise an approximation strategy to predict a
recall-oriented IR measure and propose to fine-tune LLaMA using human-labeled
relevance judgments. Experiments on the TREC 2019-2022 deep learning tracks
show that QPP-GenRE achieves state-of-the-art QPP accuracy for both lexical and
neural rankers in both precision- and recall-oriented metrics.
\\ ( https://arxiv.org/abs/2404.01012 ,  187kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01013 (*cross-listing*)
Date: Mon, 1 Apr 2024 09:34:51 GMT   (30472kb,D)

Title: Teeth-SEG: An Efficient Instance Segmentation Framework for Orthodontic
  Treatment based on Anthropic Prior Knowledge
Authors: Bo Zou, Shaofeng Wang, Hao Liu, Gaoyue Sun, Yajie Wang, FeiFei Zuo,
  Chengbin Quan, Youjian Zhao
Categories: cs.CV cs.AI
Comments: This paper has been accepted by CVPR 2024
\\
  Teeth localization, segmentation, and labeling in 2D images have great
potential in modern dentistry to enhance dental diagnostics, treatment
planning, and population-based studies on oral health. However, general
instance segmentation frameworks are incompetent due to 1) the subtle
differences between some teeth' shapes (e.g., maxillary first premolar and
second premolar), 2) the teeth's position and shape variation across subjects,
and 3) the presence of abnormalities in the dentition (e.g., caries and
edentulism). To address these problems, we propose a ViT-based framework named
TeethSEG, which consists of stacked Multi-Scale Aggregation (MSA) blocks and an
Anthropic Prior Knowledge (APK) layer. Specifically, to compose the two
modules, we design 1) a unique permutation-based upscaler to ensure high
efficiency while establishing clear segmentation boundaries with 2) multi-head
self/cross-gating layers to emphasize particular semantics meanwhile
maintaining the divergence between token embeddings. Besides, we collect 3) the
first open-sourced intraoral image dataset IO150K, which comprises over 150k
intraoral photos, and all photos are annotated by orthodontists using a
human-machine hybrid algorithm. Experiments on IO150K demonstrate that our
TeethSEG outperforms the state-of-the-art segmentation models on dental image
segmentation.
\\ ( https://arxiv.org/abs/2404.01013 ,  30472kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01030 (*cross-listing*)
Date: Mon, 1 Apr 2024 10:19:05 GMT   (62kb)

Title: Survey of Bias In Text-to-Image Generation: Definition, Evaluation, and
  Mitigation
Authors: Yixin Wan, Arjun Subramonian, Anaelia Ovalle, Zongyu Lin, Ashima
  Suvarna, Christina Chance, Hritik Bansal, Rebecca Pattichis, Kai-Wei Chang
Categories: cs.CV cs.AI cs.CY
\\
  The recent advancement of large and powerful models with Text-to-Image (T2I)
generation abilities -- such as OpenAI's DALLE-3 and Google's Gemini -- enables
users to generate high-quality images from textual prompts. However, it has
become increasingly evident that even simple prompts could cause T2I models to
exhibit conspicuous social bias in generated images. Such bias might lead to
both allocational and representational harms in society, further marginalizing
minority groups. Noting this problem, a large body of recent works has been
dedicated to investigating different dimensions of bias in T2I systems.
However, an extensive review of these studies is lacking, hindering a
systematic understanding of current progress and research gaps. We present the
first extensive survey on bias in T2I generative models. In this survey, we
review prior studies on dimensions of bias: Gender, Skintone, and Geo-Culture.
Specifically, we discuss how these works define, evaluate, and mitigate
different aspects of bias. We found that: (1) while gender and skintone biases
are widely studied, geo-cultural bias remains under-explored; (2) most works on
gender and skintone bias investigated occupational association, while other
aspects are less frequently studied; (3) almost all gender bias works overlook
non-binary identities in their studies; (4) evaluation datasets and metrics are
scattered, with no unified framework for measuring biases; and (5) current
mitigation methods fail to resolve biases comprehensively. Based on current
limitations, we point out future research directions that contribute to
human-centric definitions, evaluations, and mitigation of biases. We hope to
highlight the importance of studying biases in T2I systems, as well as
encourage future efforts to holistically understand and tackle biases, building
fair and trustworthy T2I technologies for everyone.
\\ ( https://arxiv.org/abs/2404.01030 ,  62kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01036 (*cross-listing*)
Date: Mon, 1 Apr 2024 10:43:50 GMT   (609kb)

Title: Higher education assessment practice in the era of generative AI tools
Authors: Bayode Ogunleye, Kudirat Ibilola Zakariyyah, Oluwaseun Ajao, Olakunle
  Olayinka and Hemlata Sharma
Categories: cs.IR cs.AI cs.CV cs.LG
Comments: 11 pages, 7 tables published in the Journal of Applied Learning &
  Teaching
ACM-class: I.2.7; I.2.10; H.3.3
Journal-ref: Higher education assessment practice in the era of generative AI
  tools. (2024). Journal of applied learning and teaching, 7(1)
DOI: 10.37074/jalt.2024.7.1.28
\\
  The higher education (HE) sector benefits every nation's economy and society
at large. However, their contributions are challenged by advanced technologies
like generative artificial intelligence (GenAI) tools. In this paper, we
provide a comprehensive assessment of GenAI tools towards assessment and
pedagogic practice and, subsequently, discuss the potential impacts. This study
experimented using three assessment instruments from data science, data
analytics, and construction management disciplines. Our findings are two-fold:
first, the findings revealed that GenAI tools exhibit subject knowledge,
problem-solving, analytical, critical thinking, and presentation skills and
thus can limit learning when used unethically. Secondly, the design of the
assessment of certain disciplines revealed the limitations of the GenAI tools.
Based on our findings, we made recommendations on how AI tools can be utilised
for teaching and learning in HE.
\\ ( https://arxiv.org/abs/2404.01036 ,  609kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01089 (*cross-listing*)
Date: Mon, 1 Apr 2024 12:43:22 GMT   (3024kb,D)

Title: Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On
Authors: Xu Yang, Changxing Ding, Zhibin Hong, Junhao Huang, Jin Tao, Xiangmin
  Xu
Categories: cs.CV cs.AI
Comments: CVPR 2024
\\
  Image-based virtual try-on is an increasingly important task for online
shopping. It aims to synthesize images of a specific person wearing a specified
garment. Diffusion model-based approaches have recently become popular, as they
are excellent at image synthesis tasks. However, these approaches usually
employ additional image encoders and rely on the cross-attention mechanism for
texture transfer from the garment to the person image, which affects the
try-on's efficiency and fidelity. To address these issues, we propose an
Texture-Preserving Diffusion (TPD) model for virtual try-on, which enhances the
fidelity of the results and introduces no additional image encoders.
Accordingly, we make contributions from two aspects. First, we propose to
concatenate the masked person and reference garment images along the spatial
dimension and utilize the resulting image as the input for the diffusion
model's denoising UNet. This enables the original self-attention layers
contained in the diffusion model to achieve efficient and accurate texture
transfer. Second, we propose a novel diffusion-based method that predicts a
precise inpainting mask based on the person and reference garment images,
further enhancing the reliability of the try-on results. In addition, we
integrate mask prediction and image synthesis into a single compact model. The
experimental results show that our approach can be applied to various try-on
tasks, e.g., garment-to-person and person-to-person try-ons, and significantly
outperforms state-of-the-art methods on popular VITON, VITON-HD databases.
\\ ( https://arxiv.org/abs/2404.01089 ,  3024kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01109 (*cross-listing*)
Date: Mon, 1 Apr 2024 13:33:40 GMT   (990kb,D)

Title: An incremental hybrid adaptive network-based IDS in Software Defined
  Networks to detect stealth attacks
Authors: Abdullah H Alqahtani
Categories: cs.CR cs.AI
\\
  Network attacks have became increasingly more sophisticated and stealthy due
to the advances in technologies and the growing sophistication of attackers.
Advanced Persistent Threats (APTs) are a type of attack that implement a wide
range of strategies to evade detection and be under the defence radar. Software
Defined Network (SDN) is a network paradigm that implements dynamic
configuration by separating the control plane from the network plane. This
approach improves security aspects by facilitating the employment of network
intrusion detection systems. Implementing Machine Learning (ML) techniques in
Intrusion Detection Systems (IDSs) is widely used to detect such attacks but
has a challenge when the data distribution changes. Concept drift is a term
that describes the change in the relationship between the input data and the
target value (label or class). The model is expected to degrade as certain
forms of change occur. In this paper, the primary form of change will be in
user behaviour (particularly changes in attacker behaviour). It is essential
for a model to adapt itself to deviations in data distribution. SDN can help in
monitoring changes in data distribution. This paper discusses changes in
stealth attacker behaviour. The work described here investigates various
concept drift detection algorithms. An incremental hybrid adaptive Network
Intrusion Detection System (NIDS) is proposed to tackle the issue of concept
drift in SDN. It can detect known and unknown attacks. The model is evaluated
over different datasets showing promising results.
\\ ( https://arxiv.org/abs/2404.01109 ,  990kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01127 (*cross-listing*)
Date: Mon, 1 Apr 2024 14:06:48 GMT   (543kb,D)

Title: Medical Visual Prompting (MVP): A Unified Framework for Versatile and
  High-Quality Medical Image Segmentation
Authors: Yulin Chen, Guoheng Huang, Kai Huang, Zijin Lin, Guo Zhong, Shenghong
  Luo, Jie Deng, Jian Zhou
Categories: cs.CV cs.AI
\\
  Accurate segmentation of lesion regions is crucial for clinical diagnosis and
treatment across various diseases. While deep convolutional networks have
achieved satisfactory results in medical image segmentation, they face
challenges such as loss of lesion shape information due to continuous
convolution and downsampling, as well as the high cost of manually labeling
lesions with varying shapes and sizes. To address these issues, we propose a
novel medical visual prompting (MVP) framework that leverages pre-training and
prompting concepts from natural language processing (NLP). The framework
utilizes three key components: Super-Pixel Guided Prompting (SPGP) for
superpixelating the input image, Image Embedding Guided Prompting (IEGP) for
freezing patch embedding and merging with superpixels to provide visual
prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for
pinpointing prompt content and efficiently adapting all layers. By integrating
SPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn
shape prompting information and facilitates mutual learning across different
tasks. Extensive experiments conducted on five datasets demonstrate superior
performance of this method in various challenging medical image tasks, while
simplifying single-task medical segmentation models. This novel framework
offers improved performance with fewer parameters and holds significant
potential for accurate segmentation of lesion regions in various medical tasks,
making it clinically valuable.
\\ ( https://arxiv.org/abs/2404.01127 ,  543kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01131 (*cross-listing*)
Date: Mon, 1 Apr 2024 14:19:00 GMT   (4162kb,D)

Title: GOV-REK: Governed Reward Engineering Kernels for Designing Robust
  Multi-Agent Reinforcement Learning Systems
Authors: Ashish Rana, Michael Oesterle, Jannik Brinkmann
Categories: cs.MA cs.AI
Comments: Extended Abstract accepted in the 23rd International Conference on
  Autonomous Agents and Multi-Agent Systems (AAMAS 2024)
\\
  For multi-agent reinforcement learning systems (MARLS), the problem
formulation generally involves investing massive reward engineering effort
specific to a given problem. However, this effort often cannot be translated to
other problems; worse, it gets wasted when system dynamics change drastically.
This problem is further exacerbated in sparse reward scenarios, where a
meaningful heuristic can assist in the policy convergence task. We propose
GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward
distributions to agents in MARLS during its learning stage. We also introduce
governance kernels, which exploit the underlying structure in either state or
joint action space for assigning meaningful agent reward distributions. During
the agent learning stage, it iteratively explores different reward distribution
configurations with a Hyperband-like algorithm to learn ideal agent reward
models in a problem-agnostic manner. Our experiments demonstrate that our
meaningful reward priors robustly jumpstart the learning process for
effectively learning different MARL problems.
\\ ( https://arxiv.org/abs/2404.01131 ,  4162kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01135 (*cross-listing*)
Date: Mon, 1 Apr 2024 14:29:58 GMT   (395kb)

Title: Enhancing Reasoning Capacity of SLM using Cognitive Enhancement
Authors: Jonathan Pan, Swee Liang Wong, Xin Wei Chia, Yidi Yuan
Categories: cs.CR cs.AI
\\
  Large Language Models (LLMs) have been applied to automate cyber security
activities and processes including cyber investigation and digital forensics.
However, the use of such models for cyber investigation and digital forensics
should address accountability and security considerations. Accountability
ensures models have the means to provide explainable reasonings and outcomes.
This information can be extracted through explicit prompt requests. For
security considerations, it is crucial to address privacy and confidentiality
of the involved data during data processing as well. One approach to deal with
this consideration is to have the data processed locally using a local instance
of the model. Due to limitations of locally available resources, namely memory
and GPU capacities, a Smaller Large Language Model (SLM) will typically be
used. These SLMs have significantly fewer parameters compared to the LLMs.
However, such size reductions have notable performance reduction, especially
when tasked to provide reasoning explanations. In this paper, we aim to
mitigate performance reduction through the integration of cognitive strategies
that humans use for problem-solving. We term this as cognitive enhancement
through prompts. Our experiments showed significant improvement gains of the
SLMs' performances when such enhancements were applied. We believe that our
exploration study paves the way for further investigation into the use of
cognitive enhancement to optimize SLM for cyber security applications.
\\ ( https://arxiv.org/abs/2404.01135 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01143 (*cross-listing*)
Date: Mon, 1 Apr 2024 14:42:57 GMT   (5448kb,D)

Title: Condition-Aware Neural Network for Controlled Image Generation
Authors: Han Cai, Muyang Li, Zhuoyang Zhang, Qinsheng Zhang, Ming-Yu Liu, Song
  Han
Categories: cs.CV cs.AI
Comments: CVPR 2024
\\
  We present Condition-Aware Neural Network (CAN), a new method for adding
control to image generative models. In parallel to prior conditional control
methods, CAN controls the image generation process by dynamically manipulating
the weight of the neural network. This is achieved by introducing a
condition-aware weight generation module that generates conditional weight for
convolution/linear layers based on the input condition. We test CAN on
class-conditional image generation on ImageNet and text-to-image generation on
COCO. CAN consistently delivers significant improvements for diffusion
transformer models, including DiT and UViT. In particular, CAN combined with
EfficientViT (CaT) achieves 2.78 FID on ImageNet 512x512, surpassing DiT-XL/2
while requiring 52x fewer MACs per sampling step.
\\ ( https://arxiv.org/abs/2404.01143 ,  5448kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01154 (*cross-listing*)
Date: Mon, 1 Apr 2024 14:59:13 GMT   (45508kb,D)

Title: Uncovering the Text Embedding in Text-to-Image Diffusion Models
Authors: Hu Yu, Hao Luo, Fan Wang, Feng Zhao
Categories: cs.CV cs.AI
\\
  The correspondence between input text and the generated image exhibits
opacity, wherein minor textual modifications can induce substantial deviations
in the generated image. While, text embedding, as the pivotal intermediary
between text and images, remains relatively underexplored. In this paper, we
address this research gap by delving into the text embedding space, unleashing
its capacity for controllable image editing and explicable semantic direction
attributes within a learning-free framework. Specifically, we identify two
critical insights regarding the importance of per-word embedding and their
contextual correlations within text embedding, providing instructive principles
for learning-free image editing. Additionally, we find that text embedding
inherently possesses diverse semantic potentials, and further reveal this
property through the lens of singular value decomposition (SVD). These
uncovered properties offer practical utility for image editing and semantic
discovery. More importantly, we expect the in-depth analyses and findings of
the text embedding can enhance the understanding of text-to-image diffusion
models.
\\ ( https://arxiv.org/abs/2404.01154 ,  45508kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01156 (*cross-listing*)
Date: Mon, 1 Apr 2024 15:01:38 GMT   (12571kb,D)

Title: SyncMask: Synchronized Attentional Masking for Fashion-centric
  Vision-Language Pretraining
Authors: Chull Hwan Song, Taebaek Hwang, Jooyoung Yoon, Shunghyun Choi, Yeong
  Hyeon Gu
Categories: cs.CV cs.AI
Comments: CVPR2024 Accepted
\\
  Vision-language models (VLMs) have made significant strides in cross-modal
understanding through large-scale paired datasets. However, in fashion domain,
datasets often exhibit a disparity between the information conveyed in image
and text. This issue stems from datasets containing multiple images of a single
fashion item all paired with one text, leading to cases where some textual
details are not visible in individual images. This mismatch, particularly when
non-co-occurring elements are masked, undermines the training of conventional
VLM objectives like Masked Language Modeling and Masked Image Modeling, thereby
hindering the model's ability to accurately align fine-grained visual and
textual features. Addressing this problem, we propose Synchronized attentional
Masking (SyncMask), which generate masks that pinpoint the image patches and
word tokens where the information co-occur in both image and text. This
synchronization is accomplished by harnessing cross-attentional features
obtained from a momentum model, ensuring a precise alignment between the two
modalities. Additionally, we enhance grouped batch sampling with semi-hard
negatives, effectively mitigating false negative issues in Image-Text Matching
and Image-Text Contrastive learning objectives within fashion datasets. Our
experiments demonstrate the effectiveness of the proposed approach,
outperforming existing methods in three downstream tasks.
\\ ( https://arxiv.org/abs/2404.01156 ,  12571kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01163 (*cross-listing*)
Date: Mon, 1 Apr 2024 15:13:46 GMT   (1400kb,D)

Title: Capturing Shock Waves by Relaxation Neural Networks
Authors: Nan Zhou, Zheng Ma
Categories: math.NA cs.AI cs.NA
MSC-class: 76L05, 35D99, 68T07, 65D15
\\
  In this paper, we put forward a neural network framework to solve the
nonlinear hyperbolic systems. This framework, named relaxation neural
networks(RelaxNN), is a simple and scalable extension of physics-informed
neural networks(PINN). It is shown later that a typical PINN framework
struggles to handle shock waves that arise in hyperbolic systems' solutions.
This ultimately results in the failure of optimization that is based on
gradient descent in the training process. Relaxation systems provide a smooth
asymptotic to the discontinuity solution, under the expectation that
macroscopic problems can be solved from a microscopic perspective. Based on
relaxation systems, the RelaxNN framework alleviates the conflict of losses in
the training process of the PINN framework. In addition to the remarkable
results demonstrated in numerical simulations, most of the acceleration
techniques and improvement strategies aimed at the standard PINN framework can
also be applied to the RelaxNN framework.
\\ ( https://arxiv.org/abs/2404.01163 ,  1400kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01223 (*cross-listing*)
Date: Mon, 1 Apr 2024 16:31:04 GMT   (28767kb,D)

Title: Feature Splatting: Language-Driven Physics-Based Scene Synthesis and
  Editing
Authors: Ri-Zhao Qiu, Ge Yang, Weijia Zeng, Xiaolong Wang
Categories: cs.CV cs.AI cs.GR cs.LG
Comments: Project website: https://feature-splatting.github.io/
\\
  Scene representations using 3D Gaussian primitives have produced excellent
results in modeling the appearance of static and dynamic 3D scenes. Many
graphics applications, however, demand the ability to manipulate both the
appearance and the physical properties of objects. We introduce Feature
Splatting, an approach that unifies physics-based dynamic scene synthesis with
rich semantics from vision language foundation models that are grounded by
natural language. Our first contribution is a way to distill high-quality,
object-centric vision-language features into 3D Gaussians, that enables
semi-automatic scene decomposition using text queries. Our second contribution
is a way to synthesize physics-based dynamics from an otherwise static scene
using a particle-based simulator, in which material properties are assigned
automatically via text queries. We ablate key techniques used in this pipeline,
to illustrate the challenge and opportunities in using feature-carrying 3D
Gaussians as a unified format for appearance, geometry, material properties and
semantics grounded on natural language. Project website:
https://feature-splatting.github.io/
\\ ( https://arxiv.org/abs/2404.01223 ,  28767kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01258 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:28:16 GMT   (3439kb,D)

Title: Direct Preference Optimization of Video Large Multimodal Models from
  Language Model Reward
Authors: Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu,
  Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, and
  Yiming Yang
Categories: cs.CV cs.AI
\\
  Preference modeling techniques, such as direct preference optimization (DPO),
has shown effective in enhancing the generalization abilities of large language
model (LLM). However, in tasks involving video instruction-following, providing
informative feedback, especially for detecting hallucinations in generated
responses, remains a significant challenge. Previous studies have explored
using large large multimodal models (LMMs) as reward models to guide preference
modeling, but their ability to accurately assess the factuality of generated
responses compared to corresponding videos has not been conclusively
established. This paper introduces a novel framework that utilizes detailed
video captions as a proxy of video content, enabling language models to
incorporate this information as supporting evidence for scoring video Question
Answering (QA) predictions. Our approach demonstrates robust alignment with
OpenAI GPT-4V model's reward mechanism, which directly takes video frames as
input. Furthermore, we show that applying this tailored reward through DPO
significantly improves the performance of video LMMs on video QA tasks.
\\ ( https://arxiv.org/abs/2404.01258 ,  3439kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01260 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:30:56 GMT   (15558kb,D)

Title: Bridging Remote Sensors with Multisensor Geospatial Foundation Models
Authors: Boran Han, Shuai Zhang, Xingjian Shi, Markus Reichstein
Categories: cs.CV cs.AI cs.LG
Comments: Accepted to CVPR
\\
  In the realm of geospatial analysis, the diversity of remote sensors,
encompassing both optical and microwave technologies, offers a wealth of
distinct observational capabilities. Recognizing this, we present msGFM, a
multisensor geospatial foundation model that effectively unifies data from four
key sensor modalities. This integration spans an expansive dataset of two
million multisensor images. msGFM is uniquely adept at handling both paired and
unpaired sensor data. For data originating from identical geolocations, our
model employs an innovative cross-sensor pretraining approach in masked image
modeling, enabling the synthesis of joint representations from diverse sensors.
msGFM, incorporating four remote sensors, upholds strong performance, forming a
comprehensive model adaptable to various sensor types. msGFM has demonstrated
enhanced proficiency in a range of both single-sensor and multisensor
downstream tasks. These include scene classification, segmentation, cloud
removal, and pan-sharpening. A key discovery of our research is that
representations derived from natural images are not always compatible with the
distinct characteristics of geospatial remote sensors, underscoring the
limitations of existing representations in this field. Our work can serve as a
guide for developing multisensor geospatial pretraining models, paving the way
for more advanced geospatial capabilities.
\\ ( https://arxiv.org/abs/2404.01260 ,  15558kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01291 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:58:06 GMT   (5155kb,D)

Title: Evaluating Text-to-Visual Generation with Image-to-Text Generation
Authors: Zhiqiu Lin and Deepak Pathak and Baiqi Li and Jiayao Li and Xide Xia
  and Graham Neubig and Pengchuan Zhang and Deva Ramanan
Categories: cs.CV cs.AI cs.CL cs.LG cs.MM
Comments: We open-source our data, model, and code at:
  https://github.com/linzhiqiu/t2v_metrics ; Project page:
  https://linzhiqiu.github.io/papers/vqascore
\\
  Despite significant progress in generative AI, comprehensive evaluation
remains challenging because of the lack of effective metrics and standardized
benchmarks. For instance, the widely-used CLIPScore measures the alignment
between a (generated) image and text prompt, but it fails to produce reliable
scores for complex prompts involving compositions of objects, attributes, and
relations. One reason is that text encoders of CLIP can notoriously act as a
"bag of words", conflating prompts such as "the horse is eating the grass" with
"the grass is eating the horse". To address this, we introduce the VQAScore,
which uses a visual-question-answering (VQA) model to produce an alignment
score by computing the probability of a "Yes" answer to a simple "Does this
figure show '{text}'?" question. Though simpler than prior art, VQAScore
computed with off-the-shelf models produces state-of-the-art results across
many (8) image-text alignment benchmarks. We also compute VQAScore with an
in-house model that follows best practices in the literature. For example, we
use a bidirectional image-question encoder that allows image embeddings to
depend on the question being asked (and vice versa). Our in-house model,
CLIP-FlanT5, outperforms even the strongest baselines that make use of the
proprietary GPT-4V. Interestingly, although we train with only images, VQAScore
can also align text with video and 3D models. VQAScore allows researchers to
benchmark text-to-visual generation using complex texts that capture the
compositional structure of real-world prompts. We introduce GenAI-Bench, a more
challenging benchmark with 1,600 compositional text prompts that require
parsing scenes, objects, attributes, relationships, and high-order reasoning
like comparison and logic. GenAI-Bench also offers over 15,000 human ratings
for leading image and video generation models such as Stable Diffusion, DALL-E
3, and Gen2.
\\ ( https://arxiv.org/abs/2404.01291 ,  5155kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01299 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:59:53 GMT   (12717kb,D)

Title: CausalChaos! Dataset for Comprehensive Causal Action Question Answering
  Over Longer Causal Chains Grounded in Dynamic Visual Scenes
Authors: Ting En Lam, Yuhan Chen, Elston Tan, Eric Peh, Ruirui Chen, Paritosh
  Parmar, Basura Fernando
Categories: cs.CV cs.AI cs.CL cs.LG
\\
  Causal video question answering (QA) has garnered increasing interest, yet
existing datasets often lack depth in causal reasoning analysis. To address
this gap, we capitalize on the unique properties of cartoons and construct
CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic
"Tom and Jerry" cartoon series. With thoughtful questions and multi-level
answers, our dataset contains much longer causal chains embedded in dynamic
interactions and visuals, at the same time principles of animation allows
animators to create well-defined, unambiguous causal relationships. These
factors allow models to solve more challenging, yet well-defined causal
relationships. We also introduce hard negative mining, including
CausalConfusion version. While models perform well, there is much room for
improvement, especially, on open-ended answers. We identify more
advanced/explicit causal relationship modeling and joint modeling of vision and
language as the immediate areas for future efforts to focus upon. Along with
the other complementary datasets, our new challenging dataset will pave the way
for these developments in the field. We will release our dataset, codes, and
models to help future efforts in this domain.
\\ ( https://arxiv.org/abs/2404.01299 ,  12717kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01300 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:59:55 GMT   (7525kb,D)

Title: NeRF-MAE : Masked AutoEncoders for Self Supervised 3D representation
  Learning for Neural Radiance Fields
Authors: Muhammad Zubair Irshad, Sergey Zakahrov, Vitor Guizilini, Adrien
  Gaidon, Zsolt Kira, Rares Ambrus
Categories: cs.CV cs.AI cs.LG
Comments: 29 pages, 13 figures. Project Page: https://nerf-mae.github.io/
\\
  Neural fields excel in computer vision and robotics due to their ability to
understand the 3D visual world such as inferring semantics, geometry, and
dynamics. Given the capabilities of neural fields in densely representing a 3D
scene from 2D images, we ask the question: Can we scale their self-supervised
pretraining, specifically using masked autoencoders, to generate effective 3D
representations from posed RGB images. Owing to the astounding success of
extending transformers to novel data modalities, we employ standard 3D Vision
Transformers to suit the unique formulation of NeRFs. We leverage NeRF's
volumetric grid as a dense input to the transformer, contrasting it with other
3D representations such as pointclouds where the information density can be
uneven, and the representation is irregular. Due to the difficulty of applying
masked autoencoders to an implicit representation, such as NeRF, we opt for
extracting an explicit representation that canonicalizes scenes across domains
by employing the camera trajectory for sampling. Our goal is made possible by
masking random patches from NeRF's radiance and density grid and employing a
standard 3D Swin Transformer to reconstruct the masked patches. In doing so,
the model can learn the semantic and spatial structure of complete scenes. We
pretrain this representation at scale on our proposed curated posed-RGB data,
totaling over 1.6 million images. Once pretrained, the encoder is used for
effective 3D transfer learning. Our novel self-supervised pretraining for
NeRFs, NeRF-MAE, scales remarkably well and improves performance on various
challenging 3D tasks. Utilizing unlabeled posed 2D data for pretraining,
NeRF-MAE significantly outperforms self-supervised 3D pretraining and NeRF
scene understanding baselines on Front3D and ScanNet datasets with an absolute
performance improvement of over 20% AP50 and 8% AP25 for 3D object detection.
\\ ( https://arxiv.org/abs/2404.01300 ,  7525kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00011 (*cross-listing*)
Date: Tue, 12 Mar 2024 02:37:24 GMT   (682kb)

Title: A novel interface for adversarial trivia question-writing
Authors: Jason Liu
Categories: cs.HC cs.CL
Comments: 17 pages, 1 figure, 1 table
\\
  A critical component when developing question-answering AIs is an adversarial
dataset that challenges models to adapt to the complex syntax and reasoning
underlying our natural language. Present techniques for procedurally generating
adversarial texts are not robust enough for training on complex tasks such as
answering multi-sentence trivia questions. We instead turn to human-generated
data by introducing an interface for collecting adversarial human-written
trivia questions. Our interface is aimed towards question writers and players
of Quiz Bowl, a buzzer-based trivia competition where paragraph-long questions
consist of a sequence of clues of decreasing difficulty. To incentivize usage,
a suite of machine learning-based tools in our interface assist humans in
writing questions that are more challenging to answer for Quiz Bowl players and
computers alike. Not only does our interface gather training data for the
groundbreaking Quiz Bowl AI project QANTA, but it is also a proof-of-concept of
future adversarial data collection for question-answering systems. The results
of performance-testing our interface with ten originally-composed questions
indicate that, despite some flaws, our interface's novel question-writing
features as well as its real-time exposure of useful responses from our machine
models could facilitate and enhance the collection of adversarial questions.
\\ ( https://arxiv.org/abs/2404.00011 ,  682kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00226 (*cross-listing*)
Date: Sat, 30 Mar 2024 02:56:54 GMT   (1612kb,D)

Title: Design as Desired: Utilizing Visual Question Answering for Multimodal
  Pre-training
Authors: Tongkun Su and Jun Li and Xi Zhang and Haibo Jin and Hao Chen and
  Qiong Wang and Faqin Lv and Baoliang Zhao and Yin Hu
Categories: cs.CV cs.CL
\\
  Multimodal pre-training demonstrates its potential in the medical domain,
which learns medical visual representations from paired medical reports.
However, many pre-training tasks require extra annotations from clinicians, and
most of them fail to explicitly guide the model to learn the desired features
of different pathologies. To the best of our knowledge, we are the first to
utilize Visual Question Answering (VQA) for multimodal pre-training to guide
the framework focusing on targeted pathological features. In this work, we
leverage descriptions in medical reports to design multi-granular
question-answer pairs associated with different diseases, which assist the
framework in pre-training without requiring extra annotations from experts. We
also propose a novel pre-training framework with a quasi-textual feature
transformer, a module designed to transform visual features into a
quasi-textual space closer to the textual domain via a contrastive learning
strategy. This narrows the vision-language gap and facilitates modality
alignment. Our framework is applied to four downstream tasks: report
generation, classification, segmentation, and detection across five datasets.
Extensive experiments demonstrate the superiority of our framework compared to
other state-of-the-art methods. Our code will be released upon acceptance.
\\ ( https://arxiv.org/abs/2404.00226 ,  1612kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00227 (*cross-listing*)
Date: Sat, 30 Mar 2024 02:57:55 GMT   (7720kb,D)

Title: A Survey of using Large Language Models for Generating Infrastructure as
  Code
Authors: Kalahasti Ganesh Srivatsa, Sabyasachi Mukhopadhyay, Ganesh Katrapati,
  Manish Shrivastava
Categories: cs.SE cs.CL
Comments: Accepted in ICON2023
\\
  Infrastructure as Code (IaC) is a revolutionary approach which has gained
significant prominence in the Industry. IaC manages and provisions IT
infrastructure using machine-readable code by enabling automation, consistency
across the environments, reproducibility, version control, error reduction and
enhancement in scalability. However, IaC orchestration is often a painstaking
effort which requires specialised skills as well as a lot of manual effort.
Automation of IaC is a necessity in the present conditions of the Industry and
in this survey, we study the feasibility of applying Large Language Models
(LLM) to address this problem. LLMs are large neural network-based models which
have demonstrated significant language processing abilities and shown to be
capable of following a range of instructions within a broad scope. Recently,
they have also been adapted for code understanding and generation tasks
successfully, which makes them a promising choice for the automatic generation
of IaC configurations. In this survey, we delve into the details of IaC, usage
of IaC in different platforms, their challenges, LLMs in terms of
code-generation aspects and the importance of LLMs in IaC along with our own
experiments. Finally, we conclude by presenting the challenges in this area and
highlighting the scope for future research.
\\ ( https://arxiv.org/abs/2404.00227 ,  7720kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00236 (*cross-listing*)
Date: Sat, 30 Mar 2024 03:56:53 GMT   (684kb,D)

Title: Enhancing Content-based Recommendation via Large Language Model
Authors: Wentao Xu, Qianqian Xie, Shuo Yang, Jiangxia Cao, Shuchao Pang
Categories: cs.IR cs.CL
Comments: Work in progress
\\
  In real-world applications, users express different behaviors when they
interact with different items, including implicit click/like interactions, and
explicit comments/reviews interactions. Nevertheless, almost all recommender
works are focused on how to describe user preferences by the implicit
click/like interactions, to find the synergy of people. For the content-based
explicit comments/reviews interactions, some works attempt to utilize them to
mine the semantic knowledge to enhance recommender models. However, they still
neglect the following two points: (1) The content semantic is a universal world
knowledge; how do we extract the multi-aspect semantic information to empower
different domains? (2) The user/item ID feature is a fundamental element for
recommender models; how do we align the ID and content semantic feature space?
In this paper, we propose a `plugin' semantic knowledge transferring method
\textbf{LoID}, which includes two major components: (1) LoRA-based large
language model pretraining to extract multi-aspect semantic information; (2)
ID-based contrastive objective to align their feature spaces. We conduct
extensive experiments with SOTA baselines on real-world datasets, the detailed
results demonstrating significant improvements of our method LoID.
\\ ( https://arxiv.org/abs/2404.00236 ,  684kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00419 (*cross-listing*)
Date: Sat, 30 Mar 2024 16:54:45 GMT   (44493kb,D)

Title: Do Vision-Language Models Understand Compound Nouns?
Authors: Sonal Kumar and Sreyan Ghosh and S Sakshi and Utkarsh Tyagi and Dinesh
  Manocha
Categories: cs.CV cs.CL
Comments: Accepted to NAACL 2024 Main Conference
\\
  Open-vocabulary vision-language models (VLMs) like CLIP, trained using
contrastive loss, have emerged as a promising new paradigm for text-to-image
retrieval. However, do VLMs understand compound nouns (CNs) (e.g., lab coat) as
well as they understand nouns (e.g., lab)? We curate Compun, a novel benchmark
with 400 unique and commonly used CNs, to evaluate the effectiveness of VLMs in
interpreting CNs. The Compun benchmark challenges a VLM for text-to-image
retrieval where, given a text prompt with a CN, the task is to select the
correct image that shows the CN among a pair of distractor images that show the
constituent nouns that make up the CN. Next, we perform an in-depth analysis to
highlight CLIPs' limited understanding of certain types of CNs. Finally, we
present an alternative framework that moves beyond hand-written templates for
text prompts widely used by CLIP-like models. We employ a Large Language Model
to generate multiple diverse captions that include the CN as an object in the
scene described by the caption. Our proposed method improves CN understanding
of CLIP by 8.25% on Compun. Code and benchmark are available at:
https://github.com/sonalkum/Compun
\\ ( https://arxiv.org/abs/2404.00419 ,  44493kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00566 (*cross-listing*)
Date: Sun, 31 Mar 2024 05:20:53 GMT   (1128kb,D)

Title: CodeBenchGen: Creating Scalable Execution-based Code Generation
  Benchmarks
Authors: Yiqing Xie, Alex Xie, Divyanshu Sheth, Pengfei Liu, Daniel Fried,
  Carolyn Rose
Categories: cs.SE cs.CL
\\
  To facilitate evaluation of code generation systems across diverse scenarios,
we present CodeBenchGen, a framework to create scalable execution-based
benchmarks that only requires light guidance from humans. Specifically, we
leverage a large language model (LLM) to convert an arbitrary piece of code
into an evaluation example, including test cases for execution-based
evaluation. We illustrate the usefulness of our framework by creating a
dataset, Exec-CSN, which includes 1,931 examples involving 293 libraries
revised from code in 367 GitHub repositories taken from the CodeSearchNet
dataset. To demonstrate the complexity and solvability of examples in Exec-CSN,
we present a human study demonstrating that 81.3% of the examples can be solved
by humans and 61% are rated as ``requires effort to solve''. We conduct code
generation experiments on open-source and proprietary models and analyze the
performance of both humans and models. We will release the code of both the
framework and the dataset upon acceptance.
\\ ( https://arxiv.org/abs/2404.00566 ,  1128kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00569 (*cross-listing*)
Date: Sun, 31 Mar 2024 05:38:08 GMT   (1694kb,D)

Title: CM-TTS: Enhancing Real Time Text-to-Speech Synthesis Efficiency through
  Weighted Samplers and Consistency Models
Authors: Xiang Li, Fan Bu, Ambuj Mehrish, Yingting Li, Jiale Han, Bo Cheng,
  Soujanya Poria
Categories: cs.SD cs.CL eess.AS
Comments: Accepted by Findings of NAACL 2024. Code is available at
  https://github.com/XiangLi2022/CM-TTS
\\
  Neural Text-to-Speech (TTS) systems find broad applications in voice
assistants, e-learning, and audiobook creation. The pursuit of modern models,
like Diffusion Models (DMs), holds promise for achieving high-fidelity,
real-time speech synthesis. Yet, the efficiency of multi-step sampling in
Diffusion Models presents challenges. Efforts have been made to integrate GANs
with DMs, speeding up inference by approximating denoising distributions, but
this introduces issues with model convergence due to adversarial training. To
overcome this, we introduce CM-TTS, a novel architecture grounded in
consistency models (CMs). Drawing inspiration from continuous-time diffusion
models, CM-TTS achieves top-quality speech synthesis in fewer steps without
adversarial training or pre-trained model dependencies. We further design
weighted samplers to incorporate different sampling positions into model
training with dynamic probabilities, ensuring unbiased learning throughout the
entire training process. We present a real-time mel-spectrogram generation
consistency model, validated through comprehensive evaluations. Experimental
results underscore CM-TTS's superiority over existing single-step speech
synthesis systems, representing a significant advancement in the field.
\\ ( https://arxiv.org/abs/2404.00569 ,  1694kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00590 (*cross-listing*)
Date: Sun, 31 Mar 2024 07:49:23 GMT   (965kb,D)

Title: CuSINeS: Curriculum-driven Structure Induced Negative Sampling for
  Statutory Article Retrieval
Authors: T.Y.S.S Santosh, Kristina Kaiser, Matthias Grabmair
Categories: cs.IR cs.CL
Comments: Accepted to LREC-COLING 2024
\\
  In this paper, we introduce CuSINeS, a negative sampling approach to enhance
the performance of Statutory Article Retrieval (SAR). CuSINeS offers three key
contributions. Firstly, it employs a curriculum-based negative sampling
strategy guiding the model to focus on easier negatives initially and
progressively tackle more difficult ones. Secondly, it leverages the
hierarchical and sequential information derived from the structural
organization of statutes to evaluate the difficulty of samples. Lastly, it
introduces a dynamic semantic difficulty assessment using the being-trained
model itself, surpassing conventional static methods like BM25, adapting the
negatives to the model's evolving competence. Experimental results on a
real-world expert-annotated SAR dataset validate the effectiveness of CuSINeS
across four different baselines, demonstrating its versatility.
\\ ( https://arxiv.org/abs/2404.00590 ,  965kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00925 (*cross-listing*)
Date: Mon, 1 Apr 2024 05:07:13 GMT   (216kb,D)

Title: LLMs are Good Sign Language Translators
Authors: Jia Gong, Lin Geng Foo, Yixuan He, Hossein Rahmani, Jun Liu
Categories: cs.CV cs.CL
Comments: Accepted to CVPR 2024
\\
  Sign Language Translation (SLT) is a challenging task that aims to translate
sign videos into spoken language. Inspired by the strong translation
capabilities of large language models (LLMs) that are trained on extensive
multilingual text corpora, we aim to harness off-the-shelf LLMs to handle SLT.
In this paper, we regularize the sign videos to embody linguistic
characteristics of spoken language, and propose a novel SignLLM framework to
transform sign videos into a language-like representation for improved
readability by off-the-shelf LLMs. SignLLM comprises two key modules: (1) The
Vector-Quantized Visual Sign module converts sign videos into a sequence of
discrete character-level sign tokens, and (2) the Codebook Reconstruction and
Alignment module converts these character-level tokens into word-level sign
representations using an optimal transport formulation. A sign-text alignment
loss further bridges the gap between sign and text tokens, enhancing semantic
compatibility. We achieve state-of-the-art gloss-free results on two
widely-used SLT benchmarks.
\\ ( https://arxiv.org/abs/2404.00925 ,  216kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00938 (*cross-listing*)
Date: Mon, 1 Apr 2024 05:50:56 GMT   (16kb)

Title: How Can Large Language Models Enable Better Socially Assistive
  Human-Robot Interaction: A Brief Survey
Authors: Zhonghao Shi, Ellen Landrum, Amy O' Connell, Mina Kian, Leticia
  Pinto-Alva, Kaleen Shrestha, Xiaoyuan Zhu, Maja J Matari\'c
Categories: cs.HC cs.CL cs.CV cs.RO
Comments: 2 pages, to be submitted to 2024 AAAI Spring Symposium
\\
  Socially assistive robots (SARs) have shown great success in providing
personalized cognitive-affective support for user populations with special
needs such as older adults, children with autism spectrum disorder (ASD), and
individuals with mental health challenges. The large body of work on SAR
demonstrates its potential to provide at-home support that complements
clinic-based interventions delivered by mental health professionals, making
these interventions more effective and accessible. However, there are still
several major technical challenges that hinder SAR-mediated interactions and
interventions from reaching human-level social intelligence and efficacy. With
the recent advances in large language models (LLMs), there is an increased
potential for novel applications within the field of SAR that can significantly
expand the current capabilities of SARs. However, incorporating LLMs introduces
new risks and ethical concerns that have not yet been encountered, and must be
carefully be addressed to safely deploy these more advanced systems. In this
work, we aim to conduct a brief survey on the use of LLMs in SAR technologies,
and discuss the potentials and risks of applying LLMs to the following three
major technical challenges of SAR: 1) natural language dialog; 2) multimodal
understanding; 3) LLMs as robot policies.
\\ ( https://arxiv.org/abs/2404.00938 ,  16kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01240 (*cross-listing*)
Date: Mon, 1 Apr 2024 16:58:32 GMT   (3384kb,D)

Title: AURORA: Navigating UI Tarpits via Automated Neural Screen Understanding
Authors: Safwat Ali Khan, Wenyu Wang, Yiran Ren, Bin Zhu, Jiangfan Shi, Alyssa
  McGowan, Wing Lam, Kevin Moran
Categories: cs.SE cs.CL cs.CV cs.HC
Comments: Published at 17th IEEE International Conference on Software Testing,
  Verification and Validation (ICST) 2024, 12 pages
\\
  Nearly a decade of research in software engineering has focused on automating
mobile app testing to help engineers in overcoming the unique challenges
associated with the software platform. Much of this work has come in the form
of Automated Input Generation tools (AIG tools) that dynamically explore app
screens. However, such tools have repeatedly been demonstrated to achieve
lower-than-expected code coverage - particularly on sophisticated proprietary
apps. Prior work has illustrated that a primary cause of these coverage
deficiencies is related to so-called tarpits, or complex screens that are
difficult to navigate.
  In this paper, we take a critical step toward enabling AIG tools to
effectively navigate tarpits during app exploration through a new form of
automated semantic screen understanding. We introduce AURORA, a technique that
learns from the visual and textual patterns that exist in mobile app UIs to
automatically detect common screen designs and navigate them accordingly. The
key idea of AURORA is that there are a finite number of mobile app screen
designs, albeit with subtle variations, such that the general patterns of
different categories of UI designs can be learned. As such, AURORA employs a
multi-modal, neural screen classifier that is able to recognize the most common
types of UI screen designs. After recognizing a given screen, it then applies a
set of flexible and generalizable heuristics to properly navigate the screen.
We evaluated AURORA both on a set of 12 apps with known tarpits from prior
work, and on a new set of five of the most popular apps from the Google Play
store. Our results indicate that AURORA is able to effectively navigate tarpit
screens, outperforming prior approaches that avoid tarpits by 19.6% in terms of
method coverage. The improvements can be attributed to AURORA's UI design
classification and heuristic navigation techniques.
\\ ( https://arxiv.org/abs/2404.01240 ,  3384kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01245 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:03:41 GMT   (3449kb,D)

Title: A Statistical Framework of Watermarks for Large Language Models: Pivot,
  Detection Efficiency and Optimal Rules
Authors: Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su
Categories: math.ST cs.CL cs.CR cs.LG stat.ML stat.TH
\\
  Since ChatGPT was introduced in November 2022, embedding (nearly)
unnoticeable statistical signals into text generated by large language models
(LLMs), also known as watermarking, has been used as a principled approach to
provable detection of LLM-generated text from its human-written counterpart. In
this paper, we introduce a general and flexible framework for reasoning about
the statistical efficiency of watermarks and designing powerful detection
rules. Inspired by the hypothesis testing formulation of watermark detection,
our framework starts by selecting a pivotal statistic of the text and a secret
key -- provided by the LLM to the verifier -- to enable controlling the false
positive rate (the error of mistakenly detecting human-written text as
LLM-generated). Next, this framework allows one to evaluate the power of
watermark detection rules by obtaining a closed-form expression of the
asymptotic false negative rate (the error of incorrectly classifying
LLM-generated text as human-written). Our framework further reduces the problem
of determining the optimal detection rule to solving a minimax optimization
program. We apply this framework to two representative watermarks -- one of
which has been internally implemented at OpenAI -- and obtain several findings
that can be instrumental in guiding the practice of implementing watermarks. In
particular, we derive optimal detection rules for these watermarks under our
framework. These theoretically derived detection rules are demonstrated to be
competitive and sometimes enjoy a higher power than existing detection
approaches through numerical experiments.
\\ ( https://arxiv.org/abs/2404.01245 ,  3449kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00015 (*cross-listing*)
Date: Fri, 15 Mar 2024 16:42:03 GMT   (554kb,D)

Title: Empowering Credit Scoring Systems with Quantum-Enhanced Machine Learning
Authors: Javier Mancilla, Andr\'e Sequeira, Iraitz Montalb\'an, Tomas Tagliani,
  Frnacisco Llaneza, Claudio Beiza
Categories: q-fin.RM cs.LG q-fin.ST quant-ph stat.ML
Comments: Preprint
\\
  Quantum Kernels are projected to provide early-stage usefulness for quantum
machine learning. However, highly sophisticated classical models are hard to
surpass without losing interpretability, particularly when vast datasets can be
exploited. Nonetheless, classical models struggle once data is scarce and
skewed. Quantum feature spaces are projected to find better links between data
features and the target class to be predicted even in such challenging
scenarios and most importantly, enhanced generalization capabilities. In this
work, we propose a novel approach called Systemic Quantum Score (SQS) and
provide preliminary results indicating potential advantage over purely
classical models in a production grade use case for the Finance sector. SQS
shows in our specific study an increased capacity to extract patterns out of
fewer data points as well as improved performance over data-hungry algorithms
such as XGBoost, providing advantage in a competitive market as it is the
FinTech and Neobank regime.
\\ ( https://arxiv.org/abs/2404.00015 ,  554kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00016 (*cross-listing*)
Date: Fri, 15 Mar 2024 17:02:59 GMT   (652kb,D)

Title: SOMson -- Sonification of Multidimensional Data in Kohonen Maps
Authors: Simon Linke and Tim Ziemer
Categories: cs.HC cs.LG
Comments: 8 pages, 5 figures, linked YouTube videos and interactive demos
MSC-class: 68T07 (Primary), 97P80, 97U80 (Secondary)
ACM-class: I.2; I.5
\\
  Kohonen Maps, aka. Self-organizing maps (SOMs) are neural networks that
visualize a high-dimensional feature space on a low-dimensional map. While SOMs
are an excellent tool for data examination and exploration, they inherently
cause a loss of detail. Visualizations of the underlying data do not integrate
well and, therefore, fail to provide an overall picture. Consequently, we
suggest SOMson, an interactive sonification of the underlying data, as a data
augmentation technique. The sonification increases the amount of information
provided simultaneously by the SOM. Instead of a user study, we present an
interactive online example, so readers can explore SOMson themselves. Its
strengths, weaknesses, and prospects are discussed.
\\ ( https://arxiv.org/abs/2404.00016 ,  652kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00030 (*cross-listing*)
Date: Fri, 22 Mar 2024 07:13:10 GMT   (1241kb,D)

Title: Visualization of Unstructured Sports Data -- An Example of Cricket Short
  Text Commentary
Authors: Swarup Ranjan Behera and Vijaya V Saradhi
Categories: cs.HC cs.LG
ACM-class: I.2.7
\\
  Sports visualization focuses on the use of structured data, such as box-score
data and tracking data. Unstructured data sources pertaining to sports are
available in various places such as blogs, social media posts, and online news
articles. Sports visualization methods either not fully exploited the
information present in these sources or the proposed visualizations through the
use of these sources did not augment to the body of sports visualization
methods. We propose the use of unstructured data, namely cricket short text
commentary for visualization. The short text commentary data is used for
constructing individual player's strength rules and weakness rules. A
computationally feasible definition for player's strength rule and weakness
rule is proposed. A visualization method for the constructed rules is
presented. In addition, players having similar strength rules or weakness rules
is computed and visualized. We demonstrate the usefulness of short text
commentary in visualization by analyzing the strengths and weaknesses of
cricket players using more than one million text commentaries. We validate the
constructed rules through two validation methods. The collected data, source
code, and obtained results on more than 500 players are made publicly
available.
\\ ( https://arxiv.org/abs/2404.00030 ,  1241kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00031 (*cross-listing*)
Date: Fri, 22 Mar 2024 13:34:46 GMT   (365kb,D)

Title: Towards gaze-independent c-VEP BCI: A pilot study
Authors: S. Narayanan, S. Ahmadi, P. Desain, J. Thielen
Categories: cs.HC cs.LG
Comments: 6 pages, 3 figures, 9th Graz Brain-Computer Interface Conference 2024
\\
  A limitation of brain-computer interface (BCI) spellers is that they require
the user to be able to move the eyes to fixate on targets. This poses an issue
for users who cannot voluntarily control their eye movements, for instance,
people living with late-stage amyotrophic lateral sclerosis (ALS). This pilot
study makes the first step towards a gaze-independent speller based on the
code-modulated visual evoked potential (c-VEP). Participants were presented
with two bi-laterally located stimuli, one of which was flashing, and were
tasked to attend to one of these stimuli either by directly looking at the
stimuli (overt condition) or by using spatial attention, eliminating the need
for eye movement (covert condition). The attended stimuli were decoded from
electroencephalography (EEG) and classification accuracies of 88% and 100% were
obtained for the covert and overt conditions, respectively. These fundamental
insights show the promising feasibility of utilizing the c-VEP protocol for
gaze-independent BCIs that use covert spatial attention when both stimuli flash
simultaneously.
\\ ( https://arxiv.org/abs/2404.00031 ,  365kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00034 (*cross-listing*)
Date: Sat, 23 Mar 2024 09:36:31 GMT   (4017kb,D)

Title: Investigating Similarities Across Decentralized Financial (DeFi)
  Services
Authors: Junliang Luo, Stefan Kitzler, Pietro Saggese
Categories: q-fin.ST cs.LG q-fin.GN
\\
  We explore the adoption of graph representation learning (GRL) algorithms to
investigate similarities across services offered by Decentralized Finance
(DeFi) protocols. Following existing literature, we use Ethereum transaction
data to identify the DeFi building blocks. These are sets of protocol-specific
smart contracts that are utilized in combination within single transactions and
encapsulate the logic to conduct specific financial services such as swapping
or lending cryptoassets. We propose a method to categorize these blocks into
clusters based on their smart contract attributes and the graph structure of
their smart contract calls. We employ GRL to create embedding vectors from
building blocks and agglomerative models for clustering them. To evaluate
whether they are effectively grouped in clusters of similar functionalities, we
associate them with eight financial functionality categories and use this
information as the target label. We find that in the best-case scenario purity
reaches .888. We use additional information to associate the building blocks
with protocol-specific target labels, obtaining comparable purity (.864) but
higher V-Measure (.571); we discuss plausible explanations for this difference.
In summary, this method helps categorize existing financial products offered by
DeFi protocols, and can effectively automatize the detection of similar DeFi
services, especially within protocols.
\\ ( https://arxiv.org/abs/2404.00034 ,  4017kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00043 (*cross-listing*)
Date: Sun, 24 Mar 2024 21:19:17 GMT   (5614kb)

Title: Improve accessibility for Low Vision and Blind people using Machine
  Learning and Computer Vision
Authors: Jasur Shukurov
Categories: cs.HC cs.CV cs.LG
\\
  With the ever-growing expansion of mobile technology worldwide, there is an
increasing need for accommodation for those who are disabled. This project
explores how machine learning and computer vision could be utilized to improve
accessibility for people with visual impairments. There have been many attempts
to develop various software that would improve accessibility in the day-to-day
lives of blind people. However, applications on the market have low accuracy
and only provide audio feedback. This project will concentrate on building a
mobile application that helps blind people to orient in space by receiving
audio and haptic feedback, e.g. vibrations, about their surroundings in
real-time. The mobile application will have 3 main features. The initial
feature is scanning text from the camera and reading it to a user. This feature
can be used on paper with text, in the environment, and on road signs. The
second feature is detecting objects around the user, and providing audio
feedback about those objects. It also includes providing the description of the
objects and their location, and giving haptic feedback if the user is too close
to an object. The last feature is currency detection which provides a total
amount of currency value to the user via the camera.
\\ ( https://arxiv.org/abs/2404.00043 ,  5614kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00048 (*cross-listing*)
Date: Mon, 25 Mar 2024 11:10:49 GMT   (7654kb)

Title: SLIMBRAIN: Augmented Reality Real-Time Acquisition and Processing System
  For Hyperspectral Classification Mapping with Depth Information for In-Vivo
  Surgical Procedures
Authors: Jaime Sancho, Manuel Villa, Miguel Chavarr\'ias, Eduardo Juarez,
  Alfonso Lagares, C\'esar Sanz
Categories: cs.HC cs.CV cs.LG
Comments: 14 pages, 19 figues
DOI: 10.1016/j.sysarc.2023.102893
\\
  Over the last two decades, augmented reality (AR) has led to the rapid
development of new interfaces in various fields of social and technological
application domains. One such domain is medicine, and to a higher extent
surgery, where these visualization techniques help to improve the effectiveness
of preoperative and intraoperative procedures. Following this trend, this paper
presents SLIMBRAIN, a real-time acquisition and processing AR system suitable
to classify and display brain tumor tissue from hyperspectral (HS) information.
This system captures and processes HS images at 14 frames per second (FPS)
during the course of a tumor resection operation to detect and delimit cancer
tissue at the same time the neurosurgeon operates. The result is represented in
an AR visualization where the classification results are overlapped with the
RGB point cloud captured by a LiDAR camera. This representation allows natural
navigation of the scene at the same time it is captured and processed,
improving the visualization and hence effectiveness of the HS technology to
delimit tumors. The whole system has been verified in real brain tumor
resection operations.
\\ ( https://arxiv.org/abs/2404.00048 ,  7654kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00050 (*cross-listing*)
Date: Mon, 25 Mar 2024 15:11:15 GMT   (9270kb,D)

Title: Grappa -- A Machine Learned Molecular Mechanics Force Field
Authors: Leif Seute, Eric Hartmann, Jan St\"uhmer, Frauke Gr\"ater
Categories: physics.chem-ph cs.LG physics.comp-ph
\\
  Simulating large molecular systems over long timescales requires force fields
that are both accurate and efficient. In recent years, E(3) equivariant neural
networks have lifted the tension between computational efficiency and accuracy
of force fields, but they are still several orders of magnitude more expensive
than classical molecular mechanics (MM) force fields.
  Here, we propose a novel machine learning architecture to predict MM
parameters from the molecular graph, employing a graph attentional neural
network and a transformer with symmetry-preserving positional encoding. The
resulting force field, Grappa, outperforms established and other
machine-learned MM force fields in terms of accuracy at the same computational
efficiency and can be used in existing Molecular Dynamics (MD) engines like
GROMACS and OpenMM. It predicts energies and forces of small molecules,
peptides, RNA and - showcasing its extensibility to uncharted regions of
chemical space - radicals at state-of-the-art MM accuracy. We demonstrate
Grappa's transferability to macromolecules in MD simulations, during which
large protein are kept stable and small proteins can fold. Our force field sets
the stage for biomolecular simulations close to chemical accuracy, but with the
same computational cost as established protein force fields.
\\ ( https://arxiv.org/abs/2404.00050 ,  9270kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00054 (*cross-listing*)
Date: Tue, 26 Mar 2024 01:42:13 GMT   (5865kb,D)

Title: Choreographing the Digital Canvas: A Machine Learning Approach to
  Artistic Performance
Authors: Siyuan Peng, Kate Ladenheim, Snehesh Shrestha, Cornelia Ferm\"uller
Categories: cs.HC cs.GR cs.LG
\\
  This paper introduces the concept of a design tool for artistic performances
based on attribute descriptions. To do so, we used a specific performance of
falling actions. The platform integrates a novel machine-learning (ML) model
with an interactive interface to generate and visualize artistic movements. Our
approach's core is a cyclic Attribute-Conditioned Variational Autoencoder
(AC-VAE) model developed to address the challenge of capturing and generating
realistic 3D human body motions from motion capture (MoCap) data. We created a
unique dataset focused on the dynamics of falling movements, characterized by a
new ontology that divides motion into three distinct phases: Impact, Glitch,
and Fall. The ML model's innovation lies in its ability to learn these phases
separately. It is achieved by applying comprehensive data augmentation
techniques and an initial pose loss function to generate natural and plausible
motion. Our web-based interface provides an intuitive platform for artists to
engage with this technology, offering fine-grained control over motion
attributes and interactive visualization tools, including a 360-degree view and
a dynamic timeline for playback manipulation. Our research paves the way for a
future where technology amplifies the creative potential of human expression,
making sophisticated motion generation accessible to a wider artistic
community.
\\ ( https://arxiv.org/abs/2404.00054 ,  5865kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00056 (*cross-listing*)
Date: Tue, 26 Mar 2024 17:24:28 GMT   (2574kb,D)

Title: Fingerprinting web servers through Transformer-encoded HTTP response
  headers
Authors: Patrick Darwinkel
Categories: cs.CR cs.LG cs.NI
Comments: Based on a bachelor's thesis. Submission to arXiv approved by
  supervisor
ACM-class: I.2.6
\\
  We explored leveraging state-of-the-art deep learning, big data, and natural
language processing to enhance the detection of vulnerable web server versions.
Focusing on improving accuracy and specificity over rule-based systems, we
conducted experiments by sending various ambiguous and non-standard HTTP
requests to 4.77 million domains and capturing HTTP response status lines. We
represented these status lines through training a BPE tokenizer and RoBERTa
encoder for unsupervised masked language modeling. We then dimensionality
reduced and concatenated encoded response lines to represent each domain's web
server. A Random Forest and multilayer perceptron (MLP) classified these web
servers, and achieved 0.94 and 0.96 macro F1-score, respectively, on detecting
the five most popular origin web servers. The MLP achieved a weighted F1-score
of 0.55 on classifying 347 major type and minor version pairs. Analysis
indicates that our test cases are meaningful discriminants of web server types.
Our approach demonstrates promise as a powerful and flexible alternative to
rule-based systems.
\\ ( https://arxiv.org/abs/2404.00056 ,  2574kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00068 (*cross-listing*)
Date: Thu, 28 Mar 2024 09:41:24 GMT   (1442kb,D)

Title: A Data-Driven Predictive Analysis on Cyber Security Threats with Key
  Risk Factors
Authors: Fatama Tuz Johora (1), Md Shahedul Islam Khan (2), Esrath Kanon (1),
  Mohammad Abu Tareq Rony (3), Md Zubair (4) and (5) Iqbal H. Sarker ((1)
  Department of Computer Science and Engineering, University of Chittagong,
  Chattogram, Bangladesh, (2) Department of School of Electronics and
  Information, Northwestern Polytechnical University, Xi'an, Shaanxi, China (3)
  Department of Statistics, Noakhali Science and Technology University,
  Noakhali, Bangladesh (4) Department of Computer Science and Engineering,
  Chittagong University of Engineering & Technology, Chattogram, Bangladesh (5)
  Centre for Securing Digital Futures, Edith Cowan University, Perth, WA,
  Australia)
Categories: cs.CR cs.LG
Comments: The paper contains 15 pages, 7 tables and 6 figures
\\
  Cyber risk refers to the risk of defacing reputation, monetary losses, or
disruption of an organization or individuals, and this situation usually occurs
by the unconscious use of cyber systems. The cyber risk is unhurriedly
increasing day by day and it is right now a global threat. Developing countries
like Bangladesh face major cyber risk challenges. The growing cyber threat
worldwide focuses on the need for effective modeling to predict and manage the
associated risk. This paper exhibits a Machine Learning(ML) based model for
predicting individuals who may be victims of cyber attacks by analyzing
socioeconomic factors. We collected the dataset from victims and non-victims of
cyberattacks based on socio-demographic features. The study involved the
development of a questionnaire to gather data, which was then used to measure
the significance of features. Through data augmentation, the dataset was
expanded to encompass 3286 entries, setting the stage for our investigation and
modeling. Among several ML models with 19, 20, 21, and 26 features, we proposed
a novel Pertinent Features Random Forest (RF) model, which achieved maximum
accuracy with 20 features (95.95\%) and also demonstrated the association among
the selected features using the Apriori algorithm with Confidence (above 80\%)
according to the victim. We generated 10 important association rules and
presented the framework that is rigorously evaluated on real-world datasets,
demonstrating its potential to predict cyberattacks and associated risk factors
effectively. Looking ahead, future efforts will be directed toward refining the
predictive model's precision and delving into additional risk factors, to
fortify the proposed framework's efficacy in navigating the complex terrain of
cybersecurity threats.
\\ ( https://arxiv.org/abs/2404.00068 ,  1442kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00082 (*cross-listing*)
Date: Fri, 29 Mar 2024 10:48:32 GMT   (3066kb,D)

Title: Data-Driven Room Acoustic Modeling Via Differentiable Feedback Delay
  Networks With Learnable Delay Lines
Authors: Alessandro Ilic Mezza, Riccardo Giampiccolo, Enzo De Sena, Alberto
  Bernardini
Categories: eess.AS cs.LG cs.SD
Comments: The article has been submitted to EURASIP Journal on Audio, Speech,
  and Music Processing on Jan 02, 2024 and is currently under review
\\
  Over the past few decades, extensive research has been devoted to the design
of artificial reverberation algorithms aimed at emulating the room acoustics of
physical environments. Despite significant advancements, automatic parameter
tuning of delay-network models remains an open challenge. We introduce a novel
method for finding the parameters of a Feedback Delay Network (FDN) such that
its output renders the perceptual qualities of a measured room impulse
response. The proposed approach involves the implementation of a differentiable
FDN with trainable delay lines, which, for the first time, allows us to
simultaneously learn each and every delay-network parameter via
backpropagation. The iterative optimization process seeks to minimize a
time-domain loss function incorporating differentiable terms accounting for
energy decay and echo density. Through experimental validation, we show that
the proposed method yields time-invariant frequency-independent FDNs capable of
closely matching the desired acoustical characteristics, and outperforms
existing methods based on genetic algorithms and analytical filter design.
\\ ( https://arxiv.org/abs/2404.00082 ,  3066kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00130 (*cross-listing*)
Date: Fri, 29 Mar 2024 19:51:34 GMT   (16235kb,D)

Title: FISBe: A real-world benchmark dataset for instance segmentation of
  long-range thin filamentous structures
Authors: Lisa Mais, Peter Hirsch, Claire Managan, Ramya Kandarpa, Josef Lorenz
  Rumberger, Annika Reinke, Lena Maier-Hein, Gudrun Ihrke, Dagmar Kainmueller
Categories: cs.CV cs.LG
Comments: CVPR2024, Project page: https://kainmueller-lab.github.io/fisbe
\\
  Instance segmentation of neurons in volumetric light microscopy images of
nervous systems enables groundbreaking research in neuroscience by facilitating
joint functional and morphological analyses of neural circuits at cellular
resolution. Yet said multi-neuron light microscopy data exhibits extremely
challenging properties for the task of instance segmentation: Individual
neurons have long-ranging, thin filamentous and widely branching morphologies,
multiple neurons are tightly inter-weaved, and partial volume effects, uneven
illumination and noise inherent to light microscopy severely impede local
disentangling as well as long-range tracing of individual neurons. These
properties reflect a current key challenge in machine learning research, namely
to effectively capture long-range dependencies in the data. While respective
methodological research is buzzing, to date methods are typically benchmarked
on synthetic datasets. To address this gap, we release the FlyLight Instance
Segmentation Benchmark (FISBe) dataset, the first publicly available
multi-neuron light microscopy dataset with pixel-wise annotations. In addition,
we define a set of instance segmentation metrics for benchmarking that we
designed to be meaningful with regard to downstream analyses. Lastly, we
provide three baselines to kick off a competition that we envision to both
advance the field of machine learning regarding methodology for capturing
long-range data dependencies, and facilitate scientific discovery in basic
neuroscience.
\\ ( https://arxiv.org/abs/2404.00130 ,  16235kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00145 (*cross-listing*)
Date: Fri, 29 Mar 2024 20:36:58 GMT   (343kb,D)

Title: Verifying the Selected Completely at Random Assumption in
  Positive-Unlabeled Learning
Authors: Pawe{\l} Teisseyre, Konrad Furma\'nczyk, Jan Mielniczuk
Categories: stat.ML cs.LG
\\
  The goal of positive-unlabeled (PU) learning is to train a binary classifier
on the basis of training data containing positive and unlabeled instances,
where unlabeled observations can belong either to the positive class or to the
negative class. Modeling PU data requires certain assumptions on the labeling
mechanism that describes which positive observations are assigned a label. The
simplest assumption, considered in early works, is SCAR (Selected Completely at
Random Assumption), according to which the propensity score function, defined
as the probability of assigning a label to a positive observation, is constant.
On the other hand, a much more realistic assumption is SAR (Selected at
Random), which states that the propensity function solely depends on the
observed feature vector. SCAR-based algorithms are much simpler and
computationally much faster compared to SAR-based algorithms, which usually
require challenging estimation of the propensity score. In this work, we
propose a relatively simple and computationally fast test that can be used to
determine whether the observed data meet the SCAR assumption. Our test is based
on generating artificial labels conforming to the SCAR case, which in turn
allows to mimic the distribution of the test statistic under the null
hypothesis of SCAR. We justify our method theoretically. In experiments, we
demonstrate that the test successfully detects various deviations from SCAR
scenario and at the same time it is possible to effectively control the type I
error. The proposed test can be recommended as a pre-processing step to decide
which final PU algorithm to choose in cases when nature of labeling mechanism
is not known.
\\ ( https://arxiv.org/abs/2404.00145 ,  343kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00158 (*cross-listing*)
Date: Fri, 29 Mar 2024 21:12:25 GMT   (44kb)

Title: Fully Zeroth-Order Bilevel Programming via Gaussian Smoothing
Authors: Alireza Aghasi, Saeed Ghadimi
Categories: math.OC cs.LG
\\
  In this paper, we study and analyze zeroth-order stochastic approximation
algorithms for solving bilvel problems, when neither the upper/lower objective
values, nor their unbiased gradient estimates are available. In particular,
exploiting Stein's identity, we first use Gaussian smoothing to estimate first-
and second-order partial derivatives of functions with two independent block of
variables. We then used these estimates in the framework of a stochastic
approximation algorithm for solving bilevel optimization problems and establish
its non-asymptotic convergence analysis. To the best of our knowledge, this is
the first time that sample complexity bounds are established for a fully
stochastic zeroth-order bilevel optimization algorithm.
\\ ( https://arxiv.org/abs/2404.00158 ,  44kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00179 (*cross-listing*)
Date: Fri, 29 Mar 2024 22:24:12 GMT   (7517kb,D)

Title: Multi-Region Transfer Learning for Segmentation of Crop Field Boundaries
  in Satellite Images with Limited Labels
Authors: Hannah Kerner, Saketh Sundar, Mathan Satish
Categories: cs.CV cs.LG
Comments: Accepted for 2023 AAAI Workshop on AI to Accelerate Science and
  Engineering
\\
  The goal of field boundary delineation is to predict the polygonal boundaries
and interiors of individual crop fields in overhead remotely sensed images
(e.g., from satellites or drones). Automatic delineation of field boundaries is
a necessary task for many real-world use cases in agriculture, such as
estimating cultivated area in a region or predicting end-of-season yield in a
field. Field boundary delineation can be framed as an instance segmentation
problem, but presents unique research challenges compared to traditional
computer vision datasets used for instance segmentation. The practical
applicability of previous work is also limited by the assumption that a
sufficiently-large labeled dataset is available where field boundary
delineation models will be applied, which is not the reality for most regions
(especially under-resourced regions such as Sub-Saharan Africa). We present an
approach for segmentation of crop field boundaries in satellite images in
regions lacking labeled data that uses multi-region transfer learning to adapt
model weights for the target region. We show that our approach outperforms
existing methods and that multi-region transfer learning substantially boosts
performance for multiple model architectures. Our implementation and datasets
are publicly available to enable use of the approach by end-users and serve as
a benchmark for future work.
\\ ( https://arxiv.org/abs/2404.00179 ,  7517kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00204 (*cross-listing*)
Date: Sat, 30 Mar 2024 00:46:43 GMT   (20806kb)

Title: A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust
  Autonomous Flights
Authors: Junyang Zhang, Cristian Emanuel Ocampo Rivera, Kyle Tyni, Steven
  Nguyen
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: 9 pages, 12 figures
\\
  This project aims to revolutionize drone flight control by implementing a
nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for
traditional linear Proportional Integral Derivative (PID) controllers. The
primary objective is to seamlessly transition drones between manual and
autonomous modes, enhancing responsiveness and stability. We utilize the
Proximal Policy Optimization (PPO) reinforcement learning strategy within the
Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking
system offers <1mm positioning accuracy, which significantly improves
autonomous flight precision. To navigate the drone in the shortest
collision-free trajectory, we also build a 3 dimensional A* path planner and
implement it into the real flight successfully.
\\ ( https://arxiv.org/abs/2404.00204 ,  20806kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00218 (*cross-listing*)
Date: Sat, 30 Mar 2024 02:23:01 GMT   (7248kb,D)

Title: Functional-Edged Network Modeling
Authors: Haijie Xu and Chen Zhang
Categories: stat.ML cs.LG
\\
  Contrasts with existing works which all consider nodes as functions and use
edges to represent the relationships between different functions. We target at
network modeling whose edges are functional data and transform the adjacency
matrix into a functional adjacency tensor, introducing an additional dimension
dedicated to function representation. Tucker functional decomposition is used
for the functional adjacency tensor, and to further consider the community
between nodes, we regularize the basis matrices to be symmetrical. Furthermore,
to deal with irregular observations of the functional edges, we conduct model
inference to solve a tensor completion problem. It is optimized by a Riemann
conjugate gradient descent method. Besides these, we also derive several
theorems to show the desirable properties of the functional edged network
model. Finally, we evaluate the efficacy of our proposed model using simulation
data and real metro system data from Hong Kong and Singapore.
\\ ( https://arxiv.org/abs/2404.00218 ,  7248kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00220 (*cross-listing*)
Date: Sat, 30 Mar 2024 02:32:53 GMT   (9304kb,D)

Title: Partially-Observable Sequential Change-Point Detection for
  Autocorrelated Data via Upper Confidence Region
Authors: Haijie Xu, Xiaochen Xian, Chen Zhang, Kaibo Liu
Categories: stat.ML cs.LG
\\
  Sequential change point detection for multivariate autocorrelated data is a
very common problem in practice. However, when the sensing resources are
limited, only a subset of variables from the multivariate system can be
observed at each sensing time point. This raises the problem of partially
observable multi-sensor sequential change point detection. For it, we propose a
detection scheme called adaptive upper confidence region with state space model
(AUCRSS). It models multivariate time series via a state space model (SSM), and
uses an adaptive sampling policy for efficient change point detection and
localization. A partially-observable Kalman filter algorithm is developed for
online inference of SSM, and accordingly, a change point detection scheme based
on a generalized likelihood ratio test is developed. How its detection power
relates to the adaptive sampling strategy is analyzed. Meanwhile, by treating
the detection power as a reward, its connection with the online combinatorial
multi-armed bandit (CMAB) problem is formulated and an adaptive upper
confidence region algorithm is proposed for adaptive sampling policy design.
Theoretical analysis of the asymptotic average detection delay is performed,
and thorough numerical studies with synthetic data and real-world data are
conducted to demonstrate the effectiveness of our method.
\\ ( https://arxiv.org/abs/2404.00220 ,  9304kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00232 (*cross-listing*)
Date: Sat, 30 Mar 2024 03:26:51 GMT   (367kb,D)

Title: Efficient Automatic Tuning for Data-driven Model Predictive Control via
  Meta-Learning
Authors: Baoyu Li, William Edwards, Kris Hauser
Categories: cs.RO cs.LG
Comments: ICRA 2023 Workshop on Effective Representations, Abstractions, and
  Priors for Robot Learning (RAP4Robots)
\\
  AutoMPC is a Python package that automates and optimizes data-driven model
predictive control. However, it can be computationally expensive and unstable
when exploring large search spaces using pure Bayesian Optimization (BO). To
address these issues, this paper proposes to employ a meta-learning approach
called Portfolio that improves AutoMPC's efficiency and stability by
warmstarting BO. Portfolio optimizes initial designs for BO using a diverse set
of configurations from previous tasks and stabilizes the tuning process by
fixing initial configurations instead of selecting them randomly. Experimental
results demonstrate that Portfolio outperforms the pure BO in finding desirable
solutions for AutoMPC within limited computational resources on 11 nonlinear
control simulation benchmarks and 1 physical underwater soft robot dataset.
\\ ( https://arxiv.org/abs/2404.00232 ,  367kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00235 (*cross-listing*)
Date: Sat, 30 Mar 2024 03:52:58 GMT   (9782kb)

Title: Information Security and Privacy in the Digital World: Some Selected
  Topics
Authors: Jaydip Sen, Joceli Mayer, Subhasis Dasgupta, Subrata Nandi, Srinivasan
  Krishnaswamy, Pinaki Mitra, Mahendra Pratap Singh, Naga Prasanthi Kundeti,
  Chandra Sekhara Rao MVP, Sudha Sree Chekuri, Seshu Babu Pallapothu, Preethi
  Nanjundan, Jossy P. George, Abdelhadi El Allahi, Ilham Morino, Salma AIT
  Oussous, Siham Beloualid, Ahmed Tamtaoui, and Abderrahim Bajit
Categories: cs.CR cs.LG
Comments: Published by IntechOpen, London Uk in Nov 2023, the book contains 8
  chapters spanning over 131 pages. arXiv admin note: text overlap with
  arXiv:2307.02055, arXiv:2304.00258
DOI: 10.5772/intechopen.100776
\\
  In the era of generative artificial intelligence and the Internet of Things,
while there is explosive growth in the volume of data and the associated need
for processing, analysis, and storage, several new challenges are faced in
identifying spurious and fake information and protecting the privacy of
sensitive data. This has led to an increasing demand for more robust and
resilient schemes for authentication, integrity protection, encryption,
non-repudiation, and privacy-preservation of data. The chapters in this book
present some of the state-of-the-art research works in the field of
cryptography and security in computing and communications.
\\ ( https://arxiv.org/abs/2404.00235 ,  9782kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00323 (*cross-listing*)
Date: Sat, 30 Mar 2024 11:28:05 GMT   (2120kb,D)

Title: CLIP-driven Outliers Synthesis for few-shot OOD detection
Authors: Hao Sun, Rundong He, Zhongyi Han, Zhicong Lin, Yongshun Gong, Yilong
  Yin
Categories: cs.CV cs.LG
Comments: 9 pages,5 figures
\\
  Few-shot OOD detection focuses on recognizing out-of-distribution (OOD)
images that belong to classes unseen during training, with the use of only a
small number of labeled in-distribution (ID) images. Up to now, a mainstream
strategy is based on large-scale vision-language models, such as CLIP. However,
these methods overlook a crucial issue: the lack of reliable OOD supervision
information, which can lead to biased boundaries between in-distribution (ID)
and OOD. To tackle this problem, we propose CLIP-driven Outliers
Synthesis~(CLIP-OS). Firstly, CLIP-OS enhances patch-level features' perception
by newly proposed patch uniform convolution, and adaptively obtains the
proportion of ID-relevant information by employing CLIP-surgery-discrepancy,
thus achieving separation between ID-relevant and ID-irrelevant. Next, CLIP-OS
synthesizes reliable OOD data by mixing up ID-relevant features from different
classes to provide OOD supervision information. Afterward, CLIP-OS leverages
synthetic OOD samples by unknown-aware prompt learning to enhance the
separability of ID and OOD. Extensive experiments across multiple benchmarks
demonstrate that CLIP-OS achieves superior few-shot OOD detection capability.
\\ ( https://arxiv.org/abs/2404.00323 ,  2120kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00327 (*cross-listing*)
Date: Sat, 30 Mar 2024 11:41:19 GMT   (4434kb,D)

Title: YNetr: Dual-Encoder architecture on Plain Scan Liver Tumors (PSLT)
Authors: Wen Sheng, Zhong Zheng, Jiajun Liu, Han Lu, Hanyuan Zhang, Zhengyong
  Jiang, Zhihong Zhang and Daoping Zhu
Categories: eess.IV cs.CV cs.LG
Comments: 15 pages
\\
  Background: Liver tumors are abnormal growths in the liver that can be either
benign or malignant, with liver cancer being a significant health concern
worldwide. However, there is no dataset for plain scan segmentation of liver
tumors, nor any related algorithms. To fill this gap, we propose Plain Scan
Liver Tumors(PSLT) and YNetr. Methods: A collection of 40 liver tumor plain
scan segmentation datasets was assembled and annotated. Concurrently, we
utilized Dice coefficient as the metric for assessing the segmentation outcomes
produced by YNetr, having advantage of capturing different frequency
information. Results: The YNetr model achieved a Dice coefficient of 62.63% on
the PSLT dataset, surpassing the other publicly available model by an accuracy
margin of 1.22%. Comparative evaluations were conducted against a range of
models including UNet 3+, XNet, UNetr, Swin UNetr, Trans-BTS, COTr, nnUNetv2
(2D), nnUNetv2 (3D fullres), MedNext (2D) and MedNext(3D fullres). Conclusions:
We not only proposed a dataset named PSLT(Plain Scan Liver Tumors), but also
explored a structure called YNetr that utilizes wavelet transform to extract
different frequency information, which having the SOTA in PSLT by experiments.
\\ ( https://arxiv.org/abs/2404.00327 ,  4434kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00390 (*cross-listing*)
Date: Sat, 30 Mar 2024 15:03:52 GMT   (29812kb,D)

Title: Learning truly monotone operators with applications to nonlinear inverse
  problems
Authors: Younes Belkouchi, Jean-Christophe Pesquet, Audrey Repetti and Hugues
  Talbot
Categories: math.OC cs.LG cs.NA math.NA
\\
  This article introduces a novel approach to learning monotone neural networks
through a newly defined penalization loss. The proposed method is particularly
effective in solving classes of variational problems, specifically monotone
inclusion problems, commonly encountered in image processing tasks. The
Forward-Backward-Forward (FBF) algorithm is employed to address these problems,
offering a solution even when the Lipschitz constant of the neural network is
unknown. Notably, the FBF algorithm provides convergence guarantees under the
condition that the learned operator is monotone. Building on plug-and-play
methodologies, our objective is to apply these newly learned operators to
solving non-linear inverse problems. To achieve this, we initially formulate
the problem as a variational inclusion problem. Subsequently, we train a
monotone neural network to approximate an operator that may not inherently be
monotone. Leveraging the FBF algorithm, we then show simulation examples where
the non-linear inverse problem is successfully solved.
\\ ( https://arxiv.org/abs/2404.00390 ,  29812kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00411 (*cross-listing*)
Date: Sat, 30 Mar 2024 16:41:24 GMT   (47254kb,D)

Title: Aardvark Weather: end-to-end data-driven weather forecasting
Authors: Anna Vaughan, Stratis Markou, Will Tebbutt, James Requeima, Wessel P.
  Bruinsma, Tom R. Andersson, Michael Herzog, Nicholas D. Lane, J. Scott
  Hosking and Richard E. Turner
Categories: physics.ao-ph cs.LG
\\
  Machine learning is revolutionising medium-range weather prediction. However
it has only been applied to specific and individual components of the weather
prediction pipeline. Consequently these data-driven approaches are unable to be
deployed without input from conventional operational numerical weather
prediction (NWP) systems, which is computationally costly and does not support
end-to-end optimisation. In this work, we take a radically different approach
and replace the entire NWP pipeline with a machine learning model. We present
Aardvark Weather, the first end-to-end data-driven forecasting system which
takes raw observations as input and provides both global and local forecasts.
These global forecasts are produced for 24 variables at multiple pressure
levels at one-degree spatial resolution and 24 hour temporal resolution, and
are skillful with respect to hourly climatology at five to seven day lead
times. Local forecasts are produced for temperature, mean sea level pressure,
and wind speed at a geographically diverse set of weather stations, and are
skillful with respect to an IFS-HRES interpolation baseline at multiple
lead-times. Aardvark, by virtue of its simplicity and scalability, opens the
door to a new paradigm for performing accurate and efficient data-driven
medium-range weather forecasting.
\\ ( https://arxiv.org/abs/2404.00411 ,  47254kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00412 (*cross-listing*)
Date: Sat, 30 Mar 2024 16:43:40 GMT   (171079kb,D)

Title: SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive
  Canvas Layout
Authors: Ayan Banerjee, Nityanand Mathur, Josep Llad\'os, Umapada Pal, Anjan
  Dutta
Categories: cs.CV cs.LG
\\
  Generating VectorArt from text prompts is a challenging vision task,
requiring diverse yet realistic depictions of the seen as well as unseen
entities. However, existing research has been mostly limited to the generation
of single objects, rather than comprehensive scenes comprising multiple
elements. In response, this work introduces SVGCraft, a novel end-to-end
framework for the creation of vector graphics depicting entire scenes from
textual descriptions. Utilizing a pre-trained LLM for layout generation from
text prompts, this framework introduces a technique for producing masked
latents in specified bounding boxes for accurate object placement. It
introduces a fusion mechanism for integrating attention maps and employs a
diffusion U-Net for coherent composition, speeding up the drawing process. The
resulting SVG is optimized using a pre-trained encoder and LPIPS loss with
opacity modulation to maximize similarity. Additionally, this work explores the
potential of primitive shapes in facilitating canvas completion in constrained
environments. Through both qualitative and quantitative assessments, SVGCraft
is demonstrated to surpass prior works in abstraction, recognizability, and
detail, as evidenced by its performance metrics (CLIP-T: 0.4563, Cosine
Similarity: 0.6342, Confusion: 0.66, Aesthetic: 6.7832). The code will be
available at https://github.com/ayanban011/SVGCraft.
\\ ( https://arxiv.org/abs/2404.00412 ,  171079kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00420 (*cross-listing*)
Date: Sat, 30 Mar 2024 16:58:42 GMT   (724kb,D)

Title: Learning Service Selection Decision Making Behaviors During Scientific
  Workflow Development
Authors: Xihao Xie and Jia Zhang and Rahul Ramachandran and Tsengdar J. Lee and
  Seungwon Lee
Categories: cs.SE cs.LG
Comments: 14 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:2205.11771
\\
  Increasingly, more software services have been published onto the Internet,
making it a big challenge to recommend services in the process of a scientific
workflow composition. In this paper, a novel context-aware approach is proposed
to recommending next services in a workflow development process, through
learning service representation and service selection decision making behaviors
from workflow provenance. Inspired by natural language sentence generation, the
composition process of a scientific workflow is formalized as a step-wise
procedure within the context of the goal of workflow, and the problem of next
service recommendation is mapped to next word prediction. Historical service
dependencies are first extracted from scientific workflow provenance to build a
knowledge graph. Service sequences are then generated based on diverse
composition path generation strategies. Afterwards, the generated corpus of
composition paths are leveraged to study previous decision making strategies.
Such a trained goal-oriented next service prediction model will be used to
recommend top K candidate services during workflow composition process.
Extensive experiments on a real-word repository have demonstrated the
effectiveness of this approach.
\\ ( https://arxiv.org/abs/2404.00420 ,  724kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00431 (*cross-listing*)
Date: Sat, 30 Mar 2024 17:32:26 GMT   (17985kb,D)

Title: Visualizing Routes with AI-Discovered Street-View Patterns
Authors: Tsung Heng Wu, Md Amiruzzaman, Ye Zhao, Deepshikha Bhati, and Jing
  Yang
Categories: cs.HC cs.LG
Comments: 12 pages, 10 figures, and 3 tables
\\
  Street-level visual appearances play an important role in studying social
systems, such as understanding the built environment, driving routes, and
associated social and economic factors. It has not been integrated into a
typical geographical visualization interface (e.g., map services) for planning
driving routes. In this paper, we study this new visualization task with
several new contributions. First, we experiment with a set of AI techniques and
propose a solution of using semantic latent vectors for quantifying visual
appearance features. Second, we calculate image similarities among a large set
of street-view images and then discover spatial imagery patterns. Third, we
integrate these discovered patterns into driving route planners with new
visualization techniques. Finally, we present VivaRoutes, an interactive
visualization prototype, to show how visualizations leveraged with these
discovered patterns can help users effectively and interactively explore
multiple routes. Furthermore, we conducted a user study to assess the
usefulness and utility of VivaRoutes.
\\ ( https://arxiv.org/abs/2404.00431 ,  17985kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00470 (*cross-listing*)
Date: Sat, 30 Mar 2024 20:32:35 GMT   (2386kb)

Title: Classification of Short Segment Pediatric Heart Sounds Based on a
  Transformer-Based Convolutional Neural Network
Authors: Md Hassanuzzaman, Nurul Akhtar Hasan, Mohammad Abdullah Al Mamun,
  Khawza I Ahmed, Ahsan H Khandoker, Raqibul Mostafa
Categories: cs.SD cs.LG eess.AS
Comments: 16 pages,11 Figures
\\
  Congenital anomalies arising as a result of a defect in the structure of the
heart and great vessels are known as congenital heart diseases or CHDs. A PCG
can provide essential details about the mechanical conduction system of the
heart and point out specific patterns linked to different kinds of CHD. This
study aims to investigate the minimum signal duration required for the
automatic classification of heart sounds. This study also investigated the
optimum signal quality assessment indicator (Root Mean Square of Successive
Differences) RMSSD and (Zero Crossings Rate) ZCR value. Mel-frequency cepstral
coefficients (MFCCs) based feature is used as an input to build a
Transformer-Based residual one-dimensional convolutional neural network, which
is then used for classifying the heart sound. The study showed that 0.4 is the
ideal threshold for getting suitable signals for the RMSSD and ZCR indicators.
Moreover, a minimum signal length of 5s is required for effective heart sound
classification. It also shows that a shorter signal (3 s heart sound) does not
have enough information to categorize heart sounds accurately, and the longer
signal (15 s heart sound) may contain more noise. The best accuracy, 93.69%, is
obtained for the 5s signal to distinguish the heart sound.
\\ ( https://arxiv.org/abs/2404.00470 ,  2386kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00471 (*cross-listing*)
Date: Sat, 30 Mar 2024 20:34:49 GMT   (2002kb,D)

Title: Score-Based Diffusion Models for Photoacoustic Tomography Image
  Reconstruction
Authors: Sreemanti Dey, Snigdha Saha, Berthy T. Feng, Manxiu Cui, Laure
  Delisle, Oscar Leong, Lihong V. Wang, Katherine L. Bouman
Categories: physics.med-ph cs.CV cs.LG eess.IV
Comments: 5 pages
Journal-ref: ICASSP 2024 - 2024 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP), Seoul, Korea, Republic of, 2024, pp.
  2470-2474
DOI: 10.1109/ICASSP48485.2024.10447579
\\
  Photoacoustic tomography (PAT) is a rapidly-evolving medical imaging modality
that combines optical absorption contrast with ultrasound imaging depth. One
challenge in PAT is image reconstruction with inadequate acoustic signals due
to limited sensor coverage or due to the density of the transducer array. Such
cases call for solving an ill-posed inverse reconstruction problem. In this
work, we use score-based diffusion models to solve the inverse problem of
reconstructing an image from limited PAT measurements. The proposed approach
allows us to incorporate an expressive prior learned by a diffusion model on
simulated vessel structures while still being robust to varying transducer
sparsity conditions.
\\ ( https://arxiv.org/abs/2404.00471 ,  2002kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00473 (*cross-listing*)
Date: Sat, 30 Mar 2024 20:43:53 GMT   (3414kb,D)

Title: Privacy Backdoors: Stealing Data with Corrupted Pretrained Models
Authors: Shanglun Feng and Florian Tram\`er
Categories: cs.CR cs.LG
Comments: Code at https://github.com/ShanglunFengatETHZ/PrivacyBackdoor
\\
  Practitioners commonly download pretrained machine learning models from open
repositories and finetune them to fit specific applications. We show that this
practice introduces a new risk of privacy backdoors. By tampering with a
pretrained model's weights, an attacker can fully compromise the privacy of the
finetuning data. We show how to build privacy backdoors for a variety of
models, including transformers, which enable an attacker to reconstruct
individual finetuning samples, with a guaranteed success! We further show that
backdoored models allow for tight privacy attacks on models trained with
differential privacy (DP). The common optimistic practice of training DP models
with loose privacy guarantees is thus insecure if the model is not trusted.
Overall, our work highlights a crucial and overlooked supply chain attack on
machine learning privacy.
\\ ( https://arxiv.org/abs/2404.00473 ,  3414kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00481 (*cross-listing*)
Date: Sat, 30 Mar 2024 22:20:02 GMT   (596kb,D)

Title: Convolutional Bayesian Filtering
Authors: Wenhan Cao, Shiqi Liu, Chang Liu, Zeyu He, Stephen S.-T. Yau, Shengbo
  Eben Li
Categories: stat.ML cs.LG cs.SY eess.SY
\\
  Bayesian filtering serves as the mainstream framework of state estimation in
dynamic systems. Its standard version utilizes total probability rule and
Bayes' law alternatively, where how to define and compute conditional
probability is critical to state distribution inference. Previously, the
conditional probability is assumed to be exactly known, which represents a
measure of the occurrence probability of one event, given the second event. In
this paper, we find that by adding an additional event that stipulates an
inequality condition, we can transform the conditional probability into a
special integration that is analogous to convolution. Based on this
transformation, we show that both transition probability and output probability
can be generalized to convolutional forms, resulting in a more general
filtering framework that we call convolutional Bayesian filtering. This new
framework encompasses standard Bayesian filtering as a special case when the
distance metric of the inequality condition is selected as Dirac delta
function. It also allows for a more nuanced consideration of model mismatch by
choosing different types of inequality conditions. For instance, when the
distance metric is defined in a distributional sense, the transition
probability and output probability can be approximated by simply rescaling them
into fractional powers. Under this framework, a robust version of Kalman filter
can be constructed by only altering the noise covariance matrix, while
maintaining the conjugate nature of Gaussian distributions. Finally, we
exemplify the effectiveness of our approach by reshaping classic filtering
algorithms into convolutional versions, including Kalman filter, extended
Kalman filter, unscented Kalman filter and particle filter.
\\ ( https://arxiv.org/abs/2404.00481 ,  596kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00529 (*cross-listing*)
Date: Sun, 31 Mar 2024 02:03:35 GMT   (886kb,D)

Title: Super Non-singular Decompositions of Polynomials and their Application
  to Robustly Learning Low-degree PTFs
Authors: Ilias Diakonikolas and Daniel M. Kane and Vasilis Kontonis and Sihan
  Liu and Nikos Zarifis
Categories: cs.DS cs.LG
Comments: To appear in STOC2024
\\
  We study the efficient learnability of low-degree polynomial threshold
functions (PTFs) in the presence of a constant fraction of adversarial
corruptions. Our main algorithmic result is a polynomial-time PAC learning
algorithm for this concept class in the strong contamination model under the
Gaussian distribution with error guarantee $O_{d, c}(\text{opt}^{1-c})$, for
any desired constant $c>0$, where $\text{opt}$ is the fraction of corruptions.
In the strong contamination model, an omniscient adversary can arbitrarily
corrupt an $\text{opt}$-fraction of the data points and their labels. This
model generalizes the malicious noise model and the adversarial label noise
model. Prior to our work, known polynomial-time algorithms in this corruption
model (or even in the weaker adversarial label noise model) achieved error
$\tilde{O}_d(\text{opt}^{1/(d+1)})$, which deteriorates significantly as a
function of the degree $d$.
  Our algorithm employs an iterative approach inspired by localization
techniques previously used in the context of learning linear threshold
functions. Specifically, we use a robust perceptron algorithm to compute a good
partial classifier and then iterate on the unclassified points. In order to
achieve this, we need to take a set defined by a number of polynomial
inequalities and partition it into several well-behaved subsets. To this end,
we develop new polynomial decomposition techniques that may be of independent
interest.
\\ ( https://arxiv.org/abs/2404.00529 ,  886kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00545 (*cross-listing*)
Date: Sun, 31 Mar 2024 03:23:29 GMT   (8932kb,D)

Title: Unified, Verifiable Neural Simulators for Electromagnetic Wave Inverse
  Problems
Authors: Charles Dove, Jatearoon Boondicharern, Laura Waller
Categories: physics.optics cs.LG eess.IV
\\
  Simulators based on neural networks offer a path to orders-of-magnitude
faster electromagnetic wave simulations. Existing models, however, only address
narrowly tailored classes of problems and only scale to systems of a few dozen
degrees of freedom (DoFs). Here, we demonstrate a single, unified model capable
of addressing scattering simulations with thousands of DoFs, of any wavelength,
any illumination wavefront, and freeform materials, within broad configurable
bounds. Based on an attentional multi-conditioning strategy, our method also
allows non-recurrent supervision on and prediction of intermediate physical
states, which provides improved generalization with no additional
data-generation cost. Using this O(1)-time intermediate prediction capability,
we propose and prove a rigorous, efficiently computable upper bound on
prediction error, allowing accuracy guarantees at inference time for all
predictions. After training solely on randomized systems, we demonstrate the
unified model across a suite of challenging multi-disciplinary inverse
problems, finding strong efficacy and speed improvements up to 96% for problems
in optical tomography, beam shaping through volumetric random media, and
freeform photonic inverse design, with no problem-specific training. Our
findings demonstrate a path to universal, verifiably accurate neural surrogates
for existing scattering simulators, and our conditioning and training methods
are directly applicable to any PDE admitting a time-domain iterative solver.
\\ ( https://arxiv.org/abs/2404.00545 ,  8932kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00551 (*cross-listing*)
Date: Sun, 31 Mar 2024 03:39:04 GMT   (269kb,D)

Title: Convergence of Continuous Normalizing Flows for Learning Probability
  Distributions
Authors: Yuan Gao, Jian Huang, Yuling Jiao, and Shurong Zheng
Categories: stat.ML cs.LG
Comments: 60 pages, 3 tables, and 3 figures
MSC-class: 62G05, 68T07
\\
  Continuous normalizing flows (CNFs) are a generative method for learning
probability distributions, which is based on ordinary differential equations.
This method has shown remarkable empirical success across various applications,
including large-scale image synthesis, protein structure prediction, and
molecule generation. In this work, we study the theoretical properties of CNFs
with linear interpolation in learning probability distributions from a finite
random sample, using a flow matching objective function. We establish
non-asymptotic error bounds for the distribution estimator based on CNFs, in
terms of the Wasserstein-2 distance. The key assumption in our analysis is that
the target distribution satisfies one of the following three conditions: it
either has a bounded support, is strongly log-concave, or is a finite or
infinite mixture of Gaussian distributions. We present a convergence analysis
framework that encompasses the error due to velocity estimation, the
discretization error, and the early stopping error. A key step in our analysis
involves establishing the regularity properties of the velocity field and its
estimator for CNFs constructed with linear interpolation. This necessitates the
development of uniform error bounds with Lipschitz regularity control of deep
ReLU networks that approximate the Lipschitz function class, which could be of
independent interest. Our nonparametric convergence analysis offers theoretical
guarantees for using CNFs to learn probability distributions from a finite
random sample.
\\ ( https://arxiv.org/abs/2404.00551 ,  269kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00639 (*cross-listing*)
Date: Sun, 31 Mar 2024 10:43:33 GMT   (2692kb,D)

Title: RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning
Authors: Dongsheng Zuo, Jiadong Zhu, Yikang Ouyang, Yuzhe Ma
Categories: cs.AR cs.LG
Comments: Extension of DAC 2023 version
\\
  Multiplication is a fundamental operation in many applications, and
multipliers are widely adopted in various circuits. However, optimizing
multipliers is challenging and non-trivial due to the huge design space. In
this paper, we propose RL-MUL, a multiplier design optimization framework based
on reinforcement learning. Specifically, we utilize matrix and tensor
representations for the compressor tree of a multiplier, based on which the
convolutional neural networks can be seamlessly incorporated as the agent
network. The agent can learn to optimize the multiplier structure based on a
Pareto-driven reward which is customized to accommodate the trade-off between
area and delay. Additionally, the capability of RL-MUL is extended to optimize
the fused multiply-accumulator (MAC) designs. Experiments are conducted on
different bit widths of multipliers. The results demonstrate that the
multipliers produced by RL-MUL can dominate all baseline designs in terms of
area and delay. The performance gain of RL-MUL is further validated by
comparing the area and delay of processing element arrays using multipliers
from RL-MUL and baseline approaches.
\\ ( https://arxiv.org/abs/2404.00639 ,  2692kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00640 (*cross-listing*)
Date: Sun, 31 Mar 2024 10:47:38 GMT   (31398kb,D)

Title: Face It Yourselves: An LLM-Based Two-Stage Strategy to Localize
  Configuration Errors via Logs
Authors: Shiwen Shan, Yintong Huo, Yuxin Su, Yichen Li, Dan Li and Zibin Zheng
Categories: cs.SE cs.LG
DOI: 10.1145/3650212.3652106
\\
  Configurable software systems are prone to configuration errors, resulting in
significant losses to companies. However, diagnosing these errors is
challenging due to the vast and complex configuration space. These errors pose
significant challenges for both experienced maintainers and new end-users,
particularly those without access to the source code of the software systems.
Given that logs are easily accessible to most end-users, we conduct a
preliminary study to outline the challenges and opportunities of utilizing logs
in localizing configuration errors. Based on the insights gained from the
preliminary study, we propose an LLM-based two-stage strategy for end-users to
localize the root-cause configuration properties based on logs. We further
implement a tool, LogConfigLocalizer, aligned with the design of the
aforementioned strategy, hoping to assist end-users in coping with
configuration errors through log analysis.
  To the best of our knowledge, this is the first work to localize the
root-cause configuration properties for end-users based on Large Language
Models~(LLMs) and logs. We evaluate the proposed strategy on Hadoop by
LogConfigLocalizer and prove its efficiency with an average accuracy as high as
99.91%. Additionally, we also demonstrate the effectiveness and necessity of
different phases of the methodology by comparing it with two other variants and
a baseline tool. Moreover, we validate the proposed methodology through a
practical case study to demonstrate its effectiveness and feasibility.
\\ ( https://arxiv.org/abs/2404.00640 ,  31398kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00696 (*cross-listing*)
Date: Sun, 31 Mar 2024 14:14:00 GMT   (379kb)

Title: Privacy Re-identification Attacks on Tabular GANs
Authors: Abdallah Alshantti, Adil Rasheed, Frank Westad
Categories: cs.CR cs.LG
\\
  Generative models are subject to overfitting and thus may potentially leak
sensitive information from the training data. In this work. we investigate the
privacy risks that can potentially arise from the use of generative adversarial
networks (GANs) for creating tabular synthetic datasets. For the purpose, we
analyse the effects of re-identification attacks on synthetic data, i.e.,
attacks which aim at selecting samples that are predicted to correspond to
memorised training samples based on their proximity to the nearest synthetic
records. We thus consider multiple settings where different attackers might
have different access levels or knowledge of the generative model and
predictive, and assess which information is potentially most useful for
launching more successful re-identification attacks. In doing so we also
consider the situation for which re-identification attacks are formulated as
reconstruction attacks, i.e., the situation where an attacker uses evolutionary
multi-objective optimisation for perturbing synthetic samples closer to the
training space. The results indicate that attackers can indeed pose major
privacy risks by selecting synthetic samples that are likely representative of
memorised training samples. In addition, we notice that privacy threats
considerably increase when the attacker either has knowledge or has black-box
access to the generative models. We also find that reconstruction attacks
through multi-objective optimisation even increase the risk of identifying
confidential samples.
\\ ( https://arxiv.org/abs/2404.00696 ,  379kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00726 (*cross-listing*)
Date: Sun, 31 Mar 2024 15:56:41 GMT   (1402kb,D)

Title: MugenNet: A Novel Combined Convolution Neural Network and Transformer
  Network with its Application for Colonic Polyp Image Segmentation
Authors: Chen Peng, Zhiqin Qian, Kunyu Wang, Qi Luo, Zhuming Bi, Wenjun Zhang
Categories: eess.IV cs.CV cs.LG
\\
  Biomedical image segmentation is a very important part in disease diagnosis.
The term "colonic polyps" refers to polypoid lesions that occur on the surface
of the colonic mucosa within the intestinal lumen. In clinical practice, early
detection of polyps is conducted through colonoscopy examinations and
biomedical image processing. Therefore, the accurate polyp image segmentation
is of great significance in colonoscopy examinations. Convolutional Neural
Network (CNN) is a common automatic segmentation method, but its main
disadvantage is the long training time. Transformer utilizes a self-attention
mechanism, which essentially assigns different importance weights to each piece
of information, thus achieving high computational efficiency during
segmentation. However, a potential drawback is the risk of information loss. In
the study reported in this paper, based on the well-known hybridization
principle, we proposed a method to combine CNN and Transformer to retain the
strengths of both, and we applied this method to build a system called MugenNet
for colonic polyp image segmentation. We conducted a comprehensive experiment
to compare MugenNet with other CNN models on five publicly available datasets.
The ablation experiment on MugentNet was conducted as well. The experimental
results show that MugenNet achieves significantly higher processing speed and
accuracy compared with CNN alone. The generalized implication with our work is
a method to optimally combine two complimentary methods of machine learning.
\\ ( https://arxiv.org/abs/2404.00726 ,  1402kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00729 (*cross-listing*)
Date: Sun, 31 Mar 2024 16:17:59 GMT   (10251kb,D)

Title: Nonparametric End-to-End Probabilistic Forecasting of Distributed
  Generation Outputs Considering Missing Data Imputation
Authors: Minghui Chen, Zichao Meng, Yanping Liu, Longbo Luo, Ye Guo, Kang Wang
Categories: eess.SY cs.LG cs.SY
\\
  In this paper, we introduce a nonparametric end-to-end method for
probabilistic forecasting of distributed renewable generation outputs while
including missing data imputation. Firstly, we employ a nonparametric
probabilistic forecast model utilizing the long short-term memory (LSTM)
network to model the probability distributions of distributed renewable
generations' outputs. Secondly, we design an end-to-end training process that
includes missing data imputation through iterative imputation and iterative
loss-based training procedures. This two-step modeling approach effectively
combines the strengths of the nonparametric method with the end-to-end
approach. Consequently, our approach demonstrates exceptional capabilities in
probabilistic forecasting for the outputs of distributed renewable generations
while effectively handling missing values. Simulation results confirm the
superior performance of our approach compared to existing alternatives.
\\ ( https://arxiv.org/abs/2404.00729 ,  10251kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00751 (*cross-listing*)
Date: Sun, 31 Mar 2024 17:43:37 GMT   (435kb,D)

Title: C-XGBoost: A tree boosting model for causal effect estimation
Authors: Niki Kiriakidou, Ioannis E. Livieris, Christos Diou
Categories: stat.ML cs.LG stat.ME
Comments: This paper has been accepted for presentation at IFIP International
  Conference on Artificial Intelligence Applications and Innovations
\\
  Causal effect estimation aims at estimating the Average Treatment Effect as
well as the Conditional Average Treatment Effect of a treatment to an outcome
from the available data. This knowledge is important in many safety-critical
domains, where it often needs to be extracted from observational data. In this
work, we propose a new causal inference model, named C-XGBoost, for the
prediction of potential outcomes. The motivation of our approach is to exploit
the superiority of tree-based models for handling tabular data together with
the notable property of causal inference neural network-based models to learn
representations that are useful for estimating the outcome for both the
treatment and non-treatment cases. The proposed model also inherits the
considerable advantages of XGBoost model such as efficiently handling features
with missing values requiring minimum preprocessing effort, as well as it is
equipped with regularization techniques to avoid overfitting/bias. Furthermore,
we propose a new loss function for efficiently training the proposed causal
inference model. The experimental analysis, which is based on the performance
profiles of Dolan and Mor{\'e} as well as on post-hoc and non-parametric
statistical tests, provide strong evidence about the effectiveness of the
proposed approach.
\\ ( https://arxiv.org/abs/2404.00751 ,  435kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00785 (*cross-listing*)
Date: Sun, 31 Mar 2024 20:08:23 GMT   (3713kb,D)

Title: Disentangling Hippocampal Shape Variations: A Study of Neurological
  Disorders Using Graph Variational Autoencoder with Contrastive Learning
Authors: Jakaria Rabbi, Johannes Kiechle, Christian Beaulieu, Nilanjan Ray,
  Dana Cobzas
Categories: cs.CV cs.LG q-bio.NC
Comments: Length: 23 pages and submitted to the journal: MELBA (Machine
  Learning for Biomedical Imaging)
\\
  This paper presents a comprehensive study focused on disentangling
hippocampal shape variations from diffusion tensor imaging (DTI) datasets
within the context of neurological disorders. Leveraging a Graph Variational
Autoencoder (VAE) enhanced with Supervised Contrastive Learning, our approach
aims to improve interpretability by disentangling two distinct latent variables
corresponding to age and the presence of diseases. In our ablation study, we
investigate a range of VAE architectures and contrastive loss functions,
showcasing the enhanced disentanglement capabilities of our approach. This
evaluation uses synthetic 3D torus mesh data and real 3D hippocampal mesh
datasets derived from the DTI hippocampal dataset. Our supervised
disentanglement model outperforms several state-of-the-art (SOTA) methods like
attribute and guided VAEs in terms of disentanglement scores. Our model
distinguishes between age groups and disease status in patients with Multiple
Sclerosis (MS) using the hippocampus data. Our Graph VAE with Supervised
Contrastive Learning shows the volume changes of the hippocampus of MS
populations at different ages, and the result is consistent with the current
neuroimaging literature. This research provides valuable insights into the
relationship between neurological disorder and hippocampal shape changes in
different age groups of MS populations using a Graph VAE with Supervised
Contrastive loss.
\\ ( https://arxiv.org/abs/2404.00785 ,  3713kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00797 (*cross-listing*)
Date: Sun, 31 Mar 2024 20:59:58 GMT   (2162kb,D)

Title: Metarobotics for Industry and Society: Vision, Technologies, and
  Opportunities
Authors: Eric Guiffo Kaigom
Categories: cs.RO cs.CY cs.LG cs.SY eess.SY
Comments: Published on IEEE Transactions on Industrial Informatics (early
  access), 2023
Journal-ref: IEEE Transactions on Industrial Informatics, 2023
DOI: 10.1109/TII.2023.3337380
\\
  Metarobotics aims to combine next generation wireless communication,
multi-sense immersion, and collective intelligence to provide a pervasive,
itinerant, and non-invasive access and interaction with distant robotized
applications. Industry and society are expected to benefit from these
functionalities. For instance, robot programmers will no longer travel
worldwide to plan and test robot motions, even collaboratively. Instead, they
will have a personalized access to robots and their environments from anywhere,
thus spending more time with family and friends. Students enrolled in robotics
courses will be taught under authentic industrial conditions in real-time. This
paper describes objectives of Metarobotics in society, industry, and
in-between. It identifies and surveys technologies likely to enable their
completion and provides an architecture to put forward the interplay of key
components of Metarobotics. Potentials for self-determination, self-efficacy,
and work-life-flexibility in robotics-related applications in Society 5.0,
Industry 4.0, and Industry 5.0 are outlined.
\\ ( https://arxiv.org/abs/2404.00797 ,  2162kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00836 (*cross-listing*)
Date: Mon, 1 Apr 2024 00:21:11 GMT   (1229kb)

Title: Rethinking Resource Management in Edge Learning: A Joint Pre-training
  and Fine-tuning Design Paradigm
Authors: Zhonghao Lyu, Yuchen Li, Guangxu Zhu, Jie Xu, H. Vincent Poor,
  Shuguang Cui
Categories: cs.IT cs.DC cs.LG math.IT
\\
  In some applications, edge learning is experiencing a shift in focusing from
conventional learning from scratch to new two-stage learning unifying
pre-training and task-specific fine-tuning. This paper considers the problem of
joint communication and computation resource management in a two-stage edge
learning system. In this system, model pre-training is first conducted at an
edge server via centralized learning on local pre-stored general data, and then
task-specific fine-tuning is performed at edge devices based on the pre-trained
model via federated edge learning. For the two-stage learning model, we first
analyze the convergence behavior (in terms of the average squared gradient norm
bound), which characterizes the impacts of various system parameters such as
the number of learning rounds and batch sizes in the two stages on the
convergence rate. Based on our analytical results, we then propose a joint
communication and computation resource management design to minimize an average
squared gradient norm bound, subject to constraints on the transmit power,
overall system energy consumption, and training delay. The decision variables
include the number of learning rounds, batch sizes, clock frequencies, and
transmit power control for both pre-training and fine-tuning stages. Finally,
numerical results are provided to evaluate the effectiveness of our proposed
design. It is shown that the proposed joint resource management over the
pre-training and fine-tuning stages well balances the system performance
trade-off among the training accuracy, delay, and energy consumption. The
proposed design is also shown to effectively leverage the inherent trade-off
between pre-training and fine-tuning, which arises from the differences in data
distribution between pre-stored general data versus real-time task-specific
data, thus efficiently optimizing overall system performance.
\\ ( https://arxiv.org/abs/2404.00836 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00837 (*cross-listing*)
Date: Mon, 1 Apr 2024 00:23:22 GMT   (1686kb)

Title: Automated HER2 Scoring in Breast Cancer Images Using Deep Learning and
  Pyramid Sampling
Authors: Sahan Yoruc Selcuk, Xilin Yang, Bijie Bai, Yijie Zhang, Yuzhu Li, Musa
  Aydin, Aras Firat Unal, Aditya Gomatam, Zhen Guo, Darrow Morgan Angus, Goren
  Kolodney, Karine Atlan, Tal Keidar Haran, Nir Pillar, Aydogan Ozcan
Categories: eess.IV cs.CV cs.LG physics.med-ph
Comments: 21 Pages, 7 Figures
\\
  Human epidermal growth factor receptor 2 (HER2) is a critical protein in
cancer cell growth that signifies the aggressiveness of breast cancer (BC) and
helps predict its prognosis. Accurate assessment of immunohistochemically (IHC)
stained tissue slides for HER2 expression levels is essential for both
treatment guidance and understanding of cancer mechanisms. Nevertheless, the
traditional workflow of manual examination by board-certified pathologists
encounters challenges, including inter- and intra-observer inconsistency and
extended turnaround times. Here, we introduce a deep learning-based approach
utilizing pyramid sampling for the automated classification of HER2 status in
IHC-stained BC tissue images. Our approach analyzes morphological features at
various spatial scales, efficiently managing the computational load and
facilitating a detailed examination of cellular and larger-scale tissue-level
details. This method addresses the tissue heterogeneity of HER2 expression by
providing a comprehensive view, leading to a blind testing classification
accuracy of 84.70%, on a dataset of 523 core images from tissue microarrays.
Our automated system, proving reliable as an adjunct pathology tool, has the
potential to enhance diagnostic precision and evaluation speed, and might
significantly impact cancer treatment planning.
\\ ( https://arxiv.org/abs/2404.00837 ,  1686kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00846 (*cross-listing*)
Date: Mon, 1 Apr 2024 01:23:58 GMT   (834kb,D)

Title: Transfer Learning with Point Transformers
Authors: Kartik Gupta and Rahul Vippala and Sahima Srivastava
Categories: cs.CV cs.LG
\\
  Point Transformers are near state-of-the-art models for classification,
segmentation, and detection tasks on Point Cloud data. They utilize a self
attention based mechanism to model large range spatial dependencies between
multiple point sets. In this project we explore two things: classification
performance of these attention based networks on ModelNet10 dataset and then,
we use the trained model to classify 3D MNIST dataset after finetuning. We also
train the model from scratch on 3D MNIST dataset to compare the performance of
finetuned and from-scratch model on the MNIST dataset. We observe that since
the two datasets have a large difference in the degree of the distributions,
transfer learned models do not outperform the from-scratch models in this case.
Although we do expect transfer learned models to converge faster since they
already know the lower level edges, corners, etc features from the ModelNet10
dataset.
\\ ( https://arxiv.org/abs/2404.00846 ,  834kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00852 (*cross-listing*)
Date: Mon, 1 Apr 2024 01:45:30 GMT   (5287kb,D)

Title: Ensemble Learning for Vietnamese Scene Text Spotting in Urban
  Environments
Authors: Hieu Nguyen, Cong-Hoang Ta, Phuong-Thuy Le-Nguyen, Minh-Triet Tran and
  Trung-Nghia Le
Categories: cs.CV cs.LG
Comments: RIVF 2023
Journal-ref: In 2023 RIVF International Conference on Computing and
  Communication Technologies (RIVF) (pp. 177-182). IEEE
DOI: 10.1109/rivf60135.2023.10471878
\\
  This paper presents a simple yet efficient ensemble learning framework for
Vietnamese scene text spotting. Leveraging the power of ensemble learning,
which combines multiple models to yield more accurate predictions, our approach
aims to significantly enhance the performance of scene text spotting in
challenging urban settings. Through experimental evaluations on the VinText
dataset, our proposed method achieves a significant improvement in accuracy
compared to existing methods with an impressive accuracy of 5%. These results
unequivocally demonstrate the efficacy of ensemble learning in the context of
Vietnamese scene text spotting in urban environments, highlighting its
potential for real world applications, such as text detection and recognition
in urban signage, advertisements, and various text-rich urban scenes.
\\ ( https://arxiv.org/abs/2404.00852 ,  5287kb)
------------------------------------------------------------------------------
\\
arXiv:2404.00928 (*cross-listing*)
Date: Mon, 1 Apr 2024 05:12:30 GMT   (468kb,D)

Title: Instance-Aware Group Quantization for Vision Transformers
Authors: Jaehyeon Moon, Dohyung Kim, Junyong Cheon, Bumsub Ham
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\
  Post-training quantization (PTQ) is an efficient model compression technique
that quantizes a pretrained full-precision model using only a small calibration
set of unlabeled samples without retraining. PTQ methods for convolutional
neural networks (CNNs) provide quantization results comparable to
full-precision counterparts. Directly applying them to vision transformers
(ViTs), however, incurs severe performance degradation, mainly due to the
differences in architectures between CNNs and ViTs. In particular, the
distribution of activations for each channel vary drastically according to
input instances, making PTQ methods for CNNs inappropriate for ViTs. To address
this, we introduce instance-aware group quantization for ViTs (IGQ-ViT). To
this end, we propose to split the channels of activation maps into multiple
groups dynamically for each input instance, such that activations within each
group share similar statistical properties. We also extend our scheme to
quantize softmax attentions across tokens. In addition, the number of groups
for each layer is adjusted to minimize the discrepancies between predictions
from quantized and full-precision models, under a bit-operation (BOP)
constraint. We show extensive experimental results on image classification,
object detection, and instance segmentation, with various transformer
architectures, demonstrating the effectiveness of our approach.
\\ ( https://arxiv.org/abs/2404.00928 ,  468kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01049 (*cross-listing*)
Date: Mon, 1 Apr 2024 11:08:53 GMT   (3334kb,D)

Title: A Novel Sector-Based Algorithm for an Optimized Star-Galaxy
  Classification
Authors: Anumanchi Agastya Sai Ram Likhit, Divyansh Tripathi, Akshay Agarwal
Categories: astro-ph.IM cs.LG
\\
  This paper introduces a novel sector-based methodology for star-galaxy
classification, leveraging the latest Sloan Digital Sky Survey data
(SDSS-DR18). By strategically segmenting the sky into sectors aligned with SDSS
observational patterns and employing a dedicated convolutional neural network
(CNN), we achieve state-of-the-art performance for star galaxy classification.
Our preliminary results demonstrate a promising pathway for efficient and
precise astronomical analysis, especially in real-time observational settings.
\\ ( https://arxiv.org/abs/2404.01049 ,  3334kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01050 (*cross-listing*)
Date: Mon, 1 Apr 2024 11:09:40 GMT   (30431kb,D)

Title: Drag Your Noise: Interactive Point-based Editing via Diffusion Semantic
  Propagation
Authors: Haofeng Liu, Chenshu Xu, Yifei Yang, Lihua Zeng, Shengfeng He
Categories: cs.CV cs.GR cs.HC cs.LG
Comments: Accepted by CVPR 2024
\\
  Point-based interactive editing serves as an essential tool to complement the
controllability of existing generative models. A concurrent work,
DragDiffusion, updates the diffusion latent map in response to user inputs,
causing global latent map alterations. This results in imprecise preservation
of the original content and unsuccessful editing due to gradient vanishing. In
contrast, we present DragNoise, offering robust and accelerated editing without
retracing the latent map. The core rationale of DragNoise lies in utilizing the
predicted noise output of each U-Net as a semantic editor. This approach is
grounded in two critical observations: firstly, the bottleneck features of
U-Net inherently possess semantically rich features ideal for interactive
editing; secondly, high-level semantics, established early in the denoising
process, show minimal variation in subsequent stages. Leveraging these
insights, DragNoise edits diffusion semantics in a single denoising step and
efficiently propagates these changes, ensuring stability and efficiency in
diffusion editing. Comparative experiments reveal that DragNoise achieves
superior control and semantic retention, reducing the optimization time by over
50% compared to DragDiffusion. Our codes are available at
https://github.com/haofengl/DragNoise.
\\ ( https://arxiv.org/abs/2404.01050 ,  30431kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01058 (*cross-listing*)
Date: Mon, 1 Apr 2024 11:40:09 GMT   (369kb)

Title: A Novel Audio Representation for Music Genre Identification in MIR
Authors: Navin Kamuni, Mayank Jindal, Arpita Soni, Sukender Reddy Mallreddy,
  Sharath Chandra Macha
Categories: cs.SD cs.IR cs.LG eess.AS
\\
  For Music Information Retrieval downstream tasks, the most common audio
representation is time-frequency-based, such as Mel spectrograms. In order to
identify musical genres, this study explores the possibilities of a new form of
audio representation one of the most usual MIR downstream tasks. Therefore, to
discretely encoding music using deep vector quantization; a novel audio
representation was created for the innovative generative music model i.e.
Jukebox. The effectiveness of Jukebox's audio representation is compared to Mel
spectrograms using a dataset that is almost equivalent to State-of-the-Art
(SOTA) and an almost same transformer design. The results of this study imply
that, at least when the transformers are pretrained using a very modest dataset
of 20k tracks, Jukebox's audio representation is not superior to Mel
spectrograms. This could be explained by the fact that Jukebox's audio
representation does not sufficiently take into account the peculiarities of
human hearing perception. On the other hand, Mel spectrograms are specifically
created with the human auditory sense in mind.
\\ ( https://arxiv.org/abs/2404.01058 ,  369kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01074 (*cross-listing*)
Date: Mon, 1 Apr 2024 12:16:00 GMT   (1604kb)

Title: Prompt Learning for Oriented Power Transmission Tower Detection in
  High-Resolution SAR Images
Authors: Tianyang Li, Chao Wang, Hong Zhang
Categories: cs.CV cs.LG
Comments: 22 pages, 12figures
\\
  Detecting transmission towers from synthetic aperture radar (SAR) images
remains a challenging task due to the comparatively small size and side-looking
geometry, with background clutter interference frequently hindering tower
identification. A large number of interfering signals superimposes the return
signal from the tower. We found that localizing or prompting positions of power
transmission towers is beneficial to address this obstacle. Based on this
revelation, this paper introduces prompt learning into the oriented object
detector (P2Det) for multimodal information learning. P2Det contains the sparse
prompt coding and cross-attention between the multimodal data. Specifically,
the sparse prompt encoder (SPE) is proposed to represent point locations,
converting prompts into sparse embeddings. The image embeddings are generated
through the Transformer layers. Then a two-way fusion module (TWFM) is proposed
to calculate the cross-attention of the two different embeddings. The
interaction of image-level and prompt-level features is utilized to address the
clutter interference. A shape-adaptive refinement module (SARM) is proposed to
reduce the effect of aspect ratio. Extensive experiments demonstrated the
effectiveness of the proposed model on high-resolution SAR images. P2Det
provides a novel insight for multimodal object detection due to its competitive
performance.
\\ ( https://arxiv.org/abs/2404.01074 ,  1604kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01100 (*cross-listing*)
Date: Mon, 1 Apr 2024 13:13:25 GMT   (82kb,D)

Title: Finite Sample Frequency Domain Identification
Authors: Anastasios Tsiamis, Mohamed Abdalmoaty, Roy S. Smith, John Lygeros
Categories: eess.SY cs.LG cs.SY math.OC stat.ML
\\
  We study non-parametric frequency-domain system identification from a
finite-sample perspective. We assume an open loop scenario where the excitation
input is periodic and consider the Empirical Transfer Function Estimate (ETFE),
where the goal is to estimate the frequency response at certain desired
(evenly-spaced) frequencies, given input-output samples. We show that under
sub-Gaussian colored noise (in time-domain) and stability assumptions, the ETFE
estimates are concentrated around the true values. The error rate is of the
order of
$\mathcal{O}((d_{\mathrm{u}}+\sqrt{d_{\mathrm{u}}d_{\mathrm{y}}})\sqrt{M/N_{\mathrm{tot}}})$,
where $N_{\mathrm{tot}}$ is the total number of samples, $M$ is the number of
desired frequencies, and $d_{\mathrm{u}},\,d_{\mathrm{y}}$ are the dimensions
of the input and output signals respectively. This rate remains valid for
general irrational transfer functions and does not require a finite order
state-space representation. By tuning $M$, we obtain a
$N_{\mathrm{tot}}^{-1/3}$ finite-sample rate for learning the frequency
response over all frequencies in the $ \mathcal{H}_{\infty}$ norm. Our result
draws upon an extension of the Hanson-Wright inequality to semi-infinite
matrices. We study the finite-sample behavior of ETFE in simulations.
\\ ( https://arxiv.org/abs/2404.01100 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01101 (*cross-listing*)
Date: Mon, 1 Apr 2024 13:21:05 GMT   (1550kb,D)

Title: UFID: A Unified Framework for Input-level Backdoor Detection on
  Diffusion Models
Authors: Zihan Guan, Mengxuan Hu, Sheng Li, Anil Vullikanti
Categories: cs.CR cs.CV cs.LG
Comments: 20 pages,18 figures
\\
  Diffusion Models are vulnerable to backdoor attacks, where malicious
attackers inject backdoors by poisoning some parts of the training samples
during the training stage. This poses a serious threat to the downstream users,
who query the diffusion models through the API or directly download them from
the internet. To mitigate the threat of backdoor attacks, there have been a
plethora of investigations on backdoor detections. However, none of them
designed a specialized backdoor detection method for diffusion models,
rendering the area much under-explored. Moreover, these prior methods mainly
focus on the traditional neural networks in the classification task, which
cannot be adapted to the backdoor detections on the generative task easily.
Additionally, most of the prior methods require white-box access to model
weights and architectures, or the probability logits as additional information,
which are not always practical. In this paper, we propose a Unified Framework
for Input-level backdoor Detection (UFID) on the diffusion models, which is
motivated by observations in the diffusion models and further validated with a
theoretical causality analysis. Extensive experiments across different datasets
on both conditional and unconditional diffusion models show that our method
achieves a superb performance on detection effectiveness and run-time
efficiency. The code is available at
https://github.com/GuanZihan/official_UFID.
\\ ( https://arxiv.org/abs/2404.01101 ,  1550kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01102 (*cross-listing*)
Date: Mon, 1 Apr 2024 13:23:04 GMT   (1226kb,D)

Title: Diffusion based Zero-shot Medical Image-to-Image Translation for Cross
  Modality Segmentation
Authors: Zihao Wang, Yingyu Yang, Yuzhou Chen, Tingting Yuan, Maxime Sermesant,
  Herve Delingette
Categories: eess.IV cs.CV cs.LG
Comments: Neurips 2023 Diffusion Workshop
\\
  Cross-modality image segmentation aims to segment the target modalities using
a method designed in the source modality. Deep generative models can translate
the target modality images into the source modality, thus enabling
cross-modality segmentation. However, a vast body of existing cross-modality
image translation methods relies on supervised learning. In this work, we aim
to address the challenge of zero-shot learning-based image translation tasks
(extreme scenarios in the target modality is unseen in the training phase). To
leverage generative learning for zero-shot cross-modality image segmentation,
we propose a novel unsupervised image translation method. The framework learns
to translate the unseen source image to the target modality for image
segmentation by leveraging the inherent statistical consistency between
different modalities for diffusion guidance. Our framework captures identical
cross-modality features in the statistical domain, offering diffusion guidance
without relying on direct mappings between the source and target domains. This
advantage allows our method to adapt to changing source domains without the
need for retraining, making it highly practical when sufficient labeled source
domain data is not available. The proposed framework is validated in zero-shot
cross-modality image segmentation tasks through empirical comparisons with
influential generative models, including adversarial-based and diffusion-based
models.
\\ ( https://arxiv.org/abs/2404.01102 ,  1226kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01145 (*cross-listing*)
Date: Mon, 1 Apr 2024 14:45:16 GMT   (33kb)

Title: Sequential-in-time training of nonlinear parametrizations for solving
  time-dependent partial differential equations
Authors: Huan Zhang and Yifan Chen and Eric Vanden-Eijnden and Benjamin
  Peherstorfer
Categories: math.NA cs.LG cs.NA
\\
  Sequential-in-time methods solve a sequence of training problems to fit
nonlinear parametrizations such as neural networks to approximate solution
trajectories of partial differential equations over time. This work shows that
sequential-in-time training methods can be understood broadly as either
optimize-then-discretize (OtD) or discretize-then-optimize (DtO) schemes, which
are well known concepts in numerical analysis. The unifying perspective leads
to novel stability and a posteriori error analysis results that provide
insights into theoretical and numerical aspects that are inherent to either OtD
or DtO schemes such as the tangent space collapse phenomenon, which is a form
of over-fitting. Additionally, the unified perspective facilitates establishing
connections between variants of sequential-in-time training methods, which is
demonstrated by identifying natural gradient descent methods on energy
functionals as OtD schemes applied to the corresponding gradient flows.
\\ ( https://arxiv.org/abs/2404.01145 ,  33kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01153 (*cross-listing*)
Date: Mon, 1 Apr 2024 14:58:16 GMT   (8641kb,D)

Title: TransFusion: Covariate-Shift Robust Transfer Learning for
  High-Dimensional Regression
Authors: Zelin He, Ying Sun, Jingyuan Liu, Runze Li
Categories: stat.ML cs.DC cs.LG math.ST stat.ME stat.TH
Comments: Accepted by the 27th International Conference on Artificial
  Intelligence and Statistics (AISTATS 2024)
\\
  The main challenge that sets transfer learning apart from traditional
supervised learning is the distribution shift, reflected as the shift between
the source and target models and that between the marginal covariate
distributions. In this work, we tackle model shifts in the presence of
covariate shifts in the high-dimensional regression setting. Specifically, we
propose a two-step method with a novel fused-regularizer that effectively
leverages samples from source tasks to improve the learning performance on a
target task with limited samples. Nonasymptotic bound is provided for the
estimation error of the target model, showing the robustness of the proposed
method to covariate shifts. We further establish conditions under which the
estimator is minimax-optimal. Additionally, we extend the method to a
distributed setting, allowing for a pretraining-finetuning strategy, requiring
just one round of communication while retaining the estimation rate of the
centralized version. Numerical tests validate our theory, highlighting the
method's robustness to covariate shifts.
\\ ( https://arxiv.org/abs/2404.01153 ,  8641kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01179 (*cross-listing*)
Date: Mon, 1 Apr 2024 15:31:04 GMT   (6871kb,D)

Title: BEM: Balanced and Entropy-based Mix for Long-Tailed Semi-Supervised
  Learning
Authors: Hongwei Zheng, Linyuan Zhou, Han Li, Jinming Su, Xiaoming Wei,
  Xiaoming Xu
Categories: cs.CV cs.LG
Comments: This paper is accepted to CVPR 2024. The supplementary material is
  included
\\
  Data mixing methods play a crucial role in semi-supervised learning (SSL),
but their application is unexplored in long-tailed semi-supervised learning
(LTSSL). The primary reason is that the in-batch mixing manner fails to address
class imbalance. Furthermore, existing LTSSL methods mainly focus on
re-balancing data quantity but ignore class-wise uncertainty, which is also
vital for class balance. For instance, some classes with sufficient samples
might still exhibit high uncertainty due to indistinguishable features. To this
end, this paper introduces the Balanced and Entropy-based Mix (BEM), a
pioneering mixing approach to re-balance the class distribution of both data
quantity and uncertainty. Specifically, we first propose a class balanced mix
bank to store data of each class for mixing. This bank samples data based on
the estimated quantity distribution, thus re-balancing data quantity. Then, we
present an entropy-based learning approach to re-balance class-wise
uncertainty, including entropy-based sampling strategy, entropy-based selection
module, and entropy-based class balanced loss. Our BEM first leverages data
mixing for improving LTSSL, and it can also serve as a complement to the
existing re-balancing methods. Experimental results show that BEM significantly
enhances various LTSSL frameworks and achieves state-of-the-art performances
across multiple benchmarks.
\\ ( https://arxiv.org/abs/2404.01179 ,  6871kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01184 (*cross-listing*)
Date: Mon, 1 Apr 2024 15:36:39 GMT   (2054kb,D)

Title: Efficient Motion Planning for Manipulators with Control Barrier
  Function-Induced Neural Controller
Authors: Mingxin Yu, Chenning Yu, M-Mahdi Naddaf-Sh, Devesh Upadhyay, Sicun
  Gao, and Chuchu Fan
Categories: cs.RO cs.LG
Comments: Accepted by IEEE International Conference on Robotics and Automation
  (ICRA2024)
\\
  Sampling-based motion planning methods for manipulators in crowded
environments often suffer from expensive collision checking and high sampling
complexity, which make them difficult to use in real time. To address this
issue, we propose a new generalizable control barrier function (CBF)-based
steering controller to reduce the number of samples needed in a sampling-based
motion planner RRT. Our method combines the strength of CBF for real-time
collision-avoidance control and RRT for long-horizon motion planning, by using
CBF-induced neural controller (CBF-INC) to generate control signals that steer
the system towards sampled configurations by RRT. CBF-INC is learned as Neural
Networks and has two variants handling different inputs, respectively: state
(signed distance) input and point-cloud input from LiDAR. In the latter case,
we also study two different settings: fully and partially observed
environmental information. Compared to manually crafted CBF which suffers from
over-approximating robot geometry, CBF-INC can balance safety and goal-reaching
better without being over-conservative. Given state-based input, our neural
CBF-induced neural controller-enhanced RRT (CBF-INC-RRT) can increase the
success rate by 14% while reducing the number of nodes explored by 30%,
compared with vanilla RRT on hard test cases. Given LiDAR input where vanilla
RRT is not directly applicable, we demonstrate that our CBF-INC-RRT can improve
the success rate by 10%, compared with planning with other steering
controllers. Our project page with supplementary material is at
https://mit-realm.github.io/CBF-INC-RRT-website/.
\\ ( https://arxiv.org/abs/2404.01184 ,  2054kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01200 (*cross-listing*)
Date: Mon, 1 Apr 2024 15:56:58 GMT   (4276kb,D)

Title: Large-Scale Non-convex Stochastic Constrained Distributionally Robust
  Optimization
Authors: Qi Zhang, Yi Zhou, Ashley Prater-Bennette, Lixin Shen, Shaofeng Zou
Categories: stat.ML cs.LG
Comments: We have corrected Theorem 1 in Sec 4 for AAAI 2024 version, where the
  order of $n_z$ changes from ${\epsilon}^{-k_*} )$ to ${\epsilon}^{-2k_*-2}$
\\
  Distributionally robust optimization (DRO) is a powerful framework for
training robust models against data distribution shifts. This paper focuses on
constrained DRO, which has an explicit characterization of the robustness
level. Existing studies on constrained DRO mostly focus on convex loss
function, and exclude the practical and challenging case with non-convex loss
function, e.g., neural network. This paper develops a stochastic algorithm and
its performance analysis for non-convex constrained DRO. The computational
complexity of our stochastic algorithm at each iteration is independent of the
overall dataset size, and thus is suitable for large-scale applications. We
focus on the general Cressie-Read family divergence defined uncertainty set
which includes $\chi^2$-divergences as a special case. We prove that our
algorithm finds an $\epsilon$-stationary point with a computational complexity
of $\mathcal O(\epsilon^{-3k_*-5})$, where $k_*$ is the parameter of the
Cressie-Read divergence. The numerical results indicate that our method
outperforms existing methods.} Our method also applies to the smoothed
conditional value at risk (CVaR) DRO.
\\ ( https://arxiv.org/abs/2404.01200 ,  4276kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01220 (*cross-listing*)
Date: Mon, 1 Apr 2024 16:25:08 GMT   (10289kb,D)

Title: Entity-Centric Reinforcement Learning for Object Manipulation from
  Pixels
Authors: Dan Haramati, Tal Daniel, Aviv Tamar
Categories: cs.RO cs.CV cs.LG
Comments: ICLR 2024 Spotlight. Videos and code are available on the project
  website: https://sites.google.com/view/entity-centric-rl
\\
  Manipulating objects is a hallmark of human intelligence, and an important
task in domains such as robotics. In principle, Reinforcement Learning (RL)
offers a general approach to learn object manipulation. In practice, however,
domains with more than a few objects are difficult for RL agents due to the
curse of dimensionality, especially when learning from raw image observations.
In this work we propose a structured approach for visual RL that is suitable
for representing multiple objects and their interaction, and use it to learn
goal-conditioned manipulation of several objects. Key to our method is the
ability to handle goals with dependencies between the objects (e.g., moving
objects in a certain order). We further relate our architecture to the
generalization capability of the trained agent, based on a theoretical result
for compositional generalization, and demonstrate agents that learn with 3
objects but generalize to similar tasks with over 10 objects. Videos and code
are available on the project website:
https://sites.google.com/view/entity-centric-rl
\\ ( https://arxiv.org/abs/2404.01220 ,  10289kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01231 (*cross-listing*)
Date: Mon, 1 Apr 2024 16:50:54 GMT   (109kb,D)

Title: Privacy Backdoors: Enhancing Membership Inference through Poisoning
  Pre-trained Models
Authors: Yuxin Wen, Leo Marchyok, Sanghyun Hong, Jonas Geiping, Tom Goldstein,
  Nicholas Carlini
Categories: cs.CR cs.LG
\\
  It is commonplace to produce application-specific models by fine-tuning large
pre-trained models using a small bespoke dataset. The widespread availability
of foundation model checkpoints on the web poses considerable risks, including
the vulnerability to backdoor attacks. In this paper, we unveil a new
vulnerability: the privacy backdoor attack. This black-box privacy attack aims
to amplify the privacy leakage that arises when fine-tuning a model: when a
victim fine-tunes a backdoored model, their training data will be leaked at a
significantly higher rate than if they had fine-tuned a typical model. We
conduct extensive experiments on various datasets and models, including both
vision-language models (CLIP) and large language models, demonstrating the
broad applicability and effectiveness of such an attack. Additionally, we carry
out multiple ablation studies with different fine-tuning methods and inference
strategies to thoroughly analyze this new threat. Our findings highlight a
critical privacy concern within the machine learning community and call for a
reevaluation of safety protocols in the use of open-source pre-trained models.
\\ ( https://arxiv.org/abs/2404.01231 ,  109kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01233 (*cross-listing*)
Date: Mon, 1 Apr 2024 16:51:19 GMT   (2130kb,D)

Title: Optimal Ridge Regularization for Out-of-Distribution Prediction
Authors: Pratik Patil, Jin-Hong Du, Ryan J. Tibshirani
Categories: math.ST cs.LG stat.ML stat.TH
Comments: 59 pages, 14 figures
\\
  We study the behavior of optimal ridge regularization and optimal ridge risk
for out-of-distribution prediction, where the test distribution deviates
arbitrarily from the train distribution. We establish general conditions that
determine the sign of the optimal regularization level under covariate and
regression shifts. These conditions capture the alignment between the
covariance and signal structures in the train and test data and reveal stark
differences compared to the in-distribution setting. For example, a negative
regularization level can be optimal under covariate shift or regression shift,
even when the training features are isotropic or the design is
underparameterized. Furthermore, we prove that the optimally-tuned risk is
monotonic in the data aspect ratio, even in the out-of-distribution setting and
when optimizing over negative regularization levels. In general, our results do
not make any modeling assumptions for the train or the test distributions,
except for moment bounds, and allow for arbitrary shifts and the widest
possible range of (negative) regularization levels.
\\ ( https://arxiv.org/abs/2404.01233 ,  2130kb)
------------------------------------------------------------------------------
\\
arXiv:2404.01292 (*cross-listing*)
Date: Mon, 1 Apr 2024 17:58:30 GMT   (16501kb,D)

Title: Measuring Style Similarity in Diffusion Models
Authors: Gowthami Somepalli, Anubhav Gupta, Kamal Gupta, Shramay Palta, Micah
  Goldblum, Jonas Geiping, Abhinav Shrivastava, Tom Goldstein
Categories: cs.CV cs.LG
\\
  Generative models are now widely used by graphic designers and artists. Prior
works have shown that these models remember and often replicate content from
their training data during generation. Hence as their proliferation increases,
it has become important to perform a database search to determine whether the
properties of the image are attributable to specific training data, every time
before a generated image is used for professional purposes. Existing tools for
this purpose focus on retrieving images of similar semantic content. Meanwhile,
many artists are concerned with style replication in text-to-image models. We
present a framework for understanding and extracting style descriptors from
images. Our framework comprises a new dataset curated using the insight that
style is a subjective property of an image that captures complex yet meaningful
interactions of factors including but not limited to colors, textures, shapes,
etc. We also propose a method to extract style descriptors that can be used to
attribute style of a generated image to the images used in the training dataset
of a text-to-image model. We showcase promising results in various style
retrieval tasks. We also quantitatively and qualitatively analyze style
attribution and matching in the Stable Diffusion model. Code and artifacts are
available at https://github.com/learn2phoenix/CSD.
\\ ( https://arxiv.org/abs/2404.01292 ,  16501kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2209.03499
replaced with revised version Fri, 29 Mar 2024 20:22:00 GMT   (6898kb)

Title: Regulating eXplainable Artificial Intelligence (XAI) May Harm Consumers
Authors: Behnam Mohammadi, Nikhil Malik, Tim Derdenger, Kannan Srinivasan
Categories: cs.AI
Comments: Corrected the title
\\ ( https://arxiv.org/abs/2209.03499 ,  6898kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09846
replaced with revised version Sun, 31 Mar 2024 07:50:22 GMT   (1604kb,D)

Title: G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction
  System
Authors: Aryan Garg, Renu M. Rameshan
Categories: cs.AI cs.CV cs.LG cs.RO
Comments: Notable ICLR Tiny Paper 2024
\\ ( https://arxiv.org/abs/2210.09846 ,  1604kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11074
replaced with revised version Sat, 30 Mar 2024 10:45:22 GMT   (7818kb,D)

Title: Tram: A Token-level Retrieval-augmented Mechanism for Source Code
  Summarization
Authors: Tong Ye, Lingfei Wu, Tengfei Ma, Xuhong Zhang, Yangkai Du, Peiyu Liu,
  Shouling Ji, Wenhai Wang
Categories: cs.AI
Comments: NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2305.11074 ,  7818kb)
------------------------------------------------------------------------------
\\
arXiv:2308.00225
replaced with revised version Sun, 31 Mar 2024 12:20:25 GMT   (500kb,D)

Title: Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent
  Cognitive Bias
Authors: Itay Itzhak, Gabriel Stanovsky, Nir Rosenfeld, Yonatan Belinkov
Categories: cs.AI cs.CY cs.LG
Comments: TACL 2024. Presented at ACL 2024. 12 pages
\\ ( https://arxiv.org/abs/2308.00225 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09724
replaced with revised version Mon, 1 Apr 2024 13:50:51 GMT   (8036kb,D)

Title: OVM, Outcome-supervised Value Models for Planning in Mathematical
  Reasoning
Authors: Fei Yu, Anningzhe Gao, Benyou Wang
Categories: cs.AI cs.CL
Comments: Accepted to NAACL findings.
  https://github.com/FreedomIntelligence/OVM
\\ ( https://arxiv.org/abs/2311.09724 ,  8036kb)
------------------------------------------------------------------------------
\\
arXiv:2311.11211
replaced with revised version Mon, 1 Apr 2024 02:04:25 GMT   (240kb)

Title: Leveraging Generative AI for Clinical Evidence Summarization Needs to
  Ensure Trustworthiness
Authors: Gongbo Zhang, Qiao Jin, Denis Jered McInerney, Yong Chen, Fei Wang,
  Curtis L. Cole, Qian Yang, Yanshan Wang, Bradley A. Malin, Mor Peleg, Byron
  C. Wallace, Zhiyong Lu, Chunhua Weng, Yifan Peng
Categories: cs.AI
\\ ( https://arxiv.org/abs/2311.11211 ,  240kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09238
replaced with revised version Sat, 30 Mar 2024 15:35:16 GMT   (8245kb,D)

Title: Auto MC-Reward: Automated Dense Reward Design with Large Language Models
  for Minecraft
Authors: Hao Li, Xue Yang, Zhaokai Wang, Xizhou Zhu, Jie Zhou, Yu Qiao,
  Xiaogang Wang, Hongsheng Li, Lewei Lu, Jifeng Dai
Categories: cs.AI cs.CL cs.CV cs.LG
Comments: Accepted by CVPR2024
\\ ( https://arxiv.org/abs/2312.09238 ,  8245kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04154
replaced with revised version Mon, 1 Apr 2024 15:18:57 GMT   (1492kb,D)

Title: Read to Play (R2-Play): Decision Transformer with Multimodal Game
  Instruction
Authors: Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu
  Xiang, Shawn Yue, Stephen W. Huang, Zhaofeng He and Jie Fu
Categories: cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.04154 ,  1492kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09391
replaced with revised version Mon, 1 Apr 2024 17:28:16 GMT   (807kb,D)

Title: LlaSMol: Advancing Large Language Models for Chemistry with a
  Large-Scale, Comprehensive, High-Quality Instruction Tuning Dataset
Authors: Botao Yu, Frazier N. Baker, Ziqi Chen, Xia Ning, Huan Sun
Categories: cs.AI cs.CE cs.CL
Comments: Added further analysis experiments. Work in progress
\\ ( https://arxiv.org/abs/2402.09391 ,  807kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14589
replaced with revised version Mon, 1 Apr 2024 17:37:15 GMT   (341kb,D)

Title: ReAct Meets ActRe: When Language Agents Enjoy Training Data Autonomy
Authors: Zonghan Yang, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu
Categories: cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.14589 ,  341kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17306
replaced with revised version Sun, 31 Mar 2024 03:52:14 GMT   (25437kb,D)

Title: Visual Hallucination: Definition, Quantification, and Prescriptive
  Remediations
Authors: Anku Rani, Vipula Rawte, Harshad Sharma, Neeraj Anand, Krishnav
  Rajbangshi, Amit Sheth, Amitava Das
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.17306 ,  25437kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17683
replaced with revised version Sun, 31 Mar 2024 14:44:06 GMT   (3448kb,D)

Title: Solution for Emotion Prediction Competition of Workshop on Emotionally
  and Culturally Intelligent AI
Authors: Shengdong Xu, Zhouyang Chi, Yang Yang
Categories: cs.AI
\\ ( https://arxiv.org/abs/2403.17683 ,  3448kb)
------------------------------------------------------------------------------
\\
arXiv:2110.08604
replaced with revised version Mon, 1 Apr 2024 15:44:11 GMT   (1665kb,D)

Title: LSA: Modeling Aspect Sentiment Coherency via Local Sentiment Aggregation
Authors: Heng Yang, Ke Li
Categories: cs.CL
Comments: Accepted to EACL 2024
\\ ( https://arxiv.org/abs/2110.08604 ,  1665kb)
------------------------------------------------------------------------------
\\
arXiv:2207.14000
replaced with revised version Sat, 30 Mar 2024 08:18:15 GMT   (628kb,D)

Title: Multi-Step Deductive Reasoning Over Natural Language: An Empirical Study
  on Out-of-Distribution Generalisation
Authors: Qiming Bao, Alex Yuxuan Peng, Tim Hartill, Neset Tan, Zhenyun Deng,
  Michael Witbrock, Jiamou Liu
Categories: cs.CL cs.AI cs.LG cs.LO
Comments: 10 pages, 3 figures, The 2nd International Joint Conference on
  Learning & Reasoning and 16th International Workshop on Neural-Symbolic
  Learning and Reasoning (IJCLR-NeSy 2022)
\\ ( https://arxiv.org/abs/2207.14000 ,  628kb)
------------------------------------------------------------------------------
\\
arXiv:2210.02941
replaced with revised version Mon, 1 Apr 2024 15:30:20 GMT   (7283kb,D)

Title: BootAug: Boosting Text Augmentation via Hybrid Instance Filtering
  Framework
Authors: Heng Yang, Ke Li
Categories: cs.CL
Comments: Source code and examples:
  https://github.com/yangheng95/BoostTextAugmentation
\\ ( https://arxiv.org/abs/2210.02941 ,  7283kb)
------------------------------------------------------------------------------
\\
arXiv:2211.05985
replaced with revised version Mon, 1 Apr 2024 04:08:28 GMT   (3754kb,D)

Title: Using Persuasive Writing Strategies to Explain and Detect Health
  Misinformation
Authors: Danial Kamali, Joseph Romain, Huiyi Liu, Wei Peng, Jingbo Meng, Parisa
  Kordjamshidi
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2211.05985 ,  3754kb)
------------------------------------------------------------------------------
\\
arXiv:2302.07730
replaced with revised version Sun, 31 Mar 2024 21:20:30 GMT   (3757kb,D)

Title: Transformer models: an introduction and catalog
Authors: Xavier Amatriain, Ananth Sankar, Jie Bing, Praveen Kumar Bodigutla,
  Timothy J. Hazen, and Michaeel Kazi
Categories: cs.CL
\\ ( https://arxiv.org/abs/2302.07730 ,  3757kb)
------------------------------------------------------------------------------
\\
arXiv:2304.08103
replaced with revised version Mon, 1 Apr 2024 04:05:11 GMT   (2290kb,D)

Title: Low-code LLM: Graphical User Interface over Large Language Models
Authors: Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge,
  Chenfei Wu, Wang You, Ting Song, Yan Xia, Jonathan Tien, Nan Duan, Furu Wei
Categories: cs.CL cs.HC
Comments: Accepted as a Demo Track paper at NAACL 2024
\\ ( https://arxiv.org/abs/2304.08103 ,  2290kb)
------------------------------------------------------------------------------
\\
arXiv:2304.11766
replaced with revised version Mon, 1 Apr 2024 01:07:54 GMT   (858kb,D)

Title: NAIST-SIC-Aligned: an Aligned English-Japanese Simultaneous
  Interpretation Corpus
Authors: Jinming Zhao, Yuka Ko, Kosuke Doi, Ryo Fukuda, Katsuhito Sudoh,
  Satoshi Nakamura
Categories: cs.CL
Comments: LREC-Coling 2024
\\ ( https://arxiv.org/abs/2304.11766 ,  858kb)
------------------------------------------------------------------------------
\\
arXiv:2305.04067
replaced with revised version Mon, 1 Apr 2024 15:48:15 GMT   (5975kb,D)

Title: The Best Defense is Attack: Repairing Semantics in Textual Adversarial
  Examples
Authors: Heng Yang, Ke Li
Categories: cs.CL
\\ ( https://arxiv.org/abs/2305.04067 ,  5975kb)
------------------------------------------------------------------------------
\\
arXiv:2305.09781
replaced with revised version Mon, 1 Apr 2024 02:18:42 GMT   (9137kb,D)

Title: SpecInfer: Accelerating Generative Large Language Model Serving with
  Tree-based Speculative Inference and Verification
Authors: Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang,
  Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi,
  Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao Jia
Categories: cs.CL cs.DC cs.LG
Comments: ASPLOS'24
DOI: 10.1145/3620666.3651335
\\ ( https://arxiv.org/abs/2305.09781 ,  9137kb)
------------------------------------------------------------------------------
\\
arXiv:2305.12599
replaced with revised version Sat, 30 Mar 2024 11:14:55 GMT   (4140kb,D)

Title: Abstract Meaning Representation-Based Logic-Driven Data Augmentation for
  Logical Reasoning
Authors: Qiming Bao, Alex Yuxuan Peng, Zhenyun Deng, Wanjun Zhong, Gael
  Gendron, Timothy Pistotti, Neset Tan, Nathan Young, Yang Chen, Yonghua Zhu,
  Paul Denny, Michael Witbrock, Jiamou Liu
Categories: cs.CL cs.AI
Comments: The short version (v2) was accepted for oral presentation at the
  first LLM@IJCAI 2023 non-archival symposium; the full version is under
  review. Update the mistake in Figure 1
\\ ( https://arxiv.org/abs/2305.12599 ,  4140kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14124
replaced with revised version Sat, 30 Mar 2024 08:49:04 GMT   (4924kb,D)

Title: When Does Monolingual Data Help Multilingual Translation: The Role of
  Domain and Model Scale
Authors: Christos Baziotis, Biao Zhang, Alexandra Birch, Barry Haddow
Categories: cs.CL
Comments: Accepted to NAACL 2024 (Main conference)
\\ ( https://arxiv.org/abs/2305.14124 ,  4924kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14623
replaced with revised version Mon, 1 Apr 2024 03:23:23 GMT   (13300kb,D)

Title: Self-Checker: Plug-and-Play Modules for Fact-Checking with Large
  Language Models
Authors: Miaoran Li, Baolin Peng, Michel Galley, Jianfeng Gao, Zhu Zhang
Categories: cs.CL
Comments: Accepted by NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2305.14623 ,  13300kb)
------------------------------------------------------------------------------
\\
arXiv:2306.00739
replaced with revised version Sat, 30 Mar 2024 17:22:44 GMT   (1204kb,D)

Title: SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL
  (extended)
Authors: Ruoxi Sun, Sercan \"O. Arik, Alex Muzio, Lesly Miculicich, Satya
  Gundabathula, Pengcheng Yin, Hanjun Dai, Hootan Nakhost, Rajarishi Sinha,
  Zifeng Wang, Tomas Pfister
Categories: cs.CL cs.AI cs.DB
\\ ( https://arxiv.org/abs/2306.00739 ,  1204kb)
------------------------------------------------------------------------------
\\
arXiv:2306.02796
replaced with revised version Sat, 30 Mar 2024 07:55:41 GMT   (576kb,D)

Title: MCTS: A Multi-Reference Chinese Text Simplification Dataset
Authors: Ruining Chong, Luming Lu, Liner Yang, Jinran Nie, Zhenghao Liu, Shuo
  Wang, Shuhan Zhou, Yaoxin Li, Erhong Yang
Categories: cs.CL
\\ ( https://arxiv.org/abs/2306.02796 ,  576kb)
------------------------------------------------------------------------------
\\
arXiv:2307.02185
replaced with revised version Sun, 31 Mar 2024 19:47:47 GMT   (183kb,D)

Title: Citation: A Key to Building Responsible and Accountable Large Language
  Models
Authors: Jie Huang, Kevin Chen-Chuan Chang
Categories: cs.CL cs.AI cs.CR
Comments: NAACL 2024 Findings
\\ ( https://arxiv.org/abs/2307.02185 ,  183kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01263
replaced with revised version Mon, 1 Apr 2024 11:50:35 GMT   (335kb,D)

Title: XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in
  Large Language Models
Authors: Paul R\"ottger, Hannah Rose Kirk, Bertie Vidgen, Giuseppe Attanasio,
  Federico Bianchi, Dirk Hovy
Categories: cs.CL cs.AI
Comments: Accepted at NAACL 2024 (Main Conference)
\\ ( https://arxiv.org/abs/2308.01263 ,  335kb)
------------------------------------------------------------------------------
\\
arXiv:2308.07922
replaced with revised version Mon, 1 Apr 2024 06:32:12 GMT   (1687kb,D)

Title: RAVEN: In-Context Learning with Retrieval-Augmented Encoder-Decoder
  Language Models
Authors: Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan
  Chang, Bryan Catanzaro
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2308.07922 ,  1687kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08043
replaced with revised version Sun, 31 Mar 2024 04:39:05 GMT   (8116kb,D)

Title: DiagGPT: An LLM-based Chatbot with Automatic Topic Management for
  Task-Oriented Dialogue
Authors: Lang Cao
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2308.08043 ,  8116kb)
------------------------------------------------------------------------------
\\
arXiv:2308.08973
replaced with revised version Mon, 1 Apr 2024 08:30:38 GMT   (8638kb,D)

Title: End-to-End Beam Retrieval for Multi-Hop Question Answering
Authors: Jiahao Zhang, Haiyang Zhang, Dongmei Zhang, Yong Liu and Shen Huang
Categories: cs.CL
Comments: Accepted by NAACL 2024 Main Conference. Code is available at
  https://github.com/canghongjian/beam_retriever
\\ ( https://arxiv.org/abs/2308.08973 ,  8638kb)
------------------------------------------------------------------------------
\\
arXiv:2308.10509
replaced with revised version Sat, 30 Mar 2024 16:09:14 GMT   (3963kb,D)

Title: An Examination of the Compositionality of Large Generative
  Vision-Language Models
Authors: Teli Ma, Rong Li, Junwei Liang
Categories: cs.CL cs.AI cs.CV
\\ ( https://arxiv.org/abs/2308.10509 ,  3963kb)
------------------------------------------------------------------------------
\\
arXiv:2308.11696
replaced with revised version Mon, 1 Apr 2024 17:34:34 GMT   (11535kb,D)

Title: Efficient Benchmarking of Language Models
Authors: Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat Ein-Dor,
  Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, Leshem Choshen
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: Accepted to NAACL main track
\\ ( https://arxiv.org/abs/2308.11696 ,  11535kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04766
replaced with revised version Mon, 1 Apr 2024 03:26:26 GMT   (2825kb,D)

Title: SeaEval for Multilingual Foundation Models: From Cross-Lingual Alignment
  to Cultural Reasoning
Authors: Bin Wang and Zhengyuan Liu and Xin Huang and Fangkai Jiao and Yang
  Ding and AiTi Aw and Nancy F. Chen
Categories: cs.CL cs.AI
Comments: To appear in NAACL 2024. 20 pages. More datasets (2 on Cross-Lingual
  Consistency and 4 on Cultural Understanding) and more supported languages.
  Code: https://seaeval.github.io/
Journal-ref: NAACL 2024
\\ ( https://arxiv.org/abs/2309.04766 ,  2825kb)
------------------------------------------------------------------------------
\\
arXiv:2309.06415
replaced with revised version Sun, 31 Mar 2024 02:24:39 GMT   (3391kb,D)

Title: Down the Toxicity Rabbit Hole: A Novel Framework to Bias Audit Large
  Language Models
Authors: Arka Dutta and Adel Khorramrouz and Sujan Dutta and Ashiqur R.
  KhudaBukhsh
Categories: cs.CL cs.CY
\\ ( https://arxiv.org/abs/2309.06415 ,  3391kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08591
replaced with revised version Sat, 30 Mar 2024 17:13:58 GMT   (2200kb,D)

Title: Are Multilingual LLMs Culturally-Diverse Reasoners? An Investigation
  into Multicultural Proverbs and Sayings
Authors: Chen Cecilia Liu, Fajri Koto, Timothy Baldwin, Iryna Gurevych
Categories: cs.CL
Comments: NAACL
\\ ( https://arxiv.org/abs/2309.08591 ,  2200kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14771
replaced with revised version Sun, 31 Mar 2024 13:55:39 GMT   (1377kb,D)

Title: Knowledgeable In-Context Tuning: Exploring and Exploiting Factual
  Knowledge for In-Context Learning
Authors: Jianing Wang, Chengyu Wang, Chuanqi Tan, Jun Huang, Ming Gao
Categories: cs.CL cs.AI
Comments: naacl 2024
\\ ( https://arxiv.org/abs/2309.14771 ,  1377kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00322
replaced with revised version Mon, 1 Apr 2024 09:09:16 GMT   (2517kb,D)

Title: Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language
  Models
Authors: Chengdong Ma, Ziran Yang, Minquan Gao, Hai Ci, Jun Gao, Xuehai Pan,
  Yaodong Yang
Categories: cs.CL cs.GT
\\ ( https://arxiv.org/abs/2310.00322 ,  2517kb)
------------------------------------------------------------------------------
\\
arXiv:2310.00836
replaced with revised version Sun, 31 Mar 2024 01:02:55 GMT   (775kb,D)

Title: Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical
  Reasoning Capabilities of Language Models
Authors: Man Luo, Shrinidhi Kumbhar, Ming shen, Mihir Parmar, Neeraj Varshney,
  Pratyay Banerjee, Somak Aditya, Chitta Baral
Categories: cs.CL cs.AI
Comments: Work in progress
\\ ( https://arxiv.org/abs/2310.00836 ,  775kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06588
replaced with revised version Fri, 29 Mar 2024 23:53:28 GMT   (116kb,D)

Title: FTFT: Efficient and Robust Fine-Tuning by Transferring Training Dynamics
Authors: Yupei Du, Albert Gatt, Dong Nguyen
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2310.06588 ,  116kb)
------------------------------------------------------------------------------
\\
arXiv:2310.08461
replaced with revised version Sun, 31 Mar 2024 03:06:51 GMT   (555kb,D)

Title: DistillSpec: Improving Speculative Decoding via Knowledge Distillation
Authors: Yongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon,
  Afshin Rostamizadeh, Sanjiv Kumar, Jean-Fran\c{c}ois Kagy, Rishabh Agarwal
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2310.08461 ,  555kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09430
replaced with revised version Sat, 30 Mar 2024 09:49:19 GMT   (89kb)

Title: Assessing and Enhancing the Robustness of Large Language Models with
  Task Structure Variations for Logical Reasoning
Authors: Qiming Bao, Gael Gendron, Alex Yuxuan Peng, Wanjun Zhong, Neset Tan,
  Yang Chen, Michael Witbrock, Jiamou Liu
Categories: cs.CL cs.AI
Comments: The short version (v3) was accepted for oral presentation at the
  first LLM@IJCAI 2023 non-archival symposium; the full version is under review
\\ ( https://arxiv.org/abs/2310.09430 ,  89kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09518
replaced with revised version Sun, 31 Mar 2024 13:39:44 GMT   (2447kb,D)

Title: Instruction Tuning with Human Curriculum
Authors: Bruce W. Lee, Hyunsoo Cho, Kang Min Yoo
Categories: cs.CL cs.AI cs.LG
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2310.09518 ,  2447kb)
------------------------------------------------------------------------------
\\
arXiv:2310.13522
replaced with revised version Sun, 31 Mar 2024 18:12:16 GMT   (351kb,D)

Title: Teaching Language Models to Self-Improve through Interactive
  Demonstrations
Authors: Xiao Yu, Baolin Peng, Michel Galley, Jianfeng Gao, Zhou Yu
Categories: cs.CL
Comments: NAACL 2024 main
\\ ( https://arxiv.org/abs/2310.13522 ,  351kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14159
replaced with revised version Sun, 31 Mar 2024 10:51:06 GMT   (11696kb,D)

Title: Can Language Models Laugh at YouTube Short-form Videos?
Authors: Dayoon Ko, Sangho Lee, Gunhee Kim
Categories: cs.CL cs.CV
Comments: EMNLP 2023; references added
\\ ( https://arxiv.org/abs/2310.14159 ,  11696kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18964
replaced with revised version Sat, 30 Mar 2024 15:01:08 GMT   (499kb)

Title: LLMs and Finetuning: Benchmarking cross-domain performance for hate
  speech detection
Authors: Ahmad Nasir, Aadish Sharma, Kokil Jaidka
Categories: cs.CL
Comments: 9 pages, 3 figures, 4 tables
\\ ( https://arxiv.org/abs/2310.18964 ,  499kb)
------------------------------------------------------------------------------
\\
arXiv:2311.05915
replaced with revised version Mon, 1 Apr 2024 03:32:14 GMT   (2130kb,D)

Title: Fake Alignment: Are LLMs Really Aligned Well?
Authors: Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei
  Zhang, Xingjun Ma, Yu-Gang Jiang, Yu Qiao, Yingchun Wang
Categories: cs.CL cs.AI
Comments: Accepted to the NAACL 2024
\\ ( https://arxiv.org/abs/2311.05915 ,  2130kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06899
replaced with revised version Mon, 1 Apr 2024 03:16:03 GMT   (1157kb,D)

Title: Flames: Benchmarking Value Alignment of Chinese Large Language Models
Authors: Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun,
  Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, Dahua
  Lin
Categories: cs.CL cs.AI
Comments: Accepted to the NAACL 2024
\\ ( https://arxiv.org/abs/2311.06899 ,  1157kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07194
replaced with revised version Mon, 1 Apr 2024 16:37:50 GMT   (2029kb,D)

Title: Exploring the Factual Consistency in Dialogue Comprehension of Large
  Language Models
Authors: Shuaijie She, Shujian Huang, Xingyun Wang, Yanke Zhou, Jiajun Chen
Categories: cs.CL
Comments: Accepted at NAACL2024 Main
\\ ( https://arxiv.org/abs/2311.07194 ,  2029kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07362
replaced with revised version Mon, 1 Apr 2024 08:01:30 GMT   (3732kb,D)

Title: Volcano: Mitigating Multimodal Hallucination through Self-Feedback
  Guided Revision
Authors: Seongyun Lee and Sue Hyun Park and Yongrae Jo and Minjoon Seo
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2311.07362 ,  3732kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07772
replaced with revised version Sun, 31 Mar 2024 19:33:50 GMT   (288kb,D)

Title: In-context Learning and Gradient Descent Revisited
Authors: Gilad Deutch, Nadav Magar, Tomer Bar Natan, Guy Dar
Categories: cs.CL cs.LG
Comments: Accepted to NAACL 2024 main conference
\\ ( https://arxiv.org/abs/2311.07772 ,  288kb)
------------------------------------------------------------------------------
\\
arXiv:2311.07884
replaced with revised version Sat, 30 Mar 2024 03:54:06 GMT   (8169kb,D)

Title: Fair Abstractive Summarization of Diverse Perspectives
Authors: Yusen Zhang, Nan Zhang, Yixin Liu, Alexander Fabbri, Junru Liu, Ryo
  Kamoi, Xiaoxin Lu, Caiming Xiong, Jieyu Zhao, Dragomir Radev, Kathleen
  McKeown, Rui Zhang
Categories: cs.CL
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2311.07884 ,  8169kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08572
replaced with revised version Sun, 31 Mar 2024 17:01:34 GMT   (152kb,D)

Title: Low-Rank Adaptation for Multilingual Summarization: An Empirical Study
Authors: Chenxi Whitehouse, Fantine Huot, Jasmijn Bastings, Mostafa Dehghani,
  Chu-Cheng Lin, Mirella Lapata
Categories: cs.CL cs.AI cs.LG
Comments: Findings of NAACL 2024
\\ ( https://arxiv.org/abs/2311.08572 ,  152kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08685
replaced with revised version Sun, 31 Mar 2024 22:42:03 GMT   (270kb,D)

Title: Safer-Instruct: Aligning Language Models with Automated Preference Data
Authors: Taiwei Shi, Kai Chen, Jieyu Zhao
Categories: cs.CL cs.AI
Comments: 16 pages. NAACL 2024 Camera-ready
\\ ( https://arxiv.org/abs/2311.08685 ,  270kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08968
replaced with revised version Fri, 29 Mar 2024 22:14:30 GMT   (9306kb,D)

Title: Identifying Linear Relational Concepts in Large Language Models
Authors: David Chanin, Anthony Hunter, Oana-Maria Camburu
Categories: cs.CL cs.AI
Comments: To be published in NAACL 2024
\\ ( https://arxiv.org/abs/2311.08968 ,  9306kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09117
replaced with revised version Mon, 1 Apr 2024 15:04:42 GMT   (2493kb,D)

Title: R-Spin: Efficient Speaker and Noise-invariant Representation Learning
  with Acoustic Pieces
Authors: Heng-Jui Chang, James Glass
Categories: cs.CL cs.SD eess.AS
Comments: Accepted to NAACL 2024
\\ ( https://arxiv.org/abs/2311.09117 ,  2493kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09174
replaced with revised version Mon, 1 Apr 2024 16:24:24 GMT   (804kb,D)

Title: AbsPyramid: Benchmarking the Abstraction Ability of Language Models with
  a Unified Entailment Graph
Authors: Zhaowei Wang, Haochen Shi, Weiqi Wang, Tianqing Fang, Hongming Zhang,
  Sehyun Choi, Xin Liu, Yangqiu Song
Categories: cs.CL cs.AI
Comments: Findings of NAACL2024
\\ ( https://arxiv.org/abs/2311.09174 ,  804kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09476
replaced with revised version Sun, 31 Mar 2024 20:58:46 GMT   (658kb,D)

Title: ARES: An Automated Evaluation Framework for Retrieval-Augmented
  Generation Systems
Authors: Jon Saad-Falcon, Omar Khattab, Christopher Potts, Matei Zaharia
Categories: cs.CL cs.AI cs.IR
Comments: NAACL 2024
\\ ( https://arxiv.org/abs/2311.09476 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09818
replaced with revised version Mon, 1 Apr 2024 07:47:54 GMT   (10965kb,D)

Title: SMILE: Multimodal Dataset for Understanding Laughter in Video with
  Language Models
Authors: Lee Hyun, Kim Sung-Bin, Seungju Han, Youngjae Yu, Tae-Hyun Oh
Categories: cs.CL cs.AI
Comments: 19 pages, 14 figures
\\ ( https://arxiv.org/abs/2312.09818 ,  10965kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11511
replaced with revised version Sat, 30 Mar 2024 03:01:42 GMT   (573kb,D)

Title: ComplexityNet: Increasing LLM Inference Efficiency by Learning Task
  Complexity
Authors: Henry Bae, Aghyad Deeb, Alex Fleury, Kehang Zhu
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2312.11511 ,  573kb)
------------------------------------------------------------------------------
\\
arXiv:2312.11517
replaced with revised version Sat, 30 Mar 2024 21:14:37 GMT   (1151kb,D)

Title: A Natural Language Processing-Based Classification and Mode-Based
  Ranking of Musculoskeletal Disorder Risk Factors
Authors: Md Abrar Jahin and Subrata Talapatra
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2312.11517 ,  1151kb)
------------------------------------------------------------------------------
\\
arXiv:2312.17044
replaced with revised version Mon, 1 Apr 2024 03:03:54 GMT   (6589kb,D)

Title: Length Extrapolation of Transformers: A Survey from the Perspective of
  Position Encoding
Authors: Liang Zhao, Xiaocheng Feng, Xiachong Feng, Bing Qin, Ting Liu
Categories: cs.CL
Comments: Work in progress
\\ ( https://arxiv.org/abs/2312.17044 ,  6589kb)
------------------------------------------------------------------------------
\\
arXiv:2401.05952
replaced with revised version Sat, 30 Mar 2024 09:15:50 GMT   (2383kb,D)

Title: LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be
  Detected?
Authors: Qihui Zhang, Chujie Gao, Dongping Chen, Yue Huang, Yixin Huang,
  Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, Lichao Sun
Categories: cs.CL
Comments: Accepted by NAACL 2024
\\ ( https://arxiv.org/abs/2401.05952 ,  2383kb)
------------------------------------------------------------------------------
\\
arXiv:2401.06877
replaced with revised version Fri, 29 Mar 2024 18:27:17 GMT   (364kb,D)

Title: Promptly Predicting Structures: The Return of Inference
Authors: Maitrey Mehta, Valentina Pyatkin, Vivek Srikumar
Categories: cs.CL
Comments: 19 pages, 13 figures Accepted to NAACL'2024 (Main)
\\ ( https://arxiv.org/abs/2401.06877 ,  364kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08565
replaced with revised version Mon, 1 Apr 2024 17:33:49 GMT   (6533kb,D)

Title: Tuning Language Models by Proxy
Authors: Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi,
  Noah A. Smith
Categories: cs.CL
Comments: new section on proxy-tuning GPT-3.5, additional experiments &
  analysis in appendix
\\ ( https://arxiv.org/abs/2401.08565 ,  6533kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10471
replaced with revised version Mon, 1 Apr 2024 16:12:50 GMT   (1732kb,D)

Title: DeepEdit: Knowledge Editing as Decoding with Constraints
Authors: Yiwei Wang, Muhao Chen, Nanyun Peng, Kai-Wei Chang
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2401.10471 ,  1732kb)
------------------------------------------------------------------------------
\\
arXiv:2401.10472
replaced with revised version Mon, 1 Apr 2024 03:06:40 GMT   (1531kb,D)

Title: Named Entity Recognition Under Domain Shift via Metric Learning for Life
  Sciences
Authors: Hongyi Liu, Qingyun Wang, Payam Karisani, Heng Ji
Categories: cs.CL
Comments: 21 pages; Accepted by the 2024 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies; Code, data, and resources are publicly available for research
  purposes: https://github.com/Lhtie/Bio-Domain-Transfer
\\ ( https://arxiv.org/abs/2401.10472 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12987
replaced with revised version Sun, 31 Mar 2024 09:55:51 GMT   (2708kb,D)

Title: TelME: Teacher-leading Multimodal Fusion Network for Emotion Recognition
  in Conversation
Authors: Taeyang Yun, Hyunkuk Lim, Jeonghwan Lee, Min Song
Categories: cs.CL cs.LG cs.SD eess.AS
Comments: NAACL 2024 main conference
\\ ( https://arxiv.org/abs/2401.12987 ,  2708kb)
------------------------------------------------------------------------------
\\
arXiv:2401.14295
replaced with revised version Sat, 30 Mar 2024 16:33:36 GMT   (2290kb,D)

Title: Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of
  Thoughts
Authors: Maciej Besta, Florim Memedi, Zhenyu Zhang, Robert Gerstenberger,
  Guangyuan Piao, Nils Blach, Piotr Nyczyk, Marcin Copik, Grzegorz
  Kwa\'sniewski, J\"urgen M\"uller, Lukas Gianinazzi, Ales Kubicek, Hubert
  Niewiadomski, Aidan O'Mahony, Onur Mutlu, Torsten Hoefler
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.14295 ,  2290kb)
------------------------------------------------------------------------------
\\
arXiv:2401.15043
replaced with revised version Fri, 29 Mar 2024 19:01:27 GMT   (16965kb,D)

Title: Health Text Simplification: An Annotated Corpus for Digestive Cancer
  Education and Novel Strategies for Reinforcement Learning
Authors: Md Mushfiqur Rahman, Mohammad Sabik Irbaz, Kai North, Michelle S.
  Williams, Marcos Zampieri, Kevin Lybarger
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2401.15043 ,  16965kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04616
replaced with revised version Mon, 1 Apr 2024 01:28:48 GMT   (1043kb,D)

Title: TinyLLM: Learning a Small Student from Multiple Large Language Models
Authors: Yijun Tian, Yikun Han, Xiusi Chen, Wei Wang, Nitesh V. Chawla
Categories: cs.CL cs.AI cs.LG
\\ ( https://arxiv.org/abs/2402.04616 ,  1043kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08498
replaced with revised version Sat, 30 Mar 2024 15:08:39 GMT   (1077kb)

Title: Auditing Counterfire: Evaluating Advanced Counterargument Generation
  with Evidence and Style
Authors: Preetika Verma, Kokil Jaidka, Svetlana Churina
Categories: cs.CL
Comments: 19 pages, 10 figures, 11 tables
\\ ( https://arxiv.org/abs/2402.08498 ,  1077kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09394
replaced with revised version Fri, 29 Mar 2024 21:17:23 GMT   (3081kb,D)

Title: Long-form evaluation of model editing
Authors: Domenic Rosati, Robie Gonzales, Jinkun Chen, Xuemin Yu, Melis Erkan,
  Yahya Kayani, Satya Deepika Chavatapalli, Frank Rudzicz, Hassan Sajjad
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.09394 ,  3081kb)
------------------------------------------------------------------------------
\\
arXiv:2402.10038
replaced with revised version Sat, 30 Mar 2024 16:10:47 GMT   (1062kb,D)

Title: RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization
  Method for Alignment of Large Language Models
Authors: Saeed Khaki, JinJin Li, Lan Ma, Liu Yang, Prathap Ramachandra
Categories: cs.CL cs.AI cs.CV cs.LG
Comments: 16 pages, 4 figures
\\ ( https://arxiv.org/abs/2402.10038 ,  1062kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11271
replaced with revised version Sat, 30 Mar 2024 22:05:59 GMT   (14743kb,D)

Title: MONAL: Model Autophagy Analysis for Modeling Human-AI Interactions
Authors: Shu Yang, Muhammad Asif Ali, Lu Yu, Lijie Hu, and Di Wang
Categories: cs.CL cs.CY cs.HC
\\ ( https://arxiv.org/abs/2402.11271 ,  14743kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11863
replaced with revised version Mon, 1 Apr 2024 08:33:11 GMT   (9765kb,D)

Title: How Interpretable are Reasoning Explanations from Prompting Large
  Language Models?
Authors: Wei Jie Yeo, Ranjan Satapathy, Rick Siow Mong Goh, Erik Cambria
Categories: cs.CL
Comments: NAACL Findings 2024
\\ ( https://arxiv.org/abs/2402.11863 ,  9765kb)
------------------------------------------------------------------------------
\\
arXiv:2402.13249
replaced with revised version Sun, 31 Mar 2024 15:30:34 GMT   (10253kb,D)

Title: TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue
  Summarization
Authors: Liyan Tang, Igor Shalyminov, Amy Wing-mei Wong, Jon Burnsky, Jake W.
  Vincent, Yu'an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia
  Sun, Yi Zhang, Saab Mansour, Kathleen McKeown
Categories: cs.CL cs.AI
Comments: NAACL 2024; Linguistic annotations available at
  https://github.com/amazon-science/tofueval
\\ ( https://arxiv.org/abs/2402.13249 ,  10253kb)
------------------------------------------------------------------------------
\\
arXiv:2402.14320
replaced with revised version Mon, 1 Apr 2024 07:46:01 GMT   (150kb,D)

Title: Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve
  Knowledge Base Question Answering
Authors: Chang Zong, Yuchen Yan, Weiming Lu, Eliot Huang, Jian Shao, Yueting
  Zhuang
Categories: cs.CL cs.AI
Comments: 8 pages
MSC-class: 68T50
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2402.14320 ,  150kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16671
replaced with revised version Sun, 31 Mar 2024 20:14:20 GMT   (1141kb,D)

Title: StructLM: Towards Building Generalist Models for Structured Knowledge
  Grounding
Authors: Alex Zhuang, Ge Zhang, Tianyu Zheng, Xinrun Du, Junjie Wang, Weiming
  Ren, Stephen W. Huang, Jie Fu, Xiang Yue, Wenhu Chen
Categories: cs.CL
\\ ( https://arxiv.org/abs/2402.16671 ,  1141kb)
------------------------------------------------------------------------------
\\
arXiv:2402.17916
replaced with revised version Sat, 30 Mar 2024 04:16:20 GMT   (4508kb,D)

Title: LLM-Resistant Math Word Problem Generation via Adversarial Attacks
Authors: Roy Xie, Chengxuan Huang, Junlin Wang, Bhuwan Dhingra
Categories: cs.CL cs.AI
Comments: Code/data: https://github.com/ruoyuxie/adversarial_mwps_generation
\\ ( https://arxiv.org/abs/2402.17916 ,  4508kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00813
replaced with revised version Sun, 31 Mar 2024 06:32:03 GMT   (606kb,D)

Title: UrbanGPT: Spatio-Temporal Large Language Models
Authors: Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia,
  Dawei Yin, Chao Huang
Categories: cs.CL cs.AI cs.CY
Comments: 11 pages
\\ ( https://arxiv.org/abs/2403.00813 ,  606kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01139
replaced with revised version Sat, 30 Mar 2024 15:40:03 GMT   (4489kb,D)

Title: ParallelPARC: A Scalable Pipeline for Generating Natural-Language
  Analogies
Authors: Oren Sultan, Yonatan Bitton, Ron Yosef, Dafna Shahaf
Categories: cs.CL cs.AI
Comments: NAACL 2024 main
\\ ( https://arxiv.org/abs/2403.01139 ,  4489kb)
------------------------------------------------------------------------------
\\
arXiv:2403.04801
replaced with revised version Sun, 31 Mar 2024 04:33:56 GMT   (3386kb,D)

Title: Alpaca against Vicuna: Using LLMs to Uncover Memorization of LLMs
Authors: Aly M. Kassem, Omar Mahmoud, Niloofar Mireshghallah, Hyunwoo Kim,
  Yulia Tsvetkov, Yejin Choi, Sherif Saad, Santu Rana
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.04801 ,  3386kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05766
replaced with revised version Sun, 31 Mar 2024 19:45:22 GMT   (7368kb,D)

Title: FLAP: Flow-Adhering Planning with Constrained Decoding in LLMs
Authors: Shamik Roy, Sailik Sengupta, Daniele Bonadiman, Saab Mansour, Arshit
  Gupta
Categories: cs.CL
Comments: NAACL 2024 (Camera Ready)
\\ ( https://arxiv.org/abs/2403.05766 ,  7368kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07378
replaced with revised version Mon, 1 Apr 2024 15:04:15 GMT   (382kb,D)

Title: SVD-LLM: Truncation-aware Singular Value Decomposition for Large
  Language Model Compression
Authors: Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang
Categories: cs.CL cs.LG
Comments: Work in progress; Code: https://github.com/AIoT-MLSys-Lab/SVD-LLM
\\ ( https://arxiv.org/abs/2403.07378 ,  382kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07440
replaced with revised version Sat, 30 Mar 2024 04:36:54 GMT   (1385kb,D)

Title: Matrix-Transformation Based Low-Rank Adaptation (MTLoRA): A
  Brain-Inspired Method for Parameter-Efficient Fine-Tuning
Authors: Yao Liang, Yuwei Wang, Yang Li, Yi Zeng
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.07440 ,  1385kb)
------------------------------------------------------------------------------
\\
arXiv:2403.08994
replaced with revised version Mon, 1 Apr 2024 05:45:47 GMT   (297kb,D)

Title: Ethos: Rectifying Language Models in Orthogonal Parameter Space
Authors: Lei Gao, Yue Niu, Tingting Tang, Salman Avestimehr, Murali Annavaram
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.08994 ,  297kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09522
replaced with revised version Mon, 1 Apr 2024 07:11:32 GMT   (8573kb,D)

Title: MT-PATCHER: Selective and Extendable Knowledge Distillation from Large
  Language Models for Machine Translation
Authors: Jiahuan Li, Shanbo Cheng, Shujian Huang and Jiajun Chen
Categories: cs.CL
Comments: Accepted to NAACL-2024 main conference
\\ ( https://arxiv.org/abs/2403.09522 ,  8573kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11124
replaced with revised version Sat, 30 Mar 2024 16:48:16 GMT   (203kb,D)

Title: Scaling Data Diversity for Fine-Tuning Language Models in Human
  Alignment
Authors: Feifan Song, Bowen Yu, Hao Lang, Haiyang Yu, Fei Huang, Houfeng Wang,
  Yongbin Li
Categories: cs.CL cs.AI
Comments: Accepted by LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.11124 ,  203kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11369
replaced with revised version Mon, 1 Apr 2024 13:58:34 GMT   (9700kb,D)

Title: What Makes Math Word Problems Challenging for LLMs?
Authors: KV Aditya Srivatsa and Ekaterina Kochmar
Categories: cs.CL
Comments: Accepted to NAACL Findings 2024
\\ ( https://arxiv.org/abs/2403.11369 ,  9700kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11399
replaced with revised version Mon, 1 Apr 2024 06:57:20 GMT   (12284kb,D)

Title: X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment
Authors: Dongjae Shin, Hyeonseok Lim, Inho Won, Changsu Choi, Minjun Kim,
  Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.11399 ,  12284kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15278
replaced with revised version Fri, 29 Mar 2024 22:43:37 GMT   (618kb,D)

Title: Specifying Genericity through Inclusiveness and Abstractness Continuous
  Scales
Authors: Claudia Collacciani, Andrea Amelio Ravelli, Marianna Marcella
  Bolognesi
Categories: cs.CL
Comments: Accepted at LREC-COLING 2024
\\ ( https://arxiv.org/abs/2403.15278 ,  618kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15351
replaced with revised version Sun, 31 Mar 2024 23:23:46 GMT   (8553kb,D)

Title: Multi-Review Fusion-in-Context
Authors: Aviv Slobodkin, Ori Shapira, Ran Levy, Ido Dagan
Categories: cs.CL
Comments: NAACL 2024, findings
\\ ( https://arxiv.org/abs/2403.15351 ,  8553kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15615
replaced with revised version Mon, 1 Apr 2024 15:18:59 GMT   (1289kb)

Title: NaturalTurn: A Method to Segment Transcripts into Naturalistic
  Conversational Turns
Authors: Gus Cooney and Andrew Reece
Categories: cs.CL
Comments: 37 pages, 5 figures
\\ ( https://arxiv.org/abs/2403.15615 ,  1289kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15796
replaced with revised version Sat, 30 Mar 2024 09:55:12 GMT   (354kb,D)

Title: Understanding Emergent Abilities of Language Models from the Loss
  Perspective
Authors: Zhengxiao Du, Aohan Zeng, Yuxiao Dong, Jie Tang
Categories: cs.CL cs.AI cs.LG
Comments: 18 pages, 6 figures
\\ ( https://arxiv.org/abs/2403.15796 ,  354kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17104
replaced with revised version Mon, 1 Apr 2024 17:57:40 GMT   (8450kb,D)

Title: Attribute First, then Generate: Locally-attributable Grounded Text
  Generation
Authors: Aviv Slobodkin, Eran Hirsch, Arie Cattan, Tal Schuster, Ido Dagan
Categories: cs.CL
\\ ( https://arxiv.org/abs/2403.17104 ,  8450kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17407
replaced with revised version Mon, 1 Apr 2024 13:27:41 GMT   (2083kb,D)

Title: Transcribing Bengali Text with Regional Dialects to IPA using District
  Guided Tokens
Authors: S M Jishanul Islam, Sadia Ahmmed and Sahid Hossain Mustakim
Categories: cs.CL cs.AI cs.LG
Comments: This work became the champion of the Bhashamul challenge
ACM-class: F.2.2; I.2.7
\\ ( https://arxiv.org/abs/2403.17407 ,  2083kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18314
replaced with revised version Fri, 29 Mar 2024 18:48:35 GMT   (8834kb,D)

Title: Chinese Offensive Language Detection:Current Status and Future
  Directions
Authors: Yunze Xiao, Houda Bouamor and Wajdi Zaghouani
Categories: cs.CL cs.AI
\\ ( https://arxiv.org/abs/2403.18314 ,  8834kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18647
replaced with revised version Mon, 1 Apr 2024 09:48:46 GMT   (5384kb,D)

Title: SDSAT: Accelerating LLM Inference through Speculative Decoding with
  Semantic Adaptive Tokens
Authors: Chengbo Liu, Yong Zhu
Categories: cs.CL
Comments: 12 pages, 7 figures
\\ ( https://arxiv.org/abs/2403.18647 ,  5384kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18933
replaced with revised version Mon, 1 Apr 2024 14:58:44 GMT   (5257kb,D)

Title: SemEval Task 1: Semantic Textual Relatedness for African and Asian
  Languages
Authors: Nedjma Ousidhoum, Shamsuddeen Hassan Muhammad, Mohamed Abdalla, Idris
  Abdulmumin, Ibrahim Said Ahmad, Sanchit Ahuja, Alham Fikri Aji, Vladimir
  Araujo, Meriem Beloucif, Christine De Kock, Oumaima Hourrane, Manish
  Shrivastava, Thamar Solorio, Nirmal Surange, Krishnapriya Vishnubhotla, Seid
  Muhie Yimam, Saif M. Mohammad
Categories: cs.CL
Comments: SemEval 2024 Task Description Paper. arXiv admin note: text overlap
  with arXiv:2402.08638
\\ ( https://arxiv.org/abs/2403.18933 ,  5257kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19135
replaced with revised version Sun, 31 Mar 2024 08:16:58 GMT   (217kb,D)

Title: Compressing Large Language Models by Streamlining the Unimportant Layer
Authors: Xiaodong Chen, Yuxuan Hu, Jing Zhang
Categories: cs.CL cs.AI
ACM-class: I.2.7
\\ ( https://arxiv.org/abs/2403.19135 ,  217kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19318
replaced with revised version Mon, 1 Apr 2024 05:10:56 GMT   (3244kb,D)

Title: TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office
  Usage Scenarios
Authors: Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin
  Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao,
  Juanzi Li, Jie Tang
Categories: cs.CL
Comments: https://tablellm.github.io/
\\ ( https://arxiv.org/abs/2403.19318 ,  3244kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19713
replaced with revised version Mon, 1 Apr 2024 14:16:42 GMT   (822kb)

Title: NJUST-KMG at TRAC-2024 Tasks 1 and 2: Offline Harm Potential
  Identification
Authors: Jingyuan Wang, Shengdong Xu, Yang Yang
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.19713 ,  822kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19928
replaced with revised version Mon, 1 Apr 2024 09:17:01 GMT   (252kb,D)

Title: DiJiang: Efficient Large Language Models through Compact Kernelization
Authors: Hanting Chen, Zhicheng Liu, Xutao Wang, Yuchuan Tian, Yunhe Wang
Categories: cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.19928 ,  252kb)
------------------------------------------------------------------------------
\\
arXiv:1912.05830
replaced with revised version Mon, 1 Apr 2024 00:56:31 GMT   (82kb)

Title: Provably Efficient Exploration in Policy Optimization
Authors: Qi Cai, Zhuoran Yang, Chi Jin, Zhaoran Wang
Categories: cs.LG math.OC stat.ML
Comments: We have fixed a technical issue in the first version of this paper.
  We remark the technical assumption of the linear MDP in this version of the
  paper is different from that in the first version
\\ ( https://arxiv.org/abs/1912.05830 ,  82kb)
------------------------------------------------------------------------------
\\
arXiv:2006.04761
replaced with revised version Mon, 1 Apr 2024 04:03:28 GMT   (54kb)

Title: Can Temporal-Difference and Q-Learning Learn Representation? A
  Mean-Field Theory
Authors: Yufeng Zhang, Qi Cai, Zhuoran Yang, Yongxin Chen, Zhaoran Wang
Categories: cs.LG math.OC stat.ML
Comments: add acknowledgement
\\ ( https://arxiv.org/abs/2006.04761 ,  54kb)
------------------------------------------------------------------------------
\\
arXiv:2012.11554
replaced with revised version Mon, 1 Apr 2024 03:56:23 GMT   (286kb)

Title: Variational Transport: A Convergent Particle-BasedAlgorithm for
  Distributional Optimization
Authors: Zhuoran Yang, Yufeng Zhang, Yongxin Chen, Zhaoran Wang
Categories: cs.LG math.OC math.ST stat.ML stat.TH
Comments: 58 pages, add acknowledgement
\\ ( https://arxiv.org/abs/2012.11554 ,  286kb)
------------------------------------------------------------------------------
\\
arXiv:2101.00746
replaced with revised version Mon, 1 Apr 2024 03:12:01 GMT   (12414kb,D)

Title: MetaVIM: Meta Variationally Intrinsic Motivated Reinforcement Learning
  for Decentralized Traffic Signal Control
Authors: Liwen Zhu, Peixi Peng, Zongqing Lu, Xiangqian Wang, Yonghong Tian
Categories: cs.LG cs.AI cs.MA
\\ ( https://arxiv.org/abs/2101.00746 ,  12414kb)
------------------------------------------------------------------------------
\\
arXiv:2105.11866
replaced with revised version Mon, 1 Apr 2024 03:36:20 GMT   (9923kb,D)

Title: GraphFM: Graph Factorization Machines for Feature Interaction Modeling
Authors: Shu Wu, Zekun Li, Yunyue Su, Zeyu Cui, Xiaoyu Zhang, Liang Wang
Categories: cs.LG cs.AI cs.IR
Comments: The code and data are available at
  https://github.com/CRIPAC-DIG/GraphCTR
\\ ( https://arxiv.org/abs/2105.11866 ,  9923kb)
------------------------------------------------------------------------------
\\
arXiv:2111.03262
replaced with revised version Mon, 1 Apr 2024 15:14:06 GMT   (779kb,D)

Title: CGCL: Collaborative Graph Contrastive Learning without Handcrafted Graph
  Data Augmentations
Authors: Tianyu Zhang and Yuxiang Ren and Wenzheng Feng and Weitao Du and
  Xuecang Zhang
Categories: cs.LG cs.AI
Comments: Accepted by DASFAA'24
\\ ( https://arxiv.org/abs/2111.03262 ,  779kb)
------------------------------------------------------------------------------
\\
arXiv:2112.13530
replaced with revised version Mon, 1 Apr 2024 04:09:22 GMT   (108kb)

Title: Wasserstein Flow Meets Replicator Dynamics: A Mean-Field Analysis of
  Representation Learning in Actor-Critic
Authors: Yufeng Zhang, Siyu Chen, Zhuoran Yang, Michael I. Jordan, Zhaoran Wang
Categories: cs.LG math.OC stat.ML
Comments: 41 pages, accepted to NeurIPS 2021, add acknowledgement
\\ ( https://arxiv.org/abs/2112.13530 ,  108kb)
------------------------------------------------------------------------------
\\
arXiv:2201.03172
replaced with revised version Mon, 1 Apr 2024 10:37:53 GMT   (13460kb,D)

Title: Communication-Efficient Federated Learning with Accelerated Client
  Gradient
Authors: Geeho Kim, Jinkyu Kim, Bohyung Han
Categories: cs.LG cs.AI
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2201.03172 ,  13460kb)
------------------------------------------------------------------------------
\\
arXiv:2202.13046
replaced with revised version Sun, 31 Mar 2024 01:49:22 GMT   (2730kb,D)

Title: Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced
  Local Value Functions
Authors: Gangshan Jing, He Bai, Jemin George, Aranya Chakrabortty, Piyush K.
  Sharma
Categories: cs.LG cs.AI cs.MA
Comments: This paper proposes a graph-induced distributed RL framework for
  seeking the optimal distributed control of multi-agent systems. It has been
  accepted by IEEE Transactions on Automatic Control as a full paper
\\ ( https://arxiv.org/abs/2202.13046 ,  2730kb)
------------------------------------------------------------------------------
\\
arXiv:2204.09787
replaced with revised version Mon, 1 Apr 2024 00:46:06 GMT   (68kb)

Title: Reinforcement Learning from Partial Observation: Linear Function
  Approximation with Provable Sample Efficiency
Authors: Qi Cai, Zhuoran Yang, Zhaoran Wang
Categories: cs.LG math.OC stat.ML
\\ ( https://arxiv.org/abs/2204.09787 ,  68kb)
------------------------------------------------------------------------------
\\
arXiv:2205.10490
replaced with revised version Sat, 30 Mar 2024 08:52:40 GMT   (13845kb,D)

Title: Aligning Logits Generatively for Principled Black-Box Knowledge
  Distillation
Authors: Jing Ma, Xiang Xiang, Ke Wang, Yuchuan Wu, Yongbin Li
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: To appear at CVPR 2024; significantly rewritten with extra
  experiments since the preliminary report
\\ ( https://arxiv.org/abs/2205.10490 ,  13845kb)
------------------------------------------------------------------------------
\\
arXiv:2205.11720
replaced with revised version Sat, 30 Mar 2024 15:27:22 GMT   (10324kb,D)

Title: ENS-t-SNE: Embedding Neighborhoods Simultaneously t-SNE
Authors: Jacob Miller, Vahan Huroyan, Raymundo Navarrete, Md Iqbal Hossain,
  Stephen Kobourov
Categories: cs.LG cs.DS cs.HC
\\ ( https://arxiv.org/abs/2205.11720 ,  10324kb)
------------------------------------------------------------------------------
\\
arXiv:2205.13476
replaced with revised version Mon, 1 Apr 2024 01:53:31 GMT   (58kb)

Title: Embed to Control Partially Observed Systems: Representation Learning
  with Provable Sample Efficiency
Authors: Lingxiao Wang, Qi Cai, Zhuoran Yang, Zhaoran Wang
Categories: cs.LG cs.AI cs.SY eess.SY stat.ML
Comments: Accepted by ICLR 2022
\\ ( https://arxiv.org/abs/2205.13476 ,  58kb)
------------------------------------------------------------------------------
\\
arXiv:2205.13589
replaced with revised version Mon, 1 Apr 2024 04:49:15 GMT   (76kb)

Title: Pessimism in the Face of Confounders: Provably Efficient Offline
  Reinforcement Learning in Partially Observable Markov Decision Processes
Authors: Miao Lu, Yifei Min, Zhaoran Wang, Zhuoran Yang
Categories: cs.LG cs.AI math.ST stat.ME stat.ML stat.TH
Comments: Updates. 52 pages
\\ ( https://arxiv.org/abs/2205.13589 ,  76kb)
------------------------------------------------------------------------------
\\
arXiv:2207.09237
replaced with revised version Sat, 30 Mar 2024 11:55:26 GMT   (2565kb,D)

Title: Semi-supervised Predictive Clustering Trees for (Hierarchical)
  Multi-label Classification
Authors: Jurica Levati\'c, Michelangelo Ceci, Dragi Kocev, Sa\v{s}o
  D\v{z}eroski
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2207.09237 ,  2565kb)
------------------------------------------------------------------------------
\\
arXiv:2208.02649
replaced with revised version Sat, 30 Mar 2024 02:49:29 GMT   (0kb,I)

Title: Visually Evaluating Generative Adversarial Networks Using Itself under
  Multivariate Time Series
Authors: Qilong Pan
Categories: cs.LG eess.SP stat.AP
Comments: This is just a manuscript draft where the experiment is not evident,
  and need to be studied further
\\ ( https://arxiv.org/abs/2208.02649 ,  0kb)
------------------------------------------------------------------------------
\\
arXiv:2208.14161
replaced with revised version Sun, 31 Mar 2024 23:09:38 GMT   (2874kb,D)

Title: Identifiable Latent Causal Content for Domain Adaptation under Latent
  Covariate Shift
Authors: Yuhang Liu, Zhen Zhang, Dong Gong, Mingming Gong, Biwei Huang, Anton
  van den Hengel, Kun Zhang, Javen Qinfeng Shi
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2208.14161 ,  2874kb)
------------------------------------------------------------------------------
\\
arXiv:2210.06554
replaced with revised version Sat, 30 Mar 2024 14:56:26 GMT   (2588kb,D)

Title: Toward the application of XAI methods in EEG-based systems
Authors: Andrea Apicella, Francesco Isgr\`o, Andrea Pollastro, Roberto Prevete
Categories: cs.LG cs.AI eess.SP
Comments: Accepted to be presented at XAI.it 2022 - Italian Workshop on
  Explainable Artificial Intelligence (https://ceur-ws.org/Vol-3277/paper1.pdf)
\\ ( https://arxiv.org/abs/2210.06554 ,  2588kb)
------------------------------------------------------------------------------
\\
arXiv:2210.09903
replaced with revised version Fri, 29 Mar 2024 18:25:28 GMT   (595kb,D)

Title: Online Convex Optimization with Unbounded Memory
Authors: Raunak Kumar, Sarah Dean, and Robert Kleinberg
Categories: cs.LG math.OC stat.ML
Comments: Proceedings of the 37th Conference on Neural Information Processing
  Systems (NeurIPS 2023)
\\ ( https://arxiv.org/abs/2210.09903 ,  595kb)
------------------------------------------------------------------------------
\\
arXiv:2210.16863
replaced with revised version Mon, 1 Apr 2024 09:13:22 GMT   (956kb,D)

Title: Time-aware Metapath Feature Augmentation for Ponzi Detection in Ethereum
Authors: Chengxiang Jin, Jiajun Zhou, Jie Jin, Jiajing Wu, Qi Xuan
Categories: cs.LG q-fin.ST
Comments: Accepted by IEEE Transactions on Network Science and Engineering
DOI: 10.1109/TNSE.2024.3384499
\\ ( https://arxiv.org/abs/2210.16863 ,  956kb)
------------------------------------------------------------------------------
\\
arXiv:2212.04612
replaced with revised version Fri, 29 Mar 2024 21:33:29 GMT   (321kb,D)

Title: Training Data Influence Analysis and Estimation: A Survey
Authors: Zayd Hammoudeh, Daniel Lowd
Categories: cs.LG
Comments: Published in Springer journal "Machine Learning"
Journal-ref: Machine Learning (2024)
DOI: 10.1007/s10994-023-06495-7
\\ ( https://arxiv.org/abs/2212.04612 ,  321kb)
------------------------------------------------------------------------------
\\
arXiv:2212.14852
replaced with revised version Mon, 1 Apr 2024 03:51:06 GMT   (883kb,D)

Title: An Analysis of Attention via the Lens of Exchangeability and Latent
  Variable Models
Authors: Yufeng Zhang, Boyi Liu, Qi Cai, Lingxiao Wang, Zhaoran Wang
Categories: cs.LG
Comments: 85 pages, 7 figures, add acknowledgement
\\ ( https://arxiv.org/abs/2212.14852 ,  883kb)
------------------------------------------------------------------------------
\\
arXiv:2302.11774
replaced with revised version Sun, 31 Mar 2024 09:09:16 GMT   (1371kb,D)

Title: Semantic-Fused Multi-Granularity Cross-City Traffic Prediction
Authors: Kehua Chen, Yuxuan Liang, Jindong Han, Siyuan Feng, Meixin Zhu, Hai
  Yang
Categories: cs.LG
\\ ( https://arxiv.org/abs/2302.11774 ,  1371kb)
------------------------------------------------------------------------------
\\
arXiv:2303.00720
replaced with revised version Sat, 30 Mar 2024 21:13:14 GMT   (5879kb)

Title: Cross-Modal Entity Matching for Visually Rich Documents
Authors: Ritesh Sarkhel, Arnab Nandi
Categories: cs.LG cs.DB cs.IR
\\ ( https://arxiv.org/abs/2303.00720 ,  5879kb)
------------------------------------------------------------------------------
\\
arXiv:2303.05101
replaced with revised version Sun, 31 Mar 2024 22:58:36 GMT   (2223kb,D)

Title: Scalable Stochastic Gradient Riemannian Langevin Dynamics in
  Non-Diagonal Metrics
Authors: Hanlin Yu, Marcelo Hartmann, Bernardo Williams and Arto Klami
Categories: cs.LG stat.CO
Comments: Adjust the template and minor fixes
\\ ( https://arxiv.org/abs/2303.05101 ,  2223kb)
------------------------------------------------------------------------------
\\
arXiv:2304.13061
replaced with revised version Mon, 1 Apr 2024 06:42:17 GMT   (5963kb,D)

Title: iMixer: hierarchical Hopfield network implies an invertible, implicit
  and iterative MLP-Mixer
Authors: Toshihiro Ota, Masato Taki
Categories: cs.LG cond-mat.dis-nn cs.CV cs.NE
Comments: 19 pages. v2: minor improvements
\\ ( https://arxiv.org/abs/2304.13061 ,  5963kb)
------------------------------------------------------------------------------
\\
arXiv:2305.11584
replaced with revised version Mon, 1 Apr 2024 04:21:28 GMT   (1857kb,D)

Title: Dynamic Regularized Sharpness Aware Minimization in Federated Learning:
  Approaching Global Consistency and Smooth Landscape
Authors: Yan Sun, Li Shen, Shixiang Chen, Liang Ding, Dacheng Tao
Categories: cs.LG cs.DC math.OC
Comments: ICML2023, Oral Presentation
Journal-ref: PMLR 202:32991-33013, 2023
\\ ( https://arxiv.org/abs/2305.11584 ,  1857kb)
------------------------------------------------------------------------------
\\
arXiv:2305.14749
replaced with revised version Sun, 31 Mar 2024 10:03:17 GMT   (1679kb,D)

Title: gRNAde: Geometric Deep Learning for 3D RNA inverse design
Authors: Chaitanya K. Joshi, Arian R. Jamasb, Ramon Vi\~nas, Charles Harris,
  Simon Mathis, Alex Morehead, Pietro Li\`o
Categories: cs.LG q-bio.BM q-bio.QM
Comments: Previously titled 'Multi-State RNA Design with Geometric Multi-Graph
  Neural Networks', presented at ICML 2023 Computational Biology Workshop
\\ ( https://arxiv.org/abs/2305.14749 ,  1679kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03506
replaced with revised version Sat, 30 Mar 2024 10:39:13 GMT   (3942kb,D)

Title: Subgraph Networks Based Contrastive Learning
Authors: Jinhuan Wang, Jiafei Shao, Zeyu Wang, Shanqing Yu, Qi Xuan, Xiaoniu
  Yang
Categories: cs.LG cs.AI
Comments: 12 pages, 6 figures
\\ ( https://arxiv.org/abs/2306.03506 ,  3942kb)
------------------------------------------------------------------------------
\\
arXiv:2307.00185
replaced with revised version Fri, 29 Mar 2024 19:10:30 GMT   (1194kb,D)

Title: Interpretable Constructive Algorithm for Random Weight Neural Networks
Authors: Jing Nan, Wei Dai, Guan Yuan, and Ping Zhou
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2307.00185 ,  1194kb)
------------------------------------------------------------------------------
\\
arXiv:2307.05775
replaced with revised version Sun, 31 Mar 2024 17:03:00 GMT   (1704kb,D)

Title: Weisfeiler and Leman Go Measurement Modeling: Probing the Validity of
  the WL Test
Authors: Arjun Subramonian, Adina Williams, Maximilian Nickel, Yizhou Sun,
  Levent Sagun
Categories: cs.LG cs.SI
\\ ( https://arxiv.org/abs/2307.05775 ,  1704kb)
------------------------------------------------------------------------------
\\
arXiv:2307.10438
replaced with revised version Mon, 1 Apr 2024 02:13:37 GMT   (21925kb)

Title: Uncertainty Quantification for Molecular Property Predictions with Graph
  Neural Architecture Search
Authors: Shengli Jiang, Shiyi Qin, Reid C. Van Lehn, Prasanna Balaprakash,
  Victor M. Zavala
Categories: cs.LG physics.chem-ph q-bio.BM
\\ ( https://arxiv.org/abs/2307.10438 ,  21925kb)
------------------------------------------------------------------------------
\\
arXiv:2307.16120
replaced with revised version Sun, 31 Mar 2024 08:40:15 GMT   (5086kb,D)

Title: Deep Unrolling Networks with Recurrent Momentum Acceleration for
  Nonlinear Inverse Problems
Authors: Qingping Zhou, Jiayu Qian, Junqi Tang, Jinglai Li
Categories: cs.LG
MSC-class: 68U10, 94A08, 68T99
\\ ( https://arxiv.org/abs/2307.16120 ,  5086kb)
------------------------------------------------------------------------------
\\
arXiv:2309.12701
replaced with revised version Fri, 29 Mar 2024 21:06:26 GMT   (3020kb,D)

Title: Interpretable Decision Tree Search as a Markov Decision Process
Authors: Hector Kohler, Riad Akrour, Philippe Preux
Categories: cs.LG
\\ ( https://arxiv.org/abs/2309.12701 ,  3020kb)
------------------------------------------------------------------------------
\\
arXiv:2309.14592
replaced with revised version Sun, 31 Mar 2024 23:05:53 GMT   (8257kb,D)

Title: Efficient Post-training Quantization with FP8 Formats
Authors: Haihao Shen, Naveen Mellempudi, Xin He, Qun Gao, Chang Wang, and
  Mengni Wang
Categories: cs.LG cs.AI cs.CL
\\ ( https://arxiv.org/abs/2309.14592 ,  8257kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02279
replaced with revised version Sat, 30 Mar 2024 06:29:48 GMT   (2657kb,D)

Title: Consistency Trajectory Models: Learning Probability Flow ODE Trajectory
  of Diffusion
Authors: Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta
  Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, Stefano Ermon
Categories: cs.LG cs.AI cs.CV stat.ML
Comments: International Conference on Learning Representations
\\ ( https://arxiv.org/abs/2310.02279 ,  2657kb)
------------------------------------------------------------------------------
\\
arXiv:2310.02391
replaced with revised version Sun, 31 Mar 2024 16:38:57 GMT   (45869kb,D)

Title: SE(3)-Stochastic Flow Matching for Protein Backbone Generation
Authors: Avishek Joey Bose, Tara Akhound-Sadegh, Guillaume Huguet, Kilian
  Fatras, Jarrid Rector-Brooks, Cheng-Hao Liu, Andrei Cristian Nica, Maksym
  Korablyov, Michael Bronstein, and Alexander Tong
Categories: cs.LG cs.AI
Comments: ICLR 2024 Spotlight
\\ ( https://arxiv.org/abs/2310.02391 ,  45869kb)
------------------------------------------------------------------------------
\\
arXiv:2310.03294
replaced with revised version Sun, 31 Mar 2024 21:11:08 GMT   (14827kb,D)

Title: DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context
  LLMs Training
Authors: Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Xuezhe Ma, Ion Stoica,
  Joseph E. Gonzalez, Hao Zhang
Categories: cs.LG cs.AI cs.DC
\\ ( https://arxiv.org/abs/2310.03294 ,  14827kb)
------------------------------------------------------------------------------
\\
arXiv:2310.04041
replaced with revised version Mon, 1 Apr 2024 10:42:02 GMT   (42118kb,D)

Title: Observation-Guided Diffusion Probabilistic Models
Authors: Junoh Kang, Jinyoung Choi, Sungik Choi, Bohyung Han
Categories: cs.LG cs.AI
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2310.04041 ,  42118kb)
------------------------------------------------------------------------------
\\
arXiv:2310.09486
replaced with revised version Mon, 1 Apr 2024 03:43:22 GMT   (1970kb,D)

Title: Mirage: Model-Agnostic Graph Distillation for Graph Classification
Authors: Mridul Gupta and Sahil Manchanda and Hariprasad Kodamana and Sayan
  Ranu
Categories: cs.LG cs.AI
Comments: 14 pages, 14 figures
\\ ( https://arxiv.org/abs/2310.09486 ,  1970kb)
------------------------------------------------------------------------------
\\
arXiv:2310.10908
replaced with revised version Mon, 1 Apr 2024 11:37:39 GMT   (1810kb,D)

Title: Unlocking Emergent Modularity in Large Language Models
Authors: Zihan Qiu, Zeyu Huang, Jie Fu
Categories: cs.LG cs.AI
Comments: NAACL2024 Main Conference
Journal-ref: 2024 Annual Conference of the North American Chapter of the
  Association for Computational Linguistics
\\ ( https://arxiv.org/abs/2310.10908 ,  1810kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14770
replaced with revised version Sun, 31 Mar 2024 09:37:14 GMT   (46kb)

Title: Theoretically Grounded Loss Functions and Algorithms for Score-Based
  Multi-Class Abstention
Authors: Anqi Mao, Mehryar Mohri, Yutao Zhong
Categories: cs.LG stat.ML
Comments: AISTATS 2024
\\ ( https://arxiv.org/abs/2310.14770 ,  46kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14772
replaced with revised version Sun, 31 Mar 2024 09:05:24 GMT   (75kb,D)

Title: Predictor-Rejector Multi-Class Abstention: Theoretical Analysis and
  Algorithms
Authors: Anqi Mao, Mehryar Mohri, Yutao Zhong
Categories: cs.LG stat.ML
Comments: ALT 2024
\\ ( https://arxiv.org/abs/2310.14772 ,  75kb)
------------------------------------------------------------------------------
\\
arXiv:2310.14774
replaced with revised version Sun, 31 Mar 2024 09:15:36 GMT   (38kb)

Title: Principled Approaches for Learning to Defer with Multiple Experts
Authors: Anqi Mao, Mehryar Mohri, Yutao Zhong
Categories: cs.LG stat.ML
Comments: ISAIM 2024
\\ ( https://arxiv.org/abs/2310.14774 ,  38kb)
------------------------------------------------------------------------------
\\
arXiv:2310.15333
replaced with revised version Mon, 1 Apr 2024 14:46:17 GMT   (700kb,D)

Title: Safe and Interpretable Estimation of Optimal Treatment Regimes
Authors: Harsh Parikh, Quinn Lanners, Zade Akras, Sahar F. Zafar, M. Brandon
  Westover, Cynthia Rudin, Alexander Volfovsky
Categories: cs.LG stat.AP stat.ME
Comments: Accepted for publication in the proceedings of AISTATS 2025
\\ ( https://arxiv.org/abs/2310.15333 ,  700kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17247
replaced with revised version Mon, 1 Apr 2024 02:54:46 GMT   (15439kb,D)

Title: Grokking Beyond Neural Networks: An Empirical Exploration with Model
  Complexity
Authors: Jack Miller, Charles O'Neill, Thang Bui
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2310.17247 ,  15439kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18715
replaced with revised version Sat, 30 Mar 2024 16:16:56 GMT   (419kb,D)

Title: Robust Offline Reinforcement learning with Heavy-Tailed Rewards
Authors: Jin Zhu, Runzhe Wan, Zhengling Qi, Shikai Luo and Chengchun Shi
Categories: cs.LG cs.AI stat.ML
Comments: 23 pages, 6 figures. Proceedings of the 27th International Conference
  on Artificial Intelligence and Statistics (AISTATS) 2024
\\ ( https://arxiv.org/abs/2310.18715 ,  419kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18743
replaced with revised version Sat, 30 Mar 2024 08:53:49 GMT   (263kb,D)

Title: Optimization of utility-based shortfall risk: A non-asymptotic viewpoint
Authors: Sumedh Gupte, Prashanth L. A., Sanjay P. Bhat
Categories: cs.LG
\\ ( https://arxiv.org/abs/2310.18743 ,  263kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01276
replaced with revised version Sun, 31 Mar 2024 14:28:51 GMT   (15365kb,D)

Title: Neural Atoms: Propagating Long-range Interaction in Molecular Graphs
  through Efficient Communication Channel
Authors: Xuan Li, Zhanke Zhou, Jiangchao Yao, Yu Rong, Lu Zhang, Bo Han
Categories: cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2311.01276 ,  15365kb)
------------------------------------------------------------------------------
\\
arXiv:2311.02766
replaced with revised version Sun, 31 Mar 2024 23:15:10 GMT   (9060kb,D)

Title: Riemannian Laplace Approximation with the Fisher Metric
Authors: Hanlin Yu, Marcelo Hartmann, Bernardo Williams, Mark Girolami, Arto
  Klami
Categories: cs.LG stat.ME stat.ML
Comments: AISTATS 2024, with additional fixes and improvements
\\ ( https://arxiv.org/abs/2311.02766 ,  9060kb)
------------------------------------------------------------------------------
\\
arXiv:2311.03733
replaced with revised version Mon, 1 Apr 2024 09:05:20 GMT   (4031kb,D)

Title: Improved weight initialization for deep and narrow feedforward neural
  network
Authors: Hyunwoo Lee, Yunho Kim, Seung Yeop Yang, Hayoung Choi
Categories: cs.LG cs.NE
Comments: 13 pages
\\ ( https://arxiv.org/abs/2311.03733 ,  4031kb)
------------------------------------------------------------------------------
\\
arXiv:2311.08118
replaced with revised version Sat, 30 Mar 2024 23:49:19 GMT   (77kb,D)

Title: Evaluating Neighbor Explainability for Graph Neural Networks
Authors: Oscar Llorente, Rana Fawzy, Jared Keown, Michal Horemuz, P\'eter
  Vaderna, S\'andor Laki, Roland Kotrocz\'o, Rita Csoma and J\'anos M\'ark
  Szalai-Gindl
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2311.08118 ,  77kb)
------------------------------------------------------------------------------
\\
arXiv:2311.10707
replaced with revised version Mon, 1 Apr 2024 16:56:13 GMT   (724kb,D)

Title: Multimodal Representation Learning by Alternating Unimodal Adaptation
Authors: Xiaohui Zhang, Jaehong Yoon, Mohit Bansal, Huaxiu Yao
Categories: cs.LG cs.CV
Comments: Accepted by CVPR 2024
\\ ( https://arxiv.org/abs/2311.10707 ,  724kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00276
replaced with revised version Sat, 30 Mar 2024 03:25:51 GMT   (323kb,D)

Title: Automating Continual Learning
Authors: Kazuki Irie, R\'obert Csord\'as, J\"urgen Schmidhuber
Categories: cs.LG
\\ ( https://arxiv.org/abs/2312.00276 ,  323kb)
------------------------------------------------------------------------------
\\
arXiv:2312.00600
replaced with revised version Sun, 31 Mar 2024 12:45:09 GMT   (9372kb,D)

Title: Improving Plasticity in Online Continual Learning via Collaborative
  Learning
Authors: Maorong Wang, Nicolas Michel, Ling Xiao, Toshihiko Yamasaki
Categories: cs.LG
Comments: Update Camera-ready revision for CVPR'24
\\ ( https://arxiv.org/abs/2312.00600 ,  9372kb)
------------------------------------------------------------------------------
\\
arXiv:2312.03041
replaced with revised version Sun, 31 Mar 2024 04:21:08 GMT   (957kb)

Title: Transformer-Based Deep Learning Model for Bored Pile Load-Deformation
  Prediction in Bangkok Subsoil
Authors: Sompote Youwai and Chissanupong Thongnoo
Categories: cs.LG cs.CE
\\ ( https://arxiv.org/abs/2312.03041 ,  957kb)
------------------------------------------------------------------------------
\\
arXiv:2312.12835
replaced with revised version Sun, 31 Mar 2024 08:19:12 GMT   (1853kb)

Title: Near-Optimal Resilient Aggregation Rules for Distributed Learning Using
  1-Center and 1-Mean Clustering with Outliers
Authors: Yuhao Yi, Ronghui You, Hong Liu, Changxin Liu, Yuan Wang, Jiancheng Lv
Categories: cs.LG cs.DC
Comments: 17 pages, 4 figures. Accepted by the 38th Annual AAAI Conference on
  Artificial Intelligence (AAAI'24)
Journal-ref: AAAI 2024, 38, 16469-16477
DOI: 10.1609/aaai.v38i15.29584
\\ ( https://arxiv.org/abs/2312.12835 ,  1853kb)
------------------------------------------------------------------------------
\\
arXiv:2312.13978
replaced with revised version Mon, 1 Apr 2024 14:13:22 GMT   (165kb,D)

Title: Metalearning with Very Few Samples Per Task
Authors: Maryam Aliakbarpour and Konstantina Bairaktari and Gavin Brown and
  Adam Smith and Nathan Srebro and Jonathan Ullman
Categories: cs.LG cs.DS
\\ ( https://arxiv.org/abs/2312.13978 ,  165kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00104
replaced with revised version Mon, 1 Apr 2024 04:31:34 GMT   (5862kb,D)

Title: Causal State Distillation for Explainable Reinforcement Learning
Authors: Wenhao Lu, Xufeng Zhao, Thilo Fryen, Jae Hee Lee, Mengdi Li, Sven
  Magg, Stefan Wermter
Categories: cs.LG cs.AI stat.ME
Comments: https://lukaswill.github.io/; Accepted as oral by CLeaR 2024
\\ ( https://arxiv.org/abs/2401.00104 ,  5862kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12610
replaced with revised version Mon, 1 Apr 2024 16:07:28 GMT   (1229kb,D)

Title: The twin peaks of learning neural networks
Authors: Elizaveta Demyanenko, Christoph Feinauer, Enrico M. Malatesta, Luca
  Saglietti
Categories: cs.LG cond-mat.dis-nn math.PR math.ST stat.TH
Comments: 37 pages, 31 figures
\\ ( https://arxiv.org/abs/2401.12610 ,  1229kb)
------------------------------------------------------------------------------
\\
arXiv:2401.16025
replaced with revised version Mon, 1 Apr 2024 06:51:38 GMT   (18002kb,D)

Title: Simple Policy Optimization
Authors: Zhengpeng Xie
Categories: cs.LG
\\ ( https://arxiv.org/abs/2401.16025 ,  18002kb)
------------------------------------------------------------------------------
\\
arXiv:2402.00976
replaced with revised version Sun, 31 Mar 2024 23:06:53 GMT   (142kb)

Title: Investigating Recurrent Transformers with Dynamic Halt
Authors: Jishnu Ray Chowdhury, Cornelia Caragea
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2402.00976 ,  142kb)
------------------------------------------------------------------------------
\\
arXiv:2402.01858
replaced with revised version Fri, 29 Mar 2024 21:18:37 GMT   (28459kb,D)

Title: Explaining latent representations of generative models with large
  multimodal models
Authors: Mengdan Zhu, Zhenke Liu, Bo Pan, Abhinav Angirekula, Liang Zhao
Categories: cs.LG cs.AI cs.CL cs.CV
Comments: ICLR 2024 Workshop Paper on Reliable and Responsible Foundation
  Models
\\ ( https://arxiv.org/abs/2402.01858 ,  28459kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02277
replaced with revised version Mon, 1 Apr 2024 16:13:23 GMT   (2082kb,D)

Title: Causal Bayesian Optimization via Exogenous Distribution Learning
Authors: Shaogang Ren, Xiaoning Qian
Categories: cs.LG stat.ML
\\ ( https://arxiv.org/abs/2402.02277 ,  2082kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07710
replaced with revised version Sat, 30 Mar 2024 09:20:36 GMT   (1057kb,D)

Title: Optimizing Sparse Convolution on GPUs with CUDA for 3D Point Cloud
  Processing in Embedded Systems
Authors: Chester Luo, Kevin Lai
Categories: cs.LG cs.CV
Comments: 9 pages
\\ ( https://arxiv.org/abs/2402.07710 ,  1057kb)
------------------------------------------------------------------------------
\\
arXiv:2402.11686
replaced with revised version Fri, 29 Mar 2024 19:16:16 GMT   (412kb,D)

Title: Learning the Topology and Behavior of Discrete Dynamical Systems
Authors: Zirou Qiu, Abhijin Adiga, Madhav V. Marathe, S. S. Ravi, Daniel J.
  Rosenkrantz, Richard E. Stearns, Anil Vullikanti
Categories: cs.LG
Comments: Accepted at AAAI-24
\\ ( https://arxiv.org/abs/2402.11686 ,  412kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16562
replaced with revised version Fri, 29 Mar 2024 18:05:17 GMT   (375kb)

Title: Q-FOX Learning: Breaking Tradition in Reinforcement Learning
Authors: Mahmood A. Jumaah, Yossra H. Ali and Tarik A. Rashid
Categories: cs.LG cs.AI cs.NE
\\ ( https://arxiv.org/abs/2402.16562 ,  375kb)
------------------------------------------------------------------------------
\\
arXiv:2403.01590
replaced with revised version Sun, 31 Mar 2024 14:31:14 GMT   (15078kb,D)

Title: The Hidden Attention of Mamba Models
Authors: Ameen Ali, Itamar Zimerman, Lior Wolf
Categories: cs.LG
MSC-class: F.2.2, I.2.7
ACM-class: F.2.2; I.2.7
\\ ( https://arxiv.org/abs/2403.01590 ,  15078kb)
------------------------------------------------------------------------------
\\
arXiv:2403.07953
replaced with revised version Sun, 31 Mar 2024 23:47:47 GMT   (3034kb,D)

Title: Abstracting Sparse DNN Acceleration via Structured Sparse Tensor
  Decomposition
Authors: Geonhwa Jeong, Po-An Tsai, Abhimanyu R. Bambhaniya, Stephen W.
  Keckler, Tushar Krishna
Categories: cs.LG cs.AI cs.AR
\\ ( https://arxiv.org/abs/2403.07953 ,  3034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.09863
replaced with revised version Mon, 1 Apr 2024 12:03:24 GMT   (790kb,D)

Title: Towards White Box Deep Learning
Authors: Maciej Satkiewicz
Categories: cs.LG cs.AI cs.NE
Comments: 14 pages, 9 figures, independent research, v3 changes: Extracted
  Related Work and Methodology to self-contained sections
\\ ( https://arxiv.org/abs/2403.09863 ,  790kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10800
replaced with revised version Fri, 29 Mar 2024 20:34:26 GMT   (7282kb,D)

Title: Model Reprogramming Outperforms Fine-tuning on Out-of-distribution Data
  in Text-Image Encoders
Authors: Andrew Geng, Pin-Yu Chen
Categories: cs.LG
Comments: Accepted in SatML 2024
\\ ( https://arxiv.org/abs/2403.10800 ,  7282kb)
------------------------------------------------------------------------------
\\
arXiv:2403.12418
replaced with revised version Sun, 31 Mar 2024 13:28:19 GMT   (3041kb,D)

Title: STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space
  Model
Authors: Lincan Li, Hanchen Wang, Wenjie Zhang, Adelle Coster
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.12418 ,  3041kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14608
replaced with revised version Mon, 1 Apr 2024 15:11:36 GMT   (3290kb,D)

Title: Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey
Authors: Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang, Sai Qian Zhang
Categories: cs.LG
Comments: 24 pages, 12 figures
\\ ( https://arxiv.org/abs/2403.14608 ,  3290kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16108
replaced with revised version Sat, 30 Mar 2024 23:46:29 GMT   (211kb,D)

Title: A Transformer approach for Electricity Price Forecasting
Authors: Oscar Llorente and Jose Portela
Categories: cs.LG cs.AI
Comments: 7 pages
\\ ( https://arxiv.org/abs/2403.16108 ,  211kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17561
replaced with revised version Sun, 31 Mar 2024 03:02:10 GMT   (1990kb,D)

Title: A Survey on Deep Learning and State-of-the-art Applications
Authors: Mohd Halim Mohd Noor and Ayokunle Olalekan Ige
Categories: cs.LG
Comments: Submitted to Elsevier Neural Networks
\\ ( https://arxiv.org/abs/2403.17561 ,  1990kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18035
replaced with revised version Sat, 30 Mar 2024 13:28:54 GMT   (13320kb,D)

Title: Bidirectional Consistency Models
Authors: Liangchen Li and Jiajun He
Categories: cs.LG cs.CV
Comments: 40 pages, 25 figures
\\ ( https://arxiv.org/abs/2403.18035 ,  13320kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18539
replaced with revised version Sat, 30 Mar 2024 10:07:21 GMT   (658kb,D)

Title: Safe and Robust Reinforcement Learning: Principles and Practice
Authors: Taku Yamagata, Raul Santos-Rodriguez
Categories: cs.LG cs.SY eess.SY
\\ ( https://arxiv.org/abs/2403.18539 ,  658kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18710
replaced with revised version Mon, 1 Apr 2024 06:51:56 GMT   (484kb,D)

Title: Energy-Guided Data Sampling for Traffic Prediction with Mini Training
  Datasets
Authors: Zhaohui Yang, Kshitij Jerath
Categories: cs.LG
\\ ( https://arxiv.org/abs/2403.18710 ,  484kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19076
replaced with revised version Fri, 29 Mar 2024 21:33:39 GMT   (36537kb,D)

Title: Tiny Machine Learning: Progress and Futures
Authors: Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Song Han
Categories: cs.LG cs.AI cs.CV
Comments: arXiv admin note: text overlap with arXiv:2206.15472
Journal-ref: IEEE Circuits and Systems Magazine, 23(3), pp. 8-34, October 2023
DOI: 10.1109/MCAS.2023.3302182
\\ ( https://arxiv.org/abs/2403.19076 ,  36537kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19165
replaced with revised version Mon, 1 Apr 2024 10:20:09 GMT   (690kb)

Title: Evaluating Fair Feature Selection in Machine Learning for Healthcare
Authors: Md Rahat Shahriar Zawad, Peter Washington
Categories: cs.LG cs.CY
Comments: 10 pages, 7 figures. This is a Preprint
\\ ( https://arxiv.org/abs/2403.19165 ,  690kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19561
replaced with revised version Sat, 30 Mar 2024 03:47:25 GMT   (583kb,D)

Title: Self-Improved Learning for Scalable Neural Combinatorial Optimization
Authors: Fu Luo, Xi Lin, Zhenkun Wang, Xialiang Tong, Mingxuan Yuan, Qingfu
  Zhang
Categories: cs.LG cs.AI
\\ ( https://arxiv.org/abs/2403.19561 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19647
replaced with revised version Sun, 31 Mar 2024 16:54:50 GMT   (7985kb,D)

Title: Sparse Feature Circuits: Discovering and Editing Interpretable Causal
  Graphs in Language Models
Authors: Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau,
  Aaron Mueller
Categories: cs.LG cs.AI cs.CL
Comments: Code and data at https://github.com/saprmarks/feature-circuits.
  Demonstration at https://feature-circuits.xyz
\\ ( https://arxiv.org/abs/2403.19647 ,  7985kb)
------------------------------------------------------------------------------
\\
arXiv:2110.07872
replaced with revised version Mon, 1 Apr 2024 04:51:37 GMT   (352kb)

Title: Role Similarity Metric Based on Spanning Rooted Forest
Authors: Qi Bao, Zhongzhi Zhang, Haibin Kan
Categories: cs.SI cs.AI
Comments: 10 pages, 5 figures
\\ ( https://arxiv.org/abs/2110.07872 ,  352kb)
------------------------------------------------------------------------------
\\
arXiv:2202.03583 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 18:57:25 GMT   (2194kb,D)

Title: Multi-Label Classification of Thoracic Diseases using Dense
  Convolutional Network on Chest Radiographs
Authors: Dipkamal Bhusal, Sanjeeb Prasad Panday
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 13 pages
\\ ( https://arxiv.org/abs/2202.03583 ,  2194kb)
------------------------------------------------------------------------------
\\
arXiv:2205.14375
replaced with revised version Sat, 30 Mar 2024 13:49:58 GMT   (2706kb,D)

Title: WaveMix: A Resource-efficient Neural Network for Image Analysis
Authors: Pranav Jeevan, Kavitha Viswanathan, Anandu A S, Amit Sethi
Categories: cs.CV cs.AI cs.LG
Comments: 20 pages, 5 figures
ACM-class: I.2.10; I.4.0; I.4.1; I.4.2; I.4.6; I.4.7; I.4.8; I.4.9; I.4.10;
  I.2.10; I.5.1; I.5.2; I.5.4; J.2
\\ ( https://arxiv.org/abs/2205.14375 ,  2706kb)
------------------------------------------------------------------------------
\\
arXiv:2208.13474
replaced with revised version Sun, 31 Mar 2024 08:12:21 GMT   (7486kb,D)

Title: Prompt Tuning with Soft Context Sharing for Vision-Language Models
Authors: Kun Ding and Ying Wang and Pengzhang Liu and Qiang Yu and Haojian
  Zhang and Shiming Xiang and Chunhong Pan
Categories: cs.CV cs.AI
Comments: 20 pages
\\ ( https://arxiv.org/abs/2208.13474 ,  7486kb)
------------------------------------------------------------------------------
\\
arXiv:2212.00210
replaced with revised version Mon, 1 Apr 2024 17:19:02 GMT   (22505kb,D)

Title: Shape-Guided Diffusion with Inside-Outside Attention
Authors: Dong Huk Park, Grace Luo, Clayton Toste, Samaneh Azadi, Xihui Liu,
  Maka Karalashvili, Anna Rohrbach, Trevor Darrell
Categories: cs.CV cs.AI cs.LG
Comments: WACV 2024
\\ ( https://arxiv.org/abs/2212.00210 ,  22505kb)
------------------------------------------------------------------------------
\\
arXiv:2301.04090
replaced with revised version Fri, 29 Mar 2024 19:18:18 GMT   (3047kb,D)

Title: Finding Nontrivial Minimum Fixed Points in Discrete Dynamical Systems
Authors: Zirou Qiu, Chen Chen, Madhav V. Marathe, S. S. Ravi, Daniel J.
  Rosenkrantz, Richard E. Stearns, Anil Vullikanti
Categories: cs.SI cs.AI
Comments: Accepted at AAAI-22
\\ ( https://arxiv.org/abs/2301.04090 ,  3047kb)
------------------------------------------------------------------------------
\\
arXiv:2301.08530 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 14:47:08 GMT   (2099kb,D)

Title: Self-Organization Towards $1/f$ Noise in Deep Neural Networks
Authors: Nicholas Chong Jia Le and Ling Feng
Categories: physics.data-an cs.AI nlin.AO
\\ ( https://arxiv.org/abs/2301.08530 ,  2099kb)
------------------------------------------------------------------------------
\\
arXiv:2301.13418
replaced with revised version Mon, 1 Apr 2024 06:41:14 GMT   (21012kb,D)

Title: BRAIxDet: Learning to Detect Malignant Breast Lesion with Incomplete
  Annotations
Authors: Yuanhong Chen, Yuyuan Liu, Chong Wang, Michael Elliott, Chun Fung
  Kwok, Carlos Pena-Solorzano, Yu Tian, Fengbei Liu, Helen Frazer, Davis J.
  McCarthy, Gustavo Carneiro
Categories: cs.CV cs.AI cs.LG
Comments: Under Review
\\ ( https://arxiv.org/abs/2301.13418 ,  21012kb)
------------------------------------------------------------------------------
\\
arXiv:2303.09373
replaced with revised version Sat, 30 Mar 2024 22:10:36 GMT   (3091kb,D)

Title: MAPSeg: Unified Unsupervised Domain Adaptation for Heterogeneous Medical
  Image Segmentation Based on 3D Masked Autoencoding and Pseudo-Labeling
Authors: Xuzhe Zhang, Yuhao Wu, Elsa Angelini, Ang Li, Jia Guo, Jerod M.
  Rasmussen, Thomas G. O'Connor, Pathik D. Wadhwa, Andrea Parolin Jackowski,
  Hai Li, Jonathan Posner, Andrew F. Laine, Yun Wang
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024 camera-ready (8 pages, 3 figures) with the supplemental
  materials (5 pages, 4 figures). Xuzhe Zhang and Yuhao Wu are co-first
  authors. Andrew F. Laine and Yun Wang are co-senior supervising authors
\\ ( https://arxiv.org/abs/2303.09373 ,  3091kb)
------------------------------------------------------------------------------
\\
arXiv:2303.09383
replaced with revised version Sat, 30 Mar 2024 18:22:34 GMT   (30633kb,D)

Title: Unifying Top-down and Bottom-up Scanpath Prediction Using Transformers
Authors: Zhibo Yang, Sounak Mondal, Seoyoung Ahn, Ruoyu Xue, Gregory Zelinsky,
  Minh Hoai, Dimitris Samaras
Categories: cs.CV cs.AI
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2303.09383 ,  30633kb)
------------------------------------------------------------------------------
\\
arXiv:2305.01157
replaced with revised version Sun, 31 Mar 2024 19:56:37 GMT   (3402kb,D)

Title: Complex Logical Reasoning over Knowledge Graphs using Large Language
  Models
Authors: Nurendra Choudhary and Chandan K. Reddy
Categories: cs.LO cs.AI cs.IR
Comments: Code available at https://github.com/Akirato/LLM-KG-Reasoning
ACM-class: F.4.1; H.3.3; I.1.1
\\ ( https://arxiv.org/abs/2305.01157 ,  3402kb)
------------------------------------------------------------------------------
\\
arXiv:2305.19556
replaced with revised version Mon, 1 Apr 2024 04:45:30 GMT   (2039kb,D)

Title: Exploring Phonetic Context-Aware Lip-Sync For Talking Face Generation
Authors: Se Jin Park, Minsu Kim, Jeongsoo Choi, Yong Man Ro
Categories: cs.CV cs.AI cs.SD eess.AS eess.IV
Comments: Accepted at ICASSP 2024
\\ ( https://arxiv.org/abs/2305.19556 ,  2039kb)
------------------------------------------------------------------------------
\\
arXiv:2306.13549
replaced with revised version Mon, 1 Apr 2024 17:51:54 GMT   (583kb,D)

Title: A Survey on Multimodal Large Language Models
Authors: Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, Enhong
  Chen
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Project
  page:https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models
\\ ( https://arxiv.org/abs/2306.13549 ,  583kb)
------------------------------------------------------------------------------
\\
arXiv:2307.04192
replaced with revised version Sun, 31 Mar 2024 12:10:24 GMT   (500kb,D)

Title: Self-Adaptive Sampling for Efficient Video Question-Answering on
  Image--Text Models
Authors: Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria
Categories: cs.CV cs.AI cs.CL cs.MM
Comments: 13 pages, 7 figures, accepted to Findings of NAACL 2024
\\ ( https://arxiv.org/abs/2307.04192 ,  500kb)
------------------------------------------------------------------------------
\\
arXiv:2308.01813
replaced with revised version Sun, 31 Mar 2024 12:27:16 GMT   (7214kb,D)

Title: Deep Neural Networks Fused with Textures for Image Classification
Authors: Asish Bera, Debotosh Bhattacharjee, and Mita Nasipuri
Categories: cs.CV cs.AI
Comments: 14 pages, 6 figures, 4 tables, conference
Journal-ref: Proceedings of International Conference on Frontiers in Computing
  and Systems. COMSYS 2022
DOI: 10.1007/978-981-99-2680-0_10
\\ ( https://arxiv.org/abs/2308.01813 ,  7214kb)
------------------------------------------------------------------------------
\\
arXiv:2309.01327
replaced with revised version Sat, 30 Mar 2024 06:50:28 GMT   (2394kb,D)

Title: Can I Trust Your Answer? Visually Grounded Video Question Answering
Authors: Junbin Xiao, Angela Yao, Yicong Li, Tat Seng Chua
Categories: cs.CV cs.AI cs.MM
Comments: Accepted to CVPR'24. (Compared with preprint version, we mainly
  improve the presentation, discuss more related works, and extend experiments
  in Appendix.)
\\ ( https://arxiv.org/abs/2309.01327 ,  2394kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04019 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 05:02:23 GMT   (2634kb)

Title: Evaluation of large language models for discovery of gene set function
Authors: Mengzhou Hu and Sahar Alkhairy, Ingoo Lee, Rudolf T. Pillich, Dylan
  Fong, Kevin Smith, Robin Bachelder, Trey Ideker, and Dexter Pratt
Categories: q-bio.GN cs.AI cs.CL q-bio.MN
\\ ( https://arxiv.org/abs/2309.04019 ,  2634kb)
------------------------------------------------------------------------------
\\
arXiv:2309.16211
replaced with revised version Mon, 1 Apr 2024 02:49:49 GMT   (1924kb,D)

Title: VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by
  Multimodal Large Language Models
Authors: Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu
Categories: cs.CV cs.AI
Comments: Accepted to ICLR 2024
\\ ( https://arxiv.org/abs/2309.16211 ,  1924kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07889
replaced with revised version Sat, 30 Mar 2024 22:00:22 GMT   (1591kb,D)

Title: LangNav: Language as a Perceptual Representation for Navigation
Authors: Bowen Pan, Rameswar Panda, SouYoung Jin, Rogerio Feris, Aude Oliva,
  Phillip Isola, Yoon Kim
Categories: cs.CV cs.AI cs.CL cs.RO
\\ ( https://arxiv.org/abs/2310.07889 ,  1591kb)
------------------------------------------------------------------------------
\\
arXiv:2311.01017
replaced with revised version Mon, 1 Apr 2024 15:41:50 GMT   (39838kb,D)

Title: Copilot4D: Learning Unsupervised World Models for Autonomous Driving via
  Discrete Diffusion
Authors: Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, Raquel
  Urtasun
Categories: cs.CV cs.AI cs.LG cs.RO
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2311.01017 ,  39838kb)
------------------------------------------------------------------------------
\\
arXiv:2311.12304
replaced with revised version Sat, 30 Mar 2024 19:16:29 GMT   (1675kb,D)

Title: Discovering Effective Policies for Land-Use Planning with Neuroevolution
Authors: Risto Miikkulainen, Olivier Francon, Daniel Young, Elliot Meyerson,
  Clemens Schwingshackl, Jacob Bieker, Hugo Cunha, and Babak Hodjat
Categories: cs.NE cs.AI cs.LG
\\ ( https://arxiv.org/abs/2311.12304 ,  1675kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17076
replaced with revised version Mon, 1 Apr 2024 03:17:09 GMT   (6252kb,D)

Title: Compositional Chain-of-Thought Prompting for Large Multimodal Models
Authors: Chancharik Mitra, Brandon Huang, Trevor Darrell, Roei Herzig
Categories: cs.CV cs.AI cs.CL cs.LG
\\ ( https://arxiv.org/abs/2311.17076 ,  6252kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18254
replaced with revised version Sun, 31 Mar 2024 13:13:37 GMT   (26874kb,D)

Title: Sketch Input Method Editor: A Comprehensive Dataset and Methodology for
  Systematic Input Recognition
Authors: Guangming Zhu, Siyuan Wang, Qing Cheng, Kelong Wu, Hao Li, Liang Zhang
Categories: cs.CV cs.AI
Comments: The paper has been accepted by ACM Multimedia 2023
\\ ( https://arxiv.org/abs/2311.18254 ,  26874kb)
------------------------------------------------------------------------------
\\
arXiv:2311.18608
replaced with revised version Mon, 1 Apr 2024 11:44:25 GMT   (14853kb,D)

Title: Contrastive Denoising Score for Text-guided Latent Diffusion Image
  Editing
Authors: Hyelin Nam, Gihyun Kwon, Geon Yeong Park, Jong Chul Ye
Categories: cs.CV cs.AI cs.LG
Comments: CVPR 2024 (poster); Project page: https://hyelinnam.github.io/CDS/
\\ ( https://arxiv.org/abs/2311.18608 ,  14853kb)
------------------------------------------------------------------------------
\\
arXiv:2312.02010
replaced with revised version Mon, 1 Apr 2024 07:21:52 GMT   (3203kb,D)

Title: Towards Learning a Generalist Model for Embodied Navigation
Authors: Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, Liwei Wang
Categories: cs.CV cs.AI
Comments: Accepted by CVPR 2024 (14 pages, 3 figures)
\\ ( https://arxiv.org/abs/2312.02010 ,  3203kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06226
replaced with revised version Mon, 1 Apr 2024 06:57:31 GMT   (2063kb,D)

Title: Invariant Representation via Decoupling Style and Spurious Features from
  Images
Authors: Ruimeng Li, Yuanhao Pu, Zhaoyi Li, Hong Xie, Defu Lian
Categories: cs.CV cs.AI
Comments: 10 pages, 12 figures
ACM-class: I.2.6; I.2.10
\\ ( https://arxiv.org/abs/2312.06226 ,  2063kb)
------------------------------------------------------------------------------
\\
arXiv:2312.06742
replaced with revised version Mon, 1 Apr 2024 03:00:06 GMT   (3793kb,D)

Title: Honeybee: Locality-enhanced Projector for Multimodal LLM
Authors: Junbum Cha, Wooyoung Kang, Jonghwan Mun, Byungseok Roh
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: CVPR 2024 camera-ready
\\ ( https://arxiv.org/abs/2312.06742 ,  3793kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09313
replaced with revised version Sat, 30 Mar 2024 14:01:27 GMT   (18101kb,D)

Title: LatentEditor: Text Driven Local Editing of 3D Scenes
Authors: Umar Khalid, Hasan Iqbal, Nazmul Karim, Jing Hua, Chen Chen
Categories: cs.CV cs.AI
Comments: Project Page: https://latenteditor.github.io/
\\ ( https://arxiv.org/abs/2312.09313 ,  18101kb)
------------------------------------------------------------------------------
\\
arXiv:2312.09788
replaced with revised version Sat, 30 Mar 2024 01:21:42 GMT   (24718kb,D)

Title: Collaborating Foundation Models for Domain Generalized Semantic
  Segmentation
Authors: Yasser Benigmim, Subhankar Roy, Slim Essid, Vicky Kalogeiton,
  St\'ephane Lathuili\`ere
Categories: cs.CV cs.AI cs.LG
Comments: https://github.com/yasserben/CLOUDS ; Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2312.09788 ,  24718kb)
------------------------------------------------------------------------------
\\
arXiv:2401.01482
replaced with revised version Fri, 29 Mar 2024 18:52:59 GMT   (47908kb,D)

Title: Incorporating Geo-Diverse Knowledge into Prompting for Increased
  Geographical Robustness in Object Recognition
Authors: Kyle Buettner, Sina Malakouti, Xiang Lorraine Li, Adriana Kovashka
Categories: cs.CV cs.AI cs.LG
Comments: To appear in IEEE/CVF Computer Vision and Pattern Recognition
  Conference (CVPR), 2024
\\ ( https://arxiv.org/abs/2401.01482 ,  47908kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04105
replaced with revised version Sat, 30 Mar 2024 08:06:01 GMT   (1100kb,D)

Title: Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for
  Memory-Efficient Finetuning
Authors: Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng Qian, Fatimah
  Zohra, Abdulmohsen Alghannam, Jitendra Malik, Bernard Ghanem
Categories: cs.CV cs.AI
Journal-ref: the IEEE/CVF Conference on Computer Vision and Pattern Recognition
  (CVPR) 2024
\\ ( https://arxiv.org/abs/2401.04105 ,  1100kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08103
replaced with revised version Mon, 1 Apr 2024 06:50:45 GMT   (214kb,D)

Title: Resolving Ethics Trade-offs in Implementing Responsible AI
Authors: Conrad Sanderson, Emma Schleiger, David Douglas, Petra Kuhnert,
  Qinghua Lu
Categories: cs.CY cs.AI
MSC-class: 68T01
ACM-class: K.4.1; I.2.m; C.4
\\ ( https://arxiv.org/abs/2401.08103 ,  214kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11351 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 00:32:13 GMT   (1294kb,D)

Title: A comprehensive review of Quantum Machine Learning: from NISQ to Fault
  Tolerance
Authors: Yunfei Wang, Junyu Liu
Categories: quant-ph cs.AI cs.LG stat.ML
Comments: 28 pages. Invited review
\\ ( https://arxiv.org/abs/2401.11351 ,  1294kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11748
replaced with revised version Mon, 1 Apr 2024 12:15:44 GMT   (7832kb,D)

Title: GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient
  Inversion Attacks?
Authors: Yu Sun, Gaojian Xiong, Xianxun Yao, Kailang Ma, Jian Cui
Categories: cs.CR cs.AI cs.LG
DOI: 10.1109/ICASSP48485.2024.10445924
\\ ( https://arxiv.org/abs/2401.11748 ,  7832kb)
------------------------------------------------------------------------------
\\
arXiv:2401.12554
replaced with revised version Mon, 1 Apr 2024 05:34:36 GMT   (792kb,D)

Title: Can Large Language Models Write Parallel Code?
Authors: Daniel Nichols, Joshua H. Davis, Zhaojun Xie, Arjun Rajaram, Abhinav
  Bhatele
Categories: cs.DC cs.AI
\\ ( https://arxiv.org/abs/2401.12554 ,  792kb)
------------------------------------------------------------------------------
\\
arXiv:2401.17513 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 01:53:02 GMT   (501kb,D)

Title: A PNP ion channel deep learning solver with local neural network and
  finite element input data
Authors: Hwi Lee, Zhen Chao, Harris Cobb, Yingjie Liu, Dexuan Xie
Categories: physics.bio-ph cs.AI physics.comp-ph
Comments: 17 pages, 4 figures, 5 tables
MSC-class: 92-08
\\ ( https://arxiv.org/abs/2401.17513 ,  501kb)
------------------------------------------------------------------------------
\\
arXiv:2402.03355
replaced with revised version Sat, 30 Mar 2024 21:46:48 GMT   (24816kb)

Title: Unlocking Criminal Hierarchies: A Survey, Experimental, and Comparative
  Exploration of Techniques for Identifying Leaders within Criminal Networks
Authors: Kamal Taha, Abdulhadi Shoufan, and Aya Taha
Categories: cs.SI cs.AI
\\ ( https://arxiv.org/abs/2402.03355 ,  24816kb)
------------------------------------------------------------------------------
\\
arXiv:2402.04476
replaced with revised version Sat, 30 Mar 2024 05:18:05 GMT   (2521kb,D)

Title: Dual-View Visual Contextualization for Web Navigation
Authors: Jihyung Kil, Chan Hee Song, Boyuan Zheng, Xiang Deng, Yu Su, Wei-Lun
  Chao
Categories: cs.CV cs.AI cs.CL
Comments: Accepted to CVPR 2024
\\ ( https://arxiv.org/abs/2402.04476 ,  2521kb)
------------------------------------------------------------------------------
\\
arXiv:2402.05355
replaced with revised version Sat, 30 Mar 2024 22:31:11 GMT   (616kb,D)

Title: A Survey on Safe Multi-Modal Learning System
Authors: Tianyi Zhao, Liangliang Zhang, Yao Ma and Lu Cheng
Categories: cs.CY cs.AI
\\ ( https://arxiv.org/abs/2402.05355 ,  616kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07148 (*cross-listing*)
replaced with revised version Sat, 30 Mar 2024 20:18:54 GMT   (43034kb,D)

Title: X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for
  Large Language Models with Applications in Protein Mechanics and Molecular
  Design
Authors: Eric L. Buehler, Markus J. Buehler
Categories: cond-mat.soft cond-mat.dis-nn cs.AI cs.CL cs.LG q-bio.QM
\\ ( https://arxiv.org/abs/2402.07148 ,  43034kb)
------------------------------------------------------------------------------
\\
arXiv:2403.10988
replaced with revised version Sat, 30 Mar 2024 04:56:05 GMT   (30429kb,D)

Title: Boosting Flow-based Generative Super-Resolution Models via Learned Prior
Authors: Li-Yuan Tsao, Yi-Chen Lo, Chia-Che Chang, Hao-Wei Chen, Roy Tseng,
  Chien Feng, Chun-Yi Lee
Categories: cs.CV cs.AI
Comments: Accepted to CVPR2024
\\ ( https://arxiv.org/abs/2403.10988 ,  30429kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11487
replaced with revised version Sun, 31 Mar 2024 22:21:18 GMT   (9669kb,D)

Title: Can LLMs Generate Human-Like Wayfinding Instructions? Towards
  Platform-Agnostic Embodied Instruction Synthesis
Authors: Vishnu Sashank Dorbala, Sanjoy Chowdhury, Dinesh Manocha
Categories: cs.RO cs.AI
Comments: 14 Pages
\\ ( https://arxiv.org/abs/2403.11487 ,  9669kb)
------------------------------------------------------------------------------
\\
arXiv:2403.11821
replaced with revised version Fri, 29 Mar 2024 19:27:23 GMT   (39549kb,D)

Title: Evaluating Text-to-Image Synthesis: Survey and Taxonomy of Image Quality
  Metrics
Authors: Sebastian Hartwig, Dominik Engel, Leon Sick, Hannah Kniesel, Tristan
  Payer, Poonam Poonam, Michael Gl\"ockler, Alex B\"auerle, Timo Ropinski
Categories: cs.CV cs.AI cs.GR
Comments: preprint, 21 pages, 2 figures, 1 table
\\ ( https://arxiv.org/abs/2403.11821 ,  39549kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13362
replaced with revised version Sat, 30 Mar 2024 03:10:48 GMT   (4259kb,AD)

Title: Incentivizing News Consumption on Social Media Platforms Using Large
  Language Models and Realistic Bot Accounts
Authors: Hadi Askari, Anshuman Chhabra, Bernhard Clemm von Hohenberg, Michael
  Heseltine, Magdalena Wojcieszak
Categories: cs.SI cs.AI cs.CL
\\ ( https://arxiv.org/abs/2403.13362 ,  4259kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13421
replaced with revised version Fri, 29 Mar 2024 19:06:34 GMT   (278kb,D)

Title: Caching-Augmented Lifelong Multi-Agent Path Finding
Authors: Yimin Tang, Zhenghong Yu, Yi Zheng, T. K. Satish Kumar, Jiaoyang Li,
  Sven Koenig
Categories: cs.RO cs.AI cs.MA
\\ ( https://arxiv.org/abs/2403.13421 ,  278kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13802
replaced with revised version Mon, 1 Apr 2024 17:58:02 GMT   (34652kb,D)

Title: ZigMa: A DiT-style Zigzag Mamba Diffusion Model
Authors: Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova,
  Pingchuan Ma, Johannes Fischer, Bj\"orn Ommer
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: Project Page: https://taohu.me/zigma/
\\ ( https://arxiv.org/abs/2403.13802 ,  34652kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14119
replaced with revised version Sun, 31 Mar 2024 13:36:54 GMT   (5980kb,D)

Title: C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via
  Text Feature Dispersion
Authors: Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark
  Hasegawa-Johnson, Yingzhen Li, Chang D. Yoo
Categories: cs.CV cs.AI cs.CL cs.LG
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2403.14119 ,  5980kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14691
replaced with revised version Mon, 1 Apr 2024 05:03:45 GMT   (440kb)

Title: Large Language Models and User Trust: Consequence of Self-Referential
  Learning Loop and the Deskilling of Healthcare Professionals
Authors: Avishek Choudhury, Zaria Chaudhry
Categories: cs.CY cs.AI
Comments: 1 figure
DOI: 10.2196/56764
\\ ( https://arxiv.org/abs/2403.14691 ,  440kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15388
replaced with revised version Mon, 1 Apr 2024 14:08:06 GMT   (537kb,D)

Title: LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal
  Models
Authors: Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, Yan Yan
Categories: cs.CV cs.AI cs.CL
Comments: Project page: https://llava-prumerge.github.io/
\\ ( https://arxiv.org/abs/2403.15388 ,  537kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15443 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 16:37:08 GMT   (848kb)

Title: Introducing an ensemble method for the early detection of Alzheimer's
  disease through the analysis of PET scan images
Authors: Arezoo Borji, Taha-Hossein Hejazi, Abbas Seifi
Categories: eess.SP cs.AI cs.CV cs.LG eess.IV
\\ ( https://arxiv.org/abs/2403.15443 ,  848kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15955
replaced with revised version Sat, 30 Mar 2024 06:42:02 GMT   (2074kb,D)

Title: Finding needles in a haystack: A Black-Box Approach to Invisible
  Watermark Detection
Authors: Minzhou Pan, Zhenting Wang, Xin Dong, Vikash Sehwag, Lingjuan Lyu, Xue
  Lin
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2403.15955 ,  2074kb)
------------------------------------------------------------------------------
\\
arXiv:2403.18998
replaced with revised version Sun, 31 Mar 2024 16:15:58 GMT   (2583kb,D)

Title: Few-Shot Cross-System Anomaly Trace Classification for
  Microservice-based systems
Authors: Yuqing Wang and Mika V. Mantyl\"a and Serge Demeyer and Mutlu Beyazit
  and Joanna Kisaakye and Jesse Nyyss\"ol\"a
Categories: cs.SE cs.AI cs.LG
Comments: 12 pages
\\ ( https://arxiv.org/abs/2403.18998 ,  2583kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19001
replaced with revised version Sat, 30 Mar 2024 02:42:08 GMT   (445kb,D)

Title: Cross-domain Fiber Cluster Shape Analysis for Language Performance
  Cognitive Score Prediction
Authors: Yui Lo, Yuqian Chen, Dongnan Liu, Wan Liu, Leo Zekelman, Fan Zhang,
  Yogesh Rathi, Nikos Makris, Alexandra J. Golby, Weidong Cai, Lauren J.
  O'Donnell
Categories: cs.CV cs.AI eess.IV q-bio.NC
Comments: 2 figures, 11 pages
\\ ( https://arxiv.org/abs/2403.19001 ,  445kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20261 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 05:18:57 GMT   (1465kb,D)

Title: FABind+: Enhancing Molecular Docking through Improved Pocket Prediction
  and Pose Generation
Authors: Kaiyuan Gao, Qizhi Pei, Jinhua Zhu, Tao Qin, Kun He, Lijun Wu
Categories: q-bio.BM cs.AI cs.LG
Comments: 17 pages, 14 figures, 5 tables
\\ ( https://arxiv.org/abs/2403.20261 ,  1465kb)
------------------------------------------------------------------------------
\\
arXiv:2206.09557
replaced with revised version Mon, 1 Apr 2024 06:09:41 GMT   (480kb)

Title: LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient
  Inference in Large-Scale Generative Language Models
Authors: Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim,
  Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee
Categories: cs.DC cs.CL
Comments: ICLR 2024
\\ ( https://arxiv.org/abs/2206.09557 ,  480kb)
------------------------------------------------------------------------------
\\
arXiv:2301.10856
replaced with revised version Sun, 31 Mar 2024 22:34:37 GMT   (9286kb,D)

Title: Partial Mobilization: Tracking Multilingual Information Flows Amongst
  Russian Media Outlets and Telegram
Authors: Hans W. A. Hanley and Zakir Durumeric
Categories: cs.CY cs.CL cs.LG cs.SI
Comments: Accepted to ICWSM 2024
\\ ( https://arxiv.org/abs/2301.10856 ,  9286kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03377
replaced with revised version Mon, 1 Apr 2024 11:55:46 GMT   (5430kb,D)

Title: TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision
Authors: Yukun Zhai, Xiaoqiang Zhang, Xiameng Qin, Sanyuan Zhao, Xingping Dong,
  Jianbing Shen
Categories: cs.CV cs.CL
Comments: Machine Intelligence Research, MIR 2024
DOI: 10.1007/s11633-023-1460-6
\\ ( https://arxiv.org/abs/2306.03377 ,  5430kb)
------------------------------------------------------------------------------
\\
arXiv:2309.04372
replaced with revised version Sat, 30 Mar 2024 05:05:52 GMT   (5516kb,D)

Title: MoEController: Instruction-based Arbitrary Image Manipulation with
  Mixture-of-Expert Controllers
Authors: Sijia Li, Chen Chen, Haonan Lu
Categories: cs.CV cs.CL
Comments: 6 pages,6 figures
\\ ( https://arxiv.org/abs/2309.04372 ,  5516kb)
------------------------------------------------------------------------------
\\
arXiv:2310.07676
replaced with revised version Sat, 30 Mar 2024 16:09:34 GMT   (2227kb,D)

Title: Composite Backdoor Attacks Against Large Language Models
Authors: Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang
Categories: cs.CR cs.CL cs.LG
Comments: To Appear in Findings of the Association for Computational
  Linguistics: NAACL 2024, June 2024
\\ ( https://arxiv.org/abs/2310.07676 ,  2227kb)
------------------------------------------------------------------------------
\\
arXiv:2311.09618 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 01:12:35 GMT   (15629kb,D)

Title: Simulating Opinion Dynamics with Networks of LLM-based Agents
Authors: Yun-Shiuan Chuang, Agam Goyal, Nikunj Harlalka, Siddharth Suresh,
  Robert Hawkins, Sijia Yang, Dhavan Shah, Junjie Hu, Timothy T. Rogers
Categories: physics.soc-ph cs.CL
\\ ( https://arxiv.org/abs/2311.09618 ,  15629kb)
------------------------------------------------------------------------------
\\
arXiv:2311.17049
replaced with revised version Mon, 1 Apr 2024 13:06:06 GMT   (1526kb,D)

Title: MobileCLIP: Fast Image-Text Models through Multi-Modal Reinforced
  Training
Authors: Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja
  Vemulapalli, Oncel Tuzel
Categories: cs.CV cs.CL cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2311.17049 ,  1526kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09989
replaced with revised version Sun, 31 Mar 2024 07:47:59 GMT   (9815kb,D)

Title: LLMs as Bridges: Reformulating Grounded Multimodal Named Entity
  Recognition
Authors: Jinyuan Li, Han Li, Di Sun, Jiahao Wang, Wenkun Zhang, Zan Wang, Gang
  Pan
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2402.09989 ,  9815kb)
------------------------------------------------------------------------------
\\
arXiv:2402.15733
replaced with revised version Mon, 1 Apr 2024 06:59:21 GMT   (819kb)

Title: ArEEG_Chars: Dataset for Envisioned Speech Recognition using EEG for
  Arabic Characters
Authors: Hazem Darwish, Abdalrahman Al Malah, Khloud Al Jallad, Nada Ghneim
Categories: cs.HC cs.CL cs.LG eess.SP
\\ ( https://arxiv.org/abs/2402.15733 ,  819kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15638
replaced with revised version Mon, 1 Apr 2024 15:02:32 GMT   (866kb,D)

Title: Differentially Private Next-Token Prediction of Large Language Models
Authors: James Flemings, Meisam Razaviyayn, Murali Annavaram
Categories: cs.CR cs.CL cs.LG
\\ ( https://arxiv.org/abs/2403.15638 ,  866kb)
------------------------------------------------------------------------------
\\
arXiv:2403.15952
replaced with revised version Sat, 30 Mar 2024 13:21:42 GMT   (3235kb,D)

Title: IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language
  Models
Authors: Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi
  Uddin Ahmad, Yue Dong, Rifat Shahriyar
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2403.15952 ,  3235kb)
------------------------------------------------------------------------------
\\
arXiv:1910.07655
replaced with revised version Sun, 31 Mar 2024 02:57:09 GMT   (4536kb,D)

Title: Deep Semantic Segmentation of Natural and Medical Images: A Review
Authors: Saeid Asgari Taghanaki, Kumar Abhishek, Joseph Paul Cohen, Julien
  Cohen-Adad, Ghassan Hamarneh
Categories: cs.CV cs.LG eess.IV
Comments: 45 pages, 16 figures. Accepted for publication in Springer Artificial
  Intelligence Review
\\ ( https://arxiv.org/abs/1910.07655 ,  4536kb)
------------------------------------------------------------------------------
\\
arXiv:2107.00363 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 12:30:49 GMT   (1313kb,D)

Title: Valid prediction intervals for regression problems
Authors: Nicolas Dewolf, Bernard De Baets, Willem Waegeman
Categories: stat.ML cs.LG
Comments: Minor correction (bibliography and typo in Fig. 3). Thanks to Dr.
  Mar\'ia Moreno de Castro for spotting this typo
DOI: 10.1007/s10462-022-10178-5
\\ ( https://arxiv.org/abs/2107.00363 ,  1313kb)
------------------------------------------------------------------------------
\\
arXiv:2112.14233 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 22:52:41 GMT   (2370kb,D)

Title: Multitask Learning and Bandits via Robust Statistics
Authors: Kan Xu, Hamsa Bastani
Categories: stat.ML cs.LG math.ST stat.TH
\\ ( https://arxiv.org/abs/2112.14233 ,  2370kb)
------------------------------------------------------------------------------
\\
arXiv:2209.07028 (*cross-listing*)
replaced with revised version Fri, 29 Mar 2024 18:50:04 GMT   (50kb,D)

Title: Estimating large causal polytrees from small samples
Authors: Sourav Chatterjee, Mathukumalli Vidyasagar
Categories: stat.ME cs.LG math.PR math.ST stat.ML stat.TH
Comments: 26 pages. An R package has been developed (see link in the article),
  and a real data example has been added
MSC-class: 62D20
\\ ( https://arxiv.org/abs/2209.07028 ,  50kb)
------------------------------------------------------------------------------
\\
arXiv:2209.09404 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 02:02:52 GMT   (19136kb,D)

Title: Machine Learning-Augmented Optimization of Large Bilevel and Two-stage
  Stochastic Programs: Application to Cycling Network Design
Authors: Timothy C. Y. Chan, Bo Lin, Shoshanna Saxe
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2209.09404 ,  19136kb)
------------------------------------------------------------------------------
\\
arXiv:2302.02314 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 11:58:28 GMT   (3577kb,D)

Title: CECT: Controllable Ensemble CNN and Transformer for COVID-19 Image
  Classification
Authors: Zhaoshan Liu, Lei Shen
Categories: eess.IV cs.CV cs.LG
Comments: Computers in Biology and Medicine Accepted
\\ ( https://arxiv.org/abs/2302.02314 ,  3577kb)
------------------------------------------------------------------------------
\\
arXiv:2302.07615 (*cross-listing*)
replaced with revised version Sat, 30 Mar 2024 15:00:07 GMT   (107kb,D)

Title: Similarity, Compression and Local Steps: Three Pillars of Efficient
  Communications for Distributed Variational Inequalities
Authors: Aleksandr Beznosikov, Martin Tak\'a\v{c}, Alexander Gasnikov
Categories: math.OC cs.DC cs.GT cs.LG stat.ML
Comments: Appears in: Advances in Neural Information Processing Systems 36
  (NeurIPS 2023)
  (https://proceedings.neurips.cc/paper_files/paper/2023/hash/5b4a459db23e6db9be2a128380953d96-Abstract-Conference.html).
  36 pages, 3 algorithms, 1 figure, 1 table
\\ ( https://arxiv.org/abs/2302.07615 ,  107kb)
------------------------------------------------------------------------------
\\
arXiv:2302.10174
replaced with revised version Mon, 1 Apr 2024 04:00:31 GMT   (22263kb,D)

Title: Towards Universal Fake Image Detectors that Generalize Across Generative
  Models
Authors: Utkarsh Ojha, Yuheng Li, Yong Jae Lee
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2302.10174 ,  22263kb)
------------------------------------------------------------------------------
\\
arXiv:2302.13087 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 02:57:46 GMT   (935kb,D)

Title: Gauss-Newton Temporal Difference Learning with Nonlinear Function
  Approximation
Authors: Zhifa Ke, Junyu Zhang, Zaiwen Wen
Categories: math.OC cs.LG
\\ ( https://arxiv.org/abs/2302.13087 ,  935kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15357 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 09:29:49 GMT   (1331kb,D)

Title: Solving Diffusion ODEs with Optimal Boundary Conditions for Better Image
  Super-Resolution
Authors: Yiyang Ma, Huan Yang, Wenhan Yang, Jianlong Fu, Jiaying Liu
Categories: eess.IV cs.CV cs.LG
Comments: Accepted by ICLR 2024
\\ ( https://arxiv.org/abs/2305.15357 ,  1331kb)
------------------------------------------------------------------------------
\\
arXiv:2305.15938 (*cross-listing*)
replaced with revised version Sat, 30 Mar 2024 13:50:06 GMT   (51kb)

Title: First Order Methods with Markovian Noise: from Acceleration to
  Variational Inequalities
Authors: Aleksandr Beznosikov, Sergey Samsonov, Marina Sheshukova, Alexander
  Gasnikov, Alexey Naumov, Eric Moulines
Categories: math.OC cs.LG stat.ML
Comments: Appears in: Advances in Neural Information Processing Systems 36
  (NeurIPS 2023). 41 pages, 3 algorithms, 2 tables
Journal-ref: https://proceedings.neurips.cc/paper_files/paper/2023/hash/8c3e38ce55a0fa44bc325bc6fdb7f4e5-Abstract-Conference.html
\\ ( https://arxiv.org/abs/2305.15938 ,  51kb)
------------------------------------------------------------------------------
\\
arXiv:2305.18171
replaced with revised version Sun, 31 Mar 2024 22:58:38 GMT   (1618kb,D)

Title: Improved Probabilistic Image-Text Representations
Authors: Sanghyuk Chun
Categories: cs.CV cs.LG
Comments: ICLR 2024 camera-ready; Code: https://github.com/naver-ai/pcmepp.
  Project page: https://naver-ai.github.io/pcmepp/. 30 pages, 2.2 MB
\\ ( https://arxiv.org/abs/2305.18171 ,  1618kb)
------------------------------------------------------------------------------
\\
arXiv:2306.03897
replaced with revised version Mon, 1 Apr 2024 14:40:30 GMT   (3671kb,D)

Title: DANSE: Data-driven Non-linear State Estimation of Model-free Process in
  Unsupervised Learning Setup
Authors: Anubhab Ghosh, Antoine Honor\'e and Saikat Chatterjee
Categories: eess.SY cs.LG cs.SY eess.SP
Comments: 12 pages, Accepted for publication in IEEE Transactions in Signal
  Processing
DOI: 10.1109/TSP.2024.3383277
\\ ( https://arxiv.org/abs/2306.03897 ,  3671kb)
------------------------------------------------------------------------------
\\
arXiv:2307.08919
replaced with revised version Fri, 29 Mar 2024 18:19:36 GMT   (1071kb,D)

Title: Systematic comparison of semi-supervised and self-supervised learning
  for medical image classification
Authors: Zhe Huang, Ruijie Jiang, Shuchin Aeron, and Michael C. Hughes
Categories: cs.CV cs.LG
Comments: CVPR 2024
\\ ( https://arxiv.org/abs/2307.08919 ,  1071kb)
------------------------------------------------------------------------------
\\
arXiv:2307.12872
replaced with revised version Sat, 30 Mar 2024 06:59:35 GMT   (9325kb,D)

Title: Latent Code Augmentation Based on Stable Diffusion for Data-free
  Substitute Attacks
Authors: Mingwen Shao, Lingzhuang Meng, Yuanjian Qiao, Lixu Zhang, Wangmeng Zuo
Categories: cs.CV cs.CR cs.LG
Comments: This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible
\\ ( https://arxiv.org/abs/2307.12872 ,  9325kb)
------------------------------------------------------------------------------
\\
arXiv:2308.04185
replaced with revised version Sun, 31 Mar 2024 05:11:35 GMT   (1992kb,D)

Title: Iterative Sketching for Secure Coded Regression
Authors: Neophytos Charalambides, Hessam Mahdavifar, Mert Pilanci, Alfred O.
  Hero III
Categories: cs.IT cs.CR cs.DC cs.LG cs.NA math.IT math.NA
Comments: 29 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:2201.08522
MSC-class: 65B99, 68P20, 68P25, 68P27, 68P30, 94-10, 94A11, 94A16, 94B60
ACM-class: E.3; E.4; F.2.1; G.1.3
\\ ( https://arxiv.org/abs/2308.04185 ,  1992kb)
------------------------------------------------------------------------------
\\
arXiv:2308.05864 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 16:11:58 GMT   (12793kb,D)

Title: The Multi-modality Cell Segmentation Challenge: Towards Universal
  Solutions
Authors: Jun Ma, Ronald Xie, Shamini Ayyadhury, Cheng Ge, Anubha Gupta, Ritu
  Gupta, Song Gu, Yao Zhang, Gihun Lee, Joonkee Kim, Wei Lou, Haofeng Li, Eric
  Upschulte, Timo Dickscheid, Jos\'e Guilherme de Almeida, Yixin Wang, Lin Han,
  Xin Yang, Marco Labagnara, Vojislav Gligorovski, Maxime Scheder, Sahand Jamal
  Rahi, Carly Kempster, Alice Pollitt, Leon Espinosa, T\^am Mignot, Jan Moritz
  Middeke, Jan-Niklas Eckardt, Wangkai Li, Zhaoyang Li, Xiaochen Cai, Bizhe
  Bai, Noah F. Greenwald, David Van Valen, Erin Weisbart, Beth A. Cimini,
  Trevor Cheung, Oscar Br\"uck, Gary D. Bader, and Bo Wang
Categories: eess.IV cs.CV cs.LG q-bio.QM
Comments: NeurIPS22 Cell Segmentation Challenge:
  https://neurips22-cellseg.grand-challenge.org/ . Nature Methods (2024)
DOI: 10.1038/s41592-024-02233-6
\\ ( https://arxiv.org/abs/2308.05864 ,  12793kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09552
replaced with revised version Sat, 30 Mar 2024 03:48:00 GMT   (3096kb,D)

Title: Attesting Distributional Properties of Training Data for Machine
  Learning
Authors: Vasisht Duddu, Anudeep Das, Nora Khayata, Hossein Yalame, Thomas
  Schneider, N. Asokan
Categories: cs.CR cs.LG
Comments: European Symposium on Research in Computer Security (ESORICS), 2024
\\ ( https://arxiv.org/abs/2308.09552 ,  3096kb)
------------------------------------------------------------------------------
\\
arXiv:2308.09730 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 19:28:25 GMT   (853kb)

Title: Virtual imaging trials improved the transparency and reliability of AI
  systems in COVID-19 imaging
Authors: Fakrul Islam Tushar, Lavsen Dahal, Saman Sotoudeh-Paima, Ehsan Abadi,
  W. Paul Segars, Ehsan Samei, Joseph Y. Lo
Categories: eess.IV cs.LG
Comments: 3 tables, 4 figures, 1 Supplement
\\ ( https://arxiv.org/abs/2308.09730 ,  853kb)
------------------------------------------------------------------------------
\\
arXiv:2308.15651
replaced with revised version Sun, 31 Mar 2024 04:20:36 GMT   (73kb,D)

Title: Ensuring User-side Fairness in Dynamic Recommender Systems
Authors: Hyunsik Yoo, Zhichen Zeng, Jian Kang, Ruizhong Qiu, David Zhou,
  Zhining Liu, Fei Wang, Charlie Xu, Eunice Chan, and Hanghang Tong
Categories: cs.IR cs.CY cs.LG
Comments: 19 pages, 20 figures, 2 tables, ACM Web Conference 2024
\\ ( https://arxiv.org/abs/2308.15651 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2308.16859 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 16:03:24 GMT   (204kb,D)

Title: Information Theoretically Optimal Sample Complexity of Learning
  Dynamical Directed Acyclic Graphs
Authors: Mishfad Shaikh Veedu, Deepjyoti Deka, and Murti V. Salapaka
Categories: stat.ML cs.LG cs.SY eess.SY math.OC
Comments: 21 pages. Accepted for publication in AISTATS 2024
\\ ( https://arxiv.org/abs/2308.16859 ,  204kb)
------------------------------------------------------------------------------
\\
arXiv:2309.03835
replaced with revised version Sun, 31 Mar 2024 07:53:19 GMT   (18428kb,D)

Title: Instructing Robots by Sketching: Learning from Demonstration via
  Probabilistic Diagrammatic Teaching
Authors: Weiming Zhi and Tianyi Zhang and Matthew Johnson-Roberson
Categories: cs.RO cs.LG
Comments: To appear in ICRA 2024
\\ ( https://arxiv.org/abs/2309.03835 ,  18428kb)
------------------------------------------------------------------------------
\\
arXiv:2309.10298
replaced with revised version Sat, 30 Mar 2024 00:11:04 GMT   (18802kb,D)

Title: Learning Orbitally Stable Systems for Diagrammatically Teaching
Authors: Weiming Zhi, Tianyi Zhang, Matthew Johnson-Roberson
Categories: cs.RO cs.LG
\\ ( https://arxiv.org/abs/2309.10298 ,  18802kb)
------------------------------------------------------------------------------
\\
arXiv:2309.11798
replaced with revised version Mon, 1 Apr 2024 03:47:40 GMT   (676kb,D)

Title: A Comprehensive Review of Community Detection in Graphs
Authors: Jiakang Li, Songning Lai, Zhihao Shuai, Yuan Tan, Yifan Jia, Mianyang
  Yu, Zichen Song, Xiaokang Peng, Ziyang Xu, Yongxin Ni, Haifeng Qiu, Jiayu
  Yang, Yutong Liu, Yonggang Lu
Categories: cs.SI cs.LG
\\ ( https://arxiv.org/abs/2309.11798 ,  676kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06081 (*cross-listing*)
replaced with revised version Sat, 30 Mar 2024 16:21:39 GMT   (72kb)

Title: Ito Diffusion Approximation of Universal Ito Chains for Sampling,
  Optimization and Boosting
Authors: Aleksei Ustimenko, Aleksandr Beznosikov
Categories: math.OC cs.LG math.PR stat.ML
Comments: Appears in: The Twelfth International Conference on Learning
  Representations (ICLR 2024). 27 pages, 3 tables. Reference:
  https://openreview.net/forum?id=fjpfCOV4ru
\\ ( https://arxiv.org/abs/2310.06081 ,  72kb)
------------------------------------------------------------------------------
\\
arXiv:2310.17584 (*cross-listing*)
replaced with revised version Sat, 30 Mar 2024 10:43:19 GMT   (2764kb,D)

Title: A minimax optimal control approach for robust neural ODEs
Authors: Cristina Cipriani, Alessandro Scagliotti, Tobias W\"ohrer
Categories: math.OC cs.LG cs.SY eess.SY
Comments: 7 pages, 2 figures and 1 table. Correction of typos and improvement
  of Section 4 (Numerics)
\\ ( https://arxiv.org/abs/2310.17584 ,  2764kb)
------------------------------------------------------------------------------
\\
arXiv:2310.18824 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 14:12:15 GMT   (9410kb,D)

Title: Intrinsic Gaussian Vector Fields on Manifolds
Authors: Daniel Robert-Nicoud, Andreas Krause, Viacheslav Borovitskiy
Categories: stat.ML cs.LG
Comments: Version accepted at AISTATS 2024
\\ ( https://arxiv.org/abs/2310.18824 ,  9410kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04808
replaced with revised version Fri, 29 Mar 2024 18:56:23 GMT   (1405kb,D)

Title: A Lightweight Architecture for Real-Time Neuronal-Spike Classification
Authors: Muhammad Ali Siddiqi, David Vrijenhoek, Lennart P. L. Landsmeer, Job
  van der Kleij, Anteneh Gebregiorgis, Vincenzo Romano, Rajendra Bishnoi, Said
  Hamdioui, Christos Strydis
Categories: cs.AR cs.LG eess.SP
Journal-ref: 21st ACM International Conference on Computing Frontiers
  Proceedings, 2024
DOI: 10.1145/3649153.3649186
\\ ( https://arxiv.org/abs/2311.04808 ,  1405kb)
------------------------------------------------------------------------------
\\
arXiv:2311.06234
replaced with revised version Sun, 31 Mar 2024 18:18:05 GMT   (37504kb,D)

Title: EVORA: Deep Evidential Traversability Learning for Risk-Aware Off-Road
  Autonomy
Authors: Xiaoyi Cai, Siddharth Ancha, Lakshay Sharma, Philip R. Osteen,
  Bernadette Bucher, Stephen Phillips, Jiuguang Wang, Michael Everett, Nicholas
  Roy, Jonathan P. How
Categories: cs.RO cs.LG cs.SY eess.SY
Comments: Under review. Journal extension for arXiv:2210.00153. Project
  website: https://xiaoyi-cai.github.io/evora/
\\ ( https://arxiv.org/abs/2311.06234 ,  37504kb)
------------------------------------------------------------------------------
\\
arXiv:2311.13958 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 22:39:12 GMT   (7593kb,D)

Title: Handling The Non-Smooth Challenge in Tensor SVD: A Multi-Objective
  Tensor Recovery Framework
Authors: Jingjing Zheng, Wanglong Lu, Wenzhe Wang, Yankai Cao, Xiaoqin Zhang,
  Xianta Jiang
Categories: stat.ML cs.CV cs.LG
\\ ( https://arxiv.org/abs/2311.13958 ,  7593kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05264
replaced with revised version Sat, 30 Mar 2024 00:11:28 GMT   (617kb,D)

Title: All Rivers Run to the Sea: Private Learning with Asymmetric Flows
Authors: Yue Niu, Ramy E. Ali, Saurav Prakash, Salman Avestimehr
Categories: cs.CR cs.LG
Comments: Camera-ready for CVPR 2024
\\ ( https://arxiv.org/abs/2312.05264 ,  617kb)
------------------------------------------------------------------------------
\\
arXiv:2312.08255 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 09:33:50 GMT   (18042kb,D)

Title: OCTDL: Optical Coherence Tomography Dataset for Image-Based Deep
  Learning Methods
Authors: Mikhail Kulyabin, Aleksei Zhdanov, Anastasia Nikiforova, Andrey
  Stepichev, Anna Kuznetsova, Mikhail Ronkin, Vasilii Borisov, Alexander
  Bogachev, Sergey Korotkich, Paul A Constable, and Andreas Maier
Categories: eess.IV cs.CV cs.LG
DOI: 10.1038/s41597-024-03182-7
\\ ( https://arxiv.org/abs/2312.08255 ,  18042kb)
------------------------------------------------------------------------------
\\
arXiv:2401.00909
replaced with revised version Fri, 29 Mar 2024 18:04:37 GMT   (11516kb,D)

Title: Taming Mode Collapse in Score Distillation for Text-to-3D Generation
Authors: Peihao Wang, Dejia Xu, Zhiwen Fan, Dilin Wang, Sreyas Mohan, Forrest
  Iandola, Rakesh Ranjan, Yilei Li, Qiang Liu, Zhangyang Wang, Vikas Chandra
Categories: cs.CV cs.LG
Comments: Project page: https://vita-group.github.io/3D-Mode-Collapse/
\\ ( https://arxiv.org/abs/2401.00909 ,  11516kb)
------------------------------------------------------------------------------
\\
arXiv:2401.04071
replaced with revised version Sat, 30 Mar 2024 12:05:52 GMT   (867kb,D)

Title: Fun with Flags: Robust Principal Directions via Flag Manifolds
Authors: Nathan Mankovich, Gustau Camps-Valls, Tolga Birdal
Categories: cs.CV cs.LG math.DG math.OC stat.ML
\\ ( https://arxiv.org/abs/2401.04071 ,  867kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08409 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 19:01:07 GMT   (5524kb,D)

Title: Faster ISNet for Background Bias Mitigation on Deep Neural Networks
Authors: Pedro R. A. S. Bassi, Sergio Decherchi and Andrea Cavalli
Categories: eess.IV cs.CV cs.CY cs.LG
\\ ( https://arxiv.org/abs/2401.08409 ,  5524kb)
------------------------------------------------------------------------------
\\
arXiv:2401.09627 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 17:03:08 GMT   (638kb)

Title: SymTC: A Symbiotic Transformer-CNN Net for Instance Segmentation of
  Lumbar Spine MRI
Authors: Jiasong Chen, Linchen Qian, Linhai Ma, Timur Urakov, Weiyong Gu, Liang
  Liang
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2401.09627 ,  638kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07365 (*cross-listing*)
replaced with revised version Sat, 30 Mar 2024 09:29:01 GMT   (2132kb,D)

Title: A Deep Learning Method for Optimal Investment Under Relative Performance
  Criteria Among Heterogeneous Agents
Authors: Mathieu Lauri\`ere, Ludovic Tangpi, Xuchen Zhou
Categories: math.OC cs.GT cs.LG
\\ ( https://arxiv.org/abs/2402.07365 ,  2132kb)
------------------------------------------------------------------------------
\\
arXiv:2402.07868 (*cross-listing*)
replaced with revised version Sat, 30 Mar 2024 08:31:53 GMT   (73kb)

Title: Nesting Particle Filters for Experimental Design in Dynamical Systems
Authors: Sahel Iqbal, Adrien Corenflos, Simo S\"arkk\"a, Hany Abdulsamad
Categories: stat.ML cs.LG stat.ME
\\ ( https://arxiv.org/abs/2402.07868 ,  73kb)
------------------------------------------------------------------------------
\\
arXiv:2402.09075
replaced with revised version Mon, 1 Apr 2024 02:09:32 GMT   (137kb,D)

Title: Steady-State Error Compensation for Reinforcement Learning with
  Quadratic Rewards
Authors: Liyao Wang, Zishun Zheng and Yuan Lin
Categories: eess.SY cs.LG cs.SY
\\ ( https://arxiv.org/abs/2402.09075 ,  137kb)
------------------------------------------------------------------------------
\\
arXiv:2402.16326 (*cross-listing*)
replaced with revised version Sun, 31 Mar 2024 08:45:51 GMT   (10841kb,D)

Title: A Provably Accurate Randomized Sampling Algorithm for Logistic
  Regression
Authors: Agniva Chowdhury, Pradeep Ramuhalli
Categories: stat.ML cs.DS cs.LG
Comments: Published in the proceedings of AAAI 2024
DOI: 10.1609/aaai.v38i10.29042
\\ ( https://arxiv.org/abs/2402.16326 ,  10841kb)
------------------------------------------------------------------------------
\\
arXiv:2403.00344
replaced with revised version Mon, 1 Apr 2024 08:29:44 GMT   (1713kb,D)

Title: Robustifying a Policy in Multi-Agent RL with Diverse Cooperative
  Behaviors and Adversarial Style Sampling for Assistive Tasks
Authors: Takayuki Osa and Tatsuya Harada
Categories: cs.RO cs.LG
Comments: 7 pages, accepted for ICRA 2024
\\ ( https://arxiv.org/abs/2403.00344 ,  1713kb)
------------------------------------------------------------------------------
\\
arXiv:2403.13300 (*cross-listing*)
replaced with revised version Sat, 30 Mar 2024 16:58:59 GMT   (977kb,D)

Title: Kernel Multigrid: Accelerate Back-fitting via Sparse Gaussian Process
  Regression
Authors: Lu Zou and Liang Ding
Categories: stat.ML cs.LG
\\ ( https://arxiv.org/abs/2403.13300 ,  977kb)
------------------------------------------------------------------------------
\\
arXiv:2403.14671
replaced with revised version Sat, 30 Mar 2024 16:25:21 GMT   (517kb)

Title: Understanding the Transit Gap: A Comparative Study of On-Demand Bus
  Services and Urban Climate Resilience in South End, Charlotte, NC and
  Avondale, Chattanooga, TN
Authors: Sanaz Sadat Hosseini, Babak Rahimi Ardabili, Mona Azarbayjani,
  Srinivas Pulugurtha, Hamed Tabkhi
Categories: cs.CY cs.LG physics.soc-ph
Comments: 6 pages, 4 figures, Recently accepted by the PLEA 2024 Conference for
  Sustainable Architecture and Urban Design (Re-Thinking Resilience), Wroclaw,
  Poland
\\ ( https://arxiv.org/abs/2403.14671 ,  517kb)
------------------------------------------------------------------------------
\\
arXiv:2403.17701 (*cross-listing*)
replaced with revised version Mon, 1 Apr 2024 02:31:10 GMT   (984kb,D)

Title: Rotate to Scan: UNet-like Mamba with Triplet SSM Module for Medical
  Image Segmentation
Authors: Hao Tang, Lianglun Cheng, Guoheng Huang, Zhengguang Tan, Junhao Lu and
  Kaihong Wu
Categories: eess.IV cs.CV cs.LG
\\ ( https://arxiv.org/abs/2403.17701 ,  984kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
